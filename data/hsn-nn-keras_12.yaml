- en: Generative Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: In the last chapter, we submerged ourselves in the world of autoencoding neural
    networks. We saw how these models can be used to estimate parameterized functions
    capable of reconstructing given inputs with respect to target outputs. While at
    prima facie this may seem trivial, we now know that this manner of self-supervised
    encoding has several theoretical and practical implications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们沉浸在自编码神经网络的世界里。我们看到了这些模型如何用于估计能够根据目标输出重建给定输入的参数化函数。虽然乍一看这似乎很简单，但我们现在知道，这种自监督编码方式具有多种理论和实际意义。
- en: 'In fact, from a **machine learning** (**ML**) perspective, the ability to approximate
    a connected set of points in a higher dimensional space on to a lower dimensional
    space (that is, manifold learning) has several advantages, ranging from higher
    data storage efficiency to more efficient memory consumption. Practically speaking,
    this allows us to discover ideal coding schemes for different types of data, or
    to perform dimensionality reduction thereupon, for use cases such as **Principal
    Component Analysis** (**PCA**) or even information retrieval. The task of searching
    for specific information using similar queries, for example, can be largely augmented
    by learning useful representations from a set of data, stored in a lower dimensional
    space. Moreover, the learned representations can even be used thereafter as feature
    detectors to classify new, incoming data. This sort of application may allow us
    to construct powerful databases capable of high-level inference and reasoning,
    when presented with a query. Derivative implementations may include legal databases
    used by lawyers to efficiently search for precedents by similarity to the current
    case, or medical systems that allow doctors to efficiently diagnose patients based
    on the noisy data available per patient. These latent variable models allow researchers
    and businesses alike to address various use cases, ranging from sequence-to-sequence
    machine translation, to attributing complex intents to customer reviews. Essentially,
    with generative models, we attempt to answer this question: *How likely are these
    features (*x*) present in an instance of data, given that it belongs to a certain
    class (*y*)*? This is very different than asking this question: *How likely is
    this instance part of a class (*y*), given the features (*x*) present?*, as we
    would for supervised learning tasks. To better understand this reversal of roles,
    we will further explore the idea behind latent variable modeling, introduced in
    the previous chapter.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，从**机器学习**（**ML**）的角度来看，能够将高维空间中的一组连接点近似到低维空间（即流形学习）具有多个优点，从更高的数据存储效率到更高效的内存消耗。实际上，这使我们能够为不同类型的数据发现理想的编码方案，或在此基础上执行降维，以应用于**主成分分析**（**PCA**）或信息检索等用例。例如，使用相似查询搜索特定信息的任务可以通过从低维空间中学习有用的表示来大大增强。此外，学习到的表示还可以作为特征检测器，用于分类新的、传入的数据。这种应用可能使我们能够构建强大的数据库，在面对查询时能够进行高级推理和推断。衍生的实现包括律师用来根据当前案件的相似性高效检索先例的法律数据库，或允许医生根据每个患者的噪声数据高效诊断患者的医疗系统。这些潜变量模型使研究人员和企业能够解决各种用例，从序列到序列的机器翻译，到将复杂意图归因于客户评论。实质上，使用生成模型，我们试图回答这个问题：*在给定数据实例属于某个类别（*y*）的情况下，这些特征（*x*）出现的可能性有多大？*
    这与我们在监督学习任务中会问的这个问题是完全不同的：*在给定特征（*x*）的情况下，这个实例属于类别（*y*）的可能性有多大？* 为了更好地理解这种角色的反转，我们将进一步探讨前一章中介绍的潜变量建模的理念。
- en: In this chapter, we will see how we can take the concept of latent variables
    a step further. Instead of simply learning a parameterized function, which maps
    inputs to outputs, we can use neural networks to learn a function that represents
    the probability distribution over the latent space. We can then sample from such
    a probability distribution to generate novel, synthetic instances of the input
    data. This is the core theoretical foundation behind generative modeling, as we
    are about to discover.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何进一步发展潜变量的概念。我们不再只是简单地学习一个参数化的函数，它将输入映射到输出，而是可以使用神经网络来学习一个表示潜在空间中概率分布的函数。然后，我们可以从这个概率分布中采样，生成新的、合成的输入数据实例。这就是生成模型背后的核心理论基础，正如我们即将发现的那样。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Replicating versus generating content
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制与生成内容
- en: Understand the notion of latent space
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解潜在空间的概念
- en: Diving deeper into generative networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解生成网络
- en: Using randomness to augment outputs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机性来增强输出
- en: Sampling from the latent space
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从潜在空间进行采样
- en: Understanding types of Generative Adversial Networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解生成对抗网络的类型
- en: Understanding VAEs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解变分自编码器（VAE）
- en: Designing VAEs in Keras
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中设计变分自编码器（VAE）
- en: Building the encoding module in a VAE
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在VAE中构建编码模块
- en: Building the decoder module
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解码器模块
- en: Visualizing the latent space
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化潜在空间
- en: Latent space sampling and output generation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在空间采样与输出生成
- en: Exploring GANs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索生成对抗网络（GANs）
- en: Diving deeper into GANs
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解生成对抗网络（GANs）
- en: Designing a GAN in Keras
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中设计生成对抗网络（GAN）
- en: Designing the generator module
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计生成器模块
- en: Designing the discriminator module
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计判别器模块
- en: Putting the GAN together
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整合生成对抗网络（GAN）
- en: The training function
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练函数
- en: Defining the discriminator labels
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义判别器标签
- en: Training the generator per batch
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按批次训练生成器
- en: Executing the training session
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行训练会话
- en: Replicating versus generating content
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制与生成内容
- en: 'While our autoencoding use cases in the last chapter were limited to image
    reconstruction and denoising, these use cases are quite distinct from the one
    we are about to address in this chapter. So far, we made our autoencoders reconstruct
    some given inputs, by learning an arbitrary mapping function. In this chapter,
    we want to understand how to train a model to create new instances of some content,
    instead of simply replicating its inputs. In other words, what if we asked a neural
    network to truly be creative and generate content just like human beings do?.
    Can this even be achieved? The canonical answer common in the realm of **Artificial
    Intelligence** (**AI**) is yes, but it is complicated. In the search for a more
    detailed answer, we arrive at the topic of this chapter: generative networks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在上一章中的自动编码应用仅限于图像重建和去噪，这些应用与我们将在本章讨论的案例有很大区别。到目前为止，我们让自动编码器通过学习任意映射函数来重建某些给定的输入。在本章中，我们希望理解如何训练一个模型来创造某些内容的新实例，而不仅仅是复制它的输入。换句话说，如果我们要求神经网络像人类一样真正具备创造力并生成内容，这是否可行？在**人工智能**（**AI**）领域，标准答案是肯定的，但过程相当复杂。在寻找更详细的答案时，我们来到了本章的主题：生成网络。
- en: While a plethora of generative networks exist, ranging from the variations of
    the **Deep Boltzman Machine** to **Deep Belief Networks**, most of them have fallen
    out of fashion, given their restrictive applicability and the appearance of more
    computationally efficient methods. A few, however, continue to remain in the spotlight,
    due to their eerie ability to generate synthetic content, such as faces that have
    never existed, movie reviews and news articles that were never written, or videos
    that were never actually filmed! To better understand the mechanics behind this
    wizardry, let's dedicate a few lines to the notion of latent spaces, to better
    understand how these models transform their learned representations to create
    something seemingly new.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在众多生成网络，从**深度玻尔兹曼机**到**深度信念网络**的变种，但大多数已经失去了流行，原因在于它们的适用性有限且出现了更具计算效率的方法。然而，少数几种依然处于焦点之中，因为它们具备生成合成内容的神奇能力，比如从未存在过的面孔、从未写过的电影评论和新闻文章，或是从未拍摄过的视频！为了更好地理解这些魔术背后的机制，我们将花几行文字来介绍潜在空间的概念，从而更好地理解这些模型如何转化其学习到的表示，创造出看似全新的东西。
- en: Understanding the notion of latent space
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解潜在空间的概念
- en: Recall from the previous chapter that a **latent space** is nothing but a compressed
    representation of the input data in a lower dimensional space. It essentially
    includes features that are crucial to the identification of the original input.
    To better understand this notion, it is helpful to try to mentally visualize what
    type of information may be encoded by the latent space. A useful analogy can be
    to think of how we ourselves create content, with our imagination. Suppose you
    were asked to create an imaginary animal. What information would you be relying
    on to create this creature? You will sample features from animals you have previously
    seen, features such as their color, or whether they are bi-pedal, quadri-pedal,
    a mammal or reptile, land-or sea-dwelling, and so on. As it turns out, we ourselves
    develop latent models of the world, as we navigate through it. When we attempt
    to imagine a new instance of a class, we are actually sampling some latent variable
    models, learned throughout the course of our existence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾上一章，**潜在空间**不过是输入数据在低维空间中的压缩表示。它本质上包含了对于识别原始输入至关重要的特征。为了更好地理解这个概念，尝试在脑海中可视化潜在空间可能编码的那些信息是很有帮助的。一个有用的类比是考虑我们如何用想象力创造内容。假设你被要求创造一个虚构的动物，你会依赖哪些信息来创造这个生物？你将从之前见过的动物身上采样特征，比如它们的颜色，或者它们是双足的，四足的，是哺乳动物还是爬行动物，生活在陆地还是海洋等。事实证明，我们自己也在世界中航行时，逐渐发展出潜在的世界模型。当我们尝试想象某个类别的新实例时，实际上是在采样一些潜在变量模型，这些模型是在我们存在的过程中学习得到的。
- en: Think about it. Throughout our lives, we came across countless animals of different
    colors, sizes, and morphologies. We reduce these rich representations to more
    manageable dimensions all the time. For example, we all know what a lion looks
    like, because we have mentally encoded properties (or latent variables) that represent
    a lion (such as their four legs, tail, furry coat, color, and so on). These learned
    properties are a testament to how we store information in lower dimensions, to
    create functional models of the world around us. We hypothesize that such information
    is stored in lower dimensions, as most of us, for example, are not able to perfectly
    recreate the image of a lion on paper. Some may not even come close to it, as
    is the case for the author of this work. Yet, we are all instantly and collectively
    able to agree on what the general morphology of a lion would be, just by mentioning
    the word *lion*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想。在我们的一生中，我们遇到了无数种不同颜色、大小和形态的动物。我们不断地将这些丰富的表示减少到更可管理的维度。例如，我们都知道狮子长什么样，因为我们已经在脑海中编码了代表狮子的属性（或潜在变量），比如它们的四条腿、尾巴、毛茸茸的皮毛、颜色等。这些学习到的属性证明了我们如何在低维空间中存储信息，从而创造出周围世界的功能性模型。我们假设，这些信息是存储在低维空间中的，因为大多数人，例如，无法在纸上完美重现狮子的形象。有些人甚至无法接近，这对于本书的作者来说就是如此。然而，我们所有人只要提到*狮子*这个词，都会立即并集体地同意狮子的总体形态。
- en: Identifying concept vectors
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别概念向量
- en: 'This little thought experiment demonstrates the sheer power of latent variable
    models, in creating functional representations of the world. Our brain would very
    likely consume a lot more than the meagre 12 watts of energy, were it not constantly
    downsampling the information received from our sensory inputs to create manageable
    and realistic models of the world. Thus, using latent variable models essentially
    allows us to query reduced representation (or properties) of the input, which
    may in turn be recombined with other representations to generate a seemingly novel
    output (for example: unicorn = body and face from horse + horn from rhino/narwhal).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小小的思维实验展示了潜在变量模型在创造世界的功能性表示方面的巨大力量。如果我们的脑袋没有不断将从感官输入接收到的信息进行下采样，以创造可管理且现实的世界模型，它很可能会消耗远超过那可怜的12瓦特的能量。因此，使用潜在变量模型本质上允许我们查询输入的简化表示（或属性），这些表示可能会与其他表示重新组合，从而生成看似新颖的输出（例如：独角兽
    = 来自马的身体和脸 + 来自犀牛/独角鲸的角）。
- en: Similarly, neural networks may also transform samples from a learned latent
    space, to generate novel content. One way of achieving this is by identifying
    concept vectors, embedded in the learned latent space. The idea here is quite
    simple. Suppose we are to sample a face (*f*) from a latent space representing
    faces. Then, another point, (*f + c*), can be thought of as the embedded representation
    of the same face, along with some modification (that is, the presence of a smile,
    or glasses, or facial hair, on top of the original face). These concept vectors
    essentially encode various axes of disparities from the input data, and can then
    be used to alter interesting properties of the input images. In other words, we
    can probe the latent space for vectors that elude to a concept present within
    the input data. After identifying such vectors, we can then modify them to change
    properties of the input data. A smile vector, for example, can be learned and
    used to modify the degree to which a person is smiling, in a given image. Similarly,
    a gender vector could be used to modify the appearance of a person, to look more
    female, than male, or vice versa. Now that we have a better idea of what kind
    of information may be queried from latent spaces, and subsequently be modified
    to generate new content, we can continue on our exploratory journey.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，神经网络也可以从学习到的潜在空间中转换样本，以生成新的内容。实现这一目标的一种方法是识别嵌入在学习到的潜在空间中的概念向量。这里的想法非常简单。假设我们要从代表面孔的潜在空间中采样一个面孔（*f*）。然后，另一个点（*f
    + c*）可以被认为是相同面孔的嵌入式表示，并且包含某些修改（即在原始面孔上添加微笑、眼镜或面部毛发）。这些概念向量本质上编码了输入数据的各个差异维度，随后可以用于改变输入图像的某些有趣特性。换句话说，我们可以在潜在空间中寻找与输入数据中存在的概念相关的向量。在识别出这些向量后，我们可以修改它们，以改变输入数据的特征。例如，微笑向量可以被学习并用来修改某人在图像中的微笑程度。同样，性别向量可以用来修改某人的外观，使其看起来更女性化或男性化，反之亦然。现在，我们对潜在空间中可能查询到的信息以及如何修改这些信息以生成新内容有了更好的理解，我们可以继续我们的探索之旅。
- en: Diving deeper into generative networks
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨生成网络
- en: So, let's try to understand the core mechanics of generative networks and how
    such approaches differ from the ones we already know. In our quest thus far, most
    of the networks we have implemented are for the purpose of executing a deterministic
    transformation of some inputs, in order to get to some sort of outputs. It was
    not until we explored the topic of reinforcement learning ([Chapter 7](f011d850-2ed4-4506-8fc9-4930e6a85d85.xhtml),
    *Reinforcement Learning with Deep Q-Networks*) that we learned the benefits of
    introducing a degree of **stochasticity** (that is, randomness) to our modeling
    efforts. This is a core notion that we will be further exploring as we familiarize
    ourselves with the manner in which generative networks function. As we mentioned
    earlier, the central idea behind generative networks is to use a deep neural network
    to learn the probability distribution of variables over a reduced latent space.
    Then, the latent space can be sampled and transformed in a quasi-random manner,
    to generate some outputs (*y*).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们尝试理解生成网络的核心机制，以及这种方法与我们已知的其他方法有何不同。到目前为止，我们实现的大多数网络都是为了执行某些输入的确定性变换，从而得到某种输出。直到我们探讨强化学习的话题（[第7章](f011d850-2ed4-4506-8fc9-4930e6a85d85.xhtml)，*使用深度
    Q 网络进行强化学习*），我们才了解了将一定程度的**随机性**（即随机因素）引入建模过程的好处。这是一个核心概念，我们将在进一步熟悉生成网络如何运作的过程中进行探讨。正如我们之前提到的，生成网络的核心思想是使用深度神经网络来学习在简化的潜在空间中，变量的概率分布。然后，可以以准随机的方式对潜在空间进行采样和转换，从而生成一些输出（*y*）。
- en: As you may notice, this is quite different than the approach we employed in
    the previous chapter. With autoencoders, we simply estimated an arbitrary function,
    mapping inputs (*x*) to a compressed latent space using an encoder, from which
    we reconstructed outputs (*y*) using a decoder. In the case of generative networks,
    we instead learn a latent variable model for our input data (*x*). Then, we can
    transform samples from the latent space to get to our generated output. Neat,
    don't you think? Yet, before we further explore how this concept is operationalized,
    let's briefly go over the role of randomness in relation to generating creative
    content.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所注意到的，这与我们在上一章中采用的方法有很大的不同。对于自编码器，我们只是估计了一个任意函数，通过编码器将输入（*x*）映射到压缩的潜在空间，再通过解码器重构输出（*y*）。而在生成对抗网络中，我们则学习输入数据（*x*）的潜在变量模型。然后，我们可以将潜在空间中的样本转化为生成的输出。不错吧？然而，在我们进一步探讨如何将这个概念操作化之前，让我们简要回顾一下随机性在生成创造性内容方面的作用。
- en: Controlled randomness and creativity
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受控随机性与创造力
- en: 'Recall that we introduced an element of randomness in the deep reinforcement
    learning algorithm by using the **epsilon greedy selection** strategy, which basically
    allowed our network to not rely too much on the same actions and allowed it to
    explore new actions to solve the given environment. Introducing this randomness,
    in a sense, brought creativity to the process, as our network was able to systematically
    create novel state-action pairs without relying on what it had learned previously
    learned. Do note, however, that labeling the consequence of introducing randomness
    in a system as creativity may be the result of some anthropomorphism on our part.
    In fact, the true processes that gave birth to creativity in humans (our go-to
    benchmark) are still vastly elusive and poorly understood by the scientific community
    at large. On the other hand, this link between randomness and creativity itself
    is a long recognized one, especially in the realm of AI. As early as 1956, AI
    researchers have been interested in transcending the seemingly deterministic limitations
    of machines. Back then, the prominence of rule-based systems made it seem as though
    notions such as creativity could only be observed in advanced biological organisms.
    Despite this widespread belief, one of the paramount documents that shaped AI
    history (arguably for the following century to come), the *Dartmouth Summer Research
    Project Proposal* (1956), specifically mentioned the role of controlled randomness
    in AI systems, and its link to generating creative content. While we encourage
    you to read the entire document, we present an extract from it that is relevant
    to the point at hand:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们通过使用**epsilon贪心选择**策略在深度强化学习算法中引入了随机性，这基本上使我们的网络不再过度依赖相同的动作，而是能够探索新动作来解决给定的环境。引入这种随机性，从某种意义上说，为这个过程带来了创造性，因为我们的网络能够系统地创建新的状态-动作对，而不依赖于之前学到的知识。然而需要注意的是，将引入随机性的后果称为创造力，可能是我们某种程度上的拟人化。实际上，赋予人类（我们的基准）创造力的真实过程仍然极为难以捉摸，并且科学界对其理解甚少。另一方面，随机性与创造力之间的联系早已得到认可，特别是在人工智能领域。早在1956年，人工智能研究者就对超越机器看似决定性的局限性产生了兴趣。当时，基于规则的系统的突出地位使得人们认为，诸如创造力这样的概念只能在高级生物有机体中观察到。尽管有这种广泛的信念，但塑造人工智能历史的最重要文件之一（可以说影响了未来一个世纪），即*达特茅斯夏季研究项目提案*（1956年），特别提到了受控随机性在人工智能系统中的作用，以及它与生成创造性内容的联系。虽然我们鼓励你阅读整个文件，但我们提取了其中与当前话题相关的部分：
- en: '"A fairly attractive and yet clearly incomplete conjecture is that the difference
    between creative thinking and unimaginative competent thinking lies in the injection
    of some randomness. The randomness must be guided by intuition to be efficient.
    In other words, the educated guess or the hunch include controlled randomness
    in otherwise orderly thinking."'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: “一个相当有吸引力但显然不完整的猜想是，创造性思维与缺乏想象力的有能力的思维之间的区别在于引入了一些随机性。这些随机性必须通过直觉来指导，以提高效率。换句话说，教育性猜测或直觉包括在其他有序思维中引入受控的随机性。”
- en: '*- John McCarthy, Marvin L Minsky, Nathaniel Rochester, and Claude E Shannon*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*约翰·麦卡锡、马文·L·明斯基、内森尼尔·罗切斯特和克劳德·E·香农*'
- en: Using randomness to augment outputs
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机性来增强输出
- en: Over the years, we developed methods that operationalize this notion of injecting
    some controlled randomness, which in a sense are guided by the intuition of the
    inputs. When we speak of generative models, we essentially wish to implement a
    mechanism that allows controlled and quasi-randomized transformations of our input,
    to generate something new, yet still plausibly resembling the original input.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，我们开发了可以实现这种注入受控随机性的机制，这些方法在某种程度上是由输入的直觉引导的。当我们谈论生成模型时，本质上是希望实现一种机制，允许我们对输入进行受控且准随机的变换，从而生成新的东西，但仍然在可行的范围内与原始输入相似。
- en: Let's consider for a moment how this can be achieved. We wish to train a neural
    network to use some input variables (*x*) to generate some output variables (*y*),
    from a latent space produced by a model. An easy way to solve this is to simply
    add an element of randomness as input to our generator network, defined here by
    the variable (*z*). The value of *z* may be sampled from some probability distribution
    (a Gaussian distribution, for example) and fed to a neural network along with
    the inputs. Hence, this network will actually be estimating the function *f(x,
    z)* and not simply *f(x)*. Naturally, to an independent observer who is not able
    to measure the value of *z*, this function will seem stochastic, yet this will
    not be the case in reality.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间考虑一下如何实现这一点。我们希望训练一个神经网络，利用一些输入变量（*x*）生成一些输出变量（*y*），这些输出来自模型生成的潜在空间。解决这一问题的一种简单方法是向我们的生成器网络输入添加一个随机性元素，这里由变量（*z*）定义。*z*的值可以从某个概率分布中抽样（例如，高斯分布），并与输入一起传递给神经网络。因此，这个网络实际上是在估计函数*f(x,
    z)*，而不仅仅是*f(x)*。自然地，对于一个无法测量*z*值的独立观察者来说，这个函数看起来是随机的，但在现实中并非如此。
- en: Sampling from the latent space
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从潜在空间进行采样
- en: 'To further elaborate, suppose we had to draw some samples (*y*) from a probability
    distribution of variables from a latent space, with a mean of (μ) and a variance
    of (σ2):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步说明，假设我们必须从潜在空间的变量的概率分布中抽取一些样本（*y*），其中均值为（μ），方差为（σ2）：
- en: '**Sampling operation**: *y ̴ N(μ , σ2)*'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样操作**：*y ̴ N(μ , σ2)*'
- en: 'Since we use a sampling process to draw from this distribution, each individual
    sample may change every time the process is queried. We can''t exactly differentiate
    the generated sample (*y*) with respect to the distribution parameters (μ and
    σ2), since we are dealing with a sampling operation, and not a function. So, how
    exactly can we backpropagate our model''s errors? Well, one solution could be
    to redefine the sampling process, such as performing a transformation on a random
    variable (*z*), to get to our generated output (*y*), like so:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用采样过程从该分布中抽取样本，每次查询该过程时，每个单独的样本可能会发生变化。我们无法准确地根据分布参数（μ 和 σ2）对生成的样本（*y*）进行求导，因为我们处理的是采样操作，而不是函数。那么，我们究竟如何进行反向传播模型的误差呢？一种解决方法可能是重新定义采样过程，例如对随机变量（*z*）进行变换，以得到我们的生成输出（*y*），如下所示：
- en: '**Sampling equation**: *y = μ + σz*'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样方程**：*y = μ + σz*'
- en: This is a crucial step, as we can now use the backpropagation algorithm to compute
    gradients of the generated output (*y*), with respect to the sampling operation
    itself *(μ + σz)*. What changed? Essentially, we are now treating the sampling
    operation as a deterministic one that includes the mean(μ) and standard deviation
    (σ) from our probability distribution, as well as a random variable (*z*), whose
    distribution is not related to that of any of the other variables we seek to estimate.
    We use this method to estimate how changes in our distribution's mean (μ) or standard
    deviation (σ) affect the generated output (*y*), given that the sampling operation
    is reproduced with the same value of *z*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关键步骤，因为现在我们可以使用反向传播算法来计算生成输出（*y*）相对于采样操作本身（*(μ + σz)*）的梯度。有什么变化？本质上，我们现在将采样操作视为一个确定性操作，它包含了概率分布中的均值（μ）和标准差（σ），以及一个与我们要估计的其他变量分布无关的随机变量（*z*）。我们使用这种方法来估计当我们分布的均值（μ）或标准差（σ）发生变化时，如何影响生成输出（*y*），前提是采样操作以相同的*z*值被重现。
- en: Learning a probability distribution
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习概率分布
- en: 'Since we can now backpropagate through the sampling operation, we can include
    this step as part of a larger network. By plugging this into a larger network,
    we can then redefine the parameters of the earlier sampling operation (μ and σ),
    as functions that can be estimated by parts of this larger neural network! More
    mathematically put, we can redefine the mean and standard deviation of the probability
    distribution as functions that can be approximated by the parameters of a neural
    network (for example, *μ = f(x ;θ)* and *σ = g(x; θ)*, where the term *θ* denotes
    the learnable parameters of a neural network). We can then use these defined functions
    to generate an output (*y*):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们现在可以通过采样操作进行反向传播，我们就可以将这一步骤作为更大网络的一部分。将其插入到更大的网络中后，我们可以重新定义早期采样操作（μ和σ）的参数，作为可以通过这个更大神经网络的部分来估算的函数！更数学化地说，我们可以将概率分布的均值和标准差重新定义为可以通过神经网络参数（例如，*μ
    = f(x ;θ)* 和 *σ = g(x; θ)*，其中*θ*表示神经网络的可学习参数）来逼近的函数。然后，我们可以使用这些定义的函数来生成输出（*y*）：
- en: '**Sample function**: *y = μ + σz*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样函数**：*y = μ + σz*'
- en: In this function, *μ = f(x ;θ)* and *σ = g(x; θ)*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，*μ = f(x ;θ)* 和 *σ = g(x; θ)*。
- en: Now that we know how to sample outputs (*y*), we can finally train our larger
    network by differentiating a defined loss function, *J(y)*, with respect to these
    outputs. Recall that we use the chain rule of differentiation to redefine this
    process with respect to the intermediate layers, which here represent the parameterized
    functions (*μ* and *σ*). Hence, differentiating this loss function provides us
    with its derivatives, used to iteratively update the parameters of the network,
    where the parameters themselves represent a probability distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何对输出进行采样（*y*），我们可以通过对定义的损失函数*J(y)*进行微分，来最终训练我们的更大网络。回想一下，我们使用链式法则重新定义这个过程，关于中间层，这些中间层在此表示参数化的函数（*μ*和*σ*）。因此，微分这个损失函数可以得到它的导数，利用这些导数迭代更新网络的参数，而这些参数本身代表了一个概率分布。
- en: Great! Now we have an overarching theoretical understanding of how these models
    can generate outputs. This entire process permits us to first estimate, and then
    sample from, a probability distribution of densely encoded variables, generated
    by an encoder function. Later in the chapter, we will further explore how different
    generative networks learn by benchmarking their outputs, and perform weight updates
    using the backpropagation algorithm.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在我们有了关于这些模型如何生成输出的全面理论理解。这个整个过程允许我们首先估计，然后从由编码器函数生成的密集编码变量的概率分布中采样。在本章后面，我们将进一步探讨不同生成网络如何通过对比其输出进行学习，并使用反向传播算法进行权重更新。
- en: Understanding types of generative networks
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解生成网络的类型
- en: So, all we are actually doing here is generating an output by transforming a
    sample taken from the probability distribution representing the encoded latent
    space. In the last chapter, we saw how to produce such a latent space from some
    input data using encoding functions. In this chapter, we will see how to learn
    a continuous latent space (*l*), then sample from it to generate novel outputs.
    To do this, we essentially learn a differentiable generator function, *g (l ;
    θ(g) ),* which transforms samples from a continuous latent space (*l*) to generate
    an output. Here, this function itself is what is being approximated by the neural
    network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们实际上在做的就是通过转换从表示编码潜在空间的概率分布中采样得到的样本来生成输出。在上一章中，我们展示了如何使用编码函数从一些输入数据生成这样的潜在空间。在本章中，我们将展示如何学习一个连续的潜在空间（*l*），然后从中采样以生成新的输出。为了实现这一点，我们本质上学习一个可微分的生成器函数，*g
    (l ; θ(g))，* 该函数将来自连续潜在空间（*l*）的样本转换为输出。在这里，神经网络实际上在逼近这个函数本身。
- en: The family of generative networks includes both **Variational Autoencoders**
    (**VAEs**) as well as **Generative Adversarial Networks** (**GANs**). As we mentioned
    before, there exist many types of generative models, but in this chapter, we will
    focus on these two variations, given their widespread applicability across various
    cognitive tasks (such as, computer vision and natural language generation). Notably,
    VAEs distinguish themselves by coupling the generator network with an approximate
    inference network, which is simply the encoding architecture we saw in the last
    chapter. GANs, on the other hand, couple the generator network with a separate
    discriminator network, which receives samples both from the actual training data
    and the generated outputs, and is tasked with distinguishing the original image
    from the computer-generated one. Once the generator is considered fooled, your
    GAN is considered trained. Essentially, these two different types of generative
    models employ different methodologies for learning the latent space. This gives
    each of them unique applicability for different types of use cases. For example,
    VAEs perform notably well at learning well-structured spaces, where significant
    variations may be encoded due to the specific composition of the input data (as
    we will see shortly, using the MNIST dataset). However, VAEs also suffer from
    blurry reconstructions, the causes of which are not yet properly understood. GANs,
    on the other hand, do much better at generating realistic content, despite sampling
    from an unstructured and discontinuous latent space, as we will see later in the
    chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网络家族包括**变分自编码器**（**VAEs**）和**生成对抗网络**（**GANs**）。如前所述，存在许多类型的生成模型，但在本章中，我们将重点讨论这两种变体，因为它们在各种认知任务（如计算机视觉和自然语言生成）中具有广泛的应用性。值得注意的是，VAEs通过将生成网络与近似推断网络结合，来区分自己，而近似推断网络其实就是我们在上一章看到的编码架构。另一方面，GANs将生成网络与独立的判别网络结合，后者接收来自实际训练数据和生成输出的样本，并负责区分原始图像与计算机生成的图像。一旦生成器被认为“骗过”了判别器，你的GAN就被认为训练完成了。本质上，这两种不同类型的生成模型采用不同的学习潜在空间的方法，这使得它们在不同类型的使用场景中具有独特的适用性。例如，VAEs在学习良好结构化的空间方面表现出色，在这些空间中，由于输入数据的特定组成，可能会编码出显著的变化（正如我们稍后会看到，使用MNIST数据集时）。然而，VAEs也存在模糊重建的问题，其原因尚未得到充分理解。相比之下，GANs在生成逼真内容方面表现得更好，尽管它们是从一个无结构且不连续的潜在空间进行采样的，正如我们将在本章后面看到的那样。
- en: Understanding VAEs
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解变分自编码器（VAEs）
- en: Now we have a high-level understanding of what generative networks entail, we
    can focus on a specific type of generative models. One of them is the VAE, proposed
    by both Kingma and Welling (2013) as well as Rezende, Mohamed, and Wierstra (2014). 
    This model is actually very similar to the autoencoders we saw in the last chapter,
    but they come with a slight twist—well, several twists, to be more specific. For
    one, the latent space being learned is no longer a discrete one, but a continuous
    one by design! So, what's the big deal? Well, as we explained earlier, we will
    be sampling from this latent space to generate our outputs. However, sampling
    from a discrete latent space is problematic. The fact that it is discrete implies
    that there will be regions in the latent space with discontinuities, meaning that
    if these regions were to be randomly sampled, the output would look completely
    unrealistic. On the other hand, learning a continuous latent space allows the
    model to learn the transitions from one class to another in a probabilistic manner.
    Furthermore, since the latent space being learned is continuous, it becomes possible
    to identify and manipulate the concept vectors we spoke of earlier, which encodes
    various axes of variance present in the input data in a meaningful way. At this
    point, many of you may be wondering how a VAE exactly learns to model a continuous
    latent space. Well, wonder no more.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对生成网络的概念有了一个大致的理解，我们可以专注于一种特定类型的生成模型。VAE 就是其中之一，它由 Kingma 和 Welling（2013）以及
    Rezende、Mohamed 和 Wierstra（2014）提出。这个模型实际上与我们在上一章中看到的自动编码器非常相似，但它们有一个小小的变化——或者更准确地说，有几个变化。首先，学习的潜在空间不再是离散的，而是通过设计变成了连续的！那么，这有什么大不了的呢？正如我们之前所解释的那样，我们将从这个潜在空间中进行采样来生成输出。然而，从离散的潜在空间中进行采样是有问题的。因为它是离散的，意味着潜在空间中会有不连续的区域，这样如果这些区域被随机采样，输出将看起来完全不真实。另一方面，学习一个连续的潜在空间使得模型能够以概率的方式学习从一个类别到另一个类别的过渡。此外，由于学习的潜在空间是连续的，因此可以识别和操控我们之前提到的概念向量，这些向量以一种有意义的方式编码了输入数据中存在的各种方差轴。在这一点上，许多人可能会好奇
    VAE 如何准确地学习建模一个连续的潜在空间。嗯，别再好奇了。
- en: 'Earlier, we saw how we can redefine the sampling process from a latent space,
    so as to be able to plug it into a larger network to estimate a probability distribution.
    We did this by breaking the latent space down by using parameterized functions
    (that is, parts of a neural network) to estimate both the mean (μ) and the standard
    deviation (σ) of variables in the latent space. In a VAE, its encoder function
    does exactly this. This is what forces the model to learn a statistical distribution
    of variables over a continuous latent space. This process permits us to presume
    that the input image was generated in a probabilistic manner, given that the latent
    space encodes a probability distribution. Thus, we can use the learned mean and
    standard deviation parameters to randomly sample from the distribution, and decode
    it on to the original dimension of the data. The illustration here helps us better
    understand the workflow of a VAE:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们已经看到如何重新定义从潜在空间中采样的过程，以便能够将其插入到一个更大的网络中来估计概率分布。我们通过使用参数化函数（也就是神经网络的部分）来估计潜在空间中变量的均值（μ）和标准差（σ），从而将潜在空间分解。在
    VAE 中，它的编码器函数正是做了这件事。这就迫使模型学习在连续潜在空间上变量的统计分布。这个过程使我们可以假设输入图像是以概率的方式生成的，因为潜在空间编码了一个概率分布。因此，我们可以使用学习到的均值和标准差参数，从该分布中进行随机采样，并将其解码到数据的原始维度。这里的插图帮助我们更好地理解
    VAE 的工作流程：
- en: '![](img/6ac4ab13-f5df-422a-afbd-bdf52e3127ab.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ac4ab13-f5df-422a-afbd-bdf52e3127ab.png)'
- en: This process is what allows us to first learn, and then sample from, a continuous
    latent space, generating plausible outputs. Is this still a bit fuzzy? Well, perhaps
    a demonstrative example is in order, to help clarify this notion. Let's begin
    by building a VAE in Keras, and go over both the theory and implementational side
    of things as we construct our model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程使我们能够首先学习，然后从连续潜在空间中采样，生成合理的输出。还觉得有些模糊吗？嗯，也许一个演示示例可以帮助澄清这个概念。让我们从在 Keras
    中构建一个 VAE 开始，边构建模型边讨论理论和实现方面的内容。
- en: Designing a VAE in Keras
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中设计 VAE
- en: 'For this exercise, we will go back to a well-known dataset that is easily available
    to all: the MNIST dataset. The visual features of handwritten digits make this
    dataset uniquely suited to experiment with VAEs, allowing us to better understand
    how these models work. We start by importing the necessary libraries:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将回到一个所有人都能轻松获取的知名数据集：MNIST 数据集。手写数字的视觉特征使得这个数据集特别适合用来实验变分自编码器（VAE），从而帮助我们更好地理解这些模型是如何工作的。我们首先导入必要的库：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading and pre-processing the data
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和预处理数据
- en: 'Next, we load the dataset, just as we did in [Chapter 3](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml), *Signal
    Processing – Data Analysis with Neural Networks*. We also take the liberty to
    define some variables that can be reused later, when designing our network. Here,
    we simply define the image size used to define the original dimensions of the
    images (784 pixels each). We choose an encoding dimension of `2` to represent
    the latent space, and an intermediate dimension of `256`. These variables defined
    here will be later fed to the dense layers of our VAE, defining the number of
    neurons per layer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载数据集，就像在[第 3 章](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml)《*信号处理–神经网络数据分析*》中做的那样。我们还自定义了一些变量，稍后在设计网络时可以复用。在这里，我们简单地定义了用于定义原始图像尺寸的图像大小（每个图像
    784 像素）。我们选择了一个 `2` 的编码维度来表示潜在空间，并且选择了一个 `256` 的中间维度。这里定义的这些变量稍后将被传递到 VAE 的全连接层，用来定义每层的神经元数量：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we simply pre-process the images by first flattening them into 2D vectors
    (of dimension (784) per image). Finally, we normalize the pixel values in these
    2D vectors between 0 and 1.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们简单地通过将图像先展平为二维向量（每个图像的维度为 784）来预处理图像。最后，我们将这些二维向量中的像素值归一化到 0 和 1 之间。
- en: Building the encoding module in a VAE
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 VAE 中的编码模块
- en: 'Next, we will start building the encoding module of our VAE. This part is almost
    identical to the shallow encoder we built in the last chapter, except that it
    splits into two separate layers: one estimating the mean and the other estimating
    the variance over the latent space:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始构建 VAE 的编码模块。这一部分几乎与我们在上一章中构建的浅层编码器相同，只不过它拆分成了两个独立的层：一个估计均值，另一个估计潜在空间上的方差：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You could optionally add the `name` argument while defining a layer, to be
    able to visualize our model intuitively. If we want, we can actually visualize
    the network we have built so far, by initializing it already and summarizing it,
    as shown here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义一个层时，你可以选择性地添加 `name` 参数，以便直观地可视化我们的模型。如果我们愿意，实际上可以通过初始化并总结它来可视化我们到目前为止构建的网络，如下所示：
- en: '![](img/c20c90de-b96d-4e59-8b8c-96983e8a7344.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c20c90de-b96d-4e59-8b8c-96983e8a7344.png)'
- en: Note how the outputs from the intermediate layer connect to both the mean estimation
    layer (`z_mean`) and the variance estimation layer (`z_log_var`), both representing
    the latent space encoded by the network. Together, these separate layers estimate
    the probability distribution of variables over the latent space, as described
    earlier in this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，中间层的输出如何连接到均值估计层（`z_mean`）和方差估计层（`z_log_var`），这两个层都表示由网络编码的潜在空间。总的来说，这些分离的层估计了潜在空间上变量的概率分布，正如本章前面所述。
- en: So, now we have a probability distribution being learned by the intermediate
    layers of our VAE. Next, we need a mechanism to randomly sample from this probability
    distribution, to generate our outputs. This brings us to the sampling equation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们有了一个由 VAE 中间层学习的概率分布。接下来，我们需要一个机制从这个概率分布中随机采样，以生成我们的输出。这就引出了采样方程。
- en: Sampling the latent space
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对潜在空间进行采样
- en: 'The idea behind this process here is quite simple. We defined a sample (*z*)
    simply by using the learned mean (`z_mean`) and variance (`z_log_variance`) from
    our latent space in an equation that may be formulated as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程背后的思想非常简单。我们通过在方程中使用从潜在空间中学到的均值（`z_mean`）和方差（`z_log_variance`）来定义一个样本（*z*），这个方程可以表述如下：
- en: '*z = z_mean + exp(z_log_variance) * epsilon*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = z_mean + exp(z_log_variance) * epsilon*'
- en: Here, *epsilon* is simply a random tensor consisting of very small values, ensuring
    a degree of randomness seeps into the queried sample every time. Since it is a
    tensor of very small values, it ensures that each decoded image will plausibly
    resemble the input image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*epsilon* 只是一个由非常小的值组成的随机张量，确保每次查询的样本中都会渗透一定程度的随机性。由于它是一个由非常小的值组成的张量，确保每个解码后的图像在可信度上会与输入图像相似。
- en: 'The sampling function presented here simply takes the values (that is, mean
    and variance) learned by the encoder network, defines a tensor of small values
    matching the latent dimensions, and then returns a sample from the probability
    distribution, using the sampling equation defined previously:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的采样函数简单地利用编码器网络学习到的值（即均值和方差），定义一个匹配潜在维度的小值张量，然后通过之前定义的采样方程返回一个来自概率分布的样本：
- en: '![](img/5f70630a-5833-402a-971c-67687476c237.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f70630a-5833-402a-971c-67687476c237.png)'
- en: Since Keras requires all operations to be nested in layers, we use a custom
    Lambda layer to nest this sampling function, along with a defined output shape.
    This layer, defined here as (`z`), will be responsible for generating samples
    from the learned latent space.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Keras 要求所有操作都必须嵌套在层中，我们使用一个自定义 Lambda 层来嵌套这个采样函数，并定义输出形状。这个层，在这里定义为（`z`），将负责从学习到的潜在空间生成样本。
- en: Building the decoder module
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建解码器模块
- en: 'Now that we have a mechanism implemented to sample from the latent space, we
    can proceed to build a decoder capable of mapping this sample to the output space,
    thereby generating a novel instance of the input data. Recall that just as the
    encoder funnels the data by narrowing the layer dimensions till the encoded representation
    is reached, the decoder layers progressively enlarge the representations sampled
    from the latent space, mapping them back to the original image dimension:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了从潜在空间采样的机制，可以继续构建解码器模块，将该样本映射到输出空间，从而生成输入数据的新实例。回想一下，就像编码器通过逐渐缩小层的维度直到得到编码表示，解码器层则逐步扩大从潜在空间采样的表示，将它们映射回原始图像维度：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Defining a custom variational layer
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个自定义变分层
- en: Now that we have constructed both the encoder and the decoder modules of our
    network, there remains but one implementational matter to divert our attention
    to before we can start training our VAE. It is quite an important one, as it related
    to how our network will calculate the loss and update itself to create more realistic
    generations. This may seem a little odd at first glance. What are we comparing
    our generations to? It's not as if we have a target representation to compare
    our model's generations to, so how can we compute our model's errors? Well, the
    answer is quite simple. We will use two separate `loss` functions, each tracking
    our model's performance over different aspects of the generated image. The first
    loss function is known as the reconstruction loss, which simply ensures that the
    decoded output of our model matches the supplied inputs. The second `loss` function
    is described as the regularization loss. This function actually aids our model
    to not overfit on the training data by simply copying it, thereby learning ideally
    composed latent spaces from the inputs. Unfortunately, these `loss` functions
    are not implemented in Keras as it is, and hence require a little more technical
    attention to operationalize.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了网络的编码器和解码器模块，在开始训练 VAE 之前，我们还需要关注一个实现问题。这个问题非常重要，因为它涉及到我们的网络如何计算损失并更新自己，以生成更逼真的输出。乍一看，这可能有点奇怪。我们将生成结果与什么进行比较呢？并不是说我们有一个目标表示来与模型的生成结果进行比较，那么我们如何计算模型的误差呢？其实答案很简单。我们将使用两个独立的`loss`函数，每个函数跟踪模型在生成图像的不同方面的表现。第一个损失函数被称为重建损失，它简单地确保我们的模型的解码输出与提供的输入匹配。第二个`loss`函数是正则化损失。这个函数实际上帮助模型避免仅仅复制训练数据，从而避免过拟合，进而从输入中学习理想的潜在空间。不幸的是，这些`loss`函数在
    Keras 中并没有直接实现，因此需要一些额外的技术处理才能运作。
- en: 'We operationalize these two `loss` functions by building a custom variational
    layer class, this will actually be the final layer of our network, and perform
    the computation of the two different loss metrics, and use their mean value to
    compute gradients of the loss with respect to the network parameters:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过构建一个自定义变分层类来操作这两个`loss`函数，这将成为我们网络的最终层，执行两个不同损失度量的计算，并使用它们的均值来计算关于网络参数的损失梯度：
- en: '![](img/ca004458-98df-4ad2-9ba8-199b86c8ba14.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca004458-98df-4ad2-9ba8-199b86c8ba14.png)'
- en: As you can see, the custom layer includes three functions. The first is for
    initialization. The second function is responsible for computing both losses.
    It uses the binary cross-entropy metric to compute the reconstruction loss, and
    the **Kullback–Leibler** (**KL**) divergence formula to compute the regularization
    loss. The KL-divergence term essentially allows us to compute the relative entropy
    of the generated output, with respect to the sampled latent space (*z*). It allows
    us to iteratively assess the difference in the probability distribution of the
    outputs different than that of the latent space. The `vae_loss` function then
    returns a combined loss value, which is simply the mean of both these computed
    metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，自定义层包含三个函数。第一个是初始化函数。第二个函数负责计算两个损失。它使用二元交叉熵度量来计算重建损失，并使用**Kullback–Leibler**（**KL**）散度公式来计算正则化损失。KL散度项本质上允许我们计算生成输出相对于采样潜在空间（*z*）的相对熵。它让我们能够迭代地评估输出概率分布与潜在空间分布之间的差异。然后，`vae_loss`函数返回一个综合损失值，这个值就是这两项计算得出的度量的均值。
- en: Finally, the `call` function is used to implement the custom layer, by using
    the built-in `add_loss` layer method. This part essentially defines the last layer
    of our network as the loss layer, thereby using our arbitrarily defined `loss`
    function to generate the loss value, with which backpropagation can be performed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`call`函数用于实现自定义层，方法是使用内建的`add_loss`层方法。这个部分实际上是将我们网络的最后一层定义为损失层，从而使用我们任意定义的`loss`函数生成损失值，进而可以进行反向传播。
- en: Compiling and inspecting the model
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译并检查模型
- en: 'Next, we define our network''s last layer (*y*) using the custom variational
    layer class we just implemented, as shown here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用我们刚刚实现的自定义变分层类来定义网络的最后一层（*y*），如下面所示：
- en: '![](img/8a5c1a25-1c15-44bf-9249-3e11e7058355.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a5c1a25-1c15-44bf-9249-3e11e7058355.png)'
- en: 'Now we are ready to finally compile and train our model! First, we put together
    the entire model, using the `Model` object from the functional API, and passing
    it the input layer from our encoder module, as well as the last custom loss layer
    we just defined. Then, we use the usual `compile` syntax on our initialized network,
    equipping it with the `rmsprop` optimizer. Do note, however, that since we have
    a custom loss function, the `compile` statement actually does not take any loss
    metric, where one would usually be present. At this point, we can visualize the
    entire model, by calling `.summary()` on the `vae` model object, as shown here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以编译并训练我们的模型了！首先，我们将整个模型组合在一起，使用功能性 API 中的`Model`对象，并将其输入层传入自编码器模块，以及我们刚刚定义的最后一个自定义损失层。接着，我们使用通常的`compile`语法来初始化网络，并为其配备`rmsprop`优化器。然而需要注意的是，由于我们使用了自定义损失函数，`compile`语句实际上不接受任何损失度量，通常应该存在的地方并没有出现。在这时，我们可以通过调用`.summary()`方法来可视化整个模型，如下所示：
- en: '![](img/f2705dc9-4a46-4bdc-895e-f227bb7cc57c.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2705dc9-4a46-4bdc-895e-f227bb7cc57c.png)'
- en: As you can see, this architecture takes in the input images and funnels them
    down to two distinct encoded representations: `z_mean` and `z_log_var` (that is,
    a learned mean and variance over the latent space). This probability distribution
    is then sampled using the added Lambda layer to produce a point in the latent
    space. This point is then decoded by dense layers (`dense_5` and `dense_6`), before
    a loss can be computed by our final custom-built loss layer. Now you have seen
    everything.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个架构将输入图像传入并将其压缩为两个不同的编码表示：`z_mean`和`z_log_var`（即在潜在空间中学习得到的均值和方差）。然后，使用添加的Lambda层对这个概率分布进行采样，从而生成潜在空间中的一个点。接着，这个点通过全连接层（`dense_5`和`dense_6`）进行解码，最后通过我们自定义的损失层计算损失值。现在您已经看到了整个流程。
- en: Initiating the training session
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动训练会话
- en: 'Now comes the time to actually train our network. There is nothing out of the
    ordinary here, except for the fact that we do not have to specify a target variable
    (that is, `y_train`). This is simply because the target is normally used to compute
    the loss metrics, which is now being computed by our final custom layer. You may
    also notice that the loss values displayed during training are quite large, compared
    to previous implementations. Don''t be alarmed at their magnitude, as this is
    simply the result of the manner in which loss is computed for this architecture:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候真正训练我们的网络了。这里没有什么特别之处，除了我们不需要指定目标变量（即`y_train`）之外。只是因为目标通常用于计算损失指标，而现在这些损失是由我们最后的自定义层计算的。你可能还会注意到，训练过程中显示的损失值相当大，相比之前的实现。这些损失值的大小不必惊慌，因为这只是由于这种架构计算损失的方式所导致的：
- en: '![](img/5e23a05f-6ce4-4751-93a2-71e3219fd149.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e23a05f-6ce4-4751-93a2-71e3219fd149.png)'
- en: This model is trained for 50 epochs, at the end of which we were able to attain
    a validation loss of `151.71` and a training loss of `149.39`. Before we generate
    some novel-looking handwritten digits, let's try visualizing the latent space
    that our model was able to learn.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型训练了50个周期，在最后我们得到了`151.71`的验证损失和`149.39`的训练损失。在我们生成一些新颖的手写数字之前，让我们尝试可视化一下我们的模型所学到的潜在空间。
- en: Visualizing the latent space
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化潜在空间
- en: 'Since we have a two-dimensional latent space, we can simply plot out the representations
    as a 2D manifold where encoded instances of each digit class may be visualized
    with respect to their proximity to other instances. This allows us to inspect
    the continuous latent space that we spoke of before and see how the network relates
    to different features in the 10-digit classes (0 to 9) to each other. To do this,
    we revisit the encoding module from our VAE, which can now be used to produce
    a compressed latent space from some given data. Thus, we use the encoder module
    to make predictions on the test set, thereby encoding these images the latent
    space. Finally, we can use a scatterplot from Matplotlib to plot out the latent
    representation. Do note that each individual point represents an encoded instance
    from the test set. The colors denote the different digit classes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个二维潜在空间，我们可以简单地将表示绘制为一个二维流形，在这个流形中，每个数字类别的编码实例可以根据它们与其他实例的接近度进行可视化。这使我们能够检查我们之前提到的连续潜在空间，并观察网络如何将不同特征与10个数字类别（0到9）相互关联。为此，我们重新访问VAE的编码模块，该模块现在可以用来从给定的数据中生成压缩的潜在空间。因此，我们使用编码器模块对测试集进行预测，从而将这些图像编码到潜在空间中。最后，我们可以使用Matplotlib中的散点图来绘制潜在表示。请注意，每个单独的点代表测试集中的一个编码实例。颜色表示不同的数字类别：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Following is the output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/be7dbbed-fb81-43fc-863a-253cf1bd3647.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be7dbbed-fb81-43fc-863a-253cf1bd3647.png)'
- en: 'Note how there is very little discontinuity, or gaps between the different
    digit classes. Due to this, we can now sample from this encoded representation
    to produce meaningful digits. Such an operation would not produce meaningful results
    if the learned latent space were discrete, as was the case for the autoencoders
    we built in the last chapter. The latent space for these models looks much different,
    when compared to the one learned by the VAE:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，不同数字类别之间几乎没有不连续性或间隙。因此，我们现在可以从这种编码表示中采样，生成有意义的数字。如果学到的潜在空间是离散的，像我们在上一章构建的自编码器那样，这样的操作将不会产生有意义的结果。这些模型的潜在空间看起来与VAE所学到的潜在空间截然不同：
- en: '![](img/7c1af03c-318e-4c8c-9f87-54102b80461f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c1af03c-318e-4c8c-9f87-54102b80461f.png)'
- en: Latent space sampling and output generation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在空间采样与输出生成
- en: 'Finally, we can proceed to generate some novel handwritten digits with our
    VAE. To do this, we simply revisit the decoder part of our VAE (which naturally
    excludes the loss layer). We will be using it to decode samples from the latent
    space and generate some handwritten digits that were never actually written by
    anyone:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以利用我们的变分自编码器（VAE）生成一些新的手写数字。为此，我们只需要重新访问VAE的解码器部分（自然不包括损失层）。我们将使用它从潜在空间解码样本，并生成一些从未由任何人实际书写过的手写数字：
- en: '![](img/621b3f8b-dfee-404f-84e2-cfc351b28d03.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/621b3f8b-dfee-404f-84e2-cfc351b28d03.png)'
- en: 'Next, we will display a grid of 15 x 15 digits, each of size 28\. To do this,
    we initialize a matrix of zeros, matching the dimensions of the entire output
    to be generated. Then, we use the `ppf` function from SciPy to transform some
    linearly placed coordinates to get to the grid values of the latent variables
    (`z`). After this, we enumerate through these grids to obtain a sampled (`z`)
    value. We can now feed this sample to the generator network, which will decode
    the latent representation, to subsequently reshape the output to the correct format,
    resulting in the screenshot shown here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将显示一个15 x 15的数字网格，每个数字的大小为28。为了实现这一点，我们初始化一个全为零的矩阵，矩阵的尺寸与要生成的整个输出相匹配。然后，我们使用SciPy的`ppf`函数，将一些线性排列的坐标转化为潜在变量（`z`）的网格值。之后，我们遍历这些网格以获取一个采样的（`z`）值。我们现在可以将这个样本输入到生成器网络中，生成器将解码潜在表示，随后将输出重塑为正确的格式，最终得到如下截图：
- en: '![](img/9034a8e3-8267-4dcd-8800-960886fe9bc5.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9034a8e3-8267-4dcd-8800-960886fe9bc5.png)'
- en: Do note that this grid demonstrates how sampling from a continuous space allows
    us to literally visualize the underlining factors of variance in the input data.
    We notice that digits transform into other digits, as we move along the *x* or
    *y* axis. For example, consider moving from the center of the image. Moving to
    the right can change the digit **8** into a **9**, while moving left will change
    it into a **6**. Similarly, moving diagonally upward on the right-hand side changes
    the **8** into a **5** first, and then finally a **1**. These different axes can
    be thought of as representing the presence of certain properties on a given digit.
    These properties become accentuated as we progress further and further in the
    direction of a given axis, moulding the digit into an instance of a specific digit
    class .
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个网格展示了如何从连续空间进行采样，从而使我们能够直观地呈现输入数据中潜在的变化因素。我们注意到，随着沿着*x*或*y*轴的移动，数字会变换成其他数字。例如，考虑从图像的中心开始移动。向右移动会将数字**8**变成**9**，而向左移动则会将其变为**6**。同样，沿右上对角线向上移动，数字**8**会先变为**5**，然后最终变为**1**。这些不同的轴可以被看作是代表给定数字上某些特征的存在。这些特征随着我们沿给定轴的方向进一步推进而变得更加突出，最终将数字塑造成特定数字类别的一个实例。
- en: Concluding remarks on VAEs
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VAE的总结性评论
- en: As we saw in our MNIST experiments, the VAE excels at learning a well-composed
    continuous latent space, from which we may sample and decode outputs. These models
    are excellent for editing images, or producing psychedelic transitions where images
    mould into other images. Some businesses have even started experimenting with
    VAE-based models to allow customers to try out fashion items such as jewelry,
    sun glasses, or other apparel completely virtually, using the cameras on customers'
    phones! This is due to the fact that VAEs are uniquely suited to learning and
    editing concept vectors, as we discussed earlier. For instance, if you want to
    generate a new sample halfway between a 1 and a 0, we can simply compute the difference
    between their mean vectors from the latent space and add half the difference to
    the original before decoding it. This will produce a 6, as we can see in the previous
    screenshot. The same concept applies to a VAE trained on images of faces (using
    the CelebFaces dataset, for example), as we can sample a face between two different
    celebrities, to then create their synthetic sibling. Similarly, if we wanted to
    generate specific features, such as a mustache on a face, all we would have to
    do is find a sample of a face with and without a mustache. Then, we can retrieve
    their respective encoded vectors using the encoding function, and simply save
    the difference between these two vectors. Now our saved mustache vector is ready
    to be applied to any image, by adding it to the encoded space of the new image,
    before decoding it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 MNIST 实验中所见，VAE 擅长学习一个结构良好的连续潜在空间，从中我们可以采样并解码输出。这些模型非常适合编辑图像，或者生成视觉效果类似于图像转化为其他图像的迷幻过渡效果。一些公司甚至开始尝试使用基于
    VAE 的模型，让顾客通过手机摄像头完全虚拟地试戴珠宝、太阳镜或其他服饰！这是因为 VAE 在学习和编辑概念向量方面独具优势，正如我们之前讨论的那样。例如，如果你想生成一个介于
    1 和 0 之间的新样本，我们只需计算它们在潜在空间中的均值向量差异，并将差异的一半加到原始样本上，然后再解码。这将生成一个 6，就像我们在之前的截图中看到的那样。同样的概念也适用于训练面部图像的
    VAE（例如使用 CelebFaces 数据集），我们可以在两位不同名人的面部之间采样，从而创建他们的“合成兄弟”。类似地，如果我们想要生成特定的面部特征，比如胡子，只需要找到有胡子和没有胡子的面部样本。接着，我们可以使用编码函数获取它们各自的编码向量，并保存这两个向量之间的差异。现在，我们保存的胡子向量就可以应用到任何图像上，只需将它加到新图像的编码空间中，再进行解码即可。
- en: Other amusing use cases with VAEs involve swapping faces on a live feed, or
    adding additional elements for the sake of entertainment. These networks are quite
    unique in their ability to realistically modify images and produce ones that never
    originally existed. Naturally, it makes you wonder whether such technologies can
    be used for less-amusing purposes; misusing these models to misrepresent people
    or situations could potentially lead to some dire outcomes. However, since we
    can train neural networks to fool humans, we can also train them to help us distinguish
    such forgeries. This brings us to the next topic of this chapter: **GANs**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其他有趣的 VAE 应用案例包括在实时视频中交换面孔，或者为了娱乐添加额外的元素。这些网络非常独特，因为它们能够逼真地修改图像并生成那些原本不存在的图像。自然地，这也让人想知道这些技术是否可以用于一些不那么有趣的目的；滥用这些模型来歪曲人们或情况的真实性，可能会导致严重后果。然而，既然我们可以训练神经网络来欺骗我们人类，我们也可以训练它们来帮助我们辨别这种伪造。这引出了本章的下一个主题：**GANs**。
- en: Exploring GANs
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 GANs
- en: The idea behind GANs is much more understandable when compared to other similar
    models. In essence, we use several neural networks to play a rather elaborate
    game. Just like in the movie C*atch-me-if-you-can*. For those who are not familiar
    with the plot of this film, we apologize in advance for any missed allusions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类似模型相比，GANs 的思想更容易理解。从本质上讲，我们使用多个神经网络来进行一场相当复杂的博弈。就像电影《猫鼠游戏》中的情节一样。对于那些不熟悉该电影情节的人，我们提前为错过的暗示表示歉意。
- en: We can think of a GAN as a system of two actors. On one side, we have a Di Caprio-like
    network that attempts to recreate some Monets and Dalis and ship them off to unsuspecting
    art dealers. We also have a vigilant Tom Hanks-style network that intercepts these
    shipments and identifies any forgeries present. As time goes by, both individuals
    become better at what they do, leading to realistic forgeries on the conman's
    side, and a keen eye for them on the cop's side. This variation of a commonly
    used analogy indeed does well at introducing the idea behind these architectures.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将GAN看作是一个由两个参与者组成的系统。在一方，我们有一个类似Di Caprio的网络，试图重新创作一些莫奈和达利的作品，并将它们发送给毫无戒心的艺术经销商。另一方面，我们有一个警觉的、像汤姆·汉克斯风格的网络，拦截这些货物并识别其中的伪作。随着时间的推移，两者都变得越来越擅长自己的工作，导致骗子那一方能够创造出逼真的伪作，而警察那一方则具备了敏锐的眼光来识别它们。这种常用的类比变体，确实很好地介绍了这些架构背后的理念。
- en: 'A GAN essentially has two parts: a generator and a discriminator. Each of these
    parts can be thought of as separate neural networks, which work together by checking
    each other''s outputs as the model trains. The generator network is tasked to
    generate fake data points, by sampling random vectors from a latent space. Then,
    the discriminator receives these generated data points, along with actual data
    points, and proceeds to distinguish which one of the data points is real, and
    which are not (hence the name, *discriminator*). As our network trains, both the
    generator and the discriminator get better at creating synthetic data and recognizing
    synthetic data, respectively:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个GAN本质上有两个部分：生成器和判别器。这两个部分可以看作是独立的神经网络，它们在模型训练时通过相互检查输出进行协同工作。生成器网络的任务是通过从潜在空间中抽取随机向量来生成虚假的数据点。然后，判别器接收这些生成的数据点，以及实际的数据点，并识别哪些数据点是真的，哪些是假的（因此得名*判别器*）。随着网络的训练，生成器和判别器分别在生成合成数据和识别合成数据的能力上不断提升：
- en: '![](img/0797705e-f322-4fe7-99f9-2c7dea54b923.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0797705e-f322-4fe7-99f9-2c7dea54b923.png)'
- en: Utility and practical applications for GANS
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN的实用性和实际应用
- en: 'This architecture was first introduced by Goodfellow and others, 2014, and
    it has since been popularized by researchers spanning several domains. Their rise
    to fame was due to their ability to generate synthetic images that are virtually
    indistinguishable from real ones. While we have discussed some of the more amusing
    and mundane applications that derive from such methods, more complex ones also
    exist. For instance, while GANs are mostly used for computer vision tasks such
    as texture editing and image modification, they are increasingly becoming popular
    in a multitude of academic disciplines, making appearances in more and more research
    methodologies. Nowadays, you may find GANs being used for medical image synthesis,
    or even in domains such as particle physics and astrophysics. The same methodology
    for generating synthetic data can be used to regenerate denoised images from galaxies
    far, far away or to simulate realistic radiation patterns that would arise from
    high-energy particle collisions. The true utility of GANs lies in their ability
    to learn underlining statistical distributions in data, allowing them to generate
    synthetic instances of the original inputs. Such an approach is especially useful
    for researchers when collecting real data, but this may be prohibitively expensive,
    or physically impossible. Furthermore, the utility of GANs is not limited to the
    domain of computer vision. Other applications have included using variations of
    these networks to generate fine-grained images from natural language data, such
    as a sentence describing some scenery:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构最早由Goodfellow等人于2014年提出，随后在多个领域的研究人员中得到了广泛应用。它们之所以迅速走红，是因为它们能够生成几乎无法与真实图像区分的合成图像。虽然我们已经讨论了由此方法衍生的一些较为有趣和日常的应用，但也有一些更复杂的应用。例如，虽然GAN主要用于计算机视觉任务，如纹理编辑和图像修改，但它们在多个学术领域中的应用也日益增加，出现在越来越多的研究方法中。如今，你可能会发现GAN被应用于医学图像合成，甚至在粒子物理学和天体物理学等领域中得到应用。生成合成数据的相同方法可以用来再生来自遥远星系的去噪图像，或模拟高能粒子碰撞产生的真实辐射模式。GAN的真正实用性在于它们能够学习数据中潜在的统计分布，使其能够生成原始输入的合成实例。这种方法尤其对研究人员有用，因为收集真实数据可能既昂贵又物理上不可能实现。此外，GAN的应用不仅限于计算机视觉领域。其他应用包括使用这些网络的变种从自然语言数据生成精细的图像，比如描述某种风景的句子：
- en: '![](img/08bc6c1d-aaa9-4497-b53c-6c87d163b7b4.png)[https://arxiv.org/pdf/1612.03242v1.pdf](https://arxiv.org/pdf/1612.03242v1.pdf)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08bc6c1d-aaa9-4497-b53c-6c87d163b7b4.png)[https://arxiv.org/pdf/1612.03242v1.pdf](https://arxiv.org/pdf/1612.03242v1.pdf)'
- en: These use cases all show how GANs permit us to address novel tasks, with creative
    as well as practical implications. Yet, these architectures are not all fun and
    games. They are notoriously difficult to train, and those who have ventured deep
    into these waters describe it as more of an art than a science.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用案例展示了GAN如何使我们能够处理新任务，既有创造性的也有实用性的影响。然而，这些架构并非都是轻松愉快的。它们以训练困难著称，深入研究这些领域的人们形容它们更像是一门艺术，而非科学。
- en: 'For more information on this subject, refer to the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的更多信息，请参考以下内容：
- en: '**Original paper by Goodfellow and others**: [http://papers.nips.cc/paper/5423-generative-adversarial-nets](http://papers.nips.cc/paper/5423-generative-adversarial-nets)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Goodfellow等人的原创论文**: [http://papers.nips.cc/paper/5423-generative-adversarial-nets](http://papers.nips.cc/paper/5423-generative-adversarial-nets)'
- en: '**GAN in astrophysics**: [https://academic.oup.com/mnrasl/article/467/1/L110/2931732](https://academic.oup.com/mnrasl/article/467/1/L110/2931732)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**天体物理学中的GAN**: [https://academic.oup.com/mnrasl/article/467/1/L110/2931732](https://academic.oup.com/mnrasl/article/467/1/L110/2931732)'
- en: '**GAN in particle physics**: [https://link.springer.com/article/10.1007/s41781-017-0004-6](https://link.springer.com/article/10.1007/s41781-017-0004-6)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**粒子物理学中的GAN**: [https://link.springer.com/article/10.1007/s41781-017-0004-6](https://link.springer.com/article/10.1007/s41781-017-0004-6)'
- en: '**Fine-grained text-to-image generation**: [http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细粒度文本到图像生成**: [http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html)'
- en: Diving deeper into GANs
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解GAN
- en: 'So, let''s try to better understand how the different parts of the GAN work
    together to generate synthetic data. Consider the parameterized function (*G*)
    (you know, the kind we usually approximate using a neural network). This will
    be our generator, which samples its input vectors (*z*) from some latent probability
    distribution, and transforms them into synthetic images. Our discriminator network
    (*D*), will then be presented with some synthetic images produced by our generator,
    mixed among real images, and attempt to classify real from forgery. Hence, our
    discriminator network is simply a binary classifier, equipped with something like
    a sigmoid activation function. Ideally, we want the discriminator to output high
    values when presented with real images, and low values when presented with generated
    fakes. Conversely, we want our generator network to try to fool the discriminator
    network, by making it output high values for the generated fakes as well. These
    concepts bring us to the mathematical formulation of training a GAN, which is
    essentially a battle between two neural networks (*D* and *G*), each trying to
    one-up the other:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们尝试更好地理解GAN的不同部分如何协作生成合成数据。考虑一下带参数的函数（*G*）（你知道的，我们通常使用神经网络来近似的那种）。这个将是我们的生成器，它从某个潜在的概率分布中采样输入向量（*z*），并将它们转换为合成图像。我们的判别网络（*D*）随后将会接收到由生成器生成的一些合成图像，这些图像与真实图像混合，并尝试将真实图像与伪造图像区分开来。因此，我们的判别网络只是一个二分类器，配备了类似于sigmoid激活函数的东西。理想情况下，我们希望判别器在看到真实图像时输出较高的值，而在看到生成的伪造图像时输出较低的值。相反，我们希望我们的生成器网络通过使判别器对生成的伪造图像也输出较高的值来愚弄判别器。这些概念引出了训练GAN的数学公式，其本质上是两个神经网络（*D*和*G*）之间的对抗，每个网络都试图超越另一个：
- en: '![](img/4a9a9cd4-0a21-4a4c-ba43-1bb3ed86478e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a9a9cd4-0a21-4a4c-ba43-1bb3ed86478e.png)'
- en: In the given formulation, the first term actually denotes the entropy relating
    to a data point (*x*) from the real distribution, presented to the discriminator.
    The goal of the discriminator is to try maximize this term to 1, as it wishes
    to correctly identify real images. Furthermore, the second term in the formulation
    denotes the entropy relating to a randomly sampled point, transformed into a synthetic
    image by the generator, *G(z)*, presented to the discriminator, *D(G(z))*. The
    discriminator wants none of this, and hence it seeks to maximize the log probability
    of the data point being fake (that is, the second term), to 0\. Hence, we can
    state that the discriminator is trying to maximize the entire *V *function. The
    generator function, on the other hand, will be doing quite the contrary. The generator's
    goal is to try to minimize the first term and maximize the second term so that
    the discriminator is not able to tell real from fake. And so begins the laborious
    game between cop and thief.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的公式中，第一个项实际上表示与来自真实分布的数据点(*x*)相关的熵，该数据点被呈现给判别器。判别器的目标是尽量将这个项最大化到1，因为它希望能够正确识别真实图像。此外，公式中的第二个项表示与随机抽样的点相关的熵，这些点被生成器转换为合成图像
    *G(z)*，并呈现给判别器 *D(G(z))*。判别器不希望看到这个，因此它试图将数据点是假的对数概率（即第二项）最大化到0。因此，我们可以说，判别器试图最大化整个
    *V* 函数。另一方面，生成器的目标则是做相反的事情。生成器的目标是尽量最小化第一个项并最大化第二个项，以便判别器无法区分真实与虚假。这就开始了警察与小偷之间的漫长博弈。
- en: Problems with optimizing GANs
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化GAN存在的问题
- en: Interestingly, since both networks take turns to optimize their own metric,
    the GAN has a dynamic loss landscape. This is different than all other examples
    we have seen in this book, where the loss hyperplane would remain the same, as
    we descended it by backpropagating our model errors, converging to more ideal
    parameters. Here, however, since both networks get a go at optimizing their parameters,
    each step down the hyperplane changes the landscape a tiny bit, until an equilibrium
    is reached between the two optimization constraints. As with many things in life,
    this equilibrium is not easily achieved, and it requires a lot of attention and
    effort. In the case of GANs, attention to aspects such as layer weight initialization,
    usage of `LeakyRelu` and `tanh` instead of **Rectified Linear Unit** (**ReLU**) and
    sigmoid activation functions, implementing batch normalization and dropout layers,
    and so on, are but a few among the vast array of considerations that may improve
    your GAN's ability to attain equilibrium. Yet, there is no better way of familiarizing
    ourselves with these issues than to get our hands on some code and actually implement
    an instance of these fascinating architectures.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，由于两个网络轮流优化自己的度量，GAN具有动态的损失景观。这与我们在本书中看到的其他所有例子不同，在那些例子中，损失超平面保持不变，随着我们通过反向传播调整模型误差，逐步收敛到更理想的参数。而在这里，由于两个网络都在优化其参数，每一步沿超平面的下降都会略微改变景观，直到两种优化约束之间达到平衡。正如生活中的许多事情一样，这种平衡并不容易实现，它需要大量的关注和努力。在GAN的情况下，关注层权重初始化、使用
    `LeakyRelu` 和 `tanh` 替代 **修正线性单元** (**ReLU**) 和sigmoid激活函数、实现批量归一化和dropout层等方面，都是提高GAN达到平衡能力的众多考虑因素之一。然而，没有比通过实际编写代码并实现这些令人着迷的架构实例更好的方式来熟悉这些问题。
- en: 'For more information on this subject, refer to the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 更多相关信息，请参考以下内容：
- en: '**Improved techniques for training GANs**: [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的GAN训练技术**: [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf)'
- en: '**Photo-realistic image generation**: [http://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html](http://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**照片级真实感图像生成**: [http://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html](http://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html)'
- en: Designing a GAN in Keras
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中设计GAN
- en: 'For this exercise, suppose you were part of a research team working for a large
    automobile manufacturer. Your boss wants you to come up with a way to generate
    synthetic designs for cars, to systematically inspire the design team. You have
    heard all the hype about GANs and have decided to investigate whether they can
    be used for the task at hand. To do this, you want to first do a proof of concept,
    so you quickly get a hold of some low-resolution pictures of cars and design a
    basic GAN in Keras to see whether the network is at least able to recreate the
    general morphology of cars. Once you can establish this, you can convince your
    manager to invest in a few *Titan x GUPs* for the office, get some higher-resolution
    data, and develop some more complex architectures. So, let''s start by implementing
    this proof of concept by first getting our hands on some pictures of cars. For
    this demonstrative use case, we use the good old CIFAR-10 dataset, and restrict
    ourselves to the commercial automobile category. We start our implementation exercise
    by importing some libraries, as shown here:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一个研究团队的一员，团队为一家大型汽车制造商工作。你的老板希望你想出一种生成汽车合成设计的方法，以系统地激发设计团队的灵感。你听说过很多关于GAN的宣传，决定研究它们是否能用于这个任务。为此，你首先想做一个概念验证，因此你迅速获取了一些低分辨率的汽车图片，并在Keras中设计了一个基础的GAN，看看网络是否至少能够重建汽车的一般形态。一旦你能够确认这一点，你就可以说服经理为办公室投资几台*Titan
    x GUPs*，获取更高分辨率的数据，并开发更复杂的架构。那么，让我们首先获取一些汽车图片，通过实现这个概念验证来开始吧。对于这个演示用例，我们使用了经典的CIFAR-10数据集，并将自己限制在商用汽车类别。我们从导入一些库开始，如下所示：
- en: '![](img/6c878754-2a18-4486-a873-2b59dad91701.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c878754-2a18-4486-a873-2b59dad91701.png)'
- en: Preparing the data
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'We proceed by simply loading up the data through Keras, and selecting only
    car images (index = 1). Then, we check the shape of our training and test arrays.
    We see that there are 5,000 training images and 1,000 test images:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续通过Keras加载数据，只选择汽车图像（索引=1）。然后，我们检查训练和测试数组的形状。我们看到有5,000张训练图像和1,000张测试图像：
- en: '![](img/4639fcd3-f2f9-4040-b49e-90cb00f67c4c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4639fcd3-f2f9-4040-b49e-90cb00f67c4c.png)'
- en: Visualizing some instances
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化一些实例
- en: 'We will now take a look at the real images from the dataset, using Matplotlib.
    Remember these, as soon we will be generating some fakes for comparison:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用Matplotlib查看数据集中的真实图像。记住这些图像，因为稍后我们将生成一些假图像进行比较：
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Following is the output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/bf4f02f7-26c2-4027-af00-eb18fe6f2254.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf4f02f7-26c2-4027-af00-eb18fe6f2254.png)'
- en: Pre-processing the data
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Next, we simply normalize our pixel values. Unlike previous attempts, however,
    this time, we normalize the pixel values between -1 and 1 (instead of between
    0 and 1). This is due to the fact that we will be using a `tanh` activation function
    for the generator network. This specific activation function outputs values between
    -1 and 1; hence, normalizing the data in a similar manner makes the learning process
    smoother:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只是简单地对像素值进行归一化。然而，与之前的尝试不同，这次我们将像素值归一化到-1到1之间（而不是0到1之间）。这是因为我们将为生成器网络使用`tanh`激活函数。这个特定的激活函数输出的值在-1到1之间；因此，以类似的方式归一化数据会使学习过程更加平滑：
- en: '![](img/5cde634d-ac51-456d-b8ae-46b1dbcfaf3f.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cde634d-ac51-456d-b8ae-46b1dbcfaf3f.png)'
- en: We encourage you to try different normalization strategies to explore how this
    affects learning as the network trains. Now we have all the components in place
    to start constructing the GAN architecture.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你尝试不同的归一化策略，探索它们如何影响网络训练中的学习过程。现在我们已经准备好所有组件，可以开始构建GAN架构了。
- en: Designing the generator module
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计生成器模块
- en: 'Now comes the fun part. We will be implementing a **Deep Convolutional Generative
    Adversarial Network** (**DCGAN**). We start with the first part of the DCGAN:
    the generator network. The generator network will essentially learn to recreate
    realistic car images, by transforming a sample from some normal probability distribution,
    representing a latent space.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是最有趣的部分。我们将实现一个**深度卷积生成对抗网络**（**DCGAN**）。我们从DCGAN的第一部分开始：生成器网络。生成器网络本质上将通过从某个正态概率分布中采样来重建真实的汽车图像，代表一个潜在空间。
- en: 'We will again use the functional API to defile our model, nesting it in a function
    with three different arguments. The first argument, `latent_dim`, refers to the
    dimension of the input data randomly sampled from a normal distribution. The `leaky_alpha`
    argument simply refers to the alpha parameter provided to the `LeakyRelu` activation
    function used throughout the network. Finally, the argument `init_stddev` simply
    refers to the standard deviation with which to initialize the random weights of
    the network, used to define the `kernel_initializer` argument, when constructing
    a layer:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用功能 API 来定义我们的模型，将其嵌套在一个带有三个不同参数的函数中。第一个参数`latent_dim`指的是从正态分布中随机采样的输入数据的维度。`leaky_alpha`参数指的是提供给网络中使用的`LeakyRelu`激活函数的
    alpha 参数。最后，`init_stddev`参数指的是初始化网络随机权重时使用的标准差，用于定义构建层时的`kernel_initializer`参数：
- en: '[PRE6]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note the number of considerations taken while designing this model here. For
    instance, the `LeakyReLU` activation function is chosen in penultimate layers
    due to their ability to relax the sparsity constraint on outputs, when compared
    to the ReLU. This is simply due to the fact that `LeakyReLU` tolerates some small
    negative gradient values as well, whereas the ReLU simply squishes all negative
    values to zero. Gradient sparsity is usually considered a desirable property when
    training neural networks, yet this does not hold true for GANs. This is the same
    reason why max-pooling operations are not very popular in DCGANs, since this downsampling
    operation often produces sparse representations. Instead, we will be using the
    stride convolutions with the Conv2D transpose layer, for our downsampling needs.
    We also implemented batch normalization layers (with a moment for moving the mean
    and variance set to 0.8), as we noticed that this had a considerable effect on
    improving the quality of the generated images. You will also notice that the size
    of the convolutional kernels is set to be divisible by the stride, for each convolutional
    layer. This has been also noted to improve generated images, while reducing discrepancy
    between areas of the generated image, since the convolutional kernel is allowed
    to equally sample all regions. Finally, the last layer of the network is equipped
    with a `tanh` activation function, as this has consistently shown to produce better
    results with the GAN architecture. The next screenshot depicts the entire generator
    module of our GAN, which will produce the 32 x 32 x 3 synthetic images of cars,
    subsequently used to try fool the discriminator module:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在设计此模型时考虑了许多因素。例如，`LeakyReLU`激活函数在倒数第二层被选中，因为与 ReLU 相比，它能放宽输出的稀疏性约束。这是因为`LeakyReLU`允许一些小的负梯度值，而
    ReLU 会将所有负值压缩为零。梯度稀疏性通常被认为是训练神经网络时的理想特性，但对于 GAN 来说并不适用。这也是为什么最大池化操作在 DCGAN 中不太流行的原因，因为这种下采样操作通常会产生稀疏的表示。相反，我们将使用带有
    Conv2D 转置层的步幅卷积进行下采样需求。我们还实现了批量归一化层（其均值和方差的移动参数设置为 0.8），因为我们发现这对改善生成图像的质量有显著的作用。你还会注意到，卷积核的大小被设置为能被步幅整除，这有助于改善生成的图像，同时减少生成图像区域之间的差异，因为卷积核可以均匀地采样所有区域。最后，网络的最后一层配备了`tanh`激活函数，因为在
    GAN 架构中，这一激活函数 consistently 显示出更好的效果。下一张截图展示了我们 GAN 的整个生成器模块，它将生成 32 x 32 x 3
    的合成汽车图像，随后用于尝试欺骗判别器模块：
- en: '![](img/659b480f-a5d1-42b4-a02d-882354a05b1f.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/659b480f-a5d1-42b4-a02d-882354a05b1f.png)'
- en: Designing the discriminator module
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计判别器模块
- en: 'Next, we continue our journey designing the discriminator module, which will
    be responsible for telling the real images from the fake ones supplied by the
    generator module we just designed. The concept behind the architecture is quite
    similar to that of the generator, with some key differences. The discriminator
    network receives images of a 32 x 32 x 3 dimension, which it then transforms into
    various representations as information propagates through deeper layers, until
    the dense classification layer is reached, equipped with one neuron and a sigmoid
    activation function. It has one neuron, since we are dealing with the binary classification
    task of distinguishing fake from real. The `sigmoid` function ensures a probabilistic
    output between 0 and 1, indicating how fake or real the network thinks a given
    image may be. Do also note the inclusion of the dropout layer before the dense
    classifier layer, introduced for the sake of robustness and generalizability:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续设计判别器模块，该模块负责区分由刚设计的生成器模块提供的真实图像和假图像。该架构的概念与生成器非常相似，但也有一些关键的不同之处。判别器网络接收尺寸为
    32 x 32 x 3 的图像，然后将其转化为多种表示，随着信息通过更深层传播，直到达到带有一个神经元和 sigmoid 激活函数的密集分类层。它有一个神经元，因为我们处理的是区分假图像与真实图像的二分类任务。`sigmoid`函数确保输出一个介于
    0 和 1 之间的概率值，表示网络认为给定图像可能有多假或多真。还需要注意的是，在密集分类器层之前引入了 dropout 层，这有助于增强模型的鲁棒性和泛化能力：
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once again, we encourage you to experiment with as many model hyperparameters
    as possible, to better get a grip of how altering these different hyperparameters
    affects the learning and the outputs generated by our GAN model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们鼓励你尽可能多地尝试不同的模型超参数，以便更好地理解如何通过调整这些超参数影响学习过程和我们的 GAN 模型生成的输出。
- en: Putting the GAN together
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组装 GAN
- en: 'Next, we weave together the two modules using this function shown here. As
    arguments, it takes the size of the latent samples for the generator, which will
    be transformed by the generator network to produce synthetic images. It also accepts
    a learning rate and a decay rate for both the generator and discriminator networks.
    Finally, the last two arguments denote the alpha value for the `LeakyReLU` activation
    function used, as well as a standard deviation value for the random initialization
    of network weights:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用此处显示的函数将两个模块组合在一起。作为参数，它接受生成器的潜在样本大小，生成器网络将通过该大小转换生成合成图像。它还接受生成器和判别器网络的学习率和衰减率。最后，最后两个参数表示用于`LeakyReLU`激活函数的
    alpha 值，以及网络权重随机初始化的标准差值：
- en: '[PRE8]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We simply ensure that no previous Keras session is running by calling `.clear_session()`
    on the imported backend object, `K`. Then, we can define the generator and discriminator
    networks by calling their respective functions that we designed earlier and supplying
    them with the appropriate arguments. Note that the discriminator is compiled,
    while the generator is not.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用导入的后端对象`K`上的`.clear_session()`来确保没有之前的 Keras 会话在运行。然后，我们可以通过调用之前设计的生成器和判别器网络函数，并为它们提供适当的参数，来定义这两个网络。需要注意的是，判别器已被编译，而生成器没有被编译。
- en: Do note that the functions are designed in a way that encourage fast experimentation
    by changing different model hyperparameters using the arguments.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些函数的设计方式鼓励通过使用参数来快速实验，调整不同的模型超参数。
- en: Finally, after compiling the discriminator network with a binary cross-entropy
    loss function, we merge the two separate networks together. We do this using the
    sequential API, which allows you to merge two fully connected models together
    with much ease. Then, we can compile the entire GAN, again using the same loss
    and optimizer, yet with a different learning rate. We chose the `Adam` optimizer
    in our experiments, with a learning rate of 0.0001 for our GAN, and 0.001 for
    the discriminator network, which happened to work well for the task at hand.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在使用二进制交叉熵损失函数编译判别器网络后，我们将这两个独立的网络合并。我们使用顺序 API 来实现这一点，顺序 API 使得将两个全连接的模型合并变得非常简单。然后，我们可以编译整个
    GAN，再次使用相同的损失函数和优化器，但使用不同的学习率。在我们的实验中，我们选择了`Adam`优化器，GAN 的学习率为 0.0001，判别器网络的学习率为
    0.001，这在当前任务中效果很好。
- en: Helper functions for training
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于训练的辅助函数
- en: 'Next, we will define some helper functions that will aid us in the training
    process. The first among them simply makes a sample of latent variables from a
    normal probability distribution. Next, we have the `make_trainable()` function,
    which helps us train the discriminator and generator networks in turn. In other
    words, it allows us to freeze the layer weights of one module (the discriminator
    or the generator), while the other one is trained. The trainable argument for
    this function is just a Boolean variable (true or false). Finally, the `make_labels()`
    function simply returns labels to train the discriminator module. These labels
    are binary, where `1` stands for real, and `0` for fake:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一些辅助函数，帮助我们在训练过程中进行操作。第一个函数只是从正态概率分布中生成潜在变量的样本。接下来，我们有`make_trainable()`函数，它帮助我们交替训练鉴别器和生成器网络。换句话说，它允许我们冻结一个模块（鉴别器或生成器）的层权重，同时训练另一个模块。此函数的trainable参数只是一个布尔变量（true或false）。最后，`make_labels()`函数只是返回用于训练鉴别器模块的标签。这些标签是二进制的，其中`1`代表真实，`0`代表伪造：
- en: '[PRE9]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Helper functions to display output
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示输出的辅助函数
- en: 'The next two helper functions allow us to visualize our losses at the end of
    the training session, as well as plot an image out at the end of each epoch, to
    visually assess how the network is doing. Since the loss landscape is dynamically
    changing, the loss values have much less meaning. As is often the case with generative
    networks, evaluation of their output is mostly left to visual inspection by human
    observers. Hence, it is important that we are able to visually inspect the model''s
    performance during the training session:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个辅助函数使我们能够在训练结束时可视化损失值，并在每个周期结束时绘制出一张图像，从而直观评估网络的表现。由于损失值的变化是动态的，因此损失值的意义较小。就像在生成对抗网络中常见的情况一样，其输出的评估通常是由人类观察者通过视觉检查来完成的。因此，我们需要能够在训练过程中实时地检查模型的表现：
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first function simply accepts a list of loss values for the discriminator
    and the generator network over the training session, to transpose and plot them
    out over the epochs. The second function allows us to visualize a grid of generated
    images at the end of each epoch.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数只是接受一个包含鉴别器和生成器网络在整个训练过程中损失值的列表，进行转置并按周期绘制。第二个函数让我们能够在每个周期结束时可视化生成的图像网格。
- en: The training function
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练函数
- en: 'Next comes the training function. Yes, it is a big one. Yet, as you will soon
    see, it is quite intuitive, and basically combines everything we have implemented
    so far:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练函数。是的，它比较复杂。但正如你很快就会看到的，它是相当直观的，并基本上结合了我们到目前为止实现的所有内容：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Arguments in the training function
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练函数中的参数
- en: 'You are already familiar with most of the arguments of the training function.
    The first four arguments simply refer to the learning rate and decay rate used
    for the generator and the discriminator networks, respectively. Similarly, the
    `leaky_alpha` parameter is the negative slope coefficient we implemented for our
    `LeakyReLU` activation function, used in both networks. The smooth argument that
    follows represents the implementation of one-sided label smoothing, as proposed
    by Goodfellow and others, 2016\. The idea behind this is to replace the real (1)
    target values for the discriminator module with smoothed values, such as 0.9,
    as this has shown to reduce the susceptibility of neural networks to fail at adversarial
    examples:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经熟悉了训练函数中的大多数参数。前四个参数仅仅是指分别用于生成器和鉴别器网络的学习率和衰减率。类似地，`leaky_alpha`参数是我们为`LeakyReLU`激活函数实现的负斜率系数，在两个网络中都使用了这个函数。接下来的smooth参数代表的是单边标签平滑的实现，如Goodfellow等人（2016）提出的那样。其背后的思想是将鉴别器模块中的真实（1）目标值替换为平滑的值，比如0.9，因为这已被证明能够减少神经网络在对抗样本面前的失败风险：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we have four more parameters, which are quite simple to follow. The first
    among them is `sample_size`, referring to the size of the sample taken from the
    latent space. Next, we have the number of training epochs and `batch_size` in
    which to perform weight updates. Finally, we have the `eval_size` argument, which
    refers to the number of generated images to evaluate at the end of each training
    epoch.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有四个简单易懂的参数。其中第一个是`sample_size`，指的是从潜在空间中提取的样本大小。接下来，我们有训练的周期数以及用于进行权重更新的`batch_size`。最后，我们有`eval_size`参数，它指的是在每个训练周期结束时用于评估的生成图像数量。
- en: Defining the discriminator labels
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义鉴别器标签
- en: 'Next, we define the label arrays to be used for the training and evaluation
    images, by calling the `make_labels()` function, and using the appropriate batch
    dimension. This will return us arrays with the labels 1 and 0 for each instance
    of the training and evaluation image:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用`make_labels()`函数，并使用合适的批次维度，来定义用于训练和评估图像的标签数组。这样会返回带有标签1和0的数组，用于每个训练和评估图像的实例：
- en: '[PRE13]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Initializing the GAN
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化GAN
- en: 'Following this, we initialize the GAN network by calling the `make_DCGAN()`
    function we defined earlier and providing it with the appropriate arguments:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们通过调用之前定义的`make_DCGAN()`函数并传入适当的参数，初始化GAN网络：
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Training the discriminator per batch
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每批次训练判别器
- en: 'Thereafter, we define a list to collect the loss values for each network during
    training. To train this network, we will actually use the `.train_on_batch()`
    method, which allows us to selectively manipulate the training process, as is
    required for our use case. Essentially, we will implement a double `for` loop:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们定义一个列表，用于在训练过程中收集每个网络的损失值。为了训练这个网络，我们实际上会使用`.train_on_batch()`方法，它允许我们有选择地操作训练过程，正如我们案例中所需要的那样。基本上，我们将实现一个双重`for`循环：
- en: '[PRE15]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Hence, for each batch in each epoch, we will first train the discriminator,
    and then the generator, on the given batch of data. We begin by taking the first
    batch of real training images, as well as sampling a batch of latent variables
    from a normal distribution. Then, we use the generator module to make a prediction
    on the latent sample, essentially generating a synthetic image of a car.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每个epoch中的每个批次里，我们将首先训练判别器，然后在给定的批次数据上训练生成器。我们首先使用第一批真实的训练图像，并从正态分布中采样一批潜在变量。然后，我们使用生成器模块对潜在样本进行预测，实质上生成一张汽车的合成图像。
- en: Following this, we allow the discriminator to be trained on both batches (that
    is, of real and generated images), using the `make_trainable()` function. This
    is where the discrimator is given the opportunity to learn to tell real from fake.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们允许判别器在两个批次（即真实图像和生成图像）上进行训练，使用`make_trainable()`函数。这时，判别器有机会学习区分真实和虚假。
- en: Training the generator per batch
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每批次训练生成器
- en: 'After this, we freeze the layers of the discriminator, again using the `make_trainable()`
    function, this time to train the rest of the network only. Now it is the generator''s
    turn to try beat the discriminator, by generating a realistic image:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们冻结判别器的层，再次使用`make_trainable()`函数，这次只训练网络的其余部分。现在轮到生成器尝试击败判别器，通过生成一张真实的图像：
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Evaluate results per epoch
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个epoch的评估结果
- en: 'Next, we exit the `nested` loop to perform some actions at the end of each
    epoch. We randomly sample some real images as well as latent variables, and then
    generate some fake images to plot out. Do note that we used the `.test_on_batch()`
    method to obtain the loss values of the discriminator and the GAN and append them
    to our loss list. At the end of each epoch, we print out the discriminator and
    generator loss and plot out a grid of 16 samples. Now all that is left is to call
    this function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们退出`nested`循环，在每个epoch的结束执行一些操作。我们随机采样一些真实图像以及潜在变量，然后生成一些假图像并进行绘制。请注意，我们使用了`.test_on_batch()`方法来获取判别器和GAN的损失值，并将其附加到损失列表中。在每个epoch的末尾，我们打印出判别器和生成器的损失，并绘制出16张样本的网格。现在，只剩下调用这个函数了：
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For more information, refer to the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下内容：
- en: '**Improved techniques for training GANs**: [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的GAN训练技巧**：[https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf)'
- en: Executing the training session
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行训练会话
- en: 'We finally initiate the training session with the respective arguments. You
    will notice the tqdm  module displaying a percentage bar indicating the number
    of processed batches per epoch. At the end of the epoch, you will be able to visualize
    a 4 x 4 grid (shown next) of samples generated from the GAN network. And there
    you have it, now you know how to implement a GAN in Keras. On a side note, it
    can be very beneficial to have `tensorflow-gpu` along with CUDA set up, if you''re
    running the code on a local machine with access to a GPU. We ran this code for
    200 epochs, yet it would not be uncommon to let it run for thousands of epochs,
    given the resources and time. Ideally, the longer the two networks battle, the
    better the results should get. Yet, this may not always be the case, and hence,
    such attempts may also require careful monitoring of the loss values:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终使用相应的参数启动了训练会话。您会注意到，tqdm模块显示一个百分比条，指示每个周期处理的批次数量。周期结束时，您将能够可视化一个4 x 4网格（如下所示），其中包含从GAN网络生成的样本。到此为止，您已经知道如何在Keras中实现GAN。顺便提一下，如果您在具有GPU的本地机器上运行代码，设置`tensorflow-gpu`和CUDA会非常有益。我们运行了200个周期的代码，但如果有足够的资源和时间，运行几千个周期也并不罕见。理想情况下，两个网络对抗的时间越长，结果应该越好。然而，这并不总是如此，因此，这样的尝试可能也需要仔细监控损失值：
- en: '![](img/712e4234-0e06-482b-9327-e60681bf9f45.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/712e4234-0e06-482b-9327-e60681bf9f45.png)'
- en: Interpreting test loss during training
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练过程中测试损失的解释
- en: 'As you can see next, the loss values on the test set change at quite an unstable
    rate. We expect different optimizers to exhibit smother or rougher loss curves,
    and we encourage you to test these assumptions using different loss functions
    (RMSProp is an excellent one to start off with, for example). While looking at
    the plotted losses is not too intuitive, visualizing the generated images across
    the epochs allows some meaningful evaluation of this exercise:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您接下来看到的，测试集上的损失值变化非常不稳定。我们预期不同的优化器会呈现出更平滑或更剧烈的损失曲线，并且我们鼓励您使用不同的损失函数来测试这些假设（例如，RMSProp
    是一个很好的起点）。虽然查看损失的曲线图不是特别直观，但跨越多个训练周期可视化生成的图像可以对这一过程进行有意义的评估：
- en: '![](img/7c11a597-c793-4fd5-90bc-8f32080b94c0.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c11a597-c793-4fd5-90bc-8f32080b94c0.png)'
- en: Visualizing results across epochs
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨周期可视化结果
- en: 'In the following, we present eight snapshots of the 16 x 16 grids of generated
    samples, spread across different times during the training session. While the
    images themselves are pretty small, they undeniably resemble the morphology of
    cars toward the end of the training session:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了在训练过程中不同时间点生成的16 x 16网格样本的八个快照。尽管图像本身相当小，但它们无可否认地呈现出训练结束时接近汽车的形态：
- en: '![](img/8ac24243-b52d-4174-8b57-3c9efd186b3e.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ac24243-b52d-4174-8b57-3c9efd186b3e.png)'
- en: 'And there you have it. As you can see, the GAN becomes quite good at generating
    realistic car images after a while, as it gets better and better at fooling the
    discriminator. Towards the final epochs, it is even hard for the human eye to
    distinguish real from fake, at least at first glance. Furthermore, we achieved
    this with a relatively simple and straightforward implementation. This feat seems
    even more remarkable when we consider the fact that the generator network never
    actually sees a single real image. Recall that it is simply sampling from a random
    probability distribution, and uses only the feedback from the discriminator to
    better its own output! As we saw, the process of training a DCGAN involved a lot
    of consideration regarding minute detail and choosing specific model constraints
    and hyperparameters. For interested readers, you may find more details on how
    to optimize and fine-tune your GANs in the following research papers:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。如您所见，GAN在训练一段时间后变得非常擅长生成逼真的汽车图像，因为它在愚弄判别器方面越来越好。到最后几个周期时，人眼几乎无法分辨真假，至少在初看时是如此。此外，我们通过相对简单且直接的实现达到了这一点。考虑到生成器网络从未实际见过一张真实图像，这一成就显得更加令人惊讶。回想一下，它仅仅是从一个随机概率分布中进行采样，并仅通过判别器的反馈来改善自己的输出！正如我们所看到的，训练DCGAN的过程涉及了大量对细节的考虑，以及选择特定模型约束和超参数。对于感兴趣的读者，您可以在以下研究论文中找到更多关于如何优化和微调您的GAN的详细信息：
- en: '**Original paper on GANs**: [http://papers.nips.cc/paper/5423-generative-adversarial-nets](http://papers.nips.cc/paper/5423-generative-adversarial-nets)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关于GAN的原始论文**：[http://papers.nips.cc/paper/5423-generative-adversarial-nets](http://papers.nips.cc/paper/5423-generative-adversarial-nets)'
- en: '**Unsupervised representation learning with DCGAN**: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用DCGAN进行无监督表示学习**: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)'
- en: '**Photo-realistic super resolution GAN**: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**照片级超分辨率GAN**: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf)'
- en: Conclusion
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this section of the chapter, we implemented a specific type of GAN (that
    is, the DCGAN) for a specific use case (image generation). The idea of using two
    networks in parallel to keep each other in check, however, can be applied to various
    types of networks, for very different use cases. For example, if you wish to generate
    synthetic timeseries data, we can implement the same concepts we learned here
    with recurrent neural networks to design a generative adversarial model! There
    have been several attempts at this in the research community, with quite successful
    results. A group of Swedish researchers, for example, used recurrent neural networks
    in a generative adversarial setup to produce synthetic segments of classical music!
    Other prominent ideas with GANs involve using attention models (a topic unfortunately
    not covered by this book) to orient network perception, as well as directing memory
    access to finer details of an image, for example. Indeed, the fundamental theory
    we covered in this part of the chapter can be applied in many different realms,
    using different types of networks so solve more and more complex problems. The
    core idea remains the same: use two different function approximators, each trying
    to stay ahead of the other. Next, we present a few links for the interested reader
    to further familiarize themselves with different GAN-based architectures and their
    respective uses. We also include a link to a very interesting tool developed by
    Google and Georgia Tech university, that allows you to visualize the entire training
    process of a GAN using different types of data distributions and sampling considerations!'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一部分，我们实现了一种特定类型的GAN（即DCGAN），用于特定的应用场景（图像生成）。然而，使用两个网络并行工作，相互制约的思路，可以应用于多种类型的网络，解决非常不同的用例。例如，如果你希望生成合成的时间序列数据，我们可以将我们在这里学到的相同概念应用于递归神经网络，设计一个生成对抗模型！在研究界，已经有几次尝试，并取得了相当成功的结果。例如，一组瑞典研究人员就使用递归神经网络，在生成对抗框架下生成古典音乐的合成片段！与GAN相关的其他重要思想包括使用注意力模型（遗憾的是本书未涉及该话题）来引导网络的感知，并将记忆访问引导至图像的更精细细节。例如，我们在本章中讨论的基础理论可以应用于许多不同的领域，使用不同类型的网络来解决越来越复杂的问题。核心思想保持不变：使用两个不同的函数近似器，每个都试图超越另一个。接下来，我们将展示一些链接，供感兴趣的读者进一步了解不同的基于GAN的架构及其各自的应用。我们还包括一个由Google和乔治亚理工大学开发的非常有趣的工具的链接，它可以让你可视化使用不同类型的数据分布和采样考虑来训练GAN的整个过程！
- en: 'For more information, refer to the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参阅以下内容：
- en: '**Music with C-RNN_GAN**: [http://mogren.one/publications/2016/c-rnn-gan/mogren2016crnngan.pdf](http://mogren.one/publications/2016/c-rnn-gan/mogren2016crnngan.pdf)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C-RNN_GAN音乐生成**: [http://mogren.one/publications/2016/c-rnn-gan/mogren2016crnngan.pdf](http://mogren.one/publications/2016/c-rnn-gan/mogren2016crnngan.pdf)'
- en: '**Self- attention GANs**: [https://arxiv.org/abs/1805.08318](https://arxiv.org/abs/1805.08318)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自注意力GAN**: [https://arxiv.org/abs/1805.08318](https://arxiv.org/abs/1805.08318)'
- en: '**OpenAI blog on generative networks**: [https://openai.com/blog/generative-models/](https://openai.com/blog/generative-models/)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI 关于生成网络的博客**: [https://openai.com/blog/generative-models/](https://openai.com/blog/generative-models/)'
- en: '**GAN Lab**: [https://poloclub.github.io/ganlab/?fbclid=IwAR0JrixZYr1Ah3c08YjC6q34X0e38J7_mPdHaSpUsrRSsi0v97Y1DNQR6eU](https://poloclub.github.io/ganlab/?fbclid=IwAR0JrixZYr1Ah3c08YjC6q34X0e38J7_mPdHaSpUsrRSsi0v97Y1DNQR6eU)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GAN实验室**: [https://poloclub.github.io/ganlab/?fbclid=IwAR0JrixZYr1Ah3c08YjC6q34X0e38J7_mPdHaSpUsrRSsi0v97Y1DNQR6eU](https://poloclub.github.io/ganlab/?fbclid=IwAR0JrixZYr1Ah3c08YjC6q34X0e38J7_mPdHaSpUsrRSsi0v97Y1DNQR6eU)'
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how to augment neural networks with randomness in a
    systematic manner, in order to make them output instances of what we humans deem
    *creative*. With VAEs, we saw how parameterized function approximation using neural
    networks can be used to learn a probability distribution, over a continuous latent
    space. We then saw how to randomly sample from such a distribution and generate
    synthetic instances of the original data. In the second part of the chapter, we
    saw how two networks can be trained in an adversarial manner for a similar task.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何以系统化的方式通过随机性来增强神经网络，从而使它们输出我们人类认为是*创造性*的实例。通过变分自编码器（VAE），我们看到如何利用神经网络的参数化函数近似来学习一个连续潜在空间上的概率分布。接着，我们学习了如何从这样的分布中随机抽样，并生成原始数据的合成实例。在本章的第二部分，我们了解了如何以对抗的方式训练两个网络来完成类似的任务。
- en: The methodology of training GANs is simply a different strategy for learning
    a latent space compared to their counterpart, the VAE. While GANs have some key
    benefits for the use case of synthetic image generation, they do have some downsides
    as well. GANs are notoriously difficult to train and often generate images from
    unstructured and discontinuous latent spaces, as opposed to VAEs, making GANs
    harder to use for mining concept vectors. Many other considerations also exist
    when deciding to choose among these generative networks. The field of generative
    modeling is continuously expanding, and while we were able to cover some of the
    fundamental conceptual notions involved, new ideas and techniques surface almost
    daily, making it an exciting time to be interested in such models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 训练生成对抗网络（GAN）的方法论与变分自编码器（VAE）不同，是学习潜在空间的另一种策略。尽管GAN在生成合成图像的应用场景中有一些关键优势，但它们也有一些缺点。GAN的训练
    notoriously 难度较大，且通常生成来自无结构且不连续潜在空间的图像，而VAE则相对更为结构化，因此GAN在挖掘概念向量时更为困难。在选择这些生成网络时，还需要考虑许多其他因素。生成建模领域在不断扩展，尽管我们能够涵盖其中一些基本的概念性内容，但新的想法和技术几乎每天都在涌现，这使得研究这类模型成为一个激动人心的时刻。
