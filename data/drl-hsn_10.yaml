- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Stocks Trading Using RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‚¡ç¥¨äº¤æ˜“
- en: 'Rather than learning new methods to solve toy reinforcement learning (RL) problems
    in this chapter, we will try to utilize our deep Q-network (DQN) knowledge to
    deal with the much more practical problem of financial trading. I canâ€™t promise
    that the code will make you super rich on the stock market or Forex, because my
    goal is much less ambitious: to demonstrate how to go beyond the Atari games and
    apply RL to a different practical domain.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æˆ‘ä»¬ä¸ä¼šå­¦ä¹ è§£å†³ç©å…·å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜çš„æ–°æ–¹æ³•ï¼Œè€Œæ˜¯å°è¯•åˆ©ç”¨æˆ‘ä»¬åœ¨æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰æ–¹é¢çš„çŸ¥è¯†æ¥å¤„ç†æ›´å®é™…çš„é‡‘èäº¤æ˜“é—®é¢˜ã€‚æˆ‘ä¸èƒ½ä¿è¯ä»£ç ä¼šè®©ä½ åœ¨è‚¡å¸‚æˆ–å¤–æ±‡å¸‚åœºä¸Šå˜å¾—è¶…çº§å¯Œæœ‰ï¼Œå› ä¸ºæˆ‘çš„ç›®æ ‡è¿œæ²¡æœ‰é‚£ä¹ˆé›„å¿ƒå‹ƒå‹ƒï¼šæˆ‘æƒ³å±•ç¤ºå¦‚ä½•è¶…è¶Šé›…è¾¾åˆ©æ¸¸æˆï¼Œå¹¶å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºä¸åŒçš„å®é™…é¢†åŸŸã€‚
- en: 'In this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: Implement our own OpenAI Gym environment to simulate the stock market
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°æˆ‘ä»¬è‡ªå·±çš„ OpenAI Gym ç¯å¢ƒä»¥æ¨¡æ‹Ÿè‚¡å¸‚
- en: Apply the DQN method that you learned in ChapterÂ [6](#) and ChapterÂ [8](ch012.xhtml#x1-1240008)
    to train an agent to trade stocks to maximize profit
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨ä½ åœ¨ç¬¬[6](#)ç« å’Œç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­å­¦åˆ°çš„ DQN æ–¹æ³•ï¼Œè®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“è¿›è¡Œè‚¡ç¥¨äº¤æ˜“ï¼Œä»¥æœ€å¤§åŒ–åˆ©æ¶¦
- en: Why trading?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåšäº¤æ˜“ï¼Ÿ
- en: 'There are a lot of financial instruments traded on markets every day: goods,
    stocks, and currencies. Even weather forecasts can be bought or sold using so-called
    â€œweather derivatives,â€ which is just a consequence of the complexity of the modern
    world and financial markets. If your income depends on future weather conditions,
    as it does for a business growing crops, then you might want to hedge the risks
    by buying weather derivatives. All these different items have a price that changes
    over time. Trading is the activity of buying and selling financial instruments
    with different goals, like making a profit (investment), gaining protection from
    future price movement (hedging), or just getting what you need (like buying steel
    or exchanging USD for JPY to pay a contract).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å¤©åœ¨å¸‚åœºä¸Šäº¤æ˜“çš„é‡‘èå·¥å…·ç§ç±»ç¹å¤šï¼šå•†å“ã€è‚¡ç¥¨å’Œè´§å¸ã€‚å³ä½¿æ˜¯å¤©æ°”é¢„æŠ¥ä¹Ÿå¯ä»¥é€šè¿‡æ‰€è°“çš„â€œå¤©æ°”è¡ç”Ÿå“â€è¿›è¡Œä¹°å–ï¼Œè¿™åªæ˜¯ç°ä»£ä¸–ç•Œå’Œé‡‘èå¸‚åœºå¤æ‚æ€§çš„ä¸€ä¸ªè¡¨ç°ã€‚å¦‚æœä½ çš„æ”¶å…¥å–å†³äºæœªæ¥çš„å¤©æ°”æ¡ä»¶ï¼Œå°±åƒç§æ¤ä½œç‰©çš„ä¼ä¸šä¸€æ ·ï¼Œä½ å¯èƒ½ä¼šé€šè¿‡è´­ä¹°å¤©æ°”è¡ç”Ÿå“æ¥å¯¹å†²é£é™©ã€‚æ‰€æœ‰è¿™äº›ä¸åŒçš„ç‰©å“éƒ½æœ‰éšæ—¶é—´å˜åŒ–çš„ä»·æ ¼ã€‚äº¤æ˜“æ˜¯ä¹°å–é‡‘èå·¥å…·çš„æ´»åŠ¨ï¼Œç›®çš„æ˜¯ä¸ºäº†ä¸åŒçš„ç›®æ ‡ï¼Œå¦‚è·å–åˆ©æ¶¦ï¼ˆæŠ•èµ„ï¼‰ã€ä»æœªæ¥çš„ä»·æ ¼æ³¢åŠ¨ä¸­è·å¾—ä¿æŠ¤ï¼ˆå¯¹å†²ï¼‰æˆ–åªæ˜¯è·å–æ‰€éœ€çš„ä¸œè¥¿ï¼ˆä¾‹å¦‚è´­ä¹°é’¢é“æˆ–å°†ç¾å…ƒå…‘æ¢ä¸ºæ—¥å…ƒæ”¯ä»˜åˆåŒï¼‰ã€‚
- en: Since the first financial market was established, people have been trying to
    predict future price movements, as this promises many benefits, like â€œprofit from
    nowhereâ€ or protecting capital from sudden market movements. This problem is known
    to be complex, and there are a lot of financial consultants, investment funds,
    banks, and individual traders trying to predict the market and find the best moments
    to buy and sell to maximize profit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»ç¬¬ä¸€ä¸ªé‡‘èå¸‚åœºå»ºç«‹ä»¥æ¥ï¼Œäººä»¬å°±ä¸€ç›´åœ¨å°è¯•é¢„æµ‹æœªæ¥çš„ä»·æ ¼èµ°åŠ¿ï¼Œå› ä¸ºè¿™èƒ½å¸¦æ¥å¾ˆå¤šå¥½å¤„ï¼Œæ¯”å¦‚â€œä»æ— ä¸­èµšå–åˆ©æ¶¦â€æˆ–ä¿æŠ¤èµ„æœ¬å…å—çªå¦‚å…¶æ¥çš„å¸‚åœºæ³¢åŠ¨ã€‚è¿™ä¸€é—®é¢˜è¢«è®¤ä¸ºæ˜¯å¤æ‚çš„ï¼Œå› æ­¤æœ‰å¾ˆå¤šé‡‘èé¡¾é—®ã€æŠ•èµ„åŸºé‡‘ã€é“¶è¡Œå’Œä¸ªäººäº¤æ˜“è€…åœ¨å°è¯•é¢„æµ‹å¸‚åœºï¼Œå¹¶å¯»æ‰¾æœ€ä½³çš„ä¹°å–æ—¶æœºä»¥æœ€å¤§åŒ–åˆ©æ¶¦ã€‚
- en: 'The question is: can we look at the problem from the RL angle? Letâ€™s say that
    we have some observation of the market, and we want to make a decision: buy, sell,
    or wait. If we buy before the price goes up, our profit will be positive; otherwise,
    we will get a negative reward. What weâ€™re trying to do is get as much profit as
    possible. The connections between market trading and RL are quite obvious. First,
    letâ€™s define the problem statement more clearly.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜æ˜¯ï¼šæˆ‘ä»¬èƒ½å¦ä»å¼ºåŒ–å­¦ä¹ çš„è§’åº¦æ¥çœ‹å¾…è¿™ä¸ªé—®é¢˜ï¼Ÿå‡è®¾æˆ‘ä»¬å¯¹å¸‚åœºæœ‰ä¸€äº›è§‚å¯Ÿï¼Œå¹¶ä¸”æˆ‘ä»¬éœ€è¦åšå‡ºä¸€ä¸ªå†³ç­–ï¼šä¹°å…¥ã€å–å‡ºæˆ–ç­‰å¾…ã€‚å¦‚æœæˆ‘ä»¬åœ¨ä»·æ ¼ä¸Šæ¶¨ä¹‹å‰ä¹°å…¥ï¼Œæˆ‘ä»¬çš„åˆ©æ¶¦å°†æ˜¯æ­£çš„ï¼›å¦åˆ™ï¼Œæˆ‘ä»¬å°†è·å¾—è´Ÿå¥–åŠ±ã€‚æˆ‘ä»¬è¯•å›¾åšçš„æ˜¯å°½å¯èƒ½è·å¾—æ›´å¤šçš„åˆ©æ¶¦ã€‚å¸‚åœºäº¤æ˜“å’Œå¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„è”ç³»éå¸¸æ˜æ˜¾ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ›´æ¸…æ¥šåœ°å®šä¹‰é—®é¢˜é™ˆè¿°ã€‚
- en: Problem statement and key decisions
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜é™ˆè¿°ä¸å…³é”®å†³ç­–
- en: The finance domain is large and complex, so you can easily spend several years
    learning something new every day. In our example, we will just scratch the surface
    a bit with our RL tools, and our problem will be formulated as simply as possible,
    using price as an observation. We will investigate whether it will be possible
    for our agent to learn when the best time is to buy one single share and then
    close the position to maximize the profit. The purpose of this example is to show
    how flexible the RL model can be and what the first steps are that you usually
    need to take to apply RL to a real-life use case.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‘èé¢†åŸŸåºå¤§è€Œå¤æ‚ï¼Œå› æ­¤ä½ å¾ˆå®¹æ˜“èŠ±è´¹å‡ å¹´æ—¶é—´æ¯å¤©å­¦ä¹ æ–°çš„å†…å®¹ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»…ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·¥å…·ç¨å¾®è§¦åŠä¸€ä¸‹è¡¨é¢ï¼Œé—®é¢˜å°†å°½å¯èƒ½ç®€å•åœ°è¢«è¡¨è¿°ï¼Œä½¿ç”¨ä»·æ ¼ä½œä¸ºè§‚å¯Ÿå€¼ã€‚æˆ‘ä»¬å°†ç ”ç©¶æˆ‘ä»¬çš„æ™ºèƒ½ä½“æ˜¯å¦èƒ½å¤Ÿå­¦ä¹ åœ¨æœ€ä½³æ—¶æœºè´­ä¹°ä¸€åªè‚¡ç¥¨ï¼Œå¹¶åœ¨éšåå¹³ä»“ä»¥æœ€å¤§åŒ–åˆ©æ¶¦ã€‚è¿™ä¸ªä¾‹å­çš„ç›®çš„æ˜¯å±•ç¤ºRLæ¨¡å‹çš„çµæ´»æ€§ï¼Œä»¥åŠä½ é€šå¸¸éœ€è¦é‡‡å–çš„ç¬¬ä¸€æ­¥æ¥å°†RLåº”ç”¨åˆ°å®é™…çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ã€‚
- en: 'As you already know, to formulate RL problems, three things are needed: observation
    of the environment, possible actions, and a reward system. In previous chapters,
    all three were already given to us, and the internal machinery of the environment
    was hidden. Now weâ€™re in a different situation, so we need to decide ourselves
    what our agent will see and what set of actions it can take. The reward system
    is also not given as a strict set of rules; rather, it will be guided by our feelings
    and knowledge of the domain, which gives us lots of flexibility.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ å·²ç»çŸ¥é“çš„ï¼Œè¦åˆ¶å®šRLé—®é¢˜ï¼Œéœ€è¦ä¸‰ä»¶äº‹ï¼šç¯å¢ƒè§‚å¯Ÿã€å¯èƒ½çš„åŠ¨ä½œå’Œå¥–åŠ±ç³»ç»Ÿã€‚åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæ‰€æœ‰ä¸‰è€…å·²ç»ç»™å®šï¼Œå¹¶ä¸”ç¯å¢ƒçš„å†…éƒ¨æœºåˆ¶æ˜¯éšè—çš„ã€‚ç°åœ¨æˆ‘ä»¬å¤„äºä¸åŒçš„æƒ…å†µï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è‡ªå·±å†³å®šæ™ºèƒ½ä½“å°†çœ‹åˆ°ä»€ä¹ˆä»¥åŠå®ƒå¯ä»¥é‡‡å–å“ªäº›åŠ¨ä½œã€‚å¥–åŠ±ç³»ç»Ÿä¹Ÿæ²¡æœ‰ä¸¥æ ¼çš„è§„åˆ™ï¼Œè€Œæ˜¯ç”±æˆ‘ä»¬å¯¹é¢†åŸŸçš„æ„Ÿè§‰å’ŒçŸ¥è¯†å¼•å¯¼ï¼Œè¿™ç»™äº†æˆ‘ä»¬å¾ˆå¤§çš„çµæ´»æ€§ã€‚
- en: 'Flexibility, in this case, is good and bad at the same time. Itâ€™s good that
    we have the freedom to pass some information to the agent that we feel will be
    important to learn efficiently. For example, you can provide to the trading agent
    not only prices, but also news or important statistics (which are known to influence
    financial markets a lot). The bad part is that this flexibility usually means
    that to find a good agent, you need to try a lot of variants of data representation,
    and itâ€™s not always obvious which will work better. In our case, we will implement
    the basic trading agent in its simplest form, as we discussed in ChapterÂ [1](ch005.xhtml#x1-190001):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: çµæ´»æ€§åœ¨è¿™ç§æƒ…å†µä¸‹æ—¢æ˜¯å¥½äº‹ï¼Œä¹Ÿæ˜¯åäº‹ã€‚å¥½çš„ä¸€é¢æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªç”±åœ°ä¼ é€’ä¸€äº›æˆ‘ä»¬è®¤ä¸ºå¯¹é«˜æ•ˆå­¦ä¹ å¾ˆé‡è¦çš„ä¿¡æ¯ç»™æ™ºèƒ½ä½“ã€‚ä¾‹å¦‚ï¼Œé™¤äº†ä»·æ ¼ï¼Œä½ è¿˜å¯ä»¥å‘äº¤æ˜“æ™ºèƒ½ä½“æä¾›æ–°é—»æˆ–é‡è¦ç»Ÿè®¡æ•°æ®ï¼ˆè¿™äº›è¢«è®¤ä¸ºå¯¹é‡‘èå¸‚åœºæœ‰å¾ˆå¤§å½±å“ï¼‰ã€‚åçš„ä¸€é¢æ˜¯ï¼Œè¿™ç§çµæ´»æ€§é€šå¸¸æ„å‘³ç€ä¸ºäº†æ‰¾åˆ°ä¸€ä¸ªä¼˜ç§€çš„æ™ºèƒ½ä½“ï¼Œä½ éœ€è¦å°è¯•è®¸å¤šä¸åŒçš„æ•°æ®è¡¨ç¤ºæ–¹å¼ï¼Œè€Œå“ªäº›æ–¹å¼æ›´æœ‰æ•ˆé€šå¸¸å¹¶ä¸æ˜æ˜¾ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°æœ€åŸºæœ¬çš„äº¤æ˜“æ™ºèƒ½ä½“ï¼Œä»¥å…¶æœ€ç®€å•çš„å½¢å¼ï¼Œå°±åƒæˆ‘ä»¬åœ¨ç¬¬[1](ch005.xhtml#x1-190001)ç« ä¸­è®¨è®ºçš„é‚£æ ·ï¼š
- en: '**Observation:** The observation will include the following information:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§‚å¯Ÿï¼š** è§‚å¯Ÿå°†åŒ…æ‹¬ä»¥ä¸‹ä¿¡æ¯ï¼š'
- en: N past bars, where each has open, high, low, and close prices
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nä¸ªè¿‡å»çš„æ—¶æ®µï¼Œå…¶ä¸­æ¯ä¸ªæ—¶æ®µéƒ½æœ‰å¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·å’Œæ”¶ç›˜ä»·ã€‚
- en: An indication that the share was bought some time ago (only one share at a time
    will be possible)
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæŒ‡ç¤ºï¼Œè¡¨æ˜è‚¡ç¥¨åœ¨ä¸€æ®µæ—¶é—´å‰å·²è¢«è´­ä¹°ï¼ˆä¸€æ¬¡åªèƒ½è´­ä¹°ä¸€åªè‚¡ç¥¨ï¼‰ã€‚
- en: The profit or loss that we currently have from our current position (the share
    bought)
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å½“å‰æŒä»“ï¼ˆå·²è´­å…¥çš„è‚¡ç¥¨ï¼‰æ‰€å¸¦æ¥çš„ç›ˆäºã€‚
- en: '**Action:** At every step, after every minuteâ€™s bar, the agent can take one
    of the following actions:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠ¨ä½œï¼š** åœ¨æ¯ä¸€æ­¥ï¼Œæ¯ä¸€åˆ†é’Ÿçš„æ—¶æ®µç»“æŸåï¼Œæ™ºèƒ½ä½“å¯ä»¥é‡‡å–ä»¥ä¸‹ä¹‹ä¸€çš„åŠ¨ä½œï¼š'
- en: 'Do nothing: Skip the bar without taking an action'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆéƒ½ä¸åšï¼šè·³è¿‡å½“å‰çš„æ—¶æ®µï¼Œä¸é‡‡å–ä»»ä½•è¡ŒåŠ¨ã€‚
- en: 'Buy a share: If the agent has already got the share, nothing will be bought;
    otherwise, we will pay the commission, which is usually some small percentage
    of the current price'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¹°å…¥è‚¡ç¥¨ï¼šå¦‚æœæ™ºèƒ½ä½“å·²ç»æ‹¥æœ‰è‚¡ç¥¨ï¼Œåˆ™ä¸ä¼šè¿›è¡Œè´­ä¹°ï¼›å¦åˆ™ï¼Œæˆ‘ä»¬å°†æ”¯ä»˜ä½£é‡‘ï¼Œé€šå¸¸æ˜¯å½“å‰ä»·æ ¼çš„ä¸€å°éƒ¨åˆ†ã€‚
- en: 'Close the position: If we do not have a previously purchased share, nothing
    will happen; otherwise, we will pay the commission for the trade'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹³ä»“ï¼šå¦‚æœæˆ‘ä»¬æ²¡æœ‰ä¹‹å‰è´­ä¹°çš„è‚¡ç¥¨ï¼Œä»€ä¹ˆéƒ½ä¸ä¼šå‘ç”Ÿï¼›å¦åˆ™ï¼Œæˆ‘ä»¬å°†æ”¯ä»˜äº¤æ˜“ä½£é‡‘ã€‚
- en: '**Reward:** The reward that the agent receives can be expressed in various
    ways:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¥–åŠ±ï¼š** æ™ºèƒ½ä½“æ”¶åˆ°çš„å¥–åŠ±å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¡¨è¾¾ï¼š'
- en: As the first option, we can split the reward into multiple steps during our
    ownership of the share. In that case, the reward on every step will be equal to
    the last barâ€™s movement.
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸€ç§é€‰æ‹©ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¥–åŠ±åˆ†æˆå¤šä¸ªæ­¥éª¤ï¼Œåœ¨æˆ‘ä»¬æŒæœ‰è‚¡ç¥¨æœŸé—´æ¯ä¸€æ­¥çš„å¥–åŠ±å°†ç­‰äºæœ€åä¸€ä¸ªæ—¶æ®µçš„ä»·æ ¼æ³¢åŠ¨ã€‚
- en: Alternatively, the agent can receive the reward only after the close action
    and get the full reward at once.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ™ºèƒ½ä½“å¯ä»¥åœ¨å¹³ä»“åŠ¨ä½œä¹‹åæ‰æ”¶åˆ°å¥–åŠ±ï¼Œå¹¶ä¸€æ¬¡æ€§è·å¾—å…¨éƒ¨å¥–åŠ±ã€‚
- en: At first sight, both variants should have the same final result, but maybe with
    different convergence speeds. However, in practice, the difference could be dramatic.
    The environment in my implementation supports both variants, so you can experiment
    with the difference.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åˆçœ‹èµ·æ¥ï¼Œä¸¤ç§å˜ä½“åº”è¯¥æœ‰ç›¸åŒçš„æœ€ç»ˆç»“æœï¼Œåªæ˜¯æ”¶æ•›é€Ÿåº¦å¯èƒ½ä¸åŒã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå·®å¼‚å¯èƒ½æ˜¯å·¨å¤§çš„ã€‚æˆ‘çš„å®ç°ä¸­çš„ç¯å¢ƒæ”¯æŒè¿™ä¸¤ç§å˜ä½“ï¼Œå› æ­¤æ‚¨å¯ä»¥å®éªŒå®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚
- en: One last decision to make is how to represent the prices in our environment
    observation. Ideally, we would like our agent to be independent of actual price
    values and take into account relative movement, such as â€œthe stock has grown 1%
    during the last barâ€ or â€œthe stock has lost 5%.â€ This makes sense, as different
    stocksâ€™ prices can vary, but they can have similar movement patterns. In finance,
    there is a branch of analytics called technical analysis that studies such patterns
    to help to make predictions from them. We would like our system to be able to
    discover the patterns (if they exist). To achieve this, we will convert every
    barâ€™s open, high, low, and close prices to three numbers showing high, low, and
    close prices represented as a percentage of the open price.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªéœ€è¦åšå‡ºçš„å†³ç­–æ˜¯å¦‚ä½•åœ¨æˆ‘ä»¬çš„ç¯å¢ƒè§‚å¯Ÿä¸­è¡¨ç¤ºä»·æ ¼ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ä»£ç†èƒ½å¤Ÿç‹¬ç«‹äºå®é™…çš„ä»·æ ¼å€¼ï¼Œå¹¶è€ƒè™‘ç›¸å¯¹å˜åŠ¨ï¼Œæ¯”å¦‚â€œè‚¡ç¥¨åœ¨ä¸Šä¸€æ ¹Kçº¿ä¸­å¢é•¿äº†1%â€æˆ–â€œè‚¡ç¥¨ä¸‹é™äº†5%â€ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºä¸åŒè‚¡ç¥¨çš„ä»·æ ¼å¯èƒ½ä¸åŒï¼Œä½†å®ƒä»¬å¯èƒ½æœ‰ç±»ä¼¼çš„å˜åŠ¨æ¨¡å¼ã€‚åœ¨é‡‘èé¢†åŸŸï¼Œæœ‰ä¸€é—¨åˆ†æå­¦ç§‘å«åšæŠ€æœ¯åˆ†æï¼Œä¸“é—¨ç ”ç©¶è¿™ç§æ¨¡å¼ï¼Œå¹¶é€šè¿‡å®ƒä»¬æ¥è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç³»ç»Ÿèƒ½å¤Ÿå‘ç°è¿™äº›æ¨¡å¼ï¼ˆå¦‚æœå®ƒä»¬å­˜åœ¨ï¼‰ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†æŠŠæ¯æ ¹Kçº¿çš„å¼€ç›˜ã€æœ€é«˜ã€æœ€ä½å’Œæ”¶ç›˜ä»·æ ¼è½¬æ¢ä¸ºä¸‰ä¸ªæ•°å€¼ï¼Œè¡¨ç¤ºå¼€ç›˜ä»·çš„ç™¾åˆ†æ¯”å½¢å¼çš„æœ€é«˜ä»·ã€æœ€ä½ä»·å’Œæ”¶ç›˜ä»·ã€‚
- en: This representation has its own drawbacks, as weâ€™re potentially losing the information
    about key price levels. For example, itâ€™s known that markets have a tendency to
    bounce from round price numbers (like $70,000 per bitcoin) and levels that were
    turning points in the past. However, as already stated, weâ€™re just playing with
    the data here and checking the concept. Representation in the form of relative
    price movement will help the system to find repeating patterns in the price level
    (if they exist, of course), regardless of the absolute price position. Potentially,
    the neural network (NN) could learn this on its own (itâ€™s just the mean price
    that needs to be subtracted from the absolute price values), but relative representation
    simplifies the NNâ€™s task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¡¨ç¤ºæ–¹æ³•æœ‰å…¶è‡ªèº«çš„ç¼ºç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½ä¼šå¤±å»å…³äºå…³é”®ä»·æ ¼æ°´å¹³çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå·²çŸ¥å¸‚åœºæœ‰ä¸€ä¸ªå€¾å‘ï¼Œå³ä»æ•´æ•°ä»·æ ¼æ°´å¹³ï¼ˆå¦‚æ¯ä¸ªæ¯”ç‰¹å¸70,000ç¾å…ƒï¼‰å’Œè¿‡å»æ›¾ç»æ˜¯è½¬æŠ˜ç‚¹çš„ä»·æ ¼æ°´å¹³åå¼¹ã€‚ç„¶è€Œï¼Œæ­£å¦‚å‰é¢æ‰€è¯´ï¼Œæˆ‘ä»¬è¿™é‡Œåªæ˜¯ç©å¼„æ•°æ®å¹¶æ£€æŸ¥è¿™ä¸ªæ¦‚å¿µã€‚ä»¥ç›¸å¯¹ä»·æ ¼å˜åŠ¨çš„å½¢å¼è¡¨ç¤ºå°†æœ‰åŠ©äºç³»ç»Ÿåœ¨ä»·æ ¼æ°´å¹³ä¸­å‘ç°é‡å¤çš„æ¨¡å¼ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰ï¼Œè€Œä¸ç®¡ç»å¯¹ä»·æ ¼ä½ç½®å¦‚ä½•ã€‚æ½œåœ¨åœ°ï¼Œç¥ç»ç½‘ç»œï¼ˆNNï¼‰å¯èƒ½ä¼šè‡ªå·±å­¦ä¼šè¿™ä¸€ç‚¹ï¼ˆåªéœ€ä»ç»å¯¹ä»·æ ¼å€¼ä¸­å‡å»å‡ä»·ï¼‰ï¼Œä½†ç›¸å¯¹è¡¨ç¤ºç®€åŒ–äº†ç¥ç»ç½‘ç»œçš„ä»»åŠ¡ã€‚
- en: Data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®
- en: In our example, we will use the Russian stock market prices from the period
    of 2015-2016, which are placed in Chapter10/data/ch10-small-quotes.tgz and have
    to be unpacked before model training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨2015-2016å¹´æœŸé—´çš„ä¿„ç½—æ–¯è‚¡å¸‚ä»·æ ¼ï¼Œè¿™äº›æ•°æ®å­˜æ”¾åœ¨Chapter10/data/ch10-small-quotes.tgzä¸­ï¼Œæ¨¡å‹è®­ç»ƒä¹‹å‰éœ€è¦è§£å‹ã€‚
- en: 'Inside the archive, we have CSV files with M1 bars, which means that every
    row in each CSV file corresponds to a single minute in time, and price movement
    during that minute is captured with four prices:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¡£æ¡ˆä¸­ï¼Œæˆ‘ä»¬æœ‰åŒ…å«M1æ¡å½¢å›¾çš„CSVæ–‡ä»¶ï¼Œè¿™æ„å‘³ç€æ¯è¡Œå¯¹åº”ä¸€ä¸ªæ—¶é—´å•ä½å†…çš„å•ä¸€åˆ†é’Ÿï¼Œå¹¶ä¸”è¯¥åˆ†é’Ÿå†…çš„ä»·æ ¼å˜åŠ¨ç”±å››ä¸ªä»·æ ¼è®°å½•ï¼š
- en: 'Open: The price at the beginning of the minute'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€ç›˜ï¼šä¸€åˆ†é’Ÿå¼€å§‹æ—¶çš„ä»·æ ¼
- en: 'High: The maximum price during the interval'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜ï¼šåŒºé—´å†…çš„æœ€é«˜ä»·æ ¼
- en: 'Low: The minimum price'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€ä½ï¼šæœ€ä½ä»·æ ¼
- en: 'Close: The last price of the minute time interval'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¶ç›˜ï¼šè¿™ä¸€åˆ†é’Ÿæ—¶é—´åŒºé—´çš„æœ€åä»·æ ¼
- en: 'Every minute interval is called a bar and allows us to have an idea of price
    movement within the interval. For example, in the YNDX_160101_161231.csv file
    (which has Yandex company stocks for 2016), we have 130k lines in this form:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸€åˆ†é’Ÿçš„æ—¶é—´é—´éš”ç§°ä¸ºä¸€ä¸ªKçº¿ï¼Œå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿäº†è§£è¿™ä¸€æ—¶é—´æ®µå†…çš„ä»·æ ¼å˜åŠ¨ã€‚ä¾‹å¦‚ï¼Œåœ¨YNDX_160101_161231.csvæ–‡ä»¶ä¸­ï¼ˆåŒ…å«2016å¹´Yandexå…¬å¸è‚¡ç¥¨æ•°æ®ï¼‰ï¼Œæˆ‘ä»¬æœ‰130,000è¡Œæ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first two columns are the date and time for the minute; the next four columns
    are open, high, low, and close prices; and the last value represents the number
    of buy and sell orders performed during the bar (also called volume). The exact
    interpretation of volume is market-dependent, but usually, it give you an idea
    about how active the market was.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸¤åˆ—æ˜¯æ—¥æœŸå’Œåˆ†é’Ÿæ—¶é—´ï¼›æ¥ä¸‹æ¥çš„å››åˆ—æ˜¯å¼€ç›˜ã€æœ€é«˜ã€æœ€ä½å’Œæ”¶ç›˜ä»·æ ¼ï¼›æœ€åä¸€ä¸ªå€¼è¡¨ç¤ºåœ¨è¯¥Kçº¿æœŸé—´æ‰§è¡Œçš„ä¹°å–è®¢å•æ•°é‡ï¼ˆä¹Ÿç§°ä¸ºæˆäº¤é‡ï¼‰ã€‚æˆäº¤é‡çš„å…·ä½“è§£é‡Šå–å†³äºå¸‚åœºï¼Œä½†é€šå¸¸å®ƒèƒ½è®©ä½ äº†è§£å¸‚åœºçš„æ´»è·ƒåº¦ã€‚
- en: 'The typical way to represent those prices is called a candlestick chart, where
    every bar is shown as a candle. Part of Yandexâ€™s quotes for one day in February
    2016 is shown in the following chart:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºè¿™äº›ä»·æ ¼çš„å…¸å‹æ–¹å¼è¢«ç§°ä¸ºèœ¡çƒ›å›¾ï¼Œæ¯ä¸ªæ¡å½¢å›¾æ˜¾ç¤ºä¸ºä¸€æ ¹èœ¡çƒ›ã€‚ä»¥ä¸‹æ˜¯ Yandex 2016 å¹´ 2 æœˆä¸€å¤©éƒ¨åˆ†æŠ¥ä»·çš„å›¾è¡¨ï¼š
- en: '![PIC](img/file95.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file95.png)'
- en: 'FigureÂ 10.1: Price data for Yandex in February 2016'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.1ï¼š2016 å¹´ 2 æœˆ Yandex çš„ä»·æ ¼æ•°æ®
- en: The archive contains two files with M1 data for 2016 and 2015\. We will use
    data from 2016 for model training and data from 2015 for validation (but the order
    is arbitrary; you can swap them or even use different time intervals and check
    the effect).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜æ¡£ä¸­åŒ…å« 2016 å¹´å’Œ 2015 å¹´çš„ M1 æ•°æ®æ–‡ä»¶ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ 2016 å¹´çš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨ 2015 å¹´çš„æ•°æ®è¿›è¡ŒéªŒè¯ï¼ˆä½†é¡ºåºæ˜¯ä»»æ„çš„ï¼Œä½ å¯ä»¥äº¤æ¢å®ƒä»¬ï¼Œç”šè‡³ä½¿ç”¨ä¸åŒçš„æ—¶é—´é—´éš”å¹¶æ£€æŸ¥æ•ˆæœï¼‰ã€‚
- en: The trading environment
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äº¤æ˜“ç¯å¢ƒ
- en: As we have a lot of code that is supposed to work with the Gym API, we will
    implement the trading functionality following Gymâ€™s Env class, which should be
    already familiar to you. Our environment is implemented in the StocksEnv class
    in the Chapter10/lib/environ.py module. It uses several internal classes to keep
    its state and encode observations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬æœ‰å¾ˆå¤šä»£ç éœ€è¦ä¸ Gym API é…åˆå·¥ä½œï¼Œæˆ‘ä»¬å°†å®ç°äº¤æ˜“åŠŸèƒ½ï¼Œéµå¾ª Gym çš„ Env ç±»ï¼Œæ‚¨åº”è¯¥å·²ç»ç†Ÿæ‚‰è¿™ä¸ªç±»äº†ã€‚æˆ‘ä»¬çš„ç¯å¢ƒåœ¨ Chapter10/lib/environ.py
    æ¨¡å—ä¸­çš„ StocksEnv ç±»ä¸­å®ç°ã€‚å®ƒä½¿ç”¨å‡ ä¸ªå†…éƒ¨ç±»æ¥ä¿æŒå…¶çŠ¶æ€å¹¶ç¼–ç è§‚å¯Ÿã€‚
- en: 'Letâ€™s first look at the public API class:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹å…¬å…± API ç±»ï¼š
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We encode all available actions as an enumeratorâ€™s fields and provide just
    three actions: do nothing, buy a single share, and close the existing position.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ‰€æœ‰å¯ç”¨çš„æ“ä½œç¼–ç ä¸ºæšä¸¾å­—æ®µï¼Œå¹¶ä»…æä¾›ä¸‰ä¸ªæ“ä½œï¼šä»€ä¹ˆéƒ½ä¸åšï¼Œä¹°å…¥å•ä¸€è‚¡ç¥¨ï¼Œå…³é—­ç°æœ‰ä»“ä½ã€‚
- en: In our market model, we allow only the single share to be bought, neither supporting
    extending existing positions nor opening â€œshort positionsâ€ (when you selling the
    share you donâ€™t have, expecting the price to decrease in the future). That was
    an intentional decision, as I tried to keep the example simple and to avoid overcomplications.
    Why donâ€™t you try experimenting with other options?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å¸‚åœºæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬åªå…è®¸è´­ä¹°å•ä¸€è‚¡ç¥¨ï¼Œä¸æ”¯æŒæ‰©å±•ç°æœ‰ä»“ä½æˆ–å¼€è®¾â€œå–ç©ºä»“ä½â€ï¼ˆå³å½“ä½ å–å‡ºä½ æ²¡æœ‰çš„è‚¡ç¥¨æ—¶ï¼Œé¢„è®¡æœªæ¥è‚¡ä»·ä¼šä¸‹é™ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰æ„çš„å†³ç­–ï¼Œå› ä¸ºæˆ‘è¯•å›¾ä¿æŒä¾‹å­ç®€æ´ï¼Œé¿å…è¿‡äºå¤æ‚ã€‚ä¸ºä»€ä¹ˆä¸å°è¯•ç”¨å…¶ä»–é€‰é¡¹è¿›è¡Œå®éªŒå‘¢ï¼Ÿ
- en: 'Next, we have the environment class:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æœ‰ç¯å¢ƒç±»ï¼š
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The field spec is required for gym.Env compatibility and registers our environment
    in the Gym internal registry.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å­—æ®µè§„èŒƒå¯¹äº gym.Env å…¼å®¹æ€§æ˜¯å¿…éœ€çš„ï¼Œå¹¶å°†æˆ‘ä»¬çš„ç¯å¢ƒæ³¨å†Œåˆ° Gym çš„å†…éƒ¨æ³¨å†Œè¡¨ä¸­ã€‚
- en: 'This class provides two ways to create its instance:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»æä¾›äº†ä¸¤ç§æ–¹å¼æ¥åˆ›å»ºå…¶å®ä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see in the preceding code, the first way is to call the class method
    from_dir with the data directory as the argument. In that case, it will load all
    quotes from the CSV files in the directory and construct the environment. To deal
    with price data in our form, we have several helper functions in Chapter10/lib/data.py.
    Another way is to construct the class instance directly. In that case, you should
    pass the prices dictionary, which has to map the quote name to the Prices dataclass
    declared in data.py. This object has five fields containing open, high, low, close,
    and volume time series as one-dimensional NumPy arrays. The module data.py also
    provides several helping functions, like converting the prices into relative format,
    enumerating files in the given directory, etc.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰é¢çš„ä»£ç æ‰€ç¤ºï¼Œç¬¬ä¸€ç§æ–¹å¼æ˜¯è°ƒç”¨ç±»æ–¹æ³• from_dirï¼Œå°†æ•°æ®ç›®å½•ä½œä¸ºå‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†ä»ç›®å½•ä¸­çš„ CSV æ–‡ä»¶åŠ è½½æ‰€æœ‰æŠ¥ä»·ï¼Œå¹¶æ„å»ºç¯å¢ƒã€‚ä¸ºäº†å¤„ç†æˆ‘ä»¬æ ¼å¼çš„ä»·æ ¼æ•°æ®ï¼ŒChapter10/lib/data.py
    ä¸­æœ‰å‡ ä¸ªè¾…åŠ©å‡½æ•°ã€‚å¦ä¸€ç§æ–¹å¼æ˜¯ç›´æ¥æ„é€ ç±»å®ä¾‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥ä¼ é€’ä»·æ ¼å­—å…¸ï¼Œè¯¥å­—å…¸å¿…é¡»å°†æŠ¥ä»·åç§°æ˜ å°„åˆ° data.py ä¸­å£°æ˜çš„ Prices æ•°æ®ç±»ã€‚è¿™ä¸ªå¯¹è±¡æœ‰äº”ä¸ªå­—æ®µï¼ŒåŒ…å«å¼€ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æ”¶ç›˜å’Œæˆäº¤é‡çš„æ—¶é—´åºåˆ—ï¼Œè¿™äº›å­—æ®µéƒ½æ˜¯ä¸€ç»´çš„
    NumPy æ•°ç»„ã€‚data.py æ¨¡å—è¿˜æä¾›äº†å‡ ä¸ªå¸®åŠ©å‡½æ•°ï¼Œå¦‚å°†ä»·æ ¼è½¬æ¢ä¸ºç›¸å¯¹æ ¼å¼ã€æšä¸¾ç»™å®šç›®å½•ä¸­çš„æ–‡ä»¶ç­‰ã€‚
- en: 'The following is the constructor of the environment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç¯å¢ƒçš„æ„é€ å‡½æ•°ï¼š
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It accepts a lot of arguments to tweak the environmentâ€™s behavior and observation
    representation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ¥å—å¾ˆå¤šå‚æ•°æ¥è°ƒæ•´ç¯å¢ƒçš„è¡Œä¸ºå’Œè§‚å¯Ÿè¡¨ç¤ºï¼š
- en: 'prices: Contains one or more stock prices for one or more instruments as a
    dict, where keys are the instrumentâ€™s name and the value is a container object
    data.Prices, which holds price data arrays.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'prices: åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè‚¡ç¥¨ä»·æ ¼çš„æ•°æ®å­—å…¸ï¼Œå…¶ä¸­é”®æ˜¯å·¥å…·çš„åç§°ï¼Œå€¼æ˜¯ä¸€ä¸ªå®¹å™¨å¯¹è±¡ data.Pricesï¼ŒåŒ…å«ä»·æ ¼æ•°æ®æ•°ç»„ã€‚'
- en: 'bars_count: The count of bars that we pass in the observation. By default,
    this is 10 bars.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bars_count: æˆ‘ä»¬åœ¨è§‚å¯Ÿä¸­ä¼ å…¥çš„æ¡å½¢æ•°é‡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ 10 ä¸ªæ¡å½¢ã€‚'
- en: 'commission: The percentage of the stock price that we have to pay to the broker
    on buying and selling the stock. By default, itâ€™s 0.1%.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'commission: æˆ‘ä»¬åœ¨ä¹°å–è‚¡ç¥¨æ—¶éœ€è¦æ”¯ä»˜ç»™ç»çºªäººçš„è‚¡ä»·ç™¾åˆ†æ¯”ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯0.1%ã€‚'
- en: 'reset_on_close: If this parameter is set to True, which it is by default, every
    time the agent asks us to close the existing position (in other words, sell a
    share), we stop the episode. Otherwise, the episode will continue until the end
    of our time series, which is one year of data.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reset_on_close: å¦‚æœæ­¤å‚æ•°è®¾ç½®ä¸ºTrueï¼ˆé»˜è®¤è®¾ç½®ï¼‰ï¼Œåˆ™æ¯å½“ä»£ç†è¯·æ±‚æˆ‘ä»¬å…³é—­ç°æœ‰ä»“ä½ï¼ˆå³å–å‡ºè‚¡ç¥¨ï¼‰æ—¶ï¼Œæˆ‘ä»¬å°†åœæ­¢å½“å‰å›åˆã€‚å¦åˆ™ï¼Œå›åˆå°†ç»§ç»­ï¼Œç›´åˆ°æ—¶é—´åºåˆ—ç»“æŸï¼Œå³ä¸€å¹´æ•°æ®ã€‚'
- en: 'conv_1d: This Boolean argument switches between different representations of
    price data in the observation passed to the agent. If it is set to True, observations
    have a 2D shape, with different price components for subsequent bars organized
    in rows. For example, high prices (max price for the bar) are placed on the first
    row, low prices on the second, and close prices on the third. This representation
    is suitable for doing 1D convolution on time series, where every row in the data
    has the same meaning as different color planes (red, green, or blue) in Atari
    2D images. If we set this option to False, we have one single array of data with
    every barâ€™s components placed together. This organization is convenient for a
    fully connected network architecture. Both representations are illustrated in
    FigureÂ [10.2](#x1-173037r2).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'conv_1d: è¿™ä¸ªå¸ƒå°”å‚æ•°ç”¨äºåœ¨ä¼ é€’ç»™ä»£ç†çš„è§‚å¯Ÿå€¼ä¸­åˆ‡æ¢ä¸åŒçš„ä»·æ ¼æ•°æ®è¡¨ç¤ºæ–¹å¼ã€‚å¦‚æœè®¾ç½®ä¸ºTrueï¼Œè§‚å¯Ÿå€¼å°†å…·æœ‰äºŒç»´å½¢çŠ¶ï¼Œä¸åŒä»·æ ¼æˆåˆ†çš„åç»­æ¡ç›®å°†æŒ‰è¡Œç»„ç»‡ã€‚ä¾‹å¦‚ï¼Œæœ€é«˜ä»·æ ¼ï¼ˆè¯¥æ¡ç›®çš„æœ€å¤§ä»·æ ¼ï¼‰æ”¾åœ¨ç¬¬ä¸€è¡Œï¼Œæœ€ä½ä»·æ ¼æ”¾åœ¨ç¬¬äºŒè¡Œï¼Œæ”¶ç›˜ä»·æ ¼æ”¾åœ¨ç¬¬ä¸‰è¡Œã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼é€‚ç”¨äºå¯¹æ—¶é—´åºåˆ—è¿›è¡Œ1Då·ç§¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®ä¸­çš„æ¯ä¸€è¡Œéƒ½åƒAtari
    2Då›¾åƒä¸­çš„ä¸åŒè‰²å½©å¹³é¢ï¼ˆçº¢è‰²ã€ç»¿è‰²æˆ–è“è‰²ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å°†æ­¤é€‰é¡¹è®¾ç½®ä¸ºFalseï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªåŒ…å«æ¯ä¸ªæ¡ç›®ç»„æˆéƒ¨åˆ†çš„å•ä¸€æ•°æ®æ•°ç»„ã€‚è¿™ç§ç»„ç»‡æ–¹å¼é€‚åˆå…¨è¿æ¥ç½‘ç»œæ¶æ„ã€‚ä¸¤ç§è¡¨ç¤ºæ–¹å¼è§å›¾[10.2](#x1-173037r2)ã€‚'
- en: 'random_ofs_on_reset: If the parameter is True (by default), on every reset
    of the environment, the random offset in the time series will be chosen. Otherwise,
    we will start from the beginning of the data.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'random_ofs_on_reset: å¦‚æœè¯¥å‚æ•°ä¸ºTrueï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œåˆ™åœ¨æ¯æ¬¡é‡ç½®ç¯å¢ƒæ—¶ï¼Œéƒ½ä¼šé€‰æ‹©æ—¶é—´åºåˆ—ä¸­çš„éšæœºåç§»é‡ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†ä»æ•°æ®çš„å¼€å¤´å¼€å§‹ã€‚'
- en: 'reward_on_close: This Boolean parameter switches between the two reward schemes
    discussed previously. If it is set to True, the agent will receive a reward only
    on the â€œcloseâ€ action issue. Otherwise, we will give a small reward every bar,
    corresponding to price movement during that bar.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward_on_close: è¿™ä¸ªå¸ƒå°”å‚æ•°åœ¨å‰é¢è®¨è®ºçš„ä¸¤ç§å¥–åŠ±æ–¹æ¡ˆä¹‹é—´åˆ‡æ¢ã€‚å¦‚æœè®¾ç½®ä¸ºTrueï¼Œä»£ç†ä»…åœ¨â€œæ”¶ç›˜â€åŠ¨ä½œæ—¶è·å¾—å¥–åŠ±ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬ä¼šåœ¨æ¯ä¸ªæ¡ç›®ä¸Šç»™äºˆä¸€ä¸ªå°å¥–åŠ±ï¼Œå¯¹åº”äºè¯¥æ¡ç›®æœŸé—´çš„ä»·æ ¼æ³¢åŠ¨ã€‚'
- en: 'volumes: This argument switches on volumes in observations and is disabled
    by default.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'volumes: è¿™ä¸ªå‚æ•°æ§åˆ¶è§‚å¯Ÿå€¼ä¸­çš„æˆäº¤é‡ï¼Œé»˜è®¤æƒ…å†µä¸‹æ˜¯ç¦ç”¨çš„ã€‚'
- en: '![PIC](img/file96.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file96.png)'
- en: 'FigureÂ 10.2: Different data representations for the NN'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.2ï¼šç¥ç»ç½‘ç»œçš„ä¸åŒæ•°æ®è¡¨ç¤ºæ–¹å¼
- en: 'Now we will continue looking at the environment constructor:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†ç»§ç»­æŸ¥çœ‹ç¯å¢ƒæ„é€ å™¨ï¼š
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Most of the functionality of the StocksEnv class is implemented in two internal
    classes: State and State1D. They are responsible for observation preparation and
    our bought share state and reward. They implement a different representation of
    our data in the observations, and we will take a look at their code later. In
    the constructor, we create the state object, action space, and observation space
    fields that are required by Gym.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: StocksEnvç±»çš„å¤§éƒ¨åˆ†åŠŸèƒ½å®ç°äºä¸¤ä¸ªå†…éƒ¨ç±»ï¼šStateå’ŒState1Dã€‚å®ƒä»¬è´Ÿè´£è§‚å¯Ÿå€¼çš„å‡†å¤‡ã€æˆ‘ä»¬è´­ä¹°çš„è‚¡ç¥¨çŠ¶æ€å’Œå¥–åŠ±ã€‚å®ƒä»¬å®ç°äº†æˆ‘ä»¬æ•°æ®åœ¨è§‚å¯Ÿå€¼ä¸­çš„ä¸åŒè¡¨ç¤ºæ–¹å¼ï¼Œæˆ‘ä»¬ç¨åä¼šæŸ¥çœ‹å®ƒä»¬çš„ä»£ç ã€‚åœ¨æ„é€ å™¨ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Gymæ‰€éœ€çš„çŠ¶æ€å¯¹è±¡ã€åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´å­—æ®µã€‚
- en: 'This method defines the reset() functionality for our environment:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•å®šä¹‰äº†æˆ‘ä»¬ç¯å¢ƒçš„reset()åŠŸèƒ½ï¼š
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: According to the gym.Env semantics, we randomly switch the time series that
    we will work on and select the starting offset in this time series. The selected
    price and offset are passed to our internal state instance, which then asks for
    an initial observation using its encode() function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®gym.Envçš„è¯­ä¹‰ï¼Œæˆ‘ä»¬éšæœºåˆ‡æ¢å°†è¦å¤„ç†çš„æ—¶é—´åºåˆ—ï¼Œå¹¶é€‰æ‹©è¯¥æ—¶é—´åºåˆ—ä¸­çš„èµ·å§‹åç§»é‡ã€‚é€‰å®šçš„ä»·æ ¼å’Œåç§»é‡è¢«ä¼ é€’ç»™æˆ‘ä»¬çš„å†…éƒ¨çŠ¶æ€å®ä¾‹ï¼Œç„¶åä½¿ç”¨å…¶encode()å‡½æ•°è¯·æ±‚åˆå§‹è§‚å¯Ÿå€¼ã€‚
- en: 'This method has to handle the action chosen by the agent and return the next
    observation, reward, and done flag:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•éœ€è¦å¤„ç†ä»£ç†é€‰æ‹©çš„åŠ¨ä½œï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªè§‚å¯Ÿå€¼ã€å¥–åŠ±å’Œå®Œæˆæ ‡å¿—ï¼š
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: All real functionality is implemented in our state classes, so this method is
    a very simple wrapper around the call to state methods.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„å®é™…åŠŸèƒ½éƒ½åœ¨æˆ‘ä»¬çš„çŠ¶æ€ç±»ä¸­å®ç°ï¼Œå› æ­¤è¿™ä¸ªæ–¹æ³•åªæ˜¯å¯¹çŠ¶æ€æ–¹æ³•è°ƒç”¨çš„ä¸€ä¸ªç®€å•åŒ…è£…ã€‚
- en: The API for gym.Env allows you to define the render() method handler, which
    is supposed to render the current state in human or machine-readable format. Generally,
    this method is used to peek inside the environment state and is useful for debugging
    or tracing the agentâ€™s behavior. For example, the market environment could render
    current prices as a chart to visualize what the agent sees at that moment. Our
    environment doesnâ€™t support rendering (as this functionality is optional), so
    we donâ€™t define this function at all.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: gym.Env çš„ API å…è®¸ä½ å®šä¹‰ render() æ–¹æ³•å¤„ç†å™¨ï¼Œå®ƒåº”è¯¥ä»¥äººç±»æˆ–æœºå™¨å¯è¯»çš„æ ¼å¼æ¸²æŸ“å½“å‰çŠ¶æ€ã€‚é€šå¸¸ï¼Œè¿™ä¸ªæ–¹æ³•ç”¨äºæŸ¥çœ‹ç¯å¢ƒçŠ¶æ€çš„å†…éƒ¨å†…å®¹ï¼Œå¯¹è°ƒè¯•æˆ–è¿½è¸ªä»£ç†è¡Œä¸ºéå¸¸æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œå¸‚åœºç¯å¢ƒå¯ä»¥å°†å½“å‰ä»·æ ¼æ¸²æŸ“ä¸ºå›¾è¡¨ï¼Œä»¥å¯è§†åŒ–ä»£ç†åœ¨é‚£ä¸€åˆ»æ‰€çœ‹åˆ°çš„å†…å®¹ã€‚æˆ‘ä»¬çš„ç¯å¢ƒä¸æ”¯æŒæ¸²æŸ“ï¼ˆå› ä¸ºè¿™ä¸ªåŠŸèƒ½æ˜¯å¯é€‰çš„ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬æ ¹æœ¬ä¸å®šä¹‰è¿™ä¸ªå‡½æ•°ã€‚
- en: 'Letâ€™s now look at the internal environ.State class, which implements the core
    of the environmentâ€™s functionality:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å†…éƒ¨çš„ environ.State ç±»ï¼Œå®ƒå®ç°äº†ç¯å¢ƒåŠŸèƒ½çš„æ ¸å¿ƒï¼š
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The constructor does nothing more than just check and remember the arguments
    in the objectâ€™s fields.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ„é€ å‡½æ•°ä»…ä»…æ˜¯æ£€æŸ¥å¹¶å°†å‚æ•°ä¿å­˜åœ¨å¯¹è±¡çš„å­—æ®µä¸­ï¼Œæ²¡æœ‰åšå…¶ä»–äº‹æƒ…ã€‚
- en: 'The reset() method is called every time that the environment is asked to reset
    and has to save the passed prices data and starting offset:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: reset() æ–¹æ³•åœ¨æ¯æ¬¡ç¯å¢ƒè¯·æ±‚é‡ç½®æ—¶è¢«è°ƒç”¨ï¼Œå¿…é¡»ä¿å­˜ä¼ å…¥çš„ä»·æ ¼æ•°æ®å’Œèµ·å§‹åç§»é‡ï¼š
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the beginning, we donâ€™t have any shares bought, so our state has have_position=False
    and open_price=0.0.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬æ²¡æœ‰è´­ä¹°ä»»ä½•è‚¡ç¥¨ï¼Œå› æ­¤æˆ‘ä»¬çš„çŠ¶æ€ä¸­æœ‰ `have_position=False` å’Œ `open_price=0.0`ã€‚
- en: 'The shape property returns the tuple with dimensions of the NumPy array with
    encoded state:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: shape å±æ€§è¿”å›åŒ…å«ç¼–ç çŠ¶æ€çš„ NumPy æ•°ç»„ç»´åº¦çš„å…ƒç»„ï¼š
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The State class is encoded into a single vector (top part in the FigureÂ [10.2](#x1-173037r2)),
    which includes prices with optional volumes and two numbers indicating the presence
    of a bought share and position profit.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: State ç±»è¢«ç¼–ç ä¸ºä¸€ä¸ªå•ä¸€çš„å‘é‡ï¼ˆå›¾[10.2](#x1-173037r2)ä¸­çš„é¡¶éƒ¨éƒ¨åˆ†ï¼‰ï¼Œè¯¥å‘é‡åŒ…æ‹¬ä»·æ ¼ï¼ˆå¯é€‰çš„æˆäº¤é‡ï¼‰å’Œä¸¤ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºæ˜¯å¦æŒæœ‰è‚¡ç¥¨ä»¥åŠä»“ä½åˆ©æ¶¦ã€‚
- en: 'The encode() method packs prices at the current offset into a NumPy array,
    which will be the observation of the agent:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: encode() æ–¹æ³•å°†å½“å‰åç§»é‡çš„ä»·æ ¼æ‰“åŒ…æˆä¸€ä¸ª NumPy æ•°ç»„ï¼Œè¿™å°†ä½œä¸ºä»£ç†çš„è§‚å¯Ÿå€¼ï¼š
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This helper method calculates the current barâ€™s close price:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¾…åŠ©æ–¹æ³•è®¡ç®—å½“å‰ K çº¿çš„æ”¶ç›˜ä»·ï¼š
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Prices passed to the State class have the relative form with respect to the
    open price: the high, low, and close components are relative ratios to the open
    price. This representation was already discussed when we talked about the training
    data, and it will (probably) help our agent to learn price patterns that are independent
    of actual price value.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ é€’ç»™ State ç±»çš„ä»·æ ¼ç›¸å¯¹äºå¼€ç›˜ä»·æ˜¯ç›¸å¯¹å½¢å¼ï¼šé«˜ã€ä½å’Œæ”¶ç›˜ä»·ç»„ä»¶æ˜¯ç›¸å¯¹äºå¼€ç›˜ä»·çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬åœ¨è®¨è®ºè®­ç»ƒæ•°æ®æ—¶å·²ç»è®¨è®ºè¿‡è¿™ç§è¡¨ç¤ºæ³•ï¼Œå®ƒï¼ˆå¯èƒ½ï¼‰æœ‰åŠ©äºæˆ‘ä»¬çš„ä»£ç†å­¦ä¹ ä¸å®é™…ä»·æ ¼å€¼æ— å…³çš„ä»·æ ¼æ¨¡å¼ã€‚
- en: 'The step() method is the most complicated piece of code in the State class:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: step() æ–¹æ³•æ˜¯ State ç±»ä¸­æœ€å¤æ‚çš„ä»£ç éƒ¨åˆ†ï¼š
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It is responsible for performing one step in our environment. On exit, it has
    to return the reward in a percentage and an indication of the episode ending.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè´Ÿè´£åœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥æ“ä½œã€‚é€€å‡ºæ—¶ï¼Œå®ƒå¿…é¡»è¿”å›ä¸€ä¸ªç™¾åˆ†æ¯”å½¢å¼çš„å¥–åŠ±ï¼Œå¹¶æŒ‡ç¤ºå‰§é›†æ˜¯å¦ç»“æŸã€‚
- en: 'If the agent has decided to buy a share, we change our state and pay the commission:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä»£ç†å†³å®šè´­ä¹°ä¸€åªè‚¡ç¥¨ï¼Œæˆ‘ä»¬ä¼šæ”¹å˜çŠ¶æ€å¹¶æ”¯ä»˜ä½£é‡‘ï¼š
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In our state, we assume the instant order execution at the current barâ€™s close
    price, which is a simplification on our side; normally, an order can be executed
    on a different price, which is called price slippage.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„çŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬å‡è®¾åœ¨å½“å‰ K çº¿çš„æ”¶ç›˜ä»·è¿›è¡Œå³æ—¶è®¢å•æ‰§è¡Œï¼Œè¿™å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ä¸€ä¸ªç®€åŒ–ï¼›é€šå¸¸ï¼Œè®¢å•å¯èƒ½åœ¨ä¸åŒçš„ä»·æ ¼ä¸Šæ‰§è¡Œï¼Œè¿™è¢«ç§°ä¸ºä»·æ ¼æ»‘ç‚¹ã€‚
- en: 'If we have a position and the agent asks us to close it, we pay commission
    again, change the done flag if weâ€™re in reset_on_close mode, give a final reward
    for the whole position, and change our state:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æœ‰æŒä»“ï¼Œä¸”ä»£ç†è¦æ±‚æˆ‘ä»¬å¹³ä»“ï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡æ”¯ä»˜ä½£é‡‘ï¼Œåœ¨é‡ç½®æ¨¡å¼ä¸‹æ”¹å˜å·²å®Œæˆæ ‡å¿—ï¼Œç»™æ•´ä¸ªä»“ä½ä¸€ä¸ªæœ€ç»ˆçš„å¥–åŠ±ï¼Œå¹¶æ”¹å˜æˆ‘ä»¬çš„çŠ¶æ€ï¼š
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the rest of the function, we modify the current offset and give the reward
    for the last bar movement:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¿®æ”¹å½“å‰åç§»é‡å¹¶ç»™äºˆæœ€åä¸€æ ¹ K çº¿è¿åŠ¨çš„å¥–åŠ±ï¼š
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Thatâ€™s it for the State class, so letâ€™s look at State1D, which has the same
    behavior and just overrides the representation of the state passed to the agent:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ State ç±»çš„å…¨éƒ¨å†…å®¹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ State1Dï¼Œå®ƒå…·æœ‰ç›¸åŒçš„è¡Œä¸ºï¼Œä»…ä»…æ˜¯é‡å†™äº†ä¼ é€’ç»™ä»£ç†çš„çŠ¶æ€è¡¨ç¤ºï¼š
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The shape of this representation is different, as our prices are encoded as
    a 2D matrix suitable for a 1D convolution operator.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¡¨ç¤ºçš„å½¢çŠ¶æœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬çš„ä»·æ ¼è¢«ç¼–ç ä¸ºé€‚ç”¨äº 1D å·ç§¯æ“ä½œç¬¦çš„ 2D çŸ©é˜µã€‚
- en: 'This method encodes the prices in our matrix, depending on the current offset,
    whether we need volumes, and whether we have stock:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•æ ¹æ®å½“å‰åç§»é‡ã€æ˜¯å¦éœ€è¦æˆäº¤é‡ä»¥åŠæ˜¯å¦æ‹¥æœ‰è‚¡ç¥¨ï¼Œå°†ä»·æ ¼ç¼–ç åˆ°æˆ‘ä»¬çš„çŸ©é˜µä¸­ï¼š
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Thatâ€™s it for our trading environment. Compatibility with the Gym API allows
    us to plug it into the familiar classes that we used to handle the Atari games.
    Letâ€™s do that now.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬çš„äº¤æ˜“ç¯å¢ƒã€‚ä¸ Gym API çš„å…¼å®¹æ€§ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå°†å…¶æ’å…¥åˆ°æˆ‘ä»¬ç”¨æ¥å¤„ç† Atari æ¸¸æˆçš„ç†Ÿæ‚‰ç±»ä¸­ã€‚ç°åœ¨æˆ‘ä»¬æ¥åšè¿™ä¸ªã€‚
- en: Models
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: 'In this example, two architectures of DQN are used: a simple feed-forward network
    with three layers and a network with 1D convolution as a feature extractor, followed
    by two fully connected layers to output Q-values. Both of them use the dueling
    architecture described in ChapterÂ [8](ch012.xhtml#x1-1240008). Double DQN and
    two-step Bellman unrolling have also been used. The rest of the process is the
    same as in a classical DQN (from ChapterÂ [6](#)). Both models are in Chapter10/lib/models.py
    and are very simple. Letâ€™s start with the feed-forward model:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨äº†ä¸¤ç§ DQN æ¶æ„ï¼šä¸€ä¸ªæ˜¯ç®€å•çš„ä¸‰å±‚å‰é¦ˆç½‘ç»œï¼Œå¦ä¸€ä¸ªæ˜¯ä½¿ç”¨ 1D å·ç§¯ä½œä¸ºç‰¹å¾æå–å™¨çš„ç½‘ç»œï¼Œåæ¥ä¸¤å±‚å…¨è¿æ¥å±‚è¾“å‡º Q å€¼ã€‚å®ƒä»¬éƒ½ä½¿ç”¨äº†ç¬¬
    [8](ch012.xhtml#x1-1240008) ç« ä¸­æè¿°çš„å¯¹æˆ˜æ¶æ„ã€‚åŒæ—¶ï¼Œä¹Ÿä½¿ç”¨äº†åŒé‡ DQN å’Œä¸¤æ­¥è´å°”æ›¼å±•å¼€ã€‚å…¶ä½™è¿‡ç¨‹ä¸ç»å…¸ DQN ç›¸åŒï¼ˆè§ç¬¬
    [6](#) ç« ï¼‰ã€‚è¿™ä¸¤ç§æ¨¡å‹ä½äº Chapter10/lib/models.py ä¸­ï¼Œå¹¶ä¸”éå¸¸ç®€å•ã€‚æˆ‘ä»¬å…ˆä»å‰é¦ˆæ¨¡å‹å¼€å§‹ï¼š
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The feed forward model uses independent networks for Q-value and advantage prediction.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¦ˆæ¨¡å‹ä½¿ç”¨ç‹¬ç«‹çš„ç½‘ç»œè¿›è¡Œ Q å€¼å’Œä¼˜åŠ¿é¢„æµ‹ã€‚
- en: 'The convolutional model has a common feature extraction layer with the 1D convolution
    operations and two fully connected heads to output the value of the state and
    advantages for actions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯æ¨¡å‹å…·æœ‰ä¸€ä¸ªå¸¸è§çš„ç‰¹å¾æå–å±‚ï¼Œä½¿ç”¨ 1D å·ç§¯æ“ä½œï¼Œå¹¶ä¸”æœ‰ä¸¤ä¸ªå…¨è¿æ¥å¤´ç”¨äºè¾“å‡ºçŠ¶æ€å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿ï¼š
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, the model is very similar to the DQN Dueling architecture we
    used in Atari examples.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œè¯¥æ¨¡å‹ä¸æˆ‘ä»¬åœ¨ Atari ç¤ºä¾‹ä¸­ä½¿ç”¨çš„ DQN Dueling æ¶æ„éå¸¸ç›¸ä¼¼ã€‚
- en: Training code
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒä»£ç 
- en: 'We have two very similar training modules in this example: one for the feed-forward
    model and one for 1D convolutions. For both of them, there is nothing new added
    to our examples from ChapterÂ [8](ch012.xhtml#x1-1240008):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªéå¸¸ç›¸ä¼¼çš„è®­ç»ƒæ¨¡å—ï¼šä¸€ä¸ªç”¨äºå‰é¦ˆæ¨¡å‹ï¼Œå¦ä¸€ä¸ªç”¨äº 1D å·ç§¯ã€‚å¯¹äºè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œé™¤äº†ç¬¬ [8](ch012.xhtml#x1-1240008)
    ç« ä¸­æä¾›çš„å†…å®¹å¤–ï¼Œæ²¡æœ‰ä»»ä½•æ–°çš„å†…å®¹ï¼š
- en: Theyâ€™re using epsilon-greedy action selection to perform exploration. The epsilon
    linearly decays over the first 1M steps from 1.0 to 0.1.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä½¿ç”¨ epsilon-greedy åŠ¨ä½œé€‰æ‹©æ¥è¿›è¡Œæ¢ç´¢ã€‚epsilon åœ¨å‰ 100 ä¸‡æ­¥å†…ä» 1.0 çº¿æ€§è¡°å‡åˆ° 0.1ã€‚
- en: A simple experience replay buffer of size 100k is being used, which is initially
    populated with 10k transitions.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£åœ¨ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¤§å°ä¸º 100kï¼Œæœ€åˆå¡«å……æœ‰ 10k ä¸ªè¿‡æ¸¡ã€‚
- en: For every 1,000 steps, we calculate the mean value for the fixed set of states
    to check the dynamics of the Q-values during the training.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ 1,000 æ­¥ï¼Œæˆ‘ä»¬ä¼šè®¡ç®—å›ºå®šçŠ¶æ€é›†åˆçš„å‡å€¼ï¼Œä»¥æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹ä¸­ Q å€¼çš„åŠ¨æ€å˜åŒ–ã€‚
- en: 'For every 100k steps, we perform validation: 100 episodes are played on the
    training data and on previously unseen quotes. Validation results are recorded
    in TensorBoard, such as the mean profit, the mean count of bars, and the share
    held. This step allows us to check for overfitting conditions.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ 100k æ­¥ï¼Œæˆ‘ä»¬è¿›è¡ŒéªŒè¯ï¼šåœ¨è®­ç»ƒæ•°æ®å’Œä¹‹å‰æœªè§è¿‡çš„æŠ¥ä»·ä¸Šå„è¿›è¡Œ 100 è½®æµ‹è¯•ã€‚éªŒè¯ç»“æœä¼šè®°å½•åœ¨ TensorBoard ä¸­ï¼ŒåŒ…æ‹¬å¹³å‡åˆ©æ¶¦ã€å¹³å‡æ¡æ•°ä»¥åŠæŒè‚¡æ¯”ä¾‹ã€‚æ­¤æ­¥éª¤å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆæƒ…å†µã€‚
- en: The training modules are in Chapter10/train_model.py (feed-forward model) and
    Chapter10/train_model_conv.py (with a 1D convolutional layer). Both versions accept
    the same command-line options.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å—ä½äº Chapter10/train_model.pyï¼ˆå‰é¦ˆæ¨¡å‹ï¼‰å’Œ Chapter10/train_model_conv.pyï¼ˆå« 1D å·ç§¯å±‚ï¼‰ã€‚ä¸¤ä¸ªç‰ˆæœ¬éƒ½æ¥å—ç›¸åŒçš„å‘½ä»¤è¡Œé€‰é¡¹ã€‚
- en: To start the training, you need to pass training data with the --data option,
    which could be an individual CSV file or the whole directory with files. By default,
    the training module uses Yandex quotes for 2016 (file data/YNDX_160101_161231.csv).
    For the validation data, there is an option, --val, that takes Yandex 2015 quotes
    by default. Another required option will be -r, which is used to pass the name
    of the run. This name will be used in the TensorBoard run name and to create directories
    with saved models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹è®­ç»ƒï¼Œæ‚¨éœ€è¦ä½¿ç”¨ `--data` é€‰é¡¹ä¼ é€’è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ˜¯å•ä¸ª CSV æ–‡ä»¶æˆ–åŒ…å«æ–‡ä»¶çš„æ•´ä¸ªç›®å½•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®­ç»ƒæ¨¡å—ä½¿ç”¨ 2016 å¹´çš„ Yandex
    è¡Œæƒ…ï¼ˆæ–‡ä»¶ `data/YNDX_160101_161231.csv`ï¼‰ã€‚å¯¹äºéªŒè¯æ•°æ®ï¼Œæœ‰ä¸€ä¸ª `--val` é€‰é¡¹ï¼Œé»˜è®¤ä½¿ç”¨ Yandex 2015 å¹´çš„è¡Œæƒ…ã€‚å¦ä¸€ä¸ªå¿…éœ€é€‰é¡¹æ˜¯
    `-r`ï¼Œç”¨äºä¼ é€’è¿è¡Œåç§°ã€‚æ­¤åç§°å°†ç”¨ä½œ TensorBoard è¿è¡Œåç§°å’Œç”¨äºåˆ›å»ºä¿å­˜æ¨¡å‹çš„ç›®å½•ã€‚
- en: Results
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: Now that weâ€™ve implemented them, letâ€™s compare the performance of our two models,
    starting with feed-forward variant.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®æ–½äº†å®ƒä»¬ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹æˆ‘ä»¬ä¸¤ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼Œé¦–å…ˆä»å‰é¦ˆå˜ä½“å¼€å§‹ã€‚
- en: The feed-forward model
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰é¦ˆæ¨¡å‹
- en: 'During the training, the average reward obtained by the agent was slowly but
    consistently growing. After 300k episodes, the growth slowed down. The following
    are charts (FigureÂ [10.3](#x1-177003r3)) showing the raw reward during the training
    and the same data smoothed with the simple moving average of the last 15 values:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»£ç†æ¯æ¬¡è·å¾—çš„å¹³å‡å¥–åŠ±éƒ½åœ¨ç¼“æ…¢ä½†ç¨³æ­¥å¢é•¿ã€‚åœ¨ 300k ä¸ª episode åï¼Œå¢é•¿æ”¾ç¼“ã€‚ä»¥ä¸‹æ˜¯æ˜¾ç¤ºè®­ç»ƒæœŸé—´åŸå§‹å¥–åŠ±å’Œç®€å•ç§»åŠ¨å¹³å‡å€¼çš„å›¾è¡¨ï¼ˆå›¾Â [10.3](#x1-177003r3)ï¼‰ï¼š
- en: '![PIC](img/B21150_10_03.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_03.png)'
- en: 'FigureÂ 10.3: Reward during the training. Raw values (left) and smoothed (right)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾Â 10.3: è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ã€‚åŸå§‹å€¼ï¼ˆå·¦ï¼‰å’Œå¹³æ»‘åï¼ˆå³ï¼‰'
- en: 'Another pair of charts (FigureÂ [10.4](#x1-177004r4)) shows the reward obtained
    from testing performed on the same training data but without random actions (ğœ–
    = 0):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€å¯¹å›¾è¡¨ï¼ˆå›¾Â [10.4](#x1-177004r4)ï¼‰æ˜¾ç¤ºäº†åœ¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œä¸æ‰§è¡ŒéšæœºåŠ¨ä½œï¼ˆğœ– = 0ï¼‰æ‰€è·å¾—çš„å¥–åŠ±ï¼š
- en: '![PIC](img/B21150_10_04.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_04.png)'
- en: 'FigureÂ 10.4: Reward from the tests. Raw values (left) and smoothed (right)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾Â 10.4: æµ‹è¯•çš„å¥–åŠ±ã€‚åŸå§‹å€¼ï¼ˆå·¦ï¼‰å’Œå¹³æ»‘åï¼ˆå³ï¼‰'
- en: Both the training and testing reward charts show that the agent is learning
    how to increase the profit over time.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•å¥–åŠ±å›¾è¡¨æ˜¾ç¤ºï¼Œä»£ç†æ­£åœ¨å­¦ä¹ å¦‚ä½•éšæ—¶é—´å¢åŠ åˆ©æ¶¦ã€‚
- en: '![PIC](img/B21150_10_05.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_05.png)'
- en: 'FigureÂ 10.5: Length of the episodes. Raw values (left) and smoothed (right)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾Â 10.5: episode çš„é•¿åº¦ã€‚åŸå§‹å€¼ï¼ˆå·¦ï¼‰å’Œå¹³æ»‘åï¼ˆå³ï¼‰'
- en: The length of each episode also increased after 100k episodes, as the agent
    learned that holding the share might be profitable.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª episode çš„é•¿åº¦åœ¨ 100k ä¸ª episode åä¹Ÿæœ‰æ‰€å¢åŠ ï¼Œå› ä¸ºä»£ç†å­¦åˆ°äº†æŒæœ‰è‚¡ä»½å¯èƒ½æ˜¯æœ‰åˆ©çš„ã€‚
- en: 'In addition, we monitor the predicted value of the random set of states. The
    following chart shows that the network becomes more and more optimistic about
    those states during the training:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬ç›‘æ§éšæœºçŠ¶æ€é›†çš„é¢„æµ‹å€¼ã€‚ä»¥ä¸‹å›¾è¡¨æ˜¾ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç½‘ç»œå¯¹è¿™äº›çŠ¶æ€å˜å¾—è¶Šæ¥è¶Šä¹è§‚ï¼š
- en: '![PIC](img/B21150_10_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_06.png)'
- en: 'FigureÂ 10.6: Value predicted for a random set of states'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾Â 10.6: éšæœºçŠ¶æ€é›†çš„é¢„æµ‹å€¼'
- en: 'All charts look good so far, but all of them were obtained using the training
    data. It is great that our agent is learning how to get profits on the historical
    data. But will it work on data never seen before? To check that, we perform validation
    on 2,015 quotes, and the reward is shown in the FigureÂ [10.7](#x1-177009r7):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰å›¾è¡¨çœ‹èµ·æ¥éƒ½å¾ˆå¥½ï¼Œä½†æ‰€æœ‰è¿™äº›å›¾è¡¨éƒ½æ˜¯ä½¿ç”¨è®­ç»ƒæ•°æ®è·å–çš„ã€‚å¾ˆæ£’ï¼Œæˆ‘ä»¬çš„ä»£ç†æ­£åœ¨å­¦ä¹ å¦‚ä½•åœ¨å†å²æ•°æ®ä¸Šè·åˆ©ã€‚ä½†å®ƒä¼šåœ¨ä»¥å‰ä»æœªè§è¿‡çš„æ•°æ®ä¸Šå·¥ä½œå—ï¼Ÿä¸ºäº†æ£€æŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨
    2,015 å¹´çš„æŠ¥ä»·ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¥–åŠ±æ˜¾ç¤ºåœ¨å›¾Â [10.7](#x1-177009r7) ä¸­ï¼š
- en: '![PIC](img/B21150_10_07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_07.png)'
- en: 'FigureÂ 10.7: Reward on validation dataset. Raw values (left) and smoothed (right)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾Â 10.7: éªŒè¯æ•°æ®é›†ä¸Šçš„å¥–åŠ±ã€‚åŸå§‹å€¼ï¼ˆå·¦ï¼‰å’Œå¹³æ»‘åï¼ˆå³ï¼‰'
- en: 'This chart is a bit disappointing: the reward doesnâ€™t have an uptrend. In the
    smoothed version of the chart, we might even see the opposite â€” the reward is
    slowly decreasing after the first hour of training (at that point, we had a significant
    increase in training episode length on FigureÂ [10.5](#x1-177005r5)). This might
    be an indication of overfitting of the agent, which starts after 1M training iterations.
    But still, for the first 4 hours of training, the reward is above -0.2% (which
    is a broker commission in our environment â€” 0.1% when we buy stock and 0.1% for
    selling it) and means that our agent is better than a random â€œbuying-and-selling
    monkey.â€'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ å›¾æœ‰ç‚¹ä»¤äººå¤±æœ›ï¼šå¥–åŠ±æ²¡æœ‰ä¸Šå‡è¶‹åŠ¿ã€‚åœ¨å›¾è¡¨çš„å¹³æ»‘ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬ç”šè‡³å¯èƒ½çœ‹åˆ°ç›¸åçš„æƒ…å†µâ€”â€”å¥–åŠ±åœ¨è®­ç»ƒçš„ç¬¬ä¸€å°æ—¶åç¼“æ…¢ä¸‹é™ï¼ˆåœ¨é‚£ä¸ªæ—¶åˆ»ï¼Œæˆ‘ä»¬åœ¨å›¾[10.5](#x1-177005r5)ä¸Šæœ‰äº†æ˜¾è‘—çš„è®­ç»ƒå‘¨æœŸå¢é•¿ï¼‰ã€‚è¿™å¯èƒ½æ˜¯ä»£ç†è¿‡æ‹Ÿåˆçš„è¡¨ç°ï¼Œè¿‡æ‹Ÿåˆå§‹äº100ä¸‡æ¬¡è®­ç»ƒè¿­ä»£ã€‚ç„¶è€Œï¼Œåœ¨è®­ç»ƒçš„å‰4å°æ—¶ï¼Œå¥–åŠ±ä»ç„¶é«˜äº-0.2%ï¼ˆè¿™æ˜¯æˆ‘ä»¬ç¯å¢ƒä¸­çš„ç»çºªå•†ä½£é‡‘â€”â€”ä¹°å…¥è‚¡ç¥¨æ—¶æ˜¯0.1%ï¼Œå–å‡ºæ—¶ä¹Ÿæ˜¯0.1%ï¼‰ï¼Œæ„å‘³ç€æˆ‘ä»¬çš„ä»£ç†æ¯”éšæœºçš„â€œä¹°å…¥å–å‡ºçŒ´å­â€è¡¨ç°å¾—æ›´å¥½ã€‚
- en: 'During the training, our code saves models for later experiments. It does this
    every time the mean Q-values on our held-out-states set update the maximum or
    when the reward on the validation sets beats the previous record. There is a tool
    that loads the model, trades on prices youâ€™ve provided to it with the command-line
    option, and draws the plots with the profit change over time. The tool is called
    Chapter10/run_model.py and it can be used like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ä»£ç ä¼šä¿å­˜æ¨¡å‹ä»¥ä¾›ä»¥åå®éªŒä½¿ç”¨ã€‚æ¯å½“æˆ‘ä»¬æŒæœ‰çŠ¶æ€é›†ä¸Šçš„å¹³å‡Qå€¼æ›´æ–°æœ€å¤§å€¼ï¼Œæˆ–å½“éªŒè¯é›†ä¸Šçš„å¥–åŠ±çªç ´ä»¥å‰çš„è®°å½•æ—¶ï¼Œå®ƒéƒ½ä¼šè¿™æ ·åšã€‚è¿˜æœ‰ä¸€ä¸ªå·¥å…·å¯ä»¥åŠ è½½æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å‘½ä»¤è¡Œé€‰é¡¹åœ¨ä½ æä¾›çš„ä»·æ ¼ä¸Šè¿›è¡Œäº¤æ˜“ï¼Œå¹¶ç»˜åˆ¶åˆ©æ¶¦éšæ—¶é—´å˜åŒ–çš„å›¾è¡¨ã€‚è¯¥å·¥å…·åä¸º`Chapter10/run_model.py`ï¼Œä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The options that the tool accepts are as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å·¥å…·æ¥å—çš„é€‰é¡¹å¦‚ä¸‹ï¼š
- en: '-d: This is the path to the quotes to use. In the shown command, we apply the
    model to the data that it was trained on.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -dï¼šè¿™æ˜¯ç”¨äºçš„æŠ¥ä»·è·¯å¾„ã€‚åœ¨æ‰€ç¤ºå‘½ä»¤ä¸­ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åº”ç”¨äºå®ƒè®­ç»ƒæ—¶çš„æ•°æ®ã€‚
- en: '-m: This is the path to the model file. By default, the training code saves
    it in the saves directory.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -mï¼šè¿™æ˜¯æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®­ç»ƒä»£ç ä¼šå°†å…¶ä¿å­˜åœ¨`saves`ç›®å½•ä¸­ã€‚
- en: '-b: This shows how many bars to pass to the model in the context. It has to
    match the count of bars used on training, which is 10 by default and can be changed
    in the training code.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -bï¼šæ­¤é€‰é¡¹æ˜¾ç¤ºåœ¨ä¸Šä¸‹æ–‡ä¸­ä¼ é€’ç»™æ¨¡å‹çš„æ¡å½¢å›¾æ•°ã€‚å®ƒå¿…é¡»ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¡å½¢å›¾æ•°é‡åŒ¹é…ï¼Œé»˜è®¤å€¼ä¸º10ï¼Œå¯ä»¥åœ¨è®­ç»ƒä»£ç ä¸­æ›´æ”¹ã€‚
- en: '-n: This is the suffix to be appended to the images produced.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -nï¼šè¿™æ˜¯é™„åŠ åˆ°ç”Ÿæˆçš„å›¾åƒä¸Šçš„åç¼€ã€‚
- en: '--commission: This allows you to redefine the brokerâ€™s commission, which has
    a default of 0.1%.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --commissionï¼šæ­¤é€‰é¡¹å…è®¸ä½ é‡æ–°å®šä¹‰ç»çºªå•†çš„ä½£é‡‘ï¼Œé»˜è®¤å€¼ä¸º0.1%ã€‚
- en: 'At the end, the tool creates a chart of the total profit dynamics (in percentages).
    The following is the reward chart on Yandex 2016 quotes (used for training):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå·¥å…·ä¼šåˆ›å»ºä¸€ä¸ªæ€»åˆ©æ¶¦åŠ¨æ€å›¾ï¼ˆä»¥ç™¾åˆ†æ¯”è¡¨ç¤ºï¼‰ã€‚ä»¥ä¸‹æ˜¯Yandex 2016å¹´æŠ¥ä»·ï¼ˆç”¨äºè®­ç»ƒï¼‰çš„å¥–åŠ±å›¾ï¼š
- en: '![PIC](img/B21150_10_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_10_08.png)'
- en: 'FigureÂ 10.8: Trading profit on the training data (left) and validation (right)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.8ï¼šè®­ç»ƒæ•°æ®ä¸Šçš„äº¤æ˜“åˆ©æ¶¦ï¼ˆå·¦ï¼‰ä¸éªŒè¯æ•°æ®ä¸Šçš„äº¤æ˜“åˆ©æ¶¦ï¼ˆå³ï¼‰
- en: 'The result on the training data looks amazing: 150% profit in just a year.
    However, the result on the validation dataset is much worse, as weâ€™ve seen from
    the validation plots in TensorBoard.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„ç»“æœçœ‹èµ·æ¥éå¸¸æƒŠäººï¼šä»…ä»…ä¸€å¹´å°±è·å¾—äº†150%çš„åˆ©æ¶¦ã€‚ç„¶è€Œï¼Œåœ¨éªŒè¯æ•°æ®é›†ä¸Šçš„ç»“æœè¦å·®å¾—å¤šï¼Œæ­£å¦‚æˆ‘ä»¬ä»TensorBoardä¸­çš„éªŒè¯å›¾è¡¨ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: 'To check that our system is profitable with zero commission, we can rerun on
    the same data with the --commission 0.0 option:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ£€æŸ¥æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨é›¶ä½£é‡‘ä¸‹æ˜¯å¦æœ‰ç›ˆåˆ©ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`--commission 0.0`é€‰é¡¹é‡æ–°è¿è¡Œç›¸åŒçš„æ•°æ®ï¼š
- en: '![PIC](img/file108.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file108.png)'
- en: 'FigureÂ 10.9: Trading profit on validation data without brokerâ€™s commission'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.9ï¼šæ²¡æœ‰ç»çºªå•†ä½£é‡‘çš„éªŒè¯æ•°æ®äº¤æ˜“åˆ©æ¶¦
- en: 'We have some bad days with drawdown, but the overall results are good: without
    commission, our agent can be profitable. Of course, the commission is not the
    only issue. Our order simulation is very primitive and doesnâ€™t take into account
    real-life situations, such as price spread and a slip in order execution.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€äº›å›æ’¤è¾ƒå¤§çš„ç³Ÿç³•æ—¥å­ï¼Œä½†æ•´ä½“ç»“æœè¿˜æ˜¯ä¸é”™çš„ï¼šæ²¡æœ‰ä½£é‡‘æ—¶ï¼Œæˆ‘ä»¬çš„ä»£ç†æ˜¯å¯ä»¥ç›ˆåˆ©çš„ã€‚å½“ç„¶ï¼Œä½£é‡‘å¹¶ä¸æ˜¯å”¯ä¸€çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„è®¢å•æ¨¡æ‹Ÿéå¸¸åŸå§‹ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘åˆ°ç°å®ä¸­çš„æƒ…å†µï¼Œä¾‹å¦‚ä»·æ ¼å·®è·å’Œè®¢å•æ‰§è¡Œçš„æ»‘ç‚¹ã€‚
- en: 'If we take the model with the best reward on the validation set, the reward
    dynamics are a bit better. Profitability is lower, but the drawdown on unseen
    quotes is much lower (and commission was enabled for the following charts):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬é€‰æ‹©åœ¨éªŒè¯é›†ä¸Šè·å¾—æœ€ä½³å¥–åŠ±çš„æ¨¡å‹ï¼Œå¥–åŠ±åŠ¨æ€ä¼šç¨å¥½ä¸€äº›ã€‚ç›ˆåˆ©èƒ½åŠ›è¾ƒä½ï¼Œä½†åœ¨æœªè§è¿‡çš„æŠ¥ä»·ä¸Šçš„å›æ’¤è¦ä½å¾—å¤šï¼ˆå¹¶ä¸”åœ¨ä»¥ä¸‹å›¾è¡¨ä¸­å¯ç”¨äº†ä½£é‡‘ï¼‰ï¼š
- en: '![PIC](img/B21150_10_10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B21150_10_10.png)'
- en: 'FigureÂ 10.10: The reward from the model with the best validation reward. Training
    data (left) and validation (right)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.10ï¼šæœ€ä½³éªŒè¯å¥–åŠ±æ¨¡å‹çš„å¥–åŠ±ã€‚è®­ç»ƒæ•°æ®ï¼ˆå·¦ï¼‰å’ŒéªŒè¯æ•°æ®ï¼ˆå³ï¼‰
- en: But, of course, taking the best model based on validation data is cheating â€”
    by using validation results for modelâ€™s selection, we are ruining the idea of
    validation. So, the charts above are just to illustrate that there are some models
    that might work alright even on unseen data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå½“ç„¶ï¼ŒåŸºäºéªŒè¯æ•°æ®é€‰æ‹©æœ€ä½³æ¨¡å‹æ˜¯ä½œå¼Šâ€”â€”é€šè¿‡ä½¿ç”¨éªŒè¯ç»“æœæ¥é€‰æ‹©æ¨¡å‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šç ´åäº†éªŒè¯çš„æ„ä¹‰ã€‚å› æ­¤ï¼Œä¸Šè¿°å›¾è¡¨ä»…ç”¨äºè¯´æ˜æœ‰äº›æ¨¡å‹å³ä½¿åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šä¹Ÿèƒ½è¡¨ç°å¾—è¿˜ä¸é”™ã€‚
- en: The convolution model
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·ç§¯æ¨¡å‹
- en: The second model implemented in this example uses 1D convolution filters to
    extract features from the price data. This allows us to increase the number of
    bars in the context window that our agent sees on every step without a significant
    increase in the network size. By default, the convolution model example uses 50
    bars of context. The training code is in Chapter10/train_model_conv.py, and it
    accepts the same set of command-line parameters as the feed-forward version.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç¤ºä¾‹ä¸­å®ç°çš„ç¬¬äºŒä¸ªæ¨¡å‹ä½¿ç”¨ä¸€ç»´å·ç§¯æ»¤æ³¢å™¨ä»ä»·æ ¼æ•°æ®ä¸­æå–ç‰¹å¾ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—å¢åŠ ç½‘ç»œè§„æ¨¡çš„æƒ…å†µä¸‹ï¼Œå¢åŠ æ¯æ­¥æ“ä½œä¸­ä»£ç†æ‰€çœ‹åˆ°çš„ä¸Šä¸‹æ–‡çª—å£ä¸­çš„æ¡å½¢æ•°é‡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå·ç§¯æ¨¡å‹ç¤ºä¾‹ä½¿ç”¨50ä¸ªæ¡å½¢æ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡ã€‚è®­ç»ƒä»£ç ä½äºChapter10/train_model_conv.pyï¼Œå¹¶ä¸”æ¥å—ä¸å‰é¦ˆç‰ˆæœ¬ç›¸åŒçš„ä¸€ç»„å‘½ä»¤è¡Œå‚æ•°ã€‚
- en: 'Training dynamics are almost identical, but the reward obtained on the validation
    set is slightly higher and starts to overfit later:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒåŠ¨æ€å‡ ä¹ç›¸åŒï¼Œä½†åœ¨éªŒè¯é›†ä¸Šçš„å¥–åŠ±ç¨å¾®é«˜ä¸€äº›ï¼Œå¹¶ä¸”å¼€å§‹è¿‡æ‹Ÿåˆå¾—æ›´æ™šï¼š
- en: '![PIC](img/B21150_10_11.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_11.png)'
- en: 'FigureÂ 10.11: Reward during the training. Raw values (left) and smoothed (right)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.11ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ã€‚åŸå§‹å€¼ï¼ˆå·¦ï¼‰å’Œå¹³æ»‘å€¼ï¼ˆå³ï¼‰
- en: '![PIC](img/B21150_10_12.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B21150_10_12.png)'
- en: 'FigureÂ 10.12: Reward on validation dataset. Raw values (left) and smoothed
    (right)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10.12ï¼šéªŒè¯æ•°æ®é›†ä¸Šçš„å¥–åŠ±ã€‚åŸå§‹å€¼ï¼ˆå·¦ï¼‰å’Œå¹³æ»‘å€¼ï¼ˆå³ï¼‰
- en: Things to try
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾…å°è¯•çš„äº‹é¡¹
- en: 'As already mentioned, financial markets are large and complicated. The methods
    that weâ€™ve tried are just the very beginning. Using RL to create a complete and
    profitable trading strategy is a large project, which can take several months
    of dedicated labor. However, there are things that we can try to get a better
    understanding of the topic:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œé‡‘èå¸‚åœºåºå¤§ä¸”å¤æ‚ã€‚æˆ‘ä»¬å°è¯•çš„æ–¹æ³•åªæ˜¯ä¸€ä¸ªå¼€å§‹ã€‚ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥åˆ›å»ºä¸€ä¸ªå®Œæ•´ä¸”æœ‰åˆ©å¯å›¾çš„äº¤æ˜“ç­–ç•¥æ˜¯ä¸€ä¸ªåºå¤§çš„é¡¹ç›®ï¼Œå¯èƒ½éœ€è¦æ•°æœˆçš„ä¸“æ³¨å·¥ä½œã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸€äº›æ–¹æ³•æ¥æ›´å¥½åœ°ç†è§£è¿™ä¸ªä¸»é¢˜ï¼š
- en: Our data representation is definitely not perfect. We donâ€™t take into account
    significant price levels (support and resistance), round price values, and other
    financial markets information. Incorporating them into the observation could be
    a challenging problem, which you could try exploring.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ•°æ®è¡¨ç¤ºæ–¹å¼æ˜¾ç„¶å¹¶ä¸å®Œç¾ã€‚æˆ‘ä»¬æ²¡æœ‰è€ƒè™‘é‡è¦çš„ä»·æ ¼æ°´å¹³ï¼ˆæ”¯æ’‘ä½å’Œé˜»åŠ›ä½ï¼‰ã€æ•´æ•°ä»·æ ¼å€¼å’Œå…¶ä»–é‡‘èå¸‚åœºä¿¡æ¯ã€‚å°†è¿™äº›ä¿¡æ¯çº³å…¥è§‚å¯ŸèŒƒå›´å¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæ‚¨å¯ä»¥å°è¯•è¿›ä¸€æ­¥æ¢ç´¢ã€‚
- en: Analyze market prices at different timeframes. Low-level data like one-minute
    bars are noisy (as they include lots of small price movements caused by individual
    trades), and it is like looking at the market using a microscope. At larger scales,
    such as one-hour or one-day bars, you can see large, long trends in data movement,
    which could be extremely important for price prediction.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ä¸åŒæ—¶é—´æ¡†æ¶ä¸‹åˆ†æå¸‚åœºä»·æ ¼ã€‚åƒä¸€åˆ†é’Ÿæ¡å½¢æ•°æ®è¿™æ ·çš„ä½çº§æ•°æ®éå¸¸å˜ˆæ‚ï¼ˆå› ä¸ºå®ƒä»¬åŒ…å«äº†ç”±å•ä¸ªäº¤æ˜“å¯¼è‡´çš„å¤§é‡å°å¹…ä»·æ ¼æ³¢åŠ¨ï¼‰ï¼Œå°±åƒç”¨æ˜¾å¾®é•œè§‚å¯Ÿå¸‚åœºä¸€æ ·ã€‚åœ¨æ›´å¤§çš„å°ºåº¦ä¸‹ï¼Œå¦‚ä¸€å°æ—¶æˆ–ä¸€å¤©çš„æ¡å½¢æ•°æ®ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ•°æ®è¿åŠ¨ä¸­çš„å¤§è§„æ¨¡ã€é•¿å‘¨æœŸè¶‹åŠ¿ï¼Œè¿™å¯¹ä»·æ ¼é¢„æµ‹å¯èƒ½éå¸¸é‡è¦ã€‚
- en: In principle, our agent can look at price at various scales at the same time,
    taking into account not just recent low-level movements but overall trends (and
    recent natural language processing (NLP) innovations like transformers, attention
    mechanisms, and long context windows might be really helpful there).
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬çš„ä»£ç†å¯ä»¥åŒæ—¶ä»å¤šä¸ªå°ºåº¦ä¸ŠæŸ¥çœ‹ä»·æ ¼ï¼Œè€ƒè™‘åˆ°çš„ä¸ä»…ä»…æ˜¯æœ€è¿‘çš„ä½çº§åˆ«æ³¢åŠ¨ï¼Œè¿˜åŒ…æ‹¬æ•´ä½“è¶‹åŠ¿ï¼ˆè¿‘å¹´æ¥çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åˆ›æ–°ï¼Œå¦‚transformersã€æ³¨æ„åŠ›æœºåˆ¶å’Œé•¿æ—¶é—´ä¸Šä¸‹æ–‡çª—å£ï¼Œå¯èƒ½åœ¨è¿™é‡Œéå¸¸æœ‰å¸®åŠ©ï¼‰ã€‚
- en: More training data is needed. One year of data for one stock is just 130k bars,
    which might be not enough to capture all market situations. Ideally, a real-life
    agent should be trained on a much larger dataset, such as the prices for hundreds
    of stocks for the past 10 years or more.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚å•åªè‚¡ç¥¨ä¸€å¹´çš„æ•°æ®ä»…æœ‰13ä¸‡æ¡æ•°æ®ï¼Œè¿™å¯èƒ½ä¸è¶³ä»¥æ•æ‰æ‰€æœ‰å¸‚åœºæƒ…å½¢ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼ŒçœŸå®ç”Ÿæ´»ä¸­çš„ä»£ç†åº”è¯¥åœ¨æ›´å¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚è¿‡å»10å¹´æˆ–æ›´é•¿æ—¶é—´å†…æ•°ç™¾åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®ã€‚
- en: 'Experiment with the network architecture. The convolution model has shown a
    bit faster convergence than the feed-forward model, but there are a lot of things
    to optimize: the count of layers, kernel size, residual architecture, attention
    mechanism, and so on.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•ä¸åŒçš„ç½‘ç»œæ¶æ„ã€‚å·ç§¯æ¨¡å‹æ¯”å‰é¦ˆæ¨¡å‹ç¨å¾®å¿«ä¸€ç‚¹æ”¶æ•›ï¼Œä½†æœ‰å¾ˆå¤šéœ€è¦ä¼˜åŒ–çš„åœ°æ–¹ï¼šå±‚æ•°ã€å·ç§¯æ ¸å¤§å°ã€æ®‹å·®æ¶æ„ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰ç­‰ã€‚
- en: 'There are lots of similarities between NLP and financial data analysis: both
    work with human-created sequences of data that have variable length. You can try
    to represent price bars as â€œwordsâ€ in some â€œfinancial languageâ€ (like â€œup price
    movement 1%â€ â†’ token A, â€œup price movement 2%â€ â†’ token B) and then apply NLP methods
    to this language. For example, train embeddings from the â€œsentencesâ€ to capture
    financial marketsâ€™ structure, or use transformers or even LLMs for data prediction
    and classification.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸é‡‘èæ•°æ®åˆ†æä¹‹é—´æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ï¼šä¸¤è€…éƒ½å¤„ç†äººç±»åˆ›ä½œçš„ã€å…·æœ‰å¯å˜é•¿åº¦çš„æ•°æ®åºåˆ—ã€‚ä½ å¯ä»¥å°è¯•å°†ä»·æ ¼æ¡è¡¨ç¤ºä¸ºæŸç§â€œé‡‘èè¯­è¨€â€ä¸­çš„â€œè¯æ±‡â€ï¼ˆä¾‹å¦‚ï¼Œâ€œä»·æ ¼ä¸Šæ¶¨1%â€â†’ç¬¦å·Aï¼Œâ€œä»·æ ¼ä¸Šæ¶¨2%â€â†’ç¬¦å·Bï¼‰ï¼Œç„¶åå°†NLPæ–¹æ³•åº”ç”¨äºè¿™ç§è¯­è¨€ã€‚ä¾‹å¦‚ï¼Œä»â€œå¥å­â€ä¸­è®­ç»ƒåµŒå…¥ï¼Œä»¥æ•æ‰é‡‘èå¸‚åœºçš„ç»“æ„ï¼Œæˆ–è€…ä½¿ç”¨å˜æ¢å™¨ï¼ˆtransformersï¼‰ç”šè‡³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ•°æ®é¢„æµ‹å’Œåˆ†ç±»ã€‚
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'In this chapter, we saw a practical example of RL and implemented a trading
    agent and a custom Gym environment. We tried two different architectures: a feed-forward
    network with price history on input and a 1D convolution network. Both architectures
    used the DQN method, with some of the extensions described in ChapterÂ [8](ch012.xhtml#x1-1240008).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ çš„å®é™…ä¾‹å­ï¼Œå¹¶å®ç°äº†ä¸€ä¸ªäº¤æ˜“ä»£ç†å’Œä¸€ä¸ªè‡ªå®šä¹‰çš„Gymç¯å¢ƒã€‚æˆ‘ä»¬å°è¯•äº†ä¸¤ç§ä¸åŒçš„æ¶æ„ï¼šä¸€ç§æ˜¯å°†ä»·æ ¼å†å²ä½œä¸ºè¾“å…¥çš„å‰é¦ˆç½‘ç»œï¼Œå¦ä¸€ç§æ˜¯1Då·ç§¯ç½‘ç»œã€‚ä¸¤ç§æ¶æ„éƒ½ä½¿ç”¨äº†DQNæ–¹æ³•ï¼Œå¹¶ä¸”åŠ å…¥äº†ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­æè¿°çš„ä¸€äº›æ‰©å±•ã€‚
- en: 'This was the last chapter in Part 2 of this book. In Part 3, we will talk about
    a different family of RL methods: policy gradients. Weâ€™ve touched on this approach
    a bit, but in the upcoming chapters, we will go much deeper into the subject,
    covering the REINFORCE method and the best method in the family: Asynchronous
    Advantage Actor-Critic, also known as A3C.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ¬ä¹¦ç¬¬äºŒéƒ¨åˆ†çš„æœ€åä¸€ç« ã€‚åœ¨ç¬¬ä¸‰éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦ä¸€ç±»å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼šç­–ç•¥æ¢¯åº¦ã€‚æˆ‘ä»¬å·²ç»ç®€å•æåˆ°è¿‡è¿™ç§æ–¹æ³•ï¼Œä½†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è¿™ä¸€ä¸»é¢˜ï¼Œä»‹ç»REINFORCEæ–¹æ³•ä»¥åŠè¯¥å®¶æ—ä¸­æœ€å¥½çš„æ–¹æ³•ï¼šå¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå‘˜ï¼ˆAsynchronous
    Advantage Actor-Criticï¼‰ï¼Œä¹Ÿç§°ä¸ºA3Cã€‚
- en: Leave a Review!
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç•™ä¸‹æ‚¨çš„è¯„è®ºï¼
- en: Thank you for purchasing this book from Packt Publishingâ€”we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once youâ€™ve completed
    reading it, please take a moment to leave an Amazon review; it will only take
    a minute, but it makes a big difference for readers like you. Scan the QR code
    below to receive a free ebook of your choice. [https://packt.link/NzOWQ](https://packt.link/NzOWQ)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æ‚¨ä»Packt Publishingè´­ä¹°æœ¬ä¹¦â€”â€”æˆ‘ä»¬å¸Œæœ›æ‚¨å–œæ¬¢å®ƒï¼æ‚¨çš„åé¦ˆå¯¹æˆ‘ä»¬è‡³å…³é‡è¦ï¼Œå¸®åŠ©æˆ‘ä»¬ä¸æ–­æ”¹è¿›å’Œæˆé•¿ã€‚é˜…è¯»å®Œæ¯•åï¼Œè¯·èŠ±ä¸€ç‚¹æ—¶é—´åœ¨äºšé©¬é€Šä¸Šç•™ä¸‹è¯„è®ºï¼›è¿™åªéœ€è¦ä¸€åˆ†é’Ÿï¼Œä½†å¯¹åƒæ‚¨è¿™æ ·çš„è¯»è€…æ„ä¹‰é‡å¤§ã€‚æ‰«æä¸‹æ–¹äºŒç»´ç ï¼Œè·å–æ‚¨é€‰æ‹©çš„å…è´¹ç”µå­ä¹¦ã€‚[https://packt.link/NzOWQ](https://packt.link/NzOWQ)
- en: '![PIC](img/file3.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file3.png)'
- en: Part 3
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰éƒ¨åˆ†
- en: Policy-based methods
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºç­–ç•¥çš„æ–¹æ³•
