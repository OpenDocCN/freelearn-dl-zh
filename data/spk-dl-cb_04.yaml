- en: Pain Points of Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络的痛点
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Introduction to feedforward networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈网络介绍
- en: Sequential workings of RNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络的顺序工作原理
- en: 'Paint point #1 – the vanishing gradient problem'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '痛点 #1 – 梯度消失问题'
- en: 'Pain point #2 – the exploding gradient problem'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '痛点 #2 – 梯度爆炸问题'
- en: Sequential workings of LSTMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的顺序工作原理
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Recurrent neural networks have proven to be incredibly efficient at tasks involving
    the learning and prediction of sequential data. However, when it comes to natural
    language, the question of long-term dependencies comes into play, which is basically
    remembering the context of a particular conversation, paragraph, or sentence in
    order to make better predictions in the future. For example, consider a sentence
    that says:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络在涉及学习和预测序列数据的任务中已经证明了其极高的效率。然而，当谈到自然语言时，长期依赖的问题就变得至关重要，这基本上是指记住特定对话、段落或句子的上下文，以便做出更好的预测。例如，考虑以下句子：
- en: '*Last year, I happened to visit China. Not only was Chinese food different
    from the Chinese food available everywhere else in the world, but the people were
    extremely warm and hospitable too. In my three years of stay in this beautiful
    country, I managed to pick up and speak very good....*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*去年，我偶然访问了中国。中国的食物不仅与世界其他地方的中国菜不同，而且人们也非常热情好客。在我在这个美丽的国家待了三年期间，我学会了说非常流利的...*'
- en: If the preceding sentence were fed into a recurrent neural network to predict
    the next word in the sentence (such as Chinese), the network would find it difficult
    since it has no memory of the context of the sentence. This is what we mean by
    long-term dependencies. In order to predict the word Chinese correctly, the network
    needs to know the context of the sentence as well as remember the fact that I
    happened to visit China last year. Recurrent neural networks therefore become
    inefficient at performing such tasks. However, this problem is overcome by **Long
    Short-Term Memory Units** (**LSTMs**), which are capable of remembering long-term
    dependencies and storing information in the cell state. LSTMs will be discussed
    later on, but the bulk of this chapter will focus on a basic introduction to Neural
    Networks, activation functions, Recurrent Networks, some of the main pain points
    or drawbacks of Recurrent Networks, and finally how these drawbacks may be overcome
    by the use of LSTMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将前面的句子输入到递归神经网络中，试图预测句子的下一个词（比如中文），网络将会感到困难，因为它没有记住句子的上下文。这就是我们所说的长期依赖问题。为了正确预测“中文”这个词，网络需要知道句子的上下文，并记住我去年曾经去过中国这一事实。因此，递归神经网络在执行此类任务时变得低效。然而，**长短期记忆单元**（**LSTM**）能够解决这个问题，LSTM能够记住长期依赖，并在单元状态中存储信息。关于LSTM将在后文讨论，本章的主要内容将重点介绍神经网络的基本概念、激活函数、递归网络、递归网络的一些主要痛点或缺陷，以及如何通过使用LSTM来克服这些缺陷。
- en: Introduction to feedforward networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈网络介绍
- en: To understand recurrent networks, first you have to understand the basics of
    feedforward networks. Both of these networks are named after the way they move
    information through a series of mathematical operations performed at the nodes
    of the network. One feeds information in only one direction through every node
    (never touching a given node twice), while the other cycles it through a loop
    and feeds it back to the same node (kind of like a feedback loop). It is easily
    understood how the first kind is called a **feedforward network,** while the latter
    is recurrent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解递归网络，首先你必须了解前馈网络的基础。这两种网络的命名都源于它们通过网络节点进行一系列数学运算的方式。其中一种仅通过每个节点单向传递信息（每个节点只经过一次），而另一种则通过循环将信息反馈到同一个节点（有点像反馈循环）。因此，很容易理解为什么前者被称为**前馈网络**，而后者是递归网络。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The most important concept while understanding any neural network diagram is
    the concept of computational graphs. Computational graphs are nothing but the
    nodes of the neural network connected to each other, and each node performs a
    particular mathematical function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理解任何神经网络图时，最重要的概念是计算图的概念。计算图其实就是神经网络中的各个节点通过连接互相连接，每个节点执行特定的数学函数。
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Feedforward neural networks channel the inputs (to the input layer) through
    a set of computational nodes which are nothing but mathematical operators and
    activation functions arranged in layers to calculate the network outputs. The output
    layer is the final layer of the neural network and usually contains linear functions.
    The layers between the input layer and the output layer are called **hidden layers** and
    usually contain nonlinear elements or functions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络通过一组计算节点（实际上是排列在层中的数学运算符和激活函数）来传递输入（到输入层），以计算网络的输出。输出层是神经网络的最后一层，通常包含线性函数。输入层和输出层之间的层称为**隐藏层**，通常包含非线性元素或函数：
- en: 'The following diagram (a) shows how nodes are interconnected in feedforward
    neural networks with many layers:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图（a）显示了在具有多个层的前馈神经网络中，节点如何相互连接：
- en: '![](img/d39f7304-059d-40b7-8c8f-e0638930e60b.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d39f7304-059d-40b7-8c8f-e0638930e60b.jpg)'
- en: FeedForward Neural Network
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: Feedforward neural networks mainly differ from each other by the type of functions
    (activation functions) that are used in the hidden-layer nodes. They also differ
    from each other by the algorithms that are used to optimize the other parameters
    of the network during training.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈神经网络主要通过在隐藏层节点中使用的函数（激活函数）类型来相互区分。它们也通过训练过程中优化网络其他参数的算法来相互区别。
- en: The relationships between nodes shown in the preceding diagram need not be fully
    populated for every node; optimization strategies usually start with a large number
    of hidden nodes and tune the network by eliminating connections, and possibly
    nodes, as training progresses. It may not be necessary to utilize every node during
    the training process.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述图中显示的节点之间的关系不需要对每个节点都完全填充；优化策略通常从大量隐藏节点开始，并通过在训练过程中消除连接，甚至可能是节点，来调整网络。在训练过程中可能并不需要使用每个节点。
- en: How it works...
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The neuron is the basic structural element of any neural network. A neuron
    can be thought of as a simple mathematical function or operator that operates
    on the input flowing through it to produce an output flowing out of it. The inputs
    to a neuron are multiplied by the node''s weight matrix, summed over all the inputs,
    translated, and passed through an activation function. These are basically matrix
    operations in mathematics as described here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是任何神经网络的基本结构单元。神经元可以被视为一个简单的数学函数或运算符，它作用于输入流并生成输出。神经元的输入被节点的权重矩阵乘以，对所有输入求和，进行平移后，再通过激活函数处理。这些基本上是数学中的矩阵运算，如下所示：
- en: The computational graph representation of a neuron is shown in the preceding
    diagram (b).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经元的计算图表示如前图（b）所示。
- en: 'The transfer function for a single neuron or node is written as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个神经元或节点的传递函数写作如下：
- en: '![](img/b755894e-1200-441f-b0ab-59898c595e85.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b755894e-1200-441f-b0ab-59898c595e85.png)'
- en: Here, *x*[ *i* ]is the input to the ith node, *w*[ *i* ]is the weight term associated
    with the *i*^(th) node, *b* is the bias which is generally added to prevent overfitting, *f*(⋅)
    is the activation function operating over the inputs flowing into the node, and *y* is
    the output from the node.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[*i*]是第*i*个节点的输入，*w*[*i*]是与第*i*个节点相关的权重项，*b*是通常为防止过拟合而添加的偏置项，*f*(⋅)是作用于输入流向节点的激活函数，*y*是节点的输出。
- en: Neurons with sigmoidal activation functions are commonly used in the hidden
    layer(s) of the neural network, and the identity function is usually used in the
    output layer.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Sigmoid激活函数的神经元通常用于神经网络的隐藏层，而恒等函数通常用于输出层。
- en: The activation functions are generally chosen in a manner to ensure the outputs
    from the node are strictly increasing, smooth (continuous first derivative), or
    asymptotic.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数通常选择的方式是确保节点的输出是严格递增的、平滑的（连续的第一导数），或者是渐近的。
- en: 'The following logistic function  is used as a sigmoidal activation function:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下的逻辑函数用作Sigmoid激活函数：
- en: '![](img/ce8be72e-2f97-46da-bd51-326378f9b278.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce8be72e-2f97-46da-bd51-326378f9b278.png)'
- en: A neural trained using backpropagation algorithm may learn faster if the activation
    function is antisymmetric, that is, *f*(-*x*) = -*f*(*x*) as in the case of the
    sigmoidal activation function. The backpropagation algorithm will be discussed
    in detail in the following sections of this chapter.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播算法训练的神经网络，如果激活函数是反对称的，可能会学习得更快，即 *f*(-*x*) = -*f*(*x*)，如sigmoidal激活函数的情况。反向传播算法将在本章的后续部分中详细讨论。
- en: 'The logistic function, however, is not antisymmetric, but can be made antisymmetric
    by a simple scaling and shift, resulting in the hyperbolic tangent function which
    has  a first derivative described by *f *''(*x*) = 1 - *f *²(*x*),  as shown in
    the following mathematical function:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，逻辑斯蒂函数并不是反对称的，但通过简单的缩放和平移，可以将其变为反对称，从而得到双曲正切函数，其一阶导数由*f*'(*x*) = 1 - *f*²(*x*)描述，如以下数学公式所示：
- en: '![](img/1f3c3908-266c-4400-b025-b3b641a79d06.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f3c3908-266c-4400-b025-b3b641a79d06.png)'
- en: The simple form of the sigmoidal function and its derivative allows for the
    quick and accurate calculation of the gradients needed to optimize the selection
    of the weights and biases and carry out second-order error analysis.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: sigmoidal 函数及其导数的简单形式使得能够快速且准确地计算优化权重和偏差选择所需的梯度，并进行二阶误差分析。
- en: There's more...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'At every neuron/node in the layers of a neural network, a series of matrix
    operations are performed. A more mathematical way of visualizing the feedforward
    network is given in the following diagram, which will help you to better understand
    the operations at each node/neuron:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的每个神经元/节点中，都会执行一系列的矩阵运算。以下图所示，给出了一种更具数学性的方式来可视化前馈网络，这将帮助你更好地理解每个节点/神经元的运算：
- en: '![](img/b4f7b3f7-b9a9-4cc3-8214-1dd45ccd1679.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4f7b3f7-b9a9-4cc3-8214-1dd45ccd1679.png)'
- en: Intuitively, we can see that the inputs (which are vectors or matrices) are
    first multiplied by weight matrices. A bias is added to this term and then activated
    using an activation function (such as ReLU, tanh, sigmoid, threshold, and so on)
    to produce the output. Activation functions are key in ensuring that the network
    is able to learn linear as well as non-linear functions.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直观地看，我们可以看到，输入（无论是向量还是矩阵）首先与权重矩阵相乘。接着给这一项加上偏置，并通过激活函数（例如 ReLU、tanh、sigmoid、阈值函数等）激活，产生输出。激活函数在确保网络能够学习线性和非线性函数方面至关重要。
- en: 'This output then flows into the next neuron as its input, and the same set
    of operations are performed all over again. A number of such neurons combine together
    to form a layer (which performs a certain function or learns a certain feature
    of the input vector), and many such layers combine together to form a feedforward
    neural network that can learn to recognize inputs completely, as shown in the
    following diagram:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这个输出流入下一个神经元作为其输入，重复执行相同的操作。若干个这样的神经元结合在一起形成一层（执行特定的功能或学习输入向量的某个特征），而多层这样的神经元结合在一起形成一个前馈神经网络，能够完全学习识别输入，如下图所示：
- en: '![](img/eefc8906-1179-48e1-9ed9-d04ac3a61f56.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eefc8906-1179-48e1-9ed9-d04ac3a61f56.png)'
- en: 'Let''s suppose our feedforward network has been trained to classify images
    of dogs and images of cats. Once the network is trained, as shown in the following
    diagram, it will learn to label images as dog or cat when presented with new images:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们的前馈网络已经训练完成，用于分类狗和猫的图像。一旦网络训练完成，如下图所示，它将在面对新图像时，学会将图像标记为狗或猫：
- en: '![](img/0e66f91a-dd80-4096-8aec-2f815f533387.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e66f91a-dd80-4096-8aec-2f815f533387.png)'
- en: In such networks, there is no relation between the present output and the previous
    or future outputs.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种网络中，当前的输出与之前或未来的输出之间没有关系。
- en: This means the feedforward network can basically be exposed to any random collection
    of images and the first image it is exposed to will not necessarily alter how
    it classifies the second or third images. Therefore, we can say that the output
    at time step *t* is independent of the output at time step  *t - 1*.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着前馈网络基本上可以接触到任何随机的图像集合，而它首先接触到的图像不会必然改变它如何分类第二张或第三张图像。因此，我们可以说，在时间步 *t* 时的输出与在时间步
    *t - 1* 时的输出是独立的。
- en: Feedforward networks work well in such cases as image classification, where
    the data is not sequential. Feedforward networks also perform well when used on
    two related variables such as temperature and location, height and weight, car
    speed and brand, and so on.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈网络在图像分类等数据不具有顺序性的情况下效果良好。前馈网络在处理两个相关变量时也表现出色，例如温度和位置、高度和体重、车速和品牌等。
- en: However, there may be cases where the current output is dependent on the outputs
    at previous time steps (the ordering of data is important).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，也可能存在当前输出依赖于前面时间步的输出的情况（数据的顺序性很重要）。
- en: Consider the scenario of reading a book. Your understanding of the sentences
    in the book is based on your understanding of all the words in the sentence. It
    wouldn't be possible to use a feedforward network to predict the next word in
    a sentence, as the output in such a case would depend on the previous outputs.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设想阅读一本书的情景。你对书中句子的理解，依赖于你对句子中所有单词的理解。在这种情况下，无法使用前馈网络来预测句子中的下一个单词，因为输出在这种情况下依赖于先前的输出。
- en: 'Similarly, there are many cases where the output requires the previous output
    or some information from the previous outputs (for example, stock market data,
    NLP, voice recognition, and so on). The feedforward network may be modified as
    in the following diagram to capture information from previous outputs:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，也有许多情况下输出需要依赖于先前的输出或某些来自先前输出的信息（例如股市数据、自然语言处理、语音识别等）。前馈网络可以如以下图所示进行修改，以捕获先前输出的信息：
- en: '![](img/d553d2b8-9636-4209-a475-a7709c6bc69a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d553d2b8-9636-4209-a475-a7709c6bc69a.png)'
- en: At time step *t*, the input at *t* as well as the information from *t-1* is
    both provided to the network to obtain the output at time *t*.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步*t*时，来自*t*的输入以及来自*t-1*的历史信息都会提供给网络，以获得*t*时刻的输出。
- en: Similarly, the information from *t* as well as the new input is fed into the
    network at time step *t+1* to produce the output at *t+1*. The right-hand side
    of the preceding diagram is a generalized way of representing such a network where
    the output of the network flows back in as input for future time steps. Such a
    network is called a **recurrent neural network** (**RNN**).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，来自*t*的信息以及新的输入会在时间步*t+1*时输入到网络中，从而产生*t+1*时刻的输出。前面图表的右侧是表示此类网络的一种泛化方式，其中网络的输出作为输入流回，以供未来时间步使用。这样的网络称为**递归神经网络**（**RNN**）。
- en: See also
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: '**Activation Function**: In artificial neural networks, the activation function of
    a node decides the kind of output that node produces, given an input or set of
    inputs. The output *y[k]* is given by the input *u[k]* and bias *b[k]*, which
    are passed through the activation function *φ(.)*  as shown in the following expression:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**：在人工神经网络中，节点的激活函数决定了该节点在给定输入或输入集时产生的输出。输出*y[k]*由输入*u[k]*和偏置*b[k]*给出，这些值通过激活函数*φ(.)*传递，如下所示：'
- en: '![](img/453f22b2-8584-4219-876e-a1e06896fc78.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/453f22b2-8584-4219-876e-a1e06896fc78.png)'
- en: 'There are various types of activation functions. The following are the commonly
    used ones:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有多种类型。以下是常用的几种：
- en: '**Threshold function**:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**阈值函数**：'
- en: '![](img/053acc51-b93f-4166-9ed0-f5d279beaa17.png)![](img/9a44b42b-3cab-44d7-a7d4-d473dda6897d.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/053acc51-b93f-4166-9ed0-f5d279beaa17.png)![](img/9a44b42b-3cab-44d7-a7d4-d473dda6897d.jpg)'
- en: It is clear from the preceding diagram that this kind of  function restricts
    the output values of neurons to between 0 and 1\. This may be useful in many cases.
    However, this function is non-differentiable, which means it cannot be used to
    learn non-linearities, which is vital when using the backpropagation algorithm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面图表中可以看出，这种函数将神经元的输出值限制在0到1之间。许多情况下这可能非常有用。然而，这个函数是不可微分的，意味着它不能用于学习非线性，这在使用反向传播算法时至关重要。
- en: '**Sigmoid function**:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Sigmoid函数**：'
- en: '![](img/997a504a-0af5-4ed6-a7ab-8f11ea140dd4.png)![](img/c8c7d521-a6b3-4bef-9ae9-926e1827dbf6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/997a504a-0af5-4ed6-a7ab-8f11ea140dd4.png)![](img/c8c7d521-a6b3-4bef-9ae9-926e1827dbf6.png)'
- en: The sigmoid function is a logistic function with a lower limit of 0 and an upper
    limit of 1, as with the threshold function. This activation function is continuous
    and therefore, also differentiable. In the sigmoid function, the slope parameter
    of the preceding function is given by α. The function is nonlinear in nature,
    which is critical in increasing the performance since it is able to accommodate
    non linearities in the input data unlike regular linear functions. Having non
    linear capabilities ensure that small changes in the weights and bias causes significant
    changes in the output of the neuron.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是一个逻辑函数，具有0和1的下限和上限，类似于阈值函数。该激活函数是连续的，因此也是可微的。在sigmoid函数中，前述函数的斜率参数由α给出。该函数本质上是非线性的，这在提高性能方面至关重要，因为它能够适应输入数据中的非线性，而不像常规线性函数那样。具备非线性能力确保了权重和偏差的微小变化会导致神经元输出的显著变化。
- en: '**Hyperbolic Tangent function (tanh)**:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**双曲正切函数（tanh）**：'
- en: '![](img/72529d7e-7c5e-43ad-814f-8487b8ca231c.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72529d7e-7c5e-43ad-814f-8487b8ca231c.png)'
- en: This function enables activation functions to range from -1 to +1 instead of
    between 0 and 1 as in the previous cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使激活函数的范围从-1到+1，而不是像之前的情况那样在0和1之间。
- en: '**Rectified Linear Unit (ReLU) function**: ReLUs are the smooth approximation
    to the sum of many logistic units, and produce sparse activity vectors. The following
    is the equation of the function:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修正线性单元（ReLU）函数**：ReLU是许多逻辑单元之和的平滑近似，并产生稀疏活动向量。以下是该函数的公式：'
- en: '![](img/e434cc61-8a42-4a32-904f-535621776d8f.png)![](img/a212b63d-3aa8-4b20-931f-745e2d4d6301.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e434cc61-8a42-4a32-904f-535621776d8f.png)![](img/a212b63d-3aa8-4b20-931f-745e2d4d6301.jpg)'
- en: ReLU function graph
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数图
- en: In the preceding diagram, softplus ![](img/af6aae9d-9854-4827-afdd-5f175d48a865.png)(x)
    = log ( 1 + e^x) is the smooth approximation to the rectifier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，softplus ![](img/af6aae9d-9854-4827-afdd-5f175d48a865.png)(x) = log ( 1
    + e^x) 是对修正函数的平滑近似。
- en: '**Maxout function**: This function utilizes a technique known as **"dropout"** and
    improves the accuracy of the dropout technique''s fast approximate model averaging
    in order to facilitate optimization.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Maxout函数**：该函数采用一种称为**“dropout”**的技术，并通过改进dropout技术的快速近似模型平均，提高优化的准确性。'
- en: 'Maxout networks learn not just the relationship between hidden units, but also
    the activation function of each hidden unit. By actively dropping out hidden units,
    the network is forced to find other paths to get to the output from a given input
    during the training process. The following diagram is the graphical depiction
    of how this works:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout网络不仅学习隐藏单元之间的关系，还学习每个隐藏单元的激活函数。通过主动丢弃隐藏单元，网络在训练过程中被迫寻找其他路径以从给定的输入得到输出。下图是其工作原理的图形化表示：
- en: '![](img/1d788bef-0294-4200-ae1c-2597a552912a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d788bef-0294-4200-ae1c-2597a552912a.png)'
- en: Maxout Network
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout网络
- en: 'The preceding diagram shows the Maxout network with five visible units, three
    hidden units, and two neurons for each hidden unit. The Maxout function is given
    by the following equations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个包含五个可见单元、三个隐藏单元和每个隐藏单元两个神经元的Maxout网络。Maxout函数由以下公式给出：
- en: '![](img/748c5423-9b57-4857-88fc-e4470c077d46.png)![](img/3ae85183-c7da-4f19-9e0d-f5d3c8de7156.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/748c5423-9b57-4857-88fc-e4470c077d46.png)![](img/3ae85183-c7da-4f19-9e0d-f5d3c8de7156.png)'
- en: 'Here  W..[ij ] is the mean vector of the size of the input obtained by accessing
    the matrix W ∈  ![](img/26730fcb-dc57-47ea-a46f-7ef2981a45f8.png) at the second
    coordinate *i* and third coordinate *j*. The number of intermediate units (*k) *is
    called the number of pieces used by the Maxout nets. The following diagram shows
    how the Maxout function compares to the ReLU and **Parametric Rectified Linear
    Unit** (**PReLU**) functions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的W..[ij]是通过访问矩阵W∈ ![](img/26730fcb-dc57-47ea-a46f-7ef2981a45f8.png)在第二坐标*i*和第三坐标*j*上的输入大小所获得的均值向量。中间单元的数量（*k）*称为Maxout网络使用的分段数。下图展示了Maxout函数与ReLU和**参数化修正线性单元（PReLU）**函数的比较：
- en: '![](img/a0d75714-9de0-455a-a461-1eca6e9a13dc.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0d75714-9de0-455a-a461-1eca6e9a13dc.png)'
- en: Graphical comparison of Maxout, ReLU and PReLU function
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Maxout、ReLU和PReLU函数的图形比较
- en: Sequential workings of RNNs
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的顺序工作
- en: 'Recurrent neural networks are a type of artificial neural network designed
    to recognize and learn patterns in sequences of data. Some of the examples of
    such sequential data are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是一种人工神经网络，旨在识别和学习数据序列中的模式。以下是一些此类顺序数据的示例：
- en: Handwriting
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写文字
- en: Text such as customer reviews, books, source code, and so on
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如客户评价、书籍、源代码等文本
- en: Spoken word / Natural Language
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口语/自然语言
- en: Numerical time series / sensor data
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值时间序列/传感器数据
- en: Stock price variation data
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票价格变化数据
- en: Getting ready
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In recurrent neural networks, the hidden state from the previous time step
    is fed back into the network at the next time step, as shown in the following
    diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络中，前一个时间步的隐藏状态会在下一个时间步反馈到网络中，正如下图所示：
- en: '![](img/dd9b9ab7-9c66-45ee-8200-bc520bbdf0d3.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd9b9ab7-9c66-45ee-8200-bc520bbdf0d3.png)'
- en: Basically, the upward facing arrows going into the network represent the inputs
    (matrices/vectors) to the RNN at each time step, while the upward-facing arrows
    coming out of the network represent the output of each RNN unit. The horizontal
    arrows indicate the transfer of information learned in a particular time step
    (by a particular neuron) onto the next time step.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，朝上箭头进入网络表示每个时间步的RNN输入（矩阵/向量），而朝上箭头从网络中出来表示每个RNN单元的输出。横向箭头表示将特定时间步（由特定神经元学习到的信息）传递到下一个时间步。
- en: 'More information about using RNNs can be found at :'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用RNN的更多信息，请访问：
- en: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
- en: How to do it...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'At every node/neuron of a recurrent network, a series of matrix multiplication
    steps are carried out. The input vector/matrix is multiplied by a weight vector/matrix
    first, a bias is added to this term, and this is finally passed through an activation
    function to produce the output (just as in the case of feedforward networks):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归网络的每个节点/神经元中，都进行一系列的矩阵乘法步骤。输入向量/矩阵首先与权重向量/矩阵相乘，然后加上偏置，最后通过激活函数生成输出（与前馈网络相似）：
- en: 'The following diagram shows an intuitive and mathematical way of visualizing
    RNNs in the form of a computational graph:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图表以计算图的形式展示了RNN的直观和数学表示方法：
- en: '![](img/8f28fe47-36f1-491d-a3e9-b4c7920b29f3.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f28fe47-36f1-491d-a3e9-b4c7920b29f3.png)'
- en: At the first time step (which is *t=0*), *h*[*0* ]is calculated using the first
    formula on the right-hand side of the preceding diagram. Since *h*^(*-1* )does
    not exist, the middle term becomes zero.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个时间步（即 *t=0*）时，*h*[*0*] 根据前图右侧的第一个公式计算。由于 *h*^(*-1*) 不存在，因此中间项变为零。
- en: The input matrix *x*[*0* ]is multiplied by the weight matrix *w[i]* and a bias
    *b[h]* is added to this term.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入矩阵 *x*[*0*] 与权重矩阵 *w[i]* 相乘，并在此基础上加上一个偏置 *b[h]*。
- en: The two preceding matrices are added and then passed through an activation function
    *g[h]* to obtain *h[0]*.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面两个矩阵相加后，通过激活函数 *g[h]* 计算得到 *h[0]*。
- en: Similarly, *y[0]* is calculated using the second equation on the right-hand
    side of the preceding diagram by multiplying *h[0]* with the weight matrix *w[y]*,
    adding a bias *b[y]* to it, and passing it through an activation function *g[y]*.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，*y[0]* 通过前图右侧的第二个公式计算，方法是将 *h[0]* 与权重矩阵 *w[y]* 相乘，添加偏置 *b[y]*，然后通过激活函数 *g[y]*
    处理。
- en: At the next time step (which is *t=1*), *h^((t-1))* does exist. It is nothing
    but *h[0]*. This term, multiplied with the weight matrix *w[R]*, is also provided
    as the input to the network along with the new input matrix *x[1]*.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个时间步（即 *t=1*）时，*h^((t-1))* 确实存在，它就是 *h[0]*。这个项与权重矩阵 *w[R]* 相乘后，连同新的输入矩阵 *x[1]*
    一起作为输入提供给网络。
- en: This process is repeated over a number of time steps, and the weights, matrices,
    and biases flow through the entire network over different time steps.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会在多个时间步中重复，权重、矩阵和偏置会在不同的时间步流经整个网络。
- en: This entire process is executed over one single iteration, which constitutes
    the forward pass of the network.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程在一个迭代中完成，这就是网络的前向传播。
- en: How it works...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'To train feedforward neural networks the most commonly used technique is backpropagation
    through time. It is a supervised learning method used to reduce the loss function
    by updating weights and biases in the network after every time step. A number
    of training cycles (also known as epochs) are executed where the error determined
    by the loss function is backward propagated by a technique called gradient descent.
    At the end of each training cycle, the network updates its weights and biases
    to produce an output which is closer to the desired output, until a sufficiently
    small error is achieved :'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练前馈神经网络，最常用的技术是通过时间的反向传播。这是一种监督学习方法，通过在每个时间步后更新网络中的权重和偏置来减少损失函数。执行多个训练周期（也称为迭代），在每个周期中，损失函数确定的错误通过一种叫做梯度下降的技术向后传播。在每个训练周期结束时，网络更新其权重和偏置，以生成一个接近目标输出的输出，直到达到足够小的误差：
- en: 'The backpropagation algorithm basically implements the following three fundamental
    steps during every iteration:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播算法基本上在每次迭代中执行以下三个基本步骤：
- en: The forward pass of the input data and calculating the loss function
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据的前向传播并计算损失函数
- en: The computation of gradients and errors
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度和误差的计算
- en: Backpropagation through time and adjustment of weights and biases accordingly
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过时间的反向传播以及相应的权重和偏置调整
- en: After the weighted sum of inputs (passed through an activation function after
    adding a bias) is fed into the network and an output is obtained, the network
    immediately compares how different the predicted output is from the actual case
    (correct output).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在加上偏置并通过激活函数后的输入加权和被传入网络并得到输出后，网络立即比较预测输出与实际输出（正确输出）之间的差异。
- en: Next, the error is calculated by the network. This is nothing but the network
    output subtracted from the actual/correct output.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，网络通过将网络输出减去实际/正确输出来计算误差。
- en: The next step involves backpropagation through the entire network based on the
    calculated error. The weights and biases are then updated to notice whether the
    error increases or decreases.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是基于计算出的误差对整个网络进行反向传播，然后更新权重和偏置，观察误差是增加还是减少。
- en: The network also remembers whether the error increases by increasing the weights
    and biases or by decreasing the weights and biases.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络还记得是通过增加权重和偏置，还是通过减少权重和偏置，导致错误的增加。
- en: Based on the preceding inferences, the network continues to update the weights
    and biases during every iteration in a manner such that the error becomes minimal.
    The following example will make things clearer.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于前面的推论，网络在每次迭代中继续更新权重和偏置，以使得误差最小化。以下示例将使问题更清晰。
- en: 'Consider a simple case of teaching a machine how to double a number, as shown
    in the following table:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个简单的例子，教学机器如何将一个数字翻倍，如下表所示：
- en: '![](img/600d03c8-8815-4d5f-8465-1b4f8eba843f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/600d03c8-8815-4d5f-8465-1b4f8eba843f.png)'
- en: As you can see, by initializing the weights randomly (*W = 3*), we obtain outputs
    of 0, 3, 6, and 9.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，通过随机初始化权重（*W = 3*），我们得到了0、3、6和9的输出。
- en: The error is calculated by subtracting the column of correct outputs from the
    column of model outputs. The square error is nothing but each error term multiplied
    by itself. It is usually a better practice to use square error as it eliminates
    negative values from error terms.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误是通过将正确输出列减去模型输出列来计算的。平方误差就是每个错误项自乘的结果。通常，使用平方误差是一种更好的做法，因为它消除了错误项中的负值。
- en: The model then realizes that in order to minimize the error, the weight needs
    to be updated.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型接着意识到，为了最小化误差，权重需要被更新。
- en: 'Let''s suppose the model updates its weight to *W = 4* during the next iteration.
    This would result in the following output:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设模型在下一次迭代中将权重更新为*W = 4*，这将导致以下输出：
- en: '![](img/00dc3ea3-a4e2-4fd9-b66c-67daf62635fc.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00dc3ea3-a4e2-4fd9-b66c-67daf62635fc.png)'
- en: The model now realizes that the error actually increased by increasing the weight
    to *W = 4*. Therefore, the model updates its weight by reducing it to *W = 2*
    in its next iteration, which results in the actual/correct output.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，模型意识到，通过将权重增加到*W = 4*，错误实际上增加了。因此，模型在下一次迭代中将权重更新为*W = 2*，从而得到实际/正确的输出。
- en: 'Note that, in this simple case, the error increases when the weight is increased
    and reduces when the weight is decreased, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在这个简单的例子中，当权重增加时，错误增加，而当权重减少时，错误减少，如下所示：
- en: '![](img/08d9f110-c76b-4e1a-8cf5-4e3cdca260b9.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08d9f110-c76b-4e1a-8cf5-4e3cdca260b9.png)'
- en: In an actual neural network, a number of such weight updates are performed during
    every iteration until the model converges with the actual/correct output.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实际的神经网络中，每次迭代都会执行多次这样的权重更新，直到模型收敛到实际/正确的输出。
- en: There's more...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'As seen in the preceding case, the error increased when the weight was increased
    but decreased when the weight was decreased. But this may not always be the case.
    The network uses the following graph to determine how to update weights and when
    to stop updating them:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当权重增加时，错误增加；而当权重减小时，错误减小。但这并不总是如此。网络使用以下图表来确定如何更新权重以及何时停止更新权重：
- en: '![](img/f17c4901-1b95-4398-92e5-7f12a8765452.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f17c4901-1b95-4398-92e5-7f12a8765452.png)'
- en: Let the weights be initialized to zero at the beginning of the first iteration.
    As the network updates its weights by increasing them from point A to B, the error
    rate begins to decrease.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让权重在第一次迭代开始时初始化为零。随着网络通过将权重从A点增加到B点来更新权重，错误率开始降低。
- en: Once the weights reach point B, the error rate becomes minimal. The network
    constantly keeps track of the error rate.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦权重达到B点，错误率变得最小。网络会不断追踪错误率。
- en: On further increasing the weights from point B towards point C, the network
    realizes that the error rate begins to increase again. Thus, the network stops
    updating its weights and reverts back to the weights at point B, as they are optimal.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进一步增加权重从B点到C点时，网络意识到错误率开始再次增加。因此，网络停止更新权重并恢复到B点的权重，因为它们是最优的。
- en: 'In the next scenario, consider a case where the weights are randomly initialized
    to some value (let''s say, point C), as shown in the following graph:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个场景中，考虑一个权重被随机初始化为某个值（假设为C点）的情况，如下图所示：
- en: '![](img/1d3effb7-7d07-42f1-a813-eaa414d61e61.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d3effb7-7d07-42f1-a813-eaa414d61e61.png)'
- en: On further increasing these random weights, the error also increases ( starting
    at point C and moving away from point B, indicated by the small arrow in the graph).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进一步增加这些随机权重时，错误也会增加（从C点开始，图中小箭头表示从B点移开）。
- en: The network realizes that the error increased and begins to decrease the weights
    from point C so that the error decreases (indicated by the long arrow from point
    C moving towards point B in the graph). This decrease of weights happens until
    the error reaches a minimal value (point B on the graph).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络意识到错误增加，开始从C点减少权重，以便错误减少（图中箭头从C点向B点移动）。这一权重减少一直持续到错误达到最小值（图中的B点）。
- en: The network continues to further update its weights even after reaching point
    B (indicated by the arrow moving away from point B and towards point A on the
    graph). It then realizes that the error is again increasing. As a result, it stops
    the weight update and reverts back to the weights that gave the minimal error
    value (which are the weights at point B).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络即使在达到B点后仍继续更新权重（如图中箭头从B点指向A点）。它随即意识到错误再次增加。因此，网络停止更新权重并恢复到最小错误值的权重（即B点的权重）。
- en: 'This is how neural networks perform weight updates after backpropagation. This
    kind of weight update is momentum-based. It relies on the computed gradients at
    each neuron of the network during every iteration, as shown in the following diagram:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这就是神经网络在反向传播后如何执行权重更新的方式。这种权重更新是基于动量的。它依赖于每次迭代中计算的每个神经元的梯度，如下图所示：
- en: '![](img/8b87bfbc-987b-4a20-8700-a1f650992629.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b87bfbc-987b-4a20-8700-a1f650992629.png)'
- en: Basically, the gradients are computed for each input with respect to the output
    every time an input flows into a neuron. The chain rule is used to compute the
    gradients during the backward pass of backpropagation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，每当输入流入神经元时，都会根据输出计算每个输入的梯度。在反向传播的过程中，链式法则被用来计算梯度。
- en: See also
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'A detailed explanation of the math behind backpropagation can be found at the
    following links:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 关于反向传播背后的数学原理的详细解释，可以参考以下链接：
- en: '[https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)'
- en: '[https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)'
- en: 'Andrej Karpathy''s blog has tons of useful information about recurrent neural
    networks. Here is a link explaining their unreasonable effectiveness:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Andrej Karpathy 的博客包含了大量关于递归神经网络的有用信息。以下是一个解释它们为何如此有效的链接：
- en: '[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
- en: 'Pain point #1 – The vanishing gradient problem'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '痛点 #1 – 消失梯度问题'
- en: Recurrent neural networks are great for tasks involving sequential data. However,
    they do come with their drawbacks. This section will highlight and discuss one
    such drawback, known as the **vanishing gradient problem**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络非常适合处理包含序列数据的任务。然而，它们也有一些缺点。本节将重点讨论并分析其中一个缺点，即**消失梯度问题**。
- en: Getting ready
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: The name vanishing gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度问题这个名称源于在反向传播步骤中，一些梯度消失或变为零。从技术上讲，这意味着在网络的反向传递过程中没有误差项被反向传播。当网络变得更深、更复杂时，这会成为一个问题。
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'This section will describe how the vanishing gradient problem occurs in recurrent
    neural networks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述消失梯度问题如何在递归神经网络中发生：
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用反向传播时，网络首先计算误差，这个误差就是模型输出与实际输出的差的平方（例如平方误差）。
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个误差，模型接着计算权重变化（de/dw）与误差变化之间的关系。
- en: The computed derivative multiplied by the learning rate ![](img/02520e4e-7b92-446d-a359-0cc7fd1ec5cf.png) gives ![](img/8a3a2dce-1669-41c3-90fd-ab676c3af592.png)w,
    which is nothing but the change in weights. The term ![](img/f70f52a8-c4f3-47b7-b6a9-c7bc77d6b74f.png)w
    is added to the original weights to update them to the new weights.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算出的导数与学习率相乘 ![](img/02520e4e-7b92-446d-a359-0cc7fd1ec5cf.png) 得到 ![](img/8a3a2dce-1669-41c3-90fd-ab676c3af592.png)w，这实际上就是权重的变化。这个 ![](img/f70f52a8-c4f3-47b7-b6a9-c7bc77d6b74f.png)w
    被加到原始权重上，更新为新的权重。
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is much less than 1, then that term multiplied by the learning rate ![](img/20098d2b-b23b-4c96-93fe-0e39482b627b.png) (which
    is always much less than 1) gives a very small, negligible, number which is negligible.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设 de/dw（误差关于权重的梯度或变化率）的值远小于 1，那么这个值与学习率相乘 ![](img/20098d2b-b23b-4c96-93fe-0e39482b627b.png)（学习率总是远小于
    1）将得到一个非常小的、可忽略的数字。
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为反向传播过程中权重更新只对最新的时间步准确，且随着反向传播通过之前的时间步，这种准确性会降低，经过多次时间步后几乎变得微不足道。
- en: There may be certain cases where sentences may be extremely long and the neural
    network is trying to predict the next word in a sentence. It does so based on
    the context of the sentence, for which it needs information from many previous
    time steps (these are called **long-term dependencies**). The number of previous
    times steps the network needs to backpropagate through increases with the increasing
    length of sentences. In such cases, the recurrent networks become incapable of
    remembering information from many time steps in the past and therefore are unable
    to make accurate predictions.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，句子可能非常长，神经网络需要根据上下文预测句子中的下一个单词。为了做出这样的预测，网络需要从许多前面的时间步获取信息（这些被称为**长期依赖**）。随着句子长度的增加，网络需要反向传播的时间步数也在增加。在这种情况下，递归网络无法记住过去许多时间步的信息，因此无法做出准确的预测。
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term vanishes (by reducing over time) and changes in
    weight (![](img/c5361123-15dd-4a54-a3c1-48c41ba7b1d2.png)w) become negligibly
    small. As a result, the new or updated weight is almost equal to the previous
    weight.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当发生这种情况时，网络需要更多的复杂计算，结果是迭代次数大幅增加，在此过程中，误差项的变化逐渐消失（随着时间推移减小），权重的变化（![](img/c5361123-15dd-4a54-a3c1-48c41ba7b1d2.png)w）变得微乎其微。因此，新的或更新后的权重几乎等于之前的权重。
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights, which is a problem as this will cause the model to
    overfit the data.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有发生权重更新，网络停止学习或无法更新其权重，这会导致模型过拟合数据，进而产生问题。
- en: 'This entire process is illustrated in the following diagram:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个过程如下面的图示所示：
- en: '![](img/32031e59-d92e-4da9-b804-82fe2cfc4381.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32031e59-d92e-4da9-b804-82fe2cfc4381.png)'
- en: How it works...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'This section will describe some of the repercussions of the vanishing gradient
    problem:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述梯度消失问题的一些后果：
- en: This problem occurs when we train a neural network model using some sort of
    optimization techniques which are gradient based.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个问题发生在我们使用某种基于梯度的优化技术训练神经网络模型时。
- en: Generally, adding more hidden layers tends to make the network able to learn
    more complex arbitrary functions, and thus do a better job in predicting future
    outcomes. Deep Learning makes a big difference due to the large number of hidden
    layers it has, ranging from 10 to 200\. It is now possible to make sense of complicated
    sequential data, and perform tasks such as Speech Recognition, Image Classification,
    Image Captioning, and more.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般来说，添加更多的 **隐藏层** 趋向于使网络能够学习更复杂的任意函数，从而在预测未来结果时表现得更好。深度学习因其拥有大量的 **隐藏层**（从10到200层不等）而产生了巨大差异。现在可以理解复杂的序列数据，并执行语音识别、图像分类、图像描述等任务。
- en: The problem caused by the preceding steps is that, in some cases, the gradients
    become so small that they almost vanish, which in turn prevents the weights from
    updating their values during future time steps.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面步骤导致的问题是，在某些情况下，梯度变得非常小，几乎消失，这会阻止权重在未来的时间步更新其值。
- en: In the worst case, it could result in the training process of the network being
    stopped, which means that the network stops learning the different features it
    was intended to learn through the training steps.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最坏的情况下，这可能导致网络的训练过程停止，这意味着网络停止学习它原本打算通过训练步骤学习的不同特征。
- en: The main idea behind backpropagation is that it allows us, as researchers, to
    monitor and understand how machine learning algorithms process and learn various
    features. When the gradients vanish, it becomes impossible to interpret what is
    going on with the network, and hence identifying and debugging errors becomes
    even more of a challenge.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播背后的主要思想是，它使我们这些研究人员能够监控和理解机器学习算法如何处理和学习各种特征。当梯度消失时，就无法解释网络的运作方式，因此识别和调试错误变得更加具有挑战性。
- en: There's more...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'The following are some of the ways in which the problem of vanishing gradients
    can be solved:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解决梯度消失问题的一些方法：
- en: One method to overcome this problem to some extent by using the ReLU activation
    function. It computes the function *f(x)=max(0,x) (i.e., t*he activation function
    simply thresholds the lower level of outputs at zero) and prevents the network
    from producing negative gradients.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服这个问题的一种方法是使用ReLU激活函数。它计算函数 *f(x)=max(0,x)*（即，激活函数简单地将输出的下限阈值设为零），并防止网络生成负梯度。
- en: Another way to overcome this problem is to perform unsupervised training on
    each layer separately and then fine-tune the entire network through backpropagation,
    as done by Jürgen Schmidhuber in his study of multi-level hierarchy in neural
    networks. The link to this paper is provided in the following section.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种解决这个问题的方法是对每一层分别进行无监督训练，然后通过反向传播对整个网络进行微调，正如Jürgen Schmidhuber在他关于神经网络多层次结构的研究中所做的那样。该论文的链接将在以下部分提供。
- en: A third solution to this problem is the use of **LSTM** (**Long Short-Term Memory**)
    units or **GRUs (Gated Recurrent Units)**, which are special types of RNNs.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的第三种方法是使用 **LSTM**（**长短期记忆**）单元或 **GRUs（门控递归单元）**，它们是特殊类型的RNN。
- en: See also
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'The following links provide a more in-depth description of the vanishing gradient
    problem and also some ways to tackle the issue:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了有关消失梯度问题的更深入描述，并介绍了一些解决该问题的方法：
- en: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
- en: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
- en: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
- en: 'Pain point #2 – The exploding gradient problem'
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '痛点 #2 – 梯度爆炸问题'
- en: Another drawback of recurrent neural networks is the problem of exploding gradients.
    This is similar to the vanishing gradient problem but the exact opposite. Sometimes,
    during backpropagation, the gradients explode to extraordinarily large values.
    As with the vanishing gradient problem, the problem of exploding gradients occurs
    when network architectures get deeper.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的另一个缺点是梯度爆炸问题。这与消失梯度问题类似，但恰恰相反。有时，在反向传播过程中，梯度会爆炸到非常大的值。与消失梯度问题一样，当网络架构变得更深时，梯度爆炸问题就会发生。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The name exploding gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸问题这一名称源于这样一个事实：在反向传播步骤中，一些梯度会消失或变为零。从技术上讲，这意味着在网络的反向传播过程中没有误差项被反向传播。当网络变得更深更复杂时，这就成为了一个问题。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何解决...
- en: 'This section will describe the exploding gradient problem in recurrent neural
    networks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述递归神经网络中的梯度爆炸问题：
- en: The exploding gradient problem is very similar to the vanishing gradient problem,
    but just the opposite.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度爆炸问题与消失梯度问题非常相似，但正好相反。
- en: When long-term dependencies arise in recurrent neural networks, the error term
    is propagated backward through the network sometimes explodes or becomes very
    large.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当递归神经网络中出现长期依赖时，误差项在网络中反向传播时有时会爆炸或变得非常大。
- en: This error term multiplied by the learning rate results in an extremely large ![](img/ec6e0c62-a9ca-4280-87e8-d789ac947387.png)w.
    This gives rise to new weights that look very different from the previous weights.
    It is called the exploding gradient problem because the value of the gradient
    becomes too large.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个误差项乘以学习率，会导致一个极大的![](img/ec6e0c62-a9ca-4280-87e8-d789ac947387.png)w。这会产生与之前权重差异很大的新权重。之所以叫做梯度爆炸问题，是因为梯度的值变得过大。
- en: 'The problem of exploding gradients is illustrated in an algorithmic fashion,
    in the following diagram:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度爆炸问题通过以下算法图示进行说明：
- en: '![](img/f498abc3-6230-497a-aa81-2d8083fb592b.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f498abc3-6230-497a-aa81-2d8083fb592b.png)'
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何运作...
- en: 'Since neural networks use a gradient-based optimization technique to learn
    features present in data, it is essential that these gradients are preserved in
    order for the network to calculate an error based on the change in gradients.
    This section will describe how the exploding gradient problem occurs in recurrent
    neural networks:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络使用基于梯度的优化技术来学习数据中存在的特征，因此，保持这些梯度非常重要，以便网络能够根据梯度的变化计算误差。本节将描述递归神经网络中梯度爆炸问题的发生：
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用反向传播时，网络首先计算误差，这只是模型输出与实际输出的差的平方（如平方误差）。
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用此误差，模型接着计算相对于权重变化的误差变化（de/dw）。
- en: The computed derivative multiplied by the learning rate ![](img/f2644c6b-a2bf-4afe-946a-f3449fcf08a7.png) gives ![](img/0a500dfb-7dc4-4cb8-b8b4-fa4244c67565.png)w,
    which is nothing but the change in weights. The term ![](img/f5e5c7a4-64c5-49ff-b331-1110e011cfbb.png)w
    is added to the original weights to update them to the new weights.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算得到的导数乘以学习率 ![](img/f2644c6b-a2bf-4afe-946a-f3449fcf08a7.png) 给出 ![](img/0a500dfb-7dc4-4cb8-b8b4-fa4244c67565.png)w，这只是权重的变化。项
    ![](img/f5e5c7a4-64c5-49ff-b331-1110e011cfbb.png)w 被加到原始权重中，用于更新为新的权重。
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is greater than 1, then that term multiplied by the learning rate ![](img/31f9faa4-34d0-441a-9b0d-8f7fde4012d7.png) gives
    a very, very large number that is of no use to the network while trying to optimize
    weights further, since the weights are no longer in the same range.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设 de/dw（误差相对于权重的梯度或变化率）的值大于1，那么该项乘以学习率 ![](img/31f9faa4-34d0-441a-9b0d-8f7fde4012d7.png)
    会得到一个非常非常大的数，这对于网络进一步优化权重没有任何用处，因为权重已不再处于相同的范围内。
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为反向传播过程中，权重更新仅对最近的时间步是准确的，而在反向传播经过之前的时间步时，准确性会降低，并且当权重更新穿过许多时间步时，几乎变得微不足道。
- en: The number of previous times steps the network needs to backpropagate through
    increases with the increase in the number of sequences in the input data. In such
    cases, the recurrent networks become incapable of remembering information from
    many time steps in the past and therefore are unable to make accurate predictions
    of future time steps.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着输入数据中序列数量的增加，网络需要反向传播的时间步长数量也随之增加。在这种情况下，循环神经网络无法记住来自过去多个时间步的信息，因此无法准确预测未来时间步。
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term increases beyond 1 and changes in weight (![](img/8e89f7be-6b0a-45b3-812d-7a0bc4274146.png)w)
    explode. As a result, the new or updated weight is completely out of range when
    compared to the previous weight.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当出现这种情况时，网络需要进行更多复杂的计算，导致迭代次数大幅增加，并且在此过程中误差项的变化超过1，权重的变化（![](img/8e89f7be-6b0a-45b3-812d-7a0bc4274146.png)w）爆炸。结果，新权重与之前的权重相比完全超出了范围。
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights within a specified range, which is a problem as this
    will cause the model to overfit the data.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于没有权重更新发生，网络停止学习或无法在指定范围内更新其权重，这是一个问题，因为这将导致模型对数据过拟合。
- en: There's more...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'The following are some of the ways in which the problem of exploding gradients
    can be solved:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 解决爆炸梯度问题的一些方法如下：
- en: Certain gradient clipping techniques can be applied to solve this issue of exploding
    gradients.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以应用某些梯度裁剪技术来解决爆炸梯度问题。
- en: Another way to prevent this is by using truncated Backpropagation Through Time,
    where instead of starting the backpropagation at the last time step (or output
    layer), we can choose a smaller time step (say, 15) to start backpropagating.
    This means that the network will backpropagate through only the last 15 time steps
    at one instance and learn information related to those 15-time steps only. This
    is similar to feeding in mini batches of data to the network as it would become
    far too computationally expensive to compute the gradient over every single element
    of the dataset in the case of very large datasets.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种防止此问题的方法是使用截断的时间反向传播（Backpropagation Through Time），在这种方法中，我们可以选择一个较小的时间步长（例如，15）来开始反向传播，而不是从最后一个时间步（或输出层）开始。这意味着网络将仅在一个实例中通过最后15个时间步进行反向传播，并且只学习与这15个时间步相关的信息。这类似于将数据的小批量输入网络，因为在数据集非常大的情况下，计算每个数据集元素的梯度会变得非常耗费计算资源。
- en: The final option to prevent the explosion of gradients is by monitoring them
    and adjusting the learning rate accordingly.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止梯度爆炸的最终方法是通过监控它们并相应地调整学习率。
- en: See also
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'A more detailed explanation of the vanishing and exploding gradient problems
    can be found at the following links:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 有关消失梯度和爆炸梯度问题的更详细解释，请参见以下链接：
- en: '[http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)'
- en: '[https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)'
- en: '[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)'
- en: Sequential working of LSTMs
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 的顺序工作方式
- en: '**Long Short-Term Memory Unit** (**LSTM**) cells are nothing but slightly more
    advanced architectures compared to Recurrent Networks. LSTMs can be thought of
    as a special kind of Recurrent Neural Networks with the capabilities of learning
    long-term dependencies that exist in sequential data. The main reason behind this
    is the fact that LSTMs contain memory and are able to store and update information
    within their cells unlike Recurrent Neural Networks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆单元**（**LSTM**）只是比递归网络稍微先进一些的架构。LSTM 可以被看作是具有学习顺序数据中长期依赖关系能力的特殊类型的递归神经网络。其背后的主要原因是
    LSTM 包含内存，并能够在其单元内存储和更新信息，而递归神经网络则不能。'
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The main components of a Long Short-Term Memory unit are as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆单元的主要组件如下：
- en: The input gate
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门
- en: The forget gate
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门
- en: The update gate
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新门
- en: Each of these gates is made up of a sigmoid layer followed by a pointwise multiplication
    operation. The sigmoid layer outputs numbers between zero and one. These values
    describe  how much information of each component is allowed to pass through the
    respective gate. A value of zero means the gate will allow nothing to pass through
    it, while a value of one means the gate allows all the information to pass through.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门中的每一个由一个 Sigmoid 层和随后的逐点乘法操作组成。Sigmoid 层输出的数字在零和一之间。这些值描述了每个组件允许通过各自门的多少信息。值为零表示门不会允许任何信息通过，而值为一则表示门允许所有信息通过。
- en: The best way to understand LSTM cells is through computational graphs, just
    like in the case of recurrent neural networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 LSTM 单元的最佳方式是通过计算图，就像在递归神经网络（RNN）的情况下那样。
- en: 'LSTMs were originally developed by Sepp Hochreiter and Jurgen Schmidhuber in
    1997\. The following is, link to their published paper:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 最初是由 Sepp Hochreiter 和 Jurgen Schmidhuber 在 1997 年开发的。以下是他们发表的论文链接：
- en: '[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
- en: How to do it...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'This section will describe the inner components of a single LSTM cell, primarily,
    the three different gates present inside the cell. A number of such cells stacked
    together form an LSTM network:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述单个 LSTM 单元的内部组件，主要是单元内存在的三种不同门。多个这样的单元堆叠在一起形成一个 LSTM 网络：
- en: LSTMs also have a chain-like structure like RNNs. Standard RNNs are basically
    modules of repeating units like a simple function (for example, tanh).
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 也具有类似 RNN 的链式结构。标准的 RNN 基本上是由重复单元模块组成，类似于一个简单的函数（例如 tanh）。
- en: LSTMs have the capability to retain information for long periods of time as
    compared to RNNs owing to the presence of memory in each unit. This allows them
    to learn important information during the early stages in a sequence of inputs
    and also gives it the ability to have a significant impact on the decisions made
    by the model at the end of each time step.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于每个单元内存在内存，LSTM 相比 RNN 有能力在更长时间内保持信息。这使得它们能够在输入序列的早期阶段学习重要信息，并且使其在每个时间步结束时对模型做出的决策产生重要影响。
- en: By being able to store information right from the early stages of an input sequence, LSTMs
    are actively able to preserve the error that can be backpropagated through time
    and layers instead of letting that error vanish or explode.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过能够从输入序列的早期阶段开始存储信息，LSTM 可以主动保留可以通过时间和层反向传播的误差，而不是让这些误差消失或爆炸。
- en: LSTMs are capable of learning information over many time steps and thus have
    denser layer architectures by preserving the error which is backpropagated through
    those layers.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 能够在多个时间步长上学习信息，因此通过保留在这些层中反向传播的误差，它们具有更密集的层架构。
- en: The cell structures called **"gates"** give the LSTM the ability to retain information,
    add information or remove information from the **cell state**.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 被称为 **"门"** 的单元结构赋予LSTM保持信息、添加信息或从 **单元状态** 中删除信息的能力。
- en: 'The following diagram illustrates the structure of an LSTM. The key feature
    while trying to  understand LSTMs is in understanding the LSTM network architecture
    and cell state, which can be visualized here:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下图展示了LSTM的结构。在理解LSTM时，关键在于理解LSTM网络架构和单元状态，这可以通过以下方式进行可视化：
- en: '![](img/2dfbc2de-288c-472f-ab6d-5f045fae0c79.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dfbc2de-288c-472f-ab6d-5f045fae0c79.png)'
- en: In the preceding diagram, *x[t]* and *h[t-1]* are the two inputs to the cell.
    *x*[*t* ]is the input from the current time step, while h[t-1 ]is the input from
    the previous time step (which is the output of the preceding cell during the previous
    time step). Besides these two inputs, we also have *h*[, ]which is the current
    output (i.e., time step t) from the LSTM cell after performing its operations
    on the two inputs through its gates.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中，*x[t]* 和 *h[t-1]* 是输入到单元的两个输入。*x*[*t*] 是当前时间步的输入，而 h[t-1] 是前一个时间步的输入（即前一个时间步中前一单元的输出）。除了这两个输入，我们还有
    *h*[, ]，它是LSTM单元在通过其门操作这两个输入后，生成的当前输出（即时间步 t）。
- en: In the preceding diagram, r[t ]represents the output emerging from the input
    gate, which takes in inputs *h*[*t-1* ]and *x[t]*, performs multiplication of
    these inputs with its weight matrix *W[z]*, and passes them through a sigmoid
    activation function.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中，r[t] 代表从输入门中产生的输出，该门接收输入 *h*[*t-1*] 和 *x[t]*，将这些输入与其权重矩阵 *W[z]* 相乘，并通过sigmoid激活函数处理它们。
- en: Similarly, the term *z[t]* represents the output emerging from the forget gate.
    This gate has a set of weight matrices (represented by *W[r]*) which are specific
    to this particular gate and govern how the gate functions.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，术语 *z[t]* 代表了从遗忘门中产生的输出。这个门有一组权重矩阵（用 *W[r]* 表示），这些权重矩阵是特定于这个门的，并且控制着门的功能。
- en: Finally, there is ![](img/21952021-5706-4316-b91e-71e2c99fd477.png)[t], which
    is the output emerging from the update gate. In this case, there are two parts.
    The first part is a sigmoid layer which is also called the **input gate layer**,
    and its primary function is deciding which values to update. The next layer is
    a tanh layer . The primary function of this layer is to create a vector or array
    containing new values that  could be added to the cell state.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，有 ![](img/21952021-5706-4316-b91e-71e2c99fd477.png)[t]，它是从更新门中产生的输出。在这种情况下，输出有两个部分。第一部分是一个sigmoid层，也叫做
    **输入门层**，其主要功能是决定哪些值需要更新。下一层是一个tanh层，该层的主要功能是创建一个包含可以添加到单元状态中的新值的向量或数组。
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'A combination of a number of LSTM cells/units forms an LSTM network. The architecture
    of such a network is shown in the following diagram:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由多个LSTM单元/单元组合而成的LSTM网络。这样的网络架构如以下图所示：
- en: '![](img/52895ac7-e1df-4131-a886-f2a72a0c0583.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52895ac7-e1df-4131-a886-f2a72a0c0583.png)'
- en: In the preceding diagram, the full LSTM cell is represented by ***"A"***. The
    cell takes the current input (*x**[i]*) of a sequence of inputs, and produces
    (*h**[i]*) which is nothing but the output of the current hidden state. This output
    is then sent to the next LSTM cell as its input.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的图中，完整的LSTM单元由 ***"A"*** 表示。单元接收一个序列输入中的当前输入 (*x**[i]*)，并生成 (*h**[i]*)，这就是当前隐藏状态的输出。该输出随后作为输入发送到下一个LSTM单元。
- en: An LSTM cell is slightly more complicated than an RNN cell. While the RNN cell
    has just one function/layer acting on a current input, the LSTM cell has three
    layers which are the three gates controlling the information flowing through the
    cell at any given instance in time.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM单元比RNN单元稍微复杂一些。RNN单元只有一个对当前输入进行处理的功能/层，而LSTM单元有三层，这三层是控制信息在任一时间点通过单元流动的三个门。
- en: The cell behaves a lot like the hard disk memory in a computer. The cell, therefore,
    has the capability to allow the writing, reading and storing of information within
    its cell state. The cell also makes decisions about what information to store,
    and when to allow reading, writing, and erasing information. This is facilitated
    by the gates that open or close accordingly.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个单元的行为很像计算机中的硬盘内存。因此，单元具有允许在其单元状态中写入、读取和存储信息的能力。单元还决定存储哪些信息，以及何时允许读取、写入和删除信息。这是通过相应开关的门来实现的。
- en: The gates present in LSTM cells are analog in contrast to the digital storage
    systems in today's computers. This means that the gates can only be controlled
    through an element-wise multiplication through sigmoids, yielding probability
    values between 0 and 1\. A high value will cause the gate to remain open while
    a low value will cause the gate to remain shut.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM单元中的门是模拟的，这与今天计算机中的数字存储系统不同。这意味着这些门只能通过sigmoid进行逐元素相乘来控制，从而产生介于0和1之间的概率值。高值会导致门保持开启，而低值则会导致门保持关闭。
- en: Analog systems have an edge over digital systems when it comes to neural network
    operations since they are differentiable. This makes analog systems more suitable
    for tasks like backpropagation which primarily rely on the gradients.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经网络操作方面，模拟系统相比数字系统具有优势，因为它们是可微的。这使得模拟系统更适合像反向传播这样主要依赖梯度的任务。
- en: The gates pass on information or block information or let only a part of the
    information flow through them based on its strength and importance. The information
    is filtered at every time step through the sets of weight matrices specific to
    each gate. Therefore, each gate has complete control over how to act on the information
    it receives.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 各个门基于信息的强度和重要性来传递、阻止或仅让部分信息流过它们。信息在每个时间步通过每个门特定的权重矩阵集进行过滤。因此，每个门完全控制如何处理它接收到的信息。
- en: The weight matrices associated with each gate, like the weights that modulate
    input and hidden states, are adjusted based on the recurrent network's learning
    process and gradient descent.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与每个门相关的权重矩阵，像调节输入和隐藏状态的权重一样，都会根据递归网络的学习过程和梯度下降进行调整。
- en: The first gate is called the **forget gate** and it controls what information
    is maintained from the previous state. This gate takes the previous cell output
    (*h**[t]** - 1*) as its input along with the current input (*x**[t]*), and applies
    a sigmoid activation (![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)) in order
    to produce and output value between 0 and 1 for each hidden unit. This is followed
    by the element-wise multiplication with the current state (illustrated by the
    first operation in the preceding diagram).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个门叫做**遗忘门**，它控制从前一个状态中保留哪些信息。这个门将前一个单元的输出(*h**[t]** - 1*)与当前输入(*x**[t]*)一起作为输入，应用sigmoid激活函数（![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)），以便为每个隐藏单元产生一个介于0和1之间的输出值。然后，它会与当前状态进行逐元素相乘（如前图中的第一个操作所示）。
- en: The second gate is called the **update gate** and its primary function is to
    update the cell state based on the current input. This gate passes the same input
    as the forget gate's inputs (*h**[t-1]* and *x**[t]*) into a sigmoid activation
    layer (![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)) followed by a tanh activation
    layer and then performs an element-wise multiplication between these two results.
    Next, element-wise addition is performed with the result and the current state
    (illustrated by the second operation in the preceding diagram).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个门叫做**更新门**，它的主要功能是根据当前输入更新单元状态。这个门将与遗忘门相同的输入（*h**[t-1]* 和 *x**[t]*）传递到sigmoid激活层（![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)），然后通过tanh激活层，接着执行这两个结果的逐元素相乘。接下来，进行与当前状态的逐元素加法（如前图中的第二个操作所示）。
- en: Finally, there is an output gate which controls what information and how much
    information gets transferred to the adjoining cell to act as its inputs during
    the next time step. The current cell state is passed through a tanh activation
    layer and multiplied element-wise with the cell input (*h**[t-1]* and *x**[t]*)
    after being passed through a sigmoid layer (![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png))
    for this operation.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，有一个输出门，它控制哪些信息以及多少信息被传递到相邻的单元，以作为其在下一时间步的输入。当前单元的状态通过tanh激活层，然后与单元输入(*h**[t-1]*
    和 *x**[t]*)进行逐元素相乘，在经过sigmoid层（![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)）后进行这个操作。
- en: The update gate behaves as the filter on what the cell decides to output to
    the next cell. This output, h[t], is then passed on to the next LSTM cell as its
    input, and also to the above layers if many LSTM cells are stacked on top of each
    other.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新门作为一个过滤器，决定单元格向下一个单元输出什么信息。这个输出，h[t]，随后被传递到下一个LSTM单元作为其输入，如果多个LSTM单元堆叠在一起，也会传递给上层。
- en: There's more...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: LSTMs were a big leap forward when compared to what could be accomplished with
    feedforward networks and Recurrent Neural Networks. One might wonder what the
    next big step in the near future is, or even what that step might be. A lot researchers
    do believe "attention" is the next big step when it comes to the field of artificial
    intelligence. With the amount of data growing vastly with each day it becomes
    impossible to process every single bit of that data. This is where attention could
    be a potential game-changer, causing the networks to give their attention only
    to data or areas which are of high priority or interest and disregard useless
    information. For example, if an RNN is being used to create an image captioning
    engine, it will only pick a part of the image to to give its attention to for
    every word it outputs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈神经网络和循环神经网络（RNN）相比，LSTM在处理上取得了重大突破。人们或许会想，未来的下一步大进展是什么，或者说是什么进步呢？许多研究人员相信，“注意力机制”将是人工智能领域的下一个大突破。随着数据量的日益增长，处理每一
    bit 数据已变得不可能。此时，注意力机制可能成为一个潜在的游戏规则改变者，它使得网络仅关注高优先级或感兴趣的数据或区域，并忽略无用的信息。例如，如果RNN被用来创建图像描述引擎，它会根据每个输出的词语，选择图像的一部分进行注意力聚焦。
- en: 'The recent (2015) paper by Xu, et al. does exactly this. They explore adding
    attention to LSTM cells. Reading this paper can be a good place to start learning
    about the use of attention in neural networks. There have been some good results
    with using attention for various tasks, and more research is currently being conducted
    on the subject. The paper by Xu, et al. can be found using the following link:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 徐等人最近（2015年）发表的论文正是探讨了这个问题。他们研究了在LSTM单元中加入注意力机制。阅读这篇论文是学习神经网络中注意力机制应用的一个良好起点。使用注意力机制在各种任务中取得了一些不错的结果，且目前在这一领域仍有许多研究正在进行中。徐等人论文的链接如下：
- en: '[https://arxiv.org/pdf/1502.03044v2.pdf](https://arxiv.org/pdf/1502.03044v2.pdf)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1502.03044v2.pdf](https://arxiv.org/pdf/1502.03044v2.pdf)'
- en: Attention isn't the only variant to LSTMs. Some of the other active research
    is based on the utilization of grid LSTMs, as used in the paper by Kalchbrenner,
    et al., for which the link is at: [https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制并不是LSTM的唯一变体。其他一些活跃的研究是基于使用网格LSTM的技术，正如Kalchbrenner等人在论文中使用的那样，相关论文链接为：[https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf)。
- en: See also
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Some other useful information and papers related to RNNs and LSTMs in generative
    networks can be found by visiting the following links:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些与RNN和LSTM在生成网络中的应用相关的有用信息和论文，可以通过访问以下链接找到：
- en: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
- en: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
- en: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
- en: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
