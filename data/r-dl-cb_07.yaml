- en: Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The current chapter will introduce Reinforcement Learning. We will cover the
    following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍强化学习。我们将涵盖以下主题：
- en: Setting up a Markov Decision Process
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置马尔可夫决策过程
- en: Performing model-based learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行基于模型的学习
- en: Performing model-free learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行无模型学习
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: '**Reinforcement Learning** (**RL**) is an area in machine learning that is
    inspired by psychology, such as how agents (software programs) can take actions
    in order to maximize cumulative rewards.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是机器学习的一个领域，灵感来自心理学，例如智能体（软件程序）如何采取行动以最大化累积奖励。'
- en: 'The RL is reward-based learning where the reward comes at the end or is distributed
    during the learning. For example, in chess, the reward will be assigned to winning
    or losing the game whereas in games such as tennis, every point won is a reward.
    Some of the commercial examples of RL are DeepMind from Google uses RL to master
    parkour. Similarly, Tesla is developing AI-driven technology using RL. An example
    of reinforcement architecture is shown in the following figure:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是基于奖励的学习方式，其中奖励要么在学习结束时出现，要么在学习过程中分配。例如，在国际象棋中，奖励是与胜负相关的，而在像网球这样的游戏中，每赢得一分就是奖励。一些强化学习的商业实例包括谷歌的DeepMind，它利用强化学习掌握跑酷技术。类似地，特斯拉也在使用强化学习开发人工智能驱动的技术。以下图示为强化架构的一个例子：
- en: '![](img/00111.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: Interaction of an agent with environment in Reinforcement Learning
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中智能体与环境的交互
- en: 'The basic notations for RL are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的基本符号如下：
- en: '**T(s, a, s'')**: Represents the transition model for reaching state *s''*
    when action *a* is taken at state *s*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**T(s, a, s'')**：表示当在状态 *s* 执行动作 *a* 时，达到状态 *s''* 的转移模型'
- en: '![](img/00120.gif): Represents a policy which defines what action to take at
    every possible state ![](img/00114.gif)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/00120.gif)：表示一个策略，定义了在每个可能状态下应该采取的行动！[](img/00114.gif)'
- en: '**R(s)**: Denotes the reward received by agent at state *s*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R(s)**：表示智能体在状态 *s* 时获得的奖励'
- en: The current chapter will look at how to set up reinforcement models using R.
    The next sub-section will introduce `MDPtoolbox` from R.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨如何使用 R 设置强化模型。下一小节将介绍来自 R 的 `MDPtoolbox`。
- en: Setting up a Markov Decision Process
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置马尔可夫决策过程
- en: The **Markov Decision Process** (**MDP**) forms the basis of setting up RL,
    where the outcome of a decision is semi-controlled; that is, it is partly random
    and partly controlled (by the decision-maker). An MDP is defined using a set of
    possible states (**S**), a set of possible actions (**A**), a real-values reward
    function (**R**), and a set of transition probabilities from one state to another
    state for a given action (**T**). In addition, the effects of an action performed
    on one state depends only on that state and not on its previous states.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）是设置强化学习的基础，其中决策的结果是半控制的；即，它部分是随机的，部分是由决策者控制的。一个 MDP 通过一组可能的状态（**S**）、一组可能的动作（**A**）、一个实值奖励函数（**R**）以及给定动作的状态转移概率（**T**）来定义。此外，一个动作对某个状态的影响仅依赖于该状态本身，而不依赖于其之前的状态。'
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this section, let us define an agent travelling across a 4 x 4 grid, as
    shown in following figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义一个智能体在 4 x 4 网格上移动，如下图所示：
- en: '![](img/00025.gif)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.gif)'
- en: A sample 4 x 4 grid of 16 states
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 4 x 4 的 16 状态网格示例
- en: 'This grid has 16 states (*S1*, *S2*....*S16*). In each state, the agent can
    perform four actions (*up*, *right*, *down*, *left*). However, the agent will
    be restricted to some actions based on the following constraints:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该网格有 16 个状态（*S1*、*S2*……*S16*）。在每个状态下，智能体可以执行四个动作（*上*、*右*、*下*、*左*）。然而，智能体将根据以下约束限制一些动作：
- en: The states across the edges shall be restricted to actions which point only
    toward states in the grid. For example, an agent in *S1* is restricted to the
    *right* or *down* action.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘上的状态将限制为只指向网格中的状态的动作。例如，智能体在 *S1* 时仅限于向 *右* 或 *下* 的动作。
- en: Some state transitions have barriers, marked in red. For example, the agent
    cannot go *down* from *S2* to *S3*.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些状态转移有障碍，标记为红色。例如，智能体不能从 *S2* 向下转移到 *S3*。
- en: Each state is also assigned to a reward. The objective of the agent is to reach
    the destination with minimum moves, thereby achieving the maximum reward. Except
    state *S15* with a reward value of 100, all the remaining states have a reward
    value of *-1*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态也被分配了一个奖励。智能体的目标是以最少的步骤到达目标，从而获得最大奖励。除状态 *S15* 的奖励值为 100 外，其他所有状态的奖励值均为
    *-1*。
- en: Here, we will use the `MDPtoolbox` package in R.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将使用 R 中的`MDPtoolbox`包。
- en: How to do it...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'This section will show you how to set up RL models using `MDPtoolbox` in R:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将展示如何在 R 中使用`MDPtoolbox`设置强化学习（RL）模型：
- en: 'Install and load the required package:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并加载所需的包：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the transition probabilities for action. Here, each row denotes `from
    state` and each column denotes `to state`. As we have 16 states, the transition
    probability matrix of each action shall be a 16 x 16 matrix, with each row adding
    upto 1:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义动作的转移概率。这里，每一行表示“从某个状态”，每一列表示“到某个状态”。由于我们有 16 个状态，每个动作的转移概率矩阵将是一个 16 x 16
    的矩阵，每一行的和为 1：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a list of transition probability matrices:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个转移概率矩阵列表：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define a reward matrix of dimensions: 16 (number of states) x 4 (number of
    actions):'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个维度为：16（状态数量）x 4（动作数量）的奖励矩阵：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Test whether the defined `TPMs` and `Rewards` satisfy a well-defined MDP. If
    it returns an empty string, then the MDP is valid:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试定义的 `TPMs` 和 `Rewards` 是否满足一个明确的 MDP。如果返回空字符串，则说明该 MDP 是有效的：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Performing model-based learning
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行基于模型的学习
- en: As the name suggests, the learning is augmented using a predefined model. Here,
    the model is represented in the form of transition probabilities and the key objective
    is to determine the optimal policy and value functions using these predefined
    model attributes (that is, `TPMs`). The policy is defined as a learning mechanism
    of an agent, traversing across multiple states. In other words, identifying the
    best action of an agent in a given state, to traverse to a next state, is termed
    a policy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，学习是通过使用预定义的模型来增强的。这里，模型以转移概率的形式表示，关键目标是使用这些预定义的模型属性（即 `TPMs`）来确定最优策略和价值函数。策略被定义为一个智能体的学习机制，跨多个状态进行遍历。换句话说，确定智能体在给定状态下采取的最佳动作，以便转移到下一个状态，称为策略。
- en: The objective of the policy is to maximize the cumulative reward of transitioning
    from the start state to the destination state, defined as follows, where *P(s)*
    is the cumulative policy *P* from a start state *s*, and *R* is the reward of
    transitioning from state *st* to state *s[t+1]* by performing an action at.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 策略的目标是最大化从起始状态到目标状态的累积奖励，定义如下，其中 *P(s)* 是从起始状态 *s* 开始的累积策略 *P*，*R* 是通过执行动作 at
    从状态 *st* 转移到状态 *s[t+1]* 的奖励。
- en: '![](img/00028.gif)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.gif)'
- en: 'The value function is of two types: the state-value function and the state-action
    value function. In the state-value function, for a given policy, it is defined
    as an expected reward to be in a particular state (including start state), whereas
    in the state-action value function, for a given policy, it is defined as an expected
    reward to be in a particular state (including the start state) and undertake a
    particular action.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数有两种类型：状态值函数和状态-动作值函数。在状态值函数中，对于给定的策略，它被定义为处于某一特定状态（包括起始状态）的期望奖励；而在状态-动作值函数中，对于给定的策略，它被定义为处于某一特定状态（包括起始状态）并执行某一特定动作的期望奖励。
- en: Now, a policy is said to be optimal provided it returns the maximum expected
    cumulative reward, and its corresponding states are termed optimal state-value
    functions or its corresponding states and actions are termed optimal state-action
    value functions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，若一个策略被称为最优策略，意味着它返回最大的期望累积奖励，并且其对应的状态被称为最优状态值函数，或者其对应的状态和动作被称为最优状态-动作值函数。
- en: 'In model-based learning, the following iterative steps are performed in order
    to obtain an optimum policy, as shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于模型的学习中，为了获得最优策略，执行以下迭代步骤，如下图所示：
- en: '![](img/00044.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: Iterative steps to find an optimum policy
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代步骤以找到最优策略
- en: In this section, we shall evaluate the policy using the state-value function.
    In each iteration, the policies are dynamically evaluated using the Bellman equation,
    as follows, where *V[i]* denotes the value at iteration *i*, *P* denotes an arbitrary
    policy of a given state *s* and action *a*, *T* denotes the transition probability
    from state *s* to state *s'* due to an action *a*, *R* denotes the reward at state
    *s'* while traversing from the state *s* post an action *a*, and ![](img/00057.gif)
    denotes a discount factor in the range of (0,1). The discount factor ensures higher
    importance to starting learning steps than later.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用状态值函数评估策略。在每次迭代中，使用贝尔曼方程动态评估策略，如下所示，其中 *V[i]* 表示第 *i* 次迭代时的值，*P* 表示给定状态
    *s* 和动作 *a* 的任意策略，*T* 表示由于动作 *a* 从状态 *s* 转移到状态 *s'* 的转移概率，*R* 表示在从状态 *s* 执行动作
    *a* 后到达状态 *s'* 时的奖励，![](img/00057.gif) 表示折扣因子，取值范围为（0,1）。折扣因子确保学习初期的步骤比后续步骤更为重要。
- en: '![](img/00031.gif)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.gif)'
- en: How to do it...
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'This section shows you how to set up model-based RL:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向你展示如何设置基于模型的强化学习（RL）：
- en: 'Run the policy iteration using the state-action value function with the discount
    factor *Υ = 0.9*:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用状态-动作值函数进行策略迭代，折扣因子 *Υ = 0.9*：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Get the best (optimum) policy P* as shown in the following figure. The arrows
    marked in green show the direction of traversing *S1* to *S15*:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最佳（最优）策略 P*，如下图所示。绿色箭头标记显示了从 *S1* 到 *S15* 的遍历方向：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/00034.gif)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.gif)'
- en: Optimum policy using model-based iteration with an optimum path from *S1* to
    *S15*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于模型的迭代获得最优策略，并通过最优路径从 *S1* 到 *S15*
- en: 'Get the optimum value function V* for each state and plot them as shown in
    the following figure:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每个状态的最优价值函数 V* 并如图所示绘制：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00150.gif)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00150.gif)'
- en: Value functions of the optimal policy
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略的价值函数
- en: Performing model-free learning
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行无模型学习
- en: Unlike model-based learning, where dynamics of transitions are explicitly provided
    (as transition probabilities from one state to another state), in model-free learning,
    the transitions are supposed to be deduced and learned directly from the interaction
    between states (using actions) rather explicitly provided. Widely used frameworks
    of mode-free learning are **Monte Carlo** methods and the **Q-learning** technique.
    The former is simple to implement but convergence takes time, whereas the latter
    is complex to implement but is efficient in convergence due to off-policy learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于模型的学习不同，基于模型的学习明确提供了转移的动态（例如从一个状态到另一个状态的转移概率），而在无模型学习中，转移应该通过状态之间的互动（使用动作）直接推断和学习，而不是显式提供。常见的无模型学习框架有**蒙特卡洛**方法和**Q-learning**技术。前者实现简单，但收敛较慢，而后者实现复杂，但由于离策略学习，收敛效率较高。
- en: Getting ready
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: In this section, we will implement the Q-learning algorithm in R. The simultaneous
    exploration of the surrounding environment and exploitation of existing knowledge
    is termed off-policy convergence. For example, an agent in a particular state
    first explores all the possible actions of transitioning into next states and
    observes the corresponding rewards, and then exploits current knowledge to update
    the existing state-action value using the action generating the maximum possible
    reward.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现 R 语言中的 Q-learning 算法。对周围环境的同时探索和对现有知识的利用被称为离策略收敛。例如，一个代理在特定状态下首先探索所有可能的动作，以过渡到下一个状态并观察相应的奖励，然后利用当前知识通过选取产生最大可能奖励的动作来更新现有的状态-动作值。
- en: 'The Q learning returns a 2D Q-table of the size of the number of states x the
    number of actions. The values in the Q-table are updated based on the following
    formula, where *Q* denotes the value of state *s* and action *a*, *r''* denotes
    the reward of the next state for a selected action *a*, *Υ* denotes the discount
    factor, and *α* denotes the learning rate:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习返回一个大小为状态数 × 动作数的二维 Q 表。Q 表中的值根据以下公式更新，其中 *Q* 表示状态 *s* 和动作 *a* 的值，*r'* 表示所选动作
    *a* 在下一个状态的奖励，*Υ* 表示折扣因子，*α* 表示学习率：
- en: '![](img/00071.gif)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.gif)'
- en: 'The framework for Q-learning is shown in the following figure:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图所示为 Q-learning 框架：
- en: '![](img/00078.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: Framework of Q-learning
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 框架
- en: How to do it...
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The section provide steps for how to set up Q-learning:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了如何设置 Q-learning 的步骤：
- en: 'Define 16 states:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 16 个状态：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define four actions:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义四个动作：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the `transitionStateAction` function, which can simulate the transitions
    from one state *s* to another state *s''* using an action *a*. The function takes
    in the current state *s* and selected action *a*, and it returns the next state
    *s''* and corresponding reward *r''*. In case of constrained action, the next
    state returned is the current state *s* and the existing reward *r*:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`transitionStateAction`函数，该函数可以通过动作*a*模拟从一个状态*s*到另一个状态*s'*的转移。该函数输入当前状态*s*和选定的动作*a*，返回下一个状态*s'*和相应的奖励*r'*。在存在约束动作的情况下，返回的下一个状态为当前状态*s*，并返回现有奖励*r*：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define a function to perform Q-learning using `n` iterations:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，通过`n`次迭代执行Q学习：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Set learning parameters such as `epsilon` and `learning_rate`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习参数，如`epsilon`和`learning_rate`：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Get the Q-table after 500k iterations:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取经过50万次迭代后的Q表：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Get the best (optimum) policy P*, as shown in the following figure. The arrows
    marked in green shows the direction of traversing *S1* to *S15:*
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最佳（最优）策略P*，如以下图所示。绿色标记的箭头显示了从*S1*到*S15*的遍历方向：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/00082.gif)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.gif)'
- en: Optimum policy using model-free iteration with an optimum path from *S1* to
    *S15*
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无模型迭代得到的最优策略，并展示从*S1*到*S15*的最优路径
