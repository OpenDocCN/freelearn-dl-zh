- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Improving the Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型
- en: 'The goal of modeling in machine learning is to ensure our model generalizes
    well on unseen data. Throughout our journey as data professionals who build models
    with neural networks, we are likely to come across two main issues: underfitting
    and overfitting. **Underfitting** is a scenario in which our model lacks the necessary
    complexity to capture underlying patterns in our data, while **overfitting** occurs
    when our model is too complex such that it not only learns the patterns but also
    picks up noise and outliers in our training data. In this case, our model performs
    exceptionally well on training data but fails to generalize well on unseen data.
    [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with Neural
    Networks*, examined the science behind neural networks. Here, we will explore
    the art of fine-tuning neural networks to build optimally performing models for
    image classification. We will explore various network settings in a hands-on fashion
    to gain an understanding of the impact of each of these settings (hyperparameters)
    on our model’s performance.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中建模的目标是确保我们的模型能在未见过的数据上良好泛化。在我们作为数据专业人员构建神经网络模型的过程中，可能会遇到两个主要问题：欠拟合和过拟合。**欠拟合**是指我们的模型缺乏足够的复杂性，无法捕捉数据中的潜在模式，而**过拟合**则是在模型过于复杂时，模型不仅学习到模式，还会拾取到训练数据中的噪声和异常值。在这种情况下，我们的模型在训练数据上表现非常好，但在未见过的数据上无法良好泛化。[*第5章*](B18118_05.xhtml#_idTextAnchor105)，*使用神经网络进行图像分类*，探讨了神经网络背后的科学原理。在本章中，我们将探索调整神经网络的艺术，以构建在图像分类中表现最优的模型。我们将通过动手实践来探索各种网络设置，了解这些设置（超参数）对模型性能的影响。
- en: Beyond exploring the art of hyperparameter tuning, we will also explore various
    ways of improving our data quality using data normalization, data augmentation,
    and the use of synthetic data to improve the generalization capabilities of our
    model. In the past, there was a lot of emphasis on building complex networks.
    However, in more recent times, there has been an increased interest in enhancing
    the performance of neural networks using data-centric strategies. The use of these
    data-centric strategies does not erode the need for careful model design; rather,
    we can look at them as complementary strategies working in tandem toward a desired
    goal, thus enhancing our ability to build optimal models with good generalization
    capabilities.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了探索超参数调整的艺术，我们还将探索多种改善数据质量的方法，如数据标准化、数据增强和使用合成数据来提高模型的泛化能力。过去，人们非常注重构建复杂的网络。然而，近年来，越来越多的人开始关注使用以数据为中心的策略来提升神经网络的表现。使用这些以数据为中心的策略并不会削弱精心设计模型的必要性；相反，我们可以将它们看作是互为补充的策略，协同工作以达成目标，从而增强我们构建具有良好泛化能力的最优模型的能力。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Data is key
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据至关重要
- en: Fine-tuning hyperparameters of a neural network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整神经网络的超参数
- en: By the end of this chapter, you will be equipped to effectively navigate the
    challenges posed by overfitting and underfitting using a combination of model-centric
    and data-centric ideas when building models with neural networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够有效应对过拟合和欠拟合所带来的挑战，通过结合以模型为中心和以数据为中心的思路，构建神经网络模型。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using a `python >= 3.8.0`, along with the following packages that
    can be installed using the `pip` `install` command:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`python >= 3.8.0`，并安装以下可以通过`pip` `install`命令安装的包：
- en: '`tensorflow>=2.7.0`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow>=2.7.0`'
- en: '`tensorflow-datasets==4.4.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow-datasets==4.4.0`'
- en: '`pandas==1.3.4`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas==1.3.4`'
- en: '`numpy==1.21.4`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy==1.21.4`'
- en: Data is key
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据至关重要
- en: When it comes to improving the performance of a neural network, or any other
    machine learning model for that matter, the importance of good data preparation
    cannot be overemphasized. In [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065),
    *Linear Regression with TensorFlow*, we saw the impact that normalizing our data
    had on the model’s performance. Beyond data normalization, there are other data
    preparation techniques that can make a difference in our modeling process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升神经网络或任何其他机器学习模型的性能时，良好的数据准备工作至关重要，不能过分强调。在[*第3章*](B18118_03.xhtml#_idTextAnchor065)，*使用TensorFlow进行线性回归*中，我们看到了标准化数据对模型表现的影响。除了数据标准化之外，还有其他数据准备技巧可以在建模过程中产生影响。
- en: As you must have recognized by now, machine learning requires investigating,
    experimenting, and applying different techniques, depending on the problem at
    hand. To ensure we have an optimally performing model, our journey should start
    by looking at our data thoroughly. Do we have enough representative samples from
    each of the target classes? Is our data balanced? Have we ensured the absence
    of incorrect labels? Do we have the right type of data? How are we dealing with
    missing data? These are some of the questions we have to ask and handle before
    the modeling phase.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你现在应该已经意识到的，机器学习需要根据手头的问题进行调查、实验和应用不同的技术。为了确保我们拥有最佳性能的模型，我们的旅程应从彻底审视数据开始。我们是否拥有每个目标类别的足够代表性样本？我们的数据是否平衡？我们是否确保了标签的正确性？我们的数据类型是否正确？我们如何处理缺失数据？这些问题是我们在建模阶段之前必须提问并处理的。
- en: Improving the quality of our data is a multi-faceted endeavor involving different
    techniques, such as engineering new features from existing ones in our data by
    applying some data preprocessing techniques such as data normalization. When we
    are working with imbalanced datasets, where we are short of representative samples
    of the minority class, the logical thing would be to gather more data on the minority
    class; however, this is not practical in all instances. In such cases, synthetic
    data may be an effective alternative. Start-ups such as **Anyverse.ai** and **Datagen.tech**
    focus on synthetic data development, thus making it possible to mitigate issues
    around data imbalance and data scarcity. However, synthetic data may be costly,
    and it is important that we do a cost-benefit analysis before embarking on this
    route.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提高我们数据的质量是一项多方面的工作，涉及通过应用数据预处理技术（如数据归一化）从现有数据中工程化新特征。当我们处理不平衡数据集时，尤其是当我们缺乏少数类的代表性样本时，合理的做法是收集更多少数类数据；然而，这在所有情况下并不实际。在这种情况下，合成数据可能是一个有效的替代方案。一些初创公司，如**Anyverse.ai**和**Datagen.tech**，专注于合成数据的开发，从而可以缓解数据不平衡和数据稀缺的问题。然而，合成数据可能会很昂贵，因此在选择这条路线之前，我们需要做一个成本效益分析。
- en: 'Another problem we could face is when our collected samples are not representative
    enough for our model to function properly. Imagine you train your model to recognize
    human faces. You gather thousands of images of human faces and split your data
    into training and test sets. You train your model, and it predicts perfectly on
    your test set. However, when you ship this model as a product to the open market,
    you get a result such as *Figure 6**.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能面临的另一个问题是，当我们收集的样本不足以代表模型正确运行时。例如，你训练模型识别人脸。你收集了成千上万的人脸图像，并将数据分成训练集和测试集。你训练了模型，并且在测试集上预测得非常完美。然而，当你将这个模型作为产品推向市场时，你得到的结果可能像*图
    6.1*所示：
- en: '![Figure 6.1 – The need for data augmentation](img/B18118_06_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 数据增强的必要性](img/B18118_06_01.jpg)'
- en: Figure 6.1 – The need for data augmentation
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 数据增强的必要性
- en: Surprising, right? Even though you trained the model on thousands of images,
    the model failed to learn to recognize faces if the axis is flipped vertically,
    horizontally, or otherwise. To mitigate this type of problem, we employ a technique
    called data augmentation. Data augmentation is a technique that we use to create
    new training data by altering the existing data in some way, such as randomly
    cropping, zooming in or out, or rotating or flipping the initial image. The underlying
    idea behind data augmentation is to enable our model to recognize an object in
    the image, even under unpredictable conditions such as the ones we saw in *Figure
    6**.1*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶，对吧？即使你在成千上万的图像上训练了模型，如果轴被垂直或水平翻转，或者以其他方式改变，模型也未能学会识别面孔。为了解决这种问题，我们采用了一种叫做数据增强的技术。数据增强是一种通过某种方式改变现有数据来创建新训练数据的技术，例如随机裁剪、缩放或旋转、翻转初始图像。数据增强背后的基本思想是使我们的模型即使在不可预测的条件下（例如我们在*图
    6.1* 中看到的情况），也能识别图像中的物体。
- en: Data augmentation is useful when we want more data samples from a limited training
    set; we can use data augmentation to efficiently increase the size of our dataset,
    hence giving our model more data to learn from. Also, because we can simulate
    various scenarios, our model is less likely to overfit as it learns the underlying
    patterns in the data rather than the noise in the data because our model learns
    about the data in multiple ways. Another important benefit of data augmentation
    is that it is a cost-saving technique that saves us from expensive and sometimes
    time-consuming data collection processes. We will be applying data augmentation
    in [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186)*, Handling Overfitting,* where
    we will be working on real-world image classification problems. Also, should you
    find yourself working on image or text data in the future, you may find data augmentation
    a very handy technique to know.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在我们希望从有限的训练集获得更多数据样本时非常有用；我们可以使用数据增强来有效地增加数据集的大小，从而为我们的模型提供更多的数据进行学习。此外，由于我们可以模拟各种场景，模型在学习数据中的潜在模式时不太可能发生过拟合，而不是学习数据中的噪声，因为我们的模型通过多种方式学习数据。数据增强的另一个重要好处是它是一种节省成本的技术，可以避免昂贵且有时耗时的数据收集过程。在[*第8章*](B18118_08.xhtml#_idTextAnchor186)《处理过拟合》中，我们将应用数据增强技术，在实际的图像分类问题中进行实践。此外，如果将来你需要处理图像或文本数据，数据增强将是一个非常实用的技术。
- en: Beyond addressing issues around data imbalance and data diversity, we may also
    want to refine our model itself to make it suitably complex enough to identify
    patterns in the data, and we want to do this without overfitting the model. Here,
    the aim is to improve the model’s quality by tweaking one or more settings, such
    as increasing the number of hidden layers, adding more neurons to each layer,
    changing the optimizer, or using a more sophisticated activation function. These
    settings can be tuned via experimentation until an optimal model is achieved.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决数据不平衡和数据多样性问题外，我们可能还希望进一步优化我们的模型，使其足够复杂，以便识别数据中的模式，并且我们希望做到这一点而不导致模型过拟合。在这里，目标是通过调整一个或多个设置来提高模型的质量，例如增加隐藏层的数量、为每层添加更多神经元、改变优化器或使用更复杂的激活函数。这些设置可以通过实验进行调优，直到获得最优的模型。
- en: We have discussed several ideas behind improving the performance of neural networks.
    Now, let’s see how we can improve the result achieved on the Fashion MNIST dataset
    in [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with*
    *Neural Networks*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了若干提高神经网络性能的思路。现在，让我们看看如何提高在[*第5章*](B18118_05.xhtml#_idTextAnchor105)《使用神经网络进行图像分类》中，在Fashion
    MNIST数据集上取得的结果。
- en: Fine-tuning hyperparameters of a neural network
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的超参数微调
- en: It is important to establish a **baseline model** before making any improvements
    in machine learning. A baseline model is a simple model that we can use to evaluate
    the performance of more complex models. In [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105),
    *Image Classification with Neural Networks*, we achieved an accuracy of 88.50%
    on our training data and 85.67% on our test data in just five epochs. In our quest
    to try to improve our model’s performance, we will continue with our three-step
    (*build*, *compile*, and *fit*) process of constructing a neural network using
    **TensorFlow**. In each of the steps we use to build our neural network, there
    are settings that need to be configured before training. These settings are called
    **hyperparameters**. They control how the network will learn and perform, and
    mastering the art of fine-tuning them is an essential step in building successful
    deep learning models. Common hyperparameters include the number of neurons in
    each layer, the number of hidden layers, the learning rate, the activation functions,
    and the number of epochs. By iteratively experimenting with these hyperparameters,
    we can obtain the optimal setting best suited to our use case.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行机器学习改进之前，建立一个**基线模型**非常重要。基线模型是一个简单的模型，我们可以用它来评估更复杂模型的表现。在[*第5章*](B18118_05.xhtml#_idTextAnchor105)，《使用神经网络进行图像分类》中，我们在仅五个训练周期内，训练数据的准确率为88.50%，测试数据的准确率为85.67%。为了进一步提高我们模型的表现，我们将继续按照三步流程（**构建**、**编译**、和**训练**）来构建神经网络，使用**TensorFlow**。在构建神经网络的每个步骤中，都有一些需要在训练前配置的设置，这些设置称为**超参数**。超参数决定了网络如何学习和表现，掌握调整超参数的技巧是构建成功深度学习模型的关键步骤。常见的超参数包括每层神经元的数量、隐藏层的数量、学习率、激活函数以及训练周期数。通过不断尝试这些超参数，我们可以找到最适合我们使用场景的最佳设置。
- en: When building real-world models, especially when working on domain-specific
    problems, expert knowledge could prove very useful in pinpointing the best hyperparameter
    values for the task. Let’s return to our notebook and experiment with different
    hyperparameters, and see whether we can beat our baseline model by tweaking one
    or more hyperparameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建现实世界的模型时，特别是处理特定领域问题时，专家知识可能会非常有助于定位任务的最佳超参数值。让我们回到笔记本，尝试不同的超参数，看看通过调整一个或多个超参数是否能够超越我们的基线模型。
- en: Increasing the number of epochs
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加训练周期数
- en: Imagine you are trying to teach a child the multiplication tables; each study
    session you have with the child can be likened to an epoch in machine learning.
    If you only have a small number of study sessions with the child, odds are they
    will not fully grasp the concept of multiplication. Hence, the child will be unable
    to attempt basic multiplication problems. This scenario in machine learning is
    underfitting, where the model hasn’t been able to grasp the underlying patterns
    in the data due to insufficient training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在教一个孩子乘法表；你与孩子每次的学习互动可以比作机器学习中的一个训练周期。如果你与孩子的学习次数很少，那么他们很可能无法完全理解乘法的概念。因此，孩子将无法尝试基本的乘法问题。在机器学习中，这种情况叫做欠拟合，指的是模型由于训练不足，未能捕捉到数据中的潜在模式。
- en: On the other hand, let’s say you spend a good amount of time teaching the child
    to memorize specific aspects of the times tables, say the 2s, 3s, and 4s. The
    child becomes skilled at reciting these tables; however, when faced with multiplying
    numbers such as 10 x 8, the child struggles. This happens because rather than
    understanding the principle of multiplication such that the child can apply the
    underlying idea when working with other numbers, the child simply memorized the
    examples learned during the study session. This scenario in machine learning is
    like the concept of overfitting, where our model performs well on the training
    data but fails to generalize well in new situations. In machine learning, when
    training our model, we need to strike a balance such that our model is trained
    well enough to learn the underlying patterns in our data and not to memorize the
    training data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，假设你花了很多时间教孩子记住乘法表的某些方面，比如 2 的倍数、3 的倍数和 4 的倍数。孩子在背诵这些乘法表时变得熟练；然而，当遇到类似 10
    x 8 这样的乘法题时，孩子却感到困难。这是因为，孩子并没有理解乘法的原理，以至于能够在处理其他数字时应用这个基本的思想，而只是单纯地记住了学习过程中遇到的例子。在机器学习中，这种情况就像过拟合的概念，我们的模型在训练数据上表现良好，但在新情况中无法很好地泛化。在机器学习中，当我们训练模型时，需要找到一个平衡，使得模型足够好地学习数据中的潜在模式，而不是仅仅记住训练数据。
- en: 'Let’s see what the impact of training the model for longer will have on our
    result. This time, let’s go for 40 epochs and observe what will happen:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看延长训练时间对结果的影响。这次，我们选择 40 个训练轮次，观察会发生什么：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we change the number of epochs in `Step 3` from `5` to `40`, keeping
    all other hyperparameters of our base model constant. The last five epochs of
    the output are displayed here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`步骤3`中的训练轮次从`5`改为`40`，同时保持我们基础模型的其他超参数不变。输出的最后五轮结果如下：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Notice that, when we increase the number of epochs, it takes a much longer
    time to train our model. So, it can become computationally expensive when we have
    to train for a large number of epochs. After 40 epochs, we see that our model’s
    training accuracy has jumped up to `0.9524`, and it may appear to you that you
    have found a silver bullet for solving this problem. However, our goal is to ensure
    generalization; hence, the acid test for our model is to see how it will perform
    on the unseen data. Let’s check out what the results look like on our test data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们增加训练轮次时，训练模型所需的时间会显著增加。所以，当训练轮次很大时，计算成本可能会变得很高。经过 40 个训练轮次后，我们发现模型的训练准确率达到了`0.9524`，看起来你可能认为已经找到了解决问题的灵丹妙药。然而，我们的目标是确保模型的泛化能力；因此，检验模型的关键是看它在未见过的数据上表现如何。让我们看看在测试数据上的结果如何：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we run the code, we arrive at an accuracy of `0.8692` on our test data.
    We can see that the longer the model is trained, the more accurate the model can
    be on the training data. However, if we train the model for too long, it will
    reach a point of diminishing returns, which is evident when we compare the difference
    in performance between the training and test set accuracy. It is critical to strike
    the right balance of epochs so that the model can learn and improve, but not to
    the point of overfitting on our training data. A practical approach could be to
    start with a small number of epochs, then increase the number of epochs as required.
    This approach can be effective; however, it can also be time-consuming as multiple
    experiments are required to find the optimal number of epochs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们在测试数据上的准确率为`0.8692`。可以看到，随着模型训练时间的增加，模型在训练数据上的准确率逐渐提高。然而，如果我们训练得太久，模型的效果会出现收益递减的现象，这在比较训练集和测试集准确度的表现差异时尤为明显。找到一个合适的训练轮次非常重要，以确保模型能够学习和提高，但又不会过度拟合训练数据。一种实用的方法是从较少的训练轮次开始，根据需要逐步增加训练轮次。虽然这种方法有效，但也可能比较耗时，因为需要进行多次实验来找到最优的训练轮次。
- en: What if we could perhaps set a rule to stop training before the point of diminishing
    returns? Yes, this is possible. Let’s examine this idea next and see what difference
    it can make to our result.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们能够设定一个规则，在收益递减前停止训练呢？是的，这是可能的。接下来我们来研究这个想法，看看它对结果会有什么影响。
- en: Early stopping using callbacks
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用回调函数进行早停
- en: '**Early stopping** is a regularization technique that can be used to prevent
    overfitting when training neural networks. When we hardcode the number of epochs
    into our model, we cannot halt training when the desired metric is attained, or
    when training begins to degrade or fails to improve any further. We saw this when
    we increased the number of epochs. However, to mitigate against this scenario,
    TensorFlow provides us with early stopping callbacks such that we can either use
    the in-built callback functions or design our own custom callbacks. We can monitor
    our experiments in real time with more control such that we can halt training
    before our model begins to overfit, when our model stops learning during training,
    or in line with other defined criteria. Early stopping can be invoked at various
    points during training. It can be applied at the start or end of training or based
    on attaining a specific metric.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**早停**是一种正则化技术，可以用来防止神经网络在训练时出现过拟合。当我们将训练周期数硬编码到模型中时，我们无法在达到期望的度量标准时停止训练，或者当训练开始退化或不再改进时停止训练。我们在增加训练周期数时遇到了这个问题。然而，为了应对这种情况，TensorFlow为我们提供了早停回调，使我们可以使用内置的回调函数，或者设计自定义的回调函数。我们可以实时监控实验，并且拥有更多的控制权，从而在模型开始过拟合、训练停止学习，或者符合其他定义的标准时，提前停止训练。早停可以在训练的不同阶段调用，可以在训练开始时、结束时或基于达到特定度量时应用。'
- en: Early stopping with built-in callbacks
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用内置回调实现早停
- en: 'Let’s explore built-in callbacks for early stopping with TensorFlow:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起探索TensorFlow中内置的早停回调：
- en: 'We’ll start by importing early stopping from TensorFlow:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从TensorFlow导入早停功能：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we initialize early stopping. TensorFlow allows us to pass in some arguments,
    which we use to create a `callbacks` object:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化早停。TensorFlow允许我们传入一些参数，我们利用这些参数来创建一个`callbacks`对象：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s unpack some of the arguments used in our early stopping function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解读一下我们在早停函数中使用的一些参数：
- en: '`monitor` can be used to track a metric we want to keep an eye on; in our case,
    we want to keep track of the validation loss. We could also switch it to track
    the validation accuracy. It is a good idea to track your experiment on your validation
    split, hence we set our `callbacks` to monitor the validation loss.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitor`可以用来跟踪我们想要关注的指标；在我们的情况下，我们希望跟踪验证损失。我们也可以切换为跟踪验证准确率。建议在验证集上监控实验，因此我们将`callbacks`设置为监控验证损失。'
- en: The `patience` argument is set at `5`. This means if there is no progress with
    regards to reducing the validation loss after five epochs, the training will end.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patience`参数设置为`5`。这意味着，如果在五个周期后验证损失没有任何进展，训练将结束。'
- en: We add the `restore_best_weight` argument and set it to `True`. This allows
    the callback to monitor the entire process and restores the weights from the best
    epoch found during training. If we set `restore_best_weight` to `False`, the model
    weights from the last training step.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们添加了`restore_best_weight`参数并将其设置为`True`。这使得回调可以监控整个过程，并恢复训练过程中找到的最佳训练周期的权重。如果我们将`restore_best_weight`设置为`False`，则使用最后一步训练的模型权重。
- en: When we set `verbose` to `1`, this ensures we are informed when callback actions
    take place. If we set `verbose` to `0`, training stops but we get no output message.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将`verbose`设置为`1`时，这确保了我们在回调操作发生时得到通知。如果我们将`verbose`设置为`0`，训练将停止，但我们不会收到任何输出消息。
- en: There are a few other arguments that can be used here, but these ones work well
    enough for many instances with regard to applying early stopping.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些其他参数可以使用，但这些参数在应用早停时对许多情况来说已经足够有效。
- en: 'We’ll continue with our three-step approach in which we build, compile, and
    fit the model:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将继续采用我们的三步法：构建、编译和拟合模型：
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Step 1` and `Step 2` are the same steps we previously implemented. When building
    out the model, we trained for longer epochs. However, in `Step 3`, we made a few
    tweaks to accommodate our validation split and callbacks. We use 20% of our training
    data for validation and we pass our `callbacks` object into `model.fit()`. This
    ensures our early stopping callbacks interrupt the training when our validation
    loss stops falling. The output is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`第1步`和`第2步`是我们之前实现的相同步骤。在构建模型时，我们进行了更长时间的训练周期。然而，在`第3步`中，我们做了一些调整，以适应我们的验证集拆分和回调。我们将20%的训练数据用于验证，并将`callbacks`对象传递给`model.fit()`。这确保了我们的早停回调在验证损失停止下降时中断训练。输出如下：'
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Because we set `verbose` to `1`, we can see that our experiment ends on epoch
    19\. Now, rather than worrying about how many epochs we need to train effectively,
    we can simply select a large number of epochs and implement early stopping. Next,
    we can also see that because we implemented `restore_best_weights`, the best weights
    are achieved on epoch 14 where we recorded the lowest validation loss (`0.3194`).
    With early stopping, we save compute time and take concrete steps against overfitting.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将`verbose`设置为`1`，所以可以看到我们的实验在第19个epoch结束。现在，与其担心我们需要多少个epoch才能有效训练，我们可以简单地选择一个较大的epoch数并实现早停。接下来，我们还可以看到，因为我们实现了`restore_best_weights`，最佳权重出现在第14个epoch，此时我们记录了最低的验证损失（`0.3194`）。通过早停，我们节省了计算时间，并采取了具体措施防止过拟合。
- en: 'Let’s see what our test accuracy looks like:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们的测试准确率如何：
- en: '[PRE21]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we achieved a test accuracy of `0.8847`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们达到了`0.8847`的测试准确率。
- en: Now, let’s see how we can write our own custom callbacks to implement early
    stopping.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何编写自定义回调来实现早停。
- en: Early stopping with custom callbacks
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义回调实现早停
- en: 'We can extend the capabilities of callbacks by writing our own custom callbacks
    for early stopping. This adds flexibility to callbacks so we can implement some
    desired logic during training. The TensorFlow documentation provides several ways
    to do this. Let’s implement a simple callback to track our validation accuracy:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过编写自己的自定义回调来扩展回调的功能，从而实现早停。这为回调增加了灵活性，使我们能够在训练过程中实现一些期望的逻辑。TensorFlow文档提供了几种实现方法。让我们实现一个简单的回调来跟踪我们的验证准确率：
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For example, if we want to stop our training process when the model exceeds
    85% accuracy on the validation set, we can do this by crafting our own custom
    callback called `E``arlyStop`, which takes the `tf.keras.callbacks.Callback` parameter.
    We then define a function called `on_epoch_end`, which returns the logs for each
    epoch. We set `self.model.stop_training = True` and once the accuracy exceeds
    85%, training ends and displays a message similar to what we get with `verbose`
    set to `1` when we used built-in callbacks. Now we can pass in our `callback`
    into `model.fit()` as we did with built-in callbacks. We then train our model
    using our three-step approach:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们希望在模型在验证集上超过85%的准确率时停止训练，我们可以通过编写自己的自定义回调`EarlyStop`来实现，该回调接受`tf.keras.callbacks.Callback`参数。然后我们定义一个名为`on_epoch_end`的函数，该函数返回每个epoch的日志。我们设置`self.model.stop_training
    = True`，一旦准确率超过85%，训练结束并显示类似于我们在使用内置回调时将`verbose`设置为`1`时所得到的消息。现在，我们可以像使用内置回调一样，将`callback`传入`model.fit()`中。然后我们使用我们三步法训练模型：
- en: '[PRE25]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This time, at the end of the first epoch, we arrive at over 85% validation accuracy.
    Again, this is a smart way of achieving the desired metrics with minimal use of
    computational resources.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，在第一个epoch结束时，我们的验证准确率已经超过了85%。再次强调，这是通过最小化计算资源的使用来实现期望指标的智能方法。
- en: Now that we have a good grasp of how to select epochs and apply early stopping,
    let’s now set our sights on other hyperparameters and see whether by tweaking
    one or more of them, we can improve our test accuracy of 88%. Perhaps we can start
    by trying out a more complex model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了如何选择epoch并应用早停，让我们把目光转向其他超参数，看看通过调整一个或多个超参数，我们是否能提高88%的测试准确率。也许我们可以从尝试一个更复杂的模型开始。
- en: Let’s see what happens if we add more neurons to our hidden layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们向隐藏层添加更多神经元会发生什么。
- en: Adding neurons in the hidden layer
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加隐藏层的神经元
- en: 'The hidden layers are responsible for the heavy lifting in neural networks,
    as we covered when discussing the anatomy of neural networks in [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105),
    *Image Classification with Neural Networks*. Let’s try out different numbers of
    neurons in the hidden layer. We’ll define a function called `train_model` that
    will allow us to try out different numbers of neurons. The `train_model` function
    takes the `hidden_neurons` argument that represents the number of hidden neurons
    in the model. In addition, the function also takes training images, labels, callbacks,
    validation splits, and epochs. The function builds, compiles, and fits the model
    using these parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层负责神经网络中的重负载，就像我们在讨论神经网络的结构时提到的那样，参见[*第5章*](B18118_05.xhtml#_idTextAnchor105)，《使用神经网络进行图像分类》。让我们尝试不同数量的隐藏层神经元。我们将定义一个名为`train_model`的函数，允许我们尝试不同数量的神经元。`train_model`函数接受一个名为`hidden_neurons`的参数，表示模型中隐藏层神经元的数量。此外，该函数还接受训练图像、标签、回调、验证分割和epoch数。该函数使用这些参数构建、编译并拟合模型：
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To try out a list of neurons, we created a `for` loop to iterate over the neuron
    list called `neuron_values`. Then it applies the `train_model` function to build
    and train a model for each of the neurons in the list:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试一组神经元，我们创建了一个`for`循环来遍历名为`neuron_values`的神经元列表。然后它应用`train_model`函数为列表中每个神经元构建并训练一个模型：
- en: '[PRE27]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `print` statement returns a message indicating that the model has been
    trained with 1 and 500 neurons respectively. Let’s examine the results when we
    run the function, starting with the hidden layer with one neuron:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`print`语句返回一条消息，指示模型已分别使用1个和500个神经元进行了训练。让我们检查运行该函数时的结果，从只有一个神经元的隐藏层开始：'
- en: '[PRE28]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'From our result, the model with one neuron in the hidden layer was not complex
    enough to identify patterns in the data. This model performed well below 50%,
    which is a clear case of underfitting. Next, let’s look at the result from the
    model with 500 neurons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的结果来看，隐藏层只有一个神经元的模型不足够复杂，无法识别数据中的模式。这个模型的表现远低于50%，这是典型的欠拟合案例。接下来，让我们看看使用500个神经元的模型结果：
- en: '[PRE29]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can see that the model is overfitting with more neurons. The model recorded
    an accuracy of `0.9303` on the training set but `0.8838` on the test set. In general,
    a larger hidden layer can learn more complex patterns; however, it would require
    more computational resources and be more prone to overfitting. When selecting
    the number of neurons in the hidden layer, it is important to consider the size
    of the training data. If we have a large training sample, we can afford to have
    a large number of neurons in the hidden layer. However, when the training sample
    is quite small, it may be better to consider working with a smaller number of
    neurons in the hidden layer. A larger number of neurons could lead to overfitting,
    as we saw in our experiment, and this architecture may perform even worse than
    a model with a smaller number of neurons in the hidden layer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型在使用更多神经元时出现了过拟合。模型在训练集上的准确率为`0.9303`，但在测试集上的准确率为`0.8838`。通常来说，更大的隐藏层能够学习更复杂的模式；然而，它会需要更多的计算资源，并且更容易发生过拟合。在选择隐藏层神经元数量时，考虑训练数据的大小非常重要。如果我们有大量的训练样本，我们可以选择更大的神经元数量。但当训练样本较小时，可能需要考虑使用较少的神经元。正如我们在实验中看到的，更多的神经元可能导致过拟合，而这种架构的表现可能会比拥有较少神经元的模型还要差。
- en: Another consideration to bear in mind is the type of data we are working with.
    When we work with linear data, a small number of hidden layers may be sufficient
    for our neural network. However, with non-linear data, we will need a more complex
    model to learn the complexities in the data. Finally, we must bear in mind that
    models with more neurons require longer training time. It is important to consider
    the trade-off between performance and generalization. As a rule of thumb, you
    can start training a model with a small number of neurons. This will train faster
    and avoid overfitting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是我们所使用的数据类型。当我们处理线性数据时，少量的隐藏层可能就足够了。然而，对于非线性数据，我们需要更复杂的模型来学习数据中的复杂性。最后，我们还必须牢记，拥有更多神经元的模型需要更长的训练时间。重要的是要考虑性能和泛化能力之间的权衡。通常的做法是，从少量神经元开始训练模型。这样训练速度更快，并能避免过拟合。
- en: Alternatively, we can optimize the number of neurons in the hidden layer by
    identifying neurons that have little or no impact on the performance of the network.
    This approach is called **pruning**. This is outside the scope of the exam, so
    we will stop there.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过识别对网络性能影响较小或没有影响的神经元，来优化隐藏层中的神经元数量。这种方法称为**剪枝**。这超出了考试的范围，所以我们到此为止。
- en: Let’s see the impact of adding more layers to our baseline architecture. So
    far, we have looked at making the model more complex and training for longer.
    How about we try out changing the optimizers? Let’s mix things up a bit and see
    what happens.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看向基准架构中添加更多层的影响。到目前为止，我们已经考虑过使模型更加复杂并训练更长时间。那我们试试改变优化器呢？让我们稍微变换一下，看看会发生什么。
- en: Changing the optimizers
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改优化器
- en: We have used the **Adam optimizer** as our default optimizer; however, there
    are other prominent optimizers and they all have their pros and cons. In this
    book and for your exam, we will focus on Adam, **stochastic gradient descent**
    (**SGD**), and **Root Mean Squared Propagation** (**RMSprop**). RMSprop has low
    memory requirements and offers an adaptive learning rate; on the flip side, it
    takes a much longer time to converge in comparison to Adam and SGD. RMSprop works
    well on training very deep networks such as **recurrent neural networks** (**RNNs**),
    which we will talk about later in this book.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**Adam优化器**作为默认优化器；然而，还有其他一些知名的优化器，它们各有优缺点。在本书中，以及为了你的考试，我们将重点讲解Adam、**随机梯度下降**（**SGD**）和**均方根传播**（**RMSprop**）。RMSprop具有较低的内存需求，并提供自适应学习率；但与Adam和SGD相比，它的收敛时间要长得多。RMSprop在训练非常深的网络时表现良好，比如**递归神经网络**（**RNN**），这将在本书后面讨论。
- en: On the other hand, SGD is another popular optimizer; it is simple to implement
    and efficient when the data is sparse. However, it is slow to converge and requires
    careful tuning of the learning rate. If the learning rate is too high, SGD will
    diverge; if the learning rate is too low, SGD will converge very slowly. SGD works
    well on a wide variety of problems and converges faster than other optimizers
    on large datasets, but it could sometimes converge slowly when training very large
    neural networks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，SGD是另一种流行的优化器；它简单易实现，并且当数据稀疏时效率较高。然而，它收敛较慢，并且需要仔细调整学习率。如果学习率过高，SGD会发散；如果学习率过低，SGD会收敛得非常慢。SGD在各种问题上表现良好，并且在大数据集上比其他优化器收敛得更快，但在训练非常大的神经网络时，有时会收敛得较慢。
- en: Adam is an improved version of SGD; it has low memory requirements, offers an
    adaptive learning rate, is a very efficient optimizer, and can converge to a good
    solution in fewer iterations than to SGD or RMSprop. Adam is also well suited
    for training large neural networks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Adam是SGD的改进版；它具有较低的内存需求，提供自适应学习率，是一种非常高效的优化器，并且可以比SGD或RMSprop在更少的迭代次数下收敛到一个良好的解。Adam也非常适合训练大型神经网络。
- en: 'Let’s try out these three optimizers and see which one works best on our dataset.
    We have changed the optimizer from Adam to RMSprop and SGD, and used the same
    architecture with built-in callbacks. We can see the results in *Figure 6**.2*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这三种优化器，看看哪一种在我们的数据集上效果最好。我们已经将优化器从Adam更改为RMSprop和SGD，并使用相同的架构和内建回调。我们可以在*图6.2*中看到结果：
- en: '|  | **Adam** | **RMSProp** | **SGD** |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | **Adam** | **RMSProp** | **SGD** |'
- en: '| Number of epochs before early stopping | 13 | 9 | 39 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 早停前的训练轮数 | 13 | 9 | 39 |'
- en: '| Validation accuracy | 0.8867 | 0.8788 | 0.8836 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 验证准确率 | 0.8867 | 0.8788 | 0.8836 |'
- en: '| Test accuracy | 0.8787 | 0.8749 | 0.8749 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 测试准确率 | 0.8787 | 0.8749 | 0.8749 |'
- en: Figure 6.2 – The performance of different optimizers
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 不同优化器的性能
- en: Although Adam required more training epochs, its results were marginally better
    than those of the other optimizers. Of course, any of these optimizers can be
    used for this problem. In later chapters, we will work on real-world images that
    are more complex. There, we will revisit these optimizers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Adam需要更多的训练轮次，但其结果略微优于其他优化器。当然，任何一种优化器都可以用于此问题。在后续章节中，我们将处理更复杂的真实世界图像，并会再次讨论这些优化器。
- en: Before we close this chapter, let’s look at the learning rate and its impact
    on our model’s performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们来看一下学习率及其对模型性能的影响。
- en: Changing the learning rate
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改学习率
- en: '**Learning rate** is an important hyperparameter that controls how well our
    model will learn and improve during training. An optimal learning rate will ensure
    the model converges quickly and accurately, while on the other hand, a poorly
    selected learning rate can lead to a wide range of issues such as slow convergence,
    underfitting, overfitting, or network instability.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率**是一个重要的超参数，它控制着我们的模型在训练过程中学习和改进的效果。一个合适的学习率将确保模型快速且准确地收敛，而一个选择不当的学习率则可能导致各种问题，如收敛缓慢、欠拟合、过拟合或网络不稳定。'
- en: 'To understand the impact of the learning rate, we need to know how it affects
    the model’s training process. The learning rate is the step size taken to reach
    the point where the loss function is at its minimum. In *Figure 6**.3(a)*, we
    see that when we choose a low learning rate, the model requires too many steps
    to reach the minimum point. On the flip side, when the learning rate is too high,
    the model would likely learn too quickly, taking larger steps and likely overshooting
    the minimum point, as seen in *Figure 6**.3(c)*. A high learning rate can lead
    to instability and overfitting. However, when we find the ideal learning rate
    as in *Figure 6**.3(b)*, the model is likely to experience fast convergence and
    good generalization:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解学习率的影响，我们需要了解它如何影响模型的训练过程。学习率是达到损失函数最小值所采取的步长。在 *图 6**.3(a)* 中，我们看到选择较低学习率时，模型需要太多步骤才能达到最小点。另一方面，当学习率过高时，模型可能会学习得太快，采取较大步长，并可能超过最小点，就像
    *图 6**.3(c)* 中所示。高学习率可能导致不稳定性和过拟合。然而，当我们像 *图 6**.3(b)* 中找到理想学习率时，模型很可能会快速收敛并具有良好的泛化能力：
- en: '![Figure 6.3 – A plot showing low, optimal, and high learning rates](img/B18118_06_03.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 展示低、理想和高学习率的绘图](img/B18118_06_03.jpg)'
- en: Figure 6.3 – A plot showing low, optimal, and high learning rates
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 展示低、理想和高学习率的绘图
- en: 'The question that comes to mind is: how do we find the optimal learning rate?
    One way is to try out different learning rates and see what works based on evaluating
    the model on the validation set. Another way is to use a learning rate scheduler.
    This allows us to dynamically adjust the learning rate during training. We will
    explore this approach in the later chapters of the book. Here, let’s try out a
    few different learning rates to see how they impact on our network.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的问题是：我们如何找到最佳的学习率？一种方法是尝试不同的学习率，并根据在验证集上评估模型的表现来确定有效的学习率。另一种方法是使用学习率调度器。这允许我们在训练过程中动态调整学习率。我们将在本书的后面章节探讨这种方法。在这里，让我们尝试几个不同的学习率，看看它们对我们的网络的影响。
- en: 'Let’s craft a function that will take a list of different learning rates. In
    this experiment, we will try out six different learning rates (1, 0.1, 0.01, 0.001,
    0.0001, 0.00001, and 0.000001). First, let’s create a function to create our model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个函数，该函数将接受一组不同的学习率。在这个实验中，我们将尝试六种不同的学习率（1、0.1、0.01、0.001、0.0001、0.00001和0.000001）。首先，让我们创建一个函数来创建我们的模型：
- en: '[PRE30]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will use the function to build, compile, and fit the model. It also takes
    in the learning rate as a variable that we pass into our function, and that returns
    the test accuracy as our result:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用该函数来构建、编译和拟合模型。它还将学习率作为一个变量传递到我们的函数中，并将测试准确率作为结果返回：
- en: '[PRE31]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We have now outlined the different learning rates. Here, we want to experiment
    with different learning rates, from a very high to a very low learning rate. We
    created an empty list and appended our test set accuracy. Next, let’s look at
    the numeric values in a tabular fashion. We use `pandas` to generate a DataFrame
    with the learning rate and accuracies:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经概述了不同的学习率。在这里，我们想要尝试不同的学习率，从非常高到非常低的学习率。我们创建了一个空列表，并附加了我们的测试集准确率。接下来，让我们以表格形式查看数值。我们使用
    `pandas` 生成一个包含学习率和准确率的 DataFrame：
- en: '[PRE32]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output DataFrame is shown in the following screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出的 DataFrame 截图：
- en: '![Figure 6.4 – Different learning rates and their test accuracies](img/B18118_06_04.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 不同学习率及其测试准确率](img/B18118_06_04.jpg)'
- en: Figure 6.4 – Different learning rates and their test accuracies
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 不同学习率及其测试准确率
- en: From the results, we can see that when working with a really high learning rate
    (1.0), the model performs poorly. As we reduce the learning rate value, we see
    that the model’s accuracy begins to rise; when the learning rate becomes too small,
    the model takes too long to converge. There is no silver bullet when it comes
    to choosing the ideal learning rate for a problem. It depends on several factors
    such as the model architecture, the data, and the type of optimization technique
    applied.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以看到，当使用非常高的学习率（1.0）时，模型表现很差。随着学习率值的降低，我们看到模型的准确性开始提高；当学习率变得太小时，模型收敛所需时间太长。在选择问题的理想学习率时，并没有银弹。这取决于诸多因素，如模型架构、数据以及应用的优化技术类型。
- en: Now that we have seen various ways of tweaking a model to improve its performance,
    we have come to the end of the chapter. We’ve tried adjusting different hyperparameters
    to improve our model’s performance; however, we got stuck on 88% test accuracy.
    Perhaps this is a good time to try something else, which we will do in the next
    chapter. Take a break, and when you are ready, let’s see how we can improve this
    result and also try out real-world images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了各种调整模型以提高其性能的方法，本章也已经结束。我们尝试了调整不同的超参数来改善模型的性能；然而，我们的测试准确率停滞在88%。或许现在是尝试其他方法的好时机，接下来我们将在下一章中进行尝试。休息一下，当你准备好时，让我们看看如何改善这个结果，并尝试使用真实世界的图像。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at improving the performance of a neural network.
    Although we worked with a lightweight dataset, we have learned some important
    ideas around improving our model’s performance–ideas that will come in handy,
    both in the exam and on the job. You now know that data quality and model complexity
    are two sides of the machine learning coin. If you have good-quality data, a poor
    model will yield subpar results and, on the flip side, even the most advanced
    model will yield a suboptimal result with bad data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何提高神经网络的性能。尽管我们使用的是一个轻量级的数据集，但我们已经学到了关于提高模型性能的一些重要概念——这些概念在考试和工作中都会派上用场。你现在知道，数据质量和模型复杂性是机器学习中的两个方面。如果你有高质量的数据，糟糕的模型也会产生不理想的结果；反之，即使是最先进的模型，如果数据不好，也会产生次优的结果。
- en: By now, you should have a good understanding and hands-on experience of fine-tuning
    neural networks. Like a seasoned expert, you should be able to understand the
    art of fine-tuning hyperparameters and apply this to different machine learning
    problems and not just image classification. Also, you have seen that model building
    requires a lot of experimenting. There is no silver bullet, but having a good
    understanding of the moving parts and various techniques, and how and why to apply
    them, is what differentiates a star from the average Joe.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对微调神经网络有了深入的理解和实践经验。像一个经验丰富的专家一样，你应该能够理解微调超参数的艺术，并将其应用到不同的机器学习问题中，而不仅仅是图像分类。此外，你已经看到，构建模型需要大量实验。没有银弹，但了解各个环节和各种技巧，以及如何和为什么应用它们，这正是明星与普通人之间的区别。
- en: In the next chapter, we will examine convolutional neural networks. We will
    see why they are state-of-the-art when it comes to image classification tasks.
    We will look at the power of convolutions and examine in a hands-on fashion how
    they do things differently from the simple neural networks we have been using
    so far.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨卷积神经网络。我们将看到它们在图像分类任务中为何处于最先进的水平。我们将了解卷积的强大功能，并通过动手操作，深入了解它们与我们迄今为止使用的简单神经网络有何不同。
- en: Questions
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let’s test what we’ve learned in this chapter using the CIFAR-10 notebook:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用CIFAR-10笔记本测试我们在本章中学到的内容：
- en: Build a neural network using our three-step approach.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们三步法构建神经网络。
- en: Increase the number of neurons from 5 to 100 in the hidden layer.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将隐藏层中的神经元数量从5增加到100。
- en: Use a custom callback to stop training when the training accuracy is 90%.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义回调函数，当训练准确率达到90%时停止训练。
- en: 'Try out the following learning rates: 5, 0.5, 0.01, 0.001\. What did you observe?'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试以下学习率：5、0.5、0.01、0.001。你观察到了什么？
- en: Further reading
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, you can check out the following resources:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多信息，可以查看以下资源：
- en: Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*, Packt Publishing.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amr, T., 2020. *动手实践机器学习：使用scikit-learn和科学Python工具包*，Packt出版。
- en: Gulli, A., Kapoor, A. and Pal, S., 2019\. *Deep Learning with TensorFlow 2 and
    Keras*, Packt Publishing.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulli, A., Kapoor, A. 和 Pal, S., 2019. *使用TensorFlow 2和Keras的深度学习*，Packt出版。
- en: '*How to Write Custom TensorFlow Callbacks — The Easy* *Way*: [https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c](https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何编写自定义TensorFlow回调函数——简易方法*：[https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c](https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c)'
- en: '[https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3](https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3](https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3)'
- en: '[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping](https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping](https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3)'
