- en: 7\. Temporal Difference Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 时序差分学习
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概览
- en: In this chapter, we will be introduced to **Temporal Difference** (**TD**) learning
    and focus on how it develops the ideas of the Monte Carlo methods and dynamic
    programming. TD learning is one of the key topics in the field and studying it
    allows us to have a deep understanding of reinforcement learning and how it works
    at the most fundamental level. A new perspective will allow us to see MC methods
    as a particular case of TD ones, unifying the approach and extending their applicability
    to non-episodic problems. By the end of this chapter, you will be able to implement
    the **TD(0)**, **SARSA**, **Q-learning**, and **TD(λ)** algorithms and use them
    to solve environments with both stochastic and deterministic transition dynamics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**时序差分**（**TD**）学习，并重点讨论它如何发展蒙特卡罗方法和动态规划的思想。时序差分学习是该领域的关键主题之一，研究它使我们能够深入理解强化学习及其在最基本层面上的工作原理。新的视角将使我们看到蒙特卡罗方法是时序差分方法的一个特例，从而统一了这种方法，并将其适用性扩展到非情节性问题。在本章结束时，你将能够实现**TD(0)**、**SARSA**、**Q-learning**和**TD(λ)**算法，并用它们来解决具有随机和确定性转移动态的环境。
- en: Introduction to TD Learning
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序差分学习简介
- en: After having studied dynamic programming and Monte Carlo methods in the previous
    chapters, in this chapter, we will focus on temporal difference learning, one
    of the main stepping stones of reinforcement learning. We will start with their
    simplest formulation, that is, the one-step methods, and we will build on them
    to create their most advanced formulation, which is based on the eligibility traces
    concept. We will see how this new approach allows us to frame TD and MC methods
    under the same derivation idea, giving us the ability to compare the two. Throughout
    this chapter, we will implement many different flavors of TD methods and apply
    them to the FrozenLake-v0 environment under both the deterministic and the stochastic
    environment dynamics. Finally, we will solve the stochastic version of FrozenLake-v0
    with an off-policy TD method known as Q-learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章学习了动态规划和蒙特卡罗方法之后，本章我们将重点讨论时序差分学习，这是强化学习的主要基石之一。我们将从它们最简单的形式——单步方法开始，然后在此基础上构建出它们最先进的形式，基于资格迹（eligibility
    traces）概念。我们将看到这种新方法如何使我们能够将时序差分和蒙特卡罗方法框架在相同的推导思想下，从而能够对比这两者。在本章中，我们将实现多种不同的时序差分方法，并将它们应用于FrozenLake-v0环境，涵盖确定性和随机环境动态。最后，我们将通过一种名为Q-learning的离策略时序差分方法解决FrozenLake-v0的随机版本。
- en: Temporal difference learning, whose name derives from the fact that it uses
    differences in state (or state-actions pairs) values between subsequent timesteps
    to learn, can be considered a central idea in the field of reinforcement learning
    algorithms. It shares some important aspects with the methods we studied in previous
    chapters – in fact, just like those methods, it learns through experience, with
    no need to have a model (like Monte Carlo methods do), and it "bootstraps," meaning
    it learns how to use information it's acquired before reaching the end of the
    episode (like dynamic programming methods do).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 时序差分学习，其名称来源于它通过在后续时间步之间比较状态（或状态-动作对）值的差异来进行学习，可以被视为强化学习算法领域的一个核心思想。它与我们在前几章中学习的方法有一些重要相似之处——事实上，就像那些方法一样，它通过经验进行学习，无需模型（像蒙特卡罗方法那样），并且它是“自举”的，意味着它能够在达到情节结束之前，利用已经获得的信息进行学习（就像动态规划方法那样）。
- en: 'These differences are strictly related to the advantages that TD methods offer
    with respect to MC and DP ones: it doesn''t need a model of the environment and
    it can be applied with a greater generality with respect to DP methods. Its ability
    to bootstrap, on the other hand, makes TD more suited for tasks with very long
    episodes and the only solution for non-episodic ones – to which Monte Carlo methods
    cannot be applied. As an example of a long-term or non-episodic task, think of
    an algorithm that is used to grant user access to a server that''s rewarded every
    time the first user in the queue is assigned to a resource and receiving zero
    reward if user access is not granted. The queue typically never ends, so this
    is a continuing task that has no episodes.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异与时序差分方法相对于蒙特卡洛（MC）和动态规划（DP）方法的优势紧密相关：它不需要环境模型，并且相对于DP方法，它可以更广泛地应用。另一方面，它的引导能力使得时序差分方法更适合处理非常长的任务回合，并且是非回合任务的唯一解决方案——蒙特卡洛方法无法应用于这种任务。以长期或非回合任务为例，想象一个算法，它用于授予用户访问服务器的权限，每次将排队中的第一个用户分配到资源时会获得奖励，如果没有授予用户访问权限，则没有奖励。这个队列通常永远不会结束，因此这是一个没有回合的持续任务。
- en: 'As seen in previous chapters, the exploration versus exploitation trade-off
    is a very important subject, and again also in the case of temporal difference
    algorithms. They fall into two main classes: on-policy and off-policy methods.
    As we saw in the previous chapters, in on-policy methods, the same policy that
    is learned is used to explore the environment, while in off-policy ones, the two
    can be different: one if used for exploration, while the other one is the target
    to be learned. In the following sections, we will address the general problem
    of estimating the state value function for a given policy. Then, we will see how,
    by building upon it, we can obtain a complete RL algorithm to train both on-policy
    and off-policy methods to find the optimal policy for a given problem.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前几章所见，探索与利用的权衡是一个非常重要的话题，在时序差分算法中同样如此。它们分为两大类：在政策方法和脱离政策方法。正如我们在前面章节中所看到的，在在政策方法中，所学习的政策用于探索环境，而在脱离政策方法中，二者可以不同：一个用于探索，另一个是目标政策，旨在学习。在接下来的部分中，我们将讨论为给定政策估计状态价值函数的通用问题。然后，我们将看到如何基于它构建一个完整的强化学习算法，训练在政策和脱离政策方法，以找到给定问题的最优策略。
- en: Let's start with our first steps in the temporal difference methods world.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从时序差分方法的世界开始第一步。
- en: TD(0) – SARSA and Q-Learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD(0) – SARSA 和 Q-Learning
- en: 'TD methods are model-free, meaning they do not need a model of the environment
    to learn a state value representation. For a given policy, ![1](img/B16182_07_00a.png),
    they accumulate experience associated with it and update their estimate of the
    value function for every state encountered during the corresponding experience.
    In doing so, TD methods update a given state value, visited at time `t`, using
    the value of state (or states) encountered at the next few time steps, so for
    time `t+1`, `t+2`, ..., `t+n`. An abstract example is as follows: an agent is
    initialized in the environment and starts interacting with it by following a given
    policy, without any knowledge of what results are generated by which action. Following
    a certain number of steps, the agent will eventually reach a state associated
    with a reward. This reward signal is used to increment the values of previously
    visited states (or action-state pairs) with the TD learning rule. In fact, those
    states have allowed the agent to reach the goal, so they are to be associated
    with a high value. Repeating this process over and over will allow the agent to
    build a complete and meaningful value map of all states (or state-action pairs)
    so that it will exploit this acquired knowledge to select the best actions, thereby
    leading to states associated with a reward.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 时序差分方法是无模型的，这意味着它们不需要环境模型来学习状态值表示。对于给定的策略，![1](img/B16182_07_00a.png)，它们累积与之相关的经验，并更新在相应经验中遇到的每个状态的值函数估计。在这个过程中，时序差分方法使用在接下来的时间步骤中遇到的状态（或状态）来更新给定状态值，状态是在时间`t`访问的，因此是`t+1`、`t+2`、...、`t+n`。一个抽象的例子如下：一个智能体在环境中初始化并开始通过遵循给定的策略与环境互动，而没有任何关于哪个动作会生成哪些结果的知识。经过一定数量的步骤，智能体最终会到达一个与奖励相关的状态。该奖励信号用于通过时序差分学习规则增加先前访问的状态（或动作-状态对）的值。实际上，这些状态帮助智能体达到了目标，因此应该与较高的值相关联。重复这个过程将使得智能体构建一个完整且有意义的所有状态（或状态-动作对）的价值图，以便它利用所获得的知识选择最佳动作，从而达到与奖励相关的状态。
- en: This means that TD methods do not have to wait until the end of the episode
    to improve their policy; instead, they can build upon values of states they encounter,
    and the learning process can start right after initialization.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，TD方法不需要等到本回合结束才改进策略；相反，它们可以基于遇到的状态的值进行构建，学习过程可以在初始化之后立即开始。
- en: 'In this section, we will focus on the so-called one-step method, also named
    TD(0). In this method, the only value considered to build the update for a given
    state value function is the one found at the next time step, nothing else. So,
    for example, the value function update for a state at time `t` looks as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论所谓的一步法，也称为TD(0)。在这种方法中，唯一被考虑用于构建给定状态价值函数更新的数值是下一个时间步的数值，别无其他。因此，举例来说，时间`t`时刻状态的价值函数更新如下所示：
- en: '![Figure 7.1: Value function update for a state at time ''t'''
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.1：时间''t''时刻状态的价值函数更新'
- en: '](img/B16182_07_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_01.jpg)'
- en: 'Figure 7.1: Value function update for a state at time ''t'''
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：时间't'时刻状态的价值函数更新
- en: 'Here, ![2](img/B16182_07_01a.png) is the next state where the environment transitioned,
    ![b](img/B16182_07_01b.png) is the reward obtained in the transition, ![c](img/B16182_07_01c.png)
    is the learning rate, and ![d](img/B16182_07_01d.png) is the discount factor.
    It is clear how TD methods "bootstrap": in order to update the value function
    for a state `(t)`, they use the current value function for the next state `(t+1)`
    without waiting until the end of the episode. It is worth noting that the quantity
    between square brackets in the previous equation can be interpreted as an error
    term. This error term measures the difference between the estimated value of state
    St and the new, better estimate, ![e](img/B16182_07_01e.png) . This quantity is
    called the TD error, and we will encounter it many times in RL theory:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![2](img/B16182_07_01a.png)是环境转移后的下一个状态，![b](img/B16182_07_01b.png)是转移过程中获得的奖励，![c](img/B16182_07_01c.png)是学习率，![d](img/B16182_07_01d.png)是折扣因子。很明显，TD方法是如何“自举”的：为了更新状态`(t)`的价值函数，它们使用下一个状态`(t+1)`的当前价值函数，而无需等待直到本回合结束。值得注意的是，前面方程中方括号内的量可以被解释为误差项。这个误差项衡量了状态St的估计值与新的、更好的估计值之间的差异，![e](img/B16182_07_01e.png)。这个量被称为TD误差，我们将在强化学习理论中多次遇到它：
- en: '![Figure 7.2: TD error at time ''t'''
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.2：时间''t''时刻的TD误差'
- en: '](img/B16182_07_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_02.jpg)'
- en: 'Figure 7.2: TD error at time ''t'''
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：时间't'时刻的TD误差
- en: This error is specific for the given time it has been calculated at, and it
    depends on the values at the next time step (that is, the error at time t depends
    on the values at time `t+1`).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该误差是针对计算时所使用的特定时间而言的，它依赖于下一个时间步的数值（即，时间t的误差依赖于时间`t+1`的数值）。
- en: 'One important theory result for TD methods is their proof of convergence: in
    fact, it has been demonstrated that, for any fixed policy, ![f](img/B16182_07_02a.png),
    the algorithm TD(0) described in the preceding equation converges to the state
    (or action-state pair) value function, ![i](img/B16182_07_02b.png). Convergence
    is reached for a constant step size parameter, provided it is sufficiently small,
    and with probability `1` if the step size parameter decreases according to some
    specific (but easy to comply with) stochastic approximation conditions. These
    proofs mainly apply to the tabular version of the algorithm, which are the versions
    used for RL theory introduction and understanding. These deal with problems in
    which states and actions spaces are of limited dimensions so that they can be
    exhaustively represented by a finite combination of variables.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TD方法的一个重要理论结果是它们的收敛性证明：事实上，已经证明，对于任何固定的策略，![f](img/B16182_07_02a.png)，前面方程中描述的算法TD(0)会收敛到状态（或状态-动作对）价值函数，![i](img/B16182_07_02b.png)。当步长参数足够小且为常数时，收敛是可以达到的，并且如果步长参数根据一些特定的（但容易遵循的）随机逼近条件减小，则以概率`1`收敛。这些证明主要适用于算法的表格版本，表格版本是用于强化学习理论介绍和理解的版本。这些版本处理的问题是状态和动作空间维度有限，因此可以通过有限变量组合进行穷举表示。
- en: However, the majority of these proofs can be easily extended to algorithm versions
    that rely on approximations when they are composed by general linear functions.
    These approximated versions are used when states and actions spaces are so large
    that they cannot be represented by a finite combination of variables (for example,
    when the state space is the space of RGB images).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当状态和动作空间如此庞大以至于不能通过有限变量的有限组合来表示时（例如，当状态空间是RGB图像空间时），这些证明的大多数可以轻松地推广到依赖于近似的算法版本时，这些近似版本被用于算法版本。
- en: So far, we have been dealing with state value functions. In order to approach
    the problem of temporal difference control, we need to learn a state-action value
    function rather than a state-value function. In fact, in this way, we will be
    able to associate a value with state-action pairs, thereby building a value map
    that can then be used to define our policy. How we implement this specifically
    depends on the method class. First, let's take a look at the on-policy approach,
    which is implemented by the so-called SARSA algorithm, and then the off-policy
    one, which is implemented by the so-called Q-learning algorithm.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理状态值函数。为了解决时序差异控制的问题，我们需要学习一个状态-动作值函数，而不是一个状态值函数。事实上，通过这种方式，我们将能够为状态-动作对关联一个值，从而构建一个值映射，然后可以用来定义我们的策略。我们如何具体实现这一点取决于方法类别。首先，让我们看看所谓的在线策略方法，由所谓的SARSA算法实现，然后看看所谓的Q学习算法，它实现了离线策略方法。
- en: SARSA – On-Policy Control
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SARSA – 在线策略控制
- en: 'For an on-policy method, the goal is to estimate ![j](img/B16182_07_02c.png),
    that is, the state-action value function for the current behavior policy, ![k](img/B16182_07_02d.png),
    for all states and all actions. To do so, we simply need to apply the equation
    we saw for the state-value function to the state-action function. Since the two
    cases are identical (both being Markov chains with a reward process), the theorems
    stating the convergence of the state-value function to the one corresponding to
    the optimal policy (and so, solving the problem of finding the optimal policy)
    are valid in this new setting, where the value function regards state-action pairs.
    The update equation takes the following form:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在线策略方法，目标是估计![j](img/B16182_07_02c.png)，即当前行为策略下的状态-动作值函数，![k](img/B16182_07_02d.png)，适用于所有状态和所有动作。为此，我们只需将我们在状态值函数中看到的方程应用于状态-动作函数。由于这两种情况是相同的（都是马尔可夫链和奖励过程），关于状态值函数收敛到与最优策略相对应的值函数的定理（因此解决了找到最优策略的问题）在这种新设置中也是有效的，其中值函数涉及状态-动作对。更新方程如下所示：
- en: '![Figure 7.3: State-action value function at time ''t'''
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3：时间‘t’时的状态-动作值函数'
- en: '](img/B16182_07_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_03.jpg)'
- en: 'Figure 7.3: State-action value function at time ''t'''
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：时间‘t’时的状态-动作值函数
- en: This update is supposedly performed after every transition from a non-terminal
    state, ![a](img/B16182_07_03a.png). If ![b](img/B16182_07_03b.png) is a terminal
    state, then the ![c](img/B16182_07_03c.png) value is set equal to `0`. As we can
    see, the update rule uses every element of the quintuple ![d](img/B16182_07_03d.png),
    which explains the transition from one state-action pair to the next, with the
    reward associated with the transition. This quintuple, written in this form, is
    the reason why the name **SARSA** was given to this algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种更新应该在每次从非终止状态![a](img/B16182_07_03a.png)转移到另一个状态后执行。如果![b](img/B16182_07_03b.png)是一个终止状态，则![c](img/B16182_07_03c.png)的值设为`0`。正如我们所见，更新规则使用了五元组![d](img/B16182_07_03d.png)的每个元素，这解释了与转换相关联的状态-动作对之间的转换，以及与转换相关联的奖励。正是因为这种形式的五元组，这个算法被称为**SARSA**。
- en: 'Using these elements, it is straightforward to design an on-policy control
    algorithm based on them. As we mentioned previously, all on-policy methods estimate
    ![g](img/B16182_07_03e.png) for the behavior policy, ![h](img/B16182_07_03f.png),
    and at the same time, update ![formula](img/B16182_07_03g.png) based on ![formula](img/B16182_07_03h.png).
    A scheme for the SARSA control algorithm can be depicted as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些元素，可以很容易地基于它们设计一个基于在线策略的控制算法。正如我们之前提到的，所有在线策略方法都估计行为策略![g](img/B16182_07_03e.png)的![g](img/B16182_07_03e.png)，同时基于![h](img/B16182_07_03f.png)更新![formula](img/B16182_07_03g.png)。SARSA控制算法的方案可以描述如下：
- en: 'Choose the algorithm parameters; that is, the step size, ![formula](img/B16182_07_03i.png),
    which has to be contained in the interval `(0, 1]`, and the `ε` parameter of the
    ε-greedy policy, which has to be small and greater than 0, since it represents
    the probability of choosing the non-optimal action to favor exploration. This
    can be done with the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择算法参数；即步长，![公式](img/B16182_07_03i.png)，该值必须在区间`(0, 1]`内，并且ε-贪心策略的`ε`参数必须小且大于0，因为它表示选择非最优动作的概率，以便进行探索。这可以通过以下代码实现：
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Initialize ![formula](img/B16182_07_03j.png), for all values of ![formula](img/B16182_07_03k.png),
    ![formula](img/B16182_07_03l.png), arbitrarily, except that Q(terminal, ·) = 0,
    as shown by the following code snippet, in the case of an environment with `16`
    states and `4` actions:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化![公式](img/B16182_07_03j.png)，对于所有的![公式](img/B16182_07_03k.png)，![公式](img/B16182_07_03l.png)，随意设置，唯一例外是Q(terminal,
    ·) = 0，如以下代码片段所示，在一个有`16`个状态和`4`个动作的环境中：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a loop for each episode. Initialize ![formula](img/B16182_07_03m.png)
    and choose ![formula](img/B16182_07_03n.png) from ![formula](img/B16182_07_03o.png)
    using the policy derived from Q (for example, ε-greedy). This can be done using
    the following snippet, where the initial state is provided by the environment
    `reset` function and the action is selected using a dedicated ε-greedy function:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个回合创建一个循环。初始化![公式](img/B16182_07_03m.png)，并使用从Q导出的策略（例如，ε-贪心）从![公式](img/B16182_07_03o.png)中选择![公式](img/B16182_07_03n.png)。这可以通过以下代码片段实现，其中初始状态由环境的`reset`函数提供，动作通过专门的ε-贪心函数选择：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a loop for each step of the episode. Take action ![formula](img/B16182_07_03p.png)
    and observe ![formula](img/B16182_07_03q.png). Choose ![formula](img/B16182_07_03r.png)
    from ![formula](img/B16182_07_03s.png) using the policy derived from Q (for example,
    ε-greedy). Update the state-action value function for the selected state-action
    pair using the SARSA rule, which defines the new value as the sum of the current
    one, plus the TD error multiplied by the step size, ![formula](img/B16182_07_03t.png),
    as depicted in the following expression:![Figure 7.4: Updating the state-action
    value function using the SARSA rule'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个步骤创建一个循环。执行动作![公式](img/B16182_07_03p.png)并观察![公式](img/B16182_07_03q.png)。从![公式](img/B16182_07_03s.png)中选择![公式](img/B16182_07_03r.png)，使用从Q导出的策略（例如，ε-贪心）。使用SARSA规则更新选定状态-动作对的状态-动作值函数，该规则将新值定义为当前值与TD误差乘以步长的和，![公式](img/B16182_07_03t.png)，如以下表达式所示：![图
    7.4：使用SARSA规则更新状态-动作值函数]
- en: '](img/B16182_07_04.jpg)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_04.jpg)'
- en: 'Figure 7.4: Updating the state-action value function using the SARSA rule'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：使用SARSA规则更新状态-动作值函数
- en: 'Then, update the state-action pair with the new one using ![formula](img/B16182_07_04a.png)
    until ![formula](img/B16182_07_04b.png) is a terminal state. All of this is done
    using the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用![公式](img/B16182_07_04a.png)将新状态-动作对更新至旧状态-动作对，直到![公式](img/B16182_07_04b.png)是一个终止状态。所有这些都通过以下代码实现：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The steps and code for this algorithm were originally developed and outlined
    by *Sutton, Richard S. Introduction to Reinforcement Learning. Cambridge, Mass:
    MIT Press, 2015*.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的步骤和代码最初由*Sutton, Richard S. 《强化学习导论》。剑桥，马萨诸塞州：麻省理工学院出版社，2015年*开发并概述。
- en: 'The SARSA algorithm can converge to an optimal policy and an optimal action-value
    function with probability equal to `1` under the following conditions:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下条件下，SARSA算法可以以概率`1`收敛到最优策略和最优动作值函数：
- en: All the state-action pairs need to be visited an infinite number of times.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的状态-动作对需要被访问无限多次。
- en: The policy converges in the limit to the greedy policy, which can be achieved
    with ε-greedy policies where `ε` vanishes in time (this can be done by setting
    `ε = 1/t`).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在极限情况下，该策略会收敛为贪心策略，这可以通过ε-贪心策略实现，其中`ε`随时间消失（这可以通过设置`ε = 1/t`来完成）。
- en: 'This algorithm makes use of the ε-greedy algorithm. We will explain this in
    more detail in the next chapter, so we will only briefly recall what it is here.
    When learning policies by means of state-action value functions, the value associated
    with the state-action pairs is used to decide which is the best action to take.
    At convergence, the best action is chosen among the available ones for a given
    state, and we opt for the one that has the highest value: this is the greedy approach.
    This means that for every given state, the same action will always be chosen (if
    no actions have the same value). This is not a good choice for exploration, especially
    at the beginning of training. For this reason, the ε-greedy approach is preferred
    in this phase: the best action is chosen with a probability equal to `1-ε`, while
    in the other cases, a random action is selected. By making `ε` diminishing, the
    ε-greedy approach becomes the greedy one in the limit as the number of steps approaches
    infinity.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本算法使用了 ε-greedy 算法。我们将在下一章详细解释这一点，因此这里只做简要回顾。当通过状态-动作值函数学习策略时，状态-动作对的值被用来决定采取哪个最佳动作。在收敛时，给定状态下会从可用的动作中选择最佳的那个，并选择具有最高值的动作：这就是贪婪策略。这意味着对于每个给定的状态，始终会选择相同的动作（如果没有动作具有相同的值）。这种策略对于探索来说并不是一个好选择，尤其是在训练的初期。因此，在这个阶段，优先采用
    ε-greedy 策略：最佳动作的选择概率为 `1-ε`，而其他情况下则选择一个随机动作。随着 `ε` 渐变为 0，ε-greedy 策略最终会变成贪婪策略，且当步数趋近于无穷大时，ε-greedy
    策略会趋近于贪婪策略。
- en: In order to consolidate these concepts, let's apply the SARSA control algorithm
    right away. The following exercise will show you how to implement TD(0) SARSA
    to solve the FrozenLake-v0 environment, using its deterministic version first.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固这些概念，让我们立即应用 SARSA 控制算法。以下练习将展示如何实现 TD(0) SARSA 来解决 FrozenLake-v0 环境，首先使用其确定性版本。
- en: The goal here is to see how the SARSA algorithm is able to recover the optimal
    policy, which we humans can estimate in advance, for a given configuration of
    the problem. Before jumping into it, let's quickly recap what the frozen lake
    problem is and the optimal policy we aim to make the agent find. The agent sees
    a grid world whose dimension is 4 x 4\.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是观察 SARSA 算法如何恢复最优策略，而我们人类可以提前估算出这一策略，针对给定的问题配置。在深入之前，我们先快速回顾一下冰湖问题是什么，以及我们希望代理人找到的最优策略。代理人看到的是一个
    4 x 4 的网格世界。
- en: 'The grid has a starting position, `S` (upper left-hand side), frozen tiles,
    `F`, holes, `H`, and a goal, `G` (lower right). The agent is rewarded with +1
    when it reaches the terminal goal state, while the episode ends without a reward
    if it reaches the terminal states constituted by the holes. The following table
    represents the environment:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该网格包含一个起始位置 `S`（左上角），冰冻的方块 `F`，洞 `H`，以及一个目标 `G`（右下角）。当代理人到达终极目标状态时，它会获得 +1 的奖励，而如果它到达由洞构成的终极状态，则该回合结束且没有奖励。下表表示了环境：
- en: '![Figure 7.5: The FrozenLake-v0 environment'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5：FrozenLake-v0 环境'
- en: '](img/B16182_07_05.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_05.jpg)'
- en: 'Figure 7.5: The FrozenLake-v0 environment'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：FrozenLake-v0 环境
- en: 'As you can see in the preceding diagram, `S` is the starting position, `F`
    indicates frozen tiles, `H` means holes, and `G` is the goal. For the deterministic
    environment, the optimal policy is the one that allows the agent to reach the
    goal in the shortest possible time. To be 100% precise, in this case, since, in
    this specific environment, no penalty for intermediate steps is applied, there
    is no need for the optimal path to be the shortest one. Every path that eventually
    leads to the goal is equally optimal in terms of cumulative expected reward. However,
    we will see that by appropriately using the discount factor, we will be able to
    recover the optimal policy, which also accounts for the shortest path. Under this
    condition, the optimal policy is represented in the following diagram, where each
    of the four moves (Down, Right, Left, and Up) are represented by their initial
    letter. There are two tiles for which two actions would result in the same optimal
    path:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，`S`是起始位置，`F`表示冰冻的方块，`H`表示空洞，`G`是目标。在确定性环境中，最优策略是能够让智能体在最短时间内到达目标的策略。严格来说，在这个特定环境中，由于没有对中间步骤的惩罚，因此最优路径不一定是最短的。每一条最终能够到达目标的路径在累计期望奖励方面都是同样最优的。然而，我们将看到，通过适当使用折扣因子，我们将能够恢复最优策略，而该策略也考虑了最短路径。在这种情况下，最优策略在下图中有所表示，其中每个四个动作（向下、向右、向左、向上）都由其首字母表示。有两个方块，对于它们来说，两个动作将导致相同的最优路径：
- en: '![Figure 7.6: Optimal policy'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6: 最优策略'
- en: '](img/B16182_07_06.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_06.jpg)'
- en: 'Figure 7.6: Optimal policy'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.6: 最优策略'
- en: In the preceding diagram, `D` denotes Down, `R` denotes Right, `U` denotes Up,
    and `L` denotes Left.`!` stands for the goal, and `–` refers to the holes in the
    environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，`D`表示向下，`R`表示向右，`U`表示向上，`L`表示向左。`!`代表目标，而`–`表示环境中的空洞。
- en: We will use a decreasing `ε` value in order to anneal the exploration from large
    to small, thereby making it become, in the limit, greedy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个递减的`ε`值来逐步减少探索的范围，从而使其在极限时变为贪婪的。
- en: This type of exercise is very useful when learning about classic reinforcement
    learning algorithms. Being tabular (this is a grid world example, meaning it can
    be represented by a 4x4 grid) allows us to keep track of everything that's happening
    in the domain, easily follow state-actions pairs values being updated during algorithm
    iterations, look at action choices according to the selected policy, and converge
    to the optimal policy. In this chapter, you will learn how to code a reference
    algorithm in the RL landscape and get deep hands-on experience with all these
    fundamental aspects.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的练习在学习经典强化学习算法时非常有用。由于是表格化的（这是一个网格世界示例，意味着它可以用一个 4x4 的网格表示），我们可以跟踪领域中发生的所有事情，轻松跟随在算法迭代过程中状态-动作对的值的更新，查看根据选定策略的动作选择，并收敛到最优策略。在本章中，你将学习如何在强化学习的背景下编写一个参考算法，并深入实践所有这些基本方面。
- en: Let's now move on to the implementation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续进行实现。
- en: 'Exercise 7.01: Using TD(0) SARSA to Solve FrozenLake-v0 Deterministic Transitions'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '练习 7.01: 使用 TD(0) SARSA 解决 FrozenLake-v0 确定性过渡'
- en: In this exercise, we will implement the SARSA algorithm and use it to solve
    the FrozenLake-v0 environment, where only deterministic transitions are allowed.
    This means we will look for (and actually find) the optimal policy to retrieve
    the frisbee in this environment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将实现SARSA算法，并用它来解决FrozenLake-v0环境，在该环境中仅允许确定性过渡。这意味着我们将寻找（并实际找到）一个最优策略，以便在这个环境中取回飞盘。
- en: 'The following steps will help you to complete this exercise:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: 'Import the required modules:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块：
- en: '[PRE4]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Instantiate the `gym` environment called `FrozenLake-v0`. Set the `is_slippery`
    flag to `False` to disable its stochasticity:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个名为`FrozenLake-v0`的`gym`环境。将`is_slippery`标志设置为`False`以禁用其随机性：
- en: '[PRE5]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Take a look at the action and the observation spaces:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下动作空间和观察空间：
- en: '[PRE6]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will print out the following:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '[PRE7]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create two dictionaries to easily translate action numbers into moves:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个字典，以便轻松将动作编号转换为动作：
- en: '[PRE8]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Reset the environment and render it to be able to take a look at the grid problem:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并渲染它，以便能够查看网格问题：
- en: '[PRE9]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be as follows:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.7: Environment''s initial state'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.7: 环境的初始状态'
- en: '](img/B16182_07_07.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_07.jpg)'
- en: 'Figure 7.7: Environment''s initial state'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 7.7: 环境的初始状态'
- en: 'Visualize the optimal policy for this environment:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化该环境的最优策略：
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output will be as follows:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This represents the optimal policy for this environment, showing, for each
    of the environment states represented in the 4x4 grid, the optimal action among
    the four available: *move Up*, *move Down*, *move Right*, and *move Left*. Except
    for two states, all the others have a single optimal action associated with them.
    In fact, as described previously, optimal actions here are those that bring the
    agent to the goal using the shortest possible path. Two different possibilities
    result in the same path length for two states, so they are both equally optimal.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表示该环境的最优策略，显示在4x4网格中表示的每个环境状态下，在四个可用动作中选择的最优动作：*向上移动*，*向下移动*，*向右移动*，和*向左移动*。除了两个状态外，所有其他状态都有唯一的最优动作。实际上，如前所述，最优动作是那些通过最短路径将智能体带到目标的动作。两个不同的可能性为两个状态产生相同的路径长度，因此它们是同样的最优解。
- en: 'Define functions to take ε-greedy actions. The first function implements the
    ε-greedy policy with a probability of `1 - ε`. The chosen action is the one with
    the highest value associated with the state-action pair; otherwise, a random action
    is returned. The second function simply makes the first callable when it''s passed
    as an argument by using a `lambda` function:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义函数来执行ε-贪心动作。第一个函数实现了一个具有`1 - ε`概率的ε-贪心策略。选择的动作是与状态-动作对关联的最大值所对应的动作；否则，返回一个随机动作。第二个函数仅通过`lambda`函数在传递时调用第一个函数：
- en: '[PRE12]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define a function to take greedy actions:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行贪婪动作：
- en: '[PRE13]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, define a function that will calculate the mean of the agent''s performances.
    First, we''ll define the number of episodes used to calculate the average performance
    (in this case, `500`), and then execute all these episodes in a loop. We''ll reset
    the environment and start the in-episode loop to do so. We then select an action
    according to the policy that we want to measure the performance of, step through
    the environment with the chosen action, and finally add the reward to the accumulated
    returns. We repeat these environment steps until the episode is complete:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个函数来计算智能体表现的平均值。首先，我们将定义用于计算平均表现的集数（在此例中为`500`），然后在循环中执行所有这些集数。我们将重置环境并开始该集中的循环以进行此操作。接着，我们根据要衡量表现的策略选择一个动作，使用所选动作推进环境，最后将奖励添加到累积回报中。我们重复这些环境步骤，直到集数完成：
- en: '[PRE14]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set the number of total episodes and number of steps specifying how often the
    agent''s average performance is estimated, as well as the `ε` parameters, which
    determine its decrease. Use the starting value, minimum value, and range (in terms
    of the number of episodes) over which the decrease is spread:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置总集数和步骤数，指定估算智能体的平均表现的频率，并设置`ε`参数，该参数决定其衰减方式。使用初始值、最小值和衰减范围（以集数为单位）：
- en: '[PRE15]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the SARSA training algorithm as a function. In this step, the Q-table
    is initialized. All the values are equal to `1`, but the values at terminal states
    are set equal to `0`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将SARSA训练算法定义为一个函数。在此步骤中，Q表被初始化。所有的值都等于`1`，但终止状态的值被设置为`0`：
- en: '[PRE16]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Start a `for` loop among all the episodes:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有集数中开始一个`for`循环：
- en: '[PRE17]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Inside the loop, first, the epsilon value is defined, depending on the current
    episode number:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环内，首先根据当前集数定义epsilon值：
- en: '[PRE18]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, the environment is reset, and the first action is chosen with an ε-greedy
    policy:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，重置环境，并使用ε-贪心策略选择第一个动作：
- en: '[PRE19]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we start an in-episode loop:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们开始一个集内循环：
- en: '[PRE20]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Inside the loop, the environment is stepped throughout using the selected action
    and the new state and the reward, and the done conditions are retrieved:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环内，环境通过所选动作和新状态以及奖励进行推进，并获取done条件：
- en: '[PRE21]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Select a new action with the ε-greedy policy, update the Q-table with the SARSA
    TD(0) rule, and update the state and action with their new values:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个新的动作，使用ε-贪心策略，通过SARSA TD(0)规则更新Q表，并更新状态和动作的值：
- en: '[PRE22]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, the agent''s average performance is estimated:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，估算智能体的平均表现：
- en: '[PRE23]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It may be useful to provide a brief description of the `ε` parameter''s decrease.
    This is determined by three parameters: the starting value, minimum value, and
    decrease range (called `epsilon_annealing_stop`). They are used in the following
    way: `ε` starts at the starting value, and then it is decreased linearly across
    the number of episodes defined by the parameter''s "range" until it reaches the
    minimum value, which is then kept constant.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供`ε`参数减少的简要描述可能会有所帮助。这由三个参数决定：起始值、最小值和减少范围（称为`epsilon_annealing_stop`）。它们的使用方式如下：`ε`从起始值开始，然后在由参数“范围”定义的回合数中线性递减，直到达到最小值，之后保持不变。
- en: 'Define an array that will collect all agent performance evaluations during
    training and the execution of SARSA TD(0) training:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个数组，用于在训练过程中收集所有智能体的性能评估，以及SARSA TD(0)训练的执行过程：
- en: '[PRE24]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Plot the SARSA agent''s average reward history during training:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制SARSA智能体在训练过程中平均奖励的历史记录：
- en: '[PRE25]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This generates the following output:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE26]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The plot for this can be visualized as follows. This shows the learning progress
    of the SARSA algorithm:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以通过以下方式进行可视化。它展示了SARSA算法的学习进展：
- en: '![Figure 7.8: Average reward of an epoch trend over training epochs'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.8：训练过程中每个周期的平均奖励趋势'
- en: '](img/B16182_07_08.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_08.jpg)'
- en: 'Figure 7.8: Average reward of an epoch trend over training epochs'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.8：训练过程中每个周期的平均奖励趋势
- en: As we can see, SARSA's performance grows over time as the `ε` parameter is annealed,
    thus reaching the value of `0` in the limit, thereby obtaining the greedy policy.
    This also demonstrates that the algorithm is capable of reaching 100% success
    after learning.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，随着`ε`参数的退火，SARSA的表现随着时间的推移不断提升，从而在极限时达到了`0`的值，从而获得了贪心策略。这也证明了该算法在学习后能够达到100%的成功率。
- en: 'Evaluate the greedy policy''s performance of the trained agent (Q-table):'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估经过训练的智能体（Q表）在贪心策略下的表现：
- en: '[PRE27]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output will be as follows:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE28]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Display the Q-table values:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示Q表的值：
- en: '[PRE29]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output will be as follows:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE30]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This output shows the values of the complete state-action value function for
    our problem. These values are then used to generate the optimal policy by means
    of the greedy selection rule.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该输出展示了我们问题的完整状态-动作值函数的值。这些值随后用于通过贪心选择规则生成最优策略。
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy. Having calculated the state-action value function, we are able to retrieve
    the greedy policy from it. In fact, as explained previously, the greedy policy
    chooses the action that, for a given state, is associated with the maximum value
    of the Q-table. For this purpose, we are using the `argmax` function. When applied
    to each of the 16 states (from 0 to 15), it returns the index of the four actions
    (from 0 to 3) with the highest associated value for that state. Here, we also
    directly output the label associated with the action index using the pre-built
    dictionary:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出找到的贪心策略并与最优策略进行比较。在计算出状态-动作值函数后，我们可以从中提取出贪心策略。事实上，正如前面所解释的，贪心策略选择的是对于给定状态，Q表中与之关联的最大值所对应的动作。为此，我们使用了`argmax`函数。将其应用于16个状态（从0到15）中的每一个时，它返回与该状态相关的四个动作（从0到3）中具有最大值的动作索引。在这里，我们还直接使用预先构建的字典输出与动作索引相关的标签：
- en: '[PRE31]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE32]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As the preceding output shows, the TD(0) SARSA algorithm we implemented has
    been able to successfully learn the optimal policy for this task just by interacting
    with the environment and collecting experience of it through episodes and then
    adopting the SARSA state-action pair value function update rule that was defined
    in the *SARSA – On-Policy Control* section. In fact, as we can see, for every
    state of the environment, the greedy policy that was obtained with the Q-table
    calculated by our algorithm prescribes an action that is in accordance with the
    optimal policy that was defined for analyzing the environment problem. As we already
    saw, there are two states in which there are two equally optimal actions and the
    agent correctly implements one of them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的输出所示，我们实现的TD(0) SARSA算法仅通过与环境交互并通过回合收集经验，然后采用在*SARSA – On-Policy Control*部分中定义的SARSA状态-动作对值函数更新规则，成功地学到了该任务的最优策略。实际上，正如我们所看到的，对于环境中的每个状态，我们算法计算的Q表获得的贪心策略所指定的动作与为分析环境问题而定义的最优策略一致。如我们之前所见，在两个状态中有两个同样最优的动作，智能体能够正确地执行其中之一。
- en: Note
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fJBLBh](https://packt.live/3fJBLBh).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3fJBLBh](https://packt.live/3fJBLBh)。
- en: You can also run this example online at [https://packt.live/30XeOXj](https://packt.live/30XeOXj).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在[https://packt.live/30XeOXj](https://packt.live/30XeOXj)在线运行这个示例。
- en: The Stochasticity Test
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机性测试
- en: 'Now, let''s take a look at what happens if the stochasticity is enabled in
    the FrozenLake-v0 environment. Enabling the stochasticity for this task means
    that every transition for a selected action is no longer deterministic. In particular,
    for a given action, there is a one in three chances that the action is executed
    as intended and 2 out of 3 equally distributed chances (1/3 and 1/3 each) for
    the two neighboring actions. Zero probability is assigned to the action in the
    opposite direction. So, for example, if the Down action is set, the agent will
    move down 1/3 of the time, move right 1/3 of the time, and move left the remaining
    1/3 of the time, never going up, as shown in the following diagram:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如果在FrozenLake-v0环境中启用随机性会发生什么。为这个任务启用随机性意味着每个选定动作的转移不再是确定性的。具体来说，对于一个给定的动作，有三分之一的概率该动作会按预期执行，而两个相邻动作的概率各占三分之一和三分之一。反方向的动作则没有任何概率。因此，例如，如果设置了下（Down）动作，智能体会有三分之一的时间向下移动，三分之一的时间向右移动，剩下的三分之一时间向左移动，而绝不会向上移动，如下图所示：
- en: '![Figure 7.9: Percentages for the resulting states if the Down action'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9：如果从中心瓦片执行下（Down）动作时，各个结果状态的百分比'
- en: is taken from the central tile
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 取自中心瓦片
- en: '](img/B16182_07_09.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_09.jpg)'
- en: 'Figure 7.9: Percentages for the resulting states if the Down action is taken
    from the central tile'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：如果从中心瓦片执行下（Down）动作时，各个结果状态的百分比
- en: 'The environment setting is the very same as it was for the FrozenLake-v0 deterministic
    case we saw previously. Again, we want the SARSA algorithm to recover the optimal
    policy. This can be estimated in advance in this case as well. Just to make the
    reasoning for this easier, here''s the table representing this environment:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 环境设置与我们之前看到的FrozenLake-v0确定性案例完全相同。同样，我们希望SARSA算法恢复最优策略。在这种情况下，这也可以事先进行估算。为了使推理更容易，这里有一个表示该环境的表格：
- en: '![Figure 7.10: Problem setting'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.10：问题设置'
- en: '](img/B16182_07_10.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_10.jpg)'
- en: 'Figure 7.10: Problem setting'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：问题设置
- en: 'In the preceding diagram, `S` is the starting position, `F` indicates frozen
    tiles, `H` indicate the holes, and `G` is the goal. For the stochastic environment,
    the optimal policy is very different with respect to the one that corresponds
    to the deterministic case, and it may even appear counter-intuitive. The key point
    is that in order to keep the possibility of obtaining a reward alive, our only
    chance is to avoid falling into the holes. Since there is no penalty for intermediate
    steps, we can keep going around for as long as we need to. And the only certain
    way to do so is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，`S`是起始位置，`F`表示冰冻瓦片，`H`表示坑洞，`G`是目标。对于随机环境，最优策略与对应的确定性情况有很大不同，甚至可能显得违背直觉。关键点是，为了保持获得奖励的可能性，我们唯一的机会就是避免掉入坑洞。由于中间步骤没有惩罚，我们可以继续绕行，只要我们需要。唯一确定的做法如下：
- en: Move in the opposite direction of the hole we find next to us, even if this
    means moving away from the goal.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动到我们下一个发现的坑洞的反方向，即使这意味着远离目标。
- en: 'Avoid, in every possible way, falling into those tiles where there is a chance
    greater than 0 of falling into a hole:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以各种可能的方式避免掉入那些有可能大于0的坑洞的瓦片：
- en: '![Figure 7.11: Environment setup (A), action executed by the agent (B), and
    chances of ending near the starting state in each position (C)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.11：环境设置（A），智能体执行的动作（B），以及在每个位置结束时接近起始状态的概率（C）'
- en: '](img/B16182_07_11.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_11.jpg)'
- en: 'Figure 7.11: Environment setup (A), action executed by the agent (B), and chances
    of ending near the starting state in each position (C)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：环境设置（A），智能体执行的动作（B），以及在每个位置结束时接近起始状态的概率（C）
- en: For example, let's consider the first tile on the left, in the second row from
    the top, in our problem setting, as shown in table `B` in the preceding diagram.
    In the deterministic case, the optimal action was to go down because it would
    bring us closer to the goal. In this case, instead, the best action to choose
    is to move left, even if moving left means bouncing into the wall. This is because
    moving left is the only action that won't make us fall into the hole. In addition,
    there is a 33% probability that we will end up in the tile on the third row from
    above, thereby getting closer to the goal.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们问题设置中左侧第二行的第一个瓦片，如前图表`B`所示。在确定性情况下，最优行动是向下移动，因为这样能使我们更接近目标。而在这个案例中，最佳选择是向左移动，即使向左意味着会碰到墙壁。这是因为向左是唯一不会让我们掉进坑里的行动。此外，有33%的概率我们会最终到达第三行的瓦片，从而更接近目标。
- en: Note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: The preceding behavior follows a standard boundary implementation. In that tile,
    you execute the action "Move Left" (which is a completely legal action) and the
    environment will understand that this results in "bouncing." The algorithm simply
    sends a "move left" action to the environment, which, in turn, will take the prescribed
    action into account.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述行为遵循了标准的边界实现。在该瓦片中，你执行“向左移动”这个完全合法的动作，环境会理解为“碰壁”。算法只会向环境发送一个“向左移动”的指令，环境会根据这个指令采取相应的行动。
- en: 'Similar reasoning can be applied to all the other tiles while keeping the key
    point mentioned previously in mind. However, it is worth discussing a very peculiar
    case – one that''s the only reason why we cannot achieve 100% success, even with
    the optimal policy:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的推理可以应用到其他所有瓦片，同时要记住前面提到的关键点。然而，值得讨论一个非常特殊的情况——这是我们无法实现100%成功的唯一原因，即使使用最优策略：
- en: '![Figure 7.12: Environment setup (A), "Move to the Left" action executed by
    the agent (B), and chances of ending near the starting state in each position
    (C)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.12：环境设置（A）、代理执行“向左移动”动作（B）、以及每个位置结束时接近起始状态的概率（C）'
- en: '](img/B16182_07_12.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_12.jpg)'
- en: 'Figure 7.12: Environment setup (A), "Move to the Left" action executed by the
    agent (B), and chances of ending near the starting state in each position (C)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：环境设置（A）、代理执行“向左移动”动作（B）、以及每个位置结束时接近起始状态的概率（C）
- en: Now, let's take a look at the third tile from the left in the second row from
    the top in our problem setting, as shown in table B in the preceding diagram.
    This tile is between two holes, so there is no way to take an action that is 100%
    safe. Here, the best action is actually to move toward either the left or the
    right hole! This is because by moving left or right, we have a 66% chance of moving
    up or down, and only a 33% chance of falling into the hole. Moving up or down
    means we would have a 66% chance of moving right or left, falling into the hole,
    and only a 33% chance of actually moving up or down. And since this tile is the
    reason why we cannot achieve maximum performance 100% of the time, the best thing
    is to avoid reaching that tile. In order to do so, optimal actions of the very
    first row, apart from the starting tile, are all pointing up so that it is not
    possible to land on the problematic tile.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下我们问题设置中左侧第二行的第三个瓦片，如前图表B所示。这个瓦片位于两个坑之间，因此没有任何行动是100%安全的。在这里，最佳行动实际上是向左或向右的坑移动！这是因为，向左或向右移动，我们有66%的机会向上或向下移动，只有33%的机会掉进坑里。向上或向下移动意味着我们有66%的机会向右或向左移动，掉进坑里，只有33%的机会真正向上或向下移动。由于这个瓦片是我们无法在100%情况下实现最佳表现的原因，最好的办法是避免到达这个瓦片。为了做到这一点，除了起始瓦片外，最优策略的第一行所有的行动都指向上方，以避免落到这个有问题的瓦片上。
- en: 'All other values are constrained by the hole''s proximity, except for the tile
    on the left of the goal: the optimal action choice for this tile is to move down
    since it maintains the chance of landing in the goal, while at the same time avoiding
    landing in the tile above it, where, in turn, the agent would be forced to move
    left to avoid the hole, thus risking landing on the tile between the two holes.
    The optimal policy is summarized in the following diagram:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了目标左侧的瓦片，所有其他值都受到坑的邻近影响：对于这个瓦片，最优行动选择是向下移动，因为这保持了到达目标的机会，同时避免落到上面的瓦片，在那里，代理会被迫向左移动以避免掉进坑里，从而冒着掉到两个坑之间的瓦片的风险。最优策略总结如下图所示：
- en: '![Figure 7.13: Optimal policy'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.13：最优策略'
- en: '](img/B16182_07_13.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_13.jpg)'
- en: 'Figure 7.13: Optimal policy'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：最优策略
- en: The preceding diagram displays the optimal policy of the environment explained
    previously, where `D` denotes a Down move, `R` denotes a Right move, `U` denotes
    an Up move, and `L` denotes a Left move.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了先前解释的环境的最优策略，其中 `D` 表示向下移动，`R` 表示向右移动，`U` 表示向上移动，`L` 表示向左移动。
- en: In the following example, we will use the SARSA algorithm to solve this new
    flavor of the FrozenLake-v0 environment. In order to obtain the optimal policy
    we just described, we need to adjust our hyperparameters – in particular, the
    discount factor, ![a](img/B16182_07_13a.png). In fact, we want to give the agent
    the freedom to make however many steps they need to. In order to do so, we have
    to propagate the value of the goal backward so that all the trajectories in the
    goal will benefit from it, even if those trajectories are not the shortest ones.
    For this reason, we will use a discount factor equal (or very close) to `1`. In
    code, this means that instead of using `gamma = 0.9,` we will use `gamma = 1`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们将使用 SARSA 算法来解决这个新版本的 FrozenLake-v0 环境。为了获得我们刚才描述的最优策略，我们需要调整我们的超参数
    —— 特别是折扣因子，![a](img/B16182_07_13a.png)。事实上，我们希望给予代理人足够多的步骤自由度。为了做到这一点，我们必须向目标传播价值，以便所有目标中的轨迹都能从中受益，即使这些轨迹不是最短的。因此，我们将使用一个接近`1`的折扣因子。在代码中，这意味着我们将使用`gamma
    = 1`，而不是`gamma = 0.9`。
- en: Now, let's see our SARSA algorithm working in this stochastic environment.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的 SARSA 算法在这个随机环境中的工作。
- en: 'Exercise 7.02: Using TD(0) SARSA to Solve FrozenLake-v0 Stochastic Transitions'
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.02：使用 TD(0) SARSA 解决 FrozenLake-v0 随机转移问题
- en: In this exercise, we'll use the TD(0) SARSA algorithm to solve the FrozenLake-v0
    environment, with stochastic transitions enabled. As we just saw, the optimal
    policy looks completely different with respect to the previous exercise since
    it needs to take care of the stochasticity factor. This imposes a new challenge
    for the SARSA algorithm, and we will see how it will still be able to solve this
    task. This exercise will show us how these sound TD methods are able to deal with
    different challenges, demonstrating a notable robustness.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 TD(0) SARSA 算法来解决 FrozenLake-v0 环境，并启用随机转移。正如我们刚才看到的，由于需要考虑随机性因素，最优策略看起来与之前的练习完全不同。这给
    SARSA 算法带来了新的挑战，我们将看到它如何仍然能够解决这个任务。这个练习将展示给我们这些健壮的 TD 方法如何处理不同的挑战，展示出了显著的鲁棒性。
- en: 'Follow these steps to complete this exercise:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成此练习：
- en: 'Import the required modules:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块：
- en: '[PRE33]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Instantiate the `gym` environment called `FrozenLake-v0` using the `is_slippery`
    flag set to `True` in order to enable stochasticity:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 `gym` 环境，称为 `FrozenLake-v0`，使用 `is_slippery` 标志设置为 `True` 以启用随机性：
- en: '[PRE34]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Take a look at the action and the observation spaces:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看动作和观察空间：
- en: '[PRE35]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be as follows:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE36]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create two dictionaries to easily map the `actions` indices (from `0` to `3`)
    to the labels (Left, Down, Right, and Up):'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个字典，以便轻松地将 `actions` 索引（从 `0` 到 `3`）映射到标签（左、下、右和上）：
- en: '[PRE37]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Reset the environment and render it to take a look at the grid problem:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并渲染以查看网格问题：
- en: '[PRE38]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be as follows:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.14: Environment''s initial state'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.14：环境的初始状态'
- en: '](img/B16182_07_14.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16182_07_14.jpg](img/B16182_07_14.jpg)'
- en: 'Figure 7.14: Environment''s initial state'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.14：环境的初始状态
- en: 'Visualize the optimal policy for this environment:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化此环境的最优策略：
- en: '[PRE39]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be as follows:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE40]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This represents the optimal policy for this environment. Except from two states,
    all the other ones have a single optimal action associated with them. In fact,
    as described previously, optimal actions here are those that bring the agent away
    from the holes or from tiles that have a chance greater than zero of leading the
    agent to tiles placed near holes. Two states have multiple optimal actions associated
    with them that are all equally optimal, as intended for this task.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这代表了此环境的最优策略。除了两个状态外，所有其他状态都有一个与之关联的单一最优动作。事实上，正如先前描述的，这里的最优动作是将代理人远离洞穴或具有导致代理人移动到靠近洞穴的几率大于零的瓦片的动作。两个状态有多个与之关联的同等最优动作，这正是此任务的意图。
- en: 'Define functions that will take ε-greedy actions:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义函数来执行 ε-greedy 动作：
- en: '[PRE41]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The first function implements the ε-greedy policy: with a probability of `1
    – ε`, the chosen action is the one with the highest value associated with the
    state-action pair; otherwise, a random action is returned. The second function
    simply makes the first callable when passed as an argument using a `lambda` function.'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个函数实现了ε-贪婪策略：以`1 - ε`的概率，选择与状态-动作对关联的最高值的动作；否则，返回一个随机动作。第二个函数通过`lambda`函数传递时，简单地调用第一个函数。
- en: 'Define a function that will take greedy actions:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于执行贪婪策略：
- en: '[PRE42]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define a function that will calculate average agent performance:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于计算代理的平均性能：
- en: '[PRE43]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Set the number of total episodes, the number of steps representing the interval
    by which the agent''s average performance is evaluated and the `ε` parameters,
    ruling its decrease, that is, the starting value, minimum value, and range (in
    terms of the number of episodes) over which the decrease is spread:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置总回合数，表示评估代理平均性能的间隔步数的步数，以及控制`ε`参数减少的参数，即起始值、最小值和范围（以回合数表示）：
- en: '[PRE44]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the SARSA training algorithm as a function. Initialize the Q-table with
    all the values equal to `1`, but with the values at terminal states set equal
    to `0`:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将SARSA训练算法定义为一个函数。初始化Q表时，所有值设置为`1`，但终止状态的值设置为`0`：
- en: '[PRE45]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Start a loop among all episodes:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有回合之间开始一个循环：
- en: '[PRE46]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Inside the loop, first, define the epsilon value, depending on the current
    episode number. Reset the environment and make sure that the first action is chosen
    with an ε-greedy policy:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环内，首先根据当前的回合数定义epsilon值。重置环境，并确保第一个动作是通过ε-贪婪策略选择的：
- en: '[PRE47]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, start an in-episode loop:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始一个回合内的循环：
- en: '[PRE48]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Inside the loop, step throughout the environment using the selected action
    and ensure that the new state, the reward, and the done conditions are retrieved:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环内，使用选定的动作在环境中执行步骤，并确保获取到新状态、奖励和完成条件：
- en: '[PRE49]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Select a new action with the ε-greedy policy, update the Q-table with the SARSA
    TD(0) rule, and ensure that the state and action are updated with their new values:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ε-贪婪策略选择一个新的动作，使用SARSA TD(0)规则更新Q表，并确保状态和动作更新为其新值：
- en: '[PRE50]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, estimate the agent''s average performance:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，估算代理的平均性能：
- en: '[PRE51]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'It may be useful to provide a brief description of the `ε` parameter''s decrease.
    It is ruled by three parameters: starting value, minimum value, and decrease range.
    They are used in the following way: `ε` starts at the starting value, and then
    it is decreased linearly across the number of episodes defined by the parameter''s
    "range" until it reaches the minimum value, which is then kept constant.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供关于`ε`参数减小的简要描述可能会很有用。它受三个参数的控制：起始值、最小值和减小范围。它们的使用方式如下：`ε`从起始值开始，然后在由参数“范围”定义的集数内线性减小，直到达到最小值，并保持该值不变。
- en: 'Define an array that will collect all agent performance evaluations during
    training and the execution of SARSA TD(0) training:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个数组，用来在训练和执行SARSA TD(0)训练期间收集所有代理的性能评估：
- en: '[PRE52]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Plot the SARSA agent''s mean reward history during training:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制SARSA代理在训练期间的平均奖励历史：
- en: '[PRE53]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This generates the following output, showing the learning progress for the
    SARSA algorithm:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出，展示了SARSA算法的学习进度：
- en: '[PRE54]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The plot will be as follows:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图将如下所示：
- en: '![Figure 7.15: Average reward of an epoch trend over training epochs'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.15：训练回合期间一个周期的平均奖励趋势'
- en: '](img/B16182_07_15.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_15.jpg)'
- en: 'Figure 7.15: Average reward of an epoch trend over training epochs'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.15：训练回合期间一个周期的平均奖励趋势
- en: This plot clearly shows us how the performance of the SARSA algorithm improves
    over epochs, even when stochastic dynamics are considered. The sudden performance
    drop around 60k epochs is completely normal when dealing with methods in which
    random exploration plays a major role, and especially when random transition dynamics
    are part of the environment, as in this case.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图清楚地展示了即使在考虑随机动态的情况下，SARSA算法的性能是如何随回合数提升的。在约60k回合时性能的突然下降是完全正常的，尤其是在随机探索起主要作用，且随机过渡动态是环境的一部分时，正如在这个案例中所展示的那样。
- en: 'Evaluate the greedy policy''s performance regarding the trained agent (Q-table):'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估贪婪策略在训练代理（Q表）下的表现：
- en: '[PRE55]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output will be as follows:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE56]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Display the Q-table values:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示Q表的值：
- en: '[PRE57]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following output will be generated:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将生成以下输出：
- en: '[PRE58]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This output shows the values of the complete state-action value function for
    our problem. These values are then used to generate the optimal policy by means
    of the greedy selection rule.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该输出显示了我们问题的完整状态-动作值函数的值。这些值随后通过贪婪选择规则来生成最优策略。
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy. Having calculated the state-action value function, we are able to retrieve
    the greedy policy from it. In fact, as explained previously, the greedy policy
    chooses the action that, for a given state, is associated with the maximum value
    of the Q-table. For this purpose, we are using the `argmax` function. When applied
    to each of the 16 states (from 0 to 15), it returns the index of the action that,
    among the four available (from 0 to 3), has the highest associated value for that
    state. Here, we also directly output the label associated with the action index
    using the pre-built dictionary:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出找到的贪婪策略，并与最优策略进行比较。计算出状态-动作值函数后，我们能够从中提取出贪婪策略。事实上，正如之前所解释的，贪婪策略会选择在给定状态下与
    Q 表中最大值关联的动作。为此，我们使用了 `argmax` 函数。当该函数应用于 16 个状态（从 0 到 15）时，它返回的是在四个可用动作（从 0 到
    3）中，哪个动作与该状态的最大值关联的索引。在这里，我们还通过预先构建的字典直接输出与动作索引关联的标签：
- en: '[PRE59]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output will be as follows:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE60]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As you can see, as in the previous exercise, our algorithm has been able to
    find the optimal policy by simply exploring the environment, and even in the context
    of stochastic environment transitions. As anticipated, for this setting, it is
    not possible to achieve the maximum reward 100% of the time. In fact, as we can
    see, for every state of the environment the greedy policy obtained with the Q-table
    that is calculated by our algorithm, it prescribes an action that is in accordance
    with the optimal policy that was defined by analyzing the environment problem.
    As we already saw, there are two states in which there are many different actions
    that are equally optimal, and the agent correctly implements one of them.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，和之前的练习一样，我们的算法通过简单地探索环境，成功找到了最优策略，即使在环境转移是随机的情况下。正如预期的那样，在这种设置下，不可能 100%
    的时间都达到最大奖励。事实上，正如我们所看到的，对于环境中的每个状态，通过我们的算法计算出的 Q 表所获得的贪婪策略都建议一个与通过分析环境问题定义的最优策略一致的动作。如我们之前所见，有两个状态中有许多不同的动作是同样最优的，代理正确地执行了其中之一。
- en: Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3eicsGr](https://packt.live/3eicsGr).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/3eicsGr](https://packt.live/3eicsGr)。
- en: You can also run this example online at [https://packt.live/2Z4L1JV](https://packt.live/2Z4L1JV).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/2Z4L1JV](https://packt.live/2Z4L1JV) 在线运行此示例。
- en: Now that we've become familiar with on-policy control, it is time for us to
    change track and look at off-policy control, an early breakthrough in reinforcement
    learning dating back to 1989 known as Q-learning.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了在策略控制，是时候转向离策略控制了，这是强化学习中的一次早期突破，追溯到 1989 年的 Q-learning。
- en: Note
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Q-learning algorithm was first formulated by *Watkins in Mach Learn 8, 279–292
    (1992)*. Here, we are presenting only an intuitive understanding, along with a
    brief mathematical description of it. For a much more detailed mathematical discussion,
    please refer to the original paper at [https://link.springer.com/article/10.1007/BF00992698](https://link.springer.com/article/10.1007/BF00992698).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 算法最早由 *Watkins 在 Mach Learn 8, 279–292 (1992)* 提出。在这里，我们仅提供一个直观理解，以及简要的数学描述。有关更详细的数学讨论，请参阅原始论文
    [https://link.springer.com/article/10.1007/BF00992698](https://link.springer.com/article/10.1007/BF00992698)。
- en: Q-Learning – Off-Policy Control
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q-learning – 离策略控制
- en: 'Q-learning is a name that identifies the family of off-policy control temporal
    difference algorithms. From a mathematical/implementation point of view, the only
    difference compared with on-policy algorithms is in the rule used to update the
    Q-table (or function for approximated methods), which is defined as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是识别一类离策略控制时序差分算法的名称。从数学/实现的角度来看，与在策略算法相比，唯一的区别在于用于更新 Q 表（或近似方法的函数）的规则，具体定义如下：
- en: '![Figure 7.16: Function for approximated methods'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.16：近似方法的函数'
- en: '](img/B16182_07_16.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_16.jpg)'
- en: 'Figure 7.16: Function for approximated methods'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：近似方法的函数
- en: The key point regards how the action for the next state, ![a](img/B16182_07_16a.png),
    is chosen. In fact, choosing the action with the maximum state-action value directly
    approximates what happens when the optimal Q value is found and the optimal policy
    is followed. Moreover, it is independent of the policy used to collect experience
    while interacting with the environment. The exploration policy can be entirely
    different to the optimal one; for example, it can be an ε-greedy policy to encourage
    exploration, and, under some easy-to-satisfy assumptions, it has been proven that
    Q converges to the optimal values.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于如何为下一个状态选择动作，![a](img/B16182_07_16a.png)。实际上，选择具有最大状态-动作值的动作直接近似了找到最优Q值并遵循最优策略时发生的情况。此外，它与用于收集经验的策略无关，而是在与环境互动时得到的。探索策略可以与最优策略完全不同；例如，它可以是ε-greedy策略以鼓励探索，并且在一些容易满足的假设下，已经证明Q会收敛到最优值。
- en: 'In *Chapter 9, What Is Deep Q-Learning?*, you will look at the extension of
    this approach to non-tabular methods where we use deep neural networks as function
    approximators. This method is called deep Q-learning. A scheme for the Q-learning
    control algorithm can be depicted as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第9章，什么是深度Q学习？*中，你将研究这种方法在非表格化方法中的扩展，我们使用深度神经网络作为函数近似器。这种方法叫做深度Q学习。Q-learning控制算法的方案可以如下所示：
- en: 'Choose the algorithm parameters: the step size, ![d](img/B16182_07_16b.png),
    which has to be contained in the interval (0, 1], and the `ε` parameter of the
    ε-greedy policy, which has to be small and greater than `0` since it represents
    the probability of choosing the non-optimal action in order to favor exploration:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择算法参数：步长，![d](img/B16182_07_16b.png)，它必须位于区间(0, 1]内，以及ε-greedy策略的`ε`参数，它必须较小且大于`0`，因为它表示选择非最优动作的概率，以促进探索：
- en: '[PRE61]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Initialize ![b](img/B16182_07_16c.png), for all ![c](img/B16182_07_16d.png),
    ![e](img/B16182_07_16e.png), arbitrarily, except that Q(terminal, *) = 0:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有![c](img/B16182_07_16d.png)，![e](img/B16182_07_16e.png)，![b](img/B16182_07_16c.png)进行初始化，任意设置，除了Q(terminal,
    *) = 0：
- en: '[PRE62]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create a loop among all episodes. In the loop, initialize `s`:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有回合中创建一个循环。在该循环中，初始化`s`：
- en: '[PRE63]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create a loop for each step of the episode. Within that loop, choose ![g](img/B16182_07_16f.png)
    from ![f](img/B16182_07_16g.png) using the policy derived from Q (for example,
    ε-greedy):'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个回合创建一个循环。在该循环中，使用从Q派生的策略（例如，ε-greedy）从![f](img/B16182_07_16g.png)中选择![g](img/B16182_07_16f.png)：
- en: '[PRE64]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Taking action, ![h](img/B16182_07_16h.png), observe ![i](img/B16182_07_16i.png).
    Update the state-action value function for the selected state-action pair using
    the Q-learning rule, which defines the new value as the sum of the current one,
    plus the off-policy-specific TD error multiplied by the step size, ![j](img/B16182_07_16j.png).
    This can be expressed as follows:![Figure 7.17: Expression for the updated state-action
    value function'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作，![h](img/B16182_07_16h.png)，观察![i](img/B16182_07_16i.png)。使用Q-learning规则更新所选状态-动作对的状态-动作值函数，该规则将新值定义为当前值加上与步长![j](img/B16182_07_16j.png)相乘的与离策略相关的TD误差。可以表示如下：![图
    7.17：更新的状态-动作值函数的表达式
- en: '](img/B16182_07_17.jpg)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_17.jpg)'
- en: 'Figure 7.17: Expression for the updated state-action value function'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：更新的状态-动作值函数的表达式
- en: 'The preceding explanation translates into code as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的解释可以通过代码实现如下：
- en: '[PRE65]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As we can see, we just substituted the random choice of the action to be taken
    on the new state with the action associated with the maximum q-value. This (apparently)
    minor change, which can be easily implemented by adapting the SARSA algorithm,
    has a relevant impact on the nature of the method. We'll see it at work in the
    following exercise.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们只是将新状态下采取动作的随机选择替换为与最大q值相关的动作。这种（看似）微小的变化，可以通过适配SARSA算法轻松实现，但对方法的性质有着重要影响。我们将在接下来的练习中看到它的效果。
- en: 'Exercise 7.03: Using TD(0) Q-Learning to Solve FrozenLake-v0 Deterministic
    Transitions'
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习7.03：使用TD(0) Q-Learning解决FrozenLake-v0的确定性转移问题
- en: In this exercise, we'll implement the TD(0) Q-learning algorithm to solve the
    FrozenLake-v0 environment, where only deterministic transitions are allowed. In
    this exercise, we will consider the same task of retrieving the frisbee with the
    optimal policy we addressed in *Exercise 7.01, Using TD(0) SARSA to Solve FrozenLake-v0
    Deterministic Transitions*, but this time, instead of using the SARSA algorithm
    (on-policy), we will implement Q-learning (off-policy). We will see how this algorithm
    behaves and train ourselves in implementing a new approach to estimate a q-value
    table by means of recovering an optimal policy for our agent.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实现TD(0) Q学习算法来解决FrozenLake-v0环境，其中只允许确定性转移。在本练习中，我们将考虑与*练习7.01，使用TD(0)
    SARSA解决FrozenLake-v0确定性转移*中相同的任务，即取回飞盘的最优策略，但这次我们不使用SARSA算法（基于策略），而是实现Q学习（非基于策略）。我们将观察该算法的行为，并训练自己通过恢复智能体的最优策略来实现一种新的估算q值表的方法。
- en: 'Follow these steps to complete this exercise:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成此练习：
- en: 'Import the required modules, as follows:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块，如下所示：
- en: '[PRE66]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Instantiate the `gym` environment called `FrozenLake-v0` using the `is_slippery`
    flag set to `False` in order to disable stochasticity:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化名为`FrozenLake-v0`的`gym`环境，设置`is_slippery`标志为`False`，以禁用随机性：
- en: '[PRE67]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Take a look at the action and the observation spaces:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看动作空间和观察空间：
- en: '[PRE68]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output will be as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE69]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Create two dictionaries to easily translate the `actions` numbers into moves:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个字典，方便将`actions`数字转换为动作：
- en: '[PRE70]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Reset the environment and render it to take a look at the grid problem:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并渲染以查看网格问题：
- en: '[PRE71]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output will be as follows:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.18: Environment''s initial state'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.18：环境的初始状态](img/B16182_07_18.jpg)'
- en: '](img/B16182_07_18.jpg)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_18.jpg)'
- en: 'Figure 7.18: Environment''s initial state'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.18：环境的初始状态
- en: 'Visualize the optimal policy for this environment:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化该环境的最优策略：
- en: '[PRE72]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output will be as follows:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE73]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'This represents the optimal policy for this environment and shows, for each
    of the environment states represented in the 4x4 grid, the optimal action among
    the four available: move Up, move Down, move Right, and move Left. Except for
    two states, all the others have a single optimal action associated with them.
    In fact, as described previously, optimal actions here are those that bring the
    agent to the goal in the shortest possible path. Two different possibilities result
    in the same path length for two states, so they are both equally optimal.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表示该环境的最优策略，并显示在4x4网格中表示的每个环境状态，对于四个可用动作中最优的动作：上移、下移、右移和左移。除了两个状态外，所有其他状态都有与之关联的唯一最优动作。实际上，正如前面所述，最优动作是那些将智能体带到目标的最短路径。两个不同的可能性导致两个状态具有相同的路径长度，因此它们都同样最优。
- en: 'Next, define functions that will take ε-greedy actions:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义将执行ε-贪婪动作的函数：
- en: '[PRE74]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Define a function that will take greedy actions:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行贪婪动作：
- en: '[PRE75]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a function that will calculate the mean of the agent''s performance:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来计算智能体表现的平均值：
- en: '[PRE76]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Initialize the Q-table so that all the values equal `1`, except for the values
    at terminal states:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化Q表，使得所有值都等于`1`，除了终止状态的值：
- en: '[PRE77]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Set the number of total episodes, the number of steps representing the interval
    by which we evaluate the agent''s average performance, the learning rate, the
    discounting factor, and the `ε` value for the exploration policy and define an
    array to collect all agent performance evaluations during training:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置总集数、表示我们评估智能体平均表现的间隔步数、学习率、折扣因子、`ε`值（用于探索策略），并定义一个数组以收集训练过程中所有智能体的表现评估：
- en: '[PRE78]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Train the agent using the Q-learning algorithm: the external loop takes care
    of generating the desired number of episodes. Then, the in-episode loop completes
    the following steps: first, it selects an exploration action with an ε-greedy
    policy, then the environment is stepped with the selected exploration action,
    and the `new_s`, `reward`, and `done` condition are retrieved. The new action
    for the new state is selected with the greedy policy, the Q-table is updated with
    the Q-learning TD(0) rule, and the state is updated with the new value. Every
    predefined number of steps, the agent''s average performance is estimated:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q-learning 算法训练代理：外部循环负责生成所需的回合数。然后，回合内循环完成以下步骤：首先，使用 ε-贪心策略选择一个探索动作，然后环境通过选择的探索动作进行一步，获取
    `new_s`、`reward` 和 `done` 条件。为新状态选择新动作，使用贪心策略更新 Q-table，使用 Q-learning TD(0) 规则更新状态的新值。每隔预定步骤数，就会评估代理的平均表现：
- en: '[PRE79]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Plot the Q-learning agent''s mean reward history during training:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制 Q-learning 代理在训练过程中的平均奖励历史：
- en: '[PRE80]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'This generates the following output, showing the learning progress of the Q-learning
    algorithm:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出，展示 Q-learning 算法的学习进度：
- en: '[PRE81]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The plot will be as follows:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.19: Average reward of an epoch trend over training epochs'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.19：训练轮次中每个时期的平均奖励趋势'
- en: '](img/B16182_07_19.jpg)'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_19.jpg)'
- en: 'Figure 7.19: Average reward of an epoch trend over training epochs'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.19：训练轮次中每个时期的平均奖励趋势
- en: As we can see, the plot shows how quickly Q-learning performance grows over
    epochs as the agent collects more and more experience. It also demonstrates that
    the algorithm is capable of reaching 100% success after learning. It's also evident
    how, in this case, compared to the SARSA method, the measured algorithm performance
    increases steadily and much faster.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所看到的，图表展示了随着代理收集越来越多的经验，Q-learning 性能如何在多个周期中快速增长。它还表明，算法在学习后能够达到 100% 的成功率。还可以明显看出，与
    SARSA 方法相比，在这种情况下，测量的算法性能稳步提高，且提高速度要快得多。
- en: 'Evaluate the greedy policy''s performance of the trained agent (Q-table):'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估训练好的代理（Q-table）的贪心策略表现：
- en: '[PRE82]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output will be as follows:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE83]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Display the Q-table values:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示 Q-table 的值：
- en: '[PRE84]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The following output will be generated:'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下输出将被生成：
- en: '[PRE85]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This output shows the values of the complete state-action value function for
    our problem. These values are then used to generate the optimal policy by means
    of the greedy selection rule.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此输出显示了我们问题的完整状态-动作价值函数的值。这些值随后用于通过贪心选择规则生成最优策略。
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出找到的贪心策略，并与最优策略进行比较：
- en: '[PRE86]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output will be as follows:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE87]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: As these outputs demonstrate, the Q-learning algorithm has been able to retrieve
    the optimal policy too, just like SARSA did in *Exercise 07.01, Using TD(0) SARSA
    to solve FrozenLake-v0 Deterministic Transitions*, only by means of experience
    and interaction with the environment.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些输出所示，Q-learning 算法能够像 SARSA 一样，通过经验和与环境的互动，成功地提取最优策略，正如在 *练习 07.01，使用 TD(0)
    SARSA 解决 FrozenLake-v0 确定性转移* 中所做的那样。
- en: As we can see, for every state of the grid world the greedy policy obtained
    with the Q-table calculated by our algorithm, this prescribes an action that is
    in accordance with the optimal policy that was defined by analyzing the environment
    problem. As we already saw, there are two states in which there are many different
    actions that are equally optimal, and the agent correctly implements one of them.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，对于网格世界中的每个状态，使用我们算法计算的 Q 表得到的贪心策略都能推荐一个与通过分析环境问题定义的最优策略一致的动作。正如我们已经看到的，有两个状态在其中存在多个不同的动作，它们同样是最优的，且代理正确地实现了其中的一个。
- en: Note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AUlzym](https://packt.live/2AUlzym).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2AUlzym](https://packt.live/2AUlzym)。
- en: You can also run this example online at [https://packt.live/3fJCnH5](https://packt.live/3fJCnH5).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址为 [https://packt.live/3fJCnH5](https://packt.live/3fJCnH5)。
- en: 'As for SARSA, it would be interesting to see how Q-learning behaves if we turn
    on stochastic transitions. This will be the goal of the activity at the end of
    this chapter. The procedure that the two algorithms follow is the very same one
    we adopted with SARSA: the same Q-learning algorithm used for the deterministic
    transition case is applied, and you are expected to adapt hyperparameters (especially
    the discount factor and the number of episodes) until you obtain convergence to
    the optimal policy under the stochastic transition dynamics.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SARSA 来说，如果我们启用随机转移，看看 Q-learning 的表现将会是很有趣的。这将是本章末尾活动的目标。两个算法遵循的程序与我们在 SARSA
    中采用的完全相同：用于确定性转移情况的 Q-learning 算法被应用，您需要调整超参数（特别是折扣因子和训练轮次），直到在随机转移动态下获得对最优策略的收敛。
- en: 'To complete the landscape of TD(0) algorithms, we will introduce another specific
    approach that''s obtained by applying very simple modifications of the previous
    ones: Expected SARSA.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完善 TD(0) 算法的全貌，我们将引入另一种特定的方法，该方法是通过对前面的方法进行非常简单的修改得到的：期望 SARSA。
- en: Expected SARSA
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 期望 SARSA
- en: 'Now, let''s consider a learning algorithm that is quite similar to Q-learning,
    with the only difference being the substitution of the maximum over next state-action
    pairs with the expected value. This is computed by taking into account the probability
    of each action under the current policy. This modified algorithm can be represented
    by the following update rule:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个与 Q-learning 非常相似的学习算法，唯一的区别是将下一个状态-动作对的最大值替换为期望值。这是通过考虑当前策略下每个动作的概率来计算的。这个修改后的算法可以通过以下更新规则来表示：
- en: '![Figure 7.20: State-action value function update rule'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.20：状态-动作值函数更新规则'
- en: '](img/B16182_07_20.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_20.jpg)'
- en: 'Figure 7.20: State-action value function update rule'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20：状态-动作值函数更新规则
- en: 'The additional computational complexity with respect to SARSA provides the
    advantage of eliminating variance due to the random selection of `A`t+1, which
    is a very powerful trick for improving learning and robustness considerably. It
    can be used both in an on-policy and off-policy fashion, thus becoming an abstraction
    of both SARSA and Q-learning with, in general, a performance that dominates both
    of them. An example of the update rule''s implementation is provided in the following
    snippet:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SARSA 相比，额外的计算复杂性提供了一个优势，即消除了由于随机选择 `A`t+1 所带来的方差，这对于显著提高学习效果和鲁棒性是一个非常强大的技巧。它可以同时用于在策略和非策略的方式，因此成为了
    SARSA 和 Q-learning 的一种抽象，通常其性能优于两者。以下片段提供了该更新规则的实现示例：
- en: '[PRE88]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: In the preceding code, the `pi` variable contains all the probabilities for
    each action in each state. The dot product involving `pi` and `q` is the operation
    needed to compute the expected value for the new state, taking into account all
    the actions for that state with their respective probabilities.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`pi` 变量包含了每个状态下每个动作的所有概率。涉及 `pi` 和 `q` 的点积是计算新状态期望值所需的操作，考虑到该状态下所有动作及其各自的概率。
- en: Now that we've studied the TD(0) methods, let's start learning about the N-step
    TD and TD(λ) algorithms.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了 TD(0) 方法，让我们开始学习 N 步 TD 和 TD(λ) 算法。
- en: N-Step TD and TD(λ) Algorithms
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N 步 TD 和 TD(λ) 算法
- en: 'In the previous chapter, we looked at Monte Carlo methods, while in the previous
    sections of this chapter, we learned about TD(0) ones, which, as we will discover
    soon, are also known as one-step temporal difference methods. In this section,
    we''ll unify them: in fact, they are at the extreme of a spectrum of algorithms
    (TD(0) on one side, with MC methods at the other end), and often, the best performing
    methods are somewhere in the middle of this spectrum.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了蒙特卡罗方法，而在本章前面的部分，我们学习了 TD(0) 方法，正如我们很快会发现的，它们也被称为一步时序差分方法。在本节中，我们将它们统一起来：事实上，它们处在一系列算法的极端（TD(0)
    在一端，MC 方法在另一端），而通常，性能最优的方法是处于这个范围的中间。
- en: N-step temporal difference algorithms extend one-step TD methods. More specifically,
    they generalize Monte Carlo and TD approaches, making it possible to smoothly
    transition between the two. As we already saw, MC methods must wait until the
    episode finishes to back the reward up into the previous states. One-step TD methods,
    on the other hand, make direct use of the first available future step to bootstrap
    and start updating the value function of states or state-action pairs. These extremes
    are rarely the optimal choices. The optimal choices generally fall in the middle
    of this broad range. Using N-step methods allows us to adjust the number of steps
    to consider when updating the value function, thereby distributing the bootstrapping
    approach to multiple steps.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: N步时序差分算法是对一阶时序差分方法的扩展。更具体地说，它们将蒙特卡罗和时序差分方法进行了概括，使得两者之间的平滑过渡成为可能。正如我们已经看到的，蒙特卡罗方法必须等到整个回合结束后，才能将奖励反向传播到之前的状态。而一阶时序差分方法则直接利用第一个可用的未来步骤进行自举，并开始更新状态或状态-动作对的价值函数。这两种极端情况很少是最佳选择。最佳选择通常位于这一广泛范围的中间。使用N步方法可以让我们调整在更新价值函数时考虑的步数，从而将自举方法分散到多个步骤上。
- en: A similar notion can be recalled in the context of eligibility traces, but they
    are more general, allowing us to distribute and spread bootstrapping over multiple
    time intervals at the same time. These two topics will be treated separately for
    clarity and, so as to enable you to build your knowledge incrementally, we will
    start with N-step methods first, before moving on to eligibility traces.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在资格迹的背景下也可以回忆起类似的概念，但它们更为一般，允许我们在多个时间间隔内同时分配和扩展自举。这两个话题将单独处理，以便清晰起见，并且为了帮助你逐步建立知识，我们将首先从N步方法开始，然后再讲解资格迹。
- en: N-Step TD
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N步时序差分
- en: 'As we have already seen for one-step TD methods, the first step to approaching
    the N-step method is to focus on the estimation of the state-value function for
    sample episodes generated using the policy, ![a](img/B16182_07_20a.png). We already
    recalled that the Monte Carlo algorithm must wait until the end of an episode
    before performing an update by using the entire sequence of rewards from a given
    state. On the other hand, one-step methods just need the next reward. N-step methods
    use an intermediate rule: instead of relying on just the next reward, or on all
    future rewards until the episode ends, they use a value in between these two.
    For example, a three-steps update would use the first three rewards and the estimated
    state value reached three steps ahead. This can be formalized for a generic number
    of steps.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的一阶时序差分方法一样，接近N步方法的第一步是专注于使用策略生成的样本回合来估计状态值函数，![a](img/B16182_07_20a.png)。我们已经提到，蒙特卡罗算法必须等到回合结束后，才能通过使用给定状态的整个奖励序列来进行更新。而一阶方法只需要下一个奖励。N步方法采用了一种中间规则：它们不仅依赖于下一个奖励，或者依赖于回合结束前的所有未来奖励，而是采用这两者之间的一个值。例如，三步更新将使用前三个奖励和三步后达到的估计状态值。这可以对任意步数进行形式化。
- en: This approach gives birth to a family of methods that are still temporal difference
    ones since they use the N-steps that were encountered after the target state to
    update its value. It is clear that the methods that we encountered at the beginning
    of this chapter are a special case of N-step methods. For this reason, they are
    called "one-step TD methods."
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法催生了一系列方法，它们仍然是时序差分方法，因为它们使用目标状态之后遇到的N步来更新其值。显然，我们在本章开始时遇到的方法是N步方法的特例。因此，它们被称为“一阶时序差分方法”。
- en: 'In order to define them more formally, we can consider the estimated value
    of the state, ![a](img/B16182_07_20b.png) as a result of the state-reward sequence,
    ![b](img/B16182_07_20c.png), ![c](img/B16182_07_20d.png), ![d](img/B16182_07_20e.png),
    ![e](img/B16182_07_20f.png), ..., ![f](img/B16182_07_20g.png), ![g](img/B16182_07_20h.png)
    (except the actions). In MC methods, this estimate is updated only once the episode
    is complete, and in one-step methods, right after the next step. In N-step methods,
    on the other hand, the state-value estimate is updated after N-steps using a quantity
    that discounts `n` future rewards and the value of the state encountered after
    N-steps in the future. This quantity, called N-step return, can be defined in
    an expression, as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更正式地定义它们，我们可以考虑状态的估计值，![a](img/B16182_07_20b.png)，作为状态-奖励序列的结果，![b](img/B16182_07_20c.png)，![c](img/B16182_07_20d.png)，![d](img/B16182_07_20e.png)，![e](img/B16182_07_20f.png)，...，![f](img/B16182_07_20g.png)，![g](img/B16182_07_20h.png)（不包括动作）。在MC方法中，这个估计值只有在一集结束时才会更新，而在一步法中，它会在下一步之后立即更新。另一方面，在N步法中，状态值估计是在N步之后更新的，使用一种折扣`n`未来奖励以及未来N步后遇到的状态值的量。这个量被称为N步回报，可以通过以下表达式定义：
- en: '![Figure 7.21: N-step return equation (with the state-value function)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.21：N步回报方程（带状态值函数）'
- en: '](img/B16182_07_21.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_21.jpg)'
- en: 'Figure 7.21: N-step return equation (with the state-value function)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21：N步回报方程（带状态值函数）
- en: 'A key point to note here is that in order to calculate this N-step return,
    we have to wait to reach the time `t+1` so that all the terms in the equation
    are available. By using the N-step return, it is straightforward to formalize
    the state-value function update rule, as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个关键点是，为了计算这个N步回报，我们必须等到达到时间`t+1`，以便方程中的所有项都可以使用。通过使用N步回报，可以直接将状态值函数更新规则形式化，如下所示：
- en: '![Figure 7.22: Expression for the natural state-value learning algorithm'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.22：使用N步回报的自然状态值学习算法的表达式'
- en: for using N-step returns
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 使用N步回报
- en: '](img/B16182_07_22.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_22.jpg)'
- en: 'Figure 7.22: Expression for the natural state-value learning algorithm for
    using N-step returns'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22：使用N步回报的自然状态值学习算法的表达式
- en: 'Note that the values of all the other states remain unchanged, as shown in
    the following expression:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有其他状态的值保持不变，如以下表达式所示：
- en: '![Figure 7.23: Expression specifying that all the other values are kept constant'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.23：指定所有其他值保持恒定的表达式'
- en: '](img/B16182_07_23.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_23.jpg)'
- en: 'Figure 7.23: Expression specifying that all the other values are kept constant'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23：指定所有其他值保持恒定的表达式
- en: This is the equation that formalizes the N-step TD algorithm. It is worth noting
    again that no changes are made during the first `n-1` steps before we can estimate
    the N-step return. This needs to be compensated for at the end of the episode,
    when the remaining `n-1` updates are performed all at once after reaching the
    terminal state.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将N步TD算法形式化的方程。值得再次注意的是，在我们可以估计N步回报之前，在前`n-1`步期间不会进行任何更改。需要在一集结束时进行补偿，当剩余的`n-1`更新在到达终止状态后一次性执行。
- en: Similar to what we already saw for TD(0) methods, and without talking about
    this in too much data, the state-value function estimation of the N-step TD methods
    converges to the optimal value under appropriate technical conditions.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前看到的TD(0)方法类似，并且不深入讨论数据，N步TD方法的状态值函数估计在适当的技术条件下会收敛到最优值。
- en: N-step SARSA
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N步SARSA
- en: 'It is quite straightforward to extend the SARSA algorithm, which we looked
    at when we introduced one-step methods, to its N-step version. As we did previously,
    the only thing we need to do is substitute the state-action pairs for states in
    the value function''s N-step return and in the update formulations just seen,
    coupling them with an ε-greedy policy. The definition of the N-step return (update
    targets) can be described by the following equation:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展我们在介绍一步法时看到的SARSA算法到其N步版本是非常简单的。就像我们之前做的那样，唯一需要做的就是将值函数的N步回报中的状态-动作对替换为状态，并在刚才看到的更新公式中结合ε-贪婪策略。N步回报（更新目标）的定义可以通过以下方程描述：
- en: '![Figure 7.24: N-step return equation (with the state-action value function)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.24：N步回报方程（带状态-动作值函数）'
- en: '](img/B16182_07_24.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_24.jpg)'
- en: 'Figure 7.24: N-step return equation (with the state-action value function)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24：N步回报方程（带状态-动作值函数）
- en: 'Here, ![a](img/B16182_07_24a.png) if ![b](img/B16182_07_24b.png). The update
    rule for the state-action value function is expressed as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![a](img/B16182_07_24a.png)，如果![b](img/B16182_07_24b.png)。状态-动作值函数的更新规则表达如下：
- en: '![Figure 7.25: Update rule for the state-action value function'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.25：状态-动作值函数的更新规则'
- en: '](img/B16182_07_25.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_25.jpg)'
- en: 'Figure 7.25: Update rule for the state-action value function'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25：状态-动作值函数的更新规则
- en: 'Note that the values of all the other state-action pairs remain unchanged:
    ![c](img/B16182_07_25a.png), for all values of `s`, so that ![d](img/B16182_07_25b.png)
    or ![e](img/B16182_07_25c.png). A scheme for the N-step SARSA control algorithm
    can be depicted as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，其他所有状态-动作对的值保持不变：![c](img/B16182_07_25a.png)，对所有`s`的值都适用，因此![d](img/B16182_07_25b.png)
    或![e](img/B16182_07_25c.png)。N步SARSA控制算法的方案可以如下表示：
- en: 'Choose the algorithm''s parameters: the step size, ![f](img/B16182_07_25d.png),
    which has to be contained in the interval (0, 1], and the `ε` parameter of the
    ε-greedy policy, which has to be small and greater than `0` since it represents
    the probability of choosing the non-optimal action to favor exploration. A value
    for the number of steps, `n`, has to be chosen. This can be done, for example,
    with the following code:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择算法的参数：步长![f](img/B16182_07_25d.png)，它必须位于区间(0, 1]内，和ε-贪心策略的`ε`参数，它必须小且大于`0`，因为它表示选择非最优动作以偏向探索的概率。必须选择步数`n`的值。例如，可以使用以下代码来完成此选择：
- en: '[PRE89]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Initialize ![g](img/B16182_07_25e.png), for all ![h](img/B16182_07_25f.png),
    ![i](img/B16182_07_25g.png), arbitrarily, except that Q(terminal, ·) = 0:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化![g](img/B16182_07_25e.png)，对于所有![h](img/B16182_07_25f.png)，![i](img/B16182_07_25g.png)，任意选择，除了Q(终止,
    ·) = 0：
- en: '[PRE90]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Create a loop for each episode. Initialize and store the S0 ≠ terminal. Select
    and store an action using the ε-greedy policy and initialize time, `T`, as a very
    high value:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个回合创建一个循环。初始化并存储S0 ≠ 终止状态。使用ε-贪心策略选择并存储动作，并将时间`T`初始化为一个非常大的值：
- en: '[PRE91]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Create a loop for t = 0, 1, 2, .... If t < T, then perform action ![j](img/B16182_07_25h.png).
    Observe and store the next reward as ![k](img/B16182_07_25i.png) and the next
    state as ![l](img/B16182_07_25j.png) If ![m](img/B16182_07_25k.png) is terminal,
    then set `T` equal to `t+1`:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为t = 0, 1, 2,... 创建一个循环。如果t < T，则执行动作![j](img/B16182_07_25h.png)。观察并存储下一奖励为![k](img/B16182_07_25i.png)，下一状态为![l](img/B16182_07_25j.png)。如果![m](img/B16182_07_25k.png)是终止状态，则将`T`设置为`t+1`：
- en: '[PRE92]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'If ![n](img/B16182_07_25l.png) is not terminal, select and store a new action
    for the new state:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果![n](img/B16182_07_25l.png)不是终止状态，则选择并存储新状态下的动作：
- en: '[PRE93]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Define the time for which the estimate is being updated, `tau`, equal to `t-n+1`:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于更新估计的时间`tau`，等于`t-n+1`：
- en: '[PRE94]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'If `tau` is greater than 0, calculate the N-step return by summing the discounted
    returns of the previous n steps and adding the discounted value of the next step-next
    action pair and update the state-action value function:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`tau`大于0，则通过对前n步的折扣回报求和，并加上下一步-下一动作对的折扣值来计算N步回报，并更新状态-动作值函数：
- en: '[PRE95]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: With a few minor changes, this can easily be extended to accommodate Expected
    SARSA as well. As seen previously in this chapter, it only requires us to substitute
    the expected approximate value of the state using the estimated action values
    at time, t, under the target policy at the last step of the N-steps. When the
    state in question is terminal, its expected approximate value is defined as 0.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些小的改动，这个规则可以轻松扩展以适应预期的SARSA。正如本章之前所见，它只需要我们用目标策略下第N步最后一个时间点的估计动作值来替代状态的预期近似值。当相关状态是终止状态时，它的预期近似值定义为0。
- en: N-Step Off-Policy Learning
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N步非策略学习
- en: To define off-policy learning for N-step methods, we will be taking very similar
    steps as the ones we did for one-step methods. The key point is that, as in all
    off-policy methods, we are learning the value function for a policy, ![a](img/B16182_07_25m.png),
    while following a different exploration policy; say, `b`. Typically, ![b](img/B16182_07_25n.png)
    is the greedy policy for the current state-action value function estimate, and
    b has more randomness so that it effectively explores the environment; for example,
    ε-greedy. The main difference with respect to what we already saw for one-step
    off-policy methods is that now, we need to take into account the fact that we
    are selecting actions using a different policy than the one we want to learn,
    and we are doing it for more than one step. So, we need to properly weigh the
    selected actions measuring the relative probability under the two policies of
    taking those actions.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义 N 步方法的离策略学习，我们将采取与一阶方法相似的步骤。关键点在于，像所有离策略方法一样，我们是在为策略 ![a](img/B16182_07_25m.png)
    学习价值函数，同时遵循一个不同的探索策略，假设为 `b`。通常，![b](img/B16182_07_25n.png) 是当前状态-动作值函数估计的贪婪策略，而
    `b` 具有更多的随机性，以便有效探索环境；例如，ε-贪婪策略。与我们之前看到的一阶离策略方法的主要区别在于，现在我们需要考虑到，我们正在使用不同于我们想要学习的策略来选择动作，并且我们是进行多步选择。因此，我们需要通过测量在两种策略下选择这些动作的相对概率来适当加权所选动作。
- en: 'By means of this correction, it is possible to define the rule for a simple
    off-policy version of N-step TD: the update for time `t` (actually made at time
    `t + n`) can simply be weighted by ![c](img/B16182_07_25o.png):'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个修正，我们可以定义一个简单的离策略 N 步 TD 版本的规则：在时间 `t` （实际上是在时间 `t + n`）进行的更新可以通过 ![c](img/B16182_07_25o.png)
    进行加权：
- en: '![Figure 7.26: N-step TD off-policy update rule at time ''t'''
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.26：时间 ''t'' 时刻 N 步 TD 离策略更新规则'
- en: '](img/B16182_07_26.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_26.jpg)'
- en: 'Figure 7.26: N-step TD off-policy update rule at time ''t'''
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.26：时间 't' 时刻 N 步 TD 离策略更新规则
- en: 'Here, `V` is the value function, ![e](img/B16182_07_26a.png) is the step size,
    `G` is the N-step return, and ![d](img/B16182_07_26b.png) is called the importance
    sampling ratio. The importance sampling ratio is the relative probability under
    the two policies of taking `n` actions from ![f](img/B16182_07_26c.png) to ![f](img/B16182_07_26d.png),
    which can be expressed as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`V` 是价值函数，![e](img/B16182_07_26a.png) 是步长，`G` 是 N 步回报，![d](img/B16182_07_26b.png)
    称为重要性采样比率。重要性采样比率是指在两种策略下，从 ![f](img/B16182_07_26c.png) 到 ![f](img/B16182_07_26d.png)
    执行 `n` 个动作的相对概率，其表达式如下：
- en: '![Figure 7.27: Sampling ratio equation'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.27：采样比率方程'
- en: '](img/B16182_07_27.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_27.jpg)'
- en: 'Figure 7.27: Sampling ratio equation'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27：采样比率方程
- en: Here, ![i](img/B16182_07_27a.png) is the agent policy, ![g](img/B16182_07_27b.png)
    is the exploration policy, ![h](img/B16182_07_27c.png) is the action, and ![h](img/B16182_07_27d.png)
    is the state.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![i](img/B16182_07_27a.png) 是智能体策略，![g](img/B16182_07_27b.png) 是探索策略，![h](img/B16182_07_27c.png)
    是动作，![h](img/B16182_07_27d.png) 是状态。
- en: 'By this definition, it is evident that actions that would never be selected
    under the policy we want to learn (that is, their probability is `0`) would be
    ignored (weight equal to 0). If, on the other hand, an action under the policy
    we are learning has more probability with respect to the exploratory policy, the
    weight assigned to it should be higher than `1` since it will be encountered more
    often. It is also evident that for the on-policy case, the sampling ratio is always
    equal to `1`, given the fact that ![b](img/B16182_07_27e.png) and ![b](img/B16182_07_27f.png)
    are the same policy. For this reason, the N-step SARSA on-policy update can be
    seen as a special case of the off-policy update. The general form of the update,
    from which it is possible to derive both on-policy and off-policy methods, is
    as follows:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，很明显，在我们想要学习的策略下永远不会选择的动作（即它们的概率为 `0`）会被忽略（权重为 0）。另一方面，如果我们正在学习的策略下某个动作相对于探索策略具有更高的概率，那么分配给它的权重应高于
    `1`，因为它会更频繁地被遇到。显然，对于在策略情况下，采样比率始终等于 `1`，因为 ![b](img/B16182_07_27e.png) 和 ![b](img/B16182_07_27f.png)
    是相同的策略。因此，N 步 SARSA 在策略更新可以视为离策略更新的一个特例。该更新的通用形式，可以从中推导出在策略和离策略方法，表达式如下：
- en: '![Figure 7.28: State-action value function for the off-policy N-step TD algorithm'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.28：离策略 N 步 TD 算法的状态-动作值函数'
- en: '](img/B16182_07_28.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_28.jpg)'
- en: 'Figure 7.28: State-action value function for the off-policy N-step TD algorithm'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.28：离策略 N 步 TD 算法的状态-动作值函数
- en: 'As you can see, ![a](img/B16182_07_28a.png) is the state-action value function,
    ![b](img/B16182_07_28b.png) is the step size, ![c](img/B16182_07_28c.png) is the
    N-step return, and ![d](img/B16182_07_28d.png) is the importance sampling ratio.
    The scheme for the full algorithm is as follows:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，![a](img/B16182_07_28a.png)是状态-动作值函数，![b](img/B16182_07_28b.png)是步长，![c](img/B16182_07_28c.png)是N步回报，![d](img/B16182_07_28d.png)是重要性抽样比率。完整算法的方案如下：
- en: 'Select an arbitrary behavior policy, ![e](img/B16182_07_28e.png), so that the
    probability for each action of each state is greater than 0 for all states and
    actions. Choose the algorithm parameters: the step size, ![f](img/B16182_07_28f.png),
    which has to be contained in the interval (0, 1], and a value for the number of
    steps, `n`. This can be done, for example, with the following code:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个任意行为策略，![e](img/B16182_07_28e.png)，使得每个状态的每个动作的概率对于所有状态和动作都大于0。选择算法参数：步长，![f](img/B16182_07_28f.png)，其必须在区间(0,
    1]内，并为步数选择一个值`n`。这可以通过以下代码实现：
- en: '[PRE96]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Initialize ![h](img/B16182_07_28g.png), for all ![g](img/B16182_07_28h.png),
    ![h](img/B16182_07_28i.png), arbitrarily, except that Q(terminal, ·) = 0:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化![h](img/B16182_07_28g.png)，对于所有![g](img/B16182_07_28h.png)，![h](img/B16182_07_28i.png)可以任意选择，除非Q(终止，·)
    = 0：
- en: '[PRE97]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Initialize the policy, ![i](img/B16182_07_28j.png), to be greedy with respect
    to Q, or to a fixed given policy. Create a loop for each episode. Initialize and
    store the S0 ≠ terminal. Select and store an action using the b policy and initialize
    time, `T`, as a very high value:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略，![i](img/B16182_07_28j.png)，使其对Q采取贪婪策略，或设定为一个固定的给定策略。为每个回合创建一个循环。初始化并存储S0
    ≠终止状态。使用b策略选择并存储一个动作，并将时间`T`初始化为一个非常大的值：
- en: '[PRE98]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Create a loop for t = 0, 1, 2, .... If t < T, then perform action ![k](img/B16182_07_28k.png).
    Observe and store the next reward as ![j](img/B16182_07_28l.png) and the next
    state as ![l](img/B16182_07_28m.png) . If ![m](img/B16182_07_28n.png) is terminal,
    then set `T` equal to `t+1`:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为t = 0, 1, 2, ... 创建一个循环。如果t < T，则执行动作![k](img/B16182_07_28k.png)。观察并存储下一次奖励为![j](img/B16182_07_28l.png)，并将下一个状态存储为![l](img/B16182_07_28m.png)。如果![m](img/B16182_07_28n.png)是终止状态，则将`T`设置为`t+1`：
- en: '[PRE99]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'If ![m](img/B16182_07_28o.png) is not terminal, select and store a new action
    for the new state:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果![m](img/B16182_07_28o.png)不是终止状态，则选择并存储新状态的动作：
- en: '[PRE100]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Define the time for which the estimate is being updated, `tau`, equal to `t-n+1`:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义估计更新的时间`tau`，使其等于`t-n+1`：
- en: '[PRE101]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'If `tau` is greater than or equal to `0`, calculate the sampling ratio. Calculate
    the N-step return by summing the discounted returns of the previous n steps and
    adding the discounted value of the next step-next action pair and update the state-action
    value function:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`tau`大于或等于`0`，则计算抽样比率。通过对前n步的折扣回报求和，并加上下一步-下一动作对的折扣值来计算N步回报，并更新状态-动作值函数：
- en: '[PRE102]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Now that we've studied the N-step methods, it is time to proceed to the most
    general and most performant declination of temporal difference methods, TD(λ).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了N步方法，是时候继续学习时序差分方法的最一般且最高效的变体——TD(λ)了。
- en: TD(λ)
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD(λ)
- en: The popular TD(λ) algorithm is a temporal difference algorithm that makes use
    of the eligibility trace concept, which, as we will soon see, is a procedure that
    allows us to appropriately weight contributions to the state's (or state-action
    pair's) value function using any possible number of steps. The `lambda` term introduced
    in the name is a parameter that defines and parameterizes this family of algorithms.
    As we will see shortly, it is a weighting factor that will allow us to appropriately
    weight different contributing terms involved in the estimation of the algorithm's
    return.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的TD(λ)算法是一种时序差分算法，利用了资格迹概念。正如我们很快将看到的，这是一种通过任意步数适当加权状态（或状态-动作对）的价值函数贡献的过程。名称中引入的`lambda`项是一个参数，用于定义和参数化这一系列算法。正如我们很快会看到的，它是一个加权因子，可以让我们适当地加权涉及算法回报估计的不同贡献项。
- en: It is possible to combine any temporal difference method, such as those we already
    saw (Q-learning and SARSA), with the eligibility traces concept, which we will
    implement shortly. This allows us to obtain a more general method, which is also
    more efficient. This approach, as we already anticipated previously, realizes
    the final unification and generalization of the TD and Monte Carlo methods. Similarly,
    regarding what we observed for N-step TD methods, in this case also, we have one-step
    TD methods on one extreme (`λ = 0`) and Monte Carlo methods on the other (`λ =
    1`). The space between these two boundaries contains intermediate methods (as
    is the case for N-step methods with finite `n > 1`). In addition to that, eligibility
    traces allow us to use extended Monte Carlo methods for the so-called online implementation,
    meaning they become applicable to non-episodic problems.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 任何时间差方法，例如我们已经看到的（Q-learning 和 SARSA），都可以与资格迹概念结合，我们将在接下来实现。这使我们能够获得一种更通用的方法，同时也更高效。正如我们之前预期的，这种方法实现了
    TD 和蒙特卡罗方法的最终统一与推广。同样，关于我们在 N 步 TD 方法中看到的内容，在这里我们也有一个极端（`λ = 0`）的单步 TD 方法和另一个极端（`λ
    = 1`）的蒙特卡罗方法。这两个边界之间的空间包含了中间方法（就像 N 步方法中有限的 `n > 1` 一样）。此外，资格迹还允许我们使用扩展的蒙特卡罗方法进行所谓的在线实现，这意味着它们可以应用于非回合性问题。
- en: With respect to what we already saw for N-step TD methods, eligibility traces
    have an additional advantage, allowing us to generalize these families with significant
    computational improvement. As we mentioned earlier, choosing the correct value
    of n for N-step methods can be anything but a straightforward task. Eligibility
    traces, on the other hand, allow us to "fuse" together the updates corresponding
    to different timesteps.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于我们之前看到的 N 步 TD 方法，资格迹具有额外的优势，使我们能够显著提升这些方法的计算效率。如我们之前所提到的，选择 N 步方法中 n 的正确值往往不是一项简单的任务。而资格迹则允许我们将不同时间步对应的更新“融合”在一起。
- en: To achieve this goal, we need to define a method to weigh the N-step return,
    ![c](img/B16182_07_28p.png), using a weight that decays exponentially with time.
    This is done by introducing a factor, ![d](img/B16182_07_28q.png), and weighting
    the nth return with ![b](img/B16182_07_28r.png).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们需要定义一种方法来加权 N 步返回值，![c](img/B16182_07_28p.png)，使用一个随着时间呈指数衰减的权重。通过引入一个因子
    ![d](img/B16182_07_28q.png)，并用 ![b](img/B16182_07_28r.png) 对第 n 次返回进行加权。
- en: 'The goal is to define a weighted average so that all these weights must total
    to `1`. The normalization constant is the limit value of the convergent geometric
    series: ![a](img/B16182_07_28s.png). With this, we can define the so-called ![a](img/B16182_07_28t.png)-return
    as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是定义一个加权平均值，使得所有这些权重的总和为 `1`。标准化常数是收敛几何级数的极限值：![a](img/B16182_07_28s.png)。有了这个，我们可以定义所谓的
    ![a](img/B16182_07_28t.png)-返回，如下所示：
- en: '![Figure 7.29: Expression for the lambda return'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.29：lambda 返回的表达式'
- en: '](img/B16182_07_29.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_29.jpg)'
- en: 'Figure 7.29: Expression for the lambda return'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29：lambda 返回的表达式
- en: This equation defines how our choice of ![a](img/B16182_07_29a.png) influences
    the speed at which a given return drops exponentially as a function of the number
    of steps.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程定义了我们选择的 ![a](img/B16182_07_29a.png) 如何影响给定返回随着步数的增加呈指数下降的速度。
- en: 'We can now use this new return as a target for the state (or state-action pair)
    value function, thus creating a new value function update rule. It may seem that,
    at this point, in order to consider all contributes, we should wait until the
    end of the episode, thus collecting all future returns. This problem is solved
    by means of the second fundamental novelty introduced by eligibility traces: instead
    of looking forward in time, the point of view is reversed, and the agent updates
    all states (state-action pairs) visited in the past according to the eligibility
    traces rule and using current return and values information.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这个新的返回值作为状态（或状态-动作对）值函数的目标，从而创建一个新的值函数更新规则。此时看起来，为了考虑所有的贡献，我们应该等到本回合结束，收集所有未来的返回值。这个问题通过资格迹的第二个基本新颖性得到了解决：与其向前看，我们反转了视角，智能体根据资格迹规则，使用当前返回值和价值信息来更新过去访问过的所有状态（状态-动作对）。
- en: 'The eligibility trace is initialized equal to 0 for every state (or state-action
    pair), is incremented on each time step with a value equal to 1 for the state
    (or state-action pair), visited so that it gives it the highest weight in contributing
    to the value function update, and fades away by the ![b](img/B16182_07_29b.png)
    factor. This factor is the combination of decay in time that''s typical of eligibility
    traces, as explained previously (![b](img/B16182_07_29c.png)), and the familiar
    reward discount ![c](img/B16182_07_29d.png) we''ve encountered many times in this
    chapter. With this new concept, we can now build the new value function update.
    First, we have the equation that regulates the eligibility trace evolution:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 资格迹初始化为每个状态（或状态-动作对）都为0，在每一步时间更新时，访问的状态（或状态-动作对）的值加1，从而使其在更新值函数时权重最大，并通过![b](img/B16182_07_29b.png)因子逐渐衰退。这个因子是资格迹随着时间衰退的组合，如之前所解释的
    (![b](img/B16182_07_29c.png))，以及我们在本章中多次遇到的熟悉的奖励折扣因子![c](img/B16182_07_29d.png)。有了这个新概念，我们现在可以构建新的值函数更新规则。首先，我们有一个方程式来调节资格迹的演化：
- en: '![Figure 7.30: Eligibility traces initialization and update rule at time ''t''
    (for states)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.30：时间‘t’时状态的资格迹初始化和更新规则'
- en: '](img/B16182_07_30.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_30.jpg)'
- en: 'Figure 7.30: Eligibility traces initialization and update rule at time ''t''
    (for states)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.30：时间‘t’时状态的资格迹初始化和更新规则
- en: 'Then, we have the new definition of the TD error (or δ). The state-value function
    update will be as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有了新的TD误差（或δ）的定义。状态值函数的更新规则如下：
- en: '![Figure 7.31: State-value function update rule using eligibility traces'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.31：使用资格迹的状态值函数更新规则'
- en: '](img/B16182_07_31.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_31.jpg)'
- en: 'Figure 7.31: State-value function update rule using eligibility traces'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31：使用资格迹的状态值函数更新规则
- en: Now, let's see how this idea is implemented in the SARSA algorithm to obtain
    an on-policy TD control algorithm with eligibility traces.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在SARSA算法中实现这个思想，以获得一个具有资格迹的策略控制算法。
- en: SARSA(λ)
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SARSA(λ)
- en: 'Directly translating a state-value update into a state-action-value update
    allows us to add the eligibility traces feature to our previously seen SARSA algorithm.
    The eligibility trace equation can be modified as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 直接将状态值更新转换为状态-动作值更新，允许我们将资格迹特性添加到我们之前看到的SARSA算法中。资格迹方程可以如下修改：
- en: '![Figure 7.32: Eligibility trace initialization and update rule at time ''t''
    (for state-actions pairs)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.32：时间‘t’时状态-动作对的资格迹初始化和更新规则'
- en: '](img/B16182_07_32.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_32.jpg)'
- en: 'Figure 7.32: Eligibility trace initialization and update rule at time ''t''
    (for state-actions pairs)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.32：时间‘t’时状态-动作对的资格迹初始化和更新规则
- en: 'The TD error and the state-action value function updates are written as follows:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: TD误差和状态-动作值函数更新规则如下所示：
- en: '![Figure 7.33: State-action pair''s value function update rule using eligibility
    traces'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.33：使用资格迹的状态-动作对值函数更新规则'
- en: '](img/B16182_07_33.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_33.jpg)'
- en: 'Figure 7.33: State-action pair''s value function update rule using eligibility
    traces'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.33：使用资格迹的状态-动作对值函数更新规则
- en: 'A schema that perfectly summarizes all these steps and presents the complete
    algorithm is as follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美总结所有这些步骤并展示完整算法的示意图如下：
- en: 'Choose the algorithm''s parameters: the step size ![a](img/B16182_07_33a.png),
    which has to be contained in the interval (0, 1], and the `ε` parameter of the
    ε-greedy policy, which has to be small and greater than 0, since it represents
    the probability of choosing the non-optimal action, to favor exploration. A value
    for the `lambda` parameter has to be chosen. This can be done, for example, with
    the following code:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择算法的参数：步长![a](img/B16182_07_33a.png)，该值必须位于区间(0, 1]内，和ε-贪心策略的`ε`参数，该参数必须小且大于0，因为它代表选择非最优动作的概率，旨在促进探索。必须选择一个`lambda`参数的值。例如，可以通过以下代码来实现：
- en: '[PRE103]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Initialize ![a](img/B16182_07_33b.png), for all ![b](img/B16182_07_33c.png),
    ![c](img/B16182_07_33d.png), arbitrarily, except that Q(terminal, ·) = 0:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化![a](img/B16182_07_33b.png)，对于所有![b](img/B16182_07_33c.png)，![c](img/B16182_07_33d.png)，任选初始化，除了Q(terminal,
    ·) = 0：
- en: '[PRE104]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Create a loop for each episode. Initialize the eligibility traces table to
    `0`:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个回合创建一个循环。将资格迹表初始化为`0`：
- en: '[PRE105]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Initialize the state as it is not terminal and select an action using the ε-greedy
    policy. Then, initiate the in-episode loop:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态为非终止状态，并使用ε-贪心策略选择一个动作。然后，开始回合内循环：
- en: '[PRE106]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Create a loop for each step of the episode, update the eligibility traces,
    and assign a value equal to `1` to the last visited state:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个回合的每一步创建一个循环，更新 eligibility traces，并将值为 `1` 分配给最后访问的状态：
- en: '[PRE107]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Step through the environment and choose the next action using the ε-greedy
    policy:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过环境并使用 ε-贪婪策略选择下一个动作：
- en: '[PRE108]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Calculate the ![b](img/B16182_07_33e.png) update and update the Q-table using
    the SARSA TD(![a](img/B16182_07_33f.png)) rule:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![b](img/B16182_07_33e.png) 更新，并使用 SARSA TD(![a](img/B16182_07_33f.png)) 规则更新
    Q 表：
- en: '[PRE109]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Update the state and action with new state and action values:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新状态和新动作值更新状态和动作：
- en: '[PRE110]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: We are now ready to test this new algorithm on the environment we already solved
    with one-step SARSA and Q-learning.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好在已经通过单步 SARSA 和 Q-learning 解决的环境中测试这个新算法。
- en: 'Exercise 7.04: Using TD(λ) SARSA to Solve FrozenLake-v0 Deterministic Transitions'
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.04：使用 TD(λ) SARSA 解决 FrozenLake-v0 确定性转移问题
- en: In this exercise, we will implement our SARSA(λ) algorithm to solve the FrozenLake-v0
    environment under the deterministic environment dynamics. In this exercise, we
    will consider the same task we addressed in *Exercise 7.01, Using TD(0) SARSA
    to Solve FrozenLake-v0 Deterministic Transitions*, and *Exercise 7.03, Using TD(0)
    Q-Learning to Solve FrozenLake-v0 Deterministic Transitions*, but this time, instead
    of using one-step TD methods such as SARSA (on-policy) and Q-learning (off-policy),
    we will implement TD(λ), a temporal difference method coupled with the power of
    eligibility traces. We will see how this algorithm behaves and train ourselves
    in implementing a new approach to estimate a Q-value table by means of which we'll
    recover an optimal policy for our agent.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将实现 SARSA(λ) 算法来解决 FrozenLake-v0 环境下的确定性环境动态。在这个练习中，我们将考虑与 *练习 7.01，使用
    TD(0) SARSA 解决 FrozenLake-v0 确定性转移问题* 和 *练习 7.03，使用 TD(0) Q-learning 解决 FrozenLake-v0
    确定性转移问题* 中相同的任务，但这一次，我们将不再使用像 SARSA（基于策略）和 Q-learning（非基于策略）这样的单步 TD 方法，而是实现 TD(λ)，这是一种与
    eligibility traces 功能结合的时序差分方法。我们将观察这个算法的行为，并训练自己实现一种新的方法，通过它来估算 Q 值表，从而恢复智能体的最优策略。
- en: 'Follow these steps to complete this exercise:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤完成此练习：
- en: 'Import the required modules:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需模块：
- en: '[PRE111]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Instantiate the `gym` environment called `FrozenLake-v0` using the `is_slippery`
    flag set to `False` in order to disable stochasticity:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `is_slippery` 标志设置为 `False` 来实例化名为 `FrozenLake-v0` 的 `gym` 环境，以禁用随机性：
- en: '[PRE112]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Take a look at the action and the observation spaces:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下动作和观察空间：
- en: '[PRE113]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The output will be as follows:'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE114]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Create two dictionaries to easily translate the `actions` numbers into moves:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个字典，以便轻松将 `actions` 数字转换为动作：
- en: '[PRE115]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Reset the environment and render it to take a look at the grid:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并渲染它，查看网格：
- en: '[PRE116]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'The output will be as follows:'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.34: Environment''s initial state'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.34：环境的初始状态](img/B16182_07_34.jpg)'
- en: '](img/B16182_07_34.jpg)'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_34.jpg)'
- en: 'Figure 7.34: Environment''s initial state'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.34：环境的初始状态
- en: 'Visualize the optimal policy for this environment:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化该环境的最优策略：
- en: '[PRE117]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The output will be printed as follows:'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE118]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: This is the optimal policy for the deterministic case we already encountered
    when dealing with one-step TD methods. It shows the optimal actions we hope our
    agent will learn within this environment.
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们在处理单步 TD 方法时已经遇到的确定性情况的最优策略。它展示了我们希望智能体在这个环境中学习的最优动作。
- en: 'Define the functions that will take ε-greedy actions:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将采取 ε-贪婪动作的函数：
- en: '[PRE119]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Define a function that will take greedy actions:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个将采取贪婪动作的函数：
- en: '[PRE120]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Define a function that will calculate the agent''s average performance:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个将计算智能体平均表现的函数：
- en: '[PRE121]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Set the number of total episodes, the number of steps representing the interval
    by which we evaluate the agent''s average performance, the discount factor, the
    learning rate, and the `ε` parameters ruling its decrease – the starting value,
    minimum value, and range (in terms of the number of episodes) – over which the
    decrease is spread, as well as the eligibility trace''s decay parameter:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置总回合数、表示我们评估智能体平均表现的间隔步数、折扣因子、学习率，以及控制其下降的 `ε` 参数——起始值、最小值和范围（以回合数为单位）——以及适用的
    eligibility trace 衰减参数：
- en: '[PRE122]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Initialize the Q-table, set all values equal to `1` except for terminal states,
    and set an array that will collect all the agent''s performance evaluations during
    training:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 表，除了终止状态外，所有值都设置为 `1`，并设置一个数组来收集训练过程中智能体的所有表现评估：
- en: '[PRE123]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Start the SARSA training loop by looping among all episodes:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在所有回合中循环来启动SARSA训练循环：
- en: '[PRE124]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Define an epsilon value based on the current episode''s run:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前回合的运行定义一个epsilon值：
- en: '[PRE125]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Initialize the eligibility traces table to `0`:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将资格迹表初始化为`0`：
- en: '[PRE126]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Reset the environment, choose the first action with an ε-greedy policy, and
    start the in-episode loop:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境，使用ε-贪婪策略选择第一个动作，并开始回合内循环：
- en: '[PRE127]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Update the eligibility traces and assign a weight of `1` to the last visited
    state:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新资格迹并为最后访问的状态分配一个`1`的权重：
- en: '[PRE128]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Step through the environment with the selected action and retrieve the new
    state, reward, and done conditions:'
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用选定的动作在环境中执行一步，获取新的状态、奖励和结束条件：
- en: '[PRE129]'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Select the new action with the ε-greedy policy:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ε-贪婪策略选择新的动作：
- en: '[PRE130]'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Calculate the ![b](img/B16182_07_33e.png) update and update the Q-table using
    the SARSA TD(![a](img/B16182_07_33h.png)) rule:'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![b](img/B16182_07_33e.png)更新，并使用SARSA TD(![a](img/B16182_07_33h.png))规则更新Q表：
- en: '[PRE131]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Update the state and action with new state and action values:'
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的状态和动作值更新状态和动作：
- en: '[PRE132]'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Evaluate the agent''s average performance:'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估智能体的平均表现：
- en: '[PRE133]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Plot the SARSA agent''s mean reward history during training:'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制SARSA智能体在训练过程中的平均奖励历史：
- en: '[PRE134]'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'This generates the following output:'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE135]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The plot for this can be visualized as follows:'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个过程的图表可以如下可视化：
- en: '![Figure 7.35: Average reward of an epoch trend over training epochs'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.35：训练回合中一个epoch的平均奖励趋势'
- en: '](img/B16182_07_35.jpg)'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_35.jpg)'
- en: 'Figure 7.35: Average reward of an epoch trend over training epochs'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.35：训练回合中一个epoch的平均奖励趋势
- en: As we can see, SARSA's TD(![a](img/B16182_07_35a.png)) performance grows over
    time as the `ε` parameter is annealed, thus reaching the value of 0 in the limit,
    and thereby obtaining the greedy policy. It also demonstrates that the algorithm
    is capable of reaching 100% success after learning. With respect to the one-step
    SARSA model, as seen in *Figure 7.8*, here, we can see that it reaches maximum
    performance faster, showing a notable improvement.
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，SARSA的TD(![a](img/B16182_07_35a.png))表现随着`ε`参数的退火而逐步提高，从而在极限情况下达到0，并最终获得贪婪策略。这还表明该算法能够在学习后达到100%的成功率。与单步SARSA模型相比，正如*图7.8*所示，我们可以看到它更快地达到了最大性能，表现出显著的改进。
- en: 'Evaluate the greedy policy''s performance for the trained agent (Q-table):'
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估训练后的智能体（Q表）在贪婪策略下的表现：
- en: '[PRE136]'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'The output will be as follows:'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE137]'
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Display the Q-table values:'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示Q表的值：
- en: '[PRE138]'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'This generates the following output:'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE139]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: This output shows the values of the complete state-action value function for
    our problem. These values are then used to generate the optimal policy by means
    of the greedy selection rule.
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该输出显示了我们问题的完整状态-动作价值函数的值。这些值随后用于通过贪婪选择规则生成最优策略。
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy:'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出找到的贪婪策略，并与最优策略进行比较：
- en: '[PRE140]'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'This produces the following output:'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE141]'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: As you can see, our SARSA algorithm has been able to correctly solve the FrozenLake-v0
    environment by being able to learn the optimal policy under the deterministic
    transition dynamics. In fact, as we can see, for every state of the grid world,
    the greedy policy that was obtained with the Q-table that was calculated by our
    algorithm prescribes an action that is in accordance with the optimal policy that
    was defined by analyzing the environment problem. As we already saw, there are
    two states in which there are two equally optimal actions, and the agent correctly
    implements one of them.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的SARSA算法已经能够通过在确定性转移动态下学习最优策略来正确解决FrozenLake-v0环境。实际上，正如我们所见，对于网格世界中的每个状态，通过我们的算法计算出的Q表获得的贪婪策略都会给出与通过分析环境问题定义的最优策略一致的动作。正如我们之前看到的，有两个状态具有两个同等最优的动作，智能体正确地执行了其中之一。
- en: Note
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2YdePoa](https://packt.live/2YdePoa).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参阅[https://packt.live/2YdePoa](https://packt.live/2YdePoa)。
- en: You can also run this example online at [https://packt.live/3ek4ZXa](https://packt.live/3ek4ZXa).
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/3ek4ZXa](https://packt.live/3ek4ZXa)上在线运行此示例。
- en: We can now proceed and test how it behaves when exposed to stochastic dynamics.
    We'll do this in the next exercise. Just like when using one-step SARSA, in this
    case, we want to give the agent the freedom to take advantage of the 0 penalty
    for intermediate steps to minimize risk of falling into the holes, so in this
    case, we have to set the discount factor's gamma equal to 1\. This means that
    instead of using `gamma = 0.9`, we will use `gamma = 1.0`.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续，测试它在暴露于随机动态下时的表现。我们将在下一个练习中进行测试。就像使用一步 SARSA 时一样，在这种情况下，我们希望给予智能体自由，以便利用中间步骤的零惩罚，减少掉入陷阱的风险，因此，在这种情况下，我们必须将折扣因子
    gamma 设置为 1。 这意味着我们将使用 `gamma = 1.0`，而不是使用 `gamma = 0.9`。
- en: 'Exercise 7.05: Using TD(λ) SARSA to Solve FrozenLake-v0 Stochastic Transitions'
  id: totrans-550
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.05：使用 TD(λ) SARSA 解决 FrozenLake-v0 随机过渡
- en: In this exercise, we will implement our SARSA(λ) algorithm to solve the FrozenLake-v0
    environment under the deterministic environment dynamics. As we saw earlier in
    this chapter, when talking about one-step TD methods, the optimal policy looks
    completely different with respect to the previous exercise since it needs to take
    care of the stochasticity factor. This imposes a new challenge for the SARSA(λ)
    algorithm. We will see how it will still be able to solve this task in this exercise.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实现我们的 SARSA(λ) 算法来解决在确定性环境动态下的 FrozenLake-v0 环境。正如我们在本章前面看到的，当谈论一步
    TD 方法时，最优策略与前一个练习完全不同，因为它需要考虑随机性因素。这对 SARSA(λ) 算法提出了新的挑战。我们将看到它如何仍然能够在本练习中解决这个任务。
- en: 'Follow these steps to complete this exercise:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成本练习：
- en: 'Import the required modules:'
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块：
- en: '[PRE142]'
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'Instantiate the `gym` environment called `FrozenLake-v0` using the `is_slippery`
    flag set to `True` in order to enable stochasticity:'
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用设置了 `is_slippery=True` 标志的 `gym` 环境实例化 `FrozenLake-v0`，以启用随机性：
- en: '[PRE143]'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Take a look at the action and observation spaces:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下动作和观察空间：
- en: '[PRE144]'
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'This will print out the following:'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '[PRE145]'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'Create two dictionaries to easily translate the `actions` numbers into moves:'
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个字典，以便轻松地将`actions`数字转换为移动：
- en: '[PRE146]'
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Reset the environment and render it to take a look at the grid problem:'
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并渲染它，以查看网格问题：
- en: '[PRE147]'
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'The output will be as follows:'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.36: Environment''s initial state'
  id: totrans-566
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.36：环境的初始状态](img/B16182_07_36.jpg)'
- en: '](img/B16182_07_36.jpg)'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_36.jpg)'
- en: 'Figure 7.36: Environment''s initial state'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.36：环境的初始状态
- en: 'Visualize the optimal policy for this environment:'
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化该环境的最优策略：
- en: '[PRE148]'
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'This prints out the following output:'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下输出：
- en: '[PRE149]'
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: This represents the optimal policy for this environment. Except for two states,
    all the others have a single optimal action associated with them. In fact, as
    described earlier in this chapter, optimal actions here are those that bring the
    agent away from the holes, or from tiles that have a chance greater than zero
    to lead the agent into tiles placed near holes. Two states have multiple optimal
    actions associated with them that are all equally optimal, as intended for this
    task.
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这代表了该环境的最优策略。除了两个状态，其他所有状态都与单一的最优行为相关联。事实上，正如本章前面描述的，最优行为是那些将智能体远离陷阱的行为，或者远离可能导致智能体掉入陷阱的格子。两个状态有多个等效的最优行为，这正是本任务的要求。
- en: 'Define the functions that will take ε-greedy actions:'
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将采取 ε-贪心行为的函数：
- en: '[PRE150]'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Define a function that will take greedy actions:'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个将采取贪心行为的函数：
- en: '[PRE151]'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'Define a function that will calculate the agent''s average performance:'
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，计算智能体的平均表现：
- en: '[PRE152]'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Set the number of total episodes, the number of steps representing the interval
    by which we will evaluate the agent''s average performance, the discount factor,
    the learning rate, and the `ε` parameters ruling its decrease – the starting value,
    minimum value, and range (in terms of the number of episodes) – over which the
    decrease is spread, as well as the eligibility trace''s decay parameter:'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置总回合数、表示我们将评估智能体平均表现的步数间隔、折扣因子、学习率以及控制`ε`衰减的参数——起始值、最小值以及在一定回合数内衰减的范围，以及资格迹衰减参数：
- en: '[PRE153]'
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'Initialize the Q-table, set all the values equal to one except for terminal
    states, and set an array so that it collects all agent performance evaluations
    during training:'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 表，将所有值设置为 1，除终止状态外，并设置一个数组来收集在训练过程中所有智能体的表现评估：
- en: '[PRE154]'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'Start the SARSA training loop by looping among all episodes:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在所有回合中循环，开始 SARSA 训练循环：
- en: '[PRE155]'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'Define the epsilon value based on the current episode run:'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前剧集运行定义ε值：
- en: '[PRE156]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Initialize the eligibility traces table to 0:'
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化资格迹表为0：
- en: '[PRE157]'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Reset the environment and state your choice for the first action with an ε-greedy
    policy. Then, start the in-episode loop:'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并根据ε-贪婪策略设置初始动作选择。然后，开始剧集内部循环：
- en: '[PRE158]'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Update the eligibility traces by applying decay and making the last state-action
    pair the most important one:'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过应用衰减并使最后一个状态-动作对最重要来更新资格迹：
- en: '[PRE159]'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'Define the environment step with the selected action and retrieval of the new
    state, reward, and done conditions:'
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义环境步骤，选择动作并获取新的状态、奖励和完成条件：
- en: '[PRE160]'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'Select a new action with the ε-greedy policy:'
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ε-贪婪策略选择新动作：
- en: '[PRE161]'
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'Calculate the ![b](img/B16182_07_36a.png) update and update the Q-table with
    the SARSA TD(![a](img/B16182_07_36b.png)) rule:'
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算![b](img/B16182_07_36a.png)更新并使用SARSA TD(![a](img/B16182_07_36b.png))规则更新Q表：
- en: '[PRE162]'
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'Update the state and action with new values:'
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新值更新状态和动作：
- en: '[PRE163]'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'Evaluate the average agent performance:'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估代理的平均表现：
- en: '[PRE164]'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Plot the SARSA agent''s mean reward history during training:'
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制SARSA代理在训练期间的平均奖励历史：
- en: '[PRE165]'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'This generates the following output:'
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE166]'
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'The plot for this can be visualized as follows:'
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以通过以下方式可视化该图：
- en: '![Figure 7.37: Average reward of an epoch trend over training epochs'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.37：训练过程中每个时期的平均奖励趋势'
- en: '](img/B16182_07_37.jpg)'
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_37.jpg)'
- en: 'Figure 7.37: Average reward of an epoch trend over training epochs'
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.37：训练过程中每个时期的平均奖励趋势
- en: Again, in comparison to the previous TD(0) SARSA case seen in *Figure 7.15*,
    this plot clearly shows us how the algorithm's performance improves over epochs,
    even when stochastic dynamics are considered. The behavior is very similar, and
    it also shows that, in the case of stochastic dynamics, it is not possible to
    obtain a perfect performance, in other words, reaching the goal 100% of the time.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次与*图7.15*中看到的先前TD(0) SARSA情况进行比较，图形清楚地展示了即使在考虑随机动态时，算法的性能如何随着训练轮次的增加而改善。行为非常相似，也表明在随机动态的情况下，无法获得完美的表现，换句话说，无法100%达到目标。
- en: 'Evaluate the greedy policy''s performance of the trained agent (Q-table):'
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估训练代理（Q表）的贪婪策略表现：
- en: '[PRE167]'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'This prints out the following output:'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会打印出以下输出：
- en: '[PRE168]'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Display the Q-table values:'
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示Q表的值：
- en: '[PRE169]'
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'This generates the following output:'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE170]'
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: This output shows the values of the complete state-action value function for
    our problem. These values are then used to generate the optimal policy by means
    of the greedy selection rule.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此输出显示了我们问题的完整状态-动作值函数的值。然后，通过贪婪选择规则使用这些值来生成最优策略。
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy:'
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出找到的贪婪策略，并与最优策略进行比较：
- en: '[PRE171]'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: 'This produces the following output:'
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '[PRE172]'
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Also, as in the case of stochastic environment dynamics, the SARSA algorithm
    with eligibility traces has been able to correctly learn the optimal policy.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在随机环境动态的情况下，带资格迹的SARSA算法也能够正确学习到最优策略。
- en: Note
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2CiyZVf](https://packt.live/2CiyZVf).
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2CiyZVf](https://packt.live/2CiyZVf)。
- en: You can also run this example online at [https://packt.live/2Np7zQ9](https://packt.live/2Np7zQ9).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/2Np7zQ9](https://packt.live/2Np7zQ9)上在线运行此示例。
- en: With this exercise, we've completed our study of temporal difference methods
    and covered many of the aspects, from their most simple one-step formulation to
    the most advanced ones. We are now able to combine multi-step methods without
    the restriction of having to wait until the end of the episode to update the state-value
    (or state-action pair) function. To complete our journey, we'll conclude with
    a quick comparison of the methods we explained in this chapter with those explained
    in *Chapter 5, Dynamic Programming*, and *Chapter 6, Monte Carlo Methods*.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本练习，我们完成了对时间差分方法的学习，涵盖了从最简单的一步式公式到最先进的方法。现在，我们能够在不受剧集结束后更新状态值（或状态-动作对）函数限制的情况下，结合多步方法。为了完成我们的学习旅程，我们将快速比较本章解释的方法与*第5章
    动态规划*和*第6章 蒙特卡洛方法*中解释的方法。
- en: The Relationship between DP, Monte-Carlo, and TD Learning
  id: totrans-631
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态规划（DP）、蒙特卡洛（Monte-Carlo）和时间差分（TD）学习之间的关系
- en: From what we've learned in this chapter, and as we've stated multiple times,
    it is clear how temporal difference learning has characteristics in common with
    both Monte Carlo methods and dynamic programming ones. Like the former, it learns
    directly from experience, without leveraging a model of the environment representing
    transition dynamics or knowledge of the reward function involved in the task.
    Like the latter, it bootstraps, meaning that it updates the value function estimate
    partially based on other estimates, thereby circumventing the need to wait until
    the end of the episode. This point is particularly important since, in practice,
    very long episodes (or even infinite ones) can be encountered, making MC methods
    impractical and too slow. This strict relation plays a central role in reinforcement
    learning theory.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在本章中学到的内容来看，正如我们多次提到的，时间差分学习（TD）显然具有与蒙特卡罗方法和动态规划方法相似的特点。像前者一样，TD直接从经验中学习，而不依赖于表示过渡动态的环境模型或任务中涉及的奖励函数的知识。像后者一样，TD通过自举（bootstrapping）更新，即部分基于其他估计更新值函数，这样就避免了必须等待直到一轮结束的需求。这个特点尤为重要，因为在实际中，我们可能遇到非常长的回合（甚至是无限回合），使得蒙特卡罗方法变得不切实际且过于缓慢。这种严格的关系在强化学习理论中扮演着核心角色。
- en: We have also learned about N-step methods and eligibility traces, two different
    but related topics that allow us to frame TD method's theory as a general picture
    capable of fusing together MC and TD methods. In particular, the eligibility traces
    concept allowed us to formally represent both of them, with the additional advantage
    of implementing a perspective change from a forward view to a more efficient incremental
    backward view, which allows us to extend MC methods even to non-episodic problems.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到了 N 步方法和资格迹，这两个不同但相关的主题让我们能够将 TD 方法的理论框架呈现为一个通用的图景，能够将蒙特卡罗和 TD 方法融合在一起。特别是资格迹的概念让我们能够正式地表示它们，具有额外的优势，即将视角从前向视角转变为更高效的增量反向视角，从而使我们能够将蒙特卡罗方法扩展到非周期性问题。
- en: When bringing TD and MC methods under the same theory umbrella, eligibility
    traces demonstrate their value in making TD methods more robust to non-Markovian
    tasks, a typical problem in which MC algorithms behave better than TD ones. Thus,
    eligibility traces, even if typically coupled with an increased computational
    overhead, offer a better learning capability in general since they are both faster
    and more robust.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 当把 TD 和蒙特卡罗方法纳入同一个理论框架时，资格迹（eligibility traces）展示了它们在使 TD 方法更稳健地应对非马尔科夫任务方面的价值，这是蒙特卡罗算法表现更好的典型问题。因此，资格迹即使通常伴随着计算开销的增加，通常也能提供更好的学习能力，因为它们既更快速又更稳健。
- en: It is now time for us to tackle the final activity of this chapter, where we
    will apply what we have learned from the theory and exercises we've covered on
    TD methods.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候处理本章的最后一个活动了，我们将把在理论和已覆盖的 TD 方法练习中学到的知识付诸实践。
- en: 'Activity 7.01: Using TD(0) Q-Learning to Solve FrozenLake-v0 Stochastic Transitions'
  id: totrans-636
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 7.01：使用 TD(0) Q-Learning 解决 FrozenLake-v0 随机过渡问题
- en: 'The goal of this activity is for you to adapt the TD(0) Q-learning algorithm
    to solve the FrozenLake-v0 environment under the stochastic transition dynamics.
    We have already seen that the optimal policy appears as follows:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目标是让你将 TD(0) Q-learning 算法应用于解决 FrozenLake-v0 环境中的随机过渡动态。我们已经看到，最优策略如下所示：
- en: '![Figure 7.38: Optimal policy –  D = Down move, R = Right move, U = Up move,'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.38: 最优策略 – D = 向下移动，R = 向右移动，U = 向上移动，'
- en: and L = Left move
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 和 L = 向左移动
- en: '](img/B16182_07_38.jpg)'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_07_38.jpg)'
- en: 'Figure 7.38: Optimal policy – D = Down move, R = Right move, U = Up move, and
    L = Left move'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.38: 最优策略 – D = 向下移动，R = 向右移动，U = 向上移动，L = 向左移动'
- en: 'Making Q-learning converge on this environment is not a simple task, but it
    is possible. In order to make this a little bit easier, we can use a value for
    the discount factor gamma that''s equal to `0.99`. The following steps will help
    you to complete this exercise:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 让 Q-learning 在这个环境中收敛并非易事，但这是可能的。为了让这个过程稍微简单一些，我们可以使用一个折扣因子 γ 值，设为 `0.99`。以下步骤将帮助你完成此练习：
- en: Import all the required modules.
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的模块。
- en: Instantiate the gym environment and print out the observation and action spaces.
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化健身环境并打印出观察空间和动作空间。
- en: Reset the environment and render the starting state.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并呈现初始状态。
- en: Define and print out the optimal policy for reference.
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并打印出最优策略以供参考。
- en: Define the functions for implementing the greedy and ε-greedy policies.
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义实现贪婪和 ε-贪婪策略的函数。
- en: Define a function that will evaluate the agent's average performance and initialize
    the Q-table.
  id: totrans-648
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来评估智能体的平均表现，并初始化Q表。
- en: Define the learning method hyperparameters (ε, discount factor, total number
    of episodes, and so on).
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义学习方法的超参数（ε、折扣因子、总集数等）。
- en: Implement the Q-learning algorithm.
  id: totrans-650
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现Q学习算法。
- en: Train the agent and plot the average performance as a function of training epochs.
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练智能体并绘制平均性能随训练轮次变化的图表。
- en: Display the Q-values found and print out the greedy policy while comparing it
    with the optimal one.
  id: totrans-652
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示找到的Q值，并在将其与最优策略进行比较时，打印出贪婪策略。
- en: The final output of this activity is very similar to the ones you've encountered
    for all of the exercises in this chapter. We want to compare the policy found
    by our agent that was trained using the prescribed method with the optimal one
    to make sure we succeeded in making it learn the optimal policy correctly.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的最终输出与本章所有练习中遇到的非常相似。我们希望将使用指定方法训练的智能体找到的策略与最优策略进行比较，以确保我们成功地让其正确地学习到最优策略。
- en: 'The optimal policy should be as follows:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略应如下所示：
- en: '[PRE173]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: Note
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 726.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第726页找到。
- en: By completing this activity, we've learned how to correctly implement and set
    up a one-step Q-learning algorithm by appropriately tuning its hyperparameters
    to solve an environment with stochastic transition dynamics. We monitored the
    agent's performance during training, and we confronted ourselves with the role
    of the reward discount factor. We selected a value for it, allowing us to make
    our agent learn the optimal policy for this specific task, even if the maximum
    reward for this environment is bound and there is no possibility of completing
    the episode 100% of the time.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，我们学会了如何通过适当调整超参数，正确地实现和设置单步Q学习算法，以解决具有随机过渡动态的环境问题。我们在训练过程中监控了智能体的表现，并面对了奖励折扣因子的作用。我们为其选择了一个值，使得我们的智能体能够学习到针对这一特定任务的最优策略，即便该环境的最大奖励是有限的，并且无法保证100%完成任务。
- en: Summary
  id: totrans-659
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter dealt with temporal difference learning. We started by studying
    one-step methods in both their on-policy and off-policy implementations, leading
    to us learning about the SARSA and Q-learning algorithms, respectively. We tested
    these algorithms on the FrozenLake-v0 problem and covered both deterministic and
    stochastic transition dynamics. Then, we moved on to the N-step temporal difference
    methods, the first step toward the unification of TD and MC methods. We saw how
    on-policy and off-policy methods are extended to this case. Finally, we studied
    TD methods with eligibility traces, which constitute the most relevant step toward
    the formalization of a unique theory describing both TD and MC algorithms. We
    extended SARSA to eligibility tracing, too, and learned about this through implementing
    two exercises where it has been implemented and applied to the FrozenLake-v0 environment
    under both deterministic and stochastic transition dynamics. With this, we have
    been able to successfully learn about the optimal policy in all cases, thereby
    demonstrating how these methods are sound and robust.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了时序差分学习。我们首先研究了单步方法，包括其在策略内部和策略外部的实现，进而学习了SARSA和Q学习算法。我们在FrozenLake-v0问题上测试了这些算法，涉及了确定性和随机过渡动态。接着，我们进入了N步时序差分方法，这是TD和MC方法统一的第一步。我们看到，策略内和策略外方法在这种情况下是如何扩展的。最后，我们研究了带有资格迹的时序差分方法，它们构成了描述TD和MC算法的统一理论的最重要步骤。我们还将SARSA扩展到资格迹，并通过实现两个练习在FrozenLake-v0环境中应用这一方法，涵盖了确定性和随机过渡动态。通过这些，我们能够在所有情况下成功地学习到最优策略，从而证明这些方法是可靠且健壮的。
- en: Now, it is time to move on to the next chapter, in which we will address the
    multi-armed bandit problem, a classic setting that's often encountered when studying
    reinforcement learning theory and the application of RL algorithms.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候进入下一章了，在这一章中，我们将讨论多臂老虎机问题，这是一个经典的设置，在研究强化学习理论及其算法应用时常常会遇到。
