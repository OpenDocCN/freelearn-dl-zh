- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Extracting Semantic Representations with spaCy Pipelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy管道提取语义表示
- en: In this chapter, we will apply what we learned in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056)
    *,* to the **Airline Travel Information System** ( **ATIS** ), a well-known airplane
    ticket reservation system dataset. The data consists of utterances – sentences
    of users asking for information. First, we will extract the named entities, creating
    our own extraction patterns with **SpanRuler** . Then we will determine the intent
    of the user utterance with **DepedencyMatcher** patterns. We will also use the
    code to extract the intent and create our own custom spaCy component and use it
    to process large datasets faster with the **Language.pipe()** method.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将把在[*第4章*](B22441_04.xhtml#_idTextAnchor056)中学到的知识应用到**航空公司旅行信息系统**（**ATIS**）这个知名机票预订系统数据集上。数据包括用户询问信息的句子。首先，我们将提取命名实体，使用**SpanRuler**创建自己的提取模式。然后，我们将使用**DepedencyMatcher**模式确定用户话语的意图。我们还将使用代码提取意图，并创建自己的自定义spaCy组件，使用**Language.pipe()**方法以更快的速度处理大型数据集。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Extracting named entities with **SpanRuler**
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**SpanRuler**提取命名实体
- en: Extracting dependency relations with **DependencyMatcher**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**DependencyMatcher**提取依存关系
- en: Creating a pipeline component using extension attributes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扩展属性创建管道组件
- en: Running the pipeline with large datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大数据集运行管道
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we’ll process a dataset. The dataset and the chapter code can
    be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个数据集。数据集和本章代码可以在[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)找到。
- en: We will use the **pandas** library to manipulate our dataset. We'll also used
    the **wget** command-line tool. **pandas** can be installed via **pip** and **wget**
    is preinstalled in many Linux distributions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**pandas**库来操作我们的数据集。我们还将使用**wget**命令行工具。**pandas**可以通过**pip**安装，而**wget**在许多Linux发行版中是预安装的。
- en: Extracting named entities with SpanRuler
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用**SpanRuler**提取命名实体
- en: In many NLP applications, including semantic parsing, we start looking for meaning
    in a text by examining the entity types and placing an entity extraction component
    into our NLP pipelines. Named entities play a key role in understanding the meaning
    of user text.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多自然语言处理应用中，包括语义解析，我们通过检查实体类型并将实体提取组件放入我们的NLP管道中来开始寻找文本中的意义。命名实体在理解用户文本的意义中起着关键作用。
- en: We’ll also start a semantic parsing pipeline by extracting the named entities
    from our corpus. To understand what sort of entities we want to extract, first,
    we’ll get to know the ATIS dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过从我们的语料库中提取命名实体来启动语义解析管道。为了了解我们想要提取哪种类型的实体，首先，我们将了解ATIS数据集。
- en: Getting to know the ATIS dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解ATIS数据集
- en: Throughout this chapter, we’ll work with the ATIS corpus. ATIS is a well-known
    dataset; it’s one of the standard benchmark datasets for intent classification.
    The dataset consists of utterances from customers who want to book a flight and/or
    get information about flights, including flight costs, destinations, and timetables.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用ATIS语料库。ATIS是一个知名的数据集；它是意图分类的标准基准数据集之一。该数据集包括想要预订航班和/或获取航班信息（包括航班费用、目的地和时间表）的客户的话语。
- en: 'No matter what the NLP task is, you should always go over your corpus with
    the naked eye. We want to get to know our corpus to integrate our observations
    of the corpus into our code. While viewing our text data, we usually keep an eye
    on the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无论NLP任务是什么，你都应该用肉眼仔细检查你的语料库。我们想要了解我们的语料库，以便将我们对语料库的观察整合到我们的代码中。在查看我们的文本数据时，我们通常会关注以下方面：
- en: What kinds of utterances are there? Is it a short text corpus or does the corpus
    consist of long documents or medium-length paragraphs?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪些类型的语句？是短文本语料库，还是语料库由长文档或中等长度的段落组成？
- en: What sort of entities does the corpus include? Examples include people’s names,
    city names, country names, organization names, and so on. Which ones do we want
    to extract?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语料库包括哪些类型的实体？例如，包括人名、城市名、国家名、组织名等。我们想要提取哪些？
- en: How is punctuation used? Is the text correctly punctuated, or is no punctuation
    used at all?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号是如何使用的？文本是否正确使用了标点，或者根本就没有使用标点？
- en: Did users follow the grammatical rules? How are the grammatical rules followed?
    Is the capitalization correct? Are there misspelled words?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否遵循了语法规则？语法规则是如何遵循的？大写是否正确？是否有拼写错误？
- en: 'Before starting any processing, we’ll examine our corpus. Here’s how:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始任何处理之前，我们将检查我们的语料库。以下是方法：
- en: 'Let’s go ahead and download the dataset:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续下载数据集：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset is a two-column CSV file.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是一个两列的CSV文件。
- en: Important note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you are running the code on a Jupyter notebook, you can add a **!** before
    the commands to run them there.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个Jupyter笔记本上运行代码，你可以在命令前添加一个 **!** 来在那里运行它们。
- en: 'Next, we’ll get some insights into the dataset statistics with **pandas** .
    **pandas** is a popular data manipulation library that is frequently used by data
    scientists. You can read more at [https://pandas.pydata.org/docs/getting_started/intro_tutorials/](https://pandas.pydata.org/docs/getting_started/intro_tutorials/)
    :'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 **pandas** 对数据集统计进行一些洞察。**pandas** 是一个流行的数据处理库，常被数据科学家使用。你可以在 [https://pandas.pydata.org/docs/getting_started/intro_tutorials/](https://pandas.pydata.org/docs/getting_started/intro_tutorials/)
    上了解更多信息。
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The **shape** attribute returns a tuple representing the dimensionality of **DataFrame**
    . We can see that the dataset has two columns and 4,978 rows.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**shape** 属性返回一个表示 **DataFrame** 维度的元组。我们可以看到数据集有两列和4,978行。'
- en: 'Let’s create a bar plot to see the number of utterances in the dataset:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个条形图来查看数据集中的出口数量：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The **value_counts()** method returns a series containing counts of unique
    values. The pandas library uses Matplotlib under the hood to plot this in a bar
    chart; this is the result:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**value_counts()** 方法返回一个包含唯一值计数的序列。pandas库在底层使用Matplotlib来绘制条形图；这是结果：'
- en: '![Figure 5.1 – Bar chart with utterance frequencies](img/B22441_05_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 出口频率条形图](img/B22441_05_01.jpg)'
- en: Figure 5.1 – Bar chart with utterance frequencies
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 出口频率条形图
- en: Most user requests are for information about flights, followed by requests about
    airfares. However, before extracting the utterances, we will learn how to extract
    named entities. Let’s do this in the next section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数用户请求是关于航班的信息，其次是关于机票的请求。然而，在提取出口之前，我们将学习如何提取命名实体。让我们在下一节中这样做。
- en: Defining LOCATION entities
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义位置实体
- en: 'In this section, our goal is to extract **LOCATION** entities. The pipeline
    of the **en_core_web_sm** model already has an **NER** component. Let’s see what
    entities the default NER model extracts from the **"i want to fly from boston
    at 838 am and arrive in denver at 1110 in the** **morning"** utterance:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的目标是提取 **位置** 实体。**en_core_web_sm** 模型的管道已经有一个 **NER** 组件。让我们看看默认NER模型从
    **"i want to fly from boston at 838 am and arrive in denver at 1110 in the morning"**
    这句话中提取了哪些实体：
- en: 'First, we import the libraries:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入库：
- en: '[PRE3]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then we load the spaCy model and process a sentence:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们加载spaCy模型并处理一个句子：
- en: '[PRE4]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we display the entities with **displacy** :'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 **displacy** 显示实体：
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see the result in *Figure 5* *.2* :'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在 *图5.2* 中看到结果：
- en: '![Figure 5.2 – Entities extracted by the NER component](img/B22441_05_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – NER组件提取的实体](img/B22441_05_02.jpg)'
- en: Figure 5.2 – Entities extracted by the NER component
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – NER组件提取的实体
- en: The NER model finds the **Global Political Entity** ( **GPE** ) for **boston**
    and **denver** , but knowing the cities is not sufficient. We want to know **from**
    and **where** they want to fly. This means that in this case, the **adpositions**
    (a cover term for prepositions and postpositions) are important. spaCy uses the
    Universal **Parts-of-Speech** ( **POS** ) tags, so the adposition tag is named
    **"ADP"** . You can see all the descriptions for the POS tags, dependency label,
    or entity type of spaCy in the glossary ( [https://github.com/explosion/spaCy/blob/master/spacy/glossary.py](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)
    ).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NER模型为 **boston** 和 **denver** 找到 **全球政治实体** ( **GPE** )，但仅知道这些城市是不够的。我们想知道他们想从哪里飞以及飞往何处。这意味着在这种情况下，**介词**（一个包括介词和后置介词的通用术语）很重要。spaCy使用通用的
    **词性** ( **POS** ) 标签，因此介词标签被命名为 **"ADP"**。你可以在词汇表中看到所有POS标签、依存标签或spaCy实体类型的描述（[https://github.com/explosion/spaCy/blob/master/spacy/glossary.py](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)）。
- en: Getting back to the utterance example, **from boston** and **in denver** are
    the entities we want to extract. Since we know exactly what POS tags and GPE entities
    we need to create the new entities, a good way of implementing this extraction
    is to rely on the **Tagger** and **EntityRecognizer** components of the NLP pipeline.
    We’ll do this by creating rules to extract the tokens based on the tags. spaCy
    makes this easy to do with the **SpanRuler** component.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 回到之前的句子示例，**从波士顿**和**在丹佛**是我们想要提取的实体。由于我们知道需要哪些 POS 标签和 GPE 实体来创建新的实体，因此实现这种提取的一个好方法就是依赖于
    NLP 管道中的**Tagger**和**EntityRecognizer**组件。我们将通过创建基于标签的规则来提取标记。spaCy 使用**SpanRuler**组件使得这一过程变得简单易行。
- en: Adding the SpanRuler component to our processing pipeline
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 SpanRuler 组件添加到我们的处理管道中
- en: 'Customizing NLP pipelines is very straightforward with spaCy. Each pipeline
    is created with a combination of spaCy components. It might not be very clear
    at first, but when we load an out-of-the-box spaCy model, it already comes with
    several different components:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 定制 NLP 管道非常简单。每个管道都是通过 spaCy 组件的组合创建的。一开始可能不太清楚，但当我们加载现成的 spaCy 模型时，它已经包含了几种不同的组件：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The **nlp.pipe_names** attribute returns the component names, in order. *Figure
    5* *.3* shows all of them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**nlp.pipe_names**属性按顺序返回组件名称。*图 5.3*显示了所有这些组件。'
- en: '![Figure 5.3 – The default components of the en_core_web_sm model](img/B22441_05_03.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – en_core_web_sm 模型的默认组件](img/B22441_05_03.jpg)'
- en: Figure 5.3 – The default components of the en_core_web_sm model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – en_core_web_sm 模型的默认组件
- en: We can see that the **en_core_web_sm** model comes with the **['tok2vec', 'tagger',
    'parser', 'attribute_ruler', 'lemmatizer', 'ner']** components by default. Each
    component returns a processed **Doc** object, which is then passed on to the next
    component.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，**en_core_web_sm**模型默认包含**['tok2vec', 'tagger', 'parser', 'attribute_ruler',
    'lemmatizer', 'ner']**组件。每个组件返回一个处理后的**Doc**对象，然后传递给下一个组件。
- en: You can add components to the processing pipeline using the **Language.add_pipe()**
    method (here nlp is the object of the Language class, which we will be using to
    call **add_pipe()** ). The **.add_pipe()** method expects a string with the name
    of the component. Under the hood, this method takes care of creating the component,
    adds it to the pipeline, and then returns the component object.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用**Language.add_pipe()**方法向处理管道添加组件（这里 nlp 是 Language 类的对象，我们将使用它来调用**add_pipe()**）。该方法期望一个包含组件名称的字符串。在底层，此方法负责创建组件，将其添加到管道中，然后返回组件对象。
- en: 'The **SpanRuler** component is an out-of-the-box component for rule-based span
    and named entity recognition. The component lets you add spans to **Doc.spans**
    and/or **Doc.ents** . Let’s try it for the first time:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**SpanRuler**组件是一个现成的基于规则的跨度命名实体识别组件。该组件允许您将跨度添加到**Doc.spans**和/或**Doc.ents**。让我们第一次尝试使用它：'
- en: 'First, we call the **add_pipe()** method:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们调用**add_pipe()**方法：
- en: '[PRE7]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we add the patterns using the **add_patterns()** method. They should be
    defined using a list of dictionaries containing the **"label"** and **"** **pattern"**
    keys:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用**add_patterns()**方法添加模式。它们应该使用包含**"label"**和**"pattern"**键的字典列表来定义：
- en: '[PRE8]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: spaCy uses *Thinc* ( [https://thinc.ai/docs/api-config#registry](https://thinc.ai/docs/api-config#registry)
    ) registry, a system that maps string keys to functions. The **"span_ruler"**
    string name is the string to reference the **SpanRuler** component. We then define
    a pattern named **LOCATION** and add it to the component using the **add_patterns()**
    method.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: spaCy 使用*Thinc*（[https://thinc.ai/docs/api-config#registry](https://thinc.ai/docs/api-config#registry)）注册表，这是一个将字符串键映射到函数的系统。**"span_ruler"**字符串名称是引用**SpanRuler**组件的字符串。然后我们定义一个名为**LOCATION**的模式，并使用**add_patterns()**方法将其添加到组件中。
- en: 'Unlike in **doc.ents** , overlapping matches are allowed in **doc.spans** .
    By default, **SpanRuler** adds the matches as **spans** to the **doc.spans["ruler"]**
    group of spans. Let’s process the text again and check whether **SpanRuler** did
    its job. Since the component adds the Span to the **"ruler"** key, we need to
    specify this to render the spans with **displacy** :'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与**doc.ents**不同，**doc.spans**允许重叠匹配。默认情况下，**SpanRuler**将匹配项作为**spans**添加到**doc.spans["ruler"]**跨度组中。让我们再次处理文本并检查**SpanRuler**是否完成了其工作。由于组件将
    Span 添加到**"ruler"**键，我们需要指定这一点以使用**displacy**渲染跨度：
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s see the results in *Figure 5* *.4* .
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看*图 5.4*的结果。
- en: '![Figure 5.4 – Spans extracted with SpanRuler](img/B22441_05_04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 使用 SpanRuler 提取的跨度](img/B22441_05_04.jpg)'
- en: Figure 5.4 – Spans extracted with SpanRuler
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 使用SpanRuler提取的跨度
- en: 'We can see that the component identified the **"from boston"** and **"in denver"**
    Span. **SpanRuler** has some settings that you can change. This can be done via
    the config argument on the **nlp.pipe()** method or using the **config.cfg** file.
    Let’s add the spans to **Doc.ents** instead of **doc.spans["ruler"]** :'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，组件识别了**"from boston"**和**"in denver"**跨度。**SpanRuler**有一些您可以更改的设置。这可以通过**nlp.pipe()**方法上的config参数或使用**config.cfg**文件来完成。让我们将跨度添加到**Doc.ents**而不是**doc.spans["ruler"]**：
- en: 'First, we remove the component of the pipeline because we only have one component
    with the same name:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们移除了管道中的组件，因为我们只有一个具有相同名称的组件：
- en: '[PRE10]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we set the **"annotate_ents"** parameter of the component to **True**
    . The entities added by **EntityRecognizer** are needed in our pattern, so we
    also need to set the overwrite parameter to **False** so we don’t overwrite them:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将组件的**"annotate_ents"**参数设置为**True**。由于我们的模式需要**EntityRecognizer**添加的实体，因此我们还需要将覆盖参数设置为**False**，这样我们就不覆盖它们：
- en: '[PRE11]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we create the component again with this config, add the previously created
    patterns and process the text again.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用此配置再次创建组件，添加之前创建的模式，并再次处理文本。
- en: '[PRE12]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By doing all that, the matches become entities, not spans.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过做所有这些，匹配项变成了实体，而不是跨度。
- en: 'Let’s see the new entities in *Figure 5* *.5* :'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看图5.5中的新实体：
- en: '![Figure 5.5 – Entities extracted using SpanRuler](img/B22441_05_05.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 使用SpanRuler提取的实体](img/B22441_05_05.jpg)'
- en: Figure 5.5 – Entities extracted using SpanRuler
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 使用SpanRuler提取的实体
- en: You can see that the **{'GPE', 'boston'}** and **{'GPE', 'denver'}** entities
    don’t exist anymore; they’re now **{'from boston', 'LOCATION'}** and **{'in denver',
    'LOCATION'}** , respectively. Overlapping entities are not allowed in **Doc.ents**
    , so they are filtered using the **util.filter_spans** function by default. This
    function keeps the first longest span over shorter spans.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，**{'GPE', 'boston'}**和**{'GPE', 'denver'}**实体不再存在；现在分别是**{'from boston',
    'LOCATION'}**和**{'in denver', 'LOCATION'}**。**Doc.ents**中不允许重叠实体，因此默认使用**util.filter_spans**函数进行过滤。此函数保留较短的跨度中第一个最长的跨度。
- en: 'You can override most of the **SpanRuler** settings. Some of the available
    settings are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以覆盖大多数**SpanRuler**设置。一些可用的设置如下：
- en: '**spans_filter** : A method to filter spans before they are assigned to **doc.spans**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spans_filter**：一个在将跨度分配给**doc.spans**之前过滤跨度的方法'
- en: '**ents_filter** : A method to filter spans before they are assigned to **doc.ents**'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ents_filter**：一个在将跨度分配给**doc.ents**之前过滤跨度的方法'
- en: '**validate** : A method to set whether patterns should be validated or passed
    to **Matcher** and **PhraseMatcher** as validate'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**validate**：一个设置是否应该验证模式或将其作为验证传递给**Matcher**和**PhraseMatcher**的方法'
- en: In this section, we’ve learned how to create and extract entities using **SpanRuler**
    . Having the pattern to extract **LOCATION** entities, we can now move on and
    extract the intention of utterances. Let’s do this in the next section using **DependencyMatcher**
    .
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用**SpanRuler**创建和提取实体。有了提取**位置**实体的模式，我们现在可以继续提取话语的意图。让我们在下一节中使用**DependencyMatcher**来完成这项工作。
- en: Extracting dependency relations with DependencyMatcher
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DependencyMatcher提取依赖关系
- en: To extract the *intent* of the utterances, we need to match tokens based on
    their syntax relationship with each other. The goal is to find out what sort of
    intent the user carries – to book a flight, purchase a meal on their already booked
    flight, cancel their flight, and so on. Every intent includes a verb (to book)
    and an object that the web acts on (flight, hotel, meal, and so on).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取话语的**意图**，我们需要根据它们之间的语法关系匹配标记。目标是找出用户携带的意图类型——预订航班、在他们已预订的航班上购买餐点、取消航班等。每个意图都包括一个动词（预订）和网页所作用的对象（航班、酒店、餐点等）。
- en: In this section, we’ll extract transitive verbs and their direct objects from
    utterances. We’ll begin our intent recognition section by extracting the transitive
    verb and the direct object of the verb. Before we move on to extracting transitive
    verbs and their direct objects, let’s first quickly go over the concepts of transitive
    verbs and direct/indirect objects.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从话语中提取及物动词及其直接宾语。我们将通过提取及物动词及其直接宾语来开始我们的意图识别部分。在我们继续提取及物动词及其直接宾语之前，让我们首先快速回顾一下及物动词和直接/间接宾语的概念。
- en: Linguistic primer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言学入门
- en: 'Let’s explore some linguistic concepts related to sentence structure, including
    verbs and verb-object relations. A verb is a very important component of the sentence
    as it indicates the *action* in the sentence. The object of the sentence is the
    *thing or person* that is affected by the action of the verb. Hence, there’s a
    natural connection between the sentence verb and objects. The concept of transitivity
    captures verb-object relations. A transitive verb is a verb that needs an object
    to act upon. Let’s see some examples:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些与句子结构相关的语言概念，包括动词和动词宾语关系。动词是句子中非常重要的组成部分，因为它表示句子中的*动作*。句子的宾语是受到动词动作影响的*事物或人*。因此，句子动词和宾语之间存在自然联系。及物性的概念捕捉了动词宾语关系。及物动词是需要作用对象的动词。让我们看看一些例子：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In these example sentences, **bought** , **loved** , and **borrowed** are transitive
    verbs. In the first sentence, **bought** is the transitive verb and **flowers**
    is its object, the thing that has been bought by the sentence subject. **I Loved**
    – **his cat** and **borrowed** – **my book** are transitive verb-object examples.
    We’ll focus on the first sentence again – what happens if we erase the **flowers**
    object? Let’s see that here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些例子句子中，**bought**、**loved**和**borrowed**是及物动词。在第一个句子中，**bought**是及物动词，**flowers**是其宾语，即句子主语所购买的事物。**I
    Loved** – **his cat**和**borrowed** – **my book**是及物动词宾语例子。我们再次关注第一个句子——如果我们删除**flowers**宾语会发生什么？让我们看看这里的情况：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You bought *what* ? Without an object, this sentence doesn’t carry any meaning
    at all. In the preceding sentences, each of the objects completes the meaning
    of the verb. This is a way of understanding whether a verb is transitive or not
    – erase the object and check whether the sentence remains semantically intact.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你买了*什么*？如果没有宾语，这个句子就完全没有意义了。在前面的句子中，每个宾语都完成了动词的意义。这是理解一个动词是否及物的一种方法——删除宾语并检查句子是否在语义上仍然完整。
- en: 'Some verbs are transitive and some verbs are intransitive. An intransitive
    verb is the opposite of a transitive verb; it doesn’t need an object to act upon.
    Let’s see some examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有些动词是及物的，有些动词是不及物的。不及物动词是及物动词的相反，它不需要作用对象。让我们看看一些例子：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In all the preceding sentences, the verbs make sense without an object. If
    we erase all the words other than the subject and object, these sentences are
    still meaningful:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有前面的句子中，即使没有宾语，动词也有意义。如果我们删除除了主语和宾语之外的所有单词，这些句子仍然是有意义的：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Pairing an intransitive verb with an object doesn’t make sense. You can’t run
    someone or something, you can’t shine something or someone, and you certainly
    cannot die something or someone.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将不及物动词与宾语搭配是没有意义的。你不能跑某人或某物，你不能使某人或某物发光，你当然也不能使某人或某物死亡。
- en: Sentence object
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 句子宾语
- en: 'As we remarked before, the object is the thing or person that is affected by
    the verb’s action. The action stated by the verb is committed by the sentence
    subject and the sentence object is affected. A sentence can be direct or indirect.
    A direct object answers the questions *who* and *what* . You can find the direct
    object by asking **The subject {verb} what/who?** . Here are some examples:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，宾语是受到动词动作影响的物体或人。动词所陈述的动作是由句子主语执行的，而句子宾语受到动作的影响。一个句子可以是直接的或间接的。直接宾语回答了*谁*和*什么*的问题。你可以通过问**主语{动词}什么/谁？**来找到直接宾语。以下是一些例子：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'An *indirect object* answers the questions **for what** , **for whom** , and/or
    **to whom** . Let’s see some examples:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*间接宾语*回答了**为了什么**、**为了谁**和/或**给谁**的问题。让我们看看一些例子：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Indirect objects are often preceded by the prepositions **to** , **for** , **from**
    , and so on. As you can see from these examples, an indirect object is also an
    object and is affected by the verb’s action, but its role in the sentence is a
    bit different. An indirect object is sometimes viewed as the recipient of the
    direct object.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 间接宾语通常由介词**to**、**for**、**from**等引导。正如你从这些例子中看到的，间接宾语也是一个宾语，并且受到动词动作的影响，但它在句子中的角色略有不同。间接宾语有时被视为直接宾语的接受者。
- en: This is all you need to know about transitive/intransitive verbs and direct/indirect
    objects to digest this chapter’s material. If you want to learn more about sentence
    syntax, you can read the great book *Linguistic Fundamentals for Natural Language
    Processing* by Emily Bender ( [https://dl.acm.org/doi/book/10.5555/2534456](https://dl.acm.org/doi/book/10.5555/2534456)
    ). We have covered the basics of sentence syntax, but this is still a great resource
    to learn about syntax in depth.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您需要了解的关于及物/不及物动词和直接/间接宾语的知识，以便消化本章的内容。如果您想了解更多关于句子语法的知识，可以阅读 Emily Bender
    的优秀书籍 *自然语言处理的语言学基础* ([https://dl.acm.org/doi/book/10.5555/2534456](https://dl.acm.org/doi/book/10.5555/2534456))。我们已经涵盖了句子语法的基础知识，但这仍然是一个深入了解语法的极好资源。
- en: Matching patterns with the DependencyMatcher component
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 **DependencyMatcher** 组件匹配模式
- en: 'The **DependencyMatcher** component lets us match patterns to extract information,
    but instead of defining a list of adjacent tokens as with **SpanRuler** patterns,
    the **DependencyMatcher** patterns match tokens *specifying the relations between
    them* . The component works with the dependencies extracted by the **DependencyParser**
    component. Let’s see the kind of information this component extracts in an example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**DependencyMatcher** 组件使我们能够将模式与提取信息相匹配，但与 **SpanRuler** 模式定义的相邻标记列表不同，**DependencyMatcher**
    模式匹配指定它们之间关系的标记。该组件与 **DependencyParser** 组件提取的依存关系一起工作。让我们通过一个示例看看这个组件提取的信息类型：'
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let´s see the results in *Figure 5* *.6* :'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们看看 *图 5.6* 的结果： '
- en: '![Figure 5.6 – The dependency arcs of the sentence (the rest of the sentence
    is omitted)](img/B22441_05_06.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 句子的依存弧（句子其余部分省略）](img/B22441_05_06.jpg)'
- en: Figure 5.6 – The dependency arcs of the sentence (the rest of the sentence is
    omitted)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 句子的依存弧（句子其余部分省略）
- en: The extracted dependency labels are the ones below the arcs. *Show* is a transitive
    verb, and in this sentence, its direct object is *flight* . This dependency is
    extracted by the **DependencyParser** component and is labeled as **dobj** ( direct
    object).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的依存标签是弧线下方的标签。*Show* 是一个及物动词，在这个句子中，它的直接宾语是 *flight*。这个依存关系由 **DependencyParser**
    组件提取，并标记为 **dobj**（直接宾语）。
- en: Our goal is to extract the intent, so we’ll define the pattern always looking
    for a **verb** and its **dobj** dependencies. **DependencyMatcher** uses Semgrex
    operators to define the patterns. The **Semgrex syntax** might be confusing at
    first, so let’s take it step by step.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是提取意图，因此我们将定义始终寻找 **动词** 和其 **dobj** 依存关系的模式。**DependencyMatcher** 使用 Semgrex
    操作符来定义模式。**Semgrex 语法** 可能一开始会让人困惑，所以让我们一步一步来。
- en: 'The **DependencyMatcher** patterns consist of a list of dictionaries. The first
    dictionary defines an anchor token using **RIGHT_ID** and **RIGHT_ATTRS** . **RIGHT_ID**
    is a unique name for the right-hand node in the relation and **RIGHT_ATTRS** are
    the token attributes to match. The pattern format is the same pattern used with
    **SpanRuler** . In our pattern, the anchor token will be the **dobj** token, so
    the first dictionary is defined like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**DependencyMatcher** 模式由一系列字典组成。第一个字典使用 **RIGHT_ID** 和 **RIGHT_ATTRS** 定义一个锚点标记。**RIGHT_ID**
    是关系右侧节点的唯一名称，**RIGHT_ATTRS** 是要匹配的标记属性。模式格式与 **SpanRuler** 中使用的相同模式。在我们的模式中，锚点标记将是
    **dobj** 标记，因此第一个字典定义如下：'
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As spaCy’s documentation says ( [https://spacy.io/usage/rule-based-matching/#dependencymatcher](https://spacy.io/usage/rule-based-matching/#dependencymatcher)
    ), after the first dictionary, the following dictionaries of the pattern should
    have the following keys:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如 spaCy 的文档所述 ([https://spacy.io/usage/rule-based-matching/#dependencymatcher](https://spacy.io/usage/rule-based-matching/#dependencymatcher))，在第一个字典之后，模式字典应包含以下键：
- en: '**LEFT_ID** : The name of the left-hand node in the relation, which has been
    defined in an earlier node'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LEFT_ID**：关系左侧节点的名称，该名称已在早期节点中定义'
- en: '**REL_OP** : An operator that describes how the two nodes are related'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**REL_OP**：描述两个节点之间关系的操作符'
- en: '**RIGHT_ID** : A unique name for the right-hand node in the relation'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RIGHT_ID**：关系右侧节点的唯一名称'
- en: '**RIGHT_ATTRS** : The token attributes to match for the right-hand node in
    the same format as patterns provided to the regular token-based as in **SpanRuler**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RIGHT_ATTRS**：与 **SpanRuler** 中提供的正则标记模式相同的格式，用于匹配关系右侧节点的标记属性'
- en: 'Given these keys, we construct the pattern by indicating the left-hand node
    of the relation, defining a name for the new right-hand node, and indicating the
    operator to describe the relation between the two nodes. Getting back to our example,
    after defining **direct_object_token** as the anchor we will set **RIGHT_ID**
    of the next dictionary to be the **VERB** token and define the operator as **direct_object_token
    < verb_token** because the direct object is *the immediate dependent* of the verb.
    Here are some other operators that are supported by the **DependencyMatcher**
    (you can check the full list of operators here [https://spacy.io/usage/rule-based-matching/#dependencymatcher-operators](https://spacy.io/usage/rule-based-matching/#dependencymatcher-operators)
    ):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些键，我们通过指示关系的左侧节点、为新右侧节点定义一个名称以及指示描述两个节点之间关系的运算符来构建模式。回到我们的例子，在将 **direct_object_token**
    定义为锚点后，我们将下一个字典的 **RIGHT_ID** 设置为 **VERB** 标记，并将运算符定义为 **direct_object_token <
    verb_token**，因为直接宾语是动词的 *直接依赖项*。以下是 **DependencyMatcher** 支持的一些其他运算符（您可以在[这里](https://spacy.io/usage/rule-based-matching/#dependencymatcher-operators)查看完整的运算符列表）：
- en: '**A < B** : A is the immediate dependent of B'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A < B** : A 是 B 的直接依赖项'
- en: '**A > B** : A is the immediate head of B'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A > B** : A 是 B 的直接头'
- en: '**A << B** : A is the dependent in a chain to B following dep → head paths'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A << B** : A 是在 dep → head 路径上跟随 B 的链中的依赖项'
- en: '**A >> B** : A is the head in a chain to B following head → dep paths'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A >> B** : A 是在 head → dep 路径上跟随 B 的链中的头'
- en: 'Don’t worry if these operations gave you a little headache; it also happened
    to me. Those are just a few of them, you can check the full list of operators
    at [https://spacy.io/usage/rule-based-matching#dependencymatcher-operators](https://spacy.io/usage/rule-based-matching#dependencymatcher-operators)
    . All right, let’s get back to our example and define the full pattern:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些操作让您有点头疼，那也发生在我身上。这只是其中的一小部分，您可以在[这里](https://spacy.io/usage/rule-based-matching#dependencymatcher-operators)查看完整的运算符列表。好吧，让我们回到我们的例子并定义完整的模式：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we are good to create **DependencyMatcher** :'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建 **DependencyMatcher**：
- en: 'First, we need to pass the **vocabulary** object (the vocabulary is shared
    with the documents the matcher operates on):'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要传递 **vocabulary** 对象（词汇与匹配器操作的文档共享）：
- en: '[PRE22]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to define a callback function that will take the following arguments:
    **matcher** , **doc** , **i** , and **matches** . The **matcher** argument refers
    to the matcher instance, **doc** is the document being analyzed, **i** is the
    index of the current match, and **matches** is a list detailing the matches found.
    We will create a **callback** function to show the intent in a single word, such
    as **bookFlight** , **cancelFlight** , **bookMeal** , and so on. The function
    will take the tokens of the match and print their lemma:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一个回调函数，该函数将接受以下参数：**matcher**、**doc**、**i** 和 **matches**。**matcher**
    参数指的是匹配器实例，**doc** 是正在分析的文档，**i** 是当前匹配的索引，**matches** 是一个详细说明找到的匹配项的列表。我们将创建一个
    **callback** 函数来显示一个单词的意图，例如 **bookFlight**、**cancelFlight**、**bookMeal** 等。该函数将接受匹配项的标记并打印它们的词元：
- en: '[PRE23]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To add a rule to the **matcher** , we specify an ID key, one or more patterns,
    and the optional callback function to act on the matches. Finally, we process
    the text again and call the **matcher** object, passing this **doc** as the parameter:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要向 **matcher** 添加规则，我们指定一个 ID 键、一个或多个模式以及可选的回调函数来处理匹配项。最后，我们再次处理文本并调用 **matcher**
    对象，将此 **doc** 作为参数传递：
- en: '[PRE24]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Great! The code prints **Intent: showFlightIntent** , so the recognition was
    successful. Here, we recognized a single intent, but some utterances may carry
    multiple intents. For example, consider the following utterance from the corpus:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '太棒了！代码打印出 **Intent: showFlightIntent** ，所以识别是成功的。在这里，我们识别了一个单一意图，但某些话语可能携带多个意图。例如，考虑以下来自语料库的话语：'
- en: '[PRE25]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, the user wants to list all the flights and, at the same time, see the
    fare info. One way of processing is considering these intents as a single and
    complex intent. A common way of processing this sort of utterance is to label
    the utterance with multiple intents.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，用户想要列出所有航班，同时查看票价信息。一种处理方式是将这些意图视为单一且复杂的意图。处理此类话语的常见方式是用多个意图标记话语。
- en: 'Let’s see the **DEP** dependencies extracted by **DependencyParser** :'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 **DEP** 依赖关系由 **DependencyParser** 提取的：
- en: '![Figure 5.7 – The dependency arcs of the new sentence (the rest of the sentence
    is omitted)](img/B22441_05_07.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 新句子的依存弧（句子其余部分省略）](img/B22441_05_07.jpg)'
- en: Figure 5.7 – The dependency arcs of the new sentence (the rest of the sentence
    is omitted)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 新句子的依存弧（句子其余部分省略）
- en: 'In *Figure 5* *.7* , we see that the **dobj** arc connects **show** and **flights**
    . The **conj** (conjunction) arc connects **flights** and **fares** to indicate
    the conjunction relation. This relation is built by a conjunction such as **and**
    or **or** and indicates that a noun is joined to another noun by this conjunction.
    Now let’s write the code to recognize these two intents:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.7* 中，我们看到 **dobj** 弧连接了 **show** 和 **flights**。**conj**（连词）弧将 **flights**
    和 **fares** 连接起来，表示连词关系。这种关系是通过连词如 **and** 或 **or** 构建的，表示一个名词通过这个连词与另一个名词相连。现在让我们编写代码来识别这两个意图：
- en: 'Converting the arc relationship to a **REL_OP** operator, **direct_object_token**
    will be the head of the relationship this time, so we will use the **>** operator
    since **direct_object_token** is *the immediate head* of the new **conjunction_token**
    . This is the new pattern to match two intents:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将弧关系转换为 **REL_OP** 操作符，**direct_object_token** 将成为这次关系的头，因此我们将使用 **>** 操作符，因为
    **direct_object_token** 是新的 **conjunction_token** 的 **直接头**。这是匹配两个意图的新模式：
- en: '[PRE26]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We also need to update the callback function so it can print the two intents:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要更新回调函数，使其能够打印出两个意图：
- en: '[PRE27]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we just need to add this new rule to the matcher. Since the pattern ID
    already exists, the patterns will be extended:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们只需要将这个新规则添加到匹配器中。由于模式 ID 已经存在，模式将被扩展：
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'With all of that set, we can now find the matches again:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置好所有这些之后，我们现在可以再次找到匹配项：
- en: '[PRE29]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now the matcher finds the tokens for the two patterns, the first one and this
    new one, which matches two intents. Until now, we have just been *printing the
    intent* , but in a real setting, it’s a good idea to store this information on
    the **Doc** object. To do that, we’ll create our own spaCy component. Let’s learn
    how to do it in the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在匹配器找到了两个模式的标记，第一个模式和这个新的模式，它匹配两个意图。到目前为止，我们只是 *打印意图*，但在实际设置中，将此信息存储在 **Doc**
    对象上是一个好主意。为此，我们将创建自己的 spaCy 组件。让我们在下一节学习如何做到这一点。
- en: Creating a pipeline component using extension attributes
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用扩展属性创建管道组件
- en: To create our component, we will use the **@Language.factory** decorator. A
    component factory is a callable that takes settings and returns a **pipeline component
    function** . The **@Language.factory** decorator also adds the name of the custom
    component to the registry, making it possible to use the **.add_pipe()** method
    to add the component to the pipeline.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的组件，我们将使用 **@Language.factory** 装饰器。组件工厂是一个可调用对象，它接受设置并返回一个 **pipeline
    component function**。**@Language.factory** 装饰器还将自定义组件的名称添加到注册表中，使得可以使用 **.add_pipe()**
    方法将组件添加到管道中。
- en: spaCy allows you to set any custom attributes and methods on the **Doc** , **Span**
    , and **Token** objects, which become available as **Doc._.** , **Span._.** ,
    and **Token._.** . In our case, we will add **Doc._.intent** to **Doc** , taking
    advantage of spaCy’s data structures to store our data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 允许你在 **Doc**、**Span** 和 **Token** 对象上设置任何自定义属性和方法，这些属性和方法将作为 **Doc._.**、**Span._.**
    和 **Token._.** 可用。在我们的案例中，我们将向 **Doc** 添加 **Doc._.intent** 属性，利用 spaCy 的数据结构来存储我们的数据。
- en: We will implement the component logic inside a Python class. spaCy expects the
    **__init__()** method to take the **nlp** and **name** arguments (spaCy fills
    then automatically), and the **__call__()** method should receive and return **Doc**
    .
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个 Python 类内部实现组件逻辑。spaCy 期望 **__init__()** 方法接受 **nlp** 和 **name** 参数（spaCy
    会自动填充它们），而 **__call__()** 方法应该接收并返回 **Doc**。
- en: 'Let’s create the **IntentComponent** class:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 **IntentComponent** 类：
- en: 'First, we create the class. Inside the **__init__()** method, we create the
    **DependencyMatcher** instance, add the patterns to the matcher, and set the **intent**
    extension attribute:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建类。在 **__init__()** 方法中，我们创建 **DependencyMatcher** 实例，将模式添加到匹配器中，并设置 **intent**
    扩展属性：
- en: '[PRE30]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, inside the **__call__()** method, we find the matches and check whether
    it’s a **"TWO_INTENTS"** match. If so, we extract the tokens for this pattern
    and set the **doc._.intent** attribute; if not, in the **else** block, we extract
    the tokens for the **"** **INTENT"** match:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，在 **__call__()** 方法内部，我们找到匹配项并检查是否是 **"TWO_INTENTS"** 匹配。如果是，我们提取该模式的标记并设置
    **doc._.intent** 属性；如果不是，则在 **else** 块中，我们提取 **"INTENT"** 匹配的标记：
- en: '[PRE31]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With this code, we register the custom extension in **Doc** by setting **doc._.intent
    = intent** on the **__call__()** method, where we find the matches and save the
    intent.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用这段代码，我们在**Doc**中注册自定义扩展，通过在**__call__()**方法上设置**doc._.intent = intent**来找到匹配项并保存意图。
- en: 'Now that we have the class for the custom component, the next step is to register
    it using the decorator:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了自定义组件的类，下一步是使用装饰器来注册它：
- en: '[PRE32]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you’re using a Jupyter Notebook and need to re-create the component, you’ll
    need to restart the kernel. If not, spaCy will give us an error since the component
    name was already registered.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Jupyter Notebook并且需要重新创建组件，你需要重新启动内核。如果不这样做，spaCy会给我们一个错误，因为组件名称已经被注册。
- en: 'That’s it, that’s your first custom component! Congratulations! Now, to extract
    the intent, we just need to add the component to the pipeline. If we want to see
    the intent, we can access it with **doc._.intent** . Here’s how you can do it:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，这就是你的第一个自定义组件！恭喜！现在，为了提取意图，我们只需要将组件添加到管道中。如果我们想查看意图，我们可以通过**doc._.intent**来访问它。这是你可以这样做的方式：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Cool, right? If you don’t remember, the dataset has 4,978 utterances. That’s
    not a very large number, but what if it was bigger? Could spaCy help us make it
    faster? Yes! In the next section, we will learn how to run our pipelines using
    the **Language.pipe()** method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 太酷了，对吧？如果你不记得，数据集有4,978个语音。这不是一个非常大的数字，但如果它更大呢？spaCy能帮助我们让它更快吗？是的！在下一节中，我们将学习如何使用**Language.pipe()**方法运行我们的管道。
- en: Running the pipeline with large datasets
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大型数据集运行管道
- en: 'The **Language.pipe()** method processes texts as a stream and yields **Doc**
    objects in order. It buffers the texts in batches instead of one-by-one, since
    this is usually more efficient. If we want to get a specific doc, we need to call
    **list()** first because the method returns a Python generator that yields **Doc**
    objects. This is how you can do it:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**Language.pipe()**方法将文本作为流处理，并按顺序产生**Doc**对象。它以批量而不是逐个缓冲文本，因为这通常更有效率。如果我们想获取特定的文档，我们需要先调用**list()**，因为该方法返回一个Python生成器，它产生**Doc**对象。这是你可以这样做的方式：'
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the preceding code, we are getting a list of text utterances from the DataFrame
    we loaded at the beginning of the chapter and processing it in batches using **.pipe()**
    . Let’s compare the time difference by using and not using the **.** **pipe()**
    method:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在从本章开头加载的DataFrame中获取文本语音列表，并使用**.pipe()**进行批量处理。让我们通过使用和不使用**.pipe()**方法来比较时间差异：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This gives us a period of **27.12** seconds. Now, let’s use the following method:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们27.12秒的时间。现在，让我们使用以下方法：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using **nlp.pipe()** , we get the same results in 5.90 seconds. That’s a huge
    difference. We can also specify **batch_size** and the **n_process** to set the
    number of processors to use. There is also an option to disable components if
    you need to run **.pipe()** just to get the result of the text processed by specific
    components.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**nlp.pipe()**，我们在5.90秒内得到了相同的结果。这是一个巨大的差异。我们还可以指定**batch_size**和**n_process**来设置要使用的处理器数量。还有一个选项可以禁用组件，如果你只需要运行**.pipe()**来获取特定组件处理过的文本结果。
- en: 'Awesome, we’ve finished our first pipeline with our own custom component! Congratulations!
    Here is the full code of the pipeline:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，我们使用自己的自定义组件完成了我们的第一个管道！恭喜！以下是管道的完整代码：
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: spaCy makes the pipeline code tidy and organized, two qualities that are essential
    if we want to maintain our codebase.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy使管道代码整洁有序，这是我们想要维护代码库时两个至关重要的品质。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to generate a complete semantic parse of utterances.
    First, you added a **SpanRuler** component to extract an NER entity that is significant
    to the use case context. Then, you learned how to use **DependencyMatcher** to
    perform intent recognition by analyzing sentence structure. Next, you also learned
    how to create your own custom spaCy component to extract the intent of the utterances.
    Finally, you saw how to process large datasets faster with the **Language.pipe()**
    method.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何生成语音的完整语义解析。首先，你添加了一个**SpanRuler**组件来提取与用例上下文相关的NER实体。然后，你学习了如何使用**DependencyMatcher**通过分析句子结构来进行意图识别。接下来，你还学习了如何创建自己的自定义spaCy组件来提取语音的意图。最后，你看到了如何使用**Language.pipe()**方法更快地处理大型数据集。
- en: Both **SpanRuler** and **DependencyMatcher** rely on the patterns we create.
    The process of creating these patterns is a back-and-forth process. We analyze
    the results, then test out new patterns, then analyze the results again, and so
    on. The goal of this chapter was to teach you how to use these tools so you can
    perform this process in your own projects.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**SpanRuler** 和 **DependencyMatcher** 都依赖于我们创建的模式。创建这些模式的过程是一个反复迭代的过程。我们分析结果，然后测试新的模式，然后再分析结果，如此循环。本章的目标是教会你如何使用这些工具，以便你可以在自己的项目中执行这个过程。'
- en: In the next chapters, we will shift more toward machine learning methods. [*Chapter
    6*](B22441_06.xhtml#_idTextAnchor087) will cover how to use spaCy with Transformers.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更多地转向机器学习方法。[*第6章*](B22441_06.xhtml#_idTextAnchor087) 将介绍如何使用 spaCy
    与 Transformers 结合使用。
