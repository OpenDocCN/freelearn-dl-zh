- en: Generating Uncertainty in Traffic Signs Classifier Using Bayesian Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用贝叶斯神经网络生成交通标志分类器的不确定性
- en: As humans, we love the uncertainty that comes with predictions. For example,
    we always want to know what the chances are of it raining before we leave the
    house. However, with traditional deep learning, we only have a point prediction
    and no notion of uncertainty. Predictions from these networks are assumed to be
    accurate, which is not always the case. Ideally, we would like to know the level
    of confidence of predictions from neural networks before making a decision.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们喜欢预测中的不确定性。例如，在我们离开家之前，我们总是想知道下雨的几率。然而，传统的深度学习只给出一个点预测，而没有不确定性的概念。这些网络的预测被假设为准确的，但事实并非总是如此。理想情况下，我们希望在做出决策之前，能够了解神经网络预测的置信度。
- en: 'For example, having uncertainty in the model could have potentially avoided
    the following disastrous consequences:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在模型中引入不确定性本可能避免以下灾难性后果：
- en: In May 2016, the Tesla Model S crashed in northern Florida into a truck that
    was turning left in front of it. According to the official Tesla blog ([https://www.tesla.com/en_GB/blog/tragic-loss](https://www.tesla.com/en_GB/blog/tragic-loss)),
    *Neither Autopilot nor the driver noticed the white side of the tractor trailer
    against a brightly lit sky, so the brake was not applied*.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2016年5月，特斯拉Model S在佛罗里达州北部发生了撞车事故，撞上了一辆正在左转的卡车。根据特斯拉官方博客的描述（[https://www.tesla.com/en_GB/blog/tragic-loss](https://www.tesla.com/en_GB/blog/tragic-loss)），*自动驾驶系统和驾驶员都没有注意到卡车拖车在明亮的天空下呈现的白色侧面，因此没有踩刹车*。
- en: In July 2015, two African Americans were classified as gorillas by Google's
    image classifier, raising concerns of racial discrimination. Here ([https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/](https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/))
    is the press release.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2015年7月，谷歌的图像分类器将两位非裔美国人错误地分类为大猩猩，引发了种族歧视的担忧。这里是新闻发布稿（[https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/](https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/)）。
- en: 'The thing that is pretty clear from these examples is that quantifying uncertainty
    in predictions could have avoided these disasters. The question now is: If it''s
    so obvious, why didn''t Tesla or Google implement it in the first place?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些例子中可以看出，量化预测中的不确定性本可以避免这些灾难。现在的问题是：如果这这么显而易见，为什么特斯拉或谷歌一开始没有实施呢？
- en: Bayesian algorithms (like Gaussian processes) can quantify uncertainty, but
    cannot scale to large datasets such as images and videos, whereas deep learning
    is able to produce much better accuracy—except that it lacks any notion of uncertainty.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯算法（如高斯过程）能够量化不确定性，但无法扩展到像图像和视频这样的庞大数据集，而深度学习能够提供更好的准确性——只是它缺乏不确定性的概念。
- en: 'In this chapter, we will explore the concept of Bayesian neural networks, which
    combines the concepts of deep learning and Bayesian learning with model uncertainty
    in the predictions of deep neural networks. We will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探索贝叶斯神经网络的概念，结合深度学习和贝叶斯学习的概念，以及深度神经网络预测中的模型不确定性。我们将涵盖以下主题：
- en: Introduction to Bayesian deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习简介
- en: What are Bayesian neural networks?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络是什么？
- en: Building a Bayesian neural network with the German Traffic Sign Image dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用德国交通标志图像数据集构建贝叶斯神经网络
- en: Understanding Bayesian deep learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解贝叶斯深度学习
- en: We've all understood the basics of Bayes' rule, as explained in Chapter 6, *Predicting
    Stock Prices using Gaussian Process Regression*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都已经理解了贝叶斯规则的基础知识，如第六章所解释的，*使用高斯过程回归预测股价*。
- en: 'For Bayesian machine learning, we use the same formula as Bayes'' rule to learn
    model parameters (![](img/f5d0830f-24cd-4bde-a545-131fa87abb6c.png)) from the
    given data, ![](img/839c249a-4c94-4cc9-9f42-a5eefc53e68f.png). The formula, then,
    looks like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于贝叶斯机器学习，我们使用与贝叶斯规则相同的公式来从给定数据中学习模型参数（![](img/f5d0830f-24cd-4bde-a545-131fa87abb6c.png)），![](img/839c249a-4c94-4cc9-9f42-a5eefc53e68f.png)。因此，公式如下所示：
- en: '![](img/1ca7cadc-8435-47da-9f98-f28d27c21001.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ca7cadc-8435-47da-9f98-f28d27c21001.png)'
- en: Here, ![](img/b4360234-8459-431a-8283-360b9fd20244.png) or the probability of
    observed data is also called evidence. This is always difficult to compute. One
    brute-force way is to integrate out ![](img/a534dbd7-1960-4d32-9cb1-d54dabe39401.png) for
    all the values of model parameters, but this is obviously too expensive to evaluate.![](img/610582f1-80be-46ca-b3d8-625b3794d4a0.png) is
    the prior on parameters, which is nothing but some randomly initialized value
    of parameters in most cases. Generally, we don't care about setting the priors
    perfectly as we expect the inference procedure to converge to the right value
    of parameters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/b4360234-8459-431a-8283-360b9fd20244.png) 或者观测数据的概率也叫做证据。这通常很难计算。一种暴力方法是将![](img/a534dbd7-1960-4d32-9cb1-d54dabe39401.png)
    对所有模型参数值进行积分，但显然这在计算上代价太高。![](img/610582f1-80be-46ca-b3d8-625b3794d4a0.png) 是参数的先验，在大多数情况下它只是参数的随机初始化值。通常，我们不在乎先验设置得是否完美，因为我们期望推断过程能够收敛到正确的参数值。
- en: '![](img/614510dd-708d-4882-a251-6cb8cfec80bf.png) is known as the likelihood
    of data, given the modeling parameters. Effectively, it shows how likely it is
    to obtain the given observations in the data when given the model parameters.
    We use likelihood as a measure to evaluate different models. The higher the likelihood,
    the better the model.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/614510dd-708d-4882-a251-6cb8cfec80bf.png) 被称为给定建模参数下的数据似然性。实际上，它显示了在给定模型参数时，获得数据中给定观察值的可能性。我们使用似然性作为评估不同模型的标准。似然性越高，模型越好。'
- en: Finally, ![](img/18494e17-11f3-433a-ab12-e82727d65da9.png), a posterior, is
    what we want to calculate. It's a probability distribution over model parameters
    that's obtained from the given data. Once we obtain the uncertainty in model parameters,
    we can use them to quantify the uncertainty in model predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，![](img/18494e17-11f3-433a-ab12-e82727d65da9.png)，后验，是我们想要计算的内容。它是基于给定数据得到的模型参数的概率分布。一旦我们获得了模型参数的不确定性，我们就可以用它们来量化模型预测的不确定性。
- en: Generally, in machine learning, we use **Maximum Likelihood estimation** (**MLE**)
    ([https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf))
    to get the estimates of model parameters. However, in the case of Bayesian deep
    learning, we estimate a posterior from the prior and the procedure is known as
    **Maximum a posteriori** (**MAP**) estimation ([https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在机器学习中，我们使用**最大似然估计**（**MLE**）([https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf))
    来获得模型参数的估计。然而，在贝叶斯深度学习的情况下，我们从先验和该过程估计后验，这一过程被称为**最大后验估计**（**MAP**）([https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf))。
- en: Bayes' rule in neural networks
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的贝叶斯法则
- en: 'Traditionally, neural networks produce a point estimate by optimizing weights
    and biases to minimize a loss function, such as the mean squared error in regression
    problems. As mentioned earlier, this is similar to finding parameters using the
    Maximum likelihood estimation criteria:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，神经网络通过优化权重和偏差来产生一个点估计，从而最小化损失函数，例如回归问题中的均方误差。正如前面所提到的，这类似于使用最大似然估计准则来寻找参数：
- en: '![](img/df3f26a9-ede4-43c3-886b-97655eb0ffda.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df3f26a9-ede4-43c3-886b-97655eb0ffda.png)'
- en: 'Typically, we obtain the best parameters through backpropagation in neural
    networks. To avoid overfitting, we introduce a regularizer of ![](img/9346cd9b-1b59-4039-aa0f-1d75ac2bada1.png) norm
    over weights. If you are not aware of regularization, please refer to the following
    Andrew Ng video: [http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning).
    It has been shown that ![](img/16aa6075-3e33-4e93-a7f5-6c6beeb2117c.png) normalization
    is equivalent to placing a normal prior on weights ![](img/b2ee0746-0cee-44a6-8dbc-fc6269a8f8f7.png).
    With a prior on weights, the MLE estimation problem can be framed as a MAP estimation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们通过神经网络中的反向传播获得最佳参数。为了避免过拟合，我们引入了对权重的正则化，即！[](img/9346cd9b-1b59-4039-aa0f-1d75ac2bada1.png)范数。如果你不熟悉正则化，请参考以下Andrew
    Ng的视频：[http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning)。研究表明，！[](img/16aa6075-3e33-4e93-a7f5-6c6beeb2117c.png)归一化相当于对权重！[](img/b2ee0746-0cee-44a6-8dbc-fc6269a8f8f7.png)施加了一个正态先验。在权重上有先验的情况下，MLE估计问题可以被框架化为MAP估计：
- en: '![](img/88c60ffd-1803-4bce-b8de-88ee81a6a1b3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88c60ffd-1803-4bce-b8de-88ee81a6a1b3.png)'
- en: 'Using Bayes'' rule, the preceding equation can be written as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，前面的方程可以写成如下形式：
- en: '![](img/989188bb-023e-4889-a734-f91ebb74ebe7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/989188bb-023e-4889-a734-f91ebb74ebe7.png)'
- en: The exact proof of equivalence of regularization to the Bayesian framework is
    outside the scope of this chapter. If you are interested, you can read more about
    it at the following MIT lecture: [http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf](http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化与贝叶斯框架等价的精确证明超出了本章的范围。如果你感兴趣，可以阅读以下MIT讲座，了解更多信息：[http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf](http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf)。
- en: From this, we can observe that traditional neural networks with regularization
    can be framed as a problem of inference using Bayes' rule. Bayesian neural networks
    aim to determine the posterior distribution using Monte Carlo or Variational inference
    techniques. In the rest of this chapter, we will look at how to build a Bayesian
    neural network using TensorFlow Probability.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们可以观察到，带正则化的传统神经网络可以被框架化为使用贝叶斯定理的推理问题。贝叶斯神经网络的目标是使用蒙特卡洛方法或变分推理技术来确定后验分布。在本章的其余部分，我们将看看如何使用TensorFlow
    Probability构建贝叶斯神经网络。
- en: Understanding TensorFlow probability, variational inference, and Monte Carlo
    methods
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TensorFlow概率、变分推理和蒙特卡洛方法
- en: '**TensorFlow Probability** (**tfp** in code – [https://www.tensorflow.org/probability/overview#layer_2_model_building](https://www.tensorflow.org/probability/overview#layer_2_model_building))
    was recently released by Google to perform probabilistic reasoning in a scalable
    manner. It provides tools and functionalities to define distributions, build neural
    networks with prior on weights, and perform probabilistic inference tasks such
    as Monte Carlo or Variational Inference.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Probability**（在代码中为**tfp** – [https://www.tensorflow.org/probability/overview#layer_2_model_building](https://www.tensorflow.org/probability/overview#layer_2_model_building)）是谷歌最近发布的一个工具，用于以可扩展的方式执行概率推理。它提供了定义分布、构建带权重先验的神经网络以及执行概率推理任务（如蒙特卡洛方法或变分推理）所需的工具和功能。'
- en: 'Let''s take a look at some of the functions/utilities we will be using for
    building our model:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下在构建模型时将会使用的一些函数/工具：
- en: '**Tfp.distributions.categorical**: This is a standard categorical distribution
    that''s characterized by probabilities or log-probabilities over K classes. In
    this project, we have Traffic Sign images from 43 different traffic signs. We
    will define a categorical distribution over 43 classes in this project.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tfp.distributions.categorical**：这是一个标准的类别分布，其特点是对K个类别的概率或对数概率。在这个项目中，我们有来自43种不同交通标志的交通标志图像。我们将在本项目中定义一个覆盖43个类别的类别分布。'
- en: '**Probabilistic layers**: Built on top of the TensorFlow layers implementation,
    probabilistic layers incorporate uncertainty over the functions they represent.
    Effectively, they incorporate uncertainty in the weights of the neural networks.
    They have the functionality to forward pass through the inputs by sampling from
    the posterior of weight distributions (![](img/3dc23fbc-1c62-49e1-afff-1457bbdbb861.png)).
    Specifically, we will use the Convolutional2DFlipout ([https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout))
    layer, which can compute the forward pass by sampling from the posterior of the
    weight parameters of the model.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率层**：构建在 TensorFlow 层实现之上，概率层将它们所表示的函数中的不确定性融入其中。实际上，它们将不确定性融入神经网络的权重中。它们具有通过从权重分布的后验进行采样，前向传播输入的功能（![](img/3dc23fbc-1c62-49e1-afff-1457bbdbb861.png)）。具体来说，我们将使用
    Convolutional2DFlipout 层（[https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout)），它能够通过从模型权重参数的后验进行采样，计算前向传播。'
- en: '**Kullback-leibler (KL) divergence**: If we want to measure the difference
    between two numbers, we just subtract them. What if we want to obtain a difference
    between two probability distributions? What is the equivalent of subtraction in
    this case? Often in the case in probability and statistics, we will replace the
    observed data or complex distributions with a simpler, approximating distribution.
    KL Divergence helps us measure just how much information we lose when we choose
    an approximation. Essentially, it is a measure of one probability distribution
    from others. A KL divergence of 0 indicates that two distributions are identical.
    If you want to know more about the mathematics of KL divergence, please refer
    to a great explanation from MIT open courseware, which can be found at [https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler (KL) 散度**：如果我们想要衡量两个数字之间的差异，只需将它们相减。那么，如果我们想要衡量两个概率分布之间的差异呢？在这种情况下，相减的等价物是什么？在概率和统计的情况下，我们通常会用一个更简单的近似分布来替代观察数据或复杂的分布。KL
    散度帮助我们衡量在选择近似时丢失了多少信息。从本质上讲，它是衡量一个概率分布与其他分布之间差异的度量。KL 散度为 0 表示两个分布是相同的。如果你想了解更多关于
    KL 散度的数学原理，请参考 MIT 开放课程中的精彩解释，链接为 [https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf)。'
- en: '**Variational inference**: Variational inference is a machine learning method
    that''s used to approximate complex, intractable integrals in Bayesian learning
    through optimization.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分推断**：变分推断是一种机器学习方法，用于通过优化在贝叶斯学习中近似复杂、难以求解的积分。'
- en: 'As we know, our aim in Bayesian learning is to calculate the posterior ![](img/13cfd9ef-c6bd-486c-b10f-32f04f36f217.png), given
    prior ![](img/580456b9-ca75-4a74-8ff2-2d7062f886e1.png) and data ![](img/f1181935-aeb9-47fc-95b9-606593c46334.png).
    A prerequisite for computing the posterior is the computation of distribution
    of ![](img/eafed642-dc26-40cf-b555-99b5c617f851.png) (data) in order to obtain ![](img/f0ce5197-f81a-4652-95e4-f2a4f361b607.png), or
    *evidence*. As mentioned earlier, the distribution of X is intractable as it is
    too expensive to compute using a brute-force approach. To address this problem,
    we will use something called Variational Inference (VI). In VI, we define a family
    of distributions ![](img/787fd8b6-1a02-4263-a9b9-4cb5b05afa8f.png), parameterized
    by ![](img/8f23cfcf-a85c-4cc1-b76c-7261bd3f8d4f.png). The core idea is to optimize
    ![](img/740b2ea1-bf88-4e90-a83a-406a68d690f4.png) so that the approximate distribution
    is as close to the true posterior as possible. We measure the difference between
    two distributions using KL divergence. As it turns out, it is not easy to minimize
    the KL divergence. We can show you that this KL divergence is always positive
    and that it comprises two parts using the following mathematical formula:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，我们在贝叶斯学习中的目标是计算后验概率！[](img/13cfd9ef-c6bd-486c-b10f-32f04f36f217.png)，给定先验！[](img/580456b9-ca75-4a74-8ff2-2d7062f886e1.png)
    和数据！[](img/f1181935-aeb9-47fc-95b9-606593c46334.png)。计算后验的前提是计算！[](img/eafed642-dc26-40cf-b555-99b5c617f851.png)（数据）的分布，以便获得！[](img/f0ce5197-f81a-4652-95e4-f2a4f361b607.png)，或称为
    *证据*。如前所述，X 的分布是难以处理的，因为使用暴力计算方法计算其分布代价过高。为了解决这个问题，我们将使用一种叫做变分推断（VI）的方法。在 VI 中，我们定义一个由！[](img/787fd8b6-1a02-4263-a9b9-4cb5b05afa8f.png)
    参数化的分布族！[](img/8f23cfcf-a85c-4cc1-b76c-7261bd3f8d4f.png)。核心思想是优化！[](img/740b2ea1-bf88-4e90-a83a-406a68d690f4.png)，使得近似分布尽可能接近真实的后验分布。我们通过
    KL 散度来衡量两个分布之间的差异。事实证明，最小化 KL 散度并不容易。我们可以通过以下数学公式向你展示，这个 KL 散度总是为正，并且它包含两个部分：
- en: '![](img/7644d761-4949-4895-b510-906255b0c046.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7644d761-4949-4895-b510-906255b0c046.png)'
- en: Here, **ELBO** is **Evidence Lower Bound** ([https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**ELBO** 是 **证据下界**（[https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)）。
- en: '**Monte Carlo method: **Monte Carlo methods are computational methods which
    rely on repeated random sampling to obtain the statistical behavior of some phenomenon
    (behavior). They are typically used to model uncertainties or generate what-if
    scenarios for business.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒙特卡洛方法：**蒙特卡洛方法是一种计算方法，通过反复随机抽样来获取某些现象（行为）的统计特性。它们通常用于模拟不确定性或生成商业场景的假设情况。'
- en: Let's say you commute by train every day to work. You are thinking about whether
    to take the company shuttle to the office instead. Now, there are many random
    variables associated with a bus ride, such as time of arrival, traffic, number
    of passengers boarding the bus, and so on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你每天都乘火车上下班。你在考虑是否改乘公司的班车去办公室。现在，公交车的乘坐过程涉及许多随机变量，如到达时间、交通、上车的乘客数量等等。
- en: One way that we can look at this what-if scenario is if we take the mean of
    these random variables and calculate the arrival time. However, that will be too
    naive as it doesn't take into account variance in these variables. Another way
    is to sample from these random variables (somehow, if you are able to do that!)
    and generate what-if scenarios of reaching the office.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算这些随机变量的平均值并计算到达时间来分析这种假设场景。然而，这种方法太过简单，因为它没有考虑到这些变量的方差。另一种方法是从这些随机变量中进行采样（如果你能够做到的话！），并生成假设场景以预测到达办公室的情况。
- en: To make a decision, you will need an acceptable criteron. For instance, if you
    observe that in 80% of these what-if scenarios you reach office on or before time,
    you can continue forward. This approach is also known as the Monte Carlo simulation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出决策，你需要一个可接受的标准。例如，如果你观察到在 80% 的假设场景中你能准时到达办公室，那么你可以继续前进。这种方法也称为蒙特卡洛模拟。
- en: In this project, we will model the weights of neural networks as random variables.
    To determine the final prediction, we will sample from the distributions of these
    weights repeatedly to obtain the distribution of predictions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将把神经网络的权重建模为随机变量。为了确定最终的预测，我们将反复从这些权重的分布中进行采样，以获得预测的分布。
- en: Note that we have skipped some mathematical details. Feel free to read more
    about Variational Inference ([https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)[)](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们省略了一些数学细节。如果有兴趣，可以阅读更多关于变分推理的内容（[https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)）。
- en: Building a Bayesian neural network
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建贝叶斯神经网络
- en: For this project, we will use the German Traffic Sign Dataset ([http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset))
    to build a Bayesian neural network. The training dataset contains 26,640 images
    in 43 classes. Similarly, the testing dataset contains 12,630 images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将使用德国交通标志数据集（[http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)）来构建贝叶斯神经网络。训练数据集包含43个类别的26,640张图像。类似地，测试数据集包含12,630张图像。
- en: Please read the `README.md` file in this book's repository before executing
    the code to install the appropriate dependencies and for instructions on how to
    run the code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行代码之前，请阅读本书仓库中的`README.md`文件，安装相应的依赖项并了解如何运行代码。
- en: 'The following is an image that''s present in this dataset:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该数据集中存在的一张图像：
- en: '![](img/bca6d66c-486e-4b01-8b71-8501f9a16c56.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bca6d66c-486e-4b01-8b71-8501f9a16c56.png)'
- en: You can see that there are different kinds of traffic sign depicted by different
    classes in the dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到数据集中有不同类别的交通标志。
- en: We begin by pre-processing our dataset and making it conform to the requirements
    of the learning algorithm. This is done by reshaping the images to a uniform size
    via histogram equalization, which is used to enhance contrast, and cropping them
    to only focus on the traffic signs in the image. Also, we convert the images to
    grayscale as traffic signs are identified by shape and not color in the image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对数据集进行预处理，使其符合学习算法的要求。通过直方图均衡化来调整图像的统一大小，这种方法用于增强对比度，然后将图像裁剪，专注于图像中的交通标志。此外，我们将图像转换为灰度图像，因为交通标志是通过形状而非颜色来识别的。
- en: For modeling, we define a standard Lenet model ([http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)),
    which was developed by Yann Lecun. Lenet is one of the first convolutional neural
    networks that was designed. It is small and easy to understand, yet large enough
    to provide interesting results.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于建模，我们定义了一个标准的Lenet模型（[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)），它是由Yann
    Lecun开发的。Lenet是最早设计的卷积神经网络之一。它既小巧易懂，又足够大，能提供有趣的结果。
- en: 'A standard Lenet model has the following properties:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 标准Lenet模型具有以下特点：
- en: Three convolutional layers with increasing filter sizes
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个卷积层，滤波器大小逐渐增加
- en: Four fully connected layers
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个全连接层
- en: No dropout layers
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有丢弃层
- en: '**Rectified linear** (**ReLU**) after every fully connected or convolutional
    layer'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个全连接层或卷积层后使用**线性整流函数**（**ReLU**）
- en: Max pooling after every convolutional layer
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层后进行最大池化
- en: 'We train this model to minimize the negative of ELBO loss that is defined in
    the *Understanding TensorFlow Probability, Variational Inference, and Monte Carlo
    methods* section of this chapter. Specifically, we define ELBO loss as a combination
    of two terms:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练该模型以最小化ELBO损失的负值，ELBO损失在本章的*理解TensorFlow概率、变分推理和蒙特卡洛方法*一节中进行了定义。具体来说，我们将ELBO损失定义为两个项的组合：
- en: Expected log likelihood or cross entropy that can be estimated through the Monte
    Carlo method
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望的对数似然或交叉熵，可以通过蒙特卡洛方法估计
- en: KL divergence
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KL散度
- en: Once the model is trained, we evaluate the predictions on the hold-out dataset.
    One of the major differences in Bayesian neural network evaluation is that there
    is no single set of parameters (weights of the model) that we can obtain from
    training. Instead, we obtain a distribution of all the parameters. For evaluation,
    we will have to sample values from the distribution of each parameter to obtain
    the accuracy on the testing set. We will sample the parameters of the model multiple
    times to obtain a confidence interval on our predictions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们将在保留数据集上评估预测。贝叶斯神经网络评估的一个主要区别是，我们无法从训练中获得一组固定的参数（模型的权重）。相反，我们获得所有参数的分布。在评估时，我们需要从每个参数的分布中采样值，以获得测试集的准确性。我们将多次采样模型的参数，以获得预测的置信区间。
- en: Lastly, we will show some uncertainty in our predictions in sample images from
    the testing dataset and also plot the distribution of the weight parameters we
    obtain.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在测试数据集中的一些样本图像中展示我们预测的不确定性，并绘制我们获得的权重参数的分布。
- en: Defining, training, and testing the model
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义、训练和测试模型
- en: 'Download both the training and testing datasets from [http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).
    Let''s look at the steps to build the project after you have downloaded the dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)下载训练和测试数据集。下载数据集后，接下来我们来看一下构建项目的步骤：
- en: 'Begin by transforming the images present in the dataset using histogram equalization.
    This is essential as each image in the dataset may have a different scale of illumination.
    You can see from the following two images how images of the same traffic sign
    have very different illumination. Histogram equalization helps to normalize these
    differences and makes the training data more consistent:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过直方图均衡化对数据集中存在的图像进行转换。这一步很重要，因为数据集中的每张图像可能具有不同的光照强度。从以下两张图片中可以看到，同样的交通标志在不同的光照下有很大的差异。直方图均衡化有助于标准化这些差异，使训练数据更加一致：
- en: '![](img/564c6534-4274-47d0-844e-7337c1db8f30.png)![](img/fabef10e-3b86-4de1-a49d-3f0d9f745e8a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/564c6534-4274-47d0-844e-7337c1db8f30.png)![](img/fabef10e-3b86-4de1-a49d-3f0d9f745e8a.png)'
- en: 'Once we have performed the equalization, crop the image to focus on just the
    sign, and resize the image to 32 x 32 as desired by our learning algorithm:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成均衡化操作，裁剪图像以仅聚焦于标志，并将图像大小调整为32 x 32，以符合我们的学习算法要求：
- en: Note that we use 32 x 32 as the shape of images for training as it is big enough
    to preserve the nuances of the image for detection and small enough to train the
    model faster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用32 x 32作为训练图像的形状，因为它足够大，可以保留图像的细微差别用于检测，同时又足够小，可以更快地训练模型。
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create dictionaries with label and image information for the train/test dataset
    and store them as pickle files so that we don''t have to run the pre-processing
    code every time we run the model. This means that we essentially pre-process the
    transformed data to create our train and test datasets:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建包含标签和图像信息的字典，并将其存储为pickle文件，这样我们就不必每次运行模型时都重新执行预处理代码。这意味着我们实际上是在预处理已转换的数据，以创建训练和测试数据集：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will use grayscale images in our project as our task throughout this project
    is to classify traffic sign images into one of the 43 classes and provide a measure
    of uncertainty in our classification. We do not care about the color of the image.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们将使用灰度图像，因为我们整个项目的任务是将交通标志图像分类为43个类别之一，并提供分类的不确定性度量。我们不关心图像的颜色。
- en: 'Define the model using the LeNet architecture in Keras. Finally, we will assign
    the 43 sized vector of outputs from the LeNet model into a categorical distribution
    function (`tfd.categorical`) from TensorFlow probability. This will help us generating
    the uncertainty in predictions afterwards:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras中的LeNet架构定义模型。最后，我们将LeNet模型输出的43维向量分配给TensorFlow概率中的分类分布函数（`tfd.categorical`）。这将帮助我们在后续生成预测的不确定性：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We define the loss to minimize the KL divergence up to ELBO. Compute the ELBO
    loss that is defined in the *Understanding TensorFlow Probability, Variational
    Inference, and Monte Carlo methods* section of this chapter. As you can see, we
    use the `model.losses` attribute to compute the KL divergence. This is because
    the `losses` attribute of a TensorFlow Keras Layer represents a side-effect computation
    such as regularizer penalties. Unlike regularizer penalties on specific TensorFlow
    variables, here the `losses` represent the KL divergence computation:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义损失以最小化KL散度直至ELBO。计算本章《理解TensorFlow概率、变分推理和蒙特卡洛方法》部分中定义的ELBO损失。如您所见，我们使用`model.losses`属性来计算KL散度。这是因为TensorFlow
    Keras Layer的`losses`属性表示诸如正则化惩罚之类的副作用计算。与特定TensorFlow变量上的正则化惩罚不同，`losses`在这里表示KL散度的计算：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Use the Adam optimizer, as defined in [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml),
    *Sentiment Analysis in your browser using TensorFlow.js*, to optimize the ELBO
    loss:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Adam优化器，如[第3章](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml)《使用TensorFlow.js在浏览器中进行情感分析》中所定义的，来优化ELBO损失：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that we are using the Adam optimizer because it generally performs better
    than other optimizers with default parameters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用Adam优化器，因为它通常在默认参数下比其他优化器表现更好。
- en: 'Train the model with the following parameters:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下参数训练模型：
- en: Epochs = 1,000
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮次 = 1,000
- en: Batch size = 128
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次大小 = 128
- en: 'Learning rate = 0.001:'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 = 0.001：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the model is trained, each weight in the Bayesian neural network will
    have a distribution and not a fixed value. Sample each weight multiple times (50,
    in the code) and obtain different predictions for each sample. Sampling, although
    useful, is expensive. Therefore, we should only use Bayesian neural networks where
    we require some measure of uncertainty in our predictions. Here is the code for
    Monte Carlo sampling:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，贝叶斯神经网络中的每个权重将具有分布而不是固定值。对每个权重进行多次采样（代码中为50次），并为每个样本获得不同的预测。尽管采样有用，但代价高昂。因此，我们应该仅在需要对预测结果的不确定性进行某种衡量时，才使用贝叶斯神经网络。以下是蒙特卡洛采样的代码：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once you have the samples, obtain the mean probability for each image in the
    test dataset and compute the mean accuracy like in usual machine learning classifiers.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦获得样本，计算每个测试数据集图像的平均概率，并像通常的机器学习分类器一样计算平均准确度。
- en: The mean accuracy we obtain for this dataset is ~ 89% for 1,000 Epochs. You
    can tune the parameters further or create a deeper model to obtain better accuracy.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们为这个数据集获得的平均准确度约为 ~ 89%（在1,000个训练轮次后）。您可以进一步调整参数或创建更深的模型来获得更好的准确度。
- en: 'Here is the code for getting the mean accuracy:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是获取平均准确度的代码：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next step is to calculate the distribution of accuracy for each Monte Carlo
    sample of each test image. For that, compute the predicted class and compare it
    with the test label. The predicted class can be obtained by assigning the label
    to the class with the maximum probability for a given network parameter sample.
    This way, you can get the range of accuracies and can also plot those accuracies
    on a histogram. Here is the code for obtaining accuracy and generating a histogram:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是计算每个测试图像的每个蒙特卡洛样本的准确度分布。为此，计算预测类别并与测试标签进行比较。预测类别可以通过将标签分配给具有最大概率的类别来获得，这基于给定网络参数的样本。这样，您可以获得准确度范围，并且还可以将这些准确度绘制在直方图上。以下是获取准确度并生成直方图的代码：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The histogram that''s generated will look something like the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的直方图将类似于以下内容：
- en: '![](img/467b468e-0ee2-4b1f-a89b-c39554f745f6.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/467b468e-0ee2-4b1f-a89b-c39554f745f6.png)'
- en: As you can see, we have a distribution of accuracies. This distribution can
    help us obtain the confidence interval over the accuracy of our model on the test
    dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有一个准确度的分布。这个分布可以帮助我们获得模型在测试数据集上的置信区间。
- en: Note that the plot might look differently when you run the code, since it is
    obtained through random sampling.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您运行代码时，图形可能会有所不同，因为它是通过随机采样获得的。
- en: 'Take a few images from the test dataset and see their predictions for different
    samples in Monte Carlo. Use the following function `plot_heldout_prediction` to
    generate the histogram of predictions from different samples in Monte Carlo:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从测试数据集中选取一些图像，并查看它们在蒙特卡洛方法中的不同样本预测。使用以下函数`plot_heldout_prediction`来生成来自不同样本的预测直方图：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s look at some of the images and their predictions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些图像及其预测结果：
- en: '![](img/0032af64-7c08-4a41-a99e-77a001e86ba7.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0032af64-7c08-4a41-a99e-77a001e86ba7.png)'
- en: 'For the preceding image, all of the predictions belonged to the correct class
    02, as shown in the following diagram:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的图像，所有的预测都属于正确的类别 02，如下图所示：
- en: '![](img/ce216508-385a-4f51-bf4b-56aec0ed1850.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce216508-385a-4f51-bf4b-56aec0ed1850.png)'
- en: In the following two cases, although our mean prediction was correct, some samples
    in Monte Carlo predicted the wrong class. You can imagine how quantifying uncertainty
    in such cases can make a self-driving car make better decisions on the road.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下两个案例中，尽管我们的平均预测是正确的，但在蒙特卡罗中，一些样本预测了错误的类别。你可以想象，在这样的情况下量化不确定性如何帮助自动驾驶汽车做出更好的道路决策。
- en: '**Case 1: **'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 1：**'
- en: '![](img/68be4b03-477a-477c-95c0-9947f85ef0cf.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68be4b03-477a-477c-95c0-9947f85ef0cf.png)'
- en: 'In the preceding image, some of the Monte Carlo predictions belonged to the
    wrong class, as shown in the following diagram:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，一些蒙特卡罗的预测属于错误的类别，如下图所示：
- en: '![](img/e2070f84-669b-4fcd-affa-82c0922332c2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2070f84-669b-4fcd-affa-82c0922332c2.png)'
- en: '**Case 2: **'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 2：**'
- en: '![](img/232995e8-3458-4c34-a5b7-05ddc7da72ca.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/232995e8-3458-4c34-a5b7-05ddc7da72ca.png)'
- en: 'In the preceding image, some of the Monte Carlo predictions belonged to the
    wrong class, as shown in the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，一些蒙特卡罗的预测属于错误的类别，如下图所示：
- en: '![](img/bfd0ab71-71a0-45b9-81b6-8c389e0b4cd8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfd0ab71-71a0-45b9-81b6-8c389e0b4cd8.png)'
- en: '**Case 3:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 3：**'
- en: 'In the following case, average prediction is incorrect, but some samples were correctly
    predicted:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下情况下，平均预测是错误的，但一些样本是正确预测的：
- en: '![](img/01afa093-37b6-469d-b53f-02db3dd06cf1.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01afa093-37b6-469d-b53f-02db3dd06cf1.png)'
- en: 'For the preceding image, we obtained the following histogram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的图像，我们得到了以下的直方图：
- en: '![](img/efffb17d-c64b-4d65-a4cd-621a1eb9fff3.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efffb17d-c64b-4d65-a4cd-621a1eb9fff3.png)'
- en: '**Case 4:**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 4：**'
- en: 'Obviously, we will get cases where we didn''t predict correctly for any sample:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们会遇到一些情况下预测不准确的样本：
- en: '![](img/1775ee39-0175-48a1-961c-99fa49b20639.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1775ee39-0175-48a1-961c-99fa49b20639.png)'
- en: 'For this image, we obtained the following histogram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此图像，我们得到了以下的直方图：
- en: '![](img/06c3c145-42c9-4d7e-b3f6-40442e7fbc36.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06c3c145-42c9-4d7e-b3f6-40442e7fbc36.png)'
- en: 'Finally, visualize the posterior of weights in the network. In the following
    plot we are showing both the posterior mean and standard deviation of the different
    weights in the network:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，直观地显示网络中权重的后验分布。在以下图表中，我们展示了网络中不同权重的后验均值和标准差：
- en: '![](img/ec78307f-11b7-4edc-a408-17e9f7d78b43.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec78307f-11b7-4edc-a408-17e9f7d78b43.png)'
- en: Having a distribution on weights enables us to develop predictions for the same
    image, which is extremely useful in developing a confidence interval around our
    predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对权重进行分布建模使我们能够为相同的图像开发预测，这在为预测构建置信区间时极为有用。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Neural networks, as we know, are great for point predictions, but can't help
    us identify the uncertainty in their predictions. On the other hand, Bayesian
    learning is great for quantifying uncertainty, but doesn't scale well in multiple
    dimensions or problems with big unstructured datasets such as images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知道的，神经网络非常适合点预测，但无法帮助我们识别预测中的不确定性。另一方面，贝叶斯学习非常适合量化不确定性，但在多维度问题或大规模非结构化数据集（如图像）中扩展性较差。
- en: In this chapter, we looked at how we can combine neural networks with Bayesian
    learning using Bayesian neural networks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了如何通过贝叶斯神经网络将神经网络与贝叶斯学习结合起来。
- en: 'We used the dataset of German Traffic Signs to develop a Bayesian neural network
    classifier using Google''s recently released tool: TensorFlow probability. TF
    probability provides high-level APIs and functions to perform Bayesian modeling
    and inference.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了德国交通标志数据集，利用Google最近发布的工具TensorFlow概率，开发了一个贝叶斯神经网络分类器。TF概率提供了高层次的API和函数来执行贝叶斯建模和推理。
- en: We trained the Lenet model on the dataset. Finally, we used Monte Carlo to sample
    from the posterior of the parameters of the network to obtain predictions for
    each sample of the test dataset to quantify uncertainty.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据集上训练了Lenet模型。最后，我们使用蒙特卡罗方法从网络参数的后验分布中进行采样，以获得每个测试数据集样本的预测值，从而量化不确定性。
- en: However, we have only scratched the surface in terms of the complexity of Bayesian
    neural networks. If we want to develop safe AI, then understanding uncertainty
    in our predictions is of the utmost importance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仅仅触及了贝叶斯神经网络复杂性的表面。如果我们想要开发安全的人工智能，那么理解我们预测中的不确定性至关重要。
- en: In the next chapter, we will learn about a new concept in machine learning known
    as autoencoders. We will look at how to detect credit card fraud using them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习一个在机器学习中新的概念——自编码器。我们将探讨如何利用它们来检测信用卡欺诈。
- en: Questions
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is TensorFlow probability?
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow 概率？
- en: What is Variational inference and why is it important?
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分推断是什么，为什么它很重要？
- en: What is KL divergence?
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 KL 散度？
- en: What do we mean by prior and posterior on the weights of neural networks?
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的权重上的先验和后验是什么意思？
