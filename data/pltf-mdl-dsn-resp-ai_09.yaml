- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Model Explainability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型可解释性
- en: “If you can’t explain it simply, you don’t understand it well enough.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “如果你不能简单地解释它，那说明你还没有完全理解它。”
- en: – Albert Einstein
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 阿尔伯特·爱因斯坦
- en: '**Model explainability** is an important topic in the fields of **Machine Learning**
    (**ML**) and **Artificial Intelligence** (**AI**). It refers to the ability to
    understand and explain how a model makes predictions and decisions. Explainability
    is important because it allows us to identify potential biases or errors in a
    model, and it can improve the performance and trustworthiness of AI models.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型可解释性**是**机器学习**（**ML**）和**人工智能**（**AI**）领域的重要话题。它指的是理解和解释模型如何做出预测和决策的能力。可解释性很重要，因为它使我们能够识别模型中的潜在偏见或错误，并能提高
    AI 模型的性能和可信度。'
- en: In this chapter, we will explore different methods and techniques for explaining
    and interpreting ML models. We will also examine the challenges and limitations
    of model explainability and will consider potential solutions to improve the interpretability
    of ML algorithms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索不同的解释和理解机器学习模型的方法和技术。我们还将探讨模型可解释性面临的挑战和局限性，并考虑改善机器学习算法可解释性的潜在解决方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to Explainable AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释 AI 简介
- en: '**Explain Like I’m** **Five** (**ELI5**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**像我五岁一样解释** (**ELI5**)'
- en: '**Local Interpretable Model-Agnostic** **Explanations** (**LIME**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地可解释的模型无关** **解释** (**LIME**)'
- en: Understanding churn modeling using XAI techniques
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 XAI 技术理解客户流失建模
- en: CausalNex
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CausalNex
- en: DoWhy for causal inference
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于因果推理的 DoWhy
- en: AI Explainability 360 for interpreting models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI 可解释性 360 用于解释模型
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8 along with the following Python
    packages:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求你安装 Python 3.8，并使用以下 Python 包：
- en: NumPy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: Scikit-learn
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: pandas
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: 'Install ELI5 as follows:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照以下方式安装 ELI5：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To install LIME, use the following:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 LIME，请使用以下命令：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'SHAP can be installed using this:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用以下命令安装 SHAP：
- en: '[PRE2]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Install DoWhy using the following:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令安装 DoWhy：
- en: '[PRE3]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: AI Explainability 360
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI 可解释性 360
- en: Introduction to Explainable AI
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释 AI 简介
- en: Consider a model that predicts whether a patient is likely to develop a certain
    terminal disease, a model that assists in making decisions about whether the person
    on trial is guilty or not, and a model that assists banks to determine whether
    to give someone a loan or not. Each of these models makes decisions that can have
    a profound domino effect on multiple lives (unlike a model used by Netflix that
    recommends movies to watch). Therefore, it is important that institutions with
    models employed in decision-making processes can explain the reasoning behind
    their predictions and decisions. Model explainability, or **Explainable AI** (**XAI**),
    deals with developing algorithms and techniques that allow us to understand and
    interpret the reasoning behind a model’s predictions and decisions. As we have
    seen in the preceding examples, XAI is especially important in domains such as
    healthcare, finance, and criminal justice, as the consequences of model decisions
    can have a significant impact on individuals and society.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个预测患者是否可能患某种终末期疾病的模型，一个帮助决策是否认定被告有罪的模型，以及一个帮助银行决定是否给某人贷款的模型。这些模型所做出的决策可能会对多个人的生活产生深远的连锁反应（与
    Netflix 推荐电影的模型不同）。因此，拥有决策模型的机构必须能够解释其预测和决策背后的理由。模型可解释性或**可解释 AI**（**XAI**）旨在开发能够帮助我们理解和解释模型预测和决策背后原因的算法和技术。正如我们在前面的例子中看到的，XAI
    在医疗保健、金融和刑事司法等领域尤为重要，因为模型决策的后果可能对个人和社会产生重大影响。
- en: Currently, many ML models are considered **black boxes** due to their complex
    inner workings and lack of transparency. This can lead to concerns about accountability
    and bias and can hinder stakeholders’ adoption of and trust in these models. To
    address these issues, there is a growing need for methods and techniques that
    can provide explainability and interpretability for ML models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，由于机器学习模型复杂的内部机制和缺乏透明度，许多模型被视为**黑箱**。这可能引发关于问责制和偏见的担忧，并可能阻碍利益相关者采纳这些模型并信任它们。为了解决这些问题，越来越需要能够为机器学习模型提供可解释性和解释能力的方法和技术。
- en: XAI is an ML interpretability technique that focuses on understanding the predictions
    made by an ML model and explaining how those predictions were reached in a way
    that’s understandable for humans in order to build trust in the model. As the
    name suggests, XAI broadly focuses on model explanations and provides interfaces
    for deciphering these explanations to bridge ML and human systems efficiently.
    It is a key component of broader human-centric responsible AI practices.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: XAI是一种机器学习可解释性技术，重点在于理解机器学习模型的预测，并以人类能够理解的方式解释这些预测是如何得出的，从而建立对模型的信任。顾名思义，XAI广泛关注模型解释，并提供接口以解读这些解释，从而有效地架起机器学习与人类系统之间的桥梁。它是更广泛的人本责任AI实践的关键组成部分。
- en: '*Figure 9**.1* shows the different fields that interact to build a human-centric
    XAI system:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.1*展示了构建以人为本的XAI系统所涉及的不同领域：'
- en: '![Figure 9.1 – A human-centric XAI system](img/Figure_9.01_B18681.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 以人为本的XAI系统](img/Figure_9.01_B18681.jpg)'
- en: Figure 9.1 – A human-centric XAI system
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 以人为本的XAI系统
- en: The development of XAI has accelerated alongside that of applied AI/**Deep Learning**
    (**DL**) systems from roughly 2015\. Progressively complicated DL models, such
    as deep neural networks, have revealed new explainability challenges for both
    developers and model stakeholders. When it comes to the implementation of AI,
    not even engineers or data scientists have a clear picture of what’s going on
    behind the scenes because of the characteristic black-box nature of neural networks.
    As a result, it becomes difficult to understand a model’s predictions and trust
    them in high-stakes situations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: XAI的发展与应用AI/**深度学习**（**DL**）系统的发展同步，始于大约2015年。越来越复杂的DL模型，如深度神经网络，为开发者和模型相关方揭示了新的可解释性挑战。在AI的实施过程中，由于神经网络的黑箱特性，即使是工程师或数据科学家也无法清楚了解其背后的实际情况。因此，理解一个模型的预测变得困难，且在高风险场景中难以信任这些预测。
- en: XAI is a solution when we want to determine whether we can trust the result
    of our AI model and how confident we should be that the model is correct. It allows
    us to understand how the AI model came up with its result in a certain situation
    and, consequently, builds trust in those outcomes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: XAI是当我们想要判断是否可以信任我们的AI模型的结果，并且如何确定我们有多大的信心相信该模型是正确的时的一种解决方案。它使我们能够理解AI模型在某种情况下如何得出结果，从而建立对这些结果的信任。
- en: 'To achieve a reliable XAI system, we should focus on three main components:
    prediction accuracy, traceability, and decision understanding, as shown in *Figure
    9**.2*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现一个可靠的XAI系统，我们应该关注三个主要组成部分：预测准确性、可追溯性和决策理解，如*图 9.2*所示：
- en: '![Figure 9.2 – Three main components of an XAI system](img/Figure_9.02_B18681.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – XAI系统的三个主要组成部分](img/Figure_9.02_B18681.jpg)'
- en: Figure 9.2 – Three main components of an XAI system
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – XAI系统的三个主要组成部分
- en: 'Let’s discuss each of these components:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下这些组件：
- en: '**Prediction accuracy** addresses technology requirements and is undoubtedly
    a vital component in the success of AI usage in day-to-day operations. Prediction
    accuracy can be determined by comparing XAI output to the results in the training/test
    dataset. For example, we can use algorithms such as LIME to explain predictions
    made by AI/DL/ML classifiers.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测准确性**涉及技术要求，毫无疑问，它是人工智能在日常操作中成功应用的关键组成部分。预测准确性可以通过将XAI输出与训练/测试数据集中的结果进行比较来确定。例如，我们可以使用LIME等算法来解释AI/DL/ML分类器做出的预测。'
- en: '**Decision understanding** addresses human requirements and is all about educating
    and informing teams in order to overcome AI distrust and help them to understand
    how decisions were made. This information can be presented to end users in the
    form of a dashboard displaying the primary factors in the making of a certain
    decision and the extent to which each of those factors influenced the decision.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策理解**涉及人类需求，旨在通过教育和信息传递，克服对AI的不信任，帮助团队理解决策是如何做出的。这些信息可以通过仪表盘的形式展示给最终用户，显示影响某一决策的主要因素，以及每个因素对决策的影响程度。'
- en: '**Traceability** can impact and limit decision-making, setting up a narrower
    scope for ML rules and features. Techniques such as **Deep Learning Important**
    **FeaTures** (**DeepLIFT**) can be utilized to compare the activation of each
    neuron in a neural network to its reference neuron by displaying traceability
    links and dependencies.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可追溯性**可能影响并限制决策过程，为机器学习规则和特征设定更窄的范围。像**深度学习重要特征**（**DeepLIFT**）这样的技术可以通过显示可追溯链接和依赖关系，比较神经网络中每个神经元与其参考神经元的激活情况。'
- en: With no or little understanding of how AI decisions are derived, the user’s
    trust in the framework is harmed and it may ultimately lead to model rejection.
    Consider a system making recommendations to users about connections to add to
    their network or products to buy. An explanation of these recommendations will
    act as a catalyst in the adoption of ML systems by making information more significant
    to the user.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户对AI决策的生成过程缺乏理解，信任框架将受到损害，并最终可能导致模型被拒绝。考虑一个向用户推荐社交网络连接或购买产品的系统。对这些推荐的解释将成为机器学习系统采纳的催化剂，使信息对用户更具意义。
- en: XAI is about more than building trust in the model; it is also about troubleshooting
    and improving the model’s performance. It allows us to investigate model behavior
    by tracking model insights on deployment status, fairness, quality, and model
    drift. By using XAI, model performance can be analyzed and alerts can be generated
    when a model deviates from the intended outcomes and gives a substandard performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: XAI不仅仅是建立模型信任，它还涉及故障排除和提高模型性能。它使我们能够通过追踪模型在部署状态、公平性、质量和模型漂移等方面的洞察来调查模型行为。通过使用XAI，可以分析模型性能，并在模型偏离预期结果并表现不佳时生成警报。
- en: Scope of XAI
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XAI的范围
- en: 'Having an understanding of models is crucial for many tasks involved in building
    and operating ML systems, and XAI can be used to do the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 理解模型对于构建和操作机器学习系统中的许多任务至关重要，XAI可以用于以下任务：
- en: '**Hone modeling and data collection processes**: After aggregating and comparing
    across dataset splits, XAI provides a means to identify and alert users of common
    ML pitfalls such as data skew and drift.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完善建模和数据收集过程**：在对数据集进行拆分聚合和比较后，XAI提供了一种方法，可以识别并提醒用户注意常见的机器学习问题，如数据偏斜和漂移。'
- en: '**Debug model performance**: It allows us to debug unexpected behavior from
    the model and monitor deeper feature-level insights to inform corrective actions.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调试模型性能**：XAI使我们能够调试模型的意外行为，并监控更深层次的特征级洞察，以指导修正行动。'
- en: '**Build trust**: It informs and supports the decision-making process by explaining
    predictions to build trust with end users, making model decisions equitable and
    reliable.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立信任**：通过解释预测，XAI支持决策过程，帮助与最终用户建立信任，使得模型决策更加公正和可靠。'
- en: '**Identify unexpected predictions**: It verifies model behavior and informs
    on amendatory actions. For instance, it allows regulators to efficiently validate
    that ML decisions comply with laws to mitigate the risk of generating poor model
    outcomes for end users.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别意外的预测**：它验证模型行为并提供修改措施。例如，它可以帮助监管机构有效地验证机器学习决策是否符合相关法律，以减少对最终用户产生不良模型结果的风险。'
- en: '**Act as a catalyst**: It acts as a catalyst for the adoption of ML systems
    by building user trust and presenting model outcomes in understandable forms to
    stakeholders.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**充当催化剂**：XAI通过建立用户信任，并以易于理解的形式向利益相关者展示模型结果，促进了机器学习系统的采用。'
- en: In an ideal world, an explainable and transparent model can be used across distinct
    industries, such as healthcare, by accelerating diagnostics, processing images,
    streamlining pharmaceutical process approvals, and making critical decisions in
    manufacturing, finance, logistics, and criminal justice, such as accelerating
    the conclusions of DNA analysis or prison population analysis.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界中，一个可解释且透明的模型可以在不同的行业中使用，例如在医疗健康领域，通过加速诊断、处理图像、简化药品审批流程，以及在制造、金融、物流和刑事司法领域做出关键决策，如加速DNA分析或监狱人口分析的结论。
- en: Explainability can help developers to ensure that the system is working as expected,
    meets regulatory standards, and even allows the person affected by a decision
    to challenge that outcome. The ability to understand and explain model predictions
    allows companies and institutions to make informed decisions, especially where
    stakeholders require justifications, such as in strategic business decisions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性可以帮助开发人员确保系统按预期工作，符合监管标准，甚至允许受决策影响的人挑战该结果。理解和解释模型预测的能力使公司和机构能够做出明智的决策，尤其是在利益相关者需要证明的情况下，如战略商业决策中。
- en: Challenges in XAI
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XAI的挑战
- en: 'Some of the challenging dilemmas faced when implementing XAI solutions are
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 实施XAI解决方案时面临的一些具有挑战性的困境如下：
- en: '**Algorithm confidentiality**: Often, due to security concerns, the details
    of the model and algorithms used are confidential. In such a scenario, it becomes
    a challenging task to ensure that the AI system did not learn a biased perspective
    (or an unbiased perspective of a biased world) as a result of gaps in the training
    data, objective function, or model.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法机密性**：通常，由于安全问题，所使用的模型和算法的细节是保密的。在这种情况下，确保AI系统没有因为训练数据、目标函数或模型的缺陷而学到偏见观点（或一个有偏见世界的无偏见观点）就成为了一项具有挑战性的任务。'
- en: '**Fairness**: It is challenging for XAI to determine whether a decision taken
    or an output obtained from an AI framework is fair or not, as the impression of
    fairness is subjective and relies upon the data used to train the AI/ML model.
    Furthermore, the definition of fairness can change depending on the use case,
    culture, and so on.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**：对于XAI来说，判断AI框架所做的决策或得到的输出是否公平是一项挑战，因为公平的印象是主观的，且依赖于用于训练AI/ML模型的数据。此外，公平性的定义可能会根据使用场景、文化等发生变化。'
- en: '**Reliability**: Without evaluating the process of how the AI system reached
    a certain outcome, it becomes hard to rely on the system, as clarification is
    required on whether the system is legitimate or not.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：如果不评估AI系统如何得出某个结果，就很难依赖该系统，因为需要澄清该系统是否合法。'
- en: Building a model that can be explained and interpreted easily is the key to
    overcoming the aforementioned challenges. Highly intricate algorithms can be recreated
    or replaced with simpler approaches, which are easier to explain with the help
    of XAI.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个可以轻松解释和解读的模型是克服上述挑战的关键。高度复杂的算法可以通过更简单的方式重新创建或替代，这些方法借助XAI可以更容易地解释。
- en: Classification of XAI techniques
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XAI技术分类
- en: 'XAI techniques can be classified based on two main criteria—scope and model.
    Refer to *Figure 9**.3*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: XAI技术可以根据两个主要标准进行分类——范围和模型。请参见*图9.3*：
- en: '![Figure 9.3 – Classification of XAI techniques](img/Figure_9.03_B18681.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – XAI技术分类](img/Figure_9.03_B18681.jpg)'
- en: Figure 9.3 – Classification of XAI techniques
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – XAI技术分类
- en: 'Let’s discuss each technique:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论每种技术：
- en: '**Scope**: XAI techniques can be classified based on the scope of their explanation.
    There are two main categories of XAI techniques in terms of scope: **local** and
    **global**. Local explanations focus on describing the behavior of an AI model
    for a specific input or output. These explanations provide insights into how the
    model arrived at a particular decision for a given input and are useful for understanding
    the model’s behavior in specific cases. Global explanations, on the other hand,
    provide insights into the general behavior of an AI model across a wide range
    of input and output. These explanations are useful for understanding the overall
    decision-making process of an AI model and how it handles different types of input.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围**：XAI技术可以根据其解释的范围进行分类。在范围上，XAI技术主要分为两大类：**局部**和**全局**。局部解释侧重于描述AI模型在特定输入或输出下的行为。这些解释提供了有关模型如何针对特定输入做出决策的见解，有助于理解模型在特定情况下的行为。另一方面，全局解释则提供有关AI模型在广泛输入和输出下的一般行为的见解。这些解释有助于理解AI模型的整体决策过程及其如何处理不同类型的输入。'
- en: '**Model**: XAI techniques can also be classified based on the type of model
    they are applied to. There are two main categories of XAI techniques in the context
    of models: **model-specific** and **model-agnostic**. Model-specific explanations
    are tailored to the architecture and design of a particular AI model. These explanations
    are often local in scope, providing insights into the behavior of the model for
    a specific input or output. Examples of model-specific XAI techniques include
    feature importance analysis and saliency maps. Model-agnostic explanations, on
    the other hand, are not specific to any particular AI model and can be applied
    to a wide range of different models. These explanations are often global in scope,
    providing insights into the overall behavior of an AI model across a wide range
    of input and output. Examples of model-agnostic XAI techniques include counterfactual
    analysis and model distillation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：XAI技术也可以根据应用的模型类型进行分类。在模型的上下文中，XAI技术主要分为两类：**特定模型**和**无关模型**。特定模型的解释是针对特定AI模型的架构和设计量身定制的。这些解释通常具有局部性质，提供有关特定输入或输出的模型行为的见解。特定模型的XAI技术的例子包括特征重要性分析和显著性图。另一方面，无关模型的解释并不特定于任何特定的AI模型，可以应用于广泛的不同模型。这些解释通常具有全局性质，提供有关AI模型在各种输入和输出情况下的整体行为的见解。无关模型的XAI技术的例子包括反事实分析和模型蒸馏。'
- en: '*Figure 9**.4* expands on the differences between model-specific and model-agnostic
    XAI techniques:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.4* 扩展了特定模型与无关模型的XAI技术之间的差异：'
- en: '![Figure 9.4 – Model-specific versus model-agnostic XAI techniques](img/Figure_9.04_B18681.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 特定模型与无关模型的XAI技术](img/Figure_9.04_B18681.jpg)'
- en: Figure 9.4 – Model-specific versus model-agnostic XAI techniques
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 特定模型与无关模型的XAI技术
- en: Overall, XAI techniques can be classified based on their scope (local or global)
    and how they relate to the model they are applied to (model-specific or model-agnostic).
    Understanding the different types of XAI techniques and how they can be applied
    can help practitioners choose the most appropriate technique for a given situation
    and better understand the behavior of their AI models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，XAI技术可以根据其范围（局部或全局）以及它们与所应用模型的关系（特定模型或无关模型）进行分类。了解不同类型的XAI技术以及它们如何应用，可以帮助从业者为特定情境选择最合适的技术，并更好地理解他们AI模型的行为。
- en: Let’s now explore some Python libraries that can be used for model explainability.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨一些可以用于模型可解释性的Python库。
- en: Explain Like I’m Five (ELI5)
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释像我五岁一样（ELI5）
- en: This Python library enables us to visualize and debug ML models. It displays
    functionality for both local and global interpretations. XGBoost, LightGBM, scikit-learn,
    Lightning, and CatBoost are some of the libraries supported by ELI5\. The goal
    of ELI5 is to make the explanations accessible to a general audience, including
    those who may not have a background in AI or ML.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Python库使我们能够可视化和调试机器学习模型。它提供了本地和全局解释的功能。XGBoost、LightGBM、scikit-learn、Lightning和CatBoost是ELI5支持的库之一。ELI5的目标是使解释对普通观众易于理解，包括那些没有人工智能或机器学习背景的人。
- en: Here is an example of how ELI5 could be used to provide an explanation for the
    predictions made by an AI model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，展示了如何使用ELI5来提供对AI模型预测的解释。
- en: Imagine that an AI model has been trained to predict the likelihood of a patient
    developing a particular disease based on various medical characteristics, such
    as age, gender, and medical history. The model predicts that a particular patient
    has a high likelihood of developing the disease. To provide an ELI5 explanation
    for this prediction, the model could provide a simple and easy-to-understand explanation
    such as *Based on your age, gender, and medical history, our model predicts that
    you have a high chance of developing the disease. This is because people with
    similar characteristics have a high risk of developing the disease according to
    our data*. This ELI5 explanation provides a clear and concise explanation for
    the prediction made by the model, using language and concepts that are accessible
    to a general audience. It also highlights the factors that the model considered
    when making the prediction, which can help to increase the transparency and accountability
    of the AI system.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个AI模型已经被训练用来预测一个病人根据多种医疗特征（如年龄、性别和病史）患某种特定疾病的可能性。该模型预测某个特定病人患该疾病的可能性较高。为了提供这个预测的ELI5解释，模型可以给出一个简单易懂的解释，比如：*根据你的年龄、性别和病史，我们的模型预测你患该疾病的几率较高。这是因为根据我们的数据，具有类似特征的人患此病的风险较高*。这个ELI5解释为模型做出的预测提供了清晰简洁的说明，使用了普通观众可以理解的语言和概念。它还突出了模型在做出预测时考虑的因素，有助于增加AI系统的透明度和问责性。
- en: 'With the following, we present a general outline of how ELI5 can be implemented
    in code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们介绍ELI5如何在代码中实现的一个大致框架：
- en: First, the AI model that you want to explain should be trained and deployed.
    This could be an ML model, a DL model, or any other type of AI model.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您要解释的AI模型应已经过训练并部署。这个模型可以是机器学习（ML）模型、深度学习（DL）模型或其他类型的AI模型。
- en: Next, you will need to select the input or output of the model that you want
    to explain. This could be a single prediction made by the model or a batch of
    predictions.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你需要选择你想要解释的模型输入或输出。这可以是模型做出的单个预测，也可以是一批预测。
- en: You will then need to choose an ELI5 explanation method that is appropriate
    for your model and the type of explanation you want to provide. There are many
    different ELI5 methods, including methods that provide feature importance explanations,
    saliency maps, and counterfactuals.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你需要选择一个适合你的模型和你想要提供的解释类型的ELI5解释方法。有许多不同的ELI5方法，包括提供特征重要性解释、显著性图和反事实解释等方法。
- en: Once you have selected an ELI5 method, you can use it to generate an explanation
    for the input or output of the model. This may involve running the input or output
    through the ELI5 method and then processing the resulting explanation to make
    it simple and easy to understand for a general audience.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你选择了一个ELI5方法，你就可以用它来生成模型输入或输出的解释。这可能涉及将输入或输出通过ELI5方法进行处理，然后对生成的解释进行进一步处理，使其简单易懂，适合一般观众。
- en: Finally, you can present the ELI5 explanation to the user clearly and concisely,
    using language and concepts that are accessible to a general audience. This could
    involve displaying the explanation in a user interface, printing it to the console,
    or saving it to a file.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以清晰简洁地将ELI5解释呈现给用户，使用普通观众可以理解的语言和概念。这可能涉及将解释显示在用户界面中、打印到控制台或保存到文件中。
- en: However, there are some limitations to ELI5\. One limitation is that it may
    not be possible to provide a simple and easy-to-understand explanation for every
    decision made by an AI model. In some cases, the decision-making process of an
    AI model may be too complex or nuanced to be explained simply. Another limitation
    is that ELI5 explanations may not always be sufficient to fully understand the
    behavior of an AI model, especially for those with a more technical background.
    In these cases, more detailed and technical explanations may be necessary. Also
    note that it does not work with all models – for example, with scikit-learn, it
    works only on linear classifiers, regressors, and tree-based models. With Keras,
    it supports the explanation of image classifiers using Grad-CAM visualizations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ELI5 存在一些局限性。其中一个局限性是，可能无法为 AI 模型做出的每个决策提供一个简单且易于理解的解释。在某些情况下，AI 模型的决策过程可能过于复杂或细微，无法简单地解释。另一个局限性是，ELI5
    解释可能不足以完全理解 AI 模型的行为，尤其是对于那些具有更技术背景的人。在这些情况下，可能需要更详细和技术性的解释。另外，请注意，它并不适用于所有模型——例如，在
    scikit-learn 中，它仅适用于线性分类器、回归器和基于树的模型。在 Keras 中，它支持使用 Grad-CAM 可视化来解释图像分类器。
- en: LIME
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LIME
- en: '**LIME** is a popular XAI technique that’s used to provide local explanations
    for the predictions made by an AI model. The goal of LIME is to provide an explanation
    of how an AI model arrives at a particular prediction from a specific input that
    is simple and easy to understand for a general audience.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**LIME** 是一种流行的 XAI 技术，用于为 AI 模型的预测提供局部解释。LIME 的目标是提供一个简单、易于理解的解释，帮助普通观众理解
    AI 模型是如何从特定输入中得出某个预测的。'
- en: LIME is model-agnostic, which means that it can be applied to a wide range of
    different AI models, regardless of their architecture or design. This makes it
    a flexible and widely applicable tool for explaining the behavior of AI models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 是与模型无关的，这意味着它可以应用于各种不同的 AI 模型，而不受其架构或设计的限制。这使得 LIME 成为一个灵活且广泛适用的工具，用于解释
    AI 模型的行为。
- en: When generating explanations with LIME, the technique first generates a set
    of perturbed versions of the input data, called **perturbations**. These perturbations
    are obtained by randomly changing the values of some of the input features (one
    at a time) while keeping the other features unchanged. The AI model is then used
    to make predictions for each of the perturbations, and the resulting predictions
    are used to build a simple linear model that approximates the behavior of the
    AI model in the local region around the original input. This linear model is then
    used to provide an explanation for the prediction made by the AI model. The explanation
    can be presented to the user in the form of a list of the most important features
    that contributed to the prediction, along with the relative importance of each
    feature.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 LIME 生成解释时，该技术首先生成一组扰动版本的输入数据，称为 **扰动**。这些扰动是通过随机改变某些输入特征的值（每次一个特征），同时保持其他特征不变来获得的。然后，使用
    AI 模型对每个扰动做出预测，结果的预测被用来构建一个简单的线性模型，该模型近似原始输入附近的 AI 模型行为。这个线性模型接着被用来为 AI 模型做出的预测提供解释。解释可以以一个特征列表的形式呈现给用户，这个列表列出了对预测贡献最大的特征，并显示每个特征的相对重要性。
- en: For example, consider an email classification system that uses an AI model to
    classify emails as spam or not spam. To explain the classification of a particular
    email using LIME, the algorithm might generate a set of perturbations by randomly
    altering the words in the email while keeping the other features unchanged. The
    AI model would then make predictions for each of the perturbations, and the resulting
    predictions would be used to build a simple linear model that approximates the
    behavior of the AI model in the local region around the original email.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个使用 AI 模型对电子邮件进行分类的系统，该系统将电子邮件分类为垃圾邮件或非垃圾邮件。为了使用 LIME 解释某个特定电子邮件的分类，算法可能通过随机更改电子邮件中的一些单词，同时保持其他特征不变，生成一组扰动。然后，AI
    模型将对每个扰动做出预测，结果的预测将用来构建一个简单的线性模型，该模型近似原始电子邮件附近的 AI 模型行为。
- en: The linear model generated by LIME could then be used to provide an explanation
    for the classification of the original email, such as a list of the most important
    words in the email that contributed to the classification, along with the relative
    importance of each word. This explanation would be locally faithful to the email,
    meaning that it would accurately reflect the behavior of the AI model in the local
    region around the email, and would be learned over an interpretable representation
    of the email (that is, a list of words).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LIME生成的线性模型随后可以用于解释原始电子邮件的分类结果，例如一份最重要词汇的列表，展示这些词汇对分类的贡献，并附上每个词汇的相对重要性。这个解释将在局部区域内忠实地反映AI模型的行为，意味着它能准确地体现AI模型在电子邮件周围局部区域的行为，并且是在电子邮件的可解释表示（即一份词汇列表）上学习得出的。
- en: LIME is a useful tool for understanding the behavior of AI models and for providing
    simple and easy-to-understand explanations for the predictions made by these models.
    However, it has some limitations, such as the fact that it is only able to provide
    local explanations and may not be able to capture the full complexity of an AI
    model’s decision-making process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: LIME是一个用于理解AI模型行为的有用工具，它可以为这些模型做出的预测提供简单且易于理解的解释。然而，它也有一些局限性，例如它只能提供局部解释，可能无法捕捉到AI模型决策过程的全部复杂性。
- en: 'Let’s now move on to the next tool in our toolbox: SHAP.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续介绍工具箱中的下一个工具：SHAP。
- en: SHAP
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP
- en: '**SHAP** (short for **SHapley Additive exPlanations**) is another popular XAI
    technique. The goal of SHAP is to explain the overall decision-making process
    of an AI model and how it handles different types of input and to provide an explanation
    for this process that is simple and easy to understand for a general audience.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHAP**（即**Shapley加性解释**）是另一种流行的XAI技术。SHAP的目标是解释AI模型的整体决策过程以及它如何处理不同类型的输入，并为这一过程提供一个简单且易于理解的解释，面向一般观众。'
- en: SHAP is based on the concept of Shapley values from game theory, which provides
    a way to fairly distribute the contributions of different players toward a collective
    outcome. In the context of XAI, SHAP uses Shapley values to calculate the relative
    importance of each feature in an input data point to the prediction made by an
    AI model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP基于博弈论中的Shapley值概念，Shapley值提供了一种公平地分配不同参与者对集体结果贡献的方法。在XAI的背景下，SHAP利用Shapley值计算输入数据点中每个特征对AI模型预测的相对重要性。
- en: To generate explanations with SHAP, the technique first calculates the Shapley
    values of each feature in the input data. This is done by considering all possible
    combinations of the input features and the corresponding predictions made by the
    AI model and then using these combinations to calculate the relative contribution
    of each feature to the prediction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SHAP生成解释，技术首先计算输入数据中每个特征的Shapley值。这是通过考虑所有可能的输入特征组合以及AI模型的相应预测，然后使用这些组合计算每个特征对预测的相对贡献。
- en: The Shapley values calculated by SHAP are then used to provide an explanation
    for the prediction made by the AI model. This explanation can be presented to
    the user in the form of a list of the most important features that contributed
    to the prediction, along with the relative importance of each feature.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP计算的Shapley值随后用于为AI模型的预测提供解释。这个解释可以以一份最重要特征的列表呈现给用户，这些特征对预测的贡献以及每个特征的相对重要性也会一并显示。
- en: Shapley values and cooperative games
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值与合作博弈
- en: Shapley values are based on the concept of a cooperative game in which a group
    of participants collaborates to attain a shared goal. The Shapley value is a measure
    of a player’s contribution to the collective outcome, taking into consideration
    the efforts of all other members. Shapley values offer a fair value to each player
    based on that player’s contribution to the group outcome. This means that each
    participant receives credit solely for the value they provide to the outcome while
    accounting for the efforts of the other players. Consider a group of individuals
    who are collaborating to build a house. Taking into consideration the efforts
    of all other participants, the Shapley value of each participant would represent
    their contribution to the construction of the house. A person who builds the house’s
    foundations would receive credit for the foundations’ worth, but not for the roof
    or walls, and so on, which were contributed by other people.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值基于一种合作博弈的概念，在这种博弈中，一组参与者合作以达成共同的目标。Shapley值是衡量玩家对集体结果贡献的指标，考虑到所有其他成员的努力。Shapley值为每个玩家提供了公平的价值，依据他们对集体结果的贡献。这意味着每个参与者只因自己对结果的贡献而获得功劳，同时考虑到其他玩家的努力。假设一组人正在合作建造一座房子。考虑到所有其他参与者的努力，每个参与者的Shapley值将代表他们对房屋建造的贡献。一个负责打地基的人会因地基的价值而获得功劳，但不会因为房顶或墙壁的建设而获得功劳，后者是由其他人贡献的。
- en: SHAP is a powerful and widely applicable tool for understanding the behavior
    of AI models and for providing global explanations for the predictions made by
    these models. However, it has some limitations, such as the fact that it can be
    computationally expensive to calculate Shapley values for large datasets. In addition,
    SHAP explanations may not always be easy for users with a limited technical background
    to interpret.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP是一个强大且广泛适用的工具，用于理解AI模型的行为，并为这些模型所做的预测提供全球性的解释。然而，它也有一些限制，例如，计算大数据集的Shapley值可能在计算上非常昂贵。此外，对于技术背景有限的用户来说，SHAP的解释可能并不总是易于理解。
- en: Now that we have seen some tools that can be used to explain model predictions,
    let’s see how we can use them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些可以用来解释模型预测的工具，让我们看看如何使用它们。
- en: Understanding churn modeling using XAI techniques
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用XAI技术理解客户流失建模
- en: Now that you have an idea of the ELI5, LIME, and SHAP techniques, let’s use
    them on a real-life problem. For the purpose of demonstration, we will consider
    the problem of **churn modeling**.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对ELI5、LIME和SHAP技术有了一定了解，让我们在实际问题中使用它们。为了演示，我们将考虑**客户流失建模**的问题。
- en: Churn modeling is a type of predictive modeling used to identify customers who
    are likely to stop using a company’s products or services, also known as *churning*.
    Churn modeling is commonly used in industries such as telecommunications, financial
    services, and e-commerce, where customer retention is an important factor for
    business success.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 客户流失建模是一种预测建模方法，用于识别可能停止使用公司产品或服务的客户，也称为*流失*。客户流失建模通常用于电信、金融服务和电子商务等行业，这些行业中客户保持是商业成功的重要因素。
- en: Churn modeling typically involves building a predictive model using ML or other
    statistical techniques to identify the factors that are most likely to contribute
    to customer churn. The model is trained on data covering past customers, including
    information about their demographics, usage patterns, and churn status (that is,
    whether they churned or not). The model is then used to make predictions about
    the likelihood of future customers churning based on their specific characteristics
    and behavior.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 客户流失建模通常包括使用机器学习或其他统计技术构建预测模型，以识别最可能导致客户流失的因素。该模型在涵盖过去客户的数据显示进行训练，其中包括关于客户的基本信息、使用模式和流失状态（即，是否流失）。然后，该模型将用于根据客户的特定特征和行为预测未来客户流失的可能性。
- en: It can be used to identify high-risk customers who are likely to churn so that
    the company can take proactive measures with those customers to retain their customers,
    such as offering discounts and incentives to encourage them to continue using
    the company’s products or services. It can also be used to understand the factors
    that contribute to customer churn more broadly so that companies can take steps
    to address these factors and improve general customer retention.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用来识别高风险的可能流失客户，从而公司可以对这些客户采取主动措施来保留他们，例如提供折扣和激励，鼓励他们继续使用公司的产品或服务。它还可以更广泛地帮助理解导致客户流失的因素，这样公司可以采取措施应对这些因素，从而提高客户的总体留存率。
- en: 'We will start by building a model to predict churn. For the purpose of this
    example, we are using the the Churn Modeling dataset available at [https://github.com/sharmaroshan/Churn-Modeling-Dataset](https://github.com/sharmaroshan/Churn-Modeling-Dataset).
    The data consists of 10,000 data points with 14 features. *Figure 9**.5* shows
    the results of some data exploration:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始构建一个预测流失的模型。在这个示例中，我们使用的是来自 [https://github.com/sharmaroshan/Churn-Modeling-Dataset](https://github.com/sharmaroshan/Churn-Modeling-Dataset)
    的流失建模数据集。该数据集包含 10,000 个数据点和 14 个特征。*图 9.5* 显示了一些数据探索的结果：
- en: '![Figure 9.5 – Data exploration on the churn-modeling dataset](img/Figure_9.05_B18681.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 在流失模型数据集上的数据探索](img/Figure_9.05_B18681.jpg)'
- en: Figure 9.5 – Data exploration on the churn-modeling dataset
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 在流失模型数据集上的数据探索
- en: Let’s start building the model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建模型。
- en: Building a model
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型
- en: Now that we have seen the data, let’s start building a model to use it. Since
    our focus is on explaining how to use the Python libraries discussed earlier,
    we will make a simple **Random Forest** classifier using scikit-learn.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经查看了数据，接下来我们开始构建一个模型来使用这些数据。由于我们的重点是解释如何使用之前讨论的 Python 库，因此我们将使用 scikit-learn
    构建一个简单的 **随机森林** 分类器。
- en: 'Here are the steps for it:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实现的步骤：
- en: 'First, we import all the packages needed to build the model, along with the
    packages required to build the explanation (namely, ELI5, LIME, and SHAP):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入构建模型所需的所有包，以及构建解释所需的包（即 ELI5、LIME 和 SHAP）：
- en: '[PRE4]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we download the data and get the features and output from it; then, we
    one-hot encode the categorical variables, `Geography` and `Gender`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们下载数据并从中获取特征和输出；然后，对分类变量 `Geography` 和 `Gender` 进行独热编码：
- en: '[PRE5]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s also drop all irrelevant columns from the DataFrame:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也删除数据框中所有不相关的列：
- en: '[PRE6]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we split the data into training and test datasets:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将数据分成训练集和测试集：
- en: '[PRE7]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s now define the Random Forest classifier and train it on the training
    dataset:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义随机森林分类器并在训练数据集上进行训练：
- en: '[PRE8]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we use the trained model to make predictions on the test dataset. We
    can see that it gives an accuracy of 86% on the test dataset:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用训练好的模型在测试集上进行预测。我们可以看到它在测试集上的准确率为 86%：
- en: '[PRE9]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can also see the ROC curve for the prediction on the test dataset:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以看到在测试集上的 ROC 曲线：
- en: '[PRE10]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is how the ROC curve appears:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ROC 曲线的显示方式：
- en: '![Figure 9.6 – ROC curve](img/Figure_9.06_B18681.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – ROC 曲线](img/Figure_9.06_B18681.jpg)'
- en: Figure 9.6 – ROC curve
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – ROC 曲线
- en: The area under the ROC curve is ~0.87, which is conventionally considered a
    mark of a fairly good classifier.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线下的面积大约为 0.87，通常认为这是一个相当好的分类器的标志。
- en: 'Since we used a Random Forest classifier, let’s explore the feature importance
    based on the default **Mean Decrease in Impurity** (**MDI**) criterion. The following
    is the code to obtain this. To make it easy to understand, we plot the feature
    importance as a bar chart:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们使用的是随机森林分类器，接下来让我们基于默认的 **平均减小不纯度**（**MDI**）标准来探索特征重要性。以下是获得该信息的代码。为了方便理解，我们将特征重要性以条形图的形式进行展示：
- en: '[PRE11]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is how the bar chart appears:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是条形图的显示方式：
- en: '![Figure 9.7 – Feature importance using scikit-learn’s MDI criteria](img/Figure_9.07_B18681.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 使用 scikit-learn 的 MDI 标准的特征重要性](img/Figure_9.07_B18681.jpg)'
- en: Figure 9.7 – Feature importance using scikit-learn’s MDI criteria
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 使用 scikit-learn 的 MDI 标准的特征重要性
- en: In the preceding figure, we can see that the top five features responsible for
    whether or not a given customer will stay or churn are their age, their estimated
    salary, the balance they have, their credit score, and the number of products
    they’ve bought.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到，影响客户是否留存或流失的五个最重要特征分别是他们的年龄、估计薪资、账户余额、信用评分以及他们购买的产品数量。
- en: Next, we use ELI5 to understand the classifier.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 ELI5 来理解分类器。
- en: Using ELI5 to understand classifier models
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ELI5来理解分类模型
- en: 'We use the `PermutationImportance` method of ELI5 to explain the classifier
    model we have built. `PermutationImportance` is a feature importance measure implemented
    in ELI5 that can be used to get an estimate of feature importance for any `PermutationImportance`
    is to randomly permute the values of a single feature, and then measure the impact
    of the permutation on the chosen model’s performance. The more the model’s performance
    is affected by the permutation, the more important the feature is considered to
    be. We pass the trained model and dataset as input to the `PermutationImportance`
    class defined in ELI5\. Let’s begin with this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用ELI5的`PermutationImportance`方法来解释我们构建的分类模型。`PermutationImportance`是ELI5中实现的一个特征重要性度量方法，可以用来估算任何特征的重要性。`PermutationImportance`方法通过随机排列单个特征的值，然后测量排列对所选模型性能的影响。模型的性能受到排列影响越大，特征的重要性就越高。我们将训练好的模型和数据集作为输入传递给ELI5中定义的`PermutationImportance`类。让我们从这个开始：
- en: 'In the following code, we use the area under the ROC curve as a measure to
    evaluate the importance of different features in the trained model:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用ROC曲线下面积作为度量来评估训练模型中不同特征的重要性：
- en: '[PRE12]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s calculate the feature importance using the training dataset:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用训练数据集计算特征重要性：
- en: '[PRE13]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now get the permutation importance of each feature using the `explain_weights`
    or `show_weights` function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用`explain_weights`或`show_weights`函数来获取每个特征的排列重要性：
- en: '[PRE14]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `imp_df` DataFrame contains the estimator parameters. Again, for ease of
    understanding, it would be good to reorder them and plot the results as a bar
    chart. Here, we have the relevant code followed by the bar chart:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`imp_df`数据框包含估算器参数。同样，为了便于理解，最好重新排序它们并将结果绘制成条形图。这里是相关代码和条形图：'
- en: '[PRE15]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the bar chart:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是条形图：
- en: '![Figure 9.8 – Feature importance using ELI5 PermutationImportance with AUC
    as the score](img/Figure_9.08_B18681.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 使用ELI5 PermutationImportance并以AUC作为评分计算特征重要性](img/Figure_9.08_B18681.jpg)'
- en: Figure 9.8 – Feature importance using ELI5 PermutationImportance with AUC as
    the score
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 使用ELI5 PermutationImportance并以AUC作为评分计算特征重要性
- en: According to this graph, age is the most important factor, followed by the number
    of products bought, the balance, and whether the person is an active member or
    not.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 根据此图，年龄是最重要的因素，其次是购买的产品数量、余额以及是否为活跃会员。
- en: Let’s now try and explain the model’s prediction using LIME.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用LIME来解释模型的预测结果。
- en: Hands-on with LIME
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践LIME
- en: You can use LIME for any type of dataset, whether it’s tabular, textual, or
    image-based. In this section, we will introduce the use of LIME to understand
    the predictions made by our Random Forest classifier for churn modeling.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将LIME应用于任何类型的数据集，无论是表格、文本还是图像数据。在本节中，我们将介绍如何使用LIME来理解我们的随机森林分类器在客户流失建模中的预测。
- en: 'Here, we have tabular data. By using LIME, we can gain a better understanding
    of how our model is making decisions and identify the factors that are most important
    in predicting churn:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有表格数据。通过使用LIME，我们可以更好地理解模型如何做出决策，并识别出在预测客户流失时最重要的因素：
- en: 'Since it is tabular data, we will use the `LimeTabularExplainer` class defined
    in `lime_tabular`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是表格数据，我们将使用`lime_tabular`中定义的`LimeTabularExplainer`类：
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Ideally, we should not one-hot encode the categorical features, as when LIME
    tries to make its predictions, it might give nonsensical input. For more information,
    please see the thread at [https://github.com/marcotcr/lime/issues/323](https://github.com/marcotcr/lime/issues/323).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们不应该对分类特征进行独热编码，因为当LIME尝试做出预测时，它可能会给出无意义的输入。更多信息，请参见[https://github.com/marcotcr/lime/issues/323](https://github.com/marcotcr/lime/issues/323)的讨论。
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now use LIME to explain the prediction for an instance. We chose the
    following instance:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用LIME来解释一个实例的预测结果。我们选择了以下实例：
- en: '[PRE17]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we use LIME’s `explain_instance` function to outline its interpretation
    of the input:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用LIME的`explain_instance`函数来概述其对输入的解释：
- en: '[PRE18]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is how the output appears:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 9.9 – Feature importance using the LIME explainer for the X_test.iloc[3]
    instance](img/Figure_9.09_B18681.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 使用LIME解释器解释X_test.iloc[3]实例的特征重要性](img/Figure_9.09_B18681.jpg)'
- en: Figure 9.9 – Feature importance using the LIME explainer for the X_test.iloc[3]
    instance
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 使用LIME解释器解释X_test.iloc[3]实例的特征重要性
- en: The LIME explainer tells us that for the given customer, the probability of
    churn is negligible (0.01). Further, in the middle of the preceding results, we
    can see the factors that contribute to each class along with the nature of their
    contribution. Again, we find that age has the largest impact on the prediction.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 解释器告诉我们，对于给定的客户，流失的概率微乎其微（0.01）。此外，在前面的结果中，我们可以看到各个因素对每个类别的贡献以及它们的贡献性质。再次发现，年龄对预测的影响最大。
- en: 'We can see that all tools are pointing in the same direction: `Age` is an important
    feature for churn modeling. Let’s see what SHAP says.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到所有工具都指向了同一个方向：`年龄` 是流失建模的重要特征。让我们看看 SHAP 的解释。
- en: SHAP in action
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP 实践
- en: 'In this section, we will show you how to use SHAP to explain the predictions
    made by our churn-modeling model. SHAP is a powerful XAI technique that allows
    us to understand the relative importance of each feature in an input data point
    to the prediction made by an ML model. Using SHAP, we can gain a better understanding
    of how our model is making decisions and identify the factors that are most important
    when predicting churn:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何使用 SHAP 来解释我们流失预测模型的预测结果。SHAP 是一种强大的可解释 AI（XAI）技术，可以帮助我们理解输入数据点中的每个特征对机器学习模型预测的相对重要性。通过使用
    SHAP，我们可以更好地理解模型如何做出决策，并识别出在预测流失时最重要的因素：
- en: 'We will use the SHAP `TreeExplainer` class, which is specifically designed
    for use with tree-based models. The `TreeExplainer` class works by traversing
    the decision tree of the model and calculating the contribution of each feature
    to the prediction made at each tree node. The contributions are then aggregated
    to generate a global explanation for the prediction made by the model:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 SHAP 的 `TreeExplainer` 类，该类专门设计用于树模型。`TreeExplainer` 类通过遍历模型的决策树并计算每个特征对每个树节点的预测贡献，从而工作。然后将这些贡献聚合，以生成模型预测的全局解释：
- en: '[PRE19]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s now use it on the same instance that we used for LIME, `X_test[3]`:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将对与 LIME 使用的相同实例 `X_test[3]` 进行操作：
- en: '[PRE20]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will generate a SHAP force plot for the given instance. The force plot
    visually explains the factors that contributed to the prediction of this specific
    instance. The relative importance of each feature in the input data is shown,
    along with how each feature influenced the prediction. The first argument, `explainer.expected_value[0]`,
    says that the expected value for the model for this instance is `0` – that is,
    it belongs to the 0 class, meaning no churn. The second argument, `shap_values[0][3,:]`,
    provides an array of SHAP values for our given instance. The SHAP values represent
    the contributions of each feature to the prediction made by the model for the
    0 class. Here, you can see the plot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个给定实例的 SHAP 力量图。该力量图直观地解释了各个因素对该特定实例预测的贡献。显示了输入数据中每个特征的相对重要性，以及每个特征如何影响预测。第一个参数
    `explainer.expected_value[0]` 表示该实例的模型期望值为 `0`——也就是说，它属于 0 类，即没有流失。第二个参数 `shap_values[0][3,:]`
    提供了我们给定实例的 SHAP 值数组。SHAP 值表示每个特征对模型为 0 类做出的预测的贡献。在这里，你可以看到这个图表：
- en: '![Figure 9.10 – SHAP force plot for the X_test.iloc[3] instance](img/Figure_9.10_B18681.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – SHAP 力量图用于 X_test.iloc[3] 实例](img/Figure_9.10_B18681.jpg)'
- en: Figure 9.10 – SHAP force plot for the X_test.iloc[3] instance
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – SHAP 力量图用于 X_test.iloc[3] 实例
- en: As we can see, the SHAP force plot shows a linear axis. This axis represents
    the model’s output. You should also see a set of horizontal bars that represent
    the features in the input data. The plot also includes a baseline value, which
    is the expected value of the model’s output for the class or label being explained.
    The length of each horizontal bar represents the relative importance of the corresponding
    feature to the prediction. A longer bar indicates greater importance, while a
    shorter bar indicates lower importance. The color of each bar indicates how the
    value of the corresponding feature influenced the prediction. A blue bar indicates
    that a higher value for the feature led to a higher prediction, while a red bar
    indicates that a higher value for the feature led to a lower prediction.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，SHAP力图展示了一个线性坐标轴。这个坐标轴表示模型的输出。你还应该看到一组水平条形图，它们表示输入数据中的特征。该图还包括一个基准值，它是模型在为特定类别或标签进行解释时的预期输出值。每个水平条形图的长度表示相应特征对预测的相对重要性。较长的条形图表示更高的重要性，而较短的条形图表示较低的重要性。每个条形图的颜色表示该特征值如何影响预测结果。蓝色条形图表示该特征的较高值导致较高的预测值，而红色条形图表示该特征的较高值导致较低的预测值。
- en: 'We can also use dependence plots with SHAP to get an understanding of the relationship
    between a specific feature in an input dataset and the predictions made by the
    model. The following command produces a plot showing the effect of the `Age` feature
    on the prediction for the 0 class:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用SHAP的依赖图来了解输入数据集中特定特征与模型预测之间的关系。以下命令生成一个图，展示`Age`特征对0类别预测的影响：
- en: '[PRE21]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the plot for it:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图示：
- en: '![Figure 9.11 – SHAP dependence plot for the Age feature](img/Figure_9.11_B18681.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – SHAP依赖图（Age特征）](img/Figure_9.11_B18681.jpg)'
- en: Figure 9.11 – SHAP dependence plot for the Age feature
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – SHAP依赖图（Age特征）
- en: 'The summary plot lets us visualize the feature importance of all the input
    features. Here is the code for it:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结图使我们能够可视化所有输入特征的重要性。以下是相关代码：
- en: '[PRE22]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And the plot is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图示如下：
- en: '![Figure 9.12 – SHAP summary plot](img/Figure_9.12_B18681.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – SHAP总结图](img/Figure_9.12_B18681.jpg)'
- en: Figure 9.12 – SHAP summary plot
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – SHAP总结图
- en: The SHAP summary plot consists of a vertical axis that represents the model’s
    output (often labeled as `f(x)`) and a set of horizontal violin plots that represent
    the distribution of the SHAP values for each feature. The violin plots show the
    distribution of the SHAP values for each feature across all instances in the dataset
    and can be used to understand how the value of each feature influenced the predictions
    made by the model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP总结图由一个垂直坐标轴组成，表示模型的输出（通常标记为`f(x)`），以及一组水平的小提琴图，表示每个特征的SHAP值分布。小提琴图展示了每个特征在数据集中所有实例中的SHAP值分布，可以用来理解每个特征的值是如何影响模型的预测结果的。
- en: The libraries that we’ve examined up to now offer us many insights into our
    model’s predictions based on the importance of each feature involved. Let’s move
    on to libraries that provide cause-effect analysis.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所审视的库为我们提供了许多基于各特征重要性的模型预测见解。接下来，我们将探索一些提供因果关系分析的库。
- en: CausalNex
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CausalNex
- en: 'CausalNex, an open source Python library, allows us to develop models that
    help to infer causation rather than observing correlation. The `what if` library
    offered by CausalNex is deployed to test scenarios utilizing **Bayesian networks**
    and develop causal reasoning. Some prominent features of CausalNex are as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: CausalNex是一个开源Python库，允许我们开发有助于推断因果关系的模型，而非仅仅观察相关性。CausalNex提供的`what if`库用于测试场景，利用**贝叶斯网络**进行因果推理。一些CausalNex的突出特点如下：
- en: '**Simplifying causality understanding in Bayesian networks via visualization**:
    One of the main features of CausalNex is its ability to simplify the understanding
    of causality in Bayesian networks through visualizations. The library provides
    a range of tools for visualizing Bayesian networks, including network plots, influence
    plots, and decision plots, which allow users to see how different variables are
    connected and how they influence each other.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过可视化简化贝叶斯网络中的因果关系理解**：CausalNex的主要特点之一是它通过可视化简化了贝叶斯网络中因果关系的理解。该库提供了一系列可视化贝叶斯网络的工具，包括网络图、影响图和决策图，允许用户查看不同变量之间的连接及其相互影响。'
- en: '**Understanding conditional dependencies between variables**: CausalNex also
    provides tools for understanding conditional dependencies between variables. The
    library includes state-of-the-art structure learning methods, which are algorithms
    that can automatically learn the structure of a Bayesian network from data. These
    methods allow users to identify the relationships between variables and understand
    how they are influenced by other variables in the network.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解变量之间的条件依赖关系**：CausalNex还提供了用于理解变量之间条件依赖关系的工具。该库包括最先进的结构学习方法，这些算法可以从数据中自动学习贝叶斯网络的结构。这些方法使用户能够识别变量之间的关系，并理解它们如何受到网络中其他变量的影响。'
- en: '**Augmenting domain knowledge**: CausalNex also provides tools for augmenting
    domain knowledge, which refers to the specific knowledge and expertise that users
    bring to the modeling process. The library allows users to incorporate their domain
    knowledge into the structure of the Bayesian networks, which can help improve
    the accuracy and reliability of the model.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强领域知识**：CausalNex还提供了增强领域知识的工具，领域知识指的是用户在建模过程中所带入的特定知识和专业技能。该库允许用户将其领域知识融入贝叶斯网络的结构中，从而帮助提高模型的准确性和可靠性。'
- en: '**Evaluating the quality of the model**: CausalNex includes tools for evaluating
    the quality of the model, such as statistical checks and model selection methods.
    These tools allow users to ensure that the model they have built is accurate and
    appropriate for their specific problem.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估模型质量**：CausalNex包括用于评估模型质量的工具，如统计检查和模型选择方法。这些工具使用户能够确保他们构建的模型是准确的，并且适用于他们特定的问题。'
- en: '**Building predictive models based on structural relationships**: The library
    also includes tools for building predictive models based on the structural relationships
    in the Bayesian networks, which can be useful for making predictions about future
    outcomes or for testing scenarios in a “what if” manner.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于结构关系构建预测模型**：该库还包括基于贝叶斯网络中结构关系构建预测模型的工具，这对于预测未来结果或以“假设如果”的方式测试场景非常有用。'
- en: 'In the following figure, you can see a graph showing the causal relationships
    between a student’s performance and the factors that might play a role in it:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到一个显示学生表现与可能影响因素之间因果关系的图：
- en: '![Figure 9.13 – Subgraph showing causal relationships between different factors
    affecting a student’s performance](img/Figure_9.13_B18681.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – 子图显示影响学生表现的不同因素之间的因果关系](img/Figure_9.13_B18681.jpg)'
- en: Figure 9.13 – Subgraph showing causal relationships between different factors
    affecting a student’s performance
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 子图显示影响学生表现的不同因素之间的因果关系
- en: This result is obtained from the introductory tutorials on CausalNex at [https://causalnex.readthedocs.io/en/latest/03_tutorial/01_first_tutorial.html](https://causalnex.readthedocs.io/en/latest/03_tutorial/01_first_tutorial.html).
    We can see that excessive internet usage can result in an increase in absence
    from school. Similarly, if the student increases the time spent studying, their
    grades (**G1**) go up.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 该结果来源于CausalNex的入门教程，[https://causalnex.readthedocs.io/en/latest/03_tutorial/01_first_tutorial.html](https://causalnex.readthedocs.io/en/latest/03_tutorial/01_first_tutorial.html)。我们可以看到，过度的互联网使用会导致学生缺课增加。同样地，如果学生增加学习时间，他们的成绩（**G1**）会提高。
- en: Overall, CausalNex is a powerful toolkit for causal reasoning using Bayesian
    networks. It offers a range of features for simplifying the understanding of causality,
    understanding conditional dependencies between variables, augmenting domain knowledge,
    evaluating the quality of the model, and building predictive models based on structural
    relationships.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，CausalNex是一个强大的贝叶斯网络因果推理工具包。它提供了一系列功能，用于简化因果关系的理解，理解变量之间的条件依赖关系，增强领域知识，评估模型质量，以及基于结构关系构建预测模型。
- en: Let’s now explore the next Python library for causal inference, the DoWhy library.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索下一个用于因果推断的Python库——DoWhy库。
- en: DoWhy for causal inference
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DoWhy用于因果推断
- en: DoWhy is a Python library for causal inference and analysis. It is designed
    to support interoperability with other causal estimation libraries, such as Causal
    ML and EconML, allowing users to easily combine different methods and approaches
    in their analysis.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: DoWhy是一个用于因果推断和分析的Python库。它旨在支持与其他因果估计库的互操作性，如Causal ML和EconML，使用户能够轻松地在分析中结合不同的方法和途径。
- en: One of the main features of DoWhy is its focus on robustness checks and sensitivity
    analysis. The library includes a range of methods for evaluating the robustness
    of causal estimates, such as bootstrapping and placebo tests. These methods help
    users to ensure that their estimates are reliable and not subject to bias or confounding
    factors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: DoWhy 的主要特点之一是专注于稳健性检验和敏感性分析。该库包括一系列方法来评估因果估计的稳健性，如自助法和安慰剂测试。这些方法帮助用户确保他们的估计是可靠的，不受偏差或混杂因素的影响。
- en: In addition to robustness checks, DoWhy also offers an API that follows the
    common steps involved in causal analysis. These steps include creating a causal
    model, identifying the effect of interest, estimating the effect using statistical
    estimators, and validating the estimate through sensitivity analysis and robustness
    checks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 除了稳健性检验外，DoWhy 还提供了一个 API，涵盖因果分析中的常见步骤。这些步骤包括创建因果模型、确定感兴趣的效应、使用统计估计方法估计效应，并通过敏感性分析和稳健性检验验证估计结果。
- en: To create a causal model, users can use DoWhy’s causal graph and structural
    assumption tools to represent the relationships between variables and specify
    the underlying assumptions of the model. Once the model has been created, users
    can use DoWhy’s identification tools to determine whether the expected effect
    is valid and then use the library’s estimation tools to estimate the effect. Finally,
    users can use DoWhy’s validation tools to make sure that the estimate is accurate
    and reliable using sensitivity analysis and robustness checks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建因果模型，用户可以使用 DoWhy 的因果图和结构假设工具来表示变量之间的关系并指定模型的基本假设。一旦模型创建完成，用户可以使用 DoWhy 的识别工具来确定预期效应是否有效，然后使用该库的估计工具来估算效应。最后，用户可以使用
    DoWhy 的验证工具，通过敏感性分析和稳健性检验确保估计结果的准确性和可靠性。
- en: 'This library makes three main contributions to the field of causal inference:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该库为因果推断领域做出了三大主要贡献：
- en: '**Model the problem as a causal graph**: DoWhy allows users to represent their
    problems as causal graphs, which are graphical representations of the causal relationships
    between variables. By modeling the problem as a causal graph, users can make all
    of their assumptions explicit, which helps to ensure that the assumptions are
    transparent and can be easily understood.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将问题建模为因果图**：DoWhy 允许用户将问题表示为因果图，这些图是变量之间因果关系的图形表示。通过将问题建模为因果图，用户可以明确表达他们的所有假设，从而帮助确保这些假设是透明的并且易于理解。'
- en: '**Unified interface**: DoWhy combines the two major frameworks of graphical
    models and potential outcomes, providing a unified interface for many different
    causal inference methods. This allows users to easily combine different methods
    and approaches in their analysis, and to choose the method that is most appropriate
    for their specific problem.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一接口**：DoWhy结合了图模型和潜在结果这两个主要框架，提供了一个统一的接口，支持多种因果推断方法。这使得用户能够轻松地将不同的方法和途径结合到分析中，并选择最适合其特定问题的方法。'
- en: '**Automatic tests for the validity of assumptions**: DoWhy includes a range
    of tools for testing the validity of assumptions, such as robustness checks and
    sensitivity analysis. By automatically testing for the validity of assumptions,
    users can ensure that their estimates are reliable and not subject to bias or
    confounding factors.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动测试假设有效性**：DoWhy 包括一系列测试假设有效性的工具，如稳健性检验和敏感性分析。通过自动测试假设的有效性，用户可以确保他们的估计是可靠的，不会受到偏差或混杂因素的影响。'
- en: 'DoWhy breaks down the process of causal inference into four steps: **modeling**,
    **identification**, **estimation**, and **refutation**. During the modeling step,
    users create a causal graph to encode their assumptions. In the identification
    step, users formulate what they want to estimate. During the estimation step,
    users compute the estimate using statistical estimators. Finally, in the refutation
    step, users validate the assumptions through sensitivity analysis and robustness
    checks.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: DoWhy 将因果推断过程分为四个步骤：**建模**、**识别**、**估计**和**反驳**。在建模步骤中，用户创建因果图以编码他们的假设。在识别步骤中，用户制定他们希望估算的内容。在估计步骤中，用户使用统计估计方法计算效应估计值。最后，在反驳步骤中，用户通过敏感性分析和稳健性检验验证假设。
- en: Let’s get our hands dirty and play a little with DoWhy.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们动手试试，玩一玩 DoWhy。
- en: DoWhy in action
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DoWhy 的应用
- en: 'We will use a simple synthetic dataset to demonstrate the features of the DoWhy
    library:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的合成数据集来演示 DoWhy 库的功能。
- en: 'Start by importing the DoWhy library and the components we will be using, as
    follows:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入 DoWhy 库以及我们将使用的组件，如下所示：
- en: '[PRE23]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, using DoWhy’s `linear_dataset` function, we generate a synthetic dataset
    such that the relationship between a treatment and the outcome of interest is
    linear, also called the linear treatment effect (in our case, we chose `beta=10`,
    so the true treatment effect is `10`). This creates a linear model with the specified
    treatment effect and generates a set of synthetic data points that conform to
    this model. The resulting DataFrame contains a column representing the treatment,
    a column representing the outcome, and a set of columns representing the common
    causes and instruments. Additionally, the `num_effect_modifiers` parameter specifies
    the number of effect modifiers or variables that modify the treatment effect.
    The `num_samples` parameter specifies the number of samples in the dataset, and
    the `treatment_is_binary` parameter indicates whether the treatment is binary
    or continuous. If the treatment is binary, then it can take only two values, effective
    or not and on or off; if the treatment is continuous, it can take more than two
    values. The `stddev_treatment_noise` parameter specifies the standard deviation
    of the treatment noise, which is added to the treatment effect to create the synthetic
    data. The generated data is a `dictdata type`. The DataFrame (`df`) is then extracted
    from it:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 DoWhy 的 `linear_dataset` 函数，我们生成一个合成数据集，使得干预和感兴趣结果之间的关系是线性的，这也称为线性处理效应（在我们的例子中，我们选择了
    `beta=10`，因此真实的干预效应为 `10`）。这将创建一个具有指定干预效应的线性模型，并生成符合此模型的一组合成数据点。生成的 DataFrame
    包含表示干预的列、表示结果的列，以及一组表示常见原因和工具变量的列。此外，`num_effect_modifiers` 参数指定了影响干预效应的效应修饰符或变量的数量，`num_samples`
    参数指定数据集中样本的数量，`treatment_is_binary` 参数指示干预是二元的还是连续的。如果干预是二元的，则它只能取两个值：有效或无效、开或关；如果干预是连续的，则它可以取多个值。`stddev_treatment_noise`
    参数指定了干预噪声的标准差，这个噪声会添加到干预效应中，生成合成数据。生成的数据是 `dictdata` 类型。然后从中提取 DataFrame (`df`)：
- en: '[PRE24]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `df` DataFrame will have 11 columns, with column `v0` as the treatment
    name, the outcome being in column `y`, `W0-W4` representing the five common causes,
    `Z0` and `Z1` for the two instruments, and `X0` and `X1` for the two effect modifiers.
    Now, we use the `CausalModel` class to create a causal model for our synthetic
    data. The `CausalModel` class uses the following parameters:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df` DataFrame 将有 11 列，其中 `v0` 列是干预名称，结果在 `y` 列，`W0-W4` 代表五个常见原因，`Z0` 和 `Z1`
    是两个工具变量，`X0` 和 `X1` 是两个效应修饰符。现在，我们使用 `CausalModel` 类为我们的合成数据创建因果模型。`CausalModel`
    类使用以下参数：'
- en: '`data`: A pandas DataFrame that holds the data.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`data`：一个 pandas DataFrame，包含数据。'
- en: '`treatment`: The column in the DataFrame to be treated as the `treatment` variable.
    It represents the intervention or action that is being taken to affect the outcome.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`treatment`：DataFrame 中将被视为 `treatment` 变量的列。它代表的是正在采取的干预或措施，目的是影响结果。'
- en: '`outcome`: The column in the DataFrame to be treated as the outcome variable.'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`outcome`：DataFrame 中将被视为结果变量的列。'
- en: '`common_causes`: The columns to be treated as common causes. These represent
    the variables that can affect both the treatment and outcome and are also called
    `instruments`: This represents the instruments. These are the variables that are
    used to infer causality.'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`common_causes`：将被视为常见原因的列。这些变量既能影响干预，也能影响结果，通常也称为 `instruments`（工具变量）：这些是用来推断因果关系的变量。'
- en: 'This creates a graphical model that represents the structure of the causal
    relationships in the data. The `graph` parameter specifies the causal graph, which
    encodes the structure of the causal relationships between the variables in the
    model. Here, we are using the **Graph Modeling Language** (**GML**) file format
    of our causal graph:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个图形模型，表示数据中因果关系的结构。`graph` 参数指定了因果图，它编码了模型中变量之间因果关系的结构。这里，我们使用的是**图形建模语言**（**GML**）文件格式的因果图：
- en: '[PRE25]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s view the `model.view_model()` graph:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 `model.view_model()` 图形：
- en: '![Figure 9.14 – Causal graph for the dataset](img/Figure_9.14_B18681.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – 数据集的因果图](img/Figure_9.14_B18681.jpg)'
- en: Figure 9.14 – Causal graph for the dataset
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 数据集的因果图
- en: 'Next, we see the target variables that are to be estimated and the assumptions
    needed to identify them. The result tells us the estimands (causal estimands as
    estimated by the model, which describe the causal effect of interest), their names,
    the expression (a mathematical expression of estimands in terms of variables of
    the model), and the assumptions needed:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们查看目标变量和识别它们所需的假设。结果告诉我们估计量（模型估计的因果估计量，描述了感兴趣的因果效应）、它们的名称、表达式（用模型变量表示的估计量的数学表达式）以及所需的假设：
- en: '[PRE26]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s now use the `estimate_effect` method to identify the estimand to compute
    an estimate of the treatment effect. We can see that the mean value of the estimate
    is `9.162`, which represents the average effect of the treatment on the outcome
    across all individuals in the population:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用`estimate_effect`方法来识别估计量，以计算治疗效应的估计值。我们可以看到估计的均值是`9.162`，这表示治疗对结果的平均效应，在整个群体中的所有个体上都适用：
- en: '[PRE27]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You can employ the `refute_estimate` function to test the robustness of an estimate
    of the treatment effect to various types of perturbations in the data or assumptions
    of the model.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`refute_estimate`函数来测试治疗效应估计在数据或模型假设的各种扰动下的稳健性。
- en: Overall, DoWhy is a powerful tool for causal inference and analysis. It offers
    a range of features for creating causal models, identifying effects, estimating
    effects, and validating estimates, making it a valuable resource for researchers
    and analysts working in the field of causal inference.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，DoWhy是一个强大的因果推断和分析工具。它提供了用于创建因果模型、识别效应、估计效应和验证估计的多种功能，是因果推断领域的研究人员和分析师的宝贵资源。
- en: AI Explainability 360 for interpreting models
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于解释模型的AI Explainability 360
- en: AI Explainability 360 is an open source toolkit that offers a variety of techniques
    for explaining and interpreting ML models. It supports both model-specific and
    model-agnostic approaches, as well as local and global explanations, providing
    users with a range of options for understanding their models. In addition, the
    toolkit is built on top of popular ML libraries, including scikit-learn and XGBoost,
    making it easy to integrate into existing pipelines.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: AI Explainability 360是一个开源工具包，提供了多种用于解释和理解机器学习模型的技术。它支持模型特定和模型无关的方法，以及局部和全局解释，为用户提供多种理解模型的选项。此外，该工具包构建于流行的机器学习库之上，包括scikit-learn和XGBoost，方便与现有管道集成。
- en: 'Some of the features of AI Explainability 360 include the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: AI Explainability 360的一些特点包括：
- en: '**Model-agnostic and model-specific explainability techniques**: AI Explainability
    360 provides both model-agnostic and model-specific explainability techniques
    that can be used to understand and explain the predictions of any AI model. Model-agnostic
    techniques, such as LIME and SHAP, can be used to explain the predictions of any
    model, while model-specific techniques, such as feature importance and partial
    dependence plots, are tailored to specific types of models.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型无关和模型特定的可解释性技术**：AI Explainability 360提供了模型无关和模型特定的可解释性技术，可以用来理解和解释任何AI模型的预测。模型无关技术，如LIME和SHAP，可以用来解释任何模型的预测，而模型特定技术，如特征重要性和部分依赖图，针对特定类型的模型进行定制。'
- en: '**Local and global explanations**: AI Explainability 360 provides both local
    and global explanations of AI models. Local explanations focus on understanding
    specific predictions made by the model for individual instances, while global
    explanations focus on understanding the overall behavior of the model.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部和全局解释**：AI Explainability 360提供了AI模型的局部和全局解释。局部解释侧重于理解模型对单个实例所做的特定预测，而全局解释则侧重于理解模型的整体行为。'
- en: '**Support for multiple types of data**: AI Explainability 360 supports explanations
    for a variety of data types, including tabular, text, image, and time series data.
    It provides a range of explainability techniques that are tailored to the specific
    characteristics of each data type.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持多种数据类型**：AI Explainability 360支持多种数据类型的解释，包括表格数据、文本、图像和时间序列数据。它提供了一系列针对每种数据类型特征的可解释性技术。'
- en: '**Integration with popular AI frameworks**: AI Explainability 360 is designed
    to be easily integrated with popular AI frameworks, including TensorFlow, PyTorch,
    and scikit-learn, making it easy to use in real-world applications.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与流行的AI框架集成**：AI Explainability 360被设计为可以轻松与流行的AI框架集成，包括TensorFlow、PyTorch和scikit-learn，方便在实际应用中使用。'
- en: '**Extensive documentation and examples**: AI Explainability 360 comes with
    extensive documentation and examples to help users get started with explainability
    in their own projects.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的文档和示例**：AI 可解释性 360 配备了丰富的文档和示例，帮助用户在自己的项目中开始探索可解释性。'
- en: Overall, AI Explainability 360 is a powerful toolkit for understanding and explaining
    the predictions made by AI models, and for building transparent, trustworthy,
    and fair AI systems.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，AI 可解释性 360 是一个强大的工具包，用于理解和解释人工智能模型所做的预测，并构建透明、可信、公平的人工智能系统。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The future of AI lies in enabling people to collaborate with machines to solve
    complex problems. Like any efficient collaboration, this requires good communication,
    trust, clarity, and understanding. XAI aims to address such challenges by combining
    the best of symbolic AI and traditional ML.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的未来在于使人们能够与机器协作解决复杂问题。像任何高效的合作一样，这需要良好的沟通、信任、清晰度和理解。XAI 旨在通过结合符号 AI 和传统机器学习的最佳特点来解决这些挑战。
- en: In this chapter, we explored a variety of XAI techniques that can be used to
    explain and interpret ML models. These techniques can be classified based on their
    scope (local or global) and their model type (model-specific or model-agnostic).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了多种 XAI 技术，这些技术可以用于解释和解读机器学习模型。这些技术可以根据其范围（局部或全局）和模型类型（特定模型或无关模型）进行分类。
- en: We covered various Python libraries that provide XAI features and explained
    how to use ELI5, LIME, and SHAP to explore feature importance in model prediction.
    LIME can provide instance-based explanations for predictions made by any classifier.
    LIME approximates the classifier locally with an interpretable model and generates
    a list of features that contribute to the prediction in a given instance. SHAP
    uses Shapley values to explain the contribution of each feature to a prediction,
    supports both local and global explanations, and can be used with a variety of
    model types.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了几种提供 XAI 功能的 Python 库，并解释了如何使用 ELI5、LIME 和 SHAP 探索模型预测中的特征重要性。LIME 可以为任何分类器做出基于实例的解释。LIME
    使用可解释模型局部地近似分类器，并生成一个特征列表，列出在给定实例中对预测有贡献的特征。SHAP 使用 Shapley 值来解释每个特征对预测的贡献，支持局部和全局的解释，并且可以与各种模型类型一起使用。
- en: DoWhy is another library for causal inference and analysis. It offers an API
    for the common steps in causal analysis, including modeling, identification, estimation,
    and refutation. Finally, we introduced AI Explainability 360, a comprehensive
    open source toolkit for explaining and interpreting ML models. It supports both
    model-specific and model-agnostic explanations, as well as local and global explanations.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: DoWhy 是另一个因果推断和分析库。它提供了一个 API，涵盖因果分析中的常见步骤，包括建模、识别、估计和反驳。最后，我们介绍了 AI 可解释性 360，这是一个综合性的开源工具包，用于解释和解读机器学习模型。它支持特定模型和无关模型的解释，以及局部和全局的解释。
- en: In conclusion, there are a variety of tools and libraries available for explaining
    and interpreting ML models. These tools (such as ELI5, LIME, SHAP, CausalNex,
    DoWhy, and AI Explainability 360) offer a range of options for understanding how
    models make their predictions and can be useful for researchers and practitioners
    working in the field of XAI. However, it is important to note that there are still
    limitations and challenges in this field.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，市场上有多种工具和库可用于解释和解读机器学习模型。这些工具（如 ELI5、LIME、SHAP、CausalNex、DoWhy 和 AI 可解释性
    360）提供了多种选项，用于理解模型如何做出预测，并且对于在 XAI 领域工作的研究人员和从业者非常有用。然而，必须注意的是，这一领域仍然存在一些局限性和挑战。
- en: In the next chapter, we will move on to model risk management and explore best
    practices for model governance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨模型风险管理，并探索模型治理的最佳实践。
- en: References
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Black-box vs. white-box: Understanding their advantages and weaknesses from
    a practical point of view*, Loyola-Gonzalez, Octavio. IEEE Access 7 (2019): 154096-154113.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*黑盒与白盒：从实践角度理解它们的优缺点*，Loyola-Gonzalez, Octavio。IEEE Access 7（2019）：154096-154113。'
- en: '*Opportunities and challenges in explainable artificial intelligence (xai):
    A survey*. arXiv preprint arXiv:2006.11371, Das, Arun and Rad Paul. (2020)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释人工智能（XAI）中的机遇与挑战：一项调查*。arXiv 预印本 arXiv:2006.11371，Das, Arun 和 Rad Paul。（2020）'
- en: '*A systematic review of human–computer interaction and explainable artificial
    intelligence in healthcare with artificial intelligence techniques*. IEEE Access
    9 (2021): 153316-153348, Nazar, Mobeen, et al.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人机交互与可解释人工智能在医疗保健中的系统性回顾与人工智能技术*。IEEE Access 9（2021）：153316-153348，Nazar,
    Mobeen 等人。'
- en: '*Why should I trust you? Explaining the predictions of any classifier*. Ribeiro,
    Marco Tulio, Singh Sameer, and Guestrin Carlos. Proceedings of the 22nd ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining. 2016.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么我应该信任你？解释任何分类器的预测*。Ribeiro, Marco Tulio，Singh Sameer 和 Guestrin Carlos。《第22届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集》，2016年。'
- en: '*A unified approach to interpreting model predictions*. Advances in Neural
    Information Processing Systems 30\. Lundberg, Scott M. and Lee Su-In (2017)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统一的模型预测解释方法*。《神经信息处理系统进展》30。Lundberg, Scott M. 和 Lee Su-In（2017）'
- en: '*Algorithmic transparency via quantitative input influence: Theory and experiments
    with learning systems*. 2016 IEEE Symposium on Security and Privacy (SP).Datta,
    Anupam, Sen Shayak, and Zick Yair. IEEE, 2016\.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过定量输入影响实现算法透明性：学习系统的理论与实验*。2016 IEEE 安全与隐私研讨会（SP）。Datta, Anupam，Sen Shayak
    和 Zick Yair。IEEE，2016年。'
- en: '*Explaining prediction models and individual predictions with feature contributions*.
    Knowledge and Information Systems 41.3 (2014): 647-665\. Štrumbelj, Erik and Kononenko
    Igor.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过特征贡献解释预测模型和个体预测*。《知识与信息系统》41.3 (2014)：647-665。Štrumbelj, Erik 和 Kononenko
    Igor。'
- en: '*Bayesian networks*, Pearl, Judea. (2011)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*贝叶斯网络*，Pearl, Judea. (2011)'
- en: '*A tutorial on learning with Bayesian networks*. Innovations in Bayesian Networks
    (2008): 33-82\. Heckerman, David.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于贝叶斯网络的学习教程*。《贝叶斯网络的创新》(2008)：33-82。Heckerman, David。'
- en: '*Probabilistic Graphical Models: Principles And Techniques*. MIT Press, 2009\.
    Koller, Daphne and Friedman Nir.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概率图模型：原理与技术*。MIT Press，2009年。Koller, Daphne 和 Friedman Nir。'
- en: '*Causal inference in statistics: An overview*. *Statistics surveys 3*: 96-146\.
    Pearl, Judea. (2009)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计中的因果推断：概述*。*统计调查 3*：96-146。Pearl, Judea. (2009)'
- en: 'CausalNex docs: [https://causalnex.readthedocs.io/en/latest/](https://causalnex.readthedocs.io/en/latest/)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CausalNex 文档：[https://causalnex.readthedocs.io/en/latest/](https://causalnex.readthedocs.io/en/latest/)
- en: 'DoWhy GitHub repo: [https://github.com/py-why/dowhy](https://github.com/py-why/dowhy)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DoWhy GitHub 仓库：[https://github.com/py-why/dowhy](https://github.com/py-why/dowhy)
- en: '*Introducing AI Explainability* *360*: [https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/](https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介绍 AI 可解释性* *360*：[https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/](https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/)'
