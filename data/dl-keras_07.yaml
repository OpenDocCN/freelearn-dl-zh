- en: Additional Deep Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他深度学习模型
- en: So far, most of the discussion has been focused around different models that
    do classification. These models are trained using object features and their labels
    to predict labels for hitherto unseen objects. The models also had a fairly simple
    architecture, all the ones we have seen so far have a linear pipeline modeled
    by the Keras sequential API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大部分讨论都集中在不同的分类模型上。这些模型使用对象特征和标签进行训练，以预测以前未见过的对象的标签。这些模型的架构也相对简单，迄今为止我们看到的所有模型都是通过
    Keras 的顺序 API 构建的线性管道。
- en: In this chapter, we will focus on more complex architectures where the pipelines
    are not necessarily linear. Keras provides the functional API to deal with these
    sorts of architectures. We will learn how to define our networks using the functional
    API in this chapter. Note that the functional API can be used to build linear
    architectures as well.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点讨论更复杂的架构，其中管道不一定是线性的。Keras 提供了功能性 API 来处理这些类型的架构。在本章中，我们将学习如何使用功能性
    API 定义我们的网络。需要注意的是，功能性 API 也可以用于构建线性架构。
- en: The simplest extension of classification networks are regression networks. The
    two broad subcategories under supervised machine learning are classification and
    regression. Instead of predicting a category, the network now predicts a continuous
    value. You saw an example of a regression network when we discussed stateless
    versus stateful RNNs. Many regression problems can be solved using classification
    models with very little effort. We will see an example of such a network to predict
    atmospheric benzene in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分类网络的最简单扩展是回归网络。在监督学习的两大子类别中，分别是分类和回归。与预测一个类别不同，回归网络现在预测一个连续值。我们在讨论无状态与有状态 RNN
    时，看到过回归网络的一个例子。许多回归问题可以通过分类模型轻松解决。我们将在本章中看到一个回归网络的例子，用于预测大气中的苯。
- en: Yet another class of models deal with learning the structure of the data from
    unlabeled data. These are called **unsupervised** (or more correctly, self-supervised)
    models. They are similar to classification models, but the labels are available
    implicitly within the data. We have already seen examples of this kind of model;
    for example, the CBOW and skip-gram word2vec models are self-supervised models.
    Autoencoders are another example of this type of model. We will learn about autoencoders
    and describe an example that builds compact vector representations of sentences.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一类模型处理从无标签数据中学习数据结构，这些被称为**无监督**（或更准确地说是自监督）模型。它们类似于分类模型，但标签是隐含在数据中的。我们已经看过这类模型的例子，例如，CBOW
    和 skip-gram 的 word2vec 模型就是自监督模型。自编码器是另一种这类模型的例子。在本章中，我们将学习自编码器，并描述一个构建句子紧凑向量表示的例子。
- en: We will then look at how to compose the networks we have seen so far into larger
    computation graphs. These graphs are often built to achieve some custom objective
    that is not achievable by a sequential model alone, and may have multiple inputs
    and outputs and connections to external components. We will see an example of
    composing such a network for question answering.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何将迄今为止看到的网络组合成更大的计算图。这些图通常是为了实现一些顺序模型无法单独完成的自定义目标而构建的，可能有多个输入和输出以及与外部组件的连接。我们将看到一个将网络组合起来用于问答任务的例子。
- en: We then take a detour to look at the Keras backend API, and how we can use this
    API to build custom components to extend Keras' functionality.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将绕道一看 Keras 后端 API，并学习如何使用这个 API 构建自定义组件来扩展 Keras 的功能。
- en: Going back to models for unlabeled data, another class of models that don't
    require labels are generative models. These models are trained using a set of
    existing objects and attempt to learn the distribution these objects come from.
    Once the distribution is learned, we can draw samples from this distribution that
    look like the original training data. We have seen an example of this where we
    trained a character RNN model to generate text similar to *Alice in Wonderland*
    in the previous chapter. The idea is already covered, so we won't cover this particular
    aspect of generative models here. However, we will look at how we can leverage
    the idea of a trained network learning the data distribution to create interesting
    visual effects using a VGG-16 network pre-trained on ImageNet data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 回到无标签数据的模型，另一类不需要标签的模型是生成模型。这些模型使用一组现有对象进行训练，尝试学习这些对象的分布。一旦学到了分布，我们就可以从这个分布中抽取样本，这些样本看起来像原始的训练数据。我们已经看过一个例子，在上一章中，我们训练了一个字符RNN模型来生成类似《爱丽丝梦游仙境》文本的内容。这个想法已经涵盖，所以我们这里不再讨论生成模型的这个部分。不过，我们将探讨如何利用已经训练好的网络学习数据分布的思路，使用在ImageNet数据上预训练的VGG-16网络来创建有趣的视觉效果。
- en: 'To summarize, we will learn the following topics in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们将在本章学习以下主题：
- en: The Keras functional API
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras功能性API
- en: Regression networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归网络
- en: Autoencoders for unsupervised learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于无监督学习的自编码器
- en: Composing complex networks with the functional API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用功能性API组合复杂的网络
- en: Customizing Keras
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制Keras
- en: Generative networks
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成网络
- en: Let's get started.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始吧。
- en: Keras functional API
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras功能性API
- en: 'The Keras functional API defines each layer as a function and provides operators
    to compose these functions into a larger computational graph. A function is some
    sort of transformation with a single input and single output. For example, the
    function *y = f(x)* defines a function *f* with input *x* and output *y*. Let
    us consider the simple sequential model from Keras (for more information refer
    to: [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的功能性API将每一层定义为一个函数，并提供操作符将这些函数组合成一个更大的计算图。一个函数是一种具有单一输入和单一输出的变换。例如，函数 *y
    = f(x)* 定义了一个输入为 *x*，输出为 *y* 的函数 *f*。让我们考虑Keras中的简单顺序模型（更多信息请参见：[https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)）：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, the sequential model represents the network as a linear pipeline,
    or list, of layers. We can also represent the network as the composition of the
    following nested functions. Here *x* is the input tensor of shape *(None, 784)*
    and *y* is the output tensor of *(None, 10)*. Here *None* refers to the as-yet
    undetermined batch size:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，顺序模型将网络表示为一个线性管道，或者说是一个层的列表。我们也可以将网络表示为以下嵌套函数的组合。这里 *x* 是形状为 *(None, 784)*
    的输入张量，*y* 是形状为 *(None, 10)* 的输出张量。这里的 *None* 表示尚未确定的批量大小：
- en: '![](img/func-api-1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/func-api-1.png)'
- en: 'Where:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](img/func-api-2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/func-api-2.png)'
- en: 'The network can be redefined using the Keras functional API as follows. Notice
    how the predictions variable is a composition of the same functions we defined
    in equation form previously:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 网络可以通过以下方式使用Keras功能性API重新定义。注意，`predictions` 变量是我们之前以方程式形式定义的相同函数的组合：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since a model is a composition of layers that are also functions, a model is
    also a function. Therefore, you can treat a trained model as just another layer
    by calling it on an appropriately shaped input tensor. Thus, if you have built
    a model that does something useful like image classification, you can easily extend
    it to work with a sequence of images using Keras''s `TimeDistributed` wrapper:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型是层的组合，而层本身也是函数，所以模型本身也是一个函数。因此，你可以通过对适当形状的输入张量调用模型，将训练好的模型视为另一个层。因此，如果你构建了一个用于图像分类的有用模型，你可以通过Keras的
    `TimeDistributed` 包装器轻松地将其扩展为处理一系列图像：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The functional API can be used to define any network that can be defined using
    the sequential API. In addition, the following types of network can only be defined using
    the functional API:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性API可以用来定义任何可以通过顺序API定义的网络。此外，以下类型的网络只能使用功能性API定义：
- en: Models with multiple inputs and outputs
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有多个输入和输出的模型
- en: Models composed of multiple submodels
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由多个子模型组成的模型
- en: Models that used shared layers
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用共享层的模型
- en: 'Models with multiple inputs and outputs are defined by composing the inputs
    and outputs separately, as shown in the preceding example, and then passing in
    an array of input functions and an array of output functions in the input and
    output parameters of the `Model` constructor:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有多个输入和输出的模型是通过分别组合输入和输出来定义的，如前面的示例所示，然后将输入函数数组和输出函数数组传递到`Model`构造函数的输入和输出参数中：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Models with multiple inputs and outputs also generally consist of multiple subnetworks,
    the results of whose computations are merged into the final result. The merge
    function provides multiple ways to merge intermediate results such as vector addition,
    dot product, and concatenation. We will see examples of merging in our question
    answering example later in this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有多个输入和输出的模型通常也由多个子网络组成，这些子网络的计算结果最终会合并成最终结果。合并函数提供了多种合并中间结果的方法，如向量加法、点积和拼接。稍后我们将在本章的问答示例中看到合并的示例。
- en: Another good use for the functional API are models that use shared layers. Shared
    layers are defined once, and referenced in each pipeline where their weights need
    to be shared.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 功能式API的另一个好用场景是使用共享层的模型。共享层只需定义一次，并在每个需要共享其权重的管道中引用。
- en: We will use the functional API almost exclusively in this chapter, so you will
    see quite a few examples of its use. The Keras website has many more usage examples
    for the functional API.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将几乎专门使用功能式API，因此你将看到许多使用示例。Keras官网上有更多关于功能式API的使用示例。
- en: Regression networks
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归网络
- en: The two major techniques of supervised learning are classification and regression.
    In both cases, the model is trained with data to predict known labels. In case
    of classification, these labels are discrete values such as genres of text or
    image categories. In case of regression, these labels are continuous values, such
    as stock prices or human intelligence quotients (IQ).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的两大主要技术是分类和回归。在这两种情况下，模型都会用数据进行训练，以预测已知标签。在分类的情况下，这些标签是离散值，如文本的类别或图像的种类；而在回归的情况下，这些标签是连续值，如股票价格或人的智商（IQ）。
- en: Most of the examples we have seen show deep learning models being used to perform
    classification. In this section, we will look at how to perform regression using
    such a model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的大多数示例展示了深度学习模型用于执行分类任务。在本节中，我们将探讨如何使用此类模型进行回归。
- en: Recall that classification models have a dense layer with a nonlinear activation
    at the end, the output dimension of which corresponds to the number of classes
    the model can predict. Thus, an ImageNet image classification model has a dense
    (1,000) layer at the end, corresponding to 1,000 ImageNet classes it can predict.
    Similarly, a sentiment analysis model has a dense layer at the end, corresponding
    to positive or negative sentiment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，分类模型在最后有一个带有非线性激活的密集层，其输出维度对应于模型可以预测的类别数。因此，ImageNet图像分类模型在最后有一个密集层（1,000），对应于它可以预测的1,000个ImageNet类别。类似地，情感分析模型在最后有一个密集层，对应于正面或负面的情感。
- en: 'Regression models also have a dense layer at the end, but with a single output,
    that is, an output dimension of one, and no nonlinear activation. Thus the dense
    layer just returns the sum of the activations from the previous layer. In addition,
    the loss function used is typically **mean squared error** (**MSE**), but some
    of the other objectives (listed on the Keras objectives page at: [https://keras.io/losses/](https://keras.io/losses/))
    can be used as well.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型在最后也有一个密集层，但只有一个输出，即输出维度为一，并且没有非线性激活。因此，密集层仅返回来自前一层的激活值之和。此外，通常使用的损失函数是**均方误差**（**MSE**），但也可以使用其他目标函数（详见Keras目标页面：[https://keras.io/losses/](https://keras.io/losses/)）。
- en: Keras regression example — predicting benzene levels in the air
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras回归示例——预测空气中的苯浓度
- en: In this example, we will predict the concentration of benzene in the atmosphere
    given some other variables such as concentrations of carbon monoxide, nitrous
    oxide, and so on in the atmosphere as well as temperature and relative humidity.
    The dataset we will use is the air quality dataset from the UCI Machine Learning
    Repository ([https://archive.ics.uci.edu/ml/datasets/Air+Quality](https://archive.ics.uci.edu/ml/datasets/Air+Quality)).
    The dataset contains 9,358 instances of hourly averaged readings from an array
    of five metal oxide chemical sensors. The sensor array was located in a city in
    Italy, and the recordings were made from March 2004 to February 2005.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将预测大气中苯的浓度，给定一些其他变量，例如一氧化碳、氮氧化物等的浓度，以及温度和相对湿度。我们将使用的数据集来自UCI机器学习库的空气质量数据集（[https://archive.ics.uci.edu/ml/datasets/Air+Quality](https://archive.ics.uci.edu/ml/datasets/Air+Quality)）。该数据集包含9,358条来自五个金属氧化物化学传感器的小时平均读数。传感器阵列位于意大利的一个城市，记录时间为2004年3月至2005年2月。
- en: 'As usual, first we import all our necessary libraries:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，首先我们导入所有必要的库：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The dataset is provided as a CSV file. We load the input data into a Pandas
    (for more information refer to: [http://pandas.pydata.org/](http://pandas.pydata.org/))
    data frame. Pandas is a popular data analysis library built around data frames,
    a concept borrowed from the R language. We use Pandas here to read the dataset
    for two reasons. First, the dataset contains empty fields where they could not
    be recorded for some reason. Second, the dataset uses commas for decimal points,
    a custom common in some European countries. Pandas has built-in support to handle
    both situations, along with a few other conveniences, as we will see soon:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以CSV文件形式提供。我们将输入数据加载到Pandas（更多信息参见：[http://pandas.pydata.org/](http://pandas.pydata.org/)）数据框中。Pandas是一个流行的数据分析库，围绕数据框构建，这是一个借鉴自R语言的概念。我们在这里使用Pandas加载数据集有两个原因。首先，数据集中包含一些由于某种原因无法记录的空字段。其次，数据集使用逗号作为小数点分隔符，这在一些欧洲国家中是常见的习惯。Pandas内建支持处理这两种情况，并且提供了一些其他便利功能，正如我们接下来会看到的：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding example removes the first two columns, which contains the observation
    date and time, and the last two columns which seem to be spurious. Next we replace
    the empty fields with the average value for the column. Finally, we export the
    data frame as a matrix for downstream use.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例删除了前两列，其中包含观察日期和时间，及后两列，这些列似乎是无关的。接下来，我们用该列的平均值替换空字段。最后，我们将数据框导出为矩阵，以便后续使用。
- en: 'One thing to note is that each column of the data has different scales since
    they measure different quantities. For example, the concentration of tin oxide
    is in the 1,000 range, while non-methanic hydrocarbons is in the 100 range. In
    many situations our features are homogeneous so scaling is not an issue, but in
    cases like this it is generally a good practice to scale the data. Scaling here
    consists of subtracting from each column the mean of the column and dividing by
    its standard deviation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，数据的每一列有不同的尺度，因为它们测量的是不同的量。例如，氧化锡的浓度在1,000范围内，而非甲烷烃的浓度在100范围内。在许多情况下，我们的特征是同质的，因此不需要缩放，但在像这样的情况下，通常建议对数据进行缩放。这里的缩放过程包括从每列中减去该列的均值，并除以其标准差：
- en: '![](img/zscore.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/zscore.png)'
- en: 'To do this, we use the `StandardScaler` class provided by the `scikit-learn`
    library, shown as follows. We store the mean and standard deviations because we
    will need this later when reporting results or predicting against new data. Our
    target variable is the fourth column in our input dataset, so we split this scaled
    data into input variables `X` and target variable `y`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用`scikit-learn`库提供的`StandardScaler`类，如下所示。我们存储均值和标准差，因为我们稍后在报告结果或预测新数据时会用到它们。我们的目标变量是输入数据集中的第四列，因此我们将缩放后的数据分为输入变量`X`和目标变量`y`：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then split the data into the first 70% for training and the last 30% for
    testing. This gives us 6,549 records for training and 2,808 records for testing:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据分为前70%用于训练，后30%用于测试。这为我们提供了6,549条训练记录和2,808条测试记录：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next we define our network. This is a simple two layer dense network that takes
    a vector of 12 features as input and outputs a scaled prediction. The hidden dense
    layer has eight neurons. We initialize weight matrices for both dense layers with
    a specific initialization scheme called *glorot uniform*. For a full list of initialization
    schemes, please refer to the Keras initializations here: [https://keras.io/initializers/](https://keras.io/initializers/).
    The loss function used is mean squared error (`mse`) and the optimizer is `adam`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的网络。这是一个简单的两层密集网络，输入为12个特征的向量，输出为缩放后的预测值。隐藏层密集层有八个神经元。我们使用一种特定的初始化方案叫做*glorot
    uniform*来初始化两个密集层的权重矩阵。有关初始化方案的完整列表，请参见Keras初始化文档：[https://keras.io/initializers/](https://keras.io/initializers/)。所使用的损失函数是均方误差（`mse`），优化器是`adam`：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We train this model for 20 epochs and batch size of 10:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为此模型训练了20个周期，批次大小为10：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in a model that has a mean squared error of 0.0003 (approximately
    2% RMSE) on the training set and 0.0016 (approximately 4% RMSE) on the validation
    set, as shown in the logs of the training step here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致一个模型，其训练集的均方误差为0.0003（大约2%的RMSE），验证集为0.0016（大约4%的RMSE），如下训练步骤日志所示：
- en: '![](img/ss-7-1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-7-1.png)'
- en: 'We also look at some values of benzene concentrations that were originally
    recorded and compare them to those predicted by our model. Both actual and predicted
    values are rescaled from their scaled *z*-values to actual values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还查看了一些原本记录的苯浓度值，并将它们与我们模型预测的值进行比较。实际值和预测值都从它们的缩放*z*-值重缩放为实际值：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The side-by-side comparison shows that the predictions are quite close to the
    actual values:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 并排比较显示，预测值与实际值非常接近：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we graph the actual values against the predictions for our entire
    test set. Once more, we see that the network predicts values that are very close
    to the expected values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将实际值与我们整个测试集的预测值进行比较。再次看到，网络预测的值非常接近预期值：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of the preceding example is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例的输出如下：
- en: '![](img/regression-chart.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/regression-chart.png)'
- en: Unsupervised learning — autoencoders
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习 — 自编码器
- en: Autoencoders are a class of neural network that attempt to recreate the input
    as its target using back-propagation. An autoencoder consists of two parts, an
    encoder and a decoder. The encoder will read the input and compress it to a compact
    representation, and the decoder will read the compact representation and recreate
    the input from it. In other words, the autoencoder tries to learn the identity
    function by minimizing the reconstruction error.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一类神经网络，尝试通过反向传播将输入重建为目标。自编码器由两部分组成：编码器和解码器。编码器读取输入并将其压缩为紧凑的表示，解码器则读取紧凑的表示并从中重建输入。换句话说，自编码器通过最小化重建误差来尝试学习恒等函数。
- en: Even though the identity function does not seem like a very interesting function
    to learn, the way in which this is done makes it interesting. The number of hidden
    units in the autoencoder is typically less than the number of input (and output)
    units. This forces the encoder to learn a compressed representation of the input
    which the decoder reconstructs. If there is structure in the input data in the
    form of correlations between input features, then the autoencoder will discover
    some of these correlations, and end up learning a low dimensional representation
    of the data similar to that learned using **principal component analysis** (**PCA**).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管恒等函数看起来不像是一个很有趣的函数来学习，但实现的方式使其变得有趣。自编码器中的隐藏单元数量通常小于输入（和输出）单元的数量。这迫使编码器学习输入的压缩表示，而解码器则从中重建。如果输入数据中存在特征之间的相关性，那么自编码器将发现这些相关性，并最终学习一个类似于使用**主成分分析**（**PCA**）得到的低维表示。
- en: Once the autoencoder is trained, we would typically just discard the decoder
    component and use the encoder component to generate compact representations of
    the input. Alternatively, we could use the encoder as a feature detector that
    generates a compact, semantically rich representation of our input and build a
    classifier by attaching a softmax classifier to the hidden layer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦自编码器训练完成，我们通常会丢弃解码器组件，只使用编码器组件生成输入的紧凑表示。或者，我们可以将编码器作为特征检测器，生成输入的紧凑且语义丰富的表示，并通过将软最大分类器附加到隐藏层来构建分类器。
- en: 'The encoder and decoder components of an autoencoder can be implemented using
    either dense, convolutional, or recurrent networks, depending on the kind of data
    that is being modeled. For example, dense networks might be a good choice for
    autoencoders used to build **collaborative filtering** (**CF**) models (for more
    information refer to the articles: *AutoRec: Autoencoders Meet Collaborative Filtering*,
    by S. Sedhain, Proceedings of the 24th International Conference on World Wide
    Web, ACM, 2015 and *Wide & Deep Learning for Recommender Systems*, by H. Cheng,
    Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, ACM,
    2016), where we learn a compressed model of user preferences based on actual sparse
    user ratings. Similarly, convolutional neural networks may be appropriate for
    the use case covered in the article: *See: Using Deep Learning to Remove Eyeglasses
    from Faces*, by M. Runfeldt. and recurrent networks a good choice for autoencoders
    building on text data, such as deep patient (for more information refer to the
    article: *Deep Patient: An Unsupervised Representation to Predict the Future of
    Patients from the Electronic Health Records*, by R. Miotto, Scientific Reports
    6, 2016) and skip-thought vectors ((for more information refer to the article: *Skip-Thought
    Vectors*, by R. Kiros, Advances in Neural Information Processing Systems, 2015).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '自编码器的编码器和解码器组件可以使用密集、卷积或递归网络来实现，具体取决于所建模的数据类型。例如，密集网络可能是构建**协同过滤**（**CF**）模型的自编码器的好选择（有关更多信息，请参阅文章：*AutoRec:
    Autoencoders Meet Collaborative Filtering*，S. Sedhain，2015年国际万维网大会论文集，ACM 和 *Wide
    & Deep Learning for Recommender Systems*，H. Cheng，2016年第一届推荐系统深度学习研讨会论文集，ACM），在该模型中，我们根据实际稀疏的用户评分学习压缩的用户偏好模型。类似地，卷积神经网络可能适用于文章中涵盖的用例：*See:
    Using Deep Learning to Remove Eyeglasses from Faces*，M. Runfeldt，递归网络是构建基于文本数据的自编码器的好选择，如深度患者（有关更多信息，请参阅文章：*Deep
    Patient: An Unsupervised Representation to Predict the Future of Patients from
    the Electronic Health Records*，R. Miotto，Scientific Reports 6，2016）和跳跃思想向量（有关更多信息，请参阅文章：*Skip-Thought
    Vectors*，R. Kiros，神经信息处理系统进展，2015年）。'
- en: Autoencoders can also be stacked by successively stacking encoders that compress
    their input to smaller and smaller representations, and stacking decoders in the
    opposite sequence. Stacked autoencoders have greater expressive power and the
    successive layers of representations capture a hierarchical grouping of the input,
    similar to the convolution and pooling operations in convolutional neural networks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器也可以通过逐次堆叠编码器来实现，将输入压缩为越来越小的表示，并按相反的顺序堆叠解码器。堆叠自编码器具有更强的表达能力，连续的表示层捕捉了输入的层次分组，类似于卷积神经网络中的卷积和池化操作。
- en: 'Stacked autoencoders used to be trained layer by layer. For example, in the
    network shown next, we would first train layer *X* to reconstruct layer *X''*
    using the hidden layer *H1* (ignoring *H2*). We would then train the layer *H1*
    to reconstruct layer *H1''* using the hidden layer *H2*. Finally, we would stack all
    the layers together in the configuration shown and fine tune it to reconstruct
    *X''* from *X*. With better activation and regularization functions nowadays,
    however, it is quite common to train these networks in totality:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠自编码器曾经是逐层训练的。例如，在下图所示的网络中，我们首先会训练层*X*，通过隐藏层*H1*来重构层*X'*（忽略*H2*）。然后，我们会训练层*H1*，通过隐藏层*H2*来重构层*H1'*。最后，我们会将所有层堆叠在一起，如所示配置，并微调网络，以便通过*X*重构*X'*。然而，随着现代激活和正则化函数的改进，训练这些网络的整体方法已变得非常普遍：
- en: '![](img/stacked-autoencoder.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/stacked-autoencoder.png)'
- en: The Keras blog post, *Building Autoencoders in Keras* ([https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html))
    has great examples of building autoencoders that reconstructs MNIST digit images
    using fully connected and convolutional neural networks. It also has a good discussion
    on denoising and variational autoencoders, which we will not cover here.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 博客文章，*在 Keras 中构建自编码器*（[https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html)）提供了很好的自编码器构建示例，这些自编码器使用全连接和卷积神经网络重构
    MNIST 数字图像。文章中还讨论了去噪和变分自编码器的内容，虽然我们在这里不做讲解。
- en: Keras autoencoder example — sentence vectors
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 自编码器示例 — 句子向量
- en: In this example, we will build and train an LSTM-based autoencoder to generate
    sentence vectors for documents in the Reuters-21578 corpus ([https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)).
    We have already seen in [Chapter 5](700e9954-f126-49b5-b4e4-fa7321296e85.xhtml),
    *Word Embeddings*, how to represent a word using word embeddings to create vectors
    that represent its meaning in the context of other words it appears with. Here,
    we will see how to build similar vectors for sentences. Sentences are a sequence
    of words, so a sentence vector represents the meaning of the sentence.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将构建并训练一个基于LSTM的自编码器，用于为Reuters-21578语料库中的文档生成句子向量（[https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)）。我们已经在[第5章](700e9954-f126-49b5-b4e4-fa7321296e85.xhtml)，*词嵌入*中，展示了如何使用词嵌入表示一个单词，并创建表示其在上下文中意义的向量。在这里，我们将看到如何为句子构建类似的向量。句子是词语的序列，因此，句子向量表示的是句子的意义。
- en: The easiest way to build a sentence vector is to just add up the word vectors
    and divide by the number of words. However, this treats the sentence as a bag
    of words, and does not take the order of words into account. Thus the sentences
    *The dog bit the man* and *The man bit the dog* would be treated as identical
    under this scenario. LSTMs are designed to work with sequence input and do take
    the order of words into consideration thus providing a better and more natural
    representation for the sentence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 构建句子向量的最简单方法是将词向量相加并除以词数。然而，这种方法将句子视为词袋，并没有考虑词语的顺序。因此，在这种情况下，句子*The dog bit
    the man*和*The man bit the dog*将被视为相同。LSTM被设计用于处理序列输入，并且会考虑词语的顺序，从而为句子提供更好、更自然的表示。
- en: 'First we import the necessary libraries:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The data is provided as a set of SGML files. We have already parsed and consolidated
    this data into a single text file in [Chapter 6](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml),
    *Recurrent Neural Network — RNN*, for our GRU-based POS tagging example. We will
    reuse this data to first convert each block of text into a list of sentences,
    one sentence per line:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以一组SGML文件提供。我们已经在[第6章](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml)，*递归神经网络—RNN*中解析并整合了这些数据到一个单一的文本文件中，用于我们的基于GRU的词性标注示例。我们将重用这些数据，首先将每个文本块转换为句子列表，每行一个句子：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To build up our vocabulary, we read this list of sentences again, word by word.
    Each word is normalized as it is added. The normalization is to replace any token
    that looks like a number with the digit `9` and to lowercase them. The result
    is the word frequency table, `word_freqs`. We also compute the sentence length
    for each sentence and create a list of parsed sentences by rejoining the tokens
    with space so it is easier to parse in a subsequent step:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展我们的词汇量，我们逐字逐句地再次阅读这份句子列表。每个单词在加入时都会进行标准化处理。标准化的方式是将任何看起来像数字的标记替换为数字`9`并将其转换为小写。结果是生成词频表`word_freqs`。我们还计算每个句子的长度，并通过用空格重新连接词语来创建解析后的句子列表，这样可以在后续步骤中更方便地进行解析：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This gives us some information about the corpus that will help us figure out
    good values for our constants for our LSTM network:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一些关于语料库的信息，帮助我们确定LSTM网络常数的合适值：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives us the following information about the corpus:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下关于语料库的信息：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Based on this information, we set the following constants for our LSTM model.
    We choose our `VOCAB_SIZE` as `5000`, that is, our vocabulary covers the most
    frequent 5,000 words that cover over 93% of the words used in the corpus. The
    remaining words are treated as **out of vocabulary** (**OOV**) and replaced with
    the token `UNK`. At prediction time, any word that the model hasn''t seen will
    also be assigned the token `UNK`. `SEQUENCE_LEN` is set to approximately twice
    the median length of sentences in the training set, and indeed, approximately
    110 million of our 131 million sentences are shorter than this setting. Sentences
    that are shorter than `SEQUENCE_LENGTH` will be padded by a special `PAD` character,
    and those that are longer will be truncated to fit the limit:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些信息，我们为LSTM模型设置了以下常量。我们选择将`VOCAB_SIZE`设置为`5000`，即我们的词汇表覆盖了最常用的5,000个单词，这些单词覆盖了语料库中超过93%的词汇。剩余的单词被视为**词汇表外**（**OOV**），并用`UNK`标记替换。在预测时，任何模型未见过的单词也会被分配为`UNK`标记。`SEQUENCE_LEN`设置为训练集中文本长度的中位数的大约两倍，实际上，我们的131百万句子中大约有1.1亿句子比这个设置要短。比`SEQUENCE_LENGTH`短的句子会被`PAD`字符填充，超过长度的句子会被截断以适应限制：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Since the input to our LSTM will be numeric, we need to build lookup tables
    that go back and forth between words and word IDs. Since we limit our vocabulary
    size to 5,000 and we have to add the two pseudo-words `PAD` and `UNK`, our lookup
    table contains entries for the most frequently occurring 4,998 words plus `PAD`
    and `UNK`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们LSTM的输入是数值型的，我们需要构建查找表，用于在单词和单词ID之间相互转换。由于我们将词汇表大小限制为5,000，并且需要添加两个伪单词`PAD`和`UNK`，因此我们的查找表包含最常出现的4,998个单词条目以及`PAD`和`UNK`：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The input to our network is a sequence of words, where each word is represented
    by a vector. Simplistically, we could just use a one-hot encoding for each word,
    but that makes the input data very large. So we encode each word using its 50-dimensional
    GloVe embeddings. The embedding is generated into a matrix of shape `(VOCAB_SIZE,
    EMBED_SIZE)` where each row represents the GloVe embedding for a word in our vocabulary.
    The `PAD` and `UNK` rows (`0` and `1` respectively) are populated with zeros and
    random uniform values respectively:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们网络的输入是一系列单词，每个单词由一个向量表示。简单来说，我们可以为每个单词使用一个独热编码，但那样会使输入数据变得非常庞大。因此，我们使用50维的GloVe词向量来编码每个单词。这个嵌入被生成到一个形状为`(VOCAB_SIZE,
    EMBED_SIZE)`的矩阵中，其中每一行代表我们词汇表中某个单词的GloVe嵌入。`PAD`和`UNK`行（分别为`0`和`1`）分别用零和随机均匀值填充：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our autoencoder model takes a sequence of GloVe word vectors and learns to
    produce another sequence that is similar to the input sequence. The encoder LSTM
    compresses the sequence into a fixed size context vector, which the decoder LSTM
    uses to reconstruct the original sequence. A schematic of the network is shown
    here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自动编码器模型接受一系列GloVe词向量，并学习生成一个与输入序列相似的输出序列。编码器LSTM将序列压缩成一个固定大小的上下文向量，解码器LSTM则使用这个向量来重构原始序列。网络的示意图如下所示：
- en: '![](img/sent-thoughts.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/sent-thoughts.png)'
- en: 'Because the input is quite large, we will use a generator to produce each batch
    of input. Our generator produces batches of tensors of shape `(BATCH_SIZE, SEQUENCE_LEN,
    EMBED_SIZE)`. Here `BATCH_SIZE` is `64`, and since we are using 50-dimensional
    GloVe vectors, `EMBED_SIZE` is `50`. We shuffle the sentences at the beginning
    of each epoch, and return batches of 64 sentences. Each sentence is represented
    as a vector of GloVe word vectors. If a word in the vocabulary does not have a
    corresponding GloVe embedding, it is represented by a zero vector. We construct
    two instances of the generator, one for training data and one for test data, consisting
    of 70% and 30% of the original dataset respectively:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入非常庞大，我们将使用生成器来产生每一批输入。我们的生成器会生成形状为`(BATCH_SIZE, SEQUENCE_LEN, EMBED_SIZE)`的张量批次。这里`BATCH_SIZE`为`64`，由于我们使用50维的GloVe向量，因此`EMBED_SIZE`为`50`。我们在每个epoch开始时对句子进行打乱，并返回64个句子的批次。每个句子表示为一个GloVe词向量。如果词汇表中的某个单词没有对应的GloVe嵌入，它会被表示为一个零向量。我们构建了生成器的两个实例，一个用于训练数据，另一个用于测试数据，分别包含原始数据集的70%和30%：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now we are ready to define the autoencoder. As we have shown in the diagram,
    it is composed of an encoder LSTM and a decoder LSTM. The encoder LSTM reads a
    tensor of shape `(BATCH_SIZE, SEQUENCE_LEN, EMBED_SIZE)` representing a batch
    of sentences. Each sentence is represented as a padded fixed-length sequence of
    words of size `SEQUENCE_LEN`. Each word is represented as a 300-dimensional GloVe
    vector. The output dimension of the encoder LSTM is a hyperparameter `LATENT_SIZE`,
    which is the size of the sentence vector that will get out of the encoder part
    of the trained autoencoder later. The vector space of dimensionality `LATENT_SIZE`
    represents the latent space that encodes the meaning of the sentence. The output
    of the LSTM is a vector of size (`LATENT_SIZE`) for each sentence, so for the
    batch the shape of the output tensor is `(BATCH_SIZE, LATENT_SIZE)`. This is now
    fed to a RepeatVector layer, which replicates this across the entire sequence,
    that is., the output tensor from this layer has the shape `(BATCH_SIZE, SEQUENCE_LEN,
    LATENT_SIZE)`. This tensor is now fed into the decoder LSTM, whose output dimension
    is the `EMBED_SIZE`, so the output tensor has shape `(BATCH_SIZE, SEQUENCE_LEN,
    EMBED_SIZE)`, that is, the same shape as the input tensor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好定义自动编码器了。如图所示，它由一个编码器LSTM和一个解码器LSTM组成。编码器LSTM读取一个形状为`(BATCH_SIZE, SEQUENCE_LEN,
    EMBED_SIZE)`的张量，表示一个句子批次。每个句子表示为一个填充的固定长度的单词序列，长度为`SEQUENCE_LEN`。每个单词由一个300维的GloVe向量表示。编码器LSTM的输出维度是一个超参数`LATENT_SIZE`，这是后续经过训练的自动编码器编码器部分输出的句子向量的大小。`LATENT_SIZE`维度的向量空间表示了编码句子意义的潜在空间。LSTM的输出是每个句子的一个`LATENT_SIZE`大小的向量，因此，对于整个批次，输出张量的形状是`(BATCH_SIZE,
    LATENT_SIZE)`。这个张量现在输入到RepeatVector层，该层将其在整个序列中复制，即该层的输出张量的形状为`(BATCH_SIZE, SEQUENCE_LEN,
    LATENT_SIZE)`。这个张量接着输入到解码器LSTM，其输出维度是`EMBED_SIZE`，因此输出张量的形状为`(BATCH_SIZE, SEQUENCE_LEN,
    EMBED_SIZE)`，即与输入张量的形状相同。
- en: 'We compile this model with the `SGD` optimizer and the `mse` loss function.
    The reason we use MSE is that we want to reconstruct a sentence that has a similar
    meaning, that is, something that is close to the original sentence in the embedded
    space of dimension `LATENT_SIZE`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`SGD`优化器和`mse`损失函数来编译这个模型。我们使用MSE的原因是我们希望重建一个具有相似意义的句子，也就是说，重建一个在`LATENT_SIZE`维度的嵌入空间中接近原始句子的句子：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We train the autoencoder for 10 epochs using the following code. 10 epochs
    were chosen because the MSE loss converges within this time. We also save the
    best model retrieved so far based on the MSE loss:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码训练自动编码器10个周期。选择10个周期是因为在这个时间范围内，MSE损失已经收敛。我们还会根据MSE损失保存到目前为止最好的模型：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The results of the training are shown as follows. As you can see, the training
    MSE reduces from 0.14 to 0.1 and the validation MSE reduces from 0.12 to 0.1:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结果如下所示。正如你所看到的，训练MSE从0.14降到0.1，验证MSE从0.12降到0.1：
- en: '![](img/ss-7-2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-7-2.png)'
- en: 'Or, graphically it shows as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，图形化显示如下：
- en: '![](img/autoencoder-lossfunc.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/autoencoder-lossfunc.png)'
- en: Since we are feeding in a matrix of embeddings, the output will also be a matrix
    of word embeddings. Since the embedding space is continuous and our vocabulary
    is discrete, not every output embedding will correspond to a word. The best we
    can do is to find a word that is closest to the output embedding in order to reconstruct
    the original text. This is a bit cumbersome, so we will evaluate our autoencoder
    in a different way.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们输入的是一个嵌入矩阵，输出也将是一个单词嵌入矩阵。由于嵌入空间是连续的，而我们的词汇表是离散的，并不是每个输出的嵌入都对应一个单词。我们能做的最好的方法是找到一个与输出嵌入最接近的单词，从而重建原始文本。这有点麻烦，所以我们将以不同的方式评估我们的自动编码器。
- en: 'Since the objective of the autoencoder is to produce a good latent representation,
    we compare the latent vectors produced from the encoder using the original input
    versus the output of the autoencoder. First, we extract the encoder component
    into its own network:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自动编码器的目标是生成一个良好的潜在表示，我们比较编码器生成的潜在向量，使用的是原始输入和自动编码器输出。首先，我们将编码器部分提取出来作为单独的网络：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then we run the autoencoder on the test set to return the predicted embeddings.
    We then send both the input embedding and the predicted embedding through the
    encoder to produce sentence vectors from each, and compare the two vectors using
    *cosine* similarity. Cosine similarities close to one indicate high similarity
    and those close to zero indicate low similarity. The following code runs against
    a random subset of 500 test sentences and produces some sample values of cosine
    similarities between the sentence vectors generated from the source embedding
    and the corresponding target embedding produced by the autoencoder:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在测试集上运行自动编码器以返回预测的嵌入。接着，我们将输入嵌入和预测的嵌入通过编码器传输，从每个嵌入生成句子向量，并使用*余弦*相似度比较这两个向量。接近1的余弦相似度表示高相似性，接近0则表示低相似性。以下代码对500个测试句子的随机子集进行处理，生成源嵌入和由自动编码器生成的相应目标嵌入之间的句子向量余弦相似度样本值：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The first 10 values of cosine similarities are shown as follows. As we can
    see, the vectors seem to be quite similar:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前10个余弦相似度值。如我们所见，这些向量似乎非常相似：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'A histogram of the distribution of values of cosine similarities for the sentence
    vectors from the first 500 sentences in the test set are shown as follows. As
    previously, it confirms that the sentence vectors generated from the input and
    output of the autoencoder are very similar, showing that the resulting sentence
    vector is a good representation of the sentence:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了测试集前500个句子的句子向量余弦相似度分布的直方图。如前所述，这确认了自动编码器的输入和输出生成的句子向量非常相似，表明生成的句子向量是句子的良好表示：
- en: '![](img/autoencoder-cosims.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/autoencoder-cosims.png)'
- en: Composing deep networks
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度网络
- en: We have looked extensively at these three basic deep learning networks—the **fully
    connected network** (**FCN**), the CNN and the RNN models. While each of these
    have specific use cases for which they are most suited, you can also compose larger
    and more useful models by combining these models as Lego-like building blocks
    and using the Keras functional API to glue them together in new and interesting
    ways.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入研究了这三种基本的深度学习网络——**全连接网络**（**FCN**）、卷积神经网络（CNN）和循环神经网络（RNN）模型。虽然每种网络都有其最适用的特定用例，但你也可以通过将这些模型作为类似乐高的构建块，结合使用Keras功能性API以新的、有趣的方式将它们组合在一起，构建更大、更有用的模型。
- en: Such models tend to be somewhat specialized to the task for which they were built,
    so it is impossible to generalize about them. Usually, however, they involve learning
    from multiple inputs or generating multiple outputs. One example could be a question
    answering network, where the network learns to predict answers given a story and
    a question. Another example could be a siamese network that calculates similarity
    between a pair of images, where the network is trained to predict either a binary
    (similar/not similar) or categorical (gradations of similarity) label using a
    pair of images as input. Yet another example could be an object classification
    and localization network where it learns to predict the image category as well
    as where the image is located in the picture jointly from the image. The first
    two examples are examples of composite networks with multiple inputs, and the
    last is an example of a composite network with multiple outputs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的模型往往对其构建的任务有所专门化，因此很难进行一般化。不过，通常它们涉及从多个输入中学习或生成多个输出。例如，一个问答网络，可以在给定故事和问题的情况下预测答案。另一个例子是孪生网络，它计算一对图像之间的相似度，网络通过一对图像作为输入来预测二元（相似/不相似）或类别（相似度的不同级别）标签。再一个例子是物体分类和定位网络，它从图像中共同学习预测图像的类别以及图像在图片中的位置。前两个例子是具有多个输入的复合网络，最后一个是具有多个输出的复合网络。
- en: Keras example — memory network for question answering
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 示例 —— 用于问答的记忆网络
- en: 'In this example, we will build a memory network for question answering. Memory
    networks are a specialized architecture that consist of a memory unit in addition
    to other learnable units, usually RNNs. Each input updates the memory state and
    the final output is computed by using the memory along with the output from the
    learnable unit. This architecture was suggested in 2014 via the paper (for more
    information refer to: *Memory Networks*, by J. Weston, S. Chopra, and A. Bordes,
    arXiv:1410.3916, 2014). A year later, another paper (for more information refer
    to: *Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks*,
    by J. Weston, arXiv:1502.05698, 2015) put forward the idea of a synthetic dataset
    and a standard set of 20 question answering tasks, each with a higher degree of
    difficulty than the previous one, and applied various deep learning networks to
    solve these tasks. Of these, the memory network achieved the best results across
    all the tasks. This dataset was later made available to the general public through
    Facebook''s bAbI project ([https://research.fb.com/projects/babi/](https://research.fb.com/projects/babi/)).
    The implementation of our memory network resembles most closely the one described
    in this paper (for more information refer to: *End-To-End Memory Networks*, by S.
    Sukhbaatar, J. Weston, and R. Fergus, Advances in Neural Information Processing
    Systems, 2015), in that all the training happens jointly in a single network.
    It uses the bAbI dataset to solve the first question answering task.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个例子中，我们将构建一个用于问答的记忆网络。记忆网络是一种专门的架构，除了其他可学习的单元（通常是RNN）外，还包含一个记忆单元。每个输入都会更新记忆状态，最终输出通过结合记忆和可学习单元的输出进行计算。该架构在2014年通过论文提出（更多信息请参考：*Memory
    Networks*，J. Weston, S. Chopra 和 A. Bordes，arXiv:1410.3916，2014年）。一年后，另一篇论文（更多信息请参考：*Towards
    AI-Complete Question Answering: A Set of Prerequisite Toy Tasks*，J. Weston，arXiv:1502.05698，2015年）提出了一个合成数据集的想法，并给出了20个问答任务的标准集，每个任务的难度比前一个更高，并应用各种深度学习网络来解决这些任务。在所有任务中，记忆网络取得了最佳结果。这个数据集后来通过Facebook的bAbI项目向公众发布（[https://research.fb.com/projects/babi/](https://research.fb.com/projects/babi/)）。我们记忆网络的实现最接近于这篇论文中描述的实现（更多信息请参考：*End-To-End
    Memory Networks*，S. Sukhbaatar, J. Weston 和 R. Fergus，2015年《神经信息处理系统进展》），其特点是所有的训练都在单一网络中联合进行。它使用bAbI数据集来解决第一个问答任务。'
- en: 'First, we will import the necessary libraries:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入必要的库：
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The bAbI data for the first question answering task consists of 10,000 short
    sentences each for the training and the test sets. A story consists of two to
    three sentences, followed by a question. The last sentence in each story has the
    question and the answer appended to it at the end. The following block of code
    parses each of the training and test files into a list of triplets of story, question
    and answer:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问答任务的bAbI数据集由10,000个短句组成，每个句子用于训练集和测试集。一个故事由两到三句组成，后面跟着一个问题。每个故事的最后一句话会在结尾附加问题和答案。以下代码块解析每个训练和测试文件，将其转换为包含故事、问题和答案三元组的列表：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Our next step is to run through the texts in the generated lists and build
    our vocabulary. This should be quite familiar to us by now, since we have used
    a similar idiom a few times already. Unlike the previous time, our vocabulary
    is quite small, only 22 unique words, so we will not have any out of vocabulary
    words:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是遍历生成的列表中的文本并构建我们的词汇表。现在这应该对我们非常熟悉，因为我们已经多次使用过类似的表达。与上次不同，我们的词汇表非常小，只有22个独特的词，因此我们不会遇到任何超出词汇表范围的单词：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The memory network is based on RNNs, where each sentence in the story and question
    is treated as a sequence of words, so we need to find out the maximum length of
    the sequence for our story and question. The following block of code does this.
    We find that the maximum length of a story is 14 words and the maximum length
    of a question is just four words:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆网络基于RNN，其中故事和问题中的每个句子都被视为一个单词序列，因此我们需要找出故事和问题序列的最大长度。以下代码块执行此操作。我们发现，故事的最大长度是14个单词，问题的最大长度仅为4个单词：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As previously, the input to our RNNs is a sequence of word IDs. So we need
    to use our vocabulary dictionary to convert the (story, question, and answer)
    triplet into a sequence of integer word IDs. The next block of code does this
    and zero pads the resulting sequences of story and answer to the maximum sequence
    lengths we computed previously. At this point, we have lists of padded word ID
    sequences for each triplet in the training and test sets:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的 RNN 输入是一个单词 ID 的序列。因此，我们需要使用我们的词汇字典，将（故事、问题和答案）三元组转换为整数单词 ID 的序列。下方的代码块完成了这一转换，并将结果中的故事和答案序列零填充至我们之前计算的最大序列长度。此时，我们为训练集和测试集中的每个三元组获得了填充后的单词
    ID 序列列表：
- en: '[PRE31]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We want to define the model. The definition is longer than we have seen previously,
    so it may be convenient to refer to the diagram as you look through the definition:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要定义模型。这个定义比我们之前看到的要长一些，所以在查看定义时，可能方便参考图示：
- en: '![](img/memnet.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/memnet.png)'
- en: There are two inputs to our model, the sequence of word IDs for the question
    and that for the sentence. Each of these is passed into an Embedding layer to
    convert the word IDs to a vector in the 64-dimensional embedding space. Additionally
    the story sequence is passed through an additional embedding that projects it
    to an embedding of size `max_question_length`. All these embedding layers start
    with random weights and are trained jointly with the rest of the network.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的输入有两个：问题的单词 ID 序列和句子的单词 ID 序列。每个序列都传入一个嵌入层，将单词 ID 转换为 64 维嵌入空间中的向量。此外，故事序列还会通过一个额外的嵌入层，将其投影到
    `max_question_length` 大小的嵌入空间中。所有这些嵌入层都以随机权重开始，并与网络的其余部分一起训练。
- en: The first two embeddings (story and question) are merged using a dot product
    to form the network's memory. These represent words in the story and question
    that are identical or close to each other in the embedding space. The output of
    the memory is merged with the second story embedding and summed to form the network
    response, which is once again merged with the embedding for the question to form
    the response sequence. This response sequence is sent through an LSTM, the context
    vector of which is sent to a dense layer to predict the answer, which can be one
    of the words in the vocabulary.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个嵌入（故事和问题）通过点积合并，形成网络的记忆。这些嵌入表示故事和问题中在嵌入空间中相同或相近的单词。记忆的输出与第二个故事嵌入合并，并求和形成网络的响应，之后再次与问题的嵌入合并，形成响应序列。这个响应序列被送入一个
    LSTM，其上下文向量被传递到一个全连接层来预测答案，答案是词汇表中的一个单词。
- en: 'The model is trained using the RMSprop optimizer and categorical cross-entropy
    as the loss function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用 RMSprop 优化器和分类交叉熵作为损失函数进行训练：
- en: '[PRE32]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We train this network for 50 epochs with a batch size of 32 and achieve an
    accuracy of over 81% on the validation set:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用批量大小为 32，训练 50 轮网络，并在验证集上取得了超过 81% 的准确率：
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the trace of the training logs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练日志的跟踪记录：
- en: '![](img/ss-7-3.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-7-3.png)'
- en: 'The change in training and validation loss and accuracy for this training run
    is shown graphically in this graph:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证损失以及准确度的变化在下面的图表中显示：
- en: '![](img/memnn-lossfunc-1.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/memnn-lossfunc-1.png)'
- en: 'We ran the model against the first 10 stories from our test set to verify how
    good the predictions were:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型运行于测试集中的前 10 个故事上，以验证预测的准确性：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As you can see, the predictions were mostly correct:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，预测结果大多数是正确的：
- en: '![](img/memnn-preds-1.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/memnn-preds-1.png)'
- en: Customizing Keras
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制 Keras
- en: Just as composing our basic building blocks into larger architectures enables
    us to build interesting deep learning models, sometimes we need to look at the
    other end of the spectrum. Keras has a lot of functionality built in already,
    so it is very likely that you can build all your models with the provided components
    and not feel the need for customization at all. In case you do need customization,
    Keras has you covered.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 就像将我们的基本构建块组合成更大的架构使我们能够构建有趣的深度学习模型一样，有时我们需要查看谱系的另一端。Keras 已经内置了许多功能，因此你很可能可以使用提供的组件构建所有模型，而完全不需要自定义。如果你确实需要自定义，Keras
    也能满足你的需求。
- en: As you will recall, Keras is a high level API that delegates to either a TensorFlow
    or Theano backend for the computational heavy lifting. Any code you build for
    your customization will call out to one of these backends. In order to keep your
    code portable across the two backends, your custom code should use the Keras backend
    API ([https://keras.io/backend/](https://keras.io/backend/)), which provides a
    set of functions that act like a facade over your chosen backend. Depending on
    the backend selected, the call to the backend facade will translate to the appropriate
    TensorFlow or Theano call. The full list of functions available and their detailed
    descriptions can be found on the Keras backend page.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所记得的，Keras 是一个高层次的 API，它将计算任务委托给 TensorFlow 或 Theano 后端。你为自定义功能编写的任何代码都会调用其中一个后端。为了确保你的代码在两个后端之间具有可移植性，你的自定义代码应该使用
    Keras 后端 API（[https://keras.io/backend/](https://keras.io/backend/)），它提供了一组函数，这些函数像一个
    facade 一样封装了你选择的后端。根据所选的后端，调用后端 facade 会被转换为适当的 TensorFlow 或 Theano 调用。可以在 Keras
    后端页面上找到可用函数的完整列表及其详细描述。
- en: In addition to portability, using the backend API also results in more maintainable
    code, since Keras code is generally more high-level and compact compared to equivalent
    TensorFlow or Theano code. In the unlikely case that you do need to switch to
    using the backend directly, your Keras components can be used directly inside
    TensorFlow (not Theano though) code as described in this Keras blog ([https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可移植性之外，使用后端 API 还可以使代码更加易于维护，因为与等效的 TensorFlow 或 Theano 代码相比，Keras 代码通常更高层次、更简洁。即使在极少数情况下你确实需要直接使用后端，你的
    Keras 组件也可以直接在 TensorFlow（但不是 Theano）代码中使用，正如这个 Keras 博客中所描述的那样（[https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)）。
- en: Customizing Keras typically means writing your own custom layer or custom distance
    function. In this section, we will demonstrate how to build some simple Keras
    layers. You will see more examples of using the backend functions to build other
    custom Keras components, such as objectives (loss functions), in subsequent sections.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义 Keras 通常意味着编写你自己的自定义层或自定义距离函数。在本节中，我们将演示如何构建一些简单的 Keras 层。你将在后续的章节中看到更多使用后端函数构建其他自定义
    Keras 组件（如目标函数（损失函数））的例子。
- en: Keras example — using the lambda layer
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 示例 —— 使用 lambda 层
- en: 'Keras provides a lambda layer; it can wrap a function of your choosing. For
    example, if you wanted to build a layer that squares its input tensor element-wise,
    you can say simply:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一个 lambda 层；它可以封装你选择的函数。例如，如果你想构建一个逐元素平方输入张量的层，你可以简单地这样写：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You can also wrap functions within a lambda layer. For example, if you want
    to build a custom layer that computes the element-wise euclidean distance between
    two input tensors, you would define the function to compute the value itself,
    as well as one that returns the output shape from this function, like so:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将函数封装在 lambda 层中。例如，如果你想构建一个自定义层来计算两个输入张量之间的逐元素欧几里得距离，你需要定义一个函数来计算该值，并且定义另一个函数返回该函数的输出形状，如下所示：
- en: '[PRE36]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can then call these functions using the lambda layer shown as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像下面这样使用 lambda 层来调用这些函数：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Keras example — building a custom normalization layer
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 示例 —— 构建自定义归一化层
- en: While the lambda layer can be very useful, sometimes you need more control.
    As an example, we will look at the code for a normalization layer that implements
    a technique called **local response normalization**. This technique normalizes
    the input over local input regions, but has since fallen out of favor because
    it turned out not to be as effective as other regularization methods such as dropout
    and batch normalization, as well as better initialization methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 lambda 层非常有用，但有时你可能需要更多的控制。例如，我们将查看一个实现了称为**局部响应归一化**技术的归一化层代码。这种技术对局部输入区域进行归一化，但由于它没有其他正则化方法（如
    dropout 和批量归一化）以及更好的初始化方法效果好，因此已经不再流行。
- en: Building custom layers typically involves working with the backend functions,
    so it involves thinking about the code in terms of tensors. As you will recall,
    working with tensors is a two step process. First, you define the tensors and
    arrange them in a computation graph, and then you run the graph with actual data.
    So working at this level is harder than working in the rest of Keras. The Keras
    documentation has some guidelines for building custom layers ([https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/)),
    which you should definitely read.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自定义层通常涉及与后端函数的交互，因此需要将代码视为张量的形式来思考。正如你所记得的，处理张量是一个两步过程。首先，定义张量并将其排列成计算图，然后用实际数据运行该图。因此，在这个层面上工作比在
    Keras 的其他部分更具挑战性。Keras 文档中有一些关于构建自定义层的指南（[https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/)），你一定要阅读。
- en: 'One of the ways to make it easier to develop code in the backend API is to
    have a small test harness that you can run to verify that your code is doing what
    you want it to do. Here is a small harness I adapted from the Keras source to
    run your layer against some input and return a result:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使后端 API 开发更容易的一种方法是拥有一个小的测试框架，您可以运行它来验证代码是否按预期工作。这是我从 Keras 源代码中改编的一个小测试框架，用来运行您的层并返回结果：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And here are some tests with `layer` objects provided by Keras to make sure
    that the harness runs okay:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 Keras 提供的 `layer` 对象进行的一些测试，以确保测试框架能够正常运行：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Before we begin building our local response normalization layer, we need to
    take a moment to understand what it really does. This technique was originally
    used with Caffe, and the Caffe documentation ([http://caffe.berkeleyvision.org/tutorial/layers/lrn.html](http://caffe.berkeleyvision.org/tutorial/layers/lrn.html))
    describes it as a kind of *lateral inhibition* that works by normalizing over
    local input regions. In `ACROSS_CHANNEL` mode, the local regions extend across
    nearby channels but have no spatial extent. In `WITHIN_CHANNEL` mode, the local
    regions extend spatially, but are in separate channels. We will implement the
    `WITHIN_CHANNEL` model as follows. The formula for local response normalization
    in the `WITHIN_CHANNEL` model is given by:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建本地响应归一化层之前，需要花点时间了解它到底是做什么的。这项技术最初是与 Caffe 一起使用的，Caffe 的文档（[http://caffe.berkeleyvision.org/tutorial/layers/lrn.html](http://caffe.berkeleyvision.org/tutorial/layers/lrn.html)）将其描述为一种*侧向抑制*，通过对局部输入区域进行归一化来工作。在
    `ACROSS_CHANNEL` 模式下，局部区域跨越邻近的通道，但没有空间扩展。在 `WITHIN_CHANNEL` 模式下，局部区域在空间上扩展，但位于不同的通道中。我们将实现
    `WITHIN_CHANNEL` 模型，如下所示。`WITHIN_CHANNEL` 模型中的本地响应归一化公式如下：
- en: '![](img/lrn.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/lrn.png)'
- en: The code for the custom layer follows the standard structure. The `__init__`
    method is used to set the application specific parameters, that is, the hyperparameters
    associated with the layer. Since our layer only does a forward computation and
    doesn't have any learnable weights, all we do in the build method is to set the
    input shape and delegate to the superclass's build method, which takes care of
    any necessary book-keeping. In layers where learnable weights are involved, this
    method is where you would set the initial values.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义层的代码遵循标准结构。`__init__` 方法用于设置应用程序特定的参数，也就是与层相关的超参数。由于我们的层只执行前向计算，且没有任何可学习的权重，因此我们在构建方法中做的唯一事情就是设置输入形状，并委托给父类的构建方法，后者负责处理任何必要的管理工作。在涉及可学习权重的层中，这个方法就是你设置初始值的地方。
- en: The call method does the actual computation. Notice that we need to account
    for dimension ordering. Another thing to note is that the batch size is usually
    unknown at design times, so you need to write your operations so that the batch
    size is not explicitly invoked. The computation itself is fairly straightforward
    and follows the formula closely. The sum in the denominator can also be thought
    of as average pooling over the row and column dimension with a padding size of
    *(n, n)* and a stride of *(1, 1)*. Because the pooled data is averaged already,
    we no longer need to divide the sum by *n*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`call` 方法执行实际的计算。请注意，我们需要考虑维度顺序。另一个需要注意的事情是，批次大小通常在设计时是未知的，因此您需要编写操作，使得批次大小不会被显式调用。计算本身非常直接，并且紧跟公式。分母中的求和部分也可以看作是对行和列维度进行平均池化，填充大小为
    *(n, n)*，步幅为 *(1, 1)*。由于池化数据已经是平均值，因此我们不再需要将求和结果除以 *n*。'
- en: 'The last part of the class is the `get_output_shape_for` method. Since the
    layer normalizes each element of the input tensor, the output size is identical
    to the input size:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 课堂的最后部分是`get_output_shape_for`方法。由于该层对输入张量的每个元素进行了归一化，输出大小与输入大小相同：
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can test this layer during development using the test harness we described
    here. It is easier to run this instead of trying to build a whole network to put
    this into, or worse, waiting till you have fully specified the layer before running
    it:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在开发过程中使用我们这里描述的测试工具测试这个层。运行这个工具比构建一个完整的网络并将其放入其中要容易得多，或者更糟糕的是，在完全指定该层之前等待运行：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: While building custom Keras layers seems to be fairly commonplace among experienced
    Keras developers, there are not too many examples available on the Internet. This
    is probably because custom layers are usually built to serve a specific narrow
    purpose and may not be widely useful. The variability also means that one single
    example cannot demonstrate all the possibilities of what you can do with the API.
    Now that you have a good idea of how to build a custom Keras layer, you might
    find it instructive to look at Keunwoo Choi's `melspectogram` ([https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/](https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/))
    and Shashank Gupta's `NodeEmbeddingLayer` ([http://shashankg7.github.io/2016/10/12/Custom-Layer-In-Keras-Graph-Embedding-Case-Study.html](http://shashankg7.github.io/2016/10/12/Custom-Layer-In-Keras-Graph-Embedding-Case-Study.html)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管构建自定义Keras层在经验丰富的Keras开发者中似乎非常普遍，但互联网上并没有太多可用的示例。这可能是因为自定义层通常是为了特定的狭窄目的而构建的，因此不一定具有广泛的用途。可变性也意味着一个单一的示例无法展示你可以通过API做的所有可能性。现在你已经对如何构建自定义Keras层有了很好的了解，你可能会发现查看Keunwoo
    Choi的`melspectogram`（[https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/](https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/)）和Shashank
    Gupta的`NodeEmbeddingLayer`（[http://shashankg7.github.io/2016/10/12/Custom-Layer-In-Keras-Graph-Embedding-Case-Study.html](http://shashankg7.github.io/2016/10/12/Custom-Layer-In-Keras-Graph-Embedding-Case-Study.html)）是很有启发性的。
- en: Generative models
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: Generative models are models that learn to create data similar to data it is
    trained on. We saw one example of a generative model that learns to write prose
    similar to *Alice in Wonderland* in [Chapter 6](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml),
    *Recurrent Neural Network — RNN*. In that example, we trained a model to predict
    the 11th character of text given the first 10 characters. Yet another type of
    generative model is **generative adversarial models** (**GAN**) that have recently
    emerged as a very powerful class of models—you saw examples of GANs in [Chapter
    4](a67ea944-b1a6-48a3-b8aa-4e698166c0eb.xhtml), *Generative Adversarial Networks
    and WaveNet*. The intuition for generative models is that it learns a good internal
    representation of its training data, and is therefore able to generate similar
    data during the *prediction* phase.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是学习创建与训练数据相似的数据的模型。我们在[第6章](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml)中看到过一个生成模型的例子，该模型学习写出类似于*《爱丽丝梦游仙境》*的散文。在这个例子中，我们训练了一个模型，给定前10个字符来预测文本中的第11个字符。另一种生成模型是**生成对抗模型**（**GAN**），这种模型最近成为了一类非常强大的模型——你在[第4章](a67ea944-b1a6-48a3-b8aa-4e698166c0eb.xhtml)中看到了GAN的例子，*生成对抗网络和WaveNet*。生成模型的直觉是，它学习到一个好的内部表示，可以在*预测*阶段生成类似的数据。
- en: Another perspective on generative models is the probabilistic one. A typical
    classification or regression network, also called a discriminative model, learns
    a function that maps the input data *X* to some label or output *y*, that is,
    these models learn the conditional probability *P(y|X)*. On the other hand, a
    generative model learns the joint probability and labels simultaneously, that
    is, *P(x, y)*. This knowledge can then be used to create probable new *(X, y)*
    samples. This gives generative models the ability to explain the underlying structure
    of input data even when there are no labels. This is a very important advantage
    in the real world, since unlabeled data is more abundant than labeled data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率的角度看生成模型，典型的分类或回归网络，也称为判别模型，学习一个函数，将输入数据*X*映射到某个标签或输出*y*，也就是说，这些模型学习条件概率*P(y|X)*。另一方面，生成模型学习联合概率，同时学习输入和标签，即*P(x,
    y)*。然后，这些知识可以用来创建可能的新*(X, y)*样本。这赋予了生成模型在没有标签的情况下也能解释输入数据的潜在结构的能力。这在现实世界中是一个非常重要的优势，因为无标签数据比有标签数据更为丰富。
- en: 'Simple generative models such as the example mentioned above can be extended
    to audio as well, for example, models that learn to generate and play music. One
    interesting one is described in the WaveNet paper (for more information refer
    to: *WaveNet: A Generative Model for Raw Audio*, by A. van den Oord, 2016.) which
    describes a network built using atrous convolutional layers and provides a Keras
    implementation on GithHub ([https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet)).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '像上述示例这样的简单生成模型也可以扩展到音频领域，例如，学习生成和播放音乐的模型。一个有趣的例子在WaveNet论文中描述（更多信息请参考：*WaveNet:
    A Generative Model for Raw Audio*，A. van den Oord，2016），该论文描述了一个使用空洞卷积层构建的网络，并提供了Keras的实现，代码托管在GitHub上（[https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet)）。'
- en: Keras example — deep dreaming
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 示例 — 深度梦境
- en: In this example, we will look at a slightly different generative network. We
    will see how to take a pre-trained convolutional network and use it to generate
    new objects in an image. Networks trained to discriminate between images learn
    enough about the images to generate them as well. This was first demonstrated
    by Alexander Mordvintsev of Google and described in this Google Research blog
    post ([https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)).
    It was originally called *inceptionalism* but the term *deep dreaming* became
    more popular to describe the technique.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将查看一个稍微不同的生成网络。我们将看到如何使用预训练的卷积网络生成图像中的新对象。训练用于区分图像的网络对图像有足够的了解，因此也能够生成图像。这一点最早由Google的Alexander
    Mordvintsev展示，并在Google Research博客中描述（[https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)）。它最初被称为*inceptionalism*，但*深度梦境*这一术语变得更加流行，用来描述这种技术。
- en: Deep dreaming takes the backpropagated gradient activations and adds it back
    to the image, running the same process over and over in a loop. The network optimizes
    the loss function in the process, but we get to see how it does so in the input
    image (three channels) rather than in a high dimensional hidden layer that cannot
    easily be visualized.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 深度梦境将反向传播的梯度激活值加回图像，并不断地重复同一过程。网络在这个过程中优化损失函数，但我们能够看到它如何在输入图像（三个通道）上进行优化，而不是在无法轻易可视化的高维隐藏层上。
- en: There are many variations to this basic strategy, each of which leads to new
    and interesting effects. Some variations are blurring, adding constraints on the
    total activations, decaying the gradient, infinitely zooming into the image by
    cropping and scaling, adding jitter by randomly moving the image around, and so
    on. In our example, we will show the simplest approach—we will optimize the gradient
    of the mean of the selected layer's activation for each of the pooling layers
    of a pre-trained VGG-16 and observe the effect on our input image.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本策略有许多变种，每一种都能带来新的有趣效果。一些变种包括模糊、对总激活值加约束、衰减梯度、通过裁剪和缩放来无限放大图像、通过随机移动图像来增加抖动等等。在我们的示例中，我们将展示最简单的方法——我们将优化选定层的激活值均值的梯度，针对预训练VGG-16的每个池化层，并观察其对输入图像的影响。
- en: 'First, as usual, we will declare our imports:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样，我们将声明我们的导入：
- en: '[PRE42]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next we will load up our input image. This image may be familiar to you from
    blog posts about deep learning. The original image is from here ([https://www.flickr.com/photos/billgarrett-newagecrap/14984990912](https://www.flickr.com/photos/billgarrett-newagecrap/14984990912)):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载我们的输入图像。这张图像可能你在深度学习相关的博客文章中见过。原始图像来自这里（[https://www.flickr.com/photos/billgarrett-newagecrap/14984990912](https://www.flickr.com/photos/billgarrett-newagecrap/14984990912)）：
- en: '[PRE43]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output of the preceding example is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例的输出如下：
- en: '![](img/cat-orig.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cat-orig.png)'
- en: 'Next we define a pair of functions to preprocess and deprocess the image to
    and from a four-dimensional representation suitable for input to a pre-trained
    VGG-16 network:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一对函数，用于将图像预处理和后处理成适合输入预训练VGG-16网络的四维表示：
- en: '[PRE44]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: These two functions are inverses of each other, that is, passing the image through
    `preprocess` and then through `deprocess` will return the original image.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数是互为反函数的，也就是说，将图像通过 `preprocess` 处理后再通过 `deprocess` 处理，将返回原始图像。
- en: 'Next, we load up our pre-trained VGG-16 network. This network has been pre-trained
    on ImageNet data and is available from the Keras distribution. You already learned
    how to work with pre-trained models in [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml),
    *Deep Learning with ConvNets*. We select the version whose fully connected layers
    have been removed already. Apart from saving us the trouble of having to remove
    them ourselves, this also allows us to pass in any shape of image, since the reason
    we need to specify the image width and height in our input is because this determines
    the size of the weight matrices in the fully connected layers. Because CNN transformations
    are local in nature, the size of the image doesn''t affect the sizes of the weight
    matrices for the convolutional and pooling layers. So the only constraint on image
    size is that it must be constant within the batch:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载预训练的VGG-16网络。这个网络已经在ImageNet数据上进行了预训练，并且可以从Keras发行版中获取。你已经在[第3章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)《*深度学习与卷积神经网络*》中学习了如何使用预训练模型。我们选择了那个已经去掉全连接层的版本。除了节省我们自己去移除它们的麻烦外，这还允许我们传入任何形状的图像，因为我们需要指定图像宽度和高度的原因是，这决定了全连接层中权重矩阵的大小。由于CNN变换是局部的，图像的大小不会影响卷积层和池化层权重矩阵的大小。因此，图像大小的唯一限制是它必须在一个batch内保持一致：
- en: '[PRE45]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We will need to refer to the CNN''s layer objects by name in our following
    calculations, so let us construct a dictionary. We also need to understand the
    layer naming convention, so we dump it out:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在接下来的计算中按名称引用CNN的层对象，所以我们来构建一个字典。我们还需要理解层的命名规则，因此我们将其输出：
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output of the preceding example is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例的输出如下：
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We then compute the loss at each of the five pooling layers and compute the
    gradient of the mean activation for three steps each. The gradient is added back
    to the image and the image displayed at each of the pooling layers for each step:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着计算每一层五个池化层的损失，并计算每个步骤的平均激活的梯度。梯度会被加回到图像中，并且在每一步显示在每个池化层的图像上：
- en: '[PRE48]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The resulting images are shown as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像如下所示：
- en: '![](img/cat-pool1.png)![](img/cat-pool2.png)![](img/cat-pool3.png)![](img/cat-pool5.png)![](img/cat-pool6.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cat-pool1.png)![](img/cat-pool2.png)![](img/cat-pool3.png)![](img/cat-pool5.png)![](img/cat-pool6.png)'
- en: As you can see, the process of deep dreaming amplifies the effect of the gradient
    on the chosen layer, resulting in images that are quite surreal. Later layers
    backpropagate gradients that result in more distortion, reflecting their larger
    receptive fields and their capacity to recognize more complex features.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，深度梦境的过程放大了梯度在所选层的效果，结果是产生了非常超现实的图像。后续层反向传播的梯度造成了更多的失真，反映出它们更大的感受野和识别更复杂特征的能力。
- en: 'To convince ourselves that a trained network really learns a representation
    of the various categories of the image it was trained on, let us consider a completely
    random image, shown next, and pass it through the pre-trained network:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们自己确信，训练好的网络确实学到了它所训练的图像类别的表示，我们来看一个完全随机的图像，如下所示，并将其通过预训练网络：
- en: '[PRE49]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output of the preceding example is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例的输出如下：
- en: '![](img/random-noise.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/random-noise.png)'
- en: 'Passing this image through the preceding code results in very specific patterns
    at each layer, as shown next, showing that the network is trying to find a structure
    in the random data:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 将该图像通过前面的代码处理后，在每一层会生成非常特定的模式，如下所示，表明网络试图在随机数据中寻找结构：
- en: '![](img/noise-pool1.png)![](img/noise-pool2.png)![](img/noise-pool3.png)![](img/noise-pool4.png)![](img/noise-pool5.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/noise-pool1.png)![](img/noise-pool2.png)![](img/noise-pool3.png)![](img/noise-pool4.png)![](img/noise-pool5.png)'
- en: 'We can repeat our experiment with the noise image as input and compute the
    loss from a single filter instead of taking the mean across all the filters. The
    filter we choose is for the ImageNet label African elephant (`24`). Thus, we replace
    the value of the loss in the previous code with the following. So instead of computing
    the mean across all filters, we calculate the loss as the output of the filter
    representing the African elephant class:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用噪声图像作为输入重复我们的实验，并从单一的滤波器中计算损失，而不是对所有滤波器的损失求平均。我们选择的滤波器对应ImageNet标签中的非洲象（`24`）。因此，我们将上一段代码中的损失值替换为以下内容。这样，我们就不再对所有滤波器的损失求平均，而是计算代表非洲象类别的滤波器的输出损失：
- en: '[PRE50]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We get back what looks very much like repeating images of the trunk of an elephant
    in the `block4_pool` output, as shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果看起来像是在`block4_pool`输出中重复出现的象鼻图像，如下所示：
- en: '![](img/random-african-elephant.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/random-african-elephant.png)'
- en: Keras example — style transfer
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras示例 — 风格迁移
- en: An extension of deep dreaming was described in this paper (for more information
    refer to: *Image Style Transfer Using Convolutional Neural Networks*, by L. A.
    Gatys, A. S. Ecker, and M. Bethge, Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2016), which showed that trained neural networks,
    such as the VGG-16, learn both content and style, and these two can be manipulated
    independently. Thus an image of an object (content) could be styled to look like
    a painting by combining it with the image of a painting (style).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文描述了深度梦境的扩展（有关更多信息，请参考：*使用卷积神经网络的图像风格迁移*，L. A. Gatys, A. S. Ecker, M. Bethge，IEEE计算机视觉与模式识别会议论文集，2016年），该论文表明，训练过的神经网络（例如VGG-16）可以同时学习内容和风格，并且这两者可以独立操作。因此，物体的图像（内容）可以通过将其与画作的图像（风格）结合，风格化成类似画作的效果。
- en: 'Let us start, as usual, by importing our libraries:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像往常一样，首先导入我们的库：
- en: '[PRE51]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Our example will demonstrate styling our image of a cat with this image of
    a reproduction of Claude Monet''s *The Japanese Bridge* by Rosalind Wheeler ([https://goo.gl/0VXC39](https://goo.gl/0VXC39)):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例将展示如何将我们的猫图像风格化为罗斯林·惠勒（Rosalind Wheeler）创作的克劳德·莫奈（Claude Monet）的《日本桥》（*The
    Japanese Bridge*）的复制品图像 ([https://goo.gl/0VXC39](https://goo.gl/0VXC39))：
- en: '[PRE52]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output of the preceding example is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个示例的输出如下：
- en: '![](img/cat-style.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cat-style.png)'
- en: 'As previously, we declare our two functions to convert back and forth from
    the image and the four-dimensional tensor that the CNN expects:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们声明了两个函数，用于在图像和卷积神经网络所期望的四维张量之间进行转换：
- en: '[PRE53]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We declare tensors to hold the content image and the style image, and another
    tensor to hold the combined image. The content and style images are then concatenated
    into a single input tensor. The input tensor will be fed to the pre-trained VGG-16
    network:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明张量来存储内容图像和风格图像，并声明另一个张量来存储合成图像。然后，内容图像和风格图像被连接成一个单一的输入张量。这个输入张量将被输入到预训练的VGG-16网络中：
- en: '[PRE54]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We instantiate an instance of a pre-trained VGG-16 network, pre-trained with
    the ImageNet data, and with the fully connected layers excluded:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化一个预训练的VGG-16网络实例，该网络使用ImageNet数据进行预训练，并且排除了全连接层：
- en: '[PRE55]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'As previously, we construct a layer dictionary to map the layer name to the
    output layer of the trained VGG-16 network:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们构建一个层字典，将层名称映射到训练好的VGG-16网络的输出层：
- en: '[PRE56]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The next block defines the code for computing the `content_loss`, the `style_loss`,
    and the `variational_loss`. Finally, we define our loss as a linear combination
    of these three losses:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个模块定义了计算`content_loss`、`style_loss`和`variational_loss`的代码。最后，我们将损失定义为这三种损失的线性组合：
- en: '[PRE57]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here the content loss is the root mean square distance (also known as **L2 distance**)
    between the features of the content image extracted from the target layer and
    the combination image. Minimizing this has the effect of keeping the styled image
    close to the original one.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，内容损失是目标层提取的内容图像特征与组合图像之间的均方根距离（也称为**L2 距离**）。最小化此损失有助于使风格化图像保持接近原始图像。
- en: The style loss is the L2 distance between the gram matrices of the base image
    representation and the style image. A gram matrix of a matrix *M* is the transpose
    of *M* multiplied by *M*, that is, *MT * M*. This loss measures how often features
    appear together in the content image representation and the style image. One practical
    implication of this is that the content and style matrices must be square.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 风格损失是基本图像表示和风格图像的Gram矩阵之间的L2距离。矩阵*M*的Gram矩阵是*M*的转置与*M*的乘积，即*MT * M*。这个损失度量了在内容图像表示和风格图像中，特征如何一起出现。一个实际的含义是，内容和风格矩阵必须是方阵。
- en: The total variation loss measures the difference between neighboring pixels.
    Minimizing this has the effect that neighboring pixels will be similar so the
    final image is smooth rather than *jumpy*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差损失度量了相邻像素之间的差异。最小化此损失有助于使相邻像素相似，从而使最终图像平滑而不是*跳跃*。
- en: 'We calculate the gradient and the loss function, and run our network in reverse
    for five iterations:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算梯度和损失函数，并反向运行网络五次：
- en: '[PRE58]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output from the last two iterations is shown as follows. As you can see,
    it has picked up the impressionistic fuzziness and even the texture of the canvas
    in the final images:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两次迭代的输出如下所示。正如你所看到的，它已经捕捉到了印象派模糊感，甚至是画布的纹理，出现在最终的图像中：
- en: '![](img/cat-style-epoch4.png)![](img/cat-style-epoch5.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cat-style-epoch4.png)![](img/cat-style-epoch5.png)'
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered some deep learning networks that were not covered
    in earlier chapters. We started with a brief look into the Keras functional API,
    which allows us to build networks that are more complex than the sequential networks
    we have seen so far. We then looked at regression networks, which allow us to
    do predictions in a continuous space, and opens up a whole new range of problems
    we can solve. However, a regression network is really a very simple modification
    of a standard classification network. The next area we looked at was autoencoders,
    which are a style of network that allows us to do unsupervised learning and make
    use of the massive amount of unlabeled data that all of us have access to nowadays.
    We also learned how to compose the networks we had already learned about as giant
    Lego-like building blocks into larger and more interesting networks. We then moved
    from building large networks using smaller networks, to learning how to customize
    individual layers in a network using the Keras backend layer. Finally, we looked
    at generative models, another class of models that learn to mimic the input it
    is trained on, and looked at some novel uses for this kind of model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了一些之前章节没有涉及的深度学习网络。我们首先简要了解了 Keras 函数式 API，它允许我们构建比迄今为止看到的顺序网络更复杂的网络。接着，我们研究了回归网络，它允许我们在连续空间中进行预测，并开启了我们可以解决的全新问题。然而，回归网络实际上只是标准分类网络的一种非常简单的修改。接下来我们讨论了自编码器，它是一种网络风格，允许我们进行无监督学习，并利用如今所有人都可以访问的大量未标记数据。我们还学习了如何将已经学过的网络像巨大的乐高积木一样组合成更大、更有趣的网络。然后，我们从使用较小的网络构建大型网络转向学习如何使用
    Keras 后端层定制网络中的单个层。最后，我们研究了生成模型，这是另一类模型，能够学习模仿其训练的输入，并探讨了这种模型的一些新颖应用。
- en: In the next chapter, we will turn our attention to another learning style called
    reinforcement learning, and explore its concepts by building and training a network
    in Keras to play a simple computer game.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将把注意力转向另一种学习方式——强化学习，并通过在 Keras 中构建和训练网络来玩一个简单的计算机游戏，从而探索其概念。
