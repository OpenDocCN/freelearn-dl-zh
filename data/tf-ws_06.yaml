- en: 6\. Regularization and Hyperparameter Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 正则化与超参数调整
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you will be introduced to hyperparameter tuning. You will get
    hands-on experience in using TensorFlow to perform regularization on deep learning
    models to reduce overfitting. You will explore concepts such as L1, L2, and dropout
    regularization. Finally, you will look at the Keras Tuner package for performing
    automatic hyperparameter tuning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解超参数调整。您将通过使用TensorFlow在深度学习模型上执行正则化来获得实际经验，以减少过拟合。您将探讨L1、L2和dropout正则化等概念。最后，您将介绍Keras
    Tuner包，用于执行自动超参数调整。
- en: By the end of the chapter, you will be able to apply regularization and tune
    hyperparameters in order to reduce the risk of overfitting your model and improve
    its performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够应用正则化和调整超参数，以减少过拟合模型的风险并提高其性能。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the previous chapter, you learned how classification models can solve problems
    when the response variable is discrete. You also saw different metrics used to
    assess the performance of such classifiers. You got hands-on experience in building
    and training binary, multi-class, and multi-label classifiers with TensorFlow.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了分类模型如何在响应变量是离散的情况下解决问题。您还看到了用于评估此类分类器性能的不同指标。您通过使用TensorFlow构建和训练二进制、多类别和多标签分类器获得了实际经验。
- en: 'When evaluating a model, you will face three different situations: model overfitting,
    model underfitting, and model performing. The last one is the ideal scenario,
    in which a model is accurately predicting the right outcome and is generalizing
    to unseen data well.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，您将面临三种不同的情况：模型过拟合、模型欠拟合和模型表现良好。最后一种情况是理想的，即模型准确地预测正确结果并且能够很好地泛化到未见数据上。
- en: If a model is underfitting, it means it is neither achieving satisfactory performance
    nor accurately predicting the target variable. In this case, a data scientist
    can try tuning different hyperparameters and finding the best combination that
    will boost the accuracy of the model. Another possibility is to improve the input
    dataset by handling issues such as the cleanliness of the data or feature engineering.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型欠拟合，意味着它既没有达到令人满意的性能，也没有准确预测目标变量。在这种情况下，数据科学家可以尝试调整不同的超参数，并找到最佳组合来提升模型的准确性。另一种可能性是改进输入数据集，处理诸如数据清洁度或特征工程等问题。
- en: A model is overfitting when it can only achieve high performance on the training
    set and performs poorly on the test set. In this case, the model has only learned
    patterns from the data relevant to the data used for training. Regularization
    helps to lower the risk of overfitting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型只能在训练集上表现出色，并在测试集上表现不佳时，我们称其为过拟合。在这种情况下，模型只学习了与训练数据相关的数据模式。正则化有助于降低过拟合的风险。
- en: Regularization Techniques
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化技术
- en: The main goal of a data scientist is to train a model that achieves high performance
    and generalizes to unseen data well. The model should be able to predict the right
    outcome on both data used during the training process and new data. This is the
    reason why a model is always assessed on the test set. This set of data serves
    as a proxy to evaluate the ability of the model to output correct results while
    in production.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家的主要目标是训练一个在训练过程中能够实现高性能并且在未见数据上能够泛化良好的模型。该模型应能够在训练过程中使用的数据和新数据上预测正确的结果。这也是为什么模型总是在测试集上进行评估的原因。这一组数据用作评估模型在生产中输出正确结果能力的代理。
- en: '![Figure 6.1: Model not overfitting or underfitting'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.1: 模型既不过拟合也不欠拟合'
- en: '](img/B16341_06_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_01.jpg)'
- en: 'Figure 6.1: Model not overfitting or underfitting'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.1: 模型既不过拟合也不欠拟合'
- en: In *Figure 6.1*, the linear model (line) seems to predict relatively accurate
    results for both the training (circles) and test (triangles) sets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Figure 6.1* 中，线性模型（线）似乎能够相对准确地预测训练集（圆圈）和测试集（三角形）的结果。
- en: But sometimes a model fails to generalize well and will overfit the training
    set. In this case, the performance of the model will be very different between
    the training and test sets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时模型无法很好地泛化，会对训练集过拟合。在这种情况下，模型在训练集和测试集上的表现会有很大的差异。
- en: '![Figure 6.2: Model overfitting'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.2: 模型过拟合'
- en: '](img/B16341_06_02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_02.jpg)'
- en: 'Figure 6.2: Model overfitting'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.2: 模型过拟合'
- en: '*Figure 6.2* shows the model (line) has only learned to predict accurately
    for the training set (circles) and is performing badly on the test set (triangles).
    This model is clearly overfitting.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.2* 显示模型（线条）只学会了对训练集（圆圈）进行准确预测，并且在测试集（三角形）上的表现很差。这个模型显然是过拟合的。'
- en: Fortunately, there are **regularization techniques** that a data scientist can
    use to reduce and prevent overfitting, defined in the following sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些**正则化技术**，数据科学家可以利用这些技术来减少并防止过拟合，相关内容将在以下章节中定义。
- en: L1 Regularization
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 正则化
- en: For deep learning models, overfitting happens when some of the features have
    higher weights than they should. The model puts too much emphasis on these features
    as it believes they are extremely important for predicting the training set. Unfortunately,
    these features are less relevant for the test set or any new unseen data. Regularization
    techniques try to penalize such weights and reduce their importance to the model
    predictions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习模型，当某些特征的权重高于它们应有的值时，就会发生过拟合。模型过于强调这些特征，因为它认为它们对预测训练集非常重要。不幸的是，这些特征对测试集或任何新数据的相关性较低。正则化技术会惩罚这些权重并减少它们对模型预测的影响。
- en: 'There are multiple ways to perform regularization. One of them is to add a
    regularization component to the cost function:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化有多种方式。其中一种方式是将正则化组件添加到成本函数中：
- en: '![Figure 6.3: Adding a regularization component to the cost function'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.3: 将正则化组件添加到成本函数'
- en: '](img/B16341_06_03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_03.jpg)'
- en: 'Figure 6.3: Adding a regularization component to the cost function'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.3: 将正则化组件添加到成本函数'
- en: The addition of this regularization component will lead the weights of the model
    to be smaller as neural networks try to reduce the cost function while performing
    forward and backward propagations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 添加这个正则化组件会导致模型的权重变小，因为神经网络在进行前向和反向传播时会尽量减少成本函数的值。
- en: 'One very popular regularization component is L1\. Its formula is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常流行的正则化组件是 L1。它的公式如下：
- en: '![Figure 6.4: L1 regularization'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4: L1 正则化'
- en: '](img/B16341_06_04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_04.jpg)'
- en: 'Figure 6.4: L1 regularization'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.4: L1 正则化'
- en: '![Formula](img/B16341_06_04a.png) is a hyperparameter that defines the level
    of penalization of the L1 regularization. `W` is the weight of the model. With
    L1 regularization, you add the sum of the absolute value of the weights to the
    model loss.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![公式](img/B16341_06_04a.png) 是一个超参数，用于定义 L1 正则化的惩罚程度。`W` 是模型的权重。使用 L1 正则化时，你将权重的绝对值之和加到模型的损失中。'
- en: L1 regularization is sometimes referred to as `0`. Therefore, only the relevant
    features are used for making predictions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化有时被称为 `0`。因此，只有相关特征用于进行预测。
- en: 'In TensorFlow, you can define L1 regularization with the following code snippet:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以使用以下代码片段定义 L1 正则化：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `l` parameter corresponds to the ![Formula 2](img/B16341_06_04b.png) hyperparameter.
    The instantiated L1 regularization can then be added to any layer from TensorFlow
    Keras:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`l` 参数对应于 ![公式 2](img/B16341_06_04b.png) 超参数。实例化的 L1 正则化可以被添加到 TensorFlow Keras
    的任何层中：'
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding example, you added the L1 regularizer that you defined earlier
    to a fully connected layer of `10` units.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，你将之前定义的 L1 正则化器添加到了一个具有 `10` 个单元的全连接层中。
- en: L2 Regularization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2 正则化
- en: '*L2* regularization is similar to *L1* in that it adds a regularization component
    to the cost function, but its formula is different:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*L2* 正则化与 *L1* 相似，都将正则化组件添加到成本函数中，但它们的公式不同：'
- en: '![Figure 6.5: L2 regularization'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5: L2 正则化'
- en: '](img/B16341_06_05.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_05.jpg)'
- en: 'Figure 6.5: L2 regularization'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.5: L2 正则化'
- en: L2 regularization tends to decrease the weights of the non-relevant features.
    They will be close to `0`, but not exactly `0`. So, it reduces the impact of these
    features but does not disable them as L1 does.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化倾向于减少不相关特征的权重。它们会接近 `0`，但不会完全为 `0`。因此，它减少了这些特征的影响，但不像 L1 那样完全禁用它们。
- en: 'In TensorFlow, you can define L2 regularization as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以按以下方式定义 L2 正则化：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding example, you defined an L2 regularizer and added it to a fully
    connected layer of `20` units.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，你定义了一个 L2 正则化器并将其添加到一个具有 `20` 个单元的全连接层中。
- en: 'TensorFlow provides another regularizer class that combines both L1 and L2
    regularizers. You can instantiate it with the following code snippet:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了另一个正则化器类，它结合了 L1 和 L2 正则化器。你可以使用以下代码片段实例化它：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding example, you instantiated L1 and L2 regularizers and specified
    the factors for L1 and L2 as `0.01` and `0.001`, respectively. You can observe
    that more weights are put on the L1 regularization compared to L2\. These values
    are hyperparameters that can be fine-tuned depending on the dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，您实例化了 L1 和 L2 正则化器，并将 L1 和 L2 的系数分别指定为 `0.01` 和 `0.001`。您可以观察到，相比 L2，L1
    正则化的权重较大。这些值是超参数，可以根据数据集进行微调。
- en: In the next exercise, you will put this into practice as you apply L2 regularization
    to a model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，您将通过对模型应用 L2 正则化来实践这一点。
- en: 'Exercise 6.01: Predicting a Connect-4 Game Outcome Using the L2 Regularizer'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '练习 6.01: 使用 L2 正则化器预测 Connect-4 游戏结果'
- en: In this exercise, you will build and train two multi-class models in TensorFlow
    that will predict the class outcome for player one in the game Connect-4\.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，您将使用 TensorFlow 构建并训练两个多分类模型，以预测 Connect-4 游戏中玩家一的结果。
- en: 'Each observation of this dataset contains different situations of the game
    with different positions. For each of these situations, the model tries to predict
    the outcome for the first player: win, loss, or draw. The first model will not
    have any regularization, while the second will have L2 regularization:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的每一行包含不同游戏局面的情况和不同的棋盘位置。对于每个局面，模型尝试预测第一个玩家的结果：胜、负或平。第一个模型没有任何正则化，而第二个模型应用了
    L2 正则化：
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be accessed here: [https://packt.link/xysRc](https://packt.link/xysRc).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过以下链接访问：[https://packt.link/xysRc](https://packt.link/xysRc)。
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以在以下位置找到：[http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4)。
- en: Open a new Jupyter notebook.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook。
- en: 'Import the pandas library and use `pd` as the alias:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 pandas 库，并使用 `pd` 作为别名：
- en: '[PRE4]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a variable called `file_url` that contains the URL to the dataset:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `file_url` 的变量，其中包含数据集的 URL：
- en: '[PRE5]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Load the dataset into a DataFrame called `data` using the `read_csv()` function
    and provide the URL to the CSV file. Print the first five rows using the `head()` function:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `read_csv()` 函数将数据集加载到名为 `data` 的 DataFrame 中，并提供 CSV 文件的 URL。使用 `head()`
    函数打印前五行：
- en: '[PRE6]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The expected output will be as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出将如下所示：
- en: '![Figure 6.6: First five rows of the dataset'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.6: 数据集的前五行](img/B16341_06_06.jpg)'
- en: '](img/B16341_06_06.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_06.jpg)'
- en: 'Figure 6.6: First five rows of the dataset'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6.6: 数据集的前五行'
- en: The preceding figure shows the first five rows of the dataset.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上图展示了数据集的前五行。
- en: 'Extract the target variable (the `class` column) using the `pop()` method and
    save it in a variable named `target`:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pop()` 方法提取目标变量（`class` 列），并将其保存为名为 `target` 的变量：
- en: '[PRE7]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Import the TensorFlow library and use `tf` as the alias. Then, import the `Dense`
    class from `tensorflow.keras.layers`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库，并使用 `tf` 作为别名。然后，从 `tensorflow.keras.layers` 中导入 `Dense` 类：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Set the seed as `8` to get reproducible results:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置种子为 `8`，以获得可重复的结果：
- en: '[PRE9]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Instantiate a sequential model using `tf.keras.Sequential()` and store it in
    a variable called `model`:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tf.keras.Sequential()` 实例化一个顺序模型，并将其存储在名为 `model` 的变量中：
- en: '[PRE10]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create a fully connected layer of `512` units with `Dense()` and specify ReLu
    as the activation function and the input shape as `(42,)`, which corresponds to
    the number of features from the dataset. Save it in a variable called `fc1`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Dense()` 创建一个包含 `512` 个单元的全连接层，并指定 ReLu 作为激活函数，输入形状为 `(42,)`，对应数据集中的特征数量。将其保存为名为
    `fc1` 的变量：
- en: '[PRE11]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create three fully connected layers of `512`, `128`, and `128` units with `Dense()`
    and specify ReLu as the activation function. Save them in three variables, called
    `fc2`, `fc3`, and `fc4`, respectively:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Dense()` 创建三个全连接层，分别包含 `512`、`128` 和 `128` 个单元，并指定 ReLu 作为激活函数。将它们分别保存为
    `fc2`、`fc3` 和 `fc4` 变量：
- en: '[PRE12]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a fully connected layer of three units (corresponding to the number
    of classes) with `Dense()` and specify softmax as the activation function. Save
    it in a variable called `fc5`:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Dense()` 创建一个包含三个单元的全连接层（对应类别的数量），并指定 softmax 作为激活函数。将其保存为名为 `fc5` 的变量：
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Sequentially add all five fully connected layers to the model using the `add()` method:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `add()` 方法按顺序将所有五个全连接层添加到模型中：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Print the summary of the model using the `summary()` method:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `summary()` 方法打印模型的摘要：
- en: '[PRE15]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The expected output will be as follows:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出将如下所示：
- en: '![Figure 6.7: Summary of the model architecture'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.7: 模型架构摘要](img/B16341_06_06.jpg)'
- en: '](img/B16341_06_07.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_07.jpg)'
- en: 'Figure 6.7: Summary of the model architecture'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.7：模型架构总结
- en: 'Instantiate a `SparseCategoricalCrossentropy()` function from `tf.keras.losses`
    and save it in a variable called `loss`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tf.keras.losses`实例化`SparseCategoricalCrossentropy()`函数，并将其保存为名为`loss`的变量：
- en: '[PRE16]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Instantiate `Adam()` from `tf.keras.optimizers` with `0.001` as the learning
    rate and save it in a variable called `optimizer`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tf.keras.optimizers`实例化`Adam()`，将学习率设置为`0.001`并保存为名为`optimizer`的变量：
- en: '[PRE17]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Compile the model using the `compile()` method, and specify the optimizer and
    loss you created in *steps 14* and *15* and `accuracy` as the metric to be displayed:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compile()`方法编译模型，并指定你在*步骤 14*和*步骤 15*中创建的优化器和损失函数，以及`accuracy`作为显示的度量指标：
- en: '[PRE18]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Start the model training process using the `fit()` method for five epochs and
    split the data into a validation set with 20% of the data:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法启动模型训练过程，训练五个周期，并将数据拆分为包含 20% 数据的验证集：
- en: '[PRE19]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The expected output will be as follows:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '![Figure 6.8: Logs of the training process'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.8：训练过程的日志'
- en: '](img/B16341_06_08.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_08.jpg)'
- en: 'Figure 6.8: Logs of the training process'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.8：训练过程的日志
- en: The preceding output reveals that the model is overfitting. It achieved an accuracy
    score of `0.85` on the training set and only `0.58` on the validation set. Now,
    train another model with L2 regularization.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的输出显示模型出现了过拟合。它在训练集上的准确度为`0.85`，但在验证集上的准确度仅为`0.58`。现在，训练另一个带有 L2 正则化的模型。
- en: 'Create five fully connected layers similar to the previous model''s and specify
    the L2 regularizer for the `kernel_regularizer` parameters. Use the value `0.001`
    for the regularizer factor. Save the layers in five variables, called `reg_fc1`,
    `reg_fc2`, `reg_fc3`, `reg_fc4`, and `reg_fc5`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建五个完全连接的层，类似于先前模型的结构，并为`kernel_regularizer`参数指定 L2 正则化器。将正则化器因子设置为`0.001`。将这些层保存在五个变量中，分别命名为`reg_fc1`、`reg_fc2`、`reg_fc3`、`reg_fc4`和`reg_fc5`：
- en: '[PRE20]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Instantiate a sequential model using `tf.keras.Sequential()`, store it in a
    variable called `model2`, and add sequentially all five fully connected layers
    to the model using the `add()` method:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.keras.Sequential()`实例化一个顺序模型，将其保存在名为`model2`的变量中，并使用`add()`方法按顺序将所有五个完全连接的层添加到模型中：
- en: '[PRE21]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Print the summary of the model:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型摘要：
- en: '[PRE22]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The expected output will be as follows:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '![Figure 6.9: Summary of the model architecture'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.9：模型架构总结'
- en: '](img/B16341_06_09.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_09.jpg)'
- en: 'Figure 6.9: Summary of the model architecture'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.9：模型架构总结
- en: 'Compile the model using the `compile()` method, and specify the optimizer and
    loss you created in *steps 14* and *15* and `accuracy` as the metric to be displayed:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compile()`方法编译模型，并指定你在*步骤 14*和*步骤 15*中创建的优化器和损失函数，以及`accuracy`作为显示的度量指标：
- en: '[PRE23]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Start the model training process using the `fit()` method for five epochs and
    split the data into a validation set with 20% of the data:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法启动模型训练过程，训练五个周期，并将数据拆分为包含 20% 数据的验证集：
- en: '[PRE24]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The expected output will be as follows:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '![Figure 6.10: Logs of the training process'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.10：训练过程的日志'
- en: '](img/B16341_06_10.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_10.jpg)'
- en: 'Figure 6.10: Logs of the training process'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：训练过程的日志
- en: With the addition of L2 regularization, the model now has similar accuracy scores
    between the training (`0.68`) and test (`0.58`) sets. The model is not overfitting
    as much as before, but its performance is not great.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在加入 L2 正则化后，模型在训练集（`0.68`）和测试集（`0.58`）上的准确度差异较小。模型不再像之前那样过拟合，但其性能仍不理想。
- en: Now that you know how to apply L1 and L2 regularization to neural networks,
    the next section will introduce another regularization technique, called **dropout**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何将 L1 和 L2 正则化应用于神经网络，下一部分将介绍另一种正则化技术，称为**丢弃法（dropout）**。
- en: Dropout Regularization
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 丢弃法正则化
- en: 'Unlike L1 and L2 regularization, dropout is a regularization technique specific
    to neural networks. The logic behind it is very simple: the networks will randomly
    change the weights of some features to `0`. This will force the model to rely
    on other features that would have been ignored and, therefore, bump up their weights.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与 L1 和 L2 正则化不同，丢弃法是专门针对神经网络的正则化技术。其背后的逻辑非常简单：网络将随机将某些特征的权重置为`0`。这将迫使模型依赖于其他本应被忽视的特征，从而提高它们的权重。
- en: '![Figure 6.11: Dropout of neural networks'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.11：神经网络的丢弃法'
- en: '](img/B16341_06_11.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_11.jpg)'
- en: 'Figure 6.11: Dropout of neural networks'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11：神经网络的丢弃法
- en: 'The preceding example shows an architecture with a dropout of 50%. This means
    that 50% of the units of the model are turned off at each iteration. The following
    code snippet shows you how to create a dropout layer of 50% in TensorFlow:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了一个dropout为50%的架构。这意味着在每次迭代中，模型的50%的单元被关闭。以下代码片段展示了如何在TensorFlow中创建一个50%
    dropout层：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the next exercise, you will extend the previous model by applying dropout.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，你将通过应用dropout扩展前面的模型。
- en: 'Exercise 6.02: Predicting a Connect-4 Game Outcome Using Dropout'
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.02：使用Dropout预测Connect-4游戏结果
- en: 'In this exercise, you will be using the same dataset as for *Exercise 6.01*,
    *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*. You will build
    and train a multi-class model in TensorFlow that will predict the class outcome
    for player 1 in the game Connect-4 using the dropout technique as a regularizer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将使用与*练习6.01*相同的数据集，*使用L2正则化器预测Connect-4游戏结果*。你将使用dropout技术作为正则化器，在TensorFlow中构建并训练一个多类别模型，预测Connect-4游戏中玩家1的类别结果：
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: 'The dataset can be accessed here: [https://packt.link/0Bo1B](https://packt.link/0Bo1B).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在此访问：[https://packt.link/0Bo1B](https://packt.link/0Bo1B)。
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以在此找到：[http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4)。
- en: Open a new Jupyter notebook.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter notebook。
- en: 'Import the pandas library and use `pd` as the alias:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入pandas库并使用`pd`作为别名：
- en: '[PRE26]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a variable, `file_url`, to store the URL of the dataset:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个变量`file_url`，用于存储数据集的URL：
- en: '[PRE27]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Load the dataset into a DataFrame, `data`, using the `read_csv()` function
    and provide the URL of the CSV file. Print the first five rows using the `head()` function:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`read_csv()`函数将数据集加载到一个DataFrame `data`中，并提供CSV文件的URL。使用`head()`函数打印前五行：
- en: '[PRE28]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The expected output will be as follows:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望的输出如下所示：
- en: '![Figure 6.12: First five rows of the dataset'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.12：数据集的前五行'
- en: '](img/B16341_06_12.jpg)'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_12.jpg)'
- en: 'Figure 6.12: First five rows of the dataset'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12：数据集的前五行
- en: 'Extract the target variable (the column called `class`) using the `pop()` method,
    and save it in a variable called `target`:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pop()`方法提取目标变量（名为`class`的列），并将其保存到一个名为`target`的变量中：
- en: '[PRE29]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Import the TensorFlow library and use `tf` as the alias. Then, import the `Dense`
    class from `tensorflow.keras.layers`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入TensorFlow库并使用`tf`作为别名。然后，从`tensorflow.keras.layers`导入`Dense`类：
- en: '[PRE30]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Set the seed as `8` to get reproducible results:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将种子设置为`8`，以获得可重复的结果：
- en: '[PRE31]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Instantiate a sequential model using `tf.keras.Sequential()` and store it in
    a variable called `model`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.keras.Sequential()`实例化一个顺序模型，并将其存储在一个名为`model`的变量中：
- en: '[PRE32]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a fully connected layer of `512` units with `Dense()` and specify ReLu
    as the activation function and the input shape as `(42,)`, which corresponds to
    the number of features from the dataset. Save it in a variable called `fc1`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`512`个单元的全连接层，使用`Dense()`并指定ReLu作为激活函数，输入形状为`(42,)`，对应数据集中的特征数量。将其保存为一个名为`fc1`的变量：
- en: '[PRE33]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create three fully connected layers of `512`, `128`, and `128` units with `Dense()`
    and specify ReLu as the activation function. Save them in three variables, called
    `fc2`, `fc3`, and `fc4`, respectively:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个全连接层，分别为`512`、`128`和`128`个单元，使用`Dense()`并指定ReLu作为激活函数。将它们分别保存为三个变量，命名为`fc2`、`fc3`和`fc4`：
- en: '[PRE34]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create a fully connected layer of three units (corresponding to the number
    of classes) with `Dense()` and specify softmax as the activation function. Save
    it in a variable called `fc5`:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有三个单元（对应类别数）的全连接层，使用`Dense()`并指定softmax作为激活函数。将其保存为一个名为`fc5`的变量：
- en: '[PRE35]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Sequentially add all five fully connected layers to the model with a dropout
    layer of `0.75` in between each of them using the `add()` method:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 顺序地将所有五个全连接层添加到模型中，每个层之间插入一个`0.75`的dropout层，使用`add()`方法：
- en: '[PRE36]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Print the summary of the model:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型的摘要：
- en: '[PRE37]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The expected output will be as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望的输出如下所示：
- en: '![Figure 6.13: Summary of the model architecture'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.13：模型架构总结'
- en: '](img/B16341_06_13.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_13.jpg)'
- en: 'Figure 6.13: Summary of the model architecture'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.13：模型架构总结
- en: 'Instantiate a `SparseCategoricalCrossentropy()` function from `tf.keras.losses`
    and save it in a variable called `loss`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tf.keras.losses`实例化一个`SparseCategoricalCrossentropy()`函数，并将其保存为一个名为`loss`的变量：
- en: '[PRE38]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Instantiate `Adam()` from `tf.keras.optimizers` with `0.001` as the learning
    rate and save it in a variable called `optimizer`:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tf.keras.optimizers`中实例化`Adam()`，学习率设置为`0.001`，并将其保存在名为`optimizer`的变量中：
- en: '[PRE39]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Compile the model using the `compile()` method, specify the optimizer and loss,
    and set `accuracy` as the metric to be displayed:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`compile()`方法编译模型，指定优化器和损失函数，并设置`accuracy`为要显示的指标：
- en: '[PRE40]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Start the model training process using the `fit()` method for five epochs and
    split the data into a validation set with 20% of the data:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法开始模型训练过程，进行五个epoch，并将数据分成20%的验证集：
- en: '[PRE41]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output will be as follows:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 6.14: Logs of the training process'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.14：训练过程的日志'
- en: '](img/B16341_06_14.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_14.jpg)'
- en: 'Figure 6.14: Logs of the training process'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：训练过程的日志
- en: With the addition of dropout, the model now has similar accuracy scores between
    the training (`0.69`) and test (`0.59`) sets. The model is not overfitting as
    much as before, but its performance is still less than ideal.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在加入dropout后，模型在训练集（`0.69`）和测试集（`0.59`）之间的准确度分数变得相似。模型的过拟合程度不如之前严重，但其性能仍然不理想。
- en: You have now seen how to apply L1, L2, or dropout as regularizers for a model.
    In deep learning, there is another very simple technique that you can apply to
    avoid overfitting—that is, early stopping.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何将L1、L2或dropout作为模型的正则化方法。在深度学习中，还有一种非常简单的技术可以避免过拟合，那就是提前停止。
- en: Early Stopping
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止
- en: Another reason why neural networks overfit is due to the training process. The
    more you train the model, the more it will try to improve its performance. By
    training the model for a longer duration (more epochs), it will at some point
    start finding patterns that are only relevant to the training set. In such a case,
    the difference between the scores of the training and test (or validation) sets
    will start increasing after a certain number of epochs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络过拟合的另一个原因是训练过程。你训练模型的时间越长，模型就越会试图提升其性能。通过训练更长的时间（更多的epoch），它最终会开始找到只与训练集相关的模式。在这种情况下，训练集和测试集（或验证集）之间的得分差异将在一定数量的epoch后开始增大。
- en: To prevent this situation, you can stop the model training when the difference
    between the two sets starts to increase. This technique is called **early stopping**.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况发生，当两个数据集之间的差距开始增大时，你可以停止模型训练。这个技术叫做**提前停止**。
- en: '![Figure 6.15: Early stopping to prevent overfitting'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15：通过提前停止来防止过拟合'
- en: '](img/B16341_06_15.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_15.jpg)'
- en: 'Figure 6.15: Early stopping to prevent overfitting'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：通过提前停止来防止过拟合
- en: The preceding graph shows the loss value of a model on the training and test
    (or validation) sets according to the number of epochs. In early epochs, the loss
    value is quite different between the two sets. As the training goes on, the models
    start learning the relevant patterns for making predictions and both losses converge.
    But after a while, they start diverging. The loss of the training set keeps decreasing
    while the one for the test (or validation) set is increasing. You can observe
    that the model is overfitting and is optimizing only for the training set. Stopping
    the training at the point when the difference between the two losses starts to
    increase prevents the model from overfitting.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了一个模型在训练集和测试集（或验证集）上的损失值变化，随着epoch的增加。在早期的epoch中，两个数据集的损失值相差较大。随着训练的进行，模型开始学习到预测相关的模式，并且两者的损失逐渐收敛。但过了一段时间，它们开始分歧。训练集的损失不断下降，而测试集（或验证集）的损失则在增加。你可以观察到模型出现了过拟合，并且只在优化训练集。停止训练，正好在两个损失开始增大时，可以防止模型过拟合。
- en: 'In TensorFlow, you can achieve this by setting up callbacks that analyze the
    performance of the models at each epoch and compare its score between the training
    and test sets. To define an early stopping callback, you will do the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，你可以通过设置回调函数来实现这一点，回调函数会分析每个epoch结束时模型的表现，并比较训练集和测试集之间的得分。要定义提前停止回调函数，你需要做如下操作：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The preceding code shows you how to instantiate an `EarlyStopping` class that
    will monitor the accuracy score of the validation set and wait for five successive
    epochs with no improvement before stopping the training process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码展示了如何实例化一个`EarlyStopping`类，该类会监控验证集的准确度得分，并在连续五个epoch内没有改进的情况下停止训练过程。
- en: In the next activity, you will practice applying both L1 and L2 regularization
    to a model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，你将练习将L1和L2正则化应用于模型。
- en: 'Activity 6.01: Predicting Income with L1 and L2 Regularizers'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.01：使用L1和L2正则化器预测收入
- en: 'The `census-income-train.csv` dataset contains weighted census data extracted
    from the 1994 and 1995 current population surveys conducted by the US Census Bureau.
    The dataset is the subset of the original dataset shared by the US Census Bureau.
    In this activity, you are tasked with building and training a regressor to predict
    the income of a person based on their census data. The dataset can be accessed
    here: [https://packt.link/G8xFd](https://packt.link/G8xFd).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`census-income-train.csv`数据集包含从美国人口普查局进行的1994年和1995年现有人口调查中提取的加权人口普查数据。该数据集是美国人口普查局共享的原始数据集的子集。在本次活动中，您的任务是构建并训练一个回归模型，根据个人的人口普查数据预测收入。可以通过以下链接访问数据集：[https://packt.link/G8xFd](https://packt.link/G8xFd)。'
- en: 'The following steps will help you to complete the activity:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助您完成活动：
- en: Open a new Jupyter notebook.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter笔记本。
- en: Import the required libraries.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: Create a list called `usecols` containing the column names `AAGE`, `ADTIND`,
    `ADTOCC`, `SEOTR`, `WKSWORK`, and `PTOTVAL`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`usecols`的列表，包含列名`AAGE`、`ADTIND`、`ADTOCC`、`SEOTR`、`WKSWORK`和`PTOTVAL`。
- en: Load the data using the `read_csv()` method.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`read_csv()`方法加载数据。
- en: Split the data into training (the first 15,000 rows) and test (the last 5,000
    rows) sets.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集（前15,000行）和测试集（最后5,000行）。
- en: Build the multi-class classifier with five fully connected layers of, respectively,
    `512`, `512`, `128`, `128`, and `26` units.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个多类分类器，该分类器具有五个全连接层，分别包含`512`、`512`、`128`、`128`和`26`个单元。
- en: Train the model on the training set.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练模型。
- en: 'The expected output will be as follows:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出结果如下：
- en: '![Figure 6.16: Logs of the training process'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.16：训练过程的日志](img/B16341_06_16.jpg)'
- en: '](img/B16341_06_16.jpg)'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_16.jpg)'
- en: 'Figure 6.16: Logs of the training process'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16：训练过程的日志
- en: Note
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor269).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor269)找到。
- en: In the section ahead, you will see how to tune hyperparameters to achieve better results.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，您将看到如何调整超参数以获得更好的结果。
- en: Hyperparameter Tuning
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Previously, you saw how to deal with a model that is overfitting by using different
    regularization techniques. These techniques help the model to better generalize
    to unseen data but, as you have seen, they can also lead to inferior performance
    and make the model underfit.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，您已经了解了如何通过使用不同的正则化技术来处理过拟合模型。这些技术帮助模型更好地泛化到未见过的数据，但正如您所看到的，它们也可能导致性能下降，使得模型欠拟合。
- en: With neural networks, data scientists have access to different hyperparameters
    they can tune to improve the performance of a model. For example, you can try
    different learning rates and see whether one leads to better results, you can
    try different numbers of units for each hidden layer of a network, or you can
    test to see whether different ratios of dropout can achieve a better trade-off
    between overfitting and underfitting.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络时，数据科学家可以访问不同的超参数，调整它们以改善模型的表现。例如，您可以尝试不同的学习率，看看是否有一个学习率能获得更好的结果，您可以尝试每个隐藏层的不同单元数，或者您可以测试不同的dropout比例，看是否能够在过拟合和欠拟合之间实现更好的平衡。
- en: 'However, the choice of one hyperparameter can impact the effect of another
    one. So, as the number of hyperparameters and values you want to tune grows, the
    number of combinations to be tested will increase exponentially. It will also
    take a lot of time to train models for all these combinations—especially if you
    have to do it manually. There are some packages that can automatically scan the
    hyperparameter search space you defined and find the best combination overall
    for you. In the section ahead, you will see how to use one of them: Keras Tuner.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，选择一个超参数可能会影响另一个超参数的效果。因此，随着您想调整的超参数和取值数量的增加，测试的组合数量将呈指数增长。如果您需要手动完成这些组合的模型训练，这将需要大量的时间。幸运的是，某些包可以自动扫描您定义的超参数搜索空间，并为您找到最佳的组合。在接下来的部分，您将看到如何使用其中一个包：Keras
    Tuner。
- en: Keras Tuner
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras Tuner
- en: 'Unfortunately, this package is not included in TensorFlow. You will need to
    install it manually by running the following command:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Keras Tuner包并不包含在TensorFlow中。您需要通过运行以下命令手动安装它：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This package is very simple to use. There are two concepts to understand: **hyperparameters**
    and **tuners**.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包非常简单易用。需要理解两个概念：**超参数**和**调整器**。
- en: 'Hyperparameters are the classes used to define a parameter that will be assessed
    by the tuner. You can use different types of hyperparameters. The main ones are
    the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是用于定义将在调优器评估的参数的类。你可以使用不同类型的超参数，主要有以下几种：
- en: '`hp.Boolean`: A choice between `True` and `False`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.Boolean`：一个选择 `True` 或 `False` 的选项'
- en: '`hp.Int`: A choice with a range of integers'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.Int`：一个具有整数范围的选择'
- en: '`hp.Float`: A choice with a range of decimals'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.Float`：一个具有小数范围的选择'
- en: '`hp.Choice`: A choice within a list of possible values'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hp.Choice`：一个从可能值列表中选择的选项'
- en: 'The following code snippet shows you how to define a hyperparameter called
    `learning_rate` that can only take one of four values—`0.1`, `0.01`, `0.001`,
    or `0.0001`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何定义一个超参数 `learning_rate`，它只能取四个值中的一个——`0.1`、`0.01`、`0.001` 或 `0.0001`：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'A tuner in the Keras Tuner package is an algorithm that will look at the hyperparameter
    search space, test some combinations, and find the one that gives the best result.
    The Keras Tuner package provides different tuners, and in the section ahead, you
    will look at three of them: **random search**, **Hyperband**, and **Bayesian optimization**.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Keras Tuner 包中的调优器是一种算法，它将查看超参数搜索空间，测试一些组合，并找到给出最佳结果的组合。Keras Tuner 包提供了不同的调优器，在接下来的部分中，你将了解其中的三种：**随机搜索**、**Hyperband**
    和 **贝叶斯优化**。
- en: 'Once defined with the algorithm of your choice, you can call the `search()`
    method to start the hyperparameter tuning process on the training and test sets,
    as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用你选择的算法定义了超参数，你可以调用 `search()` 方法来启动在训练集和测试集上的超参数调优过程，如下所示：
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Once the search is complete, you can access the best combination with `get_best_hyperparameters()`
    and then look specifically at one of the hyperparameters you defined:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索完成后，你可以通过 `get_best_hyperparameters()` 获取最佳组合，然后具体查看你定义的某个超参数：
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, the `hypermodel.build()` method will instantiate a TensorFlow Keras
    model with the best hyperparameters found:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`hypermodel.build()` 方法将使用找到的最佳超参数实例化一个 TensorFlow Keras 模型：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It's as simple as that. Now, let's have a look at the random search tuner.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。现在，让我们来看一下随机搜索调优器。
- en: Random Search
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Random search is one of the available algorithms in this package. As its name
    implies, it randomly defines the combinations to be tested by sampling through
    the search space. Even though this algorithm doesn't test every single possible
    combination, random search provides very good results.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索是该包中可用的算法之一。顾名思义，它通过在搜索空间中随机采样来定义待测试的组合。尽管该算法不会测试每一个可能的组合，但随机搜索通常能够提供非常好的结果。
- en: Note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The algorithm that tests every single combination of the search space is called
    grid search.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 测试每一个搜索空间组合的算法叫做网格搜索。
- en: '![Figure 6.17: Comparison between grid search and random search'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17：网格搜索与随机搜索的比较'
- en: '](img/B16341_06_17.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_06_17.jpg)'
- en: 'Figure 6.17: Comparison between grid search and random search'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17：网格搜索与随机搜索的比较
- en: The preceding figure shows an example of the difference between grid search
    and random search. You can see that grid search splits the search space into a
    grid and tests each of the combinations, but some may lead to the same loss value,
    which makes it less efficient. On the other side, random search covers the search
    space more efficiently and helps find the optimal solution.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示例展示了网格搜索与随机搜索之间的区别。你可以看到，网格搜索将搜索空间划分为网格，并测试每一个组合，但有些可能会导致相同的损失值，从而降低效率。另一方面，随机搜索更高效地覆盖了搜索空间，并帮助找到最优解。
- en: 'In Keras Tuner, before instantiating a tuner, you need to define a model-building
    function that will define the architecture of the TensorFlow Keras model to be
    trained with the hyperparameters you want to test. Here is an example of such
    a function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras Tuner 中，在实例化调优器之前，你需要定义一个模型构建函数，该函数将定义用于训练的 TensorFlow Keras 模型的架构，并设置你希望测试的超参数。以下是一个这样的函数示例：
- en: '[PRE48]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code snippet, you created a model composed of three fully connected
    layers of `512`, `128`, and `10` units that will be trained with a categorical cross-entropy
    loss function and the Adam optimizer. You defined the `learning_rate` hyperparameter
    that will be assessed by Keras Tuner.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，你创建了一个由三个全连接层（`512`、`128` 和 `10` 单元）组成的模型，该模型将使用分类交叉熵损失函数和 Adam 优化器进行训练。你定义了一个将由
    Keras Tuner 评估的 `learning_rate` 超参数。
- en: 'Once the model-building function is defined, you can instantiate a random search
    tuner like the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了模型构建函数，你可以像下面这样实例化一个随机搜索调参器：
- en: '[PRE49]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the preceding code, you instantiated a `RandomSearch` tuner that will look
    at the model and hyperparameters defined in the `model_builder` function using
    the validation accuracy as the `objective` metric and will run for a maximum of
    `10` trials.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，你实例化了一个`RandomSearch`调参器，它将使用验证准确度作为`objective`指标，查看`model_builder`函数中定义的模型和超参数，并进行最多`10`次试验。
- en: In the next exercise, you will use random search to find the best set of hyperparameters
    for a model.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，你将使用随机搜索找到模型的最佳超参数集。
- en: 'Exercise 6.03: Predicting a Connect-4 Game Outcome Using Random Search from
    Keras Tuner'
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.03：使用 Keras Tuner 的随机搜索预测 Connect-4 游戏结果
- en: 'In this exercise, you will be using the same dataset as for *Exercise 6.01*,
    *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*. You will build
    and train a multi-class model in TensorFlow that will predict the class outcome
    for player 1 in the game Connect-4 using the Keras Tuner package to find the best
    regularization factor for L2 regularization through random search:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将使用与*练习 6.01*相同的数据集，*使用 L2 正则化器预测 Connect-4 游戏结果*。你将构建并训练一个多类模型，使用 Keras
    Tuner 包通过随机搜索找到 L2 正则化的最佳正则化因子，预测 Connect-4 游戏中玩家 1 的胜负结果：
- en: Note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be accessed here: [https://packt.link/aTSbC](https://packt.link/aTSbC).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过以下链接访问：[https://packt.link/aTSbC](https://packt.link/aTSbC)。
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以在这里找到：[http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4)。
- en: Open a new Jupyter notebook.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook。
- en: 'Import the pandas library and use `pd` as the alias:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 pandas 库，并使用`pd`作为别名：
- en: '[PRE50]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create a variable called `file_url` that contains the URL to the dataset:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`file_url`的变量，包含数据集的 URL：
- en: '[PRE51]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Load the dataset into a DataFrame called `data` using the `read_csv()` method
    and provide the URL to the CSV file. Print the first five rows using the `head()` method:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`read_csv()`方法将数据集加载到一个名为`data`的 DataFrame 中，并提供 CSV 文件的 URL。使用`head()`方法打印前五行：
- en: '[PRE52]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be as follows:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 6.18: First five rows of the dataset'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.18：数据集的前五行'
- en: '](img/B16341_06_18.jpg)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_18.jpg)'
- en: 'Figure 6.18: First five rows of the dataset'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.18：数据集的前五行
- en: 'Extract the target variable (the column called `class`) using the `pop()` method
    and save it in a variable called `target`:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pop()`方法提取目标变量（名为`class`的列），并将其保存在一个名为`target`的变量中：
- en: '[PRE53]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Import `train_test_split` from `sklearn.model_selection`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.model_selection`导入`train_test_split`：
- en: '[PRE54]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Split the data into training and test sets using `train_test_split()`, with
    20% of the data for testing and `42` for `random_state`:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split()`将数据拆分为训练集和测试集，20%的数据用于测试，`42`作为`random_state`：
- en: '[PRE55]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Install the `kerastuner` package and then import it and assign it the `kt`
    alias:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`kerastuner`包，然后导入并将其别名设为`kt`：
- en: '[PRE56]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Import the TensorFlow library and use `tf` as the alias. Then, import the `Dense`
    class from `tensorflow.keras.layers`:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库，并使用`tf`作为别名。然后，从`tensorflow.keras.layers`导入`Dense`类：
- en: '[PRE57]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Set the seed as `8` using `tf.random.set_seed()` to get reproducible results:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.random.set_seed()`将种子设置为`8`，以获得可重复的结果：
- en: '[PRE58]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define a function called `model_builder` that will create a sequential model
    with the same architecture as *Exercise 6.02*, *Predicting a Connect-4 Game Outcome
    Using Dropout*, with L2 regularization, but this time, provide an `hp.Choice`
    hyperparameter for the regularization factor:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`model_builder`的函数，该函数将创建一个与*练习 6.02*相同架构的顺序模型，使用 L2 正则化，但这次为正则化因子提供一个`hp.Choice`超参数：
- en: '[PRE59]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Instantiate a `RandomSearch` tuner and assign `val_accuracy` to `objective`
    and `10` to `max_trials`:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`RandomSearch`调参器，将`val_accuracy`赋值给`objective`，并将`10`赋值给`max_trials`：
- en: '[PRE60]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Launch the hyperparameter search with the `search()` method on the training
    and test sets:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`search()`方法在训练集和测试集上启动超参数搜索：
- en: '[PRE61]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Extract the best hyperparameter combination (index `0`) with `get_best_hyperparameters()`
    and save it in a variable called `best_hps`:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get_best_hyperparameters()`提取最佳的超参数组合（索引`0`），并将其保存到一个名为`best_hps`的变量中：
- en: '[PRE62]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Extract the best value for the `l2` regularization hyperparameter, save it
    in a variable called `best_l2`, and print its value:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取`l2`正则化超参数的最佳值，保存到一个名为`best_l2`的变量中，并打印其值：
- en: '[PRE63]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'You should get the following result:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下结果：
- en: '[PRE64]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The best value for the `l2` hyperparameter found by random search is `0.0001`.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机搜索找到的 `l2` 超参数的最佳值是 `0.0001`。
- en: 'Start the model training process using the `fit()` method for five epochs and
    use the test set for `validation_data`:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法开始模型训练过程，训练五个周期，并使用测试集作为`validation_data`：
- en: '[PRE65]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You will get the following output:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '![Figure 6.19: Logs of the training process'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.19：训练过程的日志'
- en: '](img/B16341_06_19.jpg)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_19.jpg)'
- en: 'Figure 6.19: Logs of the training process'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19：训练过程的日志
- en: Using a random search tuner, you found the best value for L2 regularization
    (`0.0001`), which helped the model to achieve an accuracy of `0.83` on the training
    set and `0.81` on the test set. These scores are quite an improvement on those
    from *Exercise 6.01*, *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*
    (`0.69` for the training set and `0.59` for the test set).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机搜索调优器，你找到了 L2 正则化的最佳值（`0.0001`），该值帮助模型在训练集上达到了 `0.83` 的准确度，在测试集上达到了 `0.81`
    的准确度。这些得分相比 *练习 6.01*，*使用 L2 正则化预测 Connect-4 游戏结果*（训练集 `0.69` 和测试集 `0.59`）有了很大的提升。
- en: In the next section, you will use another Keras tuner, called Hyperband.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，你将使用另一种 Keras 调优器，名为 Hyperband。
- en: Hyperband
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hyperband
- en: 'Hyperband is another tuner available in the Keras Tuner package. Like random
    search, it randomly picks candidates from the search space, but more efficiently.
    The idea behind it is to test a set of combinations for just one or two iterations,
    keeping only the best performers and training them for longer. So, the algorithm
    doesn''t waste time in training non-performing combinations as with random search.
    Instead, it simply discards them from the next run. Only the ones that achieve
    higher performance are kept for longer training. To instantiate a Hyperband tuner,
    execute the following command:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperband 是 Keras Tuner 包中另一个可用的调优器。像随机搜索一样，它从搜索空间中随机选择候选项，但效率更高。其背后的思想是测试一组组合仅进行一到两次迭代，只保留表现最好的候选项，并对其进行更长时间的训练。因此，算法不会像随机搜索那样浪费时间训练表现不佳的组合，而是直接将它们从下一轮中丢弃。只有那些实现更高性能的组合才会进行更长时间的训练。要实例化
    Hyperband 调优器，执行以下命令：
- en: '[PRE66]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This tuner takes a model-building function and an objective metric as input
    parameters, as for random search. But it requires an additional one, `max_epochs`,
    corresponding to the maximum number of epochs a model is allowed to train for
    during the hyperparameter search.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调优器以模型构建函数和目标度量作为输入参数，类似于随机搜索。但它需要一个额外的参数`max_epochs`，对应于在超参数搜索期间，模型允许训练的最大迭代次数。
- en: 'Exercise 6.04: Predicting a Connect-4 Game Outcome Using Hyperband from Keras
    Tuner'
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.04：使用 Keras Tuner 的 Hyperband 预测 Connect-4 游戏结果
- en: 'In this exercise, you will be using the same dataset as for *Exercise 6.01*,
    *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*. You will build
    and train a multi-class model in TensorFlow that will predict the class outcome
    for player 1 in the game Connect-4 using the Keras Tuner package to find the best
    learning rate and the number of units for the input layer through Hyperband:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将使用与 *练习 6.01*，*使用 L2 正则化预测 Connect-4 游戏结果* 相同的数据集。你将构建并训练一个多分类模型，使用
    Keras Tuner 包通过 Hyperband 查找最佳学习率和输入层的单元数，以预测 Connect-4 游戏中玩家 1 的结果：
- en: Note
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be accessed here: [https://packt.link/WLgen](https://packt.link/WLgen).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过此链接访问：[https://packt.link/WLgen](https://packt.link/WLgen)。
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可以通过以下链接找到：[http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4)。
- en: Open a new Jupyter notebook.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook。
- en: 'Import the pandas library and use `pd` as the alias:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 pandas 库并将其别名设为`pd`：
- en: '[PRE67]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create a variable called `file_url` that contains the URL to the dataset:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`file_url`的变量，其中包含数据集的 URL：
- en: '[PRE68]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Load the dataset into a DataFrame called `data` using the `read_csv()` method
    and provide the URL to the CSV file. Print the first five rows using the `head()`
     method:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `read_csv()` 方法将数据集加载到名为 `data` 的 DataFrame 中，并提供 CSV 文件的 URL。使用 `head()`
    方法打印前五行：
- en: '[PRE69]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output will be as follows:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.20: First five rows of the dataset'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.20：数据集的前五行'
- en: '](img/B16341_06_20.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_20.jpg)'
- en: 'Figure 6.20: First five rows of the dataset'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.20：数据集的前五行
- en: 'Extract the target variable (`class`) using the `pop()` method, and save it
    in a variable called `target`:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pop()`方法提取目标变量（`class`），并将其保存到名为`target`的变量中：
- en: '[PRE70]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Import `train_test_split` from `sklearn.model_selection`:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.model_selection`导入`train_test_split`：
- en: '[PRE71]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Split the data into training and test sets using `train_test_split()`, with
    20% of the data for testing and `42` for `random_state`:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split()`将数据划分为训练集和测试集，20%的数据用于测试，`42`作为`random_state`：
- en: '[PRE72]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Install the `keras-tuner` package, and then import it and assign it the `kt`
    alias:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`keras-tuner`包，然后导入并将其别名为`kt`：
- en: '[PRE73]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Import the TensorFlow library and use `tf` as the alias, and then import the
    `Dense` class from `tensorflow.keras.layers`:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入TensorFlow库并使用`tf`作为别名，然后从`tensorflow.keras.layers`导入`Dense`类：
- en: '[PRE74]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Set the seed as `8` using `tf.random.set_seed()` to get reproducible results:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.random.set_seed()`将种子设置为`8`，以获得可重复的结果：
- en: '[PRE75]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a function called `model_builder` to create a sequential model with
    the same architecture as *Exercise 6.02*, *Predicting a Connect-4 Game Outcome
    Using Dropout*, with L2 regularization and a `0.0001` regularization factor. But,
    this time, provide a hyperparameter, `hp.Choice`, for the learning rate (`0.01`,
    `0.001`, or `0.0001`) and an `hp.Int` function for the number of units (between
    `128` and `512` with a step of `64`) for the input fully connected layer:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`model_builder`的函数，创建一个与*练习6.02*、*使用Dropout预测Connect-4游戏结果*相同架构的顺序模型，并应用L2正则化，正则化因子为`0.0001`。但这次，提供一个超参数`hp.Choice`用于学习率（`0.01`、`0.001`或`0.0001`），并使用`hp.Int`函数设置输入全连接层单元数（在`128`到`512`之间，步长为`64`）：
- en: '[PRE76]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Instantiate a Hyperband tuner, and assign `val_accuracy` to the `objective`
    metric and `5` to `max_epochs`:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个Hyperband调优器，并将`val_accuracy`分配给`objective`度量，将`5`赋值给`max_epochs`：
- en: '[PRE77]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Launch the hyperparameter search with `search()` on the training and test sets:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`search()`在训练集和测试集上启动超参数搜索：
- en: '[PRE78]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Extract the best hyperparameter combination (index `0`) with `get_best_hyperparameters()`
    and save it in a variable called `best_hps`:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get_best_hyperparameters()`提取最佳超参数组合（索引`0`），并将其保存到名为`best_hps`的变量中：
- en: '[PRE79]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Extract the best value for the number of units for the input layer, save it
    in a variable called `best_units`, and print its value:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取输入层单元数的最佳值，将其保存在名为`best_units`的变量中，并打印其值：
- en: '[PRE80]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'You will get the following output:'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE81]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The best value for the number of units of the input layer found by Hyperband
    is `192`.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hyperband找到的输入层单元数的最佳值是`192`。
- en: 'Extract the best value for the learning rate, save it in a variable called
    `best_lr`, and print its value:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取学习率的最佳值，将其保存在名为`best_lr`的变量中，并打印其值：
- en: '[PRE82]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output will be the following:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将是以下内容：
- en: '[PRE83]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The best value for the learning rate hyperparameter found by Hyperband is `0.001`.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hyperband找到的学习率超参数的最佳值是`0.001`。
- en: 'Start the model training process using the `fit()` method for five epochs and
    use the test set for `validation_data`:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit()`方法开始模型训练过程，训练5个epoch，并使用测试集作为`validation_data`：
- en: '[PRE84]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'You will get the following output:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 6.21: Logs of the training process'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.21：训练过程的日志'
- en: '](img/B16341_06_21.jpg)'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_21.jpg)'
- en: 'Figure 6.21: Logs of the training process'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21：训练过程的日志
- en: Using Hyperband as the tuner, you found the best number of units for the input
    layer (`192`) and learning rate (`0.001`). With these hyperparameters, the final
    model achieved an accuracy of `0.81` on both the training and test sets. It is
    not overfitting much and achieved a satisfactory accuracy score.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hyperband作为调优器，你找到了输入层单元数（`192`）和学习率（`0.001`）的最佳值。使用这些超参数，最终模型在训练集和测试集上的准确率都达到了`0.81`。模型没有过拟合，且达到了令人满意的准确度。
- en: Another very popular tuner is Bayesian optimization, which you will learn about
    in the following section.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行的调优器是贝叶斯优化，你将在接下来的章节中学习到它。
- en: Bayesian Optimization
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: 'Bayesian optimization is another very popular algorithm used for automatic
    hyperparameter tuning. It uses probabilities to determine the best combination
    of hyperparameters. The objective is to iteratively build a probability model
    that optimizes the objective function from a set of hyperparameters. At each iteration,
    the probability model is updated from the results obtained. Therefore, unlike
    random search and Hyperband, Bayesian optimization takes past results into account
    to improve new ones. The following code snippet will show you how to instantiate
    a Bayesian optimizer in Keras Tuner:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化是另一种非常流行的用于自动超参数调整的算法。它使用概率来确定最佳的超参数组合。其目标是从一组超参数中迭代地构建优化目标函数的概率模型。在每次迭代中，概率模型都会根据获得的结果进行更新。因此，与随机搜索和Hyperband不同，贝叶斯优化会考虑过去的结果以改善新的结果。下面的代码片段将展示如何在Keras
    Tuner中实例化一个贝叶斯优化器：
- en: '[PRE85]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The expected parameters are similar to random search, including the model-building
    function, the `objective` metric, and the maximum number of trials.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 预期参数类似于随机搜索，包括模型构建函数，`objective`指标和最大试验次数。
- en: In the following activity, you will use Bayesian optimization to predict the
    income of a person.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，你将使用贝叶斯优化来预测一个人的收入。
- en: 'Activity 6.02: Predicting Income with Bayesian Optimization from Keras Tuner'
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 6.02：使用Keras Tuner进行贝叶斯优化预测收入
- en: In this activity, you will use the same dataset as used in *Activity 6.01*,
    *Predicting Income with L1 and L2 Regularizers*. You are tasked with building
    and training a regressor to predict the income of a person based on their census
    data. You will perform automatic hyperparameter tuning with Keras Tuner and find
    the best combination of hyperparameters for the learning rate, the number of units
    for the input layer, and L2 regularization with Bayesian optimization.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将使用与“活动 6.01”中相同的数据集，“使用L1和L2正则化预测收入”。你的任务是基于人口普查数据构建和训练一个回归器，以预测个人的收入。你将使用Keras
    Tuner进行自动超参数调整，并找到学习率、输入层单元数和L2正则化的最佳组合。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤将帮助你完成活动：
- en: Load the data with `read_csv()` from pandas.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`read_csv()`从pandas加载数据。
- en: Extract the target variable with the `pop()` method.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pop()`方法提取目标变量。
- en: Split the data into training (the first 15,000 rows) and test (the last 5,000
    rows) sets.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割为训练集（前 15,000 行）和测试集（后 5,000 行）。
- en: 'Create the model-building function multi-class classifier with five fully connected
    layers of `512`, `512`, `128`, `128`, and `26` units and the three different hyperparameters
    to be tuned: the learning rate (between `0.01` and `0.001`), the number of units
    for the input layer (between `128` and `512` and a step of `64`), and L2 regularization
    (between `0.1`, `0.01`, and `0.001`).'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建包含五个全连接层的多类分类器模型，分别为`512`、`512`、`128`、`128`和`26`个单元，并调整三个不同的超参数：学习率（在`0.01`到`0.001`之间）、输入层单元数（在`128`到`512`之间，步长为`64`）和L2正则化（在`0.1`、`0.01`和`0.001`之间）。
- en: Find the best combination of hyperparameters with Bayesian optimization.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝叶斯优化找到最佳超参数组合。
- en: Train the model on the training set with the best hyperparameters found.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用找到的最佳超参数在训练集上训练模型。
- en: 'The expected output will be as follows:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '![Figure 6.22: Logs of the training process'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.22：训练过程的日志'
- en: '](img/B16341_06_22.jpg)'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_06_22.jpg)'
- en: 'Figure 6.22: Logs of the training process'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22：训练过程的日志
- en: Note
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor270).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor270)找到。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: You started your journey in this chapter with an introduction to the different
    scenarios of training a model. A model is overfitting when its performance is
    much better on the training set than the test set. An underfitting model is one
    that can achieve good results only after training. Finally, a good model achieves
    good performance on both the training and test sets.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这一章的旅程开始于对模型训练不同场景的介绍。当一个模型在训练集上的表现远远优于测试集时，我们称其为过拟合。欠拟合模型只能在训练后才能取得良好结果。最后，一个好的模型在训练集和测试集上都能表现良好。
- en: 'Then, you encountered several regularization techniques that can help prevent
    a model from overfitting. You first looked at the L1 and L2 regularizations, which
    add a penalty component to the cost function. This additional penalty helps to
    simplify the model by reducing the weights of some features. Then, you went through
    two different techniques specific to neural networks: dropout and early stopping.
    Dropout randomly drops some units in the model architecture and forces it to consider
    other features to make predictions. Early stopping is a mechanism that automatically
    stops the training of a model once the performance of the test set starts to deteriorate.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，你接触了几种可以帮助防止模型过拟合的正则化技术。你首先了解了 L1 和 L2 正则化，它们向成本函数中添加了惩罚项。这一额外的惩罚项有助于通过减少某些特征的权重来简化模型。随后，你学习了两种特定于神经网络的正则化技术：dropout
    和早停。Dropout 随机丢弃模型架构中的一些单元，迫使模型考虑其他特征来进行预测。早停是一种机制，一旦测试集的性能开始恶化，便自动停止模型的训练。
- en: 'After this, you learned how to use the Keras Tuner package for automatic hyperparameter
    tuning. You considered three specific types of tuners: random search, Hyperband,
    and Bayesian optimization. You saw how to instantiate them, perform a hyperparameter
    search, and extract the best values and model. This process helped you to achieve
    better performance on the models trained for the exercises and activities.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你学习了如何使用 Keras Tuner 包进行自动超参数调优。你考虑了三种特定类型的调优器：随机搜索、Hyperband 和贝叶斯优化。你了解了如何实例化它们，执行超参数搜索，并提取最佳值和模型。这个过程帮助你在训练的模型上获得了更好的性能，尤其是在练习和活动中。
- en: In the next chapter, you will learn more about **Convolutional Neural Networks**
    (**CNNs**). Such architecture has led to groundbreaking results in computer vision
    in the past few years. The following chapter will show you how to use this architecture
    to recognize objects in images.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习更多关于**卷积神经网络**（**CNNs**）的内容。这种架构在过去几年中为计算机视觉领域带来了突破性的成果。接下来的章节将向你展示如何使用这种架构识别图像中的物体。
