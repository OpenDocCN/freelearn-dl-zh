- en: Generative Models in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的生成模型
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Comparing principal component analysis with the Restricted Boltzmann machine
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较主成分分析与限制玻尔兹曼机
- en: Setting up a Restricted Boltzmann machine for Bernoulli distribution input
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为伯努利分布输入设置限制玻尔兹曼机
- en: Training a Restricted Boltzmann machine
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练限制玻尔兹曼机
- en: Backward or reconstruction phase of RBM
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM的反向或重建阶段
- en: Understanding the contrastive divergence of the reconstruction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解重建的对比散度
- en: Initializing and starting a new TensorFlow session
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化并启动一个新的TensorFlow会话
- en: Evaluating the output from an RBM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估RBM的输出
- en: Setting up a Restricted Boltzmann machine for Collaborative Filtering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为协同过滤设置限制玻尔兹曼机
- en: Performing a full run of training an RBM
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行RBM训练的完整运行
- en: Setting up a Deep Belief Network
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置深度信念网络
- en: Implementing a feed-forward backpropagation Neural Network
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现前馈反向传播神经网络
- en: Setting up a Deep Restricted Boltzmann Machine
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置深度限制玻尔兹曼机
- en: Comparing principal component analysis with the Restricted Boltzmann machine
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较主成分分析与限制玻尔兹曼机
- en: In this section, you will learn about two widely recommended dimensionality
    reduction techniques--**Principal component analysis** (**PCA**) and the **Restricted
    Boltzmann machine** (**RBM**). Consider a vector *v* in *n*-dimensional space.
    The dimensionality reduction technique essentially transforms the vector *v* into
    a relatively smaller (or sometimes equal) vector *v'* with *m*-dimensions (*m*<*n*).
    The transformation can be either linear or nonlinear.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习两种广泛推荐的降维技术——**主成分分析**（**PCA**）和**限制玻尔兹曼机**（**RBM**）。考虑在*n*维空间中的一个向量*v*。降维技术本质上将向量*v*转换为一个相对较小（有时是相等）的*m*维向量*v'*（*m*<*n*）。这种转换可以是线性或非线性的。
- en: 'PCA performs a linear transformation on features such that orthogonally adjusted
    components are generated that are later ordered based on their relative importance
    of variance capture. These *m* components can be considered as new input features,
    and can be defined as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对特征进行线性变换，从而生成正交调整的组件，这些组件之后会根据它们在方差捕捉中的相对重要性进行排序。这些*m*个组件可以视为新的输入特征，并可以如下定义：
- en: Vector *v'* = ![](img/00103.gif)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 向量*v'* = ![](img/00103.gif)
- en: Here, *w* and *c* correspond to weights (loading) and transformed components,
    respectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w* 和 *c* 分别对应于权重（加载）和转换后的组件。
- en: Unlike PCA, RBMs (or DBNs/autoencoders) perform non-linear transformations using
    connections between visible and hidden units, as described in [Chapter 4](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Data Representation Using Autoencoders*. The nonlinearity helps in better understanding
    the relationship with latent variables. Along with information capture, they also
    tend to remove noise. RBMs are generally based on stochastic distribution (either
    Bernoulli or Gaussian).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA不同，RBM（或DBN/自编码器）通过可见单元和隐藏单元之间的连接执行非线性变换，正如在[第4章](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab)
    *使用自编码器的数据表示*中所描述的那样。非线性有助于更好地理解与潜在变量之间的关系。除了信息捕获外，它们还倾向于去除噪声。RBM通常基于随机分布（无论是伯努利分布还是高斯分布）。
- en: 'A large amount of Gibbs sampling is performed to learn and optimize the connection
    weights between visible and hidden layers. The optimization happens in two passes:
    a forward pass where hidden layers are sampled using given visible layers and
    a backward pass where visible layers are resampled using given hidden layers.
    The optimization is performed to minimize the reconstruction error.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 执行大量的吉布斯采样以学习和优化可见层与隐藏层之间的连接权重。优化过程分为两个阶段：前向阶段，其中使用给定的可见层对隐藏层进行采样；反向阶段，其中使用给定的隐藏层对可见层进行重新采样。该优化旨在最小化重建误差。
- en: 'The following image represents a restricted Boltzmann machine:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像表示一个限制玻尔兹曼机：
- en: '![](img/00099.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: Getting ready
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, you will require R (the `rbm` and `ggplot2` packages) and
    the MNIST dataset. The MNIST dataset can be downloaded from the TensorFlow dataset
    library. The dataset consists of handwritten images of 28 x 28 pixels. It has
    55,000 training examples and 10,000 test examples. It can be downloaded from the
    `tensorflow` library using the following script:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，你将需要R（`rbm`和`ggplot2`包）和MNIST数据集。MNIST数据集可以从TensorFlow数据集库中下载。该数据集包含28
    x 28像素的手写图像。它有55,000个训练样本和10,000个测试样本。可以通过以下脚本从`tensorflow`库下载：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Extract the train dataset (`trainX` with all 784 independent variables and
    `trainY` with the respective 10 binary outputs):'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取训练数据集（`trainX`包含所有784个独立变量，`trainY`包含相应的10个二元输出）：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run a PCA on the `trainX` data:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`trainX`数据执行PCA：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run an RBM on the `trainX` data:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`trainX`数据运行RBM：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Predict on the train data using the generated models. In the case of the RBM
    model, generate probabilities:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成的模型对训练数据进行预测。对于RBM模型，生成概率：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Convert the outcomes into data frames:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果转换为数据框：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Convert the 10-class binary `trainY` data frame into a numeric vector:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将10类二元`trainY`数据框转换为数值向量：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Plot the components generated using PCA. Here, the *x*-axis represents component
    1 and the *y*-axis represents component 2\. The following image shows the outcome
    of the PCA model:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制使用PCA生成的组件图。这里，*x*轴表示组件1，*y*轴表示组件2。以下图片展示了PCA模型的结果：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00058.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: 'Plot the hidden layers generated using PCA. Here, the *x*-axis represents hidden
    1 and *y*-axis represents hidden 2\. The following image shows the outcome of
    the RBM model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制使用PCA生成的隐藏层。这里，*x*轴表示隐藏1，*y*轴表示隐藏2。以下图片展示了RBM模型的结果：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/00102.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: 'The following code and image shows the cumulative variance explained by the
    principal components:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码和图片展示了主成分所解释的累积方差：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/00106.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)'
- en: 'The following code and image shows the decrease in the reconstruction training
    error while generating an RBM using multiple epochs:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码和图片展示了在使用多个训练周期生成RBM时，重建训练误差的下降：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/00107.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.jpeg)'
- en: Setting up a Restricted Boltzmann machine for Bernoulli distribution input
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为伯努利分布输入设置限制玻尔兹曼机
- en: In this section, let's set up a restricted Boltzmann machine for Bernoulli distributed
    input data, where each attribute has values ranging from 0 to 1 (equivalent to
    a probability distribution). The dataset (MNIST) used in this recipe has input
    data satisfying a Bernoulli distribution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为伯努利分布的输入数据设置限制玻尔兹曼机，其中每个属性的值范围从0到1（相当于一个概率分布）。本配方中使用的数据集（MNIST）具有满足伯努利分布的输入数据。
- en: 'An RBM comprises of two layers: a visible layer and a hidden layer. The visible
    layer is an input layer of nodes equal to the number of input attributes. In our
    case, each image in the MNIST dataset is defined using 784 pixels (28 x 28 size).
    Hence, our visible layer will have 784 nodes.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机由两层组成：一个可见层和一个隐藏层。可见层是输入层，节点数量等于输入属性的数量。在我们的案例中，MNIST数据集中的每个图像由784个像素（28
    x 28大小）定义。因此，我们的可见层将包含784个节点。
- en: On the other hand, the hidden layer is generally user-defined. The hidden layer
    has a set of binary activated nodes, with each node having a probability of linkage
    with all other visible nodes. In our case, the hidden layer will have 900 nodes.
    As an initial step, all the nodes in the visible layer are connected with all
    the nodes in the hidden layer bidirectionally.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，隐藏层通常是由用户定义的。隐藏层具有一组二值激活的节点，每个节点与所有其他可见节点有一定的连接概率。在我们的案例中，隐藏层将包含900个节点。作为初步步骤，所有可见层的节点与所有隐藏层的节点是双向连接的。
- en: Each connection is defined using a weight, and hence a weight matrix is defined
    where the rows represent the number of input nodes and the columns represent the
    number of hidden nodes. In our case, the weight matrix (*w*) will be a tensor
    of dimensions 784 x 900.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个连接都由一个权重定义，因此定义了一个权重矩阵，其中行代表输入节点的数量，列代表隐藏节点的数量。在我们的案例中，权重矩阵（*w*）将是一个784 x
    900的张量。
- en: In addition to weights, all the nodes in each layer are assisted by a bias node.
    The bias node of the visible layer will have connections with all the visible
    nodes (that is, the 784 nodes) and is represented with **vb**, whereas the bias
    node of the hidden layer will have connections with all the hidden nodes (that
    is, the 900 nodes) and is represented as **vh**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权重外，每个层中的所有节点还由偏置节点辅助。可见层的偏置节点将与所有可见节点（即784个节点）连接，表示为**vb**，而隐藏层的偏置节点将与所有隐藏节点（即900个节点）连接，表示为**vh**。
- en: A point to remember with RBMs is that there will be no connections among nodes
    within each layer. In other words, the connections will be interlayer, but not
    intralayer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，RBM的一个要点是每层内部的节点之间没有连接。换句话说，连接是跨层的，而不是层内的。
- en: 'The following image represents an RBM with the visible layer, hidden layer,
    and interconnections:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像表示了包含可见层、隐藏层和连接的RBM：
- en: '![](img/00108.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: Getting ready
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section provides the requirements for setting up an RBM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了设置RBM的要求。
- en: TensorFlow in R is installed and set up
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在R中安装并设置TensorFlow
- en: The `mnist` data is downloaded and loaded for setting up RBM
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnist`数据被下载并加载以设置RBM'
- en: How to do it...
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'This section provides the steps to set up the visible and hidden layers of
    an RBM using TensorFlow:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了使用TensorFlow设置RBM的可见层和隐藏层的步骤：
- en: 'Start a new interactive TensorFlow session:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个新的交互式TensorFlow会话：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the model parameters. The `num_input` parameter defines the number of
    nodes in the visible layer and `num_hidden` defines the number of nodes in the
    hidden layer:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型参数。`num_input`参数定义可见层节点的数量，`num_hidden`定义隐藏层节点的数量：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a placeholder variable for the weight matrix:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为权重矩阵创建占位符变量：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create placeholder variables of the visible and hidden biases:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为可见和隐藏偏置创建占位符变量：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Training a Restricted Boltzmann machine
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练限制玻尔兹曼机
- en: 'Every training step of an RBM goes through two phases: the forward phase and
    the backward phase (or reconstruction phase). The reconstruction of visible units
    is fine tuned by making several iterations of the forward and backward phases.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每次训练RBM都会经历两个阶段：正向阶段和反向阶段（或重建阶段）。通过进行正向和反向阶段的多次迭代，来精细调节可见单元的重建。
- en: '**Training a forward phase**: In the forward phase, the input data is passed
    from the visible layer to the hidden layer and all the computation occurs within
    the nodes of the hidden layer. The computation is essentially to take a stochastic
    decision of each connection from the visible to the hidden layer. In the hidden
    layer, the input data (`X`) is multiplied by the weight matrix (`W`) and added
    to a hidden bias vector (`hb`).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**正向阶段训练**：在正向阶段，输入数据从可见层传递到隐藏层，所有计算发生在隐藏层的节点中。计算本质上是对每个从可见层到隐藏层连接的随机决策。在隐藏层中，输入数据（`X`）与权重矩阵（`W`）相乘，并加上一个隐藏偏置向量（`hb`）。'
- en: The resultant vector of a size equal to the number of hidden nodes is then passed
    through a sigmoid function to determine each hidden node's output (or activation
    state). In our case, each input digit will produce a tensor vector of 900 probabilities,
    and as we have 55,000 input digits, we will have an activation matrix of the size
    55,000 x 900\. Using the hidden layer's probability distribution matrix, we can
    generate samples of activation vectors that can be used later to estimate negative
    phase gradients.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的向量（大小等于隐藏层节点的数量）会通过sigmoid函数，确定每个隐藏节点的输出（或激活状态）。在我们的案例中，每个输入数字会生成一个包含900个概率的张量向量，由于我们有55,000个输入数字，所以我们会得到一个大小为55,000
    x 900的激活矩阵。利用隐藏层的概率分布矩阵，我们可以生成激活向量的样本，之后可用于估计负向阶段的梯度。
- en: Getting ready
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section provides the requirements for setting up an RBM.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了设置RBM的要求。
- en: TensorFlow in R is installed and set up
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在R中安装并设置TensorFlow
- en: The `mnist` data is downloaded and loaded for setting up the RBM
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnist`数据被下载并加载以设置RBM'
- en: The RBM model is set up as described in the recipe *Setting up a Restricted
    Boltzmann machine for Bernoulli distribution input*
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM模型的设置按照*为伯努利分布输入设置限制玻尔兹曼机*的步骤进行。
- en: Example of a sampling
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个采样的示例
- en: Consider a constant vector `s1` equivalent to a tensor vector of probabilities.
    Then, create a new random uniformly distributed sample `s2` using the distribution
    of the constant vector `s1`. Then calculate the difference and apply a rectified
    linear activation function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个常量向量`s1`，它等于一个概率的张量向量。然后，使用常量向量`s1`的分布，创建一个新的随机均匀分布样本`s2`。接着计算它们的差异，并应用一个修正的线性激活函数。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'This section provides the steps to set up the script for running the RBM model
    using TensorFlow:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了使用TensorFlow运行RBM模型的脚本设置步骤：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use the following code to execute the graph created in TensorFlow:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码执行在TensorFlow中创建的图：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Backward or reconstruction phase of RBM
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RBM的反向或重建阶段
- en: In the reconstruction phase, the data from the hidden layer is passed back to
    the visible layer. The hidden layer vector of probabilities `h0` is multiplied
    by the transpose of the weight matrix `W` and added to a visible layer bias `vb`,
    which is then passed through a sigmoid function to generate a reconstructed input
    vector `prob_v1`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在重建阶段，来自隐藏层的数据被传递回可见层。隐藏层的概率向量`h0`与权重矩阵`W`的转置相乘，并加上一个可见层偏置`vb`，然后通过Sigmoid函数生成重建的输入向量`prob_v1`。
- en: A sample input vector is created using the reconstructed input vector, which
    is then multiplied by the weight matrix `W` and added to the hidden bias vector
    `hb` to generate an updated hidden vector of probabilities `h1`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用重建的输入向量创建一个样本输入向量，该向量随后与权重矩阵`W`相乘，并加上隐藏偏置向量`hb`，生成更新后的隐藏概率向量`h1`。
- en: This is also called Gibbs sampling. In some scenarios, the sample input vector
    is not generated and the reconstructed input vector `prob_v1` is directly used
    to update the hi
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这也叫做吉布斯采样。在某些情况下，样本输入向量不会生成，而是直接使用重建的输入向量`prob_v1`来更新隐藏层
- en: '![](img/00124.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpeg)'
- en: Getting ready
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: This section provides the requirements for image reconstruction using the input
    probability vector.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了使用输入概率向量进行图像重建的要求。
- en: '`mnist` data is loaded in the environment'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnist`数据已加载到环境中'
- en: The RBM model is trained using the recipe *Training a Restricted Boltzmann machine*
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM模型是通过*训练限制玻尔兹曼机*的配方进行训练的
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This section covers the steps to perform backward reconstruction and evaluation:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了执行反向重建和评估的步骤：
- en: 'The backward image reconstruction can be performed using the input probability
    vector with the following script:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向图像重建可以使用输入概率向量通过以下脚本进行：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The evaluation can be performed using a defined metric, such as **mean squared
    error** (**MSE**), which is computed between the actual input data (`X`) and the
    reconstructed input data (`v1`). The MSE is computed after each epoch and the
    key objective is to minimize the MSE:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估可以使用一个定义的度量标准进行，如**均方误差**（**MSE**），它是在实际输入数据（`X`）和重建的输入数据（`v1`）之间计算的。MSE在每个周期后计算，关键目标是最小化MSE：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Understanding the contrastive divergence of the reconstruction
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解重建的对比散度
- en: 'As an initial start, the objective function can be defined as the minimization
    of the average negative log-likelihood of reconstructing the visible vector *v*
    where *P(v)* denotes the vector of generated probabilities:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初步设置，目标函数可以定义为最小化重建可见向量*v*的平均负对数似然，其中*P(v)*表示生成概率的向量：
- en: '![](img/00110.gif)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.gif)'
- en: Getting ready
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: This section provides the requirements for image reconstruction using the input
    probability vector.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了使用输入概率向量进行图像重建的要求。
- en: '`mnist` data is loaded in the environment'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnist`数据已加载到环境中'
- en: The images are reconstructed using the recipe *Backward or reconstruction phase*
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像是使用*反向或重建阶段*的配方重建的
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This current recipe present the steps for, a **contrastive divergence** (**CD**)
    technique used to speed up the sampling process:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当前配方介绍了使用**对比散度**（**CD**）技术加速采样过程的步骤：
- en: 'Compute a positive weight gradient by multiplying (outer product) the input
    vector `X` with a sample of the hidden vector `h0` from the given probability
    distribution `prob_h0`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将输入向量`X`与来自给定概率分布`prob_h0`的隐藏向量`h0`样本相乘（外积），计算正权重梯度：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Compute a negative weight gradient by multiplying (outer product) the sample
    of the reconstructed input data `v1` with the updated hidden activation vector
    `h1`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将重建的输入数据样本`v1`与更新的隐藏激活向量`h1`进行外积，计算负权重梯度：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, compute the `CD` matrix by subtracting the negative gradient from the
    positive gradient and dividing by the size of the input data:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过从正梯度中减去负梯度并除以输入数据的大小来计算`CD`矩阵：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, update the weight matrix `W` to `update_W` using a learning rate (*alpha*)
    and the CD matrix:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用学习率（*alpha*）和CD矩阵，将权重矩阵`W`更新为`update_W`：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Additionally, update the visible and hidden bias vectors:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，更新可见和隐藏偏置向量：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The objective function can be minimized using stochastic gradient descent by
    indirectly modifying (and optimizing) the weight matrix. The entire gradient can
    be further divided into two forms based on the probability density: positive gradient
    and negative gradient. The positive gradient primarily depends on the input data
    and the negative gradient depends only on the generated model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数可以通过随机梯度下降法最小化，间接修改（并优化）权重矩阵。整个梯度可以基于概率密度进一步分为两种形式：正梯度和负梯度。正梯度主要依赖于输入数据，而负梯度仅依赖于生成的模型。
- en: In the positive gradient, the probability toward the reconstructing training
    data increases, and in the negative gradient, the probability of randomly generated
    uniform samples by the model decreases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在正梯度中，重构训练数据的概率增加，而在负梯度中，由模型随机生成的均匀样本的概率减小。
- en: 'The CD technique is used to optimize the negative phase. In the CD technique,
    the weight matrix is adjusted in each iteration of reconstruction. The new weight
    matrix is generated using the following formula. The learning rate is defined
    as *alpha*, in our case:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: CD技术用于优化负相。使用CD技术时，在每次重构迭代中都会调整权重矩阵。新的权重矩阵通过以下公式生成。学习率定义为*alpha*，在我们的情况下：
- en: '![](img/00105.gif)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.gif)'
- en: Initializing and starting a new TensorFlow session
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化并启动新的TensorFlow会话
- en: A big part of calculating the error metric such as mean square error (MSE) is
    initialization and starting a new TensorFlow session. Here is how we proceed with
    it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 计算误差度量（如均方误差MSE）的一个重要部分是初始化和启动新的TensorFlow会话。以下是我们进行操作的步骤。
- en: Getting ready
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: This section provides the requirements for starting a new TensorFlow session
    used to compute the error metric.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了启动新的TensorFlow会话所需的要求，用于计算误差度量。
- en: '`mnist` data is loaded in the environment'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnist` 数据已加载到环境中'
- en: The TensorFlow graph for the RBM is loaded
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM的TensorFlow图已加载
- en: How to do it...
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'This section provides the steps for optimizing the error using reconstruction
    from an RBM:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了使用RBM重构优化误差的步骤：
- en: 'Initialize the current and previous vector of biases and matrices of weights:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化当前和前一个偏置向量及权重矩阵：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Start a new TensorFlow session:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个新的TensorFlow会话：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Perform a first run with the full input data (**trainX**) and obtain the first
    set of weight matrix and bias vectors:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用完整的输入数据（**trainX**）执行第一次运行，并获得第一组权重矩阵和偏置向量：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s look at the error of the first run:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们来看一下第一次运行的误差：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The full model for the RBM can be trained using the following script:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本可以训练RBM的完整模型：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Plot reconstruction using mean squared errors:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均方误差绘制重构：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Here, we will run 15 epochs (or iterations) where, in each epoch, a batchwise
    (size = 100) optimization is performed. In each batch, the CD is computed and,
    accordingly, weights and biases are updated. To keep track of the optimization,
    MSE is calculated after every batch of 10,000 rows.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将运行15个周期（或迭代），每个周期中会执行批量（大小=100）的优化。在每个批次中，计算CD并相应更新权重和偏置。为了跟踪优化过程，每处理完10,000行数据后都会计算一次MSE。
- en: 'The following image shows the declining trend of mean squared reconstruction
    errors computed for 90 batches:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了计算90个批次的均方重构误差的下降趋势：
- en: '![](img/00112.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpeg)'
- en: Evaluating the output from an RBM
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估来自RBM的输出
- en: Here, let's plot the weights of the final layer with respect to the output (reconstruction
    input data). In the current scenario, 900 is the number of nodes in the hidden
    layer and 784 is the number of nodes in the output (reconstructed) layer.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将绘制最终层的权重与输出（重构输入数据）之间的关系。在当前的场景下，900是隐藏层的节点数，784是输出（重构）层的节点数。
- en: 'In the following image, the first 400 nodes in the hidden layer are seen:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，可以看到隐藏层的前400个节点：
- en: '![](img/00113.gif)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.gif)'
- en: Here, each tile represents a vector of connections formed between a hidden node
    and all the visible layer nodes. In each tile, the black region represents negative
    weights (weight < 0), the white region represents positive weights (weight > 1),
    and the grey region represents no connection (weight = 0). The higher the positive
    value, the greater the chance of activation in hidden nodes, and vice versa. These
    activations help determine which part of the input image is being determined by
    a given hidden node.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个图块表示一个隐藏节点与所有可见层节点之间的连接向量。在每个图块中，黑色区域表示负权重（权重 < 0），白色区域表示正权重（权重 > 1），灰色区域表示没有连接（权重
    = 0）。正值越高，隐藏节点的激活概率越大，反之亦然。这些激活有助于确定给定隐藏节点正在确定输入图像的哪一部分。
- en: Getting ready
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'This section provides the requirements for running the evaluation recipe:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了运行评估教程所需的要求：
- en: '`mnist` data is loaded in the environment'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnist`数据已加载到环境中'
- en: The RBM model is executed using TensorFlow and the optimal weights are obtained
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow执行RBM模型，并获得最佳权重
- en: How to do it...
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'This recipe covers the steps for the evaluation of weights obtained from an
    RBM:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程涵盖了从RBM中获得的权重评估步骤：
- en: 'Run the following code to generate the image of 400 hidden nodes:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码生成400个隐藏节点的图像：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Select a sample of four actual input digits from the training data:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中选择四个实际输入的数字样本：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, visualize these sample digits using the following code:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用以下代码可视化这些样本数字：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, reconstruct these four sample images using the final weights and biases
    obtained:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用最终获得的权重和偏置重构这四个样本图像：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, visualize the reconstructed sample digits using the following code:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用以下代码可视化重构后的样本数字：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The following image illustrates a raw image of the four sample digits:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了四个样本数字的原始图像：
- en: '![](img/00152.jpeg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00152.jpeg)'
- en: The reconstructed images seemed to have had their noise removed, especially
    in the case of digits **3** and **6**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 重构的图像似乎去除了噪声，尤其是在数字**3**和**6**的情况下。
- en: 'The following image illustrates a reconstructed image of the same four digits:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了同四个数字的重构图像：
- en: '![](img/00069.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: Setting up a Restricted Boltzmann machine for Collaborative Filtering
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为协同过滤设置限制玻尔兹曼机
- en: In this recipe, you will learn how to build a collaborative-filtering-based
    recommendation system using an RBM. Here, for every user, the RBM tries to identify
    similar users based on their past behavior of rating various items, and then tries
    to recommend the next best item.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将学习如何使用RBM构建一个基于协同过滤的推荐系统。这里，对于每个用户，RBM会根据他们过去对各种项目的评分行为，识别出相似的用户，然后尝试推荐下一个最佳项目。
- en: Getting ready
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: In this recipe, we will use the movielens dataset from the Grouplens research
    organization. The datasets (`movies.dat` and `ratings.dat`) can be downloaded
    from the following link. `Movies.dat` contains information of 3,883 movies and
    `Ratings.dat` contains information of 1,000,209 user ratings for these movies.
    The ratings range from 1 to 5, with 5 being the highest.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用Grouplens研究机构提供的movielens数据集。数据集（`movies.dat` 和 `ratings.dat`）可以通过以下链接下载。`Movies.dat`包含3,883部电影的信息，`Ratings.dat`包含1,000,209个用户对这些电影的评分。评分范围从1到5，5为最高分。
- en: '[http://files.grouplens.org/datasets/movielens/ml-1m.zip](http://files.grouplens.org/datasets/movielens/ml-1m.zip)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://files.grouplens.org/datasets/movielens/ml-1m.zip](http://files.grouplens.org/datasets/movielens/ml-1m.zip)'
- en: How to do it...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This recipe covers the steps for setting up collaborative filtering using an
    RBM.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程涵盖了使用RBM设置协同过滤的步骤。
- en: 'Read the `movies.dat` datasets in R:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在R中读取`movies.dat`数据集：
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Add a new column (`id_order`) to the movies dataset, as the current ID column
    (`UserID`) cannot be used to index movies because they range from 1 to 3,952:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向电影数据集添加一个新列（`id_order`），因为当前的ID列（`UserID`）不能用来索引电影，因为它的值范围是从1到3,952：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Read the `ratings.dat` dataset in R:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在R中读取`ratings.dat`数据集：
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Merge the movies and ratings datasets with `all=FALSE`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`all=FALSE`合并电影和评分数据集：
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Remove the non-required columns:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除不必要的列：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Convert the ratings to percentages:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将评分转换为百分比：
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Generate a matrix of ratings across all the movies for a sample of 1,000 users:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个包含1,000个用户对所有电影评分的矩阵：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Look at the distribution of the `trX` training dataset. It seems to follow
    a Bernoulli distribution (values in the range of 0 to 1):'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`trX`训练数据集的分布。它似乎遵循伯努利分布（值的范围在0到1之间）：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the input model parameters:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入模型参数：
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Start a new TensorFlow session:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个新的TensorFlow会话：
- en: '[PRE44]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Train the RBM using 500 epoch iterations and a batch size of 100:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用500个epoch迭代和批次大小100训练RBM：
- en: '[PRE45]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Plot reconstruction mean squared errors:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制重建均方误差：
- en: '[PRE46]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Performing a full run of training an RBM
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行完整的RBM训练过程
- en: 'Using the same RBM setup mentioned in the preceding recipe, train the RBM on
    the user ratings dataset (`trX`) using 20 hidden nodes. To keep a track of the
    optimization, the MSE is calculated after every batch of 1,000 rows. The following
    image shows the declining trend of mean squared reconstruction errors computed
    for 500 batches (equal to epochs):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面配方中提到的相同RBM设置，使用20个隐藏节点对用户评分数据集（`trX`）进行训练。为了跟踪优化过程，每训练完1,000行就会计算一次MSE。以下图片展示了计算得出的500批次（即epochs）均方重建误差的下降趋势：
- en: '![](img/00027.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: '**Looking into RBM recommendations**: Let''s now look into the recommendations
    generated by RBM-based collaborative filtering for a given user ID. Here, we will
    look into the top-rated genres and top-recommended genres of this user ID, along
    with the top 10 movie recommendations.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**查看RBM推荐**：现在让我们来看一下基于RBM的协同过滤为特定用户ID生成的推荐。在这里，我们将查看该用户ID的顶级评分流派和顶级推荐流派，并列出前10名电影推荐。'
- en: 'The following image illustrates a list of top-rated genres:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了顶级评分的流派列表：
- en: '![](img/00119.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: 'The following image illustrates a list of top-recommended genres:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了顶级推荐流派列表：
- en: '![](img/00145.jpeg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00145.jpeg)'
- en: Getting ready
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'This section provides the requirements for collaborative filtering the output
    evaluation:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了协同过滤输出评估的要求：
- en: TensorFlow in R is installed and set up
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装并设置TensorFlow for R
- en: The `movies.dat` and `ratings.dat` datasets are loaded in environment
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movies.dat`和`ratings.dat`数据集已加载到环境中'
- en: The recipe *Setting up a Restricted Boltzmann machine for Collaborative Filtering*
    has been executed
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配方*为协同过滤设置限制玻尔兹曼机*已执行
- en: How to do it...
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'This recipe covers the steps for evaluating the output from RBM-based collaborative
    filtering:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方包含评估RBM协同过滤输出的步骤：
- en: 'Select the ratings of a user:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个用户的评分：
- en: '[PRE47]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Remove the movies that were not rated by the user (assuming that they have
    yet to be seen):'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除用户未评分的电影（假设这些电影还未观看）：
- en: '[PRE48]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Plot the top genres seen by the user:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制用户查看的流派：
- en: '[PRE49]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Reconstruct the input vector to obtain the recommendation percentages for all
    the genres/movies:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重建输入向量以获取所有流派/电影的推荐百分比：
- en: '[PRE50]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Plot the top-recommended genres:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制顶级推荐流派：
- en: '[PRE51]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Find the top 10 recommended movies:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找前10个推荐电影：
- en: '[PRE52]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following image shows the top 10 recommended movies:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了前10个推荐电影：
- en: '![](img/00126.gif)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.gif)'
- en: Setting up a Deep Belief Network
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置深度置信网络（DBN）
- en: Deep belief networks are a type of **Deep Neural Network** (**DNN**), and are
    composed of multiple hidden layers (or latent variables). Here, the connections
    are present only between the layers and not within the nodes of each layer. The
    DBN can be trained both as an unsupervised and supervised model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 深度置信网络（DBN）是一种**深度神经网络**（**DNN**），由多个隐藏层（或潜在变量）组成。在这里，连接只存在于各层之间，而层内的节点之间没有连接。DBN可以作为无监督模型和有监督模型进行训练。
- en: The unsupervised model is used to reconstruct the input with noise removal and
    the supervised model (after pretraining) is used to perform classification. As
    there are no connections within the nodes in each layer, the DBNs can be considered
    as a set of unsupervised RBMs or autoencoders, where each hidden layer serves
    as a visible layer to its subsequent connected hidden layer.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督模型用于去噪并重建输入，而有监督模型（经过预训练后）用于执行分类。由于每一层中的节点之间没有连接，DBN可以看作是一个由无监督RBM或自编码器组成的集合，每个隐藏层作为其后续连接隐藏层的可见层。
- en: This kind of stacked RBM enhances the performance of input reconstruction where
    CD is applied across all layers, starting from the actual input training layer
    and finishing at the last hidden (or latent) layer.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这种堆叠RBM通过在所有层之间应用CD，增强了输入重建的性能，从实际的输入训练层开始，直到最后的隐藏（或潜在）层。
- en: 'DBNs are a type of graphical model that train the stacked RBMs in a greedy
    manner. Their networks tend to learn the deep hierarchical representation using
    joint distributions between the input feature vector *i* and hidden layers *h[1,2....m]*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: DBN是一种图模型，以贪婪的方式训练堆叠的RBM。它们的网络倾向于使用输入特征向量*i*和隐藏层*h[1,2....m]*之间的联合分布来学习深层次的层次表示：
- en: '![](img/00117.gif)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.gif)'
- en: 'Here, *i* = *h[0]* ; *P(h[k-1]|h[k])* is a conditional distribution of reconstructed
    visible units on the hidden layers of the RBM at level *k*; *P(h[m-1],h[m])* is
    the joint distribution of hidden and visible units (reconstructed) at the final
    RBM layer of the DBN. The following image illustrates a DBN of four hidden layers,
    where **W** represents the weight matrix:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*i* = *h[0]*；*P(h[k-1]|h[k])*是RBM第*k*层的隐藏层上重构可见单元的条件分布；*P(h[m-1],h[m])*是DBN最终RBM层的隐藏单元和可见单元（重构）的联合分布。下图展示了一个具有四个隐藏层的DBN，其中**W**表示权重矩阵：
- en: '![](img/00142.gif)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.gif)'
- en: DBNs can also be used to enhance the robustness of DNNs. DNNs face an issue
    of local optimization while implementing backpropagation. This is possible in
    scenarios where an error surface features numerous troughs, and the gradient descent,
    due to backpropagation occurs inside a local deep trough (not a global deep trough).
    DBNs, on the other hand, perform pretraining of the input features, which helps
    the optimization direct toward the global deepest trough, and then use backpropagation,
    to perform a gradient descent to gradually minimize the error rate.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: DBN还可以增强DNN的鲁棒性。DNN在实现反向传播时面临局部优化问题。在错误表面有许多低谷的情况下，反向传播导致梯度下降发生在局部深谷中（而不是全局深谷）。而DBN则对输入特征进行预训练，这有助于优化指向全局最深的低谷，然后使用反向传播执行梯度下降，以逐步最小化误差率。
- en: '**Training a stack of three RBMs**: In this recipe, we will train a DBN using
    three stacked RBMs, where the first hidden layer will have 900 nodes, the second
    hidden layer will have 500 nodes, and the third hidden layer will have 300 nodes.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练三个RBM的堆叠**：在本配方中，我们将使用三个堆叠的RBM训练一个DBN，其中第一个隐藏层有900个节点，第二个隐藏层有500个节点，第三个隐藏层有300个节点。'
- en: Getting ready
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好
- en: This section provides the requirements for TensorFlow.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了TensorFlow的要求。
- en: The dataset is loaded and set up
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集已加载并设置完毕
- en: 'Load the `TensorFlow` package using the following script:'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下脚本加载`TensorFlow`包：
- en: '[PRE53]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How to do it...
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做...
- en: 'This recipe covers the steps for setting up **Deep belief network** (**DBM**):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方涵盖了设置**深度置信网络**（**DBM**）的步骤：
- en: 'Define the number of nodes in each hidden layer as a vector:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个隐藏层的节点数定义为一个向量：
- en: '[PRE54]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Generate an RBM function leveraging the codes illustrated in the *Setting up
    a Restricted Boltzmann Machine for Bernoulli distribution input* recipe with the
    following input and output parameters mentioned:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用*为伯努利分布输入设置限制玻尔兹曼机*配方中展示的代码，生成一个RBM函数，使用以下输入和输出参数：
- en: '![](img/00153.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00153.jpeg)'
- en: 'Here is the function for setting up up the RBM:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这是设置RBM的函数：
- en: '[PRE55]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Train the RBM for all three different types of hidden nodes in a sequence.
    In other words, first train RBM1 with 900 hidden nodes, then use the output of
    RBM1 as an input for RBM2 with 500 hidden nodes and train RBM2, then use the output
    of RBM2 as an input for RBM3 with 300 hidden nodes and train RBM3\. Store the
    outputs of all three RBMs as a list, `RBM_output`:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按顺序训练三种不同类型的隐藏节点。换句话说，首先训练包含900个隐藏节点的RBM1，然后将RBM1的输出作为输入，训练包含500个隐藏节点的RBM2，再将RBM2的输出作为输入，训练包含300个隐藏节点的RBM3。将所有三个RBM的输出存储为一个列表，`RBM_output`：
- en: '[PRE56]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Create a data frame of batch errors across three hidden layers:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含三个隐藏层批次误差的数据框：
- en: '[PRE57]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Plot reconstruction mean squared errors:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制重构均方误差：
- en: '[PRE58]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: How it works...
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '**Assessing the performance of training three stacked RBMs**: Here, we will
    run five epochs (or iterations) for each RBM. Each epoch will perform batchwise
    (size = 100) optimization. In each batch, CD is computed and, accordingly, weights
    and biases are updated.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估训练三个堆叠RBM的性能**：在这里，我们将为每个RBM运行五个时期（或迭代）。每个时期将执行批量优化（批大小 = 100）。在每个批次中，计算CD并相应地更新权重和偏置。'
- en: 'To keep a track of optimization, the MSE is calculated after every batch of
    10,000 rows. The following image shows the declining trend of mean squared reconstruction
    errors computed for 30 batches for three RBMs separately:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪优化过程，在每批10,000行数据后计算均方误差。下图显示了为三种RBM分别计算的30个批次的均方重构误差下降趋势：
- en: '![](img/00041.jpeg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00041.jpeg)'
- en: Implementing a feed-forward backpropagation Neural Network
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现前馈反向传播神经网络
- en: In this recipe, we will implement a willow neural network with backpropagation.
    The input of the neural network is the outcome of the third (or last) RBM. In
    other words, the reconstructed raw data (`trainX`) is actually used to train the
    neural network as a supervised classifier of (10) digits. The backpropagation
    technique is used to further fine-tune the performance of classification.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将实现一个带有反向传播的柳树神经网络。神经网络的输入是第三个（或最后一个）RBM的结果。换句话说，重构的原始数据（`trainX`）实际上被用来训练神经网络，作为一个监督分类器来识别（10）个数字。反向传播技术用于进一步调整分类性能。
- en: Getting ready
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This section provides the requirements for TensorFlow.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了TensorFlow的要求。
- en: The dataset is loaded and set up
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集被加载并设置完毕
- en: The `TensorFlow` package is set up and loaded
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorFlow`包已经设置并加载'
- en: How to do it...
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'This section covers the steps for setting up a feed-forward backpropagation
    Neural Network:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了设置前馈反向传播神经网络的步骤：
- en: Let's define the input parameters of the neural network as function parameters.
    The following table describes each parameter:![](img/00047.jpeg)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将神经网络的输入参数定义为函数参数。下表描述了每个参数：![](img/00047.jpeg)
- en: 'The neural network function will have a structure as shown in the following
    script:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络函数将具有如下脚本中所示的结构：
- en: '[PRE59]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Initialize a weight and bias list of length 4, with the first being a tensor
    of random normal distribution (with a standard deviation of 0.01) of dimensions
    784 x 900, the second being 900 x 500, the third being 500 x 300, and the fourth
    being 300 x 10:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个长度为4的权重和偏置列表，第一个是一个784 x 900的标准差为0.01的正态分布张量，第二个是900 x 500，第三个是500 x 300，第四个是300
    x 10：
- en: '[PRE60]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Check whether the outcome of the stacked RBM conforms to the sizes of the hidden
    layers mentioned in the `dbn_sizes` parameter:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查堆叠的RBM的结果是否符合`dbn_sizes`参数中提到的隐藏层尺寸：
- en: '[PRE61]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, place the weights and biases in suitable positions within `weight_list`
    and `bias_list`:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将权重和偏置放置在`weight_list`和`bias_list`中的合适位置：
- en: '[PRE62]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Create placeholders for the input and output data:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据的占位符：
- en: '[PRE63]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, use the weights and biases obtained from the stacked RBM to reconstruct
    the input data and store each RBM''s reconstructed data in the list `input_sub`:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用从堆叠RBM中获得的权重和偏置来重构输入数据，并将每个RBM的重构数据存储在列表`input_sub`中：
- en: '[PRE64]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Define the cost function--that is, the mean squared error of difference between
    prediction and actual digits:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义成本函数——即预测与实际数字之间的均方误差：
- en: '[PRE65]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Implement backpropagation for the purpose of minimizing the cost:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现反向传播以最小化成本：
- en: '[PRE66]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Generate the prediction results:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测结果：
- en: '[PRE67]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Perform iterations of training:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行训练迭代：
- en: '[PRE68]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, return a list of four outcomes, which are train accuracy (`train_accuracy`),
    test accuracy (`test_accuracy`), a list of weight matrices generated in each iteration
    (`weight_list`), and a list of bias vectors generated in each iteration (`bias_list`):'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，返回四个结果的列表，分别是训练准确度（`train_accuracy`）、测试准确度（`test_accuracy`）、每次迭代生成的权重矩阵列表（`weight_list`）和每次迭代生成的偏置向量列表（`bias_list`）：
- en: '[PRE69]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Run the iterations for the defined neural network for training:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对定义的神经网络进行训练迭代：
- en: '[PRE70]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The following code is used to plot the train and test accuracy:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码用于绘制训练和测试准确度：
- en: '[PRE71]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works...
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: '**Assessing the train and test performance of the neural network**: The following
    image shows the increasing trend of train and test accuracy observed while training
    the neural network:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估神经网络的训练和测试性能**：下图显示了在训练神经网络时观察到的训练和测试准确度的增长趋势：'
- en: '![](img/00068.jpeg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpeg)'
- en: Setting up a Deep Restricted Boltzmann Machine
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置深度限制玻尔兹曼机
- en: Unlike DBNs, **Deep Restricted Boltzmann Machines** (**DRBM**) are undirected
    networks of interconnected hidden layers with the capability to learn joint probabilities
    over these connections. In the current setup, centering is performed where visible
    and hidden variables are subtracted from offset bias vectors after every iteration.
    Research has shown that centering optimizes the performance of DRBMs and can reach
    higher log-likelihood values in comparison with traditional RBMs.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 与DBNs不同，**深度限制玻尔兹曼机** (**DRBM**) 是由互联的隐藏层组成的无向网络，能够学习这些连接上的联合概率。在当前的设置中，每次迭代后，能见度和隐藏变量会从偏置偏移向量中减去，进行中心化处理。研究表明，中心化优化了DRBM的性能，并且与传统的RBM相比，可以达到更高的对数似然值。
- en: Getting ready
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'This section provides the requirements for setting up a DRBM:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了设置DRBM的要求：
- en: The `MNIST` dataset is loaded and set up
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已加载并设置 `MNIST` 数据集：
- en: The `tensorflow` package is set up and loaded
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已设置并加载 `tensorflow` 包：
- en: How to do it...
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'This section covers detailed the steps for setting up the DRBM model using
    TensorFlow in R:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了如何在R中使用TensorFlow设置DRBM模型的步骤：
- en: 'Define the parameters for the DRBM:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义DRBM的参数：
- en: '[PRE72]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Define a sigmoid function using a hyperbolic arc tangent *[(log(1+x) -log(1-x))/2]*:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用双曲正切定义一个sigmoid函数 *[(log(1+x) -log(1-x))/2]*：
- en: '[PRE73]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define a sigmoid function using only a hyperbolic tangent *[(e^x-e^(-x))/(e^x+e^(-x))]*:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用双曲正切定义一个sigmoid函数 *[(e^x-e^(-x))/(e^x+e^(-x))]*：
- en: '[PRE74]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Define a `binarize` function to return a matrix of binary values (0,1):'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `binarize` 函数，返回一个二值矩阵（0,1）：
- en: '[PRE75]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Define a `re_construct` function to return a matrix of pixels:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `re_construct` 函数，返回一个像素矩阵：
- en: '[PRE76]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define a function to perform `gibbs` activation for a given layer:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行给定层的 `gibbs` 激活：
- en: '[PRE77]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Define a function to perform the reparameterization of bias vectors:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行偏置向量的重参数化：
- en: '[PRE78]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Define a function to perform the reparameterization of offset bias vectors:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行偏置偏移向量的重参数化：
- en: '[PRE79]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Define a function to initialize weights, biases, offset biases, and input matrices:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来初始化权重、偏置、偏置偏移和输入矩阵：
- en: '[PRE80]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Use the MNIST train data (`trainX`) introduced in the previous recipes. Standardize
    the `trainX` data by dividing it by 255:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前面配方中引入的MNIST训练数据（`trainX`）。通过将 `trainX` 数据除以255，进行标准化处理：
- en: '[PRE81]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Generate the initial weight matrices, bias vectors, offset bias vectors, and
    input matrices:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成初始的权重矩阵、偏置向量、偏置偏移向量和输入矩阵：
- en: '[PRE82]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Subset a sample (`minbatch_size`) of the input data `X`:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入数据 `X` 进行子集化（`minbatch_size`）：
- en: '[PRE83]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Perform a set of 1,000 iterations. Within each iteration, update the initial
    weights and biases 100 times and plot the images of the weight matrices:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行1,000次迭代。在每次迭代中，更新初始权重和偏置100次，并绘制权重矩阵的图像：
- en: '[PRE84]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: How it works...
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'As the preceding DRBM is trained using two hidden layers, we generate two weight
    matrices. The first weight matrix defines the connection between the visible layer
    and the first hidden layer. The second weight matrix defines the connection between
    the first and second hidden layer. The following image shows pixel images of the
    first weight matrices:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面的DRBM使用了两个隐藏层进行训练，我们生成了两个权重矩阵。第一个权重矩阵定义了可见层和第一个隐藏层之间的连接。第二个权重矩阵定义了第一个隐藏层和第二个隐藏层之间的连接。下图显示了第一个权重矩阵的像素图像：
- en: '![](img/00006.jpeg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00006.jpeg)'
- en: 'The following image shows the second pixel images of the second weight matrix:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了第二个权重矩阵的第二个像素图像：
- en: '![](img/00116.gif)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.gif)'
