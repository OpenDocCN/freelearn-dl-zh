- en: '*Chapter 7*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*'
- en: Long Short-Term Memory (LSTM)
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将能够：
- en: Describe the purpose of an LSTM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述LSTM的目的
- en: Evaluate the architecture of an LSTM in detail
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详细评估LSTM的架构
- en: Develop a simple binary classification model using LSTMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM开发一个简单的二分类模型
- en: Implement neural language translation and develop an English-to-German translation
    model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现神经语言翻译，并开发一个英德翻译模型
- en: This chapter briefly introduces you to the LSTM architecture and its applications
    in the world of natural language processing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了LSTM架构及其在自然语言处理领域的应用。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapters, we studied Recurrent Neural Networks (RNNs) and a
    specialized architecture called the Gated Recurrent Unit (GRU), which helps combat
    the vanishing gradient problem. LSTMs offer yet another way to tackle the vanishing
    gradient problem. In this chapter, we will take a look at the architecture of
    LSTMs and see how they enable a neural network to propagate gradients in a faithful
    manner.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了递归神经网络（RNN）以及一种专门的架构——门控递归单元（GRU），它有助于解决梯度消失问题。LSTM提供了另一种解决梯度消失问题的方法。在本章中，我们将仔细研究LSTM的架构，看看它们如何使神经网络以忠实的方式传播梯度。
- en: Additionally, we will look at an interesting application of LSTMs in the form
    of neural language translation, which will empower us to build a model that can
    be used to translate text given in one language to another language.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将看到LSTM在神经语言翻译中的有趣应用，这将使我们能够构建一个模型，用于将一种语言的文本翻译成另一种语言。
- en: LSTM
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM
- en: The vanishing gradient problem makes it difficult for the gradient to propagate
    from the later layers in the network to the early layers, causing the initial
    weights of the network to not change much from the initial values. Thus, the model
    doesn't learn well and leads to poor performance. LSTMs solve the issue by introducing
    a "memory" to the network, which leads to the retention of long-term dependencies
    in the text structure. However, LSTMs add memory in a way that is different from
    the GRU's method. In the following sections, we will see how LSTMs accomplish
    this task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题使得梯度很难从网络的后层传播到前层，导致网络的初始权重不会从初始值发生太大变化。因此，模型无法很好地学习，导致性能不佳。LSTM通过引入“记忆”到网络中解决了这个问题，这使得文本结构中的长期依赖得以保持。然而，LSTM的记忆添加方式与GRU的方式不同。在接下来的章节中，我们将看到LSTM是如何完成这一任务的。
- en: An LSTM helps a network to remember long-term dependencies in an explicit manner.
    As in the case of the GRU, this is achieved by introducing more variables in the
    structure of a simple RNN.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM帮助网络以明确的方式记住长期依赖性。与GRU相似，这是通过在简单RNN的结构中引入更多变量来实现的。
- en: Using LSTMs, we allow the network to transfer most of the knowledge from the
    activation of previous timesteps, a feat difficult to achieve with simple RNNs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSTM，我们允许网络从先前时间步的激活中转移大部分知识，这是简单RNN难以实现的壮举。
- en: 'Recall the structure of the simple RNN; it''s essentially an unfolding of the
    same unit and can be represented by the following diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下简单RNN的结构，它本质上是相同单元的展开，可以通过以下图示表示：
- en: '![Figure 7.1: The repeating module in a standard RNN](img/C13783_07_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1：标准RNN中的重复模块](img/C13783_07_01.jpg)'
- en: 'Figure 7.1: The repeating module in a standard RNN'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.1：标准RNN中的重复模块
- en: The recurrence of block "**A**"in the diagram signifies that it is the same
    structure that is repeated over time. The input to each unit is an activation
    from the previous timestep (represented by the letter "**h**"). Another input
    is the sequence value at time "**t**" (represented by the letter "**x**").
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图中块"**A**"的递归表示它是随着时间重复的相同结构。每个单元的输入是来自先前时间步的激活（由字母"**h**"表示）。另一个输入是时间"**t**"时的序列值（由字母"**x**"表示）。
- en: 'Similar to the case with a simple RNN, LSTMs also have a fixed, time-unfolding,
    repeating structure, but the repeated unit itself has a different structure. Each
    unit of an LSTM has several different kinds of modules that interoperate to impart
    memory to the model. An LSTM''s structure can be represented by the following
    diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单RNN的情况类似，LSTM也具有固定的、时间展开的重复结构，但重复的单元本身具有不同的结构。每个LSTM单元有几种不同类型的模块，它们共同作用，为模型提供记忆。LSTM的结构可以通过以下图示表示：
- en: '![Figure 7.2: The LSTM unit'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.2：LSTM单元'
- en: '](img/C13783_07_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_02.jpg)'
- en: 'Figure 7.2: The LSTM unit'
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.2：LSTM 单元
- en: 'Let''s also get familiar with the notations we''ll be using for the diagrams:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来熟悉一下在图示中使用的符号：
- en: '![Figure 7.3: Notations used in the model'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3：模型中使用的符号'
- en: '](img/C13783_07_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_03.jpg)'
- en: 'Figure 7.3: Notations used in the model'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.3：模型中使用的符号
- en: The most essential component of an LSTM is the cell state, henceforth represented
    by the letter "**C**". The cell state can be depicted by a constant bold line
    on the upper end of the boxes in the following diagram. It is often convenient
    to think of this line as a conveyor belt running through different time instances
    and carrying some information. Although there are several operations that can
    affect the value that propagates through the cell state, in practice, it is very
    easy for the information from previous cell states to reach the next cell state.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的最核心组件是单元状态，以下简称为字母“**C**”。单元状态可以通过下图中方框上端的粗线表示。通常，我们可以将这条线视为一条传送带，贯穿不同的时间步，并传递一些信息。尽管有多个操作可以影响传播通过单元状态的值，但实际上，来自前一个单元状态的信息很容易到达下一个单元状态。
- en: '![Figure 7.4: Cell state'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.4：单元状态'
- en: '](img/C13783_07_04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_04.jpg)'
- en: 'Figure 7.4: Cell state'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.4：单元状态
- en: It would be useful to understand LSTMs as seen from the perspective of the modification
    of this cell state. As with GRUs, the components of LSTMs that allow the modification
    of the cell state are called "*gates*".
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从修改单元状态的角度理解 LSTM 会非常有用。与 GRU 一样，LSTM 中允许修改单元状态的组件被称为“*门*”。
- en: An LSTM operates over several steps, which are described in the sections that
    follow.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 在多个步骤中操作，具体步骤将在接下来的章节中描述。
- en: The Forget Gate
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遗忘门
- en: 'The forget gate is responsible for determining the cell state content that
    should be forgotten from the previous timestep. The expression for the forget
    gate is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门负责确定应该从前一个时间步中忘记的单元状态内容。遗忘门的表达式如下：
- en: '![Figure 7.5: Expression for the forget gate'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5：遗忘门的表达式'
- en: '](img/C13783_07_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_05.jpg)'
- en: 'Figure 7.5: Expression for the forget gate'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.5：遗忘门的表达式
- en: 'The input at timestep **t** is multiplied by a new set of weights, **W_f**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_f**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a sigmoid function to squish
    the output, **f[t]**, within a range of [0,1]. The output has the same number
    of dimensions as there are in cell state vector C (**n_h**,**1**). The forget
    gate outputs a ''1'' or a ''0'' for each dimension. A value of ''1'' signifies
    that all information from the previous cell state for this dimension should pass,
    retained, while a value ''0'' indicates that all information from the previous
    cell state for this dimension should be forgotten. Diagrammatically, it can be
    represented as shown:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 **t**，输入与一组新的权重 **W_f** 相乘，维度为（**n_h**，**n_x**）。来自前一个时间步的激活值（**h[t-1]**）与另一组新的权重
    **U_f** 相乘，维度为（**n_h**，**n_h**）。请注意，这些乘法是矩阵乘法。然后这两个项相加，并通过 Sigmoid 函数进行压缩，使输出
    **f[t]** 的值保持在 [0,1] 范围内。输出的维度与单元状态向量 C 的维度相同（**n_h**，**1**）。遗忘门为每个维度输出 '1' 或
    '0'。值为 '1' 表示该维度的前一个单元状态的所有信息应通过并保留，而值为 '0' 表示该维度的前一个单元状态的所有信息应被忘记。图示如下：
- en: '![Figure 7.6: The forget gate](img/C13783_07_06.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6：遗忘门](img/C13783_07_06.jpg)'
- en: 'Figure 7.6: The forget gate'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.6：遗忘门
- en: 'So, how does the output of the forget gate impact the sentence construction?
    Let''s take a look at the generated sentence:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，遗忘门的输出如何影响句子的构建呢？让我们来看一下生成的句子：
- en: '"*Jack goes for a walk when his daughter goes to bed*."'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '"*Jack goes for a walk when his daughter goes to bed*"。'
- en: The first subject in the sentence is 'Jack,' which connotes the male gender.
    The cell state representing the gender of the subject has a value corresponding
    to 'Male' (this could be 0 or 1). Now, up to the word 'his' in the sentence, the
    subject of the sentence does not change, and the cell state for the subject's
    gender continues having the 'male' value. The next word, however, 'daughter,'
    is a new subject and hence there is a need to forget the old value in the cell
    state that represents the gender. Note that even if the old gender state was female,
    there is still a need to forget this value so that a value corresponding to the
    new subject can be used.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的第一个主语是 'Jack'，表示男性性别。表示主语性别的单元状态值对应于 'Male'（这可以是 0 或 1）。直到句子中的 'his'，主语没有改变，主语性别的单元状态继续保持
    'male' 值。然而，接下来的单词 'daughter' 是新的主语，因此需要遗忘表示性别的旧值。注意，即使旧的性别状态是女性，仍然需要遗忘该值，以便使用与新主语对应的值。
- en: The forget gate accomplishes the 'forget' operation by setting the subject gender
    value to 0 (that is, f[t] will output 0 for the said dimension).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门通过将主语性别值设置为 0 来完成“遗忘”操作（也就是说，f[t] 在该维度上输出 0）。
- en: 'In Python, the forget gate can be calculated with the following code snippet:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，可以使用以下代码片段计算遗忘门：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code produces the following output for `h_prev` and `x`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码为 `h_prev` 和 `x` 生成以下输出：
- en: '![Figure 7.7: Output for the previous state, ‘h_prev,’ and the current input,
    ‘x’'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7：先前状态 ''h_prev'' 和当前输入 ''x'' 的输出'
- en: '](img/C13783_07_07.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_07.jpg)'
- en: 'Figure 7.7: Output for the previous state, ''h_prev,'' and the current input,
    ''x'''
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.7：先前状态 'h_prev' 和当前输入 'x' 的输出
- en: 'We can initialize some dummy values for `W_f` and `U_f`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为 `W_f` 和 `U_f` 初始化一些虚拟值：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This produces the following values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下值：
- en: '![Figure 7.8: Output of the matrix values'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8：矩阵值的输出'
- en: '](img/C13783_07_08.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_08.jpg)'
- en: 'Figure 7.8: Output of the matrix values'
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.8：矩阵值的输出
- en: 'Now the forget gate can be calculated:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以计算遗忘门：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following values for `f[t]`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成 `f[t]` 的以下值：
- en: '![Figure 7.9: Output of the forget gate, f[t]'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9：遗忘门的输出，f[t]'
- en: '](img/C13783_07_09.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_09.jpg)'
- en: 'Figure 7.9: Output of the forget gate, f[t]'
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.9：遗忘门的输出，f[t]
- en: The Input Gate and the Candidate Cell State
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入门和候选单元状态
- en: 'At each timestep, a new candidate cell state is also calculated using the following
    expression:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，新的候选单元状态也通过以下表达式计算：
- en: '![Figure 7.10: Expression for candidate cell state'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.10：候选单元状态的表达式'
- en: '](img/C13783_07_10.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_10.jpg)'
- en: 'Figure 7.10: Expression for candidate cell state'
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.10：候选单元状态的表达式
- en: 'The input at timestep **t** is multiplied by a new set of weights, **W_c**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_c**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a hyperbolic tan function to
    squish the output, **f[t]**, within a range of [-1,1]. The output, **C_candidate**,
    has the dimensions (**n_h**,**1**). In the diagram that follows, the candidate
    cell state is represented by C tilde:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步 **t** 的输入与一组新的权重 **W_c** 相乘，维度为 (**n_h**, **n_x**)。来自前一时间步的激活值 (**h[t-1]**)
    与另一组新的权重 **U_c** 相乘，维度为 (**n_h**, **n_h**)。注意，这些乘法是矩阵乘法。然后，这两个项相加并通过双曲正切函数进行压缩，输出
    **f[t]** 在 [-1,1] 范围内。输出 **C_candidate** 的维度为 (**n_h**,**1**)。在随后的图中，候选单元状态由 C
    波浪线表示：
- en: '![Figure 7.11: Input gate and candidate state'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.11：输入门和候选状态'
- en: '](img/C13783_07_11.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_11.jpg)'
- en: 'Figure 7.11: Input gate and candidate state'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.11：输入门和候选状态
- en: The candidate aims at calculating the cell state that it deduces from the current
    timestep. In our example sentence, this corresponds to calculating the new subject
    gender value. This candidate cell state is not passed as is to update the next
    cell state but is regulated by an input gate.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 候选值旨在计算它从当前时间步推断的单元状态。在我们的示例句子中，这对应于计算新的主语性别值。这个候选单元状态不会直接传递给下一个单元状态进行更新，而是通过输入门进行调节。
- en: 'The input gate determines which values of the candidate cell state get passed
    on to the next cell state. The following expression can be used to calculate the
    input gate value:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门决定了候选单元状态中的哪些值将传递到下一个单元状态。可以使用以下表达式来计算输入门的值：
- en: '![Figure 7.12: Expression for the input gate value'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.12：输入门值的表达式'
- en: '](img/C13783_07_12.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_12.jpg)'
- en: 'Figure 7.12: Expression for the input gate value'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.12：输入门值的表达式
- en: The input at timestep **t** is multiplied by a new set of weights, **W_i**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_i**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a sigmoid function to squish
    the output, **i[t]**, within a range of **[0,1]**. The output has the same number
    of dimensions as there are in cell state vector **C** (**n_h**, **1**). In our
    example sentence, after reaching the word 'daughter,' there is a need to update
    the cell state for the values that correspond to the gender of the subject. After
    having calculated the new candidate value for the subject gender through the candidate
    cell state, only the dimension corresponding to the subject gender is set to 1
    in the input gate vector.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步 **t** 的输入被乘以一组新的权重，**W_i**，其维度为 (**n_h**, **n_x**)。上一时间步的激活值 (**h[t-1]**)
    被乘以另一组新的权重，**U_i**，其维度为 (**n_h**, **n_h**)。请注意，这些乘法是矩阵乘法。然后，这两个项相加并通过一个 sigmoid
    函数来压缩输出，**i[t]**，使其范围在 **[0,1]** 之间。输出的维度与单元状态向量 **C** 的维度相同 (**n_h**, **1**)。在我们的示例句子中，在到达词语“daughter”后，需要更新单元状态中与主语性别相关的值。通过候选单元状态计算出新的主语性别候选值后，仅将与主语性别对应的维度设置为
    1，输入门向量中的其他维度不变。
- en: 'The Python code snippet for the candidate cell state and input gate is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 用于候选单元状态和输入门的 Python 代码片段如下：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following values for the matrices:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这会为矩阵产生以下值：
- en: '![Figure 7.13: Screenshot of values of matrices for candidate cell state and
    input gate'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.13：候选单元状态和输入门的矩阵值截图'
- en: '](img/C13783_07_13.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_13.jpg)'
- en: 'Figure 7.13: Screenshot of values of matrices for candidate cell state and
    input gate'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.13：候选单元状态和输入门的矩阵值截图
- en: 'The input gate can be calculated as shown:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门可以按如下方式计算：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This outputs the following value for `i`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下的`i`值：
- en: '![Figure 7.14: Screenshot of output of input gate'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14：输入门输出的截图'
- en: '](img/C13783_07_14.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_14.jpg)'
- en: 'Figure 7.14: Screenshot of output of input gate'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.14：输入门输出的截图
- en: 'To calculate the candidate cell state, we first initialize the `W_c` and `U_c`
    matrices:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算候选单元状态，我们首先初始化`W_c`和`U_c`矩阵：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The values produced for these matrices are as given:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些矩阵所产生的值如下所示：
- en: '![Figure 7.15: Screenshot for values of matrices W_c and U_c'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.15：矩阵 W_c 和 U_c 值的截图'
- en: '](img/C13783_07_15.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_15.jpg)'
- en: 'Figure 7.15: Screenshot for values of matrices W_c and U_c'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.15：矩阵 W_c 和 U_c 值的截图
- en: 'We can now use the update equation for the candidate cell state:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用候选单元状态的更新方程：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The candidate cell state produces the following value:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 候选单元状态产生以下值：
- en: '![Figure 7.16: Screenshot of the candidate cell state'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.16：候选单元状态的截图'
- en: '](img/C13783_07_16.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_16.jpg)'
- en: 'Figure 7.16: Screenshot of the candidate cell state'
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.16：候选单元状态的截图
- en: Cell State Update
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单元状态更新
- en: 'At this point, we know what should be forgotten from the old cell state (forget
    gate), what should be allowed to affect the new cell state (input gate), and what
    value the candidate cell change should have (candidate cell state). Now, the cell
    state for the current timestep can be calculated as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点，我们已经知道应该忘记旧单元状态中的哪些内容（遗忘门），应该允许哪些内容影响新的单元状态（输入门），以及候选单元状态的变化应该是什么值（候选单元状态）。现在，可以按以下方式计算当前时间步的单元状态：
- en: '![Figure 7.17: Expression for cell state update'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.17：单元状态更新的表达式'
- en: '](img/C13783_07_17.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_17.jpg)'
- en: 'Figure 7.17: Expression for cell state update'
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.17：单元状态更新的表达式
- en: In the preceding expression, '**hadamard**' represents element-wise multiplications.
    So, the forget gate gets multiplied element wise with the old cell state, allowing
    it to forget the gender of the subject in our example sentence. On the other hand,
    the input gate allows the new candidate value for the gender of the subject to
    affect the new cell state. These two terms are then added element-wise so that
    the current cell state now has a subject gender that corresponds to a value that
    corresponds to 'female.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，'**hadamard**'表示逐元素相乘。因此，遗忘门与旧的单元状态逐元素相乘，允许它在我们的示例句子中忘记主语的性别。另一方面，输入门允许新的候选性别值影响新的单元状态。这两个项逐元素相加，使得当前单元状态现在具有与'female'对应的性别值。
- en: The next diagram depicts the operation
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了该操作
- en: '![Figure 7.18: Updated cell state'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.18：更新后的单元状态'
- en: '](img/C13783_07_18.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_18.jpg)'
- en: 'Figure 7.18: Updated cell state'
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.18：更新后的单元状态
- en: Here is the code snippet for producing the current cell state.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成当前单元状态的代码片段。
- en: 'First, initialize a value for the previous cell state:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为之前的单元状态初始化一个值：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The value becomes the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 计算结果如下：
- en: '![Figure 7.19: Screenshot for output of updated cell state'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.19：更新后的单元状态输出截图'
- en: '](img/C13783_07_19.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_19.jpg)'
- en: 'Figure 7.19: Screenshot for output of updated cell state'
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.19：更新后的单元状态输出截图
- en: Output Gate and Current Activation
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出门和当前激活值
- en: 'Note that all we have done is update the cell state until now. We need to generate
    the activation for the current state as well; that is, (**h[t]**). This is done
    using an output gate that is calculated as given:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，到目前为止，我们所做的只是更新单元状态。我们还需要为当前状态生成激活值；即（**h[t]**）。这是通过输出门来实现的，输出门的计算方式如下：
- en: '![Figure 7.20: Expression for output gate.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.20：输出门的表达式'
- en: '](img/C13783_07_20.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_20.jpg)'
- en: 'Figure 7.20: Expression for output gate.'
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.20：输出门的表达式
- en: The input at timestep **t** is multiplied by a new set of weights, **W_o**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_o**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a sigmoid function to squish
    the output, **o[t]**, within a range of [0,1]. The output has the same number
    of dimensions as there are in cell state vector **h** (**n_h**, **1**).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步**t**的输入与一组新的权重**W_o**相乘，权重的维度为（**n_h**，**n_x**）。上一时间步的激活值（**h[t-1]**）与另一组新的权重**U_o**相乘，权重的维度为（**n_h**，**n_h**）。请注意，这些乘法是矩阵乘法。然后，将这两个项相加，并通过sigmoid函数压缩输出值**o[t]**，使其落在[0,1]范围内。输出的维度与单元状态向量**h**的维度相同（**n_h**，**1**）。
- en: 'The output gate is responsible for regulating the amount by which the current
    cell state is allowed to affect the activation value for the timestep. In our
    example sentence, it is worth propagating the information that depicts whether
    the subject is singular or plural such that the correct verb form may be used.
    For example, if the word following the word ''daughter'' is a verb such as ''goes,''
    it is important to use the correct form of the word, ''go''. Hence, the output
    gate allows relevant information to be passed on to the activation, which then
    goes as an input to the next timestep. In the next diagram, the output gate is
    represented as **o_t**:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门负责调节当前单元状态对时间步的激活值的影响程度。在我们的示例句子中，值得传播的信息是描述主语是单数还是复数，以便使用正确的动词形式。例如，如果“daughter”后面的单词是动词“goes”，那么使用正确的形式“go”就显得非常重要。因此，输出门允许相关信息传递给激活值，这个激活值随后作为输入传递到下一个时间步。在下图中，输出门表示为**o_t**：
- en: '![Figure 7.21: Output gate and current activation'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.21：输出门和当前激活值'
- en: '](img/C13783_07_21.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_21.jpg)'
- en: 'Figure 7.21: Output gate and current activation'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.21：输出门和当前激活值
- en: 'The following code snippet shows how the value for the output gate can be calculated:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何计算输出门的值：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This produces the following output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下输出：
- en: '![Figure 7.22: Screenshot for output of matrices W_o and U_o'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.22：矩阵W_o和U_o输出的截图'
- en: '](img/C13783_07_22.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_22.jpg)'
- en: 'Figure 7.22: Screenshot for output of matrices W_o and U_o'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.22：矩阵W_o和U_o输出的截图
- en: 'Now the output can be calculated:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以计算输出：
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The value of the output gate is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门的值如下：
- en: '![Figure 7.23: Screenshot of the value of the output gate'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.23：输出门值的截图'
- en: '](img/C13783_07_23.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_23.jpg)'
- en: 'Figure 7.23: Screenshot of the value of the output gate'
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.23：输出门值的截图
- en: 'Once the output gate is evaluated, the value of the next activation can be
    calculated:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输出门被评估，就可以计算下一个激活的值：
- en: '![Figure 7.24: Expression to calculate the value of the next activation'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.24：计算下一个激活值的表达式'
- en: '](img/C13783_07_24.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_24.jpg)'
- en: 'Figure 7.24: Expression to calculate the value of the next activation'
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.24：计算下一个激活值的表达式
- en: First, a hyperbolic tangent function is applied to the current cell state. This
    limits the values in the vector between -1 and 1\. Then, an element-wise product
    of this value is done with the output gate value that was just calculated.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，应用一个双曲正切函数到当前单元状态。这将限制向量中的值在 -1 和 1 之间。然后，将此值与刚计算出的输出门值做逐元素乘积。
- en: 'Let''s see the code snippet for calculating the current timestep activation:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下计算当前时间步激活的代码片段：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This finally produces the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这最终会生成如下结果：
- en: '![Figure 7.25: Screenshot for the current timestep activation'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.25：当前时间步激活的截图'
- en: '](img/C13783_07_25.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_25.jpg)'
- en: 'Figure 7.25: Screenshot for the current timestep activation'
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.25：当前时间步激活的截图
- en: Now let's build a very simple binary classifier to demonstrate the use of an
    LSTM.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建一个非常简单的二元分类器来演示 LSTM 的使用。
- en: 'Exercise 27: Building an LSTM-Based Model to Classify an Email as Spam or Not
    Spam (Ham)'
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 27：构建一个基于 LSTM 的模型来将电子邮件分类为垃圾邮件或非垃圾邮件（正常邮件）
- en: 'In this exercise, we will be building an LSTM-based model that will help us
    classify emails as spam or genuine:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建一个基于 LSTM 的模型，帮助我们将电子邮件分类为垃圾邮件或真实邮件：
- en: 'We will start by importing the required Python packages:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入所需的 Python 包开始：
- en: '[PRE11]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note:'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: The LSTM unit has been imported the same way as you would for a simple RNN or
    GRU.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSTM 单元已经按照你为简单的 RNN 或 GRU 导入的方式导入。
- en: We can now read the input file containing a column that contains text and another
    column that contains the label for the text depicting whether the text is spam
    or not.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以读取包含文本列和另一列标签的输入文件，该标签指示文本是否为垃圾邮件。
- en: Note
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: '[PRE12]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The data looks as depicted here:![Figure 7.26: Screenshot of the output for
    spam classification'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据看起来如下所示：![图 7.26：垃圾邮件分类输出的截图
- en: '](img/C13783_07_26.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_07_26.jpg)'
- en: 'Figure 7.26: Screenshot of the output for spam classification'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.26：垃圾邮件分类输出的截图
- en: 'There are some irrelevant columns as well, but we only need the columns containing
    the text data and labels:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有一些无关的列，但我们只需要包含文本数据和标签的列：
- en: '[PRE13]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output should be as follows:![Figure 7.27: Screenshot for columns with
    text and labels'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出应该如下所示：![图 7.27：带有文本和标签的列的截图
- en: '](img/C13783_07_27.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_07_27.jpg)'
- en: 'Figure 7.27: Screenshot for columns with text and labels'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.27：带有文本和标签的列的截图
- en: 'We can check the label distribution:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以检查标签分布：
- en: '[PRE14]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The label distribuiton would look like this:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签分布看起来如下：
- en: '![Figure 7.28: Screenshot for label distribution'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.28：标签分布的截图'
- en: '](img/C13783_07_28.jpg)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_07_28.jpg)'
- en: 'Figure 7.28: Screenshot for label distribution'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.28：标签分布的截图
- en: 'We can now map the label distribution to 0/1 so that it can be fed to a classifier.
    Also, an array is created to contain the texts:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以将标签分布映射为 0/1，以便它可以被馈送到分类器中。同时，还会创建一个数组来存储文本：
- en: '[PRE15]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces output X and Y as follows:![Figure 7.29: Screenshot for output
    X'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成如下的输出 X 和 Y：![图 7.29：输出 X 的截图
- en: '](img/C13783_07_29.jpg)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_07_29.jpg)'
- en: 'Figure 7.29: Screenshot for output X'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.29：输出 X 的截图
- en: '![Figure 7.30: Screenshot for output Y'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.30：输出 Y 的截图'
- en: '](img/C13783_07_30.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_07_30.jpg)'
- en: 'Figure 7.30: Screenshot for output Y'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.30：输出 Y 的截图
- en: 'Next, we will restrict the maximum number of tokens to be generated for the
    100 most frequent words. We will initialize a tokenizer that assigns an integer
    value to each word being used in the text corpus:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将限制为 100 个最常见单词生成的最大标记数。我们将初始化一个分词器，为文本语料库中使用的每个单词分配一个整数值：
- en: '[PRE16]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will produce a `text_tokenized` value:![](img/C13783_07_31.jpg)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成一个 `text_tokenized` 值：![](img/C13783_07_31.jpg)
- en: 'Figure 7.31: Screenshot for the output of tokenized values'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.31：分词后值的输出截图
- en: Note that since we restricted the maximum number words to be 100, only the words
    in the text that fall within the top 100 most frequent words will be assigned
    an integer index. The rest of the works will be ignored. So, even though the first
    sequence in X has 20 words, there are 6 indices in the tokenized representation
    of this sentence.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，由于我们限制了最大单词数为 100，因此只有文本中排名前 100 的最常见单词会被分配整数索引。其余的单词将被忽略。因此，即使 X 中的第一个序列有
    20 个单词，在该句子的标记化表示中也只有 6 个索引。
- en: 'Next, we will allow a maximum sequence length of 50 words per sequence and
    pad the sequences that are shorter than this length. The longer sequences, on
    the other hand, get truncated:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将允许每个序列的最大长度为 50 个单词，并填充那些短于此长度的序列。而较长的序列则会被截断：
- en: '[PRE17]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.32: Screenshot for padded sequences'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.32：填充序列的截图'
- en: '](img/C13783_07_32.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_07_32.jpg)'
- en: 'Figure 7.32: Screenshot for padded sequences'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.32：填充序列的截图
- en: Note that the padding was done in the 'pre' mode, meaning that the initial part
    of the sequences get padded to make the sequence length equal to max_len.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，填充是在“pre”模式下进行的，这意味着序列的初始部分会被填充，以使序列长度等于 max_len。
- en: 'Next, we define the model with the LSTM layer having 64 hidden units and fit
    it to our sequence data with the respective target values:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型，LSTM 层具有 64 个隐藏单元，并将其拟合到我们的序列数据和相应的目标值：
- en: '[PRE18]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we start with an embedding layer, which ensures a fixed size for input
    to the network (20). We have a dense layer with a single sigmoid output, which
    indicates whether the target variable is 0 or 1\. We then compile the model with
    binary cross-entropy as the loss function and use Adam as the optimization strategy.
    After that, we fit the model to our data with a batch size of 128 and an epoch
    count of 10\. Note that we also keep aside 20% of the training data as validation
    data. This starts a training session:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们从一个嵌入层开始，确保输入到网络的固定大小（20）。我们有一个包含单个 sigmoid 输出的全连接层，该输出表示目标变量是 0 还是 1。然后，我们用二元交叉熵作为损失函数，并使用
    Adam 作为优化策略来编译模型。之后，我们以批大小为 128 和 epoch 数为 10 的配置拟合模型。请注意，我们还保留了 20% 的训练数据作为验证数据。这开始了训练过程：
- en: '![Figure 7.33: Screenshot of model fitting to 10 epochs'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.33：模型拟合到 10 个 epoch 的截图'
- en: '](img/C13783_07_33.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_33.jpg)'
- en: 'Figure 7.33: Screenshot of model fitting to 10 epochs'
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.33：模型拟合到 10 个 epoch 的截图
- en: After 10 epochs, a validation accuracy of 96% is achieved. This is remarkably
    good performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在 10 个 epoch 后，达到了 96% 的验证准确率。这是一个非常好的表现。
- en: 'We can now try some test sequences and obtain the probability of the sequence
    being spam:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试一些测试序列并获得该序列是垃圾邮件的概率：
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Expected output:**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 7.34: Screenshot of the output of model prediction'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.34：模型预测输出的截图'
- en: '](img/C13783_07_34.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_34.jpg)'
- en: 'Figure 7.34: Screenshot of the output of model prediction'
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.34：模型预测输出的截图
- en: There is a very high probability of the test text being spam.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 测试文本是垃圾邮件的概率非常高。
- en: 'Activity 9: Building a Spam or Ham Classifier Using a Simple RNN'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 9：使用简单 RNN 构建垃圾邮件或非垃圾邮件分类器
- en: We will be building a spam-or-ham classifier using a simple RNN with the same
    hyperparameters as earlier and compare the performance with that of our LSTM-based
    solution. For a simple dataset such as this, a simple RNN would perform very close
    to an LSTM. However, this is usually not the case with more complex models, as
    we will see in the next section.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前相同的超参数，基于简单的 RNN 构建一个垃圾邮件或非垃圾邮件分类器，并将其性能与基于 LSTM 的解决方案进行比较。对于像这样的简单数据集，简单的
    RNN 性能非常接近 LSTM。然而，对于更复杂的模型，情况通常并非如此，正如我们将在下一节看到的那样。
- en: Note
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Find the input file at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/exercise.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/exercise
    找到输入文件。
- en: Import the required Python packages.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的 Python 包。
- en: Read the input file containing a column that contains text and another column
    that contains the label for the text depicting whether the text is spam or not.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取包含文本的列和包含文本标签的另一列的输入文件，该标签表示文本是否为垃圾邮件。
- en: Convert to sequences.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换为序列。
- en: Pad the sequences.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充序列。
- en: Train the sequences.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练序列。
- en: Build the model.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型。
- en: Predict the mail category on the new test data.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对新测试数据进行邮件类别预测。
- en: '**Expected output:**'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 7.35: Output for mail category prediction'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.35：邮件类别预测的输出'
- en: '](img/C13783_07_35.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_35.jpg)'
- en: 'Figure 7.35: Output for mail category prediction'
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.35：邮件类别预测输出
- en: Note
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: The solution for the activity can be found on page 324.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第324页找到。
- en: Neural Language Translation
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经语言翻译
- en: The simple binary classifier described in the previous section is a basic use
    case for the area of natural language processing (NLP) and doesn't fully justify
    the use of any techniques that are more complex than using a simple RNN or even
    simpler techniques. However, there are many complex use cases for which it is
    imperative to use more complex units such as LSTMs. Neural language translation
    is one such application.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节描述的简单二分类器是自然语言处理（NLP）领域的基本用例，并不足以证明使用比简单RNN或更简单技术更复杂的技术的必要性。然而，确实存在许多复杂的用例，在这些用例中，必须使用更复杂的单元，例如LSTM。神经语言翻译就是这样的一个应用。
- en: The goal of a neural language translation task is to build a model that can
    translate a piece of text from a source language to a target language. Before
    starting with the code, let's discuss the architecture of this system.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言翻译任务的目标是构建一个模型，能够将一段文本从源语言翻译成目标语言。在开始编写代码之前，让我们讨论一下该系统的架构。
- en: Neural language translation represents a many-to-many NLP application, which
    means that there are many inputs to the system and the system produces many outputs
    as well.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言翻译代表了一种多对多的自然语言处理（NLP）应用，这意味着系统有多个输入，并且系统也会产生多个输出。
- en: 'Additionally, the number of inputs and outputs could be different as the same
    text can have a different number of words in the source and target language. The
    area of NLP that solves such problems is referred to as sequence-to-sequence modeling.
    The architecture consists of an encoder block and a decoder block. The following
    diagram represents the architecture:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输入和输出的数量可能不同，因为相同的文本在源语言和目标语言中的单词数量可能不同。解决此类问题的NLP领域被称为序列到序列建模。该体系结构由编码器块和解码器块组成。以下图表示该体系结构：
- en: '![Figure 7.36: Neural translation model'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.36：神经翻译模型'
- en: '](img/C13783_07_36.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_36.jpg)'
- en: 'Figure 7.36: Neural translation model'
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.36：神经翻译模型
- en: 'The left part of the architecture is the encoder block, and the right part
    is the decoder block. The diagram attempts to translate an English sentence to
    German, as here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 体系结构的左侧是编码器块，右侧是解码器块。该图尝试将一个英文句子翻译成德文，如下所示：
- en: 'English: I would like to go swimming'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 英文：我想去游泳
- en: 'German: Ich möchte schwimmen gehen'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 德文：Ich möchte schwimmen gehen
- en: Note
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: Periods have been dropped from the preceding sentences for demonstration purposes
    only. Periods are also considered valid tokens.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示的目的，前面的句子省略了句号。句号也被视为有效的标记。
- en: The encoder block takes each word of the English (source language) sentence
    as input at a given timestep. Each unit of the encoder block is an LSTM. The only
    outputs for the encoder block are the final cell state and activations. These
    are jointly referred to as the thought vector. The thought vector is used to initialize
    the activation and cell state for the decoder block, which is another LSTM block.
    During the training phase, at each timestep, the decoder output is the next word
    in the sentence. This is represented by a dense softmax layer that has a value
    1 for the next word token and 0 for all the other entries in the vector.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器块在给定的时间步长内将英文（源语言）句子的每个单词作为输入。编码器块的每个单元是一个LSTM。编码器块的唯一输出是最终的单元状态和激活值。它们合起来被称为思维向量。思维向量用于初始化解码器块的激活和单元状态，解码器块也是一个LSTM块。在训练阶段，在每个时间步长中，解码器的输出是句子中的下一个单词。它通过一个密集的softmax层表示，该层为下一个单词标记的值为1，而对于向量中的其他所有条目，值为0。
- en: The English sentence is fed to the encoder word by word, producing a final cell
    state and activation. During the training phase, the real output of the decoder
    at each timestep is known. This is simply the next German word in the sentence.
    Note that there is a '**BEGIN_**' token inserted at the sentence beginning and
    an '**_END**' token at the end of the sentence. The output for the '**BEGIN_**'
    token is the first word in the German sentence. This can be seen in the last diagram.
    At the time of training, the network is made to learn the translation word by
    word.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 英语句子逐词输入到编码器中，生成最终的单元状态和激活。在训练阶段，解码器在每个时间步的真实输出是已知的，这实际上就是句子中的下一个德语单词。注意，句子开头插入了‘**BEGIN_**’标记，句子结尾插入了‘**_END**’标记。‘**BEGIN_**’标记的输出是德语句子的第一个单词。从最后一个图可以看到这一点。在训练过程中，网络被训练学习逐字翻译。
- en: In the inference phase, the English input sentence is fed to the encoder block,
    producing a final cell state and activation. The decoder has the '**BEGIN_**'
    token as the input at the first timestep, along with the cell state and activations.
    Using these three inputs, a softmax output is produced for this timestep. In a
    well-trained network, the softmax value is the highest for the entry corresponding
    to the correct word. This next word is then fed as the input to the next timestep.
    This process is continued until an '**_END**' token is sampled or a maximum sentence
    length is reached.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段，英语输入句子被送入编码器模块，生成最终的单元状态和激活。解码器在第一个时间步输入‘**BEGIN_**’标记，以及单元状态和激活。利用这三个输入，会生成一个
    softmax 输出。在一个训练良好的网络中，softmax 值对于正确单词的对应条目是最大的。然后将这个下一个单词作为输入送入下一个时间步。这个过程会一直继续，直到采样到‘**_END**’标记或达到最大句子长度。
- en: Now let's go through the code for the model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来逐步解析模型的代码。
- en: 'We read in the file containing sentence pairs first. We also keep the number
    of pairs restricted to 20,000 for demonstration purposes:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先读取包含句对的文件。为了演示目的，我们将句对数量限制为 20,000：
- en: '[PRE20]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Output:**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Figure 7.37: Screenshot for the English-to-German translation of sentence
    pairs'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.37：英德翻译句对的截图'
- en: '](img/C13783_07_37.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_37.jpg)'
- en: 'Figure 7.37: Screenshot for the English-to-German translation of sentence pairs'
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.37：英德翻译句对的截图
- en: 'Each line has first the English sentence, followed by a tab character, and
    then the German translation of the sentence. Next, we''ll map all the numbers
    to a placeholder word, ''**NUMBER_PRESENT**'', and append the ''**BEGIN_** ''
    and '' **_END**'' tokens to each German sentence, as discussed previously:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行首先是英语句子，后面跟着一个制表符，然后是该句子的德语翻译。接下来，我们将所有的数字映射到占位符词‘**NUMBER_PRESENT**’，并将‘**BEGIN_**’和‘**_END**’标记附加到每个德语句子中，正如之前讨论的那样：
- en: '[PRE21]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the previous snippet, we obtained the input and output texts. They look
    as depicted:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们获得了输入和输出文本。它们如下所示：
- en: '![Figure 7.38: Screenshot for input and output texts after mapping'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.38：映射后的输入和输出文本截图'
- en: '](img/C13783_07_38.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_38.jpg)'
- en: 'Figure 7.38: Screenshot for input and output texts after mapping'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.38：映射后的输入和输出文本截图
- en: 'Next, we get the maximum length of the input and output sequences and get a
    list of all the words in the input and output corpus:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取输入和输出序列的最大长度，并获得输入和输出语料库中的所有单词列表：
- en: '[PRE22]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'input_words and target_words look as shown in the following figure:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: input_words 和 target_words 如下图所示：
- en: '![Figure 7.39: Screenshot for input text and target words'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.39：输入文本和目标词汇的截图'
- en: '](img/C13783_07_39.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_39.jpg)'
- en: 'Figure 7.39: Screenshot for input text and target words'
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.39：输入文本和目标词汇的截图
- en: 'Next, we generate an integer index for each token in the input and output words:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为输入和输出单词的每个标记生成一个整数索引：
- en: '[PRE23]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The values of these variables are as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量的值如下所示：
- en: '![Figure 7.40: Screenshot for output of integer index for each token'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.40：每个标记的整数索引输出截图'
- en: '](img/C13783_07_40.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_40.jpg)'
- en: 'Figure 7.40: Screenshot for output of integer index for each token'
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.40：每个标记的整数索引输出截图
- en: 'We now define the arrays for the encoder input data, which is a 2-dimensional
    matrix with as many rows as sentence pairs and as many columns as the maximum
    input sequence length. Similarly, the decoder input data is also a 2-dimensional
    matrix with as many rows as sentence pairs and as many columns as the maximum
    sequence length in the target corpus. We also need target output data, which is
    required during the training phase. This is a 3-dimensional matrix where the first
    dimension has the same value as the number of sentence pairs. The second dimension
    has the same number of elements as the maximum target sequence length. The third
    dimension represents the number of decoder tokens (the number of distinct words
    in the target corpus). We initialize these variables with zeros:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义编码器输入数据的数组，这是一个二维矩阵，行数等于句子对的数量，列数等于最大输入序列长度。同样，解码器输入数据也是一个二维矩阵，行数等于句子对的数量，列数等于目标语料库中最大序列长度。我们还需要目标输出数据，这是训练阶段所必需的。它是一个三维矩阵，第一维的大小与句子对的数量相同。第二维的大小与最大目标序列长度相同。第三维表示解码器令牌的数量（即目标语料库中不同单词的数量）。我们将这些变量初始化为零：
- en: '[PRE24]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We now populate these matrices:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们填充这些矩阵：
- en: '[PRE25]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The values look as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值如下所示：
- en: '![Figure 7.41: Screenshot of matrix population'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.41：矩阵填充截图](img/C13783_07_41.jpg)'
- en: '](img/C13783_07_41.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_41.jpg)'
- en: 'Figure 7.41: Screenshot of matrix population'
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.41：矩阵填充截图
- en: 'We will now define a model. For this exercise, we''ll use the functional API
    of Keras:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义一个模型。对于这个练习，我们将使用Keras的功能性API：
- en: '[PRE26]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s see the encoder block:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下编码器模块：
- en: '[PRE27]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: First, an Input layer with a flexible number of inputs is defined (with the
    None attribute). Then, an embedding layer is defined and applied to the encoder
    inputs. Next, an LSTM unit is defined with 50 hidden units and applied to the
    embedding layer. Note that the return_state parameter in the LSTM definition is
    set to True since we would like to obtain the final encoder states to be used
    for initializing decoder cell state and activations. The encoder LSTM is then
    applied to the embeddings and the states are collected back into variables.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义一个具有灵活输入数量的输入层（使用None属性）。然后，定义并应用一个嵌入层到编码器输入。接下来，定义一个具有50个隐藏单元的LSTM单元并应用于嵌入层。注意，LSTM定义中的return_state参数设置为True，因为我们希望获取最终的编码器状态，用于初始化解码器的细胞状态和激活值。然后将编码器LSTM应用于嵌入层，并将状态收集到变量中。
- en: 'Now let''s define the decoder block:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义解码器模块：
- en: '[PRE28]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The decoder takes in inputs and defines embedding layers in a way similar to
    that of the encoder. An LSTM block is then defined with the return_sequences and
    return_state parameters set to True. This is done since we wish to use the sequences
    and states for the decoder. A dense layer is then defined with a softmax activation
    and a number of outputs equal to the number of distinct tokens in the target corpus.
    We can now define a model that takes in the encoder and decoder inputs as its
    input and produces the decoder outputs as final outputs:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器接收输入，并以类似于编码器的方式定义嵌入层。然后定义一个LSTM模块，并将return_sequences和return_state参数设置为True。这样做是因为我们希望使用序列和状态来进行解码。接着定义一个具有softmax激活函数的全连接层，输出数量等于目标语料库中不同令牌的数量。我们现在可以定义一个模型，它以编码器和解码器的输入为输入，生成解码器输出作为最终输出：
- en: '[PRE29]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following model summary is seen:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型总结的显示：
- en: '![Figure 7.42: Screenshot of model summary'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.42：模型总结截图](img/C13783_07_42.jpg)'
- en: '](img/C13783_07_42.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_42.jpg)'
- en: 'Figure 7.42: Screenshot of model summary'
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.42：模型总结截图
- en: 'We can now fit the model for our inputs and outputs:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以为输入和输出拟合模型：
- en: '[PRE30]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We set a batch size of 128 with 20 epochs:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了一个批次大小为128，训练了20个epochs：
- en: '![Figure 7.43: Screenshot of model fitting with 20 epochs](img/C13783_07_43.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.43：模型拟合截图，训练了20个epochs](img/C13783_07_43.jpg)'
- en: 'Figure 7.43: Screenshot of model fitting with 20 epochs'
  id: totrans-294
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.43：模型拟合截图，训练了20个epochs
- en: 'The model is now trained. Now, as described in our section on neural language
    translation, the inference phase follows a slightly different architecture from
    the one used during training. We first define the encoder model, which takes encoder_inputs
    (with embedding) as input and produces encoder_states as output. This makes sense
    as the output of the encoder block is the cell state and activations:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在已经训练完成。正如我们在神经语言翻译部分所描述的那样，推理阶段的架构与训练阶段使用的架构略有不同。我们首先定义编码器模型，它以编码器输入（包含嵌入层）作为输入，并生成编码器状态作为输出。这是有意义的，因为编码器模块的输出是细胞状态和激活值：
- en: '[PRE31]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, a decoder inference model is defined:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义解码器推理模型：
- en: '[PRE32]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The initial states of decoder_lstm, which was trained earlier, are set to the
    decoder_states_inputs variable, which will be set to encoder state output later
    on. Then, we pass decoder outputs through a dense softmax layer for getting the
    index of the predicted word and define the decoder inference model:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 先前训练过的`decoder_lstm`的初始状态被设置为`decoder_states_inputs`变量，稍后将被设置为编码器的状态输出。然后，我们将解码器的输出通过密集的
    softmax 层，以获取预测单词的索引，并定义解码器推理模型：
- en: '[PRE33]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The decoder model takes multiple inputs in the form of decoder_input (with embedding)
    and decoder states. The output is also a multivariable where the dense layer output
    and decoder states are returned. The states are required here as they need to
    passed on as input states for the sampling of the word at the next timestep.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器模型接收多个输入，形式包括带嵌入的`decoder_input`和解码器状态。输出也是一个多变量，其中包含密集层的输出和解码器状态返回的内容。这里需要状态，因为它们需要作为输入状态传递，以便在下一个时间步采样单词。
- en: 'Since the output of the dense layer will return a vector, we need a reverse
    lookup dictionary to map the index for the generated word to an actual word:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 由于密集层的输出将返回一个向量，我们需要一个反向查找字典来将生成的单词的索引映射到实际单词：
- en: '[PRE34]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The values in the dictionaries are as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 字典中的值如下：
- en: '![Figure 7.44: Screenshot of dictionary values'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.44：字典值的截图](img/C13783_07_44.jpg)'
- en: '](img/C13783_07_44.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_44.jpg)'
- en: 'Figure 7.44: Screenshot of dictionary values'
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.44：字典值的截图
- en: We now need to develop a sampling logic. Given a token representation for every
    word in an input sentence, we first get the output from encoder_model using these
    word tokens as inputs for the encoder. We also initialize the first input word
    to the decoder to be a '**BEGIN_**' token. We then sample a new word token using
    these values. The input to the decoder for the next timestep is this newly generated
    token. We continue in this fashion until we either sample the '**_END**' token
    or reach the maximum allowed output sequence length.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要开发一个采样逻辑。给定输入句子中每个单词的标记表示，我们首先使用这些单词标记作为编码器的输入，从`encoder_model`获取输出。我们还将解码器的第一个输入单词初始化为'**BEGIN_**'标记。然后，我们使用这些值来采样一个新的单词标记。下一个时间步的解码器输入就是这个新生成的标记。我们以这种方式继续，直到我们采样到'**_END**'标记或达到最大允许的输出序列长度。
- en: 'The first step is encoding the input as a state vector:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将输入编码为状态向量：
- en: '[PRE35]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we generate an empty target sequence of length 1:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们生成一个长度为 1 的空目标序列：
- en: '[PRE36]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we populate the first character of the target sequence with the start
    character:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将目标序列的第一个字符填充为开始字符：
- en: '[PRE37]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we create a sampling loop for a batch of sequences:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为一批序列创建一个采样循环：
- en: '[PRE38]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we sample a token:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们采样一个标记：
- en: '[PRE39]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, we state the exit condition "**either hit max length**":'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们声明退出条件“**either hit max length**”（达到最大长度）：
- en: '[PRE40]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, we update the states:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新状态：
- en: '[PRE41]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In this instance, you can test the model by translating a user-defined English
    sentence to German:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，您可以通过将用户定义的英语句子翻译为德语来测试模型：
- en: '[PRE42]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is depicted in this screenshot:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如图所示：
- en: '![Figure 7.45: Screenshot of English-to-German translator](img/C13783_07_45.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.45：英语到德语翻译器的截图](img/C13783_07_45.jpg)'
- en: 'Figure 7.45: Screenshot of English-to-German translator'
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.45：英语到德语翻译器的截图
- en: This is, indeed, the correct translation.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正确的翻译。
- en: So, even a model trained on just 20,000 sequences for only 20 epochs is capable
    of producing good translations. With the current settings, the training session
    ran for about 90 minutes.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使是一个仅在 20,000 个序列上训练了 20 轮的模型，也能产生良好的翻译。在当前设置下，训练时长约为 90 分钟。
- en: 'Activity 10: Creating a French-to-English translation model'
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 10：创建法语到英语的翻译模型
- en: In this activity, we aim to generate a language translator model that converts
    French text into English.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们的目标是生成一个语言翻译模型，将法语文本转换为英语。
- en: Note
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the related files to the activity at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/activity.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/activity](https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/activity)
    找到与此活动相关的文件。
- en: Read in the sentence pairs (check the GitHub repository for the file).
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取句子对（请查看 GitHub 仓库中的文件）。
- en: Generate input and output texts with the '**BEGIN_**' and '**_END**' words attached
    to the output sentences.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带有'**BEGIN_**'和'**_END**'单词的输出句子生成输入和输出文本。
- en: Convert the input and output texts into input and output sequence matrices.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和输出文本转换为输入和输出序列矩阵。
- en: Define the encoder and decoder training models and train the network.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义编码器和解码器训练模型，并训练网络。
- en: Define the encoder and decoder architecture for inference.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于推理的编码器和解码器架构。
- en: 'Create the user input text (French: '' *Où est ma voiture?*''). The sample
    output text in English should be ''*Where is my car?*''. Refer to the ''*French.txt*''
    file from the GitHub repository for some sample French words.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用户输入文本（法语：' *Où est ma voiture?*'）。英文的示例输出文本应该是 '*Where is my car?*'。请参考GitHub仓库中的'*French.txt*'文件，获取一些示例法语单词。
- en: '**Expected output:**'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 7.46: Output for French to English translator model'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.46：法语到英语翻译器模型的输出'
- en: '](img/C13783_07_46.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_07_46.jpg)'
- en: 'Figure 7.46: Output for French to English translator model'
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.46：法语到英语翻译器模型的输出
- en: Note
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 327.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第327页找到。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We introduced LSTM units as a possible remedy to the vanishing gradient problem.
    We then discussed the LSTM architecture in detail and built a simple binary classifier
    using it. We then delved into a neural nanguage translation application that utilizes
    LSTM units, and we built a French-to-English translator model using the techniques
    we explored. In the next chapter, we will discuss the current state of the art
    in the NLP sphere.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了LSTM单元，作为解决梯度消失问题的一个可能方法。接着，我们详细讨论了LSTM架构，并使用它构建了一个简单的二元分类器。随后，我们深入研究了一个使用LSTM单元的神经机器翻译应用，并使用我们探讨的技术构建了一个法语到英语的翻译器模型。在下一章中，我们将讨论NLP领域的最新进展。
