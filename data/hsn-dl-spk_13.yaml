- en: Convolution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: 'The previous two chapters have covered real use case implementation of NLP
    done through RNNs/LSTMs in Apache Spark. In this and the following chapter, we
    are going to do something similar for CNNs: we are going to explore how they can
    be used in image recognition and classification. This chapter in particular covers
    the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前两章介绍了通过RNN/LSTM在Apache Spark中进行的NLP实际用例实现。在本章及接下来的章节中，我们将做类似的事情，探讨CNN如何应用于图像识别和分类。本章特别涉及以下主题：
- en: A quick recap on what convolution is, from both the mathematical and DL perspectives
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数学和深度学习的角度快速回顾卷积是什么
- en: The challenges and strategies for object recognition in real-world problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实问题中物体识别的挑战与策略
- en: 'How convolution applies to image recognition and a walk-through of hands-on
    practical implementations of an image recognition use case through DL (CNNs) by
    adopting the same approach, but using the following two different open source
    frameworks and programming languages:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积在图像识别中的应用，以及通过深度学习（卷积神经网络，CNN）实践中图像识别用例的实现，采用相同的方法，但使用以下两种不同的开源框架和编程语言：
- en: Keras (with a TensorFlow backend) in Python
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras（使用TensorFlow后端）在Python中的实现
- en: DL4J (and ND4J) in Scala
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J（及ND4J）在Scala中的实现
- en: Convolution
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: '[Chapter 5](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml), *Convolutional Neural
    Networks*, covered the theory behind CNNs, and convolution of course has been
    part of that presentation. Let''s do a recap of this concept from a mathematical
    and practical perspective before moving on to object recognition. In mathematics,
    convolution is an operation on two functions that produces a third function, which
    is the result of the integral of the product between the first two, one of which
    is flipped:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第五章](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml)，*卷积神经网络*，介绍了CNN的理论，当然卷积也是其中的一部分。在进入物体识别之前，让我们从数学和实际的角度回顾一下这个概念。在数学中，卷积是对两个函数的操作，生成一个第三个函数，该函数是前两个函数的乘积积分结果，其中一个函数被翻转：'
- en: '![](img/40f744d1-b124-4366-b7a9-e4e3b99513a4.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40f744d1-b124-4366-b7a9-e4e3b99513a4.png)'
- en: Convolution is heavily used in 2D image processing and signal filtering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积在2D图像处理和信号过滤中被广泛应用。
- en: 'To better understand what happens behind the scenes, here''s a simple Python
    code example of 1D convolution with NumPy ([http://www.numpy.org/](http://www.numpy.org/)):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解幕后发生了什么，这里是一个简单的Python代码示例，使用NumPy进行1D卷积（[http://www.numpy.org/](http://www.numpy.org/)）：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This produces the following result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生如下结果：
- en: '![](img/5543edab-f7f9-41ec-ad54-c2ba7fd928ec.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5543edab-f7f9-41ec-ad54-c2ba7fd928ec.png)'
- en: 'Let''s see how the convolution between the `x` and `y` arrays produces that
    result. The first thing the `convolve` function does is to horizontally flip the
    `y` array:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`x`和`y`数组之间的卷积如何产生该结果。`convolve`函数首先做的事情是水平翻转`y`数组：
- en: '`[1, -2, 2]` becomes `[2, -2, 1]`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`[1, -2, 2]`变为`[2, -2, 1]`'
- en: 'Then, the flipped `y` array slides over the `x` array:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，翻转后的`y`数组滑动在`x`数组上：
- en: '![](img/28dfda6e-a187-41a7-a2c5-4d899b4dd754.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28dfda6e-a187-41a7-a2c5-4d899b4dd754.png)'
- en: That's how the `result` array `[ 1  0  1  2  3 -2 10]` is generated.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何生成`result`数组`[ 1  0  1  2  3 -2 10]`的。
- en: '2D convolution happens with a similar mechanism. Here''s a simple Python code
    example with NumPy:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2D卷积使用类似机制发生。以下是一个简单的Python代码示例，使用NumPy：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This time, the SciPy ([https://www.scipy.org/](https://www.scipy.org/)) `signal.convolve2d`
    function is used to do the convolution. The result of the preceding code is as
    follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，使用了SciPy（[https://www.scipy.org/](https://www.scipy.org/)）中的`signal.convolve2d`函数来执行卷积。前面代码的结果如下：
- en: '![](img/e61da606-2f9b-4d33-840c-9658d42569af.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e61da606-2f9b-4d33-840c-9658d42569af.png)'
- en: 'When the flipped matrix is totally inside the input matrix, the results are
    called `valid` convolutions. It is possible to calculate the 2D convolution, getting
    only the valid results this way, as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当翻转后的矩阵完全位于输入矩阵内部时，结果被称为`valid`卷积。通过这种方式计算2D卷积，只获取有效结果，如下所示：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give output as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '![](img/7e17957f-f56d-4d09-823e-9100dcc1239e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e17957f-f56d-4d09-823e-9100dcc1239e.png)'
- en: 'Here''s how those results are calculated. First, the `w` array is flipped:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些结果的计算方式。首先，`w`数组被翻转：
- en: '![](img/dcc92c16-742f-4472-8959-e64f4532b799.png) becomes ![](img/a8f37131-695b-44d2-822e-ae0ebdf2bc11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcc92c16-742f-4472-8959-e64f4532b799.png)变为![](img/a8f37131-695b-44d2-822e-ae0ebdf2bc11.png)'
- en: 'Then, the same as for the 1D convolution, each window of the `a` matrix is
    multiplied, element by element, with the flipped `w` matrix, and the results are
    finally summed as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，和 1D 卷积相同，`a` 矩阵的每个窗口与翻转的 `w` 矩阵逐元素相乘，结果最终按如下方式求和：
- en: '![](img/77311521-ca82-42ee-b893-4c1877dcedc5.png)    *(1 x -1) + (0 x 3) +
    (0 x 2) + (-1 x 1) = **-2***'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/77311521-ca82-42ee-b893-4c1877dcedc5.png)    *(1 x -1) + (0 x 3) +
    (0 x 2) + (-1 x 1) = **-2***'
- en: '![](img/e6c864b0-1bca-4d37-894d-f4c9471a8cad.png)    *(3 x -1) + (1 x 0) +
    (-1 x 2) + (1 x 1) = **-4***'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/e6c864b0-1bca-4d37-894d-f4c9471a8cad.png)    *(3 x -1) + (1 x 0) +
    (-1 x 2) + (1 x 1) = **-4***'
- en: And so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推。
- en: Object recognition strategies
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体识别策略
- en: 'This section presents different computational techniques used in implementing
    the automated recognition of objects in digital images. Let''s start by giving
    a definition of object recognition. In a nutshell, it is the task of finding and
    labeling parts of a 2D image of a scene that correspond to objects inside that
    scene. The following screenshot shows an example of object recognition performed
    manually by a human using a pencil:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在数字图像中实现自动物体识别时使用的不同计算技术。首先，我们给出物体识别的定义。简而言之，它是任务：在场景的 2D 图像中找到并标记对应场景内物体的部分。以下截图展示了由人类用铅笔手动执行的物体识别示例：
- en: '![](img/4cc4dd9a-cc15-443d-94b4-439444a02389.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc4dd9a-cc15-443d-94b4-439444a02389.png)'
- en: 'Figure 13.1: An example of manual object detection'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：手动物体检测示例
- en: The image has been marked and labeled to show fruits recognizable as a banana
    and a pumpkin. This is exactly the same as what happens for calculated object
    recognition; it can be simply thought of as the process of drawing lines and outlining
    areas of an image, and finally attaching to each structure a label corresponding
    to the model that best represents it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已被标记和标签化，显示可以识别为香蕉和南瓜的水果。这与计算物体识别过程完全相同；可以简单地认为它是绘制线条、勾画图像区域的过程，最后为每个结构附上与其最匹配的模型标签。
- en: 'A combination of factors, such as the semantics of a scene context or information
    present in the image, must be used in object recognition. Context is particularly
    important when interpreting images. Let''s first have a look at the following
    screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在物体识别中，必须结合多种因素，如场景上下文的语义或图像中呈现的信息。上下文在图像解读中特别重要。让我们先看看以下截图：
- en: '![](img/1b8e0b43-cf2f-40f6-82eb-727ab48b7242.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b8e0b43-cf2f-40f6-82eb-727ab48b7242.png)'
- en: 'Figure 13.2: Object in isolation (no context)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：孤立物体（无上下文）
- en: 'It is nearly impossible to identify in isolation the object in the center of
    that image. Let''s have a look now at the following screenshot, where the same
    object appears in the position as it had in the original image:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎不可能单独识别图像中心的物体。现在让我们看看接下来的截图，其中同一物体出现在原始图像中的位置：
- en: '![](img/93026019-5f1b-47e6-809e-65501ef207f1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93026019-5f1b-47e6-809e-65501ef207f1.png)'
- en: 'Figure 13.3: The object from figure 13.2 in its original context'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：图 13.2 中物体的原始上下文
- en: Providing no further information, it is still difficult to identify that object,
    but not as difficult as for *Figure 13.2*. Given context information that the
    image in the preceding screenshot is a circuit board, the initial object is more
    easily recognized as a polarized capacitor. Cultural context plays a key role
    in enabling the proper interpretation of a scene.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不提供进一步的信息，仍然很难识别该物体，但相比于*图 13.2*，难度要小一些。给定前面截图中图像是电路板的上下文信息，初始物体更容易被识别为一个极化电容器。文化背景在正确解读场景中起着关键作用。
- en: 'Let''s now consider a second example (shown in the following screenshot), a
    consistent 3D image of a stairwell:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们考虑第二个示例（如下截图所示），一个显示楼梯间的一致性 3D 图像：
- en: '![](img/b09f73bd-f417-4f95-a7e1-11367ab01c0d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b09f73bd-f417-4f95-a7e1-11367ab01c0d.png)'
- en: 'Figure 13.4: A consistent 3D image showing a stairwell'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：显示楼梯间的一致性 3D 图像
- en: 'By changing the light in that image, the final result could make it harder
    for the eye (and also a computer) to see a consistent 3D image (as shown in the
    following screenshot):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变该图像中的光线，最终结果可能使得眼睛（以及计算机）更难看到一致的 3D 图像（如下图所示）：
- en: '![](img/c7131e66-53d6-46e4-b912-0c45380910f9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7131e66-53d6-46e4-b912-0c45380910f9.png)'
- en: 'Figure 13.5: The result of applying a different light to the image in figure
    13.4'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：在图 13.4 中应用不同光线后的结果
- en: 'Compared with the original image (*Figure 13.3*) its brightness and contrast
    have been modified (as shown in the following screenshot):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始图像（*图 13.3*）相比，它的亮度和对比度已经被修改（如以下截图所示）：
- en: '![](img/6bc78e8e-316f-4388-9c29-fcf3c04657eb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bc78e8e-316f-4388-9c29-fcf3c04657eb.png)'
- en: 'Figure 13.6: The image in figure 13.3 with changed brightness and contrast'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：图 13.3 中的图像，具有改变的亮度和对比度
- en: 'The eye can still recognize three-dimensional steps. However, using different
    brightness and contrast values to the original image looks as shown in following
    screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛仍然能够识别三维的阶梯。然而，使用与原始图像不同的亮度和对比度值，图像如下所示：
- en: '![](img/f85f93fe-1ea7-412c-b1e7-b0a55ab6495f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f85f93fe-1ea7-412c-b1e7-b0a55ab6495f.png)'
- en: 'Figure 13.7: The image in figure 13.3 with different brightness and contrast'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7：图 13.3 中的图像，具有不同的亮度和对比度
- en: It is almost impossible to recognize the same image. What we have learned is
    that although the retouched image in the previous screenshot retains a significant
    part of the important visual information in the original one (*Figure 13.3*),
    the images in *Figure 13.4* and the preceding screenshot became less interpretable
    because of the 3D details that have been removed by retouching them. The examples
    presented provide evidence that computers (like human eyes) need appropriate context
    models in order to successfully complete object recognition and scene interpretation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎无法识别出相同的图像。我们学到的是，尽管前面截图中的修饰图像保留了原始图像中的重要视觉信息（*图 13.3*），但*图 13.4*和前面的截图中的图像由于修饰去除了三维细节，变得更加难以解读。所提供的例子证明，计算机（就像人眼一样）需要合适的上下文模型才能成功完成物体识别和场景解读。
- en: Computational strategies for object recognition can be classified based on their
    suitability for complex image data or for complex models. Data complexity in a
    digital image corresponds to its signal-to-noise ratio. An image with semantic
    ambiguity corresponds to complex (or noisy) data. Data consisting of perfect outlines
    of model instances throughout an image is called simple. Image data with poor
    resolution, noise, or other kinds of anomalies, or with easily confused false
    model instances, is referred to as complex. Model complexity is indicated by the
    level of detail in the data structures in an image, and in the techniques required
    to determine the form of the data. If a model is defined by a simple criterion
    (such as a single shape template or the optimization of a single function implicitly
    containing a shape model), then no other context may be needed to attach model
    labels to a given scene. But, in cases where many atomic model components must
    be assembled or some way hierarchically related to establish the existence of
    the desired model instance, complex data structures and non-trivial techniques
    are required.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 物体识别的计算策略可以根据其对复杂图像数据或复杂模型的适用性进行分类。数字图像中的数据复杂度对应于其信噪比。具有语义歧义的图像对应于复杂（或嘈杂）的数据。图像中包含完美轮廓的模型实例数据被称为简单数据。具有较差分辨率、噪声或其他类型异常数据，或容易混淆的虚假模型实例，被称为复杂数据。模型复杂度通过图像中数据结构的细节级别以及确定数据形式所需的技术来表示。如果一个模型通过简单的标准定义（例如，单一形状模板或优化一个隐式包含形状模型的单一函数），那么可能不需要其他上下文来将模型标签附加到给定的场景中。但是，在许多原子模型组件必须组合或某种方式按层次关系建立以确认所需模型实例的存在时，需要复杂的数据结构和非平凡的技术。
- en: 'Based on the previous definitions, object recognition strategies can then be
    classified into four main categories, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的定义，物体识别策略可以分为四大类，如下所示：
- en: '**Feature vector classification**: This relies on a trivial model of an object''s
    image characteristics. Typically, it is applied only to simple data.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量分类**：这依赖于对象图像特征的一个简单模型。通常，它仅应用于简单数据。'
- en: '**Fitting model to photometry**: This is applied when simple models are sufficient
    but the photometric data of an image is noisy and ambiguous.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拟合模型到光度数据**：当简单的模型足够用，但图像的光度数据存在噪声和歧义时，应用此方法。'
- en: '**Fitting model to symbolic structures**: Applied when complex models are required,
    but reliable symbolic structures can be accurately inferred from simple data.
    These approaches look for instances of objects by matching data structures that
    represent relationships between globally object parts.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拟合模型到符号结构**：当需要复杂的模型时应用，但可以通过简单数据准确推断可靠的符号结构。这些方法通过匹配表示全局对象部件之间关系的数据结构，来寻找对象的实例。'
- en: '**Combined strategies**: Applied when both data and desired model instances
    are complex.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合策略**：在数据和所需模型实例都很复杂时应用。'
- en: The implementation of the available API to build and train CNNs for object recognition
    provided by the major open source frameworks detailed in this book have been done
    keeping these considerations and strategies in mind. While those APIs are very
    high-level, the same mindset should be taken when choosing the proper combination
    of hidden layers for a model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中详细介绍的主要开源框架所提供的用于构建和训练 CNNs（卷积神经网络）进行对象识别的可用 API 实现时，已考虑到这些因素和策略。尽管这些 API
    是非常高层次的，但在选择模型的适当隐藏层组合时，应该采取相同的思维方式。
- en: Convolution applied to image recognition
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积在图像识别中的应用
- en: In this section, we are now going hands-on by implementing an image recognition
    model, taking into account the considerations discussed in the first part of this
    chapter. We are going to implement the same use case using two different frameworks
    and programming languages.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过实现一个图像识别模型来动手实践，同时考虑本章第一部分中讨论的相关事项。我们将使用两种不同的框架和编程语言实现相同的用例。
- en: Keras implementation
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 实现
- en: The first implementation of object recognition we are going to do is in Python
    and involves the Keras framework. To train and evaluate the model, we are going
    to use a public dataset called CIFAR-10 ([http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)).
    It consists of 60,000 (50,000 for training and 10,000 for testing) small (32 x
    32 pixels) color images divided into 10 classes (airplane, automobile, bird, cat,
    deer, dog, frog, horse, ship, and truck). These 10 classes are mutually exclusive.
    The CIFAR-10 dataset (163 MB) is freely downloadable from [http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实现的第一个对象识别是在 Python 中使用 Keras 框架进行的。为了训练和评估模型，我们将使用一个名为 CIFAR-10 的公共数据集
    ([http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html))。它包含
    60,000 张（50,000 张用于训练，10,000 张用于测试）小的（32 x 32 像素）彩色图像，分为 10 类（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车）。这
    10 个类别是互斥的。CIFAR-10 数据集（163 MB）可以从 [http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)
    免费下载。
- en: 'The prerequisites for this implementation are Python 2.7.x, Keras, TensorFlow
    (it is used as the Keras backend), NumPy, and `scikit-learn` ([http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html)),
    an open source tool for ML. [Chapter 10](1066b0d4-c2f3-44f9-9cc4-d38469d72c3f.xhtml),
    *Deploying on a Distributed System*, covers the details to set up the Python environment
    for Keras and TensorFlow. `scikit-learn` can be installed as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此功能的前提条件是 Python 2.7.x、Keras、TensorFlow（作为 Keras 的后端使用）、NumPy 以及 `scikit-learn`
    ([http://scikit-learn.org/stable/index.html](http://scikit-learn.org/stable/index.html))，这是一个用于机器学习的开源工具。[第
    10 章](1066b0d4-c2f3-44f9-9cc4-d38469d72c3f.xhtml)，*在分布式系统上部署*，涵盖了为 Keras 和 TensorFlow
    设置 Python 环境的详细信息。`scikit-learn` 可以按以下方式安装：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'First of all, we need to import all of the necessary NumPy, Keras, and `scikit-learn` namespaces
    and classes, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所有必要的 NumPy、Keras 和 `scikit-learn` 命名空间和类，如下所示：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we need to load the CIFAR-10 dataset. No need to download it separately;
    Keras provides a facility to download it programmatically, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要加载 CIFAR-10 数据集。不需要单独下载，Keras 提供了一个可以通过编程方式下载它的功能，如下所示：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `load_data` function downloads it the first time it is executed. Successive
    runs will use the dataset already downloaded locally.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_data` 函数在第一次执行时会下载数据集。后续的运行将使用已下载到本地的数据集。'
- en: 'We initialize the `seed` with a constant value, in order to ensure that the
    results are then reproducible, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过常量值初始化 `seed`，以确保结果是可重复的，如下所示：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The pixel values for the input datasets are in the range 0 to 255 (for each
    of the RGB channels). We can normalize this data to a range from 0 to 1 by dividing
    the values by `255.0`, then doing the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集的像素值范围为 0 到 255（每个 RGB 通道）。我们可以通过将值除以 `255.0` 来将数据归一化到 0 到 1 的范围，然后执行以下操作：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can hot encode the output variables to transform them into a binary matrix
    (it could be a one-hot encoding, because they are defined as vectors of integers
    in the range between 0 and 1 for each of the 10 classes), as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用独热编码（one-hot encoding）将输出变量转换为二进制矩阵（因为它们被定义为整数向量，范围在 0 到 1 之间，针对每个 10
    类），如下所示：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s start the model implementation. Let''s start by implementing a simple
    CNN first, verify its accuracy level and, if the case, we will go to make the
    model more complex. The following is a possible first implementation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实现模型。首先实现一个简单的 CNN，验证它的准确性，必要时我们将使模型更加复杂。以下是可能的第一次实现：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can see the model layer details at runtime in the console output before
    the training starts (see the following screenshot):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在训练开始前在控制台输出中看到模型层的详细信息（请参见以下截图）：
- en: '![](img/0ec53bf4-5723-433c-bf95-a12a27ca5ffa.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ec53bf4-5723-433c-bf95-a12a27ca5ffa.png)'
- en: The model is a `Sequential` model. As we can see from the preceding output,
    the input layer is convolutional, with 32 feature maps of size 3 x 3 and a **Rectified
    Linear Unit** (**ReLU**) activation function. After applying a 20% dropout to
    the input to reduce overfitting, the following layer is a second convolutional
    layer with the same characteristics as the input layer. Then, we set a max pooling
    layer of size 2 x 2\. After it, there is a third convolutional layer with 64 feature
    maps of size 3 x 3 and a ReLU activation function, and a second max pooling layer
    of size 2 x 2 is set. After this second max pooling, we put a flattened layer
    and apply a 20% dropout, before sending the output to the next layer, which is
    a fully connected layer with 512 units and a ReLU activation function. We apply
    another 20% dropout before the output layer, which is another fully-connected
    layer with 10 units and a softmax activation function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个`Sequential`模型。从前面的输出可以看到，输入层是卷积层，包含32个大小为 3 x 3 的特征图，并使用**修正线性单元**（**ReLU**）激活函数。为了减少过拟合，对输入应用了
    20% 的 dropout，接下来的层是第二个卷积层，具有与输入层相同的特征。然后，我们设置了一个大小为 2 x 2 的最大池化层。接着，添加了第三个卷积层，具有
    64 个大小为 3 x 3 的特征图和 ReLU 激活函数，并设置了第二个大小为 2 x 2 的最大池化层。在第二个最大池化层后，我们加入一个 Flatten
    层，并应用 20% 的 dropout，然后将输出传递到下一个层，即具有 512 个单元和 ReLU 激活函数的全连接层。在输出层之前，我们再应用一次 20%
    的 dropout，输出层是另一个具有 10 个单元和 Softmax 激活函数的全连接层。
- en: 'We can now define the following training properties (number of epochs, learning
    rate, weight decay, and optimizer, which for this specific case has been set as
    a **Stochastic Gradient Descent** (**SGD**):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义以下训练属性（训练轮数、学习率、权重衰减和优化器，在此特定情况下已设置为**随机梯度下降**（**SGD**））：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Configure the training process for the model, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 配置模型的训练过程，如下所示：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The training can be now started, using the CIFAR-10 training data, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用 CIFAR-10 训练数据开始训练，如下所示：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When it completes, the evaluation can be done using the CIFAR-10 test data,
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当完成时，可以使用 CIFAR-10 测试数据进行评估，如下所示：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The accuracy of this model is around `75%`, as can be seen in the following
    screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的准确率大约为`75%`，如以下截图所示：
- en: '![](img/ab1124af-18fb-4c99-9c6c-795166d338f6.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab1124af-18fb-4c99-9c6c-795166d338f6.png)'
- en: 'Not a great result then. We have executed the training on 25 epochs, which
    is a small number. So, the accuracy will improve when training for a greater number
    of epochs. But, let''s see first whether things can be improved by making changes
    to the CNN model, making it deeper. Add two extra imports, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果不是很好。我们已经在 25 个训练轮次上执行了训练，轮数比较少。因此，当训练轮次增多时，准确率会有所提高。不过，首先让我们看看通过改进 CNN 模型、使其更深，能否提高结果。添加以下两个额外的导入：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The only change to the code implemented previously is for the network model.
    Here''s the new one:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对之前实现的代码的唯一更改是网络模型。以下是新的模型：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Basically, what we have done is to repeat the same pattern, each one with a
    different number of feature maps (32, 64, and 128). The advantage of adding layers
    is that each of them will learn features at different levels of abstraction. In
    our case, training a CNN to recognize objects, we can check that the first layer
    trains itself to recognize basic things (for example, the edges of objects), the
    next one trains itself to recognize shapes (which can be considered as collections
    of edges), the following layer trains itself to recognize collections of shapes
    (with reference to the CIFAR-10 dataset, they could be legs, wings, tails, and
    so on), and the following layer learns higher-order features (objects). Multiple
    layers are better because they can learn all the intermediate features between
    the input (raw data) and the high-level classification:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们所做的就是重复相同的模式，每种模式使用不同数量的特征图（32、64和128）。添加层的优势在于每一层都会学习不同抽象级别的特征。在我们的例子中，训练一个CNN来识别物体时，我们可以看到第一层会训练自己识别基本特征（例如，物体的边缘），接下来的层会训练自己识别形状（可以认为是边缘的集合），然后的层会训练自己识别形状集合（以CIFAR-10数据集为例，这些可能是腿、翅膀、尾巴等），接下来的层会学习更高阶的特征（物体）。多个层更有利于因为它们能够学习从输入（原始数据）到高层分类之间的所有中间特征：
- en: '![](img/0f26b44d-ca4c-4fdf-917d-5911f867b7e9.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f26b44d-ca4c-4fdf-917d-5911f867b7e9.png)'
- en: 'Running the training again and doing the evaluation for this new model, the
    result, is `80.57%`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行训练并进行该新模型的评估，结果是`80.57%`：
- en: '![](img/45d9b0f4-7b69-4730-83a1-2f9e53d2fb5c.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45d9b0f4-7b69-4730-83a1-2f9e53d2fb5c.png)'
- en: 'This is a sensible improvement compared to the previous model, and considering
    that we are still running 25 epochs only. But, let''s see now if we can improve
    more by doing image data augmentation. Looking at the training dataset, we can
    see that the objects in the images change their position. Typically, in a dataset,
    images have a variety of conditions (different brightness, orientation, and so
    on). We need to address these situations by training a neural network with additional
    modified data. Consider the following simple example, a training dataset of car
    images with two classes only, Volkswagen Beetle and Porsche Targa. Assume that
    all of the Volkswagen Beetle cars are aligned to the left, such as in the following
    screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的模型相比，这是一个合理的改进，考虑到我们目前只进行了25个epoch的训练。但现在，让我们看看是否可以通过图像数据增强来进一步提升性能。通过查看训练数据集，我们可以看到图像中的物体位置发生了变化。通常情况下，数据集中的图像会有不同的条件（例如亮度、方向等）。我们需要通过使用额外修改过的数据来训练神经网络来应对这些情况。考虑以下简单示例：一个仅包含两类的汽车图像训练数据集，大众甲壳虫和保时捷Targa。假设所有的大众甲壳虫汽车都排列在左侧，如下图所示：
- en: '![](img/6aa90be6-7d08-4d19-9a87-489635148c0e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6aa90be6-7d08-4d19-9a87-489635148c0e.png)'
- en: 'Figure 13.8: Volkswagen Beetle training image'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8：大众甲壳虫训练图像
- en: 'However, all of the Porsche Targa cars are aligned to the right, such as in
    the following screenshot:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有的保时捷Targa汽车都排列在右侧，如下图所示：
- en: '![](img/26cb8adb-cec1-41c0-bf25-d81bc5b345f5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26cb8adb-cec1-41c0-bf25-d81bc5b345f5.png)'
- en: 'Figure 13.9: Porsche Targa training image'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9：保时捷Targa训练图像
- en: 'After completing the training and reaching a high accuracy (90 or 95%), feeding
    the model with an image such as the following screenshot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成训练并达到较高准确率（90%或95%）后，输入以下截图所示的图像进行模型预测：
- en: '![](img/804ed180-0143-44c5-8588-4590f5b151fe.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/804ed180-0143-44c5-8588-4590f5b151fe.png)'
- en: 'Figure 13.10: Volkswagen Beetle input image'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10：大众甲壳虫输入图像
- en: 'There is a concrete risk that this car is classified as a Porsche Targa. In
    order to prevent situations such as this, we need to reduce the number of irrelevant
    features in the training dataset. With reference to this car example, one thing
    we can do is to horizontally flip the training dataset images, so that they face
    the other way. After training the neural network again on this new dataset, the
    performance of the model is more likely to be what is expected. Data augmentation
    could happen offline (which is suitable for small datasets) or online (which is
    suitable for large datasets, because transformations apply on the mini-batches
    that feed the model). Let''s try the programmatic online data augmentation of
    the training dataset for the latest implementation of a model for this section''s
    example, using the `ImageDataGenerator` class from Keras, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里存在一个具体的风险，可能会将这辆车误分类为 Porsche Targa。为了避免这种情况，我们需要减少训练数据集中无关特征的数量。以这辆车为例，我们可以做的一件事是将训练数据集中的图像水平翻转，使它们朝向另一侧。经过再次训练神经网络并使用这个新数据集后，模型的表现更有可能符合预期。数据增强可以在离线（适用于小数据集）或在线（适用于大数据集，因为变换应用于喂给模型的小批次数据）进行。让我们尝试在本节示例的最新模型实现中，使用
    Keras 中的 `ImageDataGenerator` 类进行程序化的在线数据增强，方法如下：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And the using it when fitting the model, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在拟合模型时使用它，如下所示：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'One more thing to do before starting the training is to apply a kernel regularizer
    ([https://keras.io/regularizers/](https://keras.io/regularizers/)) to the convolutional
    layers of our model, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，还需要做一件事，那就是对模型的卷积层应用一个核正则化器（[https://keras.io/regularizers/](https://keras.io/regularizers/)），如下所示：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Regularizers allow us to apply penalties (which are incorporated into the loss
    function) on layer parameters during network optimization.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器允许我们在网络优化过程中对层参数应用惩罚（这些惩罚会被纳入损失函数中）。
- en: 'After these code changes, train the model with a still relatively small number
    of epochs (64) and basic image data augmentation. The following screenshot shows
    that the accuracy improves to almost 84%:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些代码更改后，使用相对较小的训练轮次（64）和基本的图像数据增强来训练模型。下图显示，准确率提高到了接近 84%：
- en: '![](img/941edd1f-b724-4338-8b23-102e12c4aea4.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/941edd1f-b724-4338-8b23-102e12c4aea4.png)'
- en: By training for a greater number of epochs, the accuracy of the model could
    increase up to around 90 or 91%.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练更多的轮次，模型的准确率可能会增加到大约 90% 或 91%。
- en: DL4J implementation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL4J 实现
- en: The second implementation of object recognition we are going to do is in Scala
    and involves the DL4J framework. To train and evaluate the model, we are still
    going to use the CIFAR-10 dataset. The dependencies for this project are a DataVec
    data image, DL4J, NN, and ND4J, plus Guava 19.0 and Apache commons math 3.4.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第二个物体识别实现是基于 Scala，并涉及到 DL4J 框架。为了训练和评估模型，我们仍然使用 CIFAR-10 数据集。本项目的依赖项包括
    DataVec 数据图像、DL4J、NN 和 ND4J，以及 Guava 19.0 和 Apache commons math 3.4。
- en: 'If you look at the CIFAR-10 dataset download page (see the following screenshot),
    you can see that there are specific archives available for the Python, MatLab,
    and C programming languages, but not for Scala or Java:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 CIFAR-10 数据集下载页面（见下图），你会发现有专门为 Python、MatLab 和 C 编程语言提供的归档文件，但没有针对 Scala
    或 Java 的归档文件：
- en: '![](img/9af22061-e703-445e-9694-c9380aee9369.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9af22061-e703-445e-9694-c9380aee9369.png)'
- en: 'Figure 13.11: The CIFAR-10 dataset download page'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11：CIFAR-10 数据集下载页面
- en: 'There''s no need to separately download and then convert the dataset for our
    Scala application; the DL4J dataset library provides the `org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator`
    iterator to get the training and test datasets programmatically, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要单独下载并转换数据集用于 Scala 应用；DL4J 数据集库提供了 `org.deeplearning4j.datasets.iterator.impl.CifarDataSetIterator`
    迭代器，可以编程获取训练集和测试集，如下所示：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `CifarDataSetIterator` constructor expects three arguments: the number
    of batches, the number of samples, and a Boolean to specify whether the dataset
    is for training (`true`) or test (`false`).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`CifarDataSetIterator` 构造函数需要三个参数：批次数、样本数以及一个布尔值，用于指定数据集是用于训练（`true`）还是测试（`false`）。'
- en: 'We can now define the neural network. We implement a function to configure
    the model, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义神经网络了。我们实现一个函数来配置模型，如下所示：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'All the exact same considerations as for the model implemented in the *Keras
    implementation* section apply here. So, we are skipping all the intermediate steps
    and directly implementing a complex model, as shown in following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与在*Keras 实现*部分中实现的模型完全相同的考虑因素也适用于此。因此，我们跳过所有中间步骤，直接实现一个复杂的模型，如下图所示：
- en: '![](img/e8ac8a7f-f58c-462c-b4d9-5b4caf1b7734.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8ac8a7f-f58c-462c-b4d9-5b4caf1b7734.png)'
- en: 'Figure 13.12: The graphical representation of the model for this section''s
    example'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：本节示例的模型图示
- en: 'These are the details of the model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是模型的详细信息：
- en: '| **Layer type** | **Input size** | **Layer size** | **Parameter count** |
    **Weight init** | **Updater** | **Activation function** |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **层类型** | **输入大小** | **层大小** | **参数数量** | **权重初始化** | **更新器** | **激活函数**
    |'
- en: '| Input Layer |  |  |  |  |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 输入层 |  |  |  |  |  |  |'
- en: '| Convolution | 3 | 64 | 3,136 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 3 | 64 | 3,136 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Convolution | 64 | 64 | 65,600 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 64 | 64 | 65,600 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Subsampling (max pooling) |  |  |  |  |  |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 下采样（最大池化） |  |  |  |  |  |  |'
- en: '| Convolution | 64 | 96 | 98,400 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 64 | 96 | 98,400 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Convolution | 96 | 96 | 147,552 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 96 | 96 | 147,552 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Convolution | 96 | 128 | 110,720 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 96 | 128 | 110,720 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Convolution | 128 | 128 | 147,584 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 128 | 128 | 147,584 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Convolution | 128 | 256 | 131,328 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 128 | 256 | 131,328 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Convolution | 256 | 256 | 262,400 | XAVIER_UNIFORM | Adam | ReLU |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 卷积层 | 256 | 256 | 262,400 | XAVIER_UNIFORM | Adam | ReLU |'
- en: '| Subsampling (max pooling) |  |  |  |  |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 下采样（最大池化） |  |  |  |  |  |  |'
- en: '| Dense | 16,384 | 1,024 | 16,778,240 | XAVIER | Adam | Sigmoid |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 全连接层 | 16,384 | 1,024 | 16,778,240 | XAVIER | Adam | Sigmoid |'
- en: '| Dropout | 0 | 0 | 0 |  |  | Sigmoid |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | 0 | 0 | 0 |  |  | Sigmoid |'
- en: '| Dense | 1,024 | 1,024 | 1,049,600 | XAVIER | Adam | Sigmoid |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 全连接层 | 1,024 | 1,024 | 1,049,600 | XAVIER | Adam | Sigmoid |'
- en: '| Dropout | 0 | 0 | 0 |  |  | Sigmoid |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | 0 | 0 | 0 |  |  | Sigmoid |'
- en: '| Output | 1,024 | 10 | 10,250 | XAVIER | Adam | Softmax |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 输出层 | 1,024 | 10 | 10,250 | XAVIER | Adam | Softmax |'
- en: 'Let''s then initialize the model, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们接下来初始化模型，如下所示：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, start the training, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，开始训练，如下所示：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, evaluate it, as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，评估它，如下所示：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The neural network we have implemented here has quite a large number of hidden
    layers, but, following the suggestions from the previous section (adding more
    layers, doing data augmentation, and training for a bigger number of epochs) would
    drastically improve the accuracy of the model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里实现的神经网络具有相当多的隐藏层，但按照前一节的建议（增加更多层，做数据增强，以及训练更多的 epoch），可以大幅提高模型的准确度。
- en: 'The training of course can be done with Spark. The changes needed to the preceding
    code are, as detailed in [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml),
    *Training Neural Networks with Spark*, related to Spark context initialization,
    training data parallelization, `TrainingMaster` creation, and training execution
    using a `SparkDl4jMultiLayer` instance, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 训练当然可以使用 Spark 完成。前述代码中需要的更改，正如在[第 7 章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)中详细介绍的那样，*使用
    Spark 训练神经网络*，涉及 Spark 上下文初始化、训练数据并行化、`TrainingMaster` 创建，以及使用 `SparkDl4jMultiLayer`
    实例执行训练，如下所示：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After a recap of the concept of convolution and the classification of object
    recognition strategies, in this chapter, we have been implementing and training
    CNNs for object recognition using different languages (Python and Scala) and different
    open source frameworks (Keras and TensorFlow in the first case, DL4J, ND4J, and
    Apache Spark in the second) in a hands-on manner.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了卷积概念和物体识别策略分类之后，本章中，我们以实践的方式，使用不同的语言（Python 和 Scala）以及不同的开源框架（第一种情况下使用 Keras
    和 TensorFlow，第二种情况下使用 DL4J、ND4J 和 Apache Spark），实现并训练了卷积神经网络（CNN）进行物体识别。
- en: In the next chapter, we are going to implement a full image classification web
    application which, behind the scenes, uses a combination of Keras, TensorFlow,
    DL4J, ND4J, and Spark.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将实现一个完整的图像分类 web 应用程序，其背后使用了 Keras、TensorFlow、DL4J、ND4J 和 Spark 的组合。
