- en: Natural Language Processing Using Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习的自然语言处理
- en: 'This chapter will demonstrate how to use deep learning for **natural language
    processing** (**NLP**). NLP is the processing of human language text. NLP is a
    broad term for a number of different tasks involving text data, which include
    (but are not limited to) the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示如何使用深度学习进行 **自然语言处理**（**NLP**）。NLP 是对人类语言文本的处理。NLP 是一个广泛的术语，涵盖了涉及文本数据的多种任务，包括（但不限于）以下内容：
- en: '**Document classification**: Classifying documents into different categories
    based on their subject'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分类**：根据主题将文档分类为不同类别'
- en: '**Named entity recognition**: Extracting key information from documents, for
    example, people, organizations, and locations'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**：从文档中提取关键信息，例如人物、组织和地点'
- en: '**Sentiment analysis**: Classifying comments, tweets, or reviews as positive
    or negative sentiment'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：将评论、推文或评价分类为正面或负面情感'
- en: '**Language translation**: Translating text data from one language to another'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：将文本数据从一种语言翻译成另一种语言'
- en: '**Part of speech tagging**: Assigning the type to each word in a document,
    which is usually used in conjunction with another task'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注**：为文档中的每个单词分配类型，通常与其他任务一起使用'
- en: 'In this chapter, we will look at document classification, which is probably
    the most common NLP technique. This chapter follows a different structure to previous
    chapters, as we will be looking at a single use case (text classification) but
    applying multiple approaches to it. This chapter will cover:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论文档分类，这是最常见的自然语言处理技术之一。本章的结构与前几章不同，因为我们将集中讨论一个用例（文本分类），但会应用多种方法。本章将涵盖：
- en: How to perform text classification using traditional machine learning techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用传统机器学习技术进行文本分类
- en: Word vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词向量
- en: Comparing traditional text classification and deep learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较传统文本分类与深度学习
- en: Advanced deep learning text classification including 1D convolutionals, RNNs,
    LSTMs and GRUs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级深度学习文本分类，包括 1D 卷积神经网络、RNN、LSTM 和 GRU
- en: Document classification
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档分类
- en: This chapter will be looking at text classification using Keras. The dataset
    we will use is included in the Keras library. As we have done in previous chapters,
    we will use traditional machine learning techniques to create a benchmark before
    applying a deep learning algorithm. The reason for this is to show how deep learning
    models perform against other techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过 Keras 进行文本分类。我们将使用的数据集包含在 Keras 库中。与前几章一样，我们将首先使用传统机器学习技术创建基准模型，然后再应用深度学习算法。这样做的目的是展示深度学习模型与其他技术的表现差异。
- en: The Reuters dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Reuters 数据集
- en: 'We will use the Reuters dataset, which can be accessed through a function in
    the Keras library. This dataset has 11,228 records with 46 categories. To see
    more information about this dataset, run the following code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Reuters 数据集，可以通过 Keras 库中的一个函数访问该数据集。该数据集包含 11,228 条记录，涵盖 46 个类别。要查看有关该数据集的更多信息，请运行以下代码：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Although the Reuters dataset can be accessed from Keras, it is not in a format
    that can be used by other machine learning algorithms. Instead of the actual words,
    the text data is a list of word indices. We will write a short script (`Chapter7/create_reuters_data.R`)
    that downloads the data and the lookup index file and creates a data frame of
    the `y` variable and the text string. We will then save the train and test data
    into two separate files. Here is the first part of the code that creates the file
    with the train data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以通过 Keras 访问 Reuters 数据集，但它并不是其他机器学习算法可以直接使用的格式。文本数据不是实际的单词，而是单词索引的列表。我们将编写一个简短的脚本（`Chapter7/create_reuters_data.R`），它下载数据及其查找索引文件，并创建一个包含
    `y` 变量和文本字符串的数据框。然后，我们将把训练数据和测试数据分别保存到两个文件中。以下是创建训练数据文件的代码第一部分：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The second part of the code is similar, it creates the file with the test data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第二部分类似，它创建了包含测试数据的文件：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This creates two files called `../data/reuters.train.tab` and `../data/reuters.test.tab`.
    If we open the first file, this is the first data row. This sentence is a normal
    English sentence:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建两个文件，分别是 `../data/reuters.train.tab` 和 `../data/reuters.test.tab`。如果我们打开第一个文件，下面是第一行数据。这句话是一个正常的英语句子：
- en: '| **y** | **sentence** |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **y** | **句子** |'
- en: '| 3 | mcgrath rentcorp said as a result of its december acquisition of space
    co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from
    70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs
    from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from
    12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs
    reuter 3 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 3 | mcgrath rentcorp 表示，由于在12月收购了 Space Co，公司预计1987年的每股收益将在1.15至1.30美元之间，较1986年的70美分有所增长。公司表示，税前净收入将从1986年的600万美元增长到900万至1000万美元，租赁业务收入将从1250万美元增长到1900万至2200万美元。预计今年每股现金流将为2.50至3美元。路透社
    3 |'
- en: 'Now that we have the data in tabular format, we can use *traditional* NLP machine
    learning methods to create a classification model. When we merge the train and
    test sets and look at the distribution of the *y* variable, we can see that there
    are 46 classes, but that the number of instances in each class are not the same:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据转化为表格格式，我们可以使用 *传统* 的NLP机器学习方法来创建分类模型。当我们合并训练集和测试集并查看 *y* 变量的分布时，我们可以看到共有46个类别，但每个类别中的实例数并不相同：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For our test set, we will create a binary classification problem. Our task
    will be to identify the news snippets where the classification is 3 from all other
    records. When we change the labels, our *y* distribution changes to the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的测试集，我们将创建一个二分类问题。我们的任务是从所有其他记录中识别出分类为3的新闻片段。当我们更改标签时，我们的 *y* 分布将变化如下：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Traditional text classification
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统文本分类
- en: Our first NLP model will use traditional NLP techniques, that is, not deep learning.
    For the rest of this chapter, when we use the term traditional NLP, we mean approaches
    that do not use deep learning. The most used method for NLP in traditional NLP classification
    uses a *bag-of-words* approach.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个NLP模型将使用传统的NLP技术，即不使用深度学习。在本章剩余部分，当我们使用传统NLP一词时，我们指的是不使用深度学习的方法。传统NLP分类中最常用的方法是使用
    *词袋模型*。
- en: 'We will also use a set of hyperparameters and machine learning algorithms to
    maximize accuracy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用一组超参数和机器学习算法来最大化准确性：
- en: '**Feature generation**: The features can be term frequency, tf-idf, or binary
    flags'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征生成**：特征可以是词频、tf-idf或二元标志'
- en: '**Preprocessing**: We preprocess text data by stemming the words'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：我们通过对单词进行词干提取来预处理文本数据'
- en: '**Remove stop-words**: We treat the feature creation, stop-words, and stemming
    options as hyperparameters'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除停用词**：我们将特征创建、停用词和词干提取选项视为超参数'
- en: '**Machine learning algorithm**: The script applies three machine learning algorithms
    to the data (Naive Bayes, SVM, neural network, and random forest)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习算法**：该脚本将三种机器学习算法应用于数据（朴素贝叶斯、SVM、神经网络和随机森林）'
- en: 'We train 48 machine learning algorithms on the data in total, and evaluate
    which model is best. The script for this code is in the `Chapter7/classify_text.R` folder.
    The code does not contain any deep learning models, so feel free to skip it if
    you want. First we load in the necessary libraries and create a function that
    creates a set of text classification models for a combination of hyperparameters
    on multiple machine learning algorithms:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据上训练了48个机器学习算法，并评估哪一个模型表现最佳。该代码的脚本位于 `Chapter7/classify_text.R` 文件夹中。该代码不包含任何深度学习模型，所以如果你愿意，可以跳过它。首先，我们加载必要的库，并创建一个函数，用于为多种机器学习算法的超参数组合创建一组文本分类模型：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have created a sparse data-frame, we will use 4 different machine
    learning algorithms on the data: Naive Bayes, SVM, a neural network model and
    a random forest model. We use 4 machine learning algorithms because, as you see
    below, the code to call a machine learning algorithm is small compared to the
    code needed to create the data in the previous section and the code needed to
    the the NLP. It is almost always a good idea to run multiple machine learning
    algorithms when possible because no machine learning algorithm is consistently
    the best.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个稀疏数据框，我们将对数据使用4种不同的机器学习算法：朴素贝叶斯、支持向量机（SVM）、神经网络模型和随机森林模型。我们使用4种机器学习算法，因为正如你所看到的，调用机器学习算法的代码相较于创建前一部分数据和执行NLP所需的代码要少得多。通常来说，当可能时，运行多个机器学习算法总是一个好主意，因为没有任何一个机器学习算法始终是最好的。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now call the function with different hyperparameters in the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用以下代码，通过不同的超参数来调用该函数：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For each combination of hyperparameters, the script saves the best score from
    the four machine learning algorithms in the `best_acc` field. Once the training
    is complete, we can look at the results:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种超参数组合，脚本会将四种机器学习算法中的最佳得分保存在`best_acc`字段中。训练完成后，我们可以查看结果：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The results are ordered by best results, so here we can can see that our best
    accuracy overall was `95.24%`. The reason for training so many models is that
    there is no right formula for traditional NLP tasks that's work for most cases,
    so you should try multiple combinations of preprocessing and different algorithms,
    as we have done here. For example, if you searched for an example online on text
    classification, you could find an example that would suggest to use tf-idf and
    naive bayes. Here, we can see that it is one of the worst performers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果按最佳结果排序，所以我们可以看到我们的最佳准确率整体为`95.24%`。训练这么多模型的原因是，对于传统的自然语言处理任务，没有一个适用于大多数情况的固定公式，因此你应该尝试多种预处理和不同算法的组合，就像我们在这里所做的那样。例如，如果你在线搜索文本分类的示例，你可能会找到一个示例，建议使用tf-idf和朴素贝叶斯。然而，在这里，我们可以看到它是表现最差的模型之一。
- en: Deep learning text classification
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习文本分类
- en: 'The previous code ran 48 traditional machine learning algorithms over the data
    across a number of different hyperparameters. Now, it is time to see if we can
    find a deep learning model that outperforms them. The first deep learning model
    is in `Chapter7/classify_keras1.R`. The first part of the code loads the data. The
    tokens in the reuters dataset are ranked by how often they occur (in the training
    set) and the `max_features` parameter controls how many distinct tokens will be
    used in the model. We will use all the tokens by setting this to the number of
    entries in the word index. The maxlen parameter controls the length of the input
    sequences to the model, they must all be the same length. If the sequences are
    longer than the maxlen variable, they are truncated, if they are shorter, then
    padding is added to make the length=maxlen. We set this to 250, which means our
    deep learning model expects 250 tokens as input per instance:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码运行了48种传统机器学习算法，针对不同的超参数对数据进行了处理。现在，是时候看看我们能否找到一个表现优于它们的深度学习模型了。第一个深度学习模型位于`Chapter7/classify_keras1.R`。代码的第一部分加载了数据。Reuters数据集中的词项按其出现频率（在训练集中的频率）进行排名，`max_features`参数控制模型中将使用多少个不同的词项。我们将此参数设置为词汇表中的条目数，以便使用所有的词项。maxlen参数控制输入序列的长度，所有序列必须具有相同的长度。如果序列长度超过maxlen变量，则会被截断；如果序列较短，则会填充至maxlen长度。我们将其设置为250，这意味着我们的深度学习模型期望每个实例的输入为250个词项：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next section of code builds the model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分构建了模型：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The only thing in this code that we have not seen before is `layer_embedding`. 
    This takes the input and creates an embedding layer, which is a vector of numbers
    for each input token. We will describe word vectors in more detail in the next
    section. Another thing to note is that we don''t preprocess the text or create
    any features – we just feed in the word indices and let the deep learning algorithm
    make sense of it. Here is the output of the script as the model is trained:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中唯一我们之前没见过的是`layer_embedding`。它接收输入并创建一个嵌入层，为每个输入词项生成一个向量。我们将在下一节更详细地描述词向量。需要注意的另一点是，我们没有对文本进行预处理或创建任何特征——我们只是将词汇索引输入，并让深度学习算法自行处理。以下是模型训练过程中的脚本输出：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Despite the simplicity of the code, we get 94.97% accuracy on the validation
    set after just three epochs, which is only 0.27% less than the best traditional
    NLP approach. Now, it is time to discuss word vectors in more detail.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码很简单，但我们在经过仅三次训练周期后，在验证集上的准确率达到了94.97%，仅比最好的传统NLP方法少了0.27%。现在，是时候更详细地讨论词向量了。
- en: Word vectors
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词向量
- en: 'Instead of representing our text data as a bag of words, deep learning represents
    them as word vectors or embeddings. A vector/embedding is nothing more than a
    series of numbers that represent a word. You may have already heard of popular
    word vectors such as Word2Vec and GloVe. The Word2vec model was invented by Google
    (*Mikolov, Tomas, et al. Efficient estimation of word representations in vector
    space. arXiv preprint arXiv:1301.3781 (2013)*). In their paper, they provide examples
    of how these word vectors have somewhat mysterious and magical properties. If
    you take the vector of the word "*King*", subtract the vector of the word "*Man*",
    add the vector of the word "*Man*", then you get a value close to the vector of
    the word "*Queen"*. Other similarities also exist, for example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不是将文本数据表示为词袋模型，而是将其表示为词向量或嵌入。向量/嵌入不过是表示一个词的数字序列。你可能已经听说过流行的词向量，例如Word2Vec和GloVe。Word2Vec模型是由谷歌发明的（*Mikolov,
    Tomas, et al. Efficient estimation of word representations in vector space. arXiv
    preprint arXiv:1301.3781 (2013)*）。在他们的论文中，提供了一些示例，展示了这些词向量具有某种神秘和奇妙的特性。如果你取“*King*”一词的向量，减去“*Man*”一词的向量，再加上“*Man*”一词的向量，你会得到一个接近“*Queen*”一词向量的值。其他相似性也存在，例如：
- en: '*vector(''King'') - vector(''Man'') + vector(''Woman'') = vector(''Queen'')*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vector(''King'') - vector(''Man'') + vector(''Woman'') = vector(''Queen'')*'
- en: '*vector(''Paris'') - vector(''France'') + vector(''Italy'') = vector(''Rome'')*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vector(''Paris'') - vector(''France'') + vector(''Italy'') = vector(''Rome'')*'
- en: 'If this is the first time you have seen Word2Vec, then you are probably somewhat
    amazed by this. I know I was! These examples imply that word vectors *understand* language,
    so have we solved natural language processing? The answer is no – we are very
    far away from this. Vectors are learned from collections of text documents. In
    fact, the very first layer in our deep learning model is an embedding layer which
    creates a vector space for the words. Let''s look at some of the code from `Chapter7/classify_keras.R`
    again:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你第一次接触Word2Vec，那么你可能会对它感到有些惊讶。我知道我当时是！这些示例暗示着词向量*理解*语言，那么我们是否已经解决了自然语言处理的问题呢？答案是否定的——我们距离这个目标还很远。词向量是从文本文件的集合中学习得到的。实际上，我们深度学习模型中的第一层就是嵌入层，它为词语创建了一个向量空间。我们再来看一下`Chapter7/classify_keras.R`中的一些代码：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The value for `max_features` is `30979`, that is, we have `30979` unique features.
    These features are **tokens**, or words. In the traditional text classification,
    we had almost the same number of unique tokens (`30538`). The difference between
    these two numbers is not important; it is due to the different tokenization processes
    used between the two approaches, that is, how the documents were split into tokens.
    The embedding layer has `495664` parameters, which is *30,979 x 16*, that is,
    each unique feature/token is represented by a vector of `16` numbers. The word
    vectors or embeddings learned by deep learning algorithms will have some of the
    characteristics described earlier, for example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`的值是`30979`，也就是说，我们有`30979`个独特的特征。这些特征是**标记**，或者说是词。在传统的文本分类中，我们几乎有相同数量的独特标记（`30538`）。这两个数字之间的差异并不重要；它是由于两种方法中使用的不同分词过程，即文档如何被切分成标记。嵌入层有`495664`个参数，即*30,979
    x 16*，也就是说，每个独特的特征/标记由一个`16`维的向量表示。深度学习算法学习到的词向量或嵌入将具有前面提到的一些特性，例如：'
- en: Synonyms (two words that have the same meaning) will have very similar word
    vectors
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词（意义相同的两个词）会有非常相似的词向量
- en: Words from the same semantic collection will be clustered (for example, colors,
    days of the week, makes of cars, and so on)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自同一语义集合的词语会聚集在一起（例如，颜色、星期几、汽车品牌等）。
- en: The vector space between related words can signify the relationship between
    those words (for example, gender for w(King) – w(Queen))
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关词语之间的向量空间可以表示这些词语之间的关系（例如，w(国王) – w(皇后)的性别关系）
- en: 'The embedding layer creates word vectors/embeddings based on words and their
    surrounding words. The word vectors end up having these characteristics because
    of a simple fact, which can be summarized by a quote from John Firth, an English
    linguist in 1957:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层基于词语及其周围的词语来创建词向量/嵌入。词向量之所以具有这些特性，归结于一个简单的事实，可以用1957年英国语言学家约翰·弗斯的名言来总结：
- en: '"You shall know a word by the company it keeps."'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: “你可以通过一个词周围的词语来了解它的含义。”
- en: The deep learning algorithm learns the vectors for each word by looking at surrounding
    words and therefore learns some of the context. When it sees the word *King*,
    some words near this word may indicate gender, for example, The *King* picked
    up *his* sword. Another sentence could be The *Queen* looked in *her* mirror.
    The word vectors for *King* and *Queen* have a latent gender component that is
    learned from the words surrounding *King* and *Queen* in the data. But it is important
    to realize that the deep learning algorithm has no concept of what gender is,
    or what type of entities it applies to. Even so, word vectors are a huge improvement
    over bag-of-word approaches which have no way of identifying relationships between
    different tokens. Using word vectors also means that we do not have to discard
    sparse terms. Finally, as the number of unique tokens increases, they are much
    more efficient to process than bag-of-words approaches.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法通过观察周围的词汇来学习每个单词的向量，因此可以学习到一些上下文。当它看到*King*这个词时，周围的某些词可能会暗示出性别信息，例如，“The
    *King* picked up *his* sword。”另一句话可能是“The *Queen* looked in *her* mirror。”*King*和*Queen*的词向量在数据中从周围的词汇中学习到了一些潜在的性别成分。但需要意识到的是，深度学习算法并不理解性别是什么，或者它适用于什么样的实体。即便如此，词向量仍然比词袋方法有了很大的改进，因为词袋方法无法识别不同标记之间的关系。使用词向量还意味着我们不必丢弃稀疏词条。最后，随着唯一标记数量的增加，处理它们比词袋方法更加高效。
- en: We will look at embeddings again in [Chapter 9](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml), *Anomaly
    Detection and Recommendation Systems*, when we use them in auto-encoders. Now
    that we have seen some traditional machine learning and deep learning approaches
    for solving this problem, it is time to compare them in more detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第9章](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml)《异常检测与推荐系统》中再次讨论嵌入，当我们在自编码器中使用它们时。现在，我们已经了解了一些传统机器学习和深度学习方法来解决这个问题，接下来是时候更详细地比较它们了。
- en: Comparing traditional text classification and deep learning
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较传统文本分类和深度学习
- en: 'The traditional text classification performed a number of preprocessing steps,
    including word stemming, stop-word processing, and feature generation (tf-idf,
    tf or binary). The deep learning text classification did not need this preprocessing.
    You may have heard various reasons for this previously:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的文本分类执行了多个预处理步骤，包括词干提取、停用词处理和特征生成（tf-idf，tf或二进制）。而深度学习文本分类不需要这些预处理。你可能之前听过各种关于这一点的解释：
- en: Deep learning can learn features automatically, so feature creation is not needed
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以自动学习特征，因此不需要手动创建特征
- en: Deep learning algorithms for NLP tasks requires far less preprocessing than traditional text
    classification
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法在NLP任务中所需的预处理远少于传统的文本分类方法
- en: There is some truth to this, but this does not answer why we need complex feature
    generation in traditional text classification. A big reason that preprocessing
    is needed in traditional text classification is to overcome a fundamental problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实有一定的道理，但这并没有回答为什么我们在传统文本分类中需要复杂的特征生成。传统文本分类中需要预处理的一个主要原因是为了克服一个根本性的问题。
- en: For some traditional NLP approaches (for example, classification), text preprocessing
    is not just about creating better features. It is also necessary because the bag-of-words
    representation creates a sparse high-dimensional dataset. Most machine learning
    algorithms have problems with such datasets, meaning that we have to reduce the
    dimensionality before applying machine learning algorithms. Proper preprocessing
    of the text is an essential part of this to ensure that relevant data is not thrown
    away.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些传统的NLP方法（例如分类），文本预处理不仅仅是为了创建更好的特征。它也是必要的，因为词袋表示法会产生一个稀疏的高维数据集。大多数机器学习算法在处理这样的数据集时会遇到问题，这意味着我们必须在应用机器学习算法之前减少数据的维度。适当的文本预处理是这一过程的关键，确保相关数据不会被丢弃。
- en: 'For traditional text classification, we used an approach called **bag-of-words**.
    This is essentially one-hot encoding each ***token*** (word). Each column represents
    a single token, and the value of each cell is one of the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传统的文本分类，我们使用了一种叫做**词袋模型**的方法。这本质上是对每个***标记***（单词）进行独热编码。每一列代表一个单独的标记，每个单元格的值是以下之一：
- en: A **tf-idf** (**term frequency, inverse document frequency**) for that token
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tf-idf**（**词频，逆文档频率**）用于该标记'
- en: The term frequency, that is, the count of how many times that token occurs for
    that document/instance
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词频，也就是该标记在文档/实例中出现的次数
- en: A binary flag, that is, one if the token is in that document/instance; otherwise,
    it is zero
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个二进制标志，也就是说，如果该标记出现在该文档/实例中，则为1；否则为0
- en: You may not have heard of *tf-idf* before. It measures the importance of a token
    by calculating the term frequency (*tf*) of the token in the document (such as
    how many times it occurs in the document) divided by the log of how many times
    it appears in the entire corpus (*idf*). The **corpus** is the entire collection
    of documents. The *tf* part measures how important the token is within a single
    document, and the *idf* measures how unique the token is among all the documents.
    If the token appears many times in the document, but also many times in other
    documents, then it is unlikely to be useful for categorizing documents. If the
    token appears in only a few documents, then it is a potentially valuable feature
    for the classification task.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能以前没听说过*tf-idf*。它通过计算标记在文档中的词频（*tf*）（例如该标记在文档中出现的次数），除以该标记在整个语料库中的出现次数的对数（*idf*），来衡量标记的重要性。**语料库**是所有文档的集合。*tf*部分衡量标记在单个文档中的重要性，而*idf*衡量该标记在所有文档中的独特性。如果标记在文档中出现多次，但也在其他文档中出现多次，那么它不太可能对文档分类有用。如果该标记只出现在少数几个文档中，那么它可能是一个对分类任务有价值的特征。
- en: 'Our traditional text classification approach also used *stemming* and processed
    *stop-words*. Indeed, our best result in traditional text classification used
    both approaches. Stemming tries to reduce words to their word stem or root form,
    which reduces the vocabulary size. It also means that words with the same meaning
    but with different verb tenses or noun forms are standardized to the same token.
    Here is an example of stemming. Note that 6/7 of the input words have the same
    output value:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的传统文本分类方法也使用了*词干提取*（stemming）和处理*停用词*（stop-words）。实际上，我们在传统文本分类中取得的最佳结果使用了这两种方法。词干提取尝试将单词还原为它们的词干或根形式，从而减少词汇表的大小。它还意味着具有相同意义但动词时态或名词形式不同的单词会标准化为相同的标记。以下是一个词干提取的例子。请注意，输入词中的6个/7个词的输出值是相同的：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Stop-words are common words that appear in most documents for a language. They
    occur so frequently in most documents that they are almost never useful for machine
    learning. The following example shows the list of stop-words for the English language:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是指在一种语言的大多数文档中都会出现的常见词汇。它们在大多数文档中出现的频率非常高，以至于几乎永远不会对机器学习有用。以下示例展示了英语语言中的停用词列表：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The final piece we want to cover in traditional NLP is how it deals with sparse
    terms. Recall from earlier that traditional NLP uses a bag-of-words approach,
    where each unique token gets an individual column. For a large collection of documents,
    there will be thousands of unique tokens, and since most tokens will not appear
    in an individual document, this a very sparse representation, that is, most cells
    are empty. We can check this by looking at taking some of the code from `classify_text.R`,
    modifying it slightly, and looking at the `dtm` and `dtm2` variables:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在传统自然语言处理（NLP）中要讨论的最后一部分是它如何处理稀疏词项。回想一下，传统的NLP采用词袋模型（bag-of-words），其中每个唯一的标记（token）会得到一个单独的列。对于大量文档集合来说，将会有成千上万个唯一的标记，而由于大多数标记不会出现在单个文档中，这就导致了非常稀疏的表示，也就是说，大多数单元格都是空的。我们可以通过查看`classify_text.R`中的一些代码，稍作修改，然后查看`dtm`和`dtm2`变量来验证这一点：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that our first document-term matrix (dtm) has 11,228 documents and
    30,538 unique tokens. In this document-term matrix, only 768,265 (0.22%) cells
    have values. Most machine learning algorithms would struggle with such a high-dimensionality sparse
    data frame. If you tried using these machine learning algorithms (for example,
    SVM, random forest, naive bayes) on a data frame with 30,538 dimensions, they
    would fail to run in R (I tried!). This is a known problem in traditional NLP,
    so there is a function (`removeSparseTerms`) in the NLP libraries to remove sparse
    terms from the document-term matrix. This removes columns that have the most empty
    cells. We can see the effect of this, as the second document-term matrix has only
    230 unique tokens and 310,275 (12%) cells have values. This dataset is still relatively
    sparse, but it is in a usable format for machine learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的第一个文档-词项矩阵（dtm）有11,228个文档和30,538个独特的词汇。在这个文档-词项矩阵中，只有768,265个（0.22%）单元格有值。大多数机器学习算法处理这样一个高维度稀疏数据框架时都会遇到困难。如果你尝试在一个有30,538维的数据框上使用这些机器学习算法（例如，SVM、随机森林、朴素贝叶斯），它们在R中无法运行（我试过了！）。这是传统NLP中的一个已知问题，所以在NLP库中有一个函数（`removeSparseTerms`）可以从文档-词项矩阵中去除稀疏词项。这个函数会去掉那些大部分单元格为空的列。我们可以看到其效果，第二个文档-词项矩阵仅有230个独特的词汇，且310,275个（12%）单元格有值。这个数据集依然相对稀疏，但它已转化为适合机器学习的格式。
- en: 'This highlights the problem with traditional NLP approaches: the *bag-of-words* approach
    creates a very sparse high-dimensional dataset which is not usable by machine
    learning algorithms. Therefore, you need to remove some of the dimensions, and
    this results in a number of cells with values going from 768,265 to 310,275 in
    our example. We threw away almost 60% of the data before applying any machine
    learning! This also explains why text preprocessing, such as stemming and stop-word
    removal, is used in traditional NLP. Stemming helps to reduce the vocabulary and
    standardize terms by combining variations of many words into one form.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了传统NLP方法的问题：*词袋模型*方法创建了一个非常稀疏的高维数据集，而这个数据集不能被机器学习算法使用。因此，你需要去除一些维度，这就导致在我们的示例中，单元格中的有值数量从768,265减少到310,275。我们在应用任何机器学习之前就丢弃了几乎60%的数据！这也解释了为什么在传统NLP中使用文本预处理步骤，如词干提取和停用词移除。词干提取有助于减少词汇量，并通过将许多词汇的变体合并为一个形式来标准化术语。
- en: 'By combining variations, it means they are more likely to survive the culling
    of data. We process stop-words for the opposite reason: if we don''t remove stop-words,
    these terms will probably be kept after removing sparse terms. There are 174 terms
    in the `stopwords()` function in the `tm` package. If the reduced dataset had
    many of these terms, then they would probably not be useful as predictor variables
    due to their commonality throughout the documents.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过合并变体，意味着它们更有可能在数据筛选时存活下来。我们处理停用词的理由则恰恰相反：如果我们不去除停用词，这些词可能会在去除稀疏词项后被保留下来。在`tm`包中的`stopwords()`函数里有174个停用词。如果减少后的数据集中有许多这些词，它们可能不会作为预测变量发挥作用，因为它们在文档中普遍存在。
- en: It is also worth noting that this is a very small dataset in NLP terms. We only
    have 11,228 documents and 30,538 unique tokens. A larger ***corpus*** (collection
    of text documents) could have half a million unique tokens. In order to reduce
    the number of tokens to something that could be processed in R, we would have
    to throw away a lot more data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 同样值得注意的是，在自然语言处理（NLP）领域，这个数据集非常小。我们只有11,228个文档和30,538个独特的词汇。一个更大的***语料库***（文本文件集合）可能包含有五十万个独特的词汇。为了将词汇的数量减少到一个可以在R中处理的水平，我们不得不丢弃更多的数据。
- en: When we use a deep learning approach for NLP, we represent the data as word
    vectors/embeddings rather than using the bag-of-words approach in traditional
    NLP. This is much more efficient, so do not have to preprocess data to remove
    common words, reduce words to a simpler form, or reduce the number of terms before
    applying the deep learning algorithm. The only thing we do have to do is pick
    an embedding size and a max length size for the number of tokens we process for
    each instance. This is needed because deep learning algorithms cannot use variable
    length sequences as inputs to a layer. When instances have more tokens than the
    max length, they are truncated and when instances have less tokens than the max
    length, they are padded.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用深度学习方法进行NLP时，我们将数据表示为词向量/嵌入，而不是采用传统NLP中的词袋方法。这种方法更加高效，因此无需预处理数据来去除常见词汇、简化词形或在应用深度学习算法之前减少词汇数量。我们唯一需要做的就是选择嵌入大小和处理每个实例时最大令牌数的长度。这是必要的，因为深度学习算法不能将可变长度的序列作为输入传递到一个层次。当实例的令牌数量超过最大长度时，它们会被截断；当实例的令牌数量少于最大长度时，它们会被填充。
- en: 'After all of this, you may be wondering why the deep learning algorithm did
    not outperform the traditional NLP approach significantly, if the traditional NLP
    approach throws away 60% of the data. There are a few reasons for this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一切完成后，你可能会想，如果传统NLP方法丢弃了60%的数据，为什么深度学习算法并没有显著超过传统NLP方法？原因有几个：
- en: The dataset is small. If we had more data, the deep learning approach would
    improve at a faster rate than the traditional NLP approach.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集很小。如果我们拥有更多的数据，深度学习方法的提升速度将快于传统NLP方法。
- en: Certain NLP tasks such as document classification and sentiment analysis depend
    on a very small set of terms. For example, to differentiate between sports news
    and financial news, maybe 50 selected terms would be sufficient to get over 90%
    accuracy. Recall that the function to remove sparse terms in the traditional text
    classification approach – this works because it assumes (correctly) that non-sparse
    terms will be useful features for the machine learning algorithms.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些NLP任务，如文档分类和情感分析，依赖于一小部分特定的词汇。例如，为了区分体育新闻和财经新闻，也许50个精选的词汇就足以达到90%以上的准确率。回想一下传统文本分类方法中用于去除稀疏词汇的功能——之所以有效，是因为它假设（并且正确）非稀疏词汇对于机器学习算法来说是有用的特征。
- en: We ran 48 machine learning algorithms and only one deep learning approach, which
    was relatively simple! We will soon come across approaches that beat the traditional NLP
    approach.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们运行了48个机器学习算法，仅有一个深度学习方法，而且它相对简单！我们很快会遇到一些方法，它们在性能上超过了传统的NLP方法。
- en: This book has really only touched the surface of traditional NLP approaches.
    Entire books have been written on the subject. The purpose of looking at these
    approaches is to show how brittle these approaches can be. The deep learning approach
    is much simpler to understand and has far fewer settings. It does not involve
    preprocessing the text or creating features based on weightings such as tf-idf.
    Even so, our first deep learning approach is not very far away from the best model
    out of 48 models in traditional text classification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本书实际上只是触及了传统NLP方法的表面。关于这个话题已经有整本书的内容。研究这些方法的目的是展示它们的脆弱性。深度学习方法更容易理解，且设置远少于传统方法。它不涉及文本的预处理或基于加权（如tf-idf）来创建特征。即便如此，我们的第一个深度学习方法也与传统文本分类中48个模型中的最佳模型相差无几。
- en: Advanced deep learning text classification
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级深度学习文本分类
- en: 'Our basic deep learning model is much less complex than the traditional machine
    learning approach, but its performance is not quite as good. This section looks
    at some advanced techniques for text classification in deep learning. The following
    sections explain a number of different approaches and focus on code examples rather
    than heavy deep explanations. If you are interested in more detail, then look
    at the book *Deep Learning* by Goodfellow, Bengio, and Courville (*Goodfellow,
    Ian, et al. Deep learning. Vol. 1\. Cambridge: MIT Press, 2016.*). Another good
    reference that covers NLP in deep learning is a book by Yoav Goldberg (*Goldberg,
    Yoav. Neural network methods for natural language processing*).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的基本深度学习模型比传统的机器学习方法要简单得多，但其性能并不完全优越。本节将探讨一些深度学习中用于文本分类的高级技术。接下来的章节将解释多种不同的方法，并侧重于代码示例，而非过多的理论解释。如果你对更详细的内容感兴趣，可以参考Goodfellow、Bengio
    和 Courville的书《*Deep Learning*》（*Goodfellow, Ian, et al. Deep learning. Vol. 1.
    Cambridge: MIT Press, 2016.*）。另一本很好的参考书是Yoav Goldberg的书《*Neural network methods
    for natural language processing*》，它涵盖了深度学习中的自然语言处理（NLP）。'
- en: 1D convolutional neural network model
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1D 卷积神经网络模型
- en: 'We have seen that the bag-of-words approach in traditional NLP approaches ignores
    sentence structure. Consider applying a sentiment analysis task on the four movie
    reviews in the following table:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，传统NLP方法中的词袋模型忽视了句子结构。考虑在下表中的四条电影评论上应用情感分析任务：
- en: '| **Id** | **sentence** | **Rating (1=recommended, 0=not recommended)** |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **Id** | **句子** | **评分（1=推荐，0=不推荐）** |'
- en: '| 1 | this movie is very good | 1 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 这部电影非常好 | 1 |'
- en: '| 2 | this movie is not good | 0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 这部电影不好 | 0 |'
- en: '| 3 | this movie is not very good | 0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 这部电影不太好 | 0 |'
- en: '| 4 | this movie is not bad | 1 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 这部电影不好 | 1 |'
- en: 'If we represent this as a bag of words with term frequency, we will get the
    following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其表示为词袋模型，并计算词频，我们将得到以下输出：
- en: '| **Id** | **bad** | **good** | **is** | **movie** | **not** | **this** | **very**
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **Id** | **坏** | **好** | **是** | **电影** | **不** | **这** | **非常** |'
- en: '| 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 |'
- en: '| 2 | 0 | 1 | 1 | 1 | 1 | 1 | 0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 1 | 1 | 1 | 1 | 1 | 0 |'
- en: '| 3 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: '| 4 | 1 | 0 | 1 | 1 | 1 | 1 | 0 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 0 | 1 | 1 | 1 | 1 | 0 |'
- en: In this simple example, we can see some of the problems with a bag-of-words
    approach, we have lost the relationship between the negation (**not**) and the
    adjectives (**good**, **bad**). To work around this problem, traditional NLP could
    use bigrams, so instead of using single words as tokens, use two words as tokens.
    Now, for the second example, **not good** is a single token, which makes it more
    likely that the machine learning algorithm will pick it up. However, we still
    have a problem with the third example (**not very good**), where we will have
    tokens for **not very** and **very good**. These are still ambiguous, as **not
    very** implies negative sentiment, while **very good** implies positive sentiment.
    We could try higher order n-grams, but this further exacerbates the sparsity problem
    we saw in the previous section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们可以看到词袋方法的一些问题，我们丢失了否定词（**not**）与形容词（**good**，**bad**）之间的关系。为了解决这个问题，传统NLP方法可能会使用二元词组（bigrams），也就是说，不使用单一的单词作为标记，而是使用两个单词作为标记。现在，在第二个例子中，**not
    good**将作为一个标记，这样机器学习算法更有可能识别它。然而，第三个例子（**not very good**）仍然存在问题，因为我们会得到**not very**和**very
    good**两个标记。这些仍然是模糊的，**not very**暗示着负面情感，而**very good**则暗示着正面情感。我们可以尝试更高阶的n-gram，但这会进一步加剧我们在前一节看到的稀疏性问题。
- en: 'Word vectors or embeddings have the same problem. We need some method to handle
    word sequences. Fortunately, there are types of layers in deep learning algorithms
    that can handle sequential data. One that we have already seen is convolutional
    neural networks in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),* Image
    Classification Using Convolutional Neural Networks*. Recall that these are 2D
    patches that are moved across the image to identify patterns such as a diagonal
    line or an edge. In a similar manner, we can apply a 1D convolutional neural network
    across the word vectors. Here is an example of using a 1D convolutional neural
    network layer for the same text classification problem. The code is in `Chapter7/classify_keras2.R`.
    We are only showing the code for the model architecture, because that is the only
    change from the code in `Chapter7/classify_keras1.R`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量或嵌入也面临相同的问题。我们需要某种方法来处理词序列。幸运的是，深度学习算法中有一些层可以处理顺序数据。我们已经在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中看到过一种，这一章讨论了**卷积神经网络在图像分类中的应用**。回想一下，这些是移动于图像上的2D补丁，用来识别模式，如对角线或边缘。类似地，我们可以将1D卷积神经网络应用于词向量。以下是使用1D卷积神经网络层来解决相同文本分类问题的示例。代码位于`Chapter7/classify_keras2.R`。我们只展示模型架构的代码，因为这与`Chapter7/classify_keras1.R`中的代码唯一的不同：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can see that this follows the same pattern that we saw in the image data;
    we have a convolutional layer followed by a max pooling layer. There are 64 convolutional
    layers with a `length=5`, and so these *learn* local patterns in the data. Here
    is the output from the model''s training:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这与我们在图像数据中看到的模式相同；我们有一个卷积层，后面跟着一个最大池化层。这里有64个卷积层，`length=5`，因此这些层能够*学习*数据中的局部模式。以下是模型训练的输出：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This model is an improvement on our previous deep learning model; it gets 95.73%
    accuracy on the fourth epoch. This beats the traditional NLP approach by 0.49%,
    which is a significant improvement. Let's move on to other methods that also look
    to matching sequences. We will start with **recurrent neural networks** (**RNNs**).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型比我们之前的深度学习模型有所改进；它在第四个周期时取得了95.73%的准确率。这比传统的NLP方法提高了0.49%，这是一个显著的进步。接下来，我们将介绍其他也关注序列匹配的方法。我们将从**循环神经网络**（**RNNs**）开始。
- en: Recurrent neural network model
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络模型
- en: 'The deep learning networks we have seen so far have no concept of memory. Every
    new piece of information is treated as atomic and has no relation to what has
    already occurred. But, sequences are important in time series and text classification,
    especially sentiment analysis. In the previous section, we saw how word structure
    and order matters, and we used CNNs to resolve this. While this approach worked,
    it does not resolve the problem completely as we still must pick a filter size,
    which limits the range of the layer. Recurrent neural networks are deep learning
    layers which are used to solve this problem. They are networks with feedback loops
    that allow information to flow and therefore are able to *remember* important
    features:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所见的深度学习网络没有记忆的概念。每一条新信息都被视为原子信息，与已经发生的事情没有关联。但在时间序列和文本分类中，特别是在情感分析中，序列是非常重要的。在上一节中，我们看到词的结构和顺序是至关重要的，我们使用卷积神经网络（CNN）来解决这个问题。虽然这种方法有效，但它并没有完全解决问题，因为我们仍然需要选择一个过滤器大小，这限制了层的范围。循环神经网络（RNN）是用来解决这个问题的深度学习层。它们是带有反馈回路的网络，允许信息流动，因此能够*记住*重要特征：
- en: '![](img/50985a6f-072d-41b1-ad78-6a456fa9d8f1.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50985a6f-072d-41b1-ad78-6a456fa9d8f1.png)'
- en: 'Figure 7.1: A recurrent neural network'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：一个循环神经网络
- en: In the preceding diagram, we can see an example of a recurrent neural network.
    Each piece of information (X[o], X[1], X[2]) is fed into a node which predicts
    *y* variables. The predicted value is also passed to the next node as input, thus
    preserving some sequence information.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到一个循环神经网络的示例。每一条信息（X[o], X[1], X[2]）都被输入到一个节点，该节点预测*y*变量。预测值也被传递到下一个节点作为输入，从而保留了一些序列信息。
- en: 'Our first RNN model is in `Chapter7/classify_keras3.R`. We have to change some
    of the parameters for the model: we must decrease the number of features used
    to 4,000, our max length to 100, and drop the most common 100 tokens. We must
    also increase the size of the embedding layer to 32 and run it for 10 epochs:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个RNN模型位于`Chapter7/classify_keras3.R`。我们需要调整模型的一些参数：我们必须将使用的特征数减少到4,000，将最大长度调整为100，并删除最常见的100个标记。我们还需要增加嵌入层的大小至32，并运行10个周期：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the output from the model''s training:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型训练的输出：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The best validation accuracy was after epoch 7, where we got 93.90% accuracy,
    which is not as good as the CNN model. One of the problems with simple RNN models
    is that it is difficult to maintain context as the gap grows between the different
    pieces of information. Let's move onto a more complex model, that is, the LSTM
    model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳验证准确率出现在第7个训练周期，达到了93.90%的准确率，虽然不如CNN模型。简单RNN模型的一个问题是，当不同信息之间的间隔变大时，很难保持上下文。接下来我们将讨论一个更复杂的模型，即LSTM模型。
- en: Long short term memory model
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）模型
- en: LSTMs are designed to learn long-term dependencies. Similar to RNNs, they are
    chained and have four internal neural network layers. They split the state into
    two parts, where one part manages short-term state and the other adds long-term
    state. LSTMs have *gates* which control how *memories* are stored. The input gate
    controls which part of the input should be added to the long-term memory. The
    forget gate controls the part of long-term memory that should be forgotten. The
    final gate, the output gate, controls which part of the long-term memory should
    be in the output. This is a brief description of LSTMs – a good reference for
    more details is [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆网络）被设计用来学习长期依赖关系。与RNN类似，它们是链式结构，并且有四个内部神经网络层。它们将状态分为两部分，一部分管理短期状态，另一部分添加长期状态。LSTM具有*门控*机制，用于控制*记忆*的存储方式。输入门控制应该将输入的哪部分加入到长期记忆中。遗忘门控制应该遗忘长期记忆中的哪部分。最后一个门，即输出门，控制长期记忆中应该包含哪部分内容。以上是LSTM的简要描述——想了解更多细节，参考[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: 'The code for our LSTM model is in `Chapter7/classify_keras4.R`. The parameters
    for the model are max length=150, the size of the embedding layer=32, and the
    model was trained for 10 epochs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LSTM模型代码在`Chapter7/classify_keras4.R`中。模型的参数为最大长度=150，嵌入层大小=32，模型训练了10个周期：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the output from the model''s training:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型训练的输出：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The best validation accuracy was after epoch 5, when we got 95.37% accuracy,
    which is a big improvement on the simple RNN model, although still not as good
    as the CNN model. We will cover GRU cells next, which are a similar concept to LSTM.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳验证准确率出现在第5个训练周期，达到了95.37%的准确率，这是相较于简单RNN模型的一大进步，尽管仍然不如CNN模型好。接下来我们将介绍GRU单元，它与LSTM有相似的概念。
- en: Gated Recurrent Units model
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）模型
- en: '**Gated recurrent units (GRUs)** are similar to LSTM cells but simpler. They
    have one gate that combines the forget and input gates in LSTM, and there is no
    output gate. While GRUs are simpler than LSTMs and therefore quicker to train,
    it is a matter of debate on whether they are better than LSTMs, as the research
    is inconclusive. Therefore, it is recommended to try both, as the results of your
    task may vary. The code for our GRU model is in `Chapter7/classify_keras5.R`.
    The parameters for the model are max length=150, the size of the embedding layer=32,
    and the model was trained for 10 epochs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控循环单元（GRUs）**与LSTM单元相似，但更简单。它们有一个门控机制，结合了LSTM中的遗忘门和输入门，并且没有输出门。虽然GRU比LSTM更简单，因此训练速度更快，但是否优于LSTM仍然存在争议，因为研究结果尚无定论。因此，建议同时尝试两者，因为不同任务的结果可能会有所不同。我们的GRU模型代码在`Chapter7/classify_keras5.R`中。模型的参数为最大长度=150，嵌入层大小=32，模型训练了10个周期：'
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is the output from the model''s training:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型训练的输出：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The best validation accuracy was after epoch 5, when we got 95.90% accuracy,
    which is an improvement on the 95.37% we got with LSTM. In fact, this is the best
    result we have seen so far. In the next section, we will look at bidirectional
    architectures.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳验证准确率出现在第5个训练周期，达到了95.90%的准确率，较LSTM的95.37%有所提升。实际上，这是我们迄今为止看到的最佳结果。在下一部分中，我们将讨论双向架构。
- en: Bidirectional LSTM model
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向LSTM模型
- en: 'We saw in *Figure 7.1* that RNNs (as well as LSTMs and GRUs) are useful because
    they can pass information forwards. But in NLP tasks, it is also useful to look
    backwards. For example, the following two strings have the same meaning:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图7.1*中看到，RNN（以及LSTM和GRU）很有用，因为它们可以向前传递信息。但是在自然语言处理任务中，回溯信息同样也很重要。例如，下面这两句话的意思是相同的：
- en: I went to Berlin in spring
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在春天去了柏林
- en: In spring I went to Berlin
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 春天我去了柏林
- en: 'Bidirectional LSTMs can pass information backwards as well as forwards. The
    code for our bidirectional LSTM model is in `Chapter7/classify_keras6.R`. The
    parameters for the model are max length=150, the size of the embedding layer=32,
    and the model was trained for 10 epochs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 双向LSTM可以将信息从未来状态传递到当前状态。我们的双向LSTM模型的代码在`Chapter7/classify_keras6.R`中。模型的参数为最大长度=150，嵌入层的大小=32，模型训练了10个周期：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is the output from the model''s training:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型训练的输出：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The best validation accuracy was after epoch 4, when we got 95.77% accuracy.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳验证准确度是在第4个周期后得到的，当时我们获得了95.77%的准确度。
- en: Stacked bidirectional model
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠双向模型
- en: 'Bidirectional models are good at picking up information from future states
    that can affect the current state. Stacked bidirectional models allow us to stack
    multiple LSTM/GRU layers in a similar manner to how we stack multiple convolutional
    layers in computer vision tasks. The code for our bidirectional LSTM model is
    in `Chapter7/classify_keras7.R`. The parameters for the model are max length=150,
    the size of the embedding layer=32, and the model was trained for 10 epochs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 双向模型擅长从未来状态中获取信息，这些信息会影响当前状态。堆叠双向模型使我们能够像堆叠计算机视觉任务中的多个卷积层一样，堆叠多个LSTM/GRU层。我们的双向LSTM模型的代码在`Chapter7/classify_keras7.R`中。模型的参数为最大长度=150，嵌入层的大小=32，模型训练了10个周期：
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the output from the model''s training:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型训练的输出：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The best validation accuracy was after epoch 4, when we got 95.59% accuracy,
    which is worse than our bidirectional model, which got 95.77% accuracy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳验证准确度是在第4个周期后得到的，当时我们获得了95.59%的准确度，这比我们的双向模型差，后者的准确度为95.77%。
- en: Bidirectional with 1D convolutional neural network model
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向1D卷积神经网络模型
- en: So far, the best approaches we have seen are from the 1D convolutional neural
    network model which got 95.73%, and the gated recurrent units model which got
    95.90% accuracy. The following code combines them! The code for our bidirectional
    with 1D convolutional neural network model is in `Chapter7/classify_keras8.R`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的最佳方法来自1D卷积神经网络模型，其准确度为95.73%，以及门控递归单元模型，其准确度为95.90%。以下代码将它们结合在一起！我们的双向1D卷积神经网络模型的代码在`Chapter7/classify_keras8.R`中。
- en: 'The parameters for the model are max length=150, the size of the embedding
    layer=32, and the model was trained for 10 epochs:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的参数为最大长度=150，嵌入层的大小=32，模型训练了10个周期：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output from the model''s training:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型训练的输出：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The best validation accuracy was after epoch 6, when we got 96.04% accuracy,
    which beats all of the previous models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳验证准确度是在第6个周期后得到的，当时我们获得了96.04%的准确度，超越了所有之前的模型。
- en: Comparing the deep learning NLP architectures
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较深度学习NLP架构
- en: 'Here is a summary of all of the models in this chapter, ordered by their sequence
    in this chapter. We can see that the best traditional machine learning approach
    got 95.24%, which was beaten by many of the deep learning approaches. While the
    incremental changes from the best traditional machine learning to the best deep
    learning model may seem small at 0.80%, it reduces our misclassified examples
    by 17%, which is a significant relative change:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章所有模型的总结，按本章中的顺序排列。我们可以看到，最佳传统机器学习方法的准确度为95.24%，被许多深度学习方法超越。虽然最佳传统机器学习方法与最佳深度学习模型之间的增量变化看起来仅为0.80%，但它将我们的误分类示例减少了17%，这是一个显著的相对变化：
- en: '| **Model** | **Accuracy** |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **准确度** |'
- en: '| Best traditional machine learning approach | 95.24% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 最佳传统机器学习方法 | 95.24% |'
- en: '| Simple deep learning approach | 94.97% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 简单的深度学习方法 | 94.97% |'
- en: '| 1D convolutional neural network model | 95.73% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 1D卷积神经网络模型 | 95.73% |'
- en: '| Recurrent neural network model | 93.90% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 循环神经网络模型 | 93.90% |'
- en: '| Long short term memory model | 95.37% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 长短期记忆模型 | 95.37% |'
- en: '| Gated recurrent units model | 95.90% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 门控递归单元模型 | 95.90% |'
- en: '| Bidirectional LSTM model | 95.77% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 双向LSTM模型 | 95.77% |'
- en: '| Stacked bidirectional model | 95.59% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 堆叠双向模型 | 95.59% |'
- en: '| Bidirectional with 1D convolutional neural network | 96.04% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 双向1D卷积神经网络 | 96.04% |'
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We really covered a lot in this chapter! We built a fairly complex traditional
    NLP example that had many hyperparameters, as well as training it on several machine
    learning algorithms. It achieved a reputable result of getting 95.24% accuracy.
    However, when we looked into traditional NLP in more detail, we found that it
    had some major problems: it requires non-trivial feature engineering, it creates
    sparse high-dimensional data frames, and it may require discarding a substantial
    amount of data before machine learning.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们真的涵盖了很多内容！我们构建了一个相当复杂的传统 NLP 示例，包含了许多超参数，并在多个机器学习算法上进行了训练。它取得了 95.24% 的可靠准确率。然而，当我们更深入地研究传统
    NLP 时，发现它存在一些主要问题：需要复杂的特征工程，生成稀疏的高维数据框，并且可能需要在机器学习之前丢弃大量数据。
- en: In comparison, the deep learning approach uses word vectors or embeddings, which
    are much more efficient and do not require preprocessing. We ran through a number
    of deep learning approaches, including 1D convolutional layers, Recurrent Neural
    Networks, GRUs, and LSTM. We finally combined the two best previous approaches
    into one approach in our final model to get 96.08% accuracy, compared to 95.24%
    by using traditional NLP.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，深度学习方法使用词向量或嵌入，这些方法更加高效，并且不需要预处理。我们介绍了多种深度学习方法，包括 1D 卷积层、循环神经网络、GRU 和 LSTM。最后，我们将前两种最佳方法结合成一种方法，并在最终模型中获得了
    96.08% 的准确率，而传统的 NLP 方法准确率为 95.24%。
- en: In the next chapter, we will develop models using TensorFlow. We will look at
    TensorBoard, which allows us to visualize and debug complex deep learning models.
    We will also look at using TensorFlow estimators, an alternative option for using TensorFlow.
    Then, we will also look at TensorFlow Runs, which automates a lot of the steps
    for hyperparameter tuning. Finally, we will look at options for deploying deep
    learning models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将使用 TensorFlow 开发模型。我们将了解 TensorBoard，它可以帮助我们可视化和调试复杂的深度学习模型。我们还将学习如何使用
    TensorFlow 估算器，这是使用 TensorFlow 的另一种选择。接着，我们还将学习 TensorFlow Runs，它能自动化许多超参数调优的步骤。最后，我们将探索部署深度学习模型的各种选项。
