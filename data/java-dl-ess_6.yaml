- en: Chapter 6. Approaches to Practical Applications – Recurrent Neural Networks
    and More
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章：实践应用的方法——递归神经网络及其他
- en: In the previous chapters, you learned quite a lot about deep learning. You should
    now understand the fundamentals of the concepts, theories, and implementations
    of deep neural networks. You also learned that you can experiment with deep learning
    algorithms on various data relatively easily by utilizing a deep learning library.
    The next step is to examine how deep learning can be applied to a broad range
    of other fields and how to utilize it for practical applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你已经学到了很多关于深度学习的知识。现在你应该已经理解了深度神经网络的概念、理论和实现的基础知识。你还学到，通过利用深度学习库，你可以相对容易地在各种数据上进行深度学习算法的实验。接下来的步骤是探讨深度学习如何应用于广泛的其他领域，以及如何将其用于实际应用。
- en: Therefore, in this chapter, we'll first see how deep learning is actually applied.
    Here, you will see that the actual cases where deep learning is utilized are still
    very few. But why aren't there many cases even though it is such an innovative
    method? What is the problem? Later on, we'll think about the reasons. Furthermore,
    going forward we will also consider which fields we can apply deep learning to
    and will have the chance to apply deep learning and all the related areas of artificial
    intelligence.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将首先看看深度学习是如何实际应用的。在这里，你会看到，深度学习实际应用的案例仍然非常少。但为什么尽管这是如此创新的方法，应用案例却不多呢？问题出在哪里？稍后，我们将深入思考原因。接下来，我们还将讨论我们可以将深度学习应用到哪些领域，并探讨人工智能相关的所有领域中哪些可以应用深度学习。
- en: 'The topics covered in this chapter include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题包括：
- en: Image recognition, natural language processing, and the neural networks models
    and algorithms related to them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别、自然语言处理以及与它们相关的神经网络模型和算法
- en: The difficulties of turning deep learning models into practical applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度学习模型转化为实际应用的困难
- en: The possible fields where deep learning can be applied, and ideas on how to
    approach these fields
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以应用的可能领域，以及如何接近这些领域的思路
- en: We'll explore the potential of this big AI boom, which will lead to ideas and
    hints that you can utilize in deep learning for your research, business, and many
    sorts of activities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索这个巨大人工智能浪潮的潜力，这将为你在研究、商业和各种活动中使用深度学习提供思路和启示。
- en: Fields where deep learning is active
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习活跃的领域
- en: We often hear that research for deep learning has always been ongoing and that's
    a fact. Many corporations, especially large tech companies such as Google, Facebook,
    Microsoft, and IBM, invest huge amounts of money into the research of deep learning,
    and we frequently hear news that some corporation has bought these research groups.
    However, as we look through, deep learning itself has various types of algorithms,
    and fields where these algorithms can be applied. Even so, it is a fact that is
    it not widely known which fields deep learning is utilized in or can be used in.
    Since the word "AI" is so broadly used, people can't properly understand which
    technology is used for which product. Hence, in this section, we will go through
    the fields where people have been trying to adopt deep learning actively for practical
    applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常听到深度学习的研究一直在进行，这确实是事实。许多公司，特别是像谷歌、Facebook、微软和IBM这样的科技巨头，都在深度学习的研究上投入了巨额资金，我们也时常听到有公司收购了这些研究团队。然而，当我们深入了解时，会发现深度学习本身有各种类型的算法，以及这些算法可以应用的领域。即便如此，深度学习在哪些领域已经被应用或可以被应用，仍然是一个不为大众所熟知的事实。由于“人工智能”这个词被广泛使用，人们无法准确理解哪些技术用于哪些产品。因此，在本节中，我们将探讨人们在实际应用中积极尝试采用深度学习的领域。
- en: Image recognition
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像识别
- en: The field in which deep learning is most frequently incorporated is image recognition.
    It was Prof. Hinton and his team's invention that led to the term "deep learning."
    Their algorithm recorded the lowest error rates ever in an image recognition competition.
    The continuous research done to improve the algorithm led to even better results.
    Now, image recognition utilizing deep learning has gradually been adopted not
    only for studies, but also for practical applications and products. For example,
    Google utilizes deep learning to auto-generate thumbnails for YouTube or auto-tag
    and search photos in Google Photos. Like these popular products, deep learning
    is mainly applied to image tagging or categorizing and, for example, in the field
    of robotics, it is used for robots to specify things around them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最常应用的领域是图像识别。正是Hinton教授及其团队的发明提出了“深度学习”这一术语。他们的算法在一次图像识别竞赛中创下了历史最低的错误率。为改进算法所做的持续研究带来了更好的成果。如今，利用深度学习的图像识别技术已经逐渐被广泛应用于研究和实际产品中。例如，Google利用深度学习自动生成YouTube的缩略图，或在Google
    Photos中自动标记和搜索照片。像这些流行的产品一样，深度学习主要应用于图像标记或分类。例如，在机器人领域，深度学习被用来帮助机器人识别周围的物体。
- en: The reason why we can support these products and this industry is because deep
    learning is more suited to image processing, and this is because it can achieve
    higher precision rates than applications in any other field. Only because the
    precision and recall rate of image recognition is so high does it mean this industry
    has broad potential. An error rate of MNIST image classification is recorded at
    0.21 percent with a deep learning algorithm ([http://cs.nyu.edu/~wanli/dropc/](http://cs.nyu.edu/~wanli/dropc/)),
    and this rate can be no lower than the record for a human ([http://arxiv.org/pdf/0710.2231v1.pdf](http://arxiv.org/pdf/0710.2231v1.pdf)).
    In other words, if you narrow it down to just image recognition, it's nothing
    more than the fact that a machine may overcome a human. Why does only image recognition
    get such high precision while other fields need far more improvement in their
    methods?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够支持这些产品和这个行业的原因在于，深度学习更适合图像处理，这也正是它能够实现比其他领域应用更高精度的原因。只有图像识别的精度和召回率如此之高，才能说明这个行业具有广阔的潜力。使用深度学习算法进行MNIST图像分类的错误率为0.21%（[http://cs.nyu.edu/~wanli/dropc/](http://cs.nyu.edu/~wanli/dropc/)），这一比率已经接近人类的记录（[http://arxiv.org/pdf/0710.2231v1.pdf](http://arxiv.org/pdf/0710.2231v1.pdf)）。换句话说，如果我们仅仅讨论图像识别领域，那么它不过是机器可能超越人类的事实。那么，为什么只有图像识别能取得如此高的精度，而其他领域的技术方法还需要大量改进呢？
- en: One of the reasons is that the structure of feature extractions in deep learning
    is well suited for image data. In deep neural networks, many layers are stacked
    and features are extracted from training data step by step at each layer. Also,
    it can be said that image data is featured as a layered structure. When you look
    at images, you will unconsciously catch brief features first and then look into
    a more detailed feature. Therefore, the inherent property of deep learning feature
    extraction is similar to how an image is perceived and hence we can get an accurate
    realization of the features. Although image recognition with deep learning still
    needs more improvements, especially of how machines can understand images and
    their contents, obtaining high precision by just adopting deep learning to sample
    image data without preprocessing obviously means that deep learning and image
    data are a good match.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个原因是深度学习中特征提取的结构非常适合处理图像数据。在深度神经网络中，许多层次是逐层堆叠的，每一层都从训练数据中提取特征。此外，图像数据本身也可以看作是一个分层结构。当你查看图像时，你会不自觉地首先捕捉到简洁的特征，然后再深入观察更详细的特征。因此，深度学习特征提取的固有特性与我们感知图像的方式非常相似，从而使我们能够准确地识别图像的特征。尽管基于深度学习的图像识别仍然需要进一步改进，特别是在机器如何理解图像及其内容方面，但仅仅通过采用深度学习对图像数据进行采样而不进行预处理，能够获得高精度，这显然表明深度学习与图像数据是非常匹配的。
- en: The other reason is that people have been working to improve algorithms slowly
    but steadily. For example, in deep learning algorithms, CNN, which can get the
    best precision for image recognition, has been improved every time it faces difficulties/tasks.
    Local receptive fields substituted with kernels of convolutional layers were introduced
    to avoid networks becoming too dense. Also, downsampling methods such as max-pooling
    were invented to avoid the overreaction of networks towards a gap of image location.
    This was originally generated from a trial and error process on how to recognize
    handwritten letters written in a certain frame such as a postal code. As such,
    there are many cases where a new approach is sought to adapt neural networks algorithms
    for practical applications. A complicated model, CNN is also built based on these
    accumulated yet steady improvements. While we don't need feature engineering with
    deep learning, we still need to consider an appropriate approach to solve specific
    problems, that is, we can't build omnipotent models, and this is known as the
    **No Free Lunch Theorem** (**NFLT**) for optimization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是人们一直在缓慢但稳定地改进算法。例如，在深度学习算法中，卷积神经网络（CNN）在图像识别中能够获得最佳精度，并且每次遇到困难或任务时都会有所改进。卷积层中的局部感受野被替代为卷积核，以避免网络变得过于密集。此外，为了避免网络对图像位置差距过度反应，还发明了如最大池化（max-pooling）等下采样方法。这些方法最初是通过对如何识别在某个框架中书写的手写字母（如邮政编码）进行试验和错误的过程产生的。因此，许多新的方法被寻求来使神经网络算法适应实际应用。像卷积神经网络（CNN）这样复杂的模型，也是基于这些积累而来且稳定的改进。虽然深度学习不需要特征工程，但我们仍需考虑解决特定问题的合适方法，也就是说，我们无法构建全能的模型，这就是优化领域的**无免费午餐定理**（**NFLT**）。
- en: In the image recognition field, the classification accuracy that can be achieved
    by deep learning is extremely high, and it is actually beginning to be used for
    practical applications. However, there should be more fields where deep learning
    can be applied. Images have a close connection to many industries. In the future,
    there will be many cases and many more industries that utilize deep learning.
    In this book, let's think about what industries we can apply image recognition
    to, considering the emergence of deep learning in the next sections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像识别领域，深度学习能够达到的分类精度极高，实际上，它已经开始用于实际应用。然而，仍然有许多领域可以应用深度学习。图像与许多行业密切相关。未来，将会有更多的应用案例和更多行业将深度学习加以利用。在本书的接下来的章节中，让我们思考如何将图像识别应用于哪些行业。
- en: Natural language processing
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: The second most active field, after image recognition, where the research of
    deep learning has progressed is **natural language processing** (**NLP**). The
    research in this field might become the most active going forward. With regard
    to image recognition, the prediction precision we could obtain almost reaches
    the ceiling, as it can perform even better classification than a human could.
    On the other hand, in NLP, it is true that the performance of a model gets a lot
    better thanks to deep learning, but it is also a fact that there are many tasks
    that still need to be solved.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像识别之后，深度学习研究进展最为显著的领域是**自然语言处理**（**NLP**）。该领域的研究有可能成为未来最活跃的研究方向。关于图像识别，深度学习所能达到的预测精度几乎已经接近极限，因为它能够执行比人类更好的分类。另一方面，虽然深度学习大大提升了模型的性能，但在自然语言处理领域，依然有许多任务需要解决。
- en: For some products and practical applications, deep learning has already been
    applied. For example, NLP based on deep learning is applied to Google's voice
    search or voice recognition and Google translation. Also, IBM Watson, the cognitive
    computing system that understands and learns natural language and supports human
    decision-making, extracts keywords and entities from tons of documents, and has
    functions to label documents. And these functions are open to the public as the
    Watson API and anyone can utilize it without constraints.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些产品和实际应用，深度学习已经得到了应用。例如，基于深度学习的自然语言处理（NLP）被应用于谷歌的语音搜索、语音识别和谷歌翻译。此外，IBM Watson
    是一个认知计算系统，它能够理解和学习自然语言，并支持人类决策。它能够从大量文档中提取关键词和实体，并具有标注文档的功能。这些功能作为 Watson API
    向公众开放，任何人都可以在不受限制的情况下利用它。
- en: As you can see from the preceding examples, NLP itself has a broad and varied
    range of types. In terms of fundamental techniques, we have the classification
    of sentence contents, the classification of words, and the specification of word
    meanings. Furthermore, languages such as Chinese or Japanese that don't leave
    a space between words require morphological analysis, which is also another technique
    available in NLP.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如从前面的示例中可以看到的，NLP 本身有广泛且多样的类型。在基本技术方面，我们有句子内容分类、单词分类和单词意义的指定。此外，像中文或日文这类不在单词之间留空格的语言需要形态学分析，这也是
    NLP 中的另一项技术。
- en: NLP contains a lot of things that need to be researched, therefore it needs
    to clarify what its purpose is, what its problems are, and how these problems
    can be solved. What model is the best to use and how to get good precision properly
    are topics that should be examined cautiously. As for image recognition, the CNN
    method was invented by solving tasks that were faced. Now, let's consider what
    approach we can think of and what the difficulties will be respectively for neural
    networks and NLP. Understanding past trial and error processes will be useful
    for research and applications going forward.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 涉及许多需要研究的内容，因此它需要明确其目标是什么，问题是什么，以及如何解决这些问题。使用哪种模型最好，如何有效地获得高精度，是应该谨慎探讨的话题。至于图像识别，CNN
    方法是通过解决面临的任务而发明的。现在，让我们考虑在神经网络和 NLP 的应用中分别可以想到什么方法，以及可能遇到的困难。理解过去的试验和错误过程对未来的研究和应用将非常有帮助。
- en: Feed-forward neural networks for NLP
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于 NLP 的前馈神经网络
- en: 'The fundamental problem of NLP is "to predict the next word given a specific
    word or words". The problem is too simple, however; if you try to solve it with
    neural networks, then you will soon face several difficulties because documents
    or sentences as sample data using NLP have the following features:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 的基本问题是“给定一个或多个特定的单词，预测下一个单词”。然而，这个问题过于简单；如果你试图用神经网络来解决它，你很快会面临几个困难，因为作为
    NLP 样本数据的文档或句子具有以下特征：
- en: The length of each sentence is not fixed but variable, and the number of words
    is astronomical
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个句子的长度不固定而是可变的，单词的数量非常庞大
- en: There can be unforeseen problems such as misspelled words, acronyms, and so
    on
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会遇到一些不可预见的问题，比如拼写错误、缩写等
- en: Sentences are sequential data, and so contain temporal information
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子是顺序数据，因此包含时间信息
- en: Why can these features pose a problem? Remember the model structure of general
    neural networks. For training and testing with neural networks, the number of
    neurons in each layer including the input layer needs to be fixed in advance and
    the networks need to be the same size for all the sample data. In the meantime,
    the length of the input data is not fixed and can vary a lot. This means that
    sample data cannot be applied to the model, at least as it is. Classification
    or generation by neural networks cannot be done without adding/amending something
    to this data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这些特征会造成问题呢？记住一般神经网络的模型结构。对于神经网络的训练和测试，每一层的神经元数量（包括输入层）需要预先固定，并且所有样本数据的网络大小需要一致。与此同时，输入数据的长度并不固定，可能会有很大变化。这意味着样本数据不能直接应用于模型，至少不能原样应用。没有对这些数据进行添加/修改，神经网络无法进行分类或生成。
- en: We have to fix the length of input data, and one approach to handle this issue
    is a method that divides a sentence into a chunk of certain words from the beginning
    in order. This method is called **N-gram**. Here, *N* represents the size of each
    item, and an **N-gram** of size 1 is called a **unigram**, size 2 is a **bigram**,
    and size 3 is a **trigram**. When the size is larger, then it is simply called
    with the value of *N*, such as *four-gram*, *five-gram*, and so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须固定输入数据的长度，其中一种处理方法是将句子从头开始按顺序分成若干个固定长度的词块。这种方法称为**N-gram**。这里，*N*表示每个项的大小，大小为1的**N-gram**叫做**unigram**，大小为2的是**bigram**，大小为3的是**trigram**。当大小更大时，通常会根据*N*的值来命名，比如*four-gram*、*five-gram*，依此类推。
- en: 'Let''s look at how N-gram works with NLP. The goal here is to calculate the
    probability of a word ![Feed-forward neural networks for NLP](img/B04779_06_14.jpg)
    given some history ![Feed-forward neural networks for NLP](img/B04779_06_15.jpg);![Feed-forward
    neural networks for NLP](img/B04779_06_16.jpg). We''ll represent a sequence of
    ![Feed-forward neural networks for NLP](img/B04779_06_17.jpg) words as ![Feed-forward
    neural networks for NLP](img/B04779_06_18.jpg). Then, the probability we want
    to compute is ![Feed-forward neural networks for NLP](img/B04779_06_19.jpg), and
    by applying the chain rule of probability to this term, we get:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看N-gram是如何在NLP中工作的。这里的目标是计算在给定一些历史信息的情况下，某个单词的概率 ![Feed-forward neural networks
    for NLP](img/B04779_06_14.jpg)，这些历史信息包括 ![Feed-forward neural networks for NLP](img/B04779_06_15.jpg);![Feed-forward
    neural networks for NLP](img/B04779_06_16.jpg)。我们将一串 ![Feed-forward neural networks
    for NLP](img/B04779_06_17.jpg) 单词表示为 ![Feed-forward neural networks for NLP](img/B04779_06_18.jpg)。然后，我们想要计算的概率是
    ![Feed-forward neural networks for NLP](img/B04779_06_19.jpg)，通过应用概率链式法则，我们可以得到：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_20.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Feed-forward neural networks for NLP](img/B04779_06_20.jpg)'
- en: 'It might look at first glance like these conditional probabilities help us,
    but actually they don''t because we have no way of calculating the exact probability
    of a word following a long sequence of preceding words, ![Feed-forward neural
    networks for NLP](img/B04779_06_21.jpg). Since the structure of a sentence is
    very flexible, we can''t simply utilize sample documents and a corpus to estimate
    the probability. This is where N-gram works. Actually, we have two approaches
    to solve this problem: the original N-gram model and the neural networks model
    based on N-gram. We''ll look at the first one to fully understand how the fields
    of NLP have developed before we dig into neural networks.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 起初看起来这些条件概率可能对我们有帮助，但实际上并没有，因为我们没有办法计算一个单词在一长串前置单词之后的精确概率，![Feed-forward neural
    networks for NLP](img/B04779_06_21.jpg)。由于句子的结构非常灵活，我们不能简单地利用样本文档和语料库来估计这个概率。这就是N-gram的作用所在。实际上，我们有两种方法来解决这个问题：原始的N-gram模型和基于N-gram的神经网络模型。在深入研究神经网络之前，我们将先了解第一个模型，全面理解NLP领域的发展。
- en: 'With N-gram, we don''t compute the probability of a word given its whole history,
    but approximate the history with the last *N* words. For example, the bigram model
    approximates the probability of a word just by the conditional probability of
    the preceding word, ![Feed-forward neural networks for NLP](img/B04779_06_21.jpg),
    and so follows the equation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用N-gram时，我们不是计算一个单词在其完整历史下的概率，而是通过最后的*N*个单词来近似历史。例如，二元模型仅通过前一个单词的条件概率来近似一个单词的概率，![Feed-forward
    neural networks for NLP](img/B04779_06_21.jpg)，从而得到如下方程：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_22.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Feed-forward neural networks for NLP](img/B04779_06_22.jpg)'
- en: 'Similarly, we can generalize and expand the equation for N-gram. In this case,
    the probability of a word can be represented as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以将N-gram的方程进行推广和扩展。在这种情况下，单词的概率可以表示如下：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_23.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Feed-forward neural networks for NLP](img/B04779_06_23.jpg)'
- en: 'We get the following equation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下方程：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_24.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Feed-forward neural networks for NLP](img/B04779_06_24.jpg)'
- en: Just bear in mind that these approximations with N-gram are based on the probabilistic
    model called the **Markov model**, where the probability of a word depends only
    on the previous word.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些基于N-gram的近似值是基于一种叫做**马尔可夫模型**的概率模型，在这种模型中，一个单词的概率仅依赖于前一个单词。
- en: 'Now what we need to do is estimate these N-gram probabilities, but how do we
    estimate them? One simple way of doing this is called the **maximum likelihood
    estimation** (**MLE**). This method estimates the probabilities by taking counts
    from a corpus and normalizing them. So when we think of a bigram as an example,
    we get:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的是估计这些N-gram的概率，那么我们该如何估计它们呢？一种简单的方式叫做**最大似然估计**（**MLE**）。这种方法通过从语料库中获取词频并进行归一化来估计概率。所以，当我们以二元组为例时，我们可以得到：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_25.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Feed-forward neural networks for NLP](img/B04779_06_25.jpg)'
- en: 'In the preceding formula, ![Feed-forward neural networks for NLP](img/B04779_06_26.jpg)
    denotes the counts of a word or a sequence of words. Since the denominator, that
    is, the sum of all bigram counts starting with a word, ![Feed-forward neural networks
    for NLP](img/B04779_06_27.jpg) is equal to the unigram count of ![Feed-forward
    neural networks for NLP](img/B04779_06_27.jpg), the preceding equation can be
    described as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中， ![前馈神经网络用于自然语言处理](img/B04779_06_26.jpg) 表示一个单词或单词序列的计数。由于分母，即以某个单词开头的所有二元组计数之和，
    ![前馈神经网络用于自然语言处理](img/B04779_06_27.jpg) 等于该单词的单元语法计数 ![前馈神经网络用于自然语言处理](img/B04779_06_27.jpg)，因此前面的方程可以描述如下：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_28.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络用于自然语言处理](img/B04779_06_28.jpg)'
- en: 'Accordingly, we can generalize MLE for N-gram as well:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们也可以将最大似然估计（MLE）推广到N-gram。
- en: '![Feed-forward neural networks for NLP](img/B04779_06_29.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络用于自然语言处理](img/B04779_06_29.jpg)'
- en: Although this is a fundamental approach of NLP with N-gram, we now know how
    to compute N-gram probabilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是NLP中基于N-gram的基本方法，但我们现在知道如何计算N-gram的概率。
- en: 'In contrast to this approach, the neural network models predict the conditional
    probability of a word ![Feed-forward neural networks for NLP](img/B04779_06_30.jpg)
    given a specific history, ![Feed-forward neural networks for NLP](img/B04779_06_31.jpg);
    ![Feed-forward neural networks for NLP](img/B04779_06_32.jpg). One of the models
    of NLP is called the **Neural ** **Network Language Model** (**NLMM**) ([http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)),
    and it can be illustrated as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种方法不同，神经网络模型预测在给定特定历史的条件下，单词 ![前馈神经网络用于自然语言处理](img/B04779_06_30.jpg) 的条件概率，
    ![前馈神经网络用于自然语言处理](img/B04779_06_31.jpg)； ![前馈神经网络用于自然语言处理](img/B04779_06_32.jpg)。一种NLP模型被称为**神经网络语言模型**
    (**NLMM**)，([http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf))，其可以表示为如下：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络用于自然语言处理](img/B04779_06_01.jpg)'
- en: 'Here, ![Feed-forward neural networks for NLP](img/B04779_06_17.jpg) is the
    size of the vocabulary, and each word in the vocabulary is an N-dimensional vector
    where only the index of the word is set to 1 and all the other indices to 0\.
    This method of representation is called *1-of-N coding*. The inputs of NLMM are
    the indices of the ![Feed-forward neural networks for NLP](img/B04779_06_33.jpg)
    previous words ![Feed-forward neural networks for NLP](img/B04779_06_34.jpg) (so
    they are *n-grams*). Since the size *N* is typically within the range of 5,000
    to 200,000, input vectors of NLMM are very sparse. Then, each word is mapped to
    the projection layer, for continuous space representation. This linear projection
    (activation) from a discrete to a continuous space is basically a look-up table
    with ![Feed-forward neural networks for NLP](img/B04779_06_35.jpg) entries, where
    ![Feed-forward neural networks for NLP](img/B04779_06_36.jpg) denotes the feature
    dimension. The projection matrix is shared for the different word positions in
    the context, and activates the word vectors to projection layer units ![Feed-forward
    neural networks for NLP](img/B04779_06_37.jpg) with ![Feed-forward neural networks
    for NLP](img/B04779_06_38.jpg). After the projection comes the hidden layer. Since
    the projection layer is in the continuous space, the model structure is just the
    same as the other neural networks from here. So, the activation can be represented
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![前馈神经网络用于自然语言处理](img/B04779_06_17.jpg) 是词汇表的大小，词汇表中的每个单词都是一个N维向量，其中只有该单词的索引设置为1，其它索引都为0。这种表示方法叫做
    *1-of-N 编码*。NLMM 的输入是前一个单词的索引，即 ![前馈神经网络用于自然语言处理](img/B04779_06_33.jpg) 和 ![前馈神经网络用于自然语言处理](img/B04779_06_34.jpg)（因此它们是
    *n-gram*）。由于大小 *N* 通常在5,000到200,000之间，NLMM 的输入向量非常稀疏。然后，每个单词被映射到投影层，进行连续空间表示。这个从离散空间到连续空间的线性投影（激活）基本上是一个查找表，包含
    ![前馈神经网络用于自然语言处理](img/B04779_06_35.jpg) 项，其中 ![前馈神经网络用于自然语言处理](img/B04779_06_36.jpg)
    表示特征维度。投影矩阵对于上下文中不同的单词位置是共享的，并激活单词向量到投影层单位 ![前馈神经网络用于自然语言处理](img/B04779_06_37.jpg)，其维度为
    ![前馈神经网络用于自然语言处理](img/B04779_06_38.jpg)。投影之后是隐藏层。由于投影层位于连续空间中，模型结构从此处起与其他神经网络相同。因此，激活可以表示为如下形式：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_39.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络用于自然语言处理](img/B04779_06_39.jpg)'
- en: 'Here, ![Feed-forward neural networks for NLP](img/B04779_06_40.jpg) denotes
    the activation function, ![Feed-forward neural networks for NLP](img/B04779_06_41.jpg)
    the weights between the projection layer and the hidden layer, and ![Feed-forward
    neural networks for NLP](img/B04779_06_42.jpg) the biases of the hidden layer.
    Accordingly, we can get the output units as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![用于NLP的前馈神经网络](img/B04779_06_40.jpg) 表示激活函数，![用于NLP的前馈神经网络](img/B04779_06_41.jpg)
    表示投影层和隐藏层之间的权重，![用于NLP的前馈神经网络](img/B04779_06_42.jpg) 表示隐藏层的偏置。因此，我们可以得到如下的输出单元：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_43.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![用于NLP的前馈神经网络](img/B04779_06_43.jpg)'
- en: 'Here, ![Feed-forward neural networks for NLP](img/B04779_06_44.jpg) denotes
    the weights between the hidden layer and the output layer, and ![Feed-forward
    neural networks for NLP](img/B04779_06_45.jpg) denotes the biases of the output
    layer. The probability of a word *i* given a specific history ![Feed-forward neural
    networks for NLP](img/B04779_06_31.jpg) can then be calculated using the softmax
    function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![用于NLP的前馈神经网络](img/B04779_06_44.jpg) 表示隐藏层和输出层之间的权重，![用于NLP的前馈神经网络](img/B04779_06_45.jpg)
    表示输出层的偏置。给定特定历史信息的词语 *i* 的概率 ![用于NLP的前馈神经网络](img/B04779_06_31.jpg) 可以通过 softmax
    函数计算得出：
- en: '![Feed-forward neural networks for NLP](img/B04779_06_46.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![用于NLP的前馈神经网络](img/B04779_06_46.jpg)'
- en: As you can see, in NNLM, the model predicts the probability of all the words
    at the same time. Since the model is now described with the standard neural network,
    we can train the model using the standard backpropagation algorithm.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在 NNLM 中，模型同时预测所有词汇的概率。由于该模型现在使用标准神经网络描述，我们可以使用标准的反向传播算法来训练模型。
- en: NNLM is one approach of NLP using neural networks with N-gram. Though NNLM solves
    the problem of how to fix the number of inputs, the best *N* can only be found
    by trial and error, and it is the most difficult part of the whole model building
    process. In addition, we have to make sure that we don't put too much weight on
    the temporal information of the inputs here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: NNLM 是一种使用神经网络和 N-gram 的 NLP 方法。虽然 NNLM 解决了如何固定输入数量的问题，但最佳的 *N* 只能通过反复试验找到，这是整个模型构建过程中最困难的部分。此外，我们必须确保不要过多依赖输入的时间序列信息。
- en: Deep learning for NLP
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习用于NLP
- en: Neural networks with N-gram may work with certain cases, but contain some issues,
    such as what n-grams would return the best results, and do n-grams, the inputs
    of the model, still have a context? These are the problems not only of NLP, but
    of all the other fields that have time sequential data such as precipitation,
    stock prices, yearly crop of potatoes, movies, and so on. Since we have such a
    massive amount of this data in the real world, we can't ignore the potential issue.
    But then, how would it be possible to let neural networks be trained with time
    sequential data?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 N-gram 的神经网络在某些情况下可能有效，但也存在一些问题，例如，哪些 n-grams 会返回最佳结果，以及 n-grams（即模型的输入）是否仍然具有上下文？这些问题不仅存在于
    NLP 中，也存在于所有具有时间序列数据的领域，如降水、股票价格、每年土豆的产量、电影等。由于在现实世界中我们有大量这样的数据，因此不能忽视潜在的问题。那么，如何才能让神经网络在时间序列数据上进行训练呢？
- en: Recurrent neural networks
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'One of the neural network models that is able to preserve the context of data
    within networks is **recurrent neural** **network** (**RNN**), the model that
    actively studies the evolution of deep learning algorithms. The following is a
    very simple graphical model of RNN:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一种能够在网络中保留数据上下文的神经网络模型是 **循环神经网络** (**RNN**)，这一模型积极研究深度学习算法的演变。以下是一个非常简单的 RNN
    图形模型：
- en: '![Recurrent neural networks](img/B04779_06_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_02.jpg)'
- en: 'The difference between standard neural networks is that RNN has connections
    between hidden layers with respect to time. The input at time ![Recurrent neural
    networks](img/B04779_06_47.jpg) is activated in the hidden layer at time ![Recurrent
    neural networks](img/B04779_06_47.jpg), preserved in the hidden layer, and then
    propagated to the hidden layer at time ![Recurrent neural networks](img/B04779_06_48.jpg)
    with the input at time ![Recurrent neural networks](img/B04779_06_48.jpg). This
    enables the networks to contain the states of past data and reflect them. You
    might think that RNN is rather a dynamic model, but if you unfold the model at
    each time step, you can see that RNN is a static model:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 标准神经网络与RNN的区别在于，RNN在时间上存在隐藏层之间的连接。时间点![Recurrent neural networks](img/B04779_06_47.jpg)的输入会在时间点![Recurrent
    neural networks](img/B04779_06_47.jpg)的隐藏层中激活，并被保留在该隐藏层中，随后与时间点![Recurrent neural
    networks](img/B04779_06_48.jpg)的输入一起传播到时间点![Recurrent neural networks](img/B04779_06_48.jpg)的隐藏层中。这使得网络能够包含过去数据的状态并反映它们。你可能会认为RNN是一个动态模型，但如果你在每个时间步展开该模型，你会看到RNN其实是一个静态模型：
- en: '![Recurrent neural networks](img/B04779_06_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Recurrent neural networks](img/B04779_06_03.jpg)'
- en: 'Since the model structure at each time step is the same as in general neural
    networks, you can train this model using the backpropagation algorithm. However,
    you need to consider time relevance when training, and there is a technique called
    **Backpropagation through Time** (**BPTT**) to handle this. In BPTT, the errors
    and gradients of the parameter are backpropagated to the layers of the past:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个时间步的模型结构与一般神经网络相同，因此你可以使用反向传播算法训练该模型。然而，在训练时需要考虑时间相关性，且有一种称为**时间反向传播**（**BPTT**）的技术来处理这一点。在BPTT中，参数的误差和梯度会被反向传播到过去的各层：
- en: '![Recurrent neural networks](img/B04779_06_04.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Recurrent neural networks](img/B04779_06_04.jpg)'
- en: Thus, RNN can preserve contexts within the model. Theoretically, the network
    at each time step should consider the whole sequence up to then, but practically,
    time windows with a certain length are often applied to the model to make the
    calculation less complicated or to prevent the vanishing gradient problem and
    the exploding gradient problem. BPTT has enabled training among layers and this
    is why RNN is often considered to be one of the deep neural networks. We also
    have algorithms of deep RNN such as stacked RNN where hidden layers are stacked.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RNN能够在模型中保留上下文。理论上，网络在每个时间步应该考虑到直到该时刻为止的整个序列，但实际上，通常会对模型应用具有一定长度的时间窗口，以简化计算或避免消失梯度问题和梯度爆炸问题。BPTT使得层间的训练成为可能，这也是为什么RNN常被认为是深度神经网络之一的原因。我们还有像堆叠RNN这样的深度RNN算法，其中隐藏层是堆叠的。
- en: 'RNN has been adapted for NLP, and is actually one of the most successful models
    in this field. The original model optimized for NLP is called the **recurrent
    neural network language model** (**RNNLM**), introduced by Mikolov et al. ([http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)).
    The model architecture can be illustrated as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: RNN已被应用于自然语言处理（NLP），实际上是该领域最成功的模型之一。最初优化用于NLP的模型被称为**递归神经网络语言模型**（**RNNLM**），由Mikolov等人提出（[http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)）。该模型的架构可以如下所示：
- en: '![Recurrent neural networks](img/B04779_06_05.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Recurrent neural networks](img/B04779_06_05.jpg)'
- en: 'The network has three layers: an input layer ![Recurrent neural networks](img/B04779_06_49.jpg),
    a hidden layer ![Recurrent neural networks](img/B04779_06_50.jpg), and an output
    layer ![Recurrent neural networks](img/B04779_06_51.jpg). The hidden layer is
    also often called the context layer or the state layer. The value of each layer
    with respect to the time ![Recurrent neural networks](img/B04779_06_47.jpg) can
    be represented as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络有三层：输入层![Recurrent neural networks](img/B04779_06_49.jpg)、隐藏层![Recurrent
    neural networks](img/B04779_06_50.jpg)和输出层![Recurrent neural networks](img/B04779_06_51.jpg)。隐藏层也常被称为上下文层或状态层。每一层相对于时间点![Recurrent
    neural networks](img/B04779_06_47.jpg)的值可以表示如下：
- en: '![Recurrent neural networks](img/B04779_06_52.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Recurrent neural networks](img/B04779_06_52.jpg)'
- en: Here, ![Recurrent neural networks](img/B04779_06_53.jpg) denotes the sigmoid
    function, and ![Recurrent neural networks](img/B04779_06_54.jpg) the softmax function.
    Since the input layer contains the state layer at time ![Recurrent neural networks](img/B04779_06_55.jpg),
    it can reflect the whole context to the network. The model architecture implies
    that RNNLM can look up much broader contexts than feed-forward NNLM, in which
    the length of the context is constrained to *N* (-gram).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![循环神经网络](img/B04779_06_53.jpg) 表示sigmoid函数，![循环神经网络](img/B04779_06_54.jpg)
    表示softmax函数。由于输入层包含了在时间 ![循环神经网络](img/B04779_06_55.jpg) 的状态层，它能够将整个上下文传递给网络。模型架构表明，RNNLM能够查找比前馈NNLM更广泛的上下文，在后者中，上下文的长度被限制为
    *N*（-gram）。
- en: 'The whole time and the entire context should be considered while training RNN,
    but as mentioned previously, we often truncate the time length because BPTT requires
    a lot of calculations and often causes the gradient vanishing/exploding problem
    when learning long-term dependencies, hence the algorithm is often called **truncated
    BPTT**. If we unfold RNNLM with respect to time, the model can be illustrated
    as follows (in the figure, the unfolded time ![Recurrent neural networks](img/B04779_06_56.jpg)):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练RNN时，应该考虑整个时间和上下文，但如前所述，我们通常会截断时间长度，因为BPTT需要大量计算，并且在学习长期依赖时常常导致梯度消失/爆炸问题，因此该算法通常被称为**截断BPTT**。如果我们按照时间展开RNNLM，模型可以如下所示（在图中，展开的时间
    ![循环神经网络](img/B04779_06_56.jpg)）：
- en: '![Recurrent neural networks](img/B04779_06_13.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_13.jpg)'
- en: 'Here ![Recurrent neural networks](img/B04779_06_57.jpg) is the label vector
    of the output. Then, the error vector of the output can be represented as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![循环神经网络](img/B04779_06_57.jpg) 是输出的标签向量。然后，输出的误差向量可以表示如下：
- en: '![Recurrent neural networks](img/B04779_06_58.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_58.jpg)'
- en: 'We get the following equation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下方程：
- en: '![Recurrent neural networks](img/B04779_06_59.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_59.jpg)'
- en: 'Here ![Recurrent neural networks](img/B04779_06_60.jpg) is the unfolding time:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![循环神经网络](img/B04779_06_60.jpg) 是展开的时间：
- en: '![Recurrent neural networks](img/B04779_06_61.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_61.jpg)'
- en: 'The preceding image is the derivative of the activation function of the hidden
    layer. Since we use the sigmoid function here, we get the preceding equation.
    Then, we can get the error of the past as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图片是隐藏层激活函数的导数。由于我们在这里使用的是sigmoid函数，因此得到了前面的方程。然后，我们可以得到过去的误差，如下所示：
- en: '![Recurrent neural networks](img/B04779_06_62.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_62.jpg)'
- en: 'With these equations, we can now update the weight matrices of the model:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些方程式，我们现在可以更新模型的权重矩阵：
- en: '![Recurrent neural networks](img/B04779_06_63.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](img/B04779_06_63.jpg)'
- en: Here, ![Recurrent neural networks](img/B04779_06_64.jpg) is the learning rate.
    What is interesting in RNNLM is that each vector in the matrix shows the difference
    between words after training. This is because ![Recurrent neural networks](img/B04779_06_65.jpg)
    is the matrix that maps each word to a latent space, so after the training, mapped
    word vectors contain the meaning of the words. For example, the vector calculation
    of "king" – "man" + "woman" would return "queen". DL4J supports RNN, so you can
    easily implement this model with the library.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![循环神经网络](img/B04779_06_64.jpg) 是学习率。在RNNLM中有趣的是，矩阵中的每个向量显示了训练后单词之间的差异。这是因为
    ![循环神经网络](img/B04779_06_65.jpg) 是将每个单词映射到潜在空间的矩阵，因此训练后，映射的单词向量包含了单词的含义。例如，“king”
    - “man” + “woman”的向量计算将返回“queen”。DL4J支持RNN，因此你可以轻松地使用该库实现这个模型。
- en: Long short term memory networks
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: Training with the standard RNN requires the truncated BPTT. You might well doubt
    then that BPTT can really train the model enough to reflect the whole context,
    and this is very true. This is why a special kind of RNN, the **long short term
    memory** (**LSTM**) network, was introduced to solve the long-term dependency
    problem. LSTM is rather intimidating, but let's briefly explore the concept of
    LSTM.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准的RNN进行训练需要截断的BPTT。你可能会怀疑BPTT是否真的能够足够训练模型，以反映整个上下文，而这一点是非常真实的。这就是为什么引入了一种特殊的RNN——**长短期记忆**（**LSTM**）网络来解决长期依赖问题。LSTM看起来相当复杂，但让我们简要探讨一下LSTM的概念。
- en: 'To begin with, we have to think about how we can store and tell past information
    in the network. While the gradient exploding problem can be mitigated simply by
    setting a ceiling to the connection, the gradient vanishing problem still needs
    to be deeply considered. One possible approach is to introduce a unit that permanently
    preserves the value of its inputs and its gradient. So, when you look at a unit
    in the hidden layer of standard neural networks, it is simply described as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须考虑如何在网络中存储和传递过去的信息。虽然通过给连接设置上限可以简单地缓解梯度爆炸问题，但梯度消失问题仍需要深入考虑。一种可能的方法是引入一个单元，它永久地保存其输入和梯度的值。因此，当你查看标准神经网络中的隐藏层单元时，它可以简单地描述如下：
- en: '![Long short term memory networks](img/B04779_06_06.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆网络](img/B04779_06_06.jpg)'
- en: 'There''s nothing special here. Then, by adding a unit below to the network,
    the network can now memorize the past information within the neuron. The neuron
    added here has linear activation and its value is often set to 1\. This neuron,
    or cell, is called **constant error carousel** (**CEC**) because the error stays
    in the neuron like a carousel and won''t vanish. CEC works as a storage cell and
    stores past inputs. This solves the gradient vanishing problem, but raises another
    problem. Since all data propagated through is stocked in the neuron, it probably
    stores noise data as well:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么特别的。然后，通过向网络添加一个单元，网络现在可以在神经元中记住过去的信息。这里添加的神经元具有线性激活，并且其值通常设置为1。这个神经元，或单元，被称为**常数误差旋转器**（**CEC**），因为误差在神经元中像旋转木马上循环，不会消失。CEC作为存储单元，存储过去的输入。它解决了梯度消失问题，但也引发了另一个问题。由于所有传播过的数据都存储在神经元中，它可能也会存储噪声数据：
- en: '![Long short term memory networks](img/B04779_06_07.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆网络](img/B04779_06_07.jpg)'
- en: 'This problem can be broken down into two problems: *input weight conflicts*
    and *output weight conflicts*. The key idea of input weight conflicts is to keep
    certain information within the network until it''s necessary; the neuron is to
    be activated only when the relevant information comes, but is not to be activated
    otherwise. Similarly, output weight conflicts can occur in all types of neural
    networks; the value of neurons is to be propagated only when necessary, and not
    to be propagated otherwise. We can''t solve these problems as long as the connection
    between neurons is represented with the weight of the network. Therefore, another
    method or technique of representation is required that controls the propagation
    of inputs and outputs. But how do we do this? The answer is putting units that
    act like "gates" before and behind the CEC, and these are called **input gate**
    and **output gate**, respectively. The graphical model of the gate can be described
    as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以分解为两个问题：*输入权重冲突*和*输出权重冲突*。输入权重冲突的关键思想是，保持网络中的某些信息，直到它变得必要；只有当相关信息到来时，神经元才被激活，否则不激活。类似地，输出权重冲突可以出现在所有类型的神经网络中；神经元的值只有在必要时才传播，否则不传播。只要神经元之间的连接通过网络的权重来表示，我们就无法解决这些问题。因此，必须采用另一种表示方法或技术来控制输入和输出的传播。那么我们该如何做到这一点呢？答案是，在CEC前后放置像“门”一样的单元，分别称为**输入门**和**输出门**。门的图形模型可以描述如下：
- en: '![Long short term memory networks](img/B04779_06_08.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆网络](img/B04779_06_08.jpg)'
- en: Ideally, the gate should return the discrete value of 0 or 1 corresponding to
    the input, 0 when the gate is closed and 1 when open, because it is a gate, but
    programmatically, the gate is set to return the value in the range of 0 to 1 so
    that it can be well trained with BPTT.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，门应返回与输入对应的离散值0或1，门关闭时返回0，打开时返回1，因为它是一个门，但在编程上，门的值设置为0到1之间的值，以便能够通过反向传播（BPTT）进行良好的训练。
- en: 'It may seem like we can now put and fetch exact information at an exact time,
    yet another problem still remains. With just two gates, the input gate and output
    gate, memories stored in the CEC can''t be refreshed easily in a few steps. Therefore,
    we need an additional gate that dynamically changes the value of the CEC. To do
    this, we add a **forget gate** to the architecture to control when the memory
    should be erased. The value preserved in the CEC is overridden with a new memory
    when the value of the gate takes a 0 or close to it. With these three gates, a
    unit can now memorize information or contexts of the past, and so it is called
    an **LSTM block** or an **LSTM** **memory block** because it is more of a block
    than a single neuron. The following is a figure that represents an LSTM block:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来我们可以在精确的时间点获取和存储精确的信息，但仍然存在一个问题。仅有两个门——输入门和输出门——存储在CEC中的记忆无法通过几个步骤轻松刷新。因此，我们需要一个额外的门，能够动态地改变CEC的值。为此，我们在架构中添加了一个**遗忘门**，用来控制何时应当抹去记忆。当门的值变为0或接近0时，CEC中保存的值会被新记忆覆盖。通过这三个门，单元现在可以记住过去的信息或上下文，因此它被称为**LSTM块**或**LSTM**
    **记忆块**，因为它更像一个块，而非单个神经元。以下是表示LSTM块的图示：
- en: '![Long short term memory networks](img/B04779_06_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆网络](img/B04779_06_09.jpg)'
- en: 'The standard LSTM structure was fully explained previously, but there''s a
    technique to get better performance from it, which we''ll explain now. Each gate
    receives connections from the input units and the outputs of all the units in
    LSTM, but there is no direct connection from the CEC. This means we can''t see
    the true hidden state of the network because the output of a block depends so
    much on the output gate; as long as the output gate is closed, none of the gates
    can access the CEC and it is devoid of essential information, which may debase
    the performance of LSTM. One simple yet effective solution is to add connections
    from the CEC to the gates in a block. These are called **peephole connections**,
    and act as standard weighted connections except that no errors are backpropagated
    from the gates through the peephole connections. The peephole connections let
    all gates assume the hidden state even when the output gate is closed. You''ve
    learned a lot of terms now, but as a result, the basic architecture of the whole
    connection can be described as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的LSTM结构已经在之前完全解释过了，但有一种技术可以提高它的性能，我们现在将对此进行解释。每个门都接收来自输入单元和LSTM中所有单元的输出的连接，但没有来自CEC的直接连接。这意味着我们无法看到网络的真实隐藏状态，因为一个块的输出依赖于输出门；只要输出门关闭，任何门都无法访问CEC，而CEC也因此缺乏关键信息，这可能会降低LSTM的性能。一种简单但有效的解决方案是从CEC向块中的门添加连接。这些被称为**窥视孔连接**，它们作为标准的加权连接起作用，唯一的不同是，错误不会通过窥视孔连接从门传播回去。窥视孔连接允许所有的门在输出门关闭时仍能假设隐藏状态。现在你已经学到了很多术语，因此，整个连接的基本架构可以描述如下：
- en: '![Long short term memory networks](img/B04779_06_10.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆网络](img/B04779_06_10.jpg)'
- en: 'For simplicity, a single LSTM block is described in the figure. You might be
    daunted because the preceding model is very intricate. However, when you look
    at the model step by step, you can understand how an LSTM network has figured
    out how to overcome difficulties in NLP. Given an input sequence ![Long short
    term memory networks](img/B04779_06_66.jpg), each network unit can be calculated
    as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，图示中描述了单一的LSTM块。你可能会感到困惑，因为前面的模型非常复杂。然而，当你一步步地分析这个模型时，你会明白LSTM网络是如何解决NLP中的困难的。给定一个输入序列
    ![长短期记忆网络](img/B04779_06_66.jpg)，每个网络单元可以按以下方式计算：
- en: '![Long short term memory networks](img/B04779_06_67.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆网络](img/B04779_06_67.jpg)'
- en: In the preceding formulas, ![Long short term memory networks](img/B04779_06_68.jpg)
    is the matrix of weights from the input gate to the input, ![Long short term memory
    networks](img/B04779_06_68.jpg) is the one from the forget gate to the input,
    and ![Long short term memory networks](img/B04779_06_69.jpg) is the one from the
    output gate to the input. ![Long short term memory networks](img/B04779_06_70.jpg)
    is the weight matrix from the cell to the input, ![Long short term memory networks](img/B04779_06_71.jpg)
    is the one from the cell to the LSTM output, and ![Long short term memory networks](img/B04779_06_72.jpg)
    is the one from the output to the LSTM output. ![Long short term memory networks](img/B04779_06_73.jpg),
    ![Long short term memory networks](img/B04779_06_74.jpg), and ![Long short term
    memory networks](img/B04779_06_75.jpg) are diagonal weight matrices for peephole
    connections. The ![Long short term memory networks](img/B04779_06_76.jpg) terms
    denote the bias vectors, ![Long short term memory networks](img/B04779_06_77.jpg)
    is the input gate bias vector, ![Long short term memory networks](img/B04779_06_78.jpg)
    is the forget gate bias vector, ![Long short term memory networks](img/B04779_06_79.jpg)
    is the output gate bias vector, ![Long short term memory networks](img/B04779_06_80.jpg)
    is the CEC cell bias vector, and ![Long short term memory networks](img/B04779_06_81.jpg)
    is the output bias vector. Here, ![Long short term memory networks](img/B04779_06_82.jpg)
    and ![Long short term memory networks](img/B04779_06_15.jpg) are activation functions
    of the cell input and cell output. ![Long short term memory networks](img/B04779_06_83.jpg)
    denotes the sigmoid function, and ![Long short term memory networks](img/B04779_06_84.jpg)
    the softmax function. ![Long short term memory networks](img/B04779_06_85.jpg)
    is the element-wise product of the vectors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，![长短期记忆网络](img/B04779_06_68.jpg)是从输入门到输入的权重矩阵，![长短期记忆网络](img/B04779_06_68.jpg)是从遗忘门到输入的权重矩阵，![长短期记忆网络](img/B04779_06_69.jpg)是从输出门到输入的权重矩阵。![长短期记忆网络](img/B04779_06_70.jpg)是从单元到输入的权重矩阵，![长短期记忆网络](img/B04779_06_71.jpg)是从单元到LSTM输出的权重矩阵，![长短期记忆网络](img/B04779_06_72.jpg)是从输出到LSTM输出的权重矩阵。![长短期记忆网络](img/B04779_06_73.jpg)，![长短期记忆网络](img/B04779_06_74.jpg)，和![长短期记忆网络](img/B04779_06_75.jpg)是用于窥视孔连接的对角权重矩阵。![长短期记忆网络](img/B04779_06_76.jpg)项表示偏置向量，![长短期记忆网络](img/B04779_06_77.jpg)是输入门的偏置向量，![长短期记忆网络](img/B04779_06_78.jpg)是遗忘门的偏置向量，![长短期记忆网络](img/B04779_06_79.jpg)是输出门的偏置向量，![长短期记忆网络](img/B04779_06_80.jpg)是CEC单元的偏置向量，![长短期记忆网络](img/B04779_06_81.jpg)是输出偏置向量。这里，![长短期记忆网络](img/B04779_06_82.jpg)和![长短期记忆网络](img/B04779_06_15.jpg)是单元输入和单元输出的激活函数。![长短期记忆网络](img/B04779_06_83.jpg)表示Sigmoid函数，![长短期记忆网络](img/B04779_06_84.jpg)表示Softmax函数。![长短期记忆网络](img/B04779_06_85.jpg)是向量的逐元素乘积。
- en: 'We won''t follow the further math equations in this book because they become
    too complicated just by applying BPTT, but you can try LSTM with DL4J as well
    as RNN. As CNN was developed within the field of image recognition, RNN and LSTM
    have been developed to resolve the issues of NLP that arise one by one. While
    both algorithms are just one approach to get a better performance using NLP and
    still need to be improved, since we are living beings that communicate using languages,
    the development of NLP will certainly lead to technological innovations. For applications
    of LSTM, you can reference *Sequence to Sequence Learning with Neural Networks*
    (Sutskever et al., [http://arxiv.org/pdf/1409.3215v3.pdf](http://arxiv.org/pdf/1409.3215v3.pdf)),
    and for more recent algorithms, you can reference *Grid Long Short-Term Memory*
    (Kalchbrenner et al., [http://arxiv.org/pdf/1507.01526v1.pdf](http://arxiv.org/pdf/1507.01526v1.pdf))
    and *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*
    (Xu et al., [http://arxiv.org/pdf/1502.03044v2.pdf](http://arxiv.org/pdf/1502.03044v2.pdf)).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '本书中我们不会继续跟随进一步的数学公式，因为通过应用BPTT它们会变得非常复杂，但你可以尝试使用DL4J实现LSTM和RNN。由于CNN是在图像识别领域中发展起来的，RNN和LSTM则是为了逐步解决NLP中的问题而发展起来的。尽管这两种算法只是使用NLP获取更好性能的一种方法，且仍需要改进，但既然我们是使用语言进行交流的生物，NLP的发展必然会带来技术创新。关于LSTM的应用，你可以参考*Sequence
    to Sequence Learning with Neural Networks*（Sutskever等，[http://arxiv.org/pdf/1409.3215v3.pdf](http://arxiv.org/pdf/1409.3215v3.pdf)），而对于更近期的算法，你可以参考*Grid
    Long Short-Term Memory*（Kalchbrenner等，[http://arxiv.org/pdf/1507.01526v1.pdf](http://arxiv.org/pdf/1507.01526v1.pdf)）以及*Show,
    Attend and Tell: Neural Image Caption Generation with Visual Attention*（Xu等，[http://arxiv.org/pdf/1502.03044v2.pdf](http://arxiv.org/pdf/1502.03044v2.pdf)）。'
- en: The difficulties of deep learning
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的困难
- en: Deep learning has already got higher precision than humans in the image recognition
    field and has been applied to quite a lot of practical applications. Similarly,
    in the NLP field, many models have been researched. Then, how much deep learning
    is utilized in other fields? Surprisingly, there are still few fields where deep
    learning is successfully utilized. This is because deep learning is indeed innovative
    compared to past algorithms and definitely lets us take a big step towards materializing
    AI; however, it has some problems when used for practical applications.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在图像识别领域已经达到了比人类更高的精度，并且已应用于许多实际应用中。同样，在自然语言处理（NLP）领域，也进行了很多模型的研究。那么，深度学习在其他领域的应用情况如何呢？令人惊讶的是，深度学习在许多领域中仍未被成功应用。这是因为深度学习与过去的算法相比确实具有创新性，并且无疑让我们在实现人工智能的道路上迈出了重要一步；然而，当它用于实际应用时，也会遇到一些问题。
- en: The first problem is that there are too many model parameters in deep learning
    algorithms. We didn't look in detail when you learned about the theory and implementation
    of algorithms, but actually deep neural networks have many hyper parameters that
    need to be decided compared to the past neural networks or other machine learning
    algorithms. This means we have to go through more trial and error to get high
    precision. Combinations of parameters that define a structure of neural networks,
    such as how many hidden layers are to be set or how many units each hidden layer
    should have, need lots of experiments. Also, the parameters for training and test
    configurations such as the learning rate need to be determined. Furthermore, peculiar
    parameters for each algorithm such as the corruption level in SDA and the size
    of kernels in CNN need additional trial and error. Thus, the great performance
    that deep learning provides is supported by steady parameter-tuning. However,
    people only look at one side of deep learning—that it can get great precision—
    and they tend to forget the hard process required to reach that point. Deep learning
    is not magic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是深度学习算法中有过多的模型参数。当你学习算法的理论和实现时，我们并未详细讨论，但实际上，深度神经网络相较于过去的神经网络或其他机器学习算法，拥有许多需要决定的超参数。这意味着我们必须经历更多的试验与错误才能获得高精度。定义神经网络结构的参数组合，如隐藏层的数量或每个隐藏层的单位数，需要大量实验。此外，像学习率等训练和测试配置的参数也需要确定。还有，每个算法特有的参数，如SDA中的腐蚀水平和CNN中的卷积核大小，也需要额外的试错。因此，深度学习提供的卓越性能是通过持续的参数调优来支撑的。然而，人们往往只关注深度学习的一面——它可以获得极高的精度——而忽视了达到这一点所需的艰苦过程。深度学习不是魔法。
- en: 'In addition, deep learning often fails to train and classify data from simple
    problems. The shape of deep neural networks is so deep and complicated that the
    weights can''t be well optimized. In terms of optimization, data quantities are
    also important. This means that deep neural networks require a significant amount
    of time for each training. To sum up, deep learning shows its worth when:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习在训练和分类简单问题的数据时常常失败。深度神经网络的结构非常深且复杂，以至于权重无法得到良好的优化。在优化方面，数据量也是至关重要的。这意味着深度神经网络在每次训练时需要大量的时间。总而言之，深度学习的价值体现在以下几点：
- en: It solves complicated and hard problems when people have no idea what feature
    they can be classified as
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它解决了当人们不知道应该如何对特征进行分类时的复杂且困难的问题
- en: There is sufficient training data to properly optimize deep neural networks
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有足够的训练数据来适当地优化深度神经网络
- en: Compared to applications that constantly update a model using continuously updated
    data, once a model is built using a large-scale dataset that doesn't change drastically,
    applications that use the model universally are rather well suited for deep learning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与那些通过不断更新数据来持续更新模型的应用相比，一旦使用一个大规模且变化不大的数据集构建了模型，适用于深度学习的应用通常会较为合适。
- en: Therefore, when you look at business fields, you can say that there are more
    cases where the existing machine learning can get better results than using deep
    learning. For example, let's assume we would like to recommend appropriate products
    to users in an EC. In this EC, many users buy a lot of products daily, so purchase
    data is largely updated daily. In this case, do you use deep learning to get high-precision
    classification and recommendations to increase the conversion rates of users'
    purchases using this data? Probably not, because using the existing machine learning
    algorithms such as Naive Bayes, collaborative filtering, SVM, and so on, we can
    get sufficient precision from a practical perspective and can update the model
    and calculate quicker, which is usually more appreciated. This is why deep learning
    is not applied much in business fields. Of course, getting higher precision is
    better in any field, but in reality, higher precision and the necessary calculation
    time are in a trade-off relationship. Although deep learning is significant in
    the research field, it has many hurdles yet to clear considering practical applications.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你观察业务领域时，你会发现存在更多的情况是，现有的机器学习比使用深度学习能够获得更好的结果。例如，假设我们希望在电商中向用户推荐合适的产品。在这个电商平台中，许多用户每天都会购买大量商品，因此购买数据会每日大量更新。在这种情况下，你是否会使用深度学习来利用这些数据获得高精度的分类和推荐，以提高用户购买的转化率？可能不会，因为使用现有的机器学习算法，如朴素贝叶斯、协同过滤、支持向量机（SVM）等，我们可以从实际角度获得足够的精度，并且可以更快速地更新模型和进行计算，这通常是更受欢迎的。这也是深度学习在商业领域应用较少的原因。当然，在任何领域中，获得更高的精度是更好的，但实际上，高精度和所需的计算时间存在权衡关系。虽然深度学习在研究领域具有重要意义，但考虑到实际应用，它仍然面临许多障碍。
- en: Besides, deep learning algorithms are not perfect, and they still need many
    improvements to their model itself. For example, RNN, as mentioned earlier, can
    only satisfy either how past information can be reflected to a network or how
    precision can be obtained, although it's contrived with techniques such as LSTM.
    Also, deep learning is still far from the true AI, although it's definitely a
    great technique compared to the past algorithms. Research on algorithms is progressing
    actively, but in the meantime, we need one more breakthrough to spread out and
    infiltrate deep learning into broader society. Maybe this is not just the problem
    of a model. Deep learning is suddenly booming because it is reinforced by huge
    developments in hardware and software. Deep learning is closely related to development
    of the surrounding technology.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习算法并不完美，仍然需要在其模型本身上进行很多改进。例如，如前所述，RNN只能满足如何将过去的信息反映到网络中或如何获得精准度，尽管它通过像LSTM这样的技术加以改善。而且，深度学习距离真正的人工智能还很远，尽管与过去的算法相比，它无疑是一项伟大的技术。虽然算法的研究在积极推进，但与此同时，我们还需要一个突破，才能将深度学习推广并渗透到更广泛的社会中。也许这不仅仅是模型的问题。深度学习之所以突然爆发，是因为它得到了硬件和软件领域巨大发展的支撑。深度学习与周边技术的发展密切相关。
- en: As mentioned earlier, there are still many hurdles to clear before deep learning
    can be applied more practically in the real world, but this is not impossible
    to achieve. It isn't possible to suddenly invent AI to achieve technological singularity,
    but there are some fields and methods where deep learning can be applied right
    away. In the next section, we'll think about what kinds of industries deep learning
    can be utilized in. Hopefully, it will sow the seeds for new ideas in your business
    or research fields.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在深度学习能够更实际地应用于现实世界之前，仍然有许多障碍需要克服，但这并非不可能实现。我们无法突然发明出能够实现技术奇点的人工智能，但有些领域和方法可以立刻应用深度学习。在接下来的章节中，我们将思考深度学习可以在哪些行业中得到应用。希望这能为你的业务或研究领域播下新的想法种子。
- en: The approaches to maximizing deep learning possibilities and abilities
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大化深度学习可能性和能力的方法
- en: 'There are several approaches to how we can apply deep learning to various industries.
    While it is true that an approach could be different depending on the task or
    purpose, we can briefly categorize the approaches in the following three ways:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方法将深度学习应用到不同的行业。虽然根据任务或目的的不同，方法可能有所不同，但我们可以将这些方法简要地分为以下三种：
- en: '**Field-oriented approach**: This utilizes deep learning algorithms or models
    that are already thoroughly researched and can lead to great performance'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面向领域的方法**：利用已经经过充分研究的深度学习算法或模型，这些算法或模型能够带来出色的性能'
- en: '**Breakdown-oriented approach**: This replaces the problems to be solved that
    deep learning can apparently be applied to with a different problem where deep
    learning can be well adopted'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于问题拆解的方法**：这将深度学习显然可以应用的待解决问题替换为一个不同的问题，在这个问题上深度学习能够得到很好的应用。'
- en: '**Output-oriented approach**: This explores new ways of how we express the
    output with deep learning'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于结果导向的方法**：这探讨了如何通过深度学习表达输出的新方式。'
- en: These approaches are all explained in detail in the following subsections. Each
    approach is divided into its suitable industries or areas where it is not suitable,
    but any of them could be a big hint for your activities going forward. There are
    still very few use cases of deep learning and bias against fields of use, but
    this means there should be many chances to create innovative and new things. Start-ups
    that utilize deep learning have been emerging recently and some of them have already
    achieved success to some extent. You can have a significant impact on the world
    depending on your ideas.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法将在以下小节中详细解释。每种方法都根据其适用的行业或领域进行划分，并指出哪些地方不适用，但它们中的任何一种都可能为你的未来活动提供重要的启示。目前，深度学习的应用案例仍然很少，且对其应用领域存在偏见，但这也意味着应该有很多机会去创造创新和新事物。最近，利用深度学习的初创公司逐渐涌现，其中一些已经取得了一定程度的成功。根据你的想法，你可以对世界产生重大影响。
- en: Field-oriented approach
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于领域导向的方法
- en: This approach doesn't require new techniques or algorithms. There are obviously
    fields that are well suited to the current deep learning techniques, and the concept
    here is to dive into these fields. As explained previously, since deep learning
    algorithms that have been practically studied and developed are mostly in image
    recognition and NLP, we'll explore some fields that can work in great harmony
    with them.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不需要新的技术或算法。显然有一些领域非常适合当前的深度学习技术，这里提出的概念是深入挖掘这些领域。如前所述，经过实际研究和开发的深度学习算法主要集中在图像识别和自然语言处理（NLP）上，我们将探索一些与它们能很好融合的领域。
- en: Medicine
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 医学
- en: Medical fields should be developed by deep learning. Tumors or cancers are detected
    on scanned images. This means nothing more than being able to utilize one of the
    strongest features of deep learning—the technique of image recognition. It is
    possible to dramatically increase precision using deep learning to help with the
    early detection of an illness and identifying the kind of illness. Since CNN can
    be applied to 3D images, 3D scanned images should be able to be analyzed relatively
    easily. By adopting deep learning more in the current medical field, deep learning
    should greatly contribute.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 医学领域应该通过深度学习进行发展。肿瘤或癌症在扫描图像中被检测出来。这无非是能够利用深度学习的一个最强大的功能——图像识别技术。利用深度学习帮助早期发现疾病并识别疾病种类，能够显著提高精确度。由于卷积神经网络（CNN）可以应用于3D图像，因此3D扫描图像应该可以相对容易地进行分析。通过在当前医学领域更广泛地采用深度学习，深度学习应能带来巨大的贡献。
- en: We can also say that deep learning can be significantly useful for the medical
    field in the future. The medical field has been under strict regulations; however,
    there is a movement progressing to ease the regulations in some countries, probably
    because of the recent development of IT and its potential. Therefore, there will
    be opportunities in business for the medical field and IT to have a synergistic
    effect. For example, if telemedicine is more infiltrated, there is the possibility
    that diagnosing or identifying a disease can be done not only by a scanned image,
    but also by an image shown in real time on a display. Also, if electronic charts
    become widespread, it would be easier to analyze medical data using deep learning.
    This is because medical records are compatible with deep learning as they are
    a dataset of texts and images. Then the symptoms of unknown diseases can be found.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以说，深度学习在未来可以对医学领域产生显著的帮助。医学领域一直受到严格的监管；然而，某些国家正在推动放宽监管，可能是因为信息技术的近期发展及其潜力。因此，医学领域和信息技术的结合将带来商业机会，产生协同效应。例如，如果远程医疗更为普及，诊断或疾病识别可能不仅仅依靠扫描图像，还可以依靠实时显示的图像。另外，如果电子病历变得普及，利用深度学习分析医学数据将变得更加容易。这是因为医学记录本身就是一个文本和图像的数据集，非常适合深度学习。通过这种方式，未知疾病的症状也可以被发现。
- en: Automobiles
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汽车
- en: 'We can say that the surroundings of running cars are image sequences and text.
    Other cars and views are images and a road sign has text. This means we can also
    utilize deep learning techniques here, and it is possible to reduce the risk of
    accidents by improving driving assistance functions. It can be said that the ultimate
    type of driving assistance is self-driving cars, which is being tackled mainly
    by Google and Tesla. An example that is both famous and fascinating was when George
    Hotz, the first person to hack the iPhone, built a self-driving car in his garage.
    The appearance of the car was introduced in an article by Bloomberg Business ([http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/](http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/)),
    and the following image was included in the article:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，行驶中的汽车周围的环境是由图像序列和文字组成的。其他汽车和景观是图像，道路标志则是文字。这意味着我们也可以在这里运用深度学习技术，并且通过改进驾驶辅助功能，降低事故风险。可以说，驾驶辅助的终极形式是自动驾驶汽车，目前这一领域主要由谷歌和特斯拉在攻克。一个既著名又引人入胜的例子是，乔治·霍茨（George
    Hotz），iPhone的首位黑客，曾在自己的车库里打造了一辆自动驾驶汽车。该车的外观被《彭博商业周刊》的一篇文章介绍过（[http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/](http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/)），文章中还附有以下图片：
- en: '![Automobiles](img/B04779_06_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![汽车](img/B04779_06_11.jpg)'
- en: Self-driving cars have been already tested in the U.S., but since other countries
    have different traffic rules and road conditions, this idea requires further studying
    and development before self-driving cars are commonly used worldwide. The key
    to success in this field is in learning and recognizing surrounding cars, people,
    views, and traffic signs, and properly judging how to process them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车已经在美国进行了测试，但由于其他国家的交通规则和道路条件不同，这一想法需要进一步研究和开发，才能在全球范围内普及使用。该领域成功的关键在于学习和识别周围的汽车、行人、景象和交通标志，并准确判断如何处理它们。
- en: In the meantime, we don't have to just focus on utilizing deep learning techniques
    for the actual body of a car. Let's assume we could develop a smartphone app that
    has the same function as we just described, that is, recognizing and classifying
    surrounding images and text. Then, if you just set up the smartphone in your car,
    you could utilize it as a car-navigation app. In addition, for example, it could
    be used as a navigation app for blind people, providing them with good, reliable
    directions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们不必仅仅集中在利用深度学习技术应用于汽车的实际车身上。假设我们可以开发一个智能手机应用程序，具备我们刚刚描述的相同功能，即识别和分类周围的图像和文字。那么，如果你把智能手机放置在车里，就可以将其用作车载导航应用程序。此外，例如，它还可以作为盲人导航应用程序，为他们提供准确可靠的方向指引。
- en: Advert technologies
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广告技术
- en: 'Advert (ad) technologies could expand their coverage with deep learning. When
    we say ad technologies, this currently means recommendation or ad networks that
    optimize ad banners or products to be shown. On the other hand, when we say advertising,
    this doesn''t only mean banners or ad networks. There are various kinds of ads
    in the world depending on the type of media, such as TV ads, radio ads, newspaper
    ads, posters, flyers, and so on. We have also digital ad campaigns with YouTube,
    Vine, Facebook, Twitter, Snapchat, and so on. Advertising itself has changed its
    definition and content, but all ads have one thing in common: they consist of
    images and/or language. This means they are fields that deep learning is good
    at. Until now, we could only use user-behavior-based indicators, such as **page
    view** (**PV**), **click through rate** (**CTR**), and **conversion rate** (**CVR**),
    to estimate the effect of an ad, but if we apply deep learning technologies, we
    might be able to analyze the actual content of an ad and autogenerate ads going
    forward. Especially since movies and videos can only be analyzed as a result of
    image recognition and NLP, video recognition, not image recognition, will gather
    momentum besides ad technologies.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 广告（ad）技术可以通过深度学习扩展其覆盖范围。当我们说广告技术时，目前指的是优化广告横幅或展示产品的推荐系统或广告网络。另一方面，当我们说广告时，不仅仅指横幅或广告网络。根据媒体类型，世界上有各种各样的广告，如电视广告、广播广告、报纸广告、海报、传单等等。我们也有数字广告活动，如
    YouTube、Vine、Facebook、Twitter、Snapchat 等等。广告本身已经改变了其定义和内容，但所有广告有一个共同点：它们由图像和/或语言组成。这意味着它们是深度学习擅长的领域。直到现在，我们只能通过基于用户行为的指标，如**页面浏览量**
    (**PV**)、**点击率** (**CTR**) 和 **转化率** (**CVR**) 来估计广告的效果，但如果我们应用深度学习技术，未来可能能够分析广告的实际内容并自动生成广告。特别是由于电影和视频只能通过图像识别和自然语言处理进行分析，视频识别将逐渐成为广告技术之外的重要发展方向。
- en: Profession or practice
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 职业或实践
- en: Professions such as doctor, lawyer, patent attorney, and accountant are considered
    to be roles that deep learning can replace. For example, if NLP's precision and
    accuracy gets higher, any perusal that requires expertise can be left to a machine.
    As a machine can cover these time-consuming reading tasks, people can focus more
    on high-value tasks. In addition, if a machine classifies past judicial cases
    or medical cases on what disease caused what symptoms and so on, we would be able
    to build an app like Apple's Siri that answers simple questions that usually require
    professional knowledge. Then the machine could handle these professional cases
    to some extent if a doctor or a lawyer is too busy to help in a timely manner.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 医生、律师、专利律师和会计师等职业被认为是深度学习可以取代的岗位。例如，如果自然语言处理（NLP）的精确度和准确性提高，任何需要专业知识的审查工作都可以交给机器处理。由于机器可以承担这些耗时的阅读任务，人们可以更多地专注于高价值的工作。此外，如果机器能够对过去的司法案件或医疗病例进行分类，识别哪些疾病引起了哪些症状等，我们就可以建立像苹果的
    Siri 这样的应用程序，回答那些通常需要专业知识的问题。那么，当医生或律师因忙碌无法及时提供帮助时，机器也能够在一定程度上处理这些专业案件。
- en: It's often said that AI takes away a human's job, but personally, this seems
    incorrect. Rather, a machine takes away menial work, which should support humans.
    A software engineer who works on AI programming can be described as having a professional
    job, but this work will also be changed in the future. For example, think about
    a car-related job, where the current work is building standard automobiles, but
    in the future, engineers will be in a position just like pit crews for Formula
    1 cars.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说人工智能会取代人类的工作，但我个人认为这不准确。实际上，机器取代的是那些琐碎的工作，这些本应由人类完成。一个从事AI编程的软件工程师可以被视为拥有专业工作，但这种工作在未来也会发生变化。例如，想象一下与汽车相关的工作，目前的工作是制造标准化汽车，但未来，工程师将像一级方程式赛车的
    pit crew 一样工作。
- en: Sports
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 体育
- en: Deep learning can certainly contribute to sports as well. In the study field
    known as sports science, it has become increasingly important to analyze and examine
    data from sports. As an example, you may know the book or movie *Moneyball*. In
    this film, they hugely increased the win percentage of the team by adopting a
    regression model in baseball. Watching sports itself is very exciting, but on
    the other hand, sport can be seen as a chunk of image sequences and number data.
    Since deep learning is good at identifying features that humans can't find, it
    will become easier to find out why certain players get good scores while others
    don't.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习无疑也能为体育领域作出贡献。在被称为运动科学的研究领域中，分析和研究体育数据变得越来越重要。例如，你可能知道《点球成金》这本书或电影。在这部电影中，通过在棒球中采用回归模型，他们大大提高了球队的胜率。观看体育赛事本身非常激动人心，但另一方面，体育也可以看作是一段图像序列和数字数据。由于深度学习擅长识别人类无法发现的特征，因此，找出为什么某些球员得分高而其他球员得分低将变得更加容易。
- en: These fields we have mentioned are only a small part of the many fields where
    deep learning is capable of significantly contributing to development. We have
    looked into these fields from the perspective of whether a field has images or
    text, but of course deep learning should also show great performance for simple
    analysis with general number data. It should be possible to apply deep learning
    to various other fields, such as bioinformatics, finance, agriculture, chemistry,
    astronomy, economy, and more.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的这些领域只是深度学习能够显著推动发展的众多领域中的一小部分。我们从一个领域是否有图像或文本的角度进行了探讨，但当然，深度学习对于简单的数字数据分析也应当展现出强大的性能。深度学习应该能够应用到许多其他领域，如生物信息学、金融、农业、化学、天文学、经济学等。
- en: Breakdown-oriented approach
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以问题为导向的方法
- en: 'This approach might be similar to the approach considered in traditional machine
    learning algorithms. We already talked about how feature engineering is the key
    to improving precision in machine learning. Now we can say that this feature engineering
    can be divided into the following two parts:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能与传统机器学习算法中采用的方法相似。我们已经讨论过特征工程是提高机器学习精度的关键。现在我们可以说，特征工程可以分为以下两部分：
- en: Engineering under the constraints of a machine learning model. The typical case
    is to make inputs discrete or continuous.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习模型的约束下进行工程设计。典型的例子是将输入数据设为离散型或连续型。
- en: Feature engineering to increase precision by machine learning. This tends to
    rely on the sense of a researcher.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过机器学习提高精度的特征工程。这通常依赖于研究人员的直觉。
- en: In a narrower meaning, feature engineering is considered as the second one,
    and this is the part that deep learning doesn't have to focus on, whereas the
    first one is definitely the important part, even for deep learning. For example,
    it's difficult to predict stock prices using deep learning. Stock prices are volatile
    and it's difficult to define inputs. Besides, how to apply an output value is
    also a difficult problem. Enabling deep learning to handle these inputs and outputs
    is also said to be feature engineering in the wider sense. If there is no limitation
    to the value of original data and/or data you would like to predict, it's difficult
    to insert these datasets into machine learning and deep learning algorithms, including
    neural networks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在狭义上，特征工程被认为是第二部分，而这正是深度学习不需要关注的部分；而第一部分无疑是重要的，甚至对于深度学习也是如此。例如，使用深度学习预测股价是困难的。股价波动大且难以定义输入数据。此外，如何应用输出值也是一个难题。让深度学习处理这些输入和输出也被认为是广义上的特征工程。如果没有对原始数据和/或你希望预测的数据进行限制，将这些数据集插入到机器学习和深度学习算法中（包括神经网络）是困难的。
- en: However, we can take a certain approach and apply a model to these previous
    problems by breaking down the inputs and/or outputs. In terms of NLP, as explained
    earlier, you might have thought, for example, that it would be impossible to put
    numberless words into features in the first place, but as you already know, we
    can train feed-forward neural networks with words by representing them with sparse
    vectors and combining N-grams into them. Of course, we can not only use neural
    networks, but also other machine learning algorithms such as SVM here. Thus, we
    can cultivate a new field where deep learning hasn't been applied by engineering
    to fit features well into deep learning models. In the meantime, when we focus
    on NLP, we can see that RNN and LSTM were developed to properly resolve the difficulties
    or tasks encountered in NLP. This can be considered as the opposite approach to
    feature engineering because in this case, the problem is solved by breaking down
    a model to fit into features.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以采取某种方法，通过分解输入和/或输出来应用模型于这些先前的问题。就像之前在NLP中解释的那样，你可能曾想过，首先将无数的单词转换为特征是不可能的，但正如你所知道的，我们可以通过用稀疏向量表示单词，并将N-grams组合起来，来训练前馈神经网络。当然，我们不仅可以使用神经网络，还可以使用其他机器学习算法，如支持向量机（SVM）。因此，我们可以在深度学习尚未应用的领域，通过工程方法将特征与深度学习模型很好地契合，从而开辟一个新领域。与此同时，当我们聚焦于NLP时，我们可以看到，RNN和LSTM的开发是为了正确解决NLP中遇到的困难或任务。这可以被看作是与特征工程相对的做法，因为在这种情况下，问题是通过分解模型来适配特征解决的。
- en: Then, how do we do utilize engineering for stock prediction as we just mentioned?
    It's actually not difficult to think of inputs, that is, features. For example,
    if you predict stock prices daily, it's hard to calculate if you use daily stock
    prices as features, but if you use a rate of price change between a day and the
    day before, then it should be much easier to process as the price stays within
    a certain range and the gradients won't explode easily. Meanwhile, what is difficult
    is how to deal with outputs. Stock prices are of course continuous values, hence
    outputs can be various values. This means that in the neural network model where
    the number of units in the output layer is fixed, they can't handle this problem.
    What should we do here—should we give up?! No, wait a minute. Unfortunately, we
    can't predict a stock price itself, but there is an alternative prediction method.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何利用工程方法进行股价预测呢？其实，考虑输入，也就是特征，并不难。举个例子，如果你每天预测股价，使用每日股价作为特征会很难计算，但如果你使用当天与前一天之间的价格变化率，那么处理起来会更容易，因为股价保持在一定范围内，梯度也不容易爆炸。与此同时，困难的地方在于如何处理输出。股价当然是连续的数值，因此输出可能是各种值。这意味着，在神经网络模型中，输出层的单元数是固定的，它们无法处理这个问题。那么我们该怎么办呢——应该放弃吗？！不，稍等一下。很遗憾，我们无法预测股价本身，但有一种替代的预测方法。
- en: 'Here, the problem is that we can classify stock prices to be predicted into
    infinite patterns. Then, can we make them into limited patterns? Yes, we can.
    Let''s forcibly make them. Think about the most extreme but easy to understand
    case: predicting whether tomorrow''s stock price, strictly speaking a close price,
    is up or down using the data from the stock price up to today. For this case,
    we can show it with a deep learning model as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，问题在于我们可以将股价预测分类为无限多种模式。那么，我们能否将它们转换为有限模式呢？是的，我们可以。让我们强行把它们转化一下。考虑一个最极端但容易理解的案例：利用截至今天的股价数据预测明天的股价，严格来说是收盘价，是上涨还是下跌。对于这个案例，我们可以用一个深度学习模型来表示，具体如下：
- en: '![Breakdown-oriented approach](img/B04779_06_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![以细分为导向的方法](img/B04779_06_12.jpg)'
- en: 'In the preceding image, ![Breakdown-oriented approach](img/B04779_06_86.jpg)
    denotes the open price of a day, ![Breakdown-oriented approach](img/B04779_06_47.jpg);
    ![Breakdown-oriented approach](img/B04779_06_87.jpg) denotes the close price,
    ![Breakdown-oriented approach](img/B04779_06_88.jpg) is the high price, and ![Breakdown-oriented
    approach](img/B04779_06_89.jpg) is the actual price. The features used here are
    mere examples, and need to be fine-tuned when applied to real applications. The
    point here is that replacing the original task with this type of problem enables
    deep neural networks to theoretically classify data. Furthermore, if you classify
    the data by how much it will go up or down, you could make more detailed predictions.
    For example, you could classify data as shown in the following table:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图片中，![分解导向方法](img/B04779_06_86.jpg) 表示某一天的开盘价，![分解导向方法](img/B04779_06_47.jpg)；![分解导向方法](img/B04779_06_87.jpg)
    表示收盘价，![分解导向方法](img/B04779_06_88.jpg) 是最高价，![分解导向方法](img/B04779_06_89.jpg) 是实际价格。这里使用的特征仅为示例，实际应用时需要进行调整。关键在于，用这种问题替代原始任务，可以使深度神经网络理论上进行数据分类。此外，如果按照上涨或下跌的幅度来分类数据，您可以做出更精细的预测。例如，您可以按以下表格所示对数据进行分类：
- en: '| Class | Description |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 描述 |'
- en: '| --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Class 1 | Up more than 3 percent from the closing price |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 类别 1 | 比收盘价上涨超过 3 百分比 |'
- en: '| Class 2 | Up more than 1~3 percent from the closing price |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 类别 2 | 比收盘价上涨 1~3 百分比 |'
- en: '| Class 3 | Up more than 0~1 percent from the closing price |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 类别 3 | 比收盘价上涨 0~1 百分比 |'
- en: '| Class 4 | Down more than 0~-1 percent from the closing price |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 类别 4 | 比收盘价下跌 0~-1 百分比 |'
- en: '| Class 5 | Down more than -1~-3 percent from the closing price |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 类别 5 | 比收盘价下跌 1~-3 百分比 |'
- en: '| Class 6 | Down more than -3 percent from the closing price |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 类别 6 | 比收盘价下跌超过 -3 百分比 |'
- en: Whether the prediction actually works, in other words whether the classification
    works, is unknown until we examine it, but the fluctuation of stock prices can
    be predicted in quite a narrow range by dividing the outputs into multiple classes.
    Once we can adopt the task into neural networks, then what we should do is just
    examine which model gets better results. In this example, we may apply RNN because
    the stock price is time sequential data. If we look at charts showing the price
    as image data, we can also use CNN to predict the future price.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 无论预测是否真正有效，换句话说，分类是否有效，都无法在事前确定，直到我们进行验证。不过，通过将输出分为多个类别，股票价格的波动可以在相当窄的范围内进行预测。一旦我们能够将任务应用于神经网络，那么接下来我们需要做的就是检查哪个模型得到更好的结果。在这个示例中，由于股价是时间序列数据，我们可以应用
    RNN。如果我们将股价图表视为图像数据，我们也可以使用 CNN 来预测未来的股价。
- en: 'So now we''ve thought about the approach by referring to examples, but to sum
    up in general, we can say that:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们已经参考示例思考了方法，但总结起来，我们可以说：
- en: '**Feature engineering for models**: This is designing inputs or adjusting values
    to fit deep learning models, or enabling classification by setting a limitation
    for the outputs'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型特征工程**：这意味着设计输入或调整值，以适应深度学习模型，或通过设置输出限制来实现分类。'
- en: '**Model engineering for features**: This is devising new neural network models
    or algorithms to solve problems in a focused field'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征模型工程**：这意味着设计新的神经网络模型或算法，以解决某一特定领域的问题。'
- en: The first one needs ideas for the part of designing inputs and outputs to fit
    to a model, whereas the second one needs to take a mathematical approach. Feature
    engineering might be easier to start if you are conscious of making an item prediction-limited.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法需要设计输入和输出，以便适应模型，而第二个方法则需要采用数学方法。如果您意识到让项目的预测有所限制，那么特征工程可能会更容易启动。
- en: Output-oriented approach
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向输出的方法
- en: The two previously mentioned approaches are to increase the percentage of correct
    answers for a certain field's task or problem using deep learning. Of course,
    it is essential and the part where deep learning proves its worth; however, increasing
    precision to the ultimate level may not be the only way of utilizing deep learning.
    Another approach is to devise the outputs using deep learning by slightly changing
    the point of view. Let's see what this means.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的两种方法，旨在通过深度学习提高特定领域任务或问题的正确答案比例。当然，这对于深度学习而言是至关重要的部分，也是其价值体现的地方；然而，将精度提高到最终极限，可能并不是利用深度学习的唯一方法。另一种方法是通过稍微改变视角，利用深度学习来设计输出。让我们来看看这意味着什么。
- en: Deep learning is applauded as an innovative approach among researchers and technical
    experts of AI, but the world in general doesn't know much about its greatness
    yet. Rather, they pay attention to what a machine can't do. For example, people
    don't really focus on the image recognition capabilities of MNIST using CNN, which
    generates a lower error rate than humans, but they criticize that a machine can't
    recognize images perfectly. This is probably because people expect a lot when
    they hear and imagine AI. We might need to change this mindset. Let's consider
    DORAEMON, a Japanese national cartoon character who is also famous worldwide—a
    robot who has high intelligence and AI, but often makes silly mistakes. Do we
    criticize him? No, we just laugh it off or take it as a joke and don't get serious.
    Also, think about DUMMY / DUM-E, the robot arm in the movie *Iron Man*. It has
    AI as well, but makes silly mistakes. See, they make mistakes but we still like
    them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被AI研究人员和技术专家称为一种创新的方法，但大众仍然不了解它的伟大。相反，他们更关注机器做不到的事情。例如，人们并不关注使用CNN进行图像识别的MNIST，它的错误率比人类还低，但却批评机器无法完美识别图像。这可能是因为人们一提到AI，就会对其产生很高的期待，想象它的能力。我们可能需要改变这种心态。让我们考虑一下DORAEMON，这个在日本非常著名、全球也家喻户晓的漫画人物——一只拥有高智能和AI的机器人，但常常犯傻。我们会批评他吗？不会，我们只是笑一笑，或者当作笑话来看，并不会太认真。再想想电影《钢铁侠》中的DUMMY
    / DUM-E，这个机器人手臂也有AI，但同样会犯傻错误。看看，它们会犯错，但我们仍然喜欢它们。
- en: In this way, it might be better to emphasize the point that machines make mistakes.
    Changing the expression part of a user interface could be the trigger for people
    to adopt AI rather than just studying an algorithm the most. Who knows? It's highly
    likely that you can gain the world's interest by thinking of ideas in creative
    fields, not from the perspective of precision. Deep Dream by Google is one good
    example. We can do more exciting things when art or design and deep learning collaborate.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，可能更好地强调机器会犯错这一点。改变用户界面中的表达部分，可能会成为人们采用人工智能的触发点，而不仅仅是最先研究某个算法。谁知道呢？从创造性领域的角度思考，可能更容易引起世界的兴趣，而不是从精确度的角度。谷歌的Deep
    Dream就是一个很好的例子。当艺术或设计与深度学习合作时，我们可以做更多令人兴奋的事情。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned how to utilize deep learning algorithms for practical
    applications. The fields that are well studied are image recognition and NLP.
    While learning about the field of NLP, we looked through two new deep learning
    models: the RNN and LSTM networks, which can be trained with time sequential data.
    The training algorithm used in these models is BPTT. You also learned that there
    are three approaches to make the best of the deep learning ability: the field-oriented
    approach, the breakdown-oriented approach, and the output-oriented approach. Each
    approach has a different angle, and can maximize the possibility for deep learning.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学会了如何利用深度学习算法进行实际应用。已经得到广泛研究的领域包括图像识别和自然语言处理（NLP）。在学习NLP领域时，我们研究了两种新的深度学习模型：RNN和LSTM网络，这些网络可以通过时间序列数据进行训练。这些模型使用的训练算法是BPTT。你还学到了三种最大化深度学习能力的方法：面向领域的方法、面向细分的方法和面向输出的方法。每种方法有不同的角度，都可以最大化深度学习的可能性。
- en: And …congratulations! You've just accomplished the learning part of deep learning
    with Java. Although there are still some models that have not been mentioned yet
    in this book, you can be sure there will be no problem in acquiring and utilizing
    them. The next chapter will introduce some libraries that are implemented with
    other programming languages, so just relax and take a look.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 而且……恭喜你！你刚刚完成了使用Java进行深度学习的学习部分。虽然这本书中仍有一些模型尚未提及，但你可以确信，掌握和使用它们不会有问题。下一章将介绍一些用其他编程语言实现的库，所以放轻松，看看就好。
