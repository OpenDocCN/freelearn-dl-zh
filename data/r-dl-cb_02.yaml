- en: Deep Learning with R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 R 进行深度学习
- en: 'This chapter will build a foundation for neural networks followed by deep learning
    foundation and trends. We will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为神经网络打下基础，接着是深度学习基础和趋势的介绍。我们将涵盖以下主题：
- en: Starting with logistic regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从逻辑回归开始
- en: Introducing the dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入数据集
- en: Performing logistic regression using H2O
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 H2O 执行逻辑回归
- en: Performing logistic regression using TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 执行逻辑回归
- en: Visualizing TensorFlow graphs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化 TensorFlow 图
- en: Starting with multilayer perceptrons
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多层感知机开始
- en: Setting up a neural network using H2O
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 H2O 设置神经网络
- en: Tuning hyper-parameters using grid searches in H2O
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 H2O 的网格搜索调优超参数
- en: Setting up a neural network using MXNet
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MXNet 设置神经网络
- en: Setting up a neural network using TensorFlow
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 设置神经网络
- en: Starting with logistic regression
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从逻辑回归开始
- en: Before we delve into neural networks and deep learning models, let's take a
    look at logistic regression, which can be viewed as a single layer neural network.
    Even the **sigmoid** function commonly used in logistic regression is used as
    an activation function in neural networks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨神经网络和深度学习模型之前，让我们先了解一下逻辑回归，它可以看作是一个单层神经网络。即使是逻辑回归中常用的**sigmoid**函数，也被用作神经网络中的激活函数。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Logistic regression is a supervised machine learning approach for the classification
    of dichotomous/ordinal (order discrete) categories.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种监督学习方法，用于对二分类/有序（顺序离散）类别进行分类。
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Logistic regression serves as a building block for complex neural network models
    using sigmoid as an activation function. The logistic function (or sigmoid) can
    be represented as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归作为构建复杂神经网络模型的基础，使用 sigmoid 作为激活函数。逻辑函数（或 sigmoid）可以表示如下：
- en: '![](img/00003.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00003.jpeg)'
- en: 'The preceding sigmoid function forms a continuous curve with a value bound
    between [0, 1], as illustrated in the following screenshot:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的 sigmoid 函数形成一个连续曲线，其值限制在 [0, 1] 之间，如下图所示：
- en: '![](img/00005.gif)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.gif)'
- en: Sigmoid functional form
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数形式
- en: 'The formulation of a logistic regression model can be written as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的公式可以写作如下：
- en: '![](img/00008.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00008.jpeg)'
- en: 'Here, *W* is the weight associated with features *X***=** [*x[1], x[2], ...,
    x[m]*] and *b* is the model intercept, also known as the model bias. The whole
    objective is to optimize *W* for a given loss function such as cross entropy.
    Another view of the logistic regression model to attain *Pr*(*y=1|***X)** is shown
    in the following figure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W* 是与特征 *X* = ** [*x[1], x[2], ..., x[m]*] 相关的权重，*b* 是模型的截距，也称为模型偏差。整个目标是优化
    *W*，以适应给定的损失函数，例如交叉熵。另一种看待逻辑回归模型的方法是为了获得 *Pr*(*y=1|***X)**，如下图所示：
- en: '![](img/00009.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00009.jpeg)'
- en: Logistic regression architecture with the sigmoid activation function
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 带有 sigmoid 激活函数的逻辑回归架构
- en: Introducing the dataset
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入数据集
- en: This recipe shows how to prepare a dataset to be used to demonstrate different
    models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该配方展示了如何准备一个数据集，供展示不同的模型使用。
- en: Getting ready
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: As logistic regression is a linear classifier, it assumes linearity in independent
    variables and log odds. Thus, in scenarios where independent features are linear-dependent
    on log odds, the model performs very well. Higher-order features can be included
    in the model to capture nonlinear behavior. Let's see how to build logistic regression
    models using major deep learning packages as discussed in the previous chapter.
    Internet connectivity will be required to download the dataset from the UCI repository.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于逻辑回归是线性分类器，它假设独立变量与对数几率之间是线性关系。因此，在独立特征与对数几率之间存在线性依赖的情况下，模型表现非常好。可以将高阶特征包含在模型中，以捕获非线性行为。让我们看看如何使用前一章讨论的主要深度学习包构建逻辑回归模型。需要互联网连接才能从
    UCI 仓库下载数据集。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this chapter, the Occupancy Detection dataset from the **UC Irivine ML repository**
    is used to build models on logistic regression and neural networks. It is an experimental
    dataset primarily used for binary classification to determine whether a room is
    occupied (1) or not occupied (0) based on multivariate predictors as described
    in the following table. The contributor of the dataset is *Luis Candanedo* from
    UMONS.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将使用**UC Irivine ML库**中的占用检测数据集，构建逻辑回归和神经网络模型。该数据集是一个实验性数据集，主要用于二元分类，判断一个房间是否被占用（1）或未被占用（0），基于多变量预测因子，如下表所示。该数据集的贡献者是*Luis
    Candanedo*，来自UMONS。
- en: Download the dataset at [https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据集：[https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+)。
- en: There are three datasets tobe downloaded; however, we will use `datatraining.txt`
    for training/cross validation purposes and `datatest.txt` for testing purposes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个数据集需要下载；然而，我们将使用`datatraining.txt`进行训练/交叉验证，并使用`datatest.txt`进行测试。
- en: 'The dataset has seven attributes (including response occupancy) with 20,560
    instances. The following table summarizes the attribute information:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含七个属性（包括响应变量占用情况），共20,560个实例。以下表格总结了属性信息：
- en: '| **Attribute** | **Description** | **Characteristic** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **属性** | **描述** | **特性** |'
- en: '| Date time | Year-month-day hour:minute:second format | Date |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 日期时间 | 年-月-日 时:分:秒格式 | 日期 |'
- en: '| Temperature | In Celsius | Real |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 单位为摄氏度（Celsius） | 实际值 |'
- en: '| Relative Humidity | In % | Real |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 相对湿度 | 单位为百分比（%） | 实际值 |'
- en: '| Light | In Lux | Real |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 光照 | 单位为勒克斯（Lux） | 实际值 |'
- en: '| CO2 | In ppm | Real |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| CO2 | 单位为ppm | 实际值 |'
- en: '| Humidity ratio | Derived quantity from temperature and relative humidity,
    in kg water-vapor/kg-air | Real |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 湿度比 | 由温度和相对湿度推导而来，单位为水蒸气/空气，单位为kg/kg | 实际值 |'
- en: '| Occupancy | 0 for not occupied;1 for occupied | Binary class |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 占用状态 | 0表示未占用；1表示已占用 | 二元类别 |'
- en: Performing logistic regression using H2O
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O执行逻辑回归
- en: '**Generalized linear models** (**GLM**) are widely used in both regression-
    and classification-based predictive analysis. These models optimize using maximum
    likelihood and scale well with larger datasets. In H2O, GLM has the flexibility
    to handle both L1 and L2 penalties (including elastic net). It supports Gaussian,
    Binomial, Poisson, and Gamma distributions of dependent variables. It is efficient
    in handling categorical variables, computing full regularizations, and performing
    distributed *n-fold* cross validations to control for model overfitting. It has
    a feature to optimize hyperparameters such as elastic net (α) using distributed
    grid searches along with handling upper and lower bounds for predictor attribute
    coefficients. It can also handle automatic missing value imputation. It uses the
    Hogwild method for optimization, a parallel version of stochastic gradient descent.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**广义线性模型**（**GLM**）广泛应用于基于回归和分类的预测分析。这些模型通过最大似然法进行优化，并能够很好地扩展至更大的数据集。在H2O中，GLM具有处理L1和L2惩罚（包括弹性网）的灵活性。它支持高斯、二项、泊松和伽玛分布的因变量。它在处理分类变量、计算完整的正则化并执行分布式*n折*交叉验证以防止模型过拟合方面非常高效。它具有优化超参数的功能，如使用分布式网格搜索来优化弹性网（α），并处理预测属性系数的上下界。它还可以自动处理缺失值填充。它采用Hogwild方法进行优化，这是随机梯度下降的并行版本。'
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The previous chapter provided the details for the installation of H2O in R
    along with a working example using its web interface. To start modeling, load
    the `h20` package in the R environment:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章提供了H2O在R中的安装细节，并展示了使用其Web界面的工作示例。要开始建模，请在R环境中加载`h20`包：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, initialize a single-node H2O instance using the `h2o.init()` function
    on eight cores and instantiate the corresponding client module on the IP address
    `localhost` and port number `54321`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`h2o.init()`函数初始化一个单节点的H2O实例，运行在八个核心上，并在IP地址`localhost`和端口号`54321`上实例化相应的客户端模块：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The H2O package has dependency on the Java JRE. Thus, it should be pre-installed
    before executing the initialization command.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: H2O包依赖Java JRE。因此，必须在执行初始化命令之前预先安装它。
- en: How to do it...
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: The section will demonstrate steps to build the GLM model using H2O.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将演示使用H2O构建GLM模型的步骤。
- en: 'Now, load the occupancy train and test datasets in R:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在R中加载占用训练集和测试集数据集：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following independent (`x`) and dependent (`y`) variables will be used
    to model GLM:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下独立变量（`x`）和因变量（`y`）将用于建模GLM：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Based on the requirement for H2O, convert the dependent variables into factors
    as follows:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据H2O的要求，将因变量转换为因子，如下所示：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, convert the datasets to H2OParsedData objects:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将数据集转换为H2OParsedData对象：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the data is loaded and converted to H2OParsedData objects, run a GLM model
    using the `h2o.glm` function. In the current setup, we intend to train for parameters
    such as five-fold cross validation, elastic net regularization (*α = 5*), and
    optimal regularization strength (with `lamda_search = TRUE`):'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据被加载并转换为H2OParsedData对象，使用`h2o.glm`函数运行GLM模型。在当前的设置中，我们计划训练五折交叉验证、弹性网正则化（*α
    = 5*）和最优正则化强度（`lamda_search = TRUE`）等参数：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In addition to the preceding command, you can also define other parameters to
    fine-tune the model performance. The following list does not cover all the functional
    parameters, but covers some based on importance. The complete list of parameters
    can be found in the documentation of the `h2o` package.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了前面的命令外，您还可以定义其他参数来微调模型性能。以下列表未涵盖所有功能参数，而是根据重要性涵盖了一些。完整的参数列表可以在`h2o`包的文档中找到。
- en: Specify the strategy of generating cross-validation samples such as random sampling,
    stratified sampling, modulo sampling, and auto (select) using fold_assignment.
    The sampling can also be performed on a particular attribute by specifying the
    column name (fold_column).
  id: totrans-65
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用fold_assignment指定生成交叉验证样本的策略，如随机抽样、分层抽样、模数抽样和自动（选择）。也可以通过指定列名（fold_column）在特定属性上进行采样。
- en: Option to handle skewed outcomes (imbalanced data) by specifying weights to
    each observation using weights_column or performing over/under sampling using
    balance_classes.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为每个观测值指定权重（weights_column）或执行过/欠采样（balance_classes）来处理倾斜结果（不平衡数据）的选项。
- en: Option to handle missing values by mean imputation or observation skip using
    missing_values_handling.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过均值插补或跳过观测值的方式处理缺失值的选项，使用missing_values_handling。
- en: Option to restrict the coefficients to be non-negative using non_negative and
    constrain their values using beta_constraints.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用non_negative选项限制系数为非负，并使用beta_constraints约束其值的选项。
- en: Option to provide prior probability for y==1(logistic regression) in the case
    of sampled data if its mean of response does not reflect the reality (prior).
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果响应变量的均值未反映现实，可以为y==1（逻辑回归）提供先验概率（prior）以用于抽样数据。
- en: Specify the variables to be considered for interactions (interactions).
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定需要考虑交互作用（interactions）的变量。
- en: How it works...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The performance of the model can be assessed using many metrics such as accuracy,
    **Area under curve** (**AUC**), misclassification error (%), misclassification
    error count, F1-score, precision, recall, specificity, and so on. However, in
    this chapter, the assessment of model performance is based on AUC.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能可以通过多种指标进行评估，例如准确率、**曲线下面积**（**AUC**）、误分类错误（%）、误分类错误计数、F1分数、精度、召回率、特异性等。然而，在本章中，模型性能的评估是基于AUC的。
- en: 'The following is the training and cross validation accuracy of the trained
    model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练模型的训练和交叉验证准确度：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s assess the performance of the model on test data. The following
    code helps in predicting the outcome of the test data:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型在测试数据上的表现。以下代码帮助预测测试数据的结果：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, evaluate the `AUC` value based on the actual test outcome as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据实际测试结果评估`AUC`值，如下所示：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In H2O, one can also compute variable importance from the GLM model, as shown
    in the figure following this command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O中，还可以从GLM模型中计算变量的重要性，如下图所示：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/00130.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: Variable importance using H2O
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用H2O进行变量重要性评估
- en: See also
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: More functional parameters for `h2o.glm` can be found at [https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm](https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于`h2o.glm`的功能参数可以参考[https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm](https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm)。
- en: Performing logistic regression using TensorFlow
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow执行逻辑回归
- en: In this section, we will cover the application of TensorFlow in setting up a
    logistic regression model. The example will use a similar dataset to that used
    in the H2O model setup.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍TensorFlow在设置逻辑回归模型中的应用。示例将使用与H2O模型设置中类似的数据集。
- en: Getting ready
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The previous chapter provided details for the installation of TensorFlow. The
    code for this section is created on Linux but can be run on any operating system.
    To start modeling, load the `tensorflow` package in the environment. R loads the
    default TensorFlow environment variable and also the NumPy library from Python
    in the `np` variable:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章提供了 TensorFlow 安装的详细信息。本节的代码是在 Linux 上创建的，但可以在任何操作系统上运行。要开始建模，请在环境中加载 `tensorflow`
    包。R 会加载默认的 TensorFlow 环境变量，还会从 Python 中加载 NumPy 库至 `np` 变量：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How to do it...
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: The data is imported using a standard function from R, as shown in the following
    code.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据使用 R 中的标准函数导入，如以下代码所示。
- en: 'The data is imported using the `read.csv` file and transformed into the matrix
    format followed by selecting the features used to model as defined in `xFeatures`
    and `yFeatures`*.* The next step in TensorFlow is to set up a graph to run optimization:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据使用 `read.csv` 文件导入，并转换为矩阵格式，接着选择用于建模的特征，如 `xFeatures` 和 `yFeatures`*.* 接下来的步骤是在
    TensorFlow 中设置一个图形以进行优化：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Before setting up the graph, let''s reset the graph using the following command:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置图形之前，让我们使用以下命令重置图形：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Additionally, let''s start an interactive session as it will allow us to execute
    variables without referring to the session-to-session object:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，让我们启动一个交互式会话，这样可以在不引用会话间对象的情况下执行变量：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the logistic regression model in TensorFlow:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义逻辑回归模型：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The input feature `x` is defined as a constant as it will be an input to the
    system. The weight `W` and bias `b` are defined as variables that will be optimized
    during the optimization process. The y is set up as a symbolic representation
    between `x`, `W`, and `b`. The weight `W` is set up to initialize random uniform
    distribution and `b` is assigned the value zero.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入特征 `x` 被定义为常量，因为它将作为系统的输入。权重 `W` 和偏置 `b` 被定义为变量，在优化过程中会进行优化。`y` 被设置为 `x`、`W`
    和 `b` 之间的符号表示。权重 `W` 设置为初始化的随机均匀分布，而 `b` 的值被赋为零。
- en: 'The next step is to set up the cost function for logistic regression:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是设置逻辑回归的成本函数：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The variable `y_` is the response variable. Logistic regression is set up using
    cross entropy as the loss function. The loss function is passed to the gradient
    descent optimizer with a learning rate of 0.15\. Before running the optimization,
    initialize the global variables:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 `y_` 是响应变量。逻辑回归使用交叉熵作为损失函数进行设置。损失函数被传递给梯度下降优化器，学习率为 0.15。优化之前，初始化全局变量：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Execute the gradient descent algorithm for the optimization of weights using
    cross entropy as the loss function:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度下降算法，通过交叉熵作为损失函数优化权重：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The performance of the model can be evaluated using AUC:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 AUC 来评估模型的性能：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: AUC can be visualized using the `plot.auc` function from the `pROC` package,
    as shown in the screenshot following this command. The performance for training
    and testing (hold-out) is very similar.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: AUC 可以使用来自 `pROC` 包的 `plot.auc` 函数进行可视化，如以下命令的截图所示。训练和测试（hold-out）的性能非常相似。
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/00010.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00010.jpeg)'
- en: Performance of logistic regression using TensorFlow
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 执行逻辑回归的性能
- en: Visualizing TensorFlow graphs
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化 TensorFlow 图
- en: TensorFlow graphs can be visualized using TensorBoard. It is a service that
    utilizes TensorFlow event files to visualize TensorFlow models as graphs. Graph
    model visualization in TensorBoard is also used to debug TensorFlow models.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 TensorBoard 来可视化 TensorFlow 图。它是一个利用 TensorFlow 事件文件来可视化 TensorFlow 模型为图形的服务。TensorBoard
    中的图形模型可视化还可以用于调试 TensorFlow 模型。
- en: Getting ready
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'TensorBoard can be started using the following command in the terminal:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在终端中使用以下命令启动 TensorBoard：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following are the major parameters for TensorBoard:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 TensorBoard 的主要参数：
- en: '`--logdir` : To map to the directory to load TensorFlow events'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--logdir`：映射到加载 TensorFlow 事件的目录'
- en: '`--debug`: To increase log verbosity'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--debug`：增加日志的详细程度'
- en: '`--host`: To define the host to listen to its localhost (`127.0.0.1`) by default'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--host`：定义主机以监听其本地主机（`127.0.0.1`）默认地址'
- en: '`--port`: To define the port to which TensorBoard will serve'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--port`：定义 TensorBoard 提供服务的端口'
- en: 'The preceding command will launch the TensorFlow service on localhost at port
    `6006`, as shown in the following screenshot:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将在本地主机的 `6006` 端口启动 TensorFlow 服务，如下图所示：
- en: '![](img/00132.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpeg)'
- en: TensorBoard
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard
- en: The tabs on the TensorBoard capture relevant data generated during graph execution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 中的标签捕捉在图执行过程中生成的相关数据。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: The section covers how to visualize TensorFlow models and output in TernsorBoard.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何在 TensorBoard 中可视化 TensorFlow 模型和输出。
- en: 'To visualize summaries and graphs, data from TensorFlow can be exported using
    the `FileWriter` command from the summary module. A default session graph can
    be added using the following command:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了可视化摘要和图表，可以使用来自总结模块的`FileWriter`命令将 TensorFlow 的数据导出。可以使用以下命令添加默认的会话图：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The graph for logistic regression developed using the preceding code is shown
    in the following screenshot:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述代码开发的逻辑回归图如下图所示：
- en: '![](img/00067.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpeg)'
- en: Visualization of the logistic regression graph in TensorBoard
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorBoard 中可视化逻辑回归图
- en: Details about symbol descriptions on TensorBoard can be found at [https://www.tensorflow.org/get_started/graph_viz](https://www.tensorflow.org/get_started/graph_viz).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 TensorBoard 上符号描述的详细信息可以在[https://www.tensorflow.org/get_started/graph_viz](https://www.tensorflow.org/get_started/graph_viz)找到。
- en: 'Similarly, other variable summaries can be added to the TensorBoard using correct
    summaries, as shown in the following code:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，其他变量摘要可以使用正确的摘要添加到 TensorBoard，如以下代码所示：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The summaries can be a very useful way to determine how the model is performing.
    For example, for the preceding case, the cost function for test and train can
    be studied to understand optimization performance and convergence.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要是判断模型性能的非常有用的方式。例如，对于前述案例，可以通过研究测试和训练的成本函数来理解优化性能和收敛情况。
- en: 'Create a cross entropy evaluation for test. An example script to generate the
    cross entropy cost function for test and train is shown in the following command:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为测试创建交叉熵评估。生成测试和训练的交叉熵成本函数的示例脚本如下所示：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code is similar to training cross entropy calculations with a
    different dataset. The effort can be minimized by setting up a function to return
    tensor objects.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码与使用不同数据集进行训练交叉熵计算相似。通过设置一个函数返回张量对象，可以将工作量最小化。
- en: 'Add summary variables to be collected:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加要收集的摘要变量：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The script defines the summary events to be logged in the file.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本定义了要记录在文件中的总结事件。
- en: 'Open the writing object, `log_writer`. It writes the default graph to the location,
    `c:/log`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开写入对象`log_writer`。它将默认图写入位置`c:/log`：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Run the optimization and collect the summaries:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行优化并收集摘要：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Collect all the summaries to a single tensor using the`merge_all` command from
    the summary module:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用总结模块的`merge_all`命令将所有摘要收集到一个张量中：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Write the summaries to the log file using the `log_writer` object:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`log_writer`对象将摘要写入日志文件：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The section covers model performance visualization using TensorBoard. The cross
    entropy for train and test are recorded in the SCALARS tab, as shown in the following
    screenshot:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用 TensorBoard 可视化模型性能。训练和测试的交叉熵记录在 SCALARS 标签中，如以下屏幕截图所示：
- en: '![](img/00011.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: Cross entropy for train and test data
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试数据的交叉熵
- en: The objective function shows similar behaviors for train and test cost function;
    thus, the model seems to be stable for the given case with convergence attaining
    around 1,600 iterations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数在训练和测试成本函数上表现出类似的行为；因此，对于给定的案例，模型似乎是稳定的，且收敛大约在1,600次迭代时达到。
- en: Starting with multilayer perceptrons
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从多层感知机开始
- en: This section will focus on extending the logistic regression concept to neural
    networks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点介绍将逻辑回归概念扩展到神经网络。
- en: The neural network, also known as **Artificial neural network** (**ANN**), is
    a computational paradigm that is inspired by the neuronal structure of the biological
    brain.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，也称为**人工神经网络**（**ANN**），是一种受生物大脑神经结构启发的计算范式。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The ANN is a collection of artificial neurons that perform simple operations
    on the data; the output from this is passed to another neuron. The output generated
    at each neuron is called its **activation function**. An example of a multilayer
    perceptron model can be seen in the following screenshot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是一组执行数据简单操作的人工神经元；这些操作的输出传递给另一个神经元。在每个神经元生成的输出称为其**激活函数**。多层感知机模型的示例可以在以下屏幕截图中看到：
- en: '![](img/00012.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: An example of a multilayer neural network
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络的示例
- en: 'Each link in the preceding figure is associated to weights processed by a neuron.
    Each neuron can be looked at as a processing unit that takes input processing
    and the output is passed to the next layer, as shown in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图中的每一条连接都与神经元处理的权重相关。每个神经元可以看作是一个处理单元，接收输入并将输出传递给下一层，示例如下图所示：
- en: '![](img/00101.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: An example of a neuron getting three inputs and one output
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元接收三个输入并输出一个结果的示例
- en: The preceding figure demonstrates three inputs combined at neuron to give an
    output that may be further passed to another neuron. The processing conducted
    at the neuron could be a very simple operation such as the input multiplied by
    weights followed by summation or a transformation operation such as the sigmoid
    activation function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了三个输入合并到神经元中，产生一个输出，之后可能传递给另一个神经元。神经元的处理操作可以是非常简单的操作，例如输入乘以权重再求和，或者是一个变换操作，例如Sigmoid激活函数。
- en: How to do it...
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'This section covers type activation functions in multilayer perceptrons. Activation
    is one of the critical component of ANN as it defines the output of that node
    based on the given input. There are many different activation functions used while
    building a neural network:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了多层感知机中的激活函数类型。激活函数是人工神经网络中的关键组件之一，它根据给定输入定义节点的输出。在构建神经网络时，使用了多种不同的激活函数：
- en: '**Sigmoid**: The sigmoid activation function is a continuous function also
    known as a logistic function and has the form, *1/(1+exp(-x))*. The sigmoid function
    has a tendency to zero out the backpropagation terms during training leading to
    saturation in response. In TensorFlow, the sigmoid activation function is defined
    using the `tf.nn.sigmoid` function.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：Sigmoid激活函数是一个连续函数，也称为逻辑函数，形式为 *1/(1+exp(-x))*。Sigmoid函数在训练过程中有使反向传播项归零的倾向，导致响应饱和。在TensorFlow中，Sigmoid激活函数使用
    `tf.nn.sigmoid` 函数定义。'
- en: '**ReLU**: Rectified linear unit (ReLU) is one of the most famous continuous,
    but not smooth, activation functions used in neural networks to capture non-linearity.
    The ReLU function is defined as *max(0,x)*. In TensorFlow, the ReLU activation
    function is defined as `tf.nn.relu`*.*'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU**：修正线性单元（ReLU）是最著名的连续但不光滑的激活函数之一，用于神经网络中捕捉非线性。ReLU函数的定义是 *max(0,x)*。在TensorFlow中，ReLU激活函数定义为
    `tf.nn.relu`*。'
- en: '**ReLU6**: It caps the ReLU function at 6 and is defined as *min(max(0,x),
    6)*, thus the value does not become very small or large. The function is defined
    in TensorFlow as `tf.nn.relu6`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU6**：它将ReLU函数的输出上限限制为6，定义为 *min(max(0,x), 6)*，因此值不会变得过小或过大。该函数在TensorFlow中的定义是
    `tf.nn.relu6`。'
- en: '**tanh**: Hypertangent is another smooth function used as an activation function
    in neural networks and is bound [ -1 to 1] and implemented as `tf.nn.tanh`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tanh**：双曲正切函数是另一种平滑函数，作为神经网络中的激活函数使用，其值域为[ -1 到 1]，并通过 `tf.nn.tanh` 实现。'
- en: '**softplus**: It is a continuous version of ReLU, so the differential exists
    and is defined as *log(exp(x)+1)*. In TensorFlow the softplus is defined as `tf.nn.softplus`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**softplus**：它是ReLU的连续版本，因此其导数存在，定义为 *log(exp(x)+1)*。在TensorFlow中，softplus函数定义为
    `tf.nn.softplus`。'
- en: There's more...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'There are three main neural network architectures in neural networks:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中有三种主要的网络架构：
- en: '**Feedforward ANN**: This is a class of neural network models where the flow
    of information is unidirectional from input to output; thus, the architecture
    does not form any cycle. An example of a Feedforward network is shown in the following
    image:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈型人工神经网络**：这是一种神经网络模型，信息流从输入到输出是单向的，因此架构不会形成任何循环。以下图展示了一个前馈型网络的示例：'
- en: '![](img/00013.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: Feedforward architecture of neural networks
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的前馈架构
- en: '**Feedback ANN**: This is also known as the Elman recurrent network and is
    a class of neural networks where the error at the output node used as feedback
    to update iteratively to minimize errors. An example of a one layer Feedback neural
    network architecture is shown in the following image:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反馈型人工神经网络**：这也被称为Elman递归网络，是一种神经网络类型，其中输出节点的误差作为反馈，迭代更新以最小化误差。以下图展示了一个单层反馈型神经网络架构的示例：'
- en: '![](img/00014.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)'
- en: xFeedback architecture of neural networks
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的反馈架构
- en: '**Lateral ANN**: This is a class of neural networks between Feedback and Feedforward
    neural networks with neurons interacting within layers. An example lateral neural
    network architecture is shown in the following image:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**横向ANN**：这是一类神经网络，介于反馈神经网络和前馈神经网络之间，神经元在层之间相互作用。以下是一个横向神经网络架构的示例：'
- en: '![](img/00144.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00144.jpeg)'
- en: Lateral neural network architecture
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 横向神经网络架构
- en: See also
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: More activation functions supported in TensorFlow can be found at [https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_.](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中支持的更多激活函数可以在[https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_)找到。
- en: Setting up a neural network using H2O
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用H2O设置神经网络
- en: In this section, we will cover the application of H2O in setting up a neural
    network. The example will use a similar dataset as used in logistic regression.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍如何使用H2O设置神经网络。该示例将使用与逻辑回归中类似的数据集。
- en: Getting ready
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We first load all the required packages with the following code:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用以下代码加载所有所需的包：
- en: '[PRE30]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, initialize a single-node H2O instance using the `h2o.init()` function
    on eight cores and instantiate the corresponding client module on the IP address
    `localhost` and port number `54321`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`h2o.init()`函数在八个核心上初始化一个单节点H2O实例，并在IP地址`localhost`和端口号`54321`上实例化相应的客户端模块：
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How to do it...
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The section shows how to build neural network using H20.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何使用H20构建神经网络。
- en: 'Load the occupancy train and test datasets in R:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在R中加载占用训练和测试数据集：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following independent (`x`) and dependent (`y`) variables will be used
    to model GLM:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下独立变量（`x`）和因变量（`y`）将用于建模GLM：
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Based on the requirement by H2O, convert dependent variables to factors as
    follows:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据H2O的要求，将因变量转换为因子，方法如下：
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then convert the datasets to H2OParsedData objects:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将数据集转换为H2OParsedData对象：
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once the data is loaded and converted to H2OParsedData objects, build a multilayer
    Feedforward neural network using the `h2o.deeplearning` function. In the current
    setup, the following parameters are used to build the NN model:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据加载并转换为H2OParsedData对象，使用`h2o.deeplearning`函数构建多层前馈神经网络。在当前设置中，使用以下参数构建NN模型：
- en: Single hidden layer with five neurons using `hidden`
  id: totrans-207
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`hidden`创建一个包含五个神经元的单隐藏层
- en: 50 iterations using `epochs`
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`epochs`进行50次迭代
- en: Adaptive learning rate (`adaptive_rate`) instead of a fixed learning rate (rate)
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自适应学习率（`adaptive_rate`）代替固定学习率（rate）
- en: '`Rectifier` activation function based on ReLU'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于ReLU的`Rectifier`激活函数
- en: Five-fold cross validation using `nfold`
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`nfold`进行五折交叉验证
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In addition to the command described in the recipe *Performing logistic regression
    using H2O*, you can also define other parameters to fine-tune the model performance.
    The following list does not cover all the functional parameters, but covers some
    based on importance. The complete list of parameters is available in the documentation
    of the `h2o` package.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了配方中描述的*使用H2O执行逻辑回归*命令外，你还可以定义其他参数来微调模型性能。以下列表未覆盖所有功能参数，但涵盖了一些重要的参数。完整的参数列表可以在`h2o`包的文档中找到。
- en: Option to initialize a model using a pretrained autoencoder model.
  id: totrans-214
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选项可以使用预训练的自编码器模型初始化模型。
- en: Provision to fine-tune the adaptive learning rate via an option to modify the
    time decay factor (*rho*) and smoothing factor (*epsilon*). In the case of a fixed
    learning rate (*rate*), an option to modify the annealing rate (*rate_annealing*)
    and decay factor between layers (*rate_decay*).
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过修改时间衰减因子（*rho*）和平滑因子（*epsilon*）的选项来微调自适应学习率。对于固定学习率（*rate*），可以选择修改退火率（*rate_annealing*）和层之间的衰减因子（*rate_decay*）。
- en: Option to initialize weights and biases along with weight distribution and scaling.
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化权重和偏差的选项，以及权重分布和缩放。
- en: Stopping criteria based on the error fraction in the case of classification
    and mean squared errors with regard to regression (*classification_stop* and *regression_stop*).
    An option to also perform early stopping.
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分类中的错误比例和回归中的均方误差的停止标准（*classification_stop* 和 *regression_stop*）。还可以选择执行早期停止。
- en: Option to improve distributed model convergence using the elastic averaging
    method with parameters such as moving rate and regularization strength.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用弹性平均方法改进分布式模型收敛的选项，包括移动速率和正则化强度等参数。
- en: How it works...
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The performance of the model can be assessed using many metrics such as accuracy,
    AUC, misclassification error (%), misclassification error count, F1-score, precision,
    recall, specificity, and so on. However, in this chapter, the assessment of the
    model performance is based on AUC.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能可以通过多种指标进行评估，例如准确率、AUC、误分类错误率（%）、误分类错误计数、F1 分数、精确度、召回率、特异性等。然而，在本章中，模型性能的评估是基于
    AUC。
- en: 'The following is the training and cross validation accuracy for the trained
    model. The training and cross validation AUC is `0.984` and `0.982` respectively:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练模型的训练集和交叉验证准确度。训练集和交叉验证 AUC 分别为 `0.984` 和 `0.982`：
- en: '[PRE37]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we have already provided test data in the model (as a validation dataset),
    the following is its performance. The AUC on the test data is `0.991`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经在模型中提供了测试数据（作为验证数据集），下面是其性能。测试数据集上的 AUC 为 `0.991`。
- en: '[PRE38]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Tuning hyper-parameters using grid searches in H2O
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 H2O 中使用网格搜索调优超参数
- en: H2O packages also allow you to perform hyper-parameter tuning using grid search
    (`h2o.grid`).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 包还允许通过网格搜索（`h2o.grid`）执行超参数调优。
- en: Getting ready
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'We first load and initialize the H2O package with the following code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载并初始化 H2O 包，代码如下：
- en: '[PRE39]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The occupancy dataset is loaded, converted to hex format, and named *occupancy_train.hex*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 载入占用数据集，转换为十六进制格式，并命名为 *occupancy_train.hex*。
- en: How to do it...
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: The section will focus on optimizing hyper parameters in H2O using grid searches.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点介绍如何使用网格搜索优化 H2O 中的超参数。
- en: 'In our case, we will optimize for the activation function, the number of hidden
    layers (along with the number of neurons in each layer), `epochs`, and regularization
    lambda (`l1` and `l2`):'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将优化激活函数、隐藏层数量（以及每层中的神经元数量）、`epochs` 和正则化参数 lambda（`l1` 和 `l2`）：
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following search criteria have been set to perform a grid search. Adding
    to the following list, one can also specify the type of stopping metric, the minimum
    tolerance for stopping, and the maximum number of rounds for stopping:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是设置进行网格搜索的搜索标准。除了以下列表外，还可以指定停止度量的类型、停止的最小容忍度和最大停止轮次：
- en: '[PRE41]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, let''s perform a grid search on the training data as follows:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对训练数据执行网格搜索，具体如下：
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Once the grid search is complete (here, there are 216 different models), the
    best model can be selected based on multiple metrics such as logloss, residual
    deviance, mean squared error, AUC, accuracy, precision, recall, f1, and so on.
    In our scenario, let''s select the best model with the highest AUC:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦网格搜索完成（此处共有 216 个不同的模型），可以根据多个指标选择最佳模型，例如对数损失、残差偏差、均方误差、AUC、准确率、精确度、召回率、F1
    分数等。在我们的场景中，选择 AUC 最高的最佳模型：
- en: '[PRE43]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The following is the performance of the grid-searched model on both the training
    and cross-validation datasets. We can observe that the AUC has increased by one
    unit in both training and cross-validation scenarios, after performing a grid
    search. The training and cross validation AUC after the grid search is `0.996`
    and `0.997` respectively.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是网格搜索模型在训练集和交叉验证数据集上的表现。我们可以观察到，在执行网格搜索后，训练集和交叉验证数据集的 AUC 都增加了一个单位。网格搜索后的训练集和交叉验证
    AUC 分别为 `0.996` 和 `0.997`。
- en: '[PRE44]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now, let's assess the performance of the best grid-searched model on the test
    dataset. We can observe that the AUC has increased by 0.25 units after performing
    the grid search. The AUC on the test data is `0.993`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估在测试数据集上进行网格搜索后最佳模型的性能。我们可以观察到，在执行网格搜索后，AUC 增加了 0.25 个单位。测试数据集上的 AUC
    为 `0.993`。
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Setting up a neural network using MXNet
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MXNet 设置神经网络
- en: The previous chapter provided the details for the installation of MXNet in R
    along with a working example using its web interface. To start modeling, load
    the MXNet package in the R environment.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章提供了在 R 中安装 MXNet 的详细信息，并通过其 Web 界面展示了一个工作示例。要开始建模，请在 R 环境中加载 MXNet 包。
- en: Getting ready
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Load the required packages:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 加载所需的包：
- en: '[PRE46]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How to do it...
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Load the occupancy train and test datasets in R:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 R 中加载占用训练集和测试集数据集：
- en: '[PRE47]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following independent (`x`) and dependent (`y`) variables will be used
    to model GLM:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是用于建模 GLM 的独立变量（`x`）和依赖变量（`y`）：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Based on the requirement by MXNet, convert the train and test datasets to a
    matrix and ensure that the class of the outcome variable is numeric (instead of
    factor as in the case of H2O):'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 MXNet 的要求，将训练集和测试集数据集转换为矩阵，并确保结果变量的类别是数值型（而不是像 H2O 中那样的因子型）：
- en: '[PRE49]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now, let's configure a neural network manually. First, configure a symbolic
    variable with a specific name. Then configure a symbolic fully connected network
    with five neurons in a single hidden layer followed with the softmax activation
    function with logit loss (or cross entropy loss). One can also create additional
    (fully connected) hidden layers with different activation functions.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们手动配置一个神经网络。首先，配置一个具有特定名称的符号变量。然后，配置一个包含五个神经元的符号全连接网络，并跟随使用对数损失函数（或交叉熵损失函数）的
    softmax 激活函数。还可以创建额外的（全连接的）隐藏层，并使用不同的激活函数。
- en: '[PRE50]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Once the neural network is configured, let''s create (or train) the (Feedforward)
    neural network model using the `mx.model.FeedForward.create` function. The model
    is fine-tuned for parameters such as the number of iterations or epochs (*100*),
    the metric for evaluation (classification accuracy), the size of each iteration
    or epoch (100 observations), and the learning rate (*0.01*):'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦神经网络配置完成，接下来我们使用 `mx.model.FeedForward.create` 函数创建（或训练）前馈神经网络模型。模型的参数如迭代次数或周期数（*100*）、评估指标（分类准确率）、每次迭代或周期的大小（100
    个观察值）以及学习率（*0.01*）被调整：
- en: '[PRE51]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Now, let''s assess the performance of the model on train and test datasets.
    The AUC on the train data is `0.978` and on the test data is `0.982`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型在训练和测试数据集上的表现。训练数据集上的 AUC 为 `0.978`，测试数据集上的 AUC 为 `0.982`：
- en: '[PRE52]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Setting up a neural network using TensorFlow
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 设置神经网络
- en: In this section, we will cover an application of TensorFlow in setting up a
    two-layer neural network model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 TensorFlow 在设置二层神经网络模型中的应用。
- en: Getting ready
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To start modeling, load the `tensorflow` package in the environment. R loads
    the default tf environment variable and also the NumPy library from Python in
    the `np` variable:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始建模，在环境中加载 `tensorflow` 包。R 会加载默认的 tf 环境变量，并从 Python 中加载 NumPy 库到 `np` 变量：
- en: '[PRE53]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How to do it...
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The data is imported using the standard function from R, as shown in the following
    code. The data is imported using the `read.csv` file and transformed into the
    matrix format followed by selecting the features used for the modeling as defined
    in `xFeatures` and `yFeatures`*:*
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '数据通过 R 中的标准函数导入，如以下代码所示。数据通过 `read.csv` 文件导入，并转换为矩阵格式，接着选择用于建模的特征，这些特征在 `xFeatures`
    和 `yFeatures` 中定义*：* '
- en: '[PRE54]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now load both the network and model parameters. The network parameters define
    the structure of the neural network and the model parameters define its tuning
    criteria. As stated earlier, the neural network is built using two hidden layers,
    each with five neurons. The `n_input` parameter defines the number of independent
    variables and `n_classes` defines one fewer than the number of output classes.
    In cases where the output variable is one-hot encoded (one attribute with yes
    occupancy and a second attribute with no occupancy), then `n_classes` will be
    2L (equal to the number of one-hot encoded attributes). Among model parameters,
    the learning rate is `0.001` and the number of epochs (or iterations) for model
    building is `10000`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在加载网络和模型参数。网络参数定义了神经网络的结构，模型参数定义了其调整标准。如前所述，神经网络是通过两层隐藏层构建的，每一层有五个神经元。`n_input`
    参数定义了自变量的数量，`n_classes` 定义了输出类别数量减一的值。在输出变量为 one-hot 编码的情况下（一个属性为占用，另一个属性为不占用），`n_classes`
    将为 2L（等于 one-hot 编码属性的数量）。在模型参数中，学习率为 `0.001`，模型构建的周期数（或迭代次数）为 `10000`：
- en: '[PRE55]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The next step in TensorFlow is to set up a graph to run the optimization. Before
    setting up the graph, let''s reset the graph using the following command:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中的下一步是设置一个图来执行优化。在设置图之前，使用以下命令重置图：
- en: '[PRE56]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Additionally, let''s start an interactive session as it will allow us to execute
    variables without referring to the session-to-session object:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，让我们启动一个交互式会话，因为它将允许我们在不引用会话之间对象的情况下执行变量：
- en: '[PRE57]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The following script defines the graph input (x for independent variables and
    y for dependent variable). The input feature `x` is defined as a constant as it
    will be input to the system. Similarly, the output feature `y` is also defined
    as a constant with the `float32` type:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下脚本定义了图的输入（x 为自变量，y 为因变量）。输入特征 `x` 被定义为常量，因为它将作为输入提供给系统。类似地，输出特征 `y` 也被定义为常量，并具有
    `float32` 类型：
- en: '[PRE58]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, let''s create a multilayer perceptron with two hidden layers. Both the
    hidden layers are built using the ReLU activation function and the output layer
    is built using the linear activation function. The weights and biases are defined
    as variables that will be optimized during the optimization process. The initial
    values are randomly selected from a normal distribution. The following script
    is used to initialize and store a hidden layer''s weights and biases along with
    a mulitilayer perceptron model:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个具有两层隐藏层的多层感知器。两层隐藏层都使用ReLU激活函数，输出层使用线性激活函数。权重和偏置被定义为变量，在优化过程中进行优化。初始值是从正态分布中随机选择的。以下脚本用于初始化并存储隐藏层的权重和偏置以及多层感知器模型：
- en: '[PRE59]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, construct the model using the initialized `weights` and `biases`:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用初始化的`weights`和`biases`来构建模型：
- en: '[PRE60]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The next step is to define the `cost` and `optimizer` functions of the neural
    network:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义神经网络的`cost`和`optimizer`函数：
- en: '[PRE61]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The neural network is set up using sigmoid cross entropy as the cost function.
    The cost function is then passed to a gradient descent optimizer (Adam) with a
    learning rate of 0.001\. Before running the optimization, initialize the global
    variables as follows:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络使用Sigmoid交叉熵作为损失函数进行设置。然后，损失函数传递给一个梯度下降优化器（Adam），学习率为0.001。在运行优化之前，按如下方式初始化全局变量：
- en: '[PRE62]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Once the global variables are initialized along with the cost and optimizer
    functions, let''s begin training on the train dataset:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦全局变量与损失函数和优化器函数一起初始化完毕，就开始在训练数据集上进行训练：
- en: '[PRE63]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: How it works...
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The performance of the model can be evaluated using AUC:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用AUC评估模型的性能：
- en: '[PRE64]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: AUC can be visualized using the `plot.auc` function from the `pROC` package,
    as shown in the image following the next command. The performance of train and
    test (hold out) is very similar.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`pROC`包中的`plot.auc`函数来可视化AUC，如下命令执行后显示的图像所示。训练和测试（持出）性能非常相似。
- en: '[PRE65]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](img/00026.jpeg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: Performance of multilayer perceptron using TensorFlow
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow的多层感知器性能
- en: There's more...
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Neural networks are based on the philosophy from the brain; however, the brain
    consists of around 100 billion neurons with each neuron connected to 10,000 other
    neurons. Neural networks developed in the early 90s faced a lot of challenges
    in building deeper neural networks due to computation and algorithmic limitations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络基于大脑的哲学；然而，大脑由约1000亿个神经元组成，每个神经元与10000个其他神经元连接。90年代初期开发的神经网络由于计算和算法限制，面临着构建更深神经网络的许多挑战。
- en: With advances in big data, computational resources (such as GPUs), and better
    algorithms, the concept of deep learning has emerged and allows us to capture
    a deeper representation from all kinds of data such as text, image, audio, and
    so on.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据、计算资源（如GPU）和更好算法的进步，深度学习的概念应运而生，它使我们能够从文本、图像、音频等各种数据中捕捉更深的表示。
- en: '**Trends in Deep Learning**: Deep learning is an advance on neural networks,
    which are very much driven by technology enhancement. The main factors that have
    impacted the development of deep learning as a dominant area in artificial intelligence
    are as follows:'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习的趋势**：深度学习是神经网络的进步，深受技术提升的推动。深度学习作为人工智能主流领域发展的主要因素如下：'
- en: '**Computational power**: The consistency of Moore''s law, which states that
    the acceleration power of hardware will double every two years, helped in training
    more layers and bigger data within time limitations'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算能力**：摩尔定律的持续性，即硬件的加速能力每两年翻一番，帮助在时间限制内训练更多的层次和更大的数据。'
- en: '**Storage and better compression algorithms**: The ability to store big models
    due to cheaper storage and better compression algorithms have pushed this area
    with practitioners focusing on capturing real-time data feeds in the form of image,
    text, audio, and video formats'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储和更好的压缩算法**：由于存储成本降低和更好的压缩算法，存储大模型的能力得到了提升，这推动了该领域的发展，实践者专注于捕捉实时数据流，如图像、文本、音频和视频格式。'
- en: '**Scalability**: The ability to scale from a simple computer to a farm or using
    GPU devices has given a great boost to training deep learning models'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：从简单计算机到农场或使用GPU设备的扩展能力大大促进了深度学习模型的训练。'
- en: '**Deep learning architectures**: With new architectures such as Convolution
    Neural network, re-enforcement learning has provided a boost to what we can learn
    and also helped expedite learning rates'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习架构：** 随着卷积神经网络等新架构的发展，强化学习为我们能够学习的内容提供了推动力，也帮助加速了学习速率。'
- en: '**Cross-platform programming:** The ability to program and build models in
    a cross-platform architecture significantly helped increase the user base and
    in drastic development in the domain'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨平台编程：** 在跨平台架构中进行编程和构建模型的能力显著帮助了用户基础的扩大，并在该领域带来了巨大的发展。'
- en: '**Transfer learning:** This allows reusing pretrained models and further helps
    in significantly reducing training times'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移学习：** 这使得可以重用预训练模型，并进一步帮助显著减少训练时间。'
