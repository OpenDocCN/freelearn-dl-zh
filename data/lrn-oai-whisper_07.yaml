- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Exploring Advanced Voice Capabilities
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索高级语音功能
- en: Welcome to [*Chapter 7*](B21020_07.xhtml#_idTextAnchor177), where we embark
    on an exciting journey to explore the advanced voice capabilities of OpenAI’s
    Whisper. This chapter will dive into techniques that enhance Whisper’s performance,
    such as **quantization**, and uncover its potential for real-time speech recognition.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到[*第7章*](B21020_07.xhtml#_idTextAnchor177)，在这里我们将开始一段激动人心的旅程，探索 OpenAI Whisper
    的高级语音功能。本章将深入探讨提升 Whisper 性能的技术，例如 **量化**，并揭示其在实时语音识别中的潜力。
- en: We begin by examining the power of quantization, a technique that reduces the
    model’s size and computational requirements while maintaining accuracy. You will
    learn how to apply quantization to Whisper using frameworks such as **CTranslate2**
    and **Open Visual Inference and Neural Network Optimization** (**OpenVINO**),
    enabling efficient deployment on resource-constrained devices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查量化的力量，这是一种在保持准确性的同时减少模型大小和计算要求的技术。您将学习如何通过使用 **CTranslate2** 和 **Open
    Visual Inference and Neural Network Optimization**（**OpenVINO**）等框架将量化应用于 Whisper，从而在资源受限的设备上实现高效部署。
- en: While we briefly touched upon the challenges of implementing real-time ASR with
    Whisper in the previous chapter, in this chapter, we will dive deeper into the
    current limitations and ongoing research efforts to make real-time transcription
    a reality. We will explore experimental approaches to building streaming ASR demos
    using Whisper and Gradio, providing hands-on examples to showcase the potential
    of real-time speech recognition with Whisper.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们简要讨论了使用 Whisper 实现实时 ASR（自动语音识别）面临的挑战，而在本章中，我们将深入探讨当前的局限性以及正在进行的研究工作，以使实时转录成为现实。我们将探索使用
    Whisper 和 Gradio 构建流式 ASR 演示的实验性方法，并提供实际示例，展示 Whisper 在实时语音识别中的潜力。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Leveraging the power of quantization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用量化的力量
- en: Facing the challenges and opportunities of real-time speech recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面对实时语音识别的挑战与机遇
- en: By the end of this chapter, you will have a solid understanding of advanced
    techniques to optimize Whisper’s performance and appreciate the potential and
    challenges of real-time speech recognition. You will be equipped with practical
    knowledge and hands-on experience to apply these techniques in your projects,
    pushing the boundaries of what is possible with Whisper.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将深入了解优化 Whisper 性能的高级技术，理解实时语音识别的潜力与挑战。您将掌握实用的知识和动手经验，将这些技术应用到您的项目中，突破
    Whisper 的应用边界。
- en: So, let’s unlock the full potential of Whisper’s advanced voice capabilities,
    enabling you to build innovative applications that transform how we interact with
    spoken language in the digital world.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们解锁 Whisper 高级语音功能的全部潜力，使您能够构建创新应用，改变我们在数字世界中与语言互动的方式。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To harness the capabilities of OpenAI’s Whisper for advanced applications, this
    chapter leverages Python and Google Colab for ease of use and accessibility. The
    Python environment setup includes the Whisper library for transcription tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 OpenAI Whisper 的功能进行高级应用，本章使用 Python 和 Google Colab，旨在简化使用和提高可访问性。Python
    环境设置包括用于转录任务的 Whisper 库。
- en: '**Key requirements**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要求**：'
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If the **T4 GPU** runtime type is available,
    select it for better performance.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab 笔记本**：这些笔记本已经设置好，以最小所需的内存和计算能力运行我们的 Python 代码。如果 **T4 GPU**
    运行时类型可用，请选择它以获得更好的性能。'
- en: '**Python environment**: Each notebook contains directives to load the required
    Python libraries, including Whisper and Gradio.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 环境**：每个笔记本都包含加载所需 Python 库的指令，包括 Whisper 和 Gradio。'
- en: '**Hugging Face account**: Some notebooks require a Hugging Face account and
    login API key. The Colab notebooks include information about this topic.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face 账户**：某些笔记本需要 Hugging Face 账户和登录 API 密钥。Colab 笔记本中包含有关此主题的信息。'
- en: '**Microphone and speakers**: Some notebooks implement a Gradio app with voice
    recording and audio playback. A microphone and speakers connected to your computer
    might help you experience the interactive voice features. Another option is to
    open the URL link Gradio provides at runtime on your mobile phone; from there,
    you might be able to use the phone’s microphone to record your voice.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**麦克风和扬声器**：一些笔记本实现了一个 Gradio 应用，支持语音录制和音频播放。连接到计算机的麦克风和扬声器可以帮助你体验交互式语音功能。另一种选择是通过手机打开
    Gradio 在运行时提供的 URL 链接；在那里，你可以使用手机的麦克风录制语音。'
- en: '**GitHub repository access**: All Python code, including examples, is available
    in the chapter’s GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter07](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter07)).
    These Colab notebooks are ready to run, providing a practical and hands-on approach
    to learning.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 仓库访问**：所有 Python 代码，包括示例，都可以在本章的 GitHub 仓库中找到（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter07](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter07)）。这些
    Colab 笔记本已准备好运行，提供了一个实践性强、动手操作的学习方法。'
- en: By meeting these technical requirements, you will be prepared to explore Whisper
    in different contexts while enjoying the streamlined experience of Google Colab
    and the comprehensive resources available on GitHub.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过满足这些技术要求，你将能够在不同的环境中探索 Whisper，同时享受 Google Colab 提供的流畅体验和 GitHub 上的全面资源。
- en: As we continue our journey into Whisper’s advanced capabilities, we must explore
    techniques to optimize its performance and efficiency. One such technique that
    has gained significant attention is quantization. In this section, we’ll explore
    the power of quantization and how it can be leveraged to enhance Whisper’s deployment
    capabilities.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入了解 Whisper 的高级功能，我们必须探索一些技术，以优化其性能和效率。量化就是其中一个备受关注的技术。在这一部分中，我们将探索量化的强大功能，以及如何利用它来提升
    Whisper 的部署能力。
- en: Leveraging the power of quantization
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用量化的强大功能
- en: 'Quantization in machine learning, particularly in ASR, refers to reducing the
    precision of the model’s parameters. This is typically done by mapping the continuous
    range of floating-point values to a discrete set of values, often represented
    by integers. The primary goal of quantization is to decrease the model’s computational
    complexity and memory footprint, which is crucial for deploying ASR systems on
    devices with limited resources, such as mobile phones or embedded systems. Quantization
    is essential for several reasons:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的量化，特别是在 ASR 中，指的是减少模型参数的精度。通常，通过将连续的浮点值范围映射到一个离散的值集合，通常是整数，来实现量化。量化的主要目标是减少模型的计算复杂性和内存占用，这对于将
    ASR 系统部署到资源有限的设备（如手机或嵌入式系统）上至关重要。量化具有以下几个重要作用：
- en: '**Reducing model size**: Using lower precision to represent the model’s weights
    can significantly reduce the model’s overall size. This is particularly beneficial
    for on-device deployment, where storage space is at a premium.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减小模型尺寸**：使用低精度表示模型的权重可以显著减小模型的整体尺寸。这对于设备端部署尤为重要，因为设备的存储空间有限。'
- en: '**Improving inference speed**: Lower precision arithmetic is faster on many
    hardware platforms, especially those without dedicated floating-point units. This
    can lead to faster inference times, critical for real-time applications such as
    ASR.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高推理速度**：在许多硬件平台上，低精度算术运算比高精度运算更快，特别是那些没有专用浮点单元的硬件。这可以加快推理时间，对于实时应用（如自动语音识别
    ASR）至关重要。'
- en: '**Increasing energy efficiency**: Quantized models require fewer computational
    resources, lowering power consumption. This is essential for battery-powered devices.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高能效**：量化后的模型需要更少的计算资源，从而降低功耗。这对于电池供电的设备至关重要。'
- en: '**Expanding hardware compatibility**: Many edge devices are optimized for integer
    computations. Quantization allows models to leverage these hardware optimizations.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展硬件兼容性**：许多边缘设备对整数计算进行了优化。量化使得模型能够利用这些硬件优化。'
- en: 'Some standard machine-learning quantization techniques in ASR are **vector
    quantization** (**VQ**), **int8 quantization**, and **low-bit quantization**.
    Let’s briefly describe each:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ASR 中，一些标准的机器学习量化技术包括 **向量量化**（**VQ**）、**int8 量化** 和 **低比特量化**。我们简要描述每一种技术：
- en: '*VQ* is a classical technique in various domains, including speech coding and
    recognition. It involves mapping vectors from an ample vector space to a finite
    number of regions, which can be efficiently represented with fewer bits. VQ has
    been successfully applied to speech recognition systems, improving performance
    by efficiently compressing the feature space.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*VQ* 是一种经典的技术，广泛应用于包括语音编码和识别在内的各个领域。它涉及将向量从一个充足的向量空间映射到有限数量的区域，这些区域可以用更少的比特有效表示。VQ已经成功应用于语音识别系统，通过有效压缩特征空间来提高性能。'
- en: '*INT8 quantization* is a recent approach to representing model weights and
    activations using 8-bit integers instead of 32-bit floating-point numbers. This
    method can reduce the model size by a factor of 4 without significantly degrading
    performance because it carefully rounds data from one type to another rather than
    simply truncating it.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*INT8量化* 是一种最近的方法，通过使用8位整数代替32位浮点数来表示模型的权重和激活。这种方法可以在不显著降低性能的情况下，将模型大小缩小4倍，因为它在类型之间进行精确的四舍五入，而不是简单地截断数据。'
- en: Further advancements have led to *low-bit quantization* techniques, where aggressive
    quantization to even 1 bit is explored. While this can substantially reduce storage
    and runtime, it may increase the **word error rate** (**WER**) in ASR tasks. However,
    with careful design, such as DistilHuBERT ([https://huggingface.co/ntu-spml/distilhubert](https://huggingface.co/ntu-spml/distilhubert)),
    it is possible to achieve model compression with minimal accuracy loss.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着进一步的进展，出现了*低比特量化*技术，探索了甚至为1位的激进量化。尽管这可以显著减少存储和运行时，但可能会增加**词错误率**（**WER**）在ASR任务中的表现。然而，通过精心设计，例如DistilHuBERT（[https://huggingface.co/ntu-spml/distilhubert](https://huggingface.co/ntu-spml/distilhubert)），可以实现模型压缩，同时保持最小的准确度损失。
- en: Be aware that quantization introduces a quantization error, which can degrade
    the model’s performance if not properly managed. Techniques such as **quantization-aware
    training** (**QAT**) and **post-training quantization** (**PTQ**) have been developed
    to mitigate these effects. QAT simulates the quantization process during training,
    allowing the model to adapt to the lower precision. PTQ, on the other hand, applies
    quantization after training, using calibration techniques to adjust the quantization
    parameters for minimal performance loss.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，量化会引入量化误差，如果管理不当，可能会降低模型的性能。为减轻这些影响，已经开发了如**量化感知训练**（**QAT**）和**训练后量化**（**PTQ**）等技术。QAT在训练过程中模拟量化过程，使模型能够适应较低的精度。另一方面，PTQ在训练后应用量化，使用校准技术调整量化参数，以实现最小的性能损失。
- en: '*Figure 7**.1* shows a high-level view of the quantization process for ASR
    models:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1* 显示了ASR模型的量化过程的高层次视图：'
- en: '![Figure 7.1 – Quantization process for ASR models](img/B21020_07_1.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – ASR模型的量化过程](img/B21020_07_1.jpg)'
- en: Figure 7.1 – Quantization process for ASR models
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – ASR模型的量化过程
- en: 'The steps broadly outlined in the diagram are generic and intended to provide
    a foundational overview. Let’s review each step in more detail:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图中大致概述的步骤是通用的，旨在提供基础概览。让我们更详细地回顾每个步骤：
- en: '**Preparation**: The initial step involves training the ASR model using high-precision
    (32-bit floating-point) representations. This ensures the model captures the complex
    patterns necessary for accurate speech recognition.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备**：初始步骤是使用高精度（32位浮点）表示法训练ASR模型。这确保了模型捕捉到准确语音识别所需的复杂模式。'
- en: '`int8`), or even lower. Your selection should consider model size, computational
    efficiency, and accuracy.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`int8`)，甚至更低。你的选择应考虑模型大小、计算效率和准确性。'
- en: The choice of bit depth directly impacts the trade-off between model size, computational
    speed, and accuracy. Lower bit depths significantly reduce the model’s memory
    footprint and increase computational efficiency, but they can introduce quantization
    errors that potentially degrade model performance. The challenge lies in selecting
    an optimal bit depth that minimizes these errors while achieving the desired efficiency
    gains.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比特深度的选择直接影响模型大小、计算速度和准确度之间的权衡。较低的比特深度显著减少模型的内存占用并提高计算效率，但它们可能引入量化误差，从而可能降低模型性能。挑战在于选择一个最佳的比特深度，既能最小化这些误差，又能实现所需的效率提升。
- en: '**Calibration**: A representative dataset is used to run inference through
    the model for PTQ. This step helps gather statistics about the distribution of
    activations, which are crucial for determining the quantization parameters.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**校准**：使用一个代表性的数据集通过模型进行推理以进行PTQ。此步骤有助于收集激活值的分布统计信息，这对于确定量化参数至关重要。'
- en: '**Quantization of weights and activations**: The model’s weights and activations
    are quantized using the gathered statistics to the selected bit depth. This involves
    mapping the high-precision values to a lower-precision space using scale factors
    and zero points.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**权重和激活量化**：使用收集到的统计信息，将模型的权重和激活量化到选定的位深度。这涉及使用比例因子和零点将高精度值映射到较低精度的空间。'
- en: '**QAT (optional)**: In some cases, models undergo QAT, where the quantization
    effects are simulated during the training process. This helps the model to adapt
    to the reduced precision, potentially mitigating accuracy loss.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**QAT（可选）**：在某些情况下，模型会进行QAT，即在训练过程中模拟量化效果。这有助于模型适应降低的精度，从而可能减轻准确度损失。'
- en: '**Testing and fine-tuning**: After quantization, the model’s performance is
    evaluated to ensure accuracy remains within acceptable bounds. If necessary, fine-tuning
    or adjustments to the quantization parameters are made.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试与微调**：量化后，评估模型的性能，以确保准确度保持在可接受范围内。如有必要，进行微调或调整量化参数。'
- en: '**Deployment**: The quantized model is then deployed on the target hardware,
    benefiting from reduced memory usage and faster inference times. This makes it
    suitable for edge devices or environments with limited computational resources.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：量化后的模型被部署到目标硬件上，受益于减少的内存使用和更快的推理时间。这使得它适用于边缘设备或计算资源有限的环境。'
- en: 'Several quantized versions of Whisper are available, and more are being developed.
    In my experience, I have found that Faster-Whisper and Distil-Whisper offer superior
    and reliable performance. Here is a brief description of them:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个量化版本的Whisper可用，并且更多版本正在开发中。根据我的经验，我发现Faster-Whisper和Distil-Whisper提供了更优越且可靠的性能。以下是它们的简要描述：
- en: '**Faster-Whisper** implements the Whisper model in CTranslate2, a library for
    efficient inference with Transformer models. It applies various methods to increase
    efficiency, such as weight quantization, layer fusion, and batch reordering. Quantization
    plays a significant role in Faster-Whisper by reducing the model’s memory footprint
    and accelerating inference, particularly on GPUs. We will experience Faster-Whisper
    in the *Diarizing Speech with WhisperX and NVIDIA’s NeMo* chapter because WhisperX
    uses Faster-Whisper to perform **speech-to-text** (**STT**) transcriptions.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Faster-Whisper**在CTranslate2中实现了Whisper模型，这是一个用于高效推理Transformer模型的库。它通过各种方法提高效率，如权重量化、层融合和批量重排序。量化在Faster-Whisper中发挥了重要作用，通过减少模型的内存占用和加速推理，特别是在GPU上。我们将在*使用WhisperX和NVIDIA的NeMo进行语音分离*章节中体验Faster-Whisper，因为WhisperX使用Faster-Whisper进行**语音转文本**（**STT**）转录。'
- en: '`small.en`, `medium.en`, and `large-v2` models that are faster and smaller
    while maintaining a comparable WER. Quantization can further enhance Distil-Whisper’s
    efficiency by reducing the precision of the model’s parameters, thus allowing
    for faster processing and lower memory requirements.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`small.en`、`medium.en` 和 `large-v2`模型，它们在保持相当WER的同时更快且更小。量化可以进一步提高Distil-Whisper的效率，通过减少模型参数的精度，从而实现更快的处理速度和更低的内存需求。'
- en: As we explore the power of quantization, let’s dive into a practical example
    using the CTranslate2 framework. CTranslate2 provides an efficient way to quantize
    and optimize the Whisper model for deployment on resource-constrained devices.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索量化的强大功能时，让我们通过使用CTranslate2框架来深入了解一个实际示例。CTranslate2提供了一种高效的方式来量化和优化Whisper模型，以便在资源受限的设备上部署。
- en: Quantizing Whisper with CTranslate2 and running inference with Faster-Whisper
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CTranslate2量化Whisper并通过Faster-Whisper进行推理
- en: 'Please find and open the `LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb`
    Colab notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb)).
    The notebook demonstrates quantizing the Whisper model using CTranslate2 and the
    Faster-Whisper framework to load the quantized models and perform inference (transcription
    or translation). You should run the notebook using only the CPU and then the GPU.
    The CPU performance should be relatively fast because we use small Whisper models,
    short audio files, and quantization. *Figure 7**.2* provides an overview of the
    quantization process, from preparing the audio data and converting and quantizing
    the model to evaluating its performance in language detection and transcription
    tasks. Quantization is vital in optimizing the model for deployment in resource-constrained
    environments, enabling efficient and accurate speech recognition capabilities:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请查找并打开`LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb` Colab笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb)）。该笔记本演示了如何使用CTranslate2和Faster-Whisper框架来加载量化模型并进行推理（转录或翻译）。你应先在CPU上运行笔记本，然后在GPU上运行。由于我们使用的是小型Whisper模型、短音频文件和量化，CPU性能应该相对较快。*图7.2*提供了量化过程的概览，从准备音频数据、转换和量化模型，到评估其在语言检测和转录任务中的表现。量化在优化模型以适应资源受限的环境中至关重要，它使得语音识别能力高效且准确：
- en: '![Figure 7.2 – High-level view of the process of quantizing Whisper using the
    CTranslate2 framework](img/B21020_07_2.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 使用CTranslate2框架量化Whisper过程的高层视图](img/B21020_07_2.jpg)'
- en: Figure 7.2 – High-level view of the process of quantizing Whisper using the
    CTranslate2 framework
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 使用CTranslate2框架量化Whisper过程的高层视图
- en: 'The following steps provide an overview of the quantization process. For a
    complete, end-to-end implementation, please refer to the `LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb`
    notebook. This section will present the high-level steps and selected code snippets
    to illustrate the process. Remember that the notebook contains additional details
    and explanations to help you understand the quantization workflow comprehensively.
    Here’s a detailed breakdown of the process:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了量化过程。有关完整的端到端实现，请参考`LOAIW_ch07_1_Quantizing_Whisper_with_CTranslate2.ipynb`笔记本。本节将展示高层步骤和选定的代码片段来说明该过程。请记住，笔记本中包含了额外的细节和解释，帮助你全面理解量化工作流。以下是该过程的详细分解：
- en: '`ctranslate2`, `transformers`, and `faster-whisper`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ctranslate2`、`transformers`和`faster-whisper`：'
- en: '[PRE0]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These libraries are essential for quantization and leveraging the Whisper model’s
    capabilities.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些库对于量化以及充分利用Whisper模型的能力至关重要。
- en: '**Downloading sample audio files**: Two are downloaded from our GitHub repository
    to test the Whisper model’s transcription capabilities:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载示例音频文件**：从我们的GitHub仓库下载两个文件，用于测试Whisper模型的转录能力：'
- en: '[PRE1]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: import ctranslate2
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入ctranslate2
- en: from IPython.display import Audio
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从IPython.display导入Audio
- en: import librosa
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入librosa
- en: import transformers
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入transformers
- en: Load and resample the audio file.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载并重采样音频文件。
- en: sampling_frequency = 16000
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sampling_frequency = 16000
- en: audio, _ = librosa.load("Learn_OAI_Whisper_Sample_Audio01.mp3", sr=sampling_frequency,
    mono=True)
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: audio, _ = librosa.load("Learn_OAI_Whisper_Sample_Audio01.mp3", sr=sampling_frequency,
    mono=True)
- en: Audio(audio, rate=sampling_frequency)
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Audio(audio, rate=sampling_frequency)
- en: '[PRE2]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`openai/whisper-tiny`) is converted to the CTranslate2 format, a more efficient
    inference format:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`openai/whisper-tiny`) 被转换为CTranslate2格式，这是一种更高效的推理格式：'
- en: '[PRE3]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`INT8`):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`INT8`）：'
- en: '[PRE4]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 16-bit integers (`INT16`)
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 16位整数（`INT16`）
- en: 16-bit floating points (`FP16`)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 16位浮动点（`FP16`）
- en: 16-bit brain floating points (`BF16`)
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 16位大脑浮动点（`BF16`）
- en: This step significantly reduces the model’s size and computational requirements,
    making it more suitable for deployment on devices with limited resources.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步显著减少了模型的大小和计算需求，使其更适合在资源有限的设备上部署。
- en: '**Detecting language**: The quantized model detects the language of the provided
    audio samples:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检测语言**：量化模型检测所提供音频样本的语言：'
- en: '[PRE5]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This step is important for ensuring that the model accurately understands the
    context of the audio data.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一步对于确保模型准确理解音频数据的上下文非常重要。
- en: '`processor.tokenizer.convert_tokens_to_ids()` method:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`processor.tokenizer.convert_tokens_to_ids()`方法：'
- en: '[PRE6]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This demonstrates the model’s ability to transcribe speech accurately, even
    after quantization.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这展示了模型即使在量化后也能准确转录语音的能力。
- en: '**Evaluating performance**: After the audio transcription, the code evaluates
    the performance of the quantized model, such as measuring the time taken for transcription:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估性能**：音频转录后，代码评估量化后模型的性能，如测量转录所用的时间：'
- en: '[PRE7]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This evaluation is crucial for understanding the impact of quantization on the
    model’s efficiency and accuracy.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一评估对于了解量化对模型效率和准确性的影响至关重要。
- en: The results show empirical evidence that quantized models of Whisper perform
    transcription quite well using a much smaller memory and processing footprint.
    Building upon our understanding of quantization, let’s now focus on another robust
    framework, OpenVINO. We’ll investigate how OpenVINO can be used to quantize the
    Distil-Whisper model, offering a more comprehensive and rigorous quantization
    process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了实证证据，表明量化后的Whisper模型能够以更小的内存和处理足迹进行良好的转录。基于我们对量化的理解，让我们现在集中研究另一个强大的框架——OpenVINO。我们将探讨OpenVINO如何用于量化Distil-Whisper模型，提供一个更加全面和严格的量化过程。
- en: Quantizing Distil-Whisper with OpenVINO
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用OpenVINO量化Distil-Whisper
- en: This hands-on exercise relies on the `LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb`
    Colab notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb)).
    Because of OpenVINO, I recommend you run this notebook in Colab using CPU and
    high RAM. OpenVINO does not use an NVIDIA GPU, even if it is present, only an
    Intel GPU. However, the libraries OpenVINO provides are optimized to run on a
    plain CPU, thus a significant advantage when the computational processing resources
    are limited. However, you should have at least 50 GB of RAM for quantization.
    The notebook provides a comprehensive guide on utilizing Distil-Whisper (based
    on WhisperX), a distilled variant of the Whisper model, with OpenVINO for ASR.
    Distil-Whisper offers a significant reduction in the number of parameters (from
    1,550 parameters in `large-v2` to 756 in `distill-large-v2`, or about 50% reduction)
    and an increase in inference speed while maintaining close performance to the
    original Whisper model regarding WER.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实践练习依赖于`LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb` Colab笔记本
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_2_Quantizing_Distil_Whisper_with_OpenVINO.ipynb))。由于OpenVINO的原因，我建议你在Colab中使用CPU和大内存运行此笔记本。即使存在NVIDIA
    GPU，OpenVINO也不会使用它，仅使用Intel GPU。不过，OpenVINO提供的库经过优化，可以在普通CPU上运行，因此在计算处理资源有限时，这也是一个显著的优势。然而，进行量化时你至少需要50GB的内存。该笔记本提供了利用Distil-Whisper（基于WhisperX），一个经过蒸馏的Whisper模型变体，与OpenVINO结合用于自动语音识别（ASR）的全面指南。Distil-Whisper大大减少了参数数量（从`large-v2`中的1,550个参数减少到`distill-large-v2`中的756个参数，约减少50%），并在保持与原始Whisper模型在WER（字错误率）方面相近性能的同时，提高了推理速度。
- en: '*Figure 7**.3* outlines converting the Distil-Whisper model to the OpenVINO
    **intermediate representation** (**IR**) format, applying INT8 PTQ for performance
    enhancement, and running the model for speech recognition tasks:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.3*概述了将Distil-Whisper模型转换为OpenVINO **中间表示**（**IR**）格式，应用INT8 PTQ以提高性能，并运行模型进行语音识别任务：'
- en: '![Figure 7.3 – High-level architectural diagram quantizing Distil-Whisper using
    the OpenVINO framework](img/B21020_07_3.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 使用OpenVINO框架量化Distil-Whisper的高层架构图](img/B21020_07_3.jpg)'
- en: Figure 7.3 – High-level architectural diagram quantizing Distil-Whisper using
    the OpenVINO framework
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 使用OpenVINO框架量化Distil-Whisper的高层架构图
- en: The following subsections will describe the critical steps in quantizing the
    Distil-Whisper model using the OpenVINO framework. We will install the necessary
    libraries, load the model, convert it to the OpenVINO format, and apply quantization.
    We will also explore how to load the quantized model using the Optimum library
    and integrate it with Hugging Face pipelines. Finally, we will run inference with
    the quantized model and compare its performance and accuracy to the original model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下子章节将描述使用OpenVINO框架量化Distil-Whisper模型的关键步骤。我们将安装必要的库，加载模型，将其转换为OpenVINO格式，并应用量化。我们还将探索如何使用Optimum库加载量化后的模型，并将其与Hugging
    Face管道集成。最后，我们将使用量化后的模型进行推理，并将其性能和准确性与原始模型进行比较。
- en: Installing libraries
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装库
- en: 'First, the process instructs the installation of necessary Python libraries:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，过程指导安装必要的 Python 库：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s examine each one in more detail, focusing on the libraries we have not
    described before:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看每个库，重点介绍那些我们之前没有描述过的库：
- en: '**Transformers**: This library is used for NLP tasks such as text classification,
    information extraction, and question-answering. It provides access to pre-trained
    models such as BERT, GPT-2, and, in this case, the Distil-Whisper model for ASR.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers**：这个库用于处理自然语言处理任务，如文本分类、信息抽取和问答。它提供了访问预训练模型的功能，如 BERT、GPT-2，以及在本例中用于
    ASR 的 Distil-Whisper 模型。'
- en: '**Open Neural Network Exchange (ONNX)**: ONNX is an open format representing
    machine learning models. It enables models to be transferred between different
    frameworks and tools, facilitating interoperability.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Open Neural Network Exchange (ONNX)**：ONNX 是一个开放格式，用于表示机器学习模型。它使模型能够在不同的框架和工具之间转移，促进了互操作性。'
- en: '**Optimum Intel**: This is part of the Hugging Face Optimum library tailored
    for Intel hardware. It converts models to the OpenVINO IR format, which is optimized
    for Intel’s hardware, and performs tasks such as quantization to improve model
    performance.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Optimum Intel**：这是 Hugging Face Optimum 库的一部分，专为 Intel 硬件量身定制。它将模型转换为 OpenVINO
    IR 格式，这种格式经过针对 Intel 硬件的优化，并执行如量化等任务，以提高模型性能。'
- en: '**OpenVINO**: The OpenVINO toolkit is designed to facilitate fast and efficient
    inference of deep learning models on Intel hardware. It includes optimization
    tools and libraries to accelerate various computer vision and deep learning tasks.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenVINO**：OpenVINO 工具包旨在促进在 Intel 硬件上快速高效地进行深度学习模型推理。它包括优化工具和库，用于加速各种计算机视觉和深度学习任务。'
- en: '**Datasets**: This library is part of the Hugging Face ecosystem and is used
    for loading and processing datasets simply and efficiently. It is handy for machine
    learning tasks that require handling large amounts of data.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Datasets**：这是 Hugging Face 生态系统的一部分，用于简单高效地加载和处理数据集。它对于需要处理大量数据的机器学习任务非常有用。'
- en: '**Soundfile**: This library provides functions for reading from and writing
    to audio files in various formats. It handles audio data input and output operations.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Soundfile**：这个库提供了从音频文件读取和写入多种格式的功能。它处理音频数据的输入输出操作。'
- en: '**Neural Network Compression Framework (NNCF)**: This toolkit for optimizing
    deep learning models through quantization, pruning, and knowledge distillation.
    It improves neural networks’ performance, particularly regarding inference speed
    and memory usage.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络压缩框架（NNCF）**：这是一个通过量化、剪枝和知识蒸馏优化深度学习模型的工具包。它提高了神经网络的性能，特别是在推理速度和内存使用方面。'
- en: '**JiWER**: This is a library for evaluating automatic speech recognition models.
    It calculates metrics such as the WER, a standard measure of speech recognition
    systems’ performance.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JiWER**：这是一个用于评估自动语音识别模型的库。它计算诸如 WER（词错误率）等指标，这是评估语音识别系统性能的标准方法。'
- en: Each library plays a specific role in running and optimizing the Distil-Whisper
    model using OpenVINO, from model conversion and optimization to performance evaluation
    and user interface creation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 每个库在运行和优化 Distil-Whisper 模型时都扮演着特定的角色，从模型转换和优化到性能评估和用户界面创建。
- en: Loading the model
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载模型
- en: 'When initializing a PyTorch Whisper model using the `transformers` library,
    the `AutoModelForSpeechSeq2Seq.from_pretrained` method is the go-to:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `transformers` 库初始化 PyTorch Whisper 模型时，`AutoModelForSpeechSeq2Seq.from_pretrained`
    方法是常用的方法：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This tutorial will use the `distil-whisper/distil-medium.en` model as our primary
    example. It’s worth noting that the model must be downloaded during the first
    run, which may take some time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将使用 `distil-whisper/distil-medium.en` 模型作为主要示例。值得注意的是，该模型在第一次运行时需要下载，可能需要一些时间。
- en: If you want to explore alternative models, the Distil-Whisper Hugging Face collection
    offers options such as `distil-whisper/distil-large-v2` or `distil-whisper/distil-small.en`.
    Other models based on the original Whisper architecture are available, and you
    can find more information about them in the provided resources.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想探索其他模型，Distil-Whisper Hugging Face 集合提供了如 `distil-whisper/distil-large-v2`
    或 `distil-whisper/distil-small.en` 等选项。基于原始 Whisper 架构的其他模型也有提供，你可以在提供的资源中找到更多信息。
- en: It’s crucial to emphasize the significance of preprocessing and postprocessing
    in this model’s usage. The `AutoProcessor` class, used to initialize `WhisperProcessor`,
    plays a vital role in preparing the audio input data for the model. It handles
    the audio conversion into a Mel-spectrogram and decodes the `token_ids` predicted
    output back into a string using the tokenizer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 强调预处理和后处理在此模型使用中的重要性至关重要。`AutoProcessor`类用于初始化`WhisperProcessor`，它在为模型准备音频输入数据时发挥了至关重要的作用。它负责将音频转换为Mel-谱图，并使用分词器将预测的`token_ids`输出解码回字符串。
- en: By leveraging the `AutoModelForSpeechSeq2Seq.from_pretrained` method and understanding
    the preprocessing and postprocessing steps, you’ll be well equipped to work effectively
    with PyTorch Whisper models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用`AutoModelForSpeechSeq2Seq.from_pretrained`方法，并了解预处理和后处理步骤，你将能够有效地与PyTorch
    Whisper模型进行工作。
- en: Loading the OpenVINO model using the Optimum library
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Optimum库加载OpenVINO模型
- en: The Hugging Face Optimum API is a powerful tool that simplifies converting and
    quantizing models from the Hugging Face Transformers library to the OpenVINO™
    IR format. The Hugging Face Optimum documentation ([https://huggingface.co/docs/optimum/intel/inference](https://huggingface.co/docs/optimum/intel/inference))
    is an excellent resource if you’re looking for more in-depth information.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Optimum API是一个强大的工具，可以简化将Hugging Face Transformers库中的模型转换和量化为OpenVINO™
    IR格式的过程。如果你想了解更多深入的信息，[Hugging Face Optimum文档](https://huggingface.co/docs/optimum/intel/inference)是一个很好的资源。
- en: 'Optimum Intel is your friend when loading optimized models from the Hugging
    Face Hub and creating pipelines for inference with OpenVINO Runtime. What’s great
    about the Optimum Inference models is that they are API-compatible with Hugging
    Face `transformers` models. You can seamlessly replace the `AutoModelForXxx` class
    with the corresponding `OVModelForXxx` class without hassle:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Optimum Intel是加载来自Hugging Face Hub的优化模型并创建用于OpenVINO Runtime推理的管道时的好伙伴。Optimum推理模型的一个优点是它们与Hugging
    Face的`transformers`模型API兼容。你可以毫不费力地将`AutoModelForXxx`类替换为相应的`OVModelForXxx`类：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You’ll need to call the `from_pretrained` method to initialize the model class.
    When downloading and converting the `transformers` model, include the `export=True`
    parameter. This will ensure a smooth conversion process. Once you have the converted
    model, you can save it using the `save_pretrained` method:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要调用`from_pretrained`方法来初始化模型类。在下载和转换`transformers`模型时，包含`export=True`参数。这将确保平稳的转换过程。一旦你获得转换后的模型，就可以使用`save_pretrained`方法保存它：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s worth mentioning that the tokenizers and processors distributed with the
    models are also compatible with the OpenVINO model. This compatibility allows
    you to reuse the previously initialized processor, saving time and effort.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，随模型分发的分词器和处理器也与OpenVINO模型兼容。这种兼容性允许你重复使用之前初始化的处理器，节省时间和精力。
- en: 'Using the Hugging Face Optimum library, we can also convert the Distil-Whisper
    model to OpenVINO’s optimized IR format. This step is crucial for leveraging OpenVINO’s
    inference engine for efficient model execution:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hugging Face Optimum库，我们还可以将Distil-Whisper模型转换为OpenVINO的优化IR格式。这一步骤对于利用OpenVINO的推理引擎来高效执行模型至关重要：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By leveraging the Hugging Face Optimum API and Optimum Intel, you can efficiently
    convert and quantize models, load optimized models, and create pipelines for inference
    with OpenVINO Runtime. The API compatibility and the ability to reuse initialized
    processors make the process even more streamlined.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用Hugging Face Optimum API和Optimum Intel，你可以高效地转换和量化模型，加载优化后的模型，并创建用于OpenVINO
    Runtime推理的管道。API兼容性以及能够重复使用初始化处理器的能力，使得整个过程更加简化。
- en: Using the OpenVINO model with Hugging Face pipelines
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用OpenVINO模型与Hugging Face管道
- en: By combining the OpenVINO model with the Hugging Face pipeline interface and
    utilizing the chunked algorithm and batching capabilities of Distil-Whisper, you’ll
    be able to tackle long audio transcription tasks with unprecedented speed and
    ease.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将OpenVINO模型与Hugging Face管道接口结合，并利用Distil-Whisper的分块算法和批处理能力，你将能够以前所未有的速度和轻松度处理长时间的音频转录任务。
- en: 'As with the original PyTorch model, the OpenVINO model seamlessly integrates
    with the Hugging Face pipeline interface for ASR. This compatibility allows you
    to transcribe long audio files using the pipeline effortlessly:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的PyTorch模型一样，OpenVINO模型与Hugging Face的ASR管道接口完美集成。这种兼容性使得你可以轻松地使用管道转录长时间的音频文件：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Distil-Whisper takes it a step further by employing a chunked algorithm, which
    significantly speeds up the transcription process for long-form audio. This chunked
    long-form algorithm is an impressive nine times faster than the sequential algorithm
    proposed by OpenAI in their Whisper paper ([https://cdn.openai.com/papers/whisper.pdf](https://cdn.openai.com/papers/whisper.pdf)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Distil-Whisper通过采用分块算法将其推向更高级别，显著加快了长形音频的转录过程。这种分块长形算法比OpenAI在其Whisper论文中提出的顺序算法快了惊人的九倍（[https://cdn.openai.com/papers/whisper.pdf](https://cdn.openai.com/papers/whisper.pdf))。
- en: To take advantage of chunking, you only need to pass the `chunk_length_s` parameter
    to the pipeline. When working with Distil-Whisper, setting the chunk length to
    `15` seconds is the sweet spot for optimal performance. But that’s not all! If
    you want to leverage the power of batching, include the `batch_size` argument
    when calling the pipeline. This will enable you to process multiple audio chunks
    simultaneously, further boosting the efficiency of your transcription workflow.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用分块的优势，您只需将`chunk_length_s`参数传递给管道即可。在使用Distil-Whisper时，将块长度设置为`15`秒是达到最佳性能的甜蜜点。但这还不是全部！如果您想利用批处理的能力，在调用管道时包括`batch_size`参数。这将使您能够同时处理多个音频块，进一步提升转录工作流的效率。
- en: Quantizing the model
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化模型
- en: Quantization is a powerful technique for significantly reducing the model size
    and improving inference speed. NNCF makes it easier than ever to implement PTQ.
    By seamlessly integrating quantization layers into the model graph and leveraging
    a subset of the training dataset to initialize the parameters of these additional
    layers, NNCF ensures that the modifications required to your original training
    code are minimal.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种强大的技术，可以显著减少模型大小并提高推断速度。NNCF使得实现PTQ变得比以往任何时候都更加简单。通过无缝集成量化层到模型图中，并利用训练数据集的子集来初始化这些额外层的参数，NNCF确保对您原始训练代码的修改是最小的。
- en: 'To embark on the optimization journey, the first step is to create calibration
    datasets specifically tailored for quantization:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 踏上优化之旅的第一步是创建专门为量化量身定制的校准数据集：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since the Whisper encoder and decoder are quantized separately, preparing a
    calibration dataset for each model is essential. This is where the `InferRequestWrapper`
    class comes into play. Importing this class, you can intercept and collect the
    model inputs in a list. Then, you’ll run model inference on a small subset of
    audio samples. Remember that increasing the calibration dataset’s size generally
    leads to better quantization quality, so it’s worth experimenting to find the
    right balance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Whisper编码器和解码器是分别量化的，为每个模型准备一个校准数据集至关重要。这就是`InferRequestWrapper`类发挥作用的地方。导入此类，您可以拦截并收集模型输入到列表中。然后，您将在少量音频样本上运行模型推断。请记住，增加校准数据集的大小通常会导致更好的量化质量，因此值得尝试找到适当的平衡。
- en: 'Once you have your calibration datasets ready, it’s time to unleash the power
    of `nncf.quantize`. This function is your key to obtaining quantized encoder and
    decoder models. In the case of Distil-Whisper, you’ll run `nncf.quantize` on the
    `encoder` and `decoder_with_past` models. It’s worth noting that the first-step
    decoder is not quantized since its contribution to the overall inference time
    is negligible:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备好校准数据集，就是时候释放`nncf.quantize`的力量了。这个函数是获得量化编码器和解码器模型的关键。在Distil-Whisper的情况下，您将在`encoder`和`decoder_with_past`模型上运行`nncf.quantize`。值得注意的是，第一步解码器因其对整体推断时间的贡献微不足道而未被量化：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The code snippet shows that the final step is to serialize the INT8 model using
    the `openvino.save_model` function after quantization. This step ensures that
    your quantized model is ready for deployment and can be quickly loaded for inference.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段显示，最后一步是在量化后使用`openvino.save_model`函数序列化INT8模型。此步骤确保您的量化模型已准备好部署，并可以快速加载进行推断。
- en: It’s essential to remember that quantization is a computationally intensive
    operation that can be both time-consuming and memory-intensive. Running the quantization
    code may require patience, but the benefits of model size reduction and inference
    speed improvement make it well worth the effort.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，量化是一种计算密集型操作，可能既耗时又消耗内存。运行量化代码可能需要耐心，但模型尺寸减小和推断速度提高的好处使其绝对值得努力。
- en: By following these steps and leveraging the power of NNCF, you can optimize
    your models through PTQ, enabling faster and more efficient inference.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤并利用NNCF的强大功能，您可以通过PTQ优化模型，从而实现更快、更高效的推理。
- en: Running inference
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行推理
- en: 'Here, we demonstrate how to run inference with the quantized model, including
    loading the model, preparing input samples, and executing the model to transcribe
    speech. Here are the steps in detail:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们展示了如何使用量化模型进行推理，包括加载模型、准备输入样本以及执行模型进行语音转录。以下是详细步骤：
- en: '`librispeech_asr_dummy` from Hugging Face’s `datasets` library:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自Hugging Face的`librispeech_asr_dummy`，出自`datasets`库：
- en: '[PRE16]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`numpy` array format and then to a tensor that the model can process:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`numpy`数组格式转换为模型可以处理的张量：
- en: '[PRE17]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Running inference on the original model**: Use the original OpenVINO model
    to generate predictions for the input features. Decode the predicted token IDs
    into text transcription using the model’s processor:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在原始模型上运行推理**：使用原始OpenVINO模型生成输入特征的预测。通过模型的处理器将预测的令牌ID解码为文本转录：'
- en: '[PRE18]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Running inference on the quantized model**: Similarly, use the quantized
    OpenVINO model to generate predictions for the same input features. Decode the
    predicted token IDs into text transcription:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在量化模型上运行推理**：类似地，使用量化后的OpenVINO模型生成相同输入特征的预测。通过模型的处理器将预测的令牌ID解码为文本转录：'
- en: '[PRE19]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Audio` class to play the audio file used for transcription:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Audio`类播放用于转录的音频文件：
- en: '[PRE20]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Printing transcriptions**: Print the transcriptions from the original and
    quantized models to compare the results:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**打印转录文本**：打印来自原始模型和量化模型的转录文本以进行结果比较：'
- en: '[PRE21]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: After running this code in the notebook, review the transcriptions and verify
    that the transcriptions from the original and quantized models are the same, ensuring
    that quantization did not significantly impact the model’s accuracy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中运行此代码后，检查转录文本，并验证原始模型和量化模型的转录文本是否一致，确保量化没有显著影响模型的准确性。
- en: In addition, the notebook includes how to use the model with Hugging Face’s
    pipeline interface for ASR, highlighting the efficiency of chunked algorithms
    for long-form audio transcription.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，笔记本还包括如何通过Hugging Face的ASR管道接口使用模型，突出了用于长音频转录的分块算法的效率。
- en: Comparing performance and accuracy
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能和准确性比较
- en: 'Next, we compare the original and quantized Distil-Whisper models regarding
    accuracy (using WER) and performance (inference time). It illustrates the benefits
    of quantization in enhancing model inference speed without a significant drop
    in accuracy. Comparing the performance and accuracy of the original and quantized
    models involves the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将比较原始模型和量化后的Distil-Whisper模型在准确性（使用WER）和性能（推理时间）方面的差异。这说明了量化在提高模型推理速度而不会显著降低准确性方面的好处。比较原始模型和量化模型的性能和准确性包括以下内容：
- en: '**Measuring accuracy**: We use the *1 - WER* metric to measure the accuracy
    of the models. This involves comparing the transcriptions produced by the models
    against a ground truth to calculate the error rate. A lower WER indicates higher
    accuracy:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**衡量准确性**：我们使用*1 - WER*指标来衡量模型的准确性。这涉及将模型生成的转录文本与地面真实文本进行比较，以计算错误率。较低的WER表示更高的准确性：'
- en: '[PRE22]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Measuring performance**: The inference time is measured separately for the
    encoder and decoder-with-past model forwards and the whole model inference. This
    step involves timing the model’s inference process to evaluate how quickly it
    can generate predictions. Performance measurement is crucial for understanding
    the efficiency gains achieved through quantization:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**衡量性能**：推理时间分别为编码器、解码器-带有过去状态模型的前向推理以及整个模型推理进行测量。此步骤包括对模型的推理过程进行计时，以评估模型生成预测的速度。性能测量对于理解量化在提高效率方面的作用至关重要：'
- en: '[PRE23]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Comparing original and quantized models**: The notebook directly compares
    the original Distil-Whisper models and their quantized counterparts regarding
    accuracy (*using 1 - WER*) and performance (inference time). This comparison helps
    to illustrate the impact of quantization on the model’s efficiency and effectiveness:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较原始模型和量化模型**：笔记本直接比较了原始Distil-Whisper模型及其量化版本在准确性（*使用1 - WER*）和性能（推理时间）方面的表现。这种比较有助于说明量化对模型效率和效果的影响：'
- en: '[PRE24]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Based on the comparison printout from running the notebook, you can conclude
    the benefits of quantization, such as significant improvements in model inference
    time without a major drop in accuracy. These steps provide a comprehensive framework
    for evaluating the impact of quantization on the performance and accuracy of ASR
    models such as Distil-Whisper when optimized with OpenVINO. The goal is to demonstrate
    that quantization can significantly enhance model efficiency for deployment in
    resource-constrained environments without substantially compromising accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基于运行笔记本后打印出来的对比结果，你可以得出量化的好处，比如模型推理时间的显著改善，而精度几乎没有显著下降。这些步骤为评估量化对ASR模型性能和精度的影响提供了一个全面的框架，尤其是在使用OpenVINO优化Distil-Whisper等模型时。目标是证明，量化可以显著提高模型效率，使其能够在资源受限的环境中部署，而不会大幅牺牲精度。
- en: Running the interactive demo
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行交互式演示
- en: As an extra, the interactive Gradio demo allows us to test the model’s capabilities
    on their audio data or recordings. This section demonstrates the practical application
    of the quantized Distil-Whisper model in a user-friendly manner.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为附加内容，交互式Gradio演示允许我们在他们的音频数据或录音上测试模型的能力。此部分展示了量化后的Distil-Whisper模型在用户友好方式下的实际应用。
- en: I encourage you to run and experiment with the Colab notebook. It is a foundational
    tool for understanding the quantization process and, more importantly, a blueprint
    for your experimental or production work. After running the notebook, we embarked
    on a fascinating journey through the integration of cutting-edge technologies
    in ASR. The notebook meticulously outlined leveraging the Distil-Whisper model,
    a distilled variant of OpenAI’s Whisper, optimized for performance with significantly
    fewer parameters, and deploying it with Intel’s OpenVINO toolkit for enhanced
    inference speed and efficiency.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你运行并实验Colab笔记本。它是理解量化过程的基础工具，更重要的是，它是你实验或生产工作的重要蓝图。运行完笔记本后，我们开始了一个迷人的旅程，探索了在自动语音识别（ASR）中集成尖端技术。笔记本详细介绍了如何利用Distil-Whisper模型，它是OpenAI的Whisper模型的精简版，在显著减少参数的同时优化了性能，并通过英特尔的OpenVINO工具包进行部署，以提高推理速度和效率。
- en: One of the key learnings from this notebook was the seamless synergy between
    various libraries and frameworks to achieve a streamlined workflow for ASR tasks.
    Using the Hugging Face Transformers library to access pre-trained models and the
    Optimum Intel library for model conversion to OpenVINO’s IR exemplified a powerful
    approach to model deployment. This process simplified the user experience and
    paved the way for leveraging hardware acceleration capabilities offered by Intel
    architectures.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个笔记本中获得的一个关键学习是，不同库和框架之间的无缝协同工作，帮助实现了ASR任务的高效工作流。使用Hugging Face Transformers库访问预训练模型，并利用Optimum
    Intel库将模型转换为OpenVINO的IR格式，展示了模型部署的一种强大方法。这个过程简化了用户体验，为利用英特尔架构提供的硬件加速能力铺平了道路。
- en: The notebook further delved into the practical aspects of model quantization
    using NNCF. This step was crucial for optimizing model performance without significantly
    compromising accuracy. The detailed walkthrough of preparing calibration datasets,
    running quantization, and comparing the performance and accuracy of the original
    and quantized models provided invaluable insights into the nuances of model optimization.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本进一步探讨了使用NNCF进行模型量化的实际操作。这一步对于在不显著影响精度的情况下优化模型性能至关重要。准备校准数据集、运行量化并比较原始模型和量化模型的性能与精度的详细演示，提供了关于模型优化细节的宝贵见解。
- en: Another significant aspect highlighted in the notebook was the use of Gradio
    to create interactive demos. This demonstrated the practical application of the
    Distil-Whisper model in real-world scenarios, allowing users to test the model’s
    capabilities on their audio data. Including such a demo underscored the importance
    of accessibility and user engagement in developing and deploying AI models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中还强调了另一个重要方面，即使用Gradio创建交互式演示。这展示了Distil-Whisper模型在现实场景中的实际应用，允许用户在他们的音频数据上测试模型的能力。包括这样的演示突显了在开发和部署AI模型时，易用性和用户参与的重要性。
- en: You should seek ways to apply the learnings from this notebook directly to your
    experimental or production ASR tasks. They extend to the broader field of AI model
    deployment and optimization, highlighting the evolving landscape of AI technologies
    and their practical applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该寻求将此笔记本中的学习内容直接应用于你的实验或生产 ASR 任务。这些内容可以扩展到 AI 模型部署和优化的更广泛领域，突显了 AI 技术的不断发展和其实际应用。
- en: While quantization has proven to be a powerful technique for optimizing Whisper’s
    performance and enabling efficient deployment, another exciting frontier lies
    in exploring the challenges and opportunities of real-time speech recognition
    with Whisper. Real-time transcription opens up possibilities, from enhancing accessibility
    to facilitating instant communication. However, it also presents unique technical
    hurdles that must be overcome. In the following section, we will delve into the
    current limitations and ongoing research efforts to make real-time transcription
    with Whisper a reality. By understanding these challenges and the potential solutions
    on the horizon, we can appreciate the immense potential of Whisper in reshaping
    how we interact with spoken language in real-time scenarios.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然量化技术已被证明是优化 Whisper 性能和实现高效部署的强大手段，但另一个令人兴奋的前沿领域是探索使用 Whisper 进行实时语音识别的挑战与机遇。实时转录开启了许多可能性，从增强可访问性到促进即时沟通。然而，它也带来了独特的技术难题，必须加以克服。在接下来的部分，我们将深入探讨实时转录面临的当前限制和正在进行的研究工作，旨在使
    Whisper 的实时转录成为现实。通过理解这些挑战和前景中的潜在解决方案，我们可以更好地认识到 Whisper 在重塑我们如何在实时场景中与口语互动方面的巨大潜力。
- en: Facing the challenges and opportunities of real-time speech recognition
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面对实时语音识别的挑战与机遇
- en: Pursuing real-time transcription with Whisper opens up many applications that
    can benefit various sectors, including education, healthcare, and customer service.
    Real-time transcription can enhance accessibility for individuals with hearing
    impairments, facilitate instant communication in multilingual contexts, and provide
    immediate documentation of verbal exchanges. As Whisper’s capabilities evolve,
    its potential to serve as a universal translator and accessibility tool becomes
    increasingly apparent.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 追求使用 Whisper 进行实时转录打开了许多应用，这些应用能够惠及教育、医疗和客户服务等各个领域。实时转录可以增强听力障碍人士的可访问性，促进多语言环境中的即时沟通，并提供口头交流的即时文档记录。随着
    Whisper 功能的不断发展，它作为通用翻译工具和可访问性工具的潜力变得越来越明显。
- en: 'At present, however, more limitations and challenges are preventing real-time
    transcription. Let’s delve into these aspects, focusing on the technical intricacies
    and prospects of performing real-time transcription with Whisper:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前仍有更多的限制和挑战阻碍着实时转录的实现。让我们深入探讨这些方面，重点关注实现实时转录的技术复杂性和前景：
- en: '**Processing time and latency**: One of the primary challenges in achieving
    real-time transcription with Whisper is its operation’s inherent latency and processing
    time. As discussions on platforms such as GitHub and Hugging Face reveal, Whisper
    is not inherently designed for real-time STT conversion. While robust for processing
    audio files of unlimited length, the system’s architecture encounters hurdles
    in delivering instantaneous transcription results. This latency stems from the
    complex neural network models that underpin Whisper, which require significant
    computational resources to analyze and transcribe speech accurately.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理时间和延迟**：实现实时转录的主要挑战之一是 Whisper 操作的固有延迟和处理时间。正如 GitHub 和 Hugging Face 等平台上的讨论所揭示，Whisper
    并非天生为实时语音转文字（STT）转换设计。虽然它在处理任意长度的音频文件时表现稳健，但系统架构在提供即时转录结果时遇到了障碍。这种延迟源于支撑 Whisper
    的复杂神经网络模型，这些模型需要大量计算资源来准确分析和转录语音。'
- en: '**Increasing accuracy and contextual understanding**: Another limitation lies
    in Whisper’s transcriptions’ accuracy and contextual knowledge. While Whisper
    has demonstrated remarkable proficiency in transcribing diverse languages and
    accents, real-time applications pose unique challenges. The system must recognize
    speech accurately and understand context, idioms, and colloquial expressions in
    the flow of conversation. This demands a level of linguistic and cultural nuance
    that current models are still striving to perfect.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高准确性和上下文理解能力**：另一个限制在于 Whisper 转录的准确性和上下文知识。尽管 Whisper 在转录多种语言和口音方面表现出了非凡的能力，但实时应用仍然面临独特的挑战。系统必须准确识别语音，并理解对话中的上下文、习语和口语表达。这要求具备当前模型仍在努力完善的语言学和文化细微差别。'
- en: 'Despite these limitations, the potential for Whisper to transform real-time
    transcription is immense. The technology’s current capabilities and ongoing advancements
    offer a glimpse into a future where these challenges are surmountable:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，Whisper 在实时转录方面的潜力巨大。该技术当前的能力和不断发展的进展为我们展示了一个未来，届时这些挑战将变得可以克服：
- en: '**Advancing model efficiency**: Recent research efforts have focused on enhancing
    Whisper’s efficiency and reducing latency, making real-time transcription a tangible
    goal. For instance, a study on arXiv, *Turning Whisper into Real-Time Transcription
    System* ([https://arxiv.org/abs/2307.14743](https://arxiv.org/abs/2307.14743)),
    discusses methods for turning Whisper into a real-time transcription system. These
    include optimizing the model’s architecture and leveraging more powerful computational
    resources. As these advancements continue, we can anticipate significant reductions
    in processing time, bringing Whisper closer to delivering seamless real-time transcription.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升模型效率**：近期的研究集中于提高 Whisper 的效率并减少延迟，使实时转录成为一个切实可行的目标。例如，在 arXiv 上的一项研究《将
    Whisper 转变为实时转录系统》（[https://arxiv.org/abs/2307.14743](https://arxiv.org/abs/2307.14743)）探讨了将
    Whisper 转变为实时转录系统的方法。这些方法包括优化模型的架构以及利用更强大的计算资源。随着这些进展的持续，我们可以预见到处理时间将大幅减少，Whisper
    将更接近实现无缝的实时转录。'
- en: '**Integrating with edge computing**: The integration of Whisper with edge computing
    presents a promising avenue for overcoming latency issues. By processing data
    closer to the source of data generation, edge computing can drastically reduce
    the time it takes for audio to be transcribed. This approach accelerates transcription
    and alleviates bandwidth constraints, making real-time transcription more feasible
    and efficient.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与边缘计算的整合**：Whisper 与边缘计算的整合为克服延迟问题提供了一个有前景的途径。通过在数据生成源附近处理数据，边缘计算可以显著减少音频转录所需的时间。这一方法加速了转录过程，缓解了带宽限制，使实时转录变得更加可行和高效。'
- en: While the journey toward flawless real-time transcription with Whisper is fraught
    with technical challenges, the opportunities it presents are undeniably compelling.
    The latency, processing time, and contextual accuracy limitations are significant
    yet manageable. Through ongoing research, technological advancements, and innovative
    applications, Whisper stands on the cusp of redefining real-time transcription.
    As we look to the future, the integration of Whisper into our daily lives promises
    not only to enhance communication and accessibility but also to push the boundaries
    of what is possible with AI. The road ahead is challenging and exciting, underscoring
    the importance of continued exploration and development in this dynamic field.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过 Whisper 实现完美的实时转录之路充满了技术挑战，但它所带来的机遇无疑是令人振奋的。延迟、处理时间和上下文准确性方面的限制虽大，但可以管理。通过持续的研究、技术进步和创新应用，Whisper
    正站在重新定义实时转录的风口浪尖。展望未来，Whisper 在我们日常生活中的整合不仅有望提升沟通和可访问性，还将推动人工智能领域的可能性边界。前方的道路充满挑战和激动人心的机会，突显了在这一动态领域持续探索和发展的重要性。
- en: To better understand the challenges and potential of real-time speech recognition
    with Whisper, let’s dive into a practical example. In the following section, we
    will build an interactive real-time ASR demo using Hugging Face’s implementation
    of Whisper and the user-friendly Gradio library.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 Whisper 在实时语音识别中的挑战和潜力，让我们深入一个实际的示例。在接下来的部分，我们将使用 Hugging Face 对 Whisper
    的实现和用户友好的 Gradio 库构建一个互动式实时 ASR 演示。
- en: Building a real-time ASR demo with Hugging Face Whisper
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hugging Face Whisper 构建实时 ASR 演示
- en: 'In this section, we will leverage the power of Gradio, a user interface library,
    to rapidly construct an interactive demo of the Whisper model. This demo will
    allow you or others to test the model’s performance by speaking into the microphone
    on your device. Let’s find and run the `LOAIW_ch07_3_Building_real_time_ASR_with_HF_Whisper.ipynb`
    notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_3_Building_real_time_ASR_with_HF_Whisper.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_3_Building_real_time_ASR_with_HF_Whisper.ipynb)).
    The notebook is structured into three main sections:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用 Gradio（一个用户界面库）的强大功能，快速构建一个 Whisper 模型的互动演示。这个演示将允许你或其他人通过设备上的麦克风测试模型的表现。让我们找到并运行
    `LOAIW_ch07_3_Building_real_time_ASR_with_HF_Whisper.ipynb` 笔记本 ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_3_Building_real_time_ASR_with_HF_Whisper.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter07/LOAIW_ch07_3_Building_real_time_ASR_with_HF_Whisper.ipynb))。该笔记本分为三个主要部分：
- en: '`transformers` library to prepare the ASR model for our demo'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers` 库，用于为我们的演示准备 ASR 模型。'
- en: '**Creating a full-context ASR demo**: We will build a demo in which the user
    speaks the entire audio before the ASR model processes it and generates the transcription'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建完整上下文的 ASR 演示**：我们将构建一个演示，其中用户在 ASR 模型处理并生成转录之前讲述完整音频。'
- en: '**Creating a streaming ASR demo**: We will extend the previous demo to support
    real-time streaming, allowing the ASR model to transcribe the audio as the user
    speaks, providing a more interactive experience'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建流式 ASR 演示**：我们将扩展之前的演示，支持实时流式处理，让 ASR 模型在用户讲话时实时转录音频，提供更加互动的体验。'
- en: By the end of this notebook, you will have a solid understanding of creating
    engaging demos for speech recognition models using Gradio and the Hugging Face
    Transformers library.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到本笔记本的最后，你将能够充分理解如何使用 Gradio 和 Hugging Face Transformers 库为语音识别模型创建引人入胜的演示。
- en: Preparing the development environment
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备开发环境
- en: 'Before diving into building the speech recognition demos, it’s crucial to set
    up our development environment with the necessary dependencies. In this section,
    we will do the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建语音识别演示之前，首先需要设置开发环境并安装必要的依赖项。在这一部分，我们将进行以下操作：
- en: Install the required libraries, such as Gradio, to ensure a smooth development
    process.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的库，如 Gradio，以确保开发过程顺利进行。
- en: Configure the environment to work seamlessly with the Hugging Face Transformers
    library, allowing us to leverage pre-trained models and powerful NLP tools.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置环境，以便与 Hugging Face Transformers 库无缝协作，允许我们利用预训练模型和强大的 NLP 工具。
- en: By properly setting up our environment, we lay the foundation for an efficient
    and hassle-free coding experience throughout the notebook.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正确设置环境，我们为整个笔记本提供了一个高效且无烦恼的编程体验基础。
- en: To bring our exploration of real-time ASR with Whisper to life, we’ll first
    need to set up our development environment. Let’s walk through the installation
    of the necessary libraries and configuration of our setup to work seamlessly with
    the Hugging Face Transformers library.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们对实时 ASR 与 Whisper 的探索付诸实践，我们首先需要设置开发环境。让我们一起走过安装必要库和配置设置的过程，以确保能够与 Hugging
    Face Transformers 库无缝协作。
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Setting up your Hugging Face token is essential to ensure a seamless experience
    while working with this notebook. The notebook will load transformer classes and
    models from the Hugging Face repository, which requires valid token authentication.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 设置你的 Hugging Face 令牌对于确保在使用此笔记本时的流畅体验至关重要。该笔记本将从 Hugging Face 仓库加载 transformer
    类和模型，这需要有效的令牌认证。
- en: If you haven’t created a Hugging Face token yet or need a refresher on the process,
    please refer to [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb).
    This resource provides step-by-step instructions on how to create and configure
    your Hugging Face token.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有创建 Hugging Face 令牌，或者需要重新了解这一过程，请参考 [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb)。该资源提供了创建和配置
    Hugging Face 令牌的逐步说明。
- en: 'By setting up your token correctly, you’ll be able to easily access the full
    range of features and models available in the Hugging Face ecosystem, enabling
    you to build powerful speech recognition demos:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正确设置您的令牌，您将能够轻松访问Hugging Face生态系统中可用的所有功能和模型，从而使您能够构建强大的语音识别演示：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With our development environment set up, let’s begin by loading the transformers
    ASR model, which will serve as the foundation for our interactive application.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好我们的开发环境后，让我们首先加载transformers的ASR模型，它将作为我们交互式应用程序的基础。
- en: Step 1 – Loading the transformers ASR model
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 1 – 加载transformers ASR模型
- en: 'We first need an ASR model to begin building our speech recognition demo. You
    can either train your model or use a pre-trained one. Loading the `"whisper"`
    model from the Hugging Face `transformers` library is straightforward. Here’s
    the code snippet to accomplish this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要一个ASR模型来开始构建我们的语音识别演示。您可以选择训练自己的模型或使用预训练模型。加载来自Hugging Face `transformers`库的`"whisper"`模型非常简单。以下是实现此功能的代码片段：
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With just these two lines of code, we initialize a pipeline for automatic speech
    recognition using the `"openai/whisper-base.en"` model. The pipeline abstracts
    away the complexities of working with the model directly, providing a high-level
    interface for performing ASR tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过这两行代码，我们使用`"openai/whisper-base.en"`模型初始化了一个自动语音识别（ASR）管道。该管道抽象了直接使用模型时的复杂性，提供了一个高层次的接口来执行ASR任务。
- en: By utilizing a pre-trained model such as `"whisper"`, we can quickly start building
    our demo without the need for extensive model training. This allows us to focus
    on integrating the model into our application and creating an engaging user experience.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用像`"whisper"`这样的预训练模型，我们可以迅速开始构建演示，而无需进行大量的模型训练。这使我们能够专注于将模型集成到我们的应用程序中，并创造一个引人入胜的用户体验。
- en: Step 2 – Building a full-context ASR demo with transformers
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 2 – 使用transformers构建全上下文的ASR演示
- en: 'Our first step in creating the speech recognition demo is to build a *full-context*
    ASR demo. In this demo, the user will speak the entire audio before the ASR model
    processes it and generates the transcription. Thanks to Gradio’s intuitive interface,
    building this demo is a breeze:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 创建语音识别演示的第一步是构建一个*全上下文*的ASR演示。在这个演示中，用户将在ASR模型处理之前，先说完整个音频，模型生成转录文本。得益于Gradio直观的界面，构建这个演示变得非常简单：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the preceding snippet, we start by creating a function that wraps around
    the `pipeline` object we initialized earlier. This function serves as the core
    of our demo, handling the audio input and generating the transcription.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们首先创建了一个封装我们先前初始化的`pipeline`对象的函数。这个函数是我们演示的核心，负责处理音频输入并生成转录文本。
- en: We then utilize Gradio’s built-in `Audio` component to capture the user’s audio
    input. This component will be configured to accept input from the user’s microphone
    and return the file path of the recorded audio. We’ll use a simple `Textbox` component
    to display the transcribed text.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们利用Gradio内置的`Audio`组件来捕获用户的音频输入。该组件将被配置为接受来自用户麦克风的输入，并返回录制音频的文件路径。我们将使用一个简单的`Textbox`组件来显示转录文本。
- en: The `transcribe` function, the heart of our demo, takes a single parameter called
    `audio`. This parameter represents the audio data recorded by the user, stored
    as a `numpy` array. However, the `pipeline` object expects the audio data to be
    in the `float32` format. To ensure compatibility, we first convert the audio data
    to `float32` and then normalize it by dividing it by its maximum absolute value.
    Finally, we pass the processed audio data to the `pipeline` object to obtain the
    transcribed text.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`transcribe`函数是我们演示的核心，它接收一个名为`audio`的参数。这个参数代表用户录制的音频数据，以`numpy`数组的形式存储。然而，`pipeline`对象期望音频数据为`float32`格式。为了确保兼容性，我们首先将音频数据转换为`float32`格式，然后通过其最大绝对值进行归一化。最后，我们将处理后的音频数据传递给`pipeline`对象，以获得转录文本。'
- en: Step 3 – Enhancing the demo with real-time streaming capabilities
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 3 – 增强演示，添加实时流式传输功能
- en: 'To create a streaming ASR demo, we need to make the following changes in the
    Python Gradio script:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个流式ASR演示，我们需要在Python Gradio脚本中进行以下更改：
- en: Set `streaming=True` in the `Audio` component to enable continuous audio capture
    from the user’s microphone.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Audio`组件中设置`streaming=True`，以启用从用户麦克风连续捕获音频。
- en: Set `live=True` in the `Interface` component to ensure the interface updates
    dynamically as new audio data is received.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Interface`组件中设置`live=True`，以确保界面在接收到新音频数据时能够动态更新。
- en: Add a `state` variable to the interface to store the recorded audio and the
    previous transcription.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接口中添加`state`变量，以存储记录的音频和前一个转录。
- en: 'All these changes are already applied in the script:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修改已经在脚本中应用：
- en: '[PRE29]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the streaming demo, we use a `state` variable to keep track of the audio
    history and the previous transcription. The `transcribe` function is called whenever
    a new small chunk of audio is received, and it needs to process the new chunk
    along with the previously recorded audio.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式演示中，我们使用`state`变量来跟踪音频历史和前一个转录。每当收到一个新的小音频块时，都会调用`transcribe`函数，需要将新的音频块与之前记录的音频一起处理。
- en: 'To improve the accuracy and coherence of the transcription, we introduce a
    dynamic window size based on the duration of the new audio chunk and a slight
    overlap between consecutive windows. Here’s how the `transcribe` function works:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高转录的准确性和连贯性，我们引入了基于新音频块持续时间的动态窗口大小，并在连续的窗口之间设置轻微的重叠。以下是`transcribe`函数的工作原理：
- en: If the `state` is `None`, initialize an empty `numpy` array (`stream`) to store
    the audio and an empty string (`previous_text`) to store the previous transcription.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`state`为`None`，初始化一个空的`numpy`数组（`stream`）来存储音频，和一个空字符串（`previous_text`）来存储前一个转录。
- en: Extract `new_chunk`’s sampling rate (`sr`) and audio data (`y`) from `new_chunk`.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`new_chunk`中提取采样率（`sr`）和音频数据（`y`）。
- en: Calculate the duration of the new audio chunk and normalize the audio data.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新的音频块的持续时间并规范化音频数据。
- en: Introduce an overlap of half a second between consecutive windows to ensure
    continuity in the transcription.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在连续的窗口之间引入半秒的重叠，以确保转录的连续性。
- en: Concatenate the new audio chunk to the existing stream, considering the overlap.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新的音频块与现有流连接，考虑到重叠部分。
- en: Transcribe the entire stream using the `transcriber` object.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`transcriber`对象转录整个流。
- en: Update `previous_text` by removing the overlap from the end of the previous
    transcription and concatenating it with the new transcription.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新`previous_text`，通过去除前一个转录的重叠部分，并将其与新的转录合并。
- en: Return the updated `stream` and `combined_text` values as the state and the
    `combined_text` value as the transcription output.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回更新后的`stream`和`combined_text`值作为状态，以及`combined_text`值作为转录输出。
- en: By using a dynamic window size and introducing an overlap between consecutive
    windows, we can improve the accuracy and coherence of the streaming transcription.
    The small overlap helps maintain continuity in the transcription and reduces the
    occurrence of overlapping or missing words.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用动态窗口大小并在连续窗口之间引入重叠，我们可以提高流式转录的准确性和连贯性。小重叠有助于保持转录的连续性，并减少重叠或缺失单词的发生。
- en: Of course, this is a straightforward demo. It is designed to show that real-time
    with Whisper is not as far away from reality as it might appear. I encourage you
    to enhance and experiment with that demo and have fun!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个简单的演示。它的设计目的是展示Whisper的实时功能并没有看起来那么遥不可及。我鼓励你增强和实验这个演示，并玩得开心！
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we embarked on an exciting exploration of OpenAI’s Whisper’s
    advanced voice capabilities. We delved into powerful techniques that enhance Whisper’s
    performance, such as quantization, and uncovered its potential for real-time speech
    recognition.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始了对OpenAI的Whisper高级语音能力的激动人心的探索。我们深入研究了提升Whisper性能的强大技术，如量化，并揭示了其在实时语音识别中的潜力。
- en: We began by examining the power of quantization, which reduces the model’s size
    and computational requirements while maintaining accuracy. We learned how to apply
    quantization to Whisper using frameworks such as CTranslate2 and OpenVINO, enabling
    efficient deployment on resource-constrained devices. The hands-on experience
    quantizing Whisper using CTranslate2 and Distil-Whisper with OpenVINO provided
    practical insights into optimizing the model for various deployment scenarios.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从研究量化的力量开始，量化可以减少模型的大小和计算需求，同时保持准确性。我们学会了如何使用CTranslate2和OpenVINO等框架将量化应用于Whisper，实现了在资源受限设备上的高效部署。通过使用CTranslate2和Distil-Whisper与OpenVINO量化Whisper的实践经验，我们深入了解了如何优化模型以适应不同的部署场景。
- en: Furthermore, we tackled the challenges and opportunities of real-time speech
    recognition with Whisper. We gained insights into the current limitations, such
    as processing time and latency, and explored ongoing research efforts to make
    real-time transcription a reality. The experimental approach to building a streaming
    ASR demo using Whisper and Gradio provided a glimpse into the future possibilities
    of real-time speech recognition.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们利用Whisper解决了实时语音识别的挑战和机遇。我们深入了解了当前的限制，如处理时间和延迟，并探索了使实时转录成为现实的持续研究努力。利用Whisper和Gradio构建流式ASR演示的实验方法，展示了实时语音识别未来可能性的一瞥。
- en: Throughout the chapter, we acquired a solid understanding of advanced techniques
    to optimize Whisper’s performance and appreciate the potential and challenges
    of real-time speech recognition. The hands-on coding examples and practical insights
    equipped us with the knowledge and skills to apply these techniques in our projects,
    pushing the boundaries of what is possible with Whisper.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个章节中，我们获得了优化Whisper性能的高级技术的扎实理解，并且认识到了实时语音识别的潜力和挑战。通过实际操作的编码示例和实用见解，我们掌握了应用这些技术到我们的项目中的知识和技能，推动了Whisper的可能性边界。
- en: 'As we conclude this chapter, we look ahead to [*Chapter 8*](B21020_08.xhtml#_idTextAnchor186),
    *Diarizing Speech with WhisperX and NVIDIA’s NeMo*. While Whisper has proven to
    be a powerful tool for transcribing speech, there’s another crucial aspect of
    speech analysis that can significantly enhance its utility: speaker diarization.
    By augmenting Whisper with the ability to identify and attribute speech segments
    to different speakers, we open a new realm of possibilities for analyzing multispeaker
    conversations. Join me in the next chapter, and let’s explore how Whisper can
    be integrated with cutting-edge diarization techniques to unlock these capabilities.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的结束，我们展望[*第8章*](B21020_08.xhtml#_idTextAnchor186)，*使用WhisperX和NVIDIA的NeMo进行语音辨析*。虽然Whisper已被证明是一个强大的转录工具，但还有另一个关键的语音分析方面可以显著增强其效用：说话者辨析。通过增强Whisper的能力来识别和归因不同说话者的语音片段，我们为分析多说话者对话打开了新的可能性领域。请加入我在下一章，让我们探讨如何将Whisper与前沿的辨析技术结合，解锁这些功能。
