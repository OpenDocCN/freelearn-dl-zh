- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: The Mechanics of Training LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练LLMs的机制
- en: Here, we will guide you through the intricate process of training LLMs, starting
    with the crucial task of data preparation and management. This process is fundamental
    to getting LLMs to perform in a desired way. We will further explore the establishment
    of a robust training environment, delving into the science of hyperparameter tuning
    and elaborating on how to address overfitting, underfitting, and other common
    training challenges, giving you a thorough grounding in creating effective LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将引导您深入了解训练LLMs的复杂过程，从至关重要的数据准备和管理任务开始。这个过程对于使LLMs以期望的方式表现至关重要。我们将进一步探讨建立稳健的训练环境，深入研究超参数调优的科学，并详细阐述如何解决过拟合、欠拟合和其他常见的训练挑战，为您在创建有效的LLMs方面提供全面的基础。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Data – preparing the fuel for LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据——为LLMs准备燃料
- en: Setting up your training environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置您的训练环境
- en: Hyperparameter tuning – finding the sweet spot
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调优——找到最佳平衡点
- en: Challenges in training LLMs – overfitting, underfitting, and more
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练LLMs的挑战——过拟合、欠拟合等
- en: By the end of this chapter, you should understand the roadmap for training LLMs,
    emphasizing the pivotal role of comprehensive data preparation and management.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您应该了解训练LLMs的路线图，强调全面数据准备和管理的关键作用。
- en: Data – preparing the fuel for LLMs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据——为LLMs准备燃料
- en: Preparing datasets for the effective training of LLMs is a multi-step process
    that requires careful planning and execution. Here is a comprehensive guide on
    how to prepare datasets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据集以有效训练LLMs是一个多步骤的过程，需要周密的计划和执行。以下是准备数据集的全面指南。
- en: Data collection
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: '**Data collection** is a fundamental step in the development of LLMs and involves
    gathering a vast and varied set of text data that the model will use to learn.
    The quality and diversity of this corpus are critical as they directly influence
    the model’s ability to understand and generate language across different domains
    and styles. Let’s take a look at an expanded view of the data collection process:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据收集**是LLMs开发的基本步骤，涉及收集大量且多样化的文本数据，模型将使用这些数据来学习。语料库的质量和多样性至关重要，因为它们直接影响模型在不同领域和风格中理解和生成语言的能力。让我们看看数据收集过程的扩展视图：'
- en: '**Scope of corpus** : The corpus should cover a wide range of topics to prevent
    the model from developing a narrow understanding of language. It should include
    literature from various genres, informative articles from different fields, dialogues
    from conversational datasets, technical documents, and other relevant text sources.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语料库范围**：语料库应涵盖广泛的主题，以防止模型对语言形成狭窄的理解。它应包括来自各种体裁的文献、不同领域的信息文章、对话数据集的对话、技术文档和其他相关文本来源。'
- en: '**Language representation** : For multilingual models, the dataset must include
    texts in all target languages. It’s important to ensure that less-resourced languages
    are adequately represented to avoid bias toward the more dominant languages.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言表示**：对于多语言模型，数据集必须包括所有目标语言的文本。确保资源较少的语言得到充分代表，以避免对更主导语言的偏见。'
- en: '**Temporal diversity** : Including texts from different time periods can help
    the model understand language evolution and historical contexts, making it better
    at handling archaic terms and newer slang.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间多样性**：包括不同时期的文本可以帮助模型理解语言演变和历史背景，使其更擅长处理古语和新的俚语。'
- en: '**Cultural and demographic diversity** : The corpus should represent various
    cultural and demographic backgrounds to ensure that the model can understand and
    generate text that is inclusive and respectful of diversity.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文化和人口多样性**：语料库应代表各种文化和人口背景，以确保模型能够理解和生成包容且尊重多样性的文本。'
- en: '**Ethical compliance** : Data should be sourced from ethical channels, ensuring
    respect for copyright laws and intellectual property rights. This involves using
    texts that are in the public domain or obtaining appropriate licenses for protected
    content.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**道德合规性**：数据应来自道德渠道，确保尊重版权法和知识产权。这包括使用公共领域的文本或为受保护内容获取适当的许可证。'
- en: '**Legal compliance** : Comply with data privacy laws, such as GDPR or CCPA,
    especially when using texts that contain personal information. It’s essential
    to anonymize and aggregate data where necessary to protect individual privacy.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律合规性**：遵守数据隐私法律，如GDPR或CCPA，尤其是在使用包含个人信息的文本时。在必要时匿名化和汇总数据，以保护个人隐私。'
- en: '**Quality control** : Evaluate the quality of the texts to ensure they are
    free from errors and remove low-quality or spam content that could negatively
    influence the model’s learning process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制**：评估文本的质量，确保其无错误，并移除可能对模型学习过程产生负面影响的低质量或垃圾内容。'
- en: '**Balanced representation** : Avoid overrepresentation of certain topics that
    could lead to biased predictions. Ensure that the model is exposed to a balanced
    view of sensitive subjects.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡表示**：避免过度表示可能导致预测偏差的某些主题。确保模型接触到敏感主题的平衡视角。'
- en: '**Data format and annotation** : Depending on the intended use of the LLM,
    the data may need to be annotated with additional information, such as part-of-speech
    tags or named-entity labels. The format should be consistent to facilitate efficient
    processing during training.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据格式和标注**：根据LLM的预期用途，数据可能需要添加额外的信息，例如词性标签或命名实体标签。格式应保持一致，以方便在训练过程中的高效处理。'
- en: '**Data usage rights** : Secure the rights to use the data for **machine learning**
    ( **ML** ) purposes. This can involve negotiations and agreements with data providers,
    particularly for proprietary or commercial datasets.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据使用权利**：确保有权使用数据用于**机器学习**（**ML**）目的。这可能涉及与数据提供者的谈判和协议，特别是对于专有或商业数据集。'
- en: '**Ongoing collection** : Data collection is not a one-time process; it’s an
    ongoing activity that keeps the dataset up to date as languages evolve and new
    types of text emerge.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续收集**：数据收集不是一个一次性过程；它是一个持续的活动，确保数据集随着语言的发展和新型文本的出现而保持更新。'
- en: '**Source documentation** : Keep detailed records of where, when, and how data
    was collected. This documentation can be crucial for troubleshooting, audits,
    and reproducibility of research.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源文档**：详细记录数据的收集地点、时间和方式。这些文档对于故障排除、审计和研究可重复性至关重要。'
- en: By meticulously collecting and curating the data, developers can create LLMs
    that are well rounded, less biased, and more reliable in their understanding and
    generation of language.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过精心收集和整理数据，开发者可以创建出全面、偏见较少、在理解和生成语言方面更可靠的LLM。
- en: Data cleaning
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: '**Data cleaning** is a critical phase in preparing datasets for training LLMs,
    as it directly impacts the model’s ability to learn effectively. A more detailed
    look into the data cleaning process is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据清洗**是准备数据集以训练大型语言模型的关键阶段，因为它直接影响模型有效学习的能力。对数据清洗过程的更详细分析如下：'
- en: '**Correcting encoding issues** : Text data often comes from various sources,
    each potentially using different character encodings. It’s essential to standardize
    the text to a consistent encoding format, such as UTF-8, to avoid character corruption.
    Tools such as **iconv** or programming libraries in Python can automate this process.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**纠正编码问题**：文本数据通常来自各种来源，每个来源可能使用不同的字符编码。将文本标准化为一致的编码格式，如UTF-8，是避免字符损坏的关键。可以使用**iconv**或Python中的编程库来自动化此过程。'
- en: '**Removing noise** : Textual noise includes any irrelevant information that
    might confuse the model. This can be extraneous HTML tags, JavaScript code in
    web-scraped data, or corrupted text. Regular expressions and HTML parsers, such
    as Beautiful Soup, can help automate the removal of such noise.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除噪声**：文本噪声包括可能使模型混淆的任何无关信息。这可能包括额外的HTML标签、网络抓取数据中的JavaScript代码或损坏的文本。正则表达式和HTML解析器，如Beautiful
    Soup，可以帮助自动化此类噪声的删除。'
- en: '**Standardizing language** : Datasets may contain slang, abbreviations, or
    creative spellings. Depending on the model’s intended use, you might want to standardize
    these to their full forms to ensure consistency.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化语言**：数据集中可能包含俚语、缩写或创意拼写。根据模型的预期用途，您可能希望将这些标准化为全形式，以确保一致性。'
- en: '**Handling non-standard language** : If the dataset includes non-standard language
    elements, such as code snippets, mathematical formulas, or chemical equations,
    these should either be removed or systematically tagged if they are relevant to
    the model’s tasks.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理非标准语言**：如果数据集包含非标准语言元素，例如代码片段、数学公式或化学方程式，这些元素要么被删除，要么如果与模型任务相关，则应系统性地标记。'
- en: '**Anonymization** : **Personally identifiable information** ( **PII** ) must
    be detected and removed or anonymized to comply with privacy regulations. Techniques
    such as **named-entity recognition** ( **NER** ) can be used to identify PII,
    and various anonymization techniques can mask or remove this information.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**匿名化**：**个人身份信息**（**PII**）必须被检测并移除或匿名化，以符合隐私法规。可以使用**命名实体识别**（**NER**）等技术来识别PII，并使用各种匿名化技术来掩盖或删除此信息。'
- en: '**Dealing with missing values** : In structured datasets, missing values can
    be problematic. Depending on the situation, you might fill them with placeholder
    values, interpolate them based on nearby data, or omit the entries altogether.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理缺失值**：在结构化数据集中，缺失值可能成为问题。根据情况，您可能需要用占位符值填充它们，根据附近的数据进行插值，或者完全省略条目。'
- en: '**Unifying formats** : Dates, numbers, and other structured data should be
    converted to a uniform format. This can involve converting all dates to a standard
    format, such as YYYY-MM-DD, or ensuring all numbers are represented consistently.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一格式**：日期、数字和其他结构化数据应转换为统一格式。这可能包括将所有日期转换为标准格式，如YYYY-MM-DD，或确保所有数字表示一致。'
- en: '**Language correction** : Spelling errors and grammatical mistakes can be corrected
    using automated tools, such as spell checkers or language-parsing algorithms,
    although it’s important to be cautious not to over-standardize and remove nuances
    important for certain tasks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言校正**：可以使用自动工具，如拼写检查器或语言解析算法，来纠正拼写错误和语法错误，尽管重要的是要谨慎，不要过度标准化并移除对某些任务重要的小细节。'
- en: '**Duplicate removal** : Identifying and removing duplicate entries is important
    to prevent the model from giving undue weight to repeated information.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除重复项**：识别和移除重复条目对于防止模型对重复信息给予不适当的权重非常重要。'
- en: '**Data validation** : After cleaning, validate the dataset to ensure that the
    cleaning steps have been properly applied and that the data is in the correct
    format for model training.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证**：在清理后，验证数据集以确保清理步骤已正确应用，并且数据以正确的格式用于模型训练。'
- en: '**Quality assessment** : Perform a quality assessment, possibly with human
    review, to ensure the data meets the standards required for effective LLM training.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量评估**：进行质量评估，可能包括人工审查，以确保数据符合有效LLM训练所需的标准。'
- en: '**Irrelevant or outdated information** : Removing or updating irrelevant or
    outdated information ensures the model is trained on accurate and current data,
    which enhances its relevance and performance.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无关或过时信息**：移除或更新无关或过时信息确保模型在准确和当前的数据上训练，这增强了其相关性和性能。'
- en: Effective data cleaning not only improves the model’s performance but also contributes
    to the fairness and ethical use of LLMs by preventing the learning of biases and
    ensuring the privacy of individuals represented in the data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的数据清洗不仅提高了模型的表现，还有助于LLM的公平和道德使用，通过防止学习偏见并确保数据中代表个人的隐私。
- en: Tokenization
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: '**Tokenization** is a pivotal preprocessing step in preparing data for training
    LLMs. It involves breaking down the text into smaller units, known as **tokens**
    , which can be words, subwords, or even individual characters. The choice of tokenization
    granularity has a significant impact on the model’s subsequent training and performance.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**是准备数据以训练LLM的关键预处理步骤。它涉及将文本分解成更小的单元，称为**标记**，这些标记可以是单词、子词，甚至是单个字符。分词粒度的选择对模型后续的训练和性能有重大影响。'
- en: 'Here are the major tokenization approaches:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是主要的分词方法：
- en: '**Word-level tokenization** : This approach splits the text into words. It’s
    straightforward and works well for languages with clear word boundaries, such
    as English. However, it can lead to a very large vocabulary size, which in turn
    may increase the model’s complexity and resource requirements.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词级分词**：这种方法将文本分割成单词。它简单直接，对于具有清晰单词边界的语言（如英语）效果良好。然而，它可能导致词汇量非常大，这反过来又可能增加模型的复杂性和资源需求。'
- en: '**Subword tokenization** : Subword tokenization techniques, such as **byte-pair
    encoding** ( **BPE** ) or WordPiece, split words into smaller, more frequent pieces.
    This method can effectively reduce vocabulary size and handle out-of-vocabulary
    words by breaking them down into subword units. It strikes a balance between the
    flexibility of character-level models and the efficiency of word-level models.
    Subword tokenization is particularly useful for agglutinative languages where
    many morphemes combine to form a single word, or in cases where the model needs
    to handle a mix of different languages with varying morphologies.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子词标记化**：子词标记化技术，如**字节对编码**（**BPE**）或WordPiece，将单词拆分成更小、更频繁的片段。这种方法可以有效地减少词汇量，并通过将它们分解成子词单元来处理词汇表外的单词。它在字符级模型的灵活性和词级模型的效率之间取得了平衡。子词标记化对于粘着语来说特别有用，在这些语言中，许多词素组合成一个单词，或者当模型需要处理具有不同形态学的不同语言的混合时。'
- en: '**Character-level tokenization** : In character-level tokenization, each character
    is treated as a separate token. This method ensures a small, fixed vocabulary
    size and allows the model to learn all the nuances of word formation. However,
    it can make learning long-range dependencies more challenging due to the increased
    sequence lengths.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符级标记化**：在字符级标记化中，每个字符都被视为一个单独的标记。这种方法确保了词汇量小且固定，并允许模型学习单词形成的所有细微差别。然而，由于序列长度的增加，它可能会使学习长距离依赖关系更具挑战性。'
- en: '**Tokenization for specialized tasks** : For certain tasks, such as NER or
    part-of-speech tagging, tokenization might need to align with the linguistic properties
    of the text. Tokens may need to correspond to meaningful linguistic units, such
    as phrases or syntactic chunks.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**针对特定任务的标记化**：对于某些任务，如命名实体识别或词性标注，标记化可能需要与文本的语言特性对齐。标记可能需要对应于有意义的语言单位，如短语或句法块。'
- en: '**Advanced techniques** : More recent approaches, such as SentencePiece or
    Unigram language model tokenization, don’t rely on white space to determine token
    boundaries and can work well across multiple languages, including those without
    clear white space delimiters.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级技术**：更近期的技术，如SentencePiece或一元语言模型标记化，不依赖于空白字符来确定标记边界，并且可以很好地跨多种语言工作，包括那些没有明确空白字符分隔符的语言。'
- en: 'These are the considerations to take into account with tokenization:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑标记化时，需要考虑以下因素：
- en: '**Consistency** : It’s important to apply the same tokenization method consistently
    across the entire dataset to prevent discrepancies that could hinder the model’s
    learning process.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：在整个数据集上始终如一地应用相同的标记化方法很重要，以防止可能阻碍模型学习过程的差异。'
- en: '**Handling special tokens** : LLMs often require special tokens to signify
    the start and end of sequences or to separate segments within the input. The tokenization
    process should incorporate these special tokens appropriately.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理特殊标记**：LLM通常需要特殊标记来表示序列的开始和结束，或用于分隔输入中的段。标记化过程应适当纳入这些特殊标记。'
- en: '**Alignment with downstream tasks** : The tokenization granularity should consider
    the end use of the LLM. For fine-grained tasks, such as translation or text generation,
    subword- or word-level tokenization might be preferable, while for character-level
    modeling of syntax or phonetics, character-level tokenization could be more appropriate.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与下游任务的对齐**：标记化粒度应考虑LLM的最终用途。对于细粒度任务，如翻译或文本生成，子词级或词级标记化可能更合适，而对于语法或语音学的字符级建模，字符级标记化可能更合适。'
- en: Ultimately, the choice of tokenization impacts the model’s ability to understand
    and generate language and should be carefully considered in the context of the
    specific goals and constraints of the LLM training project.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，标记化的选择会影响模型理解和生成语言的能力，因此在LLM训练项目的具体目标和约束条件下应仔细考虑。
- en: Annotation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标注
- en: '**Annotation** , in the context of training LLMs for supervised learning tasks,
    is a meticulous process where the raw data is enriched with additional information
    that defines the correct output for a given input. This process allows the model
    to not only ingest the raw data but also to learn from the correct interpretations
    or classifications provided by these annotations. Let’s get a deeper insight into
    this process:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**标注**，在为监督学习任务训练LLM的背景下，是一个细致的过程，其中原始数据通过添加定义给定输入正确输出的额外信息而得到丰富。这个过程不仅允许模型摄取原始数据，而且还可以从这些标注提供的正确解释或分类中学习。让我们更深入地了解这个过程：'
- en: '**Next-word prediction** : For tasks such as language modeling, data is annotated
    in a way that the model can learn to predict the next word in a sequence. This
    often involves shifting the sequence of tokens so that for each input token, the
    output token is the next word in the original text. The model learns to associate
    sequences of tokens with their subsequent tokens.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一词预测**：对于诸如语言建模等任务，数据以模型能够学习预测序列中下一个词的方式进行标注。这通常涉及将标记序列进行移位，以便对于每个输入标记，输出标记是原始文本中的下一个词。模型学习将标记序列与其后续标记关联起来。'
- en: '**Sentiment analysis** : When preparing data for sentiment analysis, human
    annotators review text segments, such as sentences or paragraphs, and label them
    with sentiment scores or categories, such as positive, negative, or neutral. The
    precision of this annotation process is critical as it directly impacts the model’s
    ability to correctly identify sentiment in new texts.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：在准备情感分析数据时，人工标注者会审查文本片段，如句子或段落，并使用情感分数或类别（如正面、负面或中性）对其进行标注。此标注过程的精确度至关重要，因为它直接影响模型正确识别新文本中情感的能力。'
- en: '**NER** : In NER tasks, annotators label words or phrases in the text that
    correspond to entities such as person names, organizations, locations, and so
    on. This labeling is often done using a tagging schema such as **beginning, inside,
    outside** ( **BIO** ), which marks not just the entity, but also the position
    of the word within the entity.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别（NER）**：在NER任务中，标注者会对文本中的单词或短语进行标注，这些单词或短语对应于人名、组织、地点等实体。这种标注通常使用如**开始、内部、外部**（**BIO**）这样的标记方案，它不仅标记实体，还标记单词在实体中的位置。'
- en: '**Accuracy and consistency** : To ensure the model learns correctly, annotations
    must be accurate and consistent. This often involves creating a detailed annotation
    guideline that annotators can follow to reduce subjectivity and variance in the
    labeling process.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性和一致性**：为确保模型正确学习，标注必须准确且一致。这通常涉及创建详细的标注指南，标注者可以遵循以减少标注过程中的主观性和差异。'
- en: '**Annotation tools** : Specialized software tools are used to facilitate the
    annotation process. These tools can provide a user-friendly interface for annotators,
    automate parts of the annotation process with pre-annotations using heuristics
    or semi-supervised methods, and manage the workflow of large-scale annotation
    projects.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注工具**：专门软件工具被用于简化标注过程。这些工具可以为标注者提供用户友好的界面，通过使用启发式方法或半监督方法进行预标注来自动化标注过程的部分，并管理大规模标注项目的流程。'
- en: '**Quality control** : Implementing quality control mechanisms is essential.
    This may involve multiple annotators labeling the same data and using inter-annotator
    agreement metrics to ensure quality or having expert reviewers validate the annotations.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制**：实施质量控制机制是必不可少的。这可能涉及多个标注者对同一数据进行标注，并使用标注者间一致性指标来确保质量，或者让专家审查者验证标注。'
- en: '**Handling ambiguity** : For ambiguous cases, it’s important to either design
    the annotation guidelines to capture the ambiguity or have a strategy for resolving
    it, such as consensus among multiple annotators or deferring to expert judgment.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理歧义**：对于模糊情况，重要的是要么设计标注指南以捕捉歧义，要么制定解决策略，例如多个标注者之间的共识或依赖专家判断。'
- en: '**Scalability** : For LLMs, the annotation process must be scalable due to
    the large amounts of data required. This may involve crowdsourcing platforms or
    collaboration with professional data annotation companies.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：对于LLM来说，由于需要大量数据，标注过程必须是可扩展的。这可能涉及众包平台或与专业数据标注公司合作。'
- en: '**Privacy considerations** : If the data being annotated contains personal
    or sensitive information, privacy-preserving measures must be taken, including
    data anonymization and securing the consent of the data subjects, if necessary.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私考虑**：如果被标注的数据包含个人信息或敏感信息，必须采取隐私保护措施，包括数据匿名化和在必要时确保数据主体的同意。'
- en: Annotations are foundational for supervised learning as they provide the ground
    truth that the model strives to predict correctly. The quality of the training
    data annotations directly correlates with the performance of the LLM on the task
    it’s being trained for.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 标注对于监督学习是基础性的，因为它们提供了模型努力预测正确的真实信息。训练数据的标注质量直接关联到LLM在训练任务上的表现。
- en: Data augmentation
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: '**Data augmentation** is an important technique in preparing datasets for training
    LLMs as it helps to create a more robust and generalizable model by artificially
    expanding the diversity of the training data. The following is a more in-depth
    explanation of some common data augmentation techniques:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据增强**是准备用于训练大型语言模型（LLM）的数据集的重要技术，因为它通过人工扩展训练数据的多样性，有助于创建一个更稳健和更具普遍性的模型。以下是对一些常见数据增强技术的更深入解释：'
- en: '**Synthetic data generation** : This involves creating new data points from
    existing ones through various transformations. For text, this could mean using
    techniques such as random insertion, deletion, or swapping of words within a sentence
    while preserving grammatical correctness and meaning. Synonym replacement is another
    common method, where certain words are replaced with their synonyms.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合成数据生成**：这涉及通过各种转换从现有数据中创建新的数据点。对于文本，这可能意味着使用诸如随机插入、删除或交换句子内单词等技术，同时保持语法正确性和意义。'
- en: '**Back translation** : This is a popular method for augmenting text data, especially
    in the context of machine translation. Here, a sentence is translated from one
    language to another (usually with an LLM) and then translated back to the original
    language. The round-trip translation process introduces linguistic variations,
    providing a form of paraphrasing that can help the model generalize better.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回译**：这是一种流行的文本数据增强方法，尤其是在机器翻译的背景下。在这里，一个句子被翻译成另一种语言（通常使用LLM），然后将其翻译回原始语言。往返翻译过程引入了语言变体，提供了一种可以有助于模型更好地泛化的释义形式。'
- en: '**Noise injection** : Introducing noise into the data can make models more
    robust to variations and potential input errors. For textual data, this might
    involve adding typographical errors, playing with different casing, or inserting
    additional white space.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声注入**：向数据中引入噪声可以使模型对变化和潜在的输入错误更具鲁棒性。对于文本数据，这可能包括添加印刷错误、玩弄不同的大小写或插入额外的空白。'
- en: '**Paraphrasing** : Generating paraphrases of sentences or phrases can expand
    the dataset with diverse linguistic structures conveying the same meaning. Paraphrasing
    can be done using rule-based approaches or by employing models trained specifically
    for this task.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**释义**：生成句子或短语的释义可以扩展数据集，包含传达相同意义的多样化语言结构。释义可以通过基于规则的途径或通过使用专门为此任务训练的模型来完成。'
- en: '**Data warping** : In the context of sequential data, such as text, warping
    can mean altering the sequence length by summarizing or expanding passages of
    text.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据扭曲**：在序列数据（如文本）的背景下，扭曲可能意味着通过总结或扩展文本段落来改变序列长度。'
- en: '**Using external datasets** : Incorporating data from external sources that
    are not part of the original dataset can also help in improving the diversity
    and size of the training corpus.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用外部数据集**：将来自外部来源的数据（这些数据不属于原始数据集）纳入其中，也有助于提高训练语料库的多样性和规模。'
- en: '**Translation augmentation** : For multilingual models, sentences can be translated
    into various languages and added to the dataset, increasing the model’s exposure
    to different linguistic patterns.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译增强**：对于多语言模型，可以将句子翻译成各种语言并添加到数据集中，从而增加模型对不同语言模式的接触。'
- en: '**Generative models** : Advanced data augmentation may utilize other generative
    models to create new data instances. For instance, **generative adversarial networks**
    ( **GANs** ) can be trained to generate text that is similar to human-written
    text.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**：高级数据增强可能利用其他生成模型来创建新的数据实例。例如，**生成对抗网络**（**GANs**）可以被训练生成类似于人类撰写的文本。'
- en: '**Relevance to task** : The augmentation strategies chosen must be relevant
    to the task the LLM will perform. For example, while synonym replacement may be
    useful for general language-understanding models, it might not be suitable for
    domain-specific models where terminology precision is critical.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与任务的相关性**：选择的数据增强策略必须与LLM将要执行的任务相关。例如，虽然同义词替换可能对通用语言理解模型有用，但它可能不适合术语精确性至关重要的特定领域模型。'
- en: '**Balancing augmented data** : It’s essential to ensure that the augmented
    data does not introduce its own biases or imbalances. The augmented instances
    should be mixed carefully with the original data to maintain a balanced and representative
    dataset.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡增强数据**：确保增强数据不会引入其自身的偏差或不平衡是很重要的。增强实例应与原始数据仔细混合，以保持平衡且具有代表性的数据集。'
- en: '**Quality control** : After augmentation, the quality of the new data should
    be assessed to ensure that it is suitable for training. Poor-quality augmented
    data can be detrimental to the training process.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制**：在增强后，应评估新数据的质量，以确保其适合训练。低质量的数据增强可能会损害训练过程。'
- en: Data augmentation not only helps prevent overfitting by effectively increasing
    the size of the training set but also introduces the model to a wider range of
    linguistic phenomena, which is particularly important for tasks requiring high
    generalization capabilities.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强不仅通过有效地增加训练集的大小来帮助防止过拟合，而且还将模型引入更广泛的语 言现象，这对于需要高泛化能力的任务尤为重要。
- en: Preprocessing
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理
- en: '**Preprocessing** is a critical stage in preparing data for training LLMs.
    It involves various techniques to standardize and simplify the data, which can
    facilitate the model’s learning process by reducing the complexity of the input
    space. Here’s an expanded explanation of these preprocessing techniques:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理**是准备数据以训练大型语言模型的关键阶段。它涉及各种技术，用于标准化和简化数据，通过减少输入空间的复杂性，可以促进模型的 学习过程。以下是这些预处理技术的扩展说明：'
- en: '**Lowercasing** : This process converts all letters in the text to lowercase.
    It’s a way to normalize words so that “The,” “the,” and “THE” are all treated
    as the same token, reducing vocabulary size. However, it may not always be appropriate,
    especially when case is significant, such as in proper nouns or in languages where
    case changes can alter the meaning of a word.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小写化**：这个过程将文本中的所有字母转换为小写。这是一种使单词规范化的方法，以便“The”、“the”和“THE”都被视为相同的标记，从而减少词汇量。然而，这并不总是合适的，尤其是在大小写有重要意义的情况下，例如专有名词或在大小写变化可以改变词义的语言中。'
- en: '**Stemming** : Stemming reduces words to their base or root form. For example,
    “running,” “runs,” and “ran” might all be stemmed to “run.” This can help in consolidating
    different forms of a word, allowing the model to learn a more generalized representation.
    Stemming algorithms, however, can be too crude at times, as they often apply a
    set of rules without understanding the context (for example, “university” and
    “universe” might be incorrectly stemmed to the same root).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：词干提取将单词还原为其基本或词根形式。例如，“running”、“runs”和“ran”都可能被提取为“run”。这有助于合并单词的不同形式，使模型能够学习更通用的表示。然而，词干提取算法有时可能过于粗糙，因为它们通常应用一套规则，而不理解上下文（例如，“university”和“universe”可能被错误地提取到相同的词根）。'
- en: '**Lemmatization** : More sophisticated than stemming, lemmatization involves
    reducing words to their canonical or dictionary form (lemma). A lemmatizer takes
    into account the word’s part of speech and its meaning in the sentence. Thus,
    “better” would be lemmatized to “good” when used as an adjective. Lemmatization
    helps in accurately condensing the various inflected forms of a word, which can
    be particularly useful for languages with rich morphology.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原**：比词干提取更复杂，词形还原涉及将单词还原为其规范或词典形式（词元）。词形还原器会考虑单词的词性和其在句子中的意义。因此，当用作形容词时，“better”会被还原为“good”。词形还原有助于准确地压缩单词的各种屈折形式，这对于形态丰富的语言尤其有用。'
- en: '**Normalization** : Text normalization includes correcting misspellings, expanding
    contractions (for example, converting “can’t” to “cannot”), and standardizing
    expressions. This step ensures that the model isn’t learning from or perpetuating
    errors in the data.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规范化**：文本规范化包括纠正拼写错误、扩展缩写（例如，将“can’t”转换为“cannot”）和标准化表达。这一步骤确保模型不会从数据中学习或延续错误。'
- en: '**Removing punctuation and special characters** : Non-alphanumeric characters
    can be stripped out if they’re not useful for the model’s task. However, in tasks
    such as sentiment analysis or machine translation, punctuation can carry significant
    meaning and should be retained.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除标点和特殊字符**：如果非字母数字字符对模型的任务没有帮助，则可以将其删除。然而，在诸如情感分析或机器翻译等任务中，标点可能承载着重要的意义，应该保留。'
- en: '**Handling stop words** : Commonly occurring words (such as “and,” “the,” or
    “is”) that may not add much semantic value to the model’s understanding can be
    removed. However, for some LLMs, especially those aimed at understanding complete
    sentences or paragraphs, stop words can provide essential context and should be
    kept.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理停用词**：常见单词（如“and”、“the”或“is”）可能不会为模型的理解增加太多语义价值，可以将其删除。然而，对于某些大型语言模型，尤其是那些旨在理解完整句子或段落的目标模型，停用词可以提供重要的上下文，应该保留。'
- en: '**Tokenization** : As previously mentioned, tokenization is the process of
    splitting text into manageable pieces or tokens. It’s a necessary preprocessing
    step that directly affects the model’s vocabulary.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：如前所述，分词是将文本分割成可管理的片段或标记的过程。这是一个必要的预处理步骤，它直接影响模型的词汇表。'
- en: For LLMs that aim to grasp the finer nuances of language or to generate human-like
    text, it’s often important to maintain the original casing and form of words.
    In such cases, preprocessing should be carefully balanced to avoid losing meaningful
    linguistic information. For example, in NER, maintaining the case is crucial for
    distinguishing between common nouns and proper nouns.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旨在掌握语言细微差别或生成类似人类文本的LLMs，保持原始的词形和大小写通常很重要。在这种情况下，预处理应仔细平衡，以避免丢失有意义的语言信息。例如，在命名实体识别（NER）中，保持词形对于区分普通名词和专有名词至关重要。
- en: Preprocessing must be tailored to the specific requirements of the LLM and the
    nature of the task it will perform. It’s a delicate balance between simplifying
    the data to aid in learning general patterns and retaining enough complexity to
    allow the model to make nuanced linguistic distinctions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理必须根据LLM的具体要求和将要执行的任务的性质进行定制。这是在简化数据以帮助学习一般模式与保留足够的复杂性以允许模型进行细微的语言区分之间的一种微妙平衡。
- en: Validation split
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证集
- en: 'The **validation split** is a critical part of the data preparation process
    for training ML models, including LLMs. This process involves dividing the complete
    dataset into the following three distinct subsets, where each set plays a different
    role in the development and evaluation of the model:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**验证集**是训练机器学习模型（包括LLMs）数据准备过程中的关键部分。这个过程涉及将完整的数据集划分为以下三个不同的子集，每个子集在模型开发和评估中扮演不同的角色：'
- en: '**Training set** : This is the largest portion of the dataset and is used for
    the actual training of the model. The model learns to make predictions or generate
    text by finding patterns in this data. The training process involves adjusting
    the model’s weights based on the error between its predictions and the actual
    outcomes.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：这是数据集的最大部分，用于模型的实际训练。模型通过在此数据中寻找模式来学习进行预测或生成文本。训练过程涉及根据模型预测与实际结果之间的误差调整模型的权重。'
- en: '**Validation set** : The validation set is used to evaluate the model during
    the training process, but it is not used to directly train the model. After each
    **epoch** (a complete pass through the training set), the model’s performance
    is tested on the validation set. This performance serves as an indicator of how
    well the model is generalizing to unseen data. The results from the validation
    set are used to tune the model’s hyperparameters, such as the learning rate, the
    model architecture, and regularization parameters. It can also be used for early
    stopping, which is a form of regularization where training is halted once the
    model’s performance on the validation set stops improving.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：验证集用于在训练过程中评估模型，但不用于直接训练模型。在每个**epoch**（完整遍历训练集）之后，模型在验证集上的性能会被测试。这种性能作为模型对未见数据泛化能力的指示器。验证集的结果用于调整模型的超参数，如学习率、模型架构和正则化参数。它还可以用于早期停止，这是一种正则化形式，其中一旦模型在验证集上的性能停止提高，训练就会停止。'
- en: '**Test set** : This is a set of data that the model has never seen during training
    and is not used in the hyperparameter tuning process. It is kept aside and used
    only after the model has been fully trained and validated. The test set provides
    an unbiased evaluation of the final model’s performance and its ability to generalize
    to new data. It is the best estimate of how the model will perform in the real
    world on unseen data.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：这是一个模型在训练过程中从未见过的数据集，也不用于超参数调整过程。它被保留下来，仅在模型完全训练和验证后使用。测试集提供了对最终模型性能及其对新数据泛化能力的无偏评估。这是对模型在现实世界中处理未见数据的最佳估计。'
- en: The way the data is split can vary depending on the amount of data available
    and the nature of the task. A common split ratio is 70% for training, 15% for
    validation, and 15% for testing, but this can be adjusted as needed. For instance,
    in cases where data is scarce, cross-validation techniques might be used, where
    the validation set is rotated through different subsets of the data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的分割方式可能因可用数据的数量和任务性质而异。常见的分割比例是70%用于训练，15%用于验证，15%用于测试，但可以根据需要进行调整。例如，在数据稀缺的情况下，可能会使用交叉验证技术，其中验证集在不同的数据子集之间轮换。
- en: It’s crucial that the distribution of data in the training, validation, and
    test sets reflects the true distribution of the real-world data the model will
    encounter. This means that all classes or categories of interest should be represented
    proportionally in each set. The process of splitting the data should also be random
    to avoid introducing any bias.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 确保训练集、验证集和测试集中的数据分布反映了模型将遇到的现实世界数据的真实分布至关重要。这意味着所有感兴趣的类别或类别都应该在每个集中按比例表示。数据分割的过程也应该是随机的，以避免引入任何偏差。
- en: A well-constructed validation split ensures that the LLM can be effectively
    tuned and ultimately performs well on the task it was designed for, while a final
    evaluation on the test set provides confidence in the model’s real-world applicability.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个构建良好的验证集分割确保LLM可以被有效地调整，并且最终在其设计的任务上表现良好，而在测试集上的最终评估则提供了模型在实际应用中的信心。
- en: Feature engineering
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: '**Feature engineering** is a process in ML where specific information is extracted
    or derived from raw data to improve a model’s ability to learn. In the context
    of LLMs and **natural language processing** ( **NLP** ), feature engineering can
    be particularly important for tasks that require an understanding of the structure
    and meaning of the text. A detailed look at what this might entail is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**是机器学习中的一个过程，其中从原始数据中提取或推导出特定信息以提高模型的学习能力。在LLM和**自然语言处理**（**NLP**）的背景下，特征工程对于需要理解文本结构和意义的任务尤为重要。以下是对此可能包含的内容的详细探讨：'
- en: '**Parsing text for syntactic features** : Syntactic parsing involves breaking
    down a sentence into its grammatical components, such as nouns, verbs, and phrases.
    This can help an LLM understand the grammatical structure of sentences, which
    is especially useful for tasks such as translation or part-of-speech tagging.
    Syntactic features can include parse trees, parts of speech, and grammatical relationships
    between words.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解析文本以获取句法特征**：句法解析涉及将句子分解为其语法成分，如名词、动词和短语。这有助于LLM理解句子的语法结构，这对于翻译或词性标注等任务特别有用。句法特征可以包括解析树、词性和词语之间的语法关系。'
- en: '**Word embeddings** : Words can be converted into numerical vectors, known
    as embeddings, that capture their semantic meaning. Techniques such as Word2Vec,
    GloVe, or fastText analyze the text corpus and produce a high-dimensional space
    where semantically similar words are closer together. For LLMs, these embeddings
    provide a dense, information-rich representation of the input text.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：单词可以被转换为称为嵌入的数值向量，这些向量捕捉了它们的语义意义。Word2Vec、GloVe或fastText等技术分析文本语料库，并产生一个高维空间，其中语义相似的单词彼此更接近。对于LLM来说，这些嵌入提供了输入文本的密集、信息丰富的表示。'
- en: '**Character embeddings** : Similar to word embeddings, character embeddings
    represent individual characters in a vector space. This can be useful for understanding
    morphology and is beneficial for languages where word boundaries are not as clear.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符嵌入**：与词嵌入类似，字符嵌入在向量空间中代表单个字符。这有助于理解词形学，并且对于词边界不太清晰的语言来说是有益的。'
- en: '**N-gram features** : N-grams are continuous sequences of *n* items from a
    given sample of text. Creating features based on n-grams can capture the context
    around words and phrases, which can be valuable for models that need to understand
    local context.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N-gram特征**：N-gram是从给定文本样本中连续的*n*个项目的序列。基于n-gram创建的特征可以捕捉词语和短语周围的上下文，这对于需要理解局部上下文的模型来说非常有价值。'
- en: '**Entity embeddings** : In tasks that involve named entities, creating embeddings
    for entities that encode additional information about them (such as their type
    or relationships to other entities) can improve the model’s performance.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体嵌入**：在涉及命名实体的任务中，为实体创建嵌入，这些嵌入编码了关于它们额外的信息（例如它们的类型或其他实体之间的关系）可以提高模型的表现。'
- en: '**Semantic role labeling** : This is the process of assigning roles to words
    in a sentence, identifying what role each word plays in the conveyed action or
    state. Features derived from semantic role labeling can enhance the model’s understanding
    of sentence meaning.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义角色标注**：这是将角色分配给句子中的词语的过程，确定每个词语在传达的动作或状态中所扮演的角色。从语义角色标注推导出的特征可以增强模型对句子意义的理解。'
- en: '**Dependency parsing features** : Features derived from the dependencies between
    words in a sentence can help in understanding the relational structure of the
    text, which can be crucial for tasks that require a deep understanding of sentence
    semantics.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依存关系解析特征**：从句子中词语之间的依存关系推导出的特征有助于理解文本的语义结构，这对于需要深入理解句子语义的任务至关重要。'
- en: '**Part-of-speech tags** : These tags are helpful features for many NLP tasks,
    as they provide the model with information about the grammatical category of each
    word.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注**：这些标签对于许多自然语言处理（NLP）任务是有帮助的特征，因为它们为模型提供了关于每个词语语法类别的信息。'
- en: '**Transformations and interactions** : For certain tasks, it may be beneficial
    to engineer features that represent interactions between different words or parts
    of the text, such as whether two entities occur in the same sentence or paragraph.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换和交互**：对于某些任务，设计代表不同词语或文本部分之间交互的特征可能是有益的，例如，两个实体是否出现在同一句子或段落中。'
- en: '**Domain-specific features** : For specialized tasks, it might be necessary
    to engineer features that are specific to a domain. For example, in legal documents,
    features might represent references to laws or precedents.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特定领域的特征**：对于特定任务，可能需要设计特定于该领域的特征。例如，在法律文件中，特征可能代表对法律或先例的引用。'
- en: '**Sentiment scores** : For sentiment analysis tasks, features might include
    sentiment scores of sentences or phrases, which can be obtained from pre-trained
    sentiment analysis models or lexicons.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分数**：对于情感分析任务，特征可能包括句子或短语的情感分数，这些分数可以从预训练的情感分析模型或词汇表中获得。'
- en: The process of feature engineering requires domain knowledge and an understanding
    of the model’s architecture and capabilities. While deep learning models, particularly
    LLMs, are capable of automatically learning representations from raw data, manually
    engineered features can still provide a performance boost, especially in cases
    where the model needs to understand complex relationships or when training data
    is limited.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的过程需要领域知识和对模型架构及能力的理解。虽然深度学习模型，尤其是大型语言模型（LLMs），能够从原始数据中自动学习表示，但手动设计的特征仍然可以提供性能提升，尤其是在模型需要理解复杂关系或训练数据有限的情况下。
- en: Balancing the dataset
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平衡数据集
- en: 'Balancing a dataset is a key aspect of preparing data for training LLMs. The
    goal is to create a dataset that represents the variety of outputs the model will
    need to predict without overrepresenting any particular class, style, or genre.
    This is essential to avoid biases that could skew the model’s predictions when
    applied in real-world situations. Let’s go through an expanded explanation of
    dataset balancing:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据集是准备数据以训练大型语言模型（LLMs）的关键方面。目标是创建一个数据集，该数据集代表了模型需要预测的各种输出，同时不过度代表任何特定的类别、风格或体裁。这对于避免在现实世界应用时可能导致模型预测偏差的偏差至关重要。让我们通过扩展解释数据集平衡：
- en: '**Class balance** : In classification tasks, it’s crucial to have an approximately
    equal number of examples for each class. If one class is overrepresented in the
    training data, the model might become biased toward predicting that class more
    frequently, regardless of the input. Balancing can be achieved by undersampling
    the overrepresented classes, oversampling the underrepresented classes, or synthesizing
    new data for underrepresented classes.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别平衡**：在分类任务中，对于每个类别拥有大致相等的示例数量至关重要。如果某个类别在训练数据中过度代表，模型可能会倾向于更频繁地预测该类别，而不管输入如何。可以通过减少过度代表的类别的样本量、增加代表性不足的类别的样本量或为代表性不足的类别合成新数据来实现平衡。'
- en: '**Genre and style diversity** : For LLMs expected to generate or understand
    text across various genres and styles, the training data should include a mix
    of literary, journalistic, conversational, and technical writing, among others.
    This diversity ensures the model does not become biased toward a specific writing
    style or genre, which can limit its effectiveness.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体裁和风格多样性**：对于预期能够生成或理解各种体裁和风格的LLM，训练数据应包括文学、新闻、对话和技术写作等多种类型的混合。这种多样性确保模型不会偏向于特定的写作风格或体裁，这可能会限制其有效性。'
- en: '**Topic and domain coverage** : Including a wide range of topics and domains
    helps prevent the model from developing topic-specific biases. For instance, a
    model trained primarily on sports articles might struggle to understand or generate
    text related to medical information.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题和领域覆盖**：包括广泛的主题和领域有助于防止模型发展特定主题的偏见。例如，主要在体育文章上训练的模型可能难以理解或生成与医疗信息相关的文本。'
- en: '**Demographic representation** : In scenarios where the model interacts with
    users or generates user-facing content, it’s important for the dataset to represent
    the demographic diversity of the target audience. This involves including text
    that reflects different age groups, cultural backgrounds, and dialects.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口代表性**：在模型与用户互动或生成面向用户内容的情况下，数据集需要代表目标受众的群体多样性。这包括包含反映不同年龄组、文化背景和方言的文本。'
- en: '**Time period representation** : Historical balance can prevent temporal biases.
    Older texts can teach the model about outdated language forms, while newer texts
    ensure it is up to date with contemporary usage, including slang and neologisms.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间段代表性**：历史平衡可以防止时间偏见。较旧的文本可以教会模型关于过时语言形式的知识，而较新的文本确保模型能够跟上当代用法，包括俚语和新词。'
- en: '**Mitigating implicit biases** : Even with balanced classes and diversity,
    datasets can contain implicit biases that are less obvious. These can include
    gender, racial, or ideological biases. Active measures may be needed to identify
    and mitigate these biases, such as using fairness metrics or bias detection tools.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减轻隐含偏见**：即使在平衡的类别和多样性中，数据集也可能包含不那么明显的隐含偏见，如性别、种族或意识形态偏见。可能需要采取积极措施来识别和减轻这些偏见，例如使用公平性指标或偏见检测工具。'
- en: '**Data augmentation for balance** : When it’s not possible to collect more
    data for underrepresented classes or styles, data augmentation techniques can
    artificially create additional examples to improve balance.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强以平衡**：当无法收集更多数据来代表代表性不足的类别或风格时，可以通过数据增强技术人为地创建额外的示例来改善平衡。'
- en: '**Sampling strategies** : When creating training, validation, and test splits,
    ensure that each split maintains the overall balance of the full dataset. Stratified
    sampling is a technique that can help achieve this by dividing the dataset such
    that each split reflects the same class proportions as the entire dataset.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样策略**：在创建训练、验证和测试分割时，确保每个分割保持整个数据集的整体平衡。分层抽样是一种技术，可以通过将数据集分割成每个分割反映整个数据集相同类别比例的方式来帮助实现这一点。'
- en: '**Use class weights** : In cases where balancing data through sampling or augmentation
    is challenging, class weights can be used during training to give more importance
    to underrepresented classes, thereby mitigating bias in model predictions.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用类别权重**：在通过采样或增强平衡数据有挑战时，可以在训练过程中使用类别权重，以给予代表性不足的类别更多的重要性，从而减轻模型预测中的偏见。'
- en: '**Regular evaluation** : Continually evaluate the model on a balanced validation
    set to monitor for biases. If biases are detected, the training data may need
    to be rebalanced or additional de-biasing techniques may need to be applied.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期评估**：持续在一个平衡的验证集上评估模型，以监控偏见。如果检测到偏见，可能需要重新平衡训练数据或应用额外的去偏见技术。'
- en: Balancing a dataset is not always straightforward, especially when dealing with
    complex or nuanced attributes. It requires thoughtful analysis and sometimes creative
    solutions to ensure that the final trained model behaves fairly and effectively
    across a wide range of inputs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据集并不总是简单直接，尤其是在处理复杂或细微属性时。这需要深思熟虑的分析，有时还需要创造性的解决方案，以确保最终训练的模型能够在广泛的输入下公平有效地运行。
- en: Data format
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据格式
- en: 'The format in which data is stored and handled can significantly impact the
    efficiency and effectiveness of training LLMs. Proper data formatting ensures
    that the data can be easily accessed, processed, and fed into the model during
    training. Here’s an elaboration on the common formats and considerations:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储和处理的方式可以显著影响训练LLMs的效率和效果。适当的数据格式化确保数据可以轻松访问、处理并在训练期间输入模型。以下是对常见格式和考虑因素的详细说明：
- en: '**JavaScript Object Notation (JSON)** : JSON is a lightweight data-interchange
    format that is easy for humans to read and write and easy for machines to parse
    and generate. It is particularly useful for datasets that have a nested or hierarchical
    structure. For instance, an annotated dataset for NLP might store each sentence
    along with its annotations in a structured JSON format, which can then be easily
    processed and used for training.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaScript对象表示法 (JSON)**：JSON是一种轻量级的数据交换格式，易于人类阅读和编写，也易于机器解析和生成。它特别适用于具有嵌套或层次结构的数据集。例如，一个用于NLP的标注数据集可能会以结构化的JSON格式存储每个句子及其标注，然后可以轻松处理并用于训练。'
- en: '**Comma-separated values (CSVs)** : CSV files are a common format for storing
    tabular data. Each line of the file is a data record, with individual fields separated
    by commas. This format is ideal for datasets that can be represented in a table
    format, such as a collection of text samples with associated labels. CSV files
    can be easily manipulated and processed with standard data processing tools and
    libraries, such as pandas in Python.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逗号分隔值 (CSVs)**：CSV文件是存储表格数据的常见格式。文件中的每一行都是一个数据记录，各个字段由逗号分隔。这种格式非常适合可以表示为表格格式的数据集，例如带有相关标签的文本样本集合。CSV文件可以很容易地使用标准数据处理工具和库（如Python中的pandas）进行操作和处理。'
- en: '**Plain text files** : For some tasks, especially those involving large amounts
    of unstructured text, plain text files may be the most straightforward format.
    They are simple to create and can be processed by almost any programming environment.
    However, they lack the structure to represent complex relationships or annotations,
    which might be necessary for certain types of training.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**纯文本文件**：对于某些任务，尤其是涉及大量非结构化文本的任务，纯文本文件可能是最直接的格式。它们易于创建，几乎可以由任何编程环境处理。然而，它们缺乏表示复杂关系或注释的结构，这对于某些类型的训练可能是必要的。'
- en: '**TFRecord** : TensorFlow’s TFRecord file format is an efficient way to store
    data for TensorFlow models. It is particularly useful for datasets that need to
    be streamed from disk during training, which can be too large to fit into memory.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TFRecord**：TensorFlow的TFRecord文件格式是存储TensorFlow模型数据的有效方式。它特别适用于在训练期间需要从磁盘流式传输的数据集，这些数据集可能太大而无法放入内存。'
- en: '**pickle** : Python provides a module named **pickle** that can serialize and
    de-serialize Python objects, converting them to a byte stream and back. While
    convenient, **pickle** files are specific to Python and may not be suitable for
    long-term data storage or for environments that use multiple programming languages.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pickle**：Python提供了一个名为**pickle**的模块，可以序列化和反序列化Python对象，将它们转换为字节流并再次转换回来。虽然方便，但**pickle**文件是特定于Python的，可能不适合长期数据存储或使用多种编程语言的环境。'
- en: '**Hierarchical Data Format version 5 (HDF5)** : HDF5 is a file format and set
    of tools for managing complex data. It is designed for flexible and efficient
    I/O and high-volume and complex data. HDF5 can be a good choice for datasets that
    require multi-dimensional arrays, such as word embeddings.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次数据格式版本5 (HDF5)**：HDF5是一种用于管理复杂数据的文件格式和工具集。它旨在提供灵活高效的I/O以及高容量和复杂数据。HDF5对于需要多维数组的数据集来说是一个不错的选择，例如词嵌入。'
- en: '**Parquet** : Parquet is a columnar storage file format that is optimized for
    use with big data processing frameworks. It is efficient for both storage and
    performance, supporting advanced nested data structures.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parquet**：Parquet是一种列式存储文件格式，专为与大数据处理框架一起使用而优化。它在存储和性能方面都很高效，支持高级嵌套数据结构。'
- en: 'When converting data to the format best suited for the model’s training framework,
    consider the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据转换为最适合模型训练框架的格式时，请考虑以下因素：
- en: '**Scalability** : The format should be able to handle the scale of the data,
    both in terms of the number of records and the complexity of each record.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：格式应该能够处理数据的规模，无论是记录的数量还是每个记录的复杂性。'
- en: '**Performance** : The I/O performance of the format can be critical, especially
    when dealing with large datasets. The chosen format should enable efficient read
    and write operations.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**: 格式的 I/O 性能可能至关重要，尤其是在处理大型数据集时。所选格式应支持高效的读写操作。'
- en: '**Compatibility** : The format must be compatible with the tools and frameworks
    being used for model training. It should align with the expected input structure
    of the training pipeline.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性**: 格式必须与用于模型训练的工具和框架兼容。它应与训练管道预期的输入结构相一致。'
- en: '**Maintainability** : The ease of use and the ability to modify the dataset
    if needed are important. Some formats are more human-readable and easier to manipulate
    than others.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可维护性**: 易用性和在需要时修改数据集的能力很重要。一些格式比其他格式更易于阅读和操作。'
- en: '**Integrity** : The format should preserve the integrity of the data, without
    loss or corruption.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**: 格式应保留数据的完整性，不丢失或损坏。'
- en: By thoroughly preparing datasets, you can significantly enhance the performance
    of LLMs and ensure they learn a wide variety of language patterns and nuances.
    This groundwork is key to developing models that can generalize well and perform
    consistently across different tasks and domains.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过彻底准备数据集，可以显著提高 LLMs 的性能，并确保它们学习到广泛的语言模式和细微差别。这些基础工作对于开发能够在不同任务和领域中进行良好泛化和一致表现的模式至关重要。
- en: Setting up your training environment
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置您的训练环境
- en: Establishing a robust training environment for LLMs involves creating a setup
    where models can learn effectively from data and improve over time. The steps
    to create such an environment are discussed next.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为 LLMs 建立稳健的训练环境涉及创建一个模型可以从数据中有效学习并随时间改进的设置。接下来将讨论创建此类环境的步骤。
- en: Hardware infrastructure
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件基础设施
- en: 'For training LLMs, the **hardware infrastructure** is an essential foundation
    that ensures the training process is efficient and effective. Here’s an in-depth
    look at the key components:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练 LLMs，**硬件基础设施**是确保训练过程高效和有效的必要基础。以下是对关键组件的深入了解：
- en: '**Graphics processing units (GPUs)** : GPUs are specialized hardware designed
    to handle parallel tasks efficiently, which makes them ideal for the matrix and
    vector computations required in deep learning. Modern LLMs often necessitate the
    use of high-end GPUs with a large number of cores and substantial onboard memory
    to handle the computation loads.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图形处理单元 (GPU)**: GPU 是专门设计的硬件，用于高效处理并行任务，这使得它们非常适合深度学习中所需的矩阵和向量计算。现代 LLMs
    通常需要使用具有大量核心和大量板载内存的高端 GPU 来处理计算负载。'
- en: '**Tensor processing units (TPUs)** : TPUs are custom chips developed specifically
    for ML workloads. They are optimized for the operations used in neural network
    training, offering high throughput for both training and inference. TPUs can be
    particularly effective for training LLMs at scale due to their high computational
    efficiency and speed.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量处理单元 (TPUs)**: TPUs 是专门为机器学习工作负载开发的定制芯片。它们针对神经网络训练中使用的操作进行了优化，为训练和推理提供了高吞吐量。由于计算效率高且速度快，TPUs
    在大规模训练大型语言模型 (LLMs) 时尤其有效。'
- en: '**High-performance CPUs** : While GPUs and TPUs handle the bulk of model training,
    high-performance CPUs are also important. They manage the overall control flow,
    data preprocessing, and I/O operations that feed data into the GPUs/TPUs.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能 CPU**: 虽然 GPU 和 TPU 处理大部分模型训练工作，但高性能 CPU 同样重要。它们管理整体控制流、数据预处理以及将数据输入
    GPU/TPU 的 I/O 操作。'
- en: '**Memory** : Adequate RAM is necessary to load training datasets, particularly
    when preprocessing and tokenizing large corpora. Insufficient memory can lead
    to bottlenecks, as data will need to be swapped in and out of slower storage.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**: 足够的 RAM 是加载训练数据集所必需的，尤其是在预处理和标记大型语料库时。内存不足可能导致瓶颈，因为数据需要从较慢的存储中交换进和出。'
- en: '**Storage** : Fast, reliable storage is crucial for storing the large datasets
    used to train LLMs, as well as for saving the models’ parameters and checkpoints
    during training. **Solid state drives** ( **SSDs** ) are preferred over **hard
    disk drives** ( **HDDs** ) for faster read/write speeds, which can significantly
    reduce data loading times.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**: 快速、可靠的存储对于存储用于训练 LLMs 的大型数据集以及训练过程中保存模型参数和检查点至关重要。**固态硬盘 (SSDs**) 由于读写速度更快，比**硬盘驱动器
    (HDDs**) 更受欢迎，这可以显著减少数据加载时间。'
- en: '**Fast I/O capabilities** : Efficient I/O operations are vital to ensure that
    the training process is not I/O bound. This includes having a fast data pipeline
    that can supply data to the GPUs/TPUs without causing them to idle.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速 I/O 能力**：高效的 I/O 操作对于确保训练过程不受 I/O 限制至关重要。这包括拥有快速的数据管道，可以为 GPU/TPU 提供数据，而不会导致它们闲置。'
- en: '**Networking** : For distributed training across multiple machines or clusters,
    high-bandwidth and low-latency networking are important to efficiently communicate
    updates and synchronize the model’s parameters.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：对于跨多台机器或集群的分布式训练，高带宽和低延迟的网络对于有效地通信更新和同步模型参数至关重要。'
- en: '**Cooling and power** : High-performance computing generates significant heat,
    so adequate cooling systems are necessary to maintain hardware integrity and performance.
    Similarly, a stable and sufficient power supply is critical to support the operation
    of high-end GPUs and TPUs.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冷却和电源**：高性能计算会产生大量的热量，因此需要足够的冷却系统来维护硬件的完整性和性能。同样，稳定且充足的电源对于支持高端 GPU 和 TPUs
    的运行至关重要。'
- en: '**Scalability** : The infrastructure should be scalable, allowing for the addition
    of more GPUs or TPUs as the complexity of the model or the size of the dataset
    grows.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：基础设施应该是可扩展的，允许随着模型复杂度或数据集大小的增加而添加更多的 GPU 或 TPU。'
- en: '**Reliability and redundancy** : Systems should be robust, with redundancies
    in place to handle hardware failures, which can be common when training large
    models over extended periods.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性和冗余**：系统应该是健壮的，并具备冗余措施来处理硬件故障，这在长时间训练大型模型时可能会很常见。'
- en: '**Cloud computing platforms** : Many organizations opt for cloud-based services
    that offer scalable compute resources on-demand. Providers such as AWS, Google
    Cloud Platform, and Microsoft Azure offer GPU and TPU instances that can be rented,
    which can be a cost-effective alternative to purchasing and maintaining physical
    hardware.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云计算平台**：许多组织选择基于云的服务，这些服务提供按需可扩展的计算资源。例如，AWS、Google Cloud Platform 和 Microsoft
    Azure 等提供商提供可租用的 GPU 和 TPU 实例，这可以是一种比购买和维护物理硬件更具成本效益的替代方案。'
- en: '**Software compatibility** : Ensure that the hardware is compatible with the
    software stack and ML frameworks you plan to use, such as TensorFlow or PyTorch,
    which may have specific requirements for optimal performance.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件兼容性**：确保硬件与您计划使用的软件栈和机器学习框架（如 TensorFlow 或 PyTorch）兼容，这些软件可能对最佳性能有特定的要求。'
- en: Investing in the right hardware infrastructure is crucial for the successful
    training of LLMs, as it can greatly affect the speed of experimentation, the scale
    of training, and, ultimately, the quality of the models produced.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功训练大型语言模型（LLM）方面，投资合适的硬件基础设施至关重要，因为它可以极大地影响实验速度、训练规模以及最终产生的模型质量。
- en: Software and tools
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件和工具
- en: Selecting the appropriate software and tools is essential for the development
    and training of LLMs. The software stack includes not just ML frameworks, but
    also utilities that support data processing, model versioning, and experiment
    tracking. Here’s a detailed look at these components.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的软件和工具对于大型语言模型（LLM）的开发和训练至关重要。软件栈不仅包括机器学习框架，还包括支持数据处理、模型版本控制和实验跟踪的实用工具。以下是这些组件的详细说明。
- en: ML frameworks
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习框架
- en: '**ML frameworks** are pivotal in developing and deploying advanced algorithms,
    with each offering distinct features and advantages for various applications in
    the field:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习框架**在开发和部署高级算法中起着关键作用，每个框架都为该领域的各种应用提供了独特的特性和优势：'
- en: '**TensorFlow** : An open source framework developed by the Google Brain team,
    known for its flexibility and robustness in building and deploying ML models.
    It offers comprehensive libraries for various ML tasks and supports distributed
    training.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**：由 Google Brain 团队开发的开源框架，以其在构建和部署机器学习模型时的灵活性和健壮性而闻名。它为各种机器学习任务提供全面的库，并支持分布式训练。'
- en: '**PyTorch** : Developed by Meta’s AI at Meta (formerly Facebook’s AI Research
    lab), PyTorch is favored for its dynamic computation graph and user-friendly interface,
    making it particularly well suited for the research and development of deep learning
    models.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PyTorch**：由 Meta 的 AI 团队（前身为 Facebook 的 AI 研究实验室）开发，PyTorch 因其动态计算图和用户友好的界面而受到青睐，特别适合深度学习模型的研发。'
- en: '**Hugging Face’s Transformers** : A library built on top of TensorFlow and
    PyTorch, providing pre-built transformers and models for natural language understanding
    and generation. It simplifies the process of implementing state-of-the-art LLMs.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face的Transformers**：一个基于TensorFlow和PyTorch构建的库，提供预构建的转换器和模型，用于自然语言理解和生成。它简化了实现最先进LLMs的过程。'
- en: Data processing tools
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据处理工具
- en: '**Data science tools** are specialized libraries that support the manipulation,
    analysis, and processing of data across different formats and complexities:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据科学工具**是专门用于支持在不同格式和复杂性下操作、分析和处理数据的库：'
- en: '**pandas/NumPy** : These are Python libraries that offer data structures and
    operations for manipulating numerical tables and time series. They are instrumental
    in handling and preprocessing structured data.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas/NumPy**：这些是Python库，提供用于操作数值表和时间序列的数据结构和操作。它们在处理和预处理结构化数据方面至关重要。'
- en: '**Scikit-learn** : A Python library that provides simple and efficient tools
    for data mining and data analysis. It includes functions for preprocessing and
    feature extraction.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scikit-learn**：一个Python库，提供用于数据挖掘和数据分析的简单而高效的工具。它包括预处理和特征提取的功能。'
- en: '**spaCy** : An open source software library for advanced NLP in Python, offering
    robust tools for text preprocessing.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spaCy**：一个用于Python的开放源代码软件库，提供用于文本预处理的强大工具。'
- en: Version control systems
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 版本控制系统
- en: '**Version control systems** are critical tools in software and ML development,
    managing changes in code, data, and models effectively:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**版本控制系统**是软件和机器学习开发中的关键工具，有效地管理代码、数据和模型的变化：'
- en: '**Git** : A distributed version control system used for tracking changes in
    source code during software development. It is essential for managing code changes,
    especially when collaborating with a team.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git**：一个分布式版本控制系统，用于在软件开发过程中跟踪源代码的变化。它对于管理代码变化至关重要，尤其是在与团队协作时。'
- en: '**Data Version Control (DVC)** : An open source version control system for
    ML projects. It extends version control to include data and model weights, enabling
    better tracking of experiments.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据版本控制（DVC）**：一个用于机器学习项目的开源版本控制系统。它将版本控制扩展到包括数据和模型权重，从而更好地跟踪实验。'
- en: Experiment tracking and management
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验跟踪和管理
- en: '**Experiment tracking and management tools** are essential for streamlining
    the ML development process, from tracking progress to optimizing and deploying
    models:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验跟踪和管理工具**对于简化机器学习开发过程至关重要，从跟踪进度到优化和部署模型：'
- en: '**MLflow** : This open source tool streamlines the ML life cycle, supporting
    deployment, fostering consistent experimental reproducibility, and managing the
    workflow. It helps track and organize experiments and manage and deploy models.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow**：这个开源工具简化了机器学习生命周期，支持部署，促进一致的实验可重复性，并管理工作流程。它有助于跟踪和组织实验以及管理和部署模型。'
- en: '**Weights & Biases** : A tool for experiment tracking, model optimization,
    and dataset versioning. It provides a dashboard for visualizing training processes
    and comparing different runs.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weights & Biases**：一个用于实验跟踪、模型优化和数据集版本化的工具。它提供了一个仪表板来可视化训练过程并比较不同的运行。'
- en: Containerization and virtualization
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器化和虚拟化
- en: '**Containerization and virtualization technologies** , such as Docker and Kubernetes,
    are crucial for the consistent deployment and scalable management of applications
    across environments:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器化和虚拟化技术**，如Docker和Kubernetes，对于在不同环境中一致部署和可扩展管理应用程序至关重要：'
- en: '**Docker** : Platform-as-a-service solutions offered in this suite provide
    software packaged in modular units, leveraging OS-level virtualization, called
    **containers** . It ensures that the software runs reliably when moved from one
    computing environment to another.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker**：此套件中提供的平台即服务解决方案提供软件模块化包装，利用称为**容器**的操作系统级虚拟化。它确保软件在从一个计算环境移动到另一个计算环境时可靠运行。'
- en: '**Kubernetes** : An open source system used for automating the deployment,
    scaling, and management of containerized applications, ideal for managing complex
    applications such as LLMs.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：一个开源系统，用于自动化容器化应用程序的部署、扩展和管理，非常适合管理复杂的应用程序，如LLMs。'
- en: Integrated development environments (IDEs) and code editors
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成开发环境（IDE）和代码编辑器
- en: 'IDEs and code editors, such as Jupyter Notebook and VS Code, are essential
    for efficient code creation, testing, and maintenance:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: IDEs和代码编辑器，如Jupyter Notebook和VS Code，对于高效的代码创建、测试和维护至关重要：
- en: '**Jupyter Notebook** : A web-based open source application that enables the
    creation and distribution of documents with live code, equations, visualizations,
    and explanatory text'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**：一个基于网络的开源应用程序，允许创建和分发包含实时代码、方程、可视化和解释性文本的文档。'
- en: '**VS Code** : A source code editor that includes support for debugging, embedded
    Git control, syntax highlighting, and intelligent code completion'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VS Code**：一个包含调试、嵌入式Git控制、语法高亮和智能代码补全功能的源代码编辑器。'
- en: Deployment and monitoring
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署和监控
- en: 'Tools such as TensorBoard and Grafana are pivotal for visualizing and monitoring
    ML models and systems:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard和Grafana等工具对于可视化和监控机器学习模型和系统至关重要：
- en: '**TensorBoard** : With regard to deployment, this is a tool that offers key
    metrics and visualizations for ML workflows, supporting experiment tracking, model
    graph visualization, and more.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorBoard**：在部署方面，这是一个提供机器学习工作流程关键指标和可视化的工具，支持实验跟踪、模型图可视化等功能。'
- en: '**Grafana** : An open source platform for monitoring and observability. It
    can be used to create dashboards and alerts for your ML infrastructure.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grafana**：一个开源的监控和可观察性平台。它可以用来创建用于机器学习基础设施的仪表板和警报。'
- en: Choosing the right set of software and tools depends on the specific requirements
    of the project, the team’s expertise, and the existing infrastructure. It’s important
    to select tools that integrate well with each other, have strong community support,
    and can scale with the project’s needs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的软件和工具取决于项目的具体需求、团队的专长以及现有的基础设施。选择能够彼此良好集成、拥有强大社区支持并能随着项目需求扩展的工具非常重要。
- en: Other items
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他事项
- en: 'In ML workflows, a variety of components beyond model building are critical
    for success, encompassing data handling to post-deployment operations and ethics:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作流程中，除了模型构建之外，各种组件对于成功至关重要，涵盖了数据处理到部署后操作以及伦理：
- en: '**Data pipeline** : Develop a scalable and automated data pipeline. This should
    include stages for data ingestion, preprocessing, transformation, augmentation,
    and feeding data into the training loop in batches.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据管道**：开发一个可扩展和自动化的数据管道。这应该包括数据摄取、预处理、转换、增强以及将数据批量喂入训练循环的阶段。'
- en: '**Monitoring and logging** : Implement a system for monitoring and logging
    model performance and system health. Tools such as TensorBoard, Weights & Biases,
    or MLflow can track metrics, visualize training progress, and log experiments.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和日志记录**：实施一个用于监控和记录模型性能和系统健康的系统。TensorBoard、Weights & Biases或MLflow等工具可以跟踪指标、可视化训练进度并记录实验。'
- en: '**Hyperparameter tuning** : Use hyperparameter optimization tools to fine-tune
    the model’s performance. Techniques such as grid search, random search, Bayesian
    optimization, or evolutionary algorithms can be employed to find the optimal set
    of hyperparameters.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调整**：使用超参数优化工具来微调模型性能。可以使用网格搜索、随机搜索、贝叶斯优化或进化算法等技术来找到最佳的超参数集。'
- en: '**Distributed training** : For very large models, consider setting up distributed
    training across multiple machines. This involves splitting the data and computation
    across different nodes to speed up the training process.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式训练**：对于非常大的模型，考虑在多台机器上设置分布式训练。这涉及到在不同节点上分割数据和计算，以加快训练过程。'
- en: '**Regularization strategies** : Incorporate regularization strategies such
    as dropout, weight decay, or data augmentation to prevent overfitting and promote
    generalization in the model.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化策略**：结合正则化策略，如dropout、权重衰减或数据增强，以防止过拟合并促进模型泛化。'
- en: '**Testing and validation** : Create a robust testing and validation setup to
    evaluate the model against unseen data. This helps ensure the model’s performance
    generalizes beyond the training data.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试和验证**：创建一个健壮的测试和验证设置，以评估模型对未见数据的性能。这有助于确保模型的性能在训练数据之外也能泛化。'
- en: '**Security measures** : Implement security measures to protect data privacy
    and model integrity, particularly if working with sensitive information. This
    includes access controls, encryption, and compliance with data protection regulations.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全措施**：实施安全措施以保护数据隐私和模型完整性，尤其是在处理敏感信息时。这包括访问控制、加密以及遵守数据保护法规。'
- en: '**Continuous integration / continuous deployment (CI/CD)** : Establish CI/CD
    pipelines for models to streamline updates and deployment. Automated testing and
    deployment can greatly enhance the efficiency of bringing model improvements to
    production.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续集成/持续部署（CI/CD）**：为模型建立CI/CD管道，以简化更新和部署。自动测试和部署可以极大地提高将模型改进引入生产效率。'
- en: '**Reproducibility** : Ensure that every aspect of the training process is reproducible.
    This includes using fixed seeds for random number generators and maintaining detailed
    versioning of datasets and model configurations.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重现性**：确保训练过程的每个方面都是可重现的。这包括使用固定种子为随机数生成器以及维护数据集和模型配置的详细版本控制。'
- en: '**Collaboration** : Facilitate collaboration among team members with tools
    that support versioning and sharing of models, data, and experiment results.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协作**：通过支持版本控制和模型、数据以及实验结果共享的工具，促进团队成员之间的协作。'
- en: '**Documentation** : Keep comprehensive documentation for every aspect of the
    training environment. This should cover data preprocessing steps, model architectures,
    training procedures, and any assumptions or decisions made during the development
    process.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档**：为训练环境的每个方面都保持全面的文档。这应包括数据预处理步骤、模型架构、训练流程以及在开发过程中做出的任何假设或决策。'
- en: '**Ethical considerations** : Address ethical considerations proactively by
    reviewing datasets for potential biases, ensuring model transparency, and adhering
    to AI ethics guidelines.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理考量**：通过审查数据集以识别潜在偏见、确保模型透明度以及遵守人工智能伦理指南，积极处理伦理考量。'
- en: By paying attention to these components, you can create a robust training environment
    that supports the development of effective LLMs capable of performing a wide range
    of tasks while maintaining high standards of quality and reliability.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注这些组件，你可以创建一个强大的训练环境，支持开发出能够执行广泛任务且保持高质量和可靠性的有效大型语言模型（LLMs）。
- en: Hyperparameter tuning – finding the sweet spot
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整 – 寻找最佳点
- en: 'Tuning hyperparameters is an important step in optimizing the performance of
    ML models, including LLMs. Let’s look at a systematic approach to hyperparameter
    tuning:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 调整超参数是优化机器学习模型（包括LLMs）性能的重要步骤。让我们看看超参数调整的系统方法：
- en: '**Understand the hyperparameters** : Begin by understanding the hyperparameters
    that influence model performance. In LLMs, these can include learning rate, batch
    size, number of layers, number of attention heads, dropout rate, and activation
    functions, among others. The choice of values for these hyperparameters can affect
    the balance between memory requirements and training efficiency.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解超参数**：首先，了解影响模型性能的超参数。在LLMs中，这些可能包括学习率、批量大小、层数、注意力头数、dropout率以及激活函数等。这些超参数值的选取会影响内存需求和训练效率之间的平衡。'
- en: '**Establish a baseline** : Start with a set of default hyperparameters to establish
    a baseline performance. This can either come from the literature, default settings
    in popular frameworks, or empirical guesses.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立基线**：从一组默认超参数开始，以建立基线性能。这些参数可以来自文献、流行框架的默认设置或经验猜测。'
- en: '**Manual tuning** : Initially, perform some manual tuning based on intuition
    and experience to see how different hyperparameters affect performance. This can
    help set the bounds for more automated and systematic approaches.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动调整**：最初，基于直觉和经验进行一些手动调整，以了解不同的超参数如何影响性能。这有助于为更自动化和系统的方法设定界限。'
- en: '**Automated hyperparameter optimization** : Employ automated methods such as
    grid search, random search, or Bayesian optimization.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动超参数优化**：采用如网格搜索、随机搜索或贝叶斯优化等自动化方法。'
- en: '**Grid search** : This exhaustively tries all combinations within a specified
    subset of the hyperparameter space.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网格搜索**：这会尝试超参数空间中指定子集内的所有组合。'
- en: '**Random search** : This samples hyperparameter combinations randomly instead
    of exhaustively. It’s usually more efficient than grid search.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机搜索**：随机采样超参数组合而不是全面尝试。通常比网格搜索更高效。'
- en: '**Bayesian optimization** : This uses a probabilistic model to predict the
    performance of hyperparameter combinations and chooses new hyperparameters to
    test by optimizing the expected performance.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯优化**：这使用概率模型来预测超参数组合的性能，并通过优化预期性能来选择新的超参数进行测试。'
- en: '**Use gradient-based optimization** : For some hyperparameters, such as learning
    rates, gradient-based optimization methods can be applied. Learning rate schedulers
    can adjust the learning rate during training to help the model converge more effectively.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用基于梯度的优化**：对于某些超参数，例如学习率，可以应用基于梯度的优化方法。学习率调度器可以在训练过程中调整学习率，以帮助模型更有效地收敛。'
- en: '**Model-based methods** : Techniques such as Hyperband and Bayesian optimization
    with Gaussian processes can be used to find good hyperparameters in fewer experiments
    by building a model of the hyperparameter space.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的优化方法**：例如Hyperband和基于高斯过程的贝叶斯优化等技术可以通过构建超参数空间模型，在更少的实验中找到良好的超参数。'
- en: '**Early stopping** : Use early stopping during training to halt the process
    if the validation performance stops improving. This can also prevent overfitting.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早期停止**：在训练过程中使用早期停止，如果验证性能不再提高，则停止过程。这也可以防止过拟合。'
- en: '**Parallelize experiments** : If resources permit, run multiple sets of hyperparameters
    in parallel to speed up the search process.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行化实验**：如果资源允许，并行运行多组超参数以加快搜索过程。'
- en: '**Keep track of experiments** : Use experiment tracking tools to log hyperparameter
    values and corresponding model performance. This data is invaluable for understanding
    the hyperparameter space and can inform future tuning.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪实验**：使用实验跟踪工具记录超参数值和相应的模型性能。这些数据对于理解超参数空间至关重要，并且可以指导未来的调整。'
- en: '**Evaluate on validation set** : Always evaluate the impact of hyperparameters
    on a held-out validation set to ensure that performance improvements generalize
    beyond the training data.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在验证集上评估**：始终在保留的验证集上评估超参数的影响，以确保性能改进可以推广到训练数据之外。'
- en: '**Prune unpromising trials** : Implement pruning strategies to stop training
    runs that don’t show promise early on, saving computational resources.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝无望的试验**：实施剪枝策略以停止早期没有显示出希望的训练运行，从而节省计算资源。'
- en: '**Sensitivity analysis** : Perform a sensitivity analysis to understand which
    hyperparameters have the most significant impact on performance. Focus fine-tuning
    efforts on these parameters.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感性分析**：进行敏感性分析以了解哪些超参数对性能影响最大。将微调努力集中在这些参数上。'
- en: '**Final testing** : Once optimal hyperparameters are found, evaluate the model’s
    performance on a test set to ensure that the improvements hold on unseen data.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终测试**：一旦找到最佳超参数，就在测试集上评估模型的性能，以确保改进在未见过的数据上仍然有效。'
- en: '**Iterative refinement** : Hyperparameter tuning is often an iterative process.
    You may need to revisit steps based on test results or additional insights.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代优化**：超参数调整通常是一个迭代过程。您可能需要根据测试结果或额外的见解重新访问步骤。'
- en: By methodically adjusting and evaluating the impact of different hyperparameters,
    you can optimize your LLM’s performance for a variety of tasks and datasets. This
    process is part art and part science, requiring both systematic exploration and
    an intuitive understanding of model behavior.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统地调整和评估不同超参数的影响，您可以优化LLM在各种任务和数据集上的性能。这个过程既是艺术又是科学，需要系统探索和直观理解模型行为。
- en: Challenges in training LLMs – overfitting, underfitting, and more
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练大型语言模型（LLM）的挑战——过拟合、欠拟合等
- en: Training LLMs presents several challenges that can affect the quality and applicability
    of the resulting models. Overfitting and underfitting are two primary concerns,
    along with several others.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLM带来了一些挑战，这些挑战会影响结果的模型的质量和适用性。过拟合和欠拟合是两个主要问题，还有其他一些问题。
- en: '**Overfitting** occurs when an LLM learns the training data too well, including
    its noise and outliers. This typically happens when the model is too complex relative
    to the simplicity of the data or when it has been trained for too long. An overfitted
    model performs well on its training data but poorly on new, unseen data because
    it fails to generalize the underlying patterns appropriately. To combat overfitting,
    techniques such as introducing dropout layers, applying regularization, and using
    early stopping during training are employed. Data augmentation and ensuring a
    large and diverse training set can also prevent the model from learning the training
    data too closely.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**发生在LLM过度学习训练数据，包括其噪声和异常值时。这通常发生在模型相对于数据的简单性过于复杂或训练时间过长的情况下。过拟合的模型在训练数据上表现良好，但在新的、未见过的数据上表现较差，因为它未能适当地泛化基本模式。为了对抗过拟合，采用了诸如引入dropout层、应用正则化和在训练期间使用提前停止等技术。数据增强和确保有一个大而多样化的训练集也可以防止模型过度学习训练数据。'
- en: '**Underfitting** is the opposite problem, where the model is too simple to
    capture the complexity of the data or has not been trained enough. An underfitted
    model performs poorly even on the training data because it doesn’t learn the necessary
    patterns in the data. Addressing underfitting might involve increasing the model
    complexity, extending the training time, or providing more feature-rich data.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**欠拟合**是与数据复杂度或训练不足导致模型过于简单相反的问题。欠拟合的模型即使在训练数据上表现也较差，因为它没有学习到数据中的必要模式。解决欠拟合可能涉及增加模型复杂性、延长训练时间或提供更多特征丰富的数据。'
- en: 'Other challenges in training LLMs include the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLMs的其他挑战包括以下内容：
- en: '**Data quality and quantity** : LLMs require vast amounts of high-quality,
    diverse data to learn effectively. Curating such datasets can be challenging and
    resource-intensive.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量和数量**：大型语言模型（LLMs）需要大量高质量、多样化的数据才能有效学习。精心制作这样的数据集可能具有挑战性且资源密集。'
- en: '**Bias in data** : The data used to train LLMs can contain biases, which the
    model will inevitably learn and replicate in its predictions. Efforts must be
    made to identify and mitigate biases in training datasets.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据偏差**：用于训练LLMs的数据可能包含偏差，模型不可避免地会学习并在其预测中复制这些偏差。必须努力识别和减轻训练数据集中的偏差。'
- en: '**Computational resources** : Training LLMs demands substantial computational
    resources, which can be expensive and energy-intensive, posing scalability and
    environmental concerns.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**：训练LLMs需要大量的计算资源，这可能成本高昂且能耗密集，引发可扩展性和环境问题。'
- en: '**Hyperparameter tuning** : Finding the optimal set of hyperparameters for
    an LLM is a complex and often time-consuming process. It requires extensive experimentation
    and can significantly affect model performance.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调整**：为LLM找到最佳的超参数集是一个复杂且通常耗时的工作过程。它需要大量的实验，并且可以显著影响模型性能。'
- en: '**Interpretability** : LLMs, especially deep neural networks, are often considered
    “black boxes” because their decision-making processes are not easily understandable
    by humans. This lack of interpretability can be problematic, especially in applications
    that require trust and accountability.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：LLMs，尤其是深度神经网络，通常被认为是“黑盒”，因为它们的决策过程不易为人类理解。这种缺乏可解释性可能存在问题，尤其是在需要信任和问责的应用中。'
- en: '**Adaptability and continual learning** : After an LLM is trained, it should
    ideally be able to adapt to new data or tasks without extensive retraining. Developing
    models that can continually learn and adapt over time is an active area of research.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性和持续学习**：LLM训练完成后，理想情况下应能够适应新的数据或任务而无需大量重新训练。开发能够持续学习和适应的模型是研究的一个活跃领域。'
- en: '**Evaluation metrics** : Proper evaluation of LLMs goes beyond simple accuracy
    or loss metrics. It must consider the context, coherence, and relevancy of the
    model’s outputs, which can be difficult to quantify.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标**：对LLMs的适当评估不仅限于简单的准确度或损失指标。它必须考虑模型输出的上下文、连贯性和相关性，这些可能难以量化。'
- en: '**Ethical and legal considerations** : Ensuring that the use of LLMs adheres
    to ethical standards and legal regulations, especially regarding data privacy
    and user rights, is crucial.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理和法律考量**：确保LLMs的使用符合伦理标准和法律规范，特别是关于数据隐私和用户权利方面，至关重要。'
- en: '**Maintenance** : Once deployed, LLMs require ongoing maintenance to stay current
    with language trends, which can be a challenge given the rapid evolution of language
    and context in the real world.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维护**：一旦部署，LLMs需要持续维护以跟上语言趋势，鉴于现实世界中语言和语境的快速演变，这可能是一个挑战。'
- en: Addressing these challenges requires a combination of technical strategies,
    careful planning, and adherence to ethical guidelines. As the field progresses,
    new techniques and methodologies are continually being developed to mitigate these
    issues and enhance the training and functionality of LLMs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战需要技术策略的组合、周密的规划和遵守道德准则。随着该领域的发展，新的技术和方法不断被开发出来，以减轻这些问题并增强LLMs的训练和功能。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we laid out a comprehensive pathway for training LLMs, beginning
    with the imperative stage of data preparation and management. A robust corpus
    – varied, extensive, and balanced – is the bedrock upon which LLMs stand, requiring
    a diverse spectrum of text encompassing a broad scope of topics, cultural and
    linguistic representations, and temporal spans. To this end, we detailed the significance
    of collecting data that ensures a balanced representation and mitigates biases,
    hence fostering models that deliver a refined understanding of language.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了训练LLMs的全面路径，从数据准备和管理的关键阶段开始。一个强大且多样化的语料库——多样、广泛且平衡——是LLMs的基础，需要涵盖广泛主题、文化和语言表现以及时间跨度的文本范围。为此，我们详细阐述了收集确保平衡表现并减轻偏见的数据的必要性，从而培养出能够提供对语言精细理解的模型的必要性。
- en: Following the collection, rigorous processes of cleaning, tokenization, and
    annotation come into play to refine the quality and utility of data. These steps
    remove noise and standardize the text, breaking it into tokens that the model
    can efficiently process and annotate to provide contextual richness.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据后，严格的清洗、分词和标注过程开始发挥作用，以提升数据的质量和实用性。这些步骤移除了噪声并标准化了文本，将其分解成模型可以高效处理和标注的标记，以提供丰富的上下文信息。
- en: Data augmentation and preprocessing practices were emphasized as pivotal in
    expanding the scope of the data and standardizing it, thereby enabling the model
    to learn from a broader spectrum and prevent overfitting. The validation split
    underpinned the model’s tuning process, ensuring its performance is robust, not
    just on the training set, but also on novel, unseen data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强和预处理实践被强调为扩大数据范围和标准化数据的关键，从而使得模型能够从更广泛的角度学习并防止过拟合。验证分割支撑了模型的调整过程，确保其性能不仅对训练集稳健，而且对新颖、未见过的数据也稳健。
- en: Feature engineering was underscored as a critical step to extract and harness
    additional meaningful attributes from the data, enriching the model’s understanding
    of language intricacies. This, along with the crucial step of balancing the dataset,
    ensures that the model’s performance remains equitable across diverse inputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程被强调为提取和利用数据中额外有意义属性的关键步骤，丰富了模型对语言复杂性的理解。这一点，加上平衡数据集的关键步骤，确保了模型在多样化的输入上保持公平的性能。
- en: Proper data formatting was noted for setting the stage for efficient training
    and iteration, while the establishment of a solid training environment – with
    robust hardware and software infrastructure – was shown to be imperative for the
    successful training of LLMs. Hyperparameter tuning was addressed as a nuanced
    art and science necessary for optimizing the model’s performance.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的数据格式化被指出是设置高效训练和迭代的先决条件，而建立一个坚实的训练环境——拥有强大的硬件和软件基础设施——被证明对于LLMs的成功训练至关重要。超参数调整被视为优化模型性能的微妙艺术和科学。
- en: In conclusion, this chapter served as an extensive manual for practitioners
    in the field, presenting a well-orchestrated methodology for training LLMs that
    are capable, equitable, and adept at understanding and generating human language.
    It underlined the need for these models to function effectively, ethically, and
    responsibly across various applications.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章为该领域的从业者提供了一本详尽的指南，展示了训练具备能力、公平性且擅长理解和生成人类语言的LLMs的精心编排的方法论。它强调了这些模型在各种应用中有效、道德和负责任地运作的必要性。
- en: In the next chapter, we will embark on explaining advanced training strategies
    so that you can achieve your desired objectives for your LLM applications.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始解释高级训练策略，以便您能够实现您对LLM应用的期望目标。
