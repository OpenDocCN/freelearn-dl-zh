- en: Chapter 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章
- en: Next Steps in Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习的下一步
- en: Throughout this book, we’ve covered the fundamental concepts behind Bayesian
    deep learning (BDL), from understanding what uncertainty is and its role in developing
    robust machine learning systems, right through to learning how to implement and
    analyze the performance of several fundamental BDL. While what you’ve learned
    will equip you to start developing your own BDL solutions, the field is moving
    quickly, and there are many new techniques on the horizon.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经介绍了贝叶斯深度学习（BDL）的基本概念，从理解什么是不确定性及其在开发稳健的机器学习系统中的作用，到学习如何实现和分析几个基本BDL的性能。虽然你所学到的内容足以帮助你开始开发自己的BDL解决方案，但该领域发展迅速，许多新技术正在涌现。
- en: To wrap up the book, in this chapter we’ll take a look at the current trends
    in BDL, before we dive into some of the latest developments in the field. We’ll
    conclude by introducing some alternatives to BDL, and provide some advice on additional
    resources you can use to continue your journey into Bayesian machine learning
    methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾BDL的当前趋势，然后深入探讨该领域的一些最新发展。最后，我们将介绍一些BDL的替代方法，并提供一些建议，介绍你可以使用的额外资源，帮助你继续深入贝叶斯机器学习方法的探索。
- en: 'We’ll cover the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下章节：
- en: Current trends in BDL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前BDL的趋势
- en: How are BDL methods being applied to solve real-world problems?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDL方法是如何应用于解决现实问题的？
- en: Latest methods in BDL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDL的最新方法
- en: Alternatives to BDL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDL的替代方法
- en: Your next steps in BDL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在BDL中的下一步
- en: 9.1 Current trends in BDL
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 当前BDL的趋势
- en: In this section, we’ll explore the current trends in BDL. We’ll look at which
    models are particularly popular in the literature and discuss why certain models
    have been selected for certain applications. This should give you a good imdivssion
    of how the fundamentals covered throughout the book apply more broadly across
    a variety of application domains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨BDL的当前趋势。我们将查看哪些模型在文献中尤为流行，并讨论为何某些模型被选用于特定应用。这将为你提供一个关于本书中涵盖的基础知识如何更广泛应用于各种应用领域的良好印象。
- en: '![PIC](img/file182.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file182.png)'
- en: 'Figure 9.1: Popularity of key BDL search terms over time'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：BDL关键搜索词的流行度随时间的变化
- en: As we see in *Figure* [9.1](#x1-179002r1), there’s been a marked increase in
    popularity of search terms related to BDL over the past decade. Unsurprisingly,
    this follows the trend in the popularity of deep learning search terms, as we
    see in *Figure* [*9.2*](#x1-179004r2); as deep learning has become more popular,
    there has been increased interest in quantifying the uncertainty associated with
    the predictions produced by DNNs. Interestingly, these plots both show a similar
    dip in popularity in mid-late 2021, indicating that as long as deep learning is
    popular, there will also be interest in BDL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图* [9.1](#x1-179002r1)中看到的，过去十年与BDL相关的搜索词的流行度显著增加。毫不奇怪，这与深度学习相关搜索词的流行趋势一致，正如我们在*图*
    [*9.2*](#x1-179004r2)中看到的；随着深度学习的普及，人们对量化DNN预测结果的不确定性的兴趣也随之增加。有趣的是，这些图表都显示了在2021年中至晚期流行度出现了类似的下降，表明只要深度学习受欢迎，BDL也会受到关注。
- en: '![PIC](img/file183.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file183.png)'
- en: 'Figure 9.2: Popularity of key deep learning search terms over time'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：关键深度学习搜索词的流行度随时间的变化
- en: '*Figure* [9.1](#x1-179002r1) demonstrates another interesting point in that,
    generally speaking, the term *variational inference* is more popular than the
    other two BDL-related search terms we’ve used here. As mentioned in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) when
    we covered variational autoencoders, variational inference is one component of
    BDL that has made significant waves in the machine learning community, now being
    a feature of many different deep learning architectures. As such, it’s no surprise
    that it is generally more popular than the terms that explicitly include the word
    ”Bayesian.”'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图* [9.1](#x1-179002r1)展示了另一个有趣的观点，一般而言，术语*变分推断*比我们在此使用的另外两个BDL相关的搜索词更受欢迎。如在[*第5章*](CH5.xhtml#x1-600005)中提到的，[*贝叶斯深度学习的原则性方法*](CH5.xhtml#x1-600005)中我们讨论了变分自编码器，变分推断是BDL的一部分，在机器学习社区产生了显著影响，现在已成为许多不同深度学习架构的一个特点。因此，它比明确包含“贝叶斯”一词的术语更受欢迎也就不足为奇了。'
- en: But where do the methods that we’ve explored in the book fit in terms of their
    popularity and integration into a wide variety of deep learning solutions? We
    can learn more about this simply by looking at the citations for each of the original
    papers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们在书中探讨的那些方法在它们的流行度和在各种深度学习解决方案中的应用情况如何呢？我们可以通过简单地查看每篇原始论文的引用次数来了解更多信息。
- en: '![PIC](img/file184.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file184.png)'
- en: 'Figure 9.3: Popularity of key deep learning search terms over time'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：深度学习关键搜索词的流行度随时间变化
- en: 'In *Figure* [*9.3*](#x1-179007r3), we see that the MC dropout paper is by far
    the most popular paper when it comes to citations – coming in at nearly double
    that of the next most popular method. At this point in the book, the reasons for
    this should be fairly clear: not only is it one of the easiest methods to implement
    (as we saw in [*Chapter 6*](CH6.xhtml#x1-820006), [*Using the Standard Toolbox
    for Bayesian Deep Learning*](CH6.xhtml#x1-820006)), but it’s also one of the most
    attractive from a computation standpoint. It requires no more memory than a standard
    neural network and, as we saw in [*Chapter 7*](CH7.xhtml#x1-1130007), [*Practical*
    *Considerations for Bayesian Deep Learning*](CH7.xhtml#x1-1130007), it’s also
    one of the fastest models for running inference. These practical factors often
    weigh in more heavily than considerations such as uncertainty quality when it
    comes to selecting models.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*图9.3*](#x1-179007r3)中，我们看到MC dropout论文在引用次数上遥遥领先——几乎是第二种最受欢迎方法的两倍。到目前为止，书中已经相当清楚地说明了这个原因：它不仅是最容易实现的方法之一（正如我们在[*第6章*](CH6.xhtml#x1-820006)，[*使用标准工具箱进行贝叶斯深度学习*](CH6.xhtml#x1-820006)中看到的），而且从计算角度来看，它也是最具吸引力的之一。它所需的内存与标准神经网络相同，而且正如我们在[*第7章*](CH7.xhtml#x1-1130007)中看到的，[*贝叶斯深度学习的实际考虑*](CH7.xhtml#x1-1130007)，它也是运行推理时最快的模型之一。这些实际因素在选择模型时通常比不确定性质量等因素更重要。
- en: 'Practical considerations are again likely the reason behind the second most
    popular method being deep ensembles. While this may not be the most efficient
    method in terms of training time, it’s often the speed of inference that counts
    the most: and again, looking back to [*Chapter 7*](CH7.xhtml#x1-1130007), [*Practical*
    *Considerations for Bayesian Deep Learning*](CH7.xhtml#x1-1130007)’s results,
    we see that the ensemble excels here, despite needing to run inference on multiple
    different networks.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际考虑因素很可能是第二种最流行方法——深度集成法的原因。虽然从训练时间来看，这可能不是最高效的方法，但通常推理的速度才是最重要的：再次回顾[*第7章*](CH7.xhtml#x1-1130007)，[*贝叶斯深度学习的实际考虑*](CH7.xhtml#x1-1130007)的结果，我们看到尽管需要对多个不同的网络进行推理，集成法在这里表现优异。
- en: 'Deep ensembles often strike a good balance between ease of implementation and
    theoretical considerations: as discussed in [*Chapter 6*](CH6.xhtml#x1-820006),
    [*Using the Standard Toolbox* *for Bayesian Deep Learning*](CH6.xhtml#x1-820006),
    ensembling is a powerful tool within ML, and so it’s no surprise that NN ensembles
    perform well and often produce well-calibrated uncertainty estimates.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度集成法通常在实现的简便性和理论考虑之间取得良好的平衡：正如在[*第6章*](CH6.xhtml#x1-820006)，[*使用标准工具箱进行贝叶斯深度学习*](CH6.xhtml#x1-820006)中讨论的那样，集成法是机器学习中的一个强大工具，因此，神经网络集成法表现良好并且通常能生成良好的不确定性估计也就不足为奇了。
- en: 'The last two methods, taking third and fourth place respectively, are BBB and
    PBP. While BBB is far easier to implement than PBP, the fact that it requires
    some probabilistic components often means that – while in many cases it may be
    the best tool for the job – machine learning engineers may not be aware of it
    or aren’t comfortable implementing it. PBP takes this to a further extreme: as
    we saw in [*Chapter 5*](CH5.xhtml#x1-600005), [*Principled Approaches for* *Bayesian
    Deep Learning*](CH5.xhtml#x1-600005), implementing PBP is not a straightforward
    task. At the time of writing, there are no deep learning frameworks that incorporate
    an easy-to-use and well-optimized implementation of PBP, and – apart from the
    BDL community – many machine learning researchers and practitioners just aren’t
    aware of its existence, as is made clear by the fact that it has fewer citations
    (although it still has a fairly impressive citation count!).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 排名第三和第四的最后两种方法是 BBB 和 PBP。虽然 BBB 比 PBP 更容易实现，但由于它需要一些概率组件，通常意味着——尽管在许多情况下它可能是最合适的工具——机器学习工程师可能不了解它，或者不习惯实现它。PBP
    则更为极端：正如我们在[*第 5 章*](CH5.xhtml#x1-600005)中所看到的，[*贝叶斯深度学习的原则性方法*](CH5.xhtml#x1-600005)，实现
    PBP 不是一项简单的任务。写作时，没有深度学习框架提供易用且经过优化的 PBP 实现——除了 BDL 社区外，许多机器学习研究人员和实践者根本不知道它的存在，这一点从它的引用次数较少可以看出（尽管它的引用数量仍然相当可观！）。
- en: 'This analysis of the popularity of Bayesian deep learning methods seems to
    tell a pretty clear story: BNN approaches are chosen based primarily on the ease
    of implementation. Indeed, there is a significant amount of literature that discusses
    the use of BDL methods for their uncertainty estimates without considering the
    *quality* of the model uncertainty estimates. Fortunately, with the increasing
    popularity of uncertainty-aware methods, this trend is beginning to decline, and
    we hope that this book has equipped you with the necessary tools to allow you
    to be more principled in your selection of BNN methods. Irrespective of the methods
    used or how they’re selected, it’s clear that machine learning researchers and
    practitioners are increasingly interested in BDL approaches – so what are these
    methods being used for? Let’s take a look.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对贝叶斯深度学习方法流行度的分析似乎讲述了一个相当清晰的故事：BNN 方法的选择主要是基于实现的简便性。事实上，有大量文献讨论了使用 BDL 方法进行不确定性估计，而没有考虑模型不确定性估计的*质量*。幸运的是，随着不确定性感知方法的日益流行，这一趋势开始下降，我们希望本书已为你提供了必要的工具，使你在选择
    BNN 方法时更加有原则。不论所使用的方法或其选择方式如何，很明显，机器学习研究人员和实践者对 BDL 方法越来越感兴趣——那么这些方法到底是用来做什么的呢？让我们来看看。
- en: 9.2 How are BDL methods being applied to solve real-world problems?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 BDL 方法如何应用于解决实际问题？
- en: Just as deep learning is having an impact on a diverse variety of application
    domains, BDL is becoming an increasingly important tool, particularly where large
    amounts of data are being used within safety-critical or mission-critical systems.
    In these cases – as is the case for most real-world applications – being able
    to quantify when models ”know they don’t know” is crucial to developing reliable
    and robust systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 就像深度学习正在对各种应用领域产生影响一样，BDL 正在成为越来越重要的工具，特别是在安全关键或任务关键系统中使用大量数据的情况下。在这些场景中——正如大多数实际应用所面临的情况——能够量化模型何时“知道自己不知道”对于开发可靠且稳健的系统至关重要。
- en: One significant application area for BDL is in safety-critical systems. In their
    2019 paper titled *Safe Reinforcement Learning with Model Uncertainty* *Estimates*,
    Björn Lütjens *et al.* demonstrate that the use of BDL methods can produce safer
    behavior in collision-avoidance scenarios (the inspiration for our reinforcement
    learning example in [*Chapter 8*](CH8.xhtml#x1-1320008), [*Applying Bayesian Deep*
    *Learning*](CH8.xhtml#x1-1320008)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: BDL 的一个重要应用领域是安全关键系统。在他们 2019 年发表的论文《*具有模型不确定性估计的安全强化学习*》中，Björn Lütjens *等人*展示了使用
    BDL 方法可以在避撞场景中产生更安全的行为（这是我们在[*第 8 章*](CH8.xhtml#x1-1320008)中强化学习示例的灵感来源，[*应用贝叶斯深度学习*](CH8.xhtml#x1-1320008)）。
- en: Similarly, in the paper *Uncertainty-Aware Deep Learning for Safe Landing* *Site
    Selection*, authors Katharine Skinner *et al.* explore how Bayesian neural networks
    can be used for autonomous hazard detection for landing sites on planetary surfaces.
    This technology is crucial for facilitating autonomous landing, and recently DNNs
    have demonstrated significant aptitude for this application. In their paper, Skinner
    *et al.* demonstrate that the use of uncertainty-aware models can improve the
    selection of safe landing sites, and even make it possible to select safe landing
    sites from sensor data with large amounts of noise. This is testament to BDL’s
    capacity to improve both the *safety* and *robustness* of deep learning methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在论文《*不确定性感知深度学习用于安全着陆点选择*》中，作者Katharine Skinner *等人*探讨了如何利用贝叶斯神经网络进行行星表面着陆点的自主危险检测。这项技术对于促进自主着陆至关重要，最近深度神经网络（DNNs）在这一应用中展现了显著的能力。在他们的论文中，Skinner
    *等人*展示了不确定性感知模型的使用能够改善安全着陆点的选择，甚至使得能够从大量噪声的传感器数据中选择安全的着陆点。这证明了贝叶斯深度学习（BDL）在提高深度学习方法的*安全性*和*鲁棒性*方面的能力。
- en: 'Given their rising popularity in safety-critical scenarios, it should be no
    surprise that Bayesian neural networks have also been adopted within medical applications.
    As we touched on in [*Chapter 1*](CH1.xhtml#x1-150001), [*Bayesian Inference in
    the Age of* *Deep Learning*](CH1.xhtml#x1-150001), deep learning has exhibited
    particularly strong performance in the field of medical imaging. However, in these
    sorts of critical applications, uncertainty quantification is crucial: technicians
    and diagnosticians need to be able to understand the margin of error associated
    with model predictions. In the paper *Towards Safe Deep Learning: Accurately Quantifying
    Biomarker* *Uncertainty in Neural Network predictions*, Zach Eaton-Rosen *et al.*
    applied BDL methods for quantifying biomarker uncertainty when using deep networks
    for tumor volume estimation. Their work demonstrates that Bayesian neural networks
    can be used to design deep learning systems with well-calibrated error bars. These
    high-quality uncertainty estimates are necessary for the safe clinical use of
    models based on deep networks, making BDL methods crucial when it comes to incorporating
    these models in diagnostic applications.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于贝叶斯神经网络在安全关键场景中的日益流行，它们也被应用于医疗领域就不足为奇了。正如我们在[*第1章*](CH1.xhtml#x1-150001)中提到的，[*深度学习时代的贝叶斯推理*](CH1.xhtml#x1-150001)，深度学习在医学影像领域表现出了特别强的性能。然而，在这些关键应用中，不确定性量化至关重要：技术人员和诊断人员需要能够理解模型预测的误差范围。在论文《*迈向安全深度学习：精确量化神经网络预测中的生物标志物不确定性*》中，Zach
    Eaton-Rosen *等人*应用了贝叶斯深度学习（BDL）方法来量化使用深度网络进行肿瘤体积估计时的生物标志物不确定性。他们的工作展示了贝叶斯神经网络可以用来设计具有良好标定误差条的深度学习系统。这些高质量的不确定性估计对于深度网络模型在临床中的安全使用至关重要，因此贝叶斯深度学习方法在诊断应用中显得尤为重要。
- en: 'As technology advances, so does our ability to collect and organize data. This
    trend is turning a lot of ”small data” problems into ”big data” problems – which
    is no bad thing, as more data means we’re able to learn much more about the underlying
    process generating the data. One such example is that of seismic monitoring: over
    recent years, there has been a significant increase in dense seismic monitoring
    networks. This is excellent from a monitoring standpoint: scientists now have
    more data than ever before, and are thus able to better understand and monitor
    geophysical processes. However, in order to do so, they also need to be able to
    learn from large amounts of high dimensional data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术的进步，我们收集和组织数据的能力也在不断提升。这一趋势将许多“少数据”问题转变为“海量数据”问题——这并非坏事，因为更多的数据意味着我们能够更深入地了解生成数据的底层过程。一个例子就是地震监测：近年来，密集的地震监测网络显著增加。这从监测的角度来看是极好的：科学家现在比以往拥有更多的数据，因此能够更好地理解和监测地球物理过程。然而，为了做到这一点，他们还需要能够从大量高维数据中进行学习。
- en: In their paper, *Bayesian Deep Learning and Uncertainty Quantification Applied*
    *to Induced Seismicity Locations in the Groningen Gas Field in the Netherlands:*
    *What Do We Need for Safe AI?*, authors Chen Gu *et al.* tackle the problem of
    seismic monitoring of the Groningen gas reservoir. As they mention in the paper,
    while deep learning has been applied to many geophysical problems, the use of
    uncertainty-aware deep networks is rare. Their work demonstrates that Bayesian
    neural networks can successfully be applied to geophysical problems and, in the
    case of the Groningen gas reservoir, could be crucial from both a safety-critical
    *and* mission-critical standpoint. From the safety perspective, these methods
    can be used to leverage the vast amounts of data to develop models that can infer
    ground motion activity and be used for seismic early warning systems. From the
    mission-critical perspective, the same data can be ingested by these methods to
    produce models capable of reservoir production estimates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文《*贝叶斯深度学习与不确定性量化应用于荷兰格罗宁根天然气田的诱发地震位置：*我们需要什么才能确保AI安全？》中，作者陈谷*等人*探讨了格罗宁根气藏的地震监测问题。正如他们在论文中提到的，虽然深度学习已被应用于许多地球物理问题，但使用不确定性感知的深度网络仍然罕见。他们的工作展示了贝叶斯神经网络可以成功地应用于地球物理问题，并且在格罗宁根气藏的案例中，从安全关键和任务关键两个角度来看，都可能至关重要。从安全角度来看，这些方法可以利用大量数据开发模型，推测地面运动活动，并用于地震预警系统。从任务关键的角度来看，相同的数据可以通过这些方法输入，以生成能够进行储层生产估算的模型。
- en: In both cases, uncertainty quantification is key if these methods are going
    to be incorporated into any real-world systems, as the consequences for trusting
    incorrect predictions could be costly or even catastrophic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，若要将这些方法应用于任何实际系统，**不确定性量化**是关键，因为信任错误预测的后果可能是代价高昂甚至灾难性的。
- en: These examples have given us some insight into how BDL is being applied in the
    real world. As with other machine learning solutions before them, we learn more
    about potential shortcomings as the methods are used in more and more diverse
    sets of applications. In the next section, we’ll learn about some of the latest
    developments in the field, building on the core approaches covered in the book
    to develop increasingly robust BNN approximations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子让我们对BDL在现实世界中的应用有了一些了解。与之前的其他机器学习解决方案一样，随着这些方法在越来越多样化的应用场景中被使用，我们也越来越了解其潜在的不足之处。在下一节中，我们将了解一些该领域的最新进展，基于书中介绍的核心方法，开发出越来越强大的BNN近似。
- en: 9.3 Latest methods in BDL
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 BDL中的最新方法
- en: 'In this book, we’ve introduced some of the core techniques used within BDL:
    Bayes by Backprop (BBB), Probabilistic Backpropagation (PBP), Monte-Carlo dropout
    (MC dropout), and deep ensembles. Many BNN approaches you’ll encounter in the
    literature will be based on one of these methods, and having these under your
    belt provides you with a versatile toolbox of approaches for developing your own
    BDL solutions. However, as with all aspects of machine learning, the field of
    BDL is progressing rapidly, and new techniques are being developed on a regular
    basis. In this section, we’ll explore a selection of recent developments from
    the field.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们介绍了一些BDL中使用的核心技术：反向传播贝叶斯（BBB）、概率反向传播（PBP）、蒙特卡洛dropout（MC dropout）和深度集成方法。你在文献中遇到的许多BNN方法都会基于这些技术，而掌握这些技术为你提供了一套多功能的工具箱，可以帮助你开发自己的BDL解决方案。然而，正如机器学习的其他方面一样，BDL领域正在迅速发展，新的技术也在不断涌现。在本节中，我们将探讨该领域的一些最新进展。
- en: 9.3.1 Combining MC dropout and deep ensembles
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 结合MC dropout和深度集成方法
- en: 'Why use just one Bayesian neural network technique when you could use two?
    This is exactly the approach taken by University of Edinburgh researchers Remus
    Pop and Patric Fulop in their paper, *Deep Ensemble Bayesian* *Active Learning:
    Addressing the Mode Collapse Issue in Monte Carlo* *Dropout via Ensembles*. In
    this work, Pop and Fulop describe the problem of using **active learning** to
    make deep learning methods feasible in applications for which labeling data is
    time consuming or costly. The issue here is that, as we’ve discussed previously,
    deep learning methods have proven to be incredibly successful across a range of
    medical imaging tasks. The issue is that this data needs to be carefully labeled,
    and for deep networks to achieve high levels of performance, they need *a lot*
    of this data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只使用一种贝叶斯神经网络技术，而不使用两种呢？爱丁堡大学的研究人员Remus Pop和Patric Fulop在他们的论文《深度集成贝叶斯主动学习：通过集成解决蒙特卡罗Dropout中的模式崩溃问题》中正是采用了这种方法。在这项工作中，Pop和Fulop描述了使用**主动学习**使深度学习方法在标签数据耗时或昂贵的应用中变得可行的问题。这里的问题是，正如我们之前讨论过的，深度学习方法已经在一系列医学影像任务中证明了其巨大的成功。问题在于，这些数据需要经过仔细标注，并且为了使深度网络达到高水平的性能，它们需要*大量*这样的数据。
- en: 'As such, active learning has been proposed by machine learning researchers
    to automatically evaluate new data points and add them to the dataset, using **acquisition
    functions** to determine when new data should be added to the training set. Model
    uncertainty estimates are a key piece of puzzle: they provide a key measure of
    how new data points relate to the model’s existing understanding of the domain.
    In their paper, Pop and Fulop demonstrate that a popular method for **Deep Bayesian
    Active** **Learning (DBAL)** has a key shortcoming: that of over-confidence stemming
    from the MC dropout models used in DBAL. In their paper, the authors address this
    through combining both deep ensembles and MC dropout in a single model. They demonstrate
    that the resulting model has better calibrated uncertainty estimates, thus correcting
    for the over-confident predictions exhibited by MC dropout. The resulting method,
    dubbed **deep** **ensemble Bayesian active learning**, provides a framework for
    robustly employing deep learning methods in applications for which data acquisition
    is difficult or expensive – demonstrating again how BDL is proving to be an important
    building block in deploying deep networks in the real world.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习研究者提出了主动学习方法，通过使用**获取函数**来自动评估新数据点并将其添加到数据集中，以确定何时将新数据添加到训练集中。模型的不确定性估计是关键的一环：它提供了新数据点与模型当前理解领域的关系的关键衡量标准。在他们的论文中，Pop和Fulop展示了一个流行的**深度贝叶斯主动学习（DBAL）**方法的关键缺陷：即MC
    dropout模型中使用的过度自信。在他们的论文中，作者通过将深度集成和MC dropout结合在一个模型中来解决这个问题。他们证明，得到的模型具有更好的校准不确定性估计，从而纠正了MC
    dropout所表现出的过度自信预测。最终提出的方法，被称为**深度** **集成贝叶斯主动学习**，为在数据获取困难或昂贵的应用中稳健地采用深度学习方法提供了一个框架——再次证明BDL在将深度网络应用于现实世界中的重要性。
- en: '![PIC](img/file185.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file185.jpg)'
- en: 'Figure 9.4: Illustration of a combined MC dropout and deep ensemble network'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：结合MC dropout和深度集成网络的示意图
- en: This approach of combining deep ensembles and MC dropout has also been applied
    in other applications. For example, the collision avoidance paper mentioned previously
    by Lütjens *et al.* also uses a combined MC dropout and deep ensemble network.
    This goes to show that it’s not always simply a case of choosing one network over
    another – sometimes combining approaches is key to developing robust, better calibrated
    BDL solutions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度集成和MC dropout结合的这种方法也已应用于其他领域。例如，Lütjens等人之前提到的碰撞避免论文也使用了结合MC dropout和深度集成网络的方法。这表明，选择一种网络而非另一种网络并不总是最优解——有时，结合不同方法是开发稳健且更好校准的BDL解决方案的关键。
- en: 9.3.2 Improving deep ensembles by promoting diversity
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 通过促进多样性来改进深度集成
- en: As we saw earlier in the chapter, judging by the number of citations, deep ensembles
    are the second most popular of the key BDL techniques covered in this book. As
    such, it’s no surprise that researchers have been investigating methods to improve
    on the standard implementation of deep ensembles.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面看到的，按照引用次数来判断，深度集成是本书中介绍的关键BDL技术中第二受欢迎的技术。因此，研究人员一直在探索改进深度集成标准实现的方法，这也就不足为奇了。
- en: 'In Tim Pearce *et al.*’s paper, *Uncertainty in Neural Networks: Approximately*
    *Bayesian Ensembling*, the authors highlight that the standard deep ensemble approach
    has been criticized for not being Bayesian and argue that the standard approach
    likely lacks diversity in many cases, thus producing a poorly descriptive posterior.
    In other words, deep ensembles often result in over-confident predictions due
    to a lack of diversity in the ensemble.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Tim Pearce *等*人的论文《神经网络中的不确定性：近似贝叶斯集成》中，作者强调，标准的深度集成方法因其非贝叶斯性质而受到批评，并认为标准方法在许多情况下缺乏多样性，从而产生了描述性较差的后验分布。换句话说，深度集成通常由于缺乏多样性，导致过于自信的预测。
- en: 'To remedy this, the authors propose a method they term **anchored** **ensembling**.
    Anchored ensembling, like deep ensembles, uses an ensemble of NNs. However, it
    uses a specially adapted loss function that penalizes the ensemble members’ parameters
    from drifting too far from their initial values. Let’s take a look:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一点，作者提出了一种他们称之为**锚定** **集成**的方法。锚定集成像深度集成一样，使用神经网络的集成。然而，它使用了一种特别适配的损失函数，惩罚集成成员的参数过度偏离其初始值。我们来看看：
- en: '![Lossj = 1-||y − ˆy||2+ -1||Γ 12 × (𝜃j − 𝜃anc,j)||2 N 2 N 2 ](img/file186.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Lossj = 1-||y − ˆy||2+ -1||Γ 12 × (𝜃j − 𝜃anc,j)||2 N 2 N 2 ](img/file186.jpg)'
- en: Here, the *Loss*[*j*] is the loss computed for the *j*th network in the ensemble.
    We see a familiar loss in the equation in the form of ||**y** −**y**||[2]². Γ
    is a diagonal regularization matrix, and *𝜃*[*j*] are the parameters for the network.
    The key point here is the relationship between *𝜃*[*j*] and the *𝜃*[*anc,j*] variable.
    Here, the *anc* indicates the anchoring from which the method gets its name. These
    parameters, *𝜃*[*anc,j*], are the set of initial parameters for the *j*th network.
    As such (as we see by the multiplication), if this value is large – in other words,
    if *𝜃*[*j*] and *𝜃*[*anc,j*] are significantly different – the loss will increase.
    Thus, this penalizes the networks in the ensemble if they deviate too far from
    their initial values, forcing them to find parameter values that minimize the
    first term in the equation while staying as close to their initial values as possible.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Loss*[*j*] 是计算出的第 *j* 个网络的损失。我们在方程中看到了一个熟悉的损失形式，即 ||**y** − **y**||[2]²。Γ
    是一个对角正则化矩阵，*𝜃*[*j*] 是网络的参数。这里的关键是 *𝜃*[*j*] 与 *𝜃*[*anc,j*] 变量之间的关系。这里，*anc* 表示该方法名称中的锚定。参数
    *𝜃*[*anc,j*] 是第 *j* 个网络的初始参数集。因此（如通过乘法所示），如果该值很大——换句话说，如果 *𝜃*[*j*] 和 *𝜃*[*anc,j*]
    相差很大——损失将增加。因此，如果网络的参数偏离其初始值太远，这会惩罚集成中的网络，迫使它们找到能够尽量保持接近初始值的参数，同时最小化方程中的第一项。
- en: 'This is important because, if we use an initialization strategy that is more
    likely to produce a diverse set of initial parameter values, then maintaining
    that diversity will ensure that our ensemble comprises diverse networks after
    training. As the authors demonstrate in the paper, this diversity is key to producing
    principled uncertainty estimates: ensuring that the network predictions converge
    for regions of high data, and diverge in regions of low data, just as we saw in
    our GP examples from [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian*
    *Inference*](CH2.xhtml#x1-250002):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点非常重要，因为如果我们使用一种更可能产生多样化初始参数值的初始化策略，那么保持这种多样性将确保我们的集成在训练后包含多样化的网络。正如作者在论文中展示的那样，这种多样性是产生原则性不确定性估计的关键：确保网络预测在高数据区域收敛，而在低数据区域发散，就像我们在[*第2章*](CH2.xhtml#x1-250002)和[*贝叶斯推断基础*](CH2.xhtml#x1-250002)的高斯过程示例中看到的那样。
- en: '![PIC](img/file52.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file52.png)'
- en: 'Figure 9.5: Illustration of principled uncertainty estimates obtained using
    a Gaussian process'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：使用高斯过程获得的原则性不确定性估计示意图
- en: As a reminder, here the solid line is the true function, the dots are the samples
    from the function, the dotted line are the mean GP predictions, the faint dotted
    lines are a sample of possible functions, and the shaded area is the uncertainty.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这里的实线是实际函数，点是函数的样本，虚线是高斯过程的均值预测，浅灰色虚线是可能函数的样本，阴影区域表示不确定性。
- en: In their paper, Pearce *et al.* demonstrate that their anchored ensemble approach
    is able to approximate a descriptive posterior distribution such as this far more
    closely than the standard deep ensemble approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文中，Pearce *等*人展示了他们的锚定集成方法能够比标准深度集成方法更接近地逼近这样的描述性后验分布。
- en: 9.3.3 Uncertainty in very large networks
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 超大网络中的不确定性
- en: 'While the core aim topic of the book has been to introduce methods for approximating
    Bayesian inference in DNNs, we haven’t addressed how this is applied to one of
    the most successful NN architecture varieties of recent years: the transformer.
    Transformers – just as more typical deep networks before them – have achieved
    landmark performance in a variety of tasks. While deep networks were already crunching
    large amounts of data, transformers take this to the next level: crunching enormous
    volumes of data, and comprising hundreds of billions of parameters. One of the
    most well-known transformer networks is GPT-3, a transformer developed by OpenAI
    that comprises over 175 billion parameters.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的核心目标是介绍在DNN中近似贝叶斯推断的方法，但我们尚未讨论如何将其应用于近年来最成功的神经网络架构之一：变换器。变换器—就像之前的典型深度网络一样—在多种任务中实现了突破性性能。尽管深度网络已经能够处理大量数据，变换器将其推向了一个新高度：处理巨量数据，拥有数百亿的参数。其中最著名的变换器网络之一是GPT-3，这是由OpenAI开发的变换器，包含超过1750亿个参数。
- en: 'Transformers were first used in **Natural Language Processing** (**NLP**) tasks,
    and demonstrated that, through the use of self-attention and sufficient volumes
    of data, competitive performance can be achieved without the use of recurrent
    neural networks. This was an important step in NN architecture development: demonstrating
    that sequential context can be learned through self-attention and providing architectures
    capable of learning from hitherto unprecedented volumes of data.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器最初用于**自然语言处理**（**NLP**）任务，并展示了通过使用自注意力机制和足够的数据量，可以在不使用递归神经网络的情况下实现竞争性性能。这是神经网络架构发展的一个重要步骤：展示了可以通过自注意力机制学习序列上下文，并提供能够从前所未有的数据量中学习的架构。
- en: '![PIC](img/file187.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file187.png)'
- en: 'Figure 9.6: Illustration of the transformer architecture'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：变换器架构示意图
- en: However, just as with the trend of more typical deep networks before them, the
    parameters of transformers are point estimates, rather than distributions, thus
    preventing them from being used for uncertainty quantification. Authors Boyang
    Xue *et al.* sought to remedy this in their paper, *Bayesian Transformer* *Language
    Models for Speech Recognition*. In their work, they demonstrate the variational
    inference can be successfully applied to transformer models, facilitating approximate
    Bayesian inference. However, due to the large size of transformers, Bayesian parameter
    estimation for all parameters is incredibly expensive. As such, Xue *et al.* apply
    Bayesian estimation to a subset of model parameters, specifically the parameters
    in the feed-forward and multi-head self-attention modules. As we see from *Figure*
    [*9.6*](#x1-184003r6), this excludes quite a few layers from the variational sampling
    process, thus saving compute cycles.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像之前更典型的深度网络一样，变换器的参数是点估计，而不是分布，因此无法用于不确定性量化。作者徐博扬*等人*在他们的论文《*贝叶斯变换器语言模型用于语音识别*》中试图解决这个问题。在他们的工作中，他们展示了变分推断可以成功地应用于变换器模型，从而促进了近似贝叶斯推断。然而，由于变换器的庞大规模，对所有参数进行贝叶斯参数估计是非常昂贵的。因此，徐博扬*等人*将贝叶斯估计应用于模型参数的一个子集，特别是前馈和多头自注意力模块中的参数。正如我们在*图*
    [*9.6*](#x1-184003r6)中看到的，这排除了相当多的层，从而节省了计算周期。
- en: Another method proposed in the paper *Transformers Can Do Bayesian* *Inference*,
    by Samuel Müller *et al.*, approximates Bayesian inference by exploiting the large
    amount of data used to train transformers. In their approach, dubbed **Prior-Data
    Fitted Networks (PFNs)**, the authors restate the problem of posterior approximation
    as a supervised learning task. That is to say, rather than obtaining a distribution
    of predictions via sampling, their method learns to approximate the posterior
    predictive distribution directly from dataset samples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《*变换器可以进行贝叶斯推断*》中，由Samuel Müller*等人*提出的另一种方法，通过利用训练变换器时使用的大量数据来近似贝叶斯推断。在他们的方法中，称为**先验数据拟合网络（PFNs）**，作者将后验近似问题重新表述为一个监督学习任务。也就是说，他们的方法不是通过采样获得预测分布，而是直接从数据集样本中学习近似后验预测分布。
- en: 'Algorithm 1: PFN model training procedure   **Input:** A prior distribution
    over datasets *p*(*D*), from which samples can be drawn and the number of samples
    *K* to draw'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1：PFN模型训练过程   **输入：** 数据集的先验分布 *p*(*D*)，从中可以抽取样本，抽取的样本数为 *K*
- en: '**Output:** A model *q𝜃* that will approximate the PPD Initialize the neural
    network *q𝜃*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：** 一个模型 *q𝜃* 用于近似 PPD，初始化神经网络 *q𝜃*'
- en: '**for** i:=1 to 10 **do**- 1:    `Sample` *D* ∪ (*x*[*i*]*,y*[*i*])[*i*=1]^(*m*)
    ≈ *p*(*D*)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**for** i:=1 到 10 **do**- 1:    `采样` *D* ∪ (*x*[*i*]*,y*[*i*])[*i*=1]^(*m*)
    ≈ *p*(*D*)'
- en: 2:  `Compute stochastic loss approximation` *l*[*𝜃*] = ∑ [*i*=1]^(*m*)(−log
    *q*[*𝜃*](*y*[*i*]|*x*[*i*]*,D*))
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '2:  `计算随机损失近似` *l*[*𝜃*] = ∑ [*i*=1]^(*m*)(−log *q*[*𝜃*](*y*[*i*]|*x*[*i*]*,D*))'
- en: 3:  *Update parameters* *𝜃* *with stochastic gradient descent on* ▿[*𝜃*]*l*[*𝜃*]
    =0
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '3:  *使用随机梯度下降更新参数* *𝜃* *，基于* ▿[*𝜃*]*l*[*𝜃*] =0'
- en: As represented in the pseudo code here, during training, the model samples multiple
    subsets of data comprising inputs *x* and labels *y*. It then masks one of the
    labels and learns to make a probabilistic prediction for this label based on the
    other data points. This allows the PFN to do probabilistic inference in a single
    forward pass – similarly to what we saw in [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) with
    PBP. While approximating Bayesian inference in a single forward pass is desirable
    for any application, this is even more valuable with transformers given their
    huge numbers of parameters – thus making the PFN approach described here particularly
    attractive.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如此伪代码所示，在训练过程中，模型会采样多个数据子集，这些子集包含输入 *x* 和标签 *y*。接着，它会屏蔽掉一个标签，并学习基于其他数据点对该标签做出概率预测。这使得
    PFN 能够在单次前向传递中进行概率推断——类似于我们在[*第五章*](CH5.xhtml#x1-600005) 和 [*贝叶斯深度学习的原则方法*](CH5.xhtml#x1-600005)中看到的
    PBP。尽管在单次前向传递中逼近贝叶斯推断对于任何应用来说都是理想的，但对于具有大量参数的 transformers 来说，这种方法更具价值——因此，这里描述的
    PFN 方法尤其具有吸引力。
- en: 'Of course, transformers are popularly used within transfer learning contexts:
    using the rich feature embeddings from transformers as inputs to smaller, less
    computationally demanding networks. As such, perhaps the most obvious way to use
    transformers in a Bayesian context would be to use its embeddings as an input
    to a BDL network – in fact, this is probably the most sensible first step in many
    cases.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，transformers 在迁移学习中被广泛应用：使用 transformers 提取的丰富特征嵌入作为输入，送入计算要求较低的较小网络。因此，在贝叶斯背景下，使用
    transformers 的嵌入作为 BDL 网络的输入，可能是最明显的使用方式——事实上，在许多情况下，这可能是最合理的第一步。
- en: In this section, we’ve explored some of the recent developments in BDL. All
    of these build on, and apply directly to, the methods we’ve introduced in the
    book, and you may want to consider implementing these when developing your own
    solutions for approximate Bayesian inference with deep networks. However, given
    the pace of research in machine learning, the list of improvements to Bayesian
    approximations is ever-growing, and we encourage you to explore the literature
    for yourself to discover the variety of ways in which researchers are learning
    to implement Bayesian inference at scale and with a variety of computational and
    theoretical advantages. That said, BDL isn’t always the correct solution, and
    in the next section, we’ll explore why.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了贝叶斯深度学习（BDL）中的一些最新进展。所有这些都建立在本书中介绍的方法之上，并直接应用于这些方法。当你为近似贝叶斯推断开发自己的深度网络解决方案时，你可能想考虑实现这些进展。然而，鉴于机器学习研究的快速进展，贝叶斯近似的改进列表不断增长，我们鼓励你亲自探索文献，了解研究人员如何在大规模实现贝叶斯推断，并利用各种计算和理论优势。不过，BDL
    并不总是正确的解决方案，在下一节中，我们将探讨其原因。
- en: 9.4 Alternatives to Bayesian deep learning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 贝叶斯深度学习的替代方法
- en: While the focus of the book is on Bayesian inference with DNNs, these aren’t
    always the best choice for the job. Generally speaking, they’re a great choice
    when you have large amounts of high dimensional data. As we discussed in [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003) (and as you probably know),
    deep networks excel in these scenarios, and thus adapting them for Bayesian inference
    is a sensible choice. On the other hand, if you have small amounts of low-dimensional
    data (with tens of features, fewer than 10,000 data points), then you may be better
    off with more traditional, well-principled Bayesian inference, such as via sampling
    or GPs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的重点是使用深度神经网络（DNN）进行贝叶斯推断，但它们并不总是最合适的选择。一般而言，当你拥有大量高维数据时，深度网络是一个很好的选择。正如我们在[*第3章*](CH3.xhtml#x1-350003)，[*深度学习基础*](CH3.xhtml#x1-350003)中所讨论的（以及你可能已经知道的），深度网络在这些场景中表现优异，因此将它们应用于贝叶斯推断是一个明智的选择。另一方面，如果你拥有的是少量低维数据（特征数十个，数据点少于10,000），那么你可能更适合使用更传统、更有原则的贝叶斯推断方法，例如通过采样或高斯过程。
- en: That said, there has been interest in scaling GPs, and the research community
    has developed GP-based methods that both scale to large amounts of data and are
    capable of complex non-linear transformations. In this section, we’ll introduce
    these alternatives in case you wish to pursue them further.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，关于扩展高斯过程的研究一直备受关注，研究社区已经开发出能够处理大量数据并能进行复杂非线性变换的基于高斯过程的方法。在这一节中，我们将介绍这些替代方法，以便你如果有兴趣进一步探讨它们时，可以了解相关内容。
- en: 9.4.1 Scalable Gaussian processes
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 可扩展的高斯过程
- en: 'At the beginning of the book, we introduced GPs and discussed why they are
    the gold standard when it comes to principled and reasonably computationally tractable
    uncertainty quantification in machine learning. Crucially, we talked about the
    limits of GPs: they become computationally infeasible with high-dimensional data
    or large amounts of data.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的开头，我们介绍了高斯过程并讨论了为什么它们是机器学习中关于原则性和计算上可处理的不确定性量化的黄金标准。关键是，我们谈到了高斯过程的局限性：当数据维度高或数据量大时，它们变得计算上不可行。
- en: 'However, GPs are extremely powerful tools, and the machine learning community
    wasn’t ready to give up on them. In [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals
    of Bayesian* *Inference*](CH2.xhtml#x1-250002), we discussed the key prohibitive
    factor in GP training and inference: inverting the covariance matrix. While methods
    exist for making this more computationally tractable (such as Cholesky decomposition),
    these methods only get us so far. As such, the key methods for making GPs scalable
    are termed *sparse GPs*, and they looks to solve the problem of intractable GP
    training by modifying the covariance matrix via sparse GP approximation. Simply
    put, if we can shrink or simplify the covariance matrix (for example, by reducing
    the number of data points), we can make the inversion of the covariance matrix
    tractable, and thus make GP training and inference tractable.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，高斯过程（GP）是极其强大的工具，机器学习社区并没有准备放弃它们。在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)中，我们讨论了高斯过程训练和推断中的关键障碍因素：反转协方差矩阵。虽然有一些方法可以使这一过程更具计算可行性（例如，Cholesky分解），但这些方法也只能做到一定程度。因此，使高斯过程可扩展的关键方法被称为*稀疏高斯过程*，它们通过稀疏高斯过程近似法来修改协方差矩阵，从而解决了不可处理的高斯过程训练问题。简单来说，如果我们能缩小或简化协方差矩阵（例如，通过减少数据点数量），就能使协方差矩阵的反演变得可行，从而使高斯过程训练和推断变得可处理。
- en: 'One of the most popular approaches for this was introduced in the paper, *Sparse*
    *Gaussian Processes using Pseudo-Inputs* by Edward Snelson and Zoubin Ghahramani.
    As with other sparse GP approaches, the authors developed a method for tractable
    GPs that leverages large datasets. In the paper, the authors show that they can
    closely approximate training with the full dataset by using a subset of data:
    they effectively circumvent the problem of large data by turning a *large data*
    problem into a *small data* problem. However, doing so requires selecting an appropriate
    subset of data points, which the authors term **pseudo** **inputs**.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最流行的方法之一是在Edward Snelson和Zoubin Ghahramani的论文《*使用伪输入的稀疏高斯过程*》中提出的。与其他稀疏高斯过程方法一样，作者们开发了一种利用大数据集的可处理高斯过程方法。在论文中，作者们展示了他们如何通过使用数据的子集来接近全数据集的训练：他们通过将*大数据*问题转化为*小数据*问题，实际上绕过了大数据问题。然而，做到这一点需要选择一个合适的数据子集，作者们称之为**伪**
    **输入**。
- en: 'The authors achieve this using a joint optimization process that selects the
    subset of data *M* from the full set *N* while also optimizing the hyperparameters
    of the kernel. This optimization process essentially finds the subset of data
    points that can be used to best describe the overall data: we illustrate this
    in *Figure* [*9.7*](#x1-186005r7).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过联合优化过程实现这一目标，该过程从完整数据集*N*中选择出数据子集*M*，同时优化核函数的超参数。这个优化过程实质上是寻找可以最好地描述整体数据的数据子集：我们在*图*[*9.7*](#x1-186005r7)中展示了这一点。
- en: '![PIC](img/file188.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file188.png)'
- en: 'Figure 9.7: Simple illustration of pseudo inputs'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：伪输入的简单示意图
- en: 'In this diagram, all data points are illustrated, but we see that certain data
    points have been selected as they describe the key relationship between our variables.
    However, these points not only need to describe the relationship, in terms of
    mean, as a polynomial regression may do – they also need to replicate the *variance*
    in the underlying data, such that the GP still produces well-calibrated uncertainty
    estimates. That is to say, while the pseudo inputs effectively reduce the number
    of data points, the distribution of the pseudo inputs still needs to approximate
    that of the true inputs: if an area in the true data distribution is rich in data,
    thereby producing confident predictions in this region, this needs to be true
    for the pseudo inputs too.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，所有的数据点都有展示，但我们看到某些数据点被选中，因为它们描述了我们变量之间的关键关系。然而，这些点不仅需要描述关系，就像多项式回归可能做的那样——它们还需要复制底层数据中的*方差*，使得高斯过程（GP）依然能够产生良好校准的不确定性估计。换句话说，虽然伪输入有效地减少了数据点的数量，但伪输入的分布仍需近似真实输入的分布：如果真实数据分布中的某个区域数据丰富，从而在该区域产生有信心的预测，那么伪输入也需要满足这一点。
- en: Another method for scalable GPs was introduced more recently by Ke Wang *et*
    *al.* in their paper, *Exact Gaussian Processes on a Million Data Points*. In
    this work, the authors leverage recent developments in multi-GPU parallelization
    methods to implement scalable GPs. The method that enabled this is called **Blackbox
    Matrix-Matrix Multiplication** (**BBMM**), reduces the problem of GP inference
    to iterations of matrix multiplication. In so doing, it makes the process easier
    to parallelize, as the matrix multiplications can be partitioned and distributed
    over multiple GPUs. The authors show that doing so reduces the memory requirement
    for GP training to *O*(*n*) per GPU. This allows GPs to benefit from the kind
    of computational gains that have been benefiting deep learning methods for over
    a decade!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由Ke Wang *et* *al.*在他们的论文《*百万数据点上的精确高斯过程*》中提出了另一种可扩展高斯过程的方法。在这项工作中，作者利用了多GPU并行化方法的最新进展来实现可扩展高斯过程。使这一方法得以实现的技术被称为**黑盒矩阵-矩阵乘法**（**BBMM**），它将高斯过程推断问题简化为矩阵乘法的迭代过程。通过这样做，它使得该过程更容易进行并行化，因为矩阵乘法可以被划分并分配到多个GPU上。作者展示了这种方法将高斯过程训练的内存需求减少到每个GPU的*O*(*n*)。这使得高斯过程能够受益于深度学习方法在过去十多年里获得的计算优势！
- en: 'Both of the methods presented here do a great job of addressing the scalability
    issues faced by GPs. The second method is particularly impressive as it achieves
    exact GP inference, but it does require significant compute infrastructure. The
    pseudo-inputs method, on the other hand, is practical for a larger proportion
    of use cases. However, neither of these approaches tackle one of the key advantages
    of BDL: the ability of deep networks to learn rich embeddings through complex
    non-linear transformations.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里呈现的两种方法都很好地解决了高斯过程（GP）面临的可扩展性问题。第二种方法尤其令人印象深刻，因为它实现了精确的高斯过程推断，但确实需要显著的计算基础设施。另一方面，伪输入方法在更大比例的使用场景中具有实用性。然而，这两种方法都未解决贝叶斯深度学习（BDL）的关键优势之一：深度网络通过复杂的非线性变换学习丰富的嵌入的能力。
- en: 9.4.2 Deep Gaussian processes
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 深度高斯过程
- en: 'Introduced by Andreas Damianou and Neil Lawrence in their straightforwardly
    titled paper, *Deep Gaussian Processes*, deep GPs tackle the problem of rich embeddings
    through having layers of GPs, much in the same way that deep networks have multiple
    layers of neurons. Unlike the scalable GP work mentioned previously, deep GPs
    were motivated by the inverse of the scalability problem: how can we get the performance
    of deep networks with very little data?'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 深度高斯过程由Andreas Damianou和Neil Lawrence在他们标题直白的论文《*深度高斯过程*》中提出，深度高斯过程通过拥有多层高斯过程来解决丰富嵌入的问题，就像深度网络有多层神经元一样。与之前提到的可扩展高斯过程不同，深度高斯过程的提出正是受到可扩展性问题反向问题的启发：我们如何能用非常少的数据获得深度网络的性能？
- en: Faced with this problem, and with the knowledge that GPs perform very well on
    small amounts of data, Damianou and Lawrence set out to see whether GPs could
    be layered to produce similarly rich embeddings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 面对这个问题，并且意识到高斯过程在少量数据上表现非常好，Damianou和Lawrence着手研究是否可以将高斯过程分层，从而生成类似的丰富嵌入。
- en: '![PIC](img/file189.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file189.png)'
- en: 'Figure 9.8: Illustration of a deep GP'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：深度GP的示意图
- en: 'Their approach, while complex in terms of implementation, is simple in principle:
    just as a DNN comprises many layers, each receiving an input from the layer before
    it and feeding its output into the layer after it, deep GPs also assume this form
    of graphical structure – as we see in *Figure* [*9.8*](#x1-187003r8). Mathematically,
    just as with deep networks, a deep GP can be viewed as a composition of functions.
    The GP illustrated previously could thus be described as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的方法，尽管在实施上比较复杂，但原理很简单：正如一个深度神经网络（DNN）由许多层组成，每一层接收前一层的输入并将输出传递给后一层，深度高斯过程（GP）也假设有这种图形结构——正如我们在*图*
    [*9.8*](#x1-187003r8)中看到的那样。从数学角度来看，就像深度网络一样，深度GP可以视为函数的组合。因此，之前展示的GP可以描述为：
- en: '![y = g(x ) = f2(f1(x)) ](img/file190.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![y = g(x ) = f2(f1(x)) ](img/file190.jpg)'
- en: 'While this introduces the kind of rich non-linear transformations we’re accustomed
    to in deep learning to GPs, it comes at a price. As we already know, standard
    GPs have limitations when it comes to scalability. Unfortunately for deep GPs,
    composing them in this way is analytically intractable. As such, Damianou and
    Lawrence had to find a tractable way of implementing deep GPs, and they did so
    using a tool that should now be familiar to you: variational approximation. Just
    as this forms an import building block for some of the BDL methods introduced
    in this book, it’s also a key component in making deep GPs possible. In their
    paper, they show how deep GPs can be implemented with the help of variational
    approximations – making it possible not only to produce rich, non-linear embeddings
    with GPs but for rich, non-linear embeddings to be achieved with *small* *amounts
    of data*. This makes deep GPs an important tool in the arsenal of Bayesian methods
    and is thus a method worth bearing in mind going forward.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这将我们在深度学习中习惯的丰富非线性变换引入了高斯过程中，但这也带来了代价。正如我们已经知道的那样，标准的高斯过程在可扩展性方面存在限制。不幸的是，深度GP通过这种方式组合是分析上无法解决的。因此，Damianou和Lawrence必须找到一种可行的实现深度GP的方法，他们通过一种现在你应该已经熟悉的工具：变分近似，解决了这一问题。正如它在本书中介绍的一些BDL方法中构成了重要的构建块一样，它也是使深度GP成为可能的关键组成部分。在他们的论文中，他们展示了如何借助变分近似实现深度GP——这不仅使得使用GP生成丰富的非线性嵌入成为可能，而且使得在*少量*
    *数据*的情况下实现丰富的非线性嵌入成为可能。这使得深度GP成为贝叶斯方法工具箱中的一个重要工具，因此，它是一个值得记住的方法。
- en: 9.5 Your next steps in BDL
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 你在BDL中的下一步
- en: Throughout this chapter, we’ve concluded our introduction to BDL by taking a
    look at a variety of techniques that could help you to improve on the fundamental
    methods explored in the book. We’ve also taken a look at how the powerful gold-standard
    of Bayesian inference – the GP – can be adapted to tasks generally reserved for
    deep learning. While it is indeed possible to adapt GPs to these tasks, we also
    advise that it’s generally easier and more practical to use the methods presented
    in this book, or methods derived from them. As always, it’s up to you as the machine
    learning engineer to determine what is best for the task at hand, and we are confident
    that the material from the book will equip you well for the challenges ahead.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过回顾一些可以帮助你改进书中探讨的基本方法的技术，完成了对BDL的介绍。我们还研究了如何将贝叶斯推理的强大标准——高斯过程——适应于一般用于深度学习的任务。虽然确实可以将高斯过程适应这些任务，但我们也建议，一般来说，使用本书中介绍的方法或从中衍生的方法会更容易且更实用。像往常一样，作为机器学习工程师，你需要自己判断什么方法最适合当前的任务，我们相信本书的内容将为你迎接未来的挑战提供充足的准备。
- en: While this book provides you with the necessary fundamentals to get started,
    there’s always more learn – particularly in such a rapidly moving field! In the
    next section, we’ll provide a few key final recommendations, helping you to plan
    your next steps in learning about and applying BDL.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书为你提供了开始的必要基础，但在如此快速发展的领域中，总有更多的东西值得学习！在下一节中，我们将提供一些关键的最终建议，帮助你规划下一步的学习与应用BDL的过程。
- en: We hope that you’ve found this introduction to Bayesian deep learning to be
    comprehensive, practical, and enjoyable. Thank you for reading – we wish you success
    in exploring these methods further and applying them within your own machine learning
    solutions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您觉得这篇关于贝叶斯深度学习的介绍内容既全面、实用又有趣。感谢您的阅读——我们祝愿您在进一步探索这些方法并将其应用到您自己的机器学习解决方案中时取得成功。
- en: 9.6 Further reading
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 进一步阅读
- en: 'The following reading recommendations are provided for those who wish to learn
    more about the recent methods presented in this chapter. These give a great insight
    into current challenges in the field, looking beyond Bayesian neural networks
    and into scalable Bayesian inference more generally:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下阅读推荐适合那些希望进一步了解本章所呈现的最新方法的读者。这些资源为当前领域中的挑战提供了极好的洞察，关注的不仅仅是贝叶斯神经网络，还包括更广泛的可扩展贝叶斯推断：
- en: '*Deep Ensemble Bayesian Active Learning*, Pop and Fulop: This paper demonstrates
    the advantages of combining deep ensembles with MC dropout to produce better-calibrated
    uncertainty estimates, as shown when applying their method to active learning
    tasks.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度集成贝叶斯主动学习*，Pop 和 Fulop：本文展示了将深度集成与 MC dropout 结合的优势，能够提供更精确的预测不确定性估计，正如在应用该方法到主动学习任务时所展示的那样。'
- en: '*Uncertainty in Neural Networks: Approximately Bayesian Ensembling*, Pearce
    *et al.*: This paper introduces a simple and effective method for improving the
    performance of deep ensembles. The authors show that by promoting diversity through
    a simple adaptation to the loss function, the ensemble is able to produces better-calibrated
    uncertainty estimates.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经网络中的不确定性：近似贝叶斯集成*，Pearce 等人：本文介绍了一种简单且有效的方法，用于提高深度集成的性能。作者展示了通过对损失函数做简单调整来促进多样性，从而使得集成能够生成更为精确的不确定性估计。'
- en: '*Sparse Gaussian Processes Using Pseudo-Inputs*, Snelson and Gharamani: This
    paper introduces the concept of pseudo-input-based GPs, introducing a key method
    in scalable GP inference.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用伪输入的稀疏高斯过程*，Snelson 和 Gharamani：本文介绍了基于伪输入的高斯过程概念，提出了可扩展高斯过程推断中的一个关键方法。'
- en: '*Exact Gaussian Processes on a Million Data Points*, Wang *et al.*: An important
    paper demonstrating that Gaussian Processes can benefit from developments in compute
    hardware through the use of BBMM, making exact GP inference possible for big data.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*百万数据点上的精确高斯过程*，Wang 等人：一篇重要论文，展示了通过使用 BBMM 技术，高斯过程可以受益于计算硬件的发展，使得大数据的精确高斯过程推断成为可能。'
- en: '*Deep Gaussian Processes*, Damianou and Lawrence: Introducing the concept of
    deep GPs, this paper demonstrates how GPs can be used to achieve complex non-linear
    transformations with datasets far smaller than those required for deep learning.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度高斯过程*，Damianou 和 Lawrence：引入深度高斯过程（GPs）这一概念，本文展示了如何使用高斯过程实现复杂的非线性变换，且所需的数据集远小于深度学习所需的数据集。'
- en: 'We’ve selected a few key resources to help you in your next steps into BDL,
    allowing you to dive deeper into the theory and helping you to get the most out
    of the content presented here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们挑选了一些关键资源，帮助您进入贝叶斯深度学习的下一阶段，让您深入理论并帮助您更好地利用本章内容：
- en: '*Machine Learning: A Probabilistic Perspective*, Murphy: Released in 2012,
    this has since become one of the key texts on machine learning, presenting a well-principled
    approach to understanding all of the key methods within machine learning. The
    probabilistic angle of the book makes it a great addition to your collection of
    Bayesian literature.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习：一种概率视角*，Murphy：该书于 2012 年发布，之后成为机器学习领域的核心教材之一，呈现了一种理解机器学习中所有关键方法的合理方法。该书的概率视角使其成为贝叶斯文献收藏中的一大亮点。'
- en: '*Probabilistic Machine Learning: An Introduction*, Murphy: Another more recent
    Murphy text. Released in 2022, this is another important text providing a detailed
    treatment of probabilistic machine learning (including a section on Bayesian neural
    networks). While there is some overlap between this and Murphy’s previous text,
    both are worth having at your disposal.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*概率机器学习：入门*，Murphy：另一篇较新的 Murphy 作品。该书于 2022 年发布，提供了关于概率机器学习（包括贝叶斯神经网络部分）的详细内容。尽管与
    Murphy 之前的作品有所重叠，但两者都值得拥有，且各有其独到之处。'
- en: '*Gaussian Processes for Machine Learning*, Rasmussen and Williams: Perhaps
    the most important text on Gaussian Processes, this is a hugely valuable text
    when it comes to Bayesian inference. The authors’ detailed explanation of GPs
    will give you a thorough understanding of this important piece of the Bayesian
    puzzle.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习中的高斯过程*，拉斯穆森和威廉姆斯：也许是关于高斯过程最重要的文本，这本书在贝叶斯推断中具有极高的价值。作者对高斯过程的详细解释将帮助你全面理解贝叶斯难题中的这一重要部分。'
- en: '*Bayesian Analysis with Python*, Martin: Covering all the fundamentals of Bayesian
    analysis, this title is an excellent piece of foundational literature and will
    help you to dive deeper into the fundamentals of Bayesian inference.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python中的贝叶斯分析*，马丁：涵盖了贝叶斯分析的所有基础知识，这本书是一本出色的基础文献，将帮助你更深入地理解贝叶斯推断的基础。'
