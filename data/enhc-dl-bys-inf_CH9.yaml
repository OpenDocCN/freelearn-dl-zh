- en: ChapterÂ 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬9ç« 
- en: Next Steps in Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„ä¸‹ä¸€æ­¥
- en: Throughout this book, weâ€™ve covered the fundamental concepts behind Bayesian
    deep learning (BDL), from understanding what uncertainty is and its role in developing
    robust machine learning systems, right through to learning how to implement and
    analyze the performance of several fundamental BDL. While what youâ€™ve learned
    will equip you to start developing your own BDL solutions, the field is moving
    quickly, and there are many new techniques on the horizon.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»äº†è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰çš„åŸºæœ¬æ¦‚å¿µï¼Œä»ç†è§£ä»€ä¹ˆæ˜¯ä¸ç¡®å®šæ€§åŠå…¶åœ¨å¼€å‘ç¨³å¥çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„ä½œç”¨ï¼Œåˆ°å­¦ä¹ å¦‚ä½•å®ç°å’Œåˆ†æå‡ ä¸ªåŸºæœ¬BDLçš„æ€§èƒ½ã€‚è™½ç„¶ä½ æ‰€å­¦åˆ°çš„å†…å®¹è¶³ä»¥å¸®åŠ©ä½ å¼€å§‹å¼€å‘è‡ªå·±çš„BDLè§£å†³æ–¹æ¡ˆï¼Œä½†è¯¥é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œè®¸å¤šæ–°æŠ€æœ¯æ­£åœ¨æ¶Œç°ã€‚
- en: To wrap up the book, in this chapter weâ€™ll take a look at the current trends
    in BDL, before we dive into some of the latest developments in the field. Weâ€™ll
    conclude by introducing some alternatives to BDL, and provide some advice on additional
    resources you can use to continue your journey into Bayesian machine learning
    methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å›é¡¾BDLçš„å½“å‰è¶‹åŠ¿ï¼Œç„¶åæ·±å…¥æ¢è®¨è¯¥é¢†åŸŸçš„ä¸€äº›æœ€æ–°å‘å±•ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›BDLçš„æ›¿ä»£æ–¹æ³•ï¼Œå¹¶æä¾›ä¸€äº›å»ºè®®ï¼Œä»‹ç»ä½ å¯ä»¥ä½¿ç”¨çš„é¢å¤–èµ„æºï¼Œå¸®åŠ©ä½ ç»§ç»­æ·±å…¥è´å¶æ–¯æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ¢ç´¢ã€‚
- en: 'Weâ€™ll cover the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ç« èŠ‚ï¼š
- en: Current trends in BDL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“å‰BDLçš„è¶‹åŠ¿
- en: How are BDL methods being applied to solve real-world problems?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDLæ–¹æ³•æ˜¯å¦‚ä½•åº”ç”¨äºè§£å†³ç°å®é—®é¢˜çš„ï¼Ÿ
- en: Latest methods in BDL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDLçš„æœ€æ–°æ–¹æ³•
- en: Alternatives to BDL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDLçš„æ›¿ä»£æ–¹æ³•
- en: Your next steps in BDL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ åœ¨BDLä¸­çš„ä¸‹ä¸€æ­¥
- en: 9.1 Current trends in BDL
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 å½“å‰BDLçš„è¶‹åŠ¿
- en: In this section, weâ€™ll explore the current trends in BDL. Weâ€™ll look at which
    models are particularly popular in the literature and discuss why certain models
    have been selected for certain applications. This should give you a good imdivssion
    of how the fundamentals covered throughout the book apply more broadly across
    a variety of application domains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨BDLçš„å½“å‰è¶‹åŠ¿ã€‚æˆ‘ä»¬å°†æŸ¥çœ‹å“ªäº›æ¨¡å‹åœ¨æ–‡çŒ®ä¸­å°¤ä¸ºæµè¡Œï¼Œå¹¶è®¨è®ºä¸ºä½•æŸäº›æ¨¡å‹è¢«é€‰ç”¨äºç‰¹å®šåº”ç”¨ã€‚è¿™å°†ä¸ºä½ æä¾›ä¸€ä¸ªå…³äºæœ¬ä¹¦ä¸­æ¶µç›–çš„åŸºç¡€çŸ¥è¯†å¦‚ä½•æ›´å¹¿æ³›åº”ç”¨äºå„ç§åº”ç”¨é¢†åŸŸçš„è‰¯å¥½å°è±¡ã€‚
- en: '![PIC](img/file182.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file182.png)'
- en: 'FigureÂ 9.1: Popularity of key BDL search terms over time'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.1ï¼šBDLå…³é”®æœç´¢è¯çš„æµè¡Œåº¦éšæ—¶é—´çš„å˜åŒ–
- en: As we see in *Figure* [9.1](#x1-179002r1), thereâ€™s been a marked increase in
    popularity of search terms related to BDL over the past decade. Unsurprisingly,
    this follows the trend in the popularity of deep learning search terms, as we
    see in *Figure* [*9.2*](#x1-179004r2); as deep learning has become more popular,
    there has been increased interest in quantifying the uncertainty associated with
    the predictions produced by DNNs. Interestingly, these plots both show a similar
    dip in popularity in mid-late 2021, indicating that as long as deep learning is
    popular, there will also be interest in BDL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨*å›¾* [9.1](#x1-179002r1)ä¸­çœ‹åˆ°çš„ï¼Œè¿‡å»åå¹´ä¸BDLç›¸å…³çš„æœç´¢è¯çš„æµè¡Œåº¦æ˜¾è‘—å¢åŠ ã€‚æ¯«ä¸å¥‡æ€ªï¼Œè¿™ä¸æ·±åº¦å­¦ä¹ ç›¸å…³æœç´¢è¯çš„æµè¡Œè¶‹åŠ¿ä¸€è‡´ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*å›¾*
    [*9.2*](#x1-179004r2)ä¸­çœ‹åˆ°çš„ï¼›éšç€æ·±åº¦å­¦ä¹ çš„æ™®åŠï¼Œäººä»¬å¯¹é‡åŒ–DNNé¢„æµ‹ç»“æœçš„ä¸ç¡®å®šæ€§çš„å…´è¶£ä¹Ÿéšä¹‹å¢åŠ ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™äº›å›¾è¡¨éƒ½æ˜¾ç¤ºäº†åœ¨2021å¹´ä¸­è‡³æ™šæœŸæµè¡Œåº¦å‡ºç°äº†ç±»ä¼¼çš„ä¸‹é™ï¼Œè¡¨æ˜åªè¦æ·±åº¦å­¦ä¹ å—æ¬¢è¿ï¼ŒBDLä¹Ÿä¼šå—åˆ°å…³æ³¨ã€‚
- en: '![PIC](img/file183.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file183.png)'
- en: 'FigureÂ 9.2: Popularity of key deep learning search terms over time'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.2ï¼šå…³é”®æ·±åº¦å­¦ä¹ æœç´¢è¯çš„æµè¡Œåº¦éšæ—¶é—´çš„å˜åŒ–
- en: '*Figure* [9.1](#x1-179002r1) demonstrates another interesting point in that,
    generally speaking, the term *variational inference* is more popular than the
    other two BDL-related search terms weâ€™ve used here. As mentioned in [*ChapterÂ 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) when
    we covered variational autoencoders, variational inference is one component of
    BDL that has made significant waves in the machine learning community, now being
    a feature of many different deep learning architectures. As such, itâ€™s no surprise
    that it is generally more popular than the terms that explicitly include the word
    â€Bayesian.â€'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾* [9.1](#x1-179002r1)å±•ç¤ºäº†å¦ä¸€ä¸ªæœ‰è¶£çš„è§‚ç‚¹ï¼Œä¸€èˆ¬è€Œè¨€ï¼Œæœ¯è¯­*å˜åˆ†æ¨æ–­*æ¯”æˆ‘ä»¬åœ¨æ­¤ä½¿ç”¨çš„å¦å¤–ä¸¤ä¸ªBDLç›¸å…³çš„æœç´¢è¯æ›´å—æ¬¢è¿ã€‚å¦‚åœ¨[*ç¬¬5ç« *](CH5.xhtml#x1-600005)ä¸­æåˆ°çš„ï¼Œ[*è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„åŸåˆ™æ€§æ–¹æ³•*](CH5.xhtml#x1-600005)ä¸­æˆ‘ä»¬è®¨è®ºäº†å˜åˆ†è‡ªç¼–ç å™¨ï¼Œå˜åˆ†æ¨æ–­æ˜¯BDLçš„ä¸€éƒ¨åˆ†ï¼Œåœ¨æœºå™¨å­¦ä¹ ç¤¾åŒºäº§ç”Ÿäº†æ˜¾è‘—å½±å“ï¼Œç°åœ¨å·²æˆä¸ºè®¸å¤šä¸åŒæ·±åº¦å­¦ä¹ æ¶æ„çš„ä¸€ä¸ªç‰¹ç‚¹ã€‚å› æ­¤ï¼Œå®ƒæ¯”æ˜ç¡®åŒ…å«â€œè´å¶æ–¯â€ä¸€è¯çš„æœ¯è¯­æ›´å—æ¬¢è¿ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚'
- en: But where do the methods that weâ€™ve explored in the book fit in terms of their
    popularity and integration into a wide variety of deep learning solutions? We
    can learn more about this simply by looking at the citations for each of the original
    papers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬åœ¨ä¹¦ä¸­æ¢è®¨çš„é‚£äº›æ–¹æ³•åœ¨å®ƒä»¬çš„æµè¡Œåº¦å’Œåœ¨å„ç§æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆä¸­çš„åº”ç”¨æƒ…å†µå¦‚ä½•å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡ç®€å•åœ°æŸ¥çœ‹æ¯ç¯‡åŸå§‹è®ºæ–‡çš„å¼•ç”¨æ¬¡æ•°æ¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: '![PIC](img/file184.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file184.png)'
- en: 'FigureÂ 9.3: Popularity of key deep learning search terms over time'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.3ï¼šæ·±åº¦å­¦ä¹ å…³é”®æœç´¢è¯çš„æµè¡Œåº¦éšæ—¶é—´å˜åŒ–
- en: 'In *Figure* [*9.3*](#x1-179007r3), we see that the MC dropout paper is by far
    the most popular paper when it comes to citations â€“ coming in at nearly double
    that of the next most popular method. At this point in the book, the reasons for
    this should be fairly clear: not only is it one of the easiest methods to implement
    (as we saw in [*ChapterÂ 6*](CH6.xhtml#x1-820006), [*Using the Standard Toolbox
    for Bayesian Deep Learning*](CH6.xhtml#x1-820006)), but itâ€™s also one of the most
    attractive from a computation standpoint. It requires no more memory than a standard
    neural network and, as we saw in [*ChapterÂ 7*](CH7.xhtml#x1-1130007), [*Practical*
    *Considerations for Bayesian Deep Learning*](CH7.xhtml#x1-1130007), itâ€™s also
    one of the fastest models for running inference. These practical factors often
    weigh in more heavily than considerations such as uncertainty quality when it
    comes to selecting models.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[*å›¾9.3*](#x1-179007r3)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°MC dropoutè®ºæ–‡åœ¨å¼•ç”¨æ¬¡æ•°ä¸Šé¥é¥é¢†å…ˆâ€”â€”å‡ ä¹æ˜¯ç¬¬äºŒç§æœ€å—æ¬¢è¿æ–¹æ³•çš„ä¸¤å€ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¹¦ä¸­å·²ç»ç›¸å½“æ¸…æ¥šåœ°è¯´æ˜äº†è¿™ä¸ªåŸå› ï¼šå®ƒä¸ä»…æ˜¯æœ€å®¹æ˜“å®ç°çš„æ–¹æ³•ä¹‹ä¸€ï¼ˆæ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬6ç« *](CH6.xhtml#x1-820006)ï¼Œ[*ä½¿ç”¨æ ‡å‡†å·¥å…·ç®±è¿›è¡Œè´å¶æ–¯æ·±åº¦å­¦ä¹ *](CH6.xhtml#x1-820006)ä¸­çœ‹åˆ°çš„ï¼‰ï¼Œè€Œä¸”ä»è®¡ç®—è§’åº¦æ¥çœ‹ï¼Œå®ƒä¹Ÿæ˜¯æœ€å…·å¸å¼•åŠ›çš„ä¹‹ä¸€ã€‚å®ƒæ‰€éœ€çš„å†…å­˜ä¸æ ‡å‡†ç¥ç»ç½‘ç»œç›¸åŒï¼Œè€Œä¸”æ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬7ç« *](CH7.xhtml#x1-1130007)ä¸­çœ‹åˆ°çš„ï¼Œ[*è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„å®é™…è€ƒè™‘*](CH7.xhtml#x1-1130007)ï¼Œå®ƒä¹Ÿæ˜¯è¿è¡Œæ¨ç†æ—¶æœ€å¿«çš„æ¨¡å‹ä¹‹ä¸€ã€‚è¿™äº›å®é™…å› ç´ åœ¨é€‰æ‹©æ¨¡å‹æ—¶é€šå¸¸æ¯”ä¸ç¡®å®šæ€§è´¨é‡ç­‰å› ç´ æ›´é‡è¦ã€‚
- en: 'Practical considerations are again likely the reason behind the second most
    popular method being deep ensembles. While this may not be the most efficient
    method in terms of training time, itâ€™s often the speed of inference that counts
    the most: and again, looking back to [*ChapterÂ 7*](CH7.xhtml#x1-1130007), [*Practical*
    *Considerations for Bayesian Deep Learning*](CH7.xhtml#x1-1130007)â€™s results,
    we see that the ensemble excels here, despite needing to run inference on multiple
    different networks.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…è€ƒè™‘å› ç´ å¾ˆå¯èƒ½æ˜¯ç¬¬äºŒç§æœ€æµè¡Œæ–¹æ³•â€”â€”æ·±åº¦é›†æˆæ³•çš„åŸå› ã€‚è™½ç„¶ä»è®­ç»ƒæ—¶é—´æ¥çœ‹ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€é«˜æ•ˆçš„æ–¹æ³•ï¼Œä½†é€šå¸¸æ¨ç†çš„é€Ÿåº¦æ‰æ˜¯æœ€é‡è¦çš„ï¼šå†æ¬¡å›é¡¾[*ç¬¬7ç« *](CH7.xhtml#x1-1130007)ï¼Œ[*è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„å®é™…è€ƒè™‘*](CH7.xhtml#x1-1130007)çš„ç»“æœï¼Œæˆ‘ä»¬çœ‹åˆ°å°½ç®¡éœ€è¦å¯¹å¤šä¸ªä¸åŒçš„ç½‘ç»œè¿›è¡Œæ¨ç†ï¼Œé›†æˆæ³•åœ¨è¿™é‡Œè¡¨ç°ä¼˜å¼‚ã€‚
- en: 'Deep ensembles often strike a good balance between ease of implementation and
    theoretical considerations: as discussed in [*ChapterÂ 6*](CH6.xhtml#x1-820006),
    [*Using the Standard Toolbox* *for Bayesian Deep Learning*](CH6.xhtml#x1-820006),
    ensembling is a powerful tool within ML, and so itâ€™s no surprise that NN ensembles
    perform well and often produce well-calibrated uncertainty estimates.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦é›†æˆæ³•é€šå¸¸åœ¨å®ç°çš„ç®€ä¾¿æ€§å’Œç†è®ºè€ƒè™‘ä¹‹é—´å–å¾—è‰¯å¥½çš„å¹³è¡¡ï¼šæ­£å¦‚åœ¨[*ç¬¬6ç« *](CH6.xhtml#x1-820006)ï¼Œ[*ä½¿ç”¨æ ‡å‡†å·¥å…·ç®±è¿›è¡Œè´å¶æ–¯æ·±åº¦å­¦ä¹ *](CH6.xhtml#x1-820006)ä¸­è®¨è®ºçš„é‚£æ ·ï¼Œé›†æˆæ³•æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¼ºå¤§å·¥å…·ï¼Œå› æ­¤ï¼Œç¥ç»ç½‘ç»œé›†æˆæ³•è¡¨ç°è‰¯å¥½å¹¶ä¸”é€šå¸¸èƒ½ç”Ÿæˆè‰¯å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚
- en: 'The last two methods, taking third and fourth place respectively, are BBB and
    PBP. While BBB is far easier to implement than PBP, the fact that it requires
    some probabilistic components often means that â€“ while in many cases it may be
    the best tool for the job â€“ machine learning engineers may not be aware of it
    or arenâ€™t comfortable implementing it. PBP takes this to a further extreme: as
    we saw in [*ChapterÂ 5*](CH5.xhtml#x1-600005), [*Principled Approaches for* *Bayesian
    Deep Learning*](CH5.xhtml#x1-600005), implementing PBP is not a straightforward
    task. At the time of writing, there are no deep learning frameworks that incorporate
    an easy-to-use and well-optimized implementation of PBP, and â€“ apart from the
    BDL community â€“ many machine learning researchers and practitioners just arenâ€™t
    aware of its existence, as is made clear by the fact that it has fewer citations
    (although it still has a fairly impressive citation count!).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ’åç¬¬ä¸‰å’Œç¬¬å››çš„æœ€åä¸¤ç§æ–¹æ³•æ˜¯ BBB å’Œ PBPã€‚è™½ç„¶ BBB æ¯” PBP æ›´å®¹æ˜“å®ç°ï¼Œä½†ç”±äºå®ƒéœ€è¦ä¸€äº›æ¦‚ç‡ç»„ä»¶ï¼Œé€šå¸¸æ„å‘³ç€â€”â€”å°½ç®¡åœ¨è®¸å¤šæƒ…å†µä¸‹å®ƒå¯èƒ½æ˜¯æœ€åˆé€‚çš„å·¥å…·â€”â€”æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå¯èƒ½ä¸äº†è§£å®ƒï¼Œæˆ–è€…ä¸ä¹ æƒ¯å®ç°å®ƒã€‚PBP
    åˆ™æ›´ä¸ºæç«¯ï¼šæ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬ 5 ç« *](CH5.xhtml#x1-600005)ä¸­æ‰€çœ‹åˆ°çš„ï¼Œ[*è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„åŸåˆ™æ€§æ–¹æ³•*](CH5.xhtml#x1-600005)ï¼Œå®ç°
    PBP ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚å†™ä½œæ—¶ï¼Œæ²¡æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶æä¾›æ˜“ç”¨ä¸”ç»è¿‡ä¼˜åŒ–çš„ PBP å®ç°â€”â€”é™¤äº† BDL ç¤¾åŒºå¤–ï¼Œè®¸å¤šæœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜å’Œå®è·µè€…æ ¹æœ¬ä¸çŸ¥é“å®ƒçš„å­˜åœ¨ï¼Œè¿™ä¸€ç‚¹ä»å®ƒçš„å¼•ç”¨æ¬¡æ•°è¾ƒå°‘å¯ä»¥çœ‹å‡ºï¼ˆå°½ç®¡å®ƒçš„å¼•ç”¨æ•°é‡ä»ç„¶ç›¸å½“å¯è§‚ï¼ï¼‰ã€‚
- en: 'This analysis of the popularity of Bayesian deep learning methods seems to
    tell a pretty clear story: BNN approaches are chosen based primarily on the ease
    of implementation. Indeed, there is a significant amount of literature that discusses
    the use of BDL methods for their uncertainty estimates without considering the
    *quality* of the model uncertainty estimates. Fortunately, with the increasing
    popularity of uncertainty-aware methods, this trend is beginning to decline, and
    we hope that this book has equipped you with the necessary tools to allow you
    to be more principled in your selection of BNN methods. Irrespective of the methods
    used or how theyâ€™re selected, itâ€™s clear that machine learning researchers and
    practitioners are increasingly interested in BDL approaches â€“ so what are these
    methods being used for? Letâ€™s take a look.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹è´å¶æ–¯æ·±åº¦å­¦ä¹ æ–¹æ³•æµè¡Œåº¦çš„åˆ†æä¼¼ä¹è®²è¿°äº†ä¸€ä¸ªç›¸å½“æ¸…æ™°çš„æ•…äº‹ï¼šBNN æ–¹æ³•çš„é€‰æ‹©ä¸»è¦æ˜¯åŸºäºå®ç°çš„ç®€ä¾¿æ€§ã€‚äº‹å®ä¸Šï¼Œæœ‰å¤§é‡æ–‡çŒ®è®¨è®ºäº†ä½¿ç”¨ BDL æ–¹æ³•è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œè€Œæ²¡æœ‰è€ƒè™‘æ¨¡å‹ä¸ç¡®å®šæ€§ä¼°è®¡çš„*è´¨é‡*ã€‚å¹¸è¿çš„æ˜¯ï¼Œéšç€ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³•çš„æ—¥ç›Šæµè¡Œï¼Œè¿™ä¸€è¶‹åŠ¿å¼€å§‹ä¸‹é™ï¼Œæˆ‘ä»¬å¸Œæœ›æœ¬ä¹¦å·²ä¸ºä½ æä¾›äº†å¿…è¦çš„å·¥å…·ï¼Œä½¿ä½ åœ¨é€‰æ‹©
    BNN æ–¹æ³•æ—¶æ›´åŠ æœ‰åŸåˆ™ã€‚ä¸è®ºæ‰€ä½¿ç”¨çš„æ–¹æ³•æˆ–å…¶é€‰æ‹©æ–¹å¼å¦‚ä½•ï¼Œå¾ˆæ˜æ˜¾ï¼Œæœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜å’Œå®è·µè€…å¯¹ BDL æ–¹æ³•è¶Šæ¥è¶Šæ„Ÿå…´è¶£â€”â€”é‚£ä¹ˆè¿™äº›æ–¹æ³•åˆ°åº•æ˜¯ç”¨æ¥åšä»€ä¹ˆçš„å‘¢ï¼Ÿè®©æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚
- en: 9.2 How are BDL methods being applied to solve real-world problems?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 BDL æ–¹æ³•å¦‚ä½•åº”ç”¨äºè§£å†³å®é™…é—®é¢˜ï¼Ÿ
- en: Just as deep learning is having an impact on a diverse variety of application
    domains, BDL is becoming an increasingly important tool, particularly where large
    amounts of data are being used within safety-critical or mission-critical systems.
    In these cases â€“ as is the case for most real-world applications â€“ being able
    to quantify when models â€know they donâ€™t knowâ€ is crucial to developing reliable
    and robust systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæ·±åº¦å­¦ä¹ æ­£åœ¨å¯¹å„ç§åº”ç”¨é¢†åŸŸäº§ç”Ÿå½±å“ä¸€æ ·ï¼ŒBDL æ­£åœ¨æˆä¸ºè¶Šæ¥è¶Šé‡è¦çš„å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®æˆ–ä»»åŠ¡å…³é”®ç³»ç»Ÿä¸­ä½¿ç”¨å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­â€”â€”æ­£å¦‚å¤§å¤šæ•°å®é™…åº”ç”¨æ‰€é¢ä¸´çš„æƒ…å†µâ€”â€”èƒ½å¤Ÿé‡åŒ–æ¨¡å‹ä½•æ—¶â€œçŸ¥é“è‡ªå·±ä¸çŸ¥é“â€å¯¹äºå¼€å‘å¯é ä¸”ç¨³å¥çš„ç³»ç»Ÿè‡³å…³é‡è¦ã€‚
- en: One significant application area for BDL is in safety-critical systems. In their
    2019 paper titled *Safe Reinforcement Learning with Model Uncertainty* *Estimates*,
    BjÃ¶rn LÃ¼tjens *et al.* demonstrate that the use of BDL methods can produce safer
    behavior in collision-avoidance scenarios (the inspiration for our reinforcement
    learning example in [*ChapterÂ 8*](CH8.xhtml#x1-1320008), [*Applying Bayesian Deep*
    *Learning*](CH8.xhtml#x1-1320008)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: BDL çš„ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸæ˜¯å®‰å…¨å…³é”®ç³»ç»Ÿã€‚åœ¨ä»–ä»¬ 2019 å¹´å‘è¡¨çš„è®ºæ–‡ã€Š*å…·æœ‰æ¨¡å‹ä¸ç¡®å®šæ€§ä¼°è®¡çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ *ã€‹ä¸­ï¼ŒBjÃ¶rn LÃ¼tjens *ç­‰äºº*å±•ç¤ºäº†ä½¿ç”¨
    BDL æ–¹æ³•å¯ä»¥åœ¨é¿æ’åœºæ™¯ä¸­äº§ç”Ÿæ›´å®‰å…¨çš„è¡Œä¸ºï¼ˆè¿™æ˜¯æˆ‘ä»¬åœ¨[*ç¬¬ 8 ç« *](CH8.xhtml#x1-1320008)ä¸­å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹çš„çµæ„Ÿæ¥æºï¼Œ[*åº”ç”¨è´å¶æ–¯æ·±åº¦å­¦ä¹ *](CH8.xhtml#x1-1320008)ï¼‰ã€‚
- en: Similarly, in the paper *Uncertainty-Aware Deep Learning for Safe Landing* *Site
    Selection*, authors Katharine Skinner *et al.* explore how Bayesian neural networks
    can be used for autonomous hazard detection for landing sites on planetary surfaces.
    This technology is crucial for facilitating autonomous landing, and recently DNNs
    have demonstrated significant aptitude for this application. In their paper, Skinner
    *et al.* demonstrate that the use of uncertainty-aware models can improve the
    selection of safe landing sites, and even make it possible to select safe landing
    sites from sensor data with large amounts of noise. This is testament to BDLâ€™s
    capacity to improve both the *safety* and *robustness* of deep learning methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œåœ¨è®ºæ–‡ã€Š*ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ·±åº¦å­¦ä¹ ç”¨äºå®‰å…¨ç€é™†ç‚¹é€‰æ‹©*ã€‹ä¸­ï¼Œä½œè€…Katharine Skinner *ç­‰äºº*æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è´å¶æ–¯ç¥ç»ç½‘ç»œè¿›è¡Œè¡Œæ˜Ÿè¡¨é¢ç€é™†ç‚¹çš„è‡ªä¸»å±é™©æ£€æµ‹ã€‚è¿™é¡¹æŠ€æœ¯å¯¹äºä¿ƒè¿›è‡ªä¸»ç€é™†è‡³å…³é‡è¦ï¼Œæœ€è¿‘æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨è¿™ä¸€åº”ç”¨ä¸­å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼ŒSkinner
    *ç­‰äºº*å±•ç¤ºäº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å‹çš„ä½¿ç”¨èƒ½å¤Ÿæ”¹å–„å®‰å…¨ç€é™†ç‚¹çš„é€‰æ‹©ï¼Œç”šè‡³ä½¿å¾—èƒ½å¤Ÿä»å¤§é‡å™ªå£°çš„ä¼ æ„Ÿå™¨æ•°æ®ä¸­é€‰æ‹©å®‰å…¨çš„ç€é™†ç‚¹ã€‚è¿™è¯æ˜äº†è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰åœ¨æé«˜æ·±åº¦å­¦ä¹ æ–¹æ³•çš„*å®‰å…¨æ€§*å’Œ*é²æ£’æ€§*æ–¹é¢çš„èƒ½åŠ›ã€‚
- en: 'Given their rising popularity in safety-critical scenarios, it should be no
    surprise that Bayesian neural networks have also been adopted within medical applications.
    As we touched on in [*ChapterÂ 1*](CH1.xhtml#x1-150001), [*Bayesian Inference in
    the Age of* *Deep Learning*](CH1.xhtml#x1-150001), deep learning has exhibited
    particularly strong performance in the field of medical imaging. However, in these
    sorts of critical applications, uncertainty quantification is crucial: technicians
    and diagnosticians need to be able to understand the margin of error associated
    with model predictions. In the paper *Towards Safe Deep Learning: Accurately Quantifying
    Biomarker* *Uncertainty in Neural Network predictions*, Zach Eaton-Rosen *et al.*
    applied BDL methods for quantifying biomarker uncertainty when using deep networks
    for tumor volume estimation. Their work demonstrates that Bayesian neural networks
    can be used to design deep learning systems with well-calibrated error bars. These
    high-quality uncertainty estimates are necessary for the safe clinical use of
    models based on deep networks, making BDL methods crucial when it comes to incorporating
    these models in diagnostic applications.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºè´å¶æ–¯ç¥ç»ç½‘ç»œåœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„æ—¥ç›Šæµè¡Œï¼Œå®ƒä»¬ä¹Ÿè¢«åº”ç”¨äºåŒ»ç–—é¢†åŸŸå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬1ç« *](CH1.xhtml#x1-150001)ä¸­æåˆ°çš„ï¼Œ[*æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è´å¶æ–¯æ¨ç†*](CH1.xhtml#x1-150001)ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒé¢†åŸŸè¡¨ç°å‡ºäº†ç‰¹åˆ«å¼ºçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›å…³é”®åº”ç”¨ä¸­ï¼Œä¸ç¡®å®šæ€§é‡åŒ–è‡³å…³é‡è¦ï¼šæŠ€æœ¯äººå‘˜å’Œè¯Šæ–­äººå‘˜éœ€è¦èƒ½å¤Ÿç†è§£æ¨¡å‹é¢„æµ‹çš„è¯¯å·®èŒƒå›´ã€‚åœ¨è®ºæ–‡ã€Š*è¿ˆå‘å®‰å…¨æ·±åº¦å­¦ä¹ ï¼šç²¾ç¡®é‡åŒ–ç¥ç»ç½‘ç»œé¢„æµ‹ä¸­çš„ç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç¡®å®šæ€§*ã€‹ä¸­ï¼ŒZach
    Eaton-Rosen *ç­‰äºº*åº”ç”¨äº†è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰æ–¹æ³•æ¥é‡åŒ–ä½¿ç”¨æ·±åº¦ç½‘ç»œè¿›è¡Œè‚¿ç˜¤ä½“ç§¯ä¼°è®¡æ—¶çš„ç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç¡®å®šæ€§ã€‚ä»–ä»¬çš„å·¥ä½œå±•ç¤ºäº†è´å¶æ–¯ç¥ç»ç½‘ç»œå¯ä»¥ç”¨æ¥è®¾è®¡å…·æœ‰è‰¯å¥½æ ‡å®šè¯¯å·®æ¡çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚è¿™äº›é«˜è´¨é‡çš„ä¸ç¡®å®šæ€§ä¼°è®¡å¯¹äºæ·±åº¦ç½‘ç»œæ¨¡å‹åœ¨ä¸´åºŠä¸­çš„å®‰å…¨ä½¿ç”¨è‡³å…³é‡è¦ï¼Œå› æ­¤è´å¶æ–¯æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¯Šæ–­åº”ç”¨ä¸­æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚
- en: 'As technology advances, so does our ability to collect and organize data. This
    trend is turning a lot of â€small dataâ€ problems into â€big dataâ€ problems â€“ which
    is no bad thing, as more data means weâ€™re able to learn much more about the underlying
    process generating the data. One such example is that of seismic monitoring: over
    recent years, there has been a significant increase in dense seismic monitoring
    networks. This is excellent from a monitoring standpoint: scientists now have
    more data than ever before, and are thus able to better understand and monitor
    geophysical processes. However, in order to do so, they also need to be able to
    learn from large amounts of high dimensional data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œæˆ‘ä»¬æ”¶é›†å’Œç»„ç»‡æ•°æ®çš„èƒ½åŠ›ä¹Ÿåœ¨ä¸æ–­æå‡ã€‚è¿™ä¸€è¶‹åŠ¿å°†è®¸å¤šâ€œå°‘æ•°æ®â€é—®é¢˜è½¬å˜ä¸ºâ€œæµ·é‡æ•°æ®â€é—®é¢˜â€”â€”è¿™å¹¶éåäº‹ï¼Œå› ä¸ºæ›´å¤šçš„æ•°æ®æ„å‘³ç€æˆ‘ä»¬èƒ½å¤Ÿæ›´æ·±å…¥åœ°äº†è§£ç”Ÿæˆæ•°æ®çš„åº•å±‚è¿‡ç¨‹ã€‚ä¸€ä¸ªä¾‹å­å°±æ˜¯åœ°éœ‡ç›‘æµ‹ï¼šè¿‘å¹´æ¥ï¼Œå¯†é›†çš„åœ°éœ‡ç›‘æµ‹ç½‘ç»œæ˜¾è‘—å¢åŠ ã€‚è¿™ä»ç›‘æµ‹çš„è§’åº¦æ¥çœ‹æ˜¯æå¥½çš„ï¼šç§‘å­¦å®¶ç°åœ¨æ¯”ä»¥å¾€æ‹¥æœ‰æ›´å¤šçš„æ•°æ®ï¼Œå› æ­¤èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç›‘æµ‹åœ°çƒç‰©ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä»–ä»¬è¿˜éœ€è¦èƒ½å¤Ÿä»å¤§é‡é«˜ç»´æ•°æ®ä¸­è¿›è¡Œå­¦ä¹ ã€‚
- en: In their paper, *Bayesian Deep Learning and Uncertainty Quantification Applied*
    *to Induced Seismicity Locations in the Groningen Gas Field in the Netherlands:*
    *What Do We Need for Safe AI?*, authors Chen Gu *et al.* tackle the problem of
    seismic monitoring of the Groningen gas reservoir. As they mention in the paper,
    while deep learning has been applied to many geophysical problems, the use of
    uncertainty-aware deep networks is rare. Their work demonstrates that Bayesian
    neural networks can successfully be applied to geophysical problems and, in the
    case of the Groningen gas reservoir, could be crucial from both a safety-critical
    *and* mission-critical standpoint. From the safety perspective, these methods
    can be used to leverage the vast amounts of data to develop models that can infer
    ground motion activity and be used for seismic early warning systems. From the
    mission-critical perspective, the same data can be ingested by these methods to
    produce models capable of reservoir production estimates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»–ä»¬çš„è®ºæ–‡ã€Š*è´å¶æ–¯æ·±åº¦å­¦ä¹ ä¸ä¸ç¡®å®šæ€§é‡åŒ–åº”ç”¨äºè·å…°æ ¼ç½—å®æ ¹å¤©ç„¶æ°”ç”°çš„è¯±å‘åœ°éœ‡ä½ç½®ï¼š*æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ‰èƒ½ç¡®ä¿AIå®‰å…¨ï¼Ÿã€‹ä¸­ï¼Œä½œè€…é™ˆè°·*ç­‰äºº*æ¢è®¨äº†æ ¼ç½—å®æ ¹æ°”è—çš„åœ°éœ‡ç›‘æµ‹é—®é¢˜ã€‚æ­£å¦‚ä»–ä»¬åœ¨è®ºæ–‡ä¸­æåˆ°çš„ï¼Œè™½ç„¶æ·±åº¦å­¦ä¹ å·²è¢«åº”ç”¨äºè®¸å¤šåœ°çƒç‰©ç†é—®é¢˜ï¼Œä½†ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ·±åº¦ç½‘ç»œä»ç„¶ç½•è§ã€‚ä»–ä»¬çš„å·¥ä½œå±•ç¤ºäº†è´å¶æ–¯ç¥ç»ç½‘ç»œå¯ä»¥æˆåŠŸåœ°åº”ç”¨äºåœ°çƒç‰©ç†é—®é¢˜ï¼Œå¹¶ä¸”åœ¨æ ¼ç½—å®æ ¹æ°”è—çš„æ¡ˆä¾‹ä¸­ï¼Œä»å®‰å…¨å…³é”®å’Œä»»åŠ¡å…³é”®ä¸¤ä¸ªè§’åº¦æ¥çœ‹ï¼Œéƒ½å¯èƒ½è‡³å…³é‡è¦ã€‚ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆ©ç”¨å¤§é‡æ•°æ®å¼€å‘æ¨¡å‹ï¼Œæ¨æµ‹åœ°é¢è¿åŠ¨æ´»åŠ¨ï¼Œå¹¶ç”¨äºåœ°éœ‡é¢„è­¦ç³»ç»Ÿã€‚ä»ä»»åŠ¡å…³é”®çš„è§’åº¦æ¥çœ‹ï¼Œç›¸åŒçš„æ•°æ®å¯ä»¥é€šè¿‡è¿™äº›æ–¹æ³•è¾“å…¥ï¼Œä»¥ç”Ÿæˆèƒ½å¤Ÿè¿›è¡Œå‚¨å±‚ç”Ÿäº§ä¼°ç®—çš„æ¨¡å‹ã€‚
- en: In both cases, uncertainty quantification is key if these methods are going
    to be incorporated into any real-world systems, as the consequences for trusting
    incorrect predictions could be costly or even catastrophic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œè‹¥è¦å°†è¿™äº›æ–¹æ³•åº”ç”¨äºä»»ä½•å®é™…ç³»ç»Ÿï¼Œ**ä¸ç¡®å®šæ€§é‡åŒ–**æ˜¯å…³é”®ï¼Œå› ä¸ºä¿¡ä»»é”™è¯¯é¢„æµ‹çš„åæœå¯èƒ½æ˜¯ä»£ä»·é«˜æ˜‚ç”šè‡³ç¾éš¾æ€§çš„ã€‚
- en: These examples have given us some insight into how BDL is being applied in the
    real world. As with other machine learning solutions before them, we learn more
    about potential shortcomings as the methods are used in more and more diverse
    sets of applications. In the next section, weâ€™ll learn about some of the latest
    developments in the field, building on the core approaches covered in the book
    to develop increasingly robust BNN approximations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¾‹å­è®©æˆ‘ä»¬å¯¹BDLåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æœ‰äº†ä¸€äº›äº†è§£ã€‚ä¸ä¹‹å‰çš„å…¶ä»–æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆä¸€æ ·ï¼Œéšç€è¿™äº›æ–¹æ³•åœ¨è¶Šæ¥è¶Šå¤šæ ·åŒ–çš„åº”ç”¨åœºæ™¯ä¸­è¢«ä½¿ç”¨ï¼Œæˆ‘ä»¬ä¹Ÿè¶Šæ¥è¶Šäº†è§£å…¶æ½œåœ¨çš„ä¸è¶³ä¹‹å¤„ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ä¸€äº›è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŸºäºä¹¦ä¸­ä»‹ç»çš„æ ¸å¿ƒæ–¹æ³•ï¼Œå¼€å‘å‡ºè¶Šæ¥è¶Šå¼ºå¤§çš„BNNè¿‘ä¼¼ã€‚
- en: 9.3 Latest methods in BDL
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 BDLä¸­çš„æœ€æ–°æ–¹æ³•
- en: 'In this book, weâ€™ve introduced some of the core techniques used within BDL:
    Bayes by Backprop (BBB), Probabilistic Backpropagation (PBP), Monte-Carlo dropout
    (MC dropout), and deep ensembles. Many BNN approaches youâ€™ll encounter in the
    literature will be based on one of these methods, and having these under your
    belt provides you with a versatile toolbox of approaches for developing your own
    BDL solutions. However, as with all aspects of machine learning, the field of
    BDL is progressing rapidly, and new techniques are being developed on a regular
    basis. In this section, weâ€™ll explore a selection of recent developments from
    the field.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›BDLä¸­ä½¿ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ï¼šåå‘ä¼ æ’­è´å¶æ–¯ï¼ˆBBBï¼‰ã€æ¦‚ç‡åå‘ä¼ æ’­ï¼ˆPBPï¼‰ã€è’™ç‰¹å¡æ´›dropoutï¼ˆMC dropoutï¼‰å’Œæ·±åº¦é›†æˆæ–¹æ³•ã€‚ä½ åœ¨æ–‡çŒ®ä¸­é‡åˆ°çš„è®¸å¤šBNNæ–¹æ³•éƒ½ä¼šåŸºäºè¿™äº›æŠ€æœ¯ï¼Œè€ŒæŒæ¡è¿™äº›æŠ€æœ¯ä¸ºä½ æä¾›äº†ä¸€å¥—å¤šåŠŸèƒ½çš„å·¥å…·ç®±ï¼Œå¯ä»¥å¸®åŠ©ä½ å¼€å‘è‡ªå·±çš„BDLè§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ­£å¦‚æœºå™¨å­¦ä¹ çš„å…¶ä»–æ–¹é¢ä¸€æ ·ï¼ŒBDLé¢†åŸŸæ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œæ–°çš„æŠ€æœ¯ä¹Ÿåœ¨ä¸æ–­æ¶Œç°ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è¯¥é¢†åŸŸçš„ä¸€äº›æœ€æ–°è¿›å±•ã€‚
- en: 9.3.1 Combining MC dropout and deep ensembles
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 ç»“åˆMC dropoutå’Œæ·±åº¦é›†æˆæ–¹æ³•
- en: 'Why use just one Bayesian neural network technique when you could use two?
    This is exactly the approach taken by University of Edinburgh researchers Remus
    Pop and Patric Fulop in their paper, *Deep Ensemble Bayesian* *Active Learning:
    Addressing the Mode Collapse Issue in Monte Carlo* *Dropout via Ensembles*. In
    this work, Pop and Fulop describe the problem of using **active learning** to
    make deep learning methods feasible in applications for which labeling data is
    time consuming or costly. The issue here is that, as weâ€™ve discussed previously,
    deep learning methods have proven to be incredibly successful across a range of
    medical imaging tasks. The issue is that this data needs to be carefully labeled,
    and for deep networks to achieve high levels of performance, they need *a lot*
    of this data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåªä½¿ç”¨ä¸€ç§è´å¶æ–¯ç¥ç»ç½‘ç»œæŠ€æœ¯ï¼Œè€Œä¸ä½¿ç”¨ä¸¤ç§å‘¢ï¼Ÿçˆ±ä¸å ¡å¤§å­¦çš„ç ”ç©¶äººå‘˜Remus Popå’ŒPatric Fulopåœ¨ä»–ä»¬çš„è®ºæ–‡ã€Šæ·±åº¦é›†æˆè´å¶æ–¯ä¸»åŠ¨å­¦ä¹ ï¼šé€šè¿‡é›†æˆè§£å†³è’™ç‰¹å¡ç½—Dropoutä¸­çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‹ä¸­æ­£æ˜¯é‡‡ç”¨äº†è¿™ç§æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼ŒPopå’ŒFulopæè¿°äº†ä½¿ç”¨**ä¸»åŠ¨å­¦ä¹ **ä½¿æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ ‡ç­¾æ•°æ®è€—æ—¶æˆ–æ˜‚è´µçš„åº”ç”¨ä¸­å˜å¾—å¯è¡Œçš„é—®é¢˜ã€‚è¿™é‡Œçš„é—®é¢˜æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•å·²ç»åœ¨ä¸€ç³»åˆ—åŒ»å­¦å½±åƒä»»åŠ¡ä¸­è¯æ˜äº†å…¶å·¨å¤§çš„æˆåŠŸã€‚é—®é¢˜åœ¨äºï¼Œè¿™äº›æ•°æ®éœ€è¦ç»è¿‡ä»”ç»†æ ‡æ³¨ï¼Œå¹¶ä¸”ä¸ºäº†ä½¿æ·±åº¦ç½‘ç»œè¾¾åˆ°é«˜æ°´å¹³çš„æ€§èƒ½ï¼Œå®ƒä»¬éœ€è¦*å¤§é‡*è¿™æ ·çš„æ•°æ®ã€‚
- en: 'As such, active learning has been proposed by machine learning researchers
    to automatically evaluate new data points and add them to the dataset, using **acquisition
    functions** to determine when new data should be added to the training set. Model
    uncertainty estimates are a key piece of puzzle: they provide a key measure of
    how new data points relate to the modelâ€™s existing understanding of the domain.
    In their paper, Pop and Fulop demonstrate that a popular method for **Deep Bayesian
    Active** **Learning (DBAL)** has a key shortcoming: that of over-confidence stemming
    from the MC dropout models used in DBAL. In their paper, the authors address this
    through combining both deep ensembles and MC dropout in a single model. They demonstrate
    that the resulting model has better calibrated uncertainty estimates, thus correcting
    for the over-confident predictions exhibited by MC dropout. The resulting method,
    dubbed **deep** **ensemble Bayesian active learning**, provides a framework for
    robustly employing deep learning methods in applications for which data acquisition
    is difficult or expensive â€“ demonstrating again how BDL is proving to be an important
    building block in deploying deep networks in the real world.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæœºå™¨å­¦ä¹ ç ”ç©¶è€…æå‡ºäº†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨**è·å–å‡½æ•°**æ¥è‡ªåŠ¨è¯„ä¼°æ–°æ•°æ®ç‚¹å¹¶å°†å…¶æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥ç¡®å®šä½•æ—¶å°†æ–°æ•°æ®æ·»åŠ åˆ°è®­ç»ƒé›†ä¸­ã€‚æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯å…³é”®çš„ä¸€ç¯ï¼šå®ƒæä¾›äº†æ–°æ•°æ®ç‚¹ä¸æ¨¡å‹å½“å‰ç†è§£é¢†åŸŸçš„å…³ç³»çš„å…³é”®è¡¡é‡æ ‡å‡†ã€‚åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼ŒPopå’ŒFulopå±•ç¤ºäº†ä¸€ä¸ªæµè¡Œçš„**æ·±åº¦è´å¶æ–¯ä¸»åŠ¨å­¦ä¹ ï¼ˆDBALï¼‰**æ–¹æ³•çš„å…³é”®ç¼ºé™·ï¼šå³MC
    dropoutæ¨¡å‹ä¸­ä½¿ç”¨çš„è¿‡åº¦è‡ªä¿¡ã€‚åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼Œä½œè€…é€šè¿‡å°†æ·±åº¦é›†æˆå’ŒMC dropoutç»“åˆåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä»–ä»¬è¯æ˜ï¼Œå¾—åˆ°çš„æ¨¡å‹å…·æœ‰æ›´å¥½çš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä»è€Œçº æ­£äº†MC
    dropoutæ‰€è¡¨ç°å‡ºçš„è¿‡åº¦è‡ªä¿¡é¢„æµ‹ã€‚æœ€ç»ˆæå‡ºçš„æ–¹æ³•ï¼Œè¢«ç§°ä¸º**æ·±åº¦** **é›†æˆè´å¶æ–¯ä¸»åŠ¨å­¦ä¹ **ï¼Œä¸ºåœ¨æ•°æ®è·å–å›°éš¾æˆ–æ˜‚è´µçš„åº”ç”¨ä¸­ç¨³å¥åœ°é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ¡†æ¶â€”â€”å†æ¬¡è¯æ˜BDLåœ¨å°†æ·±åº¦ç½‘ç»œåº”ç”¨äºç°å®ä¸–ç•Œä¸­çš„é‡è¦æ€§ã€‚
- en: '![PIC](img/file185.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file185.jpg)'
- en: 'FigureÂ 9.4: Illustration of a combined MC dropout and deep ensemble network'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.4ï¼šç»“åˆMC dropoutå’Œæ·±åº¦é›†æˆç½‘ç»œçš„ç¤ºæ„å›¾
- en: This approach of combining deep ensembles and MC dropout has also been applied
    in other applications. For example, the collision avoidance paper mentioned previously
    by LÃ¼tjens *et al.* also uses a combined MC dropout and deep ensemble network.
    This goes to show that itâ€™s not always simply a case of choosing one network over
    another â€“ sometimes combining approaches is key to developing robust, better calibrated
    BDL solutions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ·±åº¦é›†æˆå’ŒMC dropoutç»“åˆçš„è¿™ç§æ–¹æ³•ä¹Ÿå·²åº”ç”¨äºå…¶ä»–é¢†åŸŸã€‚ä¾‹å¦‚ï¼ŒLÃ¼tjensç­‰äººä¹‹å‰æåˆ°çš„ç¢°æ’é¿å…è®ºæ–‡ä¹Ÿä½¿ç”¨äº†ç»“åˆMC dropoutå’Œæ·±åº¦é›†æˆç½‘ç»œçš„æ–¹æ³•ã€‚è¿™è¡¨æ˜ï¼Œé€‰æ‹©ä¸€ç§ç½‘ç»œè€Œéå¦ä¸€ç§ç½‘ç»œå¹¶ä¸æ€»æ˜¯æœ€ä¼˜è§£â€”â€”æœ‰æ—¶ï¼Œç»“åˆä¸åŒæ–¹æ³•æ˜¯å¼€å‘ç¨³å¥ä¸”æ›´å¥½æ ¡å‡†çš„BDLè§£å†³æ–¹æ¡ˆçš„å…³é”®ã€‚
- en: 9.3.2 Improving deep ensembles by promoting diversity
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 é€šè¿‡ä¿ƒè¿›å¤šæ ·æ€§æ¥æ”¹è¿›æ·±åº¦é›†æˆ
- en: As we saw earlier in the chapter, judging by the number of citations, deep ensembles
    are the second most popular of the key BDL techniques covered in this book. As
    such, itâ€™s no surprise that researchers have been investigating methods to improve
    on the standard implementation of deep ensembles.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬ç« å‰é¢çœ‹åˆ°çš„ï¼ŒæŒ‰ç…§å¼•ç”¨æ¬¡æ•°æ¥åˆ¤æ–­ï¼Œæ·±åº¦é›†æˆæ˜¯æœ¬ä¹¦ä¸­ä»‹ç»çš„å…³é”®BDLæŠ€æœ¯ä¸­ç¬¬äºŒå—æ¬¢è¿çš„æŠ€æœ¯ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜ä¸€ç›´åœ¨æ¢ç´¢æ”¹è¿›æ·±åº¦é›†æˆæ ‡å‡†å®ç°çš„æ–¹æ³•ï¼Œè¿™ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†ã€‚
- en: 'In Tim Pearce *et al.*â€™s paper, *Uncertainty in Neural Networks: Approximately*
    *Bayesian Ensembling*, the authors highlight that the standard deep ensemble approach
    has been criticized for not being Bayesian and argue that the standard approach
    likely lacks diversity in many cases, thus producing a poorly descriptive posterior.
    In other words, deep ensembles often result in over-confident predictions due
    to a lack of diversity in the ensemble.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Tim Pearce *ç­‰*äººçš„è®ºæ–‡ã€Šç¥ç»ç½‘ç»œä¸­çš„ä¸ç¡®å®šæ€§ï¼šè¿‘ä¼¼è´å¶æ–¯é›†æˆã€‹ä¸­ï¼Œä½œè€…å¼ºè°ƒï¼Œæ ‡å‡†çš„æ·±åº¦é›†æˆæ–¹æ³•å› å…¶éè´å¶æ–¯æ€§è´¨è€Œå—åˆ°æ‰¹è¯„ï¼Œå¹¶è®¤ä¸ºæ ‡å‡†æ–¹æ³•åœ¨è®¸å¤šæƒ…å†µä¸‹ç¼ºä¹å¤šæ ·æ€§ï¼Œä»è€Œäº§ç”Ÿäº†æè¿°æ€§è¾ƒå·®çš„åéªŒåˆ†å¸ƒã€‚æ¢å¥è¯è¯´ï¼Œæ·±åº¦é›†æˆé€šå¸¸ç”±äºç¼ºä¹å¤šæ ·æ€§ï¼Œå¯¼è‡´è¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚
- en: 'To remedy this, the authors propose a method they term **anchored** **ensembling**.
    Anchored ensembling, like deep ensembles, uses an ensemble of NNs. However, it
    uses a specially adapted loss function that penalizes the ensemble membersâ€™ parameters
    from drifting too far from their initial values. Letâ€™s take a look:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç‚¹ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä»–ä»¬ç§°ä¹‹ä¸º**é”šå®š** **é›†æˆ**çš„æ–¹æ³•ã€‚é”šå®šé›†æˆåƒæ·±åº¦é›†æˆä¸€æ ·ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œçš„é›†æˆã€‚ç„¶è€Œï¼Œå®ƒä½¿ç”¨äº†ä¸€ç§ç‰¹åˆ«é€‚é…çš„æŸå¤±å‡½æ•°ï¼Œæƒ©ç½šé›†æˆæˆå‘˜çš„å‚æ•°è¿‡åº¦åç¦»å…¶åˆå§‹å€¼ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹ï¼š
- en: '![Lossj = 1-||y âˆ’ Ë†y||2+ -1||Î“ 12 Ã— (ğœƒj âˆ’ ğœƒanc,j)||2 N 2 N 2 ](img/file186.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Lossj = 1-||y âˆ’ Ë†y||2+ -1||Î“ 12 Ã— (ğœƒj âˆ’ ğœƒanc,j)||2 N 2 N 2 ](img/file186.jpg)'
- en: Here, the *Loss*[*j*] is the loss computed for the *j*th network in the ensemble.
    We see a familiar loss in the equation in the form of ||**y** âˆ’**y**||[2]Â². Î“
    is a diagonal regularization matrix, and *ğœƒ*[*j*] are the parameters for the network.
    The key point here is the relationship between *ğœƒ*[*j*] and the *ğœƒ*[*anc,j*] variable.
    Here, the *anc* indicates the anchoring from which the method gets its name. These
    parameters, *ğœƒ*[*anc,j*], are the set of initial parameters for the *j*th network.
    As such (as we see by the multiplication), if this value is large â€“ in other words,
    if *ğœƒ*[*j*] and *ğœƒ*[*anc,j*] are significantly different â€“ the loss will increase.
    Thus, this penalizes the networks in the ensemble if they deviate too far from
    their initial values, forcing them to find parameter values that minimize the
    first term in the equation while staying as close to their initial values as possible.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*Loss*[*j*] æ˜¯è®¡ç®—å‡ºçš„ç¬¬ *j* ä¸ªç½‘ç»œçš„æŸå¤±ã€‚æˆ‘ä»¬åœ¨æ–¹ç¨‹ä¸­çœ‹åˆ°äº†ä¸€ä¸ªç†Ÿæ‚‰çš„æŸå¤±å½¢å¼ï¼Œå³ ||**y** âˆ’ **y**||[2]Â²ã€‚Î“
    æ˜¯ä¸€ä¸ªå¯¹è§’æ­£åˆ™åŒ–çŸ©é˜µï¼Œ*ğœƒ*[*j*] æ˜¯ç½‘ç»œçš„å‚æ•°ã€‚è¿™é‡Œçš„å…³é”®æ˜¯ *ğœƒ*[*j*] ä¸ *ğœƒ*[*anc,j*] å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚è¿™é‡Œï¼Œ*anc* è¡¨ç¤ºè¯¥æ–¹æ³•åç§°ä¸­çš„é”šå®šã€‚å‚æ•°
    *ğœƒ*[*anc,j*] æ˜¯ç¬¬ *j* ä¸ªç½‘ç»œçš„åˆå§‹å‚æ•°é›†ã€‚å› æ­¤ï¼ˆå¦‚é€šè¿‡ä¹˜æ³•æ‰€ç¤ºï¼‰ï¼Œå¦‚æœè¯¥å€¼å¾ˆå¤§â€”â€”æ¢å¥è¯è¯´ï¼Œå¦‚æœ *ğœƒ*[*j*] å’Œ *ğœƒ*[*anc,j*]
    ç›¸å·®å¾ˆå¤§â€”â€”æŸå¤±å°†å¢åŠ ã€‚å› æ­¤ï¼Œå¦‚æœç½‘ç»œçš„å‚æ•°åç¦»å…¶åˆå§‹å€¼å¤ªè¿œï¼Œè¿™ä¼šæƒ©ç½šé›†æˆä¸­çš„ç½‘ç»œï¼Œè¿«ä½¿å®ƒä»¬æ‰¾åˆ°èƒ½å¤Ÿå°½é‡ä¿æŒæ¥è¿‘åˆå§‹å€¼çš„å‚æ•°ï¼ŒåŒæ—¶æœ€å°åŒ–æ–¹ç¨‹ä¸­çš„ç¬¬ä¸€é¡¹ã€‚
- en: 'This is important because, if we use an initialization strategy that is more
    likely to produce a diverse set of initial parameter values, then maintaining
    that diversity will ensure that our ensemble comprises diverse networks after
    training. As the authors demonstrate in the paper, this diversity is key to producing
    principled uncertainty estimates: ensuring that the network predictions converge
    for regions of high data, and diverge in regions of low data, just as we saw in
    our GP examples from [*ChapterÂ 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian*
    *Inference*](CH2.xhtml#x1-250002):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ›´å¯èƒ½äº§ç”Ÿå¤šæ ·åŒ–åˆå§‹å‚æ•°å€¼çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œé‚£ä¹ˆä¿æŒè¿™ç§å¤šæ ·æ€§å°†ç¡®ä¿æˆ‘ä»¬çš„é›†æˆåœ¨è®­ç»ƒååŒ…å«å¤šæ ·åŒ–çš„ç½‘ç»œã€‚æ­£å¦‚ä½œè€…åœ¨è®ºæ–‡ä¸­å±•ç¤ºçš„é‚£æ ·ï¼Œè¿™ç§å¤šæ ·æ€§æ˜¯äº§ç”ŸåŸåˆ™æ€§ä¸ç¡®å®šæ€§ä¼°è®¡çš„å…³é”®ï¼šç¡®ä¿ç½‘ç»œé¢„æµ‹åœ¨é«˜æ•°æ®åŒºåŸŸæ”¶æ•›ï¼Œè€Œåœ¨ä½æ•°æ®åŒºåŸŸå‘æ•£ï¼Œå°±åƒæˆ‘ä»¬åœ¨[*ç¬¬2ç« *](CH2.xhtml#x1-250002)å’Œ[*è´å¶æ–¯æ¨æ–­åŸºç¡€*](CH2.xhtml#x1-250002)çš„é«˜æ–¯è¿‡ç¨‹ç¤ºä¾‹ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: '![PIC](img/file52.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file52.png)'
- en: 'FigureÂ 9.5: Illustration of principled uncertainty estimates obtained using
    a Gaussian process'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.5ï¼šä½¿ç”¨é«˜æ–¯è¿‡ç¨‹è·å¾—çš„åŸåˆ™æ€§ä¸ç¡®å®šæ€§ä¼°è®¡ç¤ºæ„å›¾
- en: As a reminder, here the solid line is the true function, the dots are the samples
    from the function, the dotted line are the mean GP predictions, the faint dotted
    lines are a sample of possible functions, and the shaded area is the uncertainty.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼Œè¿™é‡Œçš„å®çº¿æ˜¯å®é™…å‡½æ•°ï¼Œç‚¹æ˜¯å‡½æ•°çš„æ ·æœ¬ï¼Œè™šçº¿æ˜¯é«˜æ–¯è¿‡ç¨‹çš„å‡å€¼é¢„æµ‹ï¼Œæµ…ç°è‰²è™šçº¿æ˜¯å¯èƒ½å‡½æ•°çš„æ ·æœ¬ï¼Œé˜´å½±åŒºåŸŸè¡¨ç¤ºä¸ç¡®å®šæ€§ã€‚
- en: In their paper, Pearce *et al.* demonstrate that their anchored ensemble approach
    is able to approximate a descriptive posterior distribution such as this far more
    closely than the standard deep ensemble approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼ŒPearce *ç­‰*äººå±•ç¤ºäº†ä»–ä»¬çš„é”šå®šé›†æˆæ–¹æ³•èƒ½å¤Ÿæ¯”æ ‡å‡†æ·±åº¦é›†æˆæ–¹æ³•æ›´æ¥è¿‘åœ°é€¼è¿‘è¿™æ ·çš„æè¿°æ€§åéªŒåˆ†å¸ƒã€‚
- en: 9.3.3 Uncertainty in very large networks
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 è¶…å¤§ç½‘ç»œä¸­çš„ä¸ç¡®å®šæ€§
- en: 'While the core aim topic of the book has been to introduce methods for approximating
    Bayesian inference in DNNs, we havenâ€™t addressed how this is applied to one of
    the most successful NN architecture varieties of recent years: the transformer.
    Transformers â€“ just as more typical deep networks before them â€“ have achieved
    landmark performance in a variety of tasks. While deep networks were already crunching
    large amounts of data, transformers take this to the next level: crunching enormous
    volumes of data, and comprising hundreds of billions of parameters. One of the
    most well-known transformer networks is GPT-3, a transformer developed by OpenAI
    that comprises over 175 billion parameters.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»‹ç»åœ¨DNNä¸­è¿‘ä¼¼è´å¶æ–¯æ¨æ–­çš„æ–¹æ³•ï¼Œä½†æˆ‘ä»¬å°šæœªè®¨è®ºå¦‚ä½•å°†å…¶åº”ç”¨äºè¿‘å¹´æ¥æœ€æˆåŠŸçš„ç¥ç»ç½‘ç»œæ¶æ„ä¹‹ä¸€ï¼šå˜æ¢å™¨ã€‚å˜æ¢å™¨â€”å°±åƒä¹‹å‰çš„å…¸å‹æ·±åº¦ç½‘ç»œä¸€æ ·â€”åœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°äº†çªç ´æ€§æ€§èƒ½ã€‚å°½ç®¡æ·±åº¦ç½‘ç»œå·²ç»èƒ½å¤Ÿå¤„ç†å¤§é‡æ•°æ®ï¼Œå˜æ¢å™¨å°†å…¶æ¨å‘äº†ä¸€ä¸ªæ–°é«˜åº¦ï¼šå¤„ç†å·¨é‡æ•°æ®ï¼Œæ‹¥æœ‰æ•°ç™¾äº¿çš„å‚æ•°ã€‚å…¶ä¸­æœ€è‘—åçš„å˜æ¢å™¨ç½‘ç»œä¹‹ä¸€æ˜¯GPT-3ï¼Œè¿™æ˜¯ç”±OpenAIå¼€å‘çš„å˜æ¢å™¨ï¼ŒåŒ…å«è¶…è¿‡1750äº¿ä¸ªå‚æ•°ã€‚
- en: 'Transformers were first used in **Natural Language Processing** (**NLP**) tasks,
    and demonstrated that, through the use of self-attention and sufficient volumes
    of data, competitive performance can be achieved without the use of recurrent
    neural networks. This was an important step in NN architecture development: demonstrating
    that sequential context can be learned through self-attention and providing architectures
    capable of learning from hitherto unprecedented volumes of data.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨æœ€åˆç”¨äº**è‡ªç„¶è¯­è¨€å¤„ç†**ï¼ˆ**NLP**ï¼‰ä»»åŠ¡ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œè¶³å¤Ÿçš„æ•°æ®é‡ï¼Œå¯ä»¥åœ¨ä¸ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹å®ç°ç«äº‰æ€§æ€§èƒ½ã€‚è¿™æ˜¯ç¥ç»ç½‘ç»œæ¶æ„å‘å±•çš„ä¸€ä¸ªé‡è¦æ­¥éª¤ï¼šå±•ç¤ºäº†å¯ä»¥é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ åºåˆ—ä¸Šä¸‹æ–‡ï¼Œå¹¶æä¾›èƒ½å¤Ÿä»å‰æ‰€æœªæœ‰çš„æ•°æ®é‡ä¸­å­¦ä¹ çš„æ¶æ„ã€‚
- en: '![PIC](img/file187.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file187.png)'
- en: 'FigureÂ 9.6: Illustration of the transformer architecture'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.6ï¼šå˜æ¢å™¨æ¶æ„ç¤ºæ„å›¾
- en: However, just as with the trend of more typical deep networks before them, the
    parameters of transformers are point estimates, rather than distributions, thus
    preventing them from being used for uncertainty quantification. Authors Boyang
    Xue *et al.* sought to remedy this in their paper, *Bayesian Transformer* *Language
    Models for Speech Recognition*. In their work, they demonstrate the variational
    inference can be successfully applied to transformer models, facilitating approximate
    Bayesian inference. However, due to the large size of transformers, Bayesian parameter
    estimation for all parameters is incredibly expensive. As such, Xue *et al.* apply
    Bayesian estimation to a subset of model parameters, specifically the parameters
    in the feed-forward and multi-head self-attention modules. As we see from *Figure*
    [*9.6*](#x1-184003r6), this excludes quite a few layers from the variational sampling
    process, thus saving compute cycles.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå°±åƒä¹‹å‰æ›´å…¸å‹çš„æ·±åº¦ç½‘ç»œä¸€æ ·ï¼Œå˜æ¢å™¨çš„å‚æ•°æ˜¯ç‚¹ä¼°è®¡ï¼Œè€Œä¸æ˜¯åˆ†å¸ƒï¼Œå› æ­¤æ— æ³•ç”¨äºä¸ç¡®å®šæ€§é‡åŒ–ã€‚ä½œè€…å¾åšæ‰¬*ç­‰äºº*åœ¨ä»–ä»¬çš„è®ºæ–‡ã€Š*è´å¶æ–¯å˜æ¢å™¨è¯­è¨€æ¨¡å‹ç”¨äºè¯­éŸ³è¯†åˆ«*ã€‹ä¸­è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨ä»–ä»¬çš„å·¥ä½œä¸­ï¼Œä»–ä»¬å±•ç¤ºäº†å˜åˆ†æ¨æ–­å¯ä»¥æˆåŠŸåœ°åº”ç”¨äºå˜æ¢å™¨æ¨¡å‹ï¼Œä»è€Œä¿ƒè¿›äº†è¿‘ä¼¼è´å¶æ–¯æ¨æ–­ã€‚ç„¶è€Œï¼Œç”±äºå˜æ¢å™¨çš„åºå¤§è§„æ¨¡ï¼Œå¯¹æ‰€æœ‰å‚æ•°è¿›è¡Œè´å¶æ–¯å‚æ•°ä¼°è®¡æ˜¯éå¸¸æ˜‚è´µçš„ã€‚å› æ­¤ï¼Œå¾åšæ‰¬*ç­‰äºº*å°†è´å¶æ–¯ä¼°è®¡åº”ç”¨äºæ¨¡å‹å‚æ•°çš„ä¸€ä¸ªå­é›†ï¼Œç‰¹åˆ«æ˜¯å‰é¦ˆå’Œå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„å‚æ•°ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨*å›¾*
    [*9.6*](#x1-184003r6)ä¸­çœ‹åˆ°çš„ï¼Œè¿™æ’é™¤äº†ç›¸å½“å¤šçš„å±‚ï¼Œä»è€ŒèŠ‚çœäº†è®¡ç®—å‘¨æœŸã€‚
- en: Another method proposed in the paper *Transformers Can Do Bayesian* *Inference*,
    by Samuel MÃ¼ller *et al.*, approximates Bayesian inference by exploiting the large
    amount of data used to train transformers. In their approach, dubbed **Prior-Data
    Fitted Networks (PFNs)**, the authors restate the problem of posterior approximation
    as a supervised learning task. That is to say, rather than obtaining a distribution
    of predictions via sampling, their method learns to approximate the posterior
    predictive distribution directly from dataset samples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ã€Š*å˜æ¢å™¨å¯ä»¥è¿›è¡Œè´å¶æ–¯æ¨æ–­*ã€‹ä¸­ï¼Œç”±Samuel MÃ¼ller*ç­‰äºº*æå‡ºçš„å¦ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è®­ç»ƒå˜æ¢å™¨æ—¶ä½¿ç”¨çš„å¤§é‡æ•°æ®æ¥è¿‘ä¼¼è´å¶æ–¯æ¨æ–­ã€‚åœ¨ä»–ä»¬çš„æ–¹æ³•ä¸­ï¼Œç§°ä¸º**å…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œï¼ˆPFNsï¼‰**ï¼Œä½œè€…å°†åéªŒè¿‘ä¼¼é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä»–ä»¬çš„æ–¹æ³•ä¸æ˜¯é€šè¿‡é‡‡æ ·è·å¾—é¢„æµ‹åˆ†å¸ƒï¼Œè€Œæ˜¯ç›´æ¥ä»æ•°æ®é›†æ ·æœ¬ä¸­å­¦ä¹ è¿‘ä¼¼åéªŒé¢„æµ‹åˆ†å¸ƒã€‚
- en: 'Algorithm 1: PFN model training procedure Â  **Input:** A prior distribution
    over datasets *p*(*D*), from which samples can be drawn and the number of samples
    *K* to draw'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 1ï¼šPFNæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ Â  **è¾“å…¥ï¼š** æ•°æ®é›†çš„å…ˆéªŒåˆ†å¸ƒ *p*(*D*)ï¼Œä»ä¸­å¯ä»¥æŠ½å–æ ·æœ¬ï¼ŒæŠ½å–çš„æ ·æœ¬æ•°ä¸º *K*
- en: '**Output:** A model *qğœƒ* that will approximate the PPD Initialize the neural
    network *qğœƒ*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å‡ºï¼š** ä¸€ä¸ªæ¨¡å‹ *qğœƒ* ç”¨äºè¿‘ä¼¼ PPDï¼Œåˆå§‹åŒ–ç¥ç»ç½‘ç»œ *qğœƒ*'
- en: '**for**Â i:=1 to 10Â **do**- 1: Â Â  `Sample` *D* âˆª (*x*[*i*]*,y*[*i*])[*i*=1]^(*m*)
    â‰ˆ *p*(*D*)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**for**Â i:=1 åˆ° 10 **do**- 1: Â Â  `é‡‡æ ·` *D* âˆª (*x*[*i*]*,y*[*i*])[*i*=1]^(*m*)
    â‰ˆ *p*(*D*)'
- en: 2:Â Â `Compute stochastic loss approximation` *l*[*ğœƒ*] = âˆ‘ [*i*=1]^(*m*)(âˆ’log
    *q*[*ğœƒ*](*y*[*i*]|*x*[*i*]*,D*))
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '2:  `è®¡ç®—éšæœºæŸå¤±è¿‘ä¼¼` *l*[*ğœƒ*] = âˆ‘ [*i*=1]^(*m*)(âˆ’log *q*[*ğœƒ*](*y*[*i*]|*x*[*i*]*,D*))'
- en: 3:Â Â *Update parameters* *ğœƒ* *with stochastic gradient descent on* â–¿[*ğœƒ*]*l*[*ğœƒ*]
    =0
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '3:  *ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°* *ğœƒ* *ï¼ŒåŸºäº* â–¿[*ğœƒ*]*l*[*ğœƒ*] =0'
- en: As represented in the pseudo code here, during training, the model samples multiple
    subsets of data comprising inputs *x* and labels *y*. It then masks one of the
    labels and learns to make a probabilistic prediction for this label based on the
    other data points. This allows the PFN to do probabilistic inference in a single
    forward pass â€“ similarly to what we saw in [*ChapterÂ 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches* *for Bayesian Deep Learning*](CH5.xhtml#x1-600005) with
    PBP. While approximating Bayesian inference in a single forward pass is desirable
    for any application, this is even more valuable with transformers given their
    huge numbers of parameters â€“ thus making the PFN approach described here particularly
    attractive.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ­¤ä¼ªä»£ç æ‰€ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šé‡‡æ ·å¤šä¸ªæ•°æ®å­é›†ï¼Œè¿™äº›å­é›†åŒ…å«è¾“å…¥ *x* å’Œæ ‡ç­¾ *y*ã€‚æ¥ç€ï¼Œå®ƒä¼šå±è”½æ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œå¹¶å­¦ä¹ åŸºäºå…¶ä»–æ•°æ®ç‚¹å¯¹è¯¥æ ‡ç­¾åšå‡ºæ¦‚ç‡é¢„æµ‹ã€‚è¿™ä½¿å¾—
    PFN èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­è¿›è¡Œæ¦‚ç‡æ¨æ–­â€”â€”ç±»ä¼¼äºæˆ‘ä»¬åœ¨[*ç¬¬äº”ç« *](CH5.xhtml#x1-600005) å’Œ [*è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„åŸåˆ™æ–¹æ³•*](CH5.xhtml#x1-600005)ä¸­çœ‹åˆ°çš„
    PBPã€‚å°½ç®¡åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­é€¼è¿‘è´å¶æ–¯æ¨æ–­å¯¹äºä»»ä½•åº”ç”¨æ¥è¯´éƒ½æ˜¯ç†æƒ³çš„ï¼Œä½†å¯¹äºå…·æœ‰å¤§é‡å‚æ•°çš„ transformers æ¥è¯´ï¼Œè¿™ç§æ–¹æ³•æ›´å…·ä»·å€¼â€”â€”å› æ­¤ï¼Œè¿™é‡Œæè¿°çš„
    PFN æ–¹æ³•å°¤å…¶å…·æœ‰å¸å¼•åŠ›ã€‚
- en: 'Of course, transformers are popularly used within transfer learning contexts:
    using the rich feature embeddings from transformers as inputs to smaller, less
    computationally demanding networks. As such, perhaps the most obvious way to use
    transformers in a Bayesian context would be to use its embeddings as an input
    to a BDL network â€“ in fact, this is probably the most sensible first step in many
    cases.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œtransformers åœ¨è¿ç§»å­¦ä¹ ä¸­è¢«å¹¿æ³›åº”ç”¨ï¼šä½¿ç”¨ transformers æå–çš„ä¸°å¯Œç‰¹å¾åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œé€å…¥è®¡ç®—è¦æ±‚è¾ƒä½çš„è¾ƒå°ç½‘ç»œã€‚å› æ­¤ï¼Œåœ¨è´å¶æ–¯èƒŒæ™¯ä¸‹ï¼Œä½¿ç”¨
    transformers çš„åµŒå…¥ä½œä¸º BDL ç½‘ç»œçš„è¾“å…¥ï¼Œå¯èƒ½æ˜¯æœ€æ˜æ˜¾çš„ä½¿ç”¨æ–¹å¼â€”â€”äº‹å®ä¸Šï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½æ˜¯æœ€åˆç†çš„ç¬¬ä¸€æ­¥ã€‚
- en: In this section, weâ€™ve explored some of the recent developments in BDL. All
    of these build on, and apply directly to, the methods weâ€™ve introduced in the
    book, and you may want to consider implementing these when developing your own
    solutions for approximate Bayesian inference with deep networks. However, given
    the pace of research in machine learning, the list of improvements to Bayesian
    approximations is ever-growing, and we encourage you to explore the literature
    for yourself to discover the variety of ways in which researchers are learning
    to implement Bayesian inference at scale and with a variety of computational and
    theoretical advantages. That said, BDL isnâ€™t always the correct solution, and
    in the next section, weâ€™ll explore why.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰ä¸­çš„ä¸€äº›æœ€æ–°è¿›å±•ã€‚æ‰€æœ‰è¿™äº›éƒ½å»ºç«‹åœ¨æœ¬ä¹¦ä¸­ä»‹ç»çš„æ–¹æ³•ä¹‹ä¸Šï¼Œå¹¶ç›´æ¥åº”ç”¨äºè¿™äº›æ–¹æ³•ã€‚å½“ä½ ä¸ºè¿‘ä¼¼è´å¶æ–¯æ¨æ–­å¼€å‘è‡ªå·±çš„æ·±åº¦ç½‘ç»œè§£å†³æ–¹æ¡ˆæ—¶ï¼Œä½ å¯èƒ½æƒ³è€ƒè™‘å®ç°è¿™äº›è¿›å±•ã€‚ç„¶è€Œï¼Œé‰´äºæœºå™¨å­¦ä¹ ç ”ç©¶çš„å¿«é€Ÿè¿›å±•ï¼Œè´å¶æ–¯è¿‘ä¼¼çš„æ”¹è¿›åˆ—è¡¨ä¸æ–­å¢é•¿ï¼Œæˆ‘ä»¬é¼“åŠ±ä½ äº²è‡ªæ¢ç´¢æ–‡çŒ®ï¼Œäº†è§£ç ”ç©¶äººå‘˜å¦‚ä½•åœ¨å¤§è§„æ¨¡å®ç°è´å¶æ–¯æ¨æ–­ï¼Œå¹¶åˆ©ç”¨å„ç§è®¡ç®—å’Œç†è®ºä¼˜åŠ¿ã€‚ä¸è¿‡ï¼ŒBDL
    å¹¶ä¸æ€»æ˜¯æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å…¶åŸå› ã€‚
- en: 9.4 Alternatives to Bayesian deep learning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æ›¿ä»£æ–¹æ³•
- en: While the focus of the book is on Bayesian inference with DNNs, these arenâ€™t
    always the best choice for the job. Generally speaking, theyâ€™re a great choice
    when you have large amounts of high dimensional data. As we discussed in [*ChapterÂ 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003) (and as you probably know),
    deep networks excel in these scenarios, and thus adapting them for Bayesian inference
    is a sensible choice. On the other hand, if you have small amounts of low-dimensional
    data (with tens of features, fewer than 10,000 data points), then you may be better
    off with more traditional, well-principled Bayesian inference, such as via sampling
    or GPs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ¬ä¹¦çš„é‡ç‚¹æ˜¯ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è¿›è¡Œè´å¶æ–¯æ¨æ–­ï¼Œä½†å®ƒä»¬å¹¶ä¸æ€»æ˜¯æœ€åˆé€‚çš„é€‰æ‹©ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œå½“ä½ æ‹¥æœ‰å¤§é‡é«˜ç»´æ•°æ®æ—¶ï¼Œæ·±åº¦ç½‘ç»œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ï¼Œ[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ä¸­æ‰€è®¨è®ºçš„ï¼ˆä»¥åŠä½ å¯èƒ½å·²ç»çŸ¥é“çš„ï¼‰ï¼Œæ·±åº¦ç½‘ç»œåœ¨è¿™äº›åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå› æ­¤å°†å®ƒä»¬åº”ç”¨äºè´å¶æ–¯æ¨æ–­æ˜¯ä¸€ä¸ªæ˜æ™ºçš„é€‰æ‹©ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ æ‹¥æœ‰çš„æ˜¯å°‘é‡ä½ç»´æ•°æ®ï¼ˆç‰¹å¾æ•°åä¸ªï¼Œæ•°æ®ç‚¹å°‘äº10,000ï¼‰ï¼Œé‚£ä¹ˆä½ å¯èƒ½æ›´é€‚åˆä½¿ç”¨æ›´ä¼ ç»Ÿã€æ›´æœ‰åŸåˆ™çš„è´å¶æ–¯æ¨æ–­æ–¹æ³•ï¼Œä¾‹å¦‚é€šè¿‡é‡‡æ ·æˆ–é«˜æ–¯è¿‡ç¨‹ã€‚
- en: That said, there has been interest in scaling GPs, and the research community
    has developed GP-based methods that both scale to large amounts of data and are
    capable of complex non-linear transformations. In this section, weâ€™ll introduce
    these alternatives in case you wish to pursue them further.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œå…³äºæ‰©å±•é«˜æ–¯è¿‡ç¨‹çš„ç ”ç©¶ä¸€ç›´å¤‡å—å…³æ³¨ï¼Œç ”ç©¶ç¤¾åŒºå·²ç»å¼€å‘å‡ºèƒ½å¤Ÿå¤„ç†å¤§é‡æ•°æ®å¹¶èƒ½è¿›è¡Œå¤æ‚éçº¿æ€§å˜æ¢çš„åŸºäºé«˜æ–¯è¿‡ç¨‹çš„æ–¹æ³•ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»è¿™äº›æ›¿ä»£æ–¹æ³•ï¼Œä»¥ä¾¿ä½ å¦‚æœæœ‰å…´è¶£è¿›ä¸€æ­¥æ¢è®¨å®ƒä»¬æ—¶ï¼Œå¯ä»¥äº†è§£ç›¸å…³å†…å®¹ã€‚
- en: 9.4.1 Scalable Gaussian processes
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 å¯æ‰©å±•çš„é«˜æ–¯è¿‡ç¨‹
- en: 'At the beginning of the book, we introduced GPs and discussed why they are
    the gold standard when it comes to principled and reasonably computationally tractable
    uncertainty quantification in machine learning. Crucially, we talked about the
    limits of GPs: they become computationally infeasible with high-dimensional data
    or large amounts of data.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹¦çš„å¼€å¤´ï¼Œæˆ‘ä»¬ä»‹ç»äº†é«˜æ–¯è¿‡ç¨‹å¹¶è®¨è®ºäº†ä¸ºä»€ä¹ˆå®ƒä»¬æ˜¯æœºå™¨å­¦ä¹ ä¸­å…³äºåŸåˆ™æ€§å’Œè®¡ç®—ä¸Šå¯å¤„ç†çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„é»„é‡‘æ ‡å‡†ã€‚å…³é”®æ˜¯ï¼Œæˆ‘ä»¬è°ˆåˆ°äº†é«˜æ–¯è¿‡ç¨‹çš„å±€é™æ€§ï¼šå½“æ•°æ®ç»´åº¦é«˜æˆ–æ•°æ®é‡å¤§æ—¶ï¼Œå®ƒä»¬å˜å¾—è®¡ç®—ä¸Šä¸å¯è¡Œã€‚
- en: 'However, GPs are extremely powerful tools, and the machine learning community
    wasnâ€™t ready to give up on them. In [*ChapterÂ 2*](CH2.xhtml#x1-250002), [*Fundamentals
    of Bayesian* *Inference*](CH2.xhtml#x1-250002), we discussed the key prohibitive
    factor in GP training and inference: inverting the covariance matrix. While methods
    exist for making this more computationally tractable (such as Cholesky decomposition),
    these methods only get us so far. As such, the key methods for making GPs scalable
    are termed *sparse GPs*, and they looks to solve the problem of intractable GP
    training by modifying the covariance matrix via sparse GP approximation. Simply
    put, if we can shrink or simplify the covariance matrix (for example, by reducing
    the number of data points), we can make the inversion of the covariance matrix
    tractable, and thus make GP training and inference tractable.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰æ˜¯æå…¶å¼ºå¤§çš„å·¥å…·ï¼Œæœºå™¨å­¦ä¹ ç¤¾åŒºå¹¶æ²¡æœ‰å‡†å¤‡æ”¾å¼ƒå®ƒä»¬ã€‚åœ¨[*ç¬¬2ç« *](CH2.xhtml#x1-250002)ï¼Œ[*è´å¶æ–¯æ¨æ–­åŸºç¡€*](CH2.xhtml#x1-250002)ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†é«˜æ–¯è¿‡ç¨‹è®­ç»ƒå’Œæ¨æ–­ä¸­çš„å…³é”®éšœç¢å› ç´ ï¼šåè½¬åæ–¹å·®çŸ©é˜µã€‚è™½ç„¶æœ‰ä¸€äº›æ–¹æ³•å¯ä»¥ä½¿è¿™ä¸€è¿‡ç¨‹æ›´å…·è®¡ç®—å¯è¡Œæ€§ï¼ˆä¾‹å¦‚ï¼ŒCholeskyåˆ†è§£ï¼‰ï¼Œä½†è¿™äº›æ–¹æ³•ä¹Ÿåªèƒ½åšåˆ°ä¸€å®šç¨‹åº¦ã€‚å› æ­¤ï¼Œä½¿é«˜æ–¯è¿‡ç¨‹å¯æ‰©å±•çš„å…³é”®æ–¹æ³•è¢«ç§°ä¸º*ç¨€ç–é«˜æ–¯è¿‡ç¨‹*ï¼Œå®ƒä»¬é€šè¿‡ç¨€ç–é«˜æ–¯è¿‡ç¨‹è¿‘ä¼¼æ³•æ¥ä¿®æ”¹åæ–¹å·®çŸ©é˜µï¼Œä»è€Œè§£å†³äº†ä¸å¯å¤„ç†çš„é«˜æ–¯è¿‡ç¨‹è®­ç»ƒé—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬èƒ½ç¼©å°æˆ–ç®€åŒ–åæ–¹å·®çŸ©é˜µï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å‡å°‘æ•°æ®ç‚¹æ•°é‡ï¼‰ï¼Œå°±èƒ½ä½¿åæ–¹å·®çŸ©é˜µçš„åæ¼”å˜å¾—å¯è¡Œï¼Œä»è€Œä½¿é«˜æ–¯è¿‡ç¨‹è®­ç»ƒå’Œæ¨æ–­å˜å¾—å¯å¤„ç†ã€‚
- en: 'One of the most popular approaches for this was introduced in the paper, *Sparse*
    *Gaussian Processes using Pseudo-Inputs* by Edward Snelson and Zoubin Ghahramani.
    As with other sparse GP approaches, the authors developed a method for tractable
    GPs that leverages large datasets. In the paper, the authors show that they can
    closely approximate training with the full dataset by using a subset of data:
    they effectively circumvent the problem of large data by turning a *large data*
    problem into a *small data* problem. However, doing so requires selecting an appropriate
    subset of data points, which the authors term **pseudo** **inputs**.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æœ€æµè¡Œçš„æ–¹æ³•ä¹‹ä¸€æ˜¯åœ¨Edward Snelsonå’ŒZoubin Ghahramaniçš„è®ºæ–‡ã€Š*ä½¿ç”¨ä¼ªè¾“å…¥çš„ç¨€ç–é«˜æ–¯è¿‡ç¨‹*ã€‹ä¸­æå‡ºçš„ã€‚ä¸å…¶ä»–ç¨€ç–é«˜æ–¯è¿‡ç¨‹æ–¹æ³•ä¸€æ ·ï¼Œä½œè€…ä»¬å¼€å‘äº†ä¸€ç§åˆ©ç”¨å¤§æ•°æ®é›†çš„å¯å¤„ç†é«˜æ–¯è¿‡ç¨‹æ–¹æ³•ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…ä»¬å±•ç¤ºäº†ä»–ä»¬å¦‚ä½•é€šè¿‡ä½¿ç”¨æ•°æ®çš„å­é›†æ¥æ¥è¿‘å…¨æ•°æ®é›†çš„è®­ç»ƒï¼šä»–ä»¬é€šè¿‡å°†*å¤§æ•°æ®*é—®é¢˜è½¬åŒ–ä¸º*å°æ•°æ®*é—®é¢˜ï¼Œå®é™…ä¸Šç»•è¿‡äº†å¤§æ•°æ®é—®é¢˜ã€‚ç„¶è€Œï¼Œåšåˆ°è¿™ä¸€ç‚¹éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æ•°æ®å­é›†ï¼Œä½œè€…ä»¬ç§°ä¹‹ä¸º**ä¼ª**
    **è¾“å…¥**ã€‚
- en: 'The authors achieve this using a joint optimization process that selects the
    subset of data *M* from the full set *N* while also optimizing the hyperparameters
    of the kernel. This optimization process essentially finds the subset of data
    points that can be used to best describe the overall data: we illustrate this
    in *Figure* [*9.7*](#x1-186005r7).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…é€šè¿‡è”åˆä¼˜åŒ–è¿‡ç¨‹å®ç°è¿™ä¸€ç›®æ ‡ï¼Œè¯¥è¿‡ç¨‹ä»å®Œæ•´æ•°æ®é›†*N*ä¸­é€‰æ‹©å‡ºæ•°æ®å­é›†*M*ï¼ŒåŒæ—¶ä¼˜åŒ–æ ¸å‡½æ•°çš„è¶…å‚æ•°ã€‚è¿™ä¸ªä¼˜åŒ–è¿‡ç¨‹å®è´¨ä¸Šæ˜¯å¯»æ‰¾å¯ä»¥æœ€å¥½åœ°æè¿°æ•´ä½“æ•°æ®çš„æ•°æ®å­é›†ï¼šæˆ‘ä»¬åœ¨*å›¾*[*9.7*](#x1-186005r7)ä¸­å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚
- en: '![PIC](img/file188.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file188.png)'
- en: 'FigureÂ 9.7: Simple illustration of pseudo inputs'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.7ï¼šä¼ªè¾“å…¥çš„ç®€å•ç¤ºæ„å›¾
- en: 'In this diagram, all data points are illustrated, but we see that certain data
    points have been selected as they describe the key relationship between our variables.
    However, these points not only need to describe the relationship, in terms of
    mean, as a polynomial regression may do â€“ they also need to replicate the *variance*
    in the underlying data, such that the GP still produces well-calibrated uncertainty
    estimates. That is to say, while the pseudo inputs effectively reduce the number
    of data points, the distribution of the pseudo inputs still needs to approximate
    that of the true inputs: if an area in the true data distribution is rich in data,
    thereby producing confident predictions in this region, this needs to be true
    for the pseudo inputs too.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå›¾ç¤ºä¸­ï¼Œæ‰€æœ‰çš„æ•°æ®ç‚¹éƒ½æœ‰å±•ç¤ºï¼Œä½†æˆ‘ä»¬çœ‹åˆ°æŸäº›æ•°æ®ç‚¹è¢«é€‰ä¸­ï¼Œå› ä¸ºå®ƒä»¬æè¿°äº†æˆ‘ä»¬å˜é‡ä¹‹é—´çš„å…³é”®å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™äº›ç‚¹ä¸ä»…éœ€è¦æè¿°å…³ç³»ï¼Œå°±åƒå¤šé¡¹å¼å›å½’å¯èƒ½åšçš„é‚£æ ·â€”â€”å®ƒä»¬è¿˜éœ€è¦å¤åˆ¶åº•å±‚æ•°æ®ä¸­çš„*æ–¹å·®*ï¼Œä½¿å¾—é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä¾ç„¶èƒ½å¤Ÿäº§ç”Ÿè‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æ¢å¥è¯è¯´ï¼Œè™½ç„¶ä¼ªè¾“å…¥æœ‰æ•ˆåœ°å‡å°‘äº†æ•°æ®ç‚¹çš„æ•°é‡ï¼Œä½†ä¼ªè¾“å…¥çš„åˆ†å¸ƒä»éœ€è¿‘ä¼¼çœŸå®è¾“å…¥çš„åˆ†å¸ƒï¼šå¦‚æœçœŸå®æ•°æ®åˆ†å¸ƒä¸­çš„æŸä¸ªåŒºåŸŸæ•°æ®ä¸°å¯Œï¼Œä»è€Œåœ¨è¯¥åŒºåŸŸäº§ç”Ÿæœ‰ä¿¡å¿ƒçš„é¢„æµ‹ï¼Œé‚£ä¹ˆä¼ªè¾“å…¥ä¹Ÿéœ€è¦æ»¡è¶³è¿™ä¸€ç‚¹ã€‚
- en: Another method for scalable GPs was introduced more recently by Ke Wang *et*
    *al.* in their paper, *Exact Gaussian Processes on a Million Data Points*. In
    this work, the authors leverage recent developments in multi-GPU parallelization
    methods to implement scalable GPs. The method that enabled this is called **Blackbox
    Matrix-Matrix Multiplication** (**BBMM**), reduces the problem of GP inference
    to iterations of matrix multiplication. In so doing, it makes the process easier
    to parallelize, as the matrix multiplications can be partitioned and distributed
    over multiple GPUs. The authors show that doing so reduces the memory requirement
    for GP training to *O*(*n*) per GPU. This allows GPs to benefit from the kind
    of computational gains that have been benefiting deep learning methods for over
    a decade!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œç”±Ke Wang *et* *al.*åœ¨ä»–ä»¬çš„è®ºæ–‡ã€Š*ç™¾ä¸‡æ•°æ®ç‚¹ä¸Šçš„ç²¾ç¡®é«˜æ–¯è¿‡ç¨‹*ã€‹ä¸­æå‡ºäº†å¦ä¸€ç§å¯æ‰©å±•é«˜æ–¯è¿‡ç¨‹çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…åˆ©ç”¨äº†å¤šGPUå¹¶è¡ŒåŒ–æ–¹æ³•çš„æœ€æ–°è¿›å±•æ¥å®ç°å¯æ‰©å±•é«˜æ–¯è¿‡ç¨‹ã€‚ä½¿è¿™ä¸€æ–¹æ³•å¾—ä»¥å®ç°çš„æŠ€æœ¯è¢«ç§°ä¸º**é»‘ç›’çŸ©é˜µ-çŸ©é˜µä¹˜æ³•**ï¼ˆ**BBMM**ï¼‰ï¼Œå®ƒå°†é«˜æ–¯è¿‡ç¨‹æ¨æ–­é—®é¢˜ç®€åŒ–ä¸ºçŸ©é˜µä¹˜æ³•çš„è¿­ä»£è¿‡ç¨‹ã€‚é€šè¿‡è¿™æ ·åšï¼Œå®ƒä½¿å¾—è¯¥è¿‡ç¨‹æ›´å®¹æ˜“è¿›è¡Œå¹¶è¡ŒåŒ–ï¼Œå› ä¸ºçŸ©é˜µä¹˜æ³•å¯ä»¥è¢«åˆ’åˆ†å¹¶åˆ†é…åˆ°å¤šä¸ªGPUä¸Šã€‚ä½œè€…å±•ç¤ºäº†è¿™ç§æ–¹æ³•å°†é«˜æ–¯è¿‡ç¨‹è®­ç»ƒçš„å†…å­˜éœ€æ±‚å‡å°‘åˆ°æ¯ä¸ªGPUçš„*O*(*n*)ã€‚è¿™ä½¿å¾—é«˜æ–¯è¿‡ç¨‹èƒ½å¤Ÿå—ç›Šäºæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¿‡å»åå¤šå¹´é‡Œè·å¾—çš„è®¡ç®—ä¼˜åŠ¿ï¼
- en: 'Both of the methods presented here do a great job of addressing the scalability
    issues faced by GPs. The second method is particularly impressive as it achieves
    exact GP inference, but it does require significant compute infrastructure. The
    pseudo-inputs method, on the other hand, is practical for a larger proportion
    of use cases. However, neither of these approaches tackle one of the key advantages
    of BDL: the ability of deep networks to learn rich embeddings through complex
    non-linear transformations.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œå‘ˆç°çš„ä¸¤ç§æ–¹æ³•éƒ½å¾ˆå¥½åœ°è§£å†³äº†é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰é¢ä¸´çš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚ç¬¬äºŒç§æ–¹æ³•å°¤å…¶ä»¤äººå°è±¡æ·±åˆ»ï¼Œå› ä¸ºå®ƒå®ç°äº†ç²¾ç¡®çš„é«˜æ–¯è¿‡ç¨‹æ¨æ–­ï¼Œä½†ç¡®å®éœ€è¦æ˜¾è‘—çš„è®¡ç®—åŸºç¡€è®¾æ–½ã€‚å¦ä¸€æ–¹é¢ï¼Œä¼ªè¾“å…¥æ–¹æ³•åœ¨æ›´å¤§æ¯”ä¾‹çš„ä½¿ç”¨åœºæ™¯ä¸­å…·æœ‰å®ç”¨æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æœªè§£å†³è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰çš„å…³é”®ä¼˜åŠ¿ä¹‹ä¸€ï¼šæ·±åº¦ç½‘ç»œé€šè¿‡å¤æ‚çš„éçº¿æ€§å˜æ¢å­¦ä¹ ä¸°å¯Œçš„åµŒå…¥çš„èƒ½åŠ›ã€‚
- en: 9.4.2 Deep Gaussian processes
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 æ·±åº¦é«˜æ–¯è¿‡ç¨‹
- en: 'Introduced by Andreas Damianou and Neil Lawrence in their straightforwardly
    titled paper, *Deep Gaussian Processes*, deep GPs tackle the problem of rich embeddings
    through having layers of GPs, much in the same way that deep networks have multiple
    layers of neurons. Unlike the scalable GP work mentioned previously, deep GPs
    were motivated by the inverse of the scalability problem: how can we get the performance
    of deep networks with very little data?'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦é«˜æ–¯è¿‡ç¨‹ç”±Andreas Damianouå’ŒNeil Lawrenceåœ¨ä»–ä»¬æ ‡é¢˜ç›´ç™½çš„è®ºæ–‡ã€Š*æ·±åº¦é«˜æ–¯è¿‡ç¨‹*ã€‹ä¸­æå‡ºï¼Œæ·±åº¦é«˜æ–¯è¿‡ç¨‹é€šè¿‡æ‹¥æœ‰å¤šå±‚é«˜æ–¯è¿‡ç¨‹æ¥è§£å†³ä¸°å¯ŒåµŒå…¥çš„é—®é¢˜ï¼Œå°±åƒæ·±åº¦ç½‘ç»œæœ‰å¤šå±‚ç¥ç»å…ƒä¸€æ ·ã€‚ä¸ä¹‹å‰æåˆ°çš„å¯æ‰©å±•é«˜æ–¯è¿‡ç¨‹ä¸åŒï¼Œæ·±åº¦é«˜æ–¯è¿‡ç¨‹çš„æå‡ºæ­£æ˜¯å—åˆ°å¯æ‰©å±•æ€§é—®é¢˜åå‘é—®é¢˜çš„å¯å‘ï¼šæˆ‘ä»¬å¦‚ä½•èƒ½ç”¨éå¸¸å°‘çš„æ•°æ®è·å¾—æ·±åº¦ç½‘ç»œçš„æ€§èƒ½ï¼Ÿ
- en: Faced with this problem, and with the knowledge that GPs perform very well on
    small amounts of data, Damianou and Lawrence set out to see whether GPs could
    be layered to produce similarly rich embeddings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¯¹è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”æ„è¯†åˆ°é«˜æ–¯è¿‡ç¨‹åœ¨å°‘é‡æ•°æ®ä¸Šè¡¨ç°éå¸¸å¥½ï¼ŒDamianouå’ŒLawrenceç€æ‰‹ç ”ç©¶æ˜¯å¦å¯ä»¥å°†é«˜æ–¯è¿‡ç¨‹åˆ†å±‚ï¼Œä»è€Œç”Ÿæˆç±»ä¼¼çš„ä¸°å¯ŒåµŒå…¥ã€‚
- en: '![PIC](img/file189.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file189.png)'
- en: 'FigureÂ 9.8: Illustration of a deep GP'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.8ï¼šæ·±åº¦GPçš„ç¤ºæ„å›¾
- en: 'Their approach, while complex in terms of implementation, is simple in principle:
    just as a DNN comprises many layers, each receiving an input from the layer before
    it and feeding its output into the layer after it, deep GPs also assume this form
    of graphical structure â€“ as we see in *Figure* [*9.8*](#x1-187003r8). Mathematically,
    just as with deep networks, a deep GP can be viewed as a composition of functions.
    The GP illustrated previously could thus be described as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬çš„æ–¹æ³•ï¼Œå°½ç®¡åœ¨å®æ–½ä¸Šæ¯”è¾ƒå¤æ‚ï¼Œä½†åŸç†å¾ˆç®€å•ï¼šæ­£å¦‚ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç”±è®¸å¤šå±‚ç»„æˆï¼Œæ¯ä¸€å±‚æ¥æ”¶å‰ä¸€å±‚çš„è¾“å…¥å¹¶å°†è¾“å‡ºä¼ é€’ç»™åä¸€å±‚ï¼Œæ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ä¹Ÿå‡è®¾æœ‰è¿™ç§å›¾å½¢ç»“æ„â€”â€”æ­£å¦‚æˆ‘ä»¬åœ¨*å›¾*
    [*9.8*](#x1-187003r8)ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œå°±åƒæ·±åº¦ç½‘ç»œä¸€æ ·ï¼Œæ·±åº¦GPå¯ä»¥è§†ä¸ºå‡½æ•°çš„ç»„åˆã€‚å› æ­¤ï¼Œä¹‹å‰å±•ç¤ºçš„GPå¯ä»¥æè¿°ä¸ºï¼š
- en: '![y = g(x ) = f2(f1(x)) ](img/file190.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![y = g(x ) = f2(f1(x)) ](img/file190.jpg)'
- en: 'While this introduces the kind of rich non-linear transformations weâ€™re accustomed
    to in deep learning to GPs, it comes at a price. As we already know, standard
    GPs have limitations when it comes to scalability. Unfortunately for deep GPs,
    composing them in this way is analytically intractable. As such, Damianou and
    Lawrence had to find a tractable way of implementing deep GPs, and they did so
    using a tool that should now be familiar to you: variational approximation. Just
    as this forms an import building block for some of the BDL methods introduced
    in this book, itâ€™s also a key component in making deep GPs possible. In their
    paper, they show how deep GPs can be implemented with the help of variational
    approximations â€“ making it possible not only to produce rich, non-linear embeddings
    with GPs but for rich, non-linear embeddings to be achieved with *small* *amounts
    of data*. This makes deep GPs an important tool in the arsenal of Bayesian methods
    and is thus a method worth bearing in mind going forward.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™å°†æˆ‘ä»¬åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¹ æƒ¯çš„ä¸°å¯Œéçº¿æ€§å˜æ¢å¼•å…¥äº†é«˜æ–¯è¿‡ç¨‹ä¸­ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†ä»£ä»·ã€‚æ­£å¦‚æˆ‘ä»¬å·²ç»çŸ¥é“çš„é‚£æ ·ï¼Œæ ‡å‡†çš„é«˜æ–¯è¿‡ç¨‹åœ¨å¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨é™åˆ¶ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ·±åº¦GPé€šè¿‡è¿™ç§æ–¹å¼ç»„åˆæ˜¯åˆ†æä¸Šæ— æ³•è§£å†³çš„ã€‚å› æ­¤ï¼ŒDamianouå’ŒLawrenceå¿…é¡»æ‰¾åˆ°ä¸€ç§å¯è¡Œçš„å®ç°æ·±åº¦GPçš„æ–¹æ³•ï¼Œä»–ä»¬é€šè¿‡ä¸€ç§ç°åœ¨ä½ åº”è¯¥å·²ç»ç†Ÿæ‚‰çš„å·¥å…·ï¼šå˜åˆ†è¿‘ä¼¼ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚æ­£å¦‚å®ƒåœ¨æœ¬ä¹¦ä¸­ä»‹ç»çš„ä¸€äº›BDLæ–¹æ³•ä¸­æ„æˆäº†é‡è¦çš„æ„å»ºå—ä¸€æ ·ï¼Œå®ƒä¹Ÿæ˜¯ä½¿æ·±åº¦GPæˆä¸ºå¯èƒ½çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚åœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­ï¼Œä»–ä»¬å±•ç¤ºäº†å¦‚ä½•å€ŸåŠ©å˜åˆ†è¿‘ä¼¼å®ç°æ·±åº¦GPâ€”â€”è¿™ä¸ä»…ä½¿å¾—ä½¿ç”¨GPç”Ÿæˆä¸°å¯Œçš„éçº¿æ€§åµŒå…¥æˆä¸ºå¯èƒ½ï¼Œè€Œä¸”ä½¿å¾—åœ¨*å°‘é‡*
    *æ•°æ®*çš„æƒ…å†µä¸‹å®ç°ä¸°å¯Œçš„éçº¿æ€§åµŒå…¥æˆä¸ºå¯èƒ½ã€‚è¿™ä½¿å¾—æ·±åº¦GPæˆä¸ºè´å¶æ–¯æ–¹æ³•å·¥å…·ç®±ä¸­çš„ä¸€ä¸ªé‡è¦å·¥å…·ï¼Œå› æ­¤ï¼Œå®ƒæ˜¯ä¸€ä¸ªå€¼å¾—è®°ä½çš„æ–¹æ³•ã€‚
- en: 9.5 Your next steps in BDL
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 ä½ åœ¨BDLä¸­çš„ä¸‹ä¸€æ­¥
- en: Throughout this chapter, weâ€™ve concluded our introduction to BDL by taking a
    look at a variety of techniques that could help you to improve on the fundamental
    methods explored in the book. Weâ€™ve also taken a look at how the powerful gold-standard
    of Bayesian inference â€“ the GP â€“ can be adapted to tasks generally reserved for
    deep learning. While it is indeed possible to adapt GPs to these tasks, we also
    advise that itâ€™s generally easier and more practical to use the methods presented
    in this book, or methods derived from them. As always, itâ€™s up to you as the machine
    learning engineer to determine what is best for the task at hand, and we are confident
    that the material from the book will equip you well for the challenges ahead.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å›é¡¾ä¸€äº›å¯ä»¥å¸®åŠ©ä½ æ”¹è¿›ä¹¦ä¸­æ¢è®¨çš„åŸºæœ¬æ–¹æ³•çš„æŠ€æœ¯ï¼Œå®Œæˆäº†å¯¹BDLçš„ä»‹ç»ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†å¦‚ä½•å°†è´å¶æ–¯æ¨ç†çš„å¼ºå¤§æ ‡å‡†â€”â€”é«˜æ–¯è¿‡ç¨‹â€”â€”é€‚åº”äºä¸€èˆ¬ç”¨äºæ·±åº¦å­¦ä¹ çš„ä»»åŠ¡ã€‚è™½ç„¶ç¡®å®å¯ä»¥å°†é«˜æ–¯è¿‡ç¨‹é€‚åº”è¿™äº›ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬ä¹Ÿå»ºè®®ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œä½¿ç”¨æœ¬ä¹¦ä¸­ä»‹ç»çš„æ–¹æ³•æˆ–ä»ä¸­è¡ç”Ÿçš„æ–¹æ³•ä¼šæ›´å®¹æ˜“ä¸”æ›´å®ç”¨ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œä½œä¸ºæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œä½ éœ€è¦è‡ªå·±åˆ¤æ–­ä»€ä¹ˆæ–¹æ³•æœ€é€‚åˆå½“å‰çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬ç›¸ä¿¡æœ¬ä¹¦çš„å†…å®¹å°†ä¸ºä½ è¿æ¥æœªæ¥çš„æŒ‘æˆ˜æä¾›å……è¶³çš„å‡†å¤‡ã€‚
- en: While this book provides you with the necessary fundamentals to get started,
    thereâ€™s always more learn â€“ particularly in such a rapidly moving field! In the
    next section, weâ€™ll provide a few key final recommendations, helping you to plan
    your next steps in learning about and applying BDL.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æœ¬ä¹¦ä¸ºä½ æä¾›äº†å¼€å§‹çš„å¿…è¦åŸºç¡€ï¼Œä½†åœ¨å¦‚æ­¤å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ï¼Œæ€»æœ‰æ›´å¤šçš„ä¸œè¥¿å€¼å¾—å­¦ä¹ ï¼åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æä¾›ä¸€äº›å…³é”®çš„æœ€ç»ˆå»ºè®®ï¼Œå¸®åŠ©ä½ è§„åˆ’ä¸‹ä¸€æ­¥çš„å­¦ä¹ ä¸åº”ç”¨BDLçš„è¿‡ç¨‹ã€‚
- en: We hope that youâ€™ve found this introduction to Bayesian deep learning to be
    comprehensive, practical, and enjoyable. Thank you for reading â€“ we wish you success
    in exploring these methods further and applying them within your own machine learning
    solutions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ‚¨è§‰å¾—è¿™ç¯‡å…³äºè´å¶æ–¯æ·±åº¦å­¦ä¹ çš„ä»‹ç»å†…å®¹æ—¢å…¨é¢ã€å®ç”¨åˆæœ‰è¶£ã€‚æ„Ÿè°¢æ‚¨çš„é˜…è¯»â€”â€”æˆ‘ä»¬ç¥æ„¿æ‚¨åœ¨è¿›ä¸€æ­¥æ¢ç´¢è¿™äº›æ–¹æ³•å¹¶å°†å…¶åº”ç”¨åˆ°æ‚¨è‡ªå·±çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆä¸­æ—¶å–å¾—æˆåŠŸã€‚
- en: 9.6 Further reading
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 è¿›ä¸€æ­¥é˜…è¯»
- en: 'The following reading recommendations are provided for those who wish to learn
    more about the recent methods presented in this chapter. These give a great insight
    into current challenges in the field, looking beyond Bayesian neural networks
    and into scalable Bayesian inference more generally:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹é˜…è¯»æ¨èé€‚åˆé‚£äº›å¸Œæœ›è¿›ä¸€æ­¥äº†è§£æœ¬ç« æ‰€å‘ˆç°çš„æœ€æ–°æ–¹æ³•çš„è¯»è€…ã€‚è¿™äº›èµ„æºä¸ºå½“å‰é¢†åŸŸä¸­çš„æŒ‘æˆ˜æä¾›äº†æå¥½çš„æ´å¯Ÿï¼Œå…³æ³¨çš„ä¸ä»…ä»…æ˜¯è´å¶æ–¯ç¥ç»ç½‘ç»œï¼Œè¿˜åŒ…æ‹¬æ›´å¹¿æ³›çš„å¯æ‰©å±•è´å¶æ–¯æ¨æ–­ï¼š
- en: '*Deep Ensemble Bayesian Active Learning*, Pop and Fulop: This paper demonstrates
    the advantages of combining deep ensembles with MC dropout to produce better-calibrated
    uncertainty estimates, as shown when applying their method to active learning
    tasks.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦é›†æˆè´å¶æ–¯ä¸»åŠ¨å­¦ä¹ *ï¼ŒPop å’Œ Fulopï¼šæœ¬æ–‡å±•ç¤ºäº†å°†æ·±åº¦é›†æˆä¸ MC dropout ç»“åˆçš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæä¾›æ›´ç²¾ç¡®çš„é¢„æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ­£å¦‚åœ¨åº”ç”¨è¯¥æ–¹æ³•åˆ°ä¸»åŠ¨å­¦ä¹ ä»»åŠ¡æ—¶æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚'
- en: '*Uncertainty in Neural Networks: Approximately Bayesian Ensembling*, Pearce
    *et al.*: This paper introduces a simple and effective method for improving the
    performance of deep ensembles. The authors show that by promoting diversity through
    a simple adaptation to the loss function, the ensemble is able to produces better-calibrated
    uncertainty estimates.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç¥ç»ç½‘ç»œä¸­çš„ä¸ç¡®å®šæ€§ï¼šè¿‘ä¼¼è´å¶æ–¯é›†æˆ*ï¼ŒPearce ç­‰äººï¼šæœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºæé«˜æ·±åº¦é›†æˆçš„æ€§èƒ½ã€‚ä½œè€…å±•ç¤ºäº†é€šè¿‡å¯¹æŸå¤±å‡½æ•°åšç®€å•è°ƒæ•´æ¥ä¿ƒè¿›å¤šæ ·æ€§ï¼Œä»è€Œä½¿å¾—é›†æˆèƒ½å¤Ÿç”Ÿæˆæ›´ä¸ºç²¾ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚'
- en: '*Sparse Gaussian Processes Using Pseudo-Inputs*, Snelson and Gharamani: This
    paper introduces the concept of pseudo-input-based GPs, introducing a key method
    in scalable GP inference.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä½¿ç”¨ä¼ªè¾“å…¥çš„ç¨€ç–é«˜æ–¯è¿‡ç¨‹*ï¼ŒSnelson å’Œ Gharamaniï¼šæœ¬æ–‡ä»‹ç»äº†åŸºäºä¼ªè¾“å…¥çš„é«˜æ–¯è¿‡ç¨‹æ¦‚å¿µï¼Œæå‡ºäº†å¯æ‰©å±•é«˜æ–¯è¿‡ç¨‹æ¨æ–­ä¸­çš„ä¸€ä¸ªå…³é”®æ–¹æ³•ã€‚'
- en: '*Exact Gaussian Processes on a Million Data Points*, Wang *et al.*: An important
    paper demonstrating that Gaussian Processes can benefit from developments in compute
    hardware through the use of BBMM, making exact GP inference possible for big data.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç™¾ä¸‡æ•°æ®ç‚¹ä¸Šçš„ç²¾ç¡®é«˜æ–¯è¿‡ç¨‹*ï¼ŒWang ç­‰äººï¼šä¸€ç¯‡é‡è¦è®ºæ–‡ï¼Œå±•ç¤ºäº†é€šè¿‡ä½¿ç”¨ BBMM æŠ€æœ¯ï¼Œé«˜æ–¯è¿‡ç¨‹å¯ä»¥å—ç›Šäºè®¡ç®—ç¡¬ä»¶çš„å‘å±•ï¼Œä½¿å¾—å¤§æ•°æ®çš„ç²¾ç¡®é«˜æ–¯è¿‡ç¨‹æ¨æ–­æˆä¸ºå¯èƒ½ã€‚'
- en: '*Deep Gaussian Processes*, Damianou and Lawrence: Introducing the concept of
    deep GPs, this paper demonstrates how GPs can be used to achieve complex non-linear
    transformations with datasets far smaller than those required for deep learning.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦é«˜æ–¯è¿‡ç¨‹*ï¼ŒDamianou å’Œ Lawrenceï¼šå¼•å…¥æ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆGPsï¼‰è¿™ä¸€æ¦‚å¿µï¼Œæœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å®ç°å¤æ‚çš„éçº¿æ€§å˜æ¢ï¼Œä¸”æ‰€éœ€çš„æ•°æ®é›†è¿œå°äºæ·±åº¦å­¦ä¹ æ‰€éœ€çš„æ•°æ®é›†ã€‚'
- en: 'Weâ€™ve selected a few key resources to help you in your next steps into BDL,
    allowing you to dive deeper into the theory and helping you to get the most out
    of the content presented here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒ‘é€‰äº†ä¸€äº›å…³é”®èµ„æºï¼Œå¸®åŠ©æ‚¨è¿›å…¥è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„ä¸‹ä¸€é˜¶æ®µï¼Œè®©æ‚¨æ·±å…¥ç†è®ºå¹¶å¸®åŠ©æ‚¨æ›´å¥½åœ°åˆ©ç”¨æœ¬ç« å†…å®¹ï¼š
- en: '*Machine Learning: A Probabilistic Perspective*, Murphy: Released in 2012,
    this has since become one of the key texts on machine learning, presenting a well-principled
    approach to understanding all of the key methods within machine learning. The
    probabilistic angle of the book makes it a great addition to your collection of
    Bayesian literature.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æœºå™¨å­¦ä¹ ï¼šä¸€ç§æ¦‚ç‡è§†è§’*ï¼ŒMurphyï¼šè¯¥ä¹¦äº 2012 å¹´å‘å¸ƒï¼Œä¹‹åæˆä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„æ ¸å¿ƒæ•™æä¹‹ä¸€ï¼Œå‘ˆç°äº†ä¸€ç§ç†è§£æœºå™¨å­¦ä¹ ä¸­æ‰€æœ‰å…³é”®æ–¹æ³•çš„åˆç†æ–¹æ³•ã€‚è¯¥ä¹¦çš„æ¦‚ç‡è§†è§’ä½¿å…¶æˆä¸ºè´å¶æ–¯æ–‡çŒ®æ”¶è—ä¸­çš„ä¸€å¤§äº®ç‚¹ã€‚'
- en: '*Probabilistic Machine Learning: An Introduction*, Murphy: Another more recent
    Murphy text. Released in 2022, this is another important text providing a detailed
    treatment of probabilistic machine learning (including a section on Bayesian neural
    networks). While there is some overlap between this and Murphyâ€™s previous text,
    both are worth having at your disposal.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ¦‚ç‡æœºå™¨å­¦ä¹ ï¼šå…¥é—¨*ï¼ŒMurphyï¼šå¦ä¸€ç¯‡è¾ƒæ–°çš„ Murphy ä½œå“ã€‚è¯¥ä¹¦äº 2022 å¹´å‘å¸ƒï¼Œæä¾›äº†å…³äºæ¦‚ç‡æœºå™¨å­¦ä¹ ï¼ˆåŒ…æ‹¬è´å¶æ–¯ç¥ç»ç½‘ç»œéƒ¨åˆ†ï¼‰çš„è¯¦ç»†å†…å®¹ã€‚å°½ç®¡ä¸
    Murphy ä¹‹å‰çš„ä½œå“æœ‰æ‰€é‡å ï¼Œä½†ä¸¤è€…éƒ½å€¼å¾—æ‹¥æœ‰ï¼Œä¸”å„æœ‰å…¶ç‹¬åˆ°ä¹‹å¤„ã€‚'
- en: '*Gaussian Processes for Machine Learning*, Rasmussen and Williams: Perhaps
    the most important text on Gaussian Processes, this is a hugely valuable text
    when it comes to Bayesian inference. The authorsâ€™ detailed explanation of GPs
    will give you a thorough understanding of this important piece of the Bayesian
    puzzle.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æœºå™¨å­¦ä¹ ä¸­çš„é«˜æ–¯è¿‡ç¨‹*ï¼Œæ‹‰æ–¯ç©†æ£®å’Œå¨å»‰å§†æ–¯ï¼šä¹Ÿè®¸æ˜¯å…³äºé«˜æ–¯è¿‡ç¨‹æœ€é‡è¦çš„æ–‡æœ¬ï¼Œè¿™æœ¬ä¹¦åœ¨è´å¶æ–¯æ¨æ–­ä¸­å…·æœ‰æé«˜çš„ä»·å€¼ã€‚ä½œè€…å¯¹é«˜æ–¯è¿‡ç¨‹çš„è¯¦ç»†è§£é‡Šå°†å¸®åŠ©ä½ å…¨é¢ç†è§£è´å¶æ–¯éš¾é¢˜ä¸­çš„è¿™ä¸€é‡è¦éƒ¨åˆ†ã€‚'
- en: '*Bayesian Analysis with Python*, Martin: Covering all the fundamentals of Bayesian
    analysis, this title is an excellent piece of foundational literature and will
    help you to dive deeper into the fundamentals of Bayesian inference.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pythonä¸­çš„è´å¶æ–¯åˆ†æ*ï¼Œé©¬ä¸ï¼šæ¶µç›–äº†è´å¶æ–¯åˆ†æçš„æ‰€æœ‰åŸºç¡€çŸ¥è¯†ï¼Œè¿™æœ¬ä¹¦æ˜¯ä¸€æœ¬å‡ºè‰²çš„åŸºç¡€æ–‡çŒ®ï¼Œå°†å¸®åŠ©ä½ æ›´æ·±å…¥åœ°ç†è§£è´å¶æ–¯æ¨æ–­çš„åŸºç¡€ã€‚'
