- en: Image Analysis Applications in Self-Driving Cars
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶汽车中的图像分析应用
- en: In the previous chapters, we learned about object classification and also object
    localization. In this chapter, we will go through multiple case studies that are
    relevant to self-driving cars.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了物体分类以及物体定位。在本章中，我们将通过多个与自动驾驶汽车相关的案例研究。
- en: 'You will be learning about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习以下内容：
- en: Traffic sign identification
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交通标志识别
- en: Predicting the angle within which a car needs to be turned
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测汽车需要转动的角度范围
- en: Identifying cars on the road using the U-net architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用U-net架构识别道路上的汽车
- en: Semantic segmentation of objects on the road
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路面上物体的语义分割
- en: Traffic sign identification
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交通标志识别
- en: In this case study, we will understand the way in which we can classify a signal
    into one of the 43 possible classes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们将了解如何将信号分类为43种可能的类别之一。
- en: Getting ready
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'For this exercise, we will adopt the following strategy:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本练习，我们将采用以下策略：
- en: Download the dataset that contains all possible traffic signs
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载包含所有可能交通标志的数据集
- en: 'Perform histogram normalization on top of input images:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入图像执行直方图归一化处理：
- en: Certain images are taken in broad day light, while others might be taken in
    twilight
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些图像是在明亮的白天拍摄的，而其他一些可能是在黄昏时拍摄的
- en: Different lighting conditions result in a variation in pixel values, depending
    on the lighting condition at which the picture is taken
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的光照条件会导致像素值的变化，具体取决于拍摄照片时的光照条件
- en: Histogram normalization performs normalization on pixel values so that they
    all have a similar distribution
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直方图归一化对像素值进行归一化处理，使它们具有相似的分布
- en: Scale the input images
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放输入图像
- en: Build, compile, and fit a model to reduce the categorical cross entropy loss
    value
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建、编译并拟合模型以减少类别交叉熵损失值
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Download the dataset, as follows (the code file is available as `Traffic_signal_detection.ipynb`
    in GitHub). The dataset is available through the paper: J. Stallkamp, M. Schlipsing,
    J. Salmen, C. Igel, Man vs. computer: Benchmarking machine learning algorithms
    for traffic sign recognition:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集，如下所示（代码文件可在GitHub中的`Traffic_signal_detection.ipynb`找到）。数据集可通过论文获得：J. Stallkamp,
    M. Schlipsing, J. Salmen, C. Igel, 《人与计算机：基准机器学习算法在交通标志识别中的表现》：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Read the image paths into a list, as follows:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像路径读取到列表中，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A sample of the images looks as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的样例如下所示：
- en: '![](img/12bf31d5-f7a6-4255-afeb-78f1abe0d861.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12bf31d5-f7a6-4255-afeb-78f1abe0d861.png)'
- en: Note that certain images have a smaller shape when compared to others and also
    that certain images have more lighting when compared to others. Thus, we'll have
    to preprocess the images so that all images are normalized per exposure to lighting
    as well as shape.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，某些图像的形状较小，而某些图像的光照较强。因此，我们需要对图像进行预处理，使所有图像在光照和形状方面都进行标准化。
- en: 'Perform histogram normalization on top of the input dataset, as follows:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入数据集执行直方图归一化处理，如下所示：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we are first converting an image that is in RGB format
    into a **Hue Saturation Value (HSV)** format. By transforming the image from RGB
    to HSV format, we are essentially converting the combined RGB values into an array
    that can then be transformed into an array of single dimension.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先将RGB格式的图像转换为**色调饱和度值（HSV）**格式。通过将图像从RGB格式转换为HSV格式，我们实质上是将RGB组合值转换为一个数组，然后再将其转换为单维数组。
- en: Post that, we are normalizing the values obtained in HSV format so that they
    belong to the same scale by using the `equalize_hist` method.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`equalize_hist`方法对以HSV格式获得的值进行归一化，使它们归于相同的尺度。
- en: Once the images are normalized in the last channel of the HSV format, we convert
    them back in to RGB format.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像在HSV格式的最后一个通道中被归一化，我们将它们转换回RGB格式。
- en: Finally, we resize the images to a standard size.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将图像调整为标准尺寸。
- en: 'Check the image prior to passing it through histogram normalization and contrast
    that with post histogram normalization (post passing the image through the `preprocess_img`
    function), as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查图像在通过直方图归一化之前的状态，并将其与归一化后的状态进行对比（即通过`preprocess_img`函数处理后的图像），如下所示：
- en: '![](img/c4e7a40e-ffcd-451d-9a24-7547c009ff94.png)![](img/548e4913-54ac-4117-814e-c8b5207d0392.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4e7a40e-ffcd-451d-9a24-7547c009ff94.png)![](img/548e4913-54ac-4117-814e-c8b5207d0392.png)'
- en: From the preceding pictures, we can see that there is a considerable change
    in the visibility of the image (the image on the left) post histogram normalization
    (the image on the right).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图片中可以看出，经过直方图归一化后（右侧图像），图像的可见度发生了显著变化（左侧图像）。
- en: 'Prepare the input and output arrays, as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式准备输入和输出数组：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Build the training and test datasets, as follows:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式构建训练集和测试集：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Build and compile the model, as follows:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，构建并编译模型：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A summary of the model is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/23d3c55c-1505-4cf1-a1b8-10214e8d9207.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23d3c55c-1505-4cf1-a1b8-10214e8d9207.png)'
- en: 'Fit the model, as follows:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，拟合模型：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code, results in a model that has an accuracy of ~99%:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了一个模型，其准确率约为99%：
- en: '![](img/fe909540-e0d5-466a-bd5c-bfc9597710c9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe909540-e0d5-466a-bd5c-bfc9597710c9.png)'
- en: Additionally, if you perform the exact same analysis like we did, but without
    histogram normalization (correcting for exposure), the accuracy of the model is
    ~97%.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您执行与我们相同的分析，但没有进行直方图归一化（曝光校正），模型的准确率约为97%。
- en: Predicting the angle within which a car needs to be turned
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测汽车需要转动的角度
- en: In this case study, we will understand the angle within which a car needs to
    be turned based on the image provided.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例中，我们将基于提供的图像来理解需要转动汽车的角度。
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The strategy we adopt to build a steering angle prediction is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用的构建转向角度预测策略如下：
- en: Gather a dataset that has the images of the road and the corresponding angle
    within which the steering needs to be turned
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一个数据集，其中包含道路的图像和需要转动方向盘的相应角度
- en: Preprocess the image
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理图像
- en: Pass the image through the VGG16 model to extract features
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像传入VGG16模型以提取特征
- en: Build a neural network that performs regression to predict the steering angle,
    which is a continuous value to be predicted
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个神经网络，执行回归以预测转向角度，这是一个需要预测的连续值
- en: How to do it...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行……
- en: 'Download the following dataset. This dataset is available from the following
    link: [https://github.com/SullyChen/driving-datasets](https://github.com/SullyChen/driving-datasets): (the
    code file is available as `Car_steering_angle_detection.ipynb` in GitHub):'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载以下数据集。该数据集可以通过以下链接获得：[https://github.com/SullyChen/driving-datasets](https://github.com/SullyChen/driving-datasets)：（代码文件可以在GitHub中的`Car_steering_angle_detection.ipynb`找到）：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Import the relevant packages, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包，如下所示：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Read the images and their corresponding angles in radians into separate lists,
    as follows:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像及其对应的弧度角度分别读取到单独的列表中，如下所示：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create the train and test datasets, as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，创建训练集和测试集：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Check the output label values in the train and test datasets, as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查训练和测试数据集中的输出标签值，如下所示：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/40a861cd-fd88-473f-a389-2d38d2e18b25.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40a861cd-fd88-473f-a389-2d38d2e18b25.png)'
- en: 'Remove the pixels in the first 100 rows, as they do not correspond to the image
    of a road, and then pass the resulting image through the VGG16 model. Additionally,
    for this exercise, we will work on only the first 10,000 images in the dataset
    so that we are able to build a model faster. Remove the pixels in the first 100
    rows, as follows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除前100行的像素，因为这些像素与道路图像无关，然后将处理后的图像传入VGG16模型。此外，在此练习中，我们仅使用数据集中的前10,000张图像，以便更快地构建模型。删除前100行的像素，如下所示：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Build and compile the model, as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，构建并编译模型：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that the output layer has linear activation as the output is a continuous
    value that ranges from -9 to +9\. A summary of the model is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出层采用线性激活，因为输出是一个连续值，范围从 -9 到 +9。模型的总结如下：
- en: '![](img/edb25ad9-1735-49e8-8eac-039a1b89c67e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edb25ad9-1735-49e8-8eac-039a1b89c67e.png)'
- en: 'Now, we''ll compile the model we''ve defined as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将按如下方式编译已定义的模型：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Fit the model, as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，拟合模型：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/971589a3-0d22-4c0f-8fa9-30f7742401d4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/971589a3-0d22-4c0f-8fa9-30f7742401d4.png)'
- en: Test loss is the line that has the lower loss in the preceding diagram.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 测试损失是前面图表中损失较低的那条线。
- en: Note that we have divided the input dataset by 11 so that we can scale it to
    have a values between 0 to 1\. Now, we should be in a position to simulate the
    movement of the car based on the angle that it is predicted.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经将输入数据集除以11，以便将其缩放到0到1之间。现在，我们应该能够根据预测的角度模拟汽车的运动。
- en: 'The steering angle predictions obtained by the model for a sample of images
    are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对样本图像的转向角度预测结果如下：
- en: '![](img/6d08b8a4-ce50-464a-9b5d-46afa3198668.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d08b8a4-ce50-464a-9b5d-46afa3198668.png)'
- en: '![](img/342b5540-0861-4d72-b58c-908420c84c6f.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/342b5540-0861-4d72-b58c-908420c84c6f.png)'
- en: Note that you should be very careful while taking a model like the preceding
    one and implementing it. It should be first tested on multiple daylight conditions
    before finally going to production.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用上述模型时需要非常小心。它应首先在多种日光条件下进行测试，然后再进入生产环境。
- en: Instance segmentation using the U-net architecture
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用U-net架构进行实例分割
- en: So far, in the previous two chapters, we have learned about detecting objects
    and also about identifying the bounding boxes within which the objects within
    an image are located. In this section, we will learn about performing instance
    segmentation, where all the pixels belonging to a certain object are highlighted
    while every other pixel isn't (this is similar to masking all the other pixels
    that do not belong to an object with zeros and masking the pixels that belong
    to the object with pixel values of one).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前两章中，我们已经学习了如何检测物体，以及如何识别图像中物体所在的边界框。在本节中，我们将学习如何进行实例分割，在实例分割中，属于某个特定物体的所有像素都会被突出显示，而其他像素则不会（这类似于用零掩膜掉所有不属于物体的像素，并用像素值1掩膜属于物体的像素）。
- en: Getting ready
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To perform instance segmentation, we will perform the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行实例分割，我们将执行以下操作：
- en: 'Work on a dataset that has the input image and the corresponding masked image
    of the pixels where the object is located in the image:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个数据集上工作，该数据集具有输入图像及其对应的掩膜图像，掩膜图像显示对象在图像中的像素位置：
- en: The image and its masked image
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像及其掩膜图像
- en: We'll pass the image through the pre-trained VGG16 model to extract features
    out of each convolution layer
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过预训练的VGG16模型将图像传递，以提取每个卷积层的特征
- en: We'll gradually up sample the convolution layers so that we get an output image
    that is of 224 x 224 x 3 in shape
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将逐渐上采样卷积层，以便我们获得形状为224 x 224 x 3的输出图像
- en: We'll freeze the layers where VGG16 weights are used
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将冻结使用VGG16权重的层
- en: 'Concatenate the up sampled convolution layers with the down sampled convolution
    layers:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将上采样的卷积层与下采样的卷积层连接起来：
- en: This forms the U-shaped connection
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这形成了U形连接
- en: The U-shaped connection helps in model having the context in a way similar to
    ResNet (previously down sampled layer provides context in addition to the up sampled
    layer)
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: U形连接帮助模型获得类似于ResNet的上下文（之前下采样的层提供上下文，除了上采样的层外）
- en: Reconstructing an image is much easier if we take the first layer's output,
    as much of the image is intact in the first layer (earlier layers learn the contours).
    If we try to reconstruct an image from the last few layers by up sampling them,
    there is a good chance that the majority of the information about the image is
    lost
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们取第一层的输出，重建图像会更容易，因为大部分图像在第一层中是完好的（早期层学习图像的轮廓）。如果我们尝试通过上采样最后几层来重建图像，那么很有可能大部分图像信息会丢失
- en: 'Fit a model that maps the input image to masked image:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合一个将输入图像映射到掩膜图像的模型：
- en: Note that the masked image is binary in nature—where the black values correspond
    to a pixel value of 0 and the white pixels have a value of 1
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，掩膜图像本质上是二进制的——黑色值对应于像素值0，白色像素的值为1
- en: Minimize the binary cross entropy loss function across all the 224 x 224 x 1
    pixels
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有224 x 224 x 1像素中最小化二元交叉熵损失函数
- en: 'The reason this model is called a **U-net architecture** is because the visualization
    of the model looks as follows—a rotated U-like structure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以称该模型为**U-net架构**，是因为模型的可视化如下所示——一个旋转的U形结构：
- en: '![](img/b4e97c5a-c390-4983-92f7-c9da19ef8318.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4e97c5a-c390-4983-92f7-c9da19ef8318.png)'
- en: The U-like structure of the model is due to the early layers connecting to up
    sampled versions of the down sampled layers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的U形结构是由于早期层连接到下采样层的上采样版本。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the following code, we will perform instance segmentation to detect a car
    within an image:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将执行实例分割，以检测图像中的汽车：
- en: 'Download and import files from [https://github.com/divamgupta/image-segmentation-keras](https://github.com/divamgupta/image-segmentation-keras),
    as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/divamgupta/image-segmentation-keras](https://github.com/divamgupta/image-segmentation-keras)下载并导入文件，如下所示：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Read the images and their corresponding masks into arrays, as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像及其对应的掩膜读取为数组，如下所示：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding step, we have created the input and output arrays and also
    normalized the input array. Finally, we separated the mask of a car from everything
    else, as this dataset has 12 unique classes of which cars are masked with a pixel
    value of 8.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们创建了输入和输出数组，并且还对输入数组进行了归一化。最后，我们从所有其他内容中分离出了汽车的掩模，因为该数据集有12个唯一类别，其中汽车的像素值被标记为8。
- en: 'A sample of input and masked images are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和掩模图像的示例如下：
- en: '![](img/1c1e655a-3c45-45ce-bfda-8863dbc5dbf8.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c1e655a-3c45-45ce-bfda-8863dbc5dbf8.png)'
- en: 'Furthermore, we create input and output arrays where we scale the input array
    and reshape the output array (so that it can be passed to network), as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们创建了输入和输出数组，其中我们对输入数组进行缩放，并重新塑形输出数组（以便可以传递给网络），如下所示：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Build the model where the image is first passed through the VGG16 model layers
    and the convolution features are extracted, as follows:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型，其中图像首先通过VGG16模型层，提取卷积特征，如下所示：
- en: 'In the following code, we are importing the pre-trained VGG16 model:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们导入了预训练的VGG16模型：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the following code, the features of various convolution layers when passed
    through the VGG16 model are extracted:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，当不同的卷积层通过VGG16模型时，我们提取了特征：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the following code, we are up scaling the features using the `UpSampling` method
    and then concatenating with the down scaled VGG16 convolution features at each
    layer:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用`UpSampling`方法对特征进行上采样，并在每一层将其与下采样后的VGG16卷积特征进行拼接：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the following code, we are defining the input and output to the model, where
    the input is passed to the `base_pretrained_model` first and the output is `conv10` (which
    has the shape of 224 x 224 x 1—the intended shape of our output):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们定义了模型的输入和输出，其中输入首先传递给`base_pretrained_model`，输出是`conv10`（其形状为224 x
    224 x 1—我们输出的预期形状）：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Freeze the convolution layers obtained from the multiplication of the VGG16
    model from training, as follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结通过训练得到的VGG16模型的卷积层，如下所示：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Compile and fit the model for the first 1,000 images in our dataset, as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并拟合模型，以处理数据集中前1,000张图像，如下所示：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/3027213c-790c-4e63-8d37-c02db987f53a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3027213c-790c-4e63-8d37-c02db987f53a.png)'
- en: 'Test the preceding model on a test image (the last 2 images of our dataset—they
    are test images that have `validtion_split = 0.1`), as follows:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集的最后两张测试图像上测试之前的模型（这些是具有`validation_split = 0.1`的测试图像），如下所示：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/35f3e4be-1c52-4657-a157-aaf44df7d756.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35f3e4be-1c52-4657-a157-aaf44df7d756.png)'
- en: We can see that the generated mask is realistic for the given input of road
    and also in a way that's better than what we were doing prior as the noisy dots
    are not present in the predicted mask image.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于给定的道路输入，生成的掩模非常真实，并且比之前的方法更好，因为预测的掩模图像中没有噪点。
- en: Semantic segmentation of objects in an image
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像中对象的语义分割
- en: In the previous section, we learned about performing segmentation on top of
    an image where the image contained only one object. In this segmentation, we will
    learn about performing segmentation so that we are able to distinguish between
    multiple objects that are present in an image of a road.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何对包含单一对象的图像进行分割。在本节分割中，我们将学习如何进行分割，以便能够区分图像中存在的多个对象，尤其是在道路图像中。
- en: Getting ready
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The strategy that we''ll adopt to perform semantic segmentation on top of images
    of a road is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略是，在道路图像上执行语义分割，如下所示：
- en: 'Gather a dataset that has the annotation of where the multiple objects within
    an image are located:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一个数据集，其中包含标注了图像中多个对象位置的信息：
- en: 'A sample of the semantic image looks as follows:'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义图像的示例如下所示：
- en: '![](img/9bee9bc6-479d-41cb-a8f7-25a68aa90b73.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bee9bc6-479d-41cb-a8f7-25a68aa90b73.png)'
- en: Convert the output mask into a multi dimensional array where there are as many
    columns as the number of all possible unique objects
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出掩模转换为多维数组，其中列数等于所有可能的唯一对象的数量。
- en: 'If there are 12 possible unique values (12 unique objects), convert the output
    image into  an image that is 224 x 224 x 12 in shape:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有12个可能的唯一值（12个唯一对象），将输出图像转换为形状为224 x 224 x 12的图像：
- en: A value of a channel represents that the object corresponding to that channel
    is present in that location of image
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通道的值表示该通道对应的对象在图像中的该位置存在。
- en: Leverage the model architecture that we have seen in previous sections to train
    a model that has 12 possible output values
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用我们在前面部分看到的模型架构，训练一个具有12个可能输出值的模型
- en: 'Reshape the prediction into three channels by assigning all three channels
    to have the same output:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将所有三个通道分配相同的输出，将预测结果重塑为三个通道：
- en: The output is the argmax of prediction of the probabilities of the 12 possible
    classes
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是12个可能类别的概率预测的最大值（argmax）
- en: How to do it...
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Semantic segmentation in code is performed as follows (The code file is available
    as `Semantic_segmentation.ipynb` in GitHub):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割的代码实现如下（代码文件可以在GitHub上找到，名为`Semantic_segmentation.ipynb`）：
- en: 'Download the dataset, as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集，如下所示：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Read the images and their corresponding labels into separate lists, as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像及其对应标签分别读取到不同的列表中，如下所示：
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define a function that converts the three channel output images into 12 channels
    where there are 12 unique values of output:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将三个通道的输出图像转换为12个通道，其中有12个唯一的输出值：
- en: 'Extract the number of unique values (objects) that are present in the output,
    as follows:'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取输出中存在的唯一值（对象）的数量，如下所示：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Convert the masked image into a one-hot encoded version with as many channels
    as the number of objects in the total dataset, as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将掩模图像转换为一热编码版本，通道数量与数据集中对象的总数相同，如下所示：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Build the model:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型：
- en: 'Pass the images through the pre-trained VGG16 model, as follows:'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像传递给预训练的VGG16模型，如下所示：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Extract the VGG16 features of the image, as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取图像的VGG16特征，如下所示：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Pass the convolution features through up sampling layers and concatenate them
    to form a U-net architecture in a sim, as follows:'
  id: totrans-163
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将卷积特征通过上采样层传递，并将它们连接形成一个简单的U-net架构，如下所示：
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Freeze the VGG16 layers, as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结VGG16层，如下所示：
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Compile and fit the model, as follows:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并拟合模型，如下所示：
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/cae3a4fb-af64-4933-b45b-f0ace18dc9c3.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cae3a4fb-af64-4933-b45b-f0ace18dc9c3.png)'
- en: 'Predict on a test image, as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试图像进行预测，如下所示：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code results in an image where the predicted and actual semantic
    images are as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将生成一张图像，其中预测的语义图像与实际的语义图像如下所示：
- en: '![](img/9a872688-dd90-47ec-8d9c-675650960d84.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a872688-dd90-47ec-8d9c-675650960d84.png)'
- en: From the preceding images, we can see that we are able to accurately identify
    the semantic structures within an image with a high degree of accuracy (~90% for
    the model we trained).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像中可以看出，我们能够准确地识别图像中的语义结构，且准确度非常高（我们训练的模型约为90%）。
