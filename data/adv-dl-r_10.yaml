- en: Image Classification for Small Data Using Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习进行小数据集的图像分类
- en: In the previous chapters, we developed deep learning networks and explored various
    application examples related to image data. One major difference compared to what
    we will be discussing in this chapter is that, in the previous chapters, we developed
    models from scratch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们开发了深度学习网络，并探索了与图像数据相关的各种应用示例。与本章讨论的内容相比，前几章的一个主要区别是，我们在前几章中是从零开始开发模型的。
- en: Transfer learning can be defined as an approach where we reuse what a trained
    deep network has learned to solve a new but related problem. For example, we may
    be able to reuse a deep learning network that's been developed to classify thousands
    of different fashion items to develop a deep network to classify three different
    types of dresses. This approach is similar to what we can observe in real life,
    where a teacher transfers knowledge or learning gained over the years to students
    or a coach passes on learning or experience to new players. Another example is
    where learning to ride a bicycle is transferred to learning to ride a motorbike
    and this, in turn, becomes useful for learning how to drive a car.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习可以定义为一种方法，我们重新利用一个已训练的深度网络所学到的知识来解决一个新的但相关的问题。例如，我们可能能够重新利用一个用于分类成千上万种时尚单品的深度学习网络，来开发一个用于分类三种不同款式裙子的深度网络。这种方法类似于我们在现实生活中观察到的情况，教师将多年来获得的知识传授给学生，或教练将经验传授给新球员。另一个例子是，学会骑自行车的经验可以转移到学会骑摩托车，而这又可以用于学习如何开车。
- en: In this chapter, we will make use of pretrained deep networks while developing
    models for image classification. Pretrained models allow us to transfer useful
    features that we've learned from a much larger dataset to models we are interested
    in developing with a somewhat similar, but new and relatively smaller dataset.
    The use of pretrained models not only allows us to overcome issues as a result
    of the dataset being small, but also helps reduce the time and cost of developing
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在开发图像分类模型时使用预训练的深度网络。预训练模型使我们能够将从更大数据集中学到的有用特征转移到我们感兴趣的模型中，这些模型可能使用一个相似但全新的较小数据集进行开发。使用预训练模型不仅能帮助我们克服由于数据集较小而产生的问题，还能减少开发模型的时间和成本。
- en: 'To illustrate the use of pretrained image classification models, in this chapter,
    we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明预训练图像分类模型的使用，本章将涵盖以下主题：
- en: Using a pretrained model to identify an image
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型进行图像识别
- en: Working with the CIFAR10 dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CIFAR10数据集
- en: Image classification with CNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN进行图像分类
- en: Classifying images using the pretrained RESNET50 model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的RESNET50模型进行图像分类
- en: Model evaluation and prediction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Performance optimization tips and best practices
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: Using a pretrained model to identify an image
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练模型进行图像识别
- en: 'Before we proceed, let''s load three packages that we''ll need in this section:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们加载三个我们在本节中需要的包：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Keras and TensorFlow libraries will be used for developing the pretrained
    image classification model, while the EBImage library will be used for processing
    and visualizing image data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用Keras和TensorFlow库来开发预训练的图像分类模型，而EBImage库将用于处理和可视化图像数据。
- en: 'In Keras, the following pretrained image classification models are available:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，以下预训练的图像分类模型是可用的：
- en: Xception
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception
- en: VGG16
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG16
- en: VGG19
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG19
- en: ResNet50
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet50
- en: InceptionV3
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InceptionV3
- en: InceptionResNetV2
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InceptionResNetV2
- en: MobileNet
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileNet
- en: MobileNetV2
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileNetV2
- en: DenseNet
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: NASNet
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NASNet
- en: These pretrained models are trained on images from ImageNet ([http://www.image-net.org/](http://www.image-net.org/)).
    ImageNet is a huge image database that contains several million images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预训练模型是在ImageNet数据集上训练的（[http://www.image-net.org/](http://www.image-net.org/)）。ImageNet是一个庞大的图像数据库，包含了数百万张图像。
- en: 'We will start by using a pretrained model known as `resnet50` to identify an
    image. The following is the code we can use to utilize this pretrained model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用一个名为`resnet50`的预训练模型来识别图像。以下是我们可以用来利用这个预训练模型的代码：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we have specified `weights` as `"imagenet"`. This allows us to reuse the
    pretrained weights of the RESNET50 network. RESNET50 is a deep residual network
    that has a depth of 50 layers and includes convolutional neural network layers.
    Note that in case we only want to use the model architecture without the pretrained
    weights and we would like to train from scratch, then we can specify `weights`
    as `null`. By using `summary`, we can obtain the architecture of the RESNET50
    network. However, to conserve space, we do not provide any output from the summary.
    The total number of parameters in this network is 25,636,712\. The RESNET50 network
    is trained in using over a million images from ImageNet and has the capability
    to classify images into 1,000 different categories.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`weights`指定为`"imagenet"`，这允许我们重用RESNET50网络的预训练权重。RESNET50是一个深度残差网络，具有50层深度，包括卷积神经网络层。需要注意的是，如果我们只想使用模型架构而不使用预训练权重，且希望从头开始训练，我们可以将`weights`指定为`null`。通过使用`summary`，我们可以获得RESNET50网络的架构。不过，为了节省空间，我们不提供summary的输出。该网络的总参数数量为25,636,712。RESNET50网络在超过百万张来自ImageNet的图片上进行了训练，具备将图像分类到1,000个不同类别的能力。
- en: Reading an image
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取图像
- en: 'Let''s start by reading an image of a dog in RStudio. The following code loads
    an image file and then obtains the respective output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从RStudio中读取一张狗的图片。以下代码加载一张图像文件，然后获得相应的输出：
- en: When using the RESNET50 network, the maximum target size that's allowed is 224
    x 224 and the minimum target size that's allowed is 32 x 32.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RESNET50网络时，允许的最大目标大小为224 x 224，允许的最小目标大小为32 x 32。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding code, we can observe the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们可以观察到以下内容：
- en: A picture of a Norwich terrier dog is loaded from the computer desktop that's
    224 x 224 in size using the `image_load()` function from Keras.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras中的`image_load()`函数从计算机桌面加载一张大小为224 x 224的诺里奇梗犬图片。
- en: Note that the original image may not be 224 x 224 in size. However, specifying
    this dimension at the time of loading the image allows us to easily resize the
    original image so that it has new dimensions.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，原始图像可能不是224 x 224大小。然而，在加载图像时指定此尺寸可以让我们轻松调整原始图像的大小，使其具有新的维度。
- en: This image is converted into an array of numbers using the `image_to_array()`
    function. The structure of this array shows a dimension of 224 x 224 x 3.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这张图片通过`image_to_array()`函数被转换成数字数组。该数组的结构显示其维度为224 x 224 x 3。
- en: The summary of the array shows that it contains numbers between zero and 255.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数组的摘要显示它包含从零到255之间的数字。
- en: 'The following is the 224 x 224 color picture of a Norwich terrier dog. This
    can be obtained using a plot command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是224 x 224大小的诺里奇梗犬的彩色图片。可以使用plot命令来获得该图片：
- en: '![](img/c7f94f76-0669-4b3b-94fd-923433580bf8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7f94f76-0669-4b3b-94fd-923433580bf8.png)'
- en: The preceding image is a picture of a Norwich terrier dog sitting and looking
    forward. We will make use of this picture and check whether the RESNET50 model
    can accurately predict the type of dog in the picture.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图片是一只坐着并面向前方的诺里奇梗犬。我们将利用这张图片，检查RESNET50模型是否能准确预测图片中的狗的种类。
- en: 'A histogram that was developed from the values in the array is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从数组值生成的直方图：
- en: '![](img/6a64050d-273a-438d-b893-acc6f8172087.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a64050d-273a-438d-b893-acc6f8172087.png)'
- en: The preceding histogram of values in the array shows that the intensity values
    range from zero to 255, with most of the values concentrated around 200\. Next,
    we will preprocess the image data. This histogram can be used to compare the resulting
    changes to the image data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的数组值的直方图显示，强度值从零到255，大部分值集中在200左右。接下来，我们将对图像数据进行预处理。这个直方图可以用来比较图像数据的变化。
- en: Preprocessing the input
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对输入进行预处理
- en: 'We can now preprocess the input to prepare it so that it can be used with the
    pretrained RESNET50 model. The codes to preprocess the data are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以对输入数据进行预处理，以便与预训练的RESNET50模型一起使用。预处理数据的代码如下：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding code, we can observe the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们可以观察到以下内容：
- en: After applying the `array_reshape()` function, the dimensions of the array will
    change to 1 x 224 x 224 x 3.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用`array_reshape()`函数后，数组的维度将变为1 x 224 x 224 x 3。
- en: We used the `imagnet_preprocess_input()` function to prepare the data in the
    required format using the pretrained model.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`imagenet_preprocess_input()`函数来使用预训练模型准备所需格式的数据。
- en: 'A plot of the data in the form of a histogram after preprocessing is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理后数据的直方图如下所示：
- en: '![](img/e135198d-2aef-4e2d-8f41-9b507ea26fdf.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e135198d-2aef-4e2d-8f41-9b507ea26fdf.png)'
- en: The histogram of values after preprocessing shows a shift in location. Most
    of the values are now concentrated between 50 and 100\. However, there is no major
    change in the overall pattern of the histogram.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理后的值的直方图显示了位置的变化。大多数值现在集中在 50 到 100 之间。然而，直方图的整体模式没有发生重大变化。
- en: Top five categories
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前五个类别
- en: 'Now, we can use the pretrained model to make predictions by providing preprocessed
    image data as input. The code to achieve this is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用预训练模型通过提供预处理后的图片数据作为输入来进行预测。实现这一目标的代码如下：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding code, we can observe the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以观察到以下内容：
- en: The predictions are made using the `predict` function and contain probabilities
    for 1,000 different categories, out of which the top five categories with the
    highest probabilities are obtained using the `imagenet_decode_predictions()` function.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测是使用 `predict` 函数进行的，结果包含了 1,000 个不同类别的概率，最高的五个类别及其概率可以使用 `imagenet_decode_predictions()`
    函数获取。
- en: The highest score of about 0.7699 correctly identifies that the picture is of
    a Norwich terrier dog.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约 0.7699 的最高分数正确识别出图片是诺里奇梗犬。
- en: The second highest score is for the Norfolk terrier dog, which looks very similar
    to the Norwich terrier dog.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二高的分数是诺福克梗犬，它与诺里奇梗犬非常相似。
- en: The predictions also suggest that the picture could be of another type of terrier
    dog; however, those probabilities are relatively small or negligible.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测结果还表明，图片可能是另一种类型的梗犬；然而，这些概率相对较小或可以忽略不计。
- en: In the next section, we will look at a larger image dataset instead of a single
    image and use a pretrained network to develop an image classification model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将研究一个更大的图片数据集，而不是单张图片，并使用预训练的网络来开发图像分类模型。
- en: Working with the CIFAR10 dataset
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CIFAR10 数据集
- en: 'For illustrating the use of pretrained models with new data, we will make use
    of the CIFAR10 dataset. CIFAR stands for *Canadian Institute For Advanced Research*,
    and 10 refers to the 10 categories of images that are contained in the data. The
    CIFAR10 dataset is part of the Keras library and the code for obtaining it is
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何使用预训练模型处理新数据，我们将使用 CIFAR10 数据集。CIFAR 代表 *加拿大高级研究院*，而 10 指的是数据中包含的 10 类图片。CIFAR10
    数据集是 Keras 库的一部分，获取它的代码如下：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the preceding code, we can observe the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以观察到以下内容：
- en: We can read the dataset using the `dataset_cifar10()` function.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用 `dataset_cifar10()` 函数读取数据集。
- en: The structure of the data shows that there are 50,000 training images available
    with labels.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构显示有 50,000 张带标签的训练图片。
- en: It also contains 10,000 test images with labels.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还包含 10,000 张带标签的测试图片。
- en: 'Next, we will extract the train and test data from CIFAR10 using the following
    code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码从 CIFAR10 中提取训练和测试数据：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到以下内容：
- en: We saved the training image data in `trainx` and the test image data in `testx`.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将训练图片数据保存在 `trainx` 中，测试图片数据保存在 `testx` 中。
- en: We also carry out one-hot encoding of the train and test data labels using the `to_categorical()` 
    function and save the results in `trainy` and `testy`, respectively.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还使用 `to_categorical()` 函数对训练集和测试集的标签进行独热编码，并将结果分别保存在 `trainy` 和 `testy` 中。
- en: The table for the training data indicates that the images are classified in
    10 different categories, with each category containing exactly 5,000 images.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据表明，这些图片被分类为 10 个不同的类别，每个类别包含 5,000 张图片。
- en: Similarly, the test data contains exactly 1,000 images for each of the 10 categories.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，测试数据也包含了每个类别 1,000 张图片，共 10 个类别。
- en: 'As an example, the labels for the first 64 images in the training data can
    be obtained using the following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，可以使用以下代码获取训练数据中前 64 张图片的标签：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we can see, each picture is labeled using a number between 0 and 9\. A description
    of the 10 different categories of images can be seen in the following table:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，每张图片都使用 0 到 9 之间的数字进行标签。下表展示了 10 种不同类别图片的描述：
- en: '| Label | Description |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 描述 |'
- en: '| --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | Airplane |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 飞机 |'
- en: '| 1 | Automobile |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 汽车 |'
- en: '| 2 | Bird |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 鸟类 |'
- en: '| 3 | Cat |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 猫 |'
- en: '| 4 | Deer |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 鹿 |'
- en: '| 5 | Dog |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 狗 |'
- en: '| 6 | Frog |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 青蛙 |'
- en: '| 7 | Horse |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 马 |'
- en: '| 8 | Ship |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Ship |'
- en: '| 9 | Truck |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Truck |'
- en: Note that there is no overlap between these 10 categories. For example, the
    automobile category refers to cars and SUVs, whereas the truck category only refers
    to large trucks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这 10 个类别之间没有重叠。例如，汽车类别指的是轿车和 SUV，而卡车类别仅指大型卡车。
- en: Sample images
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例图像
- en: 'The first 64 images from the training data of CIFAR10 can be plotted using
    the following code. Doing this, we can get a glimpse of the type of images contained
    in the dataset:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码绘制 CIFAR10 训练数据中的前 64 张图像。通过这样做，我们可以初步了解数据集中包含的图像类型：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The images from CIFAR10 are all 32 x 32 color images. The following plot shows
    64 images in an 8 x 8 grid:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR10 数据集中的所有图像都是32 x 32的彩色图像。下图展示了一个 8 x 8 网格中的 64 张图像：
- en: '![](img/1db8a05c-8153-45ea-8868-1d191620f6b8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1db8a05c-8153-45ea-8868-1d191620f6b8.png)'
- en: From the preceding images, we can see that these images come with various backgrounds
    and are of low resolution. In addition, sometimes, these images aren't completely
    visible, which makes image classification a challenging task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像中，我们可以看到这些图像有各种背景，并且分辨率较低。此外，有时这些图像并不完全可见，这使得图像分类变得具有挑战性。
- en: Preprocessing and prediction
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理与预测
- en: 'We can use the pretrained RESNET50 model to identify the second image in the
    training data. Note that, since this second image in the training data is 32 x
    32 in size, whereas RESNET50 is trained on images that are 224 x 224 in size,
    we need to resize the image before applying the code that we have used earlier.
    The following code is used for identifying the image:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用预训练的 RESNET50 模型来识别训练数据中的第二张图像。请注意，由于训练数据中的第二张图像大小为 32 x 32，而 RESNET50
    模型是在 224 x 224 的图像上训练的，因此我们需要在应用之前使用的代码前对图像进行调整大小。以下代码用于识别该图像：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: From the preceding code, we can observe that the top category with a score of
    0.9988 is for a moving van. The scores for the other four categories are comparatively
    negligible.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到，得分最高的类别是 0.9988 的搬家车。其他四个类别的得分相对较低。
- en: Image classification with CNN
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNN 进行图像分类
- en: In this section, we will use a subset of the CIFAR10 dataset to develop a convolutional
    neural network-based image classification model and assess its classification
    performance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 CIFAR10 数据集的一个子集来开发一个基于卷积神经网络的图像分类模型，并评估其分类性能。
- en: Data preparation
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'We will keep the data size smaller by using only the first 2,000 images in
    the training and test data from CIFAR10\. This will allow the image classification
    model to be run on a regular computer or laptop. We will also resize the training
    and test images from 32 x 32 dimensions to 224 x 224 dimensions to be able to
    compare classification performance with the pretrained model. The following code
    includes the necessary preprocessing that we went over earlier in this chapter:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过仅使用 CIFAR10 训练和测试数据中的前 2,000 张图像来保持数据大小较小。这将使得图像分类模型能够在普通计算机或笔记本上运行。我们还将把训练和测试图像从
    32 x 32 的尺寸调整到 224 x 224 的尺寸，以便能够与预训练模型比较分类性能。以下代码包括了我们在本章之前讲解的必要预处理：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding code, while resizing dimensions from 32 x 32 to 224 x 224,
    we use bilinear interpolation, which is included as part of the EBImage package.
    Bilinear interpolation extends linear interpolation to two variables, which in
    this case is the height and width of an image. The effect of bilinear interpolation
    can be observed from the before and after images of the truck shown in the following
    image:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，在将图像尺寸从 32 x 32 调整到 224 x 224 时，我们使用了双线性插值，这是 EBImage 包的一部分。双线性插值是将线性插值扩展到两个变量，在本例中是图像的高度和宽度。双线性插值的效果可以通过下图中显示的卡车的前后图像来观察：
- en: '![](img/b87b0400-79fc-43ed-b4d3-bfc2b466229a.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b87b0400-79fc-43ed-b4d3-bfc2b466229a.png)'
- en: Here, we can see that the after image (second image) looks smoother as it contains
    more pixels compared to the original image (first image).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，第二张图像（后图）看起来更加平滑，因为它包含的像素更多，相较于原始图像（第一张图像）。
- en: CNN model
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 模型
- en: 'We will start by using a not-so-deep convolutional neural network to develop
    an image classification model. We will use the following code for this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从使用一个不那么深的卷积神经网络开始，开发一个图像分类模型。我们将使用以下代码：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到以下内容：
- en: The total number of parameters in this network is 99,136,170.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该网络的总参数数为 99,136,170。
- en: When compiling the model, we use `categorical_crossentropy` as the loss function
    since the response has 10 categories.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编译模型时，我们使用`categorical_crossentropy`作为损失函数，因为响应有10个类别。
- en: For the optimizer, we specify `rmsprop`, which is a gradient-based optimization
    method and is a popular choice that provides reasonably good performance.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于优化器，我们指定了`rmsprop`，这是一种基于梯度的优化方法，是一种广泛使用的选择，能够提供相当不错的性能。
- en: We train the model with 10 epochs and with a batch size of 10.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用10个周期和批次大小为10来训练模型。
- en: Out of 2,000 images in the training data, 20% (or 400 images) is used for assessing
    validation errors and the remaining 80% (or 1,600 images) is used for training.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2,000张训练数据图像中，20%（即400张图像）用于评估验证错误，剩余80%（即1,600张图像）用于训练。
- en: 'A plot of the accuracy and loss values after training the model is as follows
    for `model_one`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对`model_one`训练后准确率和损失值的图表如下：
- en: '![](img/a382f094-6b43-4c2c-9725-6168925657f7.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a382f094-6b43-4c2c-9725-6168925657f7.png)'
- en: 'From the preceding plot, the following observations can be made:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图表中，可以得出以下观察结果：
- en: The plot of the accuracy and loss values shows that, after about 4 epochs, the
    loss and accuracy values for both training and validation data remain more or
    less constant.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率和损失值的图表显示，在大约4个周期后，训练数据和验证数据的损失和准确率值保持相对稳定。
- en: Although the accuracy for the training data reaches high values closer to 100%,
    there seems to be no impact on the accuracy based on the images in the validation
    data.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管训练数据的准确率接近100%的高值，但验证数据中的图像似乎对准确率没有影响。
- en: In addition, the gap between the accuracy of the training and validation data
    seems to be high, suggesting the presence of overfitting. When assessing the model's
    performance, we expect to see low accuracy in terms of image classification by
    the model.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，训练数据和验证数据的准确率差距似乎较大，表明存在过拟合问题。在评估模型性能时，我们预计图像分类的准确率较低。
- en: Note that developing a decent image classification model using CNN requires
    a large number of images for training, and therefore more time and resources.
    Later in this chapter, we will learn how to use pretrained networks to help us
    overcome this problem. For now, though, let's proceed by assessing the image classification
    model's performance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用CNN开发一个合适的图像分类模型需要大量的图像进行训练，因此需要更多的时间和资源。在本章后面，我们将学习如何使用预训练网络来帮助我们克服这个问题。不过，现在我们先继续评估图像分类模型的性能。
- en: Model performance
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: For assessing the model's performance, we will carry out calculations for the
    loss, accuracy, and confusion matrix for the training and test data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的性能，我们将对训练数据和测试数据的损失、准确率和混淆矩阵进行计算。
- en: Performance assessment with training data
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练数据进行性能评估
- en: 'The code for obtaining the loss, accuracy, and confusion matrix based on the
    training data is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 获取基于训练数据的损失、准确率和混淆矩阵的代码如下：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we can see that the loss and accuracy values for the training data are
    3.335 and 0.846, respectively. The confusion matrix shows decent results based
    on the training data. However, for some types of images, misclassifications are
    high. For example, 12 images from category 7 (horse) are misclassified as category-9
    (truck). Similarly, 11 images, each belonging to category-6 (frog) and category-8
    (ship), are also misclassified as category-9 (truck).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到训练数据的损失和准确率值分别为3.335和0.846。混淆矩阵显示出基于训练数据的不错结果。然而，对于某些类型的图像，误分类率较高。例如，来自类别7（马）的12张图像被误分类为类别9（卡车）。类似地，11张分别属于类别6（青蛙）和类别8（船）的图像，也被误分类为类别9（卡车）。
- en: Performance assessment with test data
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用测试数据进行性能评估
- en: 'The code for obtaining the loss, accuracy, and confusion matrix based on the
    test data is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 获取基于测试数据的损失、准确率和混淆矩阵的代码如下：
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'From the preceding output, the following observations can be made:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的输出中，可以得出以下观察结果：
- en: The loss and accuracy values for the test data are 16.456 and 0.232, respectively.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的损失和准确率值分别为16.456和0.232。
- en: These results are not as impressive as what we observed for the training data
    due to the overfitting problem.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于过拟合问题，这些结果没有训练数据中那么令人印象深刻。
- en: Although we can try and develop a deeper network in an effort to improve image
    classification results or to try and increase training data to provide more samples
    to learn from, here, we will make use of pretrained networks to obtain better
    results.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以尝试开发一个更深的网络，以改善图像分类结果，或尝试增加训练数据以提供更多的样本进行学习，但在这里，我们将利用预训练网络来获得更好的结果。
- en: Classifying images using the pretrained RESNET50 model
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的RESNET50模型进行图像分类
- en: In this section, we will make use of the pretrained RESNET50 model to develop
    an image classification model. We will use the same training and test data that
    we used in the previous section to make comparing classification performance easier.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用预训练的RESNET50模型来开发一个图像分类模型。我们将使用与前一节相同的训练和测试数据，这样便于比较分类性能。
- en: Model architecture
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: We will upload the RESNET50 model without including the top layer. This will
    help us customize the pretrained model for use with CIFAR10 data. Since the RESNET50
    model is trained with the help of over 1 million images, it captures useful features
    and representations of images that can be reused with new but similar and smaller
    data. This reusability aspect of pretrained models not only helps to reduce the
    time and cost of developing an image classification model from scratch, but is
    especially useful when the training data is relatively small.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将上传不包含顶层的RESNET50模型。这将帮助我们定制预训练模型，以便与CIFAR10数据一起使用。由于RESNET50模型是通过超过100万张图像训练得到的，它捕捉到了有用的特征和图像表示，可以与新的但相似且较小的数据一起重用。预训练模型的这种可重用性不仅有助于减少从头开始开发图像分类模型的时间和成本，而且在训练数据相对较小时尤其有用。
- en: 'The code that''s used for developing the model is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 用于开发模型的代码如下：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When uploading the RESNET50 model, the input dimensions for the data based on
    color images are specified as 224 x 224 x 3\. Although smaller dimensions will
    work too, image dimensions cannot be less than 32 x 32 x 3\. Images in the CIFAR10
    dataset have dimensions of 32 x 32 x 3, but we have resized them to 224 x 224
    x 3 as it gives us better image classification accuracy.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上传RESNET50模型时，基于彩色图像的数据输入维度指定为224 x 224 x 3。尽管较小的维度也能工作，但图像维度不能小于32 x 32 x 3。CIFAR10数据集中的图像维度是32
    x 32 x 3，但我们已经将它们调整为224 x 224 x 3，因为这样可以提高图像分类的准确性。
- en: 'From the preceding summary, we can observe the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的总结中，我们可以观察到以下几点：
- en: The output dimensions from the RESNET50 network are 7 x 7 x 2,048.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RESNET50网络的输出维度是7 x 7 x 2,048。
- en: We use a flattened layer to change the output shape to a single column with
    7 x 7 x 2,048 = 100,352 elements.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个扁平层将输出形状更改为一个单列，其中7 x 7 x 2,048 = 100,352个元素。
- en: A dense layer with 256 units and a `relu` activation function is added.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了一个具有256个单元的密集层，并使用`relu`激活函数。
- en: This dense layer leads to (100,353 x 256) + 256 = 25,690,368 parameters.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个密集层导致了（100,353 x 256）+ 256 = 25,690,368个参数。
- en: The last dense layer has 10 units for images with 10 categories and a `softmax`
    activation function. This network has a total of 49,280,650 parameters.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个密集层具有10个单元，用于10个类别的图像，并使用`softmax`激活函数。该网络共有49,280,650个参数。
- en: Out of the total parameters in the network, 49,227,530 are trainable parameters.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中总参数中，有49,227,530个是可训练的参数。
- en: Although we can train the network with all of these parameters, this is not
    advisable. Training and updating parameters related to the RESNET50 network will
    cause us to lose the benefits that we would get as a result of the features that
    have been learned from over 1 million images. We are only using data from 2,000
    images for training and have 10 different categories. So, for each category, we
    only have approximately 200 images. Therefore, it is important to freeze the weights
    in the RESNET50 network, which will allow us to obtain the benefits of using a
    pretrained network.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以使用这些所有参数来训练网络，但这并不建议。训练和更新与RESNET50网络相关的参数会使我们失去从超过100万张图像中学习到的特征带来的好处。我们只使用了2,000张图像的数据进行训练，并且有10个不同的类别。因此，每个类别只有大约200张图像。因此，冻结RESNET50网络中的权重非常重要，这将使我们能够获得使用预训练网络的好处。
- en: Freezing pretrained network weights
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冻结预训练网络的权重
- en: 'The code for freezing the weights of the RESNET50 network and then compiling
    the model is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结RESNET50网络权重并编译模型的代码如下：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, we can observe the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以观察到以下几点：
- en: To freeze the weights in the RESNET50 network, we use the `freeze_weights()`
    function.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了冻结RESNET50网络中的权重，我们使用`freeze_weights()`函数。
- en: Note that after freezing the pretrained network weights, the model needs to
    be compiled.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，在冻结预训练网络权重之后，模型需要重新编译。
- en: After freezing the weights of the RESNET50 network, we observe that the number
    of trainable parameters goes down from 49,227,530 to a lower value of 25,692,938.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在冻结了RESNET50网络的权重后，我们观察到可训练的参数数量从49,227,530下降到较低的25,692,938。
- en: These parameters belong to the two dense layers that we added and will help
    us customize the results from the RESNET50 network so that we can apply them to
    the images from the CIFAR10 data that we are using.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些参数属于我们添加的两个全连接层，将帮助我们定制来自RESNET50网络的结果，以便我们能够将其应用于我们正在使用的CIFAR10数据的图像。
- en: Fitting the model
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'The code for fitting the model is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合模型的代码如下：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的代码中，我们可以观察到以下情况：
- en: We train the network with 10 epochs and with a batch size of 10.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用10个epoch和10的批次大小训练网络。
- en: We specify 20% (or 400 images) to be used for assessing the validation loss
    and validation accuracy, and the remaining 80% (or 1,600 images) for training.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们指定20%（或400张图像）用于评估验证损失和验证准确率，剩余的80%（或1,600张图像）用于训练。
- en: 'The plot of the accuracy and loss values after training the model is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后的准确率和损失值图表如下所示：
- en: '![](img/5deac824-d6ec-4195-917a-6a696101b72d.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5deac824-d6ec-4195-917a-6a696101b72d.png)'
- en: 'From the plot for the loss and accuracy values, we can make the following observations:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从损失和准确率值的图表中，我们可以得出以下观察结果：
- en: There is an important difference compared to the previous plot, where the pretrained
    model wasn't used. This plot shows us that the model reaches an accuracy of over
    60% by the second epoch itself compared to the previous plot, where it remained
    below 25%. Thus, we can see that the use of a pretrained model has an immediate
    impact on image classification.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与之前没有使用预训练模型的图表相比，这里有一个重要的区别。这个图表显示，在第二个epoch时，模型的准确率超过了60%，而在之前的图表中，它的准确率一直低于25%。由此我们可以看到，使用预训练模型对图像分类有直接影响。
- en: The improvements based on validation data are slow compared to those for the
    training data.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于验证数据的改进相比于训练数据来说较为缓慢。
- en: Although the accuracy values based on the validation data show gradual improvement,
    the loss values for the validation data show more variability.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管基于验证数据的准确率值逐渐提高，但验证数据的损失值显示出更多的波动。
- en: In the next section, we will evaluate the model and assess its prediction performance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将评估模型并评估其预测性能。
- en: Model evaluation and prediction
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Now, we will evaluate the performance of this model for the training and test
    data. Calculations relating to the loss, accuracy, and confusion matrix will be
    carried out so that we can evaluate the model image's classification performance.
    We will also obtain the accuracy for each of the 10 categories.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将评估该模型在训练数据和测试数据上的表现。将进行关于损失、准确率和混淆矩阵的计算，以便我们评估模型在图像分类方面的表现。我们还将获取10个类别中每个类别的准确率。
- en: Loss, accuracy, and confusion matrix with the training data
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练数据的损失、准确率和混淆矩阵
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the training
    data is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据的损失、准确率和混淆矩阵的代码如下：
- en: '[PRE17]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'From the preceding output, we can make the following observations:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的输出中，我们可以得出以下观察结果：
- en: The loss and accuracy based on the training data are 1.954 and 0.879, respectively.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于训练数据的损失和准确率分别为1.954和0.879。
- en: Both of these numbers are an improvement over the corresponding results based
    on the previous model.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个数值都优于之前模型中对应的结果。
- en: The confusion matrix shows a decent image classification performance.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵显示了相当不错的图像分类表现。
- en: The best image classification performance is seen for category 9 (truck), where
    only one image is misclassified as category-0 (airplane) and provides an accuracy
    of 99.5%.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第9类（卡车）中，图像分类表现最好，只有一张图像被错误分类为第0类（飞机），并且提供了99.5%的准确率。
- en: This model is the most confused regarding category-3 (cat), which is mostly
    classified as category-5 (dog) or category-7 (horse) and provides an accuracy
    of only 68.2% for this category.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型在第3类（猫）上表现最差，这类图像通常被错误分类为第5类（狗）或第7类（马），并且该类别的准确率仅为68.2%。
- en: Among the misclassifications, the highest case (35 images) is when category-1
    (automobile) is misclassified as category-9 (truck).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在错误分类中，最多的是类别-1（汽车）被错误分类为类别-9（卡车），共有35张图片。
- en: Next, we will assess the model's performance using the test data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用测试数据评估模型的性能。
- en: Loss, accuracy, and confusion matrix with the test data
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于测试数据的损失、准确率和混淆矩阵
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the test
    data is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 获取测试数据的损失、准确率和混淆矩阵的代码如下：
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From the preceding output, we can make the following observations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出中，我们可以得出以下观察结果：
- en: The loss and accuracy based on the test data are 4.437 and 0.768, respectively.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于测试数据的损失和准确率分别为4.437和0.768。
- en: Although this performance based on the test data is inferior to the results
    based on the training data, it is a significant improvement over the results from
    the first model.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管基于测试数据的性能不如基于训练数据的结果，但相比于第一个模型的结果，这已是一个显著的改进。
- en: The confusion matrix provides further insights into the model's performance.
    The best performance is for category 9 (truck), with 199 correct classifications
    and an accuracy of 98%.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵提供了对模型性能的进一步见解。最佳性能出现在类别9（卡车），共有199个正确分类，准确率为98%。
- en: For the test data, the model seems to be the most confused regarding category-3
    (cat), which has the most misclassifications. The accuracy of this category can
    be as low as 43.2%.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试数据，模型在类别-3（猫）上最容易混淆，这一类别的误分类最多，准确率低至43.2%。
- en: The highest misclassification for a single category (54 images) is for category-1
    (automobile), which is misclassified as category-9 (truck).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一类别的最大误分类情况（54张图片）出现在类别-1（汽车），它被错误分类为类别-9（卡车）。
- en: With 76.8% accuracy, we can say that this image classification performance is
    decent. The use of a pretrained model has allowed us to transfer our learning
    of a model trained on data involving over 1 million images to new data containing
    2,000 images from the CIFAR10 dataset. This is a huge advantage compared to building
    an image classification model totally from scratch, which would involve more time
    and computing costs. Now that we've achieved a decent performance from the model,
    we can explore how to improve this even further.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以76.8%的准确率来看，可以说这次图像分类的性能还算不错。使用预训练模型让我们得以将训练过的包含超过100万张图像的数据模型的学习迁移到包含2000张CIFAR10数据集图像的新数据上。这相比于从零开始构建图像分类模型要有很大优势，后者需要更多的时间和计算成本。现在我们已经取得了不错的性能，接下来我们可以探讨如何进一步提升它。
- en: Performance optimization tips and best practices
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: To explore further image classification improvement, in this section, we will
    try three experiments. In the first experiment, we will mainly use the `adam`
    optimizer when compiling the model. In the second experiment, we will carry out
    hyperparameter tuning by varying the number of units in the dense layer, the dropout
    percentage in the dropout layer, and the batch size when fitting the model. Finally,
    in the third experiment, we will work with another pretrained network called VGG16.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探索图像分类的提升，在这一部分，我们将进行三个实验。第一个实验中，我们将主要使用`adam`优化器来编译模型。在第二个实验中，我们将通过调整密集层单元数、丢弃层的丢弃率以及拟合模型时的批量大小来进行超参数调优。最后，在第三个实验中，我们将使用另一种预训练网络——VGG16。
- en: Experimenting with the adam optimizer
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用adam优化器进行实验
- en: In this first experiment, we will use the `adam` optimizer when compiling the
    model. At the time of training the model, we will also increase the number of
    epochs to 20.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个实验中，我们将使用`adam`优化器来编译模型。在训练模型时，我们还将把训练轮数增加到20。
- en: 'The plot of the accuracy and loss values after training the model is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后的准确率和损失值图如下：
- en: '![](img/f0987325-56b5-4aa2-ad2a-d7ea843fb6f4.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0987325-56b5-4aa2-ad2a-d7ea843fb6f4.png)'
- en: The preceding loss and accuracy plot for this model shows that the values related
    to the training data are flat after about six epochs. For the validation data,
    the loss values show a gradual increase, whereas the accuracy values are flat
    after the third epoch.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型的损失和准确率图显示，训练数据的相关值在大约六个训练轮后趋于平稳。而对于验证数据，损失值呈逐渐上升趋势，而准确率在第三个训练轮后趋于平稳。
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the test
    data is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 获取测试数据的损失、准确率和混淆矩阵的代码如下：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'From the preceding output, we can make the following observations:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出中，我们可以做出以下观察：
- en: The loss and accuracy of the test data are 4.005 and 0.772, respectively.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的损失和准确率分别为4.005和0.772。
- en: These results are marginally better than they are for `model_two`.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些结果相比于`model_two`略有改善。
- en: The confusion matrix shows a somewhat different image classification pattern
    compared to the previous model.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵显示了与前一个模型相比，稍有不同的图像分类模式。
- en: The best classification results are obtained for category 8 (ship), with 205
    correct image classifications out of 217 (94.5% accuracy).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好的分类结果出现在类别8（船）上，在217张图像中正确分类了205张（准确率94.5%）。
- en: The lowest classification performance is for category 3 (cat), with 80 correct
    predictions out of 199 (40.2% accuracy).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类性能最差的是类别3（猫），在199张图像中正确预测了80张（准确率40.2%）。
- en: The worst misclassification is of 58 images from category 3 (cat) when they
    are misclassified as category-5 (dog).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最严重的误分类是58张来自类别3（猫）的图片，它们被误分类为类别5（狗）。
- en: Next, we will carry out an experiment with hyperparameter tuning.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行超参数调优实验。
- en: Hyperparameter tuning
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调优
- en: 'In this experiment, we will vary the units in the dense layer, the dropout
    rate, and the batch size to obtain values that help us improve classification
    performance. This also illustrates an efficient way of obtaining suitable parameter
    values through experimentation. We will start by creating a `TransferLearning.R` file
    using the following code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们将改变密集层中的单元数、dropout率和批次大小，以获得能够提高分类性能的值。这也展示了一种通过实验获取适当参数值的高效方式。我们将从使用以下代码创建`TransferLearning.R`文件开始：
- en: '[PRE20]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, after reading the pretrained model, we declare three
    flags for the parameters that we want to experiment with. Now, we can use these
    flags in the model architecture (dense units and dropout rate) and in the code
    for fitting the model (batch size). We have reduced the number of epochs to five
    and for the optimizer, while compiling the model, we retain `adam`. We will save
    this R file, which we'll call `TransferLearning.R`, on the desktop of our computer.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，读取预训练模型后，我们声明了三个用于实验的参数标志。现在，我们可以在模型架构（密集单元和dropout率）中使用这些标志，并在拟合模型的代码中使用（批次大小）。我们已将训练周期减少为5，并且在编译模型时保留了`adam`优化器。我们将把这个R文件保存为`TransferLearning.R`，并保存在计算机桌面上。
- en: 'The code for running this experiment is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此实验的代码如下：
- en: '[PRE21]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, we can see that the working directory is set at the location
    of the `TransferLearning.R` file. Note that the output from this experiment will
    be saved in this directory too. For running the hyperparameter tuning experiment,
    we will use the `tfruns` library. For the number of units in the dense layer,
    we will try 256 and 512 as the values. For the dropout rate, we will experiment
    with 0.1 and 0.3\. Fnally, for the batch size, we will try 10 and 30\. With three
    parameters, each being tried at two values, the total number of experimental runs
    will be 2³ = 8.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们可以看到工作目录已设置为`TransferLearning.R`文件所在的位置。请注意，本实验的输出也将保存在此目录中。为了运行超参数调优实验，我们将使用`tfruns`库。对于密集层中的单元数，我们将尝试256和512作为值。对于dropout率，我们将尝试0.1和0.3。最后，对于批次大小，我们将尝试10和30。考虑到有三个参数，每个参数有两个可能的值，总的实验运行次数为2³
    = 8。
- en: 'An extract from the results that were obtained from this experiment is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从本次实验获得的结果摘录如下：
- en: '[PRE22]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding output shows the loss and accuracy values based on the validation
    data for all eight experimental runs. For easy reference, it also includes parameter
    values. We can make the following observations from the preceding output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了基于验证数据的损失和准确率值，适用于所有8次实验运行。为了方便参考，输出还包括了参数值。我们可以从上述输出中做出以下观察：
- en: The highest accuracy value (row 3) is obtained when the number of dense units
    is 512, the dropout rate is 0.1, and the batch size is 30.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当密集单元数量为512，dropout率为0.1，批次大小为30时，获得了最高的准确率值（第3行）。
- en: On the other hand, the lowest accuracy value (row 6) is obtained when the number
    of dense units is 256, the dropout rate is 0.3, and the batch size is 10.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，当密集单元数量为256，dropout率为0.3，批次大小为10时，获得了最低的准确率值（第6行）。
- en: 'The code for obtaining the loss, accuracy, and confusion matrix using the test
    data for row 3 of the experiment is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 获取损失、准确率和混淆矩阵的代码如下，适用于实验第3行的测试数据：
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'From the preceding results, we can make the following observations:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的结果中，我们可以得出以下观察结论：
- en: Both the loss and accuracy values for the test data are better than the results
    we've obtained so far.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的损失和准确率均优于我们迄今为止获得的结果。
- en: The best classification results are obtained for category 8 (ship), with 203
    correct image classifications out of 217 (93.5% accuracy).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好的分类结果来自类别8（船），在217张图像中正确分类了203张（准确率93.5%）。
- en: The lowest classification performance is for category 3 (cat), with 92 correct
    predictions out of 199 (46.2% accuracy).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类性能最低的是类别3（猫），在199张图像中正确预测了92张（准确率46.2%）。
- en: The worst misclassification is of 50 images from category 3 (cat) when they
    are misclassified as category-5 (dog).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最严重的误分类是将50张类别3（猫）图像误分类为类别5（狗）。
- en: 'In the next experiment, we will use another pretrained network: VGG16.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次实验中，我们将使用另一个预训练网络：VGG16。
- en: Experimenting with VGG16 as a pretrained network
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VGG16作为预训练网络进行实验
- en: 'In this experiment, we will use a pretrained network called VGG16\. VGG16 is
    a convolutional neural network that is 16 layers deep and can classify images
    into thousands of categories. This network is also trained using over 1 million
    images from the ImageNet database. The code for the model''s architecture and
    compiling and then fitting the model is as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次实验中，我们将使用一个名为VGG16的预训练网络。VGG16是一个16层深的卷积神经网络，可以将图像分类到成千上万的类别。这个网络还使用来自ImageNet数据库的超过100万张图像进行了训练。模型架构、编译代码及训练代码如下：
- en: '[PRE24]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'From the preceding summary, we can observe the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的总结中，我们可以观察到以下几点：
- en: This model has 21,140,042 parameters, which, after freezing the weights of VGG16,
    goes down to a total of 6,425,354 trainable parameters.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型有21,140,042个参数，在冻结VGG16的权重后，训练参数总数降至6,425,354个。
- en: When compiling the model, we retain the use of the `adam` optimizer.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编译模型时，我们保留了使用`adam`优化器。
- en: In addition, we run 10 epochs to train the model. All the other settings are
    the same ones that we used for the previous models.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们运行了10个epoch来训练模型。所有其他设置与我们之前使用的模型相同。
- en: 'A plot of the accuracy and loss values after training the model is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后的准确率和损失值的图表如下：
- en: '![](img/57485ef8-cf6f-45da-be86-19fd0123cf60.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57485ef8-cf6f-45da-be86-19fd0123cf60.png)'
- en: The preceding loss and accuracy plot for the training and validation data indicates
    that, after about four epochs, the model performance remains flat. This is in
    contrast to the previous model, where the loss values for the validation data
    showed a gradual increase.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的训练和验证数据的损失与准确率图表表明，在大约四个epoch后，模型的表现趋于平稳。与之前的模型相比，验证数据的损失值呈现逐渐上升的趋势。
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the test
    data is as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 获取测试数据的损失、准确率和混淆矩阵的代码如下：
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'From the preceding output, we can make the following observations:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以得出以下观察结论：
- en: The loss and accuracy of the test data are 1.674 and 0.757, respectively.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的损失和准确率分别为1.674和0.757。
- en: The confusion matrix provides further insights. This model has the best classification
    accuracy of 88.9% when classifying category 6 (frog).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵提供了更多的洞察。这款模型在分类类别6（青蛙）时具有最佳分类准确率88.9%。
- en: On the other hand, the accuracy when classifying category 4 (deer) images is
    only about 59.6%.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，分类类别4（鹿）图像时的准确率仅为约59.6%。
- en: 'In this section, we experimented with three situations:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行了三种情况的实验：
- en: The use of the `adam` optimizer improved the results a little bit and provided
    test data accuracy of about 77.2%.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`adam`优化器稍微改善了结果，并提供了约77.2%的测试数据准确率。
- en: In the second experiment, hyperparameter tuning provided the best results for
    the number of dense units at 512, a dropout rate at 0.1, and a batch size at 30\.
    This combination of parameters helped us obtain a test data accuracy of about
    79.8%.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二次实验中，超参数调优提供了最佳结果，512个密集单元、0.1的dropout率和30的批量大小。这组参数帮助我们获得了大约79.8%的测试数据准确率。
- en: The third experiment, where we used the VGG16 pretrained network, also provided
    decent results. However, it provided test data accuracy of slightly lower than
    75.7%.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三次实验，我们使用了VGG16预训练网络，同样获得了不错的结果。然而，测试数据准确率略低于75.7%。
- en: Another approach when working with smaller datasets is to use data augmentation.
    In this approach, the existing images are modified (by flipping, rotation, shifting,
    and so on) to create new samples. Since images in image datasets aren't always
    centered, such artificially created new samples help us to learn about useful
    features that, in turn, improve image classification performance.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 处理较小数据集的另一种方法是使用数据增强。在这种方法中，通过翻转、旋转、平移等方式修改现有图像，从而创建新的样本。由于图像数据集中的图像并不总是居中的，这种人工创建的新样本有助于我们学习到有用的特征，从而提高图像分类性能。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we illustrated the use of pretrained deep neural networks for
    developing image classification models. Such pretrained networks, which are trained
    using over 1 million images, capture reusable features that can be applied to
    similar but new data. This aspect becomes valuable when developing image classification
    models with relatively smaller datasets. In addition, they provide savings in
    terms of the use of computational resources and time. We started by making use
    of the RESNET50 pretrained network to identify an image of a Norwich terrier dog.
    Subsequently, we made use of 2,000 images from the CIFAR10 dataset to illustrate
    the usefulness of applying pretrained networks to a relatively smaller dataset.
    The initial convolutional neural networks model that we built from scratch suffered
    from overfitting and did not yield useful results.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们展示了如何使用预训练的深度神经网络来开发图像分类模型。这些预训练网络经过超过100万张图像的训练，捕获了可重复使用的特征，可以应用于类似但新的数据。当使用相对较小的数据集开发图像分类模型时，这一特性尤为重要。此外，预训练网络还节省了计算资源和时间。我们首先使用了RESNET50预训练网络来识别一只诺里奇梗犬的图像。随后，我们利用来自CIFAR10数据集的2,000张图像，展示了将预训练网络应用于相对较小数据集的有效性。我们从头开始构建的初始卷积神经网络模型由于过拟合未能产生有用的结果。
- en: Next, we used the pretrained RESNET50 network and customized it to suit our
    needs by adding two dense layers on top of the pretrained network. We obtained
    decent results, with a test data accuracy of about 76.8%. Although pretrained
    models can provide faster results that require fewer epochs, we need to explore
    improvements that we can make to the model's performance with the help of some
    experimentation. In an effort to explore better results, we experimented with
    the `adam` optimizer, which yielded test data accuracy of about 77.2%. We also
    carried out hyperparameter tuning, which yielded the best levels in terms of the
    number of units in the dense layer, which was 512, the dropout rate in the dropout
    layer, which was 0.1, and the batch size at the time of fitting the model, which
    was 30\. The image classification accuracy with this combination yielded a test
    data accuracy of about 79.8%. Finally, we experimented with the pretrained VGG16
    network, where we obtained test data accuracy of about 75.6%. These experiments
    illustrated how we can explore and improve model performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用了预训练的RESNET50网络，并通过在预训练网络上添加两个密集层来定制它以满足我们的需求。我们取得了不错的结果，测试数据的准确率约为76.8%。尽管预训练模型可以提供更快的结果，并且需要较少的训练轮次，但我们需要探索通过一些实验来提高模型性能。为了探索更好的结果，我们尝试了`adam`优化器，得到了约77.2%的测试数据准确率。我们还进行了超参数调优，得到了最佳的超参数设置，其中密集层的单元数为512，dropout层的丢弃率为0.1，模型训练时的批量大小为30。使用这种组合的图像分类准确率达到了约79.8%的测试数据准确率。最后，我们实验了预训练的VGG16网络，得到了约75.6%的测试数据准确率。这些实验展示了我们如何探索和提高模型性能。
- en: In the next chapter, we will explore another interesting and popular class of
    deep networks, called **generative adversarial networks** (**GANs**). We will
    make use of GANs to create new images.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一类有趣且流行的深度网络，称为**生成对抗网络**（**GANs**）。我们将利用GANs来创建新图像。
