- en: Preparing Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据
- en: Now that you have successfully prepared your system to learn about deep learning,
    see [Chapter 2](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml), *Setup and Introduction
    to Deep Learning Frameworks*, we will proceed to give you important guidelines
    about data that you may encounter frequently when practicing deep learning. When
    it comes to learning about deep learning, having well-prepared datasets will help
    you to focus more on designing your models rather than preparing your data. However,
    everyone knows that this is not a realistic expectation and if you ask any data
    scientist or machine learning professional about this, they will tell you that
    an important aspect of modeling is knowing how to prepare your data. Knowing how
    to deal with your data and how to prepare it will save you many hours of work
    that you can spend fine-tuning your models. Any time spent preparing your data
    is time well invested indeed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经成功地准备好学习深度学习，参见[第 2 章](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml)，*深度学习框架的设置与简介*，接下来我们将为您提供一些在深度学习实践中可能会频繁遇到的数据处理重要指导。对于深度学习的学习来说，准备好合适的数据集将有助于您更多地集中精力设计模型，而不是花时间准备数据。然而，大家都知道，这并不是现实的期望，如果你询问任何数据科学家或机器学习专家，他们会告诉你，建模的一个重要方面就是知道如何准备数据。知道如何处理数据以及如何准备数据，将节省你大量的时间，你可以用来微调模型。任何花费在准备数据上的时间都是值得投资的时间。
- en: This chapter will introduce you to the main concepts behind data processing
    to make it useful in deep learning. It will cover essential concepts of formatting
    outputs and inputs that are categorical or real-valued, and techniques for augmenting
    data or reducing the dimensions of data. At the end of the chapter, you should
    be able to handle the most common data manipulation techniques that can lead to
    successful choices of deep learning methodologies down the road.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向您介绍数据处理背后的主要概念，以使其在深度学习中变得有用。它将涵盖格式化输出和输入（无论是类别数据还是实值数据）的基本概念，以及数据增强或降维的技术。到本章结束时，您应该能够处理最常见的数据处理技术，这些技术将有助于您在未来选择成功的深度学习方法。
- en: 'Specifically, this chapter discusses the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章特别讨论以下内容：
- en: Binary data and binary classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制数据与二分类
- en: Categorical data and multiple classes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别数据与多分类
- en: Real-valued data and univariate regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实值数据与单变量回归
- en: Altering the distribution of data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变数据分布
- en: Data augmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data dimensionality reduction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据降维
- en: Ethical implications of manipulating data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作数据的伦理影响
- en: Binary data and binary classification
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二进制数据与二分类
- en: In this section, we will focus all our efforts on **preparing** data with binary
    inputs or targets. By binary, of course, we mean values that can be represented
    as either 0 or 1\. Notice the emphasis on the words *represented as*. The reason
    is that a column may contain data that is not necessarily a 0 or a 1, but could
    be interpreted as or represented by a 0 or a 1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将集中精力**准备**具有二进制输入或目标的数据。当然，所谓二进制，指的是可以表示为 0 或 1 的值。请注意，*表示为*这两个词的重点。原因是，一列数据可能包含的并不一定是
    0 或 1 的值，但它可以被解释为或表示为 0 或 1。
- en: 'Consider the following fragment of a dataset:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下数据集片段：
- en: '| *x*[1] | *x*[2] | ... | *y* |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | ... | *y* |'
- en: '| 0 | 5 | ... | a |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 5 | ... | a |'
- en: '| 1 | 7 | ... | a |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7 | ... | a |'
- en: '| 1 | 5 | ... | b |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 5 | ... | b |'
- en: '| 0 | 7 | ... | b |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 7 | ... | b |'
- en: In this short dataset example with only four rows, the column*x*[1] has values
    that are clearly binary and are either *0* or a *1*. However, *x*[2], at first
    glance, may not be perceived as binary, but if you pay close attention, the only
    values in that column are either *5* or *7*. This means that the data can be correctly
    and uniquely mapped to a set of two values. Therefore, we could map *5* to *0*,
    and *7* to *1*, or vice versa; it does not really matter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个只有四行的小数据集示例中，列 *x*[1] 的值显然是二进制的，要么是 *0*，要么是 *1*。然而，*x*[2] 表面上可能并不被认为是二进制的，但如果仔细观察，这一列中的唯一值是
    *5* 或 *7*。这意味着数据可以正确且唯一地映射到一组两个值。因此，我们可以将 *5* 映射为 *0*，将 *7* 映射为 *1*，或者反过来也可以；这并不重要。
- en: A similar phenomenon is observed in the target output value, *y*, which also
    contains unique values that can be mapped to a set of size two. And we can do
    such mapping by assigning, say, *b* to *0*, and *a* to ***1***.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标输出值 *y* 中也观察到类似现象，*y* 同样包含可以映射到大小为 2 的集合的独特值。我们可以通过将 *b* 映射为 *0*，将 *a* 映射为
    ***1*** 来进行这样的映射。
- en: If you are going to map from strings to binary, always make sure to check what
    type of data your specific models can handle. For example, in some Support Vector
    Machine implementations, the preferred values for targets are -1 and 1\. This
    is still binary but in a different set. Always double-check before deciding what
    mapping you will use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算将字符串映射到二进制值，请务必检查你所使用的特定模型能处理的数据类型。例如，在某些支持向量机实现中，目标的首选值是-1和1。这也是二进制的，只不过在一个不同的集合中。决定使用哪种映射之前，请务必进行仔细检查。
- en: In the next sub-section, we will deal specifically with binary targets using
    a dataset as a case study.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们将专门处理二进制目标，使用数据集作为案例研究。
- en: Binary targets on the Cleveland Heart Disease dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克里夫兰心脏病数据集的二进制目标
- en: The *Cleveland Heart Disease* (Cleveland 1988) dataset contains patient data
    for 303 subjects. Some of the columns in the dataset have missing values; we will
    deal with this, too. The dataset contains 13 columns that include cholesterol
    and age.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*克里夫兰心脏病*（克里夫兰1988）数据集包含303个受试者的病人数据。数据集中的一些列有缺失值，我们也会处理这些缺失值。数据集包含13列，其中包括胆固醇和年龄等数据。'
- en: The target is to detect whether a subject has heart disease or not, thus, is
    binary. The problem we will deal with is that the data is encoded with values
    from 0 to 4, where 0 indicates the absence of heart disease and the range 1 to
    4 indicates some type of heart disease.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是检测一个主体是否患有心脏病，因此这是一个二进制问题。我们要处理的问题是，数据用从0到4的值进行了编码，其中0表示没有心脏病，而1到4的范围表示某种类型的心脏病。
- en: 'We will use the portion of the dataset identified as `Cleveland`, which can
    be downloaded from this link: [https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用标识为`Cleveland`的数据集部分，可以从此链接下载：[https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data)
- en: 'The attributes of the dataset are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的属性如下：
- en: '| **Column** | **Description** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **列** | **描述** |'
- en: '| *x*[1] | Age |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | 年龄 |'
- en: '| *x*[2] | Sex |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| *x*[2] | 性别 |'
- en: '| *x*[3] | Chest pain type:  1: typical angina  2: atypical angina  3: non-anginal
    pain  4: asymptomatic |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| *x*[3] | 胸痛类型：1: 典型心绞痛，2: 非典型心绞痛，3: 非心绞痛性疼痛，4: 无症状 |'
- en: '| *x[4]* | Resting blood pressure (in mm Hg on admission to the hospital) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| *x[4]* | 休息时血压（住院时的毫米汞柱值） |'
- en: '| *x*[5] | Serum cholesterol in mg/dl |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| *x*[5] | 血清胆固醇（单位：mg/dl） |'
- en: '| *x[6]* | Fasting blood sugar > 120 mg/dl:  1 = true  0 = false |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| *x[6]* | 空腹血糖 > 120 mg/dl：1 = 是，0 = 否 |'
- en: '| *x[7]* | Resting electrocardiographic results:  0: normal  1: having ST-T
    wave abnormality  2: showing probable or definite left ventricular hypertrophy
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| *x[7]* | 休息时心电图结果：0: 正常，1: 存在ST-T波异常，2: 显示可能或确诊的左心室肥大 |'
- en: '| *x[8]* | Maximum heart rate achieved |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| *x[8]* | 达到的最大心率 |'
- en: '| *x[9]* | Exercise-induced angina:   1 = yes'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| *x[9]* | 运动引起的心绞痛：1 = 是'
- en: 0 = no |
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 0 = 无 |
- en: '| *x[10]* | ST depression induced by exercise relative to rest   |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| *x[10]* | 运动时ST段与静息时ST段的相对压低 |'
- en: '| *x[11]* | The slope of the peak exercise ST segment:   1: upsloping'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '| *x[11]* | 峰值运动ST段的坡度：1: 上坡'
- en: '2: flat'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 平坦'
- en: '3: downsloping'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 下降坡度'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| *x[12]* | Number of major vessels (0-3) colored by fluoroscopy  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| *x[12]* | 通过荧光透视检查的主要血管数量（0-3） |'
- en: '| *x[13]* | Thal:   3 = normal'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '| *x[13]* | Thal：3 = 正常'
- en: 6 = fixed defect
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 6 = 固定缺陷
- en: 7 = reversible defect
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 7 = 可逆性缺陷
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| *y* | Diagnosis of heart disease (angiographic disease status):   0: < 50%
    diameter narrowing'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '| *y* | 心脏病诊断（冠状动脉造影疾病状态）：0: < 50% 直径狭窄'
- en: '1: > 50% diameter narrowing |'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '1: > 50% 直径狭窄 |'
- en: 'Let''s follow the next steps in order to read the dataset into a pandas DataFrame
    and clean it:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将按照以下步骤读取数据集到pandas DataFrame并进行清理：
- en: 'In our Google Colab, we will first download the data using the `wget` command
    as follows:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的Google Colab中，我们首先使用`wget`命令下载数据，如下所示：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This, in turn, downloads the file `processed.cleveland.data` to the default
    directory for Colab. This can be verified by inspecting the Files tab on the left
    side of Colab. Please note that the preceding instruction is all one single line
    that, unfortunately, is very long.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将文件`processed.cleveland.data`下载到Colab的默认目录。可以通过检查Colab左侧的文件标签来验证此操作。请注意，前面的指令是一个非常长的单行命令。
- en: Next, we load the dataset using pandas to verify that the dataset is readable
    and accessible.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用pandas加载数据集，以验证数据集是否可读且可访问。
- en: Pandas is a Python library that is very popular among data scientists and machine
    learning scientists. It makes it easy to load and save datasets, to replace missing
    values, to retrieve basic statistical properties on data, and even perform transformations.
    Pandas is a lifesaver and now most other libraries for machine learning accept
    pandas as a valid input format.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 是一个在数据科学家和机器学习科学家中非常流行的 Python 库。它使得加载和保存数据集、替换缺失值、获取数据的基本统计属性甚至执行数据转换变得非常容易。Pandas
    就是救命稻草，现在大多数机器学习库都接受 pandas 作为有效的输入格式。
- en: 'Run the following commands in Colab to load and display some data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中运行以下命令以加载并显示一些数据：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `read_csv()` function loads a file that is formatted as **c****omma-separated
    values** (**CSV**). We use the argument `header=None` to tell pandas that the
    data does not have any actual headers; if omitted, pandas will use the first row
    of the data as the names for each column, but we do not want that in this case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv()` 函数用于加载格式为 **逗号分隔值**（**CSV**）的文件。我们使用 `header=None` 参数告诉 pandas
    数据没有实际的列标题；如果省略此参数，pandas 会将数据的第一行用作每列的名称，但在这种情况下我们不希望这样做。'
- en: The loaded data is stored in a variable called `df`, which can be any name,
    but I think it is easy to remember because pandas stores the data in a DataFrame
    object. Thus, `df` seems like an appropriate, short, memorable name for the data.
    However, if we work with multiple DataFrames, then it would be more convenient
    to name all of them differently with a name that describes the data they contain.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的数据存储在一个名为`df`的变量中，变量名可以是任何名字，但我认为它很容易记住，因为 pandas 将数据存储在一个 DataFrame 对象中。因此，`df`
    看起来是一个合适的、简短且易记的名称。然而，如果我们处理多个 DataFrame，那么最好为每个 DataFrame 起不同的名字，使用能够描述其包含数据的名称会更方便。
- en: The `head()` method that operates over a DataFrame is analog to a `unix` command
    that retrieves the first few lines of a file. On a DataFrame, the `head()` method
    returns the first five rows of data. If you wish to retrieve more, or fewer, rows
    of data, you can specify an integer as an argument to the method. Say, for example,
    that you want to retrieve the first three rows, then you would do `df.head(3)`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()` 方法是 DataFrame 上操作的方法，它类似于 `unix` 命令，用于获取文件的前几行。在 DataFrame 上，`head()`
    方法返回前五行数据。如果你希望获取更多或更少的行数据，可以将一个整数作为参数传递给该方法。例如，如果你想获取前三行，可以执行 `df.head(3)`。'
- en: 'The results of running the preceding code are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码的结果如下：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here are a few things to observe and remember for future reference:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些需要观察和记住的事项，供将来参考：
- en: On the left side, there is an unnamed column that has rows with consecutive
    numbers, 0, 1, ..., 4\. These are the indices that pandas assigns to each row
    in the dataset. These are unique numbers. Some datasets have unique identifiers,
    such as a filename for an image.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在左侧，有一个没有名称的列，其中包含连续的数字：0、1、...、4。这些是 pandas 为数据集中的每一行分配的索引值。这些是唯一的数字。有些数据集有唯一的标识符，比如图像的文件名。
- en: On the top, there is a row that goes from 0, 1, ..., 13\. These are the column
    identifiers. These are also unique and can be set if they are given to us.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在顶部，有一行从 0、1、... 到 13 的数字。这些是列标识符。它们也是唯一的，并且可以在给定的情况下进行设置。
- en: At the intersection of every row and column, we have values that are either
    floating-point decimals or integers. The entire dataset contains decimal numbers
    except for column 13, which is our target and contains integers.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一行和每一列的交点处，我们有值，这些值要么是浮动的小数，要么是整数。整个数据集包含的是小数，除了第 13 列，它是我们的目标列，包含整数。
- en: 'Because we will use this dataset as a binary classification problem, we now
    need to change the last column to contain only binary values: 0 and 1\. We will
    preserve the original meaning of 0, that is,no heart disease, and anything greater
    than or equal to 1 will be mapped to 1, indicating the diagnosis of some type
    of heart disease. We will run the following instructions:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们将使用这个数据集作为一个二分类问题，现在我们需要将最后一列修改为只包含二进制值：0 和 1。我们将保留 0 的原始含义，即没有心脏病，任何大于或等于
    1 的值都将映射为 1，表示诊断为某种类型的心脏病。我们将执行以下操作：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The instruction `df[13]` looks at the DataFrame and retrieves all the rows
    of the column whose index is `13`. Then, the `set()` method over all the rows
    of column 13 will create a set of all the unique elements in the column. In this
    way, we can know how many different values there are so that we can replace them.
    The output is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 指令`df[13]`查看DataFrame并检索索引为`13`的列的所有行。接着，对第13列所有行应用`set()`方法将创建该列中所有唯一元素的集合。通过这种方式，我们可以知道有多少不同的值，以便进行替换。输出如下：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From this, we know that 0 is no heart disease and 1 implies heart disease.
    However, 2, 3, and 4 need to be mapped to 1, because they, too, imply positive
    heart disease. We can make this change by executing the following commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从这点可以看出，0表示没有心脏病，1表示有心脏病。然而，2、3和4需要映射为1，因为它们也表示有心脏病。我们可以通过执行以下命令来进行此更改：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, the `replace()` function works on the DataFrame to replace specific values.
    In our case, it took three arguments:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`replace()`函数作用于DataFrame，用于替换特定值。在我们的例子中，它有三个参数：
- en: '`to_replace=[2,3,4]` denotes the list of items to search for, in order to replace
    them.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_replace=[2,3,4]`表示要查找的项目列表，用于进行替换。'
- en: '`value=1` denotes the value that will replace every matched entry .'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value=1`表示将替换每个匹配项的值。'
- en: '`inplace=True` indicates to pandas that we want to make the changes on the
    column.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inplace=True`表示我们希望在该列上进行修改。'
- en: In some cases, pandas DataFrames behave like an immutable object, which, in
    this case, makes it necessary to use the `inplace=True` argument. If we did not
    use this argument, we would have to do something like this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，pandas DataFrame像不可变对象一样工作，这使得必须使用`inplace=True`参数。如果我们没有使用这个参数，就需要像下面这样操作。
- en: '`df[13] = df[13].replace(to_replace=[2,3,4], value=1)`, which is not a problem
    for experienced pandas users. This means that you should be comfortable doing
    this either way.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`df[13] = df[13].replace(to_replace=[2,3,4], value=1)`，这对经验丰富的pandas用户来说不成问题。这意味着你应该能够无论如何都能轻松操作。'
- en: The main problem for people beginning to use pandas is that it does not *always*
    behave like an immutable object. Thus, you should keep all the pandas documentation
    close to you: [https://pandas.pydata.org/pandas-docs/stable/index.html](https://pandas.pydata.org/pandas-docs/stable/index.html)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 初学者使用pandas时的主要问题是它并不*总是*像不可变对象那样工作。因此，你应该随时查阅pandas文档：[https://pandas.pydata.org/pandas-docs/stable/index.html](https://pandas.pydata.org/pandas-docs/stable/index.html)
- en: 'The output for the preceding commands is the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令的输出如下：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: First, notice that when we print the first five rows, the thirteenth column
    now exclusively has the values 0 or 1\. You can compare this to the original data
    to verify that the number in bold font actually changed. We also verified, with
    `set(df[13])`, that the set of all unique values of that column is now only `{0,
    1}`, which is the desired target.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意当我们打印前五行时，第十三列现在只包含值0或1。你可以将其与原始数据进行比较，验证加粗字体中的数字确实已经更改。我们还通过`set(df[13])`验证了该列所有唯一值的集合现在仅包含`{0,
    1}`，这是我们想要的目标。
- en: With these changes, we could use the dataset to train a deep learning model
    and perhaps improve the existing documented performance [Detrano, R., *et al.* (1989)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些更改，我们可以使用该数据集训练深度学习模型，并可能改善现有的文献中记录的表现[Detrano, R., *et al.* (1989)]。
- en: The same methodology can be applied to make any other column have binary values
    in the set we need. As an exercise, let's do another example with the famous `MNIST`
    dataset.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的方法可以应用于将任何其他列转换为我们需要的二值集合。作为练习，我们再做一个关于著名`MNIST`数据集的例子。
- en: Binarizing the MNIST dataset
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对MNIST数据集进行二值化
- en: 'The MNIST dataset is well known in the deep learning community (Deng, L. (2012)).
    It is composed of thousands of images of handwritten digits. Figure 3.1 shows
    eight samples of the MNIST dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集在深度学习社区中非常著名（Deng, L. (2012)）。它包含了成千上万张手写数字的图像。图3.1展示了MNIST数据集中的八个样本：
- en: '![](img/7de1c72c-0fef-46ad-93e0-19d39bafad3b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7de1c72c-0fef-46ad-93e0-19d39bafad3b.png)'
- en: Figure 3.1 – Eight samples of the MNIST dataset. The number on top of each image
    corresponds to the target class
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – MNIST数据集的八个样本。每张图片上方的数字对应于目标类别。
- en: As you can see, the samples in this dataset are messy and are very real. Every
    image has a size of 28 x 28 pixels. And there are only 10 target classes, one
    for each digit, 0, 1, 2, ..., 9\. The complication here is usually that some digits
    may look similar to others; for example, 1 and 7, or 0 and 6\. However, most deep
    learning algorithms have successfully solved the classification problem with high
    accuracy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个数据集中的样本杂乱无章，且非常真实。每张图像的大小为28 x 28像素。并且只有10个目标类别，每个数字对应一个类别，即0，1，2，...，9。这里的复杂性通常在于某些数字可能与其他数字相似；例如，1和7，或者0和6。然而，大多数深度学习算法已经成功地以高准确度解决了这个分类问题。
- en: From *Figure 3.1*, a close inspection will reveal that the values are not exactly
    zeros and ones, that is, binary. In fact, the images are 8-bit grayscale, in the
    range [0-255]. As mentioned earlier, this is no longer a problem for most advanced
    deep learning algorithms. However, for some algorithms, such as **Restricted Boltzmann
    Machines** (**RMBs**), the input data needs to be in binary format [0,1] because
    that is how the algorithm works, traditionally.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图3.1*中仔细检查会发现，图像中的值并不完全是零和一，也就是二进制值。实际上，这些图像是8位灰度图像，值的范围在[0-255]之间。正如前面提到的，这对于大多数先进的深度学习算法来说不再是问题。然而，对于一些算法，例如**限制玻尔兹曼机**（**RMBs**），输入数据需要是二进制格式[0,1]，因为这正是该算法传统上所需要的格式。
- en: 'Thus, we will do two things:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将做两件事：
- en: Binarize the images, so as to have binary inputs
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像进行二值化，以便获得二进制输入
- en: Binarize the targets, to make it a binary classification problem
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对目标进行二值化，使其成为一个二分类问题
- en: For this example, we will arbitrarily select two numerals only, 7 and 8, as
    our target classes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将任意选择两个数字，7和8，作为我们的目标类别。
- en: Binarizing the images
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对图像进行二值化
- en: The binarization process is a common step in image processing. It is formally
    known as image thresholding because we need a threshold to decide which values
    become zeros and ones. For a full survey about this topic, please consult (Sezgin,
    M., and Sankur, B. (2004)). This is all to say that there is a science behind
    picking the perfect threshold that will minimize the range conversion error from
    [0, 255] down to [0, 1].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 二值化过程是图像处理中的常见步骤。它正式被称为图像阈值化，因为我们需要一个阈值来决定哪些值变为零，哪些变为一。关于这个主题的详细调查，请参考（Sezgin,
    M., 和 Sankur, B.（2004））。这意味着，在选择完美阈值时，背后有一门科学，这个阈值能够最小化从[0, 255]到[0, 1]的范围转换误差。
- en: However, since this is not a book about image processing, we will arbitrarily
    set a threshold of 128\. Thus, any value below 128 will become a zero, and any
    value greater than or equal to 128 will become a one.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于这不是一本关于图像处理的书籍，我们将任意设置阈值为128。这样，任何小于128的值将变为零，任何大于或等于128的值将变为一。
- en: 'This step can be easily done by using indexing in Python. To proceed, we will
    display a small portion of the dataset to make sure the data is transformed correctly.
    We will do this by executing the following commands in the next steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程可以通过在Python中使用索引轻松完成。接下来，我们将显示数据集的一小部分，以确保数据已经正确转换。我们将通过执行以下命令来实现这一点：
- en: 'To load the dataset and verify its dimensionality (shape), run the following
    command:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加载数据集并验证其维度（形状），请运行以下命令：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first thing to notice is that we are using a machine learning library known
    as `scikit learn` or `sklearn` in Python. It is one of the most used libraries
    for general-purpose machine learning. The `MNIST` dataset is loaded using the
    `fetch_openml()` method, which requires an argument with the identifier of the
    dataset to be loaded, which in this case is `'mnist_784'`. The number `784` comes
    from the size of `MNIST` images, which is 28 x 28 pixels and can be interpreted
    as a vector of 784 elements rather than a matrix of 28 columns and 28 rows. By
    verifying the `shape` property, we can see that the dataset has 70,000 images
    represented as vectors of size 784, and the targets are in the same proportion.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，我们正在使用一个名为`scikit learn`或`sklearn`的机器学习库，它是Python中最常用的通用机器学习库之一。`MNIST`数据集是通过`fetch_openml()`方法加载的，该方法需要传入一个数据集标识符，当前数据集的标识符是`'mnist_784'`。数字`784`来自于`MNIST`图像的大小，即28
    x 28像素，可以将其看作一个由784个元素组成的向量，而不是28列28行的矩阵。通过验证`shape`属性，我们可以看到该数据集包含70,000张图像，这些图像被表示为大小为784的向量，目标也是按照相同比例表示的。
- en: Please note here that, as opposed to the previous section where we used a dataset
    loaded into pandas, in this example, we use the data directly as lists or arrays
    of lists. You should feel comfortable manipulating both pandas and raw datasets.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与前一节使用加载到pandas中的数据集不同，在这个例子中，我们直接使用数据作为列表或列表的数组。你应该能自如地处理pandas和原始数据集。
- en: 'To actually do the binarization by verifying the data before and after, run
    the following:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实际进行二值化，并验证数据前后的变化，运行以下命令：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will output the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The instruction `data[0].reshape(28, 28)[10:18,10:18]` is doing three things:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 指令 `data[0].reshape(28, 28)[10:18,10:18]` 做了三件事：
- en: '`data[0]` returns the first image as an array of size (1, 784).'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`data[0]` 返回第一张图像，大小为（1, 784）的数组。'
- en: '`reshape(28, 28)` resizes the (1, 784) array as a (28, 28) matrix, which is
    the actual image; this can be useful to display the actual data, for example,
    to produce *Figure 3.1*.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`reshape(28, 28)` 将（1, 784）数组调整为（28, 28）矩阵，这就是实际的图像；这对于显示实际数据很有用，例如生成*图3.1*。'
- en: '`[10:18,10:18]` takes only a subset of the (28, 28) matrix at positions 10
    to 18 for both columns and rows; this more or less corresponds to the center area
    of the image and it is a good place to look at what is changing.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[10:18,10:18]` 仅获取（28, 28）矩阵中列和行从10到18的位置的子集；这大致对应于图像的中心区域，是查看变化的好地方。'
- en: The preceding is for looking at the data only, but the actual changes are done
    in the next lines. The line `mnist.data[mnist.data < 128] = 0` uses Python indexing.
    The instruction `mnist.data < 128` returns a multidimensional array of Boolean
    values that `mnist.data[ ]` uses as indices on which to set the value to zero.
    The key is to do so for all values strictly less than 128\. And the next line
    does the same, but for values greater than or equal to 128.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是仅用于查看数据，但实际的更改在接下来的行中完成。行`mnist.data[mnist.data < 128] = 0` 使用了Python索引。指令
    `mnist.data < 128` 返回一个布尔值的多维数组，`mnist.data[ ]` 使用该数组作为索引，将相应的值设置为零。关键是对所有严格小于128的值执行此操作。接下来的行做了相同的操作，但适用于大于或等于128的值。
- en: By inspecting the output, we can confirm that the data has successfully changed
    and has been thresholded, or binarized.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查输出，我们可以确认数据已经成功改变，并已被阈值化或二值化。
- en: Binarizing the targets
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标的二值化
- en: 'We will binarize the targets by following the next two steps:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下两个步骤对目标进行二值化：
- en: 'First, we will discard image data for other numerals and we will only keep
    7 and 8\. Then, we will map 7 to 0 and 8 to 1\. These commands will create new
    variables, `X` and `y`, that will hold only the numerals 7 and 8:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将丢弃其他数字的图像数据，只保留7和8。然后，我们将7映射为0，8映射为1。 这些命令将创建新的变量`X`和`y`，它们仅包含数字7和8：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will output the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice the use of the `OR` operator, `|`, to logically take two sets of Boolean
    indices and produce one with the `OR` operator. These indices are used to produce
    a new dataset. The shape of the new dataset contains a little over 14,000 images.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用了`OR`运算符，`|`，用于逻辑地将两个布尔索引集合进行“或”操作，并产生一个新的布尔索引集合。这些索引用于生成一个新的数据集。新数据集的形状包含略多于14,000张图像。
- en: 'To map 7 to 0 and 8 to 1, we can run the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将7映射为0，8映射为1，我们可以运行以下命令：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This outputs the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The instruction `[0 if v=='7' else 1 for v in y]` checks every element in `y`,
    and if an element is `'7'`, then it returns a `0`, otherwise (for example, when
    it is `'8'`), it returns a `1`. As the output suggests, choosing the first 10
    elements, the data is binarized to the set {`0`, `1`}.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 指令 `[0 if v=='7' else 1 for v in y]` 检查`y`中的每个元素，如果某个元素是`'7'`，则返回`0`，否则（例如，当它是`'8'`时），返回`1`。正如输出所示，选择前10个元素，数据被二值化为集合{`0`,
    `1`}。
- en: Remember, the target data in `y` was already binary in the sense that it only
    had two sets of unique possible numbers {`7`, `8`}. But we made it binary to the
    set {`0`, `1`} because often this is better when we use different deep learning
    algorithms that calculate very specific types of loss functions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`y`中的目标数据已经是二值化的，因为它只包含两个可能的唯一数字集{`7`, `8`}。但是我们将它二值化为集合{`0`, `1`}，因为在使用不同的深度学习算法时，这通常更好，尤其是当这些算法计算非常特定类型的损失函数时。
- en: With this, the dataset is ready to use with binary and general classifiers.
    But what if we actually want to have multiple classes, for example, to detect
    all 10 digits of the `MNIST` dataset and not just 2? Or what if we have features,
    columns, or inputs that are not numeric but are categorical? The next section
    will help you prepare the data in these cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，数据集已经准备好与二分类器和通用分类器一起使用。但是，如果我们实际上想要拥有多个类别呢？例如，要检测`MNIST`数据集中的所有10个数字，而不仅仅是2个数字？或者，如果我们有特征、列或输入，它们不是数字而是分类数据呢？接下来的部分将帮助你在这些情况下准备数据。
- en: Categorical data and multiple classes
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类数据和多类别
- en: Now that you know how to binarize data for different purposes, we can look into
    other types of data, such as categorical or multi-labeled data, and how to make
    them numeric. Most advanced deep learning algorithms, in fact, only accept numerical
    data. This is merely a design issue that can easily be solved later on, and it
    is not a big deal because you will learn there are easy ways to take categorical
    data and convert it to a meaningful numerical representation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何将数据二值化以适应不同目的后，我们可以看看其他类型的数据，例如分类数据或多标签数据，以及如何将它们转换为数字。事实上，大多数先进的深度学习算法只接受数值数据。这只是一个设计问题，可以在以后轻松解决，这不是什么大问题，因为你将学到有简单的方法将分类数据转换为有意义的数值表示。
- en: '**Categorical data** has information embedded as distinct categories. These
    categories can be represented as numbers or as strings. For example, a dataset
    that has a column named `country` with items such as "India", "Mexico", "France",
    and "U.S". Or, a dataset with zip codes such as 12601, 85621, and 73315\. The
    former is **non-numeric** categorical data, and the latter is **numeric** categorical
    data. Country names would need to be converted to a number to be usable at all,
    but zip codes are already numbers that are meaningless as mere numbers. Zip codes
    would be more meaningful, from a machine learning perspective, if we converted
    them to latitude and longitude coordinates; this would better capture places that
    are closer to each other than using plain numbers.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类数据**包含作为不同类别嵌入的信息。这些类别可以表示为数字或字符串。例如，一个数据集有一列名为`country`，其中包含“印度”、“墨西哥”、“法国”和“美国”等项目。或者，一个包含邮政编码的数据集，如12601、85621和73315。前者是**非数字**分类数据，后者是**数字**分类数据。国家名称需要转换为数字才能使用，但邮政编码已经是数字，作为单纯的数字是没有意义的。从机器学习的角度来看，如果我们将邮政编码转换为纬度和经度坐标，邮政编码会变得更加有意义；这种方式能更好地捕捉到彼此更近的地方，而不是直接使用数字。'
- en: To begin, we will address the issue of converting string categories to plain
    numbers and then we will convert those to numbers in a format called **one-hot
    encoding**.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将解决将字符串类别转换为普通数字的问题，然后将其转换为一种称为**独热编码**的格式。
- en: Converting string labels to numbers
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将字符串标签转换为数字
- en: 'We will take the `MNIST` dataset again and use its string labels, *0*, *1*,
    ..., *9*, and convert them to numbers. We can achieve this in many different ways:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用`MNIST`数据集，并使用其字符串标签，*0*，*1*，...，*9*，并将它们转换为数字。我们可以通过多种方式实现这一点：
- en: We could simply map all strings to integers with one simple command, `y = list(map(int,
    mnist.target))`, and be done. The variable `y` now contains only a list of integers
    such as `[8, 7, 1, 2, ... ]`. But this will only solve the problem for this particular
    case; you need to learn something that will work for all cases. So, let's not
    do this.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过一个简单的命令，`y = list(map(int, mnist.target))`，将所有字符串映射到整数，并完成任务。现在，变量`y`仅包含像`[8,
    7, 1, 2, ...]`这样的整数列表。但这只解决了这个特定情况的问题；你需要学习一些适用于所有情况的方法。所以，我们不应该这样做。
- en: We could do some hard work by iterating over the data 10 times – `mnist.target
    = [0 if v=='0' else v for v in mnist.target]` – doing this for every numeral.
    But again, this (and other similar things) will work only for this case. Let's
    not do this.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过反复遍历数据10次来做一些繁琐的工作——`mnist.target = [0 if v=='0' else v for v in mnist.target]`——对每个数字都这样做。但同样，这样做（以及其他类似的事情）只对这种情况有效。我们不应该这样做。
- en: We could use scikit-learn's `LabelEncoder()` method, which will take any list
    of labels and map them to a number. This will work for all cases.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn的`LabelEncoder()`方法，它将任何标签列表映射到一个数字。这对于所有情况都有效。
- en: 'Let''s use the `scikit` method by following these steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤使用`scikit`方法：
- en: 'Run the following code:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `sorted(list(set(mnist.target)))` command does three things:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`sorted(list(set(mnist.target)))`命令执行了三件事：'
- en: '`set(mnist.target)` retrieves the set of unique values in the data, for example, `{''8'',
    ''2'', ..., ''9''}`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set(mnist.target)` 提取数据中唯一值的集合，例如，`{''8'', ''2'', ..., ''9''}`。'
- en: '`list(set(mnist.target))` simply converts the set into a list because we need
    a list or an array for the `LabelEncoder()` method.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`list(set(mnist.target))` 只是将集合转换为列表，因为我们需要列表或数组供`LabelEncoder()`方法使用。'
- en: '`sorted(list(set(mnist.target)))` is important here so that *0* maps to 0 and
    not to have *8* map to 0, and so on. It sorts the list, and the result looks like
    this - `[''0'', ''1'', ..., ''9'']`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里`sorted(list(set(mnist.target)))`非常重要，以确保*0* 映射到 0，而不是*8* 映射到 0，依此类推。它对列表进行排序，结果如下
    - `['0', '1', ..., '9']`。
- en: The `le.fit()` method takes a list (or an array) and produces a map (a dictionary)
    to be used forward (and backward if needed) to encode labels, or strings, into
    numbers. It stores this in a `LabelEncoder` object.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`le.fit()`方法接受一个列表（或数组），并生成一个映射（字典），用于将标签或字符串编码为数字，并将其存储在`LabelEncoder`对象中，以供将来使用（如果需要，也可以向后编码）。'
- en: 'Next, we could test the encoding as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以按如下方式测试编码：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will output the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `transform()` method transforms a string-based label into a number, whereas
    the `inverse_transform()` method takes a number and returns the corresponding
    string label or category.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform()`方法将基于字符串的标签转换为数字，而`inverse_transform()`方法接受数字并返回对应的字符串标签或类别。'
- en: Any attempt to map to and from an unseen category or number will cause a `LabelEncoder` object
    to produce an error. Please be diligent in providing the list of all possible
    categories to the best of your knowledge.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 任何尝试映射到未见过的类别或数字将导致`LabelEncoder`对象产生错误。请在提供所有可能类别列表时，尽量确保其完整。
- en: 'Once the `LabelEncoder` object is fitted and tested, we can simply run the
    following instruction to encode the data:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦`LabelEncoder`对象被拟合并测试完成，我们可以简单地运行以下指令来编码数据：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will output the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The new encoded labels are now in `y` and ready to be used.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 新的编码标签现在存储在`y`中，准备使用。
- en: This method of encoding a label to an integer is also known as **Ordinal Encoding.**
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将标签编码为整数的方法也称为**顺序编码**。
- en: 'This methodology should work for all labels encoded as strings, for which you
    can simply map to numbers without losing context. In the case of the `MNIST` dataset,
    we can map *0* to 0 and *7* to 7 without losing context. Other examples of when
    you can do this include the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法适用于所有作为字符串编码的标签，在这些情况下，您可以简单地映射为数字，而不会丧失上下文。在`MNIST`数据集中，我们可以将*0*映射为0，将*7*映射为7，而不会丧失上下文。以下是您可以进行此操作的其他示例：
- en: '**Age groups**: [''18-21'', ''22-35'', ''36+''] to [0, 1, 2]'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**年龄段**：[''18-21'', ''22-35'', ''36+''] 转换为 [0, 1, 2]'
- en: '**Gender**: [''male'', ''female''] to [0, 1]'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**：[''male'', ''female''] 转换为 [0, 1]'
- en: '**Colors**: [''red'', ''black'', ''blue'', ...] to [0, 1, 2, ...]'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**颜色**：[''red'', ''black'', ''blue'', ...] 转换为 [0, 1, 2, ...]'
- en: '**Studies**: [''primary'', ''secondary'', ''high school'', ''university'']
    to [0, 1, 2, 3]'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学位**：[''primary'', ''secondary'', ''high school'', ''university''] 转换为 [0,
    1, 2, 3]'
- en: 'However, we are making one big assumption here: the labels encode no special
    meaning in themselves. As we mentioned earlier, zip codes could be simply encoded
    to smaller numbers; however, they have a geographical meaning, and doing so might
    negatively impact the performance of our deep learning algorithms. Similarly,
    in the preceding list, if studies require a special meaning that indicates that
    a *university* degree is much higher or more important than a *primary* degree,
    then perhaps we should consider different number mappings. Or perhaps we want
    our learning algorithms to *learn* such intricacies by themselves! In such cases,
    we should then use the well-known strategy of one-hot encoding.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在这里做了一个重要假设：标签本身不编码任何特殊含义。正如我们之前提到的，邮政编码可以简单地编码为较小的数字；然而，它们有地理意义，进行这样的编码可能会对我们的深度学习算法性能产生负面影响。类似地，在前述列表中，如果研究需要某种特殊含义，表示*大学*学位比*小学*学位要更高或更重要，那么我们可能需要考虑不同的数字映射。或者，也许我们希望我们的学习算法能够*学习*这些细微差别！在这种情况下，我们应该使用广为人知的独热编码策略。
- en: Converting categories to one-hot encoding
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将类别转换为独热编码
- en: Converting categories to one-hot encoding is better in most cases in which the
    categories or labels may have special meanings with respect to each other. In
    such cases, it has been reported to outperform ordinal encoding [Potdar, K., *et
    al.* (2017)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，如果类别或标签之间可能存在特殊含义，将类别转换为独热编码是更好的选择。在这种情况下，独热编码被认为比顺序编码更有效 [Potdar, K.,
    *et al.* (2017)]。
- en: 'The idea is to represent each label as a Boolean state having independent columns.
    Take, for example, a column with the following data:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其思想是将每个标签表示为一个布尔状态，并具有独立的列。例如，假设有以下数据列：
- en: '| **Gender** |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **性别** |'
- en: '| ''female'' |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ''女性'' |'
- en: '| ''male'' |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ''男性'' |'
- en: '| ''male'' |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ''男性'' |'
- en: '| ''female'' |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ''女性'' |'
- en: '| ''female'' |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| ''女性'' |'
- en: 'This can be uniquely transformed, using one-hot encoding, into the following
    new piece of data:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过一热编码唯一地转换为以下新数据：
- en: '| **Gender_Female** | **Gender_Male** |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **性别_女性** | **性别_男性** |'
- en: '| 1 | 0 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 |'
- en: '| 0 | 1 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 |'
- en: '| 0 | 1 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 |'
- en: '| 1 | 0 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 |'
- en: '| 1 | 0 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 |'
- en: As you can see, the binary bit is *hot* (is one) only if the label corresponds
    to that specific row and it is zero otherwise. Notice also that we renamed the
    columns to keep track of which label corresponds to which column; however, this
    is merely a recommended format and is not a formal rule.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，二进制位是*热*（为1），只有当标签对应于特定行时，它才为1，否则为0。还要注意，我们重新命名了列，以便追踪哪个标签对应哪个列；然而，这仅仅是推荐的格式，并不是正式规则。
- en: There are a number of ways we can do this in Python. If your data is in a pandas
    DataFrame, then you can simply do `pd.get_dummies(df, prefix=['Gender'])`, assuming
    your column is in `df` and you want to use `Gender` as a prefix.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们有多种方式可以做到这一点。如果您的数据在 pandas DataFrame 中，那么您可以直接执行 `pd.get_dummies(df,
    prefix=['性别'])`，假设您的列在 `df` 中，且您希望使用 `性别` 作为前缀。
- en: 'To reproduce the exact results as discussed in the preceding table, follow
    these steps:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要重现前表中讨论的确切结果，请按照以下步骤操作：
- en: 'Run the following command:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will output the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now simply do the encoding by running the following command:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在只需通过运行以下命令来进行编码：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And this is produced:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的结果如下：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A fun, and perhaps obvious, property of this encoding is that the `OR` and `XOR`
    operations along the rows of all the encoded columns will always be one, and the
    `AND` operation will yield zeros.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码的一个有趣且可能显而易见的特性是，所有编码列的行上的 `OR` 和 `XOR` 操作将始终为1，而 `AND` 操作将返回0。
- en: For cases in which the data is not a pandas DataFrame, for example, MNIST targets,
    we can use scikit-learn's `OneHotEncoder.transform()` method.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据不是 pandas DataFrame 的情况，例如 MNIST 目标，我们可以使用 scikit-learn 的 `OneHotEncoder.transform()`
    方法。
- en: A `OneHotEncoder` object has a constructor that will automatically initialize
    everything to reasonable assumptions and determines most of its parameters using
    the `fit()` method. It determines the size of the data, the different labels that
    exist in the data, and then creates a dynamic mapping that we can use with the
    `transform()` method.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneHotEncoder` 对象有一个构造函数，它会自动初始化所有参数并通过 `fit()` 方法确定大部分参数。它会确定数据的大小、数据中存在的不同标签，并创建一个动态映射，供我们与
    `transform()` 方法一起使用。'
- en: 'To do a one-hot encoding of the `MNIST` targets, we can do this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要对 `MNIST` 目标进行一热编码，我们可以这样做：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will output the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This code includes our classic sanity check in which we verify that label `'5'` was
    in fact converted to a row vector with 10 columns, of which number `6` is *hot*.
    It works, as expected. The new dimensionality of `y` is *n* rows and 10 columns.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码包括我们的经典合理性检查，我们验证标签 `'5'` 确实被转换为一个具有 10 列的行向量，其中数字 `6` 是*热*的。它按预期工作。`y`
    的新维度是*n*行和 10 列。
- en: This is the preferred format for the targets that use deep learning methods
    on MNIST. One-hot encoding targets are great for neural networks that will have
    exactly one neuron per class. In this case, one neuron per digit. Each neuron
    will need to learn to predict one-hot encoded behavior, that is, only one neuron
    should fire up (be "hot") while the others should be inhibited.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在 MNIST 上使用深度学习方法时，目标的首选格式。一热编码目标非常适合神经网络，其中每个类别只有一个神经元。在这种情况下，每个数字对应一个神经元。每个神经元将需要学习预测一热编码行为，即只有一个神经元应该被激活（即“热”），而其他神经元应该被抑制。
- en: The preceding process can be repeated exactly to convert any other columns into
    one-hot encoding, provided that they contain categorical data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 前述过程可以完全重复，用于将任何其他列转换为一热编码，前提是它们包含分类数据。
- en: Categories, labels, and specific mappings to integers or bits are very helpful
    when we want to classify input data into those categories, labels, or mappings.
    But what if we want to have input data that maps to continuous data? For example,
    data to predict a person's IQ by looking at their responses; or predicting the
    price of electricity depending on the input data about weather and the seasons.
    This is known as data for **regression**, which we will cover next.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 类别、标签以及将它们映射到整数或位的特定映射，在我们想将输入数据分类到这些类别、标签或映射时非常有用。但是，如果我们想要输入数据映射到连续数据该怎么办？例如，通过查看一个人的反应来预测其智商；或者根据天气和季节的输入数据预测电价。这就是所谓的回归数据，我们将在接下来讨论。
- en: Real-valued data and univariate regression
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实值数据和单变量回归
- en: Knowing how to deal with categorical data is very important when using classification
    models based on deep learning; however, knowing how to prepare data for regression
    is as important. Data that contains continuous-like real values, such as temperature,
    prices, weight, speed, and others, is suitable for regression; that is, if we
    have a dataset with columns of different types of values, and one of those is
    real-valued data, we could perform regression on that column. This implies that
    we could use all the rest of the dataset to predict the values on that column.
    This is known as **univariate regression**, or regression on one variable.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于深度学习的分类模型时，了解如何处理类别数据非常重要；然而，了解如何为回归准备数据同样重要。包含类似连续实值的数据，如温度、价格、体重、速度等，适合用于回归；也就是说，如果我们有一个包含不同类型值的
    dataset，其中有一列是实值数据，我们可以对该列进行回归分析。这意味着我们可以利用数据集中的其他所有数据来预测该列的值。这就是所谓的**单变量回归**，即对一个变量的回归分析。
- en: Most machine learning methodologies work better if the data for regression is **normalized**.
    By that, we mean that the data will have special statistical properties that will
    make calculations more stable. This is critical for many deep learning algorithms
    that suffer from vanishing or exploding gradients (Hanin, B. (2018)). For example,
    in calculating a gradient in a neural network, an error needs to be propagated
    backward from the output layer to the input layer; but if the output layer has
    a large error and the range of values (that is their **distribution**) is also
    large, then the multiplications going backward can cause overflow on variables,
    which would ruin the training process.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习方法在回归数据**归一化**后表现更好。我们所说的归一化是指数据将具有特定的统计性质，从而使计算更加稳定。这对于许多深度学习算法至关重要，因为它们容易受到梯度消失或爆炸的影响（Hanin,
    B. (2018)）。例如，在神经网络中计算梯度时，误差需要从输出层向输入层反向传播；但是，如果输出层的误差很大且值的范围（即它们的**分布**）也很大，那么反向传播时的乘法可能会导致变量溢出，从而破坏训练过程。
- en: To overcome these difficulties, it is desirable to normalize the distribution
    of variables that can be used for regression, or variables that are real-valued.
    The normalization process has many variants, but we will limit our discussion
    to two main methodologies, one that sets specific statistical properties of the
    data, and one that sets specific ranges on the data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些困难，最好对可用于回归的变量或实值变量进行归一化处理。归一化过程有许多变体，但我们将讨论两种主要方法，一种是设置数据的特定统计性质，另一种是设置数据的特定范围。
- en: Scaling to a specific range of values
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放到特定范围的值
- en: Let's go back to the heart disease dataset discussed earlier in this chapter.
    If you pay attention, many of those variables are real-valued and would be ideal
    for regression; for example, *x*[5] and *x*[10].
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章前面讨论的心脏病数据集。如果你仔细观察，许多变量是实值数据，非常适合回归；例如，*x*[5]和*x*[10]。
- en: All variables are suitable for regression. This means that, technically, we
    can predict on any numeric data. The fact that some values are real-valued makes
    them more appealing for regression for a number of reasons. For example, the fact
    that the values in that column have a meaning that goes beyond integers and natural
    numbers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所有变量都适合回归。这意味着，从技术上讲，我们可以对任何数值数据进行预测。某些值是实值数据使它们在回归中更具吸引力，原因有很多。例如，这一列中的值具有超越整数和自然数的含义。
- en: Let's focus on *x*[5 ]and *x*[10], which are the variables for measuring the
    cholesterol level and ST depression induced by exercise relative to rest, respectively.
    What if we want to change the original research question the doctors intended,
    which was to study heart disease based on different factors? What if now we want
    to use all the factors, including knowing whether patients have heart disease
    or not, to determine or predict their cholesterol level? We can do that with regression
    on *x*[5].
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们聚焦于 *x*[5] 和 *x*[10]，它们分别是用于衡量胆固醇水平和运动引起的 ST 残差的变量。如果我们想改变医生最初的研究问题，原本是基于不同因素研究心脏病，那么如果现在我们想用所有因素（包括知道病人是否患有心脏病）来确定或预测他们的胆固醇水平呢？我们可以通过对
    *x*[5] 进行回归来实现。
- en: So, to prepare the data on *x*[5] and *x*[10], we will go ahead and scale the
    data. For verification purposes, we will retrieve descriptive statistics on the
    data before and after the scaling of the data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了准备 *x*[5] 和 *x*[10] 数据，我们将继续进行数据缩放。为了验证，我们将分别检索数据缩放前后的描述性统计信息。
- en: 'To reload the dataset and display descriptive statistics, we can do the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重新加载数据集并显示描述性统计信息，我们可以执行以下操作：
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In this case, index, `4` and `9` correspond to *x*[5] and *x*[10], and the
    `describe()` method outputs the following information:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，索引 `4` 和 `9` 分别对应 *x*[5] 和 *x*[10]，`describe()` 方法输出以下信息：
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The most notable properties are the mean, and maximum/minimum values contained
    in that column. These will change once we scale the data to a different range.
    If we visualize the data as a scatter plot with respective histograms, it looks
    like *Figure 3.2*:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最显著的属性是该列中包含的均值和最大值/最小值。这些值在我们将数据缩放到不同范围时会发生变化。如果我们将数据可视化为带有相应直方图的散点图，结果就像 *图
    3.2*：
- en: '![](img/71a5370e-2596-445b-8ad1-90ec5f7e721a.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71a5370e-2596-445b-8ad1-90ec5f7e721a.png)'
- en: Figure 3.2 – Scatter plot of the two columns *x*[5] and *x*[10] and their corresponding
    histograms
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 两列 *x*[5] 和 *x*[10] 的散点图及其相应的直方图
- en: 'As can be seen from *Figure 3.2*, the ranges are quite different, and the distribution
    of the data is different as well. The new desired range here is a minimum of 0
    and a maximum of 1\. This range is typical when we scale the data. And it can
    be achieved using scikit-learn''s `MinMaxScaler` object as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 3.2* 中可以看到，范围差异很大，数据的分布也不同。这里所需的新范围是最小值为 0，最大值为 1。这个范围在我们进行数据缩放时是典型的。可以使用
    scikit-learn 的 `MinMaxScaler` 对象来实现，代码如下：
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will output the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'What the `fit()` method does internally is to determine what the current min
    and max values are for the data. Then, the `transform()` method uses that information
    to remove the minimum and divide by the maximum to achieve the desired range.
    As can be seen, the new descriptive statistics have changed, which can be confirmed
    by looking at the range in the axes of *Figure 3.3*:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 方法内部的工作原理是确定数据的当前最小值和最大值。然后，`transform()` 方法使用这些信息来去除最小值并除以最大值，从而实现所需的范围。正如所见，新的描述性统计信息已经发生变化，可以通过查看
    *图 3.3* 中坐标轴的范围来确认：'
- en: '![](img/3ddba0fa-3bb9-478d-a6f6-b4739023945d.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ddba0fa-3bb9-478d-a6f6-b4739023945d.png)'
- en: Figure 3.3 – Scatter plot of the newly scaled columns *x*[5] and *x*[10] and
    their corresponding histograms
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 新缩放后的 *x*[5] 和 *x*[10] 的散点图及其相应的直方图
- en: Notice, however, if you pay close attention, that the distribution of the data
    has not changed. That is, the histograms of the data in *Figure 3.2* and *Figure
    3.3* are still the same. And this is a very important fact because, usually, you
    do not want to change the distribution of the data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你仔细观察，你会发现数据的分布没有发生变化。也就是说，*图 3.2* 和 *图 3.3* 中数据的直方图依然相同。这是一个非常重要的事实，因为通常情况下，你并不希望改变数据的分布。
- en: Standardizing to zero mean and unit variance
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化为零均值和单位方差
- en: Another way of preprocessing real-valued data is by making it have zero mean
    and unit variance. This process is referred to by many names, such as normalizing,
    z-scoring, centering, or standardizing.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理实数值数据的另一种方式是使其具有零均值和单位方差。这个过程有许多不同的名称，比如标准化、z-score 标准化、居中处理等。
- en: 'Let''s say that ***x***=[*x*[5], *x*[10]], from our features above, then we
    can standardize ***x***as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ***x*** = [*x*[5], *x*[10]]，根据我们之前的特征，接下来可以按如下方式对 ***x*** 进行标准化：
- en: '![](img/55d3b551-c4fa-4283-8e6c-ad964388b135.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55d3b551-c4fa-4283-8e6c-ad964388b135.png)'
- en: Here, *µ* is a vector corresponding to the means of each column on ***x***,
    and *σ* is a vector of standard deviations of each column in ***x***.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*µ* 是一个对应于***x***每列均值的向量，*σ* 是一个对应于***x***每列标准差的向量。
- en: 'After the standardization of ***x***, if we recompute the mean and standard
    deviation, we should get a mean of zero and a standard deviation of one. In Python,
    we do the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准化***x***后，如果我们重新计算均值和标准差，应该得到均值为零，标准差为一。在Python中，我们可以这样做：
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will output the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出如下内容：
- en: '[PRE32]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Notice that after normalization, the mean is, for numerical purposes, zero.
    And the standard deviation is one. The same thing can be done, of course, using
    the scikit-learn `StandardScaler` object as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，标准化后，均值在数值上为零，标准差为一。当然，也可以使用scikit-learn中的`StandardScaler`对象来完成相同的操作，如下所示：
- en: '[PRE33]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This will yield the same results with negligible numerical differences. For
    practical purposes, both methods will achieve the same thing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生相同的结果，数值差异可以忽略不计。对于实际应用而言，两种方法都会达到相同的效果。
- en: Although both ways of normalizing are appropriate, in the DataFrame directly
    or using a `StandardScaler` object, you should prefer using the `StandardScaler` object
    if you are working on a production application. Once the `StandardScaler` object
    uses the `fit()` method, it can be used on new, unseen, data easily by re-invoking
    `transform()` method; however, if we do it directly on the pandas DataFrame, we
    will have to manually store the mean and standard deviation somewhere and reload
    it every time we need to standardize new data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管直接在数据框中或使用`StandardScaler`对象进行标准化都合适，但如果你在处理生产应用时，应该更倾向于使用`StandardScaler`对象。一旦`StandardScaler`对象使用`fit()`方法，它可以通过重新调用`transform()`方法轻松应用于新的、未见过的数据；然而，如果直接在pandas数据框上进行操作，我们就必须手动存储均值和标准差，并在每次需要标准化新数据时重新加载它们。
- en: 'Now, for comparison purposes, *Figure 3.4* depicts the new ranges after the
    normalization of the data. If you look at the axes closely, you will notice that
    the position of the zero values are where most of the data is, that is, where
    the mean is. Therefore, the cluster of data is centered around a mean of zero:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了进行比较，*图 3.4* 显示了数据标准化后的新范围。如果仔细观察坐标轴，你会发现零值的位置是数据最多的地方，也就是均值所在的位置。因此，数据簇围绕零均值居中：
- en: '![](img/fbe8723e-6944-46f1-90e9-3656d0333c96.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbe8723e-6944-46f1-90e9-3656d0333c96.png)'
- en: Figure 3.4 – Scatter plot of the standardized columns *x*[5] and *x*[10] and
    their corresponding histograms
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 标准化列 *x*[5] 和 *x*[10] 的散点图及其相应的直方图
- en: Notice, again, that in *Figure 3.4*, after applying the standardization process,
    the distribution of the data still does not change. But what if you actually want
    to change the distribution of the data? Keep reading on to the next section.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，在*图 3.4* 中，应用标准化过程后，数据的分布仍然没有变化。但是如果你确实想改变数据的分布呢？请继续阅读下一节。
- en: Altering the distribution of data
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改变数据分布
- en: It has been demonstrated that changing the distribution of the targets, particularly
    in the case of regression, can have positive benefits in the performance of a
    learning algorithm (Andrews, D. F., et al. (1971)).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，改变目标的分布，特别是在回归的情况下，可以在学习算法的性能上带来积极的效果（Andrews, D. F., 等人，1971年）。
- en: Here, we'll discuss one particularly useful transformation known as **Quantile
    Transformation**. This methodology aims to look at the data and manipulate it
    in such a way that its histogram follows either a **normal** distribution or a
    **uniform** distribution. It achieves this by looking at estimates of quantiles.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论一种特别有用的转换方法，称为**分位数转换**。这种方法的目标是查看数据并以某种方式对其进行处理，使得其直方图遵循**正态**分布或**均匀**分布。它通过查看分位数估计值来实现这一点。
- en: 'We can use the following commands to transform the same data as in the previous
    section:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来转换与上一节相同的数据：
- en: '[PRE34]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This will effectively map the data into a new distribution, namely, a normal
    distribution.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将有效地将数据映射到一个新的分布，即正态分布。
- en: Here, the term **normal distribution** refers to a Gaussian-like **probability
    density function** (**PDF**). This is a classic distribution found in any statistics
    textbook. It is usually identified by its bell-like shape when plotted.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**正态分布**一词指的是类似高斯的**概率密度函数**（**PDF**）。这是任何统计学教科书中都能找到的经典分布。通常，当绘制时，它会呈现出钟形曲线的形状。
- en: Note that we are also using the `fit_transform()` method, which does both `fit()`
    and `transform()` at the same time, which is convenient.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还使用了`fit_transform()`方法，它同时执行`fit()`和`transform()`，这样做很方便。
- en: 'As can be seen in *Figure 3.5*, the variable related to cholesterol data, *x*[5],
    was easily transformed into a normal distribution with a bell shape. However,
    for *x*[10], the heavy presence of data in a particular region causes the distribution
    to have a bell shape, but with a long tail, which is not ideal:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 3.5 所示，与胆固醇数据相关的变量 *x*[5] 很容易被转化为具有钟形的正态分布。然而，对于 *x*[10]，在某个特定区域数据的密集出现使得分布呈现钟形，但带有长尾，这种情况并不理想：
- en: '![](img/86423dc1-72ee-4d89-b0dc-0a3c8fbabc76.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86423dc1-72ee-4d89-b0dc-0a3c8fbabc76.png)'
- en: Figure 3.5 – Scatter plot of the normally transformed columns *x*[5] and *x*[10] and
    their corresponding Gaussian-like histograms
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 正态变换后的列 *x*[5] 和 *x*[10] 的散点图及其对应的类高斯直方图
- en: 'The process of transforming the data for a uniform distribution is very similar.
    We simply need to make a small change in one line, on the `QuantileTransformer()`
    constructor, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转化为均匀分布的过程非常相似。我们只需在`QuantileTransformer()`构造函数中的一行做一个小小的修改，如下所示：
- en: '[PRE35]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, the data is transformed into a uniform distribution, as shown in *Figure
    3.6*:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据已经被转化为均匀分布，如图 3.6 所示：
- en: '![](img/5052596d-f5ba-4467-b5f6-19630b3510d8.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5052596d-f5ba-4467-b5f6-19630b3510d8.png)'
- en: Figure 3.6 – Scatter plot of the uniformly transformed columns *x*[5] and *x*[10] and
    their corresponding uniform histograms
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 均匀变换后的列 *x*[5] 和 *x*[10] 的散点图及其对应的均匀直方图
- en: From the figure, we can see that the data has been uniformly distributed across
    each variable. Once again, the clustering of data in a particular region has the
    effect of causing a large concentration of values in the same space, which is
    not ideal. This artifact also creates a gap in the distribution of the data that
    is usually difficult to handle, unless we use techniques to augment the data,
    which we'll discuss next.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，数据在每个变量上的分布已经变得均匀。再次强调，数据在特定区域的聚集效应导致同一区域内有大量值集中，这种情况并不理想。这个伪影还会在数据的分布中造成空隙，通常很难处理，除非我们使用数据增强技术，接下来我们将讨论这一点。
- en: Data augmentation
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'Now that you have learned how to process the data to have specific distributions,
    it is important for you to know about data augmentation, which is usually associated
    with missing data or high-dimensional data. Traditional machine learning algorithms
    may have problems dealing with data where the number of dimensions surpasses the
    number of samples available. The problem is not particular to all deep learning
    algorithms, but some algorithms have a much more difficult time learning to model
    a problem that has more variables to figure out than samples to work on. We have
    a few options to correct that: either we reduce the dimensions or variables (see
    the following section) or we increase the samples in our dataset (this section).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会如何处理数据以使其具备特定的分布，接下来你需要了解数据增强，通常它与缺失数据或高维数据相关。传统的机器学习算法在处理维度超过样本数的数据时可能会遇到问题。这个问题并不适用于所有深度学习算法，但一些算法在学习建模时，若变量数目大于可用样本数目，学习就会变得更加困难。为了解决这个问题，我们有几个选择：要么减少维度或变量（参见下一节），要么增加数据集中的样本量（本节讨论）。
- en: One of the tools for adding more data is known as **data augmentation **(Van
    Dyk, D. A., and Meng, X. L. (2001)).  In this section, we will use the `MNIST`
    dataset to exemplify a few techniques for data augmentation that are particular
    to images but can be conceptually extended to other types of data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 增加更多数据的工具之一被称为**数据增强**（Van Dyk, D. A., 和 Meng, X. L. (2001)）。在本节中，我们将使用`MNIST`数据集来举例说明一些特定于图像的数据增强技术，但这些技术的概念可以扩展到其他类型的数据。
- en: 'We will cover the basics: adding noise, rotating, and rescaling. That is, from
    one original example, we will produce three new, different images of numerals.
    We will use the image processing library known as `scikit image`.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖基本内容：添加噪声、旋转和重缩放。也就是说，从一个原始示例中，我们将生成三个新的、不一样的数字图像。我们将使用被称为`scikit image`的图像处理库。
- en: Rescaling
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重缩放
- en: 'We begin by reloading the `MNIST` dataset as we have done before:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像之前那样重新加载`MNIST`数据集：
- en: '[PRE36]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then we can simply invoke the `rescale()` method to create a rescaled image.
    The whole purpose behind resizing an image is to rescale it back to its original
    size because this makes the image look like a small resolution image of the original.
    It loses some of its characteristics in the process, but it can actually make
    a more robust deep learning model. That is, a model robust to the scale of objects,
    or in this case, the scale of numerals:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以简单地调用`rescale()`方法来创建一个重新缩放的图像。重新调整图像大小的整个目的是将图像重新缩放回原始尺寸，因为这使得图像看起来像是原始图像的小分辨率版本。虽然在这个过程中会失去一些特征，但实际上它能让深度学习模型变得更健壮。也就是说，模型能适应物体的尺度，或者在这种情况下，能适应数字的尺度：
- en: '[PRE37]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once we have `x` as the original image from which we will augment, we can do
    the scaling down and up as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了作为原始图像的`x`，我们可以按照以下方式进行缩放操作：
- en: '[PRE38]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here, the augmented image (rescaled) is in `x_`*.  *Notice that, in this case,
    the image is downscaled by a factor of two (50%) and then upscaled, also by a
    factor of two (200%). The `multichannel` argument is set to `false` since the
    images have only one single channel, meaning they are grayscale.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，增强后的图像（已缩放）是`x_`*。*请注意，在这个案例中，图像先按50%的比例缩小，再按200%的比例放大。由于图像只有一个单一的通道，即为灰度图，所以`multichannel`参数设置为`false`。
- en: When rescaling, be careful of rescaling by factors that give you exact divisions.
    For example, a 28 x 28 image that is downscaled by a factor of 0.5 goes down to
    14 x 14; this is good. But if we downscale by a factor of 0.3, it will go down
    to 8.4 x 8.4, which goes up to 9 x 9; this is not good because it can add unnecessary
    complications. Keep it simple.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新缩放时，要小心使用能够得到精确除法因子的缩放比例。例如，一个28 x 28的图像，如果按0.5的比例缩小，会变成14 x 14；这是好的。但是，如果按0.3的比例缩小，图像会变成8.4
    x 8.4，最终变成9 x 9；这不好，因为它可能会带来不必要的复杂性。保持简单。
- en: Besides rescaling, we can also modify the existing data slightly so as to have
    variations of the existing data without deviating much from the original, as we'll
    discuss next.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重新缩放，我们还可以稍微修改现有的数据，使其在不偏离原始图像太多的情况下，产生一些变体，接下来我们将讨论这一点。
- en: Adding noise
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加噪声
- en: Similarly, we can also contaminate the original image with additive Gaussian
    noise. This creates random patterns all over the image to simulate a camera problem
    or noisy acquisition. Here, we use it to also augment our dataset and, in the
    end, to produce a deep learning model that is robust against noise.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们还可以向原始图像添加高斯噪声。这会在整个图像上生成随机模式，以模拟摄像头问题或噪声采集。这里我们使用它来增强我们的数据集，并最终生成一个能够抗噪声的深度学习模型。
- en: 'For this, we use the `random_noise()` method as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用`random_noise()`方法，如下所示：
- en: '[PRE39]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Once again, the augmented image (noisy) is in `x_`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，增强后的图像（带噪声）是`x_`。
- en: Besides noise, we can also change the perspective of an image slightly so as
    to preserve the original shape at a different angle, as we'll discuss next.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 除了噪声之外，我们还可以稍微改变图像的视角，以便在不同角度下保留原始形状，接下来我们会讨论这一点。
- en: Rotating
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 旋转
- en: We can use a plain rotation effect on the images to have even more data. The
    rotation of images is a crucial part of learning good features from images. Larger
    datasets contain, naturally, many versions of images that are slightly rotated
    or fully rotated. If we do not have such images in our dataset, we can manually
    rotate them and augment our data.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对图像应用简单的旋转效果，以获得更多的数据。图像的旋转是学习良好特征的关键部分。更大的数据集通常包含许多略微旋转或完全旋转的图像。如果我们的数据集中没有这样的图像，我们可以手动旋转它们并增强我们的数据。
- en: 'For this, we use the `rotate()` method like so:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用`rotate()`方法，如下所示：
- en: '[PRE40]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In this example, the number `22` specifies the angle of rotation:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，数字`22`指定了旋转的角度：
- en: When you are augmenting your dataset, you may want to consider having multiple
    rotations at random angles.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在增强数据集时，你可能想要考虑在随机角度上进行多次旋转。
- en: '![](img/039c1b15-877e-421d-884e-3903881019c0.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/039c1b15-877e-421d-884e-3903881019c0.png)'
- en: Figure 3.7 – An example of the images produced with the preceding data augmentation
    techniques
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 使用前面提到的数据增强技术生成的图像示例
- en: The first column is the original numeral of the MNIST dataset. The second column
    shows the effect of rescaling. The third column shows the original plus additive
    Gaussian noise. The last column shows a rotation of 20 degrees (top) and -20 degrees
    (bottom).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列是MNIST数据集中的原始数字。第二列显示了重新缩放的效果。第三列显示了原始图像加上高斯噪声。最后一列显示了分别为20度（上）和-20度（下）的旋转效果。
- en: Other augmentation techniques
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他增强技术
- en: 'For image datasets, there are other ideas for augmenting data that include
    the following:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像数据集，还有其他一些增强数据的想法，包括以下内容：
- en: Changing the projection of the image
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变图像的投影
- en: Adding compression noise (quantizing the image)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加压缩噪声（对图像进行量化）
- en: Other types of noise besides Gaussian, such as salt and pepper, or multiplicative
    noise
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了高斯噪声，还有其他类型的噪声，比如椒盐噪声或乘法噪声
- en: The translation of the image by different distances at random
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机以不同距离平移图像
- en: But the most robust augmentation would be a combination of all of these!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 但最强大的增强方式是将所有这些方法结合起来！
- en: 'Images are fun because they are highly correlated in local areas. But for general
    non-image datasets, such as the heart disease dataset, we can augment data in
    other ways, for example:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图像很有趣，因为它们在局部区域高度相关。但对于一般的非图像数据集，例如心脏病数据集，我们可以通过其他方式来增强数据，例如：
- en: Adding low-variance Gaussian noise
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加低方差的高斯噪声
- en: Adding compression noise (quantization)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加压缩噪声（量化）
- en: Drawing new points from a calculated probability density function over the data
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从计算得到的概率密度函数中绘制新点
- en: 'For other special datasets, such as text-based data, we can also do the following:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他特殊数据集，如基于文本的数据，我们还可以执行以下操作：
- en: Replace some words with synonyms
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用同义词替换一些单词
- en: Remove some words
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除一些单词
- en: Add words that contain errors
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加包含错误的单词
- en: Remove punctuation (only if you do not care about proper language structures)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除标点符号（仅当您不在乎语言结构的正确性时）
- en: For more information on this and many other augmentation techniques, consult
    online resources on the latest advances pertaining to your specific type of data.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此以及许多其他增强技术的更多信息，请查阅关于您特定数据类型的最新进展的在线资源。
- en: Let's now dive into some techniques for dimensionality reduction that can be
    used to alleviate the problem of high-dimensional and highly correlated datasets.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨一些可以用于缓解高维和高度相关数据集问题的降维技术。
- en: Data dimensionality reduction
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据降维
- en: As pointed out before, if we have the problem of having more dimensions (or
    variables) than samples in our data, we can either augment the data or reduce
    the dimensionality of the data. Now, we will address the basics of the latter.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果我们面临数据中维度（或变量）大于样本数的问题，我们可以通过增强数据或减少数据的维度来解决这个问题。现在，我们将讨论后者的基础知识。
- en: We will look into reducing dimensions both in supervised and unsupervised ways
    with both small and large datasets.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究如何在有监督和无监督的情况下，通过小型和大型数据集减少维度。
- en: Supervised algorithms
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有监督算法
- en: Supervised algorithms for dimensionality reduction are so called because they
    take the labels of the data into account to find better representations. Such
    methods often yield good results. Perhaps the most popular kind is called **linear
    discriminant analysis** (**LDA**), which we'll discuss next.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督的降维算法之所以称为有监督，是因为它们考虑了数据的标签，以找到更好的表示。这类方法通常会产生良好的结果。也许最流行的一种是**线性判别分析**（**LDA**），我们接下来将讨论它。
- en: Linear discriminant analysis
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: Scikit learn has a `LinearDiscriminantAnalysis` class that can easily perform
    dimensionality reduction on a desired number of components.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn有一个`LinearDiscriminantAnalysis`类，可以轻松地对所需的组件数进行降维。
- en: By **number of components**, the number of dimensions desired is understood.
    The name comes from **principal component analysis** (**PCA**), which is a statistical
    approach that determines the eigenvectors and eigenvalues of the centered covariance
    matrix of a dataset; then, the largest eigenvalues associated with specific eigenvectors
    are known to be the most important, *principal*, components. When we use PCA to
    reduce to a specific number of components, we say that we want to keep those components
    that are the most important in a space induced by the eigenvalues and eigenvectors
    of the covariance matrix of the data.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**组件数**指的是所需的维度数量。这个名称来自于**主成分分析**（**PCA**），它是一种统计方法，用于确定数据集的中心化协方差矩阵的特征向量和特征值；然后，与特定特征向量关联的最大特征值被认为是最重要的、*主*成分。当我们使用PCA降到特定数量的组件时，我们说我们想要保留那些在由数据的协方差矩阵的特征值和特征向量诱导的空间中最重要的组件。'
- en: LDA and other dimensionality reduction techniques also have a similar philosophy
    in which they aim to find low-dimensional spaces (based on the number of components
    desired) that can better represent the data based on other properties of the data.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: LDA和其他降维技术也有类似的哲学思想，旨在找到低维空间（基于所需的组件数），并根据数据的其他属性更好地表示数据。
- en: If we use the heart disease dataset as an example, we can perform LDA to reduce
    the entire dataset from 13 dimensions to 2 dimensions, all the while using the
    labels [0, 1, 2, 3, 4] to inform the LDA algorithm how to better separate the
    groups represented by those labels.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以心脏病数据集为例，我们可以使用LDA将整个数据集从13维降至2维，同时使用标签[0, 1, 2, 3, 4]来指导LDA算法更好地分离这些标签所代表的组。
- en: 'To achieve this, we can follow these steps:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们可以按照以下步骤进行：
- en: 'First, we reload the data and drop the missing values:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们重新加载数据并删除缺失值：
- en: '[PRE41]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Notice that we did not have to deal with missing values before on the heart
    disease dataset because pandas automatically ignores missing values. But here,
    because we are strictly converting data into numbers, missing values will be converted
    to `NaN` since we are specifying `errors='coerce'`, which forces any errors in
    the conversion to become `NaN`. Consequently, with `dropna()`, we ignore rows
    with those values from our dataset because they will cause LDA to fail.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在心脏病数据集中我们之前不需要处理缺失值，因为pandas会自动忽略缺失值。但在这里，由于我们严格地将数据转换为数字，缺失值会被转换为`NaN`，因为我们指定了`errors='coerce'`，这会将任何转换错误强制转换为`NaN`。因此，通过`dropna()`，我们会忽略包含这些值的行，因为它们会导致LDA失败。
- en: 'Next, we prepare the `X` and `y` variables to contain the data and targets,
    respectively, and we perform LDA as follows:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们准备`X`和`y`变量，分别包含数据和目标，然后我们执行LDA，如下所示：
- en: '[PRE42]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In this example, `X_` contains the entire dataset represented in two dimensions,
    as given by `n_components=2`. The choice of two components is simply to illustrate
    graphically how the data looks. But you can change this to any number of components
    you desire.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`X_`包含了整个数据集，并通过`n_components=2`表示为二维数据。选择两个组件仅仅是为了图形上展示数据的外观。但你可以根据需要将其更改为任意数量的组件。
- en: '*Figure 3.8* depicts how the 13-dimensional dataset looks if compressed, or
    reduced, down to two dimensions:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.8* 展示了如果将13维数据集压缩或降至二维后的效果：'
- en: '![](img/81bc6b69-a2d6-4c3d-ab8a-038d7f2b0bbe.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81bc6b69-a2d6-4c3d-ab8a-038d7f2b0bbe.png)'
- en: Figure 3.8 – Reducing dimensions from 13 to 2 using LDA
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 使用LDA将维度从13降至2
- en: Notice how the values with 0 (no heart disease) are mostly clustered toward
    the left side, while the rest of the values (that is, 1, 2, 3, and 4, which represent
    heart disease) seem to cluster toward the right side. This is a nice property
    that was not observed in *Figures* *3.2* to *3.6* when we picked two columns out
    of the 13.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，值为0（没有心脏病）的点大多数聚集在左侧，而其余的值（即1、2、3和4，表示心脏病）则似乎聚集在右侧。这是一个很好的特性，在我们从13个维度中选择两列时，*图3.2*
    到 *图3.6* 中并没有观察到这种现象。
- en: Technically speaking, the relevant information of the 13 dimensions is still
    contained in the LDA-induced two dimensions. If the data seems to be separable
    in these low-dimensional representations, a deep learning algorithm may have a
    good chance of learning representations to classify or regress on the data with
    high performance.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，13维度的相关信息仍然保留在LDA所生成的二维空间中。如果数据在这些低维表示中似乎可以分离，深度学习算法可能有很大机会学习到表现良好的分类或回归表示。
- en: While LDA can offer a very nice way to perform dimensionality reduction informed
    by the labels in the data, we might not always have labeled data, or we may not
    want to use the labels that we have. In those cases we can, and we should, explore
    other robust methodologies that require no label information, such as unsupervised
    techniques, which we'll discuss next.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LDA可以提供一种非常好的方法，通过数据中的标签进行降维，但我们并不总是拥有标注数据，或者我们可能不希望使用现有的标签。在这种情况下，我们可以并且应该探索其他不需要标签信息的鲁棒方法，比如无监督技术，接下来我们将讨论这些技术。
- en: Unsupervised techniques
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督 技术
- en: Unsupervised techniques are the most popular methods because they need no prior
    information about labels. We begin with a kernelized version of PCA and then we
    move on to methods that operate on larger datasets.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督技术是最受欢迎的方法，因为它们不需要关于标签的先验信息。我们从PCA的核化版本开始，然后再处理适用于更大数据集的方法。
- en: Kernel PCA
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核PCA
- en: This variant of PCA uses kernel methods to estimate distances, variances, and
    other parameters to determine the major components of the data (Schölkopf, B.,
    et al. (1997)). It may take a bit more time to produce a solution than regular
    PCA, but it is very much worth using it over traditional PCA.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 PCA 的变种使用核方法来估计距离、方差和其他参数，以确定数据的主要成分（Schölkopf, B. 等人，（1997））。与常规 PCA 相比，它可能需要更多的时间来得出解决方案，但相对于传统的
    PCA，使用它非常值得。
- en: 'The `KernelPCA` class of scikit-learn can be used as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 `KernelPCA` 类可以按如下方式使用：
- en: '[PRE43]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Again, we use two dimensions as the new space, and we use a `''linear''` kernel.
    Other popular choices for the kernel include the following:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们使用两个维度作为新的空间，并使用 `'linear'` 核函数。核函数的其他常见选择包括：
- en: '`''rbf''` for a radial basis function kernel'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''rbf''` 用于径向基函数核'
- en: '`''poly''` for a polynomial kernel'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''poly''` 用于多项式核函数'
- en: Personally, I like the `'rbf'` kernel in general, because it is more powerful
    and robust. But oftentimes, you spend valuable time trying to determine the best
    value for the parameter *γ*, which is how wide the bell of the radial basis function
    is. If you have the time, try `'rbf'` and experiment with the parameter `gamma`.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我通常喜欢使用 `'rbf'` 核函数，因为它更强大且稳健。但很多时候，你会花费宝贵的时间来确定参数 *γ* 的最佳值，*γ* 决定了径向基函数的“钟形”宽度。如果你有时间，可以尝试
    `'rbf'` 并实验参数 `gamma`。
- en: 'The result of using kernel PCA is shown in *Figure 3.9*. The diagram again
    shows a clustering arrangement of the negative class (no heart disease, a value
    of 0) toward the bottom left of the KPCA-induced space. The positive class (heart
    disease, values ≥ 1) tends to cluster upward:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 使用核 PCA 的结果如 *图 3.9* 所示。图中再次显示了负类（无心脏病，值为 0）在 KPCA 诱导空间的左下方的聚类排列。正类（心脏病，值 ≥
    1）则倾向于向上聚类：
- en: '![](img/88fdcb86-dfba-4e09-8355-7df6981cc28c.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88fdcb86-dfba-4e09-8355-7df6981cc28c.png)'
- en: Figure 3.9 – Reducing dimensions with kernel PCA from 13 down to 2
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 使用核 PCA 将维度从 13 降到 2
- en: Compared to *Figure 3.8*, LDA produces a slightly better space where the groups
    can be separated. However, KPCA does a good job in spite of now knowing the actual
    target classes. Now, LDA and KPCA might take no time on small datasets, but what
    if we have a lot of data? We will discuss some options next.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 与 *图 3.8* 相比，LDA 产生了一个略微更好的空间，可以将不同组分隔开。然而，尽管现在不知道实际的目标类别，KPCA 仍然做得很好。现在，LDA
    和 KPCA 在小数据集上可能几乎不需要时间，但如果数据量很大呢？我们接下来将讨论一些选项。
- en: Large datasets
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大数据集
- en: The previous examples will work well with moderate-sized datasets. However,
    when dealing with very large datasets, that is, with many dimensions or many samples,
    some algorithms may not function at their best. In the worst case, they will fail
    to produce a solution. The next two unsupervised algorithms are designed to function
    well for large datasets by using a technique called **batch training**. This technique
    is well known and has been applied in machine learning successfully (Hinton, G.
    E. (2012)).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子在中等大小的数据集上效果很好。然而，在处理非常大的数据集时，即维度或样本数量非常多时，一些算法可能无法发挥最佳效果。在最坏的情况下，它们可能无法得出解决方案。接下来的两个无监督算法通过使用一种名为
    **批量训练** 的技术，旨在对大数据集表现良好。这种技术是众所周知的，并已成功应用于机器学习（Hinton, G. E.，（2012））。
- en: The main idea is to divide the dataset into small (mini) batches and partially
    make progress toward finding a global solution to the problem at hand.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思路是将数据集划分为小（迷你）批次，并逐步向寻找问题的全局解推进。
- en: Sparse PCA
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏主成分分析（Sparse PCA）
- en: We'll first look into a sparse-coding version of PCA available in scikit-learn
    as `MiniBatchSparsePCA`. This algorithm will determine the best transformation
    into a subspace that satisfies a sparsity constraint.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看看 scikit-learn 中的稀疏编码版本的 PCA，即 `MiniBatchSparsePCA`。该算法将确定满足稀疏性约束的最佳子空间转换。
- en: '**Sparsity** is a property of matrices (or vectors) in which most of the elements
    are zeros. The opposite of sparsity is density. We like sparsity in deep learning
    because we do a lot of tensor (vector) multiplications, and if some of the elements
    are zeros, we do not have to perform those multiplications, thus saving time and
    optimizing for speed.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏性** 是矩阵（或向量）的一种特性，其中大多数元素是零。稀疏性的反面是密集性。在深度学习中，我们喜欢稀疏性，因为我们进行大量的张量（向量）乘法，如果某些元素为零，我们就不需要执行这些乘法，从而节省时间并优化速度。'
- en: 'Follow the next steps in order to use the `MNIST` dataset and reduce its dimensions,
    since it has 784 dimensions and 70,000 samples. It is large enough, but even larger
    datasets can also be used:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用 `MNIST` 数据集并减少其维度，因为它有 784 个维度和 70,000 个样本。数据集足够大，但即使是更大的数据集也可以使用：
- en: 'We begin by reloading the data and preparing it for the sparse PCA encoding:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从重新加载数据并为稀疏PCA编码做准备开始：
- en: '[PRE44]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then we perform the dimensionality reduction as follows:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们按如下方式进行降维：
- en: '[PRE45]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here, the `MiniBatchSparsePCA()` constructor takes three arguments:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`MiniBatchSparsePCA()` 构造函数接受三个参数：
- en: '`n_components`, which we set to 2 for visualization purposes.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components`，我们为了可视化的目的将其设置为 2。'
- en: '`batch_size` determines how many samples the algorithm will use at a time.
    We set it to `50`, but larger numbers may cause the algorithm to slow down.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` 决定算法每次使用多少样本。我们将其设置为 `50`，但更大的数字可能会导致算法变慢。'
- en: '`normalize_components` refers to the preprocessing of the data by *centering*
    it, that is, making it have a zero mean and a unit variance; we recommend doing
    this every time, especially if you have data that is highly correlated, such as
    images.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_components` 是指通过*中心化*数据进行预处理，即使其均值为零，方差为单位；我们建议每次都进行此操作，特别是当你的数据高度相关时，比如图像数据。'
- en: 'The `MNIST` dataset transformed using sparse PCA looks as depicted in *Figure
    3.10*:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST` 数据集通过稀疏PCA变换后，如*图 3.10*所示：'
- en: '![](img/452ad7f6-1aeb-4443-a433-7b36d4a3e64c.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/452ad7f6-1aeb-4443-a433-7b36d4a3e64c.png)'
- en: Figure 3.10 – MNIST dataset reduced to two dimensions using sparse PCA
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 使用稀疏PCA将 MNIST 数据集降维到二维
- en: As you can see, the separation between classes is not perfectly clear. There
    are some definitive clusters of digits, but it does not seem like a straightforward
    task due to the overlap between groups. This is caused in part by the fact that
    many digits may look alike. It would make sense to have the numerals 1 and 7 clustered
    together (the left side up and down), or 3 and 8 (the middle and up).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，类别之间的分离并不完全清晰。虽然有一些明显的数字聚类，但由于组之间的重叠，看起来并不是一项简单的任务。这部分是由于许多数字可能相似。例如，数字
    1 和 7 可能会聚在一起（左右上下），或者 3 和 8 可能会聚在一起（中间和上面）。
- en: But let's also use another popular and useful algorithm called Dictionary Learning.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也可以使用另一个流行且有用的算法，称为字典学习。
- en: Dictionary Learning
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字典学习
- en: Dictionary Learning is the process of learning the basis of transformations,
    called **dictionaries**, by using a process that can easily scale to very large
    datasets (Mairal, J., et al. (2009)).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 字典学习是通过使用一种可以轻松扩展到非常大数据集的过程，学习变换的基础（称为**字典**）（Mairal, J., 等, 2009）。
- en: This was not possible with PCA-based algorithms, but this technique remains
    powerful and recently received the *Test of Time* award at one of the major conferences
    in the world, the *2019* *Intern**ational Conference in Machine Learning. *
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于PCA的算法无法做到这一点，但这种技术仍然非常强大，最近在全球主要会议之一——*2019年国际机器学习大会*上获得了*终身成就奖*。
- en: 'The algorithm is available in scikit-learn through the `MiniBatchDictionaryLearning` class.
    We can use it as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法可以通过 `MiniBatchDictionaryLearning` 类在 scikit-learn 中使用。我们可以如下使用：
- en: '[PRE46]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The constructor `MiniBatchDictionaryLearning()` takes on similar arguments
    as `MiniBatchSparsePCA()` with the same meaning. The results of the learned space
    are shown in *Figure 3.11*:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数 `MiniBatchDictionaryLearning()` 接受与 `MiniBatchSparsePCA()` 相似的参数，且意义相同。所学习到的空间结果如*图
    3.11*所示：
- en: '![](img/c2c39450-0acb-48c5-9e7a-a5fcad716e5b.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2c39450-0acb-48c5-9e7a-a5fcad716e5b.png)'
- en: Figure 3.11 – Dimensionality reduction of MNIST data down to two dimensions
    using Dictionary Learning
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – 使用字典学习将 MNIST 数据降维到二维
- en: As can be seen, there is a significant overlap among classes even if there are
    clearly defined clusters. This could lead to poor performance results if this
    data, the two-dimensional data, is used as input to train a classifier. This does
    not mean that algorithms are bad, necessarily. What this could mean is that, maybe,
    two dimensions are not the best choice of final dimensions. Continue reading to
    learn more about this.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，即使存在明显定义的聚类，类别之间仍然存在显著的重叠。如果将此数据（二维数据）用作输入来训练分类器，可能会导致性能不佳。这并不意味着算法不好。可能的原因是，二维数据可能不是最终维度的最佳选择。继续阅读以了解更多信息。
- en: Regarding the number of dimensions
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于维度的数量
- en: Reducing dimensions is not always a necessary step. But it is highly recommended
    for data that is highly correlated, for example, images.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 降维并不是总是必要的步骤，但对于高度相关的数据，例如图像数据，强烈建议进行降维。
- en: All the discussed dimensionality reduction techniques actually strive to remove
    redundant information in the data and preserve the important content. If we ask
    an algorithm to reduce the dimensions of our non-correlated, non-redundant dataset
    from 13 dimensions to 2, that sounds a bit risky; perhaps 8 or 9 would be a better
    choice.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 所有讨论过的降维技术其实都是力图去除数据中的冗余信息，并保留重要内容。如果我们要求算法将一个非相关、非冗余的数据集从13维降到2维，那听起来有些冒险；或许选择8维或9维会是更好的选择。
- en: No serious-minded machine learner would try to reduce a non-correlated, non-redundant
    dataset with 784 dimensions to only 2\. Even if the data is highly correlated
    and redundant, like the `MNIST` dataset, asking to go from 784 down to 2 is a
    big stretch. It is a very risky decision that may get rid of important, discriminant,
    relevant information; perhaps 50 or 100 would be a better choice.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 任何认真的机器学习者都不会尝试将一个784维的非相关、非冗余数据集降到仅仅2维。即使数据高度相关且冗余，比如`MNIST`数据集，要求从784维降到2维也是一个极大的挑战。这是一个非常冒险的决定，可能会丢失重要的、具有判别力的相关信息；或许选择50维或100维会是更好的选择。
- en: There is no general way of finding which amount of dimensions is good. It is
    a process that requires experimentation. If you want to become good at this, you
    must do your due diligence and at least try two or more experiments with different
    dimensions.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 没有普适的方法来找出合适的维度数量。这是一个需要实验的过程。如果你想在这方面做到优秀，你必须尽职尽责，至少进行两个或更多不同维度的实验。
- en: Ethical implications of manipulating data
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作数据的伦理影响
- en: There are many ethical implications and risks when manipulating data that you
    need to know. We live in a world where most deep learning algorithms will have
    to be corrected, by re-training them, because it was found that they were biased
    or unfair. That is very unfortunate; you want to be a person who exercises responsible AI
    and produces carefully thought out models.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数据时有许多伦理影响和风险，你需要了解。我们生活在一个大多数深度学习算法必须经过纠正的世界中，因为发现它们存在偏见或不公平现象，这通常需要通过重新训练来进行修正。这是非常不幸的；你需要成为一个负责任地使用人工智能并生产经过深思熟虑模型的人。
- en: When manipulating data, be careful about removing outliers from the data just
    because you think they are decreasing your model's performance. Sometimes, outliers
    represent information about protected groups or minorities, and removing those
    perpetuates unfairness and introduces bias toward the majority groups. Avoid removing
    outliers unless you are absolutely sure that they are errors caused by faulty
    sensors or human error.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作数据时，要小心仅仅因为认为异常值会影响模型性能就把它们移除。有时，异常值代表了保护群体或少数群体的信息，删除这些异常值会助长不公平现象，并且引入对多数群体的偏见。除非你完全确定这些异常值是由故障传感器或人为错误造成的，否则应避免删除异常值。
- en: Be careful of the way you transform the distribution of the data. Altering the
    distribution is fine in most cases, but if you are dealing with demographic data,
    you need to pay close attention to what you are transforming.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 小心你对数据分布的转化方式。大多数情况下，改变数据的分布是可以的，但如果你处理的是人口统计数据，必须特别关注你所做的转化。
- en: When dealing with demographic information such as gender, encoding female and
    male as 0 and 1 could be risky if we are considering proportions; we need to be
    careful not to promote equality (or inequality) that does not reflect the reality
    of the community that will use your models. The exception is when our current
    reality shows unlawful discrimination, exclusion, and bias. Then, our models (based
    on our data) should not reflect this reality, but the lawful reality that our
    community wants. That is, we will prepare good data to create models not to perpetuate
    societal problems, but models that will reflect the society we want to become.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 处理人口信息（如性别）时，如果将女性和男性分别编码为0和1，在考虑比例时可能会存在风险；我们需要小心，不要推广一种不反映使用你模型的社区现实的平等（或不平等）。唯一的例外是，当我们当前的现实展示出非法歧视、排斥和偏见时。在这种情况下，我们的模型（基于我们的数据）不应反映这一现实，而应反映我们社区希望实现的合法现实。也就是说，我们将准备良好的数据，创建反映我们期望成为的社会的模型，而不是延续社会问题的模型。
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed many data manipulation techniques that we will
    come back to use all the time. It is good for you to spend time doing this now
    rather than later. It will make our modeling of deep learning architectures easier.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了许多数据处理技术，这些技术我们将会在以后不断使用。现在花时间学习这些内容，而不是等到以后再学，对你来说是有益的。这将使我们在建模深度学习架构时更加容易。
- en: After reading this chapter, you are now able to manipulate and produce binary
    data for classification or for feature representation. You also know how to deal
    with categorical data and labels and prepare it for classification or regression.
    When you have real-valued data, you now know how to identify statistical properties
    and how to normalize such data. If you ever have the problem of data that has
    non-normal or non-uniform distributions, now you know how to fix that. And if
    you ever encounter problems of not having enough data, you learned a few data
    augmentation techniques. Toward the end of this chapter, you learned some of the
    most popular dimensionality reduction techniques. You will learn more of these
    along the road, for example, when we talk about autoencoders, which can be used
    for dimensionality reduction as well. But sit tight, we will get there in due
    time.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 读完本章后，你现在已经能够处理和生成用于分类或特征表示的二元数据。你也知道如何处理分类数据和标签，并为分类或回归做准备。当你有实值数据时，你现在知道如何识别统计属性，以及如何规范化这些数据。如果你遇到具有非正态或非均匀分布的数据问题，现在你知道如何解决它。如果你遇到数据不足的问题，你学会了一些数据增强技术。在本章的结尾，你了解了一些最流行的降维技术。你将在以后的学习中学到更多相关内容，例如我们谈到的自动编码器，也可以用于降维。但请耐心等待，我们会在适当的时候深入讲解。
- en: For now, we will continue our journey toward the next introductory topic about
    basic machine learning. [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning
    from Data,* introduces the most elementary concepts around the theory of deep
    learning, including measuring performance on regression and classification, as
    well as the identification of overfitting. However, before we go there, please
    try to quiz yourself with the following questions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续前进，开始下一个关于基础机器学习的介绍性主题。[第4章](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml)，*从数据中学习*，介绍了深度学习理论中的最基本概念，包括回归和分类中的性能测量，以及过拟合的识别。然而，在我们深入之前，请先尝试用以下问题测试自己。
- en: Questions and answers
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**Which variables of the heart dataset are suitable for regression?**'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**心脏数据集中哪些变量适合回归？**'
- en: Actually, all of them. But the ideal ones are those that are real-valued.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，所有变量都是如此。但理想的变量是那些实值变量。
- en: '**Does the scaling of the data change the distribution of the data? **'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据的缩放会改变数据的分布吗？**'
- en: No. The distribution remains the same. Statistical metrics such as the mean
    and variance may change, but the distribution remains the same.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 不会。分布保持不变。统计量，如均值和方差，可能会发生变化，但分布保持不变。
- en: '**What is the main difference between supervised and unsupervised dimensionality
    reduction methods?**'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监督学习和无监督学习的降维方法之间的主要区别是什么？**'
- en: Supervised algorithms use the target labels, while unsupervised algorithms do
    not need that information.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法使用目标标签，而无监督算法则不需要这些信息。
- en: '**When is it better to use batch-based dimensionality reduction?**'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**什么时候使用基于批次的降维方法更好？**'
- en: When you have very large datasets.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拥有非常大的数据集时。
- en: References
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Cleveland Heart Disease Dataset (1988). Principal investigators:'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克利夫兰心脏病数据集（1988）。首席研究人员：
- en: 'a. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 匈牙利心脏病研究所。布达佩斯：安德拉斯·贾诺西，医学博士。
- en: 'b. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 苏黎世大学医院，瑞士：威廉·斯坦布伦，医学博士。
- en: 'c. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 巴塞尔大学医院，瑞士：马蒂亚斯·菲斯特尔，医学博士。
- en: 'd. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert
    Detrano, M.D., Ph.D.'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. V.A. 医疗中心，长滩和克利夫兰诊所基金会：罗伯特·德特兰诺，医学博士，博士。
- en: Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.J., Sandhu,
    S., Guppy, K.H., Lee, S. and Froelicher, V., (1989). International application
    of a new probability algorithm for the diagnosis of coronary artery disease. *The
    American journal of cardiology*, 64(5), 304-310.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.J., Sandhu,
    S., Guppy, K.H., Lee, S. 和 Froelicher, V., (1989)。冠状动脉疾病诊断中新概率算法的国际应用。*美国心脏病学杂志*，64(5)，304-310。
- en: Deng, L. (2012). The MNIST database of handwritten digit images for machine
    learning research (best of the web). *IEEE Signal Processing Magazine*, 29(6),
    141-142.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng, L. (2012). 用于机器学习研究的手写数字图像的MNIST数据库（最佳网络）。*IEEE信号处理杂志*，29(6)，141-142。
- en: Sezgin, M., and Sankur, B. (2004). Survey over image thresholding techniques
    and quantitative performance evaluation. *Journal of Electronic imaging*, 13(1),
    146-166.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sezgin, M., 和 Sankur, B. (2004). 图像阈值技术的综述与定量性能评估。*电子成像杂志*，13(1)，146-166。
- en: Potdar, K., Pardawala, T. S., and Pai, C. D. (2017). A comparative study of
    categorical variable encoding techniques for neural network classifiers. *International
    Journal of Computer Applications*, 175(4), 7-9.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Potdar, K., Pardawala, T. S., 和 Pai, C. D. (2017). 神经网络分类器的分类变量编码技术比较研究。*国际计算机应用杂志*，175(4)，7-9。
- en: Hanin, B. (2018). Which neural net architectures give rise to exploding and
    vanishing gradients?. In*Advances in Neural Information Processing Systems* (pp.
    582-591).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin, B. (2018). 哪些神经网络架构导致梯度爆炸与消失？收录于 *神经信息处理系统进展*（第582-591页）。
- en: Andrews, D. F., Gnanadesikan, R., and Warner, J. L. (1971). Transformations
    of multivariate data. *Biometrics*, 825-840.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrews, D. F., Gnanadesikan, R., 和 Warner, J. L. (1971). 多元数据的变换。*生物统计学*，825-840。
- en: Van Dyk, D. A., and Meng, X. L. (2001). The art of data augmentation. *Journal
    of Computational and Graphical Statistics*, 10(1), 1-50.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Dyk, D. A., 和 Meng, X. L. (2001). 数据增强的艺术。*计算与图形统计学杂志*，10(1)，1-50。
- en: Schölkopf, B., Smola, A., and Müller, K. R. (1997, October). Kernel principal
    component analysis. In *International conference on artificial neural networks*
    (pp. 583-588). Springer, Berlin, Heidelberg.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schölkopf, B., Smola, A., 和 Müller, K. R. (1997年10月). 核主成分分析。收录于 *国际人工神经网络会议*（第583-588页）。Springer，柏林，海德堡。
- en: 'Hinton, G. E. (2012). A practical guide to training restricted Boltzmann machines.
    In *Neural networks: Tricks of the trade* (pp. 599-619). Springer, Berlin, Heidelberg.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G. E. (2012). 限制玻尔兹曼机训练实用指南。收录于 *神经网络：行业技巧*（第599-619页）。Springer，柏林，海德堡。
- en: Mairal, J., Bach, F., Ponce, J., and Sapiro, G. (June, 2009). Online dictionary
    learning for sparse coding. In *Proceedings of the 26th annual international conference
    on machine learning* (pp. 689-696). ACM.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairal, J., Bach, F., Ponce, J., 和 Sapiro, G. (2009年6月). 稀疏编码的在线字典学习。收录于 *第26届年度国际机器学习大会论文集*（第689-696页）。ACM。
