- en: A Quick Refresher
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速回顾
- en: Welcome to *Hands-On Neural Network Development Using C#*. I want to thank you
    for purchasing this book and for taking this journey with us. It seems as if,
    everywhere you turn, everywhere you go, all you hear and read about is machine
    learning, artificial intelligence, deep learning, neuron this, artificial that,
    and on and on. And, to add to all that excitement, everyone you talk to has a
    slightly different idea about the meaning of each of those terms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到《使用C#进行动手神经网络开发》。我要感谢您购买这本书，并与我们一同踏上这段旅程。似乎无论你转向何方，无论你走到哪里，你所听到和读到的都是机器学习、人工智能、深度学习、神经元这个、人工那个，等等。而且，为了增加所有的兴奋感，你与每个人交谈时，每个人对每个这些术语的含义都有略微不同的看法。
- en: In this chapter, we are going to go over some very basic neural network terminology
    to set the stage for future chapters. We need to be speaking the same language,
    just to make sure that everything we do in later chapters is crystal clear.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些非常基础的神经网络术语，为后续章节做好准备。我们需要确保我们说的是同一种语言，以确保我们在后续章节中所做的一切都清晰明了。
- en: I should also let you know that the goal of the book is to get you, a C# developer,
    up and running as fast as possible. To do this, we will use as many open source
    libraries as possible. We must do a few custom applications, but we've provided
    the source code for these as well. In all cases, we want you to be able to add
    this functionality to your applications with maximal speed and minimal effort.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我还应该让你知道，这本书的目标是尽可能快地将你，一个C#开发者，培养起来。为了做到这一点，我们将尽可能多地使用开源库。我们必须做一些自定义应用程序，但我们已经提供了这些应用程序的源代码。在所有情况下，我们都希望你能以最快速度、最少的努力将这个功能添加到你的应用程序中。
- en: OK, let's begin.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始吧。
- en: Neural networks have been around for very many years but have made a resurgence
    over the past few years and are now a hot topic. And that, my friends, is why
    this book is being written. The goal here is to help you get through the weeds
    and into the open so you can navigate your neural path to success. There is a
    specific focus in this book on C# .NET developers. I wanted to make sure that
    the C# developers out there had handy resources that could be of some help in
    their projects, rather than the Python, R, and MATLAB code we more commonly see.
    If you have Visual Studio installed and a strong desire to learn, you are ready
    to begin your journey.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经存在了很长时间，但在过去几年中又重新兴起，现在成为了热门话题。这就是为什么这本书要被写出来的原因。我们的目标是帮助您摆脱困境，进入开阔地带，以便您能够成功导航您的神经网络之路。这本书特别关注C#
    .NET开发者。我想确保那些C#开发者能够手头上有一些有用的资源，可以在他们的项目中提供帮助，而不是我们更常见的Python、R和MATLAB代码。如果你已经安装了Visual
    Studio，并且有强烈的求知欲望，你就可以开始你的旅程了。
- en: First, let's make sure we're clear on a couple of things. In writing this book,
    the assumption was made that you, the reader, had limited exposure to neural networks.
    If you do have some exposure, that is great; you may feel free to jump to the
    sections that interest you the most. I also assumed that you are an experienced
    C# developer, and have built applications using C#, .NET, and Visual Studio, although
    I made no assumptions as to which versions of each you may have used. The goal
    is not about C# syntax, the .NET framework, or Visual Studio itself. Once again,
    the purpose is to get as many valuable resources into the hands of developers,
    so they can embellish their code and create world-class applications.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们确保我们对一些事情有清晰的认识。在编写这本书时，假设读者你对神经网络接触有限。如果你有一些接触，那很好；你可以自由地跳到你最感兴趣的章节。我还假设你是一位经验丰富的C#开发者，并使用C#、.NET和Visual
    Studio构建了应用程序，尽管我没有假设你使用过哪些版本。目标不是关于C#语法、.NET框架或Visual Studio本身。再次强调，目的是将尽可能多的宝贵资源交给开发者，让他们能够丰富他们的代码并创建世界级的应用程序。
- en: Now that we've gotten that out of the way, I know you're excited to jump right
    in and start coding, but to make you productive, we first must spend some time
    going over some basics. A little bit of theory, some fascinating insights into
    the whys and wherefores, and we're going to throw in a few visuals along the way
    to help with the rough-and-tough dry stuff. Don't worry; we won't go too deep
    on the theory, and, in a few pages from here, you'll be writing and going through
    source code!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了这些问题，我知道你迫不及待地想要直接开始编码，但为了让你更有效率，我们首先必须花一些时间来复习一些基础知识。一点理论，一些关于原因和目的的迷人洞察，我们还会在过程中加入一些视觉元素来帮助理解那些枯燥的内容。别担心，我们不会在理论上走得太深，而且，从现在起几页之后，你将开始编写和审查源代码！
- en: Also, keep in mind that research in this area is rapidly evolving. What is the
    latest and greatest today is old news next month. Therefore, consider this book
    an overview of different research and opinions. It is not the be-all-and-end-all
    bible of everything neural network-related, nor should it be perceived to be.
    You are very likely to encounter someone else with different opinions from that
    of the writer. You're going to find people who will write apps and functions differently.
    That's great—gather all the information that you can, and make informed choices
    on your own. Only doing by that will you increase your knowledge base.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，这个领域的研究正在迅速发展。今天最新和最伟大的东西，下个月可能就变成了旧闻。因此，请将这本书视为不同研究和观点的概述。它不是关于神经网络相关一切的终极圣经，也不应该被这样看待。你很可能遇到持有不同观点的人。你会遇到那些会以不同方式编写应用程序和函数的人。那很好——收集你能收集到的所有信息，并根据自己的判断做出明智的选择。只有通过那样做，你才能增加你的知识库。
- en: 'This chapter will include the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包括以下主题：
- en: Neural network overview
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络概述
- en: The role of neural networks in today's enterprises
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在当今企业中的作用
- en: Types of learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习类型
- en: Understanding perceptions
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解感知
- en: Understanding activation functions
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解激活函数
- en: Understanding back propagation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Basic knowledge of C# is a must to understand the applications that we will
    develop in this book. Also, Microsoft Visual Studio (Any version) is a preferred
    software to develop applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们将在本书中开发的应用程序，必须具备C#的基本知识。此外，Microsoft Visual Studio（任何版本）是开发应用程序的首选软件。
- en: Neural network overview
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络概述
- en: Let's start by defining exactly what we are going to call a neural network.
    Let me first note that you may also hear a neural network called an **Artificial
    Neural Network** (**ANN**). Although personally I do not like the term *artificial*,
    we'll use those terms interchangeably throughout this book.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先明确一下我们将如何称呼神经网络。首先，我要指出，你可能也会听到神经网络被称为**人工神经网络**（**ANN**）。虽然我个人不喜欢“人工”这个术语，但在这本书中，我们将交替使用这些术语。
- en: '"Let''s state that a neural network, in its simplest form, is a system comprising
    several simple but highly interconnected elements; each processes information
    based upon their response to external inputs."'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: “让我们明确指出，在它的最简单形式中，神经网络是一个由几个简单但高度互联的元素组成的系统；每个元素都根据对外部输入的反应来处理信息。”
- en: Did you know that neural networks are more commonly, but loosely, modeled after
    the cerebral cortex of a mammalian brain? Why didn't I say that they were modeled
    after humans? Because there are many instances where biological and computational
    studies are used from brains from rats, monkeys, and, yes, humans. A large neural
    network may have hundreds or maybe even thousands of processing units, where as
    a mammalian brain has billions. It's the neurons that do the magic, and we could
    in fact write an entire book on that topic alone.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗，神经网络通常更常见地，但不是很严格地，是根据哺乳动物大脑的皮层来建模的吗？为什么我没有说它们是根据人类来建模的呢？因为有很多情况下，生物和计算研究使用了老鼠、猴子，甚至是人类的脑部。一个大的神经网络可能有数百甚至数千个处理单元，而哺乳动物大脑有数十亿个。是神经元在创造奇迹，实际上我们可以就这个话题写一本书。
- en: 'Here''s why I say they do all the magic: If I showed you a picture of Halle
    Berry, you would recognize her right away. You wouldn''t have time to analyze
    things; you would know based upon a lifetime of collected knowledge. Similarly,
    if I said the word *pizza* to you, you would have an immediate mental image and
    possibly even start to get hungry. How did all that happen just like that? Neurons!
    Even though the neural networks of today continue to gain in power and speed,
    they pale in comparison to the ultimate neural network of all time, the human
    brain. There is so much we do not yet know or understand about this neural network;
    just wait and see what neural networks will become once we do!'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，它们之所以能做所有这些魔法般的事情：如果给你看一张Halle Berry的照片，你会立刻认出她。你不会有时间去分析事物；你会根据一生积累的知识立刻知道。同样，如果我对你说出“pizza”这个词，你会有一个立即的脑中图像，甚至可能开始感到饿。这一切是如何突然发生的？是神经元！尽管今天的神经网络在力量和速度上仍在不断进步，但它们与人类大脑这个终极神经网络相比，相形见绌。我们对这个神经网络了解和理解的还很少；等着看当我们了解后，神经网络会变成什么样子！
- en: Neural networks are organized into *layers* made up of what are called **nodes** or
    **neurons**. These nodes are the neurons themselves and are interconnected (throughout
    this book we use the terms *nodes* and *neurons* interchangeably). Information
    is presented to the input layer, processed by one or more *hidden* layers, then
    given to the *output* layer for final (or continued further) processing—lather,
    rinse, repeat!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络被组织成由所谓的**节点**或**神经元**组成的**层**。这些节点就是神经元本身，并且相互连接（在这本书中，我们交替使用*节点*和*神经元*这两个术语）。信息被呈现给输入层，由一个或多个**隐藏**层进行处理，然后输出给**输出**层进行最终（或进一步）处理——重复这个过程！
- en: '*But what is a neuron*, you ask? Using the following diagram, let''s state
    this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '"但什么是神经元呢？"你可能会问？使用以下图表，让我们这样表述：'
- en: '"A neuron is the basic unit of computation in a neural network"'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '"神经元是神经网络中的基本计算单元"'
- en: As I mentioned earlier, a neuron is sometimes also referred to as a node or
    a unit. It receives input from other nodes or external sources and computes an
    output. Each input has an associated **weight** (**w1 and w2 below**), which is
    assigned based on its relative importance to the other inputs. The node applies
    a function *f*(an activation function, which we will learn more about later on)
    to the weighted sum of its inputs. Although that is an extreme oversimplification
    of what a neuron is and what it can do, that's basically it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，神经元有时也被称为节点或单元。它从其他节点或外部来源接收输入并计算输出。每个输入都有一个相关的**权重**（以下称为**w1和w2**），这个权重是根据其相对于其他输入的重要性分配的。节点将其输入的加权和应用于一个函数*f*（一个激活函数，我们稍后会了解更多），这就是神经元的基本功能。尽管这只是一个极端简化的神经元是什么以及它能做什么的描述，但基本上就是这样。
- en: 'Let''s look visually at the progression from a single neuron into a very deep
    learning network. Here is what a single neuron looks like visually based on our
    description:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地看看从单个神经元到非常深的深度学习网络的演变过程。以下是根据我们的描述，单个神经元的外观：
- en: '![](img/17f2aad3-9c5c-4751-a9e4-f7316eb26fd9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![更复杂的神经网络图](img/17f2aad3-9c5c-4751-a9e4-f7316eb26fd9.png)'
- en: 'Next, the following diagram shows a very simple neural network comprised of
    several neurons:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，以下图表展示了一个由几个神经元组成的非常简单的神经网络：
- en: '![](img/205853db-764e-4b05-8b8b-ef2441ae8e7e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](img/205853db-764e-4b05-8b8b-ef2441ae8e7e.png)'
- en: 'Here is a somewhat more complicated, or deeper, network:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个稍微复杂一些，或者说更深层次的网络：
- en: '![](img/798ffc50-fffc-4712-b001-8483da96fd50.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络训练图](img/798ffc50-fffc-4712-b001-8483da96fd50.png)'
- en: Neural network training
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: Now that we know what a neural network and neurons are, we should talk about
    what they do and how they do it. How does a neural network learn? Those of you
    with children already know the answer to this one. If you want your child to learn
    what a cat is, what do you do? You show them cats (pictures or real). You want
    your child to learn what a dog is? Show them dogs. A neural network is conceptually
    no different. It has a form of **learning rule** that will modify the incoming
    weights from the input layer, process them through the hidden layers, put them
    through an activation function, and hopefully will be able to identify, in our
    case, cats and dogs. And, if done correctly, the cat does not become a dog!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了神经网络和神经元是什么，我们应该谈谈它们的功能以及它们是如何工作的。神经网络是如何学习的？那些已经有孩子的你们已经知道这个答案了。如果你想让孩子知道猫是什么，你会怎么做？你给他们看猫（图片或真实的）。如果你想让孩子知道狗是什么，就给他们看狗。从概念上讲，神经网络并没有什么不同。它有一种**学习规则**，会修改从输入层传入的权重，通过隐藏层进行处理，然后通过激活函数，并希望能够在我们的案例中识别出猫和狗。而且，如果做得正确，猫不会变成狗！
- en: One of the most common learning rules with neural networks is what is known
    as the **delta rule**. This is a *supervised* rule that is invoked each time the
    network is presented with another learning pattern. Each time this happens it
    is called a **cycle** or **epoch**. The invocation of the rule will happen each
    time that input pattern goes through one or more *forward* propagation layers,
    and then through one or more *backward* propagation layers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中最常见的学习规则之一是所谓的**delta规则**。这是一个**监督**规则，每次网络被呈现另一个学习模式时都会被调用。每次发生这种情况时，它被称为**周期**或**时代**。规则的调用将在输入模式通过一个或多个**正向**传播层，然后通过一个或多个**反向**传播层时发生。
- en: More simply put, when a neural network is presented with an image it tries to
    determine what the answer might be. The difference between the correct answer
    and our guess is the **error **or **error rate**. Our objective is that the error
    rate gets either minimized or maximized. In the case of minimization, we need
    the error rate to be as close to 0 as possible for each guess. The closer we are
    to 0, the closer we are to success.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，当神经网络被呈现一个图像时，它会试图确定可能的答案。正确答案和我们的猜测之间的差异是**误差**或**误差率**。我们的目标是使误差率最小化或最大化。在最小化的情况下，我们需要误差率尽可能接近0，对于每一个猜测。我们越接近0，就越接近成功。
- en: As we progress, we undertake what is termed a **gradient descent**, meaning
    we continue along toward what is called the **global minimum**, our lowest possible
    error, which hopefully is paramount to *success*. We descend toward the global
    minimum.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们不断前进，我们进行所谓的**梯度下降**，这意味着我们继续朝着被称为**全局最小值**的方向前进，这是我们的最低可能的误差，希望这对**成功**至关重要。我们朝着全局最小值下降。
- en: Once the network itself is trained, and you are happy, the training cycle can
    be put to bed and you can move on to the testing cycle. During the testing cycle,
    only the forward propagation layer is used. The output of this process results
    in the *model* that will be used for further analysis. Again, no back propagation
    occurs during testing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络本身被训练，并且你满意，训练周期就可以结束，然后你可以进入测试周期。在测试周期中，只使用正向传播层。这个过程的结果将产生用于进一步分析的**模型**。同样，在测试期间不会发生反向传播。
- en: A visual guide to neural networks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络视觉指南
- en: 'In this section, I could type thousands of words trying to describe all of
    the combinations of neural networks and what they look like. However, no amount
    of words would do any better than the diagram that follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我可能需要输入成千上万的字来描述所有神经网络组合及其外观。然而，无论多少文字都不如下面的图表来得有效：
- en: '![](img/31b5e961-850a-4227-9931-72415a6f64a7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/31b5e961-850a-4227-9931-72415a6f64a7.png)'
- en: Reprinted with permission, Copyright Asimov Institute
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 经Asimov Institute许可重印，版权所有
- en: 'Source: http://www.asimovinstitute.org/neural-network-zoo/'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://www.asimovinstitute.org/neural-network-zoo/
- en: 'Let''s talk about a few of the more common networks from the previous diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈前一个图中的一些更常见的网络：
- en: '**Perceptron: **This is the simplest feed-forward neural network available,
    and, as you can see, it does not contain any hidden layers:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感知器**：这是最简单的前馈神经网络，正如你所见，它不包含任何隐藏层：'
- en: '![](img/02012cca-96c6-4433-9576-4ba50de40e1f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02012cca-96c6-4433-9576-4ba50de40e1f.png)'
- en: '**Feed-forward network: **This network is perhaps the simplest type of artificial
    neural network devised. It contains multiple neurons (nodes) arranged in **layers**.
    Nodes from adjacent layers have **connections** or **edges** between them. Each
    connection has **weights** associated with them:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈网络：**这种网络可能是设计得最简单的人工神经网络。它包含多个按**层**排列的神经元（节点）。相邻层的节点之间有**连接**或**边**。每个连接都与**权重**相关联：'
- en: '![](img/1e13ce5e-a990-4897-a3d5-bb01b4795996.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e13ce5e-a990-4897-a3d5-bb01b4795996.png)'
- en: '**Recurrent neural network (RNN): **RNNs are called *recurrent* because they
    perform the same task for every element of a sequence, with the output depending
    on the previous computations. They are also able to look back at previous steps,
    which form a sort of **short-term memory**:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络（RNN）：**RNN被称为*循环*，因为它们对序列中的每个元素执行相同的任务，输出取决于之前的计算。它们还能够回顾之前的步骤，这形成了一种**短期记忆**：'
- en: '![](img/82e76013-80f7-4d91-acb5-ffc8d981e4b1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82e76013-80f7-4d91-acb5-ffc8d981e4b1.png)'
- en: The role of neural networks in today's enterprises
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络在当今企业中的作用
- en: 'As developers, our main concern is how can we apply what we are learning to
    real world scenarios. More concretely, in an enterprise environment, what are
    the opportunities for using a neural network? Here are just a few ideas (out of
    many) for applications of a neural network:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，我们主要关心的是如何将我们正在学习的内容应用到现实世界的场景中。更具体地说，在企业环境中，使用神经网络的机遇有哪些？以下是一些（许多）神经网络应用想法：
- en: In a scenario where relationships between variables are not understood
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个变量之间的关系不为人知的场景中
- en: In a scenario where relationships are difficult to describe
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个关系难以描述的场景中
- en: In a scenario where the goal is to discover irregular patterns in data
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个目标是发现数据中不规则模式的场景中
- en: Classify data to recognize patterns such as animals, vehicles, and so on
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行分类以识别动物、车辆等模式
- en: Signal processing
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号处理
- en: Image recognition (emotion, sentiment, age, gender, and so on)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别（情感、观点、年龄、性别等）
- en: Text translation
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本翻译
- en: Handwriting recognition
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写识别
- en: Autonomous vehicles
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: And tons more!
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及更多！
- en: Types of learning
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习类型
- en: Since we talked about our neural network learning, let's briefly touch on the
    three different types of learning you should be aware of. They are **supervised**,
    **unsupervised**, and **reinforcement**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们谈到了我们的神经网络学习，让我们简要地谈谈你应该了解的三种不同类型的学习。它们是**监督**、**无监督**和**强化**。
- en: Supervised learning
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: If you have a large test dataset that matches up with known results, then supervised
    learning might be a good choice for you. The neural network will process a dataset;
    compare its output against the known result, adjust, and repeat. Pretty simple,
    huh?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个与已知结果匹配的大型测试数据集，那么监督学习可能是一个不错的选择。神经网络将处理数据集；将其输出与已知结果进行比较，调整，然后重复。很简单，对吧？
- en: Unsupervised learning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: If you don't have any test data, and it is possible to somehow derive a cost
    function from the behavior of the data, then unsupervised learning might be a
    good choice for you. The neural network will process a dataset, use the `cost`
    function to tell how much the error rate is, adjust the parameters, then repeat.
    All this while working in real time!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有测试数据，并且能够从数据的行为中推导出某种成本函数，那么无监督学习可能是一个不错的选择。神经网络将处理数据集，使用`成本`函数来告知错误率，调整参数，然后重复。所有这些都在实时进行！
- en: Reinforcement learning
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Our final type of learning is **reinforcement** learning, better known in some
    circles as **carrot-and-stick**. The neural network will process a dataset, learn
    from the data, and if our error rate decreases, we get the carrot. If the error
    rate increases, we get the stick. Enough said, right?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后一种学习类型是**强化**学习，在某些圈子中更广为人知的是**胡萝卜加大棒**。神经网络将处理数据集，从数据中学习，如果我们的错误率下降，我们就得到胡萝卜。如果错误率上升，我们就得到大棒。说得够多了，对吧？
- en: Understanding perceptrons
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解感知器
- en: 'The most basic element that we will deal with is called the neuron. If we were
    to take the most basic form of an activation function that a neuron would use,
    we would have a function that has only two possible results, 1 and 0\. Visually,
    such a function would be represented like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的最基本元素被称为神经元。如果我们考虑神经元将使用的最基本形式的激活函数，我们将得到一个只有两种可能结果的函数，1和0。从视觉上看，这样的函数将表示如下：
- en: '![](img/a35b5c41-1af3-4ea1-ade3-cb43d49df7a1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a35b5c41-1af3-4ea1-ade3-cb43d49df7a1.png)'
- en: 'This function returns 1 if the input is positive or 0, otherwise it returns
    0\. A neuron whose activation function is like this is called a **perceptron**.
    It is the simplest form of neural network we could develop. Visually, it looks
    like the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数如果输入是正的或0，则返回1，否则返回0。具有这种激活函数的神经元被称为**感知器**。这是我们能够开发的最简单的神经网络形式。从视觉上看，它看起来如下：
- en: '![](img/f3759083-7ca3-4d33-9172-6d07f3088c51.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f3759083-7ca3-4d33-9172-6d07f3088c51.jpg)'
- en: The perceptron follows the feed-forward model, meaning inputs are sent into
    the neuron, processed, and then produce output. Inputs come in, and output goes
    out. Let's use an example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器遵循前馈模型，这意味着输入被发送到神经元，处理，然后产生输出。输入进来，输出出去。让我们用一个例子来说明。
- en: 'Let''s suppose that we have a single perceptron with two inputs as shown previously.
    For the purposes of this example, input 0 will be x1 and input 1 will be x2\.
    If we assign those two variable values, they will look something like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个具有两个输入的单个感知器，如图所示。为了本例的目的，输入0将是x1，输入1将是x2。如果我们分配这两个变量值，它们将类似于以下内容：
- en: '*Input 0: x1 = 12*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*输入0: x1 = 12*'
- en: '*Input 1: x2 = 4*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*输入1: x2 = 4*'
- en: 'Each of those inputs must be **weighted**, that is, multiplied by some value,
    which is often a number between -1 and 1\. When we create our perceptron, we begin
    by assigning them random weights. As an example, Input 0 (**x1**) will have a
    weight we''ll label **w1**, and input 1, **x2 **will have a weight we''ll label
    **w2**. Given this, here''s how our weights look for this perceptron:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入都必须**加权**，也就是说，乘以某个值，这个值通常在-1和1之间。当我们创建我们的感知器时，我们首先给它们分配随机权重。例如，输入0（**x1**）将有一个我们将其标记为**w1**的权重，而输入1（**x2**）将有一个我们将其标记为**w2**的权重。据此，以下是这个感知器的权重：
- en: '*Weight 0: 0.5* *Weight 1: -1*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*权重0: 0.5* *权重1: -1*'
- en: '![](img/6833c8ca-216c-493b-8c1f-d9e5433f18cb.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6833c8ca-216c-493b-8c1f-d9e5433f18cb.png)'
- en: 'Once the inputs are *weighted*, they now need to be summed. Using the previous
    example, we would have this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入被*加权*，现在需要将它们相加。使用之前的例子，我们将有如下内容：
- en: '*6 + -4 = 2*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*6 + -4 = 2*'
- en: That sum would then be passed through an activation function, which we will
    cover in much more detail in a later chapter. This would generate the output of
    the perceptron. The activation function is what will ultimately tell the perceptron
    whether it is *OK to fire*, that is, to activate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个总和然后将通过一个激活函数，我们将在后面的章节中更详细地介绍。这将生成感知器的输出。激活函数将最终告诉感知器它是否可以*触发*，也就是说，激活。
- en: Now, for our activation function we will just use a very simple one. If the
    sum is positive, the output will be 1\. If the sum is negative, the output will
    be -1\. It can't get any simpler than that, right?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于我们的激活函数，我们将只使用一个非常简单的函数。如果总和是正的，输出将是1。如果总和是负的，输出将是-1。这不能再简单了，对吧？
- en: 'So, in pseudo code, our algorithm for our single perceptron looks like the
    following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在伪代码中，我们单个感知器的算法如下所示：
- en: For every input, multiply that input by its weight
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个输入，将输入乘以它的权重
- en: Sum all the weighted inputs
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 求所有加权输入的总和
- en: Compute the output of the perceptron based on that sum passed through an activation
    function (the sign of the sum)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据激活函数（即总和的符号）计算感知器基于该总和的输出
- en: Is this useful?
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这有用吗？
- en: 'Yes, in fact it is, and let''s show you how. Consider an input vector as the
    coordinates of a point. For a vector with *n* elements, the point would like it''s in
    a n-dimensional space. Take a sheet of paper, and on this paper, draw a set of
    points. Now separate those two points by a single straight line. Your piece of
    paper should now look something like the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，事实上是有用的，让我们来展示一下。考虑一个输入向量作为点的坐标。对于一个具有*n*个元素的向量，点将位于一个n维空间中。取一张纸，在这张纸上画一组点。现在用一条直线将这两个点分开。你的纸张现在应该看起来像以下这样：
- en: '![](img/8dc93930-0e18-421d-bf82-bcda82021568.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8dc93930-0e18-421d-bf82-bcda82021568.png)'
- en: As you can see, the points are now divided into two sets, one set on each side
    of the line. If we can take a single line and clearly separate all the points,
    then those two sets are what is known as linearly separable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，点现在被分为两组，一组在直线的每一侧。如果我们能用一条直线清楚地分离所有点，那么这两组就是所谓的线性可分。
- en: Our single perceptron, believe it or not, will be able to learn where this line
    is, and when your program is complete, the perceptron will also be able to tell
    whether a single point is above or below the line (or to the left or the right
    of it, depending upon how the line was drawn).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 相信或不相信，我们的单个感知器将能够学习这条线的位置，当你的程序完成时，感知器也将能够判断一个点是在线的上方还是下方（或者根据线的绘制方式，在左侧或右侧）。
- en: 'Let''s quickly code a `Perceptron` class, just so it becomes clearer for those
    of you who love to read code more than words (like me!). The goal will be to create
    a simple perceptron that can determine which side of the line a point should be
    on, just like the previous diagram:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速编写一个`Perceptron`类，以便让那些喜欢阅读代码而不是文字的你们（像我一样）更清晰地理解！目标是创建一个简单的感知器，可以确定一个点应该在直线的哪一侧，就像之前的图示一样：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The constructor could receive an argument indicating the number of inputs (in
    this case three: *x*, *y*, and a bias) and size the array accordingly:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数可以接收一个参数，表示输入的数量（在这种情况下是三个：*x*、*y*和偏置），并相应地调整数组的大小：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `weights` are picked randomly to start with:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，`weights`是随机选择的：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we''ll need a function for the perceptron to receive its information,
    which will be the same length as the array of weights, and then return the output
    value to us. We''ll call this `feedforward`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为感知器编写一个接收其信息的函数，其长度将与权重数组相同，然后返回输出值给我们。我们将称之为`feedforward`：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is the sign of the sum, which will be either -1 or +1\. In this
    case, the perceptron is attempting to guess which side of the line the output
    should be on:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是总和的符号，它将是-1或+1。在这种情况下，感知器正在尝试猜测输出应该在直线的哪一侧：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Thus far, we have a minimally functional perceptron that should be able to make
    an educated guess as to where our point will lie.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个功能最基本但应该能够做出有根据的猜测的感知器。
- en: 'Create the `Perceptron`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`Perceptron`：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The input is 3 values: *x*, *y,* and bias:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是3个值：*x*、*y*和偏置：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Obtain the answer:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 获得答案：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The only thing left that will make our perceptron more valuable is the ability
    to train it rather than have it make educated guesses. We do that by creating
    a `train` function such as this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们的感知器更有价值的唯一事情是能够训练它而不是让它做出有根据的猜测。我们通过创建一个`train`函数来实现这一点，如下所示：
- en: 'We will introduce a new variable to control the learning rate:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将引入一个新变量来控制学习率：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will also provide the inputs and the known answer:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将提供输入和已知答案：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And we will make an educated guess according to the inputs provided:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据提供的输入，我们将做出一个有根据的猜测：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will compute the `error`, which is the difference between the answer and
    our `guess`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将计算`error`，即答案和我们的`guess`之间的差异：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And, finally, we will adjust all the weights according to the error and learning
    constant:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将根据误差和学习常数调整所有权重：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, now that you know and see what a perceptron is, let's add **activation functions**
    into the mix and take it to the next level!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在你已经知道并看到了感知器是什么，让我们加入**激活函数**，并将其提升到下一个层次！
- en: Understanding activation functions
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解激活函数
- en: An activation function is added to the output end of a neural network to determine
    the output. It usually will map the resultant values somewhere in the range of
    -1 to 1, depending upon the function. It is ultimately used to determine whether
    a neuron will *fire* or *activate*, as in a light bulb going on or off.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数被添加到神经网络的输出端，以确定输出。它通常将结果值映射到-1到1的范围内的某个地方，具体取决于函数。它最终用于确定神经元是否会*触发*或*激活*，就像灯泡打开或关闭一样。
- en: 'The activation function is the last piece of the network before the output
    and could be considered the supplier of the output value. There are many kinds
    of activation function that can be used, and this diagram highlights just a very
    small subset of these:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是网络输出前的最后一部分，可以被认为是输出值的供应商。可以使用许多种激活函数，此图仅突出显示这些函数的一小部分：
- en: '![](img/85e2b8c7-018e-461c-86e0-c1af7841571a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/85e2b8c7-018e-461c-86e0-c1af7841571a.png)'
- en: 'There are two types of activation function—linear and non-linear:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有两种类型——线性和非线性：
- en: '**Linear**: A linear function is that which is on, or nearly on, a straight
    line, as depicted here:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：线性函数是位于或几乎位于直线上的函数，如图所示：'
- en: '![](img/60046c81-0304-41ac-bc10-5a676f67c74d.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/60046c81-0304-41ac-bc10-5a676f67c74d.png)'
- en: '**Non-linear**: A non-linear function is that which is not on a straight line,
    as depicted here:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性**：非线性函数是指那些不在直线上的函数，如图中所示：'
- en: '![](img/1062fee4-b892-410a-935b-703e01f01fab.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1062fee4-b892-410a-935b-703e01f01fab.png)'
- en: Visual activation function plotting
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化激活函数绘图
- en: 'When dealing with activation functions, it is important that you visually understand
    what an activation function looks like before you use it. We are going to plot,
    and then benchmark, several activation functions for you to see:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理激活函数时，重要的是在使用之前，您能够直观地理解激活函数的形状。我们将为您绘制并基准测试几个激活函数，以便您可以看到：
- en: '![](img/dc5b93de-a4c2-42b4-956a-dd8cf395b292.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc5b93de-a4c2-42b4-956a-dd8cf395b292.png)'
- en: 'This is what the logistic steep approximation and Swish activation function
    look like when they are plotted individually. As there are many types of activation
    function, the following shows what all our activation functions are going to look
    like when they are plotted together:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是逻辑阶跃近似和Swish激活函数单独绘制时的样子。由于存在许多类型的激活函数，以下展示了当它们一起绘制时，所有我们的激活函数将呈现的样子：
- en: '![](img/7fea069a-7185-4cfb-89f5-e4e4c66ff234.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fea069a-7185-4cfb-89f5-e4e4c66ff234.png)'
- en: 'Note: You can download the program that produces the previous output from the
    SharpNeat project on GitHub [https://github.com/colgreen/sharpneat](https://github.com/colgreen/sharpneat).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以从GitHub上的SharpNeat项目下载生成之前输出的程序[https://github.com/colgreen/sharpneat](https://github.com/colgreen/sharpneat)。
- en: At this point, you may be wondering why we even care what the plots look like—great
    point. We care because you are going to be using these quite a bit once you progress
    to hands-on experience, as you dive deeper into neural networks. It's very handy
    to be able to know whether your activation function will place the value of your
    neuron in the on or off state, and what range it will keep or need the values
    in. You will no doubt encounter and/or use activation functions in your career
    as a machine-learning developer, and knowing the difference between a Tanh and
    a LeakyRelu activation function is very important.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您可能想知道我们为什么关心这些图的样子——这是一个很好的问题。我们关心，因为一旦您进入实际操作经验，深入神经网络时，您将大量使用这些。知道您的激活函数将把神经元的值置于开启或关闭状态，以及它将保持或需要的值范围是非常有用的。毫无疑问，您作为机器学习开发者，在职业生涯中会遇到并/或使用激活函数，了解Tanh和LeakyRelu激活函数之间的区别非常重要。
- en: Function plotting
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数绘图
- en: 'For this example, we are going to use the open source package **SharpNeat**.
    It is one of the most powerful machine- learning platforms anywhere, and it has
    a special activation function plotter included with it. You can find the latest
    version of SharpNeat at [https://github.com/colgreen/sharpneat](https://github.com/colgreen/sharpneat).
    For this example, we will use the ***ActivationFunctionViewer*** project included
    as shown:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用开源包**SharpNeat**。它是最强大的机器学习平台之一，并且它包含一个特殊的激活函数绘图器。您可以在[https://github.com/colgreen/sharpneat](https://github.com/colgreen/sharpneat)找到SharpNeat的最新版本。在这个例子中，我们将使用如图所示包含的***ActivationFunctionViewer***项目：
- en: '![](img/085edbec-9d2a-4552-ace2-052114a39b80.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/085edbec-9d2a-4552-ace2-052114a39b80.png)'
- en: 'Once you have that project open, search for the `PlotAllFunctions` function.
    It is this function that handles the plotting of all the activation functions
    as previously shown. Let''s go over this function in detail:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦打开该项目，搜索`PlotAllFunctions`函数。正是这个函数处理了之前展示的所有激活函数的绘图。让我们详细了解一下这个函数：
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The main point of interest from the earlier code is highlighted in yellow. This
    is where the activation function that we passed in gets executed and its value
    used for the *y* axis plot value. The famous **ZedGraph** open source plotting
    package is used for all graph plotting. Once each function is executed, the respective
    plot will be made.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 早期代码中引起关注的主要点用黄色突出显示。这就是我们传递的激活函数被执行并用于*y*轴绘图值的地方。著名的开源绘图包**ZedGraph**用于所有图形绘图。一旦每个函数执行完毕，相应的绘图将被制作。
- en: Understanding back propagation
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: '**Back propagation**, which is short for **the backward propagation of errors**,
    is an algorithm for supervised learning of neural networks using gradient descent.
    This calculates what is known as **the gradient of the error** function, with
    respect to the network''s weights. It is a generalized form of the delta rule
    for perceptrons all the way to multi-layer feed-forward neural networks.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**，即**误差的反向传播**，是一种使用梯度下降进行神经网络监督学习的算法。它计算了所谓的**误差函数的梯度**，相对于网络的权重。它是感知器从单层到多层前馈神经网络的delta规则的一般形式。'
- en: Unlike forward propagation, back-prop calculates the gradients by moving backwards
    through the network. The gradient of the final layer of weights is calculated
    first, and the gradient of the first layer is hence calculated last. With the
    recent popularity in deep learning for image and speech recognition, back-prop
    has once again taken the spotlight. It is, for all intents and purposes, an efficient
    algorithm, and today's version utilizes GPUs to further improve performance.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与前向传播不同，反向传播通过在网络中向后移动来计算梯度。首先计算权重最后一层的梯度，然后最后计算第一层的梯度。随着深度学习在图像和语音识别中的最近流行，反向传播再次成为焦点。就其目的而言，它是一个高效的算法，今天的版本利用GPU来进一步提高性能。
- en: Lastly, because the computations for back-prop are dependent upon the activations
    and outputs from the forward phase (non-error term for all layers, including hidden),
    all of these values must be computed prior to the backwards phase beginning. It
    is therefore a requirement that the forward phase precede the backward phase for
    every iteration of gradient descent.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为反向传播的计算依赖于前向阶段（包括隐藏层在内的所有层的非误差项）的激活和输出，所以所有这些值必须在反向阶段开始之前被计算。因此，对于梯度下降的每一次迭代，前向阶段必须先于反向阶段进行。
- en: Forward and back propagation differences
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向和反向传播的区别
- en: Let's take a moment to clarify the difference between feed forward and back
    propagation. Once you understand this, you can visualize and understand much better
    how the entire neural network flows.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间来澄清前向传播和反向传播之间的区别。一旦你理解了这个，你就可以更好地可视化和理解整个神经网络的流程。
- en: In neural networks, you forward-propagate data to get the output and then compare
    it with the real intended value to get the error, which is the difference between
    what the data is suppose to be versus what your machine-learning algorithm actually
    thinks it is. To minimize that error, you now must *propagate* backward by finding
    the derivative of error, with respect to each weight, and then subtract this value
    from the weight itself.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，你通过前向传播数据以获取输出，然后将其与实际预期的值进行比较以获得误差，这个误差是数据应该是什么与你的机器学习算法实际认为它是什么之间的差异。为了最小化这个误差，你现在必须通过找到误差相对于每个权重的导数来*反向传播*，然后从这个权重本身减去这个值。
- en: The basic learning that is being done in a neural network is training neurons
    *when* to get activated, when to fire, and when to be *on* or *off*. Each neuron
    should activate only for certain types of inputs, not all of them. Therefore,
    by propagating forward, you see how well your neural network is behaving and find
    the error(s). After you find out what your network error rate is, you back-propagate
    and use a form of gradient descent to update new values of the weights. Once again,
    you will forward-propagate your data to see how well those weights are performing,
    and then backward-propagate the data to update the weights. This will go on until
    you reach some minima for error value (hopefully the global minimum and not the
    local). Again, lather, rinse, repeat!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中进行的基学习是训练神经元何时被激活、何时放电以及何时处于*开启*或*关闭*状态。每个神经元应该只为某些类型的输入激活，而不是所有输入。因此，通过前向传播，你可以看到你的神经网络表现如何以及找到误差。在你了解到你的网络误差率后，你进行反向传播并使用梯度下降的形式来更新权重的新的值。再次，你将数据前向传播以查看这些权重表现如何，然后反向传播数据以更新权重。这将一直持续到达到误差值的某个最小值（希望是全球最小值而不是局部最小值）。再次，重复这个过程！
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a brief overview of various neural network terminologies.
    We reviewed perceptrons, neurons, and back propagation, among other things. In
    our next chapter, we are going to dive right into coding a complete neural network!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要概述了各种神经网络术语。我们回顾了感知器、神经元和反向传播等内容。在我们下一章中，我们将直接进入编写一个完整的神经网络！
- en: We will cover such topics as neural network training, terminology, synapses,
    neurons, forward propagation, back propagation, sigmoid function, back propagation,
    and error calculations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖诸如神经网络训练、术语、突触、神经元、前向传播、反向传播、Sigmoid函数、反向传播以及错误计算等主题。
- en: So, hold onto your hats; the code is coming!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，请系好你的帽子；代码即将到来！
- en: References
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '@EVOLVE deep-learning shared information neural network framework, copyright
    2016 Matt R Cole, [www.evolvedaisolutions.com](http://www.evolvedaisolutions.com).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@EVOLVE 深度学习共享信息神经网络框架，版权所有 2016 Matt R Cole，[www.evolvedaisolutions.com](http://www.evolvedaisolutions.com)。'
- en: 'SharpNeat Activation Functions/Viewer: SharpNeat ([https://github.com/colgreen/sharpneat](https://github.com/colgreen/sharpneat)).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SharpNeat 激活函数/查看器：SharpNeat ([https://github.com/colgreen/sharpneat](https://github.com/colgreen/sharpneat))。
