- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Machine Learning Part 1 – Statistical Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习第一部分 – 统计机器学习
- en: In this chapter, we will discuss how to apply classical statistical machine
    learning techniques such as **Naïve Bayes**, **term frequency-inverse document
    frequency** (**TF-IDF**), **support vector machines** (**SVMs**), and **conditional
    random fields** (**CRFs**) to common **natural language processing** (**NLP**)
    tasks such as classification (or intent recognition) and slot filling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何将经典的统计机器学习技术应用于常见的**自然语言处理**（**NLP**）任务，如分类（或意图识别）和槽位填充。这些技术包括**朴素贝叶斯**、**词频-逆文档频率**（**TF-IDF**）、**支持向量机**（**SVMs**）和**条件随机场**（**CRFs**）。
- en: 'There are two aspects of these classical techniques that we need to consider:
    representations and models. **Representation** refers to the format of the data
    that we are going to analyze. You will recall from [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144),
    that it is standard to represent NLP data in formats other than lists of words.
    Numeric data representation formats such as vectors make it possible to use widely
    available numeric processing techniques, and consequently open up many possibilities
    for processing. In [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144), we also explored
    data representations such as the **count bag of words**(**BoW**), TF-IDF, and
    Word2Vec. We will primarily be using TF-IDF in this chapter.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些经典技巧有两个方面需要我们考虑：表示和模型。**表示**指的是我们要分析的数据格式。你会记得在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中，标准的自然语言处理数据格式并不仅仅是单词列表。像向量这样的数值数据表示格式使得使用广泛可用的数值处理技术成为可能，从而开辟了许多处理的可能性。在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中，我们还探索了数据表示方法，如**词袋模型**（**BoW**）、TF-IDF和Word2Vec。本章中我们将主要使用TF-IDF。
- en: Once the data is in a format that is ready for further processing, that is,
    once it has been *vectorized*, we can use it to train, or build, a model that
    can be used to analyze similar data that the system may encounter in the future.
    This is the **training** phase. The future data can be test data; that is, previously
    unseen data similar to the training data that is used to evaluate the model. In
    addition, if the model is being used in a practical application, the future data
    could be new examples of queries from users or customers addressed to the runtime
    system. When the system is used to process data after the training phase, this
    is called **inference**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据格式化为可供进一步处理的形式，也就是说，一旦数据被*向量化*，我们就可以用它来训练或构建一个模型，进而分析系统未来可能遇到的相似数据。这就是**训练**阶段。未来数据可以是测试数据；也就是说，之前未见过的数据，类似于训练数据，用于评估模型。此外，如果模型在实际应用中使用，未来数据可能是用户或客户对运行时系统提出的新查询示例。当系统在训练阶段之后用于处理数据时，这被称为**推断**。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: A quick overview of evaluation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估的简要概述
- en: Representing documents with TF-IDF and classifying with naïve Bayes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TF-IDF表示文档，并通过朴素贝叶斯进行分类
- en: Classifying documents with SVMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVM进行文档分类
- en: Slot filling with conditional random fields
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用条件随机场进行槽位填充
- en: We will start this chapter with a very practical and basic set of techniques
    that should be in everyone’s toolbox, and that frequently end up being practical
    solutions to classification problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一套非常实用且基础的技巧开始本章内容，这些技巧应该是每个人工具箱中的一部分，并且经常成为分类问题的实际解决方案。
- en: A quick overview of evaluation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估的简要概述
- en: Before we look at how different statistical techniques work, we have to have
    a way to measure their performance, and there are a couple of important considerations
    that we should review first. The first consideration is the *metric* or score
    that we assign to the system’s processing. The most common and simple metric is
    **accuracy**, which is the number of correct responses divided by the overall
    number of attempts. For example, if we’re attempting to measure the performance
    of a movie review classifier, and we attempt to classify 100 reviews as positive
    or negative, if the system classifies 75 reviews correctly, the accuracy is 75%.
    A closely related metric is **error rate**, which is, in a sense, the opposite
    of accuracy because it measures how often the system made a mistake. In this example,
    the error rate would be 25%.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解不同统计技术如何工作之前，我们需要有一种方法来衡量它们的性能，并且有几个重要的考虑事项我们应该先回顾一下。第一个考虑因素是我们为系统的处理分配的*指标*或得分。最常见且简单的指标是**准确率**，它是正确响应的数量除以总尝试次数。例如，如果我们试图衡量一个电影评论分类器的性能，并且我们尝试将100条评论分类为正面或负面，如果系统正确分类了75条评论，那么准确率就是75%。一个紧密相关的指标是**错误率**，从某种意义上说，它是准确率的对立面，因为它衡量的是系统犯错的频率。在这个例子中，错误率是25%。
- en: We will only make use of accuracy in this chapter, although there are more precise
    and informative metrics that are actually more commonly used, for example, **precision**,
    **recall**, **F1**, and **area under the curve (AUC)**. We will discuss these
    in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226). For the purposes of this
    chapter, we just need a basic metric so that we can compare results, and accuracy
    will be adequate for that.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们仅使用准确率，尽管实际上有更精确和信息量更大的指标被更常用，例如**精确度**、**召回率**、**F1**和**曲线下面积（AUC）**。我们将在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中讨论这些指标。就本章而言，我们只需要一个基本的指标来进行结果对比，准确率已经足够。
- en: The second important consideration that we need to keep in mind in evaluation
    is how to treat the data that we will be using for evaluation. Machine learning
    approaches are trained and evaluated with a standard approach that involves splitting
    the dataset into *training*, *development*, also often called *validation* data,
    and *testing* subsets. The training set is the data that is used to build the
    model, and is typically about 60-80 percent of the available data, although the
    exact percentage is not critical. Typically, you would want to use as much data
    as possible for training, while reserving a reasonable amount of data for evaluation
    purposes. Once the model is built, it can be tested with development data, normally
    about 10-20 percent of the overall dataset. Problems with the training algorithm
    are generally uncovered by trying to use the model on the development set. A final
    evaluation is done once, on the remaining data – the test data, which is usually
    about 10 percent of the total data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估时需要牢记的第二个重要考虑因素是如何处理我们用于评估的数据。机器学习方法采用标准方法进行训练和评估，这涉及将数据集划分为*训练*、*开发*（也常称为*验证*）数据和*测试*子集。训练集是用于构建模型的数据，通常占可用数据的60-80%，尽管具体百分比并不是决定性因素。通常，你会希望尽可能多地使用数据来进行训练，同时保留合理数量的数据用于评估目的。一旦模型构建完成，它可以用开发数据进行测试，通常约占总数据集的10-20%。通过在开发集上使用模型，通常能发现训练算法的问题。最终的评估是在剩余的数据——测试数据上进行的，测试数据通常占总数据的10%。
- en: Again, the exact breakdown of training, development, and test data is not critical.
    Your goal is to use the training data to build a good model that will enable your
    system to accurately predict the interpretation of new, previously unseen data.
    To meet that goal, you need as much training data as possible. The goal of the
    test data is to get an accurate measure of how your model performs on new data.
    To meet that goal, you need as much test data as possible. So, the data split
    will always involve a trade-off between these goals.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，训练、开发和测试数据的具体划分并不是关键。你的目标是使用训练数据构建一个好的模型，使得你的系统能够准确地预测新的、以前未见过的数据的解释。为了实现这一目标，你需要尽可能多的训练数据。测试数据的目标是准确衡量模型在新数据上的表现。为了实现这一目标，你需要尽可能多的测试数据。因此，数据划分总是涉及这些目标之间的权衡。
- en: It is very important to keep the training data separate from the development
    data, and especially from the test data. Performance on the training data is not
    a good indicator of how the system will really perform on new, unseen data, so
    performance on the training data is not used for evaluation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 保持训练数据与开发数据，特别是测试数据的分离是非常重要的。训练数据上的表现并不能很好地反映系统在新数据上的实际表现，因此不应将训练数据的表现用于评估。
- en: Following this brief introduction to evaluation, We will now move on to the
    main topics of this chapter. We will cover some of the most well-established machine
    learning approaches for important NLP applications such as classification and
    slot filling.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要介绍完评估后，我们现在将进入本章的主要内容。我们将讨论一些最为成熟的机器学习方法，这些方法广泛应用于重要的自然语言处理（NLP）任务，如分类和槽位填充。
- en: Representing documents with TF-IDF and classifying with Naïve Bayes
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF-IDF表示文档，并通过朴素贝叶斯进行分类
- en: In addition to evaluation, two important topics in the general paradigm of machine
    learning are representation and processing algorithms. Representation involves
    converting a text, such as a document, into a numerical format that preserves
    relevant information about the text. This information is then analyzed by the
    processing algorithm to perform the NLP application. You’ve already seen a common
    approach to representation, TF-IDF, in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
    In this section, we will cover using TF-IDF with a common classification approach,
    Naïve Bayes. We will explain both techniques and show an example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评估之外，机器学习一般范式中还有两个重要话题：表示和处理算法。表示涉及将文本（如文档）转换为一种数值格式，该格式保留了关于文本的相关信息。然后，处理算法会分析这些信息，以执行自然语言处理任务。你已经在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中见过常见的表示方法——TF-IDF。在本节中，我们将讨论如何将TF-IDF与一种常见的分类方法——朴素贝叶斯结合使用。我们将解释这两种技术，并展示一个示例。
- en: Summary of TF-IDF
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF总结
- en: You will recall the discussion of TF-IDF from [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
    TF-IDF is based on the intuitive goal of trying to find words in documents that
    are particularly diagnostic of their classification topic. Words that are relatively
    infrequent in the whole corpus, but which are relatively common in a specific
    document, seem to be helpful in finding the document’s class. TF-IDF was defined
    in the equations presented in the *term frequency-inverse document frequency (TF-IDF)*
    section in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144). In addition, we saw
    partial TF-IDF vectors for some of the documents in the movie review corpus in
    *Figure 7**.4*. Here, we will take the TF-IDF vectors and use them to classify
    documents using the Naïve Bayes classification approach.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中关于TF-IDF的讨论。TF-IDF基于一个直观的目标，即试图在文档中找到特别能代表其分类主题的词汇。在整个语料库中相对较少出现，但在特定文档中相对较常见的词汇，似乎在确定文档类别时非常有帮助。TF-IDF在[*第7章*](B19005_07.xhtml#_idTextAnchor144)的*词频-逆文档频率（TF-IDF）*部分中给出了定义。此外，我们还在*图7.4*中看到了电影评论语料库中一些文档的部分TF-IDF向量。在这里，我们将使用这些TF-IDF向量，并利用朴素贝叶斯分类方法对文档进行分类。
- en: Classifying texts with Naïve Bayes
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行文本分类
- en: Bayesian classification techniques have been used for many years, and, despite
    their long history, are still very common and widely used. Bayesian classification
    is simple and fast, and can lead to acceptable results in many applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类技术已经使用多年，尽管其历史悠久，但至今仍然非常常见并被广泛使用。贝叶斯分类简单且快速，并且在许多应用中能产生令人满意的结果。
- en: The formula for Bayesian classification is shown in the following equation.
    For each category in the set of possible categories, and for each document, we
    want to compute the probability that that is the correct category for that document.
    This computation is based on some representation of the document; in our case,
    the representation will be a vector like one of the ones we’ve previously discussed
    – BoW, TF-IDF, or Word2Vec.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类的公式如以下方程所示。对于每一个可能的类别，对于每一个文档，我们想要计算该文档属于该类别的概率。这一计算基于文档的某种表示；在我们的案例中，表示将是我们之前讨论过的向量之一——如BoW、TF-IDF或Word2Vec。
- en: 'To compute this probability, we take into account the probability of the vector,
    given a category, multiplied by the probability of the category, and divided by
    the probability of the document vector, as shown in the following equation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个概率，我们考虑给定类别下向量的概率，乘以类别的概率，再除以文档向量的概率，如下所示的公式：
- en: P(category ∣ documentVector) =  P(documentVector ∣ category)P(category)   _____________________________  P(documentVector)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: P(category ∣ documentVector) =  P(documentVector ∣ category)P(category)    _____________________________
     P(documentVector)
- en: The training process will determine the probabilities of the document vectors
    in each category and the overall probabilities of the categories.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程将决定每个类别中文档向量的概率以及各类别的总体概率。
- en: The formula is called *naïve* because it makes the assumption that the features
    in the vectors are independent. This is clearly not correct for text because the
    words in sentences are not at all independent. However, this assumption makes
    the processing much simpler, and in practice does not usually make a significant
    difference in the results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式被称为*朴素*，因为它假设向量中的特征是独立的。显然，对于文本来说这是不正确的，因为句子中的单词并不是完全独立的。然而，这个假设使得处理过程大大简化，实际应用中通常不会对结果产生重大影响。
- en: There are both binary and multi-class versions of Bayesian classification. We
    will work with binary classification with the movie review corpus since we have
    only two categories of reviews.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类有二元和多类版本。由于我们只有两类评论，因此我们将使用电影评论语料库进行二元分类。
- en: TF-IDF/Bayes classification example
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF/贝叶斯分类示例
- en: 'Using TF-IDF and Naïve Bayes to classify the movie reviews, we can start by
    reading the reviews and splitting the data into training and test sets, as shown
    in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TF-IDF和朴素贝叶斯进行电影评论分类时，我们可以从读取评论并将数据拆分为训练集和测试集开始，如下代码所示：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once we have our training and test data, the next step is creating TF-IDF vectors
    from the reviews, as shown in the following code snippet. We will primarily be
    using the scikit-learn library, although we’ll use NLTK for tokenization:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练和测试数据，下一步是从评论中创建TF-IDF向量，如下代码片段所示。我们将主要使用scikit-learn库，虽然在分词时我们会使用NLTK：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The main steps in the preceding code are creating the vectorizer and then using
    the vectorizer to convert the movie reviews to TF-IDF format. This is the same
    process that we followed in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144). The
    resulting TF-IDF vectors were shown in *Figure 7**.4*, so we won’t repeat that
    here.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中的主要步骤是创建向量化器，然后使用向量化器将电影评论转换为TF-IDF格式。这与我们在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中遵循的过程相同。生成的TF-IDF向量已经在*图7.4*中展示，因此我们在此不再重复。
- en: Classifying the documents into positive and negative reviews is then done with
    the multinomial Naïve Bayes function from scikit-learn, which is one of scikit-learn’s
    Naïve Bayes packages, suitable for working with TF-IDF vector data. You can take
    a look at scikit-learn’s other Naïve Bayes packages at [https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)
    for more information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用scikit-learn中的多项式朴素贝叶斯函数将文档分类为正面和负面评论，这是scikit-learn的朴素贝叶斯包之一，适用于处理TF-IDF向量数据。你可以访问scikit-learn的其他朴素贝叶斯包以获取更多信息，网址为[https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)。
- en: 'Now that we have the TF-IDF vectors, we can initialize the naïve Bayes classifier
    and train it on the training data, as shown here:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了TF-IDF向量，可以初始化朴素贝叶斯分类器并在训练数据上进行训练，如下所示：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we can compute the accuracy of the classifier by vectorizing the test
    set (`movies_test_tfidf`), and using the classifier that was created from the
    training data to predict the classes of the test data, as shown in the following
    code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过将测试集（`movies_test_tfidf`）向量化，并使用从训练数据中创建的分类器预测测试数据的类别，从而计算分类器的准确性，如下代码所示：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see from the preceding code that the accuracy of the classifier is `0.64`.
    That is, 64% of the test data reviews were assigned the correct category (positive
    or negative). We can also get some more information about how well the classification
    worked by looking at the *confusion matrix*, which is shown in the last two lines
    of the code.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中我们可以看到分类器的准确率为`0.64`。也就是说，64%的测试数据评论被分配到了正确的类别（正面或负面）。我们还可以通过查看*混淆矩阵*来获取更多关于分类效果的信息，混淆矩阵显示在代码的最后两行。
- en: In general, a confusion matrix shows which categories were confused with which
    other categories. We have a total of `400` test items (20% of the 2,000 reviews
    that were reserved as test examples). `132` of the `190` negative reviews were
    correctly classified as negative, and `58` were incorrectly classified as positive.
    Similarly, `124` of the `210` positive reviews were correctly classified as positive,
    but `86` were misclassified as negative. That means 69% of the negative reviews
    were correctly classified, and 59% of the positive reviews were correctly classified.
    From this, we can see that our model does slightly better in classifying negative
    reviews as negative. The reasons for this difference are not clear. To understand
    this result better, we can look more carefully at the reviews that were misclassified.
    We won’t do this right now, but we will look at analyzing results more carefully
    in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，混淆矩阵显示的是哪些类别被误分类为哪些其他类别。我们总共有`400`个测试项（这占了预留为测试示例的2,000条评论的20%）。在`190`条负面评论中，`132`条被正确分类为负面，`58`条被错误分类为正面。类似地，在`210`条正面评论中，`124`条被正确分类为正面，但`86`条被误分类为负面。这意味着69%的负面评论被正确分类，59%的正面评论被正确分类。从中我们可以看到，我们的模型在将负面评论正确分类为负面方面表现稍好。造成这种差异的原因尚不清楚。为了更好地理解这个结果，我们可以更仔细地分析被误分类的评论。我们现在不做这件事，但我们将在[*第14章*](B19005_14.xhtml#_idTextAnchor248)中讨论如何更仔细地分析结果。
- en: We will now consider a more modern and generally more accurate approach to classification.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将考虑一种更现代且通常更准确的分类方法。
- en: Classifying documents with Support Vector Machines (SVMs)
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机（SVM）进行文档分类
- en: SVMs are a popular and robust tool for text classification in applications such
    as intent recognition and chatbots. Unlike neural networks, which we will discuss
    in the next chapter, the training process is usually relatively quick and normally
    doesn’t require enormous amounts of data. That means that SVMs are good for applications
    that have to be quickly deployed, perhaps as a preliminary step in the development
    of a larger-scale application.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 是一种流行且强大的文本分类工具，广泛应用于意图识别和聊天机器人等领域。与神经网络不同，我们将在下一章讨论神经网络，SVM 的训练过程通常相对较快，并且通常不需要大量数据。这意味着
    SVM 适合需要快速部署的应用程序，可能作为开发更大规模应用程序的初步步骤。
- en: The basic idea behind SVMs is that if we represent documents as **n**-dimensional
    vectors (for example, the TF-IDF vectors that we discussed in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144),
    we want to be able to identify a hyperplane that provides a boundary that separates
    the documents into two categories with as large a boundary (or *margin*) as possible.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 的基本思想是，如果我们将文档表示为**n**维向量（例如我们在[*第7章*](B19005_07.xhtml#_idTextAnchor144)中讨论的
    TF-IDF 向量），我们希望能够识别一个超平面，该超平面提供一个边界，将文档分为两个类别，并且边界（或*间隔*）尽可能大。
- en: 'An illustration of using SVMs on the movie review data is shown here. We start,
    as usual, by importing the data and creating a train/test split:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了使用支持向量机（SVM）对电影评论数据进行分类的示例。我们像往常一样，首先导入数据并进行训练/测试集划分：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have the data set up and we have generated the train/test split,
    we will create the TF-IDF vectors and perform the SVC classification in the following
    code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了数据集，并生成了训练/测试集划分，接下来我们将在以下代码中创建 TF-IDF 向量并执行 SVC 分类：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The process shown here is very similar to the Naïve Bayes classification example
    shown in the code snippets in the previous section. However, in this case, we
    use an SVM instead of Naïve Bayes for classification, although we still use TF-IDF
    to vectorize the data. In the preceding code, we can see that the accuracy result
    for the classification is `0.82`, which is considerably better than the Bayes
    accuracy shown in the previous section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的过程与前一节代码片段中的朴素贝叶斯分类示例非常相似。然而，在这种情况下，我们使用的是支持向量机（SVM）而不是朴素贝叶斯进行分类，尽管我们仍然使用TF-IDF对数据进行向量化。在前面的代码中，我们可以看到分类的准确率结果是`0.82`，明显优于前一节中展示的贝叶斯准确率。
- en: The resulting confusion matrix is also better, in that `153` of the `190` negative
    reviews were correctly classified as negative, and `37` were incorrectly classified
    as positive. Similarly, `172` of the `210` positive reviews were correctly classified
    as positive, but `38` were misclassified as negative. That means 80% of the negative
    reviews were correctly classified, and 81% of the positive reviews were correctly
    classified.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的混淆矩阵也更好，因为`190`个负面评论中有`153`个被正确分类为负面，`37`个被错误分类为正面。类似地，`210`个正面评论中有`172`个被正确分类为正面，而`38`个被误分类为负面。这意味着80%的负面评论被正确分类，81%的正面评论被正确分类。
- en: SVMs were originally designed for binary classification, as we have just seen
    with the movie review data, where we only have two categories (positive and negative,
    in this case). However, they can be extended to multi-class problems (which includes,
    most cases of intent recognition) by splitting the problem into multiple binary
    problems.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）最初是为二分类设计的，就像我们刚才看到的电影评论数据一样，其中只有两个类别（在这种情况下是正面和负面）。然而，它们可以通过将问题拆分为多个二分类问题来扩展到多类问题（包括大多数意图识别的情况）。
- en: 'Consider a multi-class problem that might occur with a generic personal assistant
    application. Suppose the application includes several intents, for example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个可能出现在通用个人助手应用中的多类问题。假设该应用包含多个意图，例如：
- en: Find out the weather
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询天气
- en: Play music
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 播放音乐
- en: Read the latest headlines
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读最新头条新闻
- en: Tell me the latest sports scores for my favorite teams
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告诉我我喜欢的球队的最新体育比分
- en: Find a nearby restaurant offering a particular cuisine
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找附近提供特定菜系的餐厅
- en: The application needs to classify the user’s query into one of these intents
    in order to process it and answer the user’s question. To use SVMs for this, it
    is necessary to recast the problem as a set of binary problems. There are two
    ways to do this.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用需要将用户的查询分类到这些意图中的一个，以便处理并回答用户的问题。为了使用SVM进行分类，有必要将问题重新表述为一组二分类问题。有两种方法可以做到这一点。
- en: One is to create multiple models, one for each pair of classes, and split the
    data so that each class is compared to every other class. With the personal assistant
    example, the classification would need to decide about questions such as “*Is
    the category weather or sports?*” and *Is the category weather or news?*. This
    is the *one versus one* approach, and you can see that this could result in a
    very large number of classifications if the number of intents is large.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是为每对类别创建多个模型，并拆分数据，以便每个类别与其他所有类别进行比较。在个人助手的例子中，分类需要决定诸如“*这个类别是天气还是体育？*”和*这个类别是天气还是新闻？*这样的问法。这就是*一对一*的方法，你可以看到，如果意图数量较多，这可能会导致非常多的分类。
- en: The other approach is called *one versus rest* or *one versus all*. Here, the
    idea is to ask questions such as *Is the category* “*weather*” *or is it something
    else?* This is the more popular approach, which we will show here.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法叫做*一对多*或*一对所有*。在这里，问题是询问诸如*这个类别是*“*天气*”*还是其他类别？* 这是更常见的方法，我们将在这里展示。
- en: 'The way to use the multiclass SVM from scikit-learn is very similar to what
    was shown previously. The difference is that we’re importing the `OneVsRestClassifier`
    and using it to create the classification model, as shown in the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的多类SVM的方法与之前展示的非常相似。不同之处在于，我们导入了`OneVsRestClassifier`并使用它来创建分类模型，代码如下所示：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Classification** is widely used in many natural language applications, both
    for categorizing documents such as movie reviews and for categorizing what a user’s
    overall goal (or *intent*) in asking their question is in applications such as
    chatbots. However, often the application will require more fine-grained information
    from the utterance or document in addition to its overall classification. This
    process is often called **slot filling**. We discussed slot-filling in [*Chapter
    8*](B19005_08.xhtml#_idTextAnchor159) and showed how to write slot-filling rules.
    In the following section, we will show another approach to slot-filling, based
    on statistical techniques, specifically **conditional random** **fields** (**CRFs**).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**在许多自然语言应用中得到了广泛应用，包括分类文档（如电影评论）和在聊天机器人等应用中分类用户提问的整体目标（或*意图*）。然而，应用程序通常除了整体分类之外，还需要从话语或文档中获取更精细的信息。这个过程通常被称为**插槽填充**。我们在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中讨论了插槽填充，并展示了如何编写插槽填充规则。在接下来的部分中，我们将展示另一种基于统计技术的插槽填充方法，特别是**条件随机场**（**CRF**）。'
- en: Slot-filling with CRFs
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CRF进行插槽填充
- en: In [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159), we discussed the popular
    application of slot-filling, and we used the spaCy rule engine to find slots for
    the restaurant search application shown in *Figure 8**.9*. This required writing
    rules for finding the fillers of each slot in the application. This approach can
    work fairly well if the potential slot fillers are known in advance, but if they
    aren’t known in advance, it won’t be possible to write rules. For example, with
    the rules in the code following *Figure 8**.9*, if a user asked for a new cuisine,
    say, *Thai*, the rules wouldn’t be able to recognize *Thai* as a new filler for
    the `CUISINE` slot, and wouldn’t be able to recognize *not too far away* as a
    filler for the `LOCATION` slot. Statistical methods, which we will discuss in
    this section, can help with this problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中，我们讨论了插槽填充的常见应用，并使用spaCy规则引擎为餐厅搜索应用程序找到插槽，如*图8.9*所示。这要求编写规则来查找应用程序中每个插槽的填充项。如果潜在的插槽填充项事先已知，这种方法可以很好地工作，但如果事先不知道，它就无法编写规则。例如，使用*图8.9*后面的代码中的规则，如果用户请求一种新的菜系，比如*泰国菜*，这些规则将无法将*泰国菜*识别为`CUISINE`插槽的填充项，也无法将*不远的地方*识别为`LOCATION`插槽的填充项。我们将在本节中讨论的统计方法可以帮助解决这个问题。
- en: With **statistical methods**, the system does not use rules but looks for patterns
    in its training data that can be applied to new examples. Statistical methods
    depend on having enough training data for the system to be able to learn accurate
    patterns, but if there is enough training data, statistical methods will generally
    provide a more robust solution to an NLP problem than rule-based methods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**统计方法**时，系统不使用规则，而是通过在训练数据中寻找可以应用于新示例的模式。统计方法依赖于足够的训练数据，以便系统能够学习到准确的模式，但如果有足够的训练数据，统计方法通常会提供比基于规则的方法更强大的NLP问题解决方案。
- en: In this section, we will look at a popular approach that can be applied to statistical
    slot filling – CRF. CRFs are a way of taking the context of textual items into
    account when trying to find the label for a span of text. Recall that the rules
    we discussed in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159) did not look at
    any nearby words or other context – they only looked at the item itself. In contrast,
    CRFs attempt to model the probability of a label for a particular section of text,
    that is, given an input *x*, they model the probability of that input being an
    example category *y (P(y|x))*. CRFs use the word (or token) sequence to estimate
    the conditional probabilities of slot labels in that context. We will not review
    the mathematics of CRF here, but you can find many detailed descriptions of the
    underlying mathematics on the web, for example, [https://arxiv.org/abs/1011.4088](https://arxiv.org/abs/1011.4088).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一种可以应用于统计插槽填充的常见方法——条件随机场（CRF）。CRF是一种在寻找文本跨度标签时，考虑文本项上下文的方法。回想一下我们在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中讨论的规则，这些规则并没有考虑任何邻近词汇或其他上下文——它们只关注项本身。相比之下，CRF试图对特定文本段落的标签概率建模，也就是说，给定一个输入
    *x*，它们建模该输入作为示例类别 *y (P(y|x)) *的概率。CRF利用单词（或标记）序列来估计在该上下文中插槽标签的条件概率。我们在这里不会回顾CRF的数学原理，但你可以在网上找到许多详细的数学描述，例如，[https://arxiv.org/abs/1011.4088](https://arxiv.org/abs/1011.4088)。
- en: To train a system for slot-tagging, the data has to be annotated so that the
    system can tell what slots it’s looking for. There are at least four different
    formats used in the NLP technical literature for representing the annotations
    for slot-tagged data, and we’ll briefly illustrate these. These formats can be
    used both for training data and for representing processed NLP results, which
    can, in turn, be used for further processing stages such as retrieving information
    from a database.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个槽位标记系统，数据必须进行注释，以便系统能够知道它要寻找哪些槽位。NLP技术文献中至少有四种不同的格式用于表示槽位标记数据的注释，我们将简要介绍这些格式。这些格式既可以用于训练数据，也可以用于表示处理后的NLP结果，后者可以进一步用于数据库检索等处理阶段。
- en: Representing slot-tagged data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示带有槽位标记的数据
- en: Training data for slot-filling applications can be found in several formats.
    Let’s look at how the sentence `show me science fiction films directed by steven
    spielberg`, a query from the MIT movie query corpus ([https://groups.csail.mit.edu/sls/downloads/](https://groups.csail.mit.edu/sls/downloads/)),
    can be represented in four different formats.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用于槽位填充应用的训练数据可以有多种格式。让我们看看如何用四种不同的格式表示句子`show me science fiction films directed
    by steven spielberg`，这是MIT电影查询语料库中的一个查询([https://groups.csail.mit.edu/sls/downloads/](https://groups.csail.mit.edu/sls/downloads/))。
- en: 'One commonly used notation is **JavaScript Object Notation** (**JSON**) format,
    as shown here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的表示法是**JavaScript对象表示法**（**JSON**）格式，如下所示：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, we see that the input sentence is shown as `tokens`, which is followed
    by a list of slots (called `entities` here). Each entity is associated with a
    name, such as `GENRE` or `DIRECTOR` and the tokens that it applies to. The example
    shows two slots, `GENRE` and `DIRECTOR`, which are filled by `science fiction
    films` and `steven` `spielberg`, respectively:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到输入句子以`tokens`形式呈现，后面跟着一列槽位（这里称为`entities`）。每个实体与一个名称关联，如`GENRE`或`DIRECTOR`，以及它适用的tokens。示例显示了两个槽位，`GENRE`和`DIRECTOR`，它们分别由`science
    fiction films`和`steven` `spielberg`填充：
- en: The second format uses `show me <GENRE>science fiction films</GENRE> directed
    by <``DIRECTOR>steven Spielberg</DIRECTOR>`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种格式使用`show me <GENRE>science fiction films</GENRE> directed by <``DIRECTOR>steven
    Spielberg</DIRECTOR>`。
- en: 'The third format is called **Beginning Inside Outside** (**BIO**), which is
    a textual format that labels the beginning, middle, and end of each slot filler
    in a sentence, as in *Figure 9**.1*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种格式称为**Beginning Inside Outside**（**BIO**），这是一种文本格式，用于标记每个槽位填充项在句子中的开始、内部和结束部分，如*图
    9.1*所示：
- en: '![Figure 9.1 – BIO tagging for “show me science fiction films directed by steven
    spielberg”](img/B19005_09_01.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – “show me science fiction films directed by steven spielberg” 的BIO标记](img/B19005_09_01.jpg)'
- en: Figure 9.1 – BIO tagging for “show me science fiction films directed by steven
    spielberg”
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – “show me science fiction films directed by steven spielberg”的BIO标记
- en: In the BIO format, words outside of any slot are labeled `O` (that is `Outside`),
    the beginning of a slot (`science` and `steven` are labeled `B`, and the inside
    of a slot is labeled `I`).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在BIO格式中，任何不属于槽位的单词标记为`O`（即`Outside`），槽位的开始部分（`science`和`steven`标记为`B`），而槽位的内部部分标记为`I`。
- en: 'Finally, another very simple format for representing tagged slots is **Markdown**,
    a simplified way of marking up text for rendering. We’ve seen Markdown before,
    in Jupyter notebooks, as a way to display comment blocks. In *Figure 9**.2*, we
    can see an example of some training data for a restaurant search application similar
    to the one we looked at in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159) (which
    was shown in *Figure 8**.9*). Slot values are shown in square brackets, and the
    slot names are shown in parentheses:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一种非常简单的表示标记槽位的格式是**Markdown**，它是一种简化的文本标记方式，便于渲染。我们之前在Jupyter笔记本中见过Markdown，它用来显示注释块。在*图
    9.2*中，我们可以看到餐馆搜索应用的一些训练数据示例，这与我们在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中看到的类似（该内容展示在*图
    8.9*中）。槽位值显示在方括号中，槽位名称显示在括号中：
- en: '![Figure 9.2 – Markdown tagging for a restaurant search application](img/B19005_09_02.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 用于餐馆搜索应用的Markdown标记](img/B19005_09_02.jpg)'
- en: Figure 9.2 – Markdown tagging for a restaurant search application
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 用于餐馆搜索应用的Markdown标记
- en: All four formats show basically the same information about the slots and values;
    the information is just represented in slightly different ways. For your own projects,
    if you are using a public dataset, it would probably be most convenient to use
    the format that the dataset already uses. However, if you are using your own data,
    you can choose whatever format is most appropriate or easiest to use for your
    application. All other considerations being equal, the XML and JSON formats are
    generally more flexible than BIO or markdown, since they can represent nested
    slots, that is, slots that contain additional values as slot fillers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 四种格式基本上显示了相同的信息，只是在表示方式上略有不同。对于你自己的项目，如果使用的是公共数据集，可能最方便的是使用数据集已经采用的格式。然而，如果你使用的是自己的数据，可以选择任何最合适或最容易使用的格式。其他条件相同的情况下，XML和JSON格式通常比BIO或Markdown更具灵活性，因为它们可以表示嵌套槽位，即包含额外值作为槽位填充物的槽位。
- en: For our example, we will be using the spaCy CRF suite library, located at [https://github.com/talmago/spacy_crfsuite](https://github.com/talmago/spacy_crfsuite),
    and we will use restaurant search as an example application. This dataset is annotated
    using the Markdown format.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将使用位于[https://github.com/talmago/spacy_crfsuite](https://github.com/talmago/spacy_crfsuite)的spaCy
    CRF套件库，并使用餐厅搜索作为示例应用程序。此数据集采用Markdown格式进行了标注。
- en: 'The following code sets up the application by importing display and Markdown
    functionality and then reading in the Markdown file from the `examples` directory.
    Reading in the Markdown file will reproduce the list of utterances shown in *Figure
    9**.2*. Note that the training data in the Markdown file is not large enough for
    a real application, but it works as an example here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过导入显示和Markdown功能来设置应用程序，然后从`examples`目录读取Markdown文件。读取Markdown文件将重现*图9.2*中显示的发话列表。请注意，Markdown文件中的训练数据对于一个实际的应用程序来说不够大，但在这里作为示例是有效的：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The next steps, shown below, will be to import `crfsuite` and `spacy` and convert
    the Markdown-formatted training dataset to a CRF format. (The code in GitHub shows
    some additional steps that are omitted here for simplicity):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤如下所示，将导入`crfsuite`和`spacy`，并将Markdown格式的训练数据集转换为CRF格式。（GitHub中的代码显示了一些额外的步骤，这里为了简便省略了）：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point, we can do the actual training of the CRF with the `CRFExtractor`
    object, as shown here:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用`CRFExtractor`对象进行实际的CRF训练，如下所示：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The classification report, which is produced in the second to last step, and
    is shown next, is based on the training dataset (`train_dataset`). Since the CRF
    was trained on this dataset, the classification report will show perfect performance
    on each slot. Obviously, this is not realistic, but it’s shown here in order to
    illustrate the classification report. Remember that we will return to the topics
    of precision, recall, and F1 score in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告（在倒数第二步中生成，并在下方展示）基于训练数据集（`train_dataset`）。由于CRF是在此数据集上训练的，因此分类报告会显示每个槽位的完美表现。显然，这不太现实，但这里展示是为了说明分类报告。记住，我们将在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中回到精确度、召回率和F1分数的主题。
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point, the CRF model has been trained and is ready to test with new
    data. If we test this model with the sentence *show me some good chinese restaurants
    near me*, we can see the result in JSON format in the following code. The CRF
    model found two slots, `CUISINE` and `QUALITY`, but missed the `LOCATION` slot,
    which should have been filled by `near me`. The results also show the model’s
    confidence in the slots, which was quite high, well over `0.9`. The result also
    includes the zero-based positions of the characters in the input that begin and
    end the slot value (`good` starts at position `10` and ends at position `14`):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，CRF模型已经训练完成，准备使用新数据进行测试。如果我们用句子*show me some good chinese restaurants near
    me*来测试这个模型，我们可以在以下代码中看到JSON格式的结果。CRF模型找到了两个槽位，`CUISINE`和`QUALITY`，但漏掉了`LOCATION`槽位，而`near
    me`应该填充这个槽位。结果还显示了模型在槽位上的置信度，置信度相当高，超过了`0.9`。结果还包括输入中字符的零基位置，这些位置标识了槽位值的开始和结束位置（`good`的起始位置是`10`，结束位置是`14`）：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can illustrate the robustness of this approach by testing our application
    with a cuisine that was not seen in the training data, `Japanese`. Let’s see if
    the system can label `Japanese` as a cuisine in a new utterance. We can try an
    utterance such as `show some good Japanese restaurants near here` and see the
    result in the following JSON:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过测试应用程序是否能处理在训练数据中未出现过的菜系来说明这种方法的鲁棒性，`Japanese`。让我们看看系统是否能在新的语句中标注`Japanese`为菜系。我们可以尝试类似“show
    some good Japanese restaurants near here”的语句，并查看以下JSON中的结果：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The system did identify `Japanese` as a cuisine in this example, but the confidence
    was much lower than we saw in the previous example, only `0.537` this time, compared
    with `0.96` for the same sentence with a known cuisine. This relatively low confidence
    is typical for slot fillers that didn’t occur in the training data. Even the confidence
    of the `QUALITY` slot (which did occur in the training data) was lower, probably
    because it was affected by the low probability of the unknown `CUISINE` slot filler.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 系统确实在这个例子中识别出了`Japanese`作为菜系，但其置信度比我们在之前的例子中看到的要低得多，这次仅为`0.537`，而相同句子中若是已知菜系的置信度则为`0.96`。这种相对较低的置信度是训练数据中未出现的槽填充项的典型表现。即使是`QUALITY`槽（该槽在训练数据中出现过）的置信度也较低，可能是因为它受到了未知`CUISINE`槽填充项低概率的影响。
- en: A final observation worth pointing out is that while it would have been possible
    to develop a rule-based slot tagger for this task, as we saw in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159),
    the resulting system would not have been able to even tentatively identify `Japanese`
    as a slot filler unless `Japanese` had been included in one of the rules. This
    is a general illustration of how the statistical approach can provide results
    that are not all or none, compared to rule-based approaches.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 值得指出的最后一个观察是，虽然我们本可以为此任务开发一个基于规则的槽标签器，正如我们在[*第8章*](B19005_08.xhtml#_idTextAnchor159)中看到的那样，但最终的系统甚至无法勉强地将`Japanese`识别为槽填充项，除非`Japanese`被包含在某条规则中。这是统计方法相较于基于规则的方法的一般性例证，展示了统计方法可以提供一些“非全有或全无”的结果。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter has explored some of the basic and most useful classical statistical
    techniques for NLP. They are especially valuable for small projects that start
    out without a large amount of training data, and for the exploratory work that
    often precedes a large-scale project.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了一些基本的、最有用的经典统计技术在自然语言处理（NLP）中的应用。它们尤其对那些起步时没有大量训练数据的小项目和常常先于大规模项目进行的探索性工作非常有价值。
- en: We started out by learning about some basic evaluation concepts. We learned
    particularly about accuracy, but we also looked at some confusion matrices. We
    also learned how to apply Naïve Bayes classification to texts represented in TF-IDF
    format, and then we worked through the same classification task using a more modern
    technique, SVMs. Comparing the results produced by Naïve Bayes and SVMs, we saw
    that we got better performance from the SVMs. We then turned our attention to
    a related NLP task, slot-filling. We learned about different ways to represent
    slot-tagged data and finally illustrated CRFs with a restaurant recommendation
    task. These are all standard approaches that are good to have in your NLP toolbox,
    especially for the initial exploration of applications with limited data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从学习一些基本的评估概念开始，特别学习了准确率，同时我们也看了一些混淆矩阵。我们还学习了如何将朴素贝叶斯分类应用于TF-IDF格式表示的文本，然后使用更现代的技术——支持向量机（SVMs）来解决相同的分类任务。在比较朴素贝叶斯和SVMs的结果后，我们发现SVMs的表现更好。接着，我们将注意力转向了与之相关的NLP任务——槽填充。我们学习了不同的槽标签数据表示方式，最终通过一个餐厅推荐任务来展示了条件随机场（CRFs）。这些都是标准的方法，尤其适合用在数据有限的应用程序初步探索中，是NLP工具箱中的有用工具。
- en: In [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), we will continue working
    on topics in machine learning, but we will move on to a very different type of
    machine learning, neural networks. There are many varieties of neural networks,
    but overall, neural networks and their variants have become the standard technologies
    for NLP in the last decade or so. The next chapter will introduce this important
    topic.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B19005_10.xhtml#_idTextAnchor184)中，我们将继续讨论机器学习的相关话题，但我们将转向一种非常不同的机器学习类型——神经网络。神经网络有许多种类，但总的来说，神经网络及其变体已成为过去十多年里NLP的标准技术。下一章将介绍这一重要话题。
