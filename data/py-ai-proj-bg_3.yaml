- en: Applications for Comment Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评论分类的应用
- en: In this chapter, we'll overview the bag-of-words model for text classification.
    We will look at predicting YouTube comment spam with the bag-of-words and the
    random forest techniques. Then we'll look at the Word2Vec models and prediction
    of positive and negative reviews with the Word2Vec approach and the k-nearest
    neighbor classifier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将概述文本分类的词袋模型。我们将研究如何使用词袋模型和随机森林技术预测 YouTube 评论是否为垃圾邮件。然后我们将研究 Word2Vec
    模型，以及通过 Word2Vec 方法和 k-近邻分类器预测正面和负面评论。
- en: In this chapter, we will particularly focus on text and words and classify internet
    comments as spam or not spam or to identify internet reviews as positive or negative.
    We will also have an overview for bag of words for text classification and prediction
    model to predict YouTube comments are spam or not using bag of words and random
    forest techniques. We will also look at Word2Vec models an k-nearest neighbor
    classifier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将特别关注文本和单词，分类互联网评论为垃圾邮件或非垃圾邮件，或将互联网评论识别为正面或负面。我们还将概述文本分类的词袋模型，并使用词袋模型和随机森林技术预测
    YouTube 评论是否为垃圾邮件。我们还将研究 Word2Vec 模型和 k-近邻分类器。
- en: 'But, before we start, we''ll answer the following question: *w**hat makes text
    classification an interesting problem?*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在我们开始之前，我们将回答以下问题：*是什么让文本分类成为一个有趣的问题？*
- en: Text classification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: 'To find the answer to our question, we will consider the famous iris flower
    dataset as an example dataset. The following image is of iris versicolor species.
    To identify the species, we need some more information other than just an image
    of the species, such as the flower''s **Petal length**, **Petal width**, **Sepal
    length**, and **Sepal width **would help us identify the image better:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答我们的问题，我们将以著名的鸢尾花数据集作为示例数据集。下图是紫花鸢尾物种的图片。为了识别物种，我们需要一些除物种图像之外的其他信息，例如花朵的**花瓣长度**、**花瓣宽度**、**花萼长度**和**花萼宽度**，这些将帮助我们更好地识别图像：
- en: '![](img/00064.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: 'The dataset not only contains examples of versicolor but also contains examples
    of setosa and virginica as well. Every example in the dataset contains these four
    measurements. The dataset contains around 150 examples, with 50 examples of each
    species. We can use a decision tree or any other model to predict the species
    of a new flower, if provided with the same four measurements. As we know same
    species will have almost similar measurements. Since similarity has different
    definition all together but here we consider similarity as the closeness on a
    graph, if we consider each point is a flower. The following graph is a comparison
    between sepal width versus petal width:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集不仅包含了紫花鸢尾的例子，还包括了 Setosa 和 Virginica 的例子。数据集中的每个示例都包含这四个测量值。数据集包含大约 150
    个例子，每个物种包含 50 个例子。我们可以使用决策树或其他模型来预测一朵新花的物种，只要给出相同的四个测量值。正如我们所知，同一物种的花朵测量值几乎相似。由于相似性有不同的定义，但在这里我们将相似性视为在图上的接近度，假设每个点代表一朵花。下图是花萼宽度与花瓣宽度的比较：
- en: '![](img/00065.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: If we had no way of measuring similarity, if, say, every flower had different
    measurements, then there'd be no way to use machine learning to build a classifier.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有办法衡量相似性，比如说每朵花都有不同的测量值，那么就无法使用机器学习构建分类器了。
- en: As we are aware of the fact that flowers of same species have same measurement
    and that helps us to distinguish different species. Consider what if every flower
    had different measurement, it would of no use to build classifier using machine
    learning to identify images of species.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知道的，同一物种的花朵有相同的测量值，这帮助我们区分不同的物种。想象一下，如果每朵花的测量值都不同，那么使用机器学习构建分类器来识别物种的图像就没有意义了。
- en: Machine learning techniques
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习技术
- en: 'Before to that we considered images, let''s now consider text. For example,
    consider the following sentences and try to find what makes the first pair of
    phrases similar to the second pair:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑图像之前，我们现在来考虑文本。例如，考虑以下句子，并尝试找出第一对短语与第二对短语的相似之处：
- en: '![](img/00066.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: I hope you got the answer to that question, otherwise we will not be able to
    build a decision tree, a random forest or anything else to predict the model.
    To answer the question, notice that the top pair of phrases are similar as they
    contain some words in common, such as **subscribe** and **channel**, while the
    second pair of sentences have fewer words in common, such as **to **and **the**.
    Consider the each phrase representing vector of numbers in a way that the top
    pair is similar to the numbers in the second pair. Only then we will be able to
    use random forest or another technique for classification, in this case, to detect
    YouTube comment spam. To achieve this, we need to use the bag-of-words model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能回答那个问题，否则我们将无法构建决策树、随机森林或其他任何预测模型。为了回答这个问题，注意到前一对短语很相似，因为它们包含一些共同的单词，如**subscribe**和**channel**，而第二对句子包含的共同单词较少，如**to**和**the**。考虑每个短语代表一个数字向量，这样前一对短语在数值上与第二对短语相似。只有这样，我们才能使用随机森林或其他分类技术，在这种情况下，检测YouTube评论垃圾邮件。为了实现这一点，我们需要使用词袋模型。
- en: Bag of words
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: The bag-of-words model does exactly we want that is to convert the phrases or
    sentences and counts the number of times a similar word appears. In the world
    of computer science, a bag refers to a data structure that keeps track of objects
    like an array or list does, but in such cases the order does not matter and if
    an object appears more than once, we just keep track of the count rather we keep
    repeating them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型正是我们所需要的，它将短语或句子转换，并计算相似单词出现的次数。在计算机科学中，词袋（bag）指的是一种数据结构，类似于数组或列表，它跟踪对象，但在这种情况下，顺序不重要，如果一个对象出现多次，我们只需要跟踪其出现次数，而不是重复记录它们。
- en: For example, consider the first phrase from the previous diagram, it has a bag
    of words that contents words such as **channel**, with one occurrence, **plz**,
    with one occurrence, **subscribe**, two occurrences, and so on. Then, we would
    collect all these counts in a vector, where one vector per phrase or sentence
    or document, depending on what you are working with. Again, the order in which
    the words appeared originally doesn't matter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑前面图表中的第一个短语，它有一个词袋，包含像**channel**这样的单词，出现一次，**plz**，出现一次，**subscribe**，出现两次，等等。然后，我们会将所有这些计数收集到一个向量中，每个短语、句子或文档对应一个向量，具体取决于你正在处理的内容。再次强调，原始单词的出现顺序不重要。
- en: 'The vector that we created can also be used to sort data alphabetically, but
    it needs to be done consistently for all the different phrases. However, we still
    have the same problem. Each phrase has a vector with different columns, because
    each phrase has different words and a different number of columns, as shown in
    the following two tables:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的向量也可以用于按字母顺序排序数据，但必须对所有不同的短语一致地进行此操作。然而，我们仍然面临相同的问题。每个短语都有一个包含不同列的向量，因为每个短语包含不同的单词和不同数量的列，如下表所示：
- en: '![](img/00067.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpeg)'
- en: 'If we make a larger vector with all the unique words across both phrases, we
    get a proper matrix representation. With each row representing a different phrase,
    notice the use of 0 to indicate that a phrase doesn''t have a word:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过所有短语中的唯一单词创建一个更大的向量，我们会得到一个合适的矩阵表示。每一行代表一个不同的短语，注意使用0来表示某个短语中没有某个单词：
- en: '![](img/00068.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpeg)'
- en: If you want to have a bag of words with lots of phrases, documents, or  we would
    need to collect all the unique words that occur across all the examples and create
    a huge matrix, *N* x *M*, where *N* is the number of examples and *M* is the number
    of occurrences. We could easily have thousands of dimensions compared in a four-dimensional
    model for the iris dataset. The bag of words matrix is likely to be sparse, meaning
    mostly zeros, since most phrases don't have most words.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想拥有一个包含大量短语、文档等的词袋，我们需要收集所有示例中出现的唯一单词，并创建一个巨大的矩阵，*N* x *M*，其中*N*是示例的数量，*M*是出现次数的数量。我们可能会轻松拥有成千上万个维度，在用于鸢尾花数据集的四维模型中进行比较。词袋矩阵可能是稀疏的，意味着大部分是零，因为大多数短语中并没有包含大部分单词。
- en: 'Before we start building our bag of words model, we need to take care of a
    few things, such as the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建我们的词袋模型之前，我们需要处理一些事情，例如以下内容：
- en: Lowercase every word
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个单词转换为小写
- en: Drop punctuation
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除标点符号
- en: Drop very common words (stop words)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除常见词（停用词）
- en: Remove plurals (for example, bunnies => bunny)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除复数形式（例如，bunnies => bunny）
- en: Perform lemmatization (for example, reader => read, reading = read)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行词形还原（例如，reader => read，reading = read）
- en: Use n-grams, such as bigrams (two-word pairs) or trigrams
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 n-gram（如二元组（两词组合）或三元组）
- en: Keep only frequent words (for example, must appear in >10 examples)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅保留频繁出现的词汇（例如，必须在 >10 个示例中出现）
- en: Keep only the most frequent *M* words (for example, keep only 1,000)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅保留最频繁的 *M* 词汇（例如，仅保留 1,000 个）
- en: Record binary counts (*1* = present, *0* = absent) rather than true counts
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录二进制计数（*1* = 存在，*0* = 不存在）而不是实际计数
- en: There are many other combinations for best practice, and finding the best that
    suits the particular data needs some research.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他最佳实践的组合，找到最适合特定数据的组合需要一些研究。
- en: The problem that we face with long documents is that they will have higher word
    counts generally, but we may still want to consider long documents about some
    topic to be considered, similar to a short document about the same topic, even
    though the word counts will differ significantly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理长文档时面临的问题是，长文档通常会有更高的词汇计数，但即使词汇计数差异显著，我们仍然可能希望将关于某个主题的长文档视为与关于同一主题的短文档相似。
- en: Furthermore, if we still wanted to reduce very common words and highlight the
    rare ones, what we would need to do is record the relative importance of each
    word rather than its raw count. This is known as **term frequency inverse document
    frequency** (**TF-IDF**), which measures how common a word or term is in the document.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们仍然想要减少非常常见的词汇并突出显示稀有的词汇，我们需要做的是记录每个词汇的相对重要性，而不是其原始计数。这被称为**词频逆文档频率**（**TF-IDF**），它衡量一个词汇或术语在文档中的常见程度。
- en: We use logarithms to ensure that long documents with many words are very similar
    to short documents with similar words. TF-IDF has two components that multiply,
    that is when TF is high, the result is high but IDF measures how common the word
    is among all the documents and that will affect the common words. So, a word that
    is common in other documents will have a low score, regardless of how many times
    it appeared.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用对数函数来确保包含大量词汇的长文档与包含相似词汇的短文档非常相似。TF-IDF 由两个相乘的部分组成，当 TF 值较高时，结果也较高，但 IDF
    衡量的是词汇在所有文档中的常见程度，这将影响常见词汇的分数。因此，出现在其他文档中的常见词汇会得到较低的分数，无论它出现了多少次。
- en: 'If a document has a low score which means the word appeared rarely and if the
    score is high it means the word appears frequently in the document. But if the
    word is quite common in all the documents then it becomes irrelevant to score
    on this document. It is anyhow considered to have low score. This shows that the
    formula for TF-IDF exhibits in a way we want our model to be. The following graph
    explains our theory:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个文档的得分较低，意味着该词汇出现得较少；如果得分较高，意味着该词汇在文档中出现频繁。但是，如果该词汇在所有文档中都很常见，那么在该文档中的得分就变得不相关。无论如何，它的得分都会被认为是低的。这表明，TF-IDF
    公式以我们希望模型展现的方式运行。以下图表解释了我们的理论：
- en: '![](img/00069.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00069.jpeg)'
- en: We will be using the bag-of-words method to detect whether YouTube comments
    are spam or .
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用词袋方法来检测 YouTube 评论是否为垃圾邮件。
- en: Detecting YouTube comment spam
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测 YouTube 评论垃圾邮件
- en: In this section, we're going to look at a technique for detecting YouTube comment
    spam using bags of words and random forests. The dataset is pretty straightforward.
    We'll use a dataset that has about 2,000 comments from popular YouTube videos
    ([https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)).
    The dataset is formatted in a way where each row has a comment followed by a value
    marked as 1 or 0 for spam or not spam.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍一种使用词袋和随机森林检测 YouTube 评论垃圾邮件的技术。数据集非常直观。我们将使用一个包含约 2,000 条来自热门 YouTube
    视频评论的数据集（[https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)）。数据集的格式是每一行包含一个评论，后面跟着一个标记为
    1 或 0 的值，表示该评论是否为垃圾邮件。
- en: 'First, we will import a single dataset. This dataset is actually split into
    four different files. Our set of comments comes from the PSY-Gangnam Style video:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入一个数据集。这个数据集实际上分成了四个不同的文件。我们的评论集来自 PSY 的《江南 Style》视频：
- en: '![](img/00070.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpeg)'
- en: 'Then we will print a few comments as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将打印出以下几条评论：
- en: '![](img/00071.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00071.jpeg)'
- en: 'Here we are able to see that there are more than two columns, but we will only
    require the content and the class columns. The content column contains the comments
    and the class column contains the values 1 or 0 for spam or not spam. For example,
    notice that the first two comments are marked as not spam, but then the comment
    **subscribe to me for call of duty vids** is spam and **hi guys please my android
    photo editor download yada yada** is spam as well. Before we start sorting comments,
    let''s look at the count of how many rows in the dataset are spam and how many
    are not spam. The result we acquired is 175 and 175 respectively, which sums up
    to 350 rows overall in this file:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到有超过两列，但我们只需要内容列和分类列。内容列包含评论，分类列包含值1或0，表示是否为垃圾评论。例如，注意到前两条评论被标记为非垃圾评论，但接下来的评论**subscribe
    to me for call of duty vids**是垃圾评论，**hi guys please my android photo editor download
    yada yada**也是垃圾评论。在开始排序评论之前，我们先看一下数据集中垃圾评论和非垃圾评论的数量。我们得到的结果是175条垃圾评论和175条非垃圾评论，总共有350行数据：
- en: '![](img/00072.gif)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.gif)'
- en: 'In scikit-learn, the bag of words technique is actually called `CountVectorizer`,
    which means counting how many times each word appears and puts them into a vector.
    To create a vector, we need to make an object for `CountVectorizer`, and then
    perform the fit and transform simultaneously:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，词袋技术实际上被称为`CountVectorizer`，即统计每个单词出现的次数并将其放入向量中。为了创建一个向量，我们需要为`CountVectorizer`创建一个对象，然后同时执行fit和transform操作：
- en: '![](img/00073.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.jpeg)'
- en: 'This performed in two different steps. First comes the fit step, where it discovers
    which words are present in the dataset, and second is the transform step, which
    gives you the bag of words matrix for those phrases. The result obtained in that
    matrix is 350 rows by 1,418 columns:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这分为两个不同的步骤进行。首先是fit步骤，它会发现数据集中有哪些单词，其次是transform步骤，它会为这些短语生成词袋矩阵。得到的矩阵结果是350行，1,418列：
- en: '![](img/00074.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.jpeg)'
- en: There are 350 rows, which means we have 350 different comments and 1,418 words.
    1418 word apparently are word that appear across all of these phrases.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有350行，这意味着我们有350条不同的评论和1,418个单词。显然，1,418个单词是出现在所有这些短语中的单词。
- en: 'Now let''s print a single comment and then run the analyzer on that comment
    so that we can see how well the phrases breaks it apart. As seen in the following
    screenshot, the comment has been printed first and then we are analyzing it below,
    which is just to see how it broke it into words:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打印一个单独的评论，然后对该评论进行分析，以便查看短语是如何被拆解的。如下面的截图所示，评论首先被打印出来，然后我们在下方进行分析，这样我们可以看到它是如何拆解成单词的：
- en: '![](img/00075.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00075.jpeg)'
- en: 'We can use the vectorizer feature to find out which word the dataset found
    after vectorizing. The following is the result found after vectorizing where it
    starts with numbers and ends with regular words:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用向量化器功能来找出数据集在向量化后找到的单词。以下是向量化后的结果，其中开始是数字，结束是常规单词：
- en: '![](img/00076.gif)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.gif)'
- en: 'Execute the following command to shuffle the dataset with fraction 100% that
    is adding `frac=1`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令来打乱数据集，设置分数为100%，即添加`frac=1`：
- en: '![](img/00077.gif)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00077.gif)'
- en: 'Now we will split the dataset into training and testing sets. Let''s assume
    that the first 300 will be for training, while the latter 50 will be for testing:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把数据集拆分为训练集和测试集。假设前300条用于训练，后50条用于测试：
- en: '![](img/00078.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpeg)'
- en: In the preceding code, `vectorizer.fit_transform(d_train['CONTENT'])` is an
    important step. At that stage, you have a training set that you want to perform
    a fit transform on, which means it will learn the words and also produce the matrix.
    However, for the testing set, we don't perform a fit transform again, since we
    don't want the model to learn different words for the testing data. We will use
    the same words that it learned on the training set. Suppose that the testing set
    has different words out of which some of them are unique to the testing set that
    might have never appeared in the training set. That's perfectly fine and anyhow
    we are going to ignore it. Because we are using the training set to build a random
    forest or decision tree or whatever would be the case, we have to use a certain
    set of words, and those words will have to be the same words, used on the testing
    set. We cannot introduce new words to the testing set since the random forest
    or any other model would not be able to gauge the new words.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，`vectorizer.fit_transform(d_train['CONTENT'])` 是一个重要步骤。在这一阶段，你有一个训练集，你希望对其进行拟合转换，这意味着它会学习单词并生成矩阵。然而，对于测试集，我们不再执行拟合转换，因为我们不希望模型为测试数据学习不同的单词。我们将使用它在训练集上学到的相同单词。假设测试集有不同的单词，其中一些可能是测试集中独有的，可能从未出现在训练集中。这是完全可以接受的，反正我们将忽略它。因为我们使用训练集来构建随机森林或决策树，或者无论是什么模型，我们必须使用一组特定的单词，而这些单词在测试集上也必须是相同的。我们不能向测试集引入新单词，因为随机森林或任何其他模型都无法评估新单词。
- en: 'Now we perform the transform on the dataset, and later we will use the answers
    for training and testing. The training set now has 300 rows and 1,287 different
    words or columns, and the testing set has 50 rows, but we have the same 1,287
    columns:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据集进行转换，稍后将使用这些答案进行训练和测试。训练集现在有300行和1,287个不同的单词或列，而测试集有50行，但我们仍然有相同的1,287列：
- en: '![](img/00079.gif)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00079.gif)'
- en: 'Even though the testing set has different words, we need to make sure it is
    transformed in the same way as the training set with the same columns. Now we
    will begin with the building of the random forest classifier. We will be converting
    this dataset into 80 different trees and we will fit the training set so that
    we can score its performance on the testing set:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 即使测试集有不同的单词，我们也需要确保它与训练集以相同的方式进行转换，并保持相同的列。现在我们将开始构建随机森林分类器。我们将把这个数据集转化成80棵不同的树，并拟合训练集，以便在测试集上评估其表现：
- en: '![](img/00080.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpeg)'
- en: 'The output of the score that we received is 98%; that''s really good. Here
    it seems it got confused between spam and not-spam. We need be sure that the accuracy
    is high; for that, we will perform a cross validation with five different splits.
    To perform a cross validation, we will use all the training data and let it split
    it into four different groups: 20%, 80%, and 20% will be testing data, and 80%
    will be the training data:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得的分数输出是98%；这真的很不错。在这里，似乎它在垃圾邮件和非垃圾邮件之间产生了混淆。我们需要确保准确率很高；为此，我们将进行五次不同拆分的交叉验证。为了执行交叉验证，我们将使用所有训练数据并将其分成四个不同的组：20%、80%
    和 20% 将作为测试数据，80% 将作为训练数据：
- en: '![](img/00081.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00081.jpeg)'
- en: 'We will now perform an average to the scores that we just obtained, which comes
    to about 95% accuracy. Now we will print all the data as seen in the following
    screenshot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对刚才获得的分数进行平均，结果大约是95%的准确率。接下来，我们将打印出所有数据，如下图所示：
- en: '![](img/00082.gif)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.gif)'
- en: 'The entire dataset has five different videos with comments, which means all
    together we have around 2,000 rows. On checking all the comments, we noticed that
    there are `1005` spam comments and `951` not-spam comments, that quite close enough
    to split it in to even parts:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集包含五个不同的视频评论，这意味着我们一共有大约2,000行。在检查所有评论时，我们注意到有`1005`条垃圾评论和`951`条非垃圾评论，它们非常接近，足够均匀地分成两部分：
- en: '![](img/00083.gif)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.gif)'
- en: 'Here we will shuffle the entire dataset and separate the comments and the answers:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将打乱整个数据集并分离评论和答案：
- en: '![](img/00084.gif)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.gif)'
- en: 'We need to perform a couple of steps here with `CountVectorizer` followed by
    the random forest. For this, we will use a feature in scikit-learn called a **Pipeline**.
    Pipeline is really convenient and will bring together two or more steps so that
    all the steps are treated as one. So, we will build a pipeline with the bag of
    words, and then use `countVectorizer` followed by the random forest classifier.
    Then we will print the pipeline, and it the steps required:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里执行几个步骤，使用`CountVectorizer`，然后是随机森林。为此，我们将使用scikit-learn中的一个特性叫做**Pipeline**。Pipeline非常方便，它可以将两个或更多步骤组合在一起，这样所有步骤就能作为一个整体来处理。因此，我们将构建一个包含词袋的管道，然后使用`countVectorizer`，接着是随机森林分类器。然后我们会打印出管道，它包含了所需的步骤：
- en: '![](img/00085.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpeg)'
- en: 'We can let the pipeline name of each step by itself by adding `CountVectorizer`
    in our `RandomForestClassifier` and it will name them `CountVectorizer` and `RandomForestclassifier`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将`CountVectorizer`添加到我们的`RandomForestClassifier`中，让管道自动为每个步骤命名，并且它会将它们命名为`CountVectorizer`和`RandomForestClassifier`：
- en: '![](img/00086.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.jpeg)'
- en: 'Once the pipeline is created you can just call it fit and it will perform the
    rest that is first it perform the fit and then transform with the `CountVectorizer`,
    followed by a fit with the `RandomForest` classifier. That''s the benefit of having
    a pipeline:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦管道创建完成，你只需调用fit，它会执行接下来的步骤：首先执行fit，然后使用`CountVectorizer`进行转换，接着使用`RandomForest`分类器进行拟合。这就是使用管道的好处：
- en: '![](img/00087.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpeg)'
- en: 'Now you call score so that it knows that when we are scoring it will to run
    it through the bag of words `countVectorizer`, followed by predicting with the
    `RandomForestClassifier`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以调用score，这样它就知道在评分时会先通过词袋`countVectorizer`，然后用`RandomForestClassifier`进行预测：
- en: '![](img/00088.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.jpeg)'
- en: 'This whole procedure will produce a score of about 94\. We can only predict
    a single example with the pipeline. For example, imagine we have a new comment
    after the dataset has been trained, and we want to know whether the user has just
    typed this comment or whether it''s spam:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程将产生大约94的得分。我们只能用管道预测一个示例。例如，假设我们在数据集训练之后有一个新的评论，我们想知道用户输入的这条评论是否为垃圾评论：
- en: '![](img/00089.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpeg)'
- en: 'As seen, it''s detected correctly; but what about the following comment:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，它检测得很准确；但对于以下评论呢：
- en: '![](img/00090.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00090.jpeg)'
- en: 'To overcome this and deploy this classifier into an environment and predict
    whether it is a `spm` or not when someone types a new comment. We will use our
    pipeline to figure out how accurate our cross-validation was. We find in this
    case that the average accuracy was about 94:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题并将此分类器部署到一个环境中，在有人输入新评论时预测它是否为`spm`。我们将使用我们的管道来计算交叉验证的准确性。在这种情况下，我们发现平均准确率约为94：
- en: '![](img/00091.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00091.jpeg)'
- en: 'It''s pretty good. Now let''s add TF-IDF to our model to make it more precise:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当不错。现在让我们在模型中加入TF-IDF，以使其更精确：
- en: '![](img/00092.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.jpeg)'
- en: 'This will be placed after `countVectorizer`. After we have produced the counts,
    we can then produce a TF-IDF score for these counts. Now we will add this in the
    pipeline and perform another cross-validation check with the same accuracy:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将放置在`countVectorizer`之后。在生成计数后，我们可以为这些计数计算TF-IDF得分。现在我们将在管道中加入这个，并用相同的准确度执行另一次交叉验证检查：
- en: '![](img/00093.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpeg)'
- en: 'This show the steps required for the pipeline:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了管道所需的步骤：
- en: '![](img/00094.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.jpeg)'
- en: The following output got us `CountVectorizer`, a TF-IDF transformer, and `RandomForestClassifier`.
    Notice that `countvectorizer` can be lower case or upper case in the dataset;
    it is on us to decide how many words you want to have. We can either use single
    words or bigrams, which would be pairs of words, or trigrams, which can be triples
    of words. We can also remove stop words, which are really common English words
    such as **and**, **or**, and **the**. With TF-IDF, you can turn off the `idf`
    component and just keep the `tf` component, which would just be a log of the count.
    You can use `idf` as well. With random forests, you've got a choice of how many
    trees you use, which is the number of estimators.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了我们得到了`CountVectorizer`、一个TF-IDF转换器和`RandomForestClassifier`。注意，`countVectorizer`可以是小写或大写，这取决于数据集；我们可以决定要使用多少个单词。我们可以使用单词，也可以使用二元组（即词对）或三元组（即词三元组）。我们还可以移除停用词，这些停用词是非常常见的英语单词，如**and**、**or**和**the**。在使用TF-IDF时，你可以关闭`idf`部分，只保留`tf`部分，这样它就只是计数的对数。你也可以使用`idf`。使用随机森林时，你可以选择使用多少棵树，这就是估算器的数量。
- en: 'There''s another feature of scikit-learn available that allows us to search
    all of these parameters. For that, it finds out what the best parameters are:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 还有另一个特性可供我们搜索所有这些参数。通过这个特性，我们可以找到最佳的参数：
- en: '![](img/00095.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpeg)'
- en: We can make a little dictionary where we say the name of the pipeline step and
    then mention what the parameter name would be and this gives us our options. For
    demonstration, we are going to try maximum number of words or maybe just a maximum
    of 1,000 or 2,000 words.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个小字典，列出管道步骤的名称，然后说明参数名称，这样我们就有了选择的选项。为了演示，我们将尝试最大单词数，或者仅仅是 1,000 或 2,000
    个单词。
- en: 'Using `ngrams`, we can mention just single words or pairs of words that are
    stop words, use the English dictionary of stop words, or don''t use stop words,
    which means in the first case we need to get rid of common words, and in the second
    case we do not get rid of common words. Using TF-IDF, we use `idf` to state whether
    it''s yes or no. The random forest we created uses 20, 50, or 100 trees. Using
    this, we can perform a grid search, which runs through all of the combinations
    of parameters and finds out what the best combination is. So, let''s give our
    pipeline number 2, which has the TF-IDF along with it. We will use `fit` to perform
    the search and the outcome can be seen in the following screenshot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ngrams`，我们可以只提及单个单词或单词对，这些单词是停用词，使用英语停用词字典，或者不使用停用词，这意味着在第一种情况下，我们需要去除常用词，在第二种情况下，我们不去除常用词。使用
    TF-IDF，我们使用 `idf` 来说明是 yes 还是 no。我们创建的随机森林使用了 20、50 或 100 棵树。利用这些，我们可以执行网格搜索，遍历所有参数组合，并找出最佳组合。所以，让我们给我们的第二个管道编号，其中包含
    TF-IDF。我们将使用 `fit` 执行搜索，结果可以在以下截图中看到：
- en: '![](img/00096.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.jpeg)'
- en: 'Since there is a large number of words, it takes a little while, around 40
    seconds, and ultimately finds the best parameters. We can get the best parameters
    out of the grid search and print them to see what the score is:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于单词数目很大，需要一点时间，大约 40 秒钟，最终找到最佳参数。我们可以从网格搜索中获取最佳参数并打印出来，看看分数如何：
- en: '![](img/00097.gif)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.gif)'
- en: So, we got nearly 96% accuracy. We used around 1,000 words, only single words,
    used yes to get rid of stop words, had 100 trees in the random forest, and used
    yes and the IDF and the TF-IDF computation. Here we've demonstrated not only bag
    of words, TF-IDF, and random forest, but also the pipeline feature and the parameter
    search feature known as grid search.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们达到了近 96% 的准确率。我们使用了大约 1,000 个单词，仅使用了单词，使用了 yes 来去除停用词，在随机森林中使用了 100 棵树，同时使用了
    yes、IDF 和 TF-IDF 计算。在这里，我们不仅演示了词袋模型、TF-IDF 和随机森林，还演示了管道特性和网格搜索特性。
- en: Word2Vec models
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec 模型
- en: In this section, we'll learn about Word2Vec, a modern and popular technique
    for working with text. Usually, Word2Vec performs better than simple bag of words
    models. A bag of words model only counts how many times each word appears in each
    document. Given two such bag of words vectors, we can compare documents to see
    how similar they are. This is the same as comparing the words used in the documents.
    In other words, if the two documents have many similar words that appear a similar
    number of times, they will be considered similar.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习关于 Word2Vec 的知识，这是一种现代和流行的处理文本的技术。通常情况下，Word2Vec 的表现优于简单的词袋模型。词袋模型只计算每个文档中每个单词出现的次数。给定两个这样的词袋向量，我们可以比较文档，看它们的相似程度。这与比较文档中使用的单词是一样的。换句话说，如果两个文档有许多相似的单词，它们出现的次数也相似，那么它们将被认为是相似的。
- en: But bag of words models have no information about how similar the words are.
    So, if two documents do not use exactly the same words but do use synonyms, such
    as **please** and **plz**, they're not regarded as similar for the bag of words
    model. Word2Vec can figure out that some words are similar to each other and we
    can exploit that fact to get better performance when doing machine learning with
    text.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是词袋模型没有关于单词相似性的信息。因此，如果两个文档没有使用完全相同的单词，而是使用同义词，比如 **please** 和 **plz**，它们在词袋模型中并不被视为相似。Word2Vec
    可以发现某些单词彼此相似，我们可以利用这一点在进行文本机器学习时获得更好的性能。
- en: 'In Word2Vec, each word itself is a vector, with perhaps 300 dimensions. For
    example, in a pre-trained Google Word2Vec model that examined millions or billions
    of pages of text, we can see that cat, dog, and spatula are 300-dimensional vectors:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Word2Vec 中，每个单词本身就是一个向量，可能有 300 维。例如，在一个预训练的 Google Word2Vec 模型中，它检查了数百万或数十亿页的文本，我们可以看到
    cat、dog 和 spatula 是 300 维向量：
- en: Cat = <0.012, 0.204, ..., -0.275, 0.056> (300 dimensions)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫 = <0.012, 0.204, ..., -0.275, 0.056>（300维）
- en: Dog = <0.051, -0.022, ..., -0.355, 0.227>
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗 = <0.051, -0.022, ..., -0.355, 0.227>
- en: Spatula = <-0.191, -0.043, ..., -0.348, 0.398>
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锅铲 = <-0.191, -0.043, ..., -0.348, 0.398>
- en: Similarity (distance) between cat and dog—0.761
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫和狗的相似度（距离）—0.761
- en: Similarity between cat and spatula—0.124
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫和锅铲的相似度—0.124
- en: If we compare the similarity of the dog and cat vectors, we will get 0.761 or
    76% of similarity. If we do the same with cat and spatula, we get 0.124\. It's
    clear that Word2Vec learned that dog and cat are similar words but cat and spatula
    are not. Word2Vec uses neural networks to learn these word vectors. At a high
    level, a neural network is similar to random forest or a decision tree and other
    machine learning techniques because they're given a bunch of inputs and a bunch
    of outputs, and they learn how to predict the outputs from the inputs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较狗和猫的向量相似度，我们将得到0.761，或者说76%的相似度。如果我们用猫和锅铲做相同的比较，我们得到0.124。很明显，Word2Vec学到了狗和猫是相似的词，而猫和锅铲则不是。Word2Vec利用神经网络来学习这些词向量。从高层次来看，神经网络类似于随机森林或决策树以及其他机器学习技术，因为它们接收一组输入和一组输出，并学习如何根据输入预测输出。
- en: For Word2Vec, the input is a single word, the word whose vector we want to learn,
    and the output is its nearby words from the text. Word2Vec also supports the reverse
    of this input-output configuration. Thus, Word2Vec learns the word vectors by
    remembering its context words. So, dog and cat will have similar word vectors
    because these two words are used in similar ways, like *she pet the dog* and *she
    pet the cat*. Neural networking with Word2Vec can take one of two forms because
    Word2Vec supports two different techniques for training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Word2Vec，输入是单个词，即我们想要学习其向量的词，输出是该词周围的词。Word2Vec也支持这种输入输出配置的反向操作。因此，Word2Vec通过记住上下文词来学习词向量。所以，狗和猫会有相似的词向量，因为这两个词在类似的场景中使用，例如
    *她抚摸了狗* 和 *她抚摸了猫*。通过Word2Vec进行神经网络训练可以采取两种形式，因为Word2Vec支持两种不同的训练技术。
- en: 'The first technique is known as continuous bag of words, where the context
    words are the input, leaving out the middle word and the word whose vector we''re
    learning, the middle word, is the output. In the following diagram, you can see
    three words before and after the word **channel**:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种技术被称为连续词袋模型，在这种模型中，上下文词作为输入，忽略中间词，正在学习向量的词（即中心词）作为输出。在以下图示中，你可以看到**channel**这个词前后各有三个词：
- en: '![](img/00098.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: Those are the context words. The continuous bag of words model slides over the
    whole sentence with every word acting as a center word in turn. The neural network
    learns the 300-dimensional vectors for each word so that the vector can predict
    the center word given the context words. In other words, it can predict the output
    given its inputs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是上下文词。连续词袋模型会在整个句子中滑动，每个词依次充当中心词。神经网络学习每个词的300维向量，以便在给定上下文词的情况下预测中心词。换句话说，它能够根据输入预测输出。
- en: 'In the second technique, we''re going to flip this. This is known as **skip-gram**,
    and the center word is the input and the context words are the outputs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种技术中，我们将会反转这一过程。这被称为**skip-gram**，其中中心词是输入，上下文词是输出：
- en: '![](img/00099.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: In this technique, the center word vector is used to predict the context words
    given that center word.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，中心词向量被用来预测给定中心词的上下文词。
- en: Both of these techniques perform well for most situations. They each have minor
    pros and cons that will not be important for our use case.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术在大多数情况下表现良好。它们各自有一些小优缺点，但对于我们的使用场景来说并不重要。
- en: Doc2Vec
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Doc2Vec
- en: 'We''re going to use Word2Vec to detect positive and negative product, restaurant,
    and movie reviews. We will do so with a slightly different form of Word2Vec known
    as **Doc2Vec**. In this case, the input is a document name, such as the filename,
    and the output is the sliding window of the words from the document. This time,
    we will not have a center word:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Word2Vec来检测正面和负面的产品、餐厅和电影评论。我们将使用一种略微不同形式的Word2Vec，称为**Doc2Vec**。在这种情况下，输入是文档名，例如文件名，输出是文档中的词汇滑动窗口。这一次，我们将没有中心词：
- en: '![](img/00100.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpeg)'
- en: In this case, as a vector that helps us predict the words, from knowing the
    filename. In fact, the input is not very important, which in this case is the
    filename. We just need to keep track of the words on the right side, and that
    they all came from the same document. So, all of those words will be connected
    to that filename, but the actual content of that filename is not important. Since
    we can predict the document's words based on its filename, we can effectively
    have a model that knows which words go together in a document. In other words,
    that documents usually talk about just one thing, for example, learning that a
    lot of different positive words are used in positive reviews and a lot of negative
    words are used in negative reviews.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，作为一个帮助我们预测单词的向量，通过了解文件名。事实上，输入并不非常重要，在本例中就是文件名。我们只需要跟踪右侧的单词，确保它们都来自同一文档。因此，所有这些单词都会与该文件名连接起来，但文件名的实际内容并不重要。因为我们可以根据文件名预测文档的单词，所以我们可以有效地拥有一个模型，知道文档中哪些单词是会一起出现的。换句话说，文档通常只谈论一件事，例如，学习到很多不同的正面词汇会出现在正面评论中，而负面评论中会出现很多负面词汇。
- en: Document vector
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档向量
- en: 'After training, we have a new document and we want to find its document vector.
    We''ll use the word similarities learned during training to construct a vector
    that will predict the words in the new document. We will use a dummy filename
    since the actual name is not important. What''s important is that it''s just one
    name. So, all of these words get connected together under that one name:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们得到一个新的文档，想要找到它的文档向量。我们将利用在训练过程中学到的单词相似性来构建一个向量，预测新文档中的单词。我们将使用一个虚拟的文件名，因为实际名称并不重要。重要的是它只是一个名称。所以，所有这些单词都将在该一个名称下连接在一起：
- en: '![](img/00101.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.jpeg)'
- en: 'Once we get that new document vector, we can compare it with other document
    vectors and find which known document from the past is the most similar, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了新的文档向量，就可以将其与其他文档向量进行比较，并找到最相似的过去已知文档，如下所示：
- en: '![](img/00102.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpeg)'
- en: Thus, we can use `Doc2Vec` to find which documents are most similar to each
    other. This will help us detect positive and negative reviews because, ideally,
    the positive reviews will have document vectors that are similar to each other
    and this will be the same for negative reviews. We expect `Doc2Vec` to perform
    better than bag of words because `Doc2Vec` learns the words that are used together
    in the same document, so those words that are similar to bag of words never actually
    learned any information about how similar the words are different.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用`Doc2Vec`来找出哪些文档彼此最相似。这将帮助我们检测正面和负面评论，因为理想情况下，正面评论将具有相似的文档向量，负面评论也是如此。我们期望`Doc2Vec`的表现优于词袋模型，因为`Doc2Vec`学习的是在同一文档中一起使用的单词，所以那些类似于词袋模型的单词并没有实际学到关于单词相似性的信息。
- en: Detecting positive or negative sentiments in user reviews
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测用户评论中的正面或负面情感
- en: 'In this section, we''re going to look at detecting positive and negative sentiments
    in user reviews. In other words, we are going to detect whether the user is typing
    a positive comment or a negative comment about the product or service. We''re
    going to use `Word2Vec` and `Doc2Vec` specifically and the `gensim` Python library
    for those services. There are two categories, which are positive and negative,
    and we have over 3,000 different reviews to look at. These come from Yelp, IMDb,
    and Amazon. Let''s begin the code by importing the `gensim` library, which provides
    `Word2Vec` and `Doc2Vec` for logging to note status of the messages:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将研究如何检测用户评论中的正面和负面情感。换句话说，我们将检测用户是对产品或服务发表正面评论还是负面评论。我们将专门使用`Word2Vec`和`Doc2Vec`，以及`gensim`
    Python库来实现这些服务。我们有两个类别，正面和负面评论，而且我们有超过3,000条不同的评论供我们分析。这些评论来自Yelp、IMDb和Amazon。让我们通过导入`gensim`库开始代码，`gensim`提供了`Word2Vec`和`Doc2Vec`，并且用于记录消息的状态：
- en: '![](img/00103.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: 'First, we will see how to load a pre-built `Word2Vec` model, provided by Google,
    that has been trained on billions of pages of text and has ultimately produced
    300-dimensional vectors for all the different words. Once the model is loaded,
    we will look at the vector for `cat`. This shows that the model is a 300-dimensional
    vector, as represented by the word `cat`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看到如何加载一个由谷歌提供的预训练的`Word2Vec`模型，该模型已在数十亿页文本上进行训练，并最终为所有不同的单词生成了300维的向量。加载模型后，我们将查看`cat`的向量。这表明模型是一个300维的向量，如同`cat`所表示的：
- en: '![](img/00104.gif)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.gif)'
- en: 'The following screenshot shows the 300-dimensional vector for the word `dog`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了单词`dog`的300维向量：
- en: '![](img/00105.gif)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.gif)'
- en: 'The following screenshot shows the 300-dimensional vector for the word `spatula`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了单词`spatula`的300维向量：
- en: '![](img/00106.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.jpeg)'
- en: 'We obtain a result of 76% when computing the similarity of dog and cat, as
    follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算狗和猫的相似度时得到了76%的结果，如下所示：
- en: '![](img/00107.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.jpeg)'
- en: 'The similarity between cat and spatula is 12%; it is a bit lower, as it should
    be:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 猫和铲子的相似度是12%；它有点低，这也是应该的：
- en: '![](img/00108.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpeg)'
- en: 'Here we train our `Word2Vec` and `Doc2Vec` model using the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用以下代码训练我们的`Word2Vec`和`Doc2Vec`模型：
- en: '![](img/00109.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)'
- en: We are using `Doc2Vec` because we want to determine a vector for each document,
    not necessarily for each word in the document, because our documents are reviews
    and we want to see whether these reviews are positive or negative, which means
    it's similar to positive reviews or similar to negative reviews. `Doc2Vec` is
    provided by `gensim` and the library has a class called `TaggedDocument` that
    allows us to use "`these are the words in the document, and Doc2Vec is the model`".
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Doc2Vec`是因为我们希望为每个文档确定一个向量，而不是每个单词的向量，因为我们的文档是评论，我们希望查看这些评论是正面的还是负面的，这意味着它类似于正面评论或类似于负面评论。`Doc2Vec`由`gensim`提供，库中有一个叫做`TaggedDocument`的类，它允许我们使用“`这些是文档中的单词，Doc2Vec是模型`”。
- en: 'Now we create a utility function that will take a sentence or a whole paragraph
    and lowercase it and remove all the HTML tags, apostrophes, punctuation, spaces,
    and repeated spaces, and then ultimately break it apart by words:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个工具函数，它将接收一个句子或整个段落，并将其转为小写，移除所有的HTML标签、撇号、标点符号、空格和重复的空格，最终将其按单词拆分：
- en: '![](img/00110.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpeg)'
- en: Now it's time for our training set. We are not going to use the 3,000 Yelp,
    IMDb, and Amazon reviews because there's simply not enough data to train for a
    good `Doc2Vec` model. If we had millions reviews, then we could take a good portion
    of that to train with and use the rest for testing, but with just 3,000 reviews
    it's not enough. So, instead, I've gathered reviews from IMDb and other places,
    including Rotten Tomato. This will be enough to train a `Doc2Vec` model, but none
    of these are actually from the dataset that we're going to use for our final prediction.
    These are simply reviews. They're positive; they're negative. I don't know which,
    as I'm not keeping track of which. What matters is that we have enough text to
    learn how words are used in these reviews. Nothing records whether the review
    is positive or negative.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候准备我们的训练集了。我们不会使用3,000条Yelp、IMDb和Amazon的评论，因为这些数据量不足以训练一个良好的`Doc2Vec`模型。如果我们有数百万条评论，那我们可以拿出一部分进行训练，其余的用作测试，但仅有3,000条评论是不够的。因此，我从IMDb和其他地方收集了评论，包括Rotten
    Tomato。这些足以训练一个`Doc2Vec`模型，但这些评论并不来自我们最终预测所使用的数据集。这些评论仅仅是评论。它们是正面的；它们是负面的。我不知道是哪种，因为我没有记录是哪种。重要的是我们有足够的文本来学习单词在这些评论中的使用方式。没有任何记录说明评论是正面的还是负面的。
- en: 'So, `Doc2Vec` and `Word2Vec` are actually being used for unsupervised training.
    That means we don''t have any answers. We simply learn how words are used together.
    Remember the context of words, and how a word is used according to the words nearby:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`Doc2Vec`和`Word2Vec`实际上是用于无监督训练。这意味着我们没有任何答案。我们只是学习单词是如何一起使用的。记住单词的上下文，以及单词如何根据附近的单词来使用：
- en: '![](img/00111.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.jpeg)'
- en: 'So, in each case, in each file, we simply make a `TaggedDocument` object with
    the words from that document or that review plus a tag, which is simply the filename.
    This is important so that it learns that all these words go together in the same
    document, and that these words are somehow related to each other. After loading,
    we have 175,000 training examples from different documents:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在每个案例中，在每个文件中，我们简单地使用该文档或评论中的单词加上一个标签，标签就是文件名。这一点很重要，因为它可以让模型学习到这些单词属于同一文档，并且这些单词之间是有某种关系的。加载后，我们从不同的文档中得到了175,000个训练示例：
- en: '![](img/00112.gif)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.gif)'
- en: 'Now let''s have a look at the first 10 sentences in the following screenshot:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下下图中的前10个句子：
- en: '![](img/00113.gif)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00113.gif)'
- en: 'We shuffle these documents and then feed them into our `Doc2Vec` trainer, using `Doc2Vec(permuter,
    dm=0, hs=1, size=50)`, where we finally do the training of the `Doc2Vec` model
    and where it learns the document vectors for all the different documents. `dm=0` and `hs=1` are
    just parameters to say how to do the training. These are just things that I found
    were the most accurate. `dm=0` is where we are using the model that was shown
    in the last section, which means it receives a filename and it predicts the words:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打乱这些文档，然后将它们输入到 `Doc2Vec` 训练器中，使用 `Doc2Vec(permuter, dm=0, hs=1, size=50)`，在这里我们最终训练了
    `Doc2Vec` 模型，并让它学习所有不同文档的文档向量。`dm=0` 和 `hs=1` 只是表示如何进行训练的参数。这些是我发现最准确的设置。`dm=0`
    是我们在上一节中展示的模型，意味着它接收一个文件名并预测单词：
- en: '![](img/00114.gif)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.gif)'
- en: Here `size=50` means that we found that 50-dimensional vectors for each document
    was best, and 300-dimensional vectors are optimal, because we don't have enough
    training examples. Since we don't have millions or billions of data. This is a
    good 300 dimensional vector, and 50 seemed to work better. Running this code uses
    the processor and all the cores you have, so it will takes some time to execute.
    You will see that it's going through all the percentages of how much it got through.
    Ultimately, it takes 300 seconds to get this information in my case, which is
    definitely not bad. That's pretty fast, but if you have millions or billions of
    training documents, it could take days.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `size=50` 表示我们发现每个文档的50维向量是最好的，而300维的向量则最优，因为我们没有足够的训练样本。由于我们没有数百万或数十亿的数据，这是一个很好的300维向量，50维似乎效果更好。运行这段代码会使用处理器和所有核心，所以执行时会花费一些时间。你会看到它显示了完成的百分比。最终，在我的案例中，它花了300秒来获取这些信息，肯定算是不错的。速度挺快的，但如果你有数百万或数十亿的训练文档，那可能需要几天时间。
- en: 'Once the training is complete, we can delete some stuff to free up some memory:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以删除一些内容以释放内存：
- en: '![](img/00115.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00115.jpeg)'
- en: We do need to keep the inference data, which is enough to bind a new document
    vector for new documents, but we don't need it to keep all the data about all
    the different words.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实需要保留推断数据，这足以为新文档绑定一个新的文档向量，但我们不需要保留所有关于不同单词的数据。
- en: 'You can save the model and then load it later with the `model = Doc2Vec.Load(''reviews.d2v'')`
    command, if you want to put it in a product and deploy it, or put it on a server:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以保存模型，然后用 `model = Doc2Vec.Load('reviews.d2v')` 命令在之后加载它，如果你想把它放到产品中部署，或者放到服务器上：
- en: '![](img/00116.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpeg)'
- en: 'After the model''s been trained, you can infer a vector, which is regarding
    what the document vector is for this new document. So, let''s extract the words
    with the utility function. Here we are using an example phrase that was found
    in a review. This is the 50-dimensional vector it learned for that phrase:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，你可以推断出一个向量，表示该新文档的文档向量。那么，让我们使用工具函数提取单词。这里我们使用的是一个在评论中找到的示例短语。这是它为该短语学习到的50维向量：
- en: '![](img/00117.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpeg)'
- en: 'Now the question that rises is what about a negative phrase? And another negative
    phrases. Are they considered similar? Well, they''re considered 48% similar, as
    seen in the following screenshot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现的问题是，负面短语呢？还有其他负面短语。它们被认为相似吗？嗯，它们被认为有48%的相似度，如下截图所示：
- en: '![](img/00118.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.jpeg)'
- en: 'What about different phrases? `Highly recommended` and `Service sucks`. They''re
    less similar:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 那不同的短语怎么样呢？`Highly recommended` 和 `Service sucks`。它们不太相似：
- en: '![](img/00119.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: The model learned about how words are used together in the same review and that
    these words go together in one way and that other words go together in a different
    way.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 模型学习了单词在同一评论中如何一起使用，并了解到这些单词以一种方式组合，而其他单词则以另一种方式组合。
- en: 'Finally, we are ready to load our real dataset for prediction:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备加载我们的真实数据集进行预测：
- en: '![](img/00120.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: To summarize, we used Yelp, Amazon, and IMDb reviews. We loaded different files
    and in each file, each line had a review. As a result, we get the words from the
    line and found out what the vector was for that document. We put that in a list,
    shuffle, and finally built a classifier. In this case, we're going to use k-nearest
    neighbors, which is a really simple technique.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们使用了Yelp、Amazon和IMDb的评论。我们加载了不同的文件，每个文件中的每一行都包含一条评论。最终，我们从行中提取了单词，找出了该文档的向量。我们将其放入一个列表中，打乱顺序，最终构建了一个分类器。在这种情况下，我们将使用k近邻算法，这是一种非常简单的技术。
- en: 'It''s just a technique that says *find all the similar documents*, in this
    case, the nine closest documents to the one that we''re looking at, and count
    votes:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是一个技术，告诉你*找到所有相似的文档*，在这里就是找到与我们正在查看的文档最相似的九个文档，并进行投票计数：
- en: '![](img/00121.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.jpeg)'
- en: We will be using nine reviews for the purposes of this example, and if you have
    a majority, let's say of positive reviews, then we will say that this is a positive
    review too. If the majority says negative, then this is a negative too. We don't
    want a tie regarding the reviews, which is why we say that there's nine instead
    of eight.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中我们将使用九条评论，如果大多数是正面评论，那么我们也会认为这是一条正面评论。如果大多数是负面评论，那么我们也认为它是负面的。我们不希望评论出现平局，这就是为什么我们选择了九条评论而不是八条的原因。
- en: 'Now we will compare the outcome with a random forest:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将与随机森林的结果进行比较：
- en: '![](img/00122.jpeg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpeg)'
- en: 'Now we need to perform cross-validation with the 9 nearest neighbors; we get
    76% accuracy for detecting positive/negative reviews with `Doc2Vec`. For experimental
    purposes, if we use a random forest without really trying to choose an amount
    of trees, we just get an accuracy of 70%:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要对9个最近邻进行交叉验证；我们使用`Doc2Vec`检测正面/负面评论的准确率为76%。为了实验目的，如果我们使用随机森林而不特意选择树的数量，我们只能得到70%的准确率：
- en: '![](img/00123.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: 'In such cases, k-nearest neighbors is both simpler and more accurate. Ultimately,
    is it all worth it? Well, let''s comparing it to the bag of words model. Let''s
    make a little pipeline with `CountVectorizer`, TF-IDF, and random forest, and
    at the end, do cross-validation on the same data, which in this case is the reviews.
    Here, we get 74%, as seen in the following screenshot:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，k近邻算法既简单又准确。最终，这一切都值得吗？那么，让我们将其与词袋模型进行比较。我们来做一个小管道，使用`CountVectorizer`、TF-IDF和随机森林，最后在相同的数据上进行交叉验证，这里指的是评论数据。我们得到的结果是74%，如下图所示：
- en: '![](img/00124.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpeg)'
- en: The outcome that we found after executing the model build we found `Doc2Vec`
    was better. `Doc2Vec` can be a lot more accurate than bag of words if we add a
    lot of training examples that are of the same style as the testing set. Hence,
    in our case, the testing set was pretty much the Yelp, Amazon, and IMDb reviews,
    which are all one sentence or one line of text and are pretty short. However,
    the training set that we found came from different reviews from different places,
    and we got about 175,000 examples. Those were often like paragraphs or just written
    in different ways.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行模型构建后，我们发现`Doc2Vec`的效果更好。如果我们添加很多与测试集风格相同的训练示例，`Doc2Vec`比词袋模型要准确得多。因此，在我们的案例中，测试集几乎都是Yelp、Amazon和IMDb上的评论，都是一句话或一行文本，内容比较简短。然而，我们找到的训练集来自不同地方的不同评论，约有175,000个示例。这些评论通常是段落形式，或者是用不同的方式书写的。
- en: Ideally, we will train a `Doc2Vec` or `Word2Vec` model on examples that are
    similar to what we're going to predict on later, but it can be difficult to find
    enough examples, as it was here so we did our best. Even so, it still turned out
    better than bag of words.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们将在与将来预测内容相似的示例上训练一个`Doc2Vec`或`Word2Vec`模型，但找到足够的示例可能会很困难，正如这里所面临的情况一样，因此我们尽力而为。即便如此，结果仍然比词袋模型要好。
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced text processing and the bag of words technique.
    We then used this technique to build a spam detector for YouTube comments. Next,
    we learned about the sophisticated Word2Vec model and put it to task with a coding
    project that detects positive and negative product, restaurant, and movie reviews.
    That's the end of this chapter about text.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们介绍了文本处理和词袋模型技术。接着，我们使用该技术为YouTube评论构建了一个垃圾评论检测器。随后，我们了解了复杂的Word2Vec模型，并通过一个编码项目来应用它，该项目可以检测正面和负面的产品、餐馆和电影评论。这就是本章关于文本处理的全部内容。
- en: In the next chapter, we're going to look at deep learning, which is a popular
    technique that's used in neural networks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论深度学习，这是一种在神经网络中广泛应用的技术。
