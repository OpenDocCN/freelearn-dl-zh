- en: Developing the ESBAS Algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发ESBAS算法
- en: By now, you are capable of approaching RL problems in a systematic and concise
    way. You are able to design and develop RL algorithms specifically for the problem
    at hand and get the most from the environment. Moreover, in the previous two chapters,
    you learned about algorithms that go beyond RL, but that can be used to solve
    the same set of tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经能够以系统化和简洁的方式处理RL问题。你能够为手头的问题设计和开发RL算法，并从环境中获得最大收益。此外，在前两章中，你学习了超越RL的算法，但这些算法也可以用来解决相同的任务。
- en: At the beginning of this chapter, we'll present a dilemma that we have already
    encountered in many of the previous chapters; namely, the exploration-exploitation
    dilemma. We have already presented potential solutions for the dilemma throughout
    the book (such as the ![](img/9474f864-505d-4f4e-bab5-674b452546f7.png)-greedy
    strategy), but we want to give you a more comprehensive outlook on the problem,
    and a more concise view of the algorithms that solve it. Many of them, such as
    the **upper confidence bound** (**UCB**) algorithm, are more sophisticated and
    better than the simple heuristics that we have used so far, such as the ![](img/9474f864-505d-4f4e-bab5-674b452546f7.png)-greedy
    strategy. We'll illustrate these strategies on a classic problem, known as multi-armed
    bandit. Despite being a simple tabular game, we'll use it as a starting point
    to then illustrate how these strategies can also be employed on non-tabular and
    more complex tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们将提出一个在许多前几章中已经遇到过的困境：即探索与利用困境。我们已经在全书中提出了该困境的潜在解决方案（例如，![](img/9474f864-505d-4f4e-bab5-674b452546f7.png)-贪心策略），但我们希望为你提供更全面的视角，并简洁地介绍解决该问题的算法。许多算法，如**上置信界**（**UCB**）算法，比我们迄今为止使用的简单启发式方法（例如，![](img/9474f864-505d-4f4e-bab5-674b452546f7.png)-贪心策略）更加复杂和优秀。我们将通过一个经典问题——多臂强盗问题来说明这些策略。尽管它是一个简单的表格游戏，我们将以此为起点，展示这些策略如何应用于非表格型和更复杂的任务。
- en: This introduction to the exploration-exploitation dilemma offers a general overview
    of the main methods that many recent RL algorithms employ in order to solve very
    hard exploration environments. We'll also provide a broader view of the applicability
    of this dilemma when solving other kinds of problems. As proof of that, we'll
    develop a meta-algorithm called **epochal stochastic bandit algorithm selection**,
    or **ESBAS**, which tackles the problem of online algorithm selection in the context
    of RL. ESBAS does this by using the ideas and strategies that emerged from the
    multi-armed bandit problem to select the best RL algorithm that maximizes the
    expected return on each episode.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了探索与利用困境，概述了许多最近的强化学习（RL）算法所采用的主要方法，用以解决非常困难的探索环境。我们还将提供一个更广泛的视角，探讨该困境在解决其他类型问题时的适用性。作为证明，我们将开发一个名为**时代随机强盗算法选择**（**ESBAS**）的元算法，该算法解决了RL背景下的在线算法选择问题。ESBAS通过利用从多臂强盗问题中出现的思想和策略，选择出最能在每个回合中最大化预期回报的RL算法。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Exploration versus exploitation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索与利用
- en: Approaches to exploration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索方法
- en: Epochal stochastic bandit algorithm selection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时代随机强盗算法选择
- en: Exploration versus exploitation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: The exploration-exploitation trade-off dilemma, or exploration-exploitation
    problem, affects many important domains. Indeed, it's not only restricted to the
    RL context, but applies to everyday life. The idea behind this dilemma is to establish
    whether it is better to take the optimal solution that is known so far, or if
    it's worth trying something new. Let's say you are buying a new book. You could
    either choose a title from your favorite author, or buy a book of the same genre
    that Amazon is suggesting to you. In the first case, you are confident about what
    you're getting, but by selecting the second option, you don't know what to expect.
    However, in the latter case, you could be incredibly pleased, and end up reading
    a very good book that is indeed better than the one written by your favorite author.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索-利用困境**或**探索-利用问题**影响着许多重要领域。实际上，它不仅限于强化学习的背景，还适用于日常生活。这个困境的核心思想是判断是选择已经知道的最优解，还是值得尝试新的选项。假设你正在买一本新书。你可以选择你最喜欢的作者的书，或者选择亚马逊推荐的同类书籍。在第一种情况下，你对自己选择的书充满信心，但在第二种情况下，你无法预测将会得到什么。然而，后者可能让你感到意外惊喜，读到一本非常好的书，甚至超越了你最喜欢作者写的书。'
- en: This conflict between exploiting what you have already learned and taking advantage
    of it or exploring new options and taking some risks, is very common in reinforcement
    learning as well. The agent may have to sacrifice a short-term reward, and explore
    a new space, in order to achieve a higher long-term reward in the future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在利用已学知识和冒险探索新选项之间的冲突，在强化学习中也非常常见。智能体可能需要牺牲短期奖励，去探索一个新的空间，以便在未来获得更高的长期奖励。
- en: All this may not sound new to you. In fact, we started dealing with this problem
    when we developed the first RL algorithm. Up until now, we have primarily adopted
    simple heuristics, such as the ![](img/bf16d037-d581-41c2-8913-ed505eb3422e.png)-greedy
    strategy, or followed a stochastic policy to decide whether to explore or exploit.
    Empirically, these strategies work very well, but there are some other techniques
    that can achieve theoretical optimal performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切对你来说可能并不新鲜。事实上，当我们开发第一个强化学习算法时，就开始处理这个问题了。直到现在，我们主要采用了简单的启发式方法，如**ε-greedy**策略，或遵循随机策略来决定是探索还是利用。经验上，这些策略效果很好，但还有一些其他技术能够实现理论上的最优表现。
- en: In this chapter, we'll start with an explanation of the exploration-exploitation
    dilemma from the ground up, and introduce some exploration algorithms that achieve
    nearly-optimal performance on tabular problems. We'll also show how the same strategies
    can be adapted to non-tabular and more complex tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从基础讲解**探索-利用困境**，并介绍一些在表格问题上实现近乎最优表现的探索算法。我们还将展示如何将相同的策略适应于非表格和更复杂的任务。
- en: 'For an RL algorithm, one of the most challenging Atari games to solve is Montezuma''s
    Revenge, rendered in the following screenshot. The objective of the game is to
    score points by gathering jewels and killing enemies. The main character has to
    find all the keys in order to navigate the rooms in the labyrinth, and gather
    the tools that are needed to move around, while avoiding obstacles. The sparse
    reward, the long-term horizon, and the partial rewards, which are not correlated
    with the end goal, make the game very challenging for every RL algorithm. Indeed,
    these four characteristics make Montezuma''s Revenge one of the best environments
    for testing exploration algorithms:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个强化学习算法，最具挑战性的Atari游戏之一是《蒙特祖玛的复仇》，其截图如下。游戏的目标是通过收集宝石和击杀敌人来获得分数。主角需要找到所有的钥匙，以便在迷宫中的房间间穿行，并收集必要的工具来移动，同时避免障碍物。稀疏奖励、长期目标和与最终目标无关的部分奖励，使得这款游戏对每个强化学习算法都非常具有挑战性。事实上，这四个特点使得《蒙特祖玛的复仇》成为测试**探索算法**的最佳环境之一：
- en: '![](img/e18a14e6-d25a-4dea-9db9-b5c8f31a1116.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e18a14e6-d25a-4dea-9db9-b5c8f31a1116.png)'
- en: Screenshot of Montezuma's Revenge
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 《蒙特祖玛的复仇》截图
- en: Let's start from the ground up, in order to give a complete overview of this
    area.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始，以便全面了解这一领域。
- en: Multi-armed bandit
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机
- en: The multi-armed bandit problem is the classic RL problem that is used to illustrate
    the exploration-exploitation trade-off dilemma. In the dilemma, an agent has to
    choose from a fixed set of resources, in order to maximize the expected reward.
    The name multi-armed bandit comes from a gambler that is playing multiple slot
    machines, each with a stochastic reward from a different probability distribution.
    The gambler has to learn the best strategy in order to achieve the highest long-term
    reward.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题是经典的强化学习问题，用来说明探索-利用的权衡困境。在这个困境中，代理必须从一组固定的资源中选择，以最大化预期奖励。多臂老虎机这个名称来源于赌徒玩多个老虎机，每个老虎机都有来自不同概率分布的随机奖励。赌徒必须学习最佳策略，以获得最高的长期奖励。
- en: 'This situation is illustrated in the following diagram. In this particular
    example, the gambler (the ghost) has to choose one of the five slot machines,
    all with different and unknown reward probabilities, in order to win the highest
    amount of money:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在以下图示中进行了说明。在这个特定的例子中，赌徒（幽灵）需要从五台不同的老虎机中选择一台，每台老虎机都有不同且未知的奖励概率，以便赢得最高金额：
- en: '![](img/83901340-859b-4b80-987d-3940058a9ae5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83901340-859b-4b80-987d-3940058a9ae5.png)'
- en: Example of a five-armed bandit problem
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 五臂老虎机问题示例
- en: If you are questioning how the multi-armed bandit problem relates to more interesting
    tasks such as Montezuma's Revenge, the answer is that they are all about deciding
    whether, in the long run, the highest reward is yielded when new behaviors are
    attempted (pulling a new arm), or when continuing to do the best thing done so
    far (pulling the best-known arm). However, the main difference between the multi-armed
    bandit and Montezuma's Revenge is that, in the latter, the state of the agent
    changes every time. In the multi-armed bandit problem, there's only one state,
    and there's no sequential structure, meaning that past actions will not influence
    the future.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想多臂老虎机问题与更有趣的任务（比如《蒙特祖玛的复仇》）有何关系，答案是它们都涉及到决定，在长期来看，当尝试新的行为（拉动新的杠杆）时，是否会获得最高奖励，还是继续做迄今为止做得最好的事情（拉动最知名的杠杆）。然而，多臂老虎机与《蒙特祖玛的复仇》之间的主要区别在于，后者每次都会改变代理的状态，而在多臂老虎机问题中，只有一个状态，并且没有顺序结构，这意味着过去的行为不会影响未来。
- en: So, how can we find the right balance between exploration and exploitation in
    the multi-armed bandit problem?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在多臂老虎机问题中找到探索与利用的正确平衡呢？
- en: Approaches to exploration
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索的方法
- en: Put simply, the multi-armed bandit problem, and in general every exploration
    problem, can be solved either through random strategies, or through smarter techniques.
    The most notorious algorithm that belongs to the first category, is called ![](img/d0a4ebbd-c39f-438a-957b-fa97d7c0799e.png)-greedy;
    whereas optimistic exploration, such as UCB, and posterior exploration, such as
    Thompson sampling, belong to the second category. In this section, we'll take
    a look particularly at the ![](img/d0a4ebbd-c39f-438a-957b-fa97d7c0799e.png)-greedy
    and UCB strategies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，多臂老虎机问题，以及一般的每个探索问题，可以通过随机策略或者更智能的技术来解决。属于第一类的最臭名昭著的算法叫做![](img/d0a4ebbd-c39f-438a-957b-fa97d7c0799e.png)-贪婪；而乐观探索，如UCB，以及后验探索，如汤普森采样，属于第二类。在本节中，我们将特别关注![](img/d0a4ebbd-c39f-438a-957b-fa97d7c0799e.png)-贪婪和UCB策略。
- en: 'It''s all about balancing the risk and the reward. But, how can we measure
    the quality of an exploration algorithm? Through *regret*. Regret is defined as
    the opportunity lost in one step that is, the regret, ![](img/05ad64e7-c41b-4736-96b5-3e17d5266074.png), at
    time, ![](img/cdaec17b-6d89-475d-92b1-afe09f9a3159.png), is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全是关于平衡风险与奖励。但我们如何衡量一个探索算法的质量呢？通过*后悔*。后悔被定义为在一步操作中失去的机会，即在时间![](img/cdaec17b-6d89-475d-92b1-afe09f9a3159.png)时的后悔，![](img/05ad64e7-c41b-4736-96b5-3e17d5266074.png)
    如下所示：
- en: '![](img/0c3a8dae-8113-4f37-a2bd-50eb09ed9d16.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c3a8dae-8113-4f37-a2bd-50eb09ed9d16.png)'
- en: Here, ![](img/c95e2c63-0f9c-436d-891c-0c33d1cbe73d.png) denotes the optimal
    value, and ![](img/9df02363-3999-4be9-a9e4-32dac4ba1ba6.png) the action-value
    of ![](img/eda7adef-c75c-41fe-bf05-e3a71d475361.png).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/c95e2c63-0f9c-436d-891c-0c33d1cbe73d.png) 表示最优值，![](img/9df02363-3999-4be9-a9e4-32dac4ba1ba6.png)
    表示 ![](img/eda7adef-c75c-41fe-bf05-e3a71d475361.png) 的行动值。
- en: 'Thus, the goal is to find a trade-off between exploration and exploitation,
    by minimizing the total regret over all the actions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标是通过最小化所有行动的总后悔，找到探索与利用之间的折衷：
- en: '![](img/c7159b28-8e5e-4a4b-a0c8-67f57c2d6bd6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7159b28-8e5e-4a4b-a0c8-67f57c2d6bd6.png)'
- en: Note that the minimization of the total regret is equivalent to the maximization
    of the cumulative reward. We'll use this idea of regret to show how exploration
    algorithms perform.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，总遗憾的最小化等同于累积奖励的最大化。我们将利用这一遗憾的概念来展示探索算法的表现。
- en: The ∈-greedy strategy
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ∈-贪心策略
- en: We have already expanded the ideas behind the ![](img/3fd4eed5-f797-441a-a7c3-820e65fd322d.png)-greedy
    strategy and implemented it to help our exploration in algorithms such as Q-learning
    and DQN. It is a very simple approach, and yet it achieves very high performance
    in non-trivial jobs as well. This is the main reason behind its widespread use
    in many deep learning algorithms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经扩展了![](img/3fd4eed5-f797-441a-a7c3-820e65fd322d.png)-贪心策略背后的思想，并将其应用于帮助我们在Q学习和DQN等算法中的探索。这是一种非常简单的方法，但它在非平凡的任务中也能达到非常高的性能。这是它在许多深度学习算法中广泛应用的主要原因。
- en: To refresh your memory, ![](img/5a4a8e7f-73b0-4c20-95dc-af5b4c0f9421.png)-greedy
    takes the best action most of the time, but from time to time, it selects a random
    action. The probability of choosing a random action is dictated by the ![](img/c59c569b-ad2e-41a2-9748-8105e758d504.png)
    value, which ranges from 0 to 1\. That is, with ![](img/472e4698-ff84-43e0-a3b7-4e017b959cb3.png)
    probability, the algorithm will exploit the best action, and with ![](img/389c51f4-4d79-4edd-8b85-b668df73d57a.png)
    probability, it will explore the alternatives with a random selection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你回忆，![](img/5a4a8e7f-73b0-4c20-95dc-af5b4c0f9421.png)-贪心大多数时候采取最佳行动，但偶尔会选择一个随机行动。选择随机行动的概率由![](img/c59c569b-ad2e-41a2-9748-8105e758d504.png)值决定，它的范围从0到1。也就是说，具有![](img/472e4698-ff84-43e0-a3b7-4e017b959cb3.png)的概率，算法会利用最佳行动，而具有![](img/389c51f4-4d79-4edd-8b85-b668df73d57a.png)的概率，则会通过随机选择来探索其他可能性。
- en: 'In the multi-armed bandit problem, the action values are estimated based on
    past experiences, by averaging the reward obtained by taking those actions:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在多臂强盗问题中，行动值是根据过去的经验估计的，方法是对采取这些行动所获得的奖励进行平均：
- en: '![](img/57550883-a938-4c7e-95b9-8c405d2ea421.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57550883-a938-4c7e-95b9-8c405d2ea421.png)'
- en: In the preceding equation, ![](img/ebaad13b-1a8f-4e8f-ab4d-ecb67a4671c8.png) is
    the number of times that the ![](img/604173e8-d30e-4512-95ed-ae8975292239.png)
    action has been picked, and ![](img/f359ceae-5338-46da-b55b-676926e84c5a.png) is
    a Boolean that indicates whether at time ![](img/8f99004d-90c0-4b56-9d58-9a9dfa0e0b67.png), action ![](img/e5f44a63-e1ec-4d04-86bd-7c0db688d83b.png) has
    been chosen. The bandit will then act according to the ![](img/ffa9e069-89e1-4464-af51-35562be61862.png)-greedy
    algorithm, and explore by choosing a random action, or exploit by picking the
    ![](img/d13df1cf-782c-41fb-aaf3-b89060846cd0.png) action with the higher ![](img/b2bdeff5-087e-4a20-ae71-bbda48c6c1e5.png) value.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，![](img/ebaad13b-1a8f-4e8f-ab4d-ecb67a4671c8.png)表示![](img/604173e8-d30e-4512-95ed-ae8975292239.png)行动被选中的次数，而![](img/f359ceae-5338-46da-b55b-676926e84c5a.png)是一个布尔值，表示在时刻![](img/8f99004d-90c0-4b56-9d58-9a9dfa0e0b67.png)时，是否选择了行动![](img/e5f44a63-e1ec-4d04-86bd-7c0db688d83b.png)。然后，强盗会根据![](img/ffa9e069-89e1-4464-af51-35562be61862.png)-贪心算法采取行动，或通过选择随机行动来探索，或通过选择![](img/d13df1cf-782c-41fb-aaf3-b89060846cd0.png)行动来利用较高的![](img/b2bdeff5-087e-4a20-ae71-bbda48c6c1e5.png)值。
- en: A drawback of ![](img/1d841b84-44bc-4b44-9395-7ef1e1147a78.png)-greedy, is that
    it has an expected linear regret. But, for the law of large numbers, the optimal
    expected total regret should be logarithmic to the number of timesteps. This means
    that the ![](img/1d841b84-44bc-4b44-9395-7ef1e1147a78.png)-greedy strategy isn't
    optimal.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/1d841b84-44bc-4b44-9395-7ef1e1147a78.png)-贪心的一个缺点是它具有期望的线性遗憾。然而，根据大数法则，最优的期望总遗憾应该是与时间步数呈对数关系的。这意味着，![](img/1d841b84-44bc-4b44-9395-7ef1e1147a78.png)-贪心策略并不是最优的。'
- en: A simple way to reach optimality involves the use of an ![](img/a4e357f5-9d4d-4712-ad9b-914fc01d6f05.png) value that
    decays as time goes by. By doing this, the overall weight of the exploration will
    vanish, until only greedy actions will be chosen. Indeed, in deep RL algorithms ![](img/5adc8777-a769-4ad0-81e6-94bc007ba882.png)-greedy
    is almost always combined with a linear, or exponential decay of ![](img/ef61d4c1-93b8-4dd2-9de5-5691ffe5ed98.png).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 达到最优的一种简单方法是使用一个![](img/a4e357f5-9d4d-4712-ad9b-914fc01d6f05.png)值，该值随着时间的推移逐渐衰减。通过这样做，探索的总体权重将逐渐消失，最终只会选择贪心行动。实际上，在深度强化学习算法中，![](img/5adc8777-a769-4ad0-81e6-94bc007ba882.png)-贪心几乎总是与![](img/ef61d4c1-93b8-4dd2-9de5-5691ffe5ed98.png)的线性或指数衰减相结合。
- en: That being said, ![](img/83a790d4-315e-44cc-b1ce-ee6317e811ff.png) and its decay
    rate is difficult to choose, and there are other strategies that solve the multi-armed
    bandit problem optimally.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，![](img/83a790d4-315e-44cc-b1ce-ee6317e811ff.png) 及其衰减率很难选择，并且还有其他策略可以最优地解决多臂赌博机问题。
- en: The UCB algorithm
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UCB 算法
- en: The UCB algorithm is related to a principle known as optimism in the face of
    uncertainty, a statistics-based principle based on the law of large numbers. UCB
    constructs an optimistic guess, based on the sample mean of the rewards, and on
    the estimation of the upper confidence bound of the reward. The optimistic guess
    determines the expected pay-off of each action, also taking into consideration
    the uncertainty of the actions. Thus, UCB is always able to pick the action with
    the higher potential reward, by balancing the risk and the reward. Then, the algorithm
    switches to another one when the optimistic estimate of the current action is
    lower than the others.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: UCB 算法与一种称为“面对不确定性时的乐观原则”（optimism in the face of uncertainty）有关，这是一种基于大数法则的统计学原理。UCB
    构建了一个乐观的猜测，基于奖励的样本均值，并根据奖励的上置信界估算。乐观的猜测决定了每个动作的期望回报，同时考虑了动作的不确定性。因此，UCB 始终能够通过平衡风险与回报，选择潜在回报更高的动作。然后，当当前动作的乐观估算低于其他动作时，算法会切换到另一个动作。
- en: 'Specifically, UCB keeps track of the average reward of each action with ![](img/3eeee2ef-bd6a-43a8-b596-2bd8f67e44f9.png),
    and the ![](img/f5fed32f-f7ac-4f64-b5db-5609545a3077.png) UCB (hence the name) for
    each action. Then, the algorithm picks the arm which maximizes the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，UCB 通过 ![](img/3eeee2ef-bd6a-43a8-b596-2bd8f67e44f9.png) 跟踪每个动作的平均奖励，以及每个动作的
    ![](img/f5fed32f-f7ac-4f64-b5db-5609545a3077.png) UCB（因此得名）。然后，算法选择最大化以下内容的动作：
- en: '![](img/a0da43c2-5a19-4a3f-af6d-2ea26cbce366.png) (12.1)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0da43c2-5a19-4a3f-af6d-2ea26cbce366.png) (12.1)'
- en: In this formula, the role of ![](img/523327c1-114e-4bfe-9b93-e5ddb595f6dd.png)
    is to provide an additional argument to the average reward that accounts for the
    uncertainty of the action.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/523327c1-114e-4bfe-9b93-e5ddb595f6dd.png) 的作用是提供一个附加的参数，作为考虑动作不确定性的平均奖励。
- en: UCB1
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UCB1
- en: UCB1 belongs to the UCB family, and its contribution is in the selection of ![](img/e065e9bf-7e8a-4c32-8e4c-07fe6eed0d99.png).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: UCB1 属于 UCB 系列，其贡献在于选择 ![](img/e065e9bf-7e8a-4c32-8e4c-07fe6eed0d99.png)。
- en: 'In UCB1, the ![](img/a2a16bcb-d54c-4a0b-9837-19a8abaa3077.png) UCB is computed
    by keeping track of the number of times an action, (![](img/a992c126-793e-4465-a30a-bdc48682554d.png)),
    has been selected, along with ![](img/fc8c5d8c-c204-4688-9f0e-f780d7494478.png),
    and the total number of actions that are selected with ![](img/c796f72d-4a04-4337-9a37-acc696f7db3d.png),
    as represented in the following formula:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 UCB1 中，UCB 通过跟踪某个动作（![](img/a992c126-793e-4465-a30a-bdc48682554d.png)）被选择的次数，以及
    ![](img/fc8c5d8c-c204-4688-9f0e-f780d7494478.png) 和 ![](img/c796f72d-4a04-4337-9a37-acc696f7db3d.png)
    这两个量的总和来计算，如下式所示：
- en: '![](img/b920c393-fb72-4f8c-b8e1-1b600b2eae41.png) (12.2)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b920c393-fb72-4f8c-b8e1-1b600b2eae41.png) (12.2)'
- en: The uncertainty of an action, is thus related to the number of times it has
    been selected. If you think about it, this makes sense as, according to the law
    of large numbers, with an infinite number of trials, you'd be sure about the expected
    value. On the contrary, if you tried an action only a few times, you'd be uncertain
    about the expected reward, and only with more experience, would you be able to
    say whether it is a good or a bad action. Therefore, we'll incentivize the exploration
    of actions that have been chosen only few times, and that therefore have a high
    uncertainty. The main takeaway is that if ![](img/3a2286d0-a84a-43ef-8f10-e66246cbbf08.png)
    is small, meaning that the action has been experienced only occasionally, then ![](img/cea73fe8-9af9-4380-846a-0657e093e516.png) will
    be large, with an overall high uncertain estimate. However, if ![](img/1da62787-139b-4dfa-80f9-8e79a925418e.png)
    is large, then ![](img/cea73fe8-9af9-4380-846a-0657e093e516.png) will be small,
    and the estimate will be accurate. We'll then follow ![](img/e808426c-badf-410c-9246-9bbc961c22ca.png) only
    if it has a high mean reward.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个动作的不确定性与它被选择的次数有关。如果你仔细想想，这也能解释清楚，因为根据大数法则，在无限次数的试验下，你可以对期望值有一个明确的了解。相反，如果你只尝试过某个动作几次，你会对期望的奖励感到不确定，只有通过更多的经验，你才能判断它是好是坏。因此，我们会激励探索那些只被选择过少数几次的动作，因为它们的不确定性较高。重点是，如果
    ![](img/3a2286d0-a84a-43ef-8f10-e66246cbbf08.png) 很小，意味着这个动作只是偶尔被尝试，那么 ![](img/cea73fe8-9af9-4380-846a-0657e093e516.png)
    会很大，从而带来较高的不确定估计。然而，如果 ![](img/1da62787-139b-4dfa-80f9-8e79a925418e.png) 很大，那么
    ![](img/cea73fe8-9af9-4380-846a-0657e093e516.png) 就会很小，估计值也会更准确。然后，我们只会在 ![](img/e808426c-badf-410c-9246-9bbc961c22ca.png)
    具有较高的平均奖励时才选择它。
- en: 'The main advantage of UCB compared to ![](img/db396bc1-261d-4231-9b85-6d4fa469c491.png)-greedy,
    is actually due to the counting of the actions. Indeed, the multi-armed bandit
    problem can be easily solved with this method, by keeping a counter for each action
    that is taken, and its average reward. These two pieces of information can be
    integrated into formula (12.1) and formula (12.2), in order to get the best action
    to take at time (![](img/367c449a-fa0e-4781-9557-c694ed252114.png)); that is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ![](img/db396bc1-261d-4231-9b85-6d4fa469c491.png)-贪婪算法相比，UCB的主要优势实际上在于对动作进行计数。事实上，多臂老虎机问题可以通过此方法轻松解决，方法是为每个被采取的动作保持一个计数器，以及它的平均奖励。这两个信息可以集成到公式（12.1）和公式（12.2）中，从而获得在时刻(![](img/367c449a-fa0e-4781-9557-c694ed252114.png))应该采取的最佳动作；即：
- en: '![](img/98fd9d22-427c-4e19-a048-41faf9767872.png) (12.3)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98fd9d22-427c-4e19-a048-41faf9767872.png) (12.3)'
- en: UCB is a very powerful method for exploration, and it achieves a logarithmic
    expected total regret on the multi-armed bandit problem, therefore reaching an
    optimal trend. It is worth noting that ![](img/e76d68d1-7fee-4a84-8f04-c37407e6900b.png)-greedy
    exploration could also obtain a logarithmic regret, but it would require careful
    design, together with a finely-tuned exponential decay, and thus it would be harder
    to balance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: UCB是一个非常强大的探索方法，它在多臂老虎机问题上实现了对数期望总遗憾，从而达到了最优趋势。值得注意的是，![](img/e76d68d1-7fee-4a84-8f04-c37407e6900b.png)-贪婪探索也可以获得对数遗憾，但它需要精心设计，并且需要细调指数衰减，因此在平衡上会更加困难。
- en: There are additional variations of UCB, such as UCB2, UCB-Tuned, and KL-UCB.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: UCB还有一些变体，如UCB2、UCB-Tuned和KL-UCB。
- en: Exploration complexity
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索复杂性
- en: We saw how UCB, and in particular UCB1, can reduce the overall regret and accomplish
    an optimal convergence on the multi-armed bandit problem with a relatively easy
    algorithm. However, this is a simple stateless task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，UCB，特别是UCB1，能够通过相对简单的算法降低总体的遗憾，并在多臂老虎机问题上实现最优收敛。然而，这仍然是一个简单的无状态任务。
- en: 'So, how will UCB perform on more complex tasks? To answer this question, we
    can oversimplify the division and group all of the problems in these three main
    categories:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，UCB在更复杂的任务上表现如何呢？为了回答这个问题，我们可以将问题过于简化，并将所有问题分为以下三大类：
- en: '**Stateless problems**: An instance of these problems is the multi-armed bandit.
    The exploration in such cases can be handled with a more sophisticated algorithm,
    such as UCB1.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无状态问题**：这些问题的一个实例就是多臂老虎机问题。在这种情况下，探索可以通过更复杂的算法来处理，如UCB1。'
- en: '**Small-to-medium tabular problems**: As a basic rule, exploration can still
    be approached with more advanced mechanisms, but in some cases, the overall benefit
    is small, and is not worth the additional complexity.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中小型表格问题**：作为基本规则，探索仍然可以通过更先进的机制来处理，但在某些情况下，总体收益较小，不值得增加额外的复杂性。'
- en: '**Large non-tabular problems**: We are now in more complex environments. In
    these settings, the outlook isn''t yet well defined, and researchers are still actively
    working to find the best exploration strategy. The reason for this is that as
    the complexity increases, optimal methods such as UCB are intractable. For example,
    UCB cannot deal with problems with continuous states. However, we don''t have
    to throw everything away, and we can use the exploration algorithms that were
    studied in the multi-armed bandit context as inspiration. That said, there are
    many approaches that approximate optimal exploration methods, and that work well
    in continuous environments, as well. For example, counting-based approaches, such
    as UCB, have been adapted with infinite state problems, by providing similar counts
    for similar states. An algorithm of these has also been capable of achieving significant
    improvement in very difficult environments, such as Montezuma''s Revenge. Still,
    in the majority of RL contexts, the additional complexity that these more complex
    approaches involve is not worth it, and simpler random strategies such as ![](img/39b9d07d-70b6-4ce6-87b6-e91698651dfd.png)-greedy
    work just fine.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大型非表格化问题**：我们现在处于更加复杂的环境中。在这些环境中，前景尚不明确，研究人员仍在积极努力寻找最佳的探索策略。其原因在于，随着复杂性的增加，像UCB这样的最优方法变得难以处理。例如，UCB无法处理具有连续状态的问题。然而，我们不必抛弃一切，可以借鉴在多臂老虎机问题中研究的探索算法。也就是说，有许多方法能够逼近最优的探索策略，并且在连续环境中也能很好地发挥作用。例如，基于计数的方法，如UCB，已经通过为相似状态提供相似的计数来适应了无限状态问题。这类算法在一些极其复杂的环境中也能显著提升性能，例如在《蒙特祖马的复仇》游戏中。尽管如此，在大多数强化学习应用场景中，这些更复杂方法所带来的额外复杂性是得不偿失的，简单的随机策略，如![](img/39b9d07d-70b6-4ce6-87b6-e91698651dfd.png)-贪婪策略，通常就足够了。'
- en: It's also worth noting that, despite the fact that we outlined only a count-based
    approach to exploration such as UCB1, there are two other sophisticated ways in
    which to deal with exploration, which achieve optimal value in regret. The first
    is called posterior sampling (an example of this is Thompson sampling), and is
    based on a posterior distribution, and the second is called information gain,
    and relies upon an internal measurement of the uncertainty through the estimation
    of entropy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管我们仅概述了基于计数的探索方法，如UCB1，但实际上还有两种更复杂的方式可以处理探索问题，它们能在遗憾值上实现最优结果。第一种方法叫做后验采样（其中一个例子是汤普森采样），它基于后验分布；第二种方法叫做信息增益，它依赖于通过估计熵来测量不确定性。
- en: Epochal stochastic bandit algorithm selection
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时代随机带算法选择
- en: The main use of exploration strategies in reinforcement learning is to help
    the agent in the exploration of the environment. We saw this use case in DQN with
    ![](img/1e16a747-c424-435e-b1e2-db30ddda9c2d.png)-greedy, and in other algorithms
    with the injection of additional noise into the policy. However, there are other
    ways of using exploration strategies. So, to better grasp the exploration concepts
    that have been presented so far, and to introduce an alternative use case of these
    algorithms, we will present and develop an algorithm called ESBAS. This algorithm
    was introduced in the paper, *Reinforcement Learning Algorithm Selection*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 探索策略在强化学习中的主要用途是帮助智能体进行环境探索。我们在DQN中看到了这种用例，使用了![](img/1e16a747-c424-435e-b1e2-db30ddda9c2d.png)-贪婪策略，而在其他算法中则通过向策略中注入额外的噪声来实现。然而，探索策略的使用方式不仅限于此。为了更好地理解到目前为止介绍的探索概念，并引入这些算法的另一种应用场景，我们将展示并开发一种名为ESBAS的算法。该算法首次出现在论文《强化学习算法选择》中。
- en: ESBAS is a meta-algorithm for online **algorithm selection** (**AS**) in the
    context of reinforcement learning. It uses exploration methods in order to choose
    the best algorithm to employ during a trajectory, so as to maximize the expected
    reward.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ESBAS是一种在线**算法选择**（**AS**）的元算法，适用于强化学习的上下文。它利用探索方法来选择在整个轨迹中使用的最佳算法，以最大化预期奖励。
- en: In order to better explain ESBAS, we'll first explain what algorithm selection
    is and how it can be used in machine learning and reinforcement learning. Then,
    we'll focus on ESBAS, and give a detailed description of its inner workings, while
    also providing its pseudocode. Finally, we'll implement ESBAS and test it on an
    environment called Acrobot.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地解释ESBAS，我们将首先解释什么是算法选择，以及它在机器学习和强化学习中的应用。接着，我们将重点介绍ESBAS，详细描述其内部工作原理，并提供其伪代码。最后，我们将实现ESBAS并在一个名为Acrobot的环境中进行测试。
- en: Unboxing algorithm selection
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法选择的开箱操作
- en: To better understand what ESBAS does, let's first focus on what algorithm selection
    (AS) is. In normal settings, a specific and fixed algorithm is developed and trained
    for a given task. The problem is that if the dataset changes over time, the dataset
    overfits, or another algorithm works better in some restricted contexts, there's
    no way of changing it. The chosen algorithm will remain the same forever. The
    task of algorithm selection overcomes this problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解ESBAS的作用，我们首先来关注算法选择（AS）是什么。在普通设置中，针对给定任务开发并训练一个特定且固定的算法。问题在于，如果数据集随着时间变化，数据集发生过拟合，或者在某些限制性情境下另一个算法表现更好，就无法改变算法。选择的算法将永远保持不变。算法选择的任务就是解决这个问题。
- en: AS is an open problem in machine learning. It is about designing an algorithm
    called a meta-algorithm that always chooses the best algorithm from a pool of
    different options, called a portfolio, which is based on current needs. A representation
    of this is shown in the following diagram. AS is based on the assumption that
    different algorithms in the portfolio will outperform the others in different
    parts of the problem space. Thus, it is important to have algorithms with complementary
    capabilities.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: AS（算法选择）是机器学习中的一个开放问题。它涉及设计一个叫做元算法的算法，该算法始终从一个不同选项的池中（称为投资组合）选择最适合当前需求的算法。以下图示展示了这一过程。AS的假设是，投资组合中的不同算法在问题空间的不同部分会优于其他算法。因此，拥有具有互补能力的算法非常重要。
- en: 'For example, in the following diagram, the meta-algorithm chooses which algorithm
    (or agent) among those available in the portfolio (such as PPO and TD3) will act
    on the environment at a given moment. These algorithms are not complementary to
    each other, but each one provides different strengths that the meta-algorithm
    can choose in order to better perform in a specific situation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下面的图示中，元算法从可用的算法（或代理）中选择哪一个（如PPO和TD3）将在给定时刻作用于环境。这些算法不是互补的，而是每个算法提供不同的优势，元算法可以选择其中一个，以便在特定情境中表现得更好：
- en: '![](img/1b763684-8acb-465e-95bf-828348ec507a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b763684-8acb-465e-95bf-828348ec507a.png)'
- en: Representation of an algorithm selection method for RL
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中算法选择方法的表示
- en: For example, if the task involves designing a self-driving car that drives on
    all kinds of terrains, then it may be useful to train one algorithm that is capable
    of amazing performance on the road, in the desert, and on ice. Then, AS could
    intelligently choose which one of these three versions to employ in each situation.
    For instance, AS may find that on rainy days, the policy that has been trained
    on ice works better than the others.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果任务是设计一个能够在各种地形上行驶的自动驾驶汽车，那么训练一个能够在道路、沙漠和冰面上表现出色的算法可能会很有用。然后，AS可以智能地选择在每种情境下使用这三种版本中的哪一个。例如，AS可能会发现，在雨天，训练过的冰面策略比其他策略更有效。
- en: In RL, the policy changes with a very high frequency, and the dataset increases
    continuously over time. This means that there can be big differences in the optimal
    neural network size and the learning rate between the starting point, when the
    agent is in an embryonic state, compared to the agent in an advanced state. For
    example, an agent may start learning with a high learning rate, and decrease it
    as more experience is accumulated. This highlights how RL is a very interesting
    playground for algorithm selection. For this reason, that's exactly where we'll
    test our AS.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，策略变化的频率非常高，且数据集随时间不断增加。这意味着在起始点（代理处于初步状态）与代理处于高级状态时，最佳神经网络大小和学习率可能存在很大差异。例如，代理可能在学习初期使用较高的学习率，并随着经验的积累逐渐降低学习率。这突出显示了强化学习是一个非常有趣的算法选择平台。因此，这正是我们将在其中测试AS的地方。
- en: Under the hood of ESBAS
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ESBAS的底层原理
- en: The paper that proposes ESBAS, tests the algorithm on batch and online settings.
    However, in the remainder of the chapter, we'll focus primarily on the former.
    The two algorithms are very similar, and if you are interested in the pure online
    version, you can find a further explanation of it in the paper. The AS in true
    online settings is renamed as **sliding stochastic bandit AS** (**SSBAS**), as
    it learns from a sliding window of the most recent selections. But let's start
    from the foundations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提出ESBAS的论文，在批处理和在线设置中对算法进行了测试。然而，在本章的剩余部分，我们将主要集中讨论前者。这两个算法非常相似，如果你对纯在线版本感兴趣，可以在论文中找到进一步的解释。在真正的在线设置中，AS被重新命名为**滑动随机带算法**（**SSBAS**），它从最新的选择的滑动窗口中学习。但我们先从基础开始。
- en: 'The first thing to say about ESBAS, is that it is based on the UCB1 strategy,
    and that it uses this bandit-style selection for choosing an off-policy algorithm
    from the fixed portfolio. In particular, ESBAS can be broken down into three main
    parts that work as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ESBAS，首先要说的是它基于UCB1策略，并使用这种带算法选择离策略算法，从固定的投资组合中进行选择。具体来说，ESBAS可以分为三个主要部分，工作原理如下：
- en: It cycles across many epochs of exponential size. Inside each epoch, the first
    thing that it does is update all of the off-policy algorithms that are available
    in the portfolio. It does this using the data that has been collected until that
    point in time (at the first epoch the dataset will be empty). The other thing
    that it does, is reset the meta-algorithm.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它循环遍历多个指数大小的epoch。在每个epoch中，它首先做的是更新投资组合中所有可用的离策略算法。它使用的是迄今为止收集的数据（在第一个epoch时，数据集将为空）。另外，它还会重置元算法。
- en: Then, during the epoch, the meta-algorithm computes the optimistic guess, following
    the formula (12.3), in order to choose the off-policy algorithm (among those in
    the portfolio) that will control the next trajectory, so as to minimize the total
    regret. The trajectory is then run with that algorithm. Meanwhile, all the transitions
    of the trajectory are collected and added to the dataset that will be later used
    by the off-policy algorithms to train the policies.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在每个epoch中，元算法根据公式（12.3）计算乐观猜测，以选择将控制下一个轨迹的离策略算法（在投资组合中），从而最小化总的遗憾。然后，使用该算法运行轨迹。同时，轨迹中的所有转换都会被收集，并添加到数据集，这些数据稍后将由离策略算法用来训练策略。
- en: When a trajectory has come to an end, the meta-algorithm updates the mean reward
    of that particular off-policy algorithm with the RL return that is obtained from
    the environment, and increases the number of occurrences. The average reward,
    and the number of occurrences, will be used by UCB1 to compute the UCB, as from
    formula (12.2). These values are used to choose the next off-policy algorithm
    that will roll out the next trajectory.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当轨迹结束时，元算法使用从环境中获得的RL回报更新该特定离策略算法的平均奖励，并增加出现次数。平均奖励和出现次数将由UCB1用来计算UCB，如公式（12.2）所示。这些值用于选择下一个离策略算法，该算法将执行下一个轨迹。
- en: 'To give you a better view of the algorithm, we also provided the pseudocode
    of ESBAS in the code block, here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你更好地理解算法，我们还在代码块中提供了ESBAS的伪代码，见下方：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, ![](img/d94da654-c2e9-4cb4-9b62-c1250b1b0df5.png) is a hyperparameter, ![](img/a13c0281-321e-4f8a-9fa1-8100853436ce.png) is
    the RL return obtained during the ![](img/832ecce0-4106-40e3-878f-114347cd69a6.png)
    trajectory, ![](img/1c4593cf-9daf-400b-b76b-c83821a22498.png) is the counter of
    algorithm ![](img/9859b9d2-396a-4d17-b3ea-2d5742f00a81.png), and ![](img/eeb52e17-7842-4111-a50f-6224ed1b0cc0.png) is
    its mean return.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/d94da654-c2e9-4cb4-9b62-c1250b1b0df5.png) 是一个超参数，![](img/a13c0281-321e-4f8a-9fa1-8100853436ce.png)
    是在 ![](img/832ecce0-4106-40e3-878f-114347cd69a6.png) 轨迹中获得的RL回报，![](img/1c4593cf-9daf-400b-b76b-c83821a22498.png)
    是算法 ![](img/9859b9d2-396a-4d17-b3ea-2d5742f00a81.png) 的计数器，![](img/eeb52e17-7842-4111-a50f-6224ed1b0cc0.png)
    是其平均回报。
- en: 'As explained in the paper, online AS addresses four practical problems that
    are inherited from RL algorithms:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如论文中所解释的，在线AS解决了四个来自RL算法的实际问题：
- en: '**Sample efficiency**: The diversification of the policies provides an additional
    source of information that makes ESBAS sample efficient. Moreover, it combines
    properties from curriculum learning and ensemble learning.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**样本效率**：策略的多样化提供了额外的信息源，使ESBAS具有样本效率。此外，它结合了课程学习和集成学习的特性。'
- en: '**Robustness**: The diversification of the portfolio provides robustness against
    bad algorithms.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：投资组合的多样化提供了对不良算法的鲁棒性。'
- en: '**Convergence**: ESBAS guarantees the minimization of the regret.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收敛性**：ESBAS保证最小化后悔值。'
- en: '**Curriculum learning**: AS is able to provide a sort of curriculum strategy,
    for example, by choosing easier, shallow models at the beginning, and deep models
    toward the end.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**课程学习**：AS能够提供一种课程策略，例如一开始选择较简单、浅层的模型，最后选择较深的模型。'
- en: Implementation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: The implementation of ESBAS is easy, as it involves the addition of only a few
    components. The most substantial part is in the definition and the optimization
    of the off-policy algorithms of the portfolio. Regarding these, ESBAS does not
    bind the choice of the algorithms. In the paper, both Q-learning and DQN are used.
    We have decided to use DQN, so as to provide an algorithm that is capable of dealing
    with more complex tasks that can be used with environments with the RGB state
    space. We went through DQN in great detail in [Chapter 5](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml),
    *Deep Q-Network*, and for ESBAS, we'll use the same implementation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实现ESBAS很简单，因为它只需要添加几个组件。最关键的部分在于对投资组合的离策略算法的定义和优化。对于这些，ESBAS并不限制算法的选择。本文中使用了Q-learning和DQN。我们决定使用DQN，以提供一个能够处理更复杂任务的算法，并且可以与具有RGB状态空间的环境一起使用。我们在[第5章](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml)中详细讲解了DQN，*深度Q网络*，对于ESBAS，我们将使用相同的实现。
- en: The last thing that we need to specify before going through the implementation
    is the portfolio's composition. We created a diversified portfolio, as regards
    the neural network architecture, but you can try with other combinations. For
    example, you could compose the portfolio with DQN algorithms of different learning
    rates.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实现之前，我们需要指定投资组合的构成。我们创建了一个多样化的投资组合，就神经网络架构而言，但你也可以尝试其他组合。例如，你可以将投资组合与具有不同学习率的DQN算法组合。
- en: 'The implementation is divided as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实现分为如下几个部分：
- en: The `DQN_optimization` class builds the computational graph, and optimizes a
    policy with DQN.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DQN_optimization`类构建计算图，并使用DQN优化策略。'
- en: The `UCB1` class defines the UCB1 algorithm.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UCB1`类定义了UCB1算法。'
- en: The `ESBAS` function implements the main pipeline for ESBAS.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ESBAS`函数实现ESBAS的主要流程。'
- en: 'We''ll provide the implementation of the last two bullet points, but you can
    find the full implementation on the GitHub repository of the book: [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供最后两点的实现，但你可以在书的GitHub仓库中找到完整的实现：[https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python)
- en: Let's start by going through `ESBAS(..)`. Besides the hyperparameters of DQN,
    there's only an additional `xi` argument that represents the ![](img/1f645c1c-9092-470b-9d46-c713a52cc18d.png)
    hyperparameter. The main outline of the `ESBAS` function is the same as the pseudocode
    that was given previously, so we can quickly go through it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解`ESBAS(..)`。除了DQN的超参数外，还有一个额外的`xi`参数，代表![](img/1f645c1c-9092-470b-9d46-c713a52cc18d.png)超参数。`ESBAS`函数的主要结构与之前给出的伪代码相同，因此我们可以快速浏览一遍。
- en: 'After having defined the function with all the arguments, we can reset the
    default graph of TensorFlow, and create two Gym environments (one for training,
    and one for testing). We can then create the portfolio, by instantiating a `DQN_optimization`
    object for each of the neural network sizes, and appending them on a list:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了所有参数的函数后，我们可以重置TensorFlow的默认图，并创建两个Gym环境（一个用于训练，另一个用于测试）。然后，我们可以通过为每个神经网络大小实例化一个`DQN_optimization`对象并将它们添加到列表中来创建投资组合：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we define an inner function, `DQNs_update`, that trains the policies in
    the portfolio in a DQN way. Take into consideration that all the algortihms in
    the portfolio are DQN, and that the only difference is in their neural network
    size. The optimization is done by the `optimize` and `update_target_network` methods
    of the `DQN_optimization` class:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个内部函数`DQNs_update`，它以DQN的方式训练投资组合中的策略。请注意，投资组合中的所有算法都是DQN，它们唯一的区别在于神经网络的大小。优化通过`DQN_optimization`类的`optimize`和`update_target_network`方法完成：
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As always, we need to initialize some (self-explanatory) variables: resetting
    the environment, instantiating an object of `ExperienceBuffer` (using the same
    classes that we used in others chapters), and setting up the exploration decay:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们需要初始化一些（不言自明的）变量：重置环境，实例化`ExperienceBuffer`对象（使用我们在其他章节中使用的相同类），并设置探索衰减：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can finally start the loop that iterates across the epochs. As for the preceding
    pseudocode, during each epoch, the following things occur:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于可以开始遍历各个时期的循环了。至于前面的伪代码，在每个时期，以下事情会发生：
- en: The policies are trained on the experience buffer
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略在经验缓冲区上进行训练
- en: The trajectories are run by the policy that is chosen by UCB1
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轨迹由UCB1选择的策略运行
- en: 'The first step is done by invoking `DQNs_update`, which we defined earlier,
    for the entire length of the epoch (which has an exponential length):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是通过调用我们之前定义的`DQNs_update`来完成的，整个时期的长度（具有指数长度）：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With regard to the second step, just before the trajectories are run, a new
    object of the `UCB1` class is instantiated and initialized. Then, a `while` loop
    iterates over the episodes of exponential size, inside of which, the `UCB1` object
    chooses which algorithm will run the next trajectory. During the trajectory, the
    actions are selected by `dqns[best_dqn]`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第二步，在轨迹运行之前，实例化并初始化了`UCB1`类的新对象。然后，一个`while`循环遍历指数大小的回合，其中，`UCB1`对象选择运行下一条轨迹的算法。在轨迹过程中，动作由`dqns[best_dqn]`选择：
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After each rollout, `ucb1` is updated with the RL return that was obtained
    in the last trajectory. Moreover, the environment is reset, and the reward of
    the current trajectory is appended to a list in order to keep track of all the
    rewards:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每次回合后，`ucb1`会根据上次轨迹获得的强化学习回报进行更新。此外，环境被重置，当前轨迹的奖励被追加到列表中，以便跟踪所有奖励：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That's all for the `ESBAS` function.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`ESBAS`函数的全部内容。
- en: '`UCB1` is made up of a constructor that initializes the attributes that are
    needed for computing (12.3); a `choose_algorithm()` method that returns the current
    best algorithm among the ones in the portfolio, as in (12.3); and `update(idx_algo,
    traj_return)` , which updates the average reward of the `idx_algo` algorithm with
    the last reward that was obtained, as understood from (12.4). The code is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`UCB1`由一个构造函数组成，该构造函数初始化计算所需的属性（见12.3）；一个`choose_algorithm()`方法，返回当前投资组合中最佳的算法（如12.3所示）；以及`update(idx_algo,
    traj_return)`，它使用获得的最后一个奖励更新`idx_algo`算法的平均奖励，如12.4所理解的那样。代码如下：'
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the code at hand, we can now test it on an environment and see how it performs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有了代码，我们现在可以在一个环境中进行测试，并查看它的表现。
- en: Solving Acrobot
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决Acrobot
- en: 'We''ll test ESBAS on yet another Gym environment—`Acrobot-v1`. As described
    in the OpenAI Gym documentation, *t**he Acrobot system includes two joints and
    two links, where the joint between the two links is actuated. Initially, the links
    are hanging downward, and the goal is to swing the end of the lower link up to
    a given height*. The following diagram shows the movement of the acrobot in a
    brief sequence of timesteps, from the start to an end position:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在另一个Gym环境——`Acrobot-v1`上测试ESBAS。正如OpenAI Gym文档中所描述的，*Acrobot系统包括两个关节和两个链节，其中两个链节之间的关节是有驱动的。最初，链节垂直挂着，目标是将下链节的末端摆动到给定的高度*。下图显示了Acrobot在简短的时间步长序列中的运动，从起始位置到结束位置：
- en: '![](img/7ecd2d52-3dc8-4336-9506-0e369fa99e02.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ecd2d52-3dc8-4336-9506-0e369fa99e02.png)'
- en: Sequence of the acrobot's movement
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Acrobot运动序列
- en: The portfolio comprises three deep neural networks of different sizes. One small
    neural network with only one hidden layer of size 64, one medium neural network
    with two hidden layers of size 16, and a large neural network with two hidden
    layers of size 64\. Furthermore, we set the hyperparameter of ![](img/d1f6a65b-3014-4152-b0e0-72b2a639c0d1.png)
    (the same value that is used in the paper).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 投资组合包含三种不同大小的深度神经网络。一种小型神经网络，只有一个大小为64的隐藏层；一种中型神经网络，具有两个大小为16的隐藏层；以及一种大型神经网络，具有两个大小为64的隐藏层。此外，我们设置了超参数！[](img/d1f6a65b-3014-4152-b0e0-72b2a639c0d1.png)（使用的值与论文中的一致）。
- en: Results
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'The following diagram shows the results. This plot presents both the learning
    curve of ESBAS: the complete portfolio (comprising the three neural networks that
    were listed previously) in the darker shade; and the learning curve of ESBAS,
    with only one best performing neural network (a deep neural network with two hidden
    layers of size 64) in *orange*. We know that ESBAS with only one algorithm in
    the portfolio will not really leverage the potential of the meta-algorithm, but
    we introduced it in order to have a baseline with which to compare the results.
    The plot speaks for itself, showing the *blue* line always above the *orange, *thus
    proving that ESBAS actually chooses the best available option. The unusual shape
    is due to the fact that we are training the DQN algorithms offline:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了结果。该图展示了ESBAS的学习曲线：较暗的阴影部分表示完整的组合（包括之前列出的三种神经网络）；较浅的*橙色*部分则表示ESBAS仅使用一个表现最好的神经网络（一个深度神经网络，具有两个隐藏层，每层大小为64）的学习曲线。我们知道，ESBAS仅使用一种算法的组合并不能真正发挥元算法的潜力，但我们引入它是为了提供一个基准，以便对比结果。图表本身说明了问题，显示了*蓝色*线始终高于*橙色*线，从而证明ESBAS实际上选择了最佳可用选项。这个不寻常的形状是因为我们正在离线训练DQN算法：
- en: '![](img/05e199cc-9517-4fb9-90bd-ace66f746543.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05e199cc-9517-4fb9-90bd-ace66f746543.png)'
- en: The performance of ESBAS with a portfolio of three algorithms in a dark shade,
    and with only one algorithm in a lighter shade
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ESBAS在使用三种算法的组合时以较深的阴影显示，而仅使用一种算法时则以较浅的阴影显示。
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章中提到的所有颜色参考，请参见[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)中的彩色图像包。
- en: Also, the spikes that you see at the start of the training, and then at around
    steps, 20K, 65K, and, 131K, are the points at which the policies are trained,
    and the meta-algorithm is reset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您在训练开始时以及在约20K、65K和131K步时看到的尖峰，是策略训练和元算法重置的时刻。
- en: 'We can now ask ourselves at which point in time ESBAS prefers one algorithm,
    compared to the others. The answer is shown in the plot of the following diagram.
    In this plot, the small neural network is characterized by the value 0, the medium
    one by the value 1, and the large by the value 2\. The dots show the algorithms
    that are chosen on each trajectory. We can see that, right at the beginning, the
    larger neural network is preferred, but that this immediately changes toward the
    medium, and then to the smaller one. After about 64K steps, the meta-algorithm
    switches back to the larger neural network:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以问自己，ESBAS在何时优先选择某个算法，而非其他算法。答案可以从以下图表中看到。在此图中，小型神经网络以值0表示，中型以值1表示，大型以值2表示。点表示每条轨迹上选择的算法。我们可以看到，在一开始，大型神经网络被优先选择，但随后立即转向中型神经网络，再到小型神经网络。大约64K步之后，元算法又切换回较大的神经网络：
- en: '![](img/ee253c6f-e3bf-448f-9b96-bee234b106ce.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee253c6f-e3bf-448f-9b96-bee234b106ce.png)'
- en: The plot shows the preferences of the meta-algorithm
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了元算法的偏好。
- en: From the preceding plot, we can also see that both of the ESBAS versions converge
    to the same values, but with very different speeds. Indeed, the version of ESBAS
    that leverages the true potential of AS (that is, the one with three algorithms
    in the portfolio) converges much faster. Both converge to the same values because,
    in the long run, the best neural network is the one that is used in the ESBAS
    version with the single option (the deep neural network with two hidden layers
    of size 64).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们还可以看到，两个ESBAS版本最终都收敛到相同的值，但收敛速度非常不同。事实上，利用AS的真正潜力的ESBAS版本（即包含三种算法的组合）收敛得要快得多。两者都收敛到相同的值，因为从长远来看，最佳的神经网络是ESBAS版本中使用的那个单一选项（即深度神经网络，具有两个隐藏层，每层大小为64）。
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we addressed the exploration-exploitation dilemma. This problem
    has already been tackled in previous chapters, but only in a light way, by employing
    simple strategies. In this chapter, we studied this dilemma in more depth, starting
    from the notorious multi-armed bandit problem. We saw how more sophisticated counter-based
    algorithms, such as UCB, can actually reach optimal performance, and with the
    expected logarithmic regret.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了探索与利用的困境。这个问题在之前的章节中已经有所提及，但当时只是以一种简单的方式，通过采用简单的策略进行探讨。本章中，我们更加深入地研究了这个困境，从著名的多臂老虎机问题开始。我们看到，更加复杂的基于计数的算法，如UCB，实际上能够达到最优表现，并且具有期望的对数后悔值。
- en: We then used exploration algorithms for AS. AS is an interesting application
    of exploratory algorithms, because the meta-algorithm has to choose the algorithm
    that best performs the task at hand. AS also has an outlet in reinforcement learning.
    For example, AS can be used to pick the best policy that has been trained with
    different algorithms from the portfolio, in order to run the next trajectory.
    That's also what ESBAS does. It tackles the problem of the online selection of
    off-policy RL algorithms by adopting UCB1\. We studied and implemented ESBAS in
    depth.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用了探索算法来解决AS。AS是探索性算法的一种有趣应用，因为元算法需要选择最适合当前任务的算法。AS在强化学习中也有应用。例如，AS可以用来选择在不同算法组合中训练出的最佳策略，以执行下一条轨迹。这也是ESBAS的作用。它通过采用UCB1来解决在线选择离策略强化学习算法的问题。我们深入研究并实现了ESBAS。
- en: Now, you know everything that is needed to design and develop highly performant
    RL algorithms that are capable of balancing between exploration and exploitation.
    Moreover, in the previous chapters, you have acquired the skills that are needed
    in order to understand which algorithm to employ in many different landscapes.
    However, until now, we have overlooked some more advanced RL topics and issues.
    In the next and final chapter, we'll fill these gaps, and talk about unsupervised
    learning, intrinsic motivation, RL challenges, and how to improve the robustness
    of algorithms. We will also see how it's possible to use transfer learning to
    switch from simulations to reality. Furthermore, we'll give some additional tips
    and best practices for training and debugging deep reinforcement learning algorithms.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经掌握了设计和开发高性能强化学习算法所需的所有知识，这些算法能够平衡探索与利用。此外，在之前的章节中，你已经获得了必要的技能，能理解在许多不同的环境中应该使用哪个算法。然而，直到现在，我们还忽视了一些更高级的强化学习话题和问题。在下一章也是最后一章，我们将填补这些空白，讨论无监督学习、内在动机、强化学习的挑战以及如何提高算法的鲁棒性。我们还将看到如何利用迁移学习从仿真环境转向现实世界。此外，我们还会给出一些关于训练和调试深度强化学习算法的额外建议和最佳实践。
- en: Questions
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What's the exploration-exploitation dilemma?
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是探索与利用的困境？
- en: What are two exploration strategies that we have already used in previous RL
    algorithms?
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在之前的强化学习算法中使用过的两种探索策略是什么？
- en: What's UCB?
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是UCB？
- en: Which problem is more difficult to solve: Montezuma's Revenge or the multi-armed
    bandit problem?
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个问题更难解决：Montezuma的复仇还是多臂老虎机问题？
- en: How does ESBAS tackle the problem of online RL algorithm selection?
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ESBAS是如何解决在线强化学习算法选择问题的？
- en: Further reading
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a more comprehensive survey about the multi-armed bandit problem, read *A
    Survey of Online Experiment Design with Stochastic Multi-Armed Bandit*: [https://arxiv.org/pdf/1510.00757.pdf.](https://arxiv.org/pdf/1510.00757.pdf)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解关于多臂老虎机问题的更全面的调查，请阅读*在线实验设计的调查与随机多臂老虎机*：[https://arxiv.org/pdf/1510.00757.pdf](https://arxiv.org/pdf/1510.00757.pdf)
- en: For reading the paper that leverages intrinsic motivation for playing Montezuma's
    Revenge, refer to *Unifying Count-Based Exploration and Intrinsic Motivation*: [https://arxiv.org/pdf/1606.01868.pdf.](https://arxiv.org/pdf/1606.01868.pdf)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要阅读利用内在动机来玩Montezuma的复仇的论文，请参阅*统一基于计数的探索与内在动机*：[https://arxiv.org/pdf/1606.01868.pdf](https://arxiv.org/pdf/1606.01868.pdf)
- en: For the original ESBAS paper, follow this link: [https://arxiv.org/pdf/1701.08810.pdf](https://arxiv.org/pdf/1701.08810.pdf).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要阅读原始的ESBAS论文，请访问此链接：[https://arxiv.org/pdf/1701.08810.pdf](https://arxiv.org/pdf/1701.08810.pdf)。
