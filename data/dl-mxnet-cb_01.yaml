- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Up and Running with MXNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速启动MXNet
- en: '**MXNet** is one of the most used deep learning frameworks and is an Apache
    open source project. Before 2016, **Amazon Web Services** (**AWS**)’s research
    efforts did not use a preferred deep learning framework, allowing each team to
    research and develop according to their choices. Although some deep learning frameworks
    have thriving communities, sometimes AWS was not able to fix code bugs at the
    required speed (among other issues). To solve these issues, at the end of 2016,
    AWS announced MXNet as its deep learning framework of choice, investing in internal
    teams to develop it further. Research institutions that support MXNet are Intel,
    Baidu, Microsoft, Carnegie Mellon University, and MIT, among others. It was co-developed
    by Carlos Guestrin at Carnegie Mellon University and the University of Washington
    (along with GraphLab).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**MXNet**是最常用的深度学习框架之一，是一个Apache开源项目。在2016年之前，**Amazon Web Services**（**AWS**）的研究团队并没有使用特定的深度学习框架，而是允许每个团队根据自己的选择进行研究和开发。尽管一些深度学习框架拥有蓬勃发展的社区，但有时AWS无法以所需的速度修复代码错误（还有其他问题）。为了解决这些问题，在2016年底，AWS宣布将MXNet作为其首选的深度学习框架，并投资内部团队进一步开发。支持MXNet的研究机构包括英特尔、百度、微软、卡内基梅隆大学和麻省理工学院等。该框架由卡内基梅隆大学的Carlos
    Guestrin与华盛顿大学（以及GraphLab）共同开发。'
- en: 'Some of its advantages are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 它的一些优势如下：
- en: Imperative/symbolic programming and hybridization (which will be covered in
    *Chapters 1* and *9*)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令式/符号式编程及其混合化（将在*第1章*和*第9章*中讲解）
- en: Support for multiple GPUs and distributed training (which will be covered in
    *Chapters 7* and *8*)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多GPU和分布式训练（将在*第7章*和*第8章*中讲解）
- en: Highly optimized for inference production systems (which will be covered in
    *Chapters 7* and *9*)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对推理生产系统进行了高度优化（将在*第7章*和*第9章*中讲解）
- en: A large number of pre-trained models on its Model Zoos in the fields of computer
    vision and natural language processing, among others (covered in *Chapters 6*,
    *7*, and *8*)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算机视觉和自然语言处理等领域，拥有大量预训练模型，这些模型存储在它的Model Zoos中（将在*第6章*、*第7章*和*第8章*中讲解）
- en: To start working with MXNet, we need to install the library. There are several
    different versions of MXNet available to be installed, and in this chapter, we
    will cover how to choose the right version. The most important parameter will
    be the available hardware we have. In order to optimize performance, it is always
    best to maximize the use of our available hardware. We will compare the usage
    of a well-known linear algebra library, NumPy, with similar operations in MXNet.
    We will then compare the performance of the different MXNet versions versus NumPy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用MXNet，我们需要安装该库。MXNet有多个不同版本可以安装，在本章中，我们将介绍如何选择合适的版本。最重要的参数将是我们所拥有的硬件。为了优化性能，最好最大化利用现有硬件的能力。我们将比较著名的线性代数库NumPy与MXNet中相似操作的使用方式。然后，我们将比较不同MXNet版本与NumPy的性能。
- en: MXNet includes its own API for deep learning, Gluon, and moreover, Gluon provides
    different libraries for computer vision and natural language processing that include
    pre-trained models and utilities. These libraries are known as GluonCV and GluonNLP.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet包含自己的深度学习API——Gluon，此外，Gluon还提供了用于计算机视觉和自然语言处理的不同库，这些库包含预训练模型和实用工具。这些库被称为GluonCV和GluonNLP。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Installing MXNet, Gluon, GluonCV, and GluonNLP
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装MXNet、Gluon、GluonCV和GluonNLP
- en: NumPy and MXNet ND arrays – comparing their performance
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy与MXNet ND数组——比较它们的性能
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Apart from the technical requirements specified in the *Preface*, no other requirements
    apply to this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*前言*中指定的技术要求外，本章没有其他要求。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub网址找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01)。
- en: 'Furthermore, you can access directly each recipe from Google Colab – for example,
    use the following for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接通过Google Colab访问每个食谱——例如，使用以下链接访问本章的第一个食谱：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb)。
- en: Installing MXNet, Gluon, GluonCV, and GluonNLP
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装MXNet、Gluon、GluonCV和GluonNLP
- en: In order to get the maximum performance out of the available software (programming
    languages) and hardware (CPU and GPU), there are different MXNet library versions
    available to install. We shall learn how to install them in this recipe.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化利用现有的软件（编程语言）和硬件（CPU和GPU）性能，有不同版本的MXNet库可以安装。在本节中，我们将学习如何安装它们。
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before getting started with the MXNet installation, let us review the different
    versions available of the software packages that we will use, including MXNet.
    The reason we do that is that our hardware configuration must map to the chosen
    versions of our software packages in order to maximize performance:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始安装MXNet之前，让我们回顾一下我们将使用的软件包的不同版本，包括MXNet。这样做的原因是，为了最大化性能，我们的硬件配置必须与所选软件包版本相匹配：
- en: '**Python**: MXNet is available for different programming languages – Python,
    Java, R, and C++, among others. We will use MXNet for Python, and Python 3.7+
    is recommended.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python**：MXNet支持多种编程语言——如Python、Java、R和C++等。我们将使用MXNet的Python版本，建议使用Python
    3.7及以上版本。'
- en: '**Jupyter**: Jupyter is an open source web application that provides an easy-to-use
    interface to show Markdown text, working code, and data visualizations. It is
    very useful for understanding deep learning, as we can describe concepts, write
    the code to run through those concepts, and visualize the results (typically comparing
    them with the input data). Jupyter Core 4.5+ is recommended.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter**：Jupyter是一个开源Web应用程序，提供了一个易于使用的界面来显示Markdown文本、可运行代码和数据可视化。它对于理解深度学习非常有用，因为我们可以描述概念，编写运行这些概念的代码，并可视化结果（通常将其与输入数据进行比较）。建议使用Jupyter
    Core 4.5及以上版本。'
- en: '**CPUs and GPUs**: MXNet can work with any hardware configuration – that is,
    any single CPU can run MXNet. However, there are several hardware components that
    MXNet can leverage to improve performance:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU和GPU**：MXNet可以与任何硬件配置兼容——也就是说，任何单一的CPU都可以运行MXNet。然而，MXNet可以利用多个硬件组件来提升性能：'
- en: '**Intel CPUs**: Intel developed a library known as **Math Kernel Library**
    (**MKL**) for optimized math operations. MXNet has support for this library, and
    using the optimized version can improve certain operations. Any modern version
    of Intel MKL is sufficient.'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Intel CPUs**：Intel开发了一种名为**数学核心库（Math Kernel Library，**MKL**）**的库，用于优化数学运算。MXNet支持该库，使用优化版本可以提高某些操作的性能。任何现代版本的Intel
    MKL都足够。'
- en: '**NVIDIA GPUs**: NVIDIA developed a library known as **Compute Unified Device
    Architecture (CUDA)** for optimized parallel operations (such as matrix operations,
    which are very common in deep learning). MXNet has support for this library, and
    using the optimized version can dramatically improve large deep learning workloads,
    such as model training. CUDA 11.0+ is recommended.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NVIDIA GPUs**：NVIDIA开发了一种名为**计算统一设备架构（CUDA）**的库，用于优化并行操作（例如在深度学习中非常常见的矩阵运算）。MXNet支持该库，使用优化版本可以显著提高大规模深度学习工作负载的性能，如模型训练。建议使用CUDA
    11.0及以上版本。'
- en: '**MXNet version**: At the time of writing, MXNet 1.9.1 is the most up-to-date
    stable version that has been released. All the code throughout the book has been
    verified with this version. MXNet, and deep learning in general, can be considered
    a live ongoing project, and therefore, new versions will be released periodically.
    These new versions will have improved functionality and new features, but they
    might also contain breaking changes from previous APIs. If you are revisiting
    this book in a few months and a new version has been released with breaking changes,
    how to install MXNet version 1.8.0 specifically is also described here.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MXNet 版本**：写作时，MXNet 1.9.1 是已发布的最新稳定版本。全书中的所有代码都已使用此版本验证。MXNet 和深度学习一般可以视为一个持续进行的项目，因此，新版本会定期发布。这些新版本将具有改进的功能和新特性，但也可能包含与旧版
    API 不兼容的重大更改。如果您在几个月后再次阅读此书，并且发布了包含重大更改的新版本，这里也有关于如何安装特定版本 MXNet 1.8.0 的说明。'
- en: Tip
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'I have used Google Colab as the platform to run the code described in this
    book. At the time of writing, it provides Python 3.10.12, up-to-date Jupyter libraries,
    Intel CPUs (Xeon @ 2.3 GHz), and NVIDIA GPUs (which can vary: K80s, T4s, P4s,
    and P100s) with CUDA 11.8 pre-installed. Therefore, minimal steps are required
    to install MXNet and get it running.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本书中使用 Google Colab 作为运行代码的平台。写作时，它提供 Python 3.10.12、最新的 Jupyter 库、英特尔 CPU（Xeon
    @ 2.3 GHz）和 NVIDIA GPU（可变：K80、T4、P4 和 P100），并预装了 CUDA 11.8。因此，安装 MXNet 并使其运行的步骤非常简便。
- en: How to do it...
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Throughout the book, we will not only use code extensively but also clarify
    comments and headings in that code to provide structure, as well as several types
    of visual information such as images or generated graphs. For these reasons, we
    will use Jupyter as the supporting development environment. Moreover, in order
    to facilitate setup, installation, and experimentation, we will use Google Colab.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在全书中，我们不仅会大量使用代码，还会在代码中添加注释和标题来提供结构，同时也会展示一些视觉信息，如图像或生成的图表。出于这些原因，我们将使用 Jupyter
    作为支持的开发环境。此外，为了简化设置、安装和实验过程，我们将使用 Google Colab。
- en: 'Google Colab is a hosted Jupyter Notebook service that requires no setup to
    use, while providing free access to computing resources, including GPUs. In order
    to set up Google Colab properly, this section is divided into two main points:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 是一个托管的 Jupyter Notebook 服务，无需设置即可使用，同时提供免费访问计算资源的权限，包括 GPU。为了正确设置
    Google Colab，本节分为两个主要部分：
- en: Setting up the notebook
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置笔记本
- en: Verifying and installing libraries
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证和安装库
- en: Important note
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you prefer, you can use any local environment that supports Python 3.7+,
    such as Anaconda, or any other Python distribution. This is highly encouraged
    if your hardware specifications are better than Google Colab’s offering, as better
    hardware will reduce computation time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，您可以使用任何支持 Python 3.7+ 的本地环境，如 Anaconda 或其他 Python 发行版。如果您的硬件规格优于 Google
    Colab 提供的硬件，强烈建议使用本地环境，因为更好的硬件可以减少计算时间。
- en: Setting up the notebook
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置笔记本
- en: 'In this section, we will learn how to work with Google Colab and set up a new
    notebook, which we will use to verify our MXNet installation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 Google Colab 并设置一个新的笔记本，我们将使用该笔记本来验证 MXNet 安装：
- en: Open your favorite web browser. In my case, I have used Google Chrome as the
    web browser throughout the book. Visit [https://colab.research.google.com/](https://colab.research.google.com/)
    and click on **NEW NOTEBOOK**.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您喜欢的网页浏览器。在本书中，我一直使用 Google Chrome 浏览器。访问 [https://colab.research.google.com/](https://colab.research.google.com/)
    并点击 **新建笔记本**。
- en: '![Figure 1.1 – The Google Colab start screen](img/B16591_01_1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Google Colab 启动画面](img/B16591_01_1.jpg)'
- en: Figure 1.1 – The Google Colab start screen
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Google Colab 启动画面
- en: Change the title of the notebook – for example, as you can see in the following
    screenshot, I have changed the title to `DL with MXNet Cookbook 1.1` `Installing
    MXNet`.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改笔记本的标题 – 例如，如下截图所示，我已将标题更改为`DL with MXNet Cookbook 1.1` `安装 MXNet`。
- en: '![Figure 1.2 – A Google Colab notebook](img/B16591_01_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 一个 Google Colab 笔记本](img/B16591_01_2.jpg)'
- en: Figure 1.2 – A Google Colab notebook
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 一个 Google Colab 笔记本
- en: 'Change your Google Colab runtime type to use a GPU:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Google Colab 的运行时类型更改为使用 GPU：
- en: Select **Change runtime type** from the **Runtime** menu.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 **运行时** 菜单中选择 **更改运行时类型**。
- en: '![Figure 1.3 – Change runtime type](img/B16591_01_3.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 更改运行时类型](img/B16591_01_3.jpg)'
- en: Figure 1.3 – Change runtime type
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 更改运行时类型
- en: In **Notebook settings**, select **GPU** as the **Hardware** **accelerator**
    option.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **Notebook 设置** 中，选择 **GPU** 作为 **硬件加速器** 选项。
- en: '![Figure 1.4 – Hardware accelerator | GPU](img/B16591_01_4.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 硬件加速器 | GPU](img/B16591_01_4.jpg)'
- en: Figure 1.4 – Hardware accelerator | GPU
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 硬件加速器 | GPU
- en: Verifying and installing libraries
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证并安装库
- en: 'In this section, go to the first cell (make sure it is a code cell) and type
    the following commands:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，转到第一个单元格（确保它是代码单元格），并输入以下命令：
- en: 'Verify the Python version by typing the following:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输入以下命令验证 Python 版本：
- en: '[PRE0]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Check the version, and make sure that it is 3.7+.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查版本，并确保版本为 3.7 或以上。
- en: Important note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In Google Colab, you can directly run commands as if you were in the Linux Terminal
    by adding the `!` character to the command. Feel free to try other commands such
    as `!ls`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 中，可以通过在命令前加上 `!` 字符直接运行命令，就像在 Linux 终端中一样。也可以尝试其他命令，例如 `!ls`。
- en: 'We now need to verify the Jupyter version (Jupyter Core 4.5.0 or above will
    suffice):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在需要验证 Jupyter 版本（Jupyter Core 4.5.0 或更高版本即可）：
- en: '[PRE1]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is one potential output from the previous command:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是前一个命令的一个潜在输出：
- en: '[PRE2]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Jupyter, an open source notebook application, is assumed to be installed, as
    is the case for Google Colab. For further instructions on how to install it, visit
    [https://jupyter.org/install](https://jupyter.org/install).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设已经安装了开源的 Jupyter 笔记本应用程序，就像 Google Colab 中一样。如需安装说明，请访问 [https://jupyter.org/install](https://jupyter.org/install)。
- en: 'Verify whether an Intel CPU is present in the hardware:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证硬件中是否存在 Intel CPU：
- en: '[PRE3]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The more up to date the processor the better, but for the purposes of this book,
    the dependency is larger with the GPU than with the CPU.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理器越新越好，但在本书的应用场景中，GPU 的依赖性比 CPU 更大。
- en: 'Verify the NVIDIA GPU is present in the hardware (there are devices listed
    below) and that NVIDIA CUDA is installed:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证硬件中是否存在 NVIDIA GPU（下面列出了相关设备），并确认已安装 NVIDIA CUDA：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will yield a similar output to the following:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生类似以下的输出：
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Important note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: CUDA 11.0 has known issues with the NVIDIA K80\. If you have an NVIDIA K80 and
    are having issues with the examples described, uninstall CUDA 11.0 and install
    CUDA 10.2\. Afterward, install MXNet for CUDA 10.2 following the steps described
    here.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 11.0 与 NVIDIA K80 已知存在兼容性问题。如果您使用的是 NVIDIA K80 并且在执行示例时遇到问题，请卸载 CUDA 11.0
    并安装 CUDA 10.2。然后，按照这里描述的步骤安装支持 CUDA 10.2 的 MXNet。
- en: 'Verify that the CUDA version is 11.0 or above:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 CUDA 版本是否为 11.0 或更高：
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will yield a similar output to the following:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生类似以下的输出：
- en: '[PRE7]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Install MXNet, depending on your hardware configuration. The following are
    the different MXNet versions that you can install:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的硬件配置安装 MXNet。以下是您可以安装的不同版本的 MXNet：
- en: '**Recommended/Google Colab**: The latest MXNet version (1.9.1) with GPU support:'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐/Google Colab**：安装支持 GPU 的最新 MXNet 版本（1.9.1）：'
- en: '[PRE8]'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**No Intel CPU nor NVIDIA GPU**: Install MXNet with the following command:'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有 Intel CPU 也没有 NVIDIA GPU**：使用以下命令安装 MXNet：'
- en: '[PRE9]'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Intel CPU without NVIDIA GPU**: Install MXNet with Intel MKL, with the following
    command:'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有 NVIDIA GPU 的 Intel CPU**：使用以下命令安装带有 Intel MKL 的 MXNet：'
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**No Intel CPU with NVIDIA GPU**: Install MXNet with NVIDIA CUDA 10.2, with
    the following command:'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有 Intel CPU 但有 NVIDIA GPU**：使用以下命令安装带有 NVIDIA CUDA 10.2 的 MXNet：'
- en: '[PRE11]'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Intel CPU and NVIDIA GPU**: Install MXNet with Intel MKL and NVIDIA CUDA
    11.0, with the following command:'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Intel CPU 和 NVIDIA GPU**：使用以下命令安装带有 Intel MKL 和 NVIDIA CUDA 11.0 的 MXNet：'
- en: '[PRE12]'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Tip
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: '`pip3`, a Python 3 package manager, is assumed to be installed, as is the case
    for Google Colab. If a different installation method for MXNet is preferred, visit
    [https://mxnet.apache.org/versions/master/get_started](https://mxnet.apache.org/versions/master/get_started)
    for instructions.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设已安装 `pip3`（Python 3 包管理器），就像 Google Colab 中的情况一样。如果您更喜欢其他安装 MXNet 的方法，请访问
    [https://mxnet.apache.org/versions/master/get_started](https://mxnet.apache.org/versions/master/get_started)
    获取说明。
- en: After version 1.6.0, MXNet is released by default with the Intel MKL library
    extension; therefore, there is no need to add the `mkl` suffix anymore when installing
    the most recent versions, as seen previously in the recommended installation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本 1.6.0 开始，MXNet 默认会发布带有 Intel MKL 库扩展的版本，因此在安装最新版本时不再需要添加 `mkl` 后缀，如先前推荐的安装方法所示。
- en: 'Verify that the MXNet installation has been successful with the following two
    steps:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下两个步骤验证 MXNet 是否成功安装：
- en: 'The following commands must not return any error and must successfully display
    MXNet version 1.9.1:'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令必须没有错误，并且必须成功显示 MXNet 版本 1.9.1：
- en: '[PRE13]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The list of features that appear in the following contain the `CUDA`, `CUDNN`,
    and `MKLDNN` features:'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下列出的特性包含 `CUDA`、`CUDNN` 和 `MKLDNN` 特性：
- en: '[PRE14]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output will list all the features and `True` for each one.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将列出所有特性，并且每个特性后面都会显示 `True`。
- en: 'Install GluonCV and GluonNLP:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 GluonCV 和 GluonNLP：
- en: '[PRE15]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This command will install the latest versions of GluonCV and GluonNLP, which
    at the time of writing were, respectively, 0.10 and 0.10.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将安装 GluonCV 和 GluonNLP 的最新版本，在写作时它们分别是 0.10 和 0.10。
- en: How it works...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The training, inference, and evaluation of deep learning networks are highly
    complex operations, involving hardware and several layers of software, including
    drivers, low-level performance libraries such as MKL and CUDA, and high-level
    programming languages and libraries such as Python and MXNet.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络的训练、推理和评估是非常复杂的操作，涉及硬件和多个软件层，包括驱动程序、低级性能库如 MKL 和 CUDA，以及高级编程语言和库如 Python
    和 MXNet。
- en: Important note
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'MXNet is an actively developed project, part of the Apache Incubator program.
    Therefore, new versions are expected to be released, and they might contain breaking
    changes. The preceding command will install the latest stable version available.
    Throughout this book, the version of MXNet used is 1.9.1\. If your code fails
    and it uses a different MXNet version, try installing MXNet version 1.9.1 by running
    the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 是一个积极开发的项目，属于 Apache Incubator 项目。因此，预计将会发布新版本，这些版本可能包含破坏性更改。前面的命令将安装最新的稳定版本。整本书中使用的
    MXNet 版本是 1.9.1。如果你的代码失败并且使用了不同的 MXNet 版本，尝试运行以下命令安装 MXNet 版本 1.9.1：
- en: '`!python3 -m pip` `install mxnet-cu117==1.9.1`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`!python3 -m pip` `install mxnet-cu117==1.9.1`'
- en: By checking all the hardware and software components, we can install the most
    optimized version of MXNet. We can use Google Colab, which easily transfers to
    other local configurations such as the Anaconda distribution.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查所有硬件和软件组件，我们可以安装最优化的 MXNet 版本。我们可以使用 Google Colab，它可以轻松迁移到其他本地配置，如 Anaconda
    发行版。
- en: Moreover, we can identify the right combination of CUDA drivers and MXNet versions
    that will maximize performance and verify a successful installation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以识别出正确的 CUDA 驱动程序和 MXNet 版本组合，这样可以最大化性能并验证安装成功。
- en: There’s more…
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: It is highly recommended to always use the latest versions of all the software
    components discussed. Deep learning is an evolving field and there are always
    improvements such as new functionalities added, changes in the APIs, and updates
    in the internal functions to increase performance, among other changes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议始终使用所有讨论的软件组件的最新版本。深度学习是一个不断发展的领域，总会有新功能的加入、API 的变化，以及内部功能的更新，以提升性能，等等。
- en: However, it is very important that all components (CPU, GPU, CUDA, and the MXNet
    version) are compatible. To match these components, it is highly recommended to
    visit [https://mxnet.apache.org/versions/master/get_started](https://mxnet.apache.org/versions/master/get_started)
    and check for the latest CUDA and MXNet versions you can install to maximize your
    hardware performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有组件（CPU、GPU、CUDA 和 MXNet 版本）之间的兼容性非常重要。为了确保这些组件匹配，强烈建议访问 [https://mxnet.apache.org/versions/master/get_started](https://mxnet.apache.org/versions/master/get_started)，查看最新的
    CUDA 和 MXNet 版本，并根据需要安装，以最大化硬件性能。
- en: As an example, for a Python 3-based Linux distribution, installed using `pip3`,
    these are the MXNet versions available (note with/without CPU acceleration and/or
    with GPU acceleration).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，对于基于 Python 3 的 Linux 发行版，通过 `pip3` 安装时，以下是可用的 MXNet 版本（请注意是否启用了 CPU 加速和/或
    GPU 加速）。
- en: 'If you are interested in knowing more about Intel’s **MKL**, the following
    link is a very good starting point: [https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html](https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于英特尔 **MKL** 的信息，以下链接是一个很好的起点：[https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html](https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html)。
- en: NumPy and MXNet ND arrays
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NumPy 和 MXNet ND 数组
- en: If you have worked with data previously in Python, chances are you have found
    yourself working with NumPy and its **N-dimensional arrays** (**ND arrays**).
    These are also known as tensors, and the 0D variants are called **scalars**, the
    1D variants are called **vectors**, and the 2D variants are called **matrixes**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前在Python中处理过数据，很可能已经接触过NumPy及其**N维数组**（**ND数组**）。这些也被称为张量，0维的变体叫做**标量**，1维的变体叫做**向量**，2维的变体叫做**矩阵**。
- en: MXNet provides its own ND array type, and there are two different ways to work
    with them. On one hand, there is the `nd` module, MXNet’s native and optimized
    way to work with MXNet ND arrays. On the other hand, there is the `np` module,
    which has the same interfaces and syntax as the NumPy ND array type and has also
    been optimized, but it’s limited due to the interface constraints. With MXNet
    ND arrays, we can leverage its underlying engine, with compute optimizations such
    as Intel MKL and/or NVIDIA CUDA, if our hardware configuration is compatible.
    This means we will be able to use almost the same syntax as when working with
    NumPy, but accelerated with the MXNet engine and our GPUs, not supported by NumPy.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet提供了它自己的ND数组类型，并且有两种不同的方式来处理它们。一方面，有`nd`模块，这是MXNet本地的、优化的处理MXNet ND数组的方式。另一方面，有`np`模块，它与NumPy
    ND数组类型有相同的接口和语法，并且也经过优化，但由于接口的限制，它的功能受到一定的局限。通过MXNet ND数组，我们可以利用其底层引擎，进行如Intel
    MKL和/或NVIDIA CUDA等计算优化，如果我们的硬件配置兼容。这意味着我们将能够使用与NumPy几乎相同的语法，但通过MXNet引擎和我们的GPU进行加速，这是NumPy所不支持的。
- en: Moreover, as we will see in the next chapters, a very common operation that
    we will execute on MXNet is automatic differentiation on these ND arrays. By using
    MXNet ND array libraries, this operation will also leverage our hardware for optimum
    performance. NumPy does not provide automatic differentiation out of the box.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们将在接下来的章节中看到的那样，我们将在MXNet上执行的一个非常常见的操作是对这些ND数组进行自动微分。通过使用MXNet ND数组库，这一操作还将利用我们的硬件，以获得最佳性能。NumPy本身并不提供自动微分功能。
- en: Getting ready
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'If you have already installed MXNet, as described in the previous recipe, in
    terms of executing accelerated code, the only remaining steps before using MXNet
    ND arrays is importing their libraries:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经按照之前的步骤安装了MXNet，那么在执行加速代码方面，使用MXNet ND数组之前唯一剩下的步骤就是导入它们的库：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: However, it is worth noting here an important underlying difference between
    NumPy ND array operations and MXNet ND array operations. NumPy follows an eager
    evaluation strategy – that is, all operations are evaluated at the moment of execution.
    Conversely, MXNet uses a lazy evaluation strategy, more optimal for large compute
    loads, where the actual calculation is deferred until the values are actually
    needed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里值得注意的是NumPy ND数组操作和MXNet ND数组操作之间的一个重要根本区别。NumPy遵循急切求值策略——也就是说，所有操作会在执行时立即求值。相反，MXNet采用懒惰求值策略，这对于大型计算负载更加优化，实际计算会推迟，直到真正需要这些值时才进行计算。
- en: 'Therefore, when comparing performances, we will need to force MXNet to finalize
    all calculations before computing the time needed for them. As we will see in
    the examples, this is achieved by calling the `wait_to_read()` function, Furthermore,
    when accessing the data with functions such as `print()` or `.asnumpy()`, execution
    is then completed before calling these functions, yielding the wrong impression
    that these functions are actually time-consuming:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在比较性能时，我们需要强制MXNet在计算所需时间之前完成所有计算。如我们将在示例中看到的那样，调用`wait_to_read()`函数可以实现这一点。此外，当通过`print()`或`.asnumpy()`等函数访问数据时，执行将会在调用这些函数之前完成，这可能会给人一种这些函数实际上很耗时的错误印象：
- en: 'Let’s check a specific example and start by running it on the CPU:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一个具体的示例，并从在CPU上运行它开始：
- en: '[PRE17]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will yield a similar output to the following:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生类似于以下的输出：
- en: '[PRE18]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'However, let’s see what happens if we measure the time without the call to
    `wait_to_read()`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，让我们看看如果没有调用`wait_to_read()`，时间测量会发生什么：
- en: '[PRE19]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following will be the output:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下将是输出：
- en: '[PRE20]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see, the first experiment indicated that the computation took ~50
    ms to complete; however, the second experiment indicated that the computation
    took ~1 ms (50 times less!), and the visualization was more than 40 ms. This is
    an incorrect result. This is because we measured our performance incorrectly in
    the second experiment. Refer to the first experiment and the call to `wait_to_read()`
    for a proper performance measurement.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，第一个实验表明计算大约花费了 50 毫秒完成；然而，第二个实验表明计算仅花费了约 1 毫秒（少了 50 倍！），而可视化则超过了 40 毫秒。这是一个错误的结果。这是因为我们在第二个实验中错误地衡量了性能。请参阅第一个实验以及调用
    `wait_to_read()` 以正确测量性能。
- en: How to do it...
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this section, we will compare performance in terms of computation time for
    two compute-intensive operations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从计算时间的角度比较两个计算密集型操作的性能：
- en: Matrix creation
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵创建
- en: Matrix multiplication
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'We will compare five different compute profiles for each operation:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较每个操作的五种不同计算配置：
- en: Using the NumPy library (no CPU or GPU acceleration)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NumPy 库（无 CPU 或 GPU 加速）
- en: Using the MXNet `np` module with CPU acceleration but no GPU
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MXNet `np` 模块进行 CPU 加速，但没有 GPU
- en: Using the MXNet `np` module with CPU acceleration and GPU acceleration
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MXNet `np` 模块进行 CPU 加速和 GPU 加速
- en: Using the MXNet `nd` module with CPU acceleration but no GPU
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MXNet `nd` 模块进行 CPU 加速，但没有 GPU
- en: Using the MXNet `nd` module with CPU acceleration and GPU acceleration
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MXNet `nd` 模块进行 CPU 加速和 GPU 加速
- en: To finalize, we will plot the results and draw some conclusions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将绘制结果并得出一些结论。
- en: Timing data structures
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定时数据结构
- en: 'We will store the computation time in five dictionaries, one for each compute
    profile (`timings_np`, `timings_mx_cpu`, and `timings_mx_gpu`). The initialization
    of the data structures is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在五个字典中存储计算时间，每个计算配置一个字典（`timings_np`、`timings_mx_cpu` 和 `timings_mx_gpu`）。数据结构的初始化如下：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will run each operation (matrix generation and matrix multiplication) with
    matrixes in a different order, namely the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按不同的顺序运行每个操作（矩阵生成和矩阵乘法），即以下顺序：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Matrix creation
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵创建
- en: 'We define three functions to generate matrixes; the first function will use
    the NumPy library to generate a matrix, and it will receive as an input parameter
    the matrix order. The second function will use the MXNet np module, and the third
    function will use the MXNet and module. For the second and third functions, as
    input parameters we will provide the context where the matrix needs to be created,
    apart from the matrix order. This context specifies whether the result (the created
    matrix in this case) must be computed in the CPU or the GPU (and which GPU if
    there are multiple devices available):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了三个函数来生成矩阵；第一个函数将使用 NumPy 库生成矩阵，并接收矩阵的维度作为输入参数。第二个函数将使用 MXNet np 模块，第三个函数将使用
    MXNet nd 模块。对于第二个和第三个函数，输入参数包括矩阵需要创建的上下文，以及矩阵的维度。该上下文指定结果（在此情况下为创建的矩阵）必须在 CPU
    或 GPU 上计算（如果有多个设备可用，则指定具体 GPU）：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To store necessary data for our performance comparison later, we use the structures
    created previously, with the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了后续的性能比较存储必要的数据，我们使用之前创建的数据结构，代码如下：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Matrix multiplication
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'We define three functions to compute the matrixes multiplication; the first
    function will use the NumPy library and will receive as input parameters the matrixes
    to multiply. The second function will use the MXNet np module, and the third function
    will use the MXNet nd module. For the second and third functions, the same parameters
    are used. The context where the multiplication will happen is given by the context
    where the matrixes were created; no parameter needs to be added. Both matrixes
    need to have been created in the same context, or an error will be triggered:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了三个函数来计算矩阵的乘法；第一个函数将使用 NumPy 库，并接收要相乘的矩阵作为输入参数。第二个函数将使用 MXNet np 模块，第三个函数将使用
    MXNet nd 模块。对于第二个和第三个函数，使用相同的参数。乘法发生的上下文由矩阵创建时的上下文给出；无需添加任何参数。两个矩阵需要在相同的上下文中创建，否则将触发错误：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To store the necessary data for our performance comparison later, we will use
    the structures created previously, with the following code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了后续的性能比较存储必要的数据，我们将使用之前创建的数据结构，代码如下：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Drawing conclusions
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 得出结论
- en: 'The first step before making any assessments is to plot the data we have captured
    in the previous steps. For this step, we will use the `pyplot` module from a library
    called Matplotlib, which will allow us to create charts easily. The following
    code plots the runtime (in seconds) for the matrix generation and all the matrix
    orders computed:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何评估之前的第一步是绘制我们在前面步骤中捕获的数据。在这一步中，我们将使用名为Matplotlib的库中的`pyplot`模块，它可以帮助我们轻松创建图表。以下代码绘制了矩阵生成的运行时间（单位：秒）以及所有计算的矩阵阶数：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Quite similarly as shown in the previous code block, the following code plots
    the runtime (in seconds) for the matrix multiplication and all the matrix orders
    computed:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的代码块非常相似，以下代码绘制了矩阵乘法的运行时间（单位：秒）以及所有计算的矩阵阶数：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'These are the plots displayed (the results will vary according to the hardware
    configuration):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是显示的图表（结果会根据硬件配置有所不同）：
- en: '![Figure 1.5 – Runtimes – a) Matrix creation, and b) Matrix multiplication](img/B16591_01_5.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 运行时间 – a) 矩阵创建，b) 矩阵乘法](img/B16591_01_5.jpg)'
- en: Figure 1.5 – Runtimes – a) Matrix creation, and b) Matrix multiplication
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 运行时间 – a) 矩阵创建，b) 矩阵乘法
- en: Important note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that the charts use a logarithmic scale for both axes, horizontal and vertical
    (the differences are larger than they seem). Furthermore, the actual values depend
    on the hardware architecture that the computations are run on; therefore, your
    specific results will vary.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，图表的横轴和纵轴都使用了对数刻度（差异比看起来的要大）。此外，实际的数值取决于运行计算的硬件架构；因此，您的具体结果可能会有所不同。
- en: 'There are several conclusions that can be drawn, both from each individual
    operation and collectively:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个单独的操作和整体上都可以得出几个结论：
- en: For smaller matrix orders, using NumPy is much faster in both operations. This
    is because MXNet works in a different memory space, and the amount of time to
    move the data to this memory space is longer than the actual compute time.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较小的矩阵阶数，使用NumPy在这两个操作中都要快得多。这是因为MXNet在不同的内存空间中工作，将数据移动到该内存空间的时间比实际计算时间要长。
- en: In matrix creation, for larger matrix orders, the difference between NumPy (remember,
    it’s CPU only) and MXNet with the np module and CPU acceleration is negligible,
    but with the nd module and CPU, acceleration is ~2x faster. For matrix multiplication,
    and depending on your hardware, MXNet with CPU acceleration can be ~2x faster
    (regardless of the module). This is because MXNet uses Intel MKL to optimize CPU
    computations.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在矩阵创建中，对于较大的矩阵阶数，NumPy（记住，它仅使用CPU）与MXNet在使用np模块和CPU加速时的差异可以忽略不计，但在使用nd模块和CPU加速时，速度大约快2倍。对于矩阵乘法，取决于您的硬件，MXNet在CPU加速下的速度可以快大约2倍（无论使用哪个模块）。这是因为MXNet使用Intel
    MKL优化CPU计算。
- en: In the ranges that are interesting for deep learning – that is, large computational
    loads involving matrix orders > 1,000 (which can represent data such as images
    composed of several megapixels or large language dictionaries), GPUs deliver typical
    gains of several orders of magnitude (~200x for creation, and ~40x for multiplication,
    exponentially growing with every increase of matrix order). This is by far the
    most compelling reason to work with GPUs when running deep learning experiments.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习中有意义的范围内——也就是说，涉及矩阵阶数大于1,000的巨大计算负载（这可能代表数据，如由多个百万像素组成的图像或大型语言词典）——GPU能提供典型的多个数量级的提升（矩阵创建约为200倍，矩阵乘法约为40倍，随着矩阵阶数的增加，提升呈指数增长）。这是运行深度学习实验时使用GPU的最有说服力的理由。
- en: When using the GPU, the MXNet np module is faster than the MXNet nd module in
    creation (~7x), but the difference is negligible in multiplication. Typically,
    deep learning algorithms are more similar to multiplications to terms of computational
    loads, and therefore, a priori, there is no significant advantage in using the
    np module or the nd module. However, *MXNet recommends using the native MXNet
    nd module* (and the author subscribes to this recommendation) because some operations
    on the np module are not supported by `autograd` (MXNet’s auto-differentiation
    module). We will see in the upcoming chapters, when we train neural networks,
    how the `autograd` module is used and why it is critical.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用 GPU 时，MXNet np 模块在创建矩阵时比 MXNet nd 模块更快（约快 7 倍），但在乘法操作中的差异可以忽略不计。通常，深度学习算法的计算负载更像是乘法操作，因此，事先并没有显著的优势来选择
    np 模块或 nd 模块。然而，*MXNet 推荐使用原生的 MXNet nd 模块*（笔者也认同这个推荐），因为 np 模块的一些操作不被 `autograd`（MXNet
    的自动微分模块）支持。我们将在接下来的章节中看到，当我们训练神经网络时，如何使用 `autograd` 模块，以及它为何如此重要。
- en: How it works...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'MXNet provides two optimized modules to work with ND arrays, including one
    that is an in-place substitute for NumPy. The advantages of operating with MXNet
    ND arrays are twofold:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 提供了两个优化模块来处理 ND 数组，其中一个是 NumPy 的原地替代品。使用 MXNet ND 数组的优势有两个：
- en: MXNet ND array operations support automatic differentiation. As we will see
    in the following chapters, automatic differentiation is a key feature that allows
    developers to concentrate on the forward pass of the models, letting the backward
    pass be automatically derived.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet ND 数组操作支持自动微分。正如我们在接下来的章节中将看到的，自动微分是一个关键特性，它允许开发者专注于模型的前向传播，而将反向传播自动推导出来。
- en: Conversely, operations with MXNet ND arrays are optimized for the underlying
    hardware, yielding impressive results with GPU acceleration. We computed results
    for matrix creation and matrix multiplication to validate this conclusion experimentally.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，MXNet ND 数组的操作经过优化，能够充分发挥底层硬件的性能，利用 GPU 加速可以获得显著的效果。我们通过矩阵创建和矩阵乘法来实验验证这一结论。
- en: There’s more…
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'In this recipe, we have barely scratched the surface of MXNet operations with
    ND arrays. If you want to read more about MXNet and ND arrays, this is the link
    to the official MXNet API reference: https://mxnet.apache.org/versions/1.0.0/api/python/ndarray/ndarray.html.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们仅仅触及了 MXNet 使用 ND 数组的一部分。如果你想了解更多关于 MXNet 和 ND 数组的内容，这里是官方 MXNet API
    文档的链接：[https://mxnet.apache.org/versions/1.0.0/api/python/ndarray/ndarray.html](https://mxnet.apache.org/versions/1.0.0/api/python/ndarray/ndarray.html)。
- en: 'Furthermore, a very interesting tutorial can be found in the official MXNet
    documentation: https://gluon.mxnet.io/chapter01_crashcourse/ndarray.html.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，官方 MXNet 文档中还有一个非常有趣的教程：[https://gluon.mxnet.io/chapter01_crashcourse/ndarray.html](https://gluon.mxnet.io/chapter01_crashcourse/ndarray.html)。
- en: 'Moreover, we have taken a glimpse at how to measure performance on MXNet. We
    will revisit this topic in the following chapters; however, a good deep-dive into
    the topic is given in the official MXNet documentation: [https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html](https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还简要了解了如何在 MXNet 上衡量性能。我们将在接下来的章节中再次讨论这个话题；不过，关于该话题的深入讲解可以在官方 MXNet 文档中找到：[https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html](https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html)。
