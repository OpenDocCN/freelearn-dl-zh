- en: Feature Engineering and Model Complexity – The Titanic Example Revisited
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程与模型复杂度——泰坦尼克号示例回顾
- en: Model complexity and assessment is a must-do step toward building a successful
    data science system. There are lots of tools that you can use to assess and choose
    your model. In this chapter, we are going to address some of the tools that can
    help you to increase the value of your data by adding more descriptive features
    and extracting meaningful information from existing ones. We are also going to
    address other tools related optimal number features and learn why it's a problem
    to have a large number of features and fewer training samples/observations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂度与评估是建立成功的数据科学系统的必要步骤。有很多工具可以用来评估和选择你的模型。在这一章中，我们将介绍一些能够通过增加更多描述性特征并从现有特征中提取有意义信息的工具。我们还将讨论与特征数量优化相关的其他工具，并了解为什么特征数量过多而训练样本/观察数过少是一个问题。
- en: 'The following are the topics that will be explained in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将解释以下主题：
- en: Feature engineering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: The curse of dimensionality
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度灾难
- en: Titanic example revisited—all together
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泰坦尼克号示例回顾——整体概览
- en: Bias-variance decomposition
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差分解
- en: Learning visibility
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习可见性
- en: Feature engineering
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering is one of the key components that contribute to the model's
    performance. A simple model with the right features can perform better than a
    complicated one with poor features. You can think of the feature engineering process
    as the most important step in determining your predictive model's success or failure.
    Feature engineering will be much easier if you understand the data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是模型性能的关键组成部分之一。一个具有正确特征的简单模型，可能比一个具有糟糕特征的复杂模型表现更好。你可以把特征工程过程看作是决定预测模型成功或失败的最重要步骤。如果你理解数据，特征工程将会变得更加容易。
- en: 'Feature engineering is used extensively by anyone who uses machine learning
    to solve only one question, which is: **how do you get the most out of your data
    samples for predictive modeling**? This is the problem that the process and practice
    of feature engineering solves, and the success of your data science skills starts
    by knowing how to represent your data well.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程广泛应用于任何使用机器学习解决单一问题的人，这个问题是：**如何从数据样本中获取最大价值用于预测建模**？这是特征工程过程和实践所解决的问题，而你的数据科学技能的成功，始于如何有效地表示你的数据。
- en: Predictive modeling is a formula or rule that transforms a list of features
    or input variables (*x*[1], *x*[2],..., *x*[n]) into an output/target of interest
    (y). So, what is feature engineering? It's the process of creating new input variables
    or features (*z*[1], *z*[2], ..., *z*[n]) from existing input variables (*x*[1],
    *x*[2],..., *x*[n]). We don't just create any new features; the newly created
    features should contribute and be relevant to the model's output. Creating such
    features that will be relevant to the model's output will be an easy process with
    knowledge of the domain (such as marketing, medical, and so on). Even if machine
    learning practitioners interact with some domain experts during this process,
    the outcome of the feature engineering process will be much better.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预测建模是一个将一组特征或输入变量（*x*[1], *x*[2], ..., *x*[n]）转换为感兴趣的输出/目标（y）的公式或规则。那么，什么是特征工程呢？它是从现有的输入变量（*x*[1],
    *x*[2], ..., *x*[n]）中创建新的输入变量或特征（*z*[1], *z*[2], ..., *z*[n]）的过程。我们不仅仅是创建任何新特征；新创建的特征应该对模型的输出有所贡献并且相关。创建与模型输出相关的特征，如果了解领域知识（如营销、医学等），将会是一个容易的过程。即使机器学习从业人员在这一过程中与领域专家互动，特征工程的结果也会更加优秀。
- en: An example where domain knowledge can be helpful is modeling the likelihood
    of rain, given a set of input variables/features (temperature, wind speed, and
    percentage of cloud cover). For this specific example, we can construct a new
    binary feature called **overcast**, where its value equals 1 or no whenever the
    percentage of cloud cover is less than 20%, and equals 0 or yes otherwise. In
    this example, domain knowledge was essential to specify the threshold or cut-off
    percentage. The more thoughtful and useful the inputs, the better the reliability
    and predictivity of your model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个领域知识有用的例子是给定一组输入变量/特征（温度、风速和云层覆盖百分比）时建模降雨的可能性。在这个特定的例子中，我们可以构建一个新的二进制特征，叫做**阴天**，其值为1或否，表示云层覆盖百分比小于20%，否则值为0或是。在这个例子中，领域知识对于指定阈值或切割百分比至关重要。输入越深思熟虑且有用，模型的可靠性和预测能力就越强。
- en: Types of feature engineering
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程的类型
- en: Feature engineering as a technique has three main subcategories. As a deep learning
    practitioner, you have the freedom to choose between them or combine them in some
    way.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种技术，特征工程有三个主要的子类别。作为深度学习的实践者，你可以自由选择它们，或以某种方式将它们结合起来。
- en: Feature selection
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: Sometimes called **feature importance**, this is the process of ranking the
    input variables according to their contribution to the target/output variable.
    Also, this process can be considered a ranking process of the input variables
    according to their value in the predictive ability of the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有时被称为**特征重要性**，这是根据输入变量对目标/输出变量的贡献对输入变量进行排序的过程。此外，这个过程也可以被视为根据输入变量在模型预测能力中的价值对其进行排序的过程。
- en: Some learning methods do this kind of feature ranking or importance as part
    of their internal procedures (such as decision trees). Mostly, these kind of methods
    uses entropy to filter out the less valuable variables. In some cases, deep learning
    practitioners use such learning methods to select the most important features
    and then feed them into a better learning algorithm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些学习方法会在其内部过程中执行这种特征排名或重要性（如决策树）。这些方法大多使用熵来过滤掉价值较低的变量。在某些情况下，深度学习实践者会使用这种学习方法来选择最重要的特征，然后将其输入到更好的学习算法中。
- en: Dimensionality reduction
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimensionality reduction is sometimes feature extraction, and it is the process
    of combining the existing input variables into a new set of a much reduced number
    of input variables. One of the most used methods for this type of feature engineering
    is **principle component analysis** (**PCA**), which utilizes the variance in
    data to come up with a reduced number of input variables that don't look like
    the original input variables.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 降维有时也被称为特征提取，它是将现有输入变量组合成一个新的、显著减少的输入变量集合的过程。用于这种类型特征工程的最常用方法之一是**主成分分析**（**PCA**），它利用数据的方差来生成一个减少后的输入变量集合，这些新变量看起来不像原始的输入变量。
- en: Feature construction
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征构建
- en: Feature construction is a commonly used type of feature engineering, and  people usually
    refer to it when they talk about feature engineering. This technique is the process
    of handcrafting or constructing new features from raw data. In this type of feature
    engineering, domain knowledge is very useful to manually make up other features
    from existing ones. Like other feature engineering techniques, the purpose of
    feature construction is to increase the predictivity of your model. A simple example
    of feature construction is using the date stamp feature to generate two new features,
    such as AM and PM, which might be useful to distinguish between day and night.
    We can also transform/convert noisy numerical features into simpler, nominal ones
    by calculating the mean value of the noisy feature and then determining whether
    a given row is more than or less than that mean value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特征构建是特征工程中常用的一种类型，通常人们在谈论特征工程时会提到它。这项技术是从原始数据中手工构建或构造新特征的过程。在这种类型的特征工程中，领域知识非常有用，可以通过现有特征手动构建其他特征。像其他特征工程技术一样，特征构建的目的是提高模型的预测能力。一个简单的特征构建例子是利用日期时间戳特征生成两个新特征，如
    AM 和 PM，这可能有助于区分白天和夜晚。我们还可以通过计算噪声特征的均值来将嘈杂的数值特征转化为更简单的名义特征，然后确定给定的行是否大于或小于该均值。
- en: Titanic example revisited
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泰坦尼克号例子重访
- en: In this section, we are going to go through the Titanic example again but from
    a different perspective while using the feature engineering tool. In case you
    skipped [Chapter 2](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml), *Data Modeling
    in Action - The Titanic Example*, the Titanic example is a Kaggle competition
    with the purpose of predicting weather a specific passenger survived or not.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将再次通过不同的角度使用特征工程工具来回顾泰坦尼克号的例子。如果你跳过了[第2章](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml)，*数据建模实践
    - 泰坦尼克号的例子*，泰坦尼克号的例子是一个 Kaggle 竞赛，目的是预测某个特定乘客是否幸存。
- en: 'During this revisit of the Titanic example, we are going to use the scikit-learn
    and pandas libraries. So first off, let''s start by reading the train and test
    sets and get some statistics about the data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新审视泰坦尼克号的例子时，我们将使用 scikit-learn 和 pandas 库。所以首先，让我们开始读取训练集和测试集，并获取一些数据的统计信息：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We need to point out a few things about the preceding code snippet:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指出几点关于前面代码片段的内容：
- en: As shown, we have used the `concat` function of pandas to combine the data frames
    of the train and test sets. This is useful for the feature engineering task as
    we need a full view of the distribution of the input variables/features.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如所示，我们使用了pandas的`concat`函数将训练集和测试集的数据框合并。这对于特征工程任务很有用，因为我们需要全面了解输入变量/特征的分布情况。
- en: After combining both data frames, we need to do some modifications to the output
    data frame.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在合并两个数据框后，我们需要对输出数据框进行一些修改。
- en: Missing values
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值
- en: This step will be the first thing to think of after getting a new dataset from
    the customer, because there will be missing/incorrect data in nearly every dataset.
    In the next chapters, you will see that some learning algorithms are able to deal
    with missing values and others need you to handle missing data. During this example,
    we are going to use the random forest classifier from scikit-learn, which requires
    separate handling of missing data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是获得客户提供的新数据集后需要首先考虑的事情，因为几乎每个数据集都会有缺失或错误的数据。在接下来的章节中，你将看到一些学习算法能够处理缺失值，而其他算法则需要你自己处理缺失数据。在这个示例中，我们将使用来自scikit-learn的随机森林分类器，它需要单独处理缺失数据。
- en: There are different approaches that you can use to handle missing data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据有不同的方法。
- en: Removing any sample with missing values in it
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除任何含有缺失值的样本
- en: This approach won't be a good choice if you have a small dataset with lots of
    missing values, as removing the samples with missing values will produce useless
    data. It could be a quick and easy choice if you have lots of data, and removing
    it won't affect the original dataset much.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个小数据集且缺失值较多，那么这种方法就不太合适，因为删除含有缺失值的样本会导致数据无效。如果你有大量数据，并且删除这些数据不会对原始数据集造成太大影响，那么这种方法可能是一个快捷且简单的选择。
- en: Missing value inputting
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值填补
- en: This approach is useful when you have categorical data. The intuition behind
    this approach is that missing values may correlate with other variables, and removing
    them will result in a loss of information that can affect the model significantly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在处理分类数据时非常有用。其背后的直觉是，缺失值可能与其他变量相关，删除这些缺失值会导致信息丢失，从而显著影响模型。
- en: 'For example, if we have a binary variable with two possible values, -1 and
    1, we can add another value (0) to indicate a missing value. You can use the following
    code to replace the null values of the **Cabin** feature with `U0`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个二值变量，可能的取值为-1和1，我们可以添加另一个值（0）来表示缺失值。你可以使用以下代码将**Cabin**特征的空值替换为`U0`：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Assigning an average value
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分配一个平均值
- en: This is also one of the common approaches because of its simplicity. In the
    case of a numerical feature, you can just replace the missing values with the
    mean or median. You can also use this approach in the case of categorical variables
    by assigning the mode (the value that has the highest occurrence) to the missing
    values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一种常见的方法，因为它的简便性。对于数值特征，你可以直接用均值或中位数替换缺失值。在处理分类变量时，你也可以通过将众数（出现频率最高的值）分配给缺失值来使用此方法。
- en: 'The following code assigns the median of the non-missing values of the `Fare`
    feature to the missing values:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将`Fare`特征中非缺失值的中位数分配给缺失值：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or, you can use the following code to find the value that has the highest occurrence
    in the `Embarked` feature and assign it to the missing values:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用以下代码查找`Embarked`特征中出现频率最高的值，并将其分配给缺失值：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using a regression or another simple model to predict the values of missing
    variables
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归或其他简单模型来预测缺失变量的值
- en: This is the approach that we will use for the `Age` feature of the Titanic example.
    The `Age` feature is an important step towards predicting the survival of passengers,
    and applying the previous approach by taking the mean will make us lose some information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将用于泰坦尼克号示例中`Age`特征的方法。`Age`特征是预测乘客生还的一个重要步骤，采用前述方法通过计算均值填补会使我们丧失一些信息。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Feature transformations
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征变换
- en: In the previous two sections, we covered reading the train and test sets and
    combining them. We also handled some missing values. Now, we will use the random
    forest classifier of scikit-learn to predict the survival of passengers. Different
    implementations of the random forest algorithm accept different types of data.
    The scikit-learn implementation of random forest accepts only numeric data. So,
    we need to transform the categorical features into numerical ones.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两部分中，我们介绍了如何读取训练集和测试集并将其合并，还处理了一些缺失值。现在，我们将使用scikit-learn的随机森林分类器来预测乘客的生还情况。不同实现的随机森林算法接受的数据类型不同。scikit-learn实现的随机森林仅接受数值数据。因此，我们需要将分类特征转换为数值特征。
- en: 'There are two types of features:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的特征：
- en: '**Quantitative**: Quantitative features are measured in a numerical scale and
    can be meaningfully sorted. In the Titanic data samples, the `Age` feature is
    an example of a quantitative feature.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定量**：定量特征是以数值尺度度量的，可以进行有意义的排序。在泰坦尼克号数据中，`Age`特征就是定量特征的一个例子。'
- en: '**Qualitative**: Qualitative variables, also called **categorical variables**,
    are variables that are not numerical. They describe data that fits into categories.
    In the Titanic data samples, the `Embarked` (indicates the name of the departure
    port) feature is an example of a qualitative feature.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定性**：定性变量，也称为**分类变量**，是非数值型变量。它们描述的是可以归类的数据。在泰坦尼克号数据中，`Embarked`（表示出发港口的名称）特征就是定性特征的一个例子。'
- en: We can apply different kinds of transformations to different variables. The
    following are some approaches that one can use to transform qualitative/categorical
    features.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对不同的变量应用不同种类的转换。以下是一些可以用来转换定性/分类特征的方法。
- en: Dummy features
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟特征
- en: These variables are also known as categorical or binary features. This approach
    will be a good choice if we have a small number of distinct values for the feature
    to be transformed. In the Titanic data samples, the `Embarked` feature has only
    three distinct values (`S`, `C`, and `Q`) that occur frequently. So, we can transform
    the `Embarked` feature into three dummy variables, (`'Embarked_S'`, `'Embarked_C'`,
    and `'Embarked_Q'`) to be able to use the random forest classifier.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量也被称为分类特征或二元特征。如果要转换的特征只有少数几个不同的值，那么这种方法将是一个不错的选择。在泰坦尼克号数据中，`Embarked`特征只有三个不同的值（`S`、`C`和`Q`），并且这些值经常出现。因此，我们可以将`Embarked`特征转换为三个虚拟变量（`'Embarked_S'`、`'Embarked_C'`和`'Embarked_Q'`），以便使用随机森林分类器。
- en: 'The following code will show you how to do this kind of transformation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将展示如何进行这种转换：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Factorizing
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因子化
- en: 'This approach is used to create a numerical categorical feature from any other
    feature. In pandas, the `factorize()` function does that. This type of transformation
    is useful if your feature is an alphanumeric categorical variable. In the Titanic
    data samples, we can transform the `Cabin` feature into a categorical feature,
    representing the letter of the cabin:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法用于从其他特征创建一个数值型分类特征。在pandas中，`factorize()`函数可以做到这一点。如果你的特征是字母数字的分类变量，那么这种转换就非常有用。在泰坦尼克号数据中，我们可以将`Cabin`特征转换为分类特征，表示舱位的字母：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can also apply transformations to quantitative features by using one of the
    following approaches.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过以下方法对定量特征应用转换。
- en: Scaling
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放
- en: This kind of transformation can be applied to numerical features only.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换仅适用于数值型特征。
- en: For example, in the Titanic data, the `Age` feature can reach 100, but the household
    income may be in millions. Some models are sensitive to the magnitude of values,
    so scaling such features will help those models perform better. Also, scaling
    can be used to squash a variable's values to be within a specific range.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在泰坦尼克号数据中，`Age`特征的值可能达到100，而家庭收入可能达到百万级别。一些模型对数值的大小比较敏感，因此对这些特征进行缩放可以帮助模型表现得更好。此外，缩放也可以将变量的值压缩到一个特定的范围内。
- en: 'The following code will scale the `Age` feature by removing its mean from each
    value and scale to the unit variance:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将通过从每个值中减去其均值并将其缩放到单位方差来缩放`Age`特征：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Binning
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分箱
- en: This kind of quantitative transformation is used to create quantiles. In this
    case, the quantitative feature values will be the transformed ordered variable.
    This approach is not a good choice for linear regression, but it might work well
    for learning algorithms that respond effectively when using ordered/categorical
    variables.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种量化转换用于创建分位数。在这种情况下，量化特征值将是转换后的有序变量。这种方法不适用于线性回归，但可能在使用有序/类别变量时，学习算法能有效地响应。
- en: 'The following code applies this kind of transformation to the `Fare` feature:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码对`Fare`特征应用了这种转换：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Derived features
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 派生特征
- en: In the previous section, we applied some transformations to the Titanic data
    in order to be able to use the random forest classifier of scikit-learn (which
    only accepts numerical data). In this section, we are going to define another
    type of variable, which is derived from one or more other features.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们对泰坦尼克号数据应用了一些转换，以便能够使用scikit-learn的随机森林分类器（该分类器只接受数值数据）。在本节中，我们将定义另一种变量类型，它是由一个或多个其他特征衍生出来的。
- en: Under this definition, we can say that some of the transformations in the previous
    section are also called **derived features**. In this section, we will look into
    other, complex transformations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个定义下，我们可以说上一节中的一些转换也叫做**派生特征**。在本节中，我们将研究其他复杂的转换。
- en: In the previous sections, we mentioned that you need to use your feature engineering
    skills to derive new features to enhance the model's predictive power. We have
    also talked about the importance of feature engineering in the data science pipeline
    and why you should spend most of your time and effort coming up with useful features.
    Domain knowledge will be very helpful in this section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们提到你需要运用特征工程技能来衍生新特征，以增强模型的预测能力。我们也谈到了特征工程在数据科学流程中的重要性，以及为什么你应该花费大部分时间和精力来提出有用的特征。在这一节中，领域知识将非常有帮助。
- en: Very simple examples of derived features will be something like extracting the
    country code and/or region code from a telephone number. You can also extract
    the country/region from the GPS coordinates.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单的派生特征的例子包括从电话号码中提取国家代码和/或地区代码。你还可以从GPS坐标中提取国家/地区信息。
- en: The Titanic data is a very simple one and doesn't contain a lot of variables
    to work with, but we can try to derive some features from the text feature that
    we have in it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号数据集非常简单，包含的变量不多，但我们可以尝试从文本特征中推导出一些新特征。
- en: Name
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 姓名
- en: 'The `name` variable by itself is useless for most datasets, but it has two
    useful properties. The first one is the length of your name. For example, the
    length of your name may reflect something about your status and hence your ability
    to get on a lifeboat:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`name`变量本身对大多数数据集来说是没有用的，但它有两个有用的属性。第一个是名字的长度。例如，名字的长度可能反映你的地位，从而影响你上救生艇的机会：'
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The second interesting property is the `Name` title, which can also be used
    to indicate status and/or gender:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个有趣的属性是`Name`标题，它也可以用来表示地位和/或性别：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can also try to come up with other interesting features from the `Name`
    feature. For example, you might think of using the last name feature to find out
    the size of family members on the Titanic ship.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试从`Name`特征中提出其他有趣的特征。例如，你可以使用姓氏特征来找出泰坦尼克号上家族成员的规模。
- en: Cabin
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 舱位
- en: 'In the Titanic data, the `Cabin` feature is represented by a letter, which
    indicates the deck, and a number, which indicates the room number. The room number
    increases towards the back of the boat, and this will provide some useful measure
    of the passenger''s location. We can also get the status of the passenger from
    the different decks, and this will help to determine who gets on the lifeboats:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在泰坦尼克号数据中，`Cabin`特征由一个字母表示甲板，和一个数字表示房间号。房间号随着船的后部增加，这将提供乘客位置的有用信息。我们还可以通过不同甲板上的乘客状态，帮助判断谁可以上救生艇：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Ticket
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 票
- en: 'The code of the `Ticket` feature is not immediately clear, but we can do some
    guesses and try to group them. After looking at the Ticket feature, you may get
    these clues:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ticket`特征的代码并不一目了然，但我们可以做一些猜测并尝试将它们分组。查看`Ticket`特征后，你可能会得到以下线索：'
- en: Almost a quarter of the tickets begin with a character while the rest consist
    of only numbers.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎四分之一的票以字符开头，而其余的票则仅由数字组成。
- en: The number part of the ticket code seems to have some indications about the
    class of the passenger. For example, numbers starting with 1 are usually first
    class tickets, 2 are usually second, and 3 are third. I say *usually* because
    it holds for the majority of examples, but not all. There are also ticket numbers
    starting with 4-9, and those are rare and almost exclusively third class.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车票代码中的数字部分似乎能提供一些关于乘客等级的提示。例如，以1开头的数字通常是头等舱票，2通常是二等舱，3是三等舱。我说*通常*是因为这适用于大多数情况，但并非所有情况。也有以4-9开头的票号，这些票号很少见，几乎完全是三等舱。
- en: Several people can share a ticket number, which might indicate a family or close
    friends traveling together and acting like a family.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几个人可以共享一个车票号码，这可能表示一家人或亲密的朋友一起旅行，并像一家人一样行动。
- en: 'The following code tries to analyze the ticket feature code to come up with
    preceding clues:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码尝试分析车票特征代码，以得出前述提示：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Interaction features
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交互特征
- en: 'Interaction features are obtained by performing mathematical operations on
    sets of features and indicate the effect of the relationship between variables.
    We use basic mathematical operations on the numerical features and see the effects
    of the relationship between variables:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 交互特征是通过对一组特征执行数学运算得到的，表示变量之间关系的影响。我们对数值特征进行基本的数学运算，观察变量之间关系的效果：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This kind of feature engineering can produce lots of features. In the preceding
    code snippet, we used 9 features to generate 176 interaction features.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特征工程可以生成大量特征。在前面的代码片段中，我们使用了9个特征来生成176个交互特征。
- en: 'We can also remove highly correlated features as the existence of these features
    won''t add any information to the model. We can use Spearman''s correlation to
    identify and remove highly correlated features. The Spearman method has a rank
    coefficient in its output that can be used to identity the highly correlated features:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以去除高度相关的特征，因为这些特征的存在不会为模型提供任何额外的信息。我们可以使用斯皮尔曼相关系数来识别和去除高度相关的特征。斯皮尔曼方法的输出中有一个秩系数，可以用来识别高度相关的特征：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The curse of dimensionality
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高维灾难
- en: In order to better explain the curse of dimensionality and the problem of overfitting,
    we are going to go through an example in which we have a set of images. Each image
    has a cat or a dog in it. So, we would like to build a model that can distinguish
    between the images with cats and the ones with dogs. Like the fish recognition
    system in *[Chapter 1](c6be0d67-2ba9-45ac-b6dd-116518853f42.xhtml)*, *Data science
    - Bird's-eye view*, we need to find an explanatory feature that the learning algorithm
    can use to distinguish between the two classes (cats and dogs). In this example,
    we can argue that color is a good descriptor to be used to differentiate between
    cats and dogs. So the average red, average blue, and average green colors can be
    used as explanatory features to distinguish between the two classes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地解释高维灾难和过拟合问题，我们将通过一个示例来说明，其中我们有一组图像。每张图像中都有一只猫或一只狗。所以，我们想建立一个模型，能够区分包含猫和包含狗的图像。就像在*[第1章](c6be0d67-2ba9-45ac-b6dd-116518853f42.xhtml)*中提到的鱼类识别系统，*数据科学
    - 鸟瞰图*，我们需要找到一个可以被学习算法用来区分这两类（猫和狗）的解释性特征。在这个示例中，我们可以认为颜色是一个很好的描述符，用来区分猫和狗。所以，平均红色、平均蓝色和平均绿色的颜色可以作为解释性特征，用来区分这两类。
- en: The algorithm will then combine these three features in some way to form a decision
    boundary between the two classes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 算法随后将以某种方式结合这三个特征，形成两个类别之间的决策边界。
- en: 'A simple linear combination of the three features can be something like the
    following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 三个特征的简单线性组合可能类似于以下形式：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'These descriptive features will not be enough to get a good performing classifie,
    so we can decide to add more features that will enhance the model predictivity
    to discriminate between cats and dogs. For example, we can consider adding some
    features such as the texture of the image by calculating the average edge or gradient
    intensity in both dimensions of the image, X and Y. After adding these two features,
    the model accuracy will improve. We can even make the model/classifier get more
    accurate classification power by adding more and more features that are based
    on color, texture histograms, statistical moments, and so on. We can easily add
    a few hundred of these features to enhance the model''s predictivity. But the
    counter-intuitive results will be worse after increasing the features beyond some
    limit. You''ll better understand this by looking at *Figure 1*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些描述性特征不足以获得一个性能良好的分类器，因此我们可以决定添加更多的特征，以增强模型的预测能力，从而区分猫和狗。例如，我们可以考虑通过计算图像在 X
    和 Y 两个维度上的平均边缘或梯度强度来添加图像的纹理特征。添加这两个特征后，模型的准确性将得到提升。我们甚至可以通过添加越来越多的基于颜色、纹理直方图、统计矩等特征，进一步提高模型的分类能力。我们可以轻松地添加几百个这些特征来增强模型的预测能力。但反直觉的结果是，当特征数量超过某个限度时，模型的性能反而会变差。你可以通过查看*图
    1*来更好地理解这一点：
- en: '![](img/c2fec23b-79b5-4122-bafc-90c2ce8c0df8.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2fec23b-79b5-4122-bafc-90c2ce8c0df8.png)'
- en: 'Figure 1: Model performance versus number of features'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：模型性能与特征数量的关系
- en: '*Figure 1* shows that as the number of features increases, the classifier''s
    performance increases as well, until we reach the optimal number of features.
    Adding more features based on the same size of the training set will then degrade
    the classifier''s performance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1* 显示了随着特征数量的增加，分类器的性能也在提升，直到我们达到最优特征数量。基于相同大小的训练集添加更多特征将会降低分类器的性能。'
- en: Avoiding the curse of dimensionality
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免维度灾难
- en: In the previous sections, we showed that the classifier's performance will decrease
    when the number of features exceeds a certain optimal point. In theory, if you
    have infinite training samples, the curse of dimensionality won't exist. So, the
    optimal number of features is totally dependent on the size of your data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们展示了当特征数量超过某个最优点时，分类器的性能将会下降。理论上，如果你有无限的训练样本，维度灾难将不存在。所以，最优的特征数量完全依赖于你的数据大小。
- en: An approach that will help you to avoid the harm of this curse is to subset
    *M* features from the large number of features *N*, where *M << N*. Each feature
    from *M* can be a combination of some features in *N*. There are some algorithms
    that can do this for you. These algorithms somehow try to find useful, uncorrelated,
    and linear combinations of the original *N* features. A commonly used technique
    for this is **principle component analysis** (**PCA**). PCA tries to find a smaller
    number of features that capture the largest variance of the original data. You
    can find more insights and a full explanation of PCA at this interesting blog: [http://www.visiondummy.com/2014/05/feature-extraction-using-pca/](http://www.visiondummy.com/2014/05/feature-extraction-using-pca/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这一“诅咒”的一种方法是从大量特征 *N* 中子集选择 *M* 个特征，其中 *M << N*。每个 *M* 中的特征可以是 *N* 中一些特征的组合。有一些算法可以为你完成这项工作。这些算法通过某种方式尝试找到原始
    *N* 特征的有用、无相关的线性组合。一个常用的技术是**主成分分析**（**PCA**）。PCA 试图找到较少数量的特征，这些特征能够捕捉原始数据的最大方差。你可以在这个有趣的博客中找到更多的见解和完整的
    PCA 解释：[http://www.visiondummy.com/2014/05/feature-extraction-using-pca/](http://www.visiondummy.com/2014/05/feature-extraction-using-pca/)。
- en: 'A useful and easy way to apply PCA over your original training features is
    by using the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单实用的方法来对原始训练特征应用 PCA 是使用以下代码：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the Titanic example, we tried to build the classifier with and without applying
    PCA on the original features. Because we used the random forest classifier at
    the end, we found that applying PCA isn't very helpful; random forest works very
    well without any feature transformations, and even correlated features don't really
    affect the model much.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在泰坦尼克号的示例中，我们尝试在原始特征上应用与不应用 PCA 来构建分类器。由于我们最终使用的是随机森林分类器，我们发现应用 PCA 并不是非常有帮助；随机森林在没有任何特征转换的情况下也能很好地工作，甚至相关的特征对模型的影响也不大。
- en: Titanic example revisited – all together
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泰坦尼克号示例回顾——整合在一起
- en: 'In this section, we are going to put all the bits and pieces of feature engineering
    and dimensionality reduction together:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把特征工程和维度减少的各个部分结合起来：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Bias-variance decomposition
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差分解
- en: In the previous section, we knew how to select the best hyperparameters for
    our model. This set of best hyperparameters was chosen based on the measure of
    minimizing the cross validated error. Now, we need to see how the model will perform
    over the unseen data, or the so-called out-of-sample data, which refers to new
    data samples that haven't been seen during the model training phase.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了如何为模型选择最佳超参数。这个最佳超参数集是基于最小化交叉验证误差的度量来选择的。现在，我们需要看看模型在未见过的数据上的表现，或者所谓的外部样本数据，指的是在模型训练阶段未曾见过的新数据样本。
- en: 'Consider the following example: we have a data sample of size 10,000, and we
    are going to train the same model with different train set sizes and plot the
    test error at each step. For example, we are going to take out 1,000 as a test
    set and use the other 9,000 for training. So for the first training round, we
    will randomly select a train set of size 100 out of those 9,000 items. We''ll
    train the model based on the *best* selected set of hyperparameters, test the
    model with the test set, and finally plot the train (in-sample) error and the
    test (out-of-sample) error. We repeat this training, testing, and plotting operation
    for different train sizes (for example, repeat with 500 out of the 9,000, then
    1,000 out of the 9,000, and so on).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下示例为例：我们有一个大小为10,000的数据样本，我们将使用不同的训练集大小训练相同的模型，并在每一步绘制测试误差。例如，我们将取出1,000作为测试集，剩余的9,000用于训练。那么，在第一次训练时，我们将从这9,000个样本中随机选择100个作为训练集。我们将基于*最佳*选择的超参数集来训练模型，使用测试集进行测试，最后绘制训练（内部样本）误差和测试（外部样本）误差。我们会为不同的训练集大小重复这个训练、测试和绘图操作（例如，使用9,000中的500个，然后是1,000个，依此类推）。
- en: After doing all this training, testing, and plotting, we will get a graph of
    two curves, representing the train and test errors with the same model but across
    different train set sizes. From this graph, we will get to know how good our model
    is.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行所有这些训练、测试和绘图后，我们将得到一个包含两条曲线的图，表示相同模型在不同训练集大小下的训练误差和测试误差。从这张图中，我们可以了解模型的表现如何。
- en: 'The output graph, which will contain two curves representing the training and
    testing error, will be one of the four possible shapes shown in *Figure 2*. The
    source of this different shapes is Andrew Ng''s Machine Learning course on Coursera
    ([https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)).
    It''s a great course with lots of insights and best practices for machine learning
    newbies:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图表将包含两条曲线，分别表示训练误差和测试误差，图表的形状可能是*图2*中展示的四种可能形状之一。这个不同的形状来源于Andrew Ng在Coursera上的机器学习课程（[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)）。这是一门非常棒的课程，适合机器学习初学者，课程内容充满了洞见和最佳实践：
- en: '![](img/9a88f3c0-8a00-4e65-816b-022e6450a072.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a88f3c0-8a00-4e65-816b-022e6450a072.png)'
- en: 'Figure 2: Possible shapes for plotting the training and testing error over
    different training set sizes'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在不同训练集大小下绘制训练误差和测试误差的可能形状
- en: 'So, when should we accept our model and put it into production? And when do
    we know that our model is not performing well over the test set and hence won''t
    have a bad generalization error? The answer to these questions depends on the
    shape that you get from plotting the train error versus the test error on different
    training set sizes:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们什么时候应该接受我们的模型并将其投入生产？我们又如何知道模型在测试集上表现不佳，从而不会产生较差的泛化误差呢？这些问题的答案取决于你从不同训练集大小中绘制训练误差与测试误差的图形所得到的形状：
- en: If your shape looks like the *top left* one, it represents a low training error
    and generalizes well over the test set. This shape is a winner and you should
    go ahead and use this model in production.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的图形类似于*左上*的那种，它代表了较低的训练误差，并且在测试集上有很好的泛化性能。这个形状是一个赢家，你应该继续使用这个模型并将其投入生产。
- en: If your shape is similar to the *top right* one, it represents a high training
    error (the model didn't manage to learn from the training samples) and even has
    worse generalization performance over the test set. This shape is a complete failure
    and you need to go back and see what's wrong with your data, chosen learning algorithm,
    and/or selected hyperparameters.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的图形类似于*右上*的那种，它代表了较高的训练误差（模型未能从训练样本中学习）并且在测试集上的泛化性能甚至更差。这个形状是完全失败的，你需要回过头来检查你的数据、选择的学习算法和/或选择的超参数是否存在问题。
- en: If your shape is similar to the *bottom left* one, it represents a bad training
    error as the model didn't manage to capture the underlying structure of the data,
    which also fits the new test data.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的形状类似于*左下方*的那个，它代表了一个较差的训练误差，因为模型没有成功地捕捉到数据的潜在结构，而这种结构也适用于新的测试数据。
- en: If your shape is similar to the *bottom right* one, it represents high bias
    and variance. This means that your model hasn't figured out the training data
    very well and hence didn't generalize well over the testing set.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的形状类似于*右下方*的那个，它代表了高偏差和高方差。这意味着你的模型没有很好地学习训练数据，因此无法很好地泛化到测试集上。
- en: Bias and variance are the components that we can use to figure out how good
    our model is. In supervised learning, there are two opposing sources of errors,
    and using the learning curves in *Figure 2*, we can figure out due to which component(s)
    our model is suffering. The problem of having high variance and low bias is called
    **overfitting**, which means that the model performed well over the training samples
    but didn't generalize well on the test set. On the other hand, the problem of
    having high bias and low variance is called **underfitting**, which means that
    the model didn't make use of the data and didn't manage to estimate the output/target
    from the input features. There are different approaches one can use to avoid getting
    into one of these problems. But usually, enhancing one of them will come at the
    expense of the second one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差是我们可以用来判断模型好坏的组成部分。在监督学习中，存在两个相互对立的误差来源，通过使用*图2*中的学习曲线，我们可以找出我们的模型是由于哪个（哪些）组成部分受到了影响。高方差和低偏差的问题叫做**过拟合**，这意味着模型在训练样本上表现良好，但在测试集上泛化不佳。另一方面，高偏差和低方差的问题叫做**欠拟合**，这意味着模型没有充分利用数据，也没能从输入特征中估计出输出/目标。我们可以采取不同的方法来避免这些问题，但通常，增强其中一个会以牺牲另一个为代价。
- en: We can solve the situation of high variance by adding more features from which
    the model can learn. This solution will most likely increase the bias, so you
    need to make some sort of trade-off between them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加更多特征来解决高方差问题，这些特征可以让模型学习。这个解决方案很可能会增加偏差，因此你需要在它们之间做出某种权衡。
- en: Learning visibility
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习可见性
- en: There are lots of great data science algorithms that one can use to solve problems
    in different domains, but the key component that makes the learning process visible
    is having enough data. You might ask how much data is needed for the learning
    process to be visible and worth doing. As a rule of thumb, researchers and machine
    learning practitioners agree that you need to have data samples at least 10 times
    the number of **degrees of freedom** in your model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多出色的数据科学算法可以用来解决不同领域的问题，但使学习过程可见的关键因素是拥有足够的数据。你可能会问，为了让学习过程变得可见且值得做，需要多少数据？作为经验法则，研究人员和机器学习从业者一致认为，你需要的数据样本至少是模型中**自由度**数量的10倍。
- en: For example, in the case of linear models, the degree of freedom represents
    the number of features that you have in your dataset. If you have 50 explanatory
    features in your data, then you need at least 500 data samples/observations in
    your data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在线性模型的情况下，自由度表示你数据集中所拥有的特征数量。如果你的数据中有50个解释性特征，那么你至少需要500个数据样本/观测值。
- en: Breaking the rule of thumb
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打破经验法则
- en: In practice, you can get away with this rule and do learning with less than
    10 times the number of features in your data; this mostly happens if your model
    is simple and you are using something called **regularization** (addressed in
    the next chapter).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，你可以打破这个规则，用少于数据中特征数10倍的数据进行学习；这通常发生在模型简单且使用了某种叫做**正则化**的方法（将在下一章讨论）。
- en: Jake Vanderplas wrote an article ([https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/](https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/))
    to show that one can learn even if the data has more parameters than examples.
    To demonstrate this, he used regularization.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Jake Vanderplas 写了一篇文章（[https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/](https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/)）来展示即使数据的参数比示例更多，依然可以进行学习。为了证明这一点，他使用了正则化方法。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the most important tools that machine learning practitioners
    use in order to make sense of their data and get the learning algorithm to get
    the most out of their data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习从业者用来理解数据并让学习算法从数据中获得最大收益的最重要工具。
- en: Feature engineering was the first and commonly used tool in data science; it's
    a must-have component in any data science pipeline. The purpose of this tool is
    to make better representations for your data and increase the predictive power
    of your model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是数据科学中第一个也是最常用的工具，它是任何数据科学管道中必不可少的组件。该工具的目的是为数据创建更好的表示，并提高模型的预测能力。
- en: We saw how a large number of features can be problematic and lead to worse classifier
    performance. We also saw that there is an optimal number of features that should
    be used to get the maximum model performance, and this optimal number of features
    is a function of the number of data samples/observations you got.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到大量特征可能会导致问题，并使分类器的表现变得更差。我们还发现，存在一个最佳特征数量，可以最大化模型的性能，而这个最佳特征数量是数据样本/观测值数量的函数。
- en: Subsequently, we introduced one of the most powerful tools, which is bias-variance
    decomposition. This tool is widely used to test how good the model is over the
    test set.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们介绍了最强大的工具之一——偏差-方差分解。这个工具广泛用于测试模型在测试集上的表现。
- en: Finally, we went through learning visibility, which answers the question of
    how much data we should need in order to get in business and do machine learning.
    The rule of thumb showed that we need data samples/observations at least 10 times
    the number of features in your data. However, this rule of thumb can be broken
    by using another tool called regularization, which will be addressed in more detail
    in the next chapter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讲解了学习可见性，它回答了一个问题：为了开展业务并进行机器学习，我们需要多少数据。经验法则表明，我们需要的样本/观测值至少是数据特征数量的10倍。然而，这个经验法则可以通过使用另一种工具——正则化来打破，正则化将在下一章详细讨论。
- en: Next up, we are going to continue to increase our data science tools that we
    can use to drive meaningful analytics from our data, and face some daily problems
    of applying machine learning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续增加我们可以使用的数据科学工具，以从数据中推动有意义的分析，并面对应用机器学习时的日常问题。
