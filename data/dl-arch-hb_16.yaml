- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Governing Deep Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型治理
- en: 'Deploying a model is just the beginning of its journey. Once it’s out in the
    real world, it’s like a living thing – it requires efficient use to make the most
    of it, upgrades to stay sharp, care to perform consistently well, and, eventually,
    a graceful exit. Imagine a car on the road: you start driving, but you also need
    to use the car effectively, fuel it, maintain it, and eventually replace it or
    its components. The same goes for deep learning models in action.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个模型只是它旅程的开始。一旦它进入现实世界，它就像一个活生生的生命体——它需要高效使用以最大化效益，需要升级以保持锐利，需要维护以保持一致性表现，最终，还需要优雅地退出。想象一下路上的一辆车：你开始驾驶，但你也需要有效使用它、加油、保养，最终更换它或它的组件。深度学习模型的运作也同样如此。
- en: Model governance acts as the guiding force that oversees the use of a model
    and maintains constant vigilance over its performance and context to ensure the
    continuous, consistent, and dependable delivery of value through the model. In
    the realm of deep learning, model governance is crucial for ensuring that these
    complex models adhere to the highest standards of quality, reliability, and fairness.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型治理作为引导力量，监督模型的使用，并始终保持对其性能和背景的警觉，确保模型能够持续、一致、可靠地传递价值。在深度学习领域，模型治理至关重要，它确保这些复杂的模型符合最高的质量、可靠性和公平性标准。
- en: 'This chapter delves into the three fundamental pillars of model governance
    for deep learning models: steering the ship of model utilization, which focuses
    on the appropriate application of deep learning models, keeping a watchful eye
    on its performance on all fronts with model monitoring, and ensuring it stays
    at its best in the ever-evolving landscape of deep learning with model maintenance.
    By implementing a robust model governance framework, deep learning architects
    can effectively manage the challenges posed by these intricate models and harness
    their immense potential to drive valuable insights and decisions in production.
    In this chapter, we will learn about these pillars of model governance in detail.
    *Figure 16**.1* shows a holistic view of the concept of model governance that
    we will explore in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了深度学习模型治理的三个基本支柱：引导模型利用的方向，着重于深度学习模型的合理应用；通过模型监控在各个方面保持对其性能的警觉；以及通过模型维护确保其在不断发展的深度学习领域保持最佳状态。通过实施健全的模型治理框架，深度学习架构师可以有效管理这些复杂模型所带来的挑战，并充分利用其巨大潜力，推动生产中的有价值的洞察和决策。本章将详细介绍这些模型治理的支柱。*图
    16.1* 展示了我们将在本章探讨的模型治理概念的整体视图：
- en: '![Figure 16.1 – Holistic overview of model governance in the context of concepts
    that will be introduced in this chapter](img/B18187_16_1.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.1 – 本章将介绍的概念背景下，模型治理的整体概述](img/B18187_16_1.jpg)'
- en: Figure 16.1 – Holistic overview of model governance in the context of concepts
    that will be introduced in this chapter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1 – 本章将介绍的概念背景下，模型治理的整体概述
- en: 'Specifically, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主题：
- en: Governing deep learning model utilization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型利用的治理
- en: Governing a deep learning model through monitoring
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过监控治理深度学习模型
- en: Governing a deep learning model through maintenance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过维护治理深度学习模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter covers a practical example of monitoring metrics and setting up
    alerts, leveraging the code from the previous tutorial in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models in Production*. This tutorial requires you to
    have a Linux machine with an NVIDIA GPU device ideally in Ubuntu with Python 3.10
    and the `nvidia-docker` tool installed. Additionally, we will require the following
    Python libraries to be installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了监控指标和设置警报的实际示例，利用前一教程中的代码，参考[*第15章*](B18187_15.xhtml#_idTextAnchor217)，*在生产中部署深度学习模型*。本教程要求你使用安装了
    Python 3.10 和 `nvidia-docker` 工具的 Ubuntu 系统，并且拥有一台配有 NVIDIA GPU 的 Linux 机器。此外，我们还需要安装以下
    Python 库：
- en: '`numpy`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`transformers==4.21.3`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers==4.21.3`'
- en: '`nvidia-tensorrt==8.4.1.5`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nvidia-tensorrt==8.4.1.5`'
- en: '`torch==1.12.0`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch==1.12.0`'
- en: '`transformers-deploy`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-deploy`'
- en: '`Tritonclient`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tritonclient`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可以在GitHub上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_16)。
- en: Governing deep learning model utilization
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型利用的治理
- en: 'Model utilization, the first pillar of model governance for deep learning models,
    is crucial for the responsible and ethical deployment of these sophisticated tools.
    In this section, we will explore the integral aspects of model utilization, including
    guardrail filters, accountability, compliance, validation, shared access, transparency,
    and decision support systems. By comprehensively addressing these aspects, deep
    learning architects can ensure effective model utilization that maximizes value
    from the model while mitigating potential risks and unintended consequences. Let’s
    dive deeper into these aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 模型利用，深度学习模型治理的第一支柱，对于这些复杂工具的负责任和道德部署至关重要。在本节中，我们将探讨模型利用的各个方面，包括护栏过滤器、问责制、合规性、验证、共享访问、透明度和决策支持系统。通过全面解决这些方面，深度学习架构师可以确保有效的模型利用，最大化模型的价值，同时减轻潜在的风险和意外后果。让我们深入了解这些方面：
- en: '**Guardrail filters**: These play a crucial role in ensuring that models operate
    within established boundaries, minimizing the risks associated with inaccurate
    or harmful predictions. These filters help maintain the original purpose of the
    models. While the objectives of using a model’s predictions can significantly
    vary based on individual use cases, several common types of guardrails are widely
    applicable:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**护栏过滤器**：这些过滤器在确保模型在既定边界内运行、最小化与不准确或有害预测相关的风险方面起着至关重要的作用。它们有助于保持模型的初衷。虽然使用模型预测的目标可以根据具体的使用案例大不相同，但一些常见类型的护栏在广泛应用中具有普适性：'
- en: '**Prevent harmful use on a per-prediction basis**: Harmful use of the model
    or its predictions can encompass a wide range of issues, including biases related
    to sensitive attributes, malicious attacks such as adversarial attacks, and harassment-related
    text generation. *Figure 16.2* shows the OpenAI ChatGPT’s way of displaying its
    predictions after the guardrail of harmful use has been triggered.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止有害使用每次预测**：模型或其预测的有害使用可能涉及广泛的问题，包括与敏感属性相关的偏见、恶意攻击（如对抗性攻击）和与骚扰相关的文本生成。*图16.2*展示了OpenAI
    ChatGPT在触发有害使用护栏后显示其预测的方式。'
- en: '![Figure 16.2 – OpenAI ChatGPT’s harmful use guardrail triggered response](img/B18187_16_2.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.2 – OpenAI ChatGPT的有害使用护栏触发响应](img/B18187_16_2.jpg)'
- en: Figure 16.2 – OpenAI ChatGPT’s harmful use guardrail triggered response
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2 – OpenAI ChatGPT的有害使用护栏触发响应
- en: '**Prevent usage of unconfident predictions on a per-prediction basis**: To
    maintain the reliability of a model’s output, it is essential to prevent the use
    of predictions with low confidence. The issue, however, is that regression model
    predictions do not have prediction values that could be treated as a confidence
    score. Additionally, although classification model prediction typically has a
    softmax operation applied to allow predictions to add up to 1, it is not properly
    calibrated toward actual statistical probabilities. *Conformal predictions* are
    a more battle-tested statistical and robust technique to provide a robust confidence
    interval for each prediction, allowing for a better understanding of the model’s
    certainty. Additionally, input data that goes out of training data bounds or has
    drifted may deteriorate the model’s performance and can be treated as a special
    case of unconfident predictions without even generating the predictions.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止使用低置信度的预测**：为了保持模型输出的可靠性，必须防止使用置信度低的预测。然而，问题在于回归模型的预测没有可以作为置信度得分的预测值。此外，尽管分类模型预测通常会应用softmax操作，使得预测值的和为1，但它并没有适当地校准为实际的统计概率。*一致性预测*是一种经过实战考验的统计和强大的技术，能够为每个预测提供一个稳健的置信区间，从而更好地理解模型的确定性。此外，超出训练数据边界或发生漂移的输入数据可能会降低模型的性能，甚至在没有生成预测的情况下将其视为低置信度预测的特殊情况。'
- en: '**Prevent the use of an inaccurate model**: By continuously monitoring and
    assessing a model’s accuracy performance, one can determine when to stop the usage
    of a model, especially in high-risk use cases, and proceed to perform model maintenance,
    which is to retrain and update the model.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止使用不准确的模型**：通过持续监控和评估模型的准确性表现，可以确定何时停止使用某个模型，特别是在高风险的使用场景下，并进行模型维护，即重新训练和更新模型。'
- en: '**Mitigating bias**: Guardrail filters can help minimize bias related to sensitive
    attributes, such as race, gender, or ethnicity. By preventing the model from producing
    predictions that may lead to discriminatory outcomes, guardrail filters contribute
    to a more equitable and fair application of these technologies.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少偏见**：护栏过滤器有助于最小化与敏感属性（如种族、性别或民族）相关的偏见。通过防止模型产生可能导致歧视性结果的预测，护栏过滤器有助于更公平、公正地应用这些技术。'
- en: '**Prevent known data conditions that can negatively affect the model’s performance**:
    For example, face recognition systems should only predict on frontal, unobstructed
    faces without masks or glasses. Adversarial performance analysis, introduced in
    *Chapter 14*, *Analyzing Adversarial Performance*, must be performed prior to
    deployment to identify the traits that could negatively affect the model’s performance.
    During deployment, appropriate thresholds of the identified traits that are estimated
    to deteriorate the model’s performance can be applied as a guardrail for prediction
    prevention.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止已知数据条件对模型性能产生负面影响**：例如，面部识别系统应仅对正面无遮挡且没有戴口罩或眼镜的面部进行预测。在*第14章* *分析对抗性表现*中介绍的对抗性表现分析必须在部署前进行，以识别可能对模型性能产生负面影响的特征。在部署过程中，可以对已识别的特征施加适当的阈值，这些特征预计会恶化模型的表现，并作为预测预防的护栏。'
- en: '**Implement human-in-the-loop oversight only for critical predictions**: In
    high-stakes scenarios, such as medical drug recommendations, it is vital to involve
    human experts in the decision-making process, and higher-level experts when specific
    predictions are made.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅对关键预测实施人工干预**：在高风险场景中，例如医学药物推荐，涉及决策过程的人工专家是至关重要的，且在进行特定预测时，需要更高层次的专家参与。'
- en: '**Accountability**: This entails the clear assignment of roles and responsibilities,
    and addresses questions related to model ownership, compliance with regulations,
    training data, and the approval process at each stage of development. Accountability
    is a critical aspect of AI and machine learning systems, ensuring that there is
    a clear understanding of roles, responsibilities, and ownership throughout the
    model’s life cycle. It encompasses the following two facets:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**：这涉及到角色和责任的明确分配，并解决与模型所有权、合规性、训练数据以及每个开发阶段的审批流程相关的问题。问责制是人工智能和机器学习系统的关键方面，确保在整个模型生命周期中，角色、责任和所有权有明确的理解。它包括以下两个方面：'
- en: '**Model ownership**: Clearly defining who owns the model is essential for establishing
    accountability. This includes determining the parties responsible for the model’s
    development, maintenance, and updates, as well as those who will be held liable
    for any adverse consequences resulting from the model’s use. Some additional key
    considerations related to model ownership are the following:'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型所有权**：明确模型的所有者对于建立问责制至关重要。这包括确定负责模型开发、维护和更新的各方，以及那些因模型使用而可能产生不良后果的责任方。与模型所有权相关的其他关键考虑因素包括：'
- en: '**Handling personnel changes**: In the event of a model owner’s departure or
    role change within the organization, a well-defined process should be in place
    to transfer ownership and responsibilities to another suitable individual or team.
    This ensures that the model continues to receive proper oversight and maintenance
    and that accountability remains clear.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理人员变动**：当模型所有者离职或角色变动时，应该有一个明确定义的流程，将所有权和责任转交给另一位合适的个人或团队。这确保了模型能够继续得到适当的监督和维护，且问责制得以清晰保持。'
- en: '**Shared access and default admin roles**: To promote effective model governance
    and minimize potential disruptions, it is vital to establish shared access and
    default admin roles. This allows multiple team members to oversee the model’s
    development, maintenance, and updates, reducing the dependency on a single individual.
    Such shared access should be accompanied by clear guidelines on roles and responsibilities
    to avoid confusion and maintain accountability.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享访问和默认管理员角色**：为了促进有效的模型治理并减少潜在的中断，建立共享访问和默认管理员角色至关重要。这允许多个团队成员监督模型的开发、维护和更新，从而减少对单个个体的依赖。此类共享访问应配备明确的角色和责任指南，以避免混乱并保持问责制。'
- en: '**Handling open source models**: Proprietary models are typically developed
    within a single organization, which makes it straightforward to deal with accountability
    with the considerations discussed previously. In open source models, the development
    process often involves multiple contributors from diverse backgrounds, which makes
    establishing accountability more challenging. To address this, it is essential
    to provide clear guidelines for contributions, maintain transparent documentation
    of the model’s development history, and implement community-driven governance
    structures or assign a core group of maintainers to oversee the project. As an
    alternative, a key model owner can be established in an organization that assumes
    all responsibility for using the open source model in that organization.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理开源模型**：专有模型通常在单一组织内开发，这使得处理问责制变得简单，遵循前述的相关考量。而在开源模型中，开发过程通常涉及来自不同背景的多个贡献者，这使得建立问责制变得更加具有挑战性。为了解决这个问题，必须提供明确的贡献指南，保持模型开发历史的透明文档，并实施社区驱动的治理结构，或者指定一组核心维护者来监督该项目。作为替代方案，可以在组织中确立一个主要模型负责人，负责该组织中开源模型的所有使用责任。'
- en: '**Predictions ownership**: Predictions ownership in high-risk use cases is
    crucial for maintaining accountability and ensuring accurate, reliable outcomes.
    Since raw predictions may not always be easily understandable, post-processing
    steps are often needed to convert them into more digestible insights or nested
    outcomes. Approvals of the outcomes at each post-processing stage further ensure
    the quality and relevance of the final outcomes, fostering the responsible and
    effective use of AI and machine learning models.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测责任**：在高风险应用场景中，预测责任对于保持问责制和确保准确、可靠的结果至关重要。由于原始预测结果可能并不总是容易理解，因此通常需要后处理步骤将其转化为更易于理解的洞察或嵌套结果。在每个后处理阶段对结果的批准进一步确保了最终结果的质量和相关性，从而促进了AI和机器学习模型的负责任和有效使用。'
- en: '**Model and prediction transparency**: This is essential for fostering trust
    and understanding in AI systems. This entails offering clear explanations and
    relevant information about the model’s development, including its architecture,
    training data, and methodology. Providing such insights enables users to grasp
    how the model generates predictions and ensures that the AI system aligns with
    ethical and responsible practices, ultimately contributing to better decision-making
    and more reliable outcomes. These can be the same explanations that were used
    to understand and compare different models and predictions during the model development
    stage.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型与预测透明度**：这对于促进对AI系统的信任和理解至关重要。这意味着要提供关于模型开发的清晰解释和相关信息，包括其架构、训练数据和方法论。提供这些洞察使用户能够理解模型如何生成预测，并确保AI系统与道德和负责任的实践一致，最终有助于更好的决策和更可靠的结果。这些解释可以是模型开发阶段用于理解和比较不同模型和预测的相同内容。'
- en: '**Decision support systems**: This involves building interfaces or platforms
    that enable decision-makers to interact with model predictions and insights. This
    includes providing user-friendly dashboards, reports, and visualization tools
    in the system while incorporating business rules, regulations, and policies into
    the decision-making process. This is again useful in high-risk use cases.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策支持系统**：这包括构建能够让决策者与模型预测和洞察互动的界面或平台。系统中应提供用户友好的仪表板、报告和可视化工具，同时将业务规则、法规和政策纳入决策过程中。这在高风险应用场景中再次具有重要作用。'
- en: Now, we will dive into the second component of model governance, which is about
    monitoring deployed models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨模型治理的第二个组成部分——监控已部署的模型。
- en: Governing a deep learning model through monitoring
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过监控来管理深度学习模型
- en: Model monitoring is essential for maintaining the performance, reliability,
    and fairness of deep learning models throughout their life cycle. As data landscapes
    and business requirements evolve, continuous monitoring enables the early detection
    of issues such as model drift, performance degradation, and potential biases,
    thereby ensuring the consistent delivery of accurate and valuable predictions.
    This process involves the collection and analysis of key performance metrics,
    the ongoing evaluation of model outputs against ground-truth data, and the identification
    of any emerging trends that could impact the model’s efficacy. By implementing
    a robust model monitoring framework, deep learning architects can proactively
    address challenges and make informed decisions about model updates, refinements,
    and retraining, ultimately optimizing the model’s value and mitigating risks associated
    with its deployment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控对于维护深度学习模型在整个生命周期中的性能、可靠性和公平性至关重要。随着数据环境和业务需求的变化，持续监控可以帮助及早发现问题，如模型漂移、性能下降和潜在的偏差，从而确保持续交付准确且有价值的预测。这个过程包括收集和分析关键性能指标，持续评估模型输出与真实数据的对比，并识别任何可能影响模型效能的新兴趋势。通过实施一个强大的模型监控框架，深度学习架构师可以主动应对挑战，并做出有关模型更新、优化和再训练的明智决策，从而最大化模型的价值并减少与模型部署相关的风险。
- en: 'Model monitoring holds value only when it results in corrective actions addressing
    deteriorating performance or concerning conditions. Thus, the objective of monitoring
    should be to identify and rectify undesirable behavior. The actions that can be
    taken are more broadly grouped into the third pillar of model governance, called
    model maintenance, which we will discuss separately in the next section. Now,
    let’s delve into the various categories and specific metrics for a deployed machine
    learning model, accompanied by examples of conditions that can prompt the initiation
    of model maintenance procedures:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控只有在能够采取纠正措施来应对性能下降或不良情况时才具有价值。因此，监控的目标应该是识别和纠正不良行为。可以采取的措施更广泛地归入模型治理的第三支柱——模型维护，我们将在下一节单独讨论。现在，让我们深入探讨已部署机器学习模型的各种类别和具体指标，并举例说明哪些条件可能促使启动模型维护程序：
- en: '**Model accuracy-based performance metrics**: These are the typical model evaluation
    metrics we introduced more comprehensively in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161),
    *Exploring Model Evaluation Methods*, such as accuracy, recall, precision, F1
    score, AUC-ROC, and log-loss. The same metrics that were used for model evaluation
    in the model development and delivery model insights stage should be reused here.
    These metrics can be monitored when the true labels can be obtained at a future
    time, in two ways:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型准确性的性能指标**：这些是我们在[*第10章*](B18187_10.xhtml#_idTextAnchor161)《探索模型评估方法》中更全面介绍的典型模型评估指标，如准确率、召回率、精确度、F1分数、AUC-ROC和对数损失。在模型开发和交付模型洞察阶段使用的相同评估指标应在此重用。当在未来能够获得真实标签时，这些指标可以通过两种方式进行监控：'
- en: '**Naturally**: When the use case is a time-series use case to predict a future
    target or the target is just not immediately accessible to the model owner, the
    targets can be obtained in the future naturally'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然情况**：当使用场景是时间序列预测未来目标，或者目标对模型拥有者并非立即可得时，目标可以在未来自然获得。'
- en: '**Manual labeling**: Labeling is recommended to be carried out in a regular
    cadence with a sample of the historical production input data to verify the validity
    of the model performance'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动标注**：建议定期进行标注，选取一部分历史生产输入数据样本来验证模型性能的有效性。'
- en: Conditions that can cause a trigger of model maintenance here are using the
    same use case validity thresholds that were referred to in the model building
    and evaluation experimentation process. As emphasized in *Chapter 10*, *Exploring
    Model Evaluation Methods*, this threshold should ideally be tied to the business
    metrics threshold in some way.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导致模型维护触发的条件是使用在模型构建和评估实验过程中提到的相同使用场景有效性阈值。正如在*第10章*《探索模型评估方法》中强调的那样，这个阈值理想情况下应该与业务指标阈值有所关联。
- en: '**Data quality metrics**: Data quality metrics provide essential insights into
    the validity, characteristics, and consistency of the input data. Data quality
    is linked to the accuracy and bias performance of the model and thus any deviations
    from the norm can potentially cause accuracy degradations. Examples of such metrics
    are the following:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量度量**：数据质量度量提供了关于输入数据的有效性、特征和一致性的基本见解。数据质量与模型的准确性和偏差表现相关，因此任何偏离常规的情况都可能导致准确性的下降。以下是一些此类度量的示例：'
- en: '**Missing or incomplete data count**: This refers to the number of instances
    in the dataset where the data is either absent or not fully available. This can
    impact the accuracy and reliability of the model, as it may not have enough information
    to infer a prediction from.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失或不完整数据计数**：指数据集中数据缺失或不完全可用的实例数量。这会影响模型的准确性和可靠性，因为模型可能没有足够的信息来进行预测。'
- en: '**Invalid data bounds count**: This refers to the instances where data values
    fall outside the acceptable or expected range. This can lead to incorrect model
    predictions, as the model may infer from incorrect data points that were not learned
    from.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无效数据边界计数**：指的是数据值超出可接受或预期范围的实例。这可能导致模型预测错误，因为模型可能从不正确的数据点中推断，而这些数据点并没有经过学习。'
- en: '**Outlier and anomaly indicator metrics**: They are used to identify unusual
    or extreme data points that deviate significantly from the overall pattern or
    trend in the dataset. This has the same root cause as invalid data bounds.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值和离群点指标度量**：用于识别与数据集中的整体模式或趋势显著偏离的异常或极端数据点。这与无效数据边界有相同的根本原因。'
- en: '**Data drift**: This occurs when the distribution of input features in the
    data changes over time. This can happen due to various reasons, such as evolving
    data sources, changing user behavior, or external factors influencing the data
    generation process. Data drift may lead to a decline in model performance as the
    model was trained on a different distribution of data and may not generalize well
    to the new distribution. Monitoring for data drift helps in identifying when retraining
    or adjusting the model is necessary to maintain its accuracy and effectiveness.
    In [*Chapter 17*](B18187_17.xhtml#_idTextAnchor247), *Managing Drift Effectively
    in a Dynamic Environment*, we will dive into the techniques that we can use to
    detect data drift focused on deep learning-specific data inputs.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移**：当数据中输入特征的分布随时间变化时，就会发生数据漂移。这可能由于多种原因，例如数据源的变化、用户行为的变化，或外部因素影响数据生成过程。数据漂移可能导致模型性能下降，因为模型是在不同数据分布上训练的，可能无法很好地适应新的数据分布。监控数据漂移有助于识别何时需要重新训练或调整模型，以保持其准确性和有效性。在[*第17章*](B18187_17.xhtml#_idTextAnchor247)，*在动态环境中有效管理漂移效应*，我们将深入探讨用于检测数据漂移的技术，重点是深度学习特定数据输入。'
- en: '**Concept drift**: It refers to the change in the relationship between input
    features and the target variable over time. This change can cause a previously
    accurate model to degrade in performance as the model’s learned patterns no longer
    align with the evolving relationships. This is also related to the label consistency
    metric introduced in the data quality section in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015),
    *Deep Learning* *Life Cycle*.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念漂移**：指的是随着时间的推移，输入特征与目标变量之间关系的变化。这种变化可能导致一个原本准确的模型在性能上退化，因为模型所学的模式不再与不断变化的关系一致。这也与数据质量部分中介绍的标签一致性度量相关，详见[*第一章*](B18187_01.xhtml#_idTextAnchor015)，*深度学习生命周期*。'
- en: '**System performance metrics**: These metrics help ensure that the deployed
    model meets the operational requirements. The key subgroups under system performance
    metrics are the following:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统性能度量**：这些度量有助于确保部署的模型满足操作要求。系统性能度量下的主要子组如下：'
- en: '**Inference latency**: Refers to the measurement of the time taken by the model
    to generate predictions or output from the input data. Low latency is crucial
    for real-time applications and user experiences, as it ensures the model provides
    quick and timely results.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理延迟**：指的是模型从输入数据生成预测或输出所需的时间。低延迟对于实时应用和用户体验至关重要，因为它确保模型提供快速及时的结果。'
- en: '**Throughput**: Measures the number of predictions or outputs the model can
    generate within a specific time frame. High throughput is vital for handling large-scale
    data processing and maintaining the desired level of performance, especially in
    high-demand scenarios.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**吞吐量**：衡量模型在特定时间范围内能够生成的预测或输出的数量。高吞吐量对于处理大规模数据并保持所需的性能水平至关重要，尤其是在高需求场景中。'
- en: '**Resource utilization**: Evaluates the efficiency of resource usage, such
    as CPU, memory, and storage, by the model during its operation. Optimizing resource
    utilization ensures that the model can run efficiently on the available infrastructure,
    reducing costs and allowing for better scalability.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源利用率**：评估模型在运行过程中对资源（如CPU、内存和存储）的使用效率。优化资源利用率可以确保模型在可用的基础设施上高效运行，降低成本并提升可扩展性。'
- en: '**Queueing delay and request counts**: Queueing delay refers to the waiting
    time experienced by each request before being processed by the deployed deep learning
    model. Monitoring the queueing delay and the number of requests can help identify
    potential bottlenecks in the system and optimize the model’s capacity to handle
    multiple requests simultaneously.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排队延迟和请求数量**：排队延迟指的是每个请求在被部署的深度学习模型处理前所经历的等待时间。监控排队延迟和请求数量有助于识别系统中的潜在瓶颈，并优化模型同时处理多个请求的能力。'
- en: '**Alert and incident metrics**: These metrics help ensure timely identification
    and resolution of problems, enabling optimal system performance. They are as follows:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报和事件指标**：这些指标有助于确保及时识别和解决问题，从而实现系统性能的最佳化。它们如下：'
- en: '**Alert frequency**: This metric refers to the number of alerts generated over
    a specific time period, indicating potential issues or anomalies in the system.
    Monitoring alert frequency helps identify patterns and trends, enabling proactive
    measures to prevent or mitigate recurring problems.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报频率**：此指标指的是在特定时间段内生成的警报数量，表示系统中的潜在问题或异常。监控警报频率有助于识别模式和趋势，从而采取主动措施防止或缓解重复出现的问题。'
- en: '**Alert severity**: This measures the degree of impact an issue has on overall
    system performance. By categorizing alerts based on severity, it is possible to
    prioritize and address the most critical issues first, ensuring efficient use
    of resources and minimizing negative impacts on the system.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报严重性**：这衡量了一个问题对整体系统性能的影响程度。通过根据严重性对警报进行分类，可以优先处理最关键的问题，从而确保资源的高效利用，并最大程度地减少对系统的负面影响。'
- en: '**Incident resolution time**: This is the time taken to address and resolve
    incidents arising from alerts. Tracking this metric helps evaluate the effectiveness
    of the incident response process and identify areas for improvement, ultimately
    leading to faster resolution times and better system performance.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件解决时间**：这是处理和解决由警报引发的事件所花费的时间。跟踪这个指标有助于评估事件响应过程的有效性，并识别需要改进的地方，最终实现更快的解决时间和更好的系统性能。'
- en: '**Model fairness and bias metrics**: The same metrics that were introduced
    in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    to compare different models in development, can also be applied to monitor model
    fairness on a deployed model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型公平性和偏差指标**：在[**第13章**](B18187_13.xhtml#_idTextAnchor196)中介绍的用于比较不同开发模型的相同指标，也可以应用于监控已部署模型的公平性。'
- en: '**Business metrics**: Monitoring business-related metrics is crucial for evaluating
    the impact of a deployed deep learning model on the organization’s goals and ensuring
    its alignment with business objectives. Not everything can be monitored with numbers,
    so figure out the components that are quantifiable. Here are some metrics to consider:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务指标**：监控与业务相关的指标对于评估部署的深度学习模型对组织目标的影响至关重要，并确保其与业务目标的对齐。并非所有东西都能通过数字来监控，因此要找出那些可以量化的组件。以下是一些需要考虑的指标：'
- en: '**Key Performance Indicators** (**KPIs**): Identify and track KPIs that are
    directly influenced by the model’s predictions, such as revenue, customer satisfaction,
    return on investment, or operational efficiency. This helps assess the model’s
    overall contribution to the business.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键绩效指标**（**KPIs**）：识别并跟踪受模型预测直接影响的关键绩效指标，例如收入、客户满意度、投资回报率或运营效率。这有助于评估模型对业务的整体贡献。'
- en: '**User adoption and engagement**: Monitor how users interact with the model,
    including usage patterns, frequency, and feedback. This can provide insights into
    the model’s relevance, ease of use, and overall effectiveness in addressing user
    needs.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户采纳与参与度**：监控用户与模型的互动方式，包括使用模式、频率和反馈。这可以为模型的相关性、易用性及其在满足用户需求方面的整体效果提供洞察。'
- en: Incorporating the monitoring of the various metric groups not only provides
    a comprehensive view of deep learning model performance, reliability, and fairness
    but also facilitates the identification of emerging trends and patterns. By closely
    monitoring these metrics, potential issues, such as model drift, performance degradation,
    and biases, can be proactively addressed, ensuring consistent delivery of accurate
    predictions. This also means that analyzing patterns from the monitored metrics
    is crucial in developing improvement plans to enhance deep learning model performance
    and address any potential issues.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监控各种指标组的变化，不仅可以全面了解深度学习模型的性能、可靠性和公平性，还有助于识别新兴趋势和模式。通过密切监控这些指标，可以积极解决潜在问题，如模型漂移、性能下降和偏见，确保准确预测的持续交付。这也意味着分析监控指标中的模式对于制定改进计划以提升深度学习模型性能和解决潜在问题至关重要。
- en: To effectively analyze and consume the metrics that are monitored, it is recommended
    to consolidate the key metrics in a comprehensive dashboard, which allows for
    easy tracking and assessment of the model’s overall health, and ultimately enhances
    the monitoring process. Grafana, a popular open source analytics and monitoring
    platform, can effectively meet these requirements by offering a variety of features
    and integrations. As we move forward, we will explore a practical tutorial on
    monitoring deep learning models by using NVIDIA Triton Inference Server, Prometheus,
    and Grafana.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为有效分析和利用所监控的指标，建议将关键指标整合到一个全面的仪表板中，以便轻松跟踪和评估模型的整体健康状况，并最终增强监控过程。Grafana，一个流行的开源分析和监控平台，可以通过提供各种功能和集成有效地满足这些要求。随着我们的进展，我们将探讨如何通过使用NVIDIA
    Triton推理服务器、Prometheus和Grafana来监控深度学习模型的实用教程。
- en: Monitoring a deployed deep learning model with NVIDIA Triton Server, Prometheus,
    and Grafana
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NVIDIA Triton Server、Prometheus和Grafana监控部署的深度学习模型
- en: NVIDIA Triton Server hosts configured metrics via a REST HTTP API in Prometheus
    format, offering real-time insights without persisting historical data. To persist
    metrics data over time, Prometheus needs to be configured to connect with NVIDIA
    Triton Server. While Prometheus tracks and logs metrics over time, it lacks visualization
    capabilities. This is where Grafana comes in. It’s a platform that can leverage
    Prometheus-logged data to create dynamic dashboards with custom graphs and tables.
    Prometheus conveniently shares its logged information through a separate REST
    HTTP API, facilitating Grafana’s seamless connectivity. Additionally, Grafana
    allows alert rules to be set up reliably.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA Triton Server通过Prometheus格式的REST HTTP API托管配置的指标，提供实时洞察，无需保留历史数据。要持续保存指标数据，需要配置Prometheus与NVIDIA
    Triton Server连接。虽然Prometheus可以随时间跟踪和记录指标，但它缺乏可视化能力。这就是Grafana的用武之地。它是一个平台，可以利用Prometheus记录的数据创建具有自定义图形和表格的动态仪表板。Prometheus通过单独的REST
    HTTP API方便地共享其记录的信息，从而实现与Grafana的无缝连接。此外，Grafana允许可靠地设置警报规则。
- en: The first step in monitoring is to plan the metrics that we want to monitor,
    which we will discuss next.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的第一步是计划我们想要监控的指标，我们将在接下来讨论。
- en: Choosing metrics to monitor
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择要监控的指标
- en: 'Any deployed model hosted through NVIDIA Triton Server will by default support
    a variety of standard metrics. These metrics are the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过NVIDIA Triton Server托管的任何部署模型默认支持各种标准指标。这些指标包括以下内容：
- en: '**Inference request metrics**: Success count, failure count, inference count,
    and execution count'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理请求指标**：成功次数、失败次数、推理次数和执行次数'
- en: '**GPU-related metrics**: Power usage, power limit, energy consumption, GPU
    utilization, GPU total memory, and GPU used memory'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与GPU相关的指标**：功率使用、功率限制、能量消耗、GPU利用率、GPU总内存和GPU已使用内存'
- en: '**CPU-related metrics**: CPU utilization, CPU total memory, and CPU used memory'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与CPU相关的指标**：CPU利用率、CPU总内存和CPU已使用内存'
- en: '**Response cache metrics**: Cache hit count, cache miss count, cache hit time,
    and cache miss time'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应缓存指标**：缓存命中次数、缓存未命中次数、缓存命中时间和缓存未命中时间'
- en: 'Note that these metrics can be manually disabled. In this practical example,
    we will be leveraging the deployed language model implementation from the previous
    chapter in using NVIDIA Triton Server and additionally using the Prometheus and
    Grafana tools. The default standard metrics that NVIDIA Triton Server logs are
    useful, but we also need potential custom metrics that can be useful for a business
    and are specific to a language model. It is well documented that NVIDIA Triton
    Server supports custom metrics through their C API, which means you need to develop
    C code! However, a fairly new way to support custom metrics, since NVIDIA Triton
    Server version 23.05, is that you can define custom metrics for NVIDIA Triton
    Server using Python! We will be exploring this new feature in our practical tutorial,
    where we will be exploring the following custom metrics for a language model that
    can be useful:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些指标可以手动禁用。在这个实际示例中，我们将利用上一章中使用 NVIDIA Triton Server 部署的语言模型实现，并额外使用 Prometheus
    和 Grafana 工具。NVIDIA Triton Server 记录的默认标准指标很有用，但我们还需要一些可能对业务有用且特定于语言模型的自定义指标。NVIDIA
    Triton Server 支持通过 C API 自定义指标，这意味着你需要开发 C 代码！然而，自从 NVIDIA Triton Server 版本 23.05
    以来，有一种相对较新的方式支持自定义指标，那就是你可以使用 Python 为 NVIDIA Triton Server 定义自定义指标！我们将在我们的实际教程中探索这一新特性，以下是我们为语言模型探索的一些自定义指标，这些指标可能会有用：
- en: '**Number of tokens processed**: The larger the input data, the longer a request
    can take'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理的令牌数量**：输入数据越大，请求的时间可能会越长。'
- en: '**Number of tokens generated**: The larger the number of output tokens, the
    longer a request can take'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成的令牌数量**：输出令牌数量越大，请求的时间可能会越长。'
- en: '**Flesch reading score**: This is a reading comprehension metric that measures
    how well a text can be understood, which can be a useful business metric, as generated
    text needs to be well understood to be useful'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flesch 阅读分数**：这是一个阅读理解指标，用来衡量文本的易读性，它是一个有用的业务指标，因为生成的文本需要易于理解，才能发挥其作用。'
- en: Now we are ready to dive into the practical example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好深入实践示例了。
- en: Tracking and visualizing the chosen metrics over time
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪和可视化所选指标随时间的变化
- en: 'Before we start, make sure you have installed `nvidia-docker`, Prometheus,
    Node Exporter, and Grafana version v10.0.3\. Also, make sure Prometheus and Grafana
    are callable from any location in the command line. Let’s start the process in
    a step-by-step manner, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保你已经安装了 `nvidia-docker`、Prometheus、Node Exporter 和 Grafana 版本 v10.0.3。另外，确保
    Prometheus 和 Grafana 可以从命令行的任何位置调用。我们将按照以下步骤逐步开始这个过程：
- en: Note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are leveraging the code from [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models to Production*. The first change that is needed
    here is that we will make changes on top of `TritonPythonModel` in the `model.py`
    file. The Custom Metrics API in Python from NVIDIA allows you to define and log
    metrics directly in the three methods that you can define in `TritonPythonModel`.
    These methods are `initialize`, `execute`, and `finalize`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在利用来自[*第15章*](B18187_15.xhtml#_idTextAnchor217)的代码，*将深度学习模型部署到生产环境*。这里需要的第一个更改是，我们将在`model.py`文件中的`TritonPythonModel`基础上进行修改。NVIDIA
    提供的 Python 自定义指标 API 允许你在`TritonPythonModel`中定义和记录指标，直接在你可以定义的三个方法中进行：`initialize`、`execute`和`finalize`。
- en: 'Firstly, the additional libraries that we will use are `textstat` and `nltk`,
    which will be used to compute the readability score:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用的附加库是`textstat`和`nltk`，这些将用于计算可读性分数：
- en: '[PRE0]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first step is to initialize the metric logging instance in the `initialize`
    method. Prometheus supports four metric types: counters for increasing values,
    gauges for fluctuating values, histograms for observing value distribution, and
    summaries for tracking quantiles in data. The three custom metrics we plan to
    add are inherently fluctuating values, and any histograms can be created in Grafana.
    Let’s define the metric family that we will use. You can set the name, description,
    and type of metric for a family. Additionally, you can create many metric families
    for any metric logical group. For our case, all three metrics we plan for are
    business metrics and are fluctuating values:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是在`initialize`方法中初始化指标日志记录实例。Prometheus 支持四种指标类型：计数器（用于增加的值）、仪表（用于波动的值）、直方图（用于观察值的分布）和摘要（用于跟踪数据中的分位数）。我们计划添加的三个自定义指标本质上是波动的值，任何直方图都可以在
    Grafana 中创建。让我们定义我们将使用的指标系列。你可以为每个指标系列设置名称、描述和类型。此外，你可以为任何逻辑组创建多个指标系列。就我们的情况而言，所有三个我们计划的指标都是业务指标，且是波动的值：
- en: '[PRE1]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s define the metrics in this metric family:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在这个指标系列中定义指标：
- en: '[PRE2]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Conveniently, you can do versioning of the metric in case any logic needs to
    be changed, which makes for a more robust monitoring process.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方便的是，如果任何逻辑需要更改，您可以对指标进行版本控制，这使得监控过程更为稳健。
- en: 'Next, we will be logging the metrics in every execution. We will be defining
    a helper method that will in turn be executed at the end of the `execute` method:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在每次执行时记录指标。我们将定义一个辅助方法，随后将在 `execute` 方法结束时执行：
- en: '[PRE3]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we will use this helper method under the `execute` method:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将在 `execute` 方法下使用这个辅助方法：
- en: '[PRE4]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we need to start the NVIDIA Triton Server `nvidia-docker` instance with
    the same command, which is the following:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要使用相同的命令启动 NVIDIA Triton Server `nvidia-docker` 实例，命令如下：
- en: '[PRE5]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After that, we are in the Docker environment, where the next step is to install
    the necessary libraries:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们进入 Docker 环境，接下来的步骤是安装必要的库：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For production usage, please be sure to create a Docker image where all the
    libraries are fixed, and you don’t need to manually install libraries anymore.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产环境，请务必创建一个 Docker 镜像，其中所有库都已经固定，这样您就不再需要手动安装库。
- en: Now, you can execute the `python triton_client.py` command in the command line
    and get your predictions.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以在命令行中执行 `python triton_client.py` 命令并获取预测结果。
- en: 'The default metrics and the custom metrics are immediately hosted in the URL
    `http://localhost:8002/metrics`, where you can view the real-time metrics in text
    form. `localhost` can be replaced with the IP of your remote server if you are
    using one. The following snippet shows the real-time Prometheus-formatted metrics
    that can be found at the preceding URL:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认指标和自定义指标立即托管在 URL `http://localhost:8002/metrics` 中，您可以查看实时的文本格式指标。如果您使用的是远程服务器，可以将
    `localhost` 替换为服务器的 IP 地址。以下代码片段展示了可以在前述 URL 中找到的实时 Prometheus 格式的指标：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As these are only real-time metrics, we need to set up a local server or use
    an online Prometheus server. In this step, we will opt for a locally hosted Prometheus
    server where the following commands need to be run in the command line:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这些只是实时指标，我们需要设置本地服务器或使用在线 Prometheus 服务器。在此步骤中，我们将选择本地托管的 Prometheus 服务器，需要在命令行中运行以下命令：
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we need to add the NVIDIA Triton Server endpoint into the Prometheus configuration
    file to track metrics. To do that, execute `sudo gedit /etc/prometheus/prometheus.yml`
    in the command line and add the following job details:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要将 NVIDIA Triton Server 端点添加到 Prometheus 配置文件中，以便跟踪指标。为此，请在命令行中执行 `sudo
    gedit /etc/prometheus/prometheus.yml`，然后添加以下作业详情：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With that, Prometheus is all set up to log metrics from NVIDIA Triton Server.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样，Prometheus 就可以从 NVIDIA Triton Server 记录指标了。
- en: Prometheus hosts its web app by default with port `9090`. So, accessing the
    link `localhost:9090` in a web browser will take you to the Prometheus home page.
    Going to the **Status** tab and clicking on **Targets** in the dropdown will show
    the following screenshot, which verifies that the Triton endpoint is being tracked.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prometheus 默认使用端口 `9090` 托管其 Web 应用。因此，通过浏览器访问链接 `localhost:9090` 将进入 Prometheus
    首页。进入 **状态** 标签，点击下拉菜单中的 **目标**，将显示以下截图，验证 Triton 端点正在被跟踪。
- en: '![Figure 16.3 – Prometheus web app home page on the left and targets that Prometheus
    is tracking and polling metrics from on the right](img/B18187_16_3.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.3 – 左侧是 Prometheus Web 应用主页，右侧是 Prometheus 正在跟踪并从中获取指标的目标](img/B18187_16_3.jpg)'
- en: Figure 16.3 – Prometheus web app home page on the left and targets that Prometheus
    is tracking and polling metrics from on the right
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3 – 左侧是 Prometheus Web 应用主页，右侧是 Prometheus 正在跟踪并从中获取指标的目标
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Prometheus by default doesn’t include user account enforcement but it can be
    configured to be enforced.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Prometheus 不强制用户账户管理，但可以配置为强制执行。
- en: 'Next, we will set up Grafana to connect to the locally hosted Prometheus instance.
    First, we have to start up the Grafana service by executing the following command
    in the command line:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置 Grafana 以连接到本地托管的 Prometheus 实例。首先，我们需要通过在命令行中执行以下命令来启动 Grafana 服务：
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure 16.4 – Screenshots showing how to navigate to the Add data source
    page in the Grafana web app](img/B18187_16_4.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.4 – 显示如何在 Grafana Web 应用中导航到添加数据源页面的截图](img/B18187_16_4.jpg)'
- en: Figure 16.4 – Screenshots showing how to navigate to the Add data source page
    in the Grafana web app
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4 – 显示如何在 Grafana Web 应用中导航到添加数据源页面的截图
- en: Next, click on Prometheus as the data source, where you will be presented with
    the screen shown in *Figure 16**.5 (a)*. Set the Prometheus default hosted web
    app link to `http://localhost:9090` and click on **Save & Test**. This should
    result in the success screen shown in *Figure* *16**.5 (b)*.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击Prometheus作为数据源，您将看到如*图16.5 (a)*所示的屏幕。将Prometheus默认托管的Web应用链接设置为`http://localhost:9090`，然后点击**保存并测试**。这应该会显示如*图16.5
    (b)*所示的成功界面。
- en: '![Figure 16.5 – Grafana Prometheus data source settings tab in (a) and successfully
    created screen (b)](img/B18187_16_5.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图16.5 – Grafana Prometheus数据源设置标签页（a）和成功创建的屏幕（b）](img/B18187_16_5.jpg)'
- en: Figure 16.5 – Grafana Prometheus data source settings tab in (a) and successfully
    created screen (b)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 – Grafana Prometheus数据源设置标签页（a）和成功创建的屏幕（b）
- en: 'With that, you will see that the data source has been created as shown in *Figure
    16**.6 (a)*. At this point, we will be able to create a dashboard to visualize
    the metrics we are monitoring. Grafana allows you to create dashboards in three
    ways: importing through its publicly shared dashboard IDs, importing through an
    exported dashboard JSON file, and creating a new dashboard. In Grafana, you can
    create many types of visualizations manually using the in-built visualization
    UI builder system or the **PromQL**-based visualizations and choose how you want
    them to be displayed. However, in this tutorial, we will be using a ready-made
    dashboard with visualizations by importing it through a dashboard JSON file. To
    do that, navigate to the dashboard page using the same three-line button dropdown
    shown in *Figure 16**.4 (a)*. Once, you are on the dashboard page, click on **New**
    and then on **Import**, as shown in *Figure* *16**.6 (b)*.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，您将看到数据源已成功创建，如*图16.6 (a)*所示。此时，我们可以创建一个仪表盘来可视化我们正在监控的指标。Grafana允许通过三种方式创建仪表盘：通过其公开共享的仪表盘ID导入，通过导出的仪表盘JSON文件导入，以及创建新的仪表盘。在Grafana中，您可以使用内置的可视化UI构建器系统或基于**PromQL**的可视化手动创建多种类型的可视化，并选择它们的展示方式。然而，在本教程中，我们将通过导入一个已有的仪表盘JSON文件来使用现成的可视化。为此，使用*图16.4
    (a)*所示的三行按钮下拉菜单导航到仪表盘页面。进入仪表盘页面后，点击**新建**，然后点击**导入**，如*图16.6 (b)*所示。
- en: '![Figure 16.6 – Grafana Data sources tab showing the created data source and
    the Dashboards tab showing the dropdown of the New button](img/B18187_16_6.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图16.6 – Grafana数据源标签页显示已创建的数据源，仪表盘标签页显示新建按钮的下拉菜单](img/B18187_16_6.jpg)'
- en: Figure 16.6 – Grafana Data sources tab showing the created data source and the
    Dashboards tab showing the dropdown of the New button
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 – Grafana数据源标签页显示已创建的数据源，仪表盘标签页显示新建按钮的下拉菜单
- en: Drag the provided `Triton Inference Server-1692252636911.json` file straight
    into the import area and then connect to the Prometheus database you created,
    and you’ll see the dashboard shown in *Figure 16**.7*.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将提供的`Triton Inference Server-1692252636911.json`文件直接拖入导入区域，然后连接到您创建的Prometheus数据库，您将看到如*图16.7*所示的仪表盘。
- en: '![Figure 16.7 – A custom Grafana dashboard for the monitoring tutorial](img/B18187_16_7.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图16.7 – 用于监控教程的自定义Grafana仪表盘](img/B18187_16_7.jpg)'
- en: Figure 16.7 – A custom Grafana dashboard for the monitoring tutorial
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 – 用于监控教程的自定义Grafana仪表盘
- en: Note that there were two GPUs in the machine that generated this metric, which
    is why there are two GPU stats. This visualization effectively represents most
    of the default NVIDIA Triton Server metrics, along with the three extra custom
    metrics we added, by displaying them on a graph that captures their historical
    values up to the present moment. However, hardware-resource-specific stats are
    an exception, as they are shown only in real-time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成该指标的机器上有两块GPU，这也是为什么会有两个GPU统计数据。此可视化有效地展示了大多数默认的NVIDIA Triton Server指标，并通过图形显示了这三个额外的自定义指标，这些图形捕捉了它们的历史值直到当前时刻。然而，硬件资源特定的统计数据是一个例外，它们仅以实时方式展示。
- en: Now, the component missing from monitoring is to create rules and conditions
    that would be considered an alarming incident, called the incident alerting component.
    Monitoring deployed deep learning models without alerts is like having a security
    camera but no alarm. You won’t know if something has gone wrong until it’s too
    late to do anything about it. Incidents can include deteriorating model accuracy,
    consistently delayed responses, consistent resource bottlenecks, consistently
    unexpected output variations during the monitoring of deployed deep learning models,
    and hardware failures. Grafana has an in-built alert management, notifications
    management, and contact management system that we will leverage in the following
    section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，监控中缺少的部分是创建规则和条件，这些规则和条件被认为是警报事件，称为事件警报组件。没有警报的深度学习模型监控就像有了安防摄像头，却没有警报系统。你不会知道问题何时发生，直到为时已晚。事件可能包括模型准确度下降、响应延迟、资源瓶颈、在监控已部署深度学习模型时的意外输出变化和硬件故障等。Grafana
    具有内建的警报管理、通知管理和联系人管理系统，我们将在接下来的部分中利用这些功能。
- en: Setting up alerts with Grafana
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Grafana 设置警报
- en: 'Let’s go through the steps on how to set up alerts with Grafana:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用 Grafana 设置警报的步骤：
- en: Click on the **Alerting** tab, shown in *Figure 16**.4 (a)*, and then on **Alert
    rules**. You will see the screen shown in *Figure 16**.8*.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**警报**标签，如*图 16**.4 (a)*所示，然后点击**警报规则**。你将看到如*图 16**.8*所示的界面。
- en: '![Figure 16.8 – Alert rules tab settings for NVIDIA Triton request failure
    alert rule](img/B18187_16_8.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.8 – NVIDIA Triton 请求失败警报规则的警报规则标签设置](img/B18187_16_8.jpg)'
- en: Figure 16.8 – Alert rules tab settings for NVIDIA Triton request failure alert
    rule
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8 – NVIDIA Triton 请求失败警报规则的警报规则标签设置
- en: In this example, we will set an alert to trigger when there is any failed NVIDIA
    Triton Server inference request. So, in the same tab, choose the `nv_inference_request_failure`
    metric tab, and set the threshold to a number that is lower than 1 so that a single
    failed request will trigger the alarm. In *Figure 16**.8*, the number is set to
    `0.8`.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将设置一个警报，当有任何 NVIDIA Triton Server 推理请求失败时触发。所以，在同一标签页中，选择`nv_inference_request_failure`指标标签，并将阈值设置为小于
    1 的数值，这样单个失败请求就会触发警报。如*图 16**.8*所示，数值被设置为`0.8`。
- en: Next, set the evaluation interval to be one minute and to raise an alarm only
    if there are consistent request failures for five minutes straight, as shown in
    *Figure 16**.9*. Then, click on the **Save and** **exit** button.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将评估间隔设置为一分钟，并且仅在连续五分钟内出现请求失败时才会触发警报，如*图 16**.9*所示。然后，点击**保存并退出**按钮。
- en: '![Figure 16.9 – Evaluation interval settings for alert rules](img/B18187_16_9.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.9 – 警报规则的评估间隔设置](img/B18187_16_9.jpg)'
- en: Figure 16.9 – Evaluation interval settings for alert rules
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9 – 警报规则的评估间隔设置
- en: 'There are three possible statuses that alerts in Grafana can have: **Normal**,
    which indicates that the condition wasn’t triggered; **Pending**, which indicates
    that the condition was partially triggered but there isn’t a consistent behavior
    yet; and **Firing**, which indicates that the condition has been consistently
    satisfied and an alarm has been triggered. Now that an alert rule is saved and
    created, you will see the screen shown in *Figure 16**.10 (a)*, where the status
    is **Normal**. *Figure 16**.10 (b)* shows the **Pending** stage, where a failure
    has been detected but is not yet consistent enough to send an alert. *Figure 16**.10
    (c)*, on the other hand, shows the **Firing** stage, where the failure has consistently
    happened per the configured time interval.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 中的警报可能有三种状态：**正常**，表示条件未触发；**待处理**，表示条件部分触发，但行为还不一致；**触发**，表示条件已持续满足，并且警报已被触发。现在，警报规则已保存并创建，你将看到如*图
    16**.10 (a)*所示的界面，状态为**正常**。*图 16**.10 (b)*展示了**待处理**阶段，其中检测到失败，但还不够一致，无法触发警报。另一方面，*图
    16**.10 (c)*展示了**触发**阶段，其中故障已按照配置的时间间隔持续发生。
- en: '![Figure 16.10 – Alert status of Normal in (a), Pending in (b), and Firing
    in (c)](img/B18187_16_10.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.10 – 正常状态（a）、待处理状态（b）和触发状态（c）的警报状态](img/B18187_16_10.jpg)'
- en: Figure 16.10 – Alert status of Normal in (a), Pending in (b), and Firing in
    (c)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.10 – 正常状态（a）、待处理状态（b）和触发状态（c）的警报状态
- en: 'To configure where and who these alerts will be sent to, we’ll work with the
    **Contact points** and **Notification policies** tabs in the **Alerting** section.
    Let’s start by clicking on the **Contact points** tab to set up the individuals
    who will receive notifications. You can even organize these into groups, but for
    simplicity in this tutorial, we’ll have notifications sent to ourselves. Grafana
    offers various contact platform integrations: Alertmanager, Cisco Webex Teams,
    DingDing, Discord, Email, Google Chat, Kafka REST policy, LINE, Microsoft Teams,
    Opsgenie, PagerDuty, Pushover, Sensu Go, Slack, Telegram, Threema Gateway, VictorOps,
    Webhook, and WeCom. To keep things straightforward, we’ll choose a widely available
    integration type: email. Grafana uses the **sSMTP** software for sending emails,
    so ensure you have an email account with credentials set up before proceeding.
    Within the contact points settings, provide your name and email, then click on
    **Test** to generate a test notification to confirm that the credentials are accurate.
    Once you’ve verified that you’ve received the email notification, save your settings.
    Refer to *Figure 16**.11* for an example of the settings interface.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置这些警报发送到哪里以及发送给谁，我们将使用**联系人**和**通知策略**选项卡，这些都在**警报**部分。首先，点击**联系人**选项卡，设置将接收通知的人员。你甚至可以将这些人员组织成小组，但为了简化本教程，我们将通知发送给自己。Grafana
    提供了多种联系方式集成：Alertmanager、Cisco Webex Teams、钉钉、Discord、电子邮件、Google Chat、Kafka REST
    策略、LINE、Microsoft Teams、Opsgenie、PagerDuty、Pushover、Sensu Go、Slack、Telegram、Threema
    Gateway、VictorOps、Webhook 和 WeCom。为了简化操作，我们将选择一个广泛可用的集成类型：电子邮件。Grafana 使用**sSMTP**软件发送电子邮件，因此在继续操作之前，请确保你已设置了电子邮件账户并配置了凭据。在联系人设置中，输入你的姓名和电子邮件，然后点击**测试**以生成一个测试通知，确认凭据准确。确认收到电子邮件通知后，保存设置。有关设置界面的示例，请参见*图16**.11*。
- en: '![Figure 16.11 – Contact points tab with email set up](img/B18187_16_11.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图16.11 – 设置电子邮件的联系人选项卡](img/B18187_16_11.jpg)'
- en: Figure 16.11 – Contact points tab with email set up
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11 – 设置电子邮件的联系人选项卡
- en: Next, we need to link up the contact as part of the default notification policy.
    Proceed to the **Notification policies** tab, click on **Settings**, and change
    the default contact point to be the email contact we set in *step 4*. You will
    then see a similar screen to *Figure 16**.12*.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将联系人与默认通知策略关联起来。转到**通知策略**选项卡，点击**设置**，并将默认联系点更改为我们在*第4步*中设置的电子邮件联系人。然后你将看到一个类似于*图16**.12*的界面。
- en: '![Figure 16.12 – The default notification policy set up to notify us](img/B18187_16_12.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图16.12 – 设置为通知我们的默认通知策略](img/B18187_16_12.jpg)'
- en: Figure 16.12 – The default notification policy set up to notify us
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12 – 设置为通知我们的默认通知策略
- en: Now, we are all set up to receive email notifications! As a challenge, try to
    figure out ways you can make the inference server request fail, and if you can’t,
    change the rule to something that will definitely trigger so you can get an example
    actual alert notification come through email. *Figure 16**.13* shows the example
    email that you will get through a mobile phone interface.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经设置好接收电子邮件通知！作为一个挑战，尝试找出一些方法让推理服务器请求失败，如果无法做到，修改规则为某个一定会触发的条件，这样你就能通过电子邮件收到一个实际的警报通知示例。*图16**.13*展示了你将通过手机界面收到的示例电子邮件。
- en: '![Figure 16.13 – Example triggered alert email notification with the Firing
    status](img/B18187_16_13.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图16.13 – 示例触发的警报电子邮件通知，状态为Firing](img/B18187_16_13.jpg)'
- en: Figure 16.13 – Example triggered alert email notification with the Firing status
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13 – 示例触发的警报电子邮件通知，状态为Firing
- en: With that, we have successfully set up a monitoring and alerting system for
    a deployed deep learning model! A notable caveat in this implementation is the
    fact that metrics are bundled up in the same execution as the model. A solution
    to make it decoupled to not increase the runtime for prediction-specific inference
    is to use the C API instead to build the custom metrics. If the time needed to
    get the metrics logged is not crucial, you can also consider hosting another “model”
    in NVIDIA Triton Server that takes in outputs from the prediction-specific model
    and log metrics. NVIDIA Triton Server also provides a tool called `perf_client`,
    which evaluates the runtime of different configurations, helping you optimize
    your system’s performance. Specifically, the tool measures and reports the throughput
    and latency with different load conditions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些，我们成功为部署的深度学习模型设置了监控和警报系统！这个实现中的一个显著注意事项是，指标与模型执行捆绑在一起。为了解耦以避免增加特定推理预测的运行时，解决方案是使用C
    API来构建自定义指标。如果记录指标所需的时间不重要，你也可以考虑在NVIDIA Triton Server中托管另一个“模型”，该模型接收来自特定推理模型的输出并记录指标。NVIDIA
    Triton Server还提供了一个名为`perf_client`的工具，可以评估不同配置的运行时，帮助你优化系统的性能。具体来说，该工具衡量并报告在不同负载条件下的吞吐量和延迟。
- en: However, just having monitoring and alerts doesn’t provide a full picture of
    model monitoring. We need to dive into those numbers, cross-reference them, spot
    connections, and find patterns. It’s like checking the fuel efficiency, tire pressure,
    and engine temperature of a car to ensure a smooth ride.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅拥有监控和警报并不能提供完整的模型监控视图。我们需要深入分析这些数据，交叉参考它们，发现连接并找出模式。这就像检查汽车的燃油效率、轮胎压力和发动机温度，确保驾驶顺畅。
- en: Additionally, alerts alone won’t fix issues. They’re like the car’s warning
    lights – they tell you something’s up, but you still need to pull over, pop the
    hood, and fix the problem. That’s where model maintenance comes in. In the next
    section, we’ll explore how to not only detect issues but also take action to keep
    your model running smoothly and efficiently over time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅靠警报无法解决问题。它们就像汽车的警示灯——它们告诉你出现了问题，但你仍然需要停车，打开引擎盖并修复问题。这就是模型维护的作用。在下一部分，我们将探讨如何不仅检测问题，还能采取行动，确保模型在长期内保持平稳高效地运行。
- en: Governing a deep learning model through maintenance
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过维护管理深度学习模型
- en: Metrics logging, dashboard building, logged metrics analysis, and alerts are
    essential components of model monitoring, but they are only effective when followed
    by appropriate actions, which are covered under model maintenance. Model maintenance
    is akin to a skilled pit crew in a car race, regularly fine-tuning and optimizing
    the performance of deep learning models to keep them running efficiently and effectively.
    Like how a pit crew conducts rapid repairs, refuels, and adjusts the car’s components
    to adapt to changing race conditions, model maintenance involves updating the
    models to account for environmental changes, improving and refining the models
    with new data obtained from feedback loops, and performing incident responses
    on miscellaneous issues. This ensures that the models consistently stay on track,
    deliver valuable insights, and drive informed decision-making in the ever-evolving
    landscape of data and business requirements.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 指标记录、仪表盘构建、记录的指标分析和警报是模型监控的核心组成部分，但它们只有在采取适当的行动之后才有效，这些行动包含在模型维护中。模型维护类似于赛车中的技术维修团队，定期对深度学习模型进行微调和优化，以确保它们高效且有效地运行。就像技术维修团队快速修理、加油和调整赛车部件以适应变化的赛道条件一样，模型维护涉及到更新模型以应对环境变化，利用反馈回路获取的新数据改进和完善模型，并针对杂项问题进行事件响应。这确保了模型始终保持在正确轨道上，提供有价值的见解，并在数据和业务需求不断变化的环境中推动明智的决策。
- en: 'Key aspects of model maintenance comprehensively include the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 模型维护的关键方面全面包括以下内容：
- en: '**Establishing a feedback loop**: Establishing a feedback loop is vital for
    capturing real-world outcomes and validating model predictions, enabling deep
    learning practitioners to identify areas for improvement, and adapting the model
    accordingly.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立反馈回路**：建立反馈回路对于捕获实际结果和验证模型预测至关重要，它使深度学习从业者能够识别改进的领域，并相应地调整模型。'
- en: '**Retraining**: Retraining is an essential part of model maintenance, ensuring
    that the model stays up to date with the latest data and trends, thereby maintaining
    its accuracy and relevance. Regular retraining enables the model to learn from
    new insights and adapt to evolving data landscapes, ensuring consistent performance.
    Fortunately, for deep learning models, a fine-tuning process can be employed,
    which is much faster than a full retraining process. Two use cases that highlight
    the importance of frequent updates with model retraining are the following:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新训练**：重新训练是模型维护的一个关键环节，确保模型能够跟上最新的数据和趋势，从而保持准确性和相关性。定期重新训练可以使模型从新的见解中学习，并适应不断变化的数据环境，保证稳定的性能。幸运的是，对于深度学习模型，可以使用微调过程，这比完全重新训练过程要快得多。以下是两个强调频繁更新和模型重新训练重要性的使用案例：'
- en: '**E-commerce product recommendation**: Consumer preferences and product availability
    change rapidly in e-commerce. To provide relevant product recommendations, deep
    learning models need to be retrained frequently, maybe weekly or even daily, to
    understand the latest trends and customer behavior.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子商务产品推荐**：在电子商务中，消费者偏好和产品可用性变化迅速。为了提供相关的产品推荐，深度学习模型需要频繁地进行重新训练，可能是每周甚至每天一次，以理解最新的趋势和客户行为。'
- en: '**Social media sentiment analysis**: Social media platforms are constantly
    evolving with new trends, hashtags, and user behaviors. To accurately gauge public
    sentiment and opinion, deep learning models need to be retrained frequently, maybe
    quarterly, to account for these changes.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体情感分析**：社交媒体平台不断发展，出现新的趋势、标签和用户行为。为了准确评估公众情感和意见，深度学习模型需要经常重新训练，可能是每季度一次，以适应这些变化。'
- en: '**Incident response handling**: When alerts signal potential issues, it’s vital
    to have a dedicated response team to triage and address the problem promptly.
    This team should be well equipped to investigate the root cause, implement corrective
    measures, and prevent similar issues from recurring in the future. Let’s discover
    response-handling recommendations for different groups of incidents:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件响应处理**：当警报信号显示潜在问题时，拥有一个专门的响应团队来及时处理和解决问题至关重要。该团队应该具备充分的能力，调查根本原因，采取纠正措施，并防止类似问题在未来再次发生。让我们来看看不同类型事件的响应处理建议：'
- en: '**Data-related incidents**: These incidents occur when the model receives incorrect,
    incomplete, or biased input data. To handle such issues, the response team should
    work closely with the data provider to identify the cause, correct the data, and
    retrain the model as needed.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据相关事件**：这些事件发生在模型接收到不正确、不完整或有偏的数据输入时。处理此类问题时，响应团队应该与数据提供者紧密合作，识别原因，纠正数据，并在需要时重新训练模型。'
- en: '**Model performance incidents**: These incidents involve the model generating
    inaccurate or unexpected predictions. Proper handling requires collaboration between
    the model owner (responsible for model creation or approving the model usage)
    and the prediction owner (responsible for approving the usage of the predictions),
    as described in the *Governing deep learning model utilization* section earlier.
    They should analyze the model’s performance, identify potential issues in its
    architecture or training, and implement improvements to ensure better performance
    in the future.'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能事件**：这些事件涉及模型生成不准确或出乎意料的预测。正确的处理方式需要模型拥有者（负责模型创建或批准模型使用）与预测拥有者（负责批准预测使用）之间的合作，正如前面*深度学习模型利用治理*部分所描述的那样。两者应该分析模型的表现，识别架构或训练中潜在的问题，并实施改进，确保未来能够提供更好的性能。'
- en: '**Infrastructure-related incidents**: These incidents are caused by hardware
    or software failures, affecting the model’s deployment environment. The response
    team should work with the infrastructure provider or team to resolve the issue
    and ensure the model runs smoothly.'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施相关事件**：这些事件由硬件或软件故障引起，影响模型的部署环境。响应团队应该与基础设施提供商或团队合作，解决问题，确保模型能够顺利运行。'
- en: '**Security incidents**: These incidents involve unauthorized access, data breaches,
    or other malicious activities targeting the model. The response team should follow
    the organization’s security policies, identify the threat, and take appropriate
    measures to mitigate the risk.'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全事件**：这些事件涉及未经授权的访问、数据泄露或其他恶意活动，目标是模型。响应团队应该遵循组织的安全政策，识别威胁，并采取适当的措施来减轻风险。'
- en: '**Compliance and regulatory incidents**: These incidents occur when the model’s
    output or operation violates legal or regulatory requirements. The response team
    should work with legal and compliance teams to address the violation and modify
    the model to comply with the necessary regulations.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性和监管事件**：当模型的输出或操作违反了法律或监管要求时，就会发生这些事件。响应团队应与法律和合规团队合作，处理违规问题并修改模型以符合必要的规定。'
- en: By comprehensively considering model maintenance components shared here, organizations
    can effectively address challenges associated with deployed deep learning models,
    ensuring their continuous improvement and alignment with business requirements.
    Traditionally, these maintenance actions are executed manually after alerts are
    raised. However, it is possible to schedule custom tasks to be executed automatically
    given an alert event. Consider using Apache Airflow to orchestrate your desired
    automated tasks from your model monitoring alerts. Apache Airflow is like a conductor
    for your data tasks, allowing you to choreograph and schedule complex workflows
    in a directed acyclic graph format. It lets you define, automate, and monitor
    sequences of tasks, making sure they happen in the right order and at the right
    time. However, there are some inherent limitations and risks with creating automated
    tasks from model monitoring alerts, which we will briefly explore next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过全面考虑此处分享的模型维护组件，组织可以有效解决已部署深度学习模型所面临的挑战，确保其持续改进并与业务需求对齐。传统上，这些维护操作是在警报触发后手动执行的。然而，也可以根据警报事件安排定制任务以自动执行。考虑使用
    Apache Airflow 来协调从模型监控警报中触发的自动化任务。Apache Airflow 就像是数据任务的指挥，允许你在有向无环图格式中编排和调度复杂的工作流。它让你定义、自动化并监控任务序列，确保它们按正确的顺序和时间发生。然而，基于模型监控警报创建自动化任务存在一些固有的局限性和风险，接下来我们将简要探讨这些问题。
- en: Exploring limitations and risks of using automated tasks triggered by model
    monitoring alerts
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨使用由模型监控警报触发的自动化任务的局限性和风险
- en: 'While automating tasks based on model monitoring alerts can save time and resources,
    it also comes with limitations and potential risks that need to be considered
    when implementing such an approach. Some limitations of automating tasks based
    on model monitoring alerts are the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于模型监控警报自动化任务可以节省时间和资源，但在实施这种方法时也存在局限性和潜在风险，必须加以考虑。基于模型监控警报自动化任务的一些局限性如下：
- en: '**Complexity of issues**: Some issues may be too complex or nuanced to be handled
    effectively by an automated process. For example, in a deep learning model for
    medical image analysis, an automated task might be triggered to retrain the model
    when the monitoring alerts indicate a drop in accuracy. However, the complexity
    of the issue may stem from an imbalance in the training data, such as an underrepresentation
    of a certain disease, which cannot be resolved by simply retraining the model.
    In this case, automated processes might not be able to effectively address the
    root cause of the problem.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题复杂性**：有些问题可能过于复杂或微妙，无法通过自动化过程有效处理。例如，在一个医学影像分析的深度学习模型中，当监控警报表明准确率下降时，可能触发一个自动化任务来重新训练模型。然而，问题的复杂性可能源于训练数据的不平衡，比如某种疾病的代表性不足，而仅仅通过重新训练模型是无法解决的。在这种情况下，自动化过程可能无法有效解决问题的根本原因。'
- en: '**Lack of context**: Automated tasks may lack the ability to consider the broader
    context of an issue or understand its potential impact on other aspects of the
    system.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏上下文**：自动化任务可能缺乏考虑问题更广泛背景的能力，也无法理解其对系统其他方面可能产生的影响。'
- en: Consider a deep learning model that predicts customer churn based on various
    behavioral and demographic factors. An automated task might be set up to send
    promotional offers to customers identified as high risk for churn. However, the
    task may not have the context to consider external factors, such as a recent negative
    publicity event or a widespread service outage, which might be causing a temporary
    increase in churn risk. This lack of context may lead to unnecessary promotional
    offers and an ineffective use of resources.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设有一个深度学习模型，根据各种行为和人口统计因素预测客户流失。可以设置一个自动化任务，当检测到高流失风险的客户时，向他们发送促销优惠。然而，该任务可能没有足够的上下文来考虑外部因素，比如近期的负面宣传事件或广泛的服务中断，这些因素可能导致流失风险的暂时性增加。这种缺乏上下文的情况可能导致不必要的促销优惠和资源的无效使用。
- en: '**Inadequate or inappropriate responses**: Automated tasks might not always
    choose the most appropriate action in response to an alert, potentially leading
    to suboptimal outcomes. For example, an AI model monitoring social media posts
    for harmful content may detect a post containing offensive language. An automated
    response system might remove the post or ban the user immediately, without considering
    the possibility of false positives or the post’s broader context (e.g., quoting
    offensive language to criticize it).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不充分或不适当的响应**：自动化任务可能无法始终选择最合适的响应措施来应对警报，这可能导致次优的结果。例如，一个监控社交媒体帖子中有害内容的AI模型可能会检测到包含冒犯性语言的帖子。自动响应系统可能会立即删除帖子或禁用用户，而没有考虑到误报的可能性或帖子的更广泛背景（例如，引用冒犯性语言以进行批评）。'
- en: 'As for risks associated with enabling automated tasks with model monitoring
    alerts, they are as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 至于启用基于模型监控警报的自动化任务所涉及的风险，如下所示：
- en: '**Over-reliance on automation**: Relying too heavily on automated tasks can
    lead to a lack of human oversight and expertise in the model maintenance process.
    This may result in overlooking subtle patterns and trends that only human intuition
    can detect, potentially leading to suboptimal model performance.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度依赖自动化**：过度依赖自动化任务可能导致模型维护过程中缺乏人工监督和专业知识。这可能导致忽略只有人类直觉才能察觉的微妙模式和趋势，进而可能导致模型性能不佳。'
- en: '**Inaccurate or premature triggers**: Automated tasks are often triggered by
    specific conditions in the monitored metrics. If these conditions are not carefully
    defined, tasks may be triggered inaccurately or prematurely, leading to unnecessary
    or even detrimental actions being taken on the model.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不准确或过早的触发**：自动化任务通常是根据监控指标中的特定条件触发的。如果这些条件没有被仔细定义，任务可能会不准确或过早地触发，导致对模型采取不必要甚至有害的行动。'
- en: '**Inflexibility**: Automated tasks are typically designed for specific scenarios
    or issues and may not be flexible enough to handle unforeseen or complex situations.
    This could limit their effectiveness in addressing unique challenges that arise
    during model maintenance.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏灵活性**：自动化任务通常是为特定场景或问题设计的，可能不足以应对无法预见或复杂的情况。这可能限制其在处理模型维护过程中出现的独特挑战时的有效性。'
- en: '**Risk of compounding errors**: When automated tasks are executed based on
    erroneous alerts or inaccurate metrics, they can compound the issue by making
    unnecessary or incorrect adjustments to the model. This may lead to further deterioration
    in model performance or even irreversible damage.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误叠加的风险**：当自动化任务根据错误的警报或不准确的指标执行时，它们可能通过对模型进行不必要或不正确的调整来加剧问题。这可能导致模型性能进一步恶化，甚至造成不可逆的损害。'
- en: '**Security risks**: Automating tasks based on alerts can expose the model and
    its infrastructure to potential security risks, especially if the automation system
    is not adequately secured. Unauthorized access or manipulation of the automation
    system could lead to unintended consequences or malicious actions on the model.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全风险**：基于警报自动化任务可能会使模型及其基础设施面临潜在的安全风险，特别是如果自动化系统没有得到充分的安全保障。未经授权的访问或操作自动化系统可能会导致意外后果或恶意行为影响模型。'
- en: To mitigate these limitations and risks, it is essential to strike a balance
    between automation and human involvement in the model maintenance process. This
    can be achieved by incorporating human-in-the-loop systems, ensuring proper validation
    and calibration of monitoring metrics and alerts, and implementing robust security
    measures to protect the automation infrastructure.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这些局限性和风险，必须在自动化和人工参与之间找到平衡。这可以通过引入人类介入系统、确保监控指标和警报的适当验证与校准，并实施强有力的安全措施来保护自动化基础设施来实现。
- en: With that, we have covered all the components of deep learning model governance.
    This holistic three-pillar approach to model governance ultimately enables organizations
    to consistently and continuously harness the full potential of deep learning models,
    driving valuable insights and informed decision-making in the real world.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们已经涵盖了深度学习模型治理的所有组成部分。这种三支柱的整体方法最终使组织能够持续并一致地发挥深度学习模型的全部潜力，在现实世界中推动有价值的洞察力和信息驱动的决策。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored the three fundamental pillars of model governance
    for deep learning models: model utilization, model monitoring, and model maintenance.
    Model utilization ensures the effective, efficient, ethical, and responsible utilization
    of deep learning models, while model monitoring allows for ongoing evaluation
    of performance, identification of potential bias or drift, and infrastructure-related
    metrics. Model maintenance, on the other hand, focuses on regular updates and
    refinements to keep models aligned with evolving data landscapes and business
    requirements.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了深度学习模型治理的三大基础支柱：模型利用、模型监控和模型维护。模型利用确保深度学习模型的有效、效率、伦理和负责任的使用，而模型监控则允许对性能进行持续评估，识别潜在的偏差或漂移，并监控基础设施相关的指标。另一方面，模型维护侧重于定期更新和完善，以确保模型与不断变化的数据环境和业务需求保持一致。
- en: We also dove into and learned about the technical steps for monitoring deep
    learning models using NVIDIA Triton Server, Prometheus, and Grafana. By diligently
    considering the components for model governance, deep learning architects can
    effectively manage the challenges posed by these complex models in production
    and consistently harness their potential for driving valuable insights and decisions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还深入了解了使用NVIDIA Triton Server、Prometheus和Grafana监控深度学习模型的技术步骤。通过认真考虑模型治理的各个组成部分，深度学习架构师可以有效管理这些复杂模型在生产环境中的挑战，并持续挖掘它们为推动有价值的洞察和决策所带来的潜力。
- en: In the next chapter, we will further dive deeper into the details of drift detection
    for deep learning models.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进一步深入探讨深度学习模型漂移检测的细节。
