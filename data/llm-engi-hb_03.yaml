- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Data Engineering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程
- en: This chapter will begin exploring the LLM Twin project in more depth. We will
    learn how to design and implement the data collection pipeline to gather the raw
    data we will use in all our LLM use cases, such as fine-tuning or inference. As
    this is not a book on data engineering, we will keep this chapter short and focus
    only on what is strictly necessary to collect the required raw data. Starting
    with *Chapter 4*, we will concentrate on LLMs and GenAI, exploring its theory
    and concrete implementation details.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将更深入地探索 LLM Twin 项目。我们将学习如何设计和实现数据收集管道，以收集我们将用于所有 LLM 用例（如微调或推理）的原始数据。由于这不是一本关于数据工程的书籍，我们将使本章简短，并仅关注严格必要的收集所需原始数据的内容。从第
    4 章开始，我们将专注于 LLM 和 GenAI，探讨其理论和具体实现细节。
- en: When working on toy projects or doing research, you usually have a static dataset
    with which you work. But in our LLM Twin use case, we want to mimic a real-world
    scenario where we must gather and curate the data ourselves. Thus, implementing
    our data pipeline will connect the dots regarding how an end-to-end ML project
    works. This chapter will explore how to design and implement an **Extract, Transform,
    Load** (**ETL**) pipeline that crawls multiple social platforms, such as Medium,
    Substack, or GitHub, and aggregates the gathered data into a MongoDB data warehouse.
    We will show you how to implement various crawling methods, standardize the data,
    and load it into a data warehouse.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当在玩具项目或进行研究时，你通常有一个静态数据集与之工作。但在我们的 LLM Twin 用例中，我们希望模拟一个真实世界的场景，我们必须自己收集和整理数据。因此，实现我们的数据管道将连接有关端到端
    ML 项目如何工作的各个点。本章将探讨如何设计和实现一个**提取、转换、加载**（**ETL**）管道，该管道爬取多个社交平台（如 Medium、Substack
    或 GitHub），并将收集到的数据聚合到一个 MongoDB 数据仓库中。我们将向您展示如何实现各种爬取方法、标准化数据并将其加载到数据仓库中。
- en: We will begin by designing the LLM Twin’s data collection pipeline and explaining
    the architecture of the ETL pipeline. Afterward, we will move directly to implementing
    the pipeline, starting with ZenML, which will orchestrate the entire process.
    We will investigate the crawler implementation and understand how to implement
    a dispatcher layer that instantiates the right crawler class based on the domain
    of the provided link while following software best practices. Next, we will learn
    how to implement each crawler individually. Also, we will show you how to implement
    a data layer on top of MongoDB to structure all our documents and interact with
    the database.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设计 LLM Twin 的数据收集管道，并解释 ETL 管道的架构。之后，我们将直接进入管道的实现，从 ZenML 开始，它将协调整个流程。我们将研究爬虫实现，并了解如何根据提供的链接的领域实现一个调度层，该层实例化正确的爬虫类，同时遵循软件最佳实践。接下来，我们将学习如何单独实现每个爬虫。此外，我们还将向您展示如何在
    MongoDB 之上实现数据层，以结构化所有我们的文档并与数据库交互。
- en: Finally, we will explore how to run the data collection pipeline using ZenML
    and query the collected data from MongoDB.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨如何使用 ZenML 运行数据收集管道，并从 MongoDB 查询收集到的数据。
- en: 'Thus, in this chapter, we will study the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将研究以下主题：
- en: Designing the LLM Twin’s data collection pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计 LLM Twin 的数据收集管道
- en: Implementing the LLM Twin’s data collection pipeline
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 LLM Twin 的数据收集管道
- en: Gathering raw data into the data warehouse
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始数据收集到数据仓库中
- en: By the end of this chapter, you will know how to design and implement an ETL
    pipeline to extract, transform, and load raw data ready to be ingested into the
    ML application.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何设计和实现一个 ETL 管道，以提取、转换和加载准备就绪的原始数据，以便被 ML 应用程序摄取。
- en: Designing the LLM Twin’s data collection pipeline
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计 LLM Twin 的数据收集管道
- en: Before digging into the implementation, we must understand the LLM Twin’s data
    collection ETL architecture, illustrated in *Figure 3.1*. We must explore what
    platforms we will crawl to extract data from and how we will design our data structures
    and processes. However, the first step is understanding how our data collection
    pipeline maps to an ETL process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实施之前，我们必须了解 LLM Twin 的数据收集 ETL 架构，如图 3.1 所示。我们必须探索我们将爬取哪些平台以提取数据，以及我们将如何设计我们的数据结构和流程。然而，第一步是理解我们的数据收集管道如何映射到
    ETL 流程。
- en: 'An ETL pipeline involves three fundamental steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 管道涉及三个基本步骤：
- en: We **extract** data from various sources. We will crawl data from platforms
    like Medium, Substack, and GitHub to gather raw data.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们**提取**来自各种来源的数据。我们将从 Medium、Substack 和 GitHub 等平台爬取数据以收集原始数据。
- en: We **transform** this data by cleaning and standardizing it into a consistent
    format suitable for storage and analysis.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过清理和标准化数据将其**转换**为适合存储和分析的统一格式。
- en: We **load** the transformed data into a data warehouse or database.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将**加载**转换后的数据到数据仓库或数据库中。
- en: For our project, we use MongoDB as our NoSQL data warehouse. Although this is
    not a standard approach, we will explain the reasoning behind this choice shortly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的项目，我们使用MongoDB作为我们的NoSQL数据仓库。尽管这不是一个标准的方法，但我们将很快解释选择这一方法的原因。
- en: '![](img/B31105_03_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_03_01.png)'
- en: 'Figure 3.1: LLM Twin’s data collection ETL pipeline architecture'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：LLM Twin的数据收集ETL管道架构
- en: We want to design an ETL pipeline that inputs a user and a list of links as
    input. Afterward, it crawls each link individually, standardizes the collected
    content, and saves it under that specific author in a MongoDB data warehouse.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望设计一个ETL管道，它以用户和链接列表作为输入。然后，它单独爬取每个链接，标准化收集的内容，并将其保存到MongoDB数据仓库中特定作者的目录下。
- en: 'Hence, the signature of the data collection pipeline will look as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据收集管道的签名将如下所示：
- en: '**Input:** A list of links and their associated user (the author)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入：** 链接列表及其关联的用户（作者）'
- en: '**Output:** A list of raw documents stored in the NoSQL data warehouse'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出：** 存储在NoSQL数据仓库中的原始文档列表'
- en: We will use `user` and `author` interchangeably, as in most scenarios across
    the ETL pipeline, a user is the author of the extracted content. However, within
    the data warehouse, we have only a user collection.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将“`user`”和“`author`”互换使用，因为在ETL管道的大多数场景中，用户是提取内容的作者。然而，在数据仓库内部，我们只有用户集合。
- en: The ETL pipeline will detect the domain of each link, based on which it will
    call a specialized crawler. We implemented four different crawlers for three different
    data categories, as seen in *Figure 3.2*. First, we will explore the three fundamental
    data categories we will work with across the book. All our collected documents
    can be boiled down to an article, repository (or code), and post. It doesn’t matter
    where the data comes from. We are primarily interested in the document’s format.
    In most scenarios, we will have to process these data categories differently.
    Thus, we created a different domain entity for each, where each entity will have
    its class and collection in MongoDB. As we save the source URL within the document’s
    metadata, we will still know its source and can reference it in our GenAI use
    cases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ETL管道将检测每个链接的域，根据这个域它将调用一个专门的爬虫。我们为三种不同的数据类别实现了四个不同的爬虫，如*图3.2*所示。首先，我们将探讨本书中我们将要工作的三个基本数据类别。我们收集的所有文档都可以归结为文章、仓库（或代码）和帖子。数据来源并不重要。我们主要对文档的格式感兴趣。在大多数情况下，我们都需要对这些数据类别进行不同的处理。因此，我们为每个类别创建了一个不同的域实体，每个实体在MongoDB中都有自己的类和集合。由于我们在文档的元数据中保存了源URL，我们仍然知道它的来源，并且可以在我们的GenAI用例中引用它。
- en: '![](img/B31105_03_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_03_02.png)'
- en: 'Figure 3.2: The relationship between the crawlers and the data categories'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：爬虫和数据类别之间的关系
- en: 'Our codebase supports four different crawlers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础代码支持四种不同的爬虫：
- en: '**Medium crawler**: Used to collect data from Medium. It outputs an article
    document. It logs in to Medium and crawls the HTML of the article’s link. Then,
    it extracts, cleans, and normalizes the text from the HTML and loads the standardized
    text of the article into the NoSQL data warehouse.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Medium爬虫**：用于从Medium收集数据。它输出一个文章文档。它登录Medium并爬取文章链接的HTML。然后，它从HTML中提取、清理和标准化文本，并将标准化的文章文本加载到NoSQL数据仓库中。'
- en: '**Custom article crawler**: It performs similar steps to the Medium crawler
    but is a more generic implementation for collecting articles from various sites.
    Thus, as it doesn’t implement any particularities of any platform, it doesn’t
    perform the login step and blindly gathers all the HTML from a particular link.
    This is enough for articles freely available online, which you can find on Substack
    and people’s blogs. We will use this crawler as a safety net when the link’s domain
    isn’t associated with the other supported crawlers. For example, when providing
    a Substack link, it will default to the custom article crawler, but when providing
    a Medium URL, it will use the Medium crawler.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义文章爬虫**：它执行与Medium爬虫类似的步骤，但是一个更通用的实现，用于从各种网站收集文章。因此，由于它不实现任何特定平台的特性，它不执行登录步骤，而是盲目地从特定链接收集所有HTML。这对于在线免费提供的文章来说足够了，您可以在Substack和人们的博客上找到这些文章。当链接的域名与其它支持的爬虫不关联时，我们将使用这个爬虫作为安全网。例如，当提供Substack链接时，它将默认使用自定义文章爬虫，但当提供Medium
    URL时，它将使用Medium爬虫。'
- en: '**GitHub crawler**:This collects data from GitHub. It outputs a repository
    document. It clones the repository, parses the repository file tree, cleans and
    normalizes the files, and loads them to the database.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub爬虫**：此爬虫从GitHub收集数据。它输出一个仓库文档。它克隆仓库，解析仓库文件树，清理和标准化文件，并将它们加载到数据库中。'
- en: '**LinkedIn crawler**:This is used to collect data from LinkedIn. It outputs
    multiple post documents. It logs in to LinkedIn, navigates to the user’s feed,
    and crawls all the user’s latest posts. For each post, it extracts its HTML, cleans
    and normalizes it, and loads it to MongoDB.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LinkedIn爬虫**：此爬虫用于从LinkedIn收集数据。它输出多个帖子文档。它登录LinkedIn，导航到用户的动态，并爬取用户的所有最新帖子。对于每篇帖子，它提取其HTML，清理和标准化它，并将其加载到MongoDB中。'
- en: In the next section, we will examine each crawler’s implementation in detail.
    For now, note that each crawler accesses a specific platform or site in a particular
    way and extracts HTML from it. Afterward, all the crawlers parse the HTML, extract
    the text from it, and clean and normalize it so it can be stored in the data warehouse
    under the same interface.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细检查每个爬虫的实现。现在，请注意，每个爬虫以特定方式访问特定的平台或网站，并从中提取HTML。之后，所有爬虫都会解析HTML，从中提取文本，并清理和标准化它，以便可以在相同接口下存储在数据仓库中。
- en: By reducing all the collected data to three data categories and not creating
    a new data category for every new data source, we can easily extend this architecture
    to multiple data sources with minimal effort. For example, if we want to start
    collecting data from X, we only have to implement a new crawler that outputs a
    post document, and that’s it. The rest of the code will remain untouched. Otherwise,
    if we introduced the source dimension in the class and document structure, we
    would have to add code to all downstream layers to support any new data source.
    For example, we would have to implement a new document class for each new source
    and adapt the feature pipeline to support it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将所有收集到的数据减少到三个数据类别，而不是为每个新的数据源创建一个新的数据类别，我们可以轻松地将此架构扩展到多个数据源，而无需付出太多努力。例如，如果我们想开始从X收集数据，我们只需要实现一个新的爬虫，该爬虫输出一个帖子文档，这就足够了。其余的代码将保持不变。否则，如果我们引入了源维度到类和文档结构中，我们就需要在所有下游层中添加代码以支持任何新的数据源。例如，我们可能需要为每个新的源实现一个新的文档类，并调整特征管道以支持它。
- en: For our proof of concept, crawling a few hundred documents is enough, but if
    we want to scale it to a real-world product, we would probably need more data
    sources to crawl from. LLMs are data-hungry. Thus, you need thousands of documents
    for ideal results instead of just a few hundred. But in many projects, it’s an
    excellent strategy to implement an end-to-end project version that isn’t the most
    accurate and iterate through it later. Thus, by using this architecture, you can
    easily add more data sources in future iterations to gather a larger dataset.
    More on LLM fine-tuning and dataset size will be covered in the next chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的概念验证，爬取几百份文档就足够了，但如果我们想将其扩展到现实世界的产品，我们可能需要更多的数据源来爬取。LLMs对数据有很高的需求。因此，为了获得理想的结果，您需要数千份文档，而不仅仅是几百份。但在许多项目中，实现一个不是最精确的端到端项目版本，并在之后迭代中对其进行改进是一个很好的策略。因此，通过使用这种架构，您可以在未来的迭代中轻松添加更多数据源以收集更大的数据集。关于LLM微调和数据集大小将在下一章中详细讨论。
- en: '**How is the ETL process connected to the feature pipeline?** The feature pipeline
    ingests the raw data from the MongoDB data warehouse, cleans it further, processes
    it into features, and stores it in the Qdrant vector DB to make it accessible
    for the LLM training and inference pipelines. *Chapter 4* provides more information
    on the feature pipeline. The ETL process is independent of the feature pipeline.
    The two pipelines communicate with each other strictly through the MongoDB data
    warehouse. Thus, the data collection pipeline can write data for MongoDB, and
    the feature pipeline can read from it independently and on different schedules.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL过程是如何与特征管道连接的？** 特征管道从MongoDB数据仓库中摄取原始数据，进一步清理它，将其处理成特征，并将其存储在Qdrant向量数据库中以使其可用于LLM训练和推理管道。**第4章**提供了有关特征管道的更多信息。ETL过程独立于特征管道。两个管道严格通过MongoDB数据仓库相互通信。因此，数据收集管道可以为MongoDB写入数据，而特征管道可以独立地并且在不同的时间表上读取它。'
- en: '**Why did we use MongoDB as a data warehouse?** Using a transactional database,
    such as MongoDB, as a data warehouse is uncommon. However, in our use case, we
    are working with small amounts of data, which MongoDB can handle. Even if we plan
    to compute statistics on top of our MongoDB collections, it will work fine at
    the scale of our LLM Twin’s data (hundreds of documents). We picked MongoDB to
    store our raw data primarily because of the nature of our unstructured data: text
    crawled from the internet. By mainly working with unstructured text, selecting
    a NoSQL database that doesn’t enforce a schema made our development easier and
    faster. Also, MongoDB is stable and easy to use. Their Python SDK is intuitive.
    They provide a Docker image that works out of the box locally and a cloud freemium
    tier that is perfect for proofs of concept, such as the LLM Twin. Thus, we can
    freely work with it locally and in the cloud. However, when working with big data
    (millions of documents or more), using a dedicated data warehouse such as Snowflake
    or BigQuery will be ideal.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我们选择MongoDB作为数据仓库？** 使用像MongoDB这样的事务型数据库作为数据仓库并不常见。然而，在我们的用例中，我们处理的数据量很小，MongoDB可以轻松处理。即使我们计划在MongoDB集合上计算统计数据，它也能在我们的LLM
    Twin数据规模（数百个文档）上运行良好。我们选择MongoDB来存储原始数据的主要原因是因为我们非结构化数据的性质：从互联网爬取的文本。通过主要处理非结构化文本，选择一个不强制执行模式的NoSQL数据库使我们的开发更加容易和快速。此外，MongoDB稳定且易于使用。他们的Python
    SDK直观。他们提供了一个开箱即用的Docker镜像，以及一个适合概念验证的云免费层，例如LLM Twin。因此，我们可以在本地和云中自由地使用它。然而，当处理大数据（数百万个文档或更多）时，使用Snowflake或BigQuery等专用数据仓库将是理想的。'
- en: Now that we’ve understood the architecture of the LLM Twin’s data collection
    pipeline, let’s move on to its implementation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了LLM Twin数据收集管道的架构，让我们继续其实现。
- en: Implementing the LLM Twin’s data collection pipeline
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现LLM Twin的数据收集管道
- en: As we presented in *Chapter 2*, the entry point to each pipeline from our LLM
    Twin project is a ZenML pipeline, which can be configured at runtime through YAML
    files and run through the ZenML ecosystem. Thus, let’s start by looking into the
    ZenML `digital_data_etl` pipeline. You’ll notice that this is the same pipeline
    we used as an example in *Chapter 2* to illustrate ZenML. But this time, we will
    dig deeper into the implementation, explaining how the data collection works behind
    the scenes. After understanding how the pipeline works, we will explore the implementation
    of each crawler used to collect data from various sites and the MongoDB documents
    used to store and query data from the data warehouse.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在**第2章**中所述，LLM Twin项目的每个管道的入口点是ZenML管道，该管道可以通过YAML文件在运行时进行配置，并通过ZenML生态系统运行。因此，让我们首先了解一下ZenML的`digital_data_etl`管道。你会注意到，这是我们**第2章**中用作示例的相同管道，用于说明ZenML。但这次，我们将更深入地探讨其实现，解释数据收集背后的工作原理。在了解管道的工作原理后，我们将探索用于从各个网站收集数据的每个爬虫的实现，以及用于存储和查询数据仓库数据的MongoDB文档。
- en: ZenML pipeline and steps
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZenML管道和步骤
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure 3.3* shows a run of the `digital_data_etl` pipeline on the ZenML dashboard.
    The next phase is to explore the `get_or_create_user` and `crawl_links` ZenML
    steps individually. The step implementation is available in our repository at
    `steps/etl`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3.3**显示了在ZenML仪表板上运行的`digital_data_etl`管道。下一阶段是单独探索`get_or_create_user`和`crawl_links`
    ZenML步骤。步骤实现可在我们的存储库`steps/etl`中找到。'
- en: '![](img/B31105_03_03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_03_03.png)'
- en: 'Figure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：从ZenML仪表板运行的digital_data_etl管道示例
- en: We will start with the `get_or_create_user` ZenML step. We begin by importing
    the necessary modules and functions used throughout the script.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 `get_or_create_user` ZenML 步骤开始。我们首先导入在整个脚本中使用的必要模块和函数。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we define the function’s signature, which takes a user’s full name as
    input and retrieves an existing user or creates a new one in the MongoDB database
    if it doesn’t exist:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义函数的签名，它接受用户的全名作为输入，并检索现有的用户或（如果不存在）在 MongoDB 数据库中创建一个新的用户：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using a utility function, we split the full name into first and last names.
    Then, we attempt to retrieve the user from the database or create a new one if
    it doesn’t exist. We also retrieve the current step context and add metadata about
    the user to the output, which will be reflected in the metadata of the `user`
    ZenML output artifact:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个实用函数，我们将全名拆分为名和姓。然后，我们尝试从数据库中检索用户或创建一个新的用户（如果不存在）。我们还检索当前的步骤上下文，并将有关用户的元数据添加到输出中，这将反映在
    `user` ZenML 输出实体的元数据中：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Additionally, we define a helper function called `_get_metadata()`, which builds
    a dictionary containing the query parameters and the retrieved user information,
    which will be added as metadata to the user artifact:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还定义了一个名为 `_get_metadata()` 的辅助函数，该函数构建一个包含查询参数和检索到的用户信息的字典，这些信息将被添加到用户实体的元数据中：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will move on to the `crawl_links` ZenML step, which collects the data from
    the provided links. The code begins by importing essential modules and libraries
    for web crawling:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进入 `crawl_links` ZenML 步骤，该步骤从提供的链接中收集数据。代码首先导入用于网络爬取的必要模块和库：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Following the imports, the main function inputs a list of links written by
    a specific author. Within this function, a crawler dispatcher is initialized and
    configured to handle specific domains such as LinkedIn, Medium, and GitHub:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入之后，主函数输入由特定作者编写的链接列表。在这个函数中，初始化并配置了一个爬虫调度器，以处理特定的域名，如 LinkedIn、Medium 和 GitHub：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The function initializes variables to store the output metadata and count successful
    crawls. It then iterates over each link. It attempts to crawl and extract data
    for each link, updating the count of successful crawls and accumulating metadata
    about each URL:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 函数初始化变量以存储输出元数据和成功爬取的计数。然后，它遍历每个链接。它尝试爬取并提取每个链接的数据，更新成功爬取的计数并累积有关每个 URL 的元数据：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After processing all links, the function attaches the accumulated metadata
    to the output artifact:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完所有链接后，该函数将累积的元数据附加到输出实体上：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The code includes a helper function that attempts to extract information from
    each link using the appropriate crawler based on the link’s domain. It handles
    any exceptions that may occur during extraction and returns a tuple indicating
    the crawl’s success and the link’s domain:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中包含一个辅助函数，该函数尝试根据链接的域名使用适当的爬虫从每个链接中提取信息。它处理在提取过程中可能发生的任何异常，并返回一个元组，指示爬取的成功与否以及链接的域名：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another helper function is provided to update the metadata dictionary with
    the results of each crawl:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还提供了一个辅助函数来更新元数据字典，以包含每次爬取的结果：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As seen in the abovementioned `_crawl_link()` function, the `CrawlerDispatcher`
    class knows what crawler to initialize based on each link’s domain. The logic
    is then abstracted away under the crawler’s `extract()` method. Let’s zoom in
    on the `CrawlerDispatcher` class to understand how this works fully.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如上述 `_crawl_link()` 函数所示，`CrawlerDispatcher` 类根据每个链接的域名知道要初始化哪个爬虫。然后，逻辑被抽象到爬虫的
    `extract()` 方法下。让我们深入探讨 `CrawlerDispatcher` 类，以全面了解其工作原理。
- en: 'The dispatcher: How do you instantiate the right crawler?'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度器：如何实例化正确的爬虫？
- en: The entry point to our crawling logic is the `CrawlerDispatcher` class. As illustrated
    in *Figure 3.4*, the dispatcher acts as the intermediate layer between the provided
    links and the crawlers. It knows what crawler to associate with each URL.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们爬取逻辑的入口点是 `CrawlerDispatcher` 类。如图 3.4 所示，调度器充当提供链接和爬虫之间的中间层。它知道要将哪个爬虫与每个
    URL 关联。
- en: The `CrawlerDispatcher` class knows how to extract the domain of each link and
    initialize the proper crawler that collects the data from that site. For example,
    if it detects the [https://medium.com](https://medium.com) domain when providing
    a link to an article, it will build an instance of the `MediumCrawler` used to
    crawl that particular platform. With that in mind, let’s explore the implementation
    of the `CrawlerDispatcher` class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrawlerDispatcher`类知道如何提取每个链接的域名并初始化从该网站收集数据的适当爬虫。例如，当提供一个指向文章的链接并检测到[https://medium.com](https://medium.com)域名时，它将构建一个用于爬取该特定平台的`MediumCrawler`实例。考虑到这一点，让我们来探讨`CrawlerDispatcher`类的实现。'
- en: All the crawling logic is available in the GitHub repository at `llm_engineering/application/crawlers`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的爬虫逻辑都可以在GitHub仓库的`llm_engineering/application/crawlers`中找到。
- en: '![](img/B31105_03_04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_03_04.png)'
- en: 'Figure 3.4: The relationship between the provided links, the CrawlerDispatcher,
    and the crawlers'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：提供的链接、CrawlerDispatcher和爬虫之间的关系
- en: 'We begin by importing the necessary Python modules for URL handling and regex,
    along with importing our crawler classes:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入处理URL和正则表达式的必要Python模块，以及导入我们的爬虫类：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `CrawlerDispatcher` class is defined to manage and dispatch appropriate
    crawler instances based on given URLs and their domains. Its constructor initializes
    a registry to store the registered crawlers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrawlerDispatcher`类被定义为根据给定的URL及其域名管理和调度适当的爬虫实例。其构造函数初始化一个注册表来存储已注册的爬虫。'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we are using the builder creational pattern to instantiate and configure
    the dispatcher, we define a `build()` class method that returns an instance of
    the dispatcher:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用构建器创建模式来实例化和配置调度器，我们定义了一个`build()`类方法，该方法返回调度器的一个实例：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The dispatcher includes methods to register crawlers for specific platforms
    like Medium, LinkedIn, and GitHub. These methods use a generic `register()` method
    under the hood to add each crawler to the registry. By returning self, we follow
    the builder creational pattern (more on the builder pattern: [https://refactoring.guru/design-patterns/builder](https://refactoring.guru/design-patterns/builder)).
    We can chain multiple `register_*()` methods when instantiating the dispatcher
    as follows: `CrawlerDispatcher.build().register_linkedin().register_medium()`.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器包括用于注册特定平台（如Medium、LinkedIn和GitHub）的爬虫的方法。这些方法在底层使用通用的`register()`方法将每个爬虫添加到注册表中。通过返回self，我们遵循构建器创建模式（更多关于构建器模式：[https://refactoring.guru/design-patterns/builder](https://refactoring.guru/design-patterns/builder)）。在实例化调度器时，我们可以链式调用多个`register_*()`方法，如下所示：`CrawlerDispatcher.build().register_linkedin().register_medium()`。
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The generic `register()` method normalizes each domain to ensure its format
    is consistent before it’s added as a key to the `self._crawlers` registry of the
    dispatcher. This is a critical step, as we will use the key of the dictionary
    as the domain pattern to match future links with a crawler:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通用的`register()`方法将每个域名标准化，以确保在将其添加到调度器的`self._crawlers`注册表作为键之前其格式是一致的。这是一个关键步骤，因为我们将使用字典的键作为域名模式来匹配未来的链接与爬虫：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Finally, the `get_crawler()` method determines the appropriate crawler for a
    given URL by matching it against the registered domains. If no match is found,
    it logs a warning and defaults to using the `CustomArticleCrawler`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`get_crawler()`方法通过将给定的URL与已注册的域名进行匹配来确定适当的爬虫。如果没有找到匹配项，它将记录一个警告并默认使用`CustomArticleCrawler`。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The next step in understanding how the data collection pipeline works is analyzing
    each crawler individually.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据收集管道的工作原理的下一步是逐个分析每个爬虫。
- en: The crawlers
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爬虫
- en: 'Before exploring each crawler’s implementation, we must present their base
    class, which defines a unified interface for all the crawlers. As shown in *Figure
    3.4*, we can implement the dispatcher layer because each crawler follows the same
    signature. Each class implements the `extract()` method, allowing us to leverage
    OOP techniques such as polymorphism, where we can work with abstract objects without
    knowing their concrete subclass. For example, in the `_crawl_link()` function
    from the ZenML steps, we had the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索每个爬虫的实现之前，我们必须介绍它们的基类，该基类为所有爬虫定义了一个统一的接口。如图*3.4*所示，我们可以实现调度器层，因为每个爬虫遵循相同的签名。每个类都实现了`extract()`方法，这使得我们可以利用面向对象技术，如多态，在不了解其具体子类的情况下与抽象对象一起工作。例如，在ZenML步骤中的`_crawl_link()`函数中，我们有以下代码：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note how we called the `extract()` method without caring about what specific
    type of crawler we instantiated. To conclude, working with abstract interfaces
    ensures core reusability and ease of extension.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何调用`extract()`方法而不关心我们实例化了哪种具体的爬虫类型。总结来说，使用抽象接口确保了核心的可重用性和扩展的便捷性。
- en: Base classes
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础类
- en: Now, let’s explore the `BaseCrawler` interface, which can be found in the repository
    at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/base.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/base.py).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索`BaseCrawler`接口，它可以在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/base.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/base.py)仓库中找到。
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As mentioned above, the interface defines an `extract()` method that takes as
    input a link. Also, it defines a model attribute at the class level that represents
    the data category document type used to save the extracted data into the MongoDB
    data warehouse. Doing so allows us to customize each subclass with different data
    categories while preserving the same attributes at the class level. We will soon
    explore the `NoSQLBaseDocument` class when digging into the document entities.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，接口定义了一个`extract()`方法，它接受一个链接作为输入。它还在类级别定义了一个模型属性，代表用于将提取的数据保存到MongoDB数据仓库的数据类别文档类型。这样做允许我们使用不同的数据类别自定义每个子类，同时保留类级别的相同属性。当我们深入研究文档实体时，我们将很快探索`NoSQLBaseDocument`类。
- en: We also extend the `BaseCrawler` class with a `BaseSeleniumCrawler` class, which
    implements reusable functionality that uses Selenium to crawl various sites, such
    as Medium or LinkedIn. **Selenium** is a tool for automating web browsers. It’s
    used to interact with web pages programmatically (like logging into LinkedIn,
    navigating through profiles, etc.).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过`BaseSeleniumCrawler`类扩展了`BaseCrawler`类，该类实现了使用Selenium爬取各种网站的可重用功能，例如Medium或LinkedIn。**Selenium**是一个自动化网页浏览器的工具。它用于以编程方式与网页交互（如登录LinkedIn，浏览个人资料等）。
- en: Selenium can programmatically control various browsers such as Chrome, Firefox,
    or Brave. For these specific platforms, we need Selenium to manipulate the browser
    programmatically to log in and scroll through the newsfeed or article before being
    able to extract the entire HTML. For other sites, where we don’t have to go through
    the login step or can directly load the whole page, we can extract the HTML from
    a particular URL using more straightforward methods than Selenium.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Selenium可以以编程方式控制各种浏览器，如Chrome、Firefox或Brave。对于这些特定平台，我们需要使用Selenium以编程方式操作浏览器以登录并滚动新闻源或文章，然后才能提取整个HTML。对于其他网站，我们不需要经过登录步骤或可以直接加载整个页面，我们可以使用比Selenium更直接的方法从特定URL提取HTML。
- en: For the Selenium-based crawlers to work, you must install Chrome on your machine
    (or a Chromium-based browser such as Brave).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使基于Selenium的爬虫能够工作，您必须在您的机器上安装Chrome（或如Brave这样的基于Chromium的浏览器）。
- en: The code begins by setting up the necessary imports and configurations for web
    crawling using Selenium and the ChromeDriver initializer. The `chromedriver_autoinstaller`
    ensures that the appropriate version of ChromeDriver is installed and added to
    the system path, maintaining compatibility with the installed version of your
    Google Chrome browser (or other Chromium-based browser). Selenium will use the
    ChromeDriver to communicate with the browser and open a headless session, where
    we can programmatically manipulate the browser to access various URLs, click on
    specific elements, such as buttons, or scroll through the newsfeed. Using the
    `chromedriver_autoinstaller`, we ensure we always have the correct ChromeDriver
    version installed that matches our machine’s Chrome browser version.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先设置了使用Selenium和ChromeDriver初始化器进行网络爬取所需的必要导入和配置。`chromedriver_autoinstaller`确保安装了正确的ChromeDriver版本并将其添加到系统路径中，以保持与已安装的Google
    Chrome浏览器（或其他基于Chromium的浏览器）的兼容性。Selenium将使用ChromeDriver与浏览器通信并打开一个无头会话，在那里我们可以通过编程方式操作浏览器以访问各种URL，点击特定的元素，如按钮，或滚动新闻源。使用`chromedriver_autoinstaller`，我们确保始终安装了与我们的机器Chrome浏览器版本匹配的正确ChromeDriver版本。
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we define the `BaseSeleniumCrawler` class for use cases where we need
    Selenium to collect the data, such as collecting data from Medium or LinkedIn.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了`BaseSeleniumCrawler`类，用于需要使用Selenium来收集数据的用例，例如从Medium或LinkedIn收集数据。
- en: 'Its constructor initializes various Chrome options to optimize performance,
    enhance security, and ensure a headless browsing environment. These options disable
    unnecessary features like GPU rendering, extensions, and notifications, which
    can interfere with automated browsing. These are standard configurations when
    crawling in headless mode:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其构造函数初始化各种Chrome选项以优化性能、增强安全性和确保无头浏览环境。这些选项禁用了如GPU渲染、扩展和通知等不必要的功能，这些功能可能会干扰自动化浏览。这些是在无头模式下爬取时的标准配置：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After configuring the Chrome options, the code allows subclasses to set any
    additional driver options by calling the `set_extra_driver_options()` method.
    It then initializes the scroll limit and creates a new instance of the Chrome
    driver with the specified options:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置Chrome选项后，代码允许子类通过调用`set_extra_driver_options()`方法设置任何额外的驱动程序选项。然后，它初始化滚动限制并创建一个新的Chrome驱动程序实例，带有指定的选项：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `BaseSeleniumCrawler` class includes placeholder methods for `set_extra_driver_options()`
    and `login()`, which subclasses can override to provide specific functionality.
    This ensures modularity, as every platform has a different login page with a different
    HTML structure:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseSeleniumCrawler`类包括`set_extra_driver_options()`和`login()`的占位符方法，子类可以覆盖以提供特定功能。这确保了模块化，因为每个平台都有一个不同的登录页面，具有不同的HTML结构：'
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, the `scroll_page()` method implements a scrolling mechanism to navigate
    through pages, such as LinkedIn, up to a specified scroll limit. It scrolls to
    the bottom of the page, waits for new content to load, and repeats the process
    until it reaches the end of the page or the scroll limit is exceeded. This method
    is essential for feeds where the content appears as the user scrolls:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`scroll_page()`方法实现了一个滚动机制，用于导航到指定滚动限制的页面，例如LinkedIn。它滚动到页面底部，等待新内容加载，并重复此过程，直到达到页面底部或超过滚动限制。此方法对于内容随用户滚动而出现的动态内容流至关重要：
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We’ve understood what the base classes of our crawlers look like. Next, we
    will look into the implementation of the following specific crawlers:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了我们爬虫的基类是什么样的。接下来，我们将探讨以下特定爬虫的实现：
- en: '`GitHubCrawler(BaseCrawler)`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GitHubCrawler(BaseCrawler)`'
- en: '`CustomArticleCrawler(BaseCrawler)`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomArticleCrawler(BaseCrawler)`'
- en: '`MediumCrawler(BaseSeleniumCrawler)`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MediumCrawler(BaseSeleniumCrawler)`'
- en: You can find the implementation of the above crawlers in the GitHub repository
    at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main /llm_engineering/application/crawlers](https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/application/crawlers).
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在GitHub仓库中找到上述爬虫的实现，网址为[https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/application/crawlers](https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/application/crawlers)。
- en: GitHubCrawler class
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHubCrawler类
- en: 'The `GithubCrawler` class is designed to scrape GitHub repositories, extending
    the functionality of the `BaseCrawler`. We don’t have to log in to GitHub through
    the browser, as we can leverage Git’s clone functionality. Thus, we don’t have
    to leverage any Selenium functionality. Upon initialization, it sets up a list
    of patterns to ignore standard files and directories found in GitHub repositories,
    such as `.git`, `.toml`, `.lock`, and `.png`, ensuring that unnecessary files
    are excluded from the scraping process:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`GithubCrawler`类旨在抓取GitHub仓库，扩展了`BaseCrawler`的功能。我们不需要通过浏览器登录GitHub，因为我们可以利用Git的克隆功能。因此，我们不需要利用任何Selenium功能。初始化时，它设置了一个要忽略的图案列表，以排除GitHub仓库中发现的标准文件和目录，如`.git`、`.toml`、`.lock`和`.png`，确保不必要的文件被排除在抓取过程之外：'
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we implement the `extract()` method, where the crawler first checks if
    the repository has already been processed and stored in the database. If it exists,
    it exits the method to prevent storing duplicates:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现`extract()`方法，其中爬虫首先检查仓库是否已经被处理并存储在数据库中。如果存在，它将退出方法以防止存储重复项：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If the repository is new, the crawler extracts the repository name from the
    link. Then, it creates a temporary directory to clone the repository to ensure
    that the cloned repository is cleaned up from the local disk after it’s processed:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仓库是新的，爬虫会从链接中提取仓库名称。然后，它创建一个临时目录来克隆仓库，以确保在处理完毕后从本地磁盘清理克隆的仓库：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Within a try block, the crawler changes the current working directory to the
    `temporary` directory and executes the `git clone` command in a different process:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在try块中，爬虫将当前工作目录更改为`temporary`目录，并在不同的进程中执行`git clone`命令：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After successfully cloning the repository, the crawler constructs the path
    to the cloned repository. It initializes an empty dictionary used to aggregate
    the content of the files in a standardized way. It walks through the directory
    tree, skipping over any directories or files that match the ignore patterns. For
    each relevant file, it reads the content, removes any spaces, and stores it in
    the dictionary with the file path as the key:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 成功克隆存储库后，爬虫构建了克隆存储库的路径。它初始化一个空字典，用于以标准化的方式聚合文件内容。它遍历目录树，跳过任何匹配忽略模式的目录或文件。对于每个相关文件，它读取内容，删除任何空格，并以文件路径作为键将其存储在字典中：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It then creates a new instance of the `RepositoryDocument` model, populating
    it with the repository content, name, link, platform information, and author details.
    The instance is then saved to MongoDB:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，它创建了一个新的`RepositoryDocument`模型实例，并用存储库内容、名称、链接、平台信息和作者详情填充。然后，这个实例被保存到MongoDB中：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, whether the scraping succeeds or an exception occurs, the crawler
    ensures that the temporary directory is removed to clean up any resources used
    during the process:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，无论抓取是否成功或发生异常，爬虫都会确保临时目录被删除，以清理在过程中使用的任何资源：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: CustomArticleCrawler class
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CustomArticleCrawler类
- en: 'The `CustomArticleCrawler` class takes a different approach to collecting data
    from the internet. It leverages the `AsyncHtmlLoader` class to read the entire
    HTML from a link and the `Html2TextTransformer` class to extract the text from
    that HTML. Both classes are made available by the `langchain_community` Python
    package, as seen below, where we import all the necessary Python modules:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`CustomArticleCrawler`类以不同的方式从互联网收集数据。它利用`AsyncHtmlLoader`类从链接中读取整个HTML，并使用`Html2TextTransformer`类从该HTML中提取文本。这两个类由`langchain_community`
    Python包提供，如下所示，其中我们导入了所有必要的Python模块：'
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we define the `CustomArticleCrawler` class, which inherits from `BaseCrawler`.
    As before, we don’t need to log in or use the scrolling functionality provided
    by Selenium. In the `extract` method, we first check if the article exists in
    the database to avoid duplicating content:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`CustomArticleCrawler`类，它继承自`BaseCrawler`。和之前一样，我们不需要登录或使用Selenium提供的滚动功能。在`extract`方法中，我们首先检查文章是否存在于数据库中，以避免内容重复：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If the article doesn’t exist, we proceed to scrape it. We use the `AsyncHtmlLoader`
    class to load the HTML from the provided link. After, we transform it into plain
    text using the `Html2TextTransformer` class, which returns a list of documents.
    We are only interested in the first document. As we delegate the whole logic to
    these two classes, we don’t control how the content is extracted and parsed. That’s
    why we used this class as a fallback system for domains where we don’t have anything
    custom implemented. These two classes follow the LangChain paradigm, which provides
    high-level functionality that works decently in most scenarios. It is fast to
    implement but hard to customize. That is one of the reasons why many developers
    avoid using LangChain in production use cases:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文章不存在，我们继续抓取它。我们使用`AsyncHtmlLoader`类从提供的链接中加载HTML。之后，我们使用`Html2TextTransformer`类将其转换为纯文本，该类返回一个文档列表。我们只对第一个文档感兴趣。由于我们将整个逻辑委托给这两个类，我们无法控制内容是如何提取和解析的。这就是为什么我们使用这个类作为没有自定义实现的域的回退系统。这两个类遵循LangChain范式，它提供了在大多数场景中表现良好的高级功能。它实现速度快，但定制困难。这也是许多开发者在生产用例中避免使用LangChain的原因之一：
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We get the page content from the extracted document, plus relevant metadata
    such as the `title`, `subtitle`, `content`, and `language`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从提取的文档中获取页面内容，以及相关的元数据，如`标题`、`副标题`、`内容`和`语言`：
- en: '[PRE35]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we parse the URL to determine the platform (or domain) from which the
    article was scraped:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们解析URL以确定文章是从哪个平台（或域名）抓取的：
- en: '[PRE36]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We then create a new instance of the article model, populating it with the
    extracted content. Finally, we save this instance to the MongoDB data warehouse:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个文章模型的新实例，并用提取的内容填充它。最后，我们将这个实例保存到MongoDB数据仓库中：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So far, we have seen how to crawl GitHub repositories and random sites using
    LangChain utility functions. Lastly, we must explore a crawler using Selenium
    to manipulate the browser programmatically. Thus, we will continue with the `MediumCrawler`
    implementation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何使用LangChain实用函数抓取GitHub存储库和随机网站。最后，我们必须探索一个使用Selenium来编程操作浏览器的爬虫。因此，我们将继续`MediumCrawler`的实现。
- en: MediumCrawler class
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MediumCrawler类
- en: 'The code begins by importing essential libraries and defining the `MediumCrawler`
    class, which inherits from `BaseSeleniumCrawler`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先导入必要的库并定义`MediumCrawler`类，该类继承自`BaseSeleniumCrawler`：
- en: '[PRE38]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Within the `MediumCrawler` class, we leverage the `set_extra_driver_options()`
    method to extend the default driver options used by Selenium:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MediumCrawler`类中，我们利用`set_extra_driver_options()`方法来扩展Selenium使用的默认驱动选项：
- en: '[PRE39]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `extract()` method implements the core functionality, first checking whether
    the article exists in the database to prevent duplicate entries.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract()`方法实现了核心功能，首先检查文章是否存在于数据库中，以防止重复条目。'
- en: 'If the article is new, the method proceeds to navigate to the article’s link
    and scroll through the page to ensure all content is loaded:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文章是新的，该方法将继续导航到文章的链接并滚动页面以确保所有内容都已加载：
- en: '[PRE40]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'After fully loading the page, the method uses `BeautifulSoup` to parse the
    HTML content and extract the article’s title, subtitle, and full text. `BeautifulSoup`
    is a popular Python library for web scraping and parsing HTML or XML documents.
    Thus, we used it to extract all the HTML elements we needed from the HTML accessed
    with Selenium. Finally, we aggregate everything into a dictionary:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在页面完全加载后，该方法使用`BeautifulSoup`解析HTML内容并提取文章的标题、副标题和全文。`BeautifulSoup`是一个流行的Python库，用于网络爬取和解析HTML或XML文档。因此，我们使用它从Selenium访问的HTML中提取所有需要的HTML元素。最后，我们将所有内容聚合到一个字典中：
- en: '[PRE41]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, the method closes the WebDriver to free up resources. It then creates
    a new `ArticleDocument` instance, populates it with the extracted content and
    user information provided via `kwargs`, and saves it to the database:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，该方法关闭WebDriver以释放资源。然后，它创建一个新的`ArticleDocument`实例，用通过`kwargs`提供的提取内容和使用户信息填充它，并将其保存到数据库中：
- en: '[PRE42]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: With that, we conclude the `MediumCrawler` implementation. The LinkedIn crawler
    follows a similar pattern to the Medium one, where it uses Selenium to log in
    and access the feed of a user’s latest posts. Then, it extracts the posts and
    scrolls through the feed to load the next page until a limit is hit. You can check
    the full implementation in our repository at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就完成了`MediumCrawler`的实现。LinkedIn爬虫遵循与Medium类似的模式，它使用Selenium登录并访问用户最新帖子的动态内容。然后，它提取帖子并滚动动态内容以加载下一页，直到达到限制。您可以在我们的仓库中查看完整的实现：[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py)。
- en: With the rise of LLMs, collecting data from the internet has become a critical
    step in many real-world AI applications. Hence, more high-level tools have appeared
    in the Python ecosystem, such as Scrapy ([https://github.com/scrapy/scrapy](https://github.com/scrapy/scrapy)),
    which crawls websites and extracts structured data from their pages, and Crawl4AI
    ([https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)),
    which is highly specialized in crawling data for LLMs and AI applications.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs的兴起，从互联网收集数据已成为许多现实世界AI应用的关键步骤。因此，Python生态系统出现了更多高级工具，例如Scrapy ([https://github.com/scrapy/scrapy](https://github.com/scrapy/scrapy))，它爬取网站并从其页面中提取结构化数据，以及Crawl4AI
    ([https://github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai))，它高度专业于为LLMs和AI应用爬取数据。
- en: 'In this section, we’ve looked at implementing three types of crawlers: one
    that leverages the `git` executable in a subprocess to clone GitHub repositories,
    one that uses LangChain utilities to extract the HTML of a single web page, and
    one that leverages Selenium for more complex scenarios where we have to navigate
    through the login page, scroll the article to load the entire HTML, and extract
    it into text format. The last step is understanding how the document classes we’ve
    used across the chapter, such as the `ArticleDocument`, work.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了实现三种类型的爬虫：一种利用子进程中的`git`可执行文件来克隆GitHub仓库的爬虫，一种使用LangChain工具提取单个网页的HTML的爬虫，以及一种利用Selenium在更复杂的场景中导航登录页面、滚动文章以加载整个HTML并将其提取为文本格式的爬虫。最后一步是理解我们本章中使用的文档类，如`ArticleDocument`，是如何工作的。
- en: The NoSQL data warehouse documents
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NoSQL数据仓库文档
- en: We had to implement three document classes to structure our data categories.
    These classes define the specific attributes we require for a document, such as
    the content, author, and source link. It is best practice to structure your data
    in classes instead of dictionaries, as the attributes we expect for each item
    are more verbose, reducing run errors. For example, when accessing a value from
    a Python dictionary, we can never be sure it is present or its type is current.
    By wrapping our data items with classes, we can ensure each attribute is as expected.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不得不实现三个文档类来结构化我们的数据类别。这些类定义了我们需要的特定属性，例如内容、作者和来源链接。将数据结构化在类中而不是字典中是一种最佳实践，因为每个项目预期的属性更加详细，这可以减少运行错误。例如，当我们从一个Python字典中访问值时，我们永远无法确定它是否存在或其类型是否正确。通过将我们的数据项包装在类中，我们可以确保每个属性都符合预期。
- en: 'By leveraging Python packages such as Pydantic, we have out-of-the-box type
    validation, which ensures consistency in our datasets. Thus, we modeled the data
    categories as the following document classes, which we already used in the code
    up until point:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 Python 包如 Pydantic，我们获得了开箱即用的类型验证，这确保了数据集的一致性。因此，我们将数据类别建模为以下文档类，这些类我们在代码中直到该点已经使用过：
- en: '`ArticleDocument` class'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ArticleDocument` 类'
- en: '`PostDocument` class'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PostDocument` 类'
- en: '`RepositoryDocument` class'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RepositoryDocument` 类'
- en: These are not simple Python data classes or Pydantic models. They support read
    and write operations on top of the MongoDB data warehouse. To inject the read-and-write
    functionality into all the document classes without repeating any code, we used
    the **Object-Document Mapping** (ODM) software pattern, which is based on the
    **object-relational mapping** (**ORM**) pattern. Thus, let’s first explore ORM,
    then move to ODM, and, finally, dig into our custom ODM implementation and document
    classes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不仅仅是简单的 Python 数据类或 Pydantic 模型。它们支持在 MongoDB 数据仓库上执行读写操作。为了将读写功能注入到所有文档类中而不重复任何代码，我们使用了
    **对象-文档映射**（ODM）软件模式，该模式基于 **对象关系映射**（ORM）模式。因此，让我们首先探索 ORM，然后转向 ODM，最后深入我们的自定义
    ODM 实现和文档类。
- en: The ORM and ODM software patterns
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ORM 和 ODM 软件模式
- en: Before we talk about software patterns, let’s see what ORM is. It’s a technique
    that lets you query and manipulate data from a database using an object-oriented
    paradigm. Instead of writing SQL or API-specific queries, you encapsulate all
    the complexity under an ORM class that knows how to handle all the database operations,
    most commonly CRUD operations. Thus, working with ORM removes the need to handle
    the database operations manually and reduces the need to write boilerplate code
    manually. An ORM interacts with a SQL database, such as PostgreSQL or MySQL.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论软件模式之前，让我们看看什么是 ORM。它是一种技术，允许你使用面向对象的方法查询和操作数据库中的数据。而不是编写 SQL 或 API 特定的查询，你将所有复杂性封装在一个
    ORM 类中，该类知道如何处理所有数据库操作，最常见的是 CRUD 操作。因此，使用 ORM 可以消除手动处理数据库操作的需要，并减少手动编写样板代码的需要。ORM
    与 SQL 数据库（如 PostgreSQL 或 MySQL）交互。
- en: Most modern Python applications use ORMs when interacting with the database.
    Even though SQL is still a popular choice in the data world, you rarely see raw
    SQL queries in Python backend components. The most popular Python ORM is SQLAlchemy
    ([https://www.sqlalchemy.org/](https://www.sqlalchemy.org/)). Also, with the rise
    of FastAPI, SQLModel is ([https://github.com/fastapi/sqlmodel](https://github.com/fastapi/sqlmodel))
    a common choice, which is a wrapper over SQLAlchemy that makes the integration
    easier with FastAPI.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代 Python 应用程序在与数据库交互时使用 ORM。尽管 SQL 在数据世界中仍然是一个流行的选择，但在 Python 后端组件中很少看到原始
    SQL 查询。最流行的 Python ORM 是 SQLAlchemy ([https://www.sqlalchemy.org/](https://www.sqlalchemy.org/))。此外，随着
    FastAPI 的兴起，SQLModel ([https://github.com/fastapi/sqlmodel](https://github.com/fastapi/sqlmodel))
    成为了一个常见选择，它是一个 SQLAlchemy 的包装器，使得与 FastAPI 的集成更加容易。
- en: For example, using SQLAlchemy, we defined a `User` ORM with the ID and name
    fields. The `User` ORM is mapped to the `users` table within the SQL database.
    Thus, when we create a new user and commit it to the database, it is automatically
    saved to the `users` table. The same applies to all the CRUD operations on top
    of the `User` class.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 SQLAlchemy，我们定义了一个包含 ID 和名称字段的 `User` ORM。`User` ORM 映射到 SQL 数据库中的 `users`
    表。因此，当我们创建一个新用户并将其提交到数据库时，它将自动保存到 `users` 表中。对 `User` 类的所有 CRUD 操作也是如此。
- en: '[PRE43]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Using the `User` ORM, we can quickly insert or query users directly from Python
    without writing a line of SQL. Note that an ORM usually supports all **CRUD**
    operations. Here is a code snippet that shows how to save an instance of the User
    ORM to a SQLite database:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `User` ORM，我们可以快速地从 Python 中直接插入或查询用户，而不需要写一行 SQL。请注意，ORM 通常支持所有 **CRUD**
    操作。以下是一个代码片段，展示了如何将 User ORM 的一个实例保存到 SQLite 数据库中：
- en: '[PRE44]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Also, this is how we can query a user from the `users` SQLite table:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这就是我们从 `users` SQLite 表中查询用户的方式：
- en: '[PRE45]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Find the entire script and how to run it in the GitHub repository at `code_snippets/03_orm.py`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 仓库 `code_snippets/03_orm.py` 中找到整个脚本及其运行方法。
- en: The ODM pattern is extremely similar to ORM, but instead of working with SQL
    databases and tables, it works with NoSQL databases (such as MongoDB) and unstructured
    collections. As we work with NoSQL databases, the data structure is centered on
    collections, which store JSON-like documents rather than rows in tables.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ODM 模式与 ORM 非常相似，但它不是与 SQL 数据库和表一起工作，而是与 NoSQL 数据库（如 MongoDB）和非结构化集合一起工作。当我们与
    NoSQL 数据库一起工作时，数据结构以集合为中心，这些集合存储类似于 JSON 的文档，而不是表中的行。
- en: To conclude, ODM simplifies working with document-based NoSQL databases and
    maps object-oriented code to JSON-like documents. We will implement a light ODM
    module on top of MongoDB to fully understand how ODM works.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，ODM 简化了与基于文档的 NoSQL 数据库的工作，并将面向对象的代码映射到类似于 JSON 的文档。我们将在 MongoDB 之上实现一个轻量级的
    ODM 模块，以完全理解 ODM 的工作原理。
- en: Implementing the ODM class
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 ODM 类
- en: This section will explore how to implement an ODM class from scratch. This is
    an excellent exercise to learn how ODM works and sharpen our skills in writing
    modular and reusable Python classes. Hence, we will implement a base ODM class
    called `NoSQLBaseDocument`, from which all the other documents will inherit to
    interact with the MongoDB data warehouse.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨如何从头开始实现一个 ODM 类。这是一个很好的练习，可以学习 ODM 的工作原理并提高我们编写模块化和可重用 Python 类的技能。因此，我们将实现一个名为
    `NoSQLBaseDocument` 的基础 ODM 类，其他所有文档都将从中继承以与 MongoDB 数据仓库交互。
- en: The class can be found in our repository at `llm_engineering/domain/base/nosql.py`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该类可以在我们的仓库中找到，位于 `llm_engineering/domain/base/nosql.py`。
- en: 'The code starts by importing essential modules and setting up the database
    connection. Through the `_database` variable, we establish a connection to the
    database specified in the settings, which is by default called `twin`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先导入必要的模块并设置数据库连接。通过 `_database` 变量，我们连接到设置中指定的数据库，默认名为 `twin`：
- en: '[PRE46]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we define a type variable `T` bound to the `NoSQLBaseDocument` class.
    The variable leverages Python’s generic module, allowing us to generalize the
    class’s types. For example, when we implement the `ArticleDocument` class, which
    will inherit from the `NoSQLBaseDocument` class, all the instances where `T` was
    used will be replaced with the `ArticleDocument` type when analyzing the signature
    of functions (more on Python generics: [https://realpython.com/python312-typing](https://realpython.com/python312-typing)).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个类型变量 `T`，它绑定到 `NoSQLBaseDocument` 类。该变量利用 Python 的泛型模块，使我们能够泛化类的类型。例如，当我们实现继承自
    `NoSQLBaseDocument` 类的 `ArticleDocument` 类时，所有使用 `T` 的实例在分析函数签名时都将被替换为 `ArticleDocument`
    类型（更多关于 Python 泛型的信息：[https://realpython.com/python312-typing](https://realpython.com/python312-typing))。
- en: 'The `NoSQLBaseDocument` class is then declared as an abstract base class inheriting
    from Pydantic’s BaseModel, Python’s Generic (which provides the functionality
    described earlier), and `ABC` (making the class abstract) classes. This class
    serves as the foundational ODM class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`NoSQLBaseDocument` 类被声明为一个继承自 Pydantic 的 BaseModel、Python 的 Generic（提供前面描述的功能）和
    `ABC`（使该类成为抽象类）类的抽象基类。这个类作为基础 ODM 类：
- en: '[PRE47]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Within the `NoSQLBaseDocument` class, an id field is defined as a UUID4, with
    a default factory generating a unique UUID. The class also implements the `__eq__`
    and `__hash__` methods to allow instances to be compared and used in hashed collections
    like sets or as dictionary keys based on their unique `id` attribute:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `NoSQLBaseDocument` 类中，定义了一个 id 字段，其类型为 UUID4，默认工厂生成一个唯一的 UUID。该类还实现了 `__eq__`
    和 `__hash__` 方法，以便实例可以被比较，并可以在基于其唯一 `id` 属性的集合或字典键等散列集合中使用：
- en: '[PRE48]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The class provides methods for converting between MongoDB documents and class
    instances. The `from_mongo()` class method transforms a dictionary retrieved from
    MongoDB into an instance of the class. The `to_mongo()` instance method converts
    the model instance into a dictionary suitable for MongoDB insertion:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 该类提供了在 MongoDB 文档和类实例之间进行转换的方法。`from_mongo()` 类方法将从 MongoDB 检索到的字典转换为类的实例。`to_mongo()`
    实例方法将模型实例转换为适合 MongoDB 插入的字典：
- en: '[PRE49]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `save()` method allows an instance of the model to be inserted into a MongoDB
    collection. It retrieves the appropriate collection, converts the instance into
    a MongoDB-compatible document leveraging the `to_mongo()` method described above,
    and attempts to insert it into the database, handling any write errors that may
    occur:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`save()` 方法允许将模型实例插入到 MongoDB 集合中。它检索适当的集合，利用上述 `to_mongo()` 方法将实例转换为 MongoDB
    兼容的文档，并尝试将其插入到数据库中，处理可能发生的任何写入错误：'
- en: '[PRE50]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The `get_or_create()` class method attempts to find a document in the database
    matching the provided filter options. If a matching document is found, it is converted
    into an instance of the class. If not, a new instance is created with the filter
    options as its initial data and saved to the database:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_or_create()` 类方法尝试在数据库中找到与提供的过滤器选项匹配的文档。如果找到匹配的文档，则将其转换为类的实例。如果没有找到，则创建一个新的实例，其初始数据为过滤器选项，并将其保存到数据库中：'
- en: '[PRE51]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `bulk_insert()` class method allows multiple documents to be inserted into
    the database at once:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`bulk_insert()` 类方法允许一次将多个文档插入到数据库中：'
- en: '[PRE52]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `find()` class method searches for a single document in the database that
    matches the given filter options:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`find()` 类方法在数据库中搜索单个文档，该文档与给定的过滤器选项匹配：'
- en: '[PRE53]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Similarly, the `bulk_find()` class method retrieves multiple documents matching
    the filter options. It converts each retrieved MongoDB document into a model instance,
    collecting them into a list:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`bulk_find()` 类方法检索与过滤器选项匹配的多个文档。它将每个检索到的 MongoDB 文档转换为模型实例，并将它们收集到一个列表中：
- en: '[PRE54]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, the `get_collection_name()` class method determines the name of the
    MongoDB collection associated with the class. It expects the class to have a nested
    `Settings` class with a name attribute specifying the collection name. If this
    configuration is missing, an `ImproperlyConfigured` exception will be raised specifying
    that the subclass should define a nested `Settings` class:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`get_collection_name()` 类方法确定与该类关联的 MongoDB 集合的名称。它期望该类有一个嵌套的 `Settings`
    类，其中包含一个指定集合名称的 `name` 属性。如果缺少此配置，将引发一个 `ImproperlyConfigured` 异常，指定子类应定义一个嵌套的
    `Settings` 类：
- en: '[PRE55]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We can configure each subclass using the nested `Settings` class, such as defining
    the collection name, or anything else specific to that subclass. Within the Python
    ecosystem, there is an ODM implementation on top of MongoDB, called `mongoengine`,
    which you can find on GitHub. It follows a pattern similar to ours but more comprehensive.
    We implemented it by ourselves, as it was an excellent exercise to practice writing
    modular and generic code following best OOP principles, which are essential for
    implementing production-level code.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用嵌套的 `Settings` 类配置每个子类，例如定义集合名称，或任何特定于该子类的其他内容。在 Python 生态系统内，有一个基于 MongoDB
    的 ODM 实现，称为 `mongoengine`，您可以在 GitHub 上找到它。它遵循与我们的类似但更全面的模式。我们自行实现了它，因为它是一个练习编写模块化和通用代码的绝佳机会，遵循最佳
    OOP 原则，这对于实现生产级代码至关重要。
- en: Data categories and user document classes
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据类别和用户文档类
- en: The last piece of the puzzle is to see the implementation of the subclasses
    that inherit from the `NoSQLBaseDocument` base class. These are the concrete classes
    that define our data categories. You’ve seen these classes used across the chapter
    when working with articles, repositories, and posts within the crawler classes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个拼图是查看从 `NoSQLBaseDocument` 基类继承的子类的实现。这些是定义我们的数据类别的具体类。您在处理爬虫类中的文章、存储库和帖子时已经看到了这些类的使用。
- en: 'We begin by importing the essential Python modules and the ODM base class:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的 Python 模块和 ODM 基类：
- en: '[PRE56]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We define an `enum` class, where we centralize all our data category types.
    These variables will act as constants in configuring all our ODM classes throughout
    the book.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个 `enum` 类，其中我们集中所有我们的数据类别类型。这些变量将在本书中配置所有我们的 ODM 类时作为常量使用。
- en: The class can be found in the repository at `llm_engineering/domain/types.py`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 该类可以在 `llm_engineering/domain/types.py` 仓库中找到。
- en: '[PRE57]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The `Document` class is introduced as an abstract base model for other documents
    on top of the `NoSQLBaseDocument` ODM class. It includes common attributes like
    content, platform, and author details, providing a standardized structure for
    documents that will inherit from it:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`Document`类被引入作为`NoSQLBaseDocument` ODM类之上的其他文档的抽象基模型。它包括常见的属性，如内容、平台和作者详情，为将继承它的文档提供了一个标准化的结构：'
- en: '[PRE58]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, specific document types are defined by extending the `Document` class.
    The `RepositoryDocument`, `PostDocument`, and `ArticleDocument` classes represent
    different categories of data, each with unique fields and settings that specify
    their respective collection names in the database:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过扩展`Document`类定义特定的文档类型。`RepositoryDocument`、`PostDocument`和`ArticleDocument`类代表不同的数据类别，每个类别都有独特的字段和设置，指定其在数据库中的相应集合名称：
- en: '[PRE59]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, we define the `UserDocument` class, which is used to store and query
    all the users from the LLM Twin project:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了`UserDocument`类，用于存储和查询LLM Twin项目中的所有用户：
- en: '[PRE60]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: By implementing the `NoSQLBaseDocument` ODM class, we had to focus solely on
    the fields and specific functionality of each document or domain entity. All the
    CRUD functionality is delegated to the parent class. Also, by leveraging Pydantic
    to define the fields, we have out-of-the-box type validation. For example, when
    creating an instance of the `ArticleDocument` class, if the provided link is `None`
    or not a string, it will throw an error signaling that the data is invalid.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现`NoSQLBaseDocument` ODM类，我们必须专注于每个文档或域实体的字段和特定功能。所有CRUD功能都委派给了父类。此外，通过利用Pydantic来定义字段，我们获得了开箱即用的类型验证。例如，当创建`ArticleDocument`类的实例时，如果提供的链接是`None`或不是字符串，它将抛出一个错误，表示数据无效。
- en: With that, we’ve finished implementing our data collection pipeline, starting
    with the ZenML components. Then, we looked into the implementation of the crawlers
    and, finally, wrapped it up with the ODM class and data category documents. The
    last step is to run the data collection pipeline and ingest raw data into the
    MongoDB data warehouse.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经完成了数据收集管道的实现，从ZenML组件开始，然后研究了爬虫的实现，最后用ODM类和数据类别文档结束了整个过程。最后一步是运行数据收集管道并将原始数据导入MongoDB数据仓库。
- en: Gathering raw data into the data warehouse
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将原始数据收集到数据仓库中
- en: ZenML orchestrates the data collection pipeline. Thus, leveraging ZenML, the
    data collection pipeline can be run manually, scheduled, or triggered by specific
    events. Here, we will show you how to run it manually, while we will discuss the
    other scenarios in *Chapter 11* when digging deeper into MLOps.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML协调数据收集管道。因此，利用ZenML，数据收集管道可以手动运行、计划或由特定事件触发。在这里，我们将向您展示如何手动运行它，而我们将讨论在*第11章*中深入挖掘MLOps的其他场景。
- en: 'We configured a different pipeline run for each author. We provided a ZenML
    configuration file for Paul Iusztin’s or Maxime Labonne’s data. To call the data
    collection pipeline to collect Maxime’s data, for example, you can run the following
    CLI command:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每位作者配置了不同的管道运行。我们为Paul Iusztin或Maxime Labonne的数据提供了ZenML配置文件。例如，要调用数据收集管道以收集Maxime的数据，您可以运行以下CLI命令：
- en: '[PRE61]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'That will call the pipeline with the following ZenML YAML configuration file:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这将调用以下ZenML YAML配置文件中的管道：
- en: '[PRE62]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In *Figure 3.3* earlier, we saw the pipeline’s run DAG and details in ZenML’s
    dashboard. Meanwhile, *Figure 3.5* shows the `user` output artifact generated
    by this data collection pipeline. You can inspect the query `user_full_name` and
    the retrieved `user` from the MongoDB database, for which we collected the links
    in this specific run.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的*图3.3*中，我们看到了管道的运行DAG和ZenML仪表板中的详细信息。同时，*图3.5*展示了由这个数据收集管道生成的`user`输出结果。您可以检查`user_full_name`查询和从MongoDB数据库检索到的`user`，这是我们在这个特定运行中收集的链接。
- en: '![](img/B31105_03_05.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![img/B31105_03_05.png]'
- en: 'Figure 3.5: Example of the user output artifact after running the data collection
    pipeline using Maxime’s configuration file'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：使用Maxime的配置文件运行数据收集管道后的用户输出结果示例
- en: Also, in *Figure 3.6*, you can observe the `crawled_links` output artifact,
    which lists all the domains from which we collected data, the total number of
    links crawled for each domain, and the number of successfully collected links.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在*图3.6*中，您可以观察到`crawled_links`输出结果，其中列出了我们从哪些域名收集了数据，每个域抓取的链接总数，以及成功收集的链接数量。
- en: We want to highlight again the power of these artifacts, as they trace each
    pipeline’s results and metadata, making it extremely easy to monitor and debug
    each pipeline run individually.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次强调这些工件的力量，因为它们跟踪每个管道的结果和元数据，使得单独监控和调试每个管道运行变得极其容易。
- en: '![](img/B31105_03_06.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_03_06.png)'
- en: 'Figure 3.6: Example of the crawled_links output artifact after running the
    data collection pipeline using Maxime’s configuration file'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：使用Maxime的配置文件运行数据收集管道后，crawled_links输出工件的示例
- en: 'Now, we can download the `crawled_links` artifact anywhere in our code by running
    the following code, where the `ID` of the artifact can be found in ZenML and is
    unique for every artifact version:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行以下代码在任何地方下载`crawled_links`工件，其中工件`ID`可以在ZenML中找到，并且对于每个工件版本都是唯一的：
- en: '[PRE63]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'For example, we can easily run the same data collection pipeline but with Paul
    Iusztin’s YAML configuration, listed below:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以轻松运行相同的数据收集管道，但使用保罗·尤斯汀的YAML配置，如下所示：
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'To run the pipeline using Paul’s configuration, we call the following `poe`
    command:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用保罗的配置运行管道，我们调用以下`poe`命令：
- en: '[PRE65]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'That, under the hood, calls the following CLI command that references Paul’s
    config file:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这调用以下CLI命令，该命令引用保罗的配置文件：
- en: '[PRE66]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You can find all the configs in the repository in the `configs/` directory.
    Also, using `poe`, we configured a command that calls the data collection pipeline
    for all the supported authors:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`configs/`目录下的存储库中找到所有配置。此外，使用`poe`，我们配置了一个命令，用于调用所有受支持作者的 数据收集管道：
- en: '[PRE67]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We can easily query the MongoDB data warehouse using our ODM classes. For example,
    let’s query all the articles collected for Paul Iusztin:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地使用我们的ODM类查询MongoDB数据仓库。例如，让我们查询为保罗·尤斯汀收集的所有文章：
- en: '[PRE68]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output of the code from above is:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是：
- en: '[PRE69]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: With only two lines of code, we can query and filter our MongoDB data warehouse
    using any ODM defined within our project.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 只需两行代码，我们就可以使用项目内定义的任何ODM查询和过滤我们的MongoDB数据仓库。
- en: 'Also, to ensure that your data collection pipeline works as expected, you can
    search your MongoDB collections using your **IDE’s MongoDB plugin,** which you
    must install separately. For example, you can use this plugin for VSCode: [https://www.mongodb.com/products/tools/vs-code](https://www.mongodb.com/products/tools/vs-code).
    For other IDEs, you can use similar plugins or external NoSQL visualization tools.
    After connecting to the MongoDB visualization tool, you can connect to our local
    database using the following URI: `mongodb://llm_engineering:llm_engineering@127.0.0.1:27017`.
    For a cloud MongoDB cluster, you must change the URI, which we will explore in
    *Chapter 11*.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为确保您的数据收集管道按预期工作，您可以使用您的**IDE的MongoDB插件**搜索您的MongoDB集合，您必须单独安装此插件。例如，您可以使用此插件为VSCode：[https://www.mongodb.com/products/tools/vs-code](https://www.mongodb.com/products/tools/vs-code)。对于其他IDE，您可以使用类似的插件或外部NoSQL可视化工具。连接到MongoDB可视化工具后，您可以使用以下URI连接到我们的本地数据库：`mongodb://llm_engineering:llm_engineering@127.0.0.1:27017`。对于云MongoDB集群，您必须更改URI，我们将在*第11章*中探讨。
- en: And just like that, you’ve learned how to run the data collection pipeline with
    different ZenML configs and how to visualize the output artifacts of each run.
    We also looked at how to query the data warehouse for a particular data category
    and author. Thus, we’ve finalized our data engineering chapter and can move to
    the conclusion.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，您已经学会了如何使用不同的ZenML配置运行数据收集管道，以及如何可视化每次运行的输出工件。我们还探讨了如何查询特定数据类别和作者的数据库仓库。因此，我们已经完成了数据工程章节，可以进入结论部分。
- en: Troubleshooting
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障排除
- en: The raw data stored in the MongoDB database is central to all future steps.
    Thus, if you haven’t successfully run the code from this chapter due to any issues
    with the crawlers, this section provides solutions for fixing potential issues
    to allow you to move forward.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在MongoDB数据库中的原始数据是所有后续步骤的核心。因此，如果您由于爬虫问题未能成功运行本章的代码，本节提供了修复潜在问题的解决方案，以便您继续前进。
- en: Selenium issues
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Selenium问题
- en: It is a well-known issue that running Selenium can cause problems due to issues
    with the browser driver, such as the `ChromeDriver`. Thus, if the crawlers that
    use Selenium, such as the `MediumCrawler`, fail due to problems with your `ChromeDriver`,
    you can easily bypass this by commenting out the Medium links added to the data
    collection YAML configs. To do so, go to the `configs/` directory and find all
    the YAML files that start with `digital_data_etl_*`, such as `digital_data_etl_maxime_labonne.yaml`.
    Open them and comment on all the Medium-related URLs, as illustrated in *Figure
    3.7*. You can leave out the Substack or personal blog URLs as these use the `CustomArticleCrawler`,
    which is not dependent on Selenium.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Selenium 可能会由于浏览器驱动程序（如 `ChromeDriver`）的问题而导致问题，这是一个众所周知的问题。因此，如果使用 Selenium
    的爬虫（如 `MediumCrawler`）由于您的 `ChromeDriver` 问题而失败，您可以通过注释掉添加到数据收集 YAML 配置中的 Medium
    链接来轻松绕过这个问题。为此，请转到 `configs/` 目录，并找到所有以 `digital_data_etl_*` 开头的 YAML 文件，例如 `digital_data_etl_maxime_labonne.yaml`。打开它们，并注释掉所有与
    Medium 相关的 URL，如图 3.7 所示。您可以省略 Substack 或个人博客 URL，因为这些使用的是 `CustomArticleCrawler`，它不依赖于
    Selenium。
- en: '![](img/B31105_03_07.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7](img/B31105_03_07.png)'
- en: 'Figure 3.7: Fix Selenium issues when crawling raw data'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：在爬取原始数据时修复 Selenium 问题
- en: Import our backed-up data
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入我们的备份数据
- en: 'If nothing works, there is the possibility of populating the MongoDB database
    with your backed-up data saved under the `data/data_warehouse_raw_data directory`.
    This will allow you to proceed to the fine-tuning and inference sections without
    running the data collection ETL code. To import all the data within this directory,
    run:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果什么方法都不奏效，你可以尝试将备份的数据填充到 MongoDB 数据库中，这些数据存储在 `data/data_warehouse_raw_data`
    目录下。这将允许你在不运行数据收集 ETL 代码的情况下，继续进行微调和推理部分。要导入此目录中的所有数据，请运行：
- en: '[PRE70]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: After running the CLI command from above, you will have a one-to-one replica
    of the dataset we used while developing the code. To ensure the import is completed
    successfully, you should have 88 articles and 3 users in your MongoDB database.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述 CLI 命令后，你将拥有我们在开发代码时使用的数据集的一对一副本。为确保导入成功完成，你的 MongoDB 数据库中应有 88 篇文章和 3
    个用户。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve learned how to design and build the data collection pipeline
    for the LLM Twin use case. Instead of relying on static datasets, we collected
    our custom data to mimic real-world situations, preparing us for real-world challenges
    in building AI systems.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何为 LLM Twin 用例设计和构建数据收集管道。我们收集了自定义数据来模拟现实世界的情况，而不是依赖于静态数据集，为我们在构建
    AI 系统中面临的现实世界挑战做好准备。
- en: 'First, we examined the architecture of LLM Twin’s data collection pipeline,
    which functions as an ETL process. Next, we started digging into the pipeline
    implementation. We began by understanding how we can orchestrate the pipeline
    using ZenML. Then, we looked into the crawler implementation. We learned how to
    crawl data in three ways: using CLI commands in subprocesses or using utility
    functions from LangChain or Selenium to build custom logic that programmatically
    manipulates the browser. Finally, we looked into how to build our own ODM class,
    which we used to define our document class hierarchy, which contains entities
    such as articles, posts, and repositories.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查了 LLM Twin 数据收集管道的架构，该架构作为一个 ETL 流程运行。接下来，我们开始深入研究管道实现。我们首先了解如何使用 ZenML
    来编排管道。然后，我们研究了爬虫实现。我们学习了三种爬取数据的方法：使用子进程中的 CLI 命令或使用 LangChain 或 Selenium 的实用函数来构建自定义逻辑，该逻辑可以编程方式操作浏览器。最后，我们研究了如何构建我们自己的
    ODM 类，我们使用它来定义我们的文档类层次结构，其中包含文章、帖子、存储库等实体。
- en: At the end of the chapter, we learned how to run ZenML pipelines with different
    YAML configuration files and explore the results in the dashboard. We also saw
    how to interact with the MongoDB data warehouse through the ODM classes.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们学习了如何使用不同的 YAML 配置文件运行 ZenML 管道，并在仪表板中探索结果。我们还看到了如何通过 ODM 类与 MongoDB
    数据仓库进行交互。
- en: In the next chapter, we will cover the key steps of the RAG feature pipeline,
    including chunking and embedding documents, ingesting these documents into a vector
    DB, and applying pre-retrieval optimizations to improve performance. We will also
    set up the necessary infrastructure programmatically using Pulumi and conclude
    by deploying the RAG ingestion pipeline to AWS.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍 RAG 特征管道的关键步骤，包括分块和嵌入文档，将这些文档摄入到向量数据库中，并应用预检索优化以提高性能。我们还将使用 Pulumi
    以编程方式设置必要的基础设施，并通过部署 RAG 摄入管道到 AWS 来结束本章。
- en: References
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Breuss, M. (2023, July 26). *Beautiful Soup: Build a Web Scraper With Python*.
    [https://realpython.com/beautiful-soup-web-scraper-python/](https://realpython.com/beautiful-soup-web-scraper-python/)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Breuss, M. (2023, July 26). *Beautiful Soup：使用 Python 构建 Web 爬虫*. [https://realpython.com/beautiful-soup-web-scraper-python/](https://realpython.com/beautiful-soup-web-scraper-python/)
- en: David, D. (2024, July 8). *Guide to Web Scraping with Selenium in 2024*. Bright
    Data. [https://brightdata.com/blog/how-tos/using-selenium-for-web-scraping](https://brightdata.com/blog/how-tos/using-selenium-for-web-scraping)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David, D. (2024, July 8). *2024 年使用 Selenium 进行网络爬取指南*. Bright Data. [https://brightdata.com/blog/how-tos/using-selenium-for-web-scraping](https://brightdata.com/blog/how-tos/using-selenium-for-web-scraping)
- en: 'Hjelle, G. A. (2023, October 21). *Python 3.12 Preview: Static Typing Improvements*.
    [https://realpython.com/python312-typing/](https://realpython.com/python312-typing/)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hjelle, G. A. (2023, October 21). *Python 3.12 预览：静态类型改进*. [https://realpython.com/python312-typing/](https://realpython.com/python312-typing/)
- en: '*ORM Quick Start — SQLAlchemy 2.0 documentation*. (n.d.). [https://docs.sqlalchemy.org/en/20/orm/quickstart.html](https://docs.sqlalchemy.org/en/20/orm/quickstart.html
    )'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ORM 快速入门 — SQLAlchemy 2.0 文档*. (n.d.). [https://docs.sqlalchemy.org/en/20/orm/quickstart.html](https://docs.sqlalchemy.org/en/20/orm/quickstart.html)'
- en: 'Ramos, L. P. (2023, August 4). *Python and MongoDB: Connecting to NoSQL Databases*.
    [https://realpython.com/introduction-to-mongodb-and-python/](https://realpython.com/introduction-to-mongodb-and-python/)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramos, L. P. (2023, August 4). *Python 和 MongoDB：连接到 NoSQL 数据库*. [https://realpython.com/introduction-to-mongodb-and-python/](https://realpython.com/introduction-to-mongodb-and-python/)
- en: Refactoring.Guru. (2024, January 1). *Builder*. [https://refactoring.guru/design-patterns/builder](https://refactoring.guru/design-patterns/builder)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refactoring.Guru. (2024, January 1). *Builder*. [https://refactoring.guru/design-patterns/builder](https://refactoring.guru/design-patterns/builder)
- en: '*What is ETL? A complete guide*. (n.d.). Qlik. [https://www.qlik.com/us/etl](https://www.qlik.com/us/etl  )'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是 ETL？完整指南*. (n.d.). Qlik. [https://www.qlik.com/us/etl](https://www.qlik.com/us/etl)'
- en: Join our book’s Discord space
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
