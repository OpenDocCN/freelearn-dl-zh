- en: 9\. What Is Deep Q-Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9. 什么是深度 Q 学习？
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will be learning about deep Q learning in detail along with
    all other possible variations. You will learn how to implement the Q function
    and use the Q learning algorithm along with deep learning to solve complex **Reinforcement
    Learning** (**RL**) problems. By the end of this chapter, you will be able to
    describe and implement the deep Q learning algorithm in PyTorch, and we will also
    do a hands-on implementation of some of the advanced variants of deep Q learning,
    such as double deep Q learning with PyTorch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细学习深度 Q 学习以及所有可能的变种。你将学习如何实现 Q 函数，并结合深度学习使用 Q 学习算法来解决复杂的**强化学习**（**RL**）问题。在本章结束时，你将能够描述并实现
    PyTorch 中的深度 Q 学习算法，我们还将实践实现一些深度 Q 学习的高级变种，例如使用 PyTorch 实现的双重深度 Q 学习。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we learned about the **Multi-Armed Bandit** (**MAB**)
    problem – a popular sequential decision-making problem that aims to maximize your
    reward when playing on the slot machines in a casino. In this chapter, we will
    combine deep learning techniques with a popular **Reinforcement Learning** (**RL**)
    technique called Q learning. Put simply, Q learning is an RL algorithm that decides
    the best action to be taken by an agent for maximum rewards. The "Q" in Q learning
    represents the quality of the action that is used to gain future rewards. In many
    RL environments, we may not have state transition dynamics (that is, the probability
    of going from one state to another), or it is too complex to gather state transition
    dynamics. In these complex RL environments, we can use the Q learning approach
    to implement RL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了**多臂赌博机**（**MAB**）问题——这是一种常见的序列决策问题，目的是在赌场的老虎机上最大化奖励。在本章中，我们将结合深度学习技术与一种流行的**强化学习**（**RL**）技术，叫做
    Q 学习。简而言之，Q 学习是一种强化学习算法，决定代理采取的最佳行动，以获得最大奖励。在 Q 学习中，“Q”表示用于获得未来奖励的行动的质量。在许多 RL
    环境中，我们可能没有状态转移动态（即从一个状态转移到另一个状态的概率），或者收集状态转移动态太复杂。在这些复杂的 RL 环境中，我们可以使用 Q 学习方法来实现
    RL。
- en: In this chapter, we will start by understanding the very basics of deep learning,
    such as what a perceptron and a gradient descent are and what steps need to be
    followed to build a deep learning model. Next, we will learn about PyTorch and
    how to build deep learning models using PyTorch. Once you have been introduced
    to Q learning, we will learn and implement a **Deep Q Network** (**DQN**) with
    the help of PyTorch. Then, we will improve the performance of DQNs with the help
    of experience replay and target networks. Finally, you will implement another
    variant of DQN called a **Double Deep Q Network** (**DDQN**).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从理解深度学习的基础知识开始，了解什么是感知器、梯度下降以及构建深度学习模型需要遵循的步骤。接下来，我们将学习 PyTorch 以及如何使用
    PyTorch 构建深度学习模型。了解了 Q 学习后，我们将学习并实现一个**深度 Q 网络**（**DQN**），并借助 PyTorch 实现。然后，我们将通过经验重放和目标网络来提高
    DQN 的性能。最后，你将实现 DQN 的另一种变体——**双重深度 Q 网络**（**DDQN**）。
- en: Basics of Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础
- en: We have already implemented deep learning algorithms in *Chapter 03*, *Deep
    Learning in Practice using TensorFlow 2*. Before we begin with deep Q learning,
    which is the focus of this chapter, it is essential that we quickly revise the
    basics of deep learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第03章*《TensorFlow 2 实战深度学习》中实现了深度学习算法。在我们开始本章重点的深度 Q 学习之前，有必要快速回顾一下深度学习的基础知识。
- en: 'Let us first understand what a perceptron is before we look into neural networks.
    The following figure represents a general perceptron:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究神经网络之前，首先了解一下什么是感知器。下图表示的是一个通用的感知器：
- en: '![Figure 9.1: Perceptron'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1：感知器'
- en: '](img/B16182_09_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_01.jpg)'
- en: 'Figure 9.1: Perceptron'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：感知器
- en: A perceptron is a binary linear classifier, where the inputs are first multiplied
    by the weights, and then we take a weighted sum of all these multiplied values.
    Then, we pass this weighted sum through an activation function or step function.
    The activation function is used to convert the input values into certain values,
    such as (0,1), as output for binary classification. This whole process can be
    visualized in the preceding figure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一种二元线性分类器，输入首先与权重相乘，然后我们将所有这些乘积的值加权求和。接着，我们将这个加权和通过激活函数或阶跃函数。激活函数用于将输入值转换为特定的输出值，例如（0，1），用于二元分类。这个过程可以在前面的图中可视化。
- en: Deep feedforward networks, which we also refer to as **Multilayer Perceptrons**
    (**MLPs**), have multiple perceptrons at multiple layers, as shown in *Figure
    9.2*. The goal of MLPs is to approximate any function. For example, for a classifier,
    the function ![A drawing of a person
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈网络，通常我们也称之为**多层感知机**（**MLPs**），在多个层次上有多个感知机，如*图9.2*所示。MLP的目标是逼近任何函数。例如，对于一个分类器，这个函数![一个人的图像
- en: Description automatically generated](img/B16182_09_01a.png), maps an input,
    ![A picture containing drawing
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_09_01a.png)，将输入映射，![绘画中的一幅图片
- en: Description automatically generated](img/B16182_09_01b.png), to a category of
    y (for binary classification, either to 0 or 1) by learning the value of parameter
    ![A picture containing furniture, table, stool, drawing
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '描述自动生成](img/B16182_09_01b.png)，通过学习参数的值将其归类为y（对于二分类问题，可能是0或1） '
- en: 'Description automatically generated](img/B16182_09_01c.png)or the weights.
    The following figure shows a general deep neural network:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_09_01c.png)或者权重。下图展示了一个通用的深度神经网络：
- en: '![Figure 9.2: A deep neural network'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.2：深度神经网络'
- en: '](img/B16182_09_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_02.jpg)'
- en: 'Figure 9.2: A deep neural network'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：深度神经网络
- en: A basic building block of MLPs consists of artificial neurons (also called nodes).
    They automatically learn the optimal weight coefficients that are then multiplied
    with the input's features in order to decide whether a neuron fires or not. The
    network consists of multiple layers where the first layer is called the input
    layer and the last layer is called the output layer. The intermediate layers are
    called hidden layers. The number of hidden layers can be of size "one" or more
    depending on how deep you want to make the network.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MLP（多层感知机）的基本构建块由人工神经元（也称为节点）组成。它们自动学习最佳的权重系数，并将其与输入的特征相乘，从而决定神经元是否激活。网络由多个层组成，其中第一层称为输入层，最后一层称为输出层。中间的层被称为隐藏层。隐藏层的数量可以是“一个”或更多，具体取决于你希望网络有多深。
- en: 'The following figure represents the general steps that are needed to train
    a deep learning model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了训练深度学习模型所需的一般步骤：
- en: '![Figure 9.3: Deep learning model training flow'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.3：深度学习模型训练流程'
- en: '](img/B16182_09_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_03.jpg)'
- en: 'Figure 9.3: Deep learning model training flow'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：深度学习模型训练流程
- en: 'The training process of a typical deep learning model can be explained as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型深度学习模型的训练过程可以解释如下：
- en: '**Decide on the network architecture**:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**决定网络架构**：'
- en: To begin, we first need to decide on the network architecture, such as how many
    layers we will have in the network and how many nodes each of these layers will have.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们需要决定网络的架构，比如网络中有多少层，以及每一层将包含多少个节点。
- en: '**Initialize the weights and biases**:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化权重和偏置**：'
- en: In the network, each neuron in a layer will be connected to all of the neurons
    in the previous layer. These connections between the neurons have a corresponding
    weight associated with them. During the training of the whole neural network,
    we first initialize the values of these weights. Each of the neurons will also
    have an associated bias component attached to it. This initialization is a one-time
    process.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在网络中，每一层的每个神经元都将与上一层的所有神经元相连。这些神经元之间的连接都有相应的权重。在整个神经网络的训练过程中，我们首先初始化这些权重的值。每个神经元还会附带一个相应的偏置组件。这一初始化过程是一次性的。
- en: '`sigmoid`, `relu`, or `tanh`) to produce a non-linear output. These values
    get propagated through each of the hidden layers and finally produce the output
    at the output layer.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sigmoid`、`relu` 或 `tanh`）用于产生非线性输出。这些值会通过每一层的隐藏层传播，最终在输出层产生输出。'
- en: '**Calculate the loss**:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算损失**：'
- en: The output of the network is then compared with the true/actual values or labels
    of the training dataset to calculate the loss of the network. The loss of the
    network is a measure of how well the network is performing. The lower the loss,
    the better the performance of the network.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络的输出与训练数据集的真实/实际值或标签进行比较，从而计算出网络的损失。网络的损失是衡量网络性能的指标。损失越低，网络性能越好。
- en: '**Update the weights (backpropagation)**:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新权重（反向传播）**：'
- en: 'Once we have calculated the loss of the network, the aim is to then minimize
    the loss of the network. This is done by using the gradient descent algorithm
    to adjust the weights associated with each node. Gradient descent is an optimization
    algorithm used for minimizing the loss in various machine learning algorithms:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们计算出网络的损失，目标就是最小化网络的损失。这是通过使用梯度下降算法来调整与每个节点相关的权重来实现的。梯度下降是一种用于最小化各种机器学习算法中损失的优化算法：
- en: '![Figure 9.4: Gradient descent'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.4：梯度下降'
- en: '](img/B16182_09_04.jpg)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_04.jpg)'
- en: 'Figure 9.4: Gradient descent'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.4：梯度下降
- en: Loss minimization, in turn, pushes the predicted values closer to the actual
    values during the training process. The learning rate plays a crucial part in
    deciding the rate at which these weights are updated. Other examples of optimizers
    are Adam, RMSProp, and Momentum.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 损失最小化反过来推动预测值在训练过程中更接近实际值。学习率在决定这些权重更新的速率中起着至关重要的作用。其他优化器的例子有 Adam、RMSProp 和
    Momentum。
- en: 'Continue the iteration:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续迭代：
- en: The preceding steps (*steps 3 to 5*) will be continued until the loss is minimized
    to a certain threshold or we have completed a certain number of iterations to
    complete the training process.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的步骤（*步骤 3 到 5*）将继续进行，直到损失被最小化到某个阈值，或者我们完成一定次数的迭代来完成训练过程。
- en: 'The following lists a few of the hyperparameters that should be tuned during
    the training process:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了在训练过程中应该调整的一些超参数：
- en: The number of layers in the network
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中的层数
- en: The number of neurons or nodes in each of the layers
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层中神经元或节点的数量
- en: The choice of activation functions in each of the layers
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层中激活函数的选择
- en: The choice of learning rate
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率的选择
- en: The choice of variants of the gradient algorithm
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度算法变种的选择
- en: The batch size if we are using the mini-batch gradient descent algorithm
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用迷你批梯度下降算法，则批次大小
- en: The number of iterations to be done for weight optimization
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于权重优化的迭代次数
- en: Now that we have a good recollection of the basic concepts of deep learning,
    let's move toward understanding PyTorch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对深度学习的基本概念有了较好的回顾，接下来我们将开始了解 PyTorch。
- en: Basics of PyTorch
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 基础
- en: 'In this chapter, we will use PyTorch to build deep learning solutions. The
    obvious question that comes to mind is, why PyTorch? The following describes a
    number of reasons as to why we should use PyTorch to build deep learning models:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 PyTorch 来构建深度学习解决方案。一个显而易见的问题是，为什么选择 PyTorch？以下是一些我们应使用 PyTorch 来构建深度学习模型的原因：
- en: '**Pythonic deep integration**:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 风格深度集成**：'
- en: The learning curve of PyTorch is smooth due to the Pythonic approach of the
    coding style and the adoption of object-oriented methods. One example of this
    is deep integration with the NumPy Python library, where you can easily convert
    a NumPy array into a torch tensor and vice versa. Also, Python debuggers work
    smoothly with PyTorch, which makes code debugging easier when using PyTorch.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 PyTorch 的 Python 风格编程方式和面向对象方法的应用，PyTorch 的学习曲线非常平滑。一个例子是与 NumPy Python 库的深度集成，您可以轻松地将
    NumPy 数组转换为 torch 张量，反之亦然。此外，Python 调试器与 PyTorch 的配合也非常流畅，使得在使用 PyTorch 时代码调试变得更加容易。
- en: '**Dynamic graph computation**:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态计算图**：'
- en: Many other deep learning frameworks come with a static computation graph; however,
    in PyTorch, dynamic graph computation is supported, which gives the developer
    a far more in-depth understanding of what is going on in each algorithm and allows
    them to change the network behavior programmatically at runtime.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多其他深度学习框架采用静态计算图；然而，在 PyTorch 中，支持动态计算图，这使得开发人员能更深入地理解每个算法中的执行过程，并且可以在运行时编程改变网络的行为。
- en: '**OpenAI adoption of PyTorch**:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI 采用 PyTorch**：'
- en: In general academics for RL, PyTorch gained a huge momentum due to its speed
    and ease of use. As you will have already noted, nowadays, OpenAI Gym is often
    the default environment for solving RL problems. Recently, OpenAI announced that
    it is adopting PyTorch as its primary framework for research and development work.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在强化学习（RL）领域，PyTorch 因其速度和易用性而获得了巨大的关注。如您所注意到的，现在 OpenAI Gym 常常是解决 RL 问题的默认环境。最近，OpenAI
    宣布将 PyTorch 作为其研究和开发工作的主要框架。
- en: 'The following are a few steps that should be followed in order to build a deep
    neural network in PyTorch:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是构建 PyTorch 深度神经网络时应该遵循的一些步骤：
- en: Import the required libraries, prepare the data, and define the source and target
    data. Please note that you will need to convert your data into torch tensors when
    working with any PyTorch models.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库，准备数据，并定义源数据和目标数据。请注意，当使用任何 PyTorch 模型时，您需要将数据转换为 torch 张量。
- en: Build the model architecture using a class.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用类构建模型架构。
- en: Define the loss function and optimizer to be used.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义要使用的损失函数和优化器。
- en: Train the model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Make predictions using the model.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型进行预测。
- en: Let's do an exercise to build a simple PyTorch deep learning model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个练习，构建一个简单的 PyTorch 深度学习模型。
- en: 'Exercise 9.01: Building a Simple Deep Learning Model in PyTorch'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 9.01：在 PyTorch 中构建一个简单的深度学习模型
- en: 'The aim of this exercise is to build a working end-to-end deep learning model
    in PyTorch. This exercise will take you through the steps of how to create a neural
    network model in PyTorch and how to train the same model in PyTorch using sample
    data. This will demonstrate the backbone process in PyTorch, which we will later
    use in the *Deep Q Learning* section:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是在 PyTorch 中构建一个工作的端到端深度学习模型。本练习将带您逐步了解如何在 PyTorch 中创建神经网络模型，并如何使用示例数据在
    PyTorch 中训练同一模型。这将展示 PyTorch 中的基本过程，我们稍后将在*深度 Q 学习*部分中使用：
- en: 'Open a new Jupyter notebook. We will import the required libraries:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter 笔记本。我们将导入所需的库：
- en: '[PRE0]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, using a NumPy array, we will convert the source and target data into
    torch tensors. Please remember that for the PyTorch model to work, you should
    always convert the source and target data into torch tensors, as shown in the
    following code snippet:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用 NumPy 数组，我们将源数据和目标数据转换为 torch 张量。请记住，为了使 PyTorch 模型正常工作，您应始终将源数据和目标数据转换为
    torch 张量，如以下代码片段所示：
- en: '[PRE1]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will be as follows:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE2]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It is very important to be aware of the input and target dataset shapes. This
    is because the deep learning model should be compatible with the shapes of the
    input and target data for the matrix multiplication operations.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非常重要的是要注意输入和目标数据集的形状。这是因为深度学习模型应与输入和目标数据的形状兼容，以进行矩阵乘法运算。
- en: 'Define the network architecture as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网络架构定义如下：
- en: '[PRE3]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once we have the source and target data in tensor format, we should create a
    class for the neural network model architecture. This class inherits the properties
    from the `nn` base class using a package called `Module`. This new class, called
    `Model`, will have a forward function along with a regular constructor, called
    (`__init__`).
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们将源数据和目标数据转换为张量格式，我们应该为神经网络模型架构创建一个类。这个类使用 `Module` 包从 `nn` 基类继承属性。这个新类叫做
    `Model`，将有一个正向函数和一个常规构造函数，叫做 (`__init__`)。
- en: The `__init__` method, at first, will call the `super` method to gain access
    to the base class. Then, all the layer definitions will be written within this
    constructor method. The role of the forward method is to provide the steps that
    are required to do the forward propagation steps of the neural network.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`__init__` 方法首先将调用 `super` 方法以访问基类。然后，在此构造方法内编写所有层的定义。forward 方法的作用是提供神经网络前向传播步骤所需的步骤。'
- en: '`nn.Linear()` has the syntax of (input size, output size) to define the linear
    layers of the model. We can use a non-linear function such as `relu` or `tanh`
    in combination with the linear layers in the forward function.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`nn.Linear()` 的语法是（输入大小，输出大小），用于定义模型的线性层。我们可以在 forward 函数中与线性层结合使用非线性函数，如 `relu`
    或 `tanh`。'
- en: The neural network architecture represents the 3 nodes in the input layer, 10
    in the hidden layer, and 1 in the output layer. Inside the forward function, we
    will use the `relu` activation function in the hidden layer. Once we define the
    model class, then we must instantiate the model.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经网络架构表示输入层中的 3 个节点，隐藏层中的 10 个节点和输出层中的 1 个节点。在 forward 函数中，我们将在隐藏层中使用 `relu`
    激活函数。一旦定义了模型类，我们必须实例化模型。
- en: Now you should have successfully created and initiated a model.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，您应该已经成功创建并启动了一个模型。
- en: Now define the loss function and optimizer. The exercise we are working on is
    a regression problem; we generally use the mean squared error as the loss function
    in regression problems. In PyTorch, we use the `MSELoss()` function for a regression
    problem. Generally, the loss is assigned to `criterion`.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在定义损失函数和优化器。我们正在处理的练习是一个回归问题；在回归问题中，我们通常使用均方误差作为损失函数。在 PyTorch 中，我们使用 `MSELoss()`
    函数用于回归问题。通常，将损失赋给 `criterion`。
- en: 'The `Model` parameter and learning rate must be passed as mandatory arguments
    to the optimizers for backpropagation. Model parameters can be accessed using
    the `model.parameters()` function. Now define the loss function and optimizer
    using the Adam optimizer. While creating the Adam optimizer, pass `0.01` as the
    learning rate along with the model parameter:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Model`参数和学习率必须作为必需的参数传递给优化器以进行反向传播。可以使用`model.parameters()`函数访问模型参数。现在使用Adam优化器定义损失函数和优化器。在创建Adam优化器时，将`0.01`作为学习率和模型参数一起传入：'
- en: '[PRE4]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this point, you should have successfully defined the loss and optimization functions.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此时，你应该已经成功定义了损失和优化函数。
- en: Note
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '`torch.optim` package.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`torch.optim`包。'
- en: 'Train the model for 20 epochs and monitor the loss. To form a training loop,
    create a variable called `n_epochs` and initialize the value as 20\. Create a
    `for` loop to run the loop for `n_epoch` times. Inside the loop, complete these
    steps: zero out the parameter gradients using `optimizer.zero_grad()`. Pass the
    input through the model to get the output. Obtain the loss using `criterion` by
    passing the outputs and targets. Use `loss.backward()` and `optimizer.step()`
    to do the backpropagation step. Print the loss after every epoch:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型训练20个周期并监控损失值。为了形成训练循环，创建一个名为`n_epochs`的变量，并将其初始化为20。创建一个`for`循环，循环`n_epoch`次。在循环内部，完成以下步骤：使用`optimizer.zero_grad()`将参数梯度清零。将输入传入模型，获取输出。使用`criterion`通过传入输出和目标来获得损失。使用`loss.backward()`和`optimizer.step()`执行反向传播步骤。每个周期后打印损失值：
- en: '[PRE5]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: PyTorch, by default, accumulates the gradients calculated at each step. We need
    to handle this during the training process so that the weights are updated with
    their proper gradients. `optimizer.zero_grad()` will zero out the gradients from
    the previous training step to stop the gradient accumulations. This step should
    be done before calculating the gradients at each epoch. To calculate the loss,
    we should pass the predicted and actual values to the loss function. The `criterion(outputs,
    targets)` step is used to calculate the loss. The `loss.backward()` step is used
    to calculate the weight gradients, and we will use these gradients to update the
    weights to get our optimum weights. Weight updates are done using the `optimizer.step()`
    function.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch会在每一步计算时累积梯度。我们需要在训练过程中处理这一点，以确保权重根据正确的梯度更新。`optimizer.zero_grad()`会将前一步训练的梯度清零，以停止梯度的累积。此步骤应在每个周期计算梯度之前完成。为了计算损失，我们应将预测值和实际值传入损失函数。`criterion(outputs,
    targets)`用于计算损失。`loss.backward()`用于计算权重梯度，我们将使用这些梯度来更新权重，从而获得最佳权重。权重更新是通过`optimizer.step()`函数完成的。
- en: 'The output will be as follows:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE6]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the output prints the loss after every epoch. You should closely
    monitor the training loss. From the preceding output, we can see that training
    loss decreases.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，输出在每个周期后打印损失值。你应该密切监控训练损失。从前面的输出中我们可以看到，训练损失在逐步减少。
- en: 'Once the model is trained, we can use the trained model to make predictions.
    Pass the input data through the model to get the predictions and observe the output:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以使用训练好的模型进行预测。将输入数据传入模型，获取预测结果并观察输出：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE8]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding output shows the model prediction of the corresponding input data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出展示了模型对相应输入数据的预测结果。
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3e2DscY](https://packt.live/3e2DscY).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/3e2DscY](https://packt.live/3e2DscY)。
- en: You can also run this example online at [https://packt.live/37q0J68](https://packt.live/37q0J68).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个例子，访问[https://packt.live/37q0J68](https://packt.live/37q0J68)。
- en: We now know how a PyTorch model works. This example will be useful for you when
    training your deep Q neural network. However, in addition to this, there are few
    other important PyTorch utilities that you should be aware of, which you will
    study in the next section. Understanding these utilities is essential for the
    implementation of deep Q learning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了一个PyTorch模型是如何工作的。这个例子对于你训练深度Q神经网络时会非常有用。然而，除了这个，还有一些其他重要的PyTorch工具是你应该了解的，接下来你将学习这些工具。理解这些工具对于实现深度Q学习至关重要。
- en: PyTorch Utilities
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch工具
- en: 'To work with the utilities, first, we will create a torch tensor of size 10
    with numbers starting from 1 to 9 using the `arange` function of PyTorch. A torch
    tensor is essentially a matrix of elements that belongs to a single data type,
    which can have multiple dimensions. Please note that, like Python, PyTorch also
    excludes the number given in the `arange` function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这些工具，首先我们将创建一个大小为 10 的 torch 张量，包含从 1 到 9 的数字，使用 PyTorch 的 `arange` 函数。torch
    张量本质上是一个元素矩阵，所有元素属于同一数据类型，可以具有多个维度。请注意，像 Python 一样，PyTorch 也会排除在 `arange` 函数中给定的数字：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let's now begin exploring the various functions, one by one.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始逐个探索不同的函数。
- en: The view Function
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: view 函数
- en: 'Use the `view` function to reshape your tensor as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `view` 函数重新调整张量的形状如下所示：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s try a new shape now:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一个新的形状：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The squeeze Function
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: squeeze 函数
- en: 'The `squeeze` function is used to remove any dimensions with a value of 1\.
    The following is an example of a tensor with a shape of (5,1):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`squeeze` 函数用于移除任何值为 1 的维度。以下是一个形状为 (5,1) 的张量示例：'
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Apply the `squeeze` function to the tensor:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对张量应用 `squeeze` 函数：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, after using `squeeze`, the dimension of 1 is removed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用 `squeeze` 后，维度为 1 的维度已被移除。
- en: The unsqueeze Function
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: unsqueeze 函数
- en: As the name suggests, the `unsqueeze` function does the reverse of `squeeze`.
    It adds a dimension of 1 to the input data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，`unsqueeze` 函数执行与 `squeeze` 相反的操作。它向输入数据添加一个维度为 1 的维度。
- en: 'Consider the following example. First, we create a tensor of shape `5`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例。首先，我们创建一个形状为 `5` 的张量：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output will be as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Apply the `unsqueeze` function to the tensor:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对张量应用 `unsqueeze` 函数：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output will be as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, a dimension of 1 has been added to the tensor.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，已向张量添加了一个维度为 1 的维度。
- en: The max Function
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: max 函数
- en: If a multidimensional tensor is passed to the `max` function, the function returns
    the max values and corresponding index in the specified axis. Please refer to
    the code comments for more details.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将多维张量传递给 `max` 函数，函数将返回指定轴上的最大值及其相应的索引。更多细节请参考代码注释。
- en: 'First, we create a tensor of dimensions `(4, 4)`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个形状为 `(4, 4)` 的张量：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let''s apply the `max` function to the tensor:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对张量应用 `max` 函数：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s now try to find the max values from the tensor:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试从张量中找出最大值：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output will be as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To find the index of the max values, use the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到最大值的索引，可以使用以下代码：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output will be as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, the indices of the max values have been displayed.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，最大值的索引已显示。
- en: The gather Function
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gather 函数
- en: 'The `gather` function works by collecting values along an axis specified by
    `dim`. The general syntax of the `gather` function is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`gather` 函数通过沿指定的轴（由 `dim` 指定）收集值来工作。`gather` 函数的一般语法如下：'
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The syntax can be explained as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 语法可以解释如下：
- en: '`input` (tensor): Specify the source tensor here.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input` (tensor): 在这里指定源张量。'
- en: '`dim` (python:int): Specify the axis along which to index.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim` (python:int): 指定索引的轴。'
- en: '`index` (`LongTensor`): Specify the indices of elements to gather.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index` (`LongTensor`): 指定要收集的元素的索引。'
- en: 'In the following example, we have `q_values`, which is a torch tensor of shape
    (`4,4`), and the action is a `LongTensor` that has indexes that we want to extract
    from the `q_values` tensor:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们有一个形状为(`4,4`)的 `q_values`，它是一个 torch 张量，而 `action` 是一个 `LongTensor`，它包含我们想从
    `q_values` 张量中提取的索引：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will apply `LongTensor` to specify the indices of the elements of
    the tensor that are to be gathered:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用 `LongTensor` 来指定要收集的张量元素的索引：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, find the shape of the `q_values` tensor:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，找到 `q_values` 张量的形状：
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output will be as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s now apply the `gather` function:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用 `gather` 函数：
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output will be as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now you have a basic understanding of neural networks and how to implement a
    simple neural network in PyTorch. Aside from a vanilla neural network (the combination
    of linear layers with a non-linear activation function), there are two other variants
    called **Convolutional Neural Networks** (**CNNs**) and **Recurrent Neural Networks**
    (**RNNs**). A CNN is mainly used for image classification and image segmentation
    tasks, and an RNN is used for data with a sequential pattern, such as time series
    data, or for language translation tasks.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对神经网络有了基本了解，并且知道如何在 PyTorch 中实现一个简单的神经网络。除了标准的神经网络（即线性层与非线性激活函数的组合）之外，还有两个变体，分别是**卷积神经网络**（**CNNs**）和**递归神经网络**（**RNNs**）。CNN
    主要用于图像分类和图像分割任务，而 RNN 则用于具有顺序模式的数据，例如时间序列数据或语言翻译任务。
- en: Now, as we have gained some knowledge of deep learning and how to build deep
    learning models in PyTorch, we will shift our focus to Q learning and how to use
    deep learning in RL with the help of PyTorch. First, we will start with the state-value
    function and the Bellman equation, and then we will move on to Q learning.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们已经掌握了深度学习的基本知识以及如何在 PyTorch 中构建深度学习模型后，我们将重点转向 Q 学习，以及如何在强化学习（RL）中利用深度学习，借助
    PyTorch 实现。首先，我们将从状态值函数和贝尔曼方程开始，然后再深入探讨 Q 学习。
- en: The State-Value Function and the Bellman Equation
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态值函数和贝尔曼方程
- en: As we are slowly moving toward the Q function and Q learning process, let's
    revisit the Bellman equation, which is the backbone of the Q learning process.
    In the following section, we will first revise our definition of an "expected
    value" and how it is used in a Bellman equation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们逐渐进入 Q 函数和 Q 学习过程的核心，让我们回顾一下贝尔曼方程，它是 Q 学习过程的支柱。在接下来的部分，我们将首先复习“期望值”的定义，并讨论它如何在贝尔曼方程中应用。
- en: Expected Value
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 期望值
- en: 'The following figure depicts the expected value in a state space:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了状态空间中的期望值：
- en: '![Figure 9.5: Expected value'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5：期望值'
- en: '](img/B16182_09_05.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_05.jpg)'
- en: 'Figure 9.5: Expected value'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：期望值
- en: Suppose that an agent is in state `S`, and it has two paths on which it can
    travel. The first path has a transition probability of 0.6 and an associated reward
    of 1, and the second path has a transition probability of 0.4 and an associated
    reward of 0\.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个智能体处于状态 `S`，并且它有两条可以选择的路径。第一条路径的转移概率为 0.6，关联的奖励为 1；第二条路径的转移概率为 0.4，关联的奖励为
    0。
- en: 'Now, the expected value or reward of state `S` would be as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，状态 `S` 的期望值或奖励如下所示：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Mathematically, it can be expressed as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它可以表示为：
- en: '![Figure 9.6: Expression for the expected value'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6：期望值的表达式'
- en: '](img/B16182_09_06.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_06.jpg)'
- en: 'Figure 9.6: Expression for the expected value'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：期望值的表达式
- en: The Value Function
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 值函数
- en: When an agent is in an environment, the value function provides the required
    information about the states. The value function provides a methodology through
    which an agent can know how good any given state is for the agent. So, if an agent
    has the option of going two states from the current state, the agent will always
    choose the state with the larger value function.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个智能体处于某个环境中时，值函数提供了关于各个状态所需的信息。值函数为智能体提供了一种方法，通过这种方法，智能体可以知道某一给定状态对其有多好。所以，如果一个智能体可以从当前状态选择两个状态，它将总是选择值函数较大的那个状态。
- en: The value function can be expressed recursively using the value function of
    future states. When we are working in a stochastic environment, we will use the
    concept of expected values, as discussed in the previous section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数可以递归地通过未来状态的值函数来表示。当我们在一个随机环境中工作时，我们将使用期望值的概念，正如前一节所讨论的那样。
- en: The Value Function for a Deterministic Environment
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定性环境的值函数
- en: For a deterministic world, the value of a state is just the sum of all future
    rewards.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个确定性世界，状态的值就是所有未来奖励的总和。
- en: 'The value function of state 1 can be expressed as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 1 的值函数可以表示如下：
- en: '![Figure 9.7: Value function of state 1'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7：状态 1 的值函数'
- en: '](img/B16182_09_07.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_07.jpg)'
- en: 'Figure 9.7: Value function of state 1'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：状态 1 的值函数
- en: 'The value function of state 1 can be expressed in terms of state 2, as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 1 的值函数可以通过状态 2 来表示，如下所示：
- en: '![Figure 9.8: Value function of state 2'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8：状态 2 的值函数'
- en: '](img/B16182_09_08.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_08.jpg)'
- en: 'Figure 9.8: Value function of state 2'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：状态 2 的值函数
- en: 'The simplified value function of state 1, using the value function of state
    2, can be expressed as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用状态 2 值函数的状态 1 简化值函数可以表示如下：
- en: '![Figure 9.9: Simplified value function of state 1 using the value function
    of state 2'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9：使用状态 2 值函数的状态 1 简化值函数'
- en: '](img/B16182_09_09.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_09.jpg)'
- en: 'Figure 9.9: Simplified value function of state 1 using the value function of
    state 2'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：使用状态 2 值函数的状态 1 简化值函数
- en: 'The simplified value function of state 1 with the discount factor can be expressed
    as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 带折扣因子的状态 1 简化值函数可以表示如下：
- en: '![Figure 9.10: Simplified value function of state 1 with the discount factor'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.10：带折扣因子的状态 1 简化值函数'
- en: '](img/B16182_09_10.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_10.jpg)'
- en: 'Figure 9.10: Simplified value function of state 1 with the discount factor'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：带折扣因子的状态 1 简化值函数
- en: 'In general, we can rewrite the value function as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以将值函数重新写为如下形式：
- en: '![Figure 9.11: Value function for a deterministic environment'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.11：确定性环境下的值函数'
- en: '](img/B16182_09_11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_11.jpg)'
- en: 'Figure 9.11: Value function for a deterministic environment'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：确定性环境下的值函数
- en: 'The Value Function for a Stochastic Environment:'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机环境下的值函数：
- en: 'For stochastic behavior, due to the randomness or uncertainty that is present
    in the environment, instead of taking the raw future rewards, we take the expected
    total reward from a state to come up with the value function. The new addition
    to the preceding equation is the expectation part. The equation is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机行为，由于环境中存在的随机性或不确定性，我们不直接使用原始的未来奖励，而是取从某个状态到达的期望总奖励来得出值函数。前述方程中新增加的是期望部分。方程如下：
- en: '![Figure 9.12: Value function for a stochastic environment'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.12：随机环境下的值函数'
- en: '](img/B16182_09_12.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_12.jpg)'
- en: 'Figure 9.12: Value function for a stochastic environment'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：随机环境下的值函数
- en: Here, `s` is the current state, ![A picture containing table, drawing
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`s` 是当前状态，![一张包含表格的图片，绘图
- en: Description automatically generated](img/B16182_09_12a.png)is the next state,
    and `r` is the reward of going from `s` to ![A picture containing table, drawing
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_09_12a.png)是下一个状态，`r` 是从 `s` 到的奖励![一张包含表格的图片，绘图
- en: Description automatically generated](img/B16182_09_12b.png).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_09_12b.png)。
- en: The Action-Value Function (Q Value Function)
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作值函数（Q值函数）
- en: 'In the previous sections, we learned about the state-value function, which
    tells us how rewarding it is to be in a particular state for an agent. Now we
    will learn about another function where we can combine the state with actions.
    The action-value function will tell us how good it is for the agent to take any
    given action from a given state. We also call the action value the **Q value**.
    The equation can be written as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了状态值函数，它告诉我们某个状态对智能体有多大的奖励。现在我们将学习另一个函数，在这个函数中，我们可以将状态与动作结合起来。动作值函数将告诉我们，对于智能体从某个给定状态采取任何特定动作的好坏。我们也称动作值为**Q值**。方程可以写成如下形式：
- en: '![Figure 9.13: Expression for the Q value function'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.13：Q 值函数表达式'
- en: '](img/B16182_09_13.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_13.jpg)'
- en: 'Figure 9.13: Expression for the Q value function'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：Q 值函数表达式
- en: 'The preceding equation can be written in an iterative fashion, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程可以按迭代方式写成如下：
- en: '![Figure 9.14: Expression for the Q value function with iterations'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14：带迭代的 Q 值函数表达式'
- en: '](img/B16182_09_14.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_14.jpg)'
- en: 'Figure 9.14: Expression for the Q value function with iterations'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：带迭代的 Q 值函数表达式
- en: This equation is also known as the **bellman equation**. From the equation,
    we can express ![A drawing of a face
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程也被称为**贝尔曼方程**。从这个方程，我们可以递归地表示![一张面部的画
- en: 'Description automatically generated](img/B16182_09_14a.png)recursively in terms
    of the Q value of the next state, ![c](img/B16182_09_14b.png). A Bellman equation
    can be described as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_09_14a.png)的Q值，表示为下一个状态的Q值![c](img/B16182_09_14b.png)。贝尔曼方程可以描述如下：
- en: '*"The total expected reward being in state s and taking action a is the sum
    of two components: the reward (which is r) that we can get from state ''s'' by
    taking action a, plus the maximum expected discounted return **![c](img/B16182_09_14c.png)
    that we can get from any possible next state-action pair (s′, a′). a′ is the next
    best possible action."*'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*“处于状态s并采取动作a的总期望奖励是两个部分的和：我们从状态‘s’采取动作a能够获得的奖励（即r），加上我们从任何可能的下一状态-动作对（s′，a′）中能够获得的最大期望折扣回报**![c](img/B16182_09_14c.png)。a′是下一最佳可能动作。”*'
- en: Implementing Q Learning to Find Optimal Actions
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现Q学习以找到最佳动作
- en: 'The process of finding the optimal action from any state using the Q function
    is called Q learning. Q learning is also a tabular method, where state and action
    combinations are stored in a tabular format. In the following section, we will
    learn how to find the optimal action using the Q learning method in a stepwise
    fashion. Consider the following table:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Q函数从任何状态中找到最佳动作的过程称为Q学习。Q学习也是一种表格方法，其中状态和动作的组合以表格格式存储。在接下来的部分中，我们将学习如何通过Q学习方法逐步找到最佳动作。考虑以下表格：
- en: '![Figure 9.15: Sample table for Q learning'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.15：Q学习的示例表格'
- en: '](img/B16182_09_15.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_15.jpg)'
- en: 'Figure 9.15: Sample table for Q learning'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：Q学习的示例表格
- en: As you can see in the preceding table, the Q values are stored in terms of a
    table, where the rows represent the states that are present in the environment,
    and the columns represent all of the possible actions for the agent. You can see
    that all of the states are represented as rows, and all the actions, such as going
    up, down, right, and left, are stored as columns.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的表格所示，Q值以表格的形式存储，其中行代表环境中的状态，列代表代理的所有可能动作。你可以看到，所有的状态都表示为行，而所有动作，如上、下、右和左，都存储为列。
- en: The values present in the intersection of any row and column is the Q value
    for that particular state-action pair.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一行和一列交叉点上的值即为该特定状态-动作对的Q值。
- en: Initially, all values of state-action pairs are initialized with zero. The agent,
    being in a state, will choose the action with the highest Q value. For example,
    as shown in the preceding figure, being in state 001, the agent will choose to
    go right, which has the highest Q value (0.98).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，所有状态-动作对的值都初始化为零。代理在某一状态下将选择具有最高Q值的动作。例如，如上图所示，当处于状态001时，代理将选择向右移动，因为它的Q值最高（0.98）。
- en: 'During the initial phase, when most of the state-action pairs will have zero
    values, we will make use of the previously discussed epsilon-greedy strategy to
    tackle the exploration-exploitation dilemma as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在初期阶段，当大多数状态-动作对的值为零时，我们将利用之前讨论的ε-贪心策略来解决探索-利用的困境，具体如下：
- en: Set the value of ε (a high value such as 0.90).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置ε的值（例如0.90这样的较高值）。
- en: 'Choose a random number between 0 and 1:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在0到1之间选择一个随机数：
- en: '[PRE40]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The state with a higher value of ε decays gradually. The idea then would be
    to initially explore and then exploit.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 状态中较高的ε值逐渐衰减。其思想是最初进行探索，然后进行利用。
- en: 'Using the **Temporal Difference** (**TD**) method described in the previous
    chapter, we iteratively update the Q values as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一章中描述的**时序差分**（**TD**）方法，我们以迭代方式更新Q值，如下所示：
- en: '![Figure 9.16: Updating the Q values using iterations'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.16：通过迭代更新Q值'
- en: '](img/B16182_09_16.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_16.jpg)'
- en: 'Figure 9.16: Updating the Q values using iterations'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16：通过迭代更新Q值
- en: The timestamp, `t`, is the current iteration, and timestamp `(t-1)` is the previous
    iteration. In this way, we are updating the previous timestamp's Q value with
    the `TD` method and pushing the Q value as close as we can to the optimal Q value,
    which is also called ![A picture containing drawing
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳`t`为当前迭代，时间戳`(t-1)`为上一轮迭代。通过这种方式，我们用`TD`方法更新上一轮时间戳的Q值，并尽可能将Q值推近到最优Q值，这也叫做![一张包含绘图的图片
- en: Description automatically generated](img/B16182_09_16a.png).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_09_16a.png)。
- en: 'We can rewrite the preceding equation as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上面的方程重新写为：
- en: '![Figure 9.17: Updated expression for updating Q values'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.17：更新Q值的表达式'
- en: '](img/B16182_09_17.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_17.jpg)'
- en: 'Figure 9.17: Updated expression for updating Q values'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17：更新Q值的表达式
- en: 'Using simple math, we can further simplify the equation as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单的数学运算，我们可以进一步简化方程，如下所示：
- en: '![Figure 9.18: Updated equation for Q values'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.18：更新后的Q值方程'
- en: '](img/B16182_09_18.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_18.jpg)'
- en: 'Figure 9.18: Updated equation for Q values'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18：Q值更新方程
- en: The learning rate decides how big or small the steps we should take should be
    in order to update the Q values. This iteration and updating of Q values will
    continue until the Q values come closer to ![A picture containing table
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率决定了我们在更新Q值时应该采取多大的步伐。这个迭代和更新Q值的过程会持续进行，直到Q值趋近于![A picture containing table
- en: 'Description automatically generated](img/B16182_09_18a.png) or if we reach
    a certain predefined number of iterations. The iteration can be visualized as
    follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_09_18a.png) 或者当我们达到某个预定义的迭代次数时。迭代过程可以如下可视化：
- en: '![Figure 9.19: The Q learning process'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.19：Q学习过程'
- en: '](img/B16182_09_19.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_19.jpg)'
- en: 'Figure 9.19: The Q learning process'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19：Q学习过程
- en: As you can see, after multiple iterations, the Q table is finally ready.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，经过多次迭代后，Q表最终准备好了。
- en: Advantages of Q Learning
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q学习的优点
- en: 'The following describes the advantages of using Q learning in the RL domain:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Q学习在强化学习领域中的一些优点：
- en: We don't need to know the full transition dynamic; this means that we don't
    have to know all of the state transition probabilities that may not be available
    for some environments.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要知道完整的转移动态；这意味着我们不必了解所有可能不存在的状态转移概率。
- en: As we store the state-action combination in a tabular format, it is easy to
    understand and implement the Q learning algorithm by fetching the details from
    the tables.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们以表格格式存储状态-动作组合，通过从表格中提取细节，理解和实现Q学习算法变得更加简单。
- en: We don't have to wait for the entire episode to finish to update the Q value
    for any state due to the continuous online update of the learning process, unlike
    in the Monte Carlo method where we have to wait for an episode to finish in order
    to update the action-value function.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不必等到整个回合结束才更新任何状态的Q值，因为学习过程是连续在线更新的，这与蒙特卡罗方法不同，在蒙特卡罗方法中我们必须等到回合结束才能更新动作值函数。
- en: It works well when the combination of states and action spaces is low.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当状态和动作空间的组合较少时，这种方法效果较好。
- en: As we have now learned about the basics of Q learning, we can implement Q learning
    using an OpenAI Gym environment. So, before going ahead with the exercise, let's
    review the concept of OpenAI Gym.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经了解了Q学习的基础知识，我们可以使用OpenAI Gym环境实现Q学习。因此，在进行练习之前，让我们回顾一下OpenAI Gym的概念。
- en: OpenAI Gym Review
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Gym回顾
- en: 'Before we implement the Q learning tabular method, let''s quickly review and
    revisit the Gym environment. OpenAI Gym is a toolkit for developing RL algorithms.
    It supports teaching agents everything from walking to playing games such as CartPole
    or FrozenLake-v0\. Gym provides an environment, and it is up to the developer
    to write and implement any RL algorithms such as tabular methods or deep Q learning.
    We can also write algorithms using existing deep learning frameworks, such as
    PyTorch or TensorFlow. The following is a sample code example to work with an
    existing Gym environment:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现Q学习表格方法之前，先快速回顾并重新审视Gym环境。OpenAI Gym是一个用于开发强化学习（RL）算法的工具包。它支持教导代理从行走到玩像CartPole或FrozenLake-v0等游戏。Gym提供了一个环境，开发者可以根据需要编写和实现任何强化学习算法，如表格方法或深度Q学习。我们也可以使用现有的深度学习框架，如PyTorch或TensorFlow来编写算法。以下是一个与现有Gym环境一起使用的示例代码：
- en: '![Figure 9.20: Gym environment'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.20：Gym环境'
- en: '](img/B16182_09_20.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_20.jpg)'
- en: 'Figure 9.20: Gym environment'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20：Gym环境
- en: 'Let''s understand a few parts of the code as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解代码的几个部分如下：
- en: '`gym.make("CartPole-v1")`'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gym.make("CartPole-v1")`'
- en: This creates an existing Gym environment (`CartPole-v1`).
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会创建一个现有的Gym环境（`CartPole-v1`）。
- en: '`env.reset()`'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env.reset()`'
- en: This resets the environment, so the environment will be at the starting state.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会重置环境，因此环境将回到初始状态。
- en: '`env.action_space.sample()`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env.action_space.sample()`'
- en: This selects a random action from the action space (a collection of available actions).
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会从动作空间（可用动作的集合）中选择一个随机动作。
- en: '`env.step(action)`'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env.step(action)`'
- en: This performs the action selected from the previous step. Once you take the
    actions, the environment will return the `new_state`, `reward`, and `done` flags
    (to indicate whether the game is over), and some extra information.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会执行上一阶段选择的动作。一旦你采取了动作，环境将返回`new_state`、`reward`和`done`标志（用于指示游戏是否结束），以及一些额外信息。
- en: '`env.render()`'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env.render()`'
- en: This renders to see the agent performing the actions or playing the game.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会呈现出代理执行动作或进行游戏的过程。
- en: We now have a theoretical understanding of the Q learning process, and we have
    also reviewed the Gym environment. Now it's your turn to implement Q learning
    using a Gym environment.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经对 Q 学习过程有了理论理解，并且我们也回顾了 Gym 环境。现在轮到你自己实现使用 Gym 环境的 Q 学习了。
- en: 'Exercise 9.02: Implementing the Q Learning Tabular Method'
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 9.02：实现 Q 学习表格方法
- en: 'In this exercise, we will implement the tabular Q Learning method using the
    OpenAI Gym environment. We will use the `FrozenLake-v0` Gym environment to implement
    the tabular Q learning method. The goal is to play and collect maximum rewards
    with the help of the Q learning process. You should already be familiar with the
    FrozenLake-v0 environment from *Chapter 5*, *Dynamic Programming*. The following
    steps will help you to complete the exercise:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 OpenAI Gym 环境实现表格 Q 学习方法。我们将使用`FrozenLake-v0` Gym 环境来实现表格 Q 学习方法。目标是通过
    Q 学习过程进行游戏并收集最大奖励。你应该已经熟悉来自*第 5 章*、*动态规划*中的 FrozenLake-v0 环境。以下步骤将帮助你完成练习：
- en: Open a new Jupyter notebook file.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook 文件。
- en: 'Import the required libraries:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE41]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create the Gym environment with `''FrozenLake-v0''` for a stochastic environment:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `'FrozenLake-v0'` Gym 环境，以实现一个随机环境：
- en: '[PRE42]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Fetch the number of states and actions:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取状态和动作的数量：
- en: '[PRE43]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output will be as follows:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE44]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create the Q table with details fetched from the previous step:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从上一步获取的详细信息创建 Q 表：
- en: '[PRE45]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE46]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now we know the shape of the Q table and that the initial values are all zero
    for every state-action pair.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们知道 Q 表的形状，并且每个状态-动作对的初始值都是零。
- en: 'Set all of the required hyperparameter values to be used for Q learning:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置所有用于 Q 学习的必需超参数值：
- en: '[PRE47]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create empty lists to store the values of the rewards and the decayed egreedy
    values for visualization:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建空列表以存储奖励值和衰减的 egreedy 值用于可视化：
- en: '[PRE48]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Implement the Q learning training process to play the episode for a fixed number
    of episodes. Use the previously learned Q learning process (from the *Implementing
    Q Learning to Find Optimal Actions* section) in order to find the optimal actions
    from any given state.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 Q 学习训练过程，通过固定次数的回合来进行游戏。使用之前学到的 Q 学习过程（来自*实施 Q 学习以寻找最佳行动*部分），以便从给定状态中找到最佳行动。
- en: 'Create a `for` loop to iterate for `NUMBER_OF_EPISODES`. Reset the environment
    and set the `done` flag equal to `False` and `current_episode_rewards` as `zero`.
    Create another `for` loop to run a single episode for `MAX_STEPS`. Inside the
    `for` loop, choose the best action using the epsilon-greedy strategy. Perform
    the action and update the Q values using the equation shown in *Figure 9.18*.
    Collect the reward and assign `new_state` as the current state. If the episode
    is over, break out from the loop, else continue taking the steps. Decay the epsilon
    value to be able to continue for the next episode:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建一个 `for` 循环，迭代 `NUMBER_OF_EPISODES`。重置环境并将 `done` 标志设置为 `False`，`current_episode_rewards`
    设置为 `zero`。创建另一个 `for` 循环，在 `MAX_STEPS` 内运行一个回合。在 `for` 循环内，使用 epsilon-greedy
    策略选择最佳动作。执行该动作并使用*图 9.18*中展示的公式更新 Q 值。收集奖励并将 `new_state` 赋值为当前状态。如果回合结束，则跳出循环，否则继续执行步骤。衰减
    epsilon 值，以便继续进行下一回合：
- en: '[PRE49]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Implement a function called `rewards_split` that will split the 10,000 rewards
    into 1,000 individual lists of rewards, and calculate the average rewards for
    each of these 1,000 lists of rewards:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个名为 `rewards_split` 的函数，该函数将 10,000 个奖励拆分为 1,000 个单独的奖励列表，并计算这些 1,000 个奖励列表的平均奖励：
- en: '[PRE50]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Visualize the average rewards or percentage of completed episodes:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化平均奖励或已完成回合的百分比：
- en: '[PRE51]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 9.21: Visualizing the percentage of episodes completed'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.21：可视化已完成回合的百分比'
- en: '](img/B16182_09_21.jpg)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_21.jpg)'
- en: 'Figure 9.21: Visualizing the percentage of episodes completed'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.21：可视化已完成回合的百分比
- en: As you can see from the preceding figure, the episodes are completed, and the
    percentage rises exponentially until it reaches a point where it becomes constant.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，回合已完成，并且百分比呈指数增长，直到达到一个点后变得恒定。
- en: 'Now we will visualize the `Egreedy` value decay:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将可视化 `Egreedy` 值的衰减：
- en: '[PRE52]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The plot will be produced as follows:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图形将如下所示：
- en: '![Figure 9.22: Egreedy value decay'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.22：Egreedy 值衰减'
- en: '](img/B16182_09_22.jpg)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_22.jpg)'
- en: 'Figure 9.22: Egreedy value decay'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22：Egreedy 值衰减
- en: In *Figure 9.22*, we can see the `Egreedy` value has been gradually decayed
    with the increasing number of steps. This means that, as the value drops toward
    zero, the algorithm becomes more and more greedy, taking the action with maximum
    reward without exploring the less rewarding actions, which, with enough exploration,
    may turn out to be more rewarding in the long term, but we do not know enough
    about the model in the initial stages.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.22*中，我们可以看到`Egreedy`值随着步骤数的增加逐渐衰减。这意味着，随着值接近零，算法变得越来越贪婪，选择具有最大奖励的动作，而不探索那些奖励较少的动作，这些动作通过足够的探索，可能会在长期内带来更多的奖励，但在初期我们对模型了解不够。
- en: This highlights the need for a higher exploration of the environment when we
    are in the early stages of learning. This is achieved with higher epsilon values.
    The epsilon value is reduced as training progresses. This results in less exploration
    and more exploitation of knowledge gained from past runs.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了在学习初期阶段需要更高探索的需求。通过更高的epsilon值可以实现这一点。随着训练的进行，epsilon值逐渐降低。这会导致较少的探索和更多利用过去运行中获得的知识。
- en: Thus, we have successfully implemented the tabular method of Q learning.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功实现了表格Q学习方法。
- en: Note
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2B3NziM](https://packt.live/2B3NziM).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2B3NziM](https://packt.live/2B3NziM)。
- en: You can also run this example online at [https://packt.live/2AjbACJ](https://packt.live/2AjbACJ).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址为[https://packt.live/2AjbACJ](https://packt.live/2AjbACJ)。
- en: 'Now that we have a good understanding of the required entities, we will study
    another important concept of RL: deep Q learning.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对所需的实体有了充分的了解，我们将学习强化学习中的另一个重要概念：深度Q学习。
- en: Deep Q Learning
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: Before diving into the details of the deep Q learning process, let's first discuss
    the disadvantages of the traditional tabular Q learning process, and then we will
    look at how combining deep learning with Q learning can help us to resolve these
    disadvantages of tabular methods.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论深度Q学习过程的细节之前，让我们先讨论传统表格Q学习方法的缺点，然后我们将看看将深度学习与Q学习结合如何帮助我们解决表格方法的这些缺点。
- en: 'The following describes several disadvantages of the tabular Q learning approach:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了表格Q学习方法的几个缺点：
- en: 'Performance issues: When the state spaces are very large, the tabular iterative
    lookup operations will be much slower and more costly.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能问题：当状态空间非常大时，表格的迭代查找操作将变得更加缓慢和昂贵。
- en: 'Storage issues: Along with the performance issues, storage will also be costly
    when it comes to storing the tabular data for large combinations of state and
    action spaces.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储问题：除了性能问题，当涉及到存储大规模状态和动作空间的表格数据时，存储成本也很高。
- en: The tabular method will work well only when an agent comes across seen discrete
    states that are present in the Q table. For the unseen states that are not present
    in the Q table, the agent's performance may be the optimal performance.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格方法仅在代理遇到Q表中已有的离散状态时表现良好。对于Q表中没有的未见过的状态，代理的表现可能是最优的。
- en: For continuous state spaces for the previously mentioned issues, the tabular
    Q learning method won't be able to approximate the Q values in an efficient or
    proper manner.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于之前提到的连续状态空间，表格Q学习方法无法以高效或恰当的方式近似Q值。
- en: Keeping all of these issues in mind, we can consider using a function approximator
    that will work as a mapping between states and Q values. In machine learning terms,
    we can think of this problem as using a non-linear function approximator to solve
    regression problems. Since we are thinking of a function approximator, a neural
    network works best as a function approximator through which we can approximate
    the Q values for each state-action pair. This act of combining Q learning with
    a neural network is called deep Q learning or DQN.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些问题，我们可以考虑使用一个函数逼近器，将其作为状态与Q值之间的映射。在机器学习中，我们可以将这个问题看作是使用非线性函数逼近器来解决回归问题。既然我们在考虑使用一个函数逼近器，神经网络作为函数逼近器最为适合，通过它我们可以为每个状态-动作对近似Q值。将Q学习与神经网络结合的这个过程称为深度Q学习或DQN。
- en: 'Let''s break down and explain each part of this puzzle:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解并解释这个难题的每个部分：
- en: '**Inputs to the DQN**:'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DQN的输入**：'
- en: The neural network accepts the states of the environment as input. For example,
    in the case of the FrozenLake-v0 environment, a state can be a simple coordinate
    of the grid at any given point in time. For more complex games such as Atari,
    the input can be a few consecutive snapshots of the screen in the form of an image
    as state representation. The number of nodes in the input layer will be the same
    as the number of states present in the environment.
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经网络接受环境的状态作为输入。例如，在 FrozenLake-v0 环境中，状态可以是任何给定时刻网格上的简单坐标。对于像 Atari 这样的复杂游戏，输入可以是几张连续的屏幕快照，作为状态表示的图像。输入层中的节点数将与环境中存在的状态数相同。
- en: '**Outputs from the DQN**:'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DQN 输出**：'
- en: The output would be the Q values for each action. For example, for any given
    environment, if there are four possible actions, then the output would have four
    Q values for each action. To choose the optimal action, we will select the action
    with the maximum Q value.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是每个动作的 Q 值。例如，对于任何给定的环境，如果有四个可能的动作，那么输出将为每个动作提供四个 Q 值。为了选择最佳动作，我们将选择具有最大
    Q 值的动作。
- en: '**The loss function and learning process**:'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数和学习过程**：'
- en: 'The DQN will accept states from the environment and, for each given input or
    state, the network will output an estimated Q value for each action. The objective
    of this is that it approximates the optimal Q value, which will satisfy the right-hand
    side of the Bellman equation, as shown in the following expression:'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DQN 将接受来自环境的状态，并且对于每个给定的输入或状态，网络将输出每个动作的估计 Q 值。其目标是逼近最优的 Q 值，这将满足贝尔曼方程右侧的要求，如下所示：
- en: '![Figure 9.23: Bellman equation'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.23：贝尔曼方程'
- en: '](img/B16182_09_23.jpg)'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_23.jpg)'
- en: 'Figure 9.23: Bellman equation'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23：贝尔曼方程
- en: To calculate the loss, we need the target Q value and the Q values coming from
    the network. From the preceding Bellman equation, the target Q values are calculated
    on the right-hand side of the equation. The loss from the DQN is calculated by
    comparing the output Q values from the DQN to the target Q values. Once we calculate
    the loss, we then update the weights of the DQN via backpropagation to minimize
    the loss and to push the DQN-output Q values closer to the optimal Q values. In
    this way, with the help of DQN, we treat the RL problem as a supervised learning
    problem with a source and a target.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算损失，我们需要目标 Q 值和来自网络的 Q 值。从前面的贝尔曼方程中，目标 Q 值是在方程的右侧计算出来的。DQN 的损失是通过将 DQN 输出的
    Q 值与目标 Q 值进行比较来计算的。一旦我们计算出损失，我们就通过反向传播更新 DQN 的权重，以最小化损失并使 DQN 输出的 Q 值更接近最优 Q 值。通过这种方式，在
    DQN 的帮助下，我们将强化学习问题视为一个有源和目标的监督学习问题。
- en: 'The DQN implementation can be visualized as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 实现可以如下可视化：
- en: '![Figure 9.24: DQN'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.24：DQN'
- en: '](img/B16182_09_24.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_24.jpg)'
- en: 'Figure 9.24: DQN'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24：DQN
- en: 'We can write the steps of the deep Q learning process as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下步骤编写深度 Q 学习过程：
- en: 'Initialize the weights to get an initial approximation of `Q(s,a)`:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重以获得`Q(s,a)`的初始近似值：
- en: '[PRE53]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'As you can see, we have initialized the DQN class with the weights. The two
    code lines in the `__init__` functions, where we create the neural network, are
    responsible for giving random weights to the network connections. We can also
    explicitly initialize the weights. A common practice nowadays is to let PyTorch
    or TensorFlow use their internal default initialization logic to create initial
    weight vectors, as you can see in the following sample code:'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，我们已经用权重初始化了 DQN 类。`__init__` 函数中的两行代码负责给网络连接赋予随机权重。我们也可以显式地初始化权重。现在常见的做法是让
    PyTorch 或 TensorFlow 使用其内部的默认初始化逻辑来创建初始权重向量，如下所示的代码示例：
- en: '[PRE54]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Doing one forward pass through the network, obtain the flags (`state`, `action`,
    `reward`, and `new_state`). The action is selected by taking the argmax of the
    Q values (selecting the index of the max Q value) or by taking random actions
    during the exploration phase. We can achieve this using the following code sample:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过网络进行一次前向传播，获取标志（`state`、`action`、`reward` 和 `new_state`）。通过对 Q 值取最大值的索引（选择最大
    Q 值的索引）来选择动作，或者在探索阶段随机选择动作。我们可以使用以下代码示例来实现这一点：
- en: '[PRE55]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As you can see in the preceding code snippet, the egreedy algorithm is being
    used to select the action. The `select_action` function passes the state through
    the DQN to obtain the Q values and selects the action with the highest Q value
    during exploitation. The `if` statement decides whether the exploration should
    be carried out or not.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你在前面的代码片段中看到的，使用了egreedy算法来选择动作。`select_action`函数通过DQN传递状态来获得Q值，并在利用过程中选择Q值最高的动作。`if`语句决定是否进行探索。
- en: 'If the episode is ended, the target Q value will be the reward obtained; otherwise,
    use the Bellman equation to estimate the target Q value. You can realize this
    in the following code sample:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果episode结束，则目标Q值将是获得的奖励；否则，使用Bellman方程来估计目标Q值。你可以在以下代码示例中实现：
- en: '[PRE56]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The loss obtained is as follows.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得的损失如下所示。
- en: If the episode ended, then the loss will be ![A drawing of a face
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果episode结束，则损失将是![A drawing of a face
- en: Description automatically generated](img/B16182_09_24a.png).
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_09_24a.png)。
- en: Otherwise, the loss will be termed as ![A close up of a logo
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则，损失将被称为![A close up of a logo
- en: Description automatically generated](img/B16182_09_24b.png) .
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_09_24b.png)。
- en: 'The following is sample code for `loss`:'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`loss`的示例代码：
- en: '[PRE57]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Using backpropagation, we update the network weights (θ). This iteration will
    run for each state until we sufficiently minimize the loss and get an approximate
    optimal Q function. The following is the sample code:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播，我们更新网络权重（θ）。此迭代将针对每个状态运行，直到我们足够地最小化损失并得到一个近似最优的Q函数。以下是示例代码：
- en: '[PRE58]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Now that we have a fair understanding of the implementation of deep Q learning,
    let's test our understanding with an exercise.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对深度Q学习的实现有了较为清晰的理解，接下来让我们通过一个练习来测试我们的理解。
- en: 'Exercise 9.03: Implementing a Working DQN Network with PyTorch in a CartPole-v0
    Environment'
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习9.03：在CartPole-v0环境中使用PyTorch实现一个有效的DQN网络
- en: In this exercise, we will implement the deep Q learning algorithm with the OpenAI
    Gym CartPole environment. The aim of this exercise is to build a PyTorch-based
    DQN model that will learn to balance the cart in the CartPole environment. Please
    refer to the PyTorch example for building neural networks, which was explained
    at the start of this chapter.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用OpenAI Gym CartPole环境实现深度Q学习算法。此练习的目的是构建一个基于PyTorch的DQN模型，学习在CartPole环境中平衡小车。请参考本章开始时解释的构建神经网络的PyTorch示例。
- en: 'Our main aims are to apply a Q learning algorithm, keep the pole steady during
    each step, and collect the maximum reward during each episode. A reward of +1
    is given for every step when the pole remains straight. The episode will end when
    the pole is more than 15 degrees away from the vertical position or when the cart
    moves more than 2.4 units away from the center position in the CartPole environment:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是应用Q学习算法，在每一步保持杆子稳定，并在每个episode中收集最大奖励。当杆子保持直立时，每一步会获得+1的奖励。当杆子偏离垂直位置超过15度，或小车在CartPole环境中偏离中心位置超过2.4单位时，episode将结束：
- en: 'Open a new Jupyter notebook and import the required libraries:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter笔记本并导入所需的库：
- en: '[PRE59]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Create a device based on the availability of a **Graphics Processing Unit**
    (**GPU**) environment:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据**图形处理单元**（**GPU**）环境的可用性创建设备：
- en: '[PRE60]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Create a Gym environment using the `''CartPole-v0''` environment:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`'CartPole-v0'`环境创建一个Gym环境：
- en: '[PRE61]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Set the `seed` for torch and the environment to guarantee reproducible results:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`seed`以保证torch和环境的可复现结果：
- en: '[PRE62]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Set all of the hyperparameter values required for the DQN process:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置DQN过程所需的所有超参数值：
- en: '[PRE63]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Implement a function for decaying the epsilon values after every step. We will
    decay with the epsilon value exponentially. The epsilon value will start with
    the value of `EGREEDY` and will be decayed until it reaches the value of `EGREEDY_FINAL`.
    Use the following formula:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个在每一步后衰减epsilon值的函数。我们将使用指数方式衰减epsilon值。epsilon值从`EGREEDY`开始，并会衰减直到达到`EGREEDY_FINAL`。使用以下公式：
- en: '[PRE64]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The code will be as follows:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码将如下所示：
- en: '[PRE65]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Fetch the number of states and actions from the environment:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中获取状态和动作的数量：
- en: '[PRE66]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output will be as follows:'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE67]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create a class, called `DQN`, that accepts the number of states as inputs and
    outputs Q values for the number of actions present in the environment, and has
    a network with a hidden layer of size `64`:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`DQN`的类，该类接受状态数量作为输入，并输出环境中动作数量的Q值，并具有一个大小为`64`的隐藏层网络：
- en: '[PRE68]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Create a `DQN_Agent` class and implement the constructor''s `_init_` function.
    This function will create an instance of the DQN class within which the hidden
    layer size is passed. It will also define the `MSE` as a loss criterion. Next,
    define `Adam` as the optimizer with model parameters and a predefined learning
    rate:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`DQN_Agent`类，并实现构造函数`_init_`。该函数将在其中创建一个DQN类的实例，并传递隐藏层大小。它还将定义`MSE`作为损失标准。接下来，定义`Adam`作为优化器，并设置模型参数及预定义的学习率：
- en: '[PRE69]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, define the `select_action` function that will accept `state` and egreedy
    values as input parameters. Use the `egreedy` algorithm to select the action.
    This function will pass the `state` through the DQN to get the Q value, and then
    select the action with the highest Q value using the `torch.max` operation during
    the exploitation phase. During this process, gradient computation is not required;
    that''s why we use the `torch.no_grad()` function to turn off the gradient calculation:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义`select_action`函数，该函数将接受`state`和epsilon值作为输入参数。使用`egreedy`算法选择动作。该函数将通过DQN传递`state`以获取Q值，然后在利用阶段使用`torch.max`操作选择具有最高Q值的动作。在此过程中，不需要梯度计算；因此我们使用`torch.no_grad()`函数来关闭梯度计算：
- en: '[PRE70]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Define the `optimize` function that will accept `state`, `action`, `new_state`,
    `reward`, and `done` as inputs and convert them into tensors, keeping their compatibility
    with the device used. If the episode is over, then we make the reward the target
    value; otherwise, the new state is passed through the DQN (which is used to detach
    and turn off the gradient calculation) to calculate the max part present in the
    right-hand side of the Bellman equation. Using the reward obtained and the discount
    factor, we can calculate the target value:![Figure 9.25: Target value equation'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`optimize`函数，该函数将接受`state`、`action`、`new_state`、`reward`和`done`作为输入，并将它们转换为张量，同时保持它们与所用设备的兼容性。如果该回合已结束，则将奖励设为目标值；否则，将新状态通过DQN（用于断开连接并关闭梯度计算）传递，以计算贝尔曼方程右侧的最大部分。利用获得的奖励和折扣因子，我们可以计算目标值：![图
    9.25：目标值方程
- en: '](img/B16182_09_25.jpg)'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_25.jpg)'
- en: '[PRE71]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Write a training process using a `for` loop. At first, instantiate the DQN
    agent using the class created earlier. Create a `steps_total` empty list to collect
    the total number of steps for each episode. Initialize `steps_counter` with zero
    and use it to calculate the decayed epsilon value for each step. Use two loops
    during the training process. The first one is to play the game for a certain number
    of steps. The second loop ensures that each episode goes on for a fixed number
    of steps. Inside the second `for` loop, the first step is to calculate the epsilon
    value for the current step. Using the present state and epsilon value, you select
    the action to perform. The next step is to take the action. Once you take the
    action, the environment returns the `new_state`, `reward`, and `done` flags. Using
    the `optimize` function, perform one step of gradient descent to optimize the
    DQN. Now make the new state the present state for the next iteration. Finally,
    check whether the episode is over or not. If the episode is over, you can collect
    and record the reward for the current episode:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`for`循环编写训练过程。首先，使用之前创建的类实例化DQN智能体。创建一个空的`steps_total`列表，用于收集每个回合的总步数。将`steps_counter`初始化为零，并用它来计算每个步骤的衰减epsilon值。在训练过程中使用两个循环。第一个循环是进行一定步数的游戏。第二个循环确保每个回合持续固定的步数。在第二个`for`循环中，第一步是计算当前步骤的epsilon值。使用当前状态和epsilon值，选择要执行的动作。接下来的步骤是执行该动作。一旦执行动作，环境将返回`new_state`、`reward`和`done`标志。利用`optimize`函数，执行一步梯度下降来优化DQN。现在，将新状态作为下次迭代的当前状态。最后，检查该回合是否结束。如果回合结束，则可以收集并记录当前回合的奖励：
- en: '[PRE72]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now observe the reward, as the reward is scalar feedback and gives you an indication
    of how well the agent is performing. You should look at the average reward and
    the average reward for the last 100 episodes:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在观察奖励，因为奖励是标量反馈，能够指示智能体的表现如何。你应该查看平均奖励以及过去100回合的平均奖励：
- en: '[PRE73]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output will be as follows:'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE74]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Perform the graphical representation of rewards. Check how the agent is performing
    while playing more episodes, and check what the reward average is for the last
    100 episodes:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行奖励的图形表示。检查代理在更多回合中如何表现，并检查过去100回合的奖励平均值：
- en: '[PRE75]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output plot should be as follows:'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出图应该如下所示：
- en: '![Figure 9.26: Rewards collected'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.26：收集的奖励'
- en: '](img/B16182_09_26.jpg)'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_26.jpg)'
- en: 'Figure 9.26: Rewards collected'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26：收集的奖励
- en: '*Figure 9.26* shows that the initial number of steps and rewards is low. However,
    with the increasing numbers of steps, we have collected a stable and higher value
    of rewards with the help of the DQN algorithm.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.26*显示了最初的步数和奖励值较低。然而，随着步数的增加，我们通过DQN算法收集到了稳定且更高的奖励值。'
- en: Note
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3cUE8Q9](https://packt.live/3cUE8Q9).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/3cUE8Q9](https://packt.live/3cUE8Q9)。
- en: You can also run this example online at [https://packt.live/37zeUpz](https://packt.live/37zeUpz).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址是[https://packt.live/37zeUpz](https://packt.live/37zeUpz)。
- en: Thus, we have successfully implemented a working DQN using PyTorch in a CartPole
    environment. Let's now look at a few challenging aspects of DQN.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功实现了在CartPole环境中使用PyTorch的DQN。现在，让我们看看DQN中的一些挑战性问题。
- en: Challenges in DQN
  id: totrans-431
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN中的挑战
- en: 'Everything that was explained in the preceding sections looks good; however,
    there are a few challenges with DQNs. Here are a couple of the challenges of a
    DQN:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 前面章节中解释的内容看起来很好；然而，DQN存在一些挑战。以下是DQN面临的几个挑战：
- en: The correlation between the steps causes a convergence issue during the training
    process
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步数之间的相关性在训练过程中造成了收敛问题
- en: The challenge of having a non-stationary target.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非稳定目标的挑战。
- en: These challenges and their corresponding solutions are explained in the following sections.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战及其相应的解决方案将在接下来的章节中进行解释。
- en: Correlation between Steps and the Convergence Issue
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步数之间的相关性和收敛问题
- en: From the previous exercise, we have seen that, during Q learning, we treat the
    RL problem as a supervised machine learning problem, where we have predictions
    and target values, and, using gradient descent optimization, we try to reduce
    the loss to find the optimal Q function.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的练习中，我们已经看到，在Q学习中，我们将RL问题视为监督学习问题，其中有预测值和目标值，并通过梯度下降优化来减少损失，找到最优的Q函数。
- en: The gradient descent algorithm assumes that the training data points are independent
    and identically distributed (that is, `i.i.d`), which is generally true in the
    case of traditional machine learning data. However, in the case of RL, each data
    point is highly correlated and dependent on the other. Put simply, the next state
    depends on the action taken from the previous state. Due to the correlation present
    in the RL data, we have a convergence issue in the case of the gradient descent
    algorithm.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法假设训练数据点是独立同分布的（即`i.i.d`），这一点在传统机器学习数据中通常成立。然而，在强化学习（RL）中，每个数据点是高度相关且依赖于其他数据点的。简而言之，下一状态取决于前一状态的动作。由于RL数据中的相关性，我们在梯度下降算法的情况下遇到了收敛问题。
- en: To solve this issue of convergence, we will now look at a possible solution,
    known as **Experience Replay**, in the following section.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决收敛问题，我们将在接下来的章节中介绍一个可能的解决方案——**经验回放**。
- en: Experience Replay
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验回放
- en: To break the correlation between the data points in the case of RL, we can use
    a technique called experience replay. Here, at each timestep during training,
    we store the agent's experience in a **Replay Buffer** (which is just a Python
    list).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 为了打破RL中数据点之间的相关性，我们可以使用一种名为经验回放（experience replay）的技术。在训练的每个时间步，我们将代理的经验存储在**回放缓冲区**（Replay
    Buffer）中（这只是一个Python列表）。
- en: For example, during the training at time t, the following agent experience is
    stored as a tuple in the replay buffer ![A picture containing object, clock
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在时间t的训练过程中，以下代理经验作为一个元组存储在回放缓冲区中 ![A picture containing object, clock
- en: 'Description automatically generated](img/B16182_09_26a.png), where:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B16182_09_26a.png)，其中：
- en: '![a](img/B16182_09_26b.png)- current state'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![a](img/B16182_09_26b.png)- 当前状态'
- en: '![b](img/B16182_09_26c.png)- action taken'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![b](img/B16182_09_26c.png)- 执行动作'
- en: '![c](img/B16182_09_26d.png)- new state'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![c](img/B16182_09_26d.png)- 新状态'
- en: '![d](img/B16182_09_26e.png)- reward'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![d](img/B16182_09_26e.png)- 奖励'
- en: '![d](img/B16182_09_26f.png)- indicates whether the episode is complete or not'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![d](img/B16182_09_26f.png)- 表示该回合是否完成'
- en: We set a maximum size for the replay buffer; we will keep on adding new tuples
    of experience as we encounter them. So, when we reach the maximum size, we will
    throw out the oldest value. At any given point in time, the replay buffer will
    always store the latest experience of maximum size.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为重放缓冲区设置了最大大小；随着新经验的出现，我们将继续添加新的经验元组。因此，当我们达到最大大小时，我们将丢弃最旧的值。在任何给定时刻，重放缓冲区始终会存储最新的经验，且大小不超过最大限制。
- en: During training, to break the correlation, we will randomly sample these experiences
    from the replay buffer to train the DQN. This process of gaining experience and
    sampling from the replay buffer that stores these experiences is called experience replay.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，为了打破相关性，我们将从重放缓冲区随机采样这些经验来训练 DQN。这个获取经验并从存储这些经验的重放缓冲区中进行采样的过程称为经验重放。
- en: During the Python implementation, we will use a push function to store the experiences
    in the replay buffer. An example function will be implemented to sample the experience
    from the buffer, the pointer, and the length method, which will help us to keep
    track of the replay buffer size.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 实现中，我们将使用一个 `push` 函数来将经验存储在重放缓冲区中。将实现一个示例函数，从缓冲区采样经验，指针和长度方法将帮助我们跟踪重放缓冲区的大小。
- en: The following is a detailed code implementation example of experience replay.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是经验重放的详细代码实现示例。
- en: 'We will implement an `ExperienceReplay` class with all of the functionality
    explained earlier. In the class, the constructor will contain the following variables:
    `capacity`, which indicates the maximum size of the replay buffer; `buffer`, which
    is an empty Python list that acts as the memory buffer; and `pointer`, which points
    to the current location of the memory buffer while pushing the memory to the buffer.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个包含之前解释的所有功能的 `ExperienceReplay` 类。在该类中，构造函数将包含以下变量：`capacity`，表示重放缓冲区的最大大小；`buffer`，一个空的
    Python 列表，充当内存缓冲区；以及 `pointer`，指向内存缓冲区当前的位置，在将内存推送到缓冲区时使用。
- en: The class will contain the `push` function, which checks whether there is any
    space in the buffer using the `pointer` variable. If there is an empty space,
    `push` adds an experience tuple at the end of the buffer, else the function will
    replace the memory from the starting point of the buffer. It also contains the
    `sample` function, which will return the experience tuple of the batch size, and
    the `__len__` function, which will return the length of the current buffer, as
    part of the implementation.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 该类将包含 `push` 函数，该函数使用 `pointer` 变量检查缓冲区中是否有空闲空间。如果有空闲空间，`push` 将在缓冲区的末尾添加一个经验元组，否则该函数将替换缓冲区起始点的内存。它还包含
    `sample` 函数，返回批量大小的经验元组，以及 `__len__` 函数，返回当前缓冲区的长度，作为实现的一部分。
- en: The following is an example of how the pointer, capacity, and modular division
    will work in experience replay.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指针、容量和模除在经验重放中的工作示例。
- en: 'We initialize the pointer with zero and capacity with three. After every operation,
    we increase the pointer value and, using modular division, we get the current
    value of the pointer. When the pointer exceeds the maximum capacity, the value
    will reset to zero:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指针初始化为零，并将容量设置为三。每次操作后，我们增加指针值，并通过模除运算得到指针的当前值。当指针超过最大容量时，值将重置为零：
- en: '![Figure 9.27: Pointer, capacity, and modular division in the experience replay
    class'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.27：经验重放类中的指针、容量和模除]'
- en: '](img/B16182_09_27.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_27.jpg)'
- en: 'Figure 9.27: Pointer, capacity, and modular division in the experience replay
    class'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27：经验重放类中的指针、容量和模除
- en: 'Adding all of the previously mentioned functionality, we can implement the
    `ExperienceReplay` class as shown in the following code snippet:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 添加上述所有功能后，我们可以实现如下代码片段所示的 `ExperienceReplay` 类：
- en: '[PRE76]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: As you can see, the experience class has been initiated.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，经验类已经被初始化。
- en: The Challenge of a Non-Stationary Target
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非平稳目标的挑战
- en: 'Consider the following code snippet. If you look closely at the following `optimize`
    function, you will see that we have two passes through the DQN network: one pass
    to calculate the target Q value (using the Bellman equation) and the other pass
    to calculate the predicted Q value. After that, we have calculated the loss:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下面的代码片段。如果仔细查看以下 `optimize` 函数，你会看到我们通过 DQN 网络进行了两次传递：一次计算目标 Q 值（使用贝尔曼方程），另一次计算预测的
    Q 值。之后，我们计算了损失：
- en: '[PRE77]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The first pass is just an approximation of the optimal Q value using the Bellman
    equation; however, to calculate the target and predicted Q values, we use the
    same weights from the network. This process makes the whole deep Q learning process
    unstable. Consider the following equation during the loss calculation:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次传递只是通过 Bellman 方程来近似最优 Q 值；然而，在计算目标 Q 值和预测 Q 值时，我们使用的是来自网络的相同权重。这个过程使整个深度
    Q 学习过程变得不稳定。在损失计算过程中，考虑以下方程：
- en: '![Figure 9.28: Expression for the loss calculation'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.28：损失计算的表达式'
- en: '](img/B16182_09_28.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_28.jpg)'
- en: 'Figure 9.28: Expression for the loss calculation'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28：损失计算的表达式
- en: After the loss is calculated, we perform one step of gradient descent and optimize
    the weights to minimize the loss. Once the weights are updated, the predicted
    Q value will change. However, our target Q values will also change because, to
    calculate the target Q values, we are using the same weights. Due to the unavailability
    of the fixed target Q value, this whole process is unstable in the current architecture.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 损失计算完成后，我们执行一次梯度下降步骤，优化权重以最小化损失。一旦权重更新，预测的 Q 值将发生变化。然而，我们的目标 Q 值也会发生变化，因为在计算目标
    Q 值时，我们使用的是相同的权重。由于固定目标 Q 值不可用，当前架构下整个过程是不稳定的。
- en: One solution to this problem would be to have a fixed target Q value during
    the whole training process.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方法是，在整个训练过程中保持固定的目标 Q 值。
- en: The Concept of a Target Network
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标网络的概念
- en: To resolve the issue of the non-stationary target, we can fix the issue by introducing
    a target neural network architecture in the pipeline. We call this network the
    **Target Network**. The target network will have the same network architecture
    as the base neural network in the architecture. We can call this base neural network
    the predicted DQN.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决非平稳目标的问题，我们可以通过在流程中引入目标神经网络架构来解决这个问题。我们称这个网络为**目标网络**。目标网络的架构与基础神经网络相同。我们可以将这个基础神经网络称为预测
    DQN。
- en: 'As discussed previously, to calculate the loss, we must do two passes through
    the DQN: one pass is to calculate the target Q values, and the second one is to
    calculate the predicted Q values.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了计算损失，我们必须通过 DQN 做两次传递：第一次是计算目标 Q 值，第二次是计算预测的 Q 值。
- en: 'Due to the architectural change, target Q values will be calculated using the
    target network and the predicted Q values process will remain the same, as shown
    in the following figure:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 由于架构的变化，目标 Q 值将通过目标网络计算，而预测 Q 值的过程保持不变，如下图所示：
- en: '![Figure 9.29: Target network'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.29：目标网络'
- en: '](img/B16182_09_29.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_29.jpg)'
- en: 'Figure 9.29: Target network'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29：目标网络
- en: 'As inferred from the preceding figure, the loss function can be written as
    follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图可以推断，损失函数可以写成如下形式：
- en: '![Figure 9.30: Expression for the loss function'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.30：损失函数的表达式'
- en: '](img/B16182_09_30.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_30.jpg)'
- en: 'Figure 9.30: Expression for the loss function'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30：损失函数的表达式
- en: The entire purpose of the target network is to calculate the max part in the
    Bellman equation using the new state-action pair.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络的整个目的就是使用新的状态-动作对来计算 Bellman 方程中的最大部分。
- en: At this point in time, the obvious question you may ask is, what about the weights
    or parameters of this target, and how can we get the target values in an optimum
    way from this target network? To ensure a balance between fixing the target value
    and the optimal target approximation using the target network, we will update
    the weights of the target network from the predicted values after every fixed
    iteration. But after how many iterations should we update the weights of the target
    network from the prediction network? Well, that's a hyperparameter that should
    be tuned during the training process of the DQN. This whole process makes the
    training process stable as the target Q values are fixed for a while.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能会问一个显而易见的问题，那就是，这个目标网络的权重或参数怎么办？我们如何从这个目标网络中以最优的方式获取目标值？为了在固定目标值和使用目标网络进行最优目标逼近之间保持平衡，我们将在每次固定的迭代后，从预测值更新目标网络的权重。但是，应该在多少次迭代后从预测网络更新目标网络的权重呢？这个问题的答案是一个超参数，在
    DQN 的训练过程中需要进行调整。整个过程使得训练过程更加稳定，因为目标 Q 值会在一段时间内保持固定。
- en: 'We can summarize the steps of training a DQN with experience replay and a target
    network as follows:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结使用经验回放和目标网络训练 DQN 的步骤如下：
- en: Initialize the replay buffer.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化回放缓冲区。
- en: Create and initialize the prediction network.
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并初始化预测网络。
- en: Create a copy of the prediction network as a target network.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建预测网络的副本作为目标网络。
- en: Run through the fixed number of episodes.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行固定次数的回合。
- en: 'Within every episode, perform the following steps:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一回合中，执行以下步骤：
- en: Use the egreedy algorithm to choose an action.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 egreedy 算法选择一个动作。
- en: Perform the action and collect the reward and new state.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作并收集奖励和新状态。
- en: Store the whole experience in the replay buffer.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将整个经验存储在重放缓冲区中。
- en: Select a random batch of experience from the replay buffer.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区中随机选择一批经验。
- en: Pass the batch of states through the prediction network to get the predicted
    Q values.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这一批状态通过预测网络传递，以获得预测的 Q 值。
- en: Using a new state, pass through the target network to calculate the target Q value.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个新的状态，通过目标网络计算目标 Q 值。
- en: Perform gradient descent to optimize the weights of the prediction network.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度下降，以优化预测网络的权重。
- en: After a fixed iteration, clone the weights of the prediction network to the
    target network.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在固定的迭代次数后，将预测网络的权重克隆到目标网络。
- en: Now we understand the concept of DQN, the disadvantages of DQN, and how we can
    overcome these disadvantages of DQN using experience replay and a target network;
    we can combine all of these to build a robust DQN algorithm. Let's implement our
    learning in the following exercise.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了 DQN 的概念、DQN 的不足之处以及如何通过经验重放和目标网络来克服这些不足；我们可以将这些结合起来，构建一个强健的 DQN 算法。让我们在接下来的练习中实现我们的学习。
- en: 'Exercise 9.04: Implementing a Working DQN Network with Experience Replay and
    a Target Network in PyTorch'
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 9.04：在 PyTorch 中实现带有经验重放和目标网络的有效 DQN 网络
- en: 'In the previous exercise, you implemented a working DQN to work with the CartPole
    environment. Then, we looked at the disadvantages of a DQN. Now, in this exercise,
    let''s implement the DQN network with experience replay and a target network using
    the same CartPole environment in PyTorch to build a more stable DQN learning process:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习中，你实现了一个有效的 DQN 来与 CartPole 环境一起工作。然后，我们看到了 DQN 的不足之处。现在，在本练习中，我们将使用 PyTorch
    实现带有经验重放和目标网络的 DQN 网络，以构建一个更加稳定的 DQN 学习过程：
- en: 'Open a new Jupyter notebook and import the required libraries:'
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook，并导入所需的库：
- en: '[PRE78]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Write code that will create a device based on the availability of a GPU environment:'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，根据 GPU 环境的可用性创建一个设备：
- en: '[PRE79]'
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Create a `gym` environment using the `''CartPole-v0''` environment:'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `'CartPole-v0'` 环境创建一个 `gym` 环境：
- en: '[PRE80]'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Set the seed for torch and the environment for reproducibility:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 torch 和环境的种子以保证可复现性：
- en: '[PRE81]'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Fetch the number of states and actions from the environment:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中获取状态和动作的数量：
- en: '[PRE82]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE83]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Set all of the hyperparameter values required for the DQN process. Please add
    several new hyperparameters, as stated here, along with the usual parameters:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 DQN 过程所需的所有超参数值。请添加几个新超参数，如这里所述，并与常规参数一起设置：
- en: '`REPLAY_BUFFER_SIZE` – This sets the replay buffer''s maximum length size.'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`REPLAY_BUFFER_SIZE` – 这设置了重放缓冲区的最大长度。'
- en: '`BATCH_SIZE` – This indicates how many sets of experiences ![A picture containing
    object, clock'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`BATCH_SIZE` – 这表示有多少组经验！'
- en: Description automatically generated](img/B16182_09_30a.png) are to be drawn
    to train the DQN.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B16182_09_30a.png) 用于训练 DQN。
- en: '`UPDATE_TARGET_FREQUENCY` – This is the periodic frequency at which target
    network weights will be refreshed from the prediction network:'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`UPDATE_TARGET_FREQUENCY` – 这是目标网络权重从预测网络中刷新周期的频率：'
- en: '[PRE84]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Use the previously implemented `calculate_epsilon` function to decay the epsilon
    value with increasing values of steps:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用先前实现的 `calculate_epsilon` 函数，通过增加的步数值来衰减 epsilon 值：
- en: '[PRE85]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Create a class, called `DQN`, that accepts the number of states as inputs and
    outputs Q values for the number of actions present in the environment, with the
    network that has a hidden layer of size `64`:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `DQN` 的类，该类接受状态数作为输入，并输出环境中动作数的 Q 值，网络的隐藏层大小为 `64`：
- en: '[PRE86]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Implement the `ExperienceReplay` class:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 `ExperienceReplay` 类：
- en: '[PRE87]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now instantiate the `ExperienceReplay` class by passing the buffer size as input:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在通过传入缓冲区大小作为输入，实例化 `ExperienceReplay` 类：
- en: '[PRE88]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Implement the `DQN_Agent` class.
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 `DQN_Agent` 类。
- en: Please note, here are several changes in the `DQN_Agent` class (which we used
    in *Exercise 9.03*, *Implementing a Working DQN Network with PyTorch in a CartPole-v0
    Environment*) that need to be incorporated with the previously implemented `DQN_Agent`
    class.
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，以下是`DQN_Agent`类中的一些更改（我们在*练习9.03*中使用了该类，即*在CartPole-v0环境中使用PyTorch实现一个有效的DQN网络*），这些更改需要与之前实现的`DQN_Agent`类进行整合。
- en: Create a replica of the normal DQN network and name it `target_dqn`. Use `target_dqn_update_counter`
    to periodically update the weights of the target DQN from the DQN network. Add
    the following steps. `memory.sample(BATCH_SIZE)` will randomly pull the experiences
    from the replay buffer for training. Pass `new_state` in the target network to
    get the target Q values from the target network. Finally, update the weights of
    the target network from the normal or predicted DQN after a certain iteration
    is specified in `UPDATE_TARGET_FREQUENCY`.
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建一个普通DQN网络的副本，并将其命名为`target_dqn`。使用`target_dqn_update_counter`周期性地从DQN网络更新目标DQN的权重。添加以下步骤：`memory.sample(BATCH_SIZE)`将从回放缓冲区随机抽取经验用于训练。将`new_state`传入目标网络，以从目标网络获取目标Q值。最后，在`UPDATE_TARGET_FREQUENCY`指定的某次迭代后，更新目标网络的权重。
- en: 'Note that we have used the `gather`, `squeeze`, and `unsqueeze` functions,
    which we studied in the dedicated *PyTorch Utilities* section:'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们使用了`gather`、`squeeze`和`unsqueeze`函数，这些是我们在专门的*PyTorch实用工具*部分中学习过的：
- en: '[PRE89]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Write the training process of the DQN network. The training process with experience
    relay and target DQN simplifies the process with less code.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写DQN网络的训练过程。使用经验回放和目标DQN的训练过程通过更少的代码简化了这个过程。
- en: 'First, instantiate the DQN agent using the class created earlier. Create a
    `steps_total` empty list to collect the total number of steps for each episode.
    Initialize `steps_counter` with zero and use it to calculate the decayed epsilon
    value for each step. Use two loops during the training process: the first one
    to play the game for a certain number of episodes; the second loop ensures that
    each episode goes on for a fixed number of steps.'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，使用之前创建的类实例化DQN代理。创建一个`steps_total`空列表，用于收集每个回合的总步数。将`steps_counter`初始化为零，并用它计算每步的衰减epsilon值。在训练过程中使用两个循环：第一个循环用于进行一定数量的回合；第二个循环确保每个回合进行固定数量的步骤。
- en: Inside the second `for` loop, the first step is to calculate the epsilon value
    for the current step. Using the present state and epsilon value, you select the
    action to perform.
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第二个`for`循环内部，第一步是计算当前步骤的epsilon值。利用当前状态和epsilon值，选择要执行的动作。
- en: The next step is to take the action. Once you take the action, the environment
    returns the `new_state`, `reward`, and `done` flags. Push `new_state`, `reward`,
    `done`, and `info` in the experience replay buffer. Using the `optimize` function,
    perform one step of gradient descent to optimize the DQN.
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一步是采取行动。一旦执行动作，环境会返回`new_state`、`reward`和`done`标志。将`new_state`、`reward`、`done`和`info`推送到经验回放缓冲区。使用`optimize`函数，执行一次梯度下降步骤来优化DQN。
- en: 'Now make the new state the present state for the next iteration. Finally, check
    whether the episode is over or not. If the episode is over, then you can collect
    and record the reward for the current episode:'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在将新的状态设为下次迭代的当前状态。最后，检查回合是否结束。如果回合结束，则可以收集并记录当前回合的奖励：
- en: '[PRE90]'
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Now observe the reward. As the reward is scalar feedback and gives you an indication
    of how well the agent is performing, you should look at the average reward and
    the average reward for the last 100 episodes. Also, perform the graphical representation
    of rewards. Check how the agent is performing while playing more episodes and
    what the reward average is for the last 100 episodes:'
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在观察奖励。由于奖励是标量反馈，并能指示代理的表现情况，您应该查看平均奖励和最后100个回合的平均奖励。同时，进行奖励的图形表示。检查代理在进行更多回合时的表现，以及最后100个回合的奖励平均值：
- en: '[PRE91]'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The output will be as follows:'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE92]'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Now we can see that the average reward for the last 100 episodes is higher for
    the DQN with experience replay, and the fixed target is higher than the vanilla
    DQN implemented in the previous exercise. This is because we have achieved stability
    during the DQN training process and because we have incorporated experience replay
    and a target network.
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们可以看到，对于最后100个回合，使用经验回放的DQN的平均奖励更高，并且固定目标比之前练习中实现的普通DQN更高。这是因为我们在DQN训练过程中实现了稳定性，并且加入了经验回放和目标网络。
- en: 'Plot the rewards in the y axis along with the number of steps in the x axis
    to see how the rewards have been collected with the increasing number of steps:'
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将奖励绘制在y轴上，并将步数绘制在x轴上，以查看随着步数增加，奖励的变化：
- en: '[PRE93]'
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '![Figure 9.31: Rewards collected'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图9.31：收集的奖励'
- en: '](img/B16182_09_31.jpg)'
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_31.jpg)'
- en: 'Figure 9.31: Rewards collected'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.31：收集的奖励
- en: As you can see in the preceding plot, using experience replay with the target
    network, the rewards are initially a bit low compared to the previous version
    (refer to *Figure 9.26*); however, after certain episodes, the rewards are relatively
    stable and the average rewards are high in the last 100 episodes.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图表中看到的，使用带有目标网络的经验回放时，最初的奖励相较于之前的版本（请参见*图9.26*）稍低；然而，经过若干回合后，奖励相对稳定，且最后100个回合的平均奖励较高。
- en: Note
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2C1KikL](https://packt.live/2C1KikL).
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/2C1KikL](https://packt.live/2C1KikL)。
- en: You can also run this example online at [https://packt.live/3dVwiqB](https://packt.live/3dVwiqB).
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，访问[https://packt.live/3dVwiqB](https://packt.live/3dVwiqB)。
- en: 'In this exercise, we have added experience replay and a target network in the
    vanilla DQN network (which was explained in *Exercise 9.03*, *Implementing a Working
    DQN Network with PyTorch in a CartPole-v0 Environment*) to overcome the drawbacks
    of a vanilla DQN. This results in much better performance in terms of rewards,
    as we have seen a more stable performance in terms of the average reward for the
    last 100 episodes. A comparison of the outputs is shown here:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们在原始DQN网络中添加了经验回放和目标网络（该网络在*练习9.03*中进行了说明，*在CartPole-v0环境中使用PyTorch实现工作DQN网络*），以克服原始DQN的缺点。结果在奖励方面表现更好，因为我们看到了在过去100个回合中的平均奖励更加稳定。输出的比较如下所示：
- en: '**Vanilla DQN Outputs**:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始DQN输出**：'
- en: '[PRE94]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '**DQN with Experience Replay and Target Network Outputs**:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '**具有经验回放和目标网络输出的DQN**：'
- en: '[PRE95]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Still, we have another issue with the DQN process, that is, overestimation in
    the DQN. We will learn more about this and how to tackle it in the next section.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DQN过程仍然存在另一个问题，即DQN中的高估问题。我们将在下一节中了解更多关于这个问题以及如何解决它。
- en: The Challenge of Overestimation in a DQN
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN中的高估问题
- en: 'In the previous section, we introduced a target network as a solution to fix
    the non-stationary target problem. Using this target network, we calculated the
    target Q value and calculated the loss. This whole process of introducing a new
    target network to calculate a fixed target value has somehow made the training
    process a bit more stable. However, in 2015, *Hado van Hasselt*, in his paper
    called *Deep Reinforcement Learning with Double Q-learning*, showed through multiple
    experiments that this process overestimates the target Q values and makes the
    whole training process unstable:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们引入了目标网络作为解决非平稳目标问题的方案。使用这个目标网络，我们计算了目标Q值并计算了损失。引入新的目标网络来计算固定目标值的整个过程在某种程度上使训练过程变得更加稳定。然而，2015年，*Hado
    van Hasselt* 在他名为《*深度强化学习与双重Q学习*》的论文中，通过多个实验展示了这一过程高估了目标Q值，使整个训练过程变得不稳定：
- en: '![Figure 9.32: Q value estimation in DQN and DDQN'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.32：DQN和DDQN中的Q值估计'
- en: '](img/B16182_09_32.jpg)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_32.jpg)'
- en: 'Figure 9.32: Q value estimation in DQN and DDQN'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.32：DQN和DDQN中的Q值估计
- en: Note
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding diagram has been sourced from the paper *Deep Reinforcement Learning
    with Double Q-learning* by *Hasselt et al., 2015*. Please refer to the following
    link for more in-depth reading on DDQN: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf).'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表来自*Hasselt等人，2015年*的论文《*深度强化学习与双重Q学习*》。欲了解DDQN的更深入阅读，请参阅以下链接：[https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)。
- en: After performing experiments on multiple Atari games, the authors of the paper
    showed that using a DQN network can lead to a high estimation of Q values (shown
    in orange), which indicates a high deviation from the true DQN values. In the
    paper, the authors proposed a new algorithm called **Double DQN**. We can see
    that, by using Double DQN, Q value estimations are much closer to true values
    and any overestimations are much lower. Now, let's discuss what Double DQN is
    and how it is different from a DQN with a target network.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在对多个Atari游戏进行实验后，论文的作者展示了使用DQN网络可能导致Q值的高估（如图中橙色所示），这表示与真实DQN值的偏差很大。在论文中，作者提出了一种新的算法叫做**双重DQN**。我们可以看到，通过使用双重DQN，Q值估计值更接近真实值，任何过估计都大大降低。现在，让我们讨论一下什么是双重DQN以及它与具有目标网络的DQN有什么不同。
- en: Double Deep Q Network (DDQN)
  id: totrans-567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双重深度Q网络（DDQN）
- en: 'In comparison to a DQN with a target network, the minor differences of a DDQN
    are given as follows:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于具有目标网络的DQN，DDQN的细微差异如下：
- en: A DDQN uses our prediction network to select the best action to take for the
    next state, by selecting the action with the highest Q values.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDQN通过选择具有最高Q值的动作，使用我们的预测网络选择下一状态下要采取的最佳动作。
- en: A DDQN uses the action from the prediction network to calculate the corresponding
    estimate of the target Q value (using the target network) at the next state.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDQN使用来自预测网络的动作来计算下一个状态下目标Q值的对应估计（使用目标网络）。
- en: 'As described in the *Deep Q Learning* section, the loss function for a DQN
    is as follows:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 如*深度Q学习*部分所述，DQN的损失函数如下：
- en: '![Figure 9.33: Loss function for a DQN'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.33：DQN的损失函数'
- en: '](img/B16182_09_33.jpg)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_33.jpg)'
- en: 'Figure 9.33: Loss function for a DQN'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.33：DQN的损失函数
- en: 'The updated loss function for a DDQN will be as follows:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN的更新损失函数如下：
- en: '![Figure 9.34: Updated loss function for a DDQN'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.34：更新后的DDQN损失函数'
- en: '](img/B16182_09_34.jpg)'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_34.jpg)'
- en: 'Figure 9.34: Updated loss function for a DDQN'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34：更新后的DDQN损失函数
- en: 'The following figure depicts the functioning of a typical DDQN:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了典型DDQN的工作原理：
- en: '![Figure 9.35: DDQN'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.35：DDQN'
- en: '](img/B16182_09_35.jpg)'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_09_35.jpg)'
- en: 'Figure 9.35: DDQN'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35：DDQN
- en: 'The following outlines the required changes in the optimize function for a
    DDQN implementation:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 以下概述了DDQN实现中优化函数所需的更改：
- en: Select an action using the prediction network.
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预测网络选择一个动作。
- en: 'We will pass the `new_state` through the prediction network to get the Q values
    for the `new_state`, as shown in the following code:'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将通过预测网络传递`new_state`，以获取`new_state`的Q值，如下代码所示：
- en: '[PRE96]'
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'To select the action, we will select the max index value from the output Q
    values, as shown here:'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了选择动作，我们将从输出的Q值中选择最大索引值，如下所示：
- en: '[PRE97]'
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Select the Q value for the best action using the target network.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标网络选择最佳动作的Q值。
- en: 'We will pass the `new_state` through the target network to get the Q values
    for the `new_state`, as shown in the following code:'
  id: totrans-590
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将通过目标网络传递`new_state`，以获取`new_state`的Q值，如下代码所示：
- en: '[PRE98]'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'For the Q values associated with the best action in the `new_state`, we use
    the target network, as shown in the following code:'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于与`new_state`相关的最佳动作的Q值，我们使用目标网络，如下代码所示：
- en: '[PRE99]'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: The `gather` function is used to select the Q values using the indexes fetched
    from the prediction network.
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`gather` 函数用于通过从预测网络获取的索引来选择Q值。'
- en: 'The following is a complete implementation of the DDQN with the required changes:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具有所需更改的完整DDQN实现：
- en: '[PRE100]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Now that we have studied the various concepts of DQN and DDQN, let's now concretize
    our understanding with an activity.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了DQN和DDQN的各种概念，让我们通过一个活动来具体化我们的理解。
- en: 'Activity 9.01: Implementing a Double Deep Q Network in PyTorch for the CartPole
    Environment'
  id: totrans-598
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 9.01：在PyTorch中为CartPole环境实现双重深度Q网络
- en: In this activity, you are tasked with implementing a DDQN in PyTorch for the
    CartPole environment to tackle the issue of overestimation in a DQN. We can summarize
    the steps of training a DQN with experience replay and the target network.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你的任务是在PyTorch中实现一个DDQN，以解决CartPole环境中DQN的过估计问题。我们可以总结出使用经验重放和目标网络训练DQN的步骤。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该活动：
- en: 'Open a new Jupyter notebook and import the required libraries:'
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter笔记本并导入所需的库：
- en: '[PRE101]'
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Write code that will create a device based on the availability of a GPU environment.
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，根据GPU环境的可用性创建设备。
- en: Create a `gym` environment using the `CartPole-v0` environment.
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`CartPole-v0`环境创建一个`gym`环境。
- en: Set the seed for torch and the environment for reproducibility.
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 torch 和环境的种子以确保结果可复现。
- en: Fetch the number of states and actions from the environment.
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中获取状态和动作的数量。
- en: Set all of the hyperparameter values required for the DQN process.
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 DQN 过程所需的所有超参数值。
- en: Implement the `calculate_epsilon` function.
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`calculate_epsilon`函数。
- en: Create a class, called `DQN`, that accepts the number of states as inputs and
    outputs Q values for the number of actions present in the environment, with the
    network that has a hidden layer of size 64.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`DQN`的类，该类接受状态数量作为输入，并输出环境中动作数量的 Q 值，网络具有 64 大小的隐藏层。
- en: Initialize the replay buffer.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化回放缓冲区。
- en: Create and initialize the prediction network in the `DQN_Agent` class, as shown
    in *Exercise 9.03*, *Implementing a Working DQN Network with Experience Replay
    and a Target Network in PyTorch*. Create a copy of the prediction network as the
    target network.
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`DQN_Agent`类中创建并初始化预测网络，如*练习 9.03*中所示，*在 PyTorch 中实现一个带有经验回放和目标网络的工作 DQN 网络*。创建预测网络的副本作为目标网络。
- en: Make changes to the `optimize` function of the `DQN_Agent` class according to
    the code example shown in the *Double Deep Q Network (DDQN)* section.
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*双深度 Q 网络（DDQN）*部分中展示的代码示例，修改`DQN_Agent`类中的`optimize`函数。
- en: Run through a fixed number of episodes. Inside the episode, use the egreedy
    algorithm to choose an action.
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行固定数量的回合。在每个回合中，使用ε-greedy算法选择一个动作。
- en: Perform the action and collect the reward and new state. Store the whole experience
    in the replay buffer.
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作并收集奖励和新状态。将整个经验存储在回放缓冲区中。
- en: Select a random batch of experience from the replay buffer. Pass the batch of
    states through the prediction network to get the predicted Q values.
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从回放缓冲区中选择一个随机的经验批次。将状态批次通过预测网络，以获得预测的 Q 值。
- en: Use our prediction network to select the best action to take for the next state
    by selecting the action with the highest Q values. Use the action from the prediction
    network to calculate the corresponding estimate of the target Q value at the next state.
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的预测网络选择下一状态要执行的最佳动作，通过选择具有最高 Q 值的动作。使用预测网络中的动作计算下一状态下目标 Q 值的对应估计。
- en: Perform gradient descent to optimize the weights of the prediction network.
    After a fixed iteration, clone the weights of the prediction network to the target network.
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行梯度下降优化预测网络的权重。经过固定迭代后，将预测网络的权重克隆到目标网络中。
- en: Check the average reward and the average reward for the last 100 episodes once
    you have trained the DDQN agent.
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 DDQN 代理后，检查平均奖励以及最后 100 回合的平均奖励。
- en: Plot the rewards collected in the y axis and the number of episodes in the x
    axis to visualize how the rewards have been collected with the increasing number
    of episodes.
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 y 轴绘制收集的奖励，x 轴绘制回合数，以可视化随着回合数增加，奖励是如何被收集的。
- en: 'The output for the average rewards should be similar to the following:'
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均奖励的输出应类似于以下内容：
- en: '[PRE102]'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'The plot for the rewards should be similar to the following:'
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励的图表应与以下类似：
- en: '![Figure 9.36: Plot for the rewards collected'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.36：奖励收集图'
- en: '](img/B16182_09_36.jpg)'
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_36.jpg)'
- en: 'Figure 9.36: Plot for the rewards collected'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.36：奖励收集图
- en: Note
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 743.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 743 页找到。
- en: 'Before we end the chapter, we present the following comparison of the average
    rewards for different DQN techniques and DDQN:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束之前，我们展示了不同 DQN 技术和 DDQN 的平均奖励对比：
- en: '**Vanilla DQN Outputs:**'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通 DQN 输出：**'
- en: '[PRE103]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '**DQN with Experience Replay and Target Network Outputs:**'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '**带有经验回放和目标网络的 DQN 输出：**'
- en: '[PRE104]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '**DDQN Outputs:**'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '**DDQN 输出：**'
- en: '[PRE105]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: As you can see from the preceding figure, along with the comparison of the results
    shown earlier, DDQN has the highest average reward, compared to other DQN implementations,
    and the average reward for the last 100 episodes is also higher. We can say that
    DDQN improves performance significantly in comparison to the other two DQN techniques.
    After completing this whole activity, we have learned how to combine a DDQN network
    with experience replay to overcome the issues of a vanilla DQN and achieve more
    stable rewards.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从前面的图表中看到的，结合之前展示的结果对比，DDQN相比其他DQN实现具有最高的平均奖励，且最后100个回合的平均奖励也较高。我们可以说，DDQN相比其他两种DQN技术显著提高了性能。在完成整个活动后，我们学会了如何将DDQN网络与经验回放结合，克服普通DQN的问题，并实现更稳定的奖励。
- en: Summary
  id: totrans-636
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started with an introduction to deep learning, and we looked
    at the different components of the deep learning process. Then, we learned how
    to build deep learning models using PyTorch.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了深度学习，并探讨了深度学习过程中的不同组成部分。然后，我们学习了如何使用PyTorch构建深度学习模型。
- en: Next, we slowly shifted our focus to RL, where we learned about value functions
    and Q learning. We demonstrated how Q learning can help us to build RL solutions
    without knowing the transition dynamics of the environment. We also investigated
    the problems associated with tabular Q learning and how to solve those performance
    and memory-related issues with deep Q learning.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们慢慢将焦点转向了强化学习（RL），在这里我们学习了价值函数和Q学习。我们展示了Q学习如何帮助我们在不知道环境过渡动态的情况下构建RL解决方案。我们还研究了表格Q学习相关的问题，以及如何通过深度Q学习解决这些与性能和内存相关的问题。
- en: Then, we looked into the issues related to a vanilla DQN implementation and
    how we can use a target network and experience replay mechanism to overcome issues
    such as correlated data and non-stationary targets during the training of a DQN.
    Finally, we learned how double deep Q learning helps us to overcome the issue
    of overestimation in a DQN. In the next chapter, you will learn how to use CNNs
    and RNNs in combination with a DQN to play the very popular Atari game Breakout.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入研究了与普通DQN实现相关的问题，以及如何使用目标网络和经验回放机制克服训练DQN时出现的相关数据和非平稳目标等问题。最后，我们学习了双重深度Q学习如何帮助我们克服DQN中的过度估计问题。在下一章，你将学习如何将卷积神经网络（CNN）和循环神经网络（RNN）与DQN结合使用来玩非常受欢迎的Atari游戏《Breakout》。
