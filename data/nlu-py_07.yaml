- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Selecting Approaches and Representing Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择方法和表示数据
- en: This chapter will cover the next steps in getting ready to implement a **natural
    language processing** (**NLP**) application. We start with some basic considerations
    about understanding how much data is needed for an application, what to do about
    specialized vocabulary and syntax, and take into account the need for different
    types of computational resources. We then discuss the first steps in NLP – text
    representation formats that will get our data ready for processing with NLP algorithms.
    These formats include symbolic and numerical approaches for representing words
    and documents. To some extent, data formats and algorithms can be mixed and matched
    in an application, so it is helpful to consider data representation independently
    from the consideration of algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖为实施**自然语言处理**（**NLP**）应用程序做好准备的下一步。我们从一些基本的考虑事项开始，包括了解应用程序所需的数据量，如何处理专有词汇和语法，并考虑不同类型计算资源的需求。然后，我们讨论
    NLP 的第一步——文本表示格式，这些格式将使我们的数据准备好用于 NLP 算法处理。这些格式包括用于表示单词和文档的符号和数值方法。在某种程度上，数据格式和算法可以在应用程序中进行混合搭配，因此，将数据表示与算法的考虑独立开来是很有帮助的。
- en: The first section will review general considerations for selecting NLP approaches
    that have to do with the type of application we’re working on, and with the data
    that we’ll be using.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第一节将回顾选择 NLP 方法的常规考虑事项，这些事项与我们正在处理的应用程序类型和我们将使用的数据有关。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Selecting NLP approaches
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择 NLP 方法
- en: Representing language for NLP applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 NLP 应用表示语言
- en: Representing language numerically with vectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用向量以数值方式表示语言
- en: Representing words with context-independent vectors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与上下文无关的向量表示单词
- en: Representing words with context-dependent vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与上下文相关的向量表示单词
- en: Selecting NLP approaches
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择 NLP 方法
- en: NLP can be done with a wide variety of possible techniques. When you get started
    on an NLP application, you have many choices to make, which are affected by a
    large number of factors. One of the most important factors is the type of application
    itself and the information that the system needs to extract from the data to perform
    the intended task. The next section addresses how the application affects the
    choice of techniques.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 可以使用多种可能的技术来实现。当你开始进行 NLP 应用时，你有许多选择需要做出，这些选择受到许多因素的影响。最重要的因素之一是应用程序本身的类型，以及系统需要从数据中提取的信息，以执行预定任务。下一节将讨论应用程序如何影响技术的选择。
- en: Fitting the approach to the task
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使方法适应任务
- en: Recall from [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016), that there are
    many different types of NLP applications divided into **interactive** and **non-interactive
    applications**. The type of application you choose will play an important role
    in choosing the technologies that will be applied to the task. Another way of
    categorizing applications is in terms of the level of detail required to extract
    the needed information from the document. At the coarsest level of analysis (for
    example, classifying documents into two different categories), techniques can
    be less sophisticated, faster to train, and less computationally intensive. On
    the other hand, if the task is training a chatbot or voice assistant that needs
    to pull out multiple entities and values from each utterance, the analysis needs
    to be more sensitive and fine-grained. We will see some specific examples of this
    in later sections of this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[*第一章*](B19005_01.xhtml#_idTextAnchor016)，NLP 应用程序可以分为**互动式**和**非互动式应用程序**。你选择的应用程序类型将在选择应用于任务的技术方面起到重要作用。另一种分类应用程序的方式是根据从文档中提取所需信息所需的详细程度。在最粗略的分析级别（例如，将文档分类为两种不同类别）时，技术可以较不复杂，训练速度较快，计算负担较轻。另一方面，如果任务是训练一个需要从每个发言中提取多个实体和值的聊天机器人或语音助手，分析需要更加敏感和精细。我们将在本章的后续部分看到一些具体的例子。
- en: In the next section, we will discuss how data affects our choice of techniques.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论数据如何影响我们选择技术的方法。
- en: Starting with the data
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据开始
- en: NLP applications are built on datasets or sets of examples of the kinds of data
    that the target system will need to process. To build a successful application,
    having the right amount of data is imperative. However, we can’t just specify
    a single number of examples for every application, because the right amount of
    data is going to be different for different kinds of applications. Not only do
    we have to have the right amount of data, but we have to have the right kind of
    data. We’ll talk about these considerations in the next two sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）应用程序是建立在数据集或目标系统需要处理的数据示例集合上的。为了构建一个成功的应用程序，拥有合适数量的数据是至关重要的。然而，我们不能为每个应用程序指定一个固定的示例数量，因为不同类型的应用程序所需的数据量是不同的。我们不仅需要拥有合适的数据量，还必须拥有合适的数据类型。我们将在接下来的两节中讨论这些考虑因素。
- en: How much data is enough?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多少数据才足够？
- en: In [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), we discussed many methods
    of obtaining data, and after going through [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    you should have a good idea of where your data will be coming from. However, in
    that chapter, we did not address the question of how to tell how much data is
    needed for your application to accomplish its goal. If there are hundreds or thousands
    of different possible classifications of documents in a task, then we will need
    a sufficient number of examples of each category for the system to be able to
    tell them apart. Obviously, the system can’t detect a category if it hasn’t ever
    seen an example of that category, but it will also be quite difficult to detect
    categories where it has seen very few examples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B19005_05.xhtml#_idTextAnchor107)中，我们讨论了获取数据的多种方法，在阅读完[*第5章*](B19005_05.xhtml#_idTextAnchor107)后，你应该对数据的来源有一个清晰的了解。然而，在该章节中，我们并没有涉及如何判断应用程序需要多少数据才能实现其目标这个问题。如果一个任务中有数百或数千种不同的文档分类，那么我们就需要足够多的每一类示例，才能使系统区分它们。显然，如果系统从未见过某一类别的示例，它是无法识别该类别的，但即使是它见过的类别，如果示例很少，也会很难进行区分。
- en: If there are many more examples of some classes than others, then we have an
    **imbalanced** dataset. Techniques for balancing classes will be discussed in
    detail in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248),but basically, they
    include **undersampling** (where some items in the more common class are discarded),
    **oversampling** (where items in the rarer classes are duplicated), and **generation**
    (where artificial examples from the rarer classes are generated through rules).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些类别的示例比其他类别多得多，那么我们就有了一个**不平衡**的数据集。平衡类别的技术将在[*第14章*](B19005_14.xhtml#_idTextAnchor248)中详细讨论，但基本方法包括**欠采样**（丢弃更多常见类别中的一些项）、**过采样**（复制稀有类别中的项目）和**生成**（通过规则生成稀有类别的人工示例）。
- en: Systems generally perform better if they have more data, but the data also has
    to be representative of the data that the system will encounter at the time of
    testing, or when the system is deployed as an application. If a lot of new vocabulary
    has been added to the task (for example, if a company’s chatbot has to deal with
    new product names), the training data needs to be periodically updated for the
    best performance. This is related to the general question of specialized vocabulary
    and syntax since product names are a kind of specialized vocabulary. In the next
    section, we will discuss this topic.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 系统通常在数据量更多时表现更好，但这些数据还必须代表系统在测试时或作为应用程序部署时将遇到的数据。如果任务中加入了许多新的词汇（例如，如果一个公司的聊天机器人需要处理新的产品名称），则训练数据需要定期更新以获得最佳性能。这与专门词汇和语法的普遍问题有关，因为产品名称是一种专门的词汇。我们将在下一节讨论这一主题。
- en: Specialized vocabulary and syntax
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专门词汇和语法
- en: Another consideration is how similar the data is to the rest of the natural
    language we will be processing. This is important because most NLP processing
    makes use of models that were derived from previous examples of the language.
    The more similar the data to be analyzed is to the rest of the language, the easier
    it will be to build a successful application. If the data is full of specialized
    jargon, vocabulary, or syntax, then it will be hard for the system to generalize
    from its original training data to the new data. If the application is full of
    specialized vocabulary and syntax, the amount of training data will need to be
    increased to include this new vocabulary and syntax.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是数据与我们将要处理的其他自然语言的相似性。这一点很重要，因为大多数NLP处理都利用了从先前语言示例中派生的模型。数据越与语言其他部分相似，构建成功应用程序就越容易。如果数据充满了专有术语、词汇或语法，那么系统从原始训练数据推广到新数据将变得困难。如果应用中充满了专业词汇和语法，那么需要增加训练数据的量，以包括这些新的词汇和语法。
- en: Considering computational efficiency
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑计算效率
- en: The computer resources that are needed to implement a particular NLP approach
    are an important consideration in selecting an approach. Some approaches that
    yield good results when tested on laboratory benchmarks can be impractical in
    applications that are intended for deployment. In the next sections, we will discuss
    the important consideration of the time required to execute the approaches, both
    at training time and inference time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 实现特定NLP方法所需的计算资源是在选择方法时需要考虑的重要因素。一些在实验室基准测试中能够获得良好结果的方法，在计划部署的应用中可能不切实际。在接下来的章节中，我们将讨论执行这些方法时所需时间的重要性，包括训练时间和推理时间。
- en: Training time
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练时间
- en: Some modern neural net models are very computationally intensive and require
    very long training periods. Even before the actual neural net training starts,
    there may need to be some exploratory efforts aimed at identifying the best values
    for hyperparameters. **Hyperparameters** are training parameters that can’t directly
    be estimated during the training process, and that have to be set by the developer.
    When we return to machine learning techniques in *Chapters 9*, *10*, *11*, and
    *12*, we will look at specific hyperparameters and talk about how to identify
    good values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些现代神经网络模型计算密集，训练周期非常长。甚至在神经网络训练开始之前，可能需要进行一些探索性工作，旨在找出超参数的最佳值。**超参数**是无法在训练过程中直接估计的训练参数，必须由开发者设置。当我们在*第9章*、*第10章*、*第11章*和*第12章*中回到机器学习技术时，我们将具体讨论超参数，并谈论如何识别合适的值。
- en: Inference time
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理时间
- en: Another important consideration is **inference time**, or the processing time
    required for a trained system to perform its task. For interactive applications
    such as chatbots, inference time is not normally a concern because today’s systems
    are fast enough to keep up with a user in an interactive application. If a system
    takes a second or two to process a user’s input, that’s acceptable. On the other
    hand, if the system needs to process a large amount of existing online text or
    audio data, the inference time should be as fast as possible. For example, Statistica.com
    ([https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/](https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/))
    estimated in February 2020 that 500 hours of videos were uploaded to YouTube every
    minute. If an application was designed to process YouTube videos and needed to
    keep up with that volume of audio, it would need to be very fast.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是**推理时间**，即经过训练的系统执行任务所需的处理时间。对于聊天机器人等互动应用，推理时间通常不是问题，因为如今的系统足够快，能够跟上用户在互动应用中的节奏。如果一个系统需要一两秒钟来处理用户输入，那是可以接受的。另一方面，如果系统需要处理大量现有的在线文本或音频数据，那么推理时间应尽可能快。例如，Statistica.com（[https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/](https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/)）在2020年2月估算，每分钟就有500小时的视频上传到YouTube。如果一个应用需要处理YouTube视频并且需要跟上这种音频的上传速度，那么它必须非常快速。
- en: Initial studies
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初步研究
- en: Practical NLP requires fitting the tool to the problem. When there are new advances
    in NLP technology, there can be very enthusiastic articles in the press about
    what the advances mean. But if you’re trying to solve a practical problem, trying
    to use new techniques can be counterproductive because the newest technologies
    might not scale. For example, new techniques might provide higher accuracy, but
    at the cost of very long training periods or very large amounts of data. For this
    reason, it is recommended that when you’re trying to solve a practical problem,
    do some initial exploratory studies with simpler techniques to see whether they’ll
    solve the problem. Only if the simpler techniques don’t address the problem’s
    requirements should more advanced techniques be utilized.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中的 NLP 需要将工具与问题相匹配。当 NLP 技术有新的进展时，媒体可能会发布关于这些进展意味着什么的热情文章。但如果你正在尝试解决一个实际问题，使用新技术可能适得其反，因为最新的技术可能无法扩展。例如，新的技术可能提供更高的准确性，但代价是非常长的训练时间或非常大的数据量。因此，建议在尝试解决实际问题时，先使用更简单的技术进行初步探索性研究，以查看它们是否能解决问题。只有在简单的技术无法满足问题要求时，才应使用更先进的技术。
- en: In the next section, we will talk about one of the important choices that need
    to be made when you design an NLP application – how to represent the data. We’ll
    look at both symbolic and numerical representations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论设计 NLP 应用时需要做出的一个重要选择——如何表示数据。我们将讨论符号化和数字化表示方法。
- en: Representing language for NLP applications
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 NLP 应用表示语言
- en: For computers to work with natural language, it has to be represented in a form
    that they can process. These representations can be **symbolic**, where the words
    in a text are processed directly, or **numeric**, where the representation is
    in the form of numbers. We will describe both of these approaches here. Although
    the numeric approach is the primary approach currently used in NLP research and
    applications, it is worth becoming somewhat familiar with the ideas behind symbolic
    processing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让计算机能够处理自然语言，必须将语言表示为它们可以处理的形式。这些表示可以是**符号化的**，即直接处理文本中的单词，或者是**数字化的**，即表示为数字的形式。我们将在这里描述这两种方法。尽管数字化方法目前是
    NLP 研究和应用中主要使用的方法，但了解符号处理背后的思想也是值得的。
- en: Symbolic representations
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号化表示
- en: 'Traditionally, NLP has been based on processing the words in texts directly,
    as words. This approach was embodied in a standard approach where the text was
    analyzed in a series of steps that were aimed at converting an input consisting
    of unanalyzed words into a meaning. In a traditional NLP pipeline, shown in *Figure
    7**.1*, each step in processing, from input text to meaning, produces an output
    that adds more structure to its input and prepares it for the next step in processing.
    All of these results are symbolic – that is, non-numerical. In some cases, the
    results might include probabilities, but the actual results are symbolic:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，NLP 一直是基于直接处理文本中的单词。这种方法体现在一种标准的方法中，即将文本分析为一系列步骤，旨在将由未经分析的单词组成的输入转换为意义。在传统的
    NLP 管道中，如*图 7.1*所示，处理的每一步，从输入文本到意义，都会生成一个输出，增加更多的结构，并为下一步处理做好准备。所有这些结果都是符号化的——也就是说，非数字化的。在某些情况下，结果可能包括概率，但实际结果是符号化的：
- en: "![Figure 7.1\uFEFF – Traditional NLP symbolic pipeline](img/B19005_07_01.jpg)"
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: "![图 7.1\uFEFF – 传统的 NLP 符号化管道](img/B19005_07_01.jpg)"
- en: Figure 7.1 – Traditional NLP symbolic pipeline
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 传统的 NLP 符号化管道
- en: 'Although we won’t review all of the components of the symbolic approach to
    processing, we can see a couple of these symbolic results in the following code
    samples, showing part of speech tagging results and parsing results, respectively.
    We will not address semantic analysis or pragmatic analysis here since these techniques
    are generally applied only for specialized problems:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会回顾符号化处理方法的所有组成部分，但我们可以在以下的代码示例中看到其中的一些符号化结果，分别展示了词性标注结果和句法分析结果。我们不会在这里讨论语义分析或语用分析，因为这些技术通常只应用于特定的问题：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code snippet shows the process of importing the movie review
    database, which we have seen previously, followed by the code for selecting the
    first sentence and part of speech tagging it. The next code snippet shows the
    results of part of speech tagging:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段展示了导入我们之前看到的电影评论数据库的过程，接着是选择第一句并进行词性标注的代码。下一个代码片段展示了词性标注的结果：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The tags (`NN`, `CD`, `NNS`, etc.) shown in the preceding results are those
    used by NLTK and are commonly used in NLP. They are originally based on the Penn
    Treebank tags (*Building a Large Annotated Corpus of English: The Penn Treebank*
    (Marcus et al., CL 1993)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结果中显示的标签（`NN`、`CD`、`NNS`等）是NLTK使用的标签，并且在自然语言处理（NLP）中广泛使用。这些标签最初基于宾夕法尼亚树库的标签（*构建大型英语注释语料库：宾夕法尼亚树库*（Marcus等，CL
    1993））。
- en: 'Another important type of symbolic processing is `plot: two teen couples go
    to a church party, drink and then drive`, which we saw in the preceding code snippet,
    in the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种重要的符号处理方法是`plot: two teen couples go to a church party, drink and then drive`，我们在前面的代码片段中看到过，在以下代码中也有体现：'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code uses spaCy to parse the first sentence in the movie review
    corpus. After the parse, we iterate through the tokens in the resulting document
    and print the token and its part of speech tag. These are followed by the text
    of the token’s head, or the word that it depends on, and the kind of dependency
    between the head and the token. For example, the word `couples` is tagged as a
    plural noun, it is dependent on the word `go`, and the dependency is `nsubj` since
    `couples` is the noun subject (`nsubj`) of `go`. In [*Chapter 4*](B19005_04.xhtml#_idTextAnchor085),
    we saw an example of a visualization of a dependency parse (*Figure 4**.6*), which
    represents the dependencies as arcs between the item and its head; however, in
    the preceding code, we see more of the underlying information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用spaCy解析电影评论语料库中的第一句话。解析后，我们遍历结果文档中的所有标记，并打印出每个标记及其词性标签。接着是该标记的主词或它依赖的词的文本，以及主词与标记之间的依赖关系。例如，单词`couples`被标记为复数名词，它依赖于单词`go`，其依赖关系是`nsubj`，因为`couples`是`go`的主语（`nsubj`）。在[*第4章*](B19005_04.xhtml#_idTextAnchor085)中，我们看到了一种依赖解析的可视化示例（*图4.6*），该图通过弧线表示项目与其主词之间的依赖关系；然而，在上述代码中，我们看到的是更底层的信息。
- en: In this section, we have seen some examples of symbolic representations of language
    based on analyzing individual words and phrases, including parts of speech of
    individual words and labels for phrases. We can also represent words and phrases
    using a completely different and completely numeric approach based on vectors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经看到了一些基于分析单个单词和短语的符号表示语言的例子，包括单个单词的词性和短语的标签。我们也可以使用完全不同的、完全基于向量的数字化方法来表示单词和短语。
- en: Representing language numerically with vectors
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用向量进行语言的数字化表示
- en: A common mathematical technique for representing language in preparation for
    machine learning is through the use of vectors. Both documents and words can be
    represented with vectors. We’ll start by discussing document vectors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在为机器学习做准备时，表示语言的常见数学方法是通过使用向量。文档和单词都可以用向量表示。我们将首先讨论文档向量。
- en: Understanding vectors for document representation
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解用于文档表示的向量
- en: We have seen that texts can be represented as sequences of symbols such as words,
    which is the way that we read them. However, it is usually more convenient for
    computational NLP purposes to represent text numerically, especially if we are
    dealing with large quantities of text. Another advantage of numerical representation
    is that we can also process text represented numerically with a much wider range
    of mathematical techniques.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，文本可以表示为单词等符号的序列，这是我们读取文本的方式。然而，出于计算机自然语言处理的目的，通常使用数字方式来表示文本，特别是当我们处理大量文本时。数字表示的另一个优点是，我们可以使用更广泛的数学技术来处理数字表示的文本。
- en: A common way to represent both documents and words is by using vectors, which
    are basically one-dimensional arrays. Along with words, we can also use vectors
    to represent other linguistic units, such as lemmas or stemmed words, which were
    described in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表示文档和单词的一种常见方式是使用向量，向量本质上是一个一维数组。除了单词外，我们还可以使用向量来表示其他语言单位，如词根或词干形式的单词，这些在[*第5章*](B19005_05.xhtml#_idTextAnchor107)中有描述。
- en: Binary bag of words
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元词袋模型
- en: In [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059) and [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134),
    we briefly discussed the `1` or `0`, depending on whether that word occurs in
    that document. Each position in the vector is a feature of the document – that
    is, whether or not the word occurs. This is the simplest form of the BoW, and
    it is called the **binary bag of words**. It is immediately clear that this is
    a very coarse way of representing documents. All it cares about is whether a word
    occurs in a document, so it fails to capture a lot of information – what words
    are nearby, where in the document the words occur, and how often the words occur
    are all missing in the binary BoW. It is also affected by the lengths of documents
    since longer documents will have more words.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 3 章*](B19005_03.xhtml#_idTextAnchor059)和[*第 6 章*](B19005_06.xhtml#_idTextAnchor134)中，我们简要讨论了根据单词是否出现在文档中来设置
    `1` 或 `0`。向量中的每个位置代表文档的一个特征 —— 即单词是否出现。这是词袋的最简单形式，称为**二元词袋**。显然，这是一个非常粗糙的表示文档的方式。它只关心单词是否出现在文档中，因此无法捕获许多信息
    —— 比如单词的邻近关系、单词在文档中的位置以及单词出现的频率等都没有在二元词袋中体现。此外，它还受到文档长度的影响，因为较长的文档会有更多的单词。
- en: A more detailed version of the BoW approach is to count not just whether a word
    appears in a document but also how many times it appears. For this, we'll move
    on to the next technique, **count bag** **of words**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的词袋方法不仅仅是计算单词是否出现在文档中，还要计算它出现的次数。为此，我们将转到下一个技术，**计数词袋**。
- en: Count bag of words
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数词袋
- en: It seems intuitive that the number of times a word occurs in a document would
    help us decide how similar two documents are to each other. However, so far, we
    haven’t used that information. In the document vectors we’ve seen so far, the
    values are just one and zero – one if the word occurs in the document and zero
    if it doesn’t. If instead, we let the values represent the number of times the
    word occurs in the document, then we have more information. The BoW that includes
    the frequencies of the words in a document is a **count BoW**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，单词在文档中出现的次数似乎能帮助我们判断两篇文档的相似度。然而，到目前为止，我们还没有利用这些信息。在我们之前看到的文档向量中，值仅为 1 和
    0 —— 如果单词出现在文档中则为 1，否则为 0。如果我们让这些值代表单词在文档中出现的次数，那么我们就能获得更多的信息。包含单词频率的词袋就是**计数词袋**。
- en: 'We saw the code to generate the binary BoW in the *Bag of words and k-means
    clustering* section (*Figure 6**.15*) in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134).
    The code can be very slightly modified to compute a count BoW. The only change
    that has to be made is to increment the total count for a word when it is found
    more than once in a document. This is shown in the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第 6 章*](B19005_06.xhtml#_idTextAnchor134)的*词袋和 k-means 聚类*部分（*图 6.15*）中看到了生成二元词袋的代码。这段代码可以略微修改来计算计数词袋。唯一需要更改的是，当单词在文档中出现多次时，递增该单词的总计数。下面的代码展示了这一点：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Comparing this code to the code in *Figure 6**.15*, we can see that the only
    difference is that the value of `features[word]`is incremented when the word is
    found in the document, rather than set to `1`. The resulting matrix, shown in
    *Figure 7**.2*, has many different values for word frequencies than the matrix
    in *Figure 6**.16*, which had only zeros and ones:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将这段代码与*图 6.15*中的代码进行对比，我们可以看到，唯一的区别是，当单词在文档中出现时，`features[word]`的值会被递增，而不是设置为`1`。得到的矩阵，如*图
    7.2*所示，包含了比*图 6.16*中的矩阵更多不同的单词频率值，后者仅包含零和一：
- en: "![Figure 7.2\uFEFF – Count BoW for the movie review corpus](img/B19005_07_02.jpg)"
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: "![图 7.2\uFEFF – 电影评论语料库的词袋计数](img/B19005_07_02.jpg)"
- en: Figure 7.2 – Count BoW for the movie review corpus
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 电影评论语料库的词袋计数
- en: In *Figure 7**.2*, we’re looking at 10 randomly selected documents, numbered
    from `0` to `9` in the first column. Looking more closely at the frequencies of
    `film`, (recall that `film` is the most common non-stopword in the corpus), we
    can see that all of the documents except document `5` and document `6` have at
    least one occurrence of `film`. In a binary BoW, they would all be lumped together,
    but here they have different values, which allows us to make finer-grained distinctions
    among documents. At this point, you might be interested in going back to the clustering
    exercise in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134), modifying the code
    in *Figure 6**.22* to use the count BoW, and looking at the resulting clusters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7**.2*中，我们查看了 10 个随机选取的文档，第一列从 `0` 到 `9` 进行编号。更仔细地查看 `film` 的频率（回想一下 `film`
    是语料库中最常见的非停用词），我们可以看到除了文档 `5` 和文档 `6` 外，所有文档至少出现了一次 `film`。在二元 BoW 中，它们将被一起归为一类，但在这里它们有不同的值，这使得我们能够在文档之间进行更精细的区分。此时，您可能有兴趣返回[*第
    6 章*](B19005_06.xhtml#_idTextAnchor134)中的聚类练习，修改*图 6**.22*中的代码以使用计数 BoW，并查看生成的聚类结果。
- en: We can see from *Figure 7**.2* that the count BoW gives us some more information
    about the words that occur in the documents than the binary BoW. However, we can
    do an even more precise analysis by using a technique called **term frequency-inverse
    document frequency** (**TF-IDF**), which we describe in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从*图 7**.2*中看到，计数 BoW 比二元 BoW 给出了一些关于文档中出现词语的更多信息。然而，通过使用一种称为**TF-IDF**的技术，我们可以进行更精确的分析，下一节将对此进行描述。
- en: Term frequency-inverse document frequency
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 术语频率-逆文档频率
- en: 'Considering that our goal is to find a representation that accurately reflects
    similarities between documents, we can make use of some other insights. Specifically,
    consider the following observations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的目标是找到一个准确反映文档之间相似性的表示，我们可以利用一些其他的见解。具体来说，请考虑以下观察：
- en: The raw frequency of words in a document will vary based on the length of the
    document. This means that a shorter document with fewer overall words might not
    appear to be similar to a longer document that has more words. So, we should be
    considering the proportion of the words in the document rather than the raw number.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档中词语的原始频率将根据文档的长度而变化。这意味着具有较少总词数的较短文档可能看起来不像具有更多词语的较长文档那样相似。因此，我们应该考虑文档中词语的比例而不是原始数量。
- en: Words that occur very often overall will not be useful in distinguishing documents,
    because every document will have a lot of them. Clearly, the most problematic
    words that commonly occur within a corpus will be the stopwords we discussed in
    [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107), but other words that are not
    strictly stopwords can have this property as well. Recall when we looked at the
    movie review corpus in [*Chapter 6*](B19005_06.xhtml#_idTextAnchor134) that the
    words `film` and `movie` were very common in both positive and negative reviews,
    so they won’t be able to help us tell those categories apart. The most helpful
    words will probably be the words that occur with different frequencies in different
    categories.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体上非常频繁出现的词语对于区分文档并不有用，因为每个文档都会包含很多这些词语。显然，在我们讨论的[*第 5 章*](B19005_05.xhtml#_idTextAnchor107)中提到的停用词中，最有问题的词语将是停用词，但其他不严格属于停用词的词语也可能具有这种属性。回想一下我们在[*第
    6 章*](B19005_06.xhtml#_idTextAnchor134)中看过的电影评论语料库，诸如 `film` 和 `movie` 这样的词语在正面和负面评论中都非常常见，因此它们无法帮助我们区分这些类别。最有帮助的词语可能是在不同类别中以不同频率出现的词语。
- en: A popular way of taking these concerns into account is the measure *TF-IDF*.
    TF-IDF consists of two measurements – **term frequency** (**TF**) and **inverse
    document** **frequency** (**IDF**).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一种考虑这些问题的流行方法是度量*TF-IDF*。TF-IDF 包括两个测量值 – **术语频率** (**TF**) 和 **逆文档频率** (**IDF**)。
- en: 'TF is the number of times a term (or word) appears in a document, divided by
    the total number of terms in the document (which takes into account the fact that
    longer documents have more words overall). We can define that value as `tf(term,
    document)`. For example, in *Figure 7**.2,* we can see that the term `film` appeared
    once in document `0`, so `tf("film",0)` is `1` over the length of document `0`.
    Since the second document contains `film` four times, `tf("film",1)` is `4` over
    the length of document `1`. The formula for term frequency is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TF是一个术语（或单词）在文档中出现的次数，除以文档中的总词数（这考虑到较长的文档通常包含更多的单词）。我们可以将该值定义为`tf(term, document)`。例如，在*图7.2*中，我们可以看到术语`film`在文档`0`中出现了一次，因此`tf("film",0)`为文档`0`长度的`1`。由于第二个文档中`film`出现了四次，`tf("film",1)`为文档`1`长度的`4`。术语频率的公式如下：
- en: tf(t, d) =  f t,d _ Σ t ′ ∈d f t ′ ,d
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: tf(t, d) =  f  t,d  _  Σ  t′ ∈d f  t′ ,d
- en: 'However, as we saw with stopwords, very frequent words don’t give us much information
    for distinguishing documents. Even if `TF(term, document)` is very large, that
    could just be due to the fact that `term` occurs frequently in every document.
    To take care of this, we introduce IDF. The numerator of `idf(term,Documents)`is
    the total number of documents in the corpus, *N*, which we divide by the number
    of documents (*D*) that contain the term, *t*. In case the term doesn’t appear
    in the corpus, the denominator would be 0, so `1` is added to the denominator
    to prevent division by 0\. `idf(term, documents)`is the log of this quotient.
    The formula for `idf` is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们看到的停用词那样，频繁出现的单词对于区分文档没有太大帮助。即使`TF(term, document)`非常大，也可能仅仅是因为`term`在每篇文档中都频繁出现。为了解决这个问题，我们引入了IDF。`idf(term,
    Documents)`的分子是语料库中的总文档数，*N*，我们将其除以包含术语*t*的文档数（*D*）。如果术语在语料库中没有出现，分母将为0，因此为了防止除以0，分母加1。`idf(term,
    documents)`是这个商的对数。`idf`的公式如下：
- en: 'idf(t, D) = log  N ____________  |{d ∈ D : t ∈ d|}'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'idf(t, D) = log  N  ____________  |{d ∈ D : t ∈ d}|'
- en: 'Then the *TF-IDF* value for a term in a document in a given corpus is just
    the product of its TF and IDF:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，文档中给定语料库中术语的*TF-IDF*值仅仅是其TF和IDF的乘积：
- en: tfidf(t, d, D) = tf ⋅ idf(t, D)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf(t, d, D) = tf ⋅ idf(t, D)
- en: To compute the TF-IDF vectors of the movie review corpus, we will use another
    very useful package, called `scikit-learn`, since NLTK and spaCy don’t have built-in
    functions for TF-IDF. The code to compute TF-IDF in those packages could be written
    by hand using the standard formulas we saw in the preceding three equations; however,
    it will be faster to implement if we use the functions in `scikit-learn`, in particular,
    `tfidfVectorizer` in the feature extraction package. The code to compute TF-IDF
    vectors for the movie review corpus of 2,000 documents is shown in *Figure 7**.3*.
    In this example, we will look only at the top 200 terms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算电影评论语料库的TF-IDF向量，我们将使用另一个非常有用的包——`scikit-learn`，因为NLTK和spaCy没有内置的TF-IDF函数。在这些包中计算TF-IDF的代码可以通过手动编写标准公式来完成，但如果我们使用`scikit-learn`中的函数，特别是在特征提取包中的`tfidfVectorizer`，实现起来会更快。计算包含2,000篇文档的电影评论语料库的TF-IDF向量的代码如*图7.3*所示。在这个例子中，我们将只关注前200个术语。
- en: A tokenizer is defined in lines 9-11\. In this example, we are using just the
    standard NLTK tokenizer, but any other text-processing function could be used
    here as well. For example, we might want to try using stemmed or lemmatized tokens,
    and these functions could be included in the `tokenize()` function. Why might
    stemming or lemmatizing text be a good idea?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器在第9-11行定义。在这个例子中，我们使用的是标准的NLTK分词器，但也可以使用任何其他的文本处理函数。例如，我们可能想尝试使用词干提取或词形还原后的词汇，这些功能可以包含在`tokenize()`函数中。为什么词干提取或词形还原文本可能是个好主意？
- en: One reason that this kind of preprocessing could be useful is that it will reduce
    the number of unique tokens in the data. This is because words that have several
    different variants will be collapsed into their root word (for example, *walk*,
    *walks*, *walking*, and *walked* will all be treated as the same word). If we
    believe that this variation is mostly just a source of noise in the data, then
    it’s a good idea to collapse the variants by stemming or lemmatization. However,
    if we believe that the variation is important, then it won’t be a good idea to
    collapse the variants, because this will cause us to lose information. We can
    make this kind of decision a priori by thinking about what information is needed
    for the goals of the application, or we can treat this decision as a hyperparameter,
    exploring different options and seeing how they affect the accuracy of the final
    result.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预处理方法可能有用的一个原因是，它可以减少数据中唯一标记的数量。这是因为具有多个不同变体的单词将被合并为它们的根词（例如，*walk*、*walks*、*walking*
    和 *walked* 都会被视为同一个词）。如果我们认为这种变化大多只是数据中的噪声源，那么通过词干提取或词形还原来合并这些变体是一个好主意。然而，如果我们认为这种变化很重要，那么合并变体就不是一个好主意，因为这会导致信息丢失。我们可以通过考虑应用目标所需的信息，提前做出这种决策，也可以将此决策视为一个超参数，探索不同的选项并观察它们如何影响最终结果的准确性。
- en: '*Figure 7**.3* shows a screenshot of the code.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.3* 显示了代码的截图。'
- en: "![Figure 7.3\uFEFF – Code to compute TF-IDF vectors for the movie review corpus](img/B19005_07_03.jpg)"
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 用于计算电影评论语料库的 TF-IDF 向量的代码](img/B19005_07_03.jpg)'
- en: Figure 7.3 – Code to compute TF-IDF vectors for the movie review corpus
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 用于计算电影评论语料库的 TF-IDF 向量的代码
- en: Returning to *Figure 7**.3*, the next step after defining the tokenization function
    is to define the path where the data can be found (line 13) and initialize the
    token dictionary at line 14\. The code then walks the data directory and collects
    the tokens from each file. While the code is collecting tokens, it lowercases
    the text and removes punctuation (line 22). The decisions to lowercase the text
    and remove punctuation are up to the developer, similar to the decision we discussed
    previously on whether or not to stem or lemmatize the tokens. We could explore
    empirically whether or not these two preprocessing steps improve processing accuracy;
    however, if we think about how much meaning is carried by case and punctuation,
    it seems clear that in many applications, case and punctuation don’t add much
    meaning. In those kinds of applications, lowercasing the text and removing punctuation
    will improve the results.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到 *图 7.3*，在定义分词函数后，下一步是定义数据存储路径（第 13 行）并在第 14 行初始化标记字典。然后，代码遍历数据目录并从每个文件中收集标记。在收集标记的过程中，代码会将文本转换为小写并移除标点符号（第
    22 行）。将文本转换为小写并移除标点符号的决定由开发者做出，类似于我们之前讨论的是否对标记进行词干提取或词形还原的决定。我们可以通过经验来探索这两步预处理是否提高了处理的准确性；然而，如果我们思考一下大小写和标点符号带来的含义，我们会发现，在许多应用中，大小写和标点符号并没有增加太多意义。在这些应用中，将文本转换为小写并移除标点符号将改善结果。
- en: Having collected and counted the tokens in the full set of files, the next step
    is to initialize `tfIdfVectorizer`, which is a built-in function in scikit-learn.
    This is accomplished in lines 25-29\. The parameters include the type of input,
    whether or not to use IDF, which tokenizer to use, how many features to use, and
    the language for stopwords (English, in this example).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集并统计完所有文件中的标记后，下一步是初始化 `tfIdfVectorizer`，这是 scikit-learn 中的一个内置函数。此操作在第 25
    至 29 行完成。参数包括输入类型、是否使用 IDF、使用哪个分词器、使用多少特征以及停用词的语言（本示例中为英语）。
- en: Line 32 is where the real work of TF-IDF is done, with the `fit_transform` method,
    where the TF-IDF vector is built from the documents and the tokens. The remaining
    code (lines 37-42) is primarily to assist in displaying the resulting TF-IDF vectors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第 32 行是 TF-IDF 实际工作的地方，使用 `fit_transform` 方法，在此方法中，TF-IDF 向量是从文档和标记中构建的。其余的代码（第
    37 至 42 行）主要用于帮助显示最终的 TF-IDF 向量。
- en: 'The resulting TF-IDF matrix is shown in *Figure 7**.4*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的 TF-IDF 矩阵显示在 *图 7.4* 中：
- en: "![Figure 7.4\uFEFF – Partial \uFEFFTF-IDF vectors for some of the documents\
    \ in the movie review corpus](img/B19005_07_04.jpg)"
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 电影评论语料库中部分文档的 TF-IDF 向量](img/B19005_07_04.jpg)'
- en: Figure 7.4 – Partial TF-IDF vectors for some of the documents in the movie review
    corpus
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 电影评论语料库中部分文档的 TF-IDF 向量
- en: We have now represented our movie review corpus as a matrix where every document
    is a vector of *N* dimensions, where *N* is the maximum size of vocabulary we
    are going to use. *Figure 7**.4* shows the TF-IDF vectors for a few of the 2,000
    documents in the corpus (documents 0-4 and 1995-1999), shown in the rows, and
    some of the words in the corpus, shown in alphabetical order at the top. Both
    the words and the documents are truncated for display purposes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将电影评论语料库表示为一个矩阵，其中每个文档是一个*N*维的向量，*N*是我们要使用的最大词汇量的大小。*图 7.4*展示了语料库中部分2,000个文档（文档0-4和1995-1999）的TF-IDF向量，这些文档显示在行中，语料库中的一些词汇按字母顺序排列在顶部。为了展示的方便，文档和词汇都进行了截断。
- en: In *Figure 7**.4*, we can see that there is quite a bit of difference in the
    TF-IDF values for the same words in different documents. For example, `acting`
    has considerably different scores in documents `0` and `1`. This will become useful
    in the next step of processing (classification), which we will return to in [*Chapter
    9*](B19005_09.xhtml#_idTextAnchor173). Note that we have not done any actual machine
    learning yet; so far, the goal has been simply to convert documents to numerical
    representations, based on the words they contain.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.4*中，我们可以看到，在不同文档中，相同词汇的TF-IDF值有很大差异。例如，`acting`在文档`0`和文档`1`中的得分差异显著。这将在下一步的处理（分类）中变得有用，我们将在[*第9章*](B19005_09.xhtml#_idTextAnchor173)中回到这一部分。请注意，我们还没有进行实际的机器学习；到目前为止，目标仅仅是将文档转换为基于其包含的词汇的数值表示。
- en: Up until now, we’ve been focusing on representing documents. But what about
    representing the words themselves? The words in a document vector are just numbers
    representing their frequency, either just in a document or their frequency in
    a document relative to their frequency in a corpus (TF-IDF). We don’t have any
    information about the meanings of the words themselves in the techniques we’ve
    looked at so far. However, it seems clear that the meanings of words in a document
    should also impact the document’s similarity to other documents. We will look
    at representing the meanings of words in the next section. This representation
    is often called **word embeddings** in the NLP literature. We will begin with
    a popular representation of words as vectors, **Word2Vec**, which captures the
    similarity in meaning of words to each other.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在关注文档的表示。但那么，如何表示词汇本身呢？文档向量中的词汇只是代表其频率的数字，要么仅在文档中出现的频率，要么是文档中词汇相对于语料库中词汇频率的频率（TF-IDF）。在我们迄今为止查看的技术中，并没有包含词汇本身的意义。然而，似乎很明显，文档中词汇的意义也应当影响文档与其他文档的相似度。我们将在下一节中探讨如何表示词汇的意义。这种表示方法在自然语言处理（NLP）领域中通常被称为**词向量**。我们将从一种流行的词向量表示方法——**Word2Vec**开始，它捕捉了词与词之间的语义相似性。
- en: Representing words with context-independent vectors
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用与上下文无关的向量表示词汇
- en: So far, we have looked at several ways of representing similarities among documents.
    However, finding out that two or more documents are similar to each other is not
    very specific, although it can be useful for some applications, such as intent
    or document classification. In this section, we will talk about representing the
    meanings of words with word vectors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了几种表示文档相似性的方法。然而，发现两个或多个文档彼此相似并不十分具体，尽管它对于某些应用场景（如意图或文档分类）可能有用。在这一节中，我们将讨论如何使用词向量表示词汇的意义。
- en: Word2Vec
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: Word2Vec is a popular library for representing words as vectors, published by
    Google in 2013 (Mikolov, Tomas; et al. (2013). *Efficient Estimation of Word Representations
    in Vector Space*. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)).
    The basic idea behind Word2Vec is that every word in a corpus is represented by
    a single vector that is computed based on all the contexts (nearby words) in which
    the word occurs. The intuition behind this approach is that words with similar
    meanings will occur in similar contexts. This intuition is summarized in a famous
    quote from the linguist J. R. Firth, “*You shall know a word by the company it
    keeps*” (*Studies in Linguistic* *Analysis*, Wiley-Blackwell).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一个流行的词向量表示库，由谷歌于2013年发布（Mikolov, Tomas 等人，2013年，《*Efficient Estimation
    of Word Representations in Vector Space*》）。Word2Vec的基本思想是，语料库中的每个词汇都由一个单一的向量表示，这个向量是根据词汇出现的所有上下文（周围词汇）计算得出的。这种方法背后的直觉是，具有相似意义的词汇会出现在相似的上下文中。这一直觉在语言学家J.
    R. Firth的名言中得到了总结：“*你可以通过一个词的伴随词来了解这个词*”（《语言学分析研究》，Wiley-Blackwell）。
- en: Let’s build up to Word2Vec by starting with the idea of assigning each word
    to a vector. The simplest vector that we can use to represent words is the idea
    of `1` in a specific position in the vector, and all the rest of the positions
    are zeros (it is called one-hot encoding because one bit is on – that is, *hot*).
    The length of the vector is the size of the vocabulary. The set of one-hot vectors
    for the words in a corpus is something like a dictionary in that, for example,
    we could say that if the word is `movie`, it will be represented by `1` in a specific
    position. If the word is `actor`, it will be represented by `1` in a different
    position.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过从将每个单词映射到一个向量的思想开始，逐步构建 Word2Vec。我们用来表示单词的最简单向量是这样的：在向量的某个特定位置上为 `1`，其他所有位置为
    `0`（这种方法被称为 one-hot 编码，因为只有一位是开着的——即 *hot*）。向量的长度是词汇表的大小。语料库中所有单词的 one-hot 向量集合有点像字典，比如，我们可以说如果单词是
    `movie`，它将在某个特定位置上用 `1` 来表示。如果单词是 `actor`，它将在另一个位置上用 `1` 来表示。
- en: 'At this point, we aren’t taking into account the surrounding words yet. The
    first step in one-hot encoding is integer encoding, where we assign a specific
    integer to each word in the corpus. The following code uses libraries from scikit-learn
    to do the integer encoding and the one-hot encoding. We also import some functions
    from the `numpy` library, `array` and `argmax`, which we’ll be returning to in
    later chapters:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们还没有考虑周围的词语。one-hot 编码的第一步是整数编码，我们为语料库中的每个单词分配一个特定的整数。以下代码使用来自 scikit-learn
    的库进行整数编码和 one-hot 编码。我们还从 `numpy` 库中导入了一些函数，如 `array` 和 `argmax`，我们将在后面的章节中再次使用这些函数：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Looking at the first 50 integer encodings in the first movie review, we see
    a vector of length `50`. This can be converted to a one-hot encoding, as shown
    in the following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下第一部电影评论中前 50 个整数编码，我们看到一个长度为 `50` 的向量。这个向量可以转换为 one-hot 编码，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output in the preceding code shows first a subset of the one-hot vectors.
    Since they are one-hot vectors, they have `0` in all positions except one position,
    whose value is `1`. Obviously, this is very sparse, and it would be good to provide
    a more condensed representation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码输出首先显示了一个子集的 one-hot 向量。由于它们是 one-hot 向量，因此它们在除一个位置之外的所有位置上都是 `0`，而唯一的那个位置值为
    `1`。显然，这种表示方式是非常稀疏的，因此提供一个更紧凑的表示方式会更好。
- en: The next to the last line shows how we can invert a one-hot vector to recover
    the original word. The sparse representation takes a large amount of memory and
    is not very practical.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 倒数第二行展示了我们如何将 one-hot 向量反转以恢复原始单词。稀疏表示占用大量内存，而且不太实际。
- en: The Word2Vec approach uses a neural net to reduce the dimensionality of the
    embedding. We will be returning to the details of neural networks in [*Chapter
    10*](B19005_10.xhtml#_idTextAnchor184), but for this example, we’ll use a library
    called `Gensim` that will compute Word2Vec for us.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 方法使用神经网络来减少嵌入的维度。我们将在 [*第 10 章*](B19005_10.xhtml#_idTextAnchor184)
    中回到神经网络的细节，但对于这个例子，我们将使用一个名为 `Gensim` 的库，它将为我们计算 Word2Vec。
- en: 'The following code uses the Gensim `Word2Vec` library to create a model of
    the movie review corpus. The Gensim `model` object that was created by Word2Vec
    includes a large number of interesting methods for working with the data. The
    following code shows one of these – `most_similar`, which, given a word, can find
    the words that are the most similar to that word in the dataset. Here, we can
    see a list of the 25 words most similar to `movie` in the corpus, along with a
    score that indicates how similar that word is to `movie`, according to the Word2Vec
    analysis:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用 Gensim 的 `Word2Vec` 库来创建一个电影评论语料库的模型。通过 Word2Vec 创建的 Gensim `model` 对象包括许多有趣的用于处理数据的方法。以下代码展示了其中一个方法——`most_similar`，该方法给定一个单词后，能够在数据集中找到与该单词最相似的单词。这里，我们可以看到与语料库中
    `movie` 最相似的 25 个单词的列表，以及根据 Word2Vec 分析，表示单词与 `movie` 相似度的得分：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As can be seen in the preceding code, the words that Word2Vec finds to be the
    most similar to `movie` based on the contexts in which they occur are very much
    what we would expect. The top two words, `film` and `picture`, are very close
    synonyms of `movie`. In [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), we will
    return to Word2Vec and see how this kind of model can be used in NLP tasks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码可以看到，Word2Vec 根据单词出现的上下文，找到的与 `movie` 最相似的单词正是我们所期待的。排名前两位的单词，`film` 和
    `picture`，是 `movie` 的近义词。在 [*第 10 章*](B19005_10.xhtml#_idTextAnchor184) 中，我们将回到
    Word2Vec，并查看这种模型如何用于 NLP 任务。
- en: While Word2Vec does take into account the contexts in which words occur in a
    dataset, every word in the vocabulary is represented by a single vector that encapsulates
    all of the contexts in which it occurs. This glosses over the fact that words
    can have different meanings in different contexts. The next section reviews approaches
    representing words depending on specific contexts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Word2Vec考虑了词语在数据集中出现的上下文，但词汇表中的每个词都由一个单一的向量表示，该向量囊括了词语出现的所有上下文。这忽视了词语在不同上下文中可能具有不同含义的事实。下一节将回顾基于特定上下文来表示词语的方法。
- en: Representing words with context-dependent vectors
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用上下文相关的向量表示词语
- en: Word2Vec’s word vectors are context-independent in that a word always has the
    same vector no matter what context it occurs in. However, in fact, the meanings
    of words are strongly affected by nearby words. For example, the meanings of the
    word *film* in *We enjoyed the film* and *the table was covered with a thin film
    of dust* are quite different. To capture these contextual differences in meanings,
    we would like to have a way to have different vector representations of these
    words that reflect the differences in meanings that result from the different
    contexts. This research direction has been extensively explored in the last few
    years, starting with the **BERT** (**Bidirectional Encoder Representations from
    Transformers**) system ([https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)
    (Devlin et al., NAACL 2019)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的词向量是上下文无关的，即一个词在任何上下文中总是具有相同的向量。然而，实际上，词语的含义受到附近词语的强烈影响。例如，*film*在句子*We
    enjoyed the film*和*the table was covered with a thin film of dust*中的含义是完全不同的。为了捕捉这些上下文中含义的差异，我们希望能够为这些词提供不同的向量表示，以反映由于不同上下文而导致的含义差异。这个研究方向在过去几年中得到了广泛的探索，最初是从**BERT**（**双向编码器表示的变换器**）系统开始的（[https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)（Devlin等，NAACL
    2019））。
- en: This approach has resulted in great improvements in NLP technology, which we
    will want to discuss in depth. For that reason, we will postpone a fuller look
    at context-dependent word representations until we get to [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193),
    where we will address this topic in detail.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法极大地推动了自然语言处理技术的进步，我们将在后文中深入讨论。因此，我们将在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中详细探讨上下文相关的词语表示，在那一章中我们将对此话题进行深入讲解。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we’ve learned how to select different NLP approaches, based
    on the available data and other requirements. In addition, we’ve learned about
    representing data for NLP applications. We’ve placed particular emphasis on vector
    representations, including vector representations of both documents and words.
    For documents, we’ve covered binary bag of words, count bag of words, and TF-IDF.
    For representing words, we’ve reviewed the Word2Vec approach and briefly introduced
    context-dependent vectors, which will be covered in much more detail in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何根据可用的数据和其他需求选择不同的自然语言处理方法。此外，我们还学习了如何表示用于自然语言处理应用的数据。我们特别强调了向量表示，包括文档和词语的向量表示。对于文档表示，我们介绍了二进制词袋模型、词袋计数模型和TF-IDF方法。对于词语表示，我们回顾了Word2Vec方法，并简要介绍了上下文相关的向量，这将在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中更加详细地讨论。
- en: In the next four chapters, we will take the representations that we’ve learned
    about in this chapter and show how to train models from them that can be applied
    to different problems such as document classification and intent recognition.
    We will start with rule-based techniques in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159),
    discuss traditional machine learning techniques in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173),
    talk about neural networks in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184),
    and discuss the most modern approaches, transformers, and pretrained models in
    [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的四章中，我们将基于本章学习的表示方法，展示如何训练模型，并将其应用于不同的问题，如文档分类和意图识别。我们将从[*第8章*](B19005_08.xhtml#_idTextAnchor159)的基于规则的方法开始，讨论在[*第9章*](B19005_09.xhtml#_idTextAnchor173)中传统的机器学习技术，讲解[*第10章*](B19005_10.xhtml#_idTextAnchor184)中的神经网络，并在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中讨论最现代的方法——变换器和预训练模型。
