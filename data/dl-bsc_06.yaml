- en: 5\. Backpropagation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 反向传播
- en: The previous chapter described neural network training. There, the gradient
    of a weight parameter in the neural network (i.e., the gradient of the loss function
    for a weight parameter) was obtained by using numerical differentiation. Numerical
    differentiation is simple, and its implementation is easy, but it has the disadvantage
    that calculation takes time. This chapter covers backpropagation, which is a more
    efficient way to calculate the gradients of weight parameters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章讲解了神经网络的训练。在那一章中，神经网络中权重参数的梯度（即损失函数对权重参数的梯度）是通过数值微分获得的。数值微分很简单，且易于实现，但它的缺点是计算速度较慢。本章介绍了反向传播，它是一种更高效地计算权重参数梯度的方法。
- en: There are two ways to understand backpropagation correctly. One of them uses
    "equations," while the other uses **computational graphs**. The former is a common
    way, and many books about machine learning expand on this by focusing on formulas.
    This is good because it is strict and simple, but it may hide essential details
    or end in a meaningless list of equations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以正确理解反向传播。一种使用“方程式”，另一种使用**计算图**。前者是一种常见方法，许多关于机器学习的书籍通过专注于公式来扩展这一点。这种方式很好，因为它严格且简单，但可能隐藏了重要细节，或者最终只是一些毫无意义的方程式列表。
- en: Therefore, this chapter will use computational graphs so that you can understand
    backpropagation "visually." Writing code will deepen your understanding further
    and convince you of this. The idea of using computational graphs to explain backpropagation
    is based on Andrej Karpathy's blog (*Hacker's guide to Neural Networks*, ([http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/)))
    and the deep learning course (*CS231n:* *Convolutional Neural Networks for Visual
    Recognition* ([http://cs231n.github.io/](http://cs231n.github.io/))) provided
    by him and Professor Fei-Fei Li at Stanford University.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章将使用计算图，以便让你“直观”地理解反向传播。编写代码将进一步加深你的理解，并使你信服这一点。使用计算图来解释反向传播的想法基于 Andrej
    Karpathy 的博客（*黑客的神经网络指南*，([http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/)))
    以及他和斯坦福大学的李飞飞教授提供的深度学习课程（*CS231n:* *用于视觉识别的卷积神经网络*，[http://cs231n.github.io/](http://cs231n.github.io/))。
- en: Computational Graphs
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算图
- en: A computational graph shows the process of calculation. This graph is used as
    a graph of data structure and is represented by multiple nodes and edges (meaning,
    straight lines that connect nodes). In this section, we will solve easy problems
    to familiarize ourselves with computational graphs before advancing step by step
    into more complex backpropagation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图展示了计算过程。这个图作为数据结构图表示，由多个节点和边（即连接节点的直线）组成。在这一节中，我们将解决简单的问题，以便在逐步进入更复杂的反向传播之前，先熟悉计算图的使用。
- en: Using Computational Graphs to Solve Problems
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用计算图解决问题
- en: The problems in this section are simple enough that you can solve them with
    mental arithmetic, but the purpose here is to get familiar with computational
    graphs. Learning to use computational graphs will be helpful for the complicated
    calculations we will cover later, so it's important to first master how to use
    them here.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的问题足够简单，你可以用心算解决它们，但这里的目的是让你熟悉计算图。学习使用计算图将对我们后续要讨论的复杂计算非常有帮助，因此首先掌握如何使用计算图非常重要。
- en: '**Question 1**: Taro bought 2 apples that were 100 yen apiece. Calculate the
    amount of money he paid if a 10% consumption tax was applied.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**：太郎买了两个每个100日元的苹果。如果加上10%的消费税，请计算他支付的金额。'
- en: 'A computational graph shows the process of calculation with nodes and arrows.
    A node is represented by a circle, and an operation is described in it. The intermediate
    result of the calculation above an arrow shows the result of each node that flows
    from left to right. The following diagram shows the computational graph that solves
    Question 1:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图展示了带有节点和箭头的计算过程。节点用圆圈表示，运算在其中描述。箭头上方的中间结果显示从左到右流动的每个节点的计算结果。下图展示了解决问题 1 的计算图：
- en: '![Figure 5.1: Answer to Question 1 using a computational graph'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：使用计算图解答问题 1'
- en: '](img/fig05_1.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig05_1.jpg)'
- en: 'Figure 5.1: Answer to Question 1 using a computational graph'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.1：使用计算图解答问题 1
- en: As shown in the preceding diagram, 100 yen for an apple that flows to the "x2"
    node becomes 200 yen, which is passed to the next node. Then, 200 yen is passed
    to the "× 1.1" node and becomes 220 yen. The result of this computational graph
    shows that the answer is 220 yen.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，100日元的苹果流向“x2”节点后变成了200日元，然后传递到下一个节点。接着，200日元传递到“× 1.1”节点后变为220日元。这个计算图的结果显示答案是220日元。
- en: In the preceding diagram, each circle contains "× 2" or "× 1.1" as one operation.
    You can also place only "x" in a circle to show the operation. In that case, as
    shown in the following diagram, you can place "*2*" and "*1.1*" outside the circles
    as the "Number of apples" and "Consumption tax" variables.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，每个圆圈内包含了“× 2”或“× 1.1”作为一个操作。你也可以只在圆圈中放入“x”来表示操作。在这种情况下，如下图所示，你可以将“*2*”和“*1.1*”放在圆圈外部，作为“苹果数量”和“消费税”变量。
- en: 'The solution to this problem can be observed in the figure below:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的解答可以从下面的图中看到：
- en: '![Figure 5.2: Answer to Question 1 using a computational graph: the "Number
    of apples" and "Consumption tax" variables are placed outside circles'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2：使用计算图解答问题1：“苹果数量”和“消费税”变量被放置在圆圈外部'
- en: '](img/Figure_5.2.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.2.jpg)'
- en: 'Figure 5.2: Answer to Question 1 using a computational graph: the "Number of
    apples" and "Consumption tax" variables are placed outside circles'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.2：使用计算图解答问题1：“苹果数量”和“消费税”变量被放置在圆圈外部
- en: '**Question 2**: Taro bought 2 apples and 3 oranges. An apple was 100 yen, and
    the orange was 150\. A 10% consumption tax was applied. Calculate the amount of
    money he paid.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题2**：太郎买了2个苹果和3个橙子。一个苹果100日元，橙子150日元。计算施加了10%的消费税后，他支付了多少钱？'
- en: 'As in Question 1, we will use a computational graph to solve Question 2\. The
    following diagram shows the computational graph for this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如问题1所示，我们将使用计算图来解答问题2。以下图示显示了这个问题的计算图：
- en: '![Figure 5.3: Answer to Question 2 using a computational graph'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3：使用计算图解答问题2'
- en: '](img/Figure_5.3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.3.jpg)'
- en: 'Figure 5.3: Answer to Question 2 using a computational graph'
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.3：使用计算图解答问题2
- en: In this question, an addition node, "+", was added to sum the amounts of apples
    and oranges. After creating a computational graph, we advance the calculation
    from left to right. The calculation result moves from left to right, just like
    an electric current flows in a circuit, and the calculation ends when the result
    reaches the rightmost side. The preceding diagram shows that the answer is 715
    yen.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，增加了一个加法节点“+”，用于求和苹果和橙子的数量。创建计算图后，我们从左到右推进计算。计算结果从左到右移动，就像电流在电路中流动一样，计算在结果到达最右端时结束。上图显示答案是715日元。
- en: 'To solve a problem using a computational graph, then, you must perform the
    following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算图解决问题时，必须执行以下操作：
- en: Create a computational graph.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个计算图。
- en: Advance the calculation from left to right on the computational graph.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算图上从左到右推进计算。
- en: Step 2 is known as propagating in the forward direction or **forward propagation**.
    In forward propagation, calculation propagates from start to finish in a computational
    graph. If forward propagation exists, we can also consider propagation in the
    backward direction—from right to left. This is called **backward** **propagation**
    and is known as backpropagation. It will play an important role when we calculate
    derivatives later.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2被称为向前传播或**前向传播**。在前向传播中，计算从开始到结束沿着计算图进行传播。如果存在前向传播，我们也可以考虑向后传播——从右到左。这被称为**反向传播**，它将在我们稍后计算导数时发挥重要作用。
- en: Local Calculation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部计算
- en: The main characteristic of a computational graph is that it can obtain the final
    result by propagating "local calculation." The word "local" means "a small range
    related to the node." A local calculation can return the next result (subsequent
    result) from information related to the node, no matter what is happening on the
    whole.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图的主要特点是，它可以通过传播“局部计算”来获得最终结果。“局部”意味着“与节点相关的一个小范围”。局部计算可以从与节点相关的信息中返回下一个结果（后续结果），无论整体上发生了什么。
- en: 'We can break down local calculations using a specific example. For example,
    let''s assume that we bought two apples and many other things at a supermarket.
    To visualize this, you can create a computational graph like so:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个具体的例子来分解局部计算。例如，假设我们在超市买了两个苹果和许多其他东西。为了可视化这一点，你可以创建一个像这样的计算图：
- en: '![Figure 5.4: Example of buying two apples and many other things'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：购买两个苹果和其他许多东西的例子'
- en: '](img/Figure_5.4.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.4.jpg)'
- en: 'Figure 5.4: Example of buying two apples and many other things'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：购买两个苹果和其他许多东西的例子
- en: Let's assume that we bought many things and that the total amount was 4,000
    yen (after a complicated calculation), as shown in the preceding computational
    graph. What is important here is that the calculation in each node is a local
    calculation. To sum the amounts of the apples and the other purchases (4,000 +
    200 -> 4,200), you can add the two figures without thinking about how 4,000 was
    obtained. In other words, what to calculate in each node is only the calculation
    related to the node—in this example, the addition of the two numbers provided.
    We do not need to think about the whole graph.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们购买了许多物品，总金额是 4,000 日元（经过复杂计算得出），如前面的计算图所示。这里重要的是，每个节点中的计算都是局部计算。为了求出苹果和其他购买物品的总金额（4,000
    + 200 -> 4,200），你只需要将这两个数字相加，而不必考虑 4,000 是如何得出的。换句话说，在每个节点中需要计算的仅仅是与该节点相关的计算——在这个例子中，就是将两个数字相加。我们不需要考虑整个图。
- en: Thus, you can focus on local calculation in a computational graph. However complicated
    the whole calculation is, what is done at each step is "local calculation" for
    the target node. By passing the results of simple local calculations, you can
    obtain the result of the complicated calculations that constitute the whole graph.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以在计算图中专注于局部计算。无论整个计算多么复杂，每一步所做的都是“目标节点的局部计算”。通过传递简单局部计算的结果，你可以获得构成整个图的复杂计算结果。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For example, the assembly of a car is complicated, but it is usually conducted
    based on the division of labor on an "assembly line." Each worker (machine) conducts
    simple work. The outcome of the workflows to the next worker, and finally, a car
    is built. A computational graph also divides complicated calculations into "simple
    and local calculations" and passes the calculation result to the next node, just
    like a car is passed down an assembly line. Like the assembly of a car, complicated
    calculations can be divided into simple calculations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，汽车的组装过程是复杂的，但通常是基于“流水线”上的分工进行的。每个工人（或机器）执行简单的工作。工作成果传递给下一个工人，最终完成一辆车的组装。计算图也将复杂的计算分解为“简单的局部计算”，并将计算结果传递给下一个节点，就像汽车沿流水线传递一样。像汽车组装一样，复杂的计算可以分解为简单的计算。
- en: Why Do We Use Computational Graphs?
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么我们使用计算图？
- en: We have solved two problems by using computational graphs and may now consider
    the advantages. One of them is "local calculation," as described earlier. However
    complicated the whole calculation is, local calculation enables you to focus on
    the simple calculations in each node in order to simplify the problem as a whole.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用计算图解决了两个问题，现在可以考虑其优势。其中之一是前面提到的“局部计算”。无论整个计算多么复杂，局部计算使你能够集中精力处理每个节点中的简单计算，从而简化整个问题。
- en: Another advantage is that you can keep all the results of intermediate calculations
    in a computational graph (for example, 200 yen after 2 apples are calculated and
    650 yen before consumption tax is added). However, the largest reason for using
    computational graphs is that you can calculate "derivatives" efficiently by propagating
    in the backward direction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优势是，你可以在计算图中保留所有中间计算的结果（例如，计算出两个苹果的 200 日元和加上消费税前的 650 日元）。然而，使用计算图的最大原因是，你可以通过反向传播高效地计算“导数”。
- en: To describe backward propagation in a computational graph, consider Question
    1 again. In this problem, you calculated the final amount paid regarding two apples
    and the consumption tax. Now say that you need to know how the final amount paid
    will be affected when the price of an apple goes up. This corresponds to obtaining
    the "derivative of the amount paid with respect to the price of an apple." It
    corresponds to obtaining ![61](img/Figure_5.4_a.png) when the price of an apple
    is x and the amount paid is L. The value of this derivative indicates how much
    the amount paid increases when the price of an apple goes up "slightly."
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述计算图中的反向传播，再次考虑问题 1。在这个问题中，你计算了关于两个苹果和消费税的最终支付金额。现在假设你需要知道当苹果价格上涨时，最终支付金额将如何变化。这就相当于求“支付金额对苹果价格的导数”。它对应于当苹果价格为
    x，支付金额为 L 时，得到![61](img/Figure_5.4_a.png)。这个导数的值表示当苹果价格“略微”上涨时，支付金额会增加多少。
- en: 'As we mentioned earlier, you can use backward propagation in a computational
    graph to obtain a value, such as the "derivative of the amount paid with respect
    to the price of an apple." First, we will only look at the result. As shown in
    the following diagram, you can obtain derivatives by using backpropagation in
    a computational graph:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您可以在计算图中使用反向传播来获得一个值，例如“支付金额对苹果价格的导数”。首先，我们只看结果。如下面的图所示，您可以通过在计算图中使用反向传播来获得导数：
- en: '![Figure 5.5: Propagating differential values using backward propagation'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5：使用反向传播传播微分值'
- en: '](img/Figure_5.5.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.5.jpg)'
- en: 'Figure 5.5: Propagating differential values using backward propagation'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.5：使用反向传播传播微分值
- en: As shown in the preceding diagram, backward propagation is shown graphically
    with arrows (thick lines) in the opposite direction to the forward direction.
    Backward propagation passes "local differentials," and the values are placed below
    the arrows. In this example, derivative values are passed from right to left,
    as in 1 -> 1.1 -> 2.2\. The result shows that the value of the "derivative of
    the amount paid with respect to the price of an apple" is 2.2\.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，反向传播通过箭头（粗线）以与前向方向相反的方向进行图示。反向传播传递“局部微分”，并将值放置在箭头下方。在此示例中，导数值从右到左传递，如1
    -> 1.1 -> 2.2。结果显示“支付金额对苹果价格的导数”的值为2.2。
- en: This indicates that the final amount paid increases by 2.2 yen when the price
    of an apple goes up 1 yen. This means that when the price of an apple goes up
    by a small amount, the final amount increases by 2.2 times that of the small value.
    Here, only the derivative with respect to the price of an apple was obtained,
    but you can also obtain the "derivative of the amount paid with respect to consumption
    tax" and the "derivative of the amount paid with respect to the number of apples"
    by using similar steps.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示当苹果的价格上涨1日元时，最终支付的金额增加了2.2日元。这意味着当苹果价格小幅上涨时，最终支付的金额会增加小幅值的2.2倍。在这里，只获得了相对于苹果价格的导数，但您也可以通过类似步骤获得“支付金额相对于消费税的导数”和“支付金额相对于苹果数量的导数”。
- en: During these steps, you can share the intermediate results of the derivatives
    (the derivatives that are passed halfway) so that you can calculate multiple derivatives
    efficiently. Thus, the advantage of a computational graph is that forward, and
    backward propagations enable you to obtain the derivative value of each variable
    efficiently.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤中，您可以共享导数的中间结果（传递到一半的导数），以便更高效地计算多个导数。因此，计算图的优势在于前向传播和反向传播使您能够高效地获得每个变量的导数值。
- en: Chain Rule
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链式法则
- en: Forward propagation in a computational graph propagates the calculation result
    in the forward direction from left to right. These calculations seem natural because
    they are usually conducted. On the other hand, in backward propagation, a "local
    derivative" is propagated in the backward direction from right to left. The principle
    that propagates the "local derivative" is based on the **chain rule**. Let's look
    at the chain rule and clarify how it corresponds to backward propagation in a
    computational graph.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图中的前向传播将计算结果从左到右沿前向方向传播。这些计算看起来很自然，因为它们通常是这样进行的。另一方面，在反向传播中，“局部导数”沿反向方向从右到左传播。传播“局部导数”的原理基于**链式法则**。让我们来看一下链式法则，明确它如何与计算图中的反向传播相对应。
- en: Backward Propagation in a Computational Graph
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算图中的反向传播
- en: 'We will now look at an example of backward propagation using a computational
    graph. Let''s assume that a calculation, *y = f (x)*, exists. The following diagram
    shows the backward propagation of this calculation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一个使用计算图进行反向传播的示例。假设存在一个计算关系 *y = f(x)*，以下图展示了该计算的反向传播：
- en: '![Figure 5.6: Backward propagation in a computational graph – the local derivative'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6：计算图中的反向传播——局部导数'
- en: is multiplied in the backward direction
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向方向上相乘
- en: '](img/Figure5.6.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure5.6.jpg)'
- en: 'Figure 5.6: Backward propagation in a computational graph – the local derivative
    is multiplied in the backward direction'
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.6：计算图中的反向传播——局部导数在反向方向上相乘
- en: As shown in the preceding diagram, backward propagation multiplies the signal
    E by the local derivative of the node, ![60](img/Figure_5.6_a.png), and propagates
    it to the next node. The local derivative here means obtaining the derivative
    of the calculation, y = f (x), in forward propagation and indicates obtaining
    the derivative, `y`, with respect to x ![59](img/Figure_5.6_b.png); for example,
    y = f (x) = x2, ![58](img/Figure_5.6_c.png). The local derivative is multiplied
    by the value propagated from the upper stream (E, in this example) and passed
    to the previous node.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，反向传播将信号E乘以节点的局部导数，![60](img/Figure_5.6_a.png)，并将其传播到下一个节点。这里的局部导数指的是在前向传播中获得计算的导数，y
    = f(x)，并表示获得y相对于x的导数，![59](img/Figure_5.6_b.png)；例如，y = f(x) = x²，![58](img/Figure_5.6_c.png)。局部导数与从上游传播下来的值（此例中的E）相乘，并传递到前一个节点。
- en: This is the procedure of backward propagation. It can obtain the target derivative
    values efficiently. The reason why this is possible can be explained by the principle
    of the chain rule, defined in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是反向传播的过程。它可以高效地获得目标导数值。之所以能够实现这一点，可以通过下一节中定义的链式法则原理来解释。
- en: What Is the Chain Rule?
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是链式法则？
- en: 'Before explaining the chain rule, we need to talk about `z = (x + y)`2 consists
    of two equations, as shown in equation (5.1):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释链式法则之前，我们需要先讨论`z = (x + y)`²由两个方程组成，如方程(5.1)所示：
- en: '| ![57](img/Figure_5.6_d.png) | (5.1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![57](img/Figure_5.6_d.png) | (5.1) |'
- en: The chain rule is the characteristic related to the derivative of a composite
    function and is defined as follows.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则是与复合函数的导数相关的特性，定义如下。
- en: When a function is expressed by a composite function, the derivative of the
    composite function can be expressed by the product of the derivative of each function
    that constitutes the composite function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个函数由复合函数表示时，复合函数的导数可以表示为构成复合函数的每个函数的导数的乘积。
- en: 'This is called the principle of the chain rule. Although it may seem difficult,
    it is actually quite simple. In the example given in equation (5.1), ![2](img/Figure_5.6_g.png)
    (a derivative of z with respect to x) is the product of ![1](img/Figure_5.6_f.png)
    (a derivative of *z* with respect to *t*) and ![3](img/Figure_5.6_h.png) (a derivative
    of *t* with respect to x). You can express this with the following equation (5.2):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是链式法则的原理。虽然看起来可能很难，但其实它相当简单。在方程(5.1)给出的例子中，![2](img/Figure_5.6_g.png)（z对x的导数）是![1](img/Figure_5.6_f.png)（*z*对*t*的导数）和![3](img/Figure_5.6_h.png)（*t*对x的导数）的乘积。你可以用以下方程(5.2)表示这个：
- en: '| ![4](img/Figure_5.6_j.png) | (5.2) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![4](img/Figure_5.6_j.png) | (5.2) |'
- en: 'You can remember equation (5.2) easily because ∂t''s cancel each other out,
    as shown here: ![5](img/Figure_5.6_k.png)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松记住方程(5.2)，因为∂t相互抵消，如下所示：![5](img/Figure_5.6_k.png)
- en: 'Now, let''s use the chain rule to obtain the derivative of equation (5.2),
    ![6](img/Figure_5.6_L.png). First, obtain the local differential (partial differential)
    of equation (5.1):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用链式法则来获得方程(5.2)的导数，![6](img/Figure_5.6_L.png)。首先，获得方程(5.1)的局部微分（偏微分）：
- en: '| ![7](img/Figure_5.6_m.png) | (5.3) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![7](img/Figure_5.6_m.png) | (5.3) |'
- en: 'As shown in equation (5.3), ![9](img/Figure_5.6_o.png) is 2t and ![10](img/Figure_5.6_p.png)
    is 1\. This result is analytically obtained from the differentiation formula.
    The final result, ![11](img/Figure_5.6_q.png), can be calculated by the product
    of the derivatives obtained in equation (5.3):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程(5.3)所示，![9](img/Figure_5.6_o.png)是2t，![10](img/Figure_5.6_p.png)是1\。这个结果是通过微分公式获得的。最终结果，![11](img/Figure_5.6_q.png)，可以通过方程(5.3)中得到的导数乘积计算得到：
- en: '| ![12](img/Figure_5.6_r.png) | (5.4) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ![12](img/Figure_5.6_r.png) | (5.4) |'
- en: The Chain Rule and Computational Graphs
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 链式法则与计算图
- en: 'Now, let''s use a computational graph to express the calculation of the chain
    rule in equation (5.4). When we represent a square with a node "**2", we can write
    a graph for it, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用计算图来表达方程(5.4)中链式法则的计算。当我们用节点"**2"表示平方时，我们可以为其绘制如下图：
- en: '![Figure 5.7: Computational graph of equation (5.4) – local derivatives are
    multiplied'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：方程(5.4)的计算图 – 局部导数相乘'
- en: and passed in the backward direction
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 并在反向传播中传递
- en: '](img/Figure_5.7.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.7.jpg)'
- en: 'Figure 5.7: Computational graph of equation (5.4) – local derivatives are multiplied
    and passed in the backward direction'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：方程(5.4)的计算图 – 局部导数相乘并在反向传播中传递
- en: As shown in the preceding diagram, backward propagation in a computational graph
    propagates signals from right to left. Backward propagation multiplies the signal
    provided to a node by the local derivative (partial derivative) of the node and
    passes it to the next node. For example, the input to "**2" in backward propagation
    is ![55](img/Figure_5.7_a.png). It is multiplied by the local derivative, ![54](img/Figure_5.7_b.png)
    (in forward propagation, the input is t and the output is z, so the (local) derivative
    at this node is ![53](img/Figure_5.7_c.png)) and then multiplied and passed to
    the next node. In the preceding diagram, the first signal in backward propagation
    ![51](img/Figure_5.7_g.png) did not appear in the previous equation. It was omitted
    there because ![51](img/Figure_5.7_g1.png) = 1.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，计算图中的反向传播将信号从右向左传播。反向传播将提供给节点的信号乘以节点的局部导数（偏导数），然后传递给下一个节点。例如，在反向传播中，“**2”中的输入是
    ![55](img/Figure_5.7_a.png)。它乘以本地导数 ![54](img/Figure_5.7_b.png)（在正向传播中，输入是 t，输出是
    z，因此此节点的（局部）导数是 ![53](img/Figure_5.7_c.png)），然后乘以并传递给下一个节点。在上图中，反向传播中的第一个信号 ![51](img/Figure_5.7_g.png)
    在前面的方程中未出现。这是因为 ![51](img/Figure_5.7_g1.png) = 1。
- en: What we should note from the preceding diagram is the result of backward propagation
    at the leftmost position. It corresponds to the "derivative of z with respect
    to x" because ![5k](img/Figure_5.7_e.png) due to the chain rule. What backward
    propagation performs is based on the principle of the chain rule.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图我们应该注意的是左侧位置的反向传播结果。这对应于链式法则导致的 *"关于 x 的 z 的导数"。反向传播执行的操作基于链式法则的原理。
- en: 'When you assign the result of equation (5.3), as shown in the preceding diagram,
    the result is as follows. Thus, ![50](img/Figure_5.7_h.png) is 2(x + y):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当您指定方程 (5.3) 的结果，如上图所示，结果如下。因此，![50](img/Figure_5.7_h.png) 是 2(x + y)：
- en: '![Figure 5.8: Based on the result of backward propagation in the computational
    graph,  is 2(x + y)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8: 基于计算图中反向传播结果，是 2(x + y)'
- en: '](img/Figure_5.8.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.8.jpg)'
- en: 'Figure 5.8: Based on the result of backward propagation in the computational
    graph, is 2(x + y)'
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.8: 基于计算图中反向传播结果，是 2(x + y)'
- en: Backward Propagation
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: The previous section described how backward propagation in a computational graph
    is based on the chain rule. We will now cover how backward propagation works by
    taking operations, such as "+" and "x", as examples.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节描述了计算图中的反向传播是基于链式法则的。我们现在将介绍通过以“+”和“x”等操作为例的反向传播工作方式。
- en: Backward Propagation in an Addition Node
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加法节点中的反向传播
- en: 'First, let''s consider backward propagation in an additional node. Here, we
    will look at backward propagation for the equation *z = x + y*. We can obtain
    the derivatives of *z = x + y* (analytically) as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑一个加法节点中的反向传播。在这里，我们将看一下方程 *z = x + y* 的反向传播。我们可以按如下方式（解析地）获得 *z = x
    + y* 的导数：
- en: '| ![13](img/Figure_5.8_b.png) | (5.5) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ![13](img/Figure_5.8_b.png) | (5.5) |'
- en: As equation (5.5) shows, both ![15](img/Figure_5.8_d.png) and ![16](img/Figure_5.8_f.png)
    are 1\. Therefore, we can represent them in a computational graph, as shown in
    the following diagram. In backward propagation, the derivative from the upper
    stream—![17](img/Figure_5.8_g.png), in this example—is multiplied by 1 and passed
    downstream. In short, backward propagation in an addition node multiplies 1, so
    it only passes the input value to the next node.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如方程 (5.5) 所示，![15](img/Figure_5.8_d.png) 和 ![16](img/Figure_5.8_f.png) 都是 1。因此，我们可以在计算图中表示它们，如下图所示。在反向传播中，从上游传来的导数
    —— 例如本例中的 ![17](img/Figure_5.8_g.png) —— 被乘以 1 并传递到下游。简言之，在加法节点的反向传播中，乘以 1，因此它只传递输入值到下一个节点。
- en: 'In this example, the differential value from the upper stream is expressed
    as ![18](img/Figure_5.8_g.png). This is because we assume a large computational
    graph that finally outputs L, as shown in *Figure 5.10*. The calculation, *z =
    x + y*, exists somewhere in the large computational graph, and the value of ![19](img/Figure_5.8_g.png)
    is passed from the upper stream. The values of ![20](img/Figure_5.8_i.png) and
    ![21](img/Figure_5.8_j.png) are propagated downstream:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，来自上游的微分值被表示为 ![18](img/Figure_5.8_g.png)。这是因为我们假设了一个大型计算图，最终输出 L，如*图
    5.10*所示。计算 *z = x + y* 存在于大型计算图的某处，并且 ![19](img/Figure_5.8_g.png) 的值从上游传递下来。![20](img/Figure_5.8_i.png)
    和 ![21](img/Figure_5.8_j.png) 的值向下传播：
- en: '![Figure 5.9: Backward propagation in an addition node – forward propagation
    on the left and backward propagation on the right.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：加法节点中的反向传播——左侧是正向传播，右侧是反向传播。'
- en: '](img/Figure_5.9.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.9.jpg)'
- en: 'Figure 5.9: Backward propagation in an addition node – forward propagation
    on the left and backward propagation on the right.'
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：加法节点中的反向传播——左侧是正向传播，右侧是反向传播。
- en: As shown on the right, backward propagation in an addition node passes a value
    from the upper stream to the lower stream without changing it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如右图所示，加法节点中的反向传播将一个值从上游传递到下游，而不做任何变化。
- en: '![Figure 5.10: This addition node exists somewhere in the final output calculation.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10：这个加法节点存在于最终输出计算的某个位置。'
- en: '](img/Figure_5.10.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.10.jpg)'
- en: 'Figure 5.10: This addition node exists somewhere in the final output calculation.'
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.10：这个加法节点存在于最终输出计算的某个位置。
- en: In backward propagation, starting from the rightmost output, local derivatives
    are propagated from node to node in the backward direction
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，从最右侧的输出开始，局部的导数从节点到节点以反向方向传播
- en: 'Now, let''s look at an example of backward propagation. For example, say that
    a calculation, "10 + 5 = 15", exists and that a value of 1.3 flows from the upper
    stream in backward propagation. The following diagram shows this in terms of a
    computational graph:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个反向传播的例子。假设存在一个计算 "10 + 5 = 15"，并且一个值为 1.3 从上游流向反向传播。以下图示表示了这一过程，基于计算图：
- en: '![Figure 5.11: Example of backward propagation in an addition node'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：加法节点中反向传播的例子'
- en: '](img/fig05_11.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig05_11.jpg)'
- en: 'Figure 5.11: Example of backward propagation in an addition node'
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.11：加法节点中反向传播的例子
- en: Because backward propagation in an addition node only outputs the input signal
    to the next node, it passes 1.3 to the next node.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因为加法节点中的反向传播只将输入信号输出到下一个节点，它将 1.3 传递给下一个节点。
- en: Backward Propagation in a Multiplication Node
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乘法节点中的反向传播
- en: 'Let''s take a look at backward propagation in a multiplication node by taking
    an equation, *z = xy*, as an example. The differential of this equation is expressed
    by the following equation (5.6):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以方程 *z = xy* 为例来看看乘法节点中的反向传播。该方程的微分由以下方程（5.6）表示：
- en: '| ![22](img/Figure_5.11_a.png) | (5.6) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ![22](img/Figure_5.11_a.png) | (5.6) |'
- en: Based on the preceding equation (5.6), you can write a computational graph as
    follows.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的方程（5.6），你可以写出如下的计算图。
- en: For the backward propagation of multiplication, the upstream value multiplied
    by the "reversed value" of the input signal for forward propagation is passed
    downstream. A reversed value means that if the signal is x in forward propagation,
    the value to multiply is y in backward propagation; and that if the signal is
    y in forward propagation, the value to multiply is x in backward propagation,
    as shown in the following diagram.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于乘法的反向传播，上游的值与正向传播输入信号的“反向值”相乘后传递给下游。反向值意味着，如果正向传播中的信号是 x，那么反向传播时要乘的值是 y；如果正向传播中的信号是
    y，那么反向传播时要乘的值是 x，如下图所示。
- en: Let's look at an example. Assume that a calculation 10 x 5 = 50 exists and that
    the value of 1.3 flows from the upper stream in backward propagation. *Figure
    5.13* shows this in the form of a computational graph.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设存在一个计算 10 x 5 = 50，并且一个值为 1.3 从上游流向反向传播。*图 5.13* 以计算图的形式显示了这一过程。
- en: 'In the backward propagation of multiplication, the reversed input signals are
    multiplied, so 1.3 x 5 = 6.5 and 1.3 x 10 = 13 are obtained. In the backward propagation
    of addition, the upstream value was only passed downstream. Therefore, the value
    of the input signal in forward propagation is not required. On the other hand,
    for the backward propagation of multiplication, the value of the input signal
    in forward propagation is required. Therefore, to implement a multiplication node,
    the input signal of forward propagation is retained:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在乘法的反向传播中，反向的输入信号相乘，因此得到 1.3 x 5 = 6.5 和 1.3 x 10 = 13。而在加法的反向传播中，上游的值仅被传递给下游。因此，正向传播中的输入信号不需要。在另一方面，对于乘法的反向传播，正向传播中的输入信号是必须的。因此，为了实现一个乘法节点，正向传播的输入信号需要保留：
- en: '![Figure 5.12: Backward propagation in a multiplication node – forward propagation
    on the left and backward propagation on the right'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12：乘法节点中的反向传播——左侧是正向传播，右侧是反向传播'
- en: '](img/fig05_12.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig05_12.jpg)'
- en: 'Figure 5.12: Backward propagation in a multiplication node – forward propagation
    on the left and backward propagation on the right'
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.12：乘法节点中的反向传播——左侧是正向传播，右侧是反向传播
- en: '![Figure 5.13: Example of backward propagation in a multiplication node'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13：乘法节点中的反向传播示例](img/fig05_13.jpg)'
- en: '](img/fig05_13.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.14.jpg)'
- en: 'Figure 5.13: Example of backward propagation in a multiplication node'
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.13：乘法节点中的反向传播示例
- en: Apples Example
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 苹果例子
- en: 'Let''s think about the example of buying apples—two apples and consumption
    tax—from the beginning of this chapter again. The problem to solve here is how
    each of the three variables (the price of an apple, the number of apples, and
    consumption tax) affect the final amount paid. This corresponds to obtaining the
    "derivative of the amount paid with respect to the price of an apple," the "derivative
    of the amount paid with respect to the number of apples," and the "derivative
    of the amount paid with respect to consumption tax." We can solve this by using
    backward propagation in a computational graph, as shown in the following diagram:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再从本章开始时考虑购买苹果的例子——两个苹果和消费税。这里要解决的问题是，三个变量（苹果的价格、苹果的数量和消费税）如何影响最终支付的金额。这对应于计算“苹果价格对支付金额的导数”，“苹果数量对支付金额的导数”和“消费税对支付金额的导数”。我们可以通过在计算图中使用反向传播来解决这个问题，如下图所示：
- en: '![Figure 5.14: Example of backward propagation for purchasing apples'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14：购买苹果的反向传播示例](img/Figure_5.14.jpg)'
- en: '](img/Figure_5.14.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.14.jpg)'
- en: 'Figure 5.14: Example of backward propagation for purchasing apples'
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.14：购买苹果的反向传播示例
- en: As mentioned previously, input signals are reversed and passed downstream in
    the backward propagation of a multiplication node. According to the result shown
    in the preceding diagram, the differential of the price of an apple is 2.2, there
    are 110 apples, and the consumption tax is 200\. They indicate that when consumption
    tax and the price of an apple increase by the same quantity, the consumption tax
    affects the final amount paid in the size of 200 and that the price of an apple
    affects it in the size of 2.2\. However, this result is brought about because
    the consumption tax and the price of an apple in this example are different in
    terms of units (1 for the consumption tax is 100%, while 1 for the price of an
    apple is 1 yen).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，输入信号在乘法节点的反向传播中被反向并传递到下游。根据前面图示的结果，苹果价格的微分为 2.2，苹果数量为 110，消费税为 200。它们表示当消费税和苹果价格增加相同的量时，消费税对最终支付金额的影响为
    200，而苹果价格的影响为 2.2。然而，这一结果是因为在这个例子中，消费税和苹果价格的单位不同（消费税的单位是 100%，而苹果价格的单位是 1 日元）。
- en: 'Lastly, let''s solve the backward propagation of "buying apples and oranges"
    as an exercise. Please obtain the derivatives of individual variables and put
    the numbers in the squares provided in the following diagram (you can find the
    answer in the next section):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过“购买苹果和橙子”这个练习来解决反向传播问题。请计算各个变量的导数，并将数字填入下图中提供的方框内（你可以在下一节找到答案）：
- en: '![Figure 5.15: Example of backward propagation for purchasing apples and oranges
    – complete this calculation by putting figures in the squares'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15：购买苹果和橙子的反向传播示例——通过将数字填入方框完成此计算](img/Figure_5.15.jpg)'
- en: '](img/Figure_5.15.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.15.jpg)'
- en: 'Figure 5.15: Example of backward propagation for purchasing apples and oranges
    – complete this calculation by putting figures in the squares'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.15：购买苹果和橙子的反向传播示例——通过将数字填入方框完成此计算
- en: Implementing a Simple Layer
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现简单层
- en: In this section, we will implement the apple example we've described in Python
    using the multiplication node in a computational graph as the **multiplication
    layer** (**MulLayer**) and the addition node as the **addition layer** (**AddLayer**).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Python 实现我们所描述的苹果例子，使用计算图中的乘法节点作为**乘法层**（**MulLayer**），而将加法节点作为**加法层**（**AddLayer**）。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In the next section, we will implement the "layers" that constitute a neural
    network in one class. The "layer" here is a functional unit in a neural network—the
    Sigmoid layer for a sigmoid function, and the Affine layer for matrix multiplication.
    Therefore, we will also implement multiplication and addition nodes here on a
    "layer" basis.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现一个类，其中包含构成神经网络的“层”。这里的“层”是神经网络中的一个功能单元——Sigmoid层用于sigmoid函数，Affine层用于矩阵乘法。因此，我们还将在“层”级别实现乘法和加法节点。
- en: Implementing a Multiplication Layer
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个乘法层
- en: 'We will implement a layer so that it has two common methods (interfaces): `forward()`
    and `backward()`, which correspond to forward propagation and backward propagation,
    respectively. Now, you can implement a multiplication layer as a class called
    `MulLayer`, as follows (the source code is located at `ch05/layer_naive.py`):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个层，使其具有两个常用的方法（接口）：`forward()`和`backward()`，分别对应正向传播和反向传播。现在，你可以将乘法层实现为一个名为`MulLayer`的类，如下所示（源代码位于`ch05/layer_naive.py`）：
- en: '[PRE0]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`__init__()` initializes the instance variables, `x,` and `y`, which are used
    to retain the input values in forward propagation. `forward()` takes two variables,
    `x` and `y`, and multiplies and outputs their product. On the other hand, `backward()`
    multiplies the derivative from the upper stream (`dout`) by the "reversed value"
    of forward propagation and passes the result downstream.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__()`初始化实例变量`x`和`y`，用于保留正向传播中的输入值。`forward()`接收两个变量`x`和`y`，并将它们相乘并输出它们的乘积。另一方面，`backward()`将来自上游的导数`dout`与正向传播的“反向值”相乘，并将结果传递给下游。'
- en: 'Now, use `MulLayer` to implement the "purchase of apples"—two apples and consumption
    tax. In the previous section, we used forward and backward propagations in a computational
    graph for this calculation, as shown in the following diagram:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`MulLayer`实现“购买苹果”——两个苹果和消费税。在前面的部分，我们使用计算图中的正向传播和反向传播来进行这个计算，如下图所示：
- en: '![Figure 5.16: Purchasing two apples'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16: 购买两个苹果](img/Figure_5.16.jpg)'
- en: '](img/Figure_5.16.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.16.jpg)'
- en: 'Figure 5.16: Purchasing two apples'
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.16: 购买两个苹果'
- en: 'By using the multiplication layer, we can implement forward propagation for
    this as follows (the source code is located at `ch05/buy_apple.py`):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用乘法层，我们可以如下实现该正向传播（源代码位于`ch05/buy_apple.py`）：
- en: '[PRE1]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can use `backward()` to obtain the differential of each variable.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`backward()`来获取每个变量的导数。
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, the order of calling `backward()` is the opposite of that of calling `forward()`.
    Note that the argument of `backward()` is the "derivative with respect to the
    output variable in forward propagation." For example, the multiplication layer,
    `mul_apple_layer`, returns `apple_price` in forward propagation, while it takes
    the derivative value of `apple_price (dapple_price)` as an argument in backward
    propagation. The execution result of this program matches the result shown in
    the preceding diagram.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，调用`backward()`的顺序与调用`forward()`的顺序相反。请注意，`backward()`的参数是“正向传播中输出变量的导数”。例如，乘法层`mul_apple_layer`在正向传播中返回`apple_price`，而在反向传播中，它将`apple_price
    (dapple_price)`的导数值作为参数传递。该程序的执行结果与前面的图示结果相符。
- en: Implementing an Addition Layer
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个加法层
- en: 'Now, we will implement an addition layer, which is an addition node, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现一个加法层，它是一个加法节点，如下所示：
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: An addition layer requires no initialization, so `__init__ ()` does nothing
    (the pass statement is "does nothing"). `forward()` in the addition layer takes
    two arguments, `x` and `y`, and adds them for output. `backward()` passes the
    differential (`dout`) from the upper stream to the lower stream.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 加法层无需初始化，因此`__init__()`什么都不做（`pass`语句表示“什么都不做”）。加法层中的`forward()`接收两个参数`x`和`y`，并将它们相加输出。`backward()`将上游的导数`dout`传递给下游。
- en: Now, let's use the addition and multiplication layers to implement the purchase
    of two apples and three oranges, as shown in the following diagram.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用加法层和乘法层来实现购买两个苹果和三个橙子的操作，如下图所示。
- en: '![Figure 5.17: Purchasing two apples and three oranges'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.17: 购买两个苹果和三个橙子](img/Figure_5.17.jpg)'
- en: '](img/Figure_5.17.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.17.jpg)'
- en: 'Figure 5.17: Purchasing two apples and three oranges'
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.17: 购买两个苹果和三个橙子'
- en: 'You can implement this computational graph in Python as follows (the source
    code is located at `ch05/buy_apple_orange.py`):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以如下方式在Python中实现这个计算图（源代码位于`ch05/buy_apple_orange.py`）：
- en: '[PRE4]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This implementation is a little long, but each statement is simple. The required
    layers are created and the forward propagation method, `forward()`, is called
    in an appropriate order. Then, the backward propagation method, `backward()`,
    is called in the opposite order to forward propagation to obtain the desired derivatives.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现稍微长一些，但每一条语句都很简单。所需的层被创建，并按适当的顺序调用前向传播方法`forward()`。然后，反向传播方法`backward()`会以与前向传播相反的顺序调用，以获得所需的导数。
- en: In this way, implementing layers (here, the addition and multiplication layers)
    in a computational graph are easy, and you can use them to obtain complicated
    derivatives. Next, we will implement the layers that are used in a neural network.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，在计算图中实现层（如加法层和乘法层）变得容易，你可以利用它们获得复杂的导数。接下来，我们将实现神经网络中使用的层。
- en: Implementing the Activation Function Layer
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现激活函数层
- en: Now, we will apply the idea of a computational graph to a neural network. Here,
    we will implement the "layers" that constitute a neural network in one class using
    the ReLU and Sigmoid layers, which are activation functions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算图的思想应用于神经网络。在这里，我们将使用 ReLU 和 Sigmoid 层（激活函数）来实现构成神经网络的“层”。
- en: ReLU Layer
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReLU 层
- en: 'A **Rectified Linear Unit** (**ReLU**) is used as an activation function and
    is expressed by the following equation (5.7):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**Rectified Linear Unit**（**ReLU**）作为激活函数，其表达式为以下方程（5.7）：'
- en: '| ![49](img/Figure_5.17_a.png) | (5.7) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| ![49](img/Figure_5.17_a.png) | (5.7) |'
- en: 'From the preceding equation (5.7), you can obtain the derivative of y with
    respect to x with equation (5.8):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程式（5.7）中，你可以通过方程式（5.8）得到 y 关于 x 的导数：
- en: '| ![48](img/Figure_5.17_b.png) | (5.8) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ![48](img/Figure_5.17_b.png) | (5.8) |'
- en: 'As equation (5.8) shows, if the input in forward propagation, x, is larger
    than 0, backward propagation passes the upstream value downstream without changing
    it. Meanwhile, if x is 0 or smaller in forward propagation, the signal stops there
    in backward propagation. You can express this in a computational graph, as shown
    in the following diagram:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程式（5.8）所示，如果前向传播中的输入 x 大于 0，反向传播会将上游的值传递给下游而不做任何改变。同时，如果前向传播中的 x 为 0 或更小，信号将在反向传播中停在那里。你可以在计算图中表示这一点，如下图所示：
- en: '![Figure 5.18: Computational graph of the ReLU layer'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.18：ReLU 层的计算图'
- en: '](img/Figure_5.18.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.18.jpg)'
- en: 'Figure 5.18: Computational graph of the ReLU layer'
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：ReLU 层的计算图
- en: 'Next, let''s implement the ReLU layer. When implementing a layer in a neural
    network, we assume that `forward()` and `backward()` take NumPy arrays as arguments.
    The implementation of the ReLU layer is located at `common/layers.py`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来实现 ReLU 层。在实现神经网络中的一个层时，我们假设 `forward()` 和 `backward()` 以 NumPy 数组作为参数。ReLU
    层的实现位于 `common/layers.py`：
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `Relu` class has an instance variable, `mask`. The `mask` variable is a
    NumPy array that consists of `True`/`False` values. If an element of the input,
    `x`, in forward propagation is `0` or smaller, the mask''s corresponding element
    is `True`. Otherwise (if it is larger than 0), the element is `False`. For example,
    the `mask` variable contains a NumPy array that consists of `True` and `False`,
    as shown in the following code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`Relu` 类有一个实例变量 `mask`。`mask` 变量是一个由 `True`/`False` 值组成的 NumPy 数组。如果前向传播中输入
    `x` 的某个元素为 0 或更小，那么 mask 中对应的元素为 `True`。否则（如果大于 0），该元素为 `False`。例如，`mask` 变量包含一个由
    `True` 和 `False` 组成的 NumPy 数组，如下面的代码所示：'
- en: '[PRE6]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As shown in the preceding diagram, the value of backward propagation is 0 when
    the input value in forward propagation is 0 or smaller. Therefore, in backward
    propagation, the mask variable stored in forward propagation is used to set `dout`
    from the upper stream. If an element of the mask is `True`, the corresponding
    element in `dout` is set to 0\.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，当前向传播中的输入值为 0 或更小时，反向传播的值为 0。因此，在反向传播中，前向传播中存储的 mask 变量被用来设置来自上游的 `dout`。如果
    mask 的某个元素为 `True`，则 `dout` 中对应的元素被设置为 0\。
- en: Note
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The ReLU layer works as a "switch" in a circuit. In forward propagation, it
    turns on the switch if an electric current flows through it, and turns off the
    switch if an electric current does not flow through it. In backward propagation,
    the electric current keeps on flowing if the switch is ON and does not flow any
    longer if the switch is OFF.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 层在电路中充当“开关”的角色。在前向传播中，当电流流过它时，它会打开开关；当电流不流过时，它会关闭开关。在反向传播中，如果开关是打开的，电流会继续流动；如果开关是关闭的，电流则不再流动。
- en: Sigmoid Layer
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid 层
- en: 'Next, let''s implement a sigmoid function. This is expressed by equation (5.9):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个 sigmoid 函数。这由方程（5.9）表示：
- en: '| ![24](img/Figure_5.18_a.png) | (5.9) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ![24](img/Figure_5.18_a.png) | （5.9） |'
- en: 'The following diagram shows the computational graph that represents equation
    (5.9):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了代表方程（5.9）的计算图：
- en: '![Figure 5.19: Computational graph of the Sigmoid layer (forward propagation
    only)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.19：Sigmoid 层的计算图（仅正向传播）'
- en: '](img/Figure_5.19.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.19.jpg)'
- en: 'Figure 5.19: Computational graph of the Sigmoid layer (forward propagation
    only)'
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.19：Sigmoid 层的计算图（仅正向传播）
- en: Here, the *exp* and */* nodes appear in addition to the *X* and *+* nodes. The
    *exp* node calculates *y = exp(x)*, while the */* node calculates ![47](img/Figure_5.19_a.png).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*exp* 和 */* 节点除了 *X* 和 *+* 节点之外还出现了。*exp* 节点计算 *y = exp(x)*，而 */* 节点计算 ![47](img/Figure_5.19_a.png)。
- en: The calculation of equation (5.9) consists of the propagation of local calculations.
    Next, let's consider the backward propagation shown in the preceding computational
    graph, looking at the flow of backward propagation step by step to summarize what
    we have described so far.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（5.9）的计算涉及局部计算的传播。接下来，让我们考虑前面计算图中展示的反向传播，逐步查看反向传播的流程，总结我们到目前为止描述的内容。
- en: '**Step 1:**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一步：**'
- en: 'The */* node represents ![46](img/Figure_5.19_a.png). Its derivative is analytically
    expressed by the following equation:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*/* 节点表示 ![46](img/Figure_5.19_a.png)。其导数通过以下方程解析表示：'
- en: '| `![47](img/Figure_5.19_b.png)` | (5.10) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| `![47](img/Figure_5.19_b.png)` | （5.10） |'
- en: 'Based on equation (5.10), in backward propagation, the node multiplies the
    upstream value by *−y*2 (the additive inverse of the square of the output in forward
    propagation) and passes the value to the lower stream. The following computational
    graph shows this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方程（5.10），在反向传播中，节点将上游值乘以 *−y*2（正向传播中输出的平方的加法逆），并将值传递给下游。以下计算图显示了这一过程：
- en: '![Figure 5.20: Computational graph of the Sigmoid layer (with the additive
    inverse of the square)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.20：Sigmoid 层的计算图（带有平方的加法逆）'
- en: '](img/Figure_5.20.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.20.jpg)'
- en: 'Figure 5.20: Computational graph of the Sigmoid layer (with the additive inverse
    of the square)'
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.20：Sigmoid 层的计算图（带有平方的加法逆）
- en: '**Step 2:**'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二步：**'
- en: 'The *+* node only passes the upstream value to the lower stream. The following
    computational graph shows this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*+* 节点仅将上游值传递给下游。以下计算图显示了这一点：'
- en: '![Figure 5.21: Computational graph of the Sigmoid layer (with passing upstream
    value)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.21：Sigmoid 层的计算图（传递上游值）'
- en: '](img/Figure_5.21.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.21.jpg)'
- en: 'Figure 5.21: Computational graph of the Sigmoid layer (with passing upstream
    value)'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.21：Sigmoid 层的计算图（传递上游值）
- en: '**Step 3:**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三步：**'
- en: 'The "exp" node represents *y = exp(x)*, and its derivative is expressed by
    the following equation:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*"exp"* 节点表示 *y = exp(x)*，其导数由以下方程表示：'
- en: '| ![48](img/Figure_5.21_a.png) | (5.11) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| ![48](img/Figure_5.21_a.png) | （5.11） |'
- en: 'In the following computational graph, the node multiplies the upstream value
    by the output in forward propagation (*exp(-x)*, in this example) and passes the
    value to the lower stream:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的计算图中，节点将上游值乘以正向传播中的输出（在本例中为 *exp(-x)*），并将值传递给下游：
- en: '![Figure 5.22: Computational graph of the Sigmoid layer'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.22：Sigmoid 层的计算图'
- en: '](img/Figure_5.22.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.22.jpg)'
- en: 'Figure 5.22: Computational graph of the Sigmoid layer'
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.22：Sigmoid 层的计算图
- en: '**Step 4:**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**第四步：**'
- en: 'The *X* node reverses the values in forward propagation for multiplication.
    Therefore, *−1* is multiplied here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向传播中，*X* 节点反转值以便进行乘法运算。因此，在这里乘以 *−1*：
- en: '![Figure 5.23: Computational graph of the Sigmoid layer (reversed values)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.23：Sigmoid 层的计算图（反转值）'
- en: '](img/Figure_5.23.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.23.jpg)'
- en: 'Figure 5.23: Computational graph of the Sigmoid layer (reversed values)'
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.23：Sigmoid 层的计算图（反转值）
- en: 'Thus, we can show the backward propagation of the Sigmoid layer in the computational
    graph shown in the preceding diagram. According to the result of the preceding
    computational graph, the output of backward propagation is ![45](img/Figure_5.23_a.png)
    and it is propagated to the downstream nodes. Note here that ![44](img/Figure_5.23_b.png)
    can be calculated from the input, *x*, and output, *y*, of forward propagation.
    Therefore, we can draw the computational graph shown in the preceding diagram
    as a grouped "sigmoid" node, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以在前面图示中的计算图中展示Sigmoid层的反向传播。根据前面计算图的结果，反向传播的输出是![45](img/Figure_5.23_a.png)，并传播到下游节点。需要注意的是，![44](img/Figure_5.23_b.png)可以通过前向传播的输入*x*和输出*y*计算得出。因此，我们可以将前面图示中的计算图绘制为一个分组的“sigmoid”节点，如下所示：
- en: '![Figure 5.24: Computational graph of the Sigmoid layer (simple version)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.24：Sigmoid层的计算图（简化版）'
- en: '](img/Figure_5.24.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.24.jpg)'
- en: 'Figure 5.24: Computational graph of the Sigmoid layer (simple version)'
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.24：Sigmoid层的计算图（简化版）
- en: The computational graph in *Figure 5.23* and the simplified computational graph
    in *Figure 5.24* provide the same calculation result. The simple version is more
    efficient because it can omit the intermediate calculation in backward propagation.
    It is also important to note that you can only concentrate on the input and output,
    without caring about the details of the Sigmoid layer, by grouping the nodes.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.23*中的计算图和*图5.24*中的简化计算图提供了相同的计算结果。简化版更高效，因为它可以省略反向传播中的中间计算。还需要注意的是，通过分组节点，你可以只关注输入和输出，而无需关心Sigmoid层的细节。'
- en: 'You can also organize ![26](img/Figure_5.24_a.png) as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将![26](img/Figure_5.24_a.png)组织如下：
- en: '| ![53](img/Figure_5.24_b.png) | (5.12) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| ![53](img/Figure_5.24_b.png) | (5.12) |'
- en: 'Therefore, you can only calculate the backward propagation in the Sigmoid layer
    shown in the preceding diagram from the output of forward propagation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你只能通过前向传播的输出计算上图所示的Sigmoid层中的反向传播：
- en: '![Figure 5.25: Computational graph of the Sigmoid layer – you can use the output,
    y, of forward propagation to calculate the backward propagation'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.25：Sigmoid层的计算图——你可以使用前向传播的输出y来计算反向传播'
- en: '](img/Figure_5.25.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.25.jpg)'
- en: 'Figure 5.25: Computational graph of the Sigmoid layer – you can use the output,
    y, of forward propagation to calculate the backward propagation'
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.25：Sigmoid层的计算图——你可以使用前向传播的输出y来计算反向传播
- en: 'Now, let''s implement the Sigmoid layer in Python. Based on the preceding diagram,
    you can implement it as follows (this implementation is located at `common/layers.py`):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Python中实现Sigmoid层。根据前面的图示，你可以按如下方式实现它（此实现位于`common/layers.py`）：
- en: '[PRE7]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This implementation retains the output of forward propagation in the `out` instance
    variable, then uses the `out` variable for calculation purposes in backward propagation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现将前向传播的输出保存在`out`实例变量中，然后在反向传播中使用`out`变量进行计算。
- en: Implementing the Affine and Softmax Layers
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现仿射层和Softmax层
- en: Affine Layer
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仿射层
- en: In forward propagation in a neural network, the product of matrices (`np.dot()`,
    in NumPy) was used to sum the weighted signals (for details, refer to the *Calculating
    Multidimensional Arrays* section in *Chapter 3*, *Neural Networks*). For example,
    do you remember the following implementation in Python?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的前向传播中，使用矩阵乘积（`np.dot()`，在NumPy中）来加权信号求和（详情请参阅*第3章：神经网络*中的*计算多维数组*部分）。例如，你是否还记得下面在Python中的实现？
- en: '[PRE8]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, assume that `X`, `W`, and `B` are multidimensional arrays of the shape
    `(2,)`, `(2, 3)`, and `(3,)`, respectively. With this, you can calculate the weighted
    sum of neurons as `Y = np.dot(X, W) + B`. `Y` is converted by the activation function
    and propagated to the next layer, which is the flow of forward propagation in
    a neural network. Note that the number of elements in corresponding dimensions
    must be the same for matrix multiplication. This means that in the product of
    `X` and `W`, the number of elements in corresponding dimensions must be the same,
    as shown in the following image. Here, the shape of a matrix is represented in
    parentheses as *(2, 3)* (this is for consistency with the output of NumPy''s shape):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`X`、`W`和`B`分别是形状为`(2,)`、`(2, 3)`和`(3,)`的多维数组。基于此，你可以计算神经元的加权和为`Y = np.dot(X,
    W) + B`。`Y`经过激活函数处理并传播到下一层，这就是神经网络中的前向传播过程。请注意，矩阵乘法时，相应维度中的元素数量必须相同。这意味着在`X`和`W`的乘积中，相应维度的元素数量必须一致，如下图所示。这里，矩阵的形状以括号表示为*(2,
    3)*（这是与NumPy的输出形状一致）：
- en: '![Figure 5.26: The number of elements in corresponding dimensions'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.26：相应维度中的元素数量必须相同'
- en: must be the same for matrix multiplication
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 必须相同以进行矩阵乘法
- en: '](img/Figure_5.26.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.26.jpg)'
- en: 'Figure 5.26: The number of elements in corresponding dimensions must be the
    same for matrix multiplication'
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.26：矩阵乘法时，相应维度中的元素数量必须相同
- en: Note
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The product of matrices in forward propagation in a neural network is called
    an "affine transformation" in the field of geometry. Therefore, we will implement
    the process that performs an affine transformation as an "Affine layer."
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的前向传播中的矩阵乘积在几何学中被称为“仿射变换”。因此，我们将实现执行仿射变换的过程，称之为“仿射层”。
- en: 'Now, let''s take a look at the calculation—the product of matrices and the
    sum of biases—in a computational graph. When we represent the node that calculates
    the product of matrices as "dot," the following computational graph can show the
    calculation `np.dot(X, W) + B`. Above each variable, the shape of the variable
    is indicated (for example, the shape of *X* is *(2,)* and that of *X – W* is *(3,)*
    are shown here):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下计算过程——矩阵的乘积和偏置的加和——在计算图中的表示。当我们将计算矩阵乘积的节点表示为“dot”时，以下计算图可以表示计算`np.dot(X,
    W) + B`。在每个变量上方，都会标明该变量的形状（例如，*X*的形状是*(2,)*，而*X – W*的形状是*(3,)*，如下所示）：
- en: '![Figure 5.27: Computational graph of the Affine layer. Note that the variables
    are matrices. Above each variable, the shape of the variable is shown'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.27：仿射层的计算图。请注意，变量是矩阵。每个变量上方显示该变量的形状'
- en: '](img/Figure_5.27.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.27.jpg)'
- en: 'Figure 5.27: Computational graph of the Affine layer. Note that the variables
    are matrices. Above each variable, the shape of the variable is shown'
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.27：仿射层的计算图。请注意，变量是矩阵。每个变量上方显示该变量的形状
- en: The preceding is a relatively simple computational graph. However, note that
    *X*, *W*, and *B* are multidimensional arrays. In the computational graphs we've
    looked at so far, "scalar values" flow between nodes, while in this example, "multidimensional
    arrays" propagate between nodes.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 前述是一个相对简单的计算图。但请注意，*X*、*W*和*B*是多维数组。在我们之前看到的计算图中，“标量值”在节点之间流动，而在这个例子中，“多维数组”在节点之间传播。
- en: 'Let''s think about the backward propagation of the preceding computational
    graph. To obtain the backward propagation for multidimensional arrays, you can
    use the same procedure as the previous computational graphs used for scalar values
    by writing each element of the multidimensional arrays. Doing this, we can obtain
    the following equation (how we can obtain equation (5.13) is omitted here):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下前述计算图的反向传播过程。为了获得多维数组的反向传播，你可以采用与之前用于标量值的计算图相同的步骤，通过逐个写出多维数组的每个元素来进行。这样，我们可以得到以下方程式（如何得到方程（5.13）在此省略）：
- en: '| ![30](img/Figure_5.27_a.png) | (5.13) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| ![30](img/Figure_5.27_a.png) | (5.13) |'
- en: 'In equation (5.13), T in WT indicates transpose. Transpose switches the (i,
    j) elements of W to the (j, i) elements, shown in the following equation:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程（5.13）中，WT中的T表示转置。转置操作将W的(i, j)元素切换为(j, i)元素，如下所示的方程：
- en: '| ![43](img/Figure_5.27_d.png) | (5.14) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| ![43](img/Figure_5.27_d.png) | (5.14) |'
- en: As shown in equation (5.14), when the shape of W is (2, 3), the shape of WT
    becomes (3, 2).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程（5.14）所示，当W的形状为(2, 3)时，WT的形状变为(3, 2)。
- en: 'Based on equation (5.13), let''s write backward propagation in the computational
    graph. The following diagram shows the result:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程 (5.13)，让我们写出计算图中的反向传播。下图展示了结果：
- en: '![Figure 5.28: Backward propagation of the Affine layer. Note that the variables
    are matrices. Below each variable, the shape of the variable is shown'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.28：仿射层的反向传播。请注意，变量是矩阵。每个变量下方显示了该变量的形状。'
- en: '](img/Figure_5.28.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.28.jpg)'
- en: 'Figure 5.28: Backward propagation of the Affine layer. Note that the variables
    are matrices. Below each variable, the shape of the variable is shown'
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.28：仿射层的反向传播。请注意，变量是矩阵。每个变量下方显示了该变量的形状。
- en: 'Let''s consider the shape of each variable carefully. Please note that X and
    ![41](img/Figure_5.28_a.png) are the same shape, and that W and ![40](img/Figure_5.28_b.png)
    are the same in terms of shape because of the following equation:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细考虑每个变量的形状。请注意，X 和 ![41](img/Figure_5.28_a.png) 形状相同，W 和 ![40](img/Figure_5.28_b.png)
    在形状上是相同的，这是因为以下方程：
- en: '| ![60](img/Figure_5.28_d.png) | (5.15) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ![60](img/Figure_5.28_d.png) | (5.15) |'
- en: 'We pay attention to the shapes of matrices because the number of elements in
    the corresponding dimensions must be the same for matrix multiplication, and checking
    that they''re the same can lead to equation (5.13). For example, consider the
    product of ![33](img/Figure_5.28_f.png) and W so that the shape of ![34](img/Figure_5.28_g.png)
    becomes (2,) when the shape of ![35](img/Figure_5.28_i.png) is (3,) and the shape
    of W is (2,3). Then, equation (5.13) follows. This can be seen in the following
    diagram:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意矩阵的形状，因为矩阵乘法要求对应维度中的元素数量相同，检查它们是否相同可以得到方程 (5.13)。例如，考虑 ![33](img/Figure_5.28_f.png)
    和 W 的乘积，使得 ![34](img/Figure_5.28_g.png) 的形状变为 (2,)，当 ![35](img/Figure_5.28_i.png)
    的形状为 (3,) 且 W 的形状为 (2,3) 时。然后，方程 (5.13) 就成立。这可以在下图中看到：
- en: '![Figure 5.29: Product of matrices (you can create backward propagation of
    the "dot" node by configuring a product so that the number of elements in the
    corresponding dimensions is the same in the matrices)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.29：矩阵乘积（通过配置矩阵乘积，使得对应维度中的元素数量相同，可以创建“点”节点的反向传播）'
- en: '](img/Figure_5.29.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.29.jpg)'
- en: 'Figure 5.29: Product of matrices (you can create backward propagation of the
    "dot" node by configuring a product so that the number of elements in the corresponding
    dimensions is the same in the matrices)'
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.29：矩阵乘积（通过配置矩阵乘积，使得对应维度中的元素数量相同，可以创建“点”节点的反向传播）
- en: Batch-Based Affine Layer
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于批量的仿射层。
- en: The Affine layer takes one piece of data as input, X. In this section, we will
    consider a batch-based Affine layer, which propagates N pieces of data collectively
    (a group of data is called a "batch"). Let's start by looking at the computational
    graph of the batch-based Affine layer (*Figure 5.30*).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 仿射层将一块数据作为输入，X。在本节中，我们将讨论基于批量的仿射层，它共同传播 N 个数据点（一个数据组被称为“批量”）。让我们从查看基于批量的仿射层的计算图（*图
    5.30*）开始。
- en: The only difference from the previous explanation is that the shape of the input,
    X, is now (N, 2). All we have to do is to calculate the matrices in the computational
    graph in the same way as we did previously. For backward propagation, we must
    be careful regarding the shapes of the matrices. Only after that can we obtain
    ![36](img/Figure_5.29_a.png) and ![37](img/Figure_5.29_b.png) in the same way.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的解释唯一不同的是，输入 X 的形状现在是 (N, 2)。我们需要做的就是按照之前的方式计算计算图中的矩阵。对于反向传播，我们必须小心矩阵的形状。只有在此之后，我们才能以相同的方式得到
    ![36](img/Figure_5.29_a.png) 和 ![37](img/Figure_5.29_b.png)。
- en: 'You must be careful when adding bias. When adding biases in forward propagation,
    a bias is added to each piece of data for X · W. For example, when N = 2 (two
    pieces of data), biases are added to each of the two pieces of data (to each calculation
    result). The following diagram shows a specific example of this:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当添加偏置时，必须小心。在前向传播中，偏置被添加到每个数据的 X · W 中。例如，当 N = 2（两个数据点）时，偏置被添加到每个数据点（每个计算结果）。下图展示了这一过程的具体示例：
- en: '![Figure 5.30: Computational graph of the batch-based Affine layer'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.30：基于批量的仿射层计算图'
- en: '](img/Figure_5.30.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.30.jpg)'
- en: 'Figure 5.30: Computational graph of the batch-based Affine layer'
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.30：基于批量的仿射层计算图。
- en: '[PRE9]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In forward propagation, the biases are added to each piece of data (the first,
    the second, and so on). Therefore, in backward propagation, the values of each
    piece of data in backward propagation must be integrated into the elements of
    biases. The following code shows this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，偏置项被加到每个数据上（第一个、第二个，依此类推）。因此，在反向传播中，反向传播中每个数据的值必须与偏置项的元素合并。以下代码展示了这一点：
- en: '[PRE10]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this example, we're assuming that there are two pieces of data (N = 2). In
    backward propagation of biases, the derivatives with respect to the two pieces
    of data are summed for each piece of data. To do that, `np.sum()` sums the elements
    of axis 0.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们假设有两条数据（N = 2）。在偏置的反向传播中，对于每条数据，关于这两条数据的导数会被加总。为了实现这一点，`np.sum()`会对轴0的元素进行求和。
- en: 'Thus, the implementation of the Affine layer is as follows. The Affine implementation
    in `common/layers.py` is slightly different from the implementation described
    here because it considers the case when input data is a tensor (four-dimensional
    array):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仿射层的实现如下所示。`common/layers.py`中的仿射层实现与这里描述的实现稍有不同，因为它考虑了输入数据为张量（四维数组）的情况：
- en: '[PRE11]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Softmax-with-Loss Layer
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax-with-Loss层
- en: Finally, we should consider a softmax function, which is the output layer. The
    softmax function normalizes entered values and outputs them (as described earlier).
    For example, the following diagram shows the output of the Softmax layer for handwritten
    digit recognition.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该考虑一个softmax函数，它是输出层。softmax函数归一化输入的值并输出它们（如前所述）。例如，下面的图显示了手写数字识别中Softmax层的输出。
- en: As we can see, the Softmax layer normalizes the entered values (meaning it converts
    them so that the total of the output values is 1) and outputs them. The Softmax
    layer has 10 inputs because handwritten digit recognition classifies data into
    10 classes.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，Softmax层归一化了输入的值（即将它们转换为输出值的总和为1），并输出它们。Softmax层有10个输入，因为手写数字识别将数据分类为10个类别。
- en: Note
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Neural network processing consists of two phases: **inference** and **training**.
    Usually, inference in a neural network does not use the Softmax layer. For example,
    for inference in the network shown in the following diagram, the output of the
    final Affine layer is used as the inference result. The unnormalized output result
    from a neural network (the output of the Affine layer before the Softmax layer
    in the following diagram) is sometimes called a "score." To obtain only one answer
    in neural network inference, you only need to calculate the maximum score, so
    you do not need a Softmax layer. However, you do need a Softmax layer in neural
    network training.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的处理包含两个阶段：**推理**和**训练**。通常，神经网络的推理阶段不使用Softmax层。例如，在下图所示的网络中，最后一个仿射层的输出作为推理结果。神经网络的未归一化输出结果（即下图中Softmax层之前的仿射层输出）有时称为“得分”。为了在神经网络推理中得到唯一的答案，只需要计算最大得分，因此不需要Softmax层。然而，在神经网络训练过程中是需要Softmax层的。
- en: '![Figure 5.31: The images are converted by the Affine and ReLU layers and 10
    input values'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.31: 图像通过仿射层和ReLU层转换，10个输入值'
- en: are normalized by the Softmax layer
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Softmax层进行归一化
- en: '](img/Figure_5.31.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.31.jpg)'
- en: 'Figure 5.31: The images are converted by the Affine and ReLU layers and 10
    input values are normalized by the Softmax layer'
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Figure 5.31: 图像通过仿射层和ReLU层转换，10个输入值被Softmax层归一化'
- en: In this example, the score for "0" is 5.3, which is converted into 0.008 (0.8%)
    by the Softmax layer. The score for "2" is 10.1, which is converted into 0.991
    (99.1%).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，“0”的得分是5.3，经过Softmax层转换为0.008（0.8%）。 “2”的得分是10.1，经过Softmax层转换为0.991（99.1%）。
- en: 'Now, we will implement the "Softmax-with-Loss layer," which also contains a
    cross-entropy error, which is a loss function. The following diagram shows the
    computational graph of the Softmax-with-Loss layer (the softmax function and cross-entropy
    error):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现“Softmax-with-Loss层”，该层也包含交叉熵误差，这是一个损失函数。下图显示了Softmax-with-Loss层的计算图（softmax函数和交叉熵误差）：
- en: '![Figure 5.32: Computational graph of the Softmax-with-Loss layer'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.32: Softmax-with-Loss层的计算图'
- en: '](img/Figure_5.32.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.32.jpg)'
- en: 'Figure 5.32: Computational graph of the Softmax-with-Loss layer'
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Figure 5.32: Softmax-with-Loss层的计算图'
- en: As you can see, the Softmax-with-Loss layer is slightly complicated. Only the
    result is shown here. If you are interested in how the Softmax-with-Loss layer
    is created, refer to the *Computational Graph of the Softmax-with-Loss Layer*
    section in the *Appendix*.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Softmax-with-Loss层略显复杂。这里只展示了结果。如果你对Softmax-with-Loss层的构建过程感兴趣，请参考*附录*中*Softmax-with-Loss层的计算图*部分。
- en: 'We can simplify the computational graph shown in the preceding diagram as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前面图示的计算图简化如下：
- en: '![Figure 5.33: "Simplified" computational graph of the Softmax-with-Loss layer'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.33：“简化”版的Softmax-with-Loss层计算图'
- en: '](img/Figure_5.33.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.33.jpg)'
- en: 'Figure 5.33: "Simplified" computational graph of the Softmax-with-Loss layer'
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.33：“简化”版的Softmax-with-Loss层计算图
- en: In the preceding computational graph, the Softmax layer indicates the Softmax
    function, while the Cross-Entropy Error layer indicates the cross-entropy error.
    Here, we're assuming that the data, which is classified into three classes and
    three inputs (scores), is received from the previous layer. As we can see, the
    Softmax layer normalizes the input (a1, a2, a3) and outputs (y1, y2, and y3).
    The Cross-Entropy Error layer receives the output from Softmax, (y1, y2, y3),
    and the label, (t1, t2, t3), and outputs the loss, L, based on this data.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的计算图中，Softmax层表示Softmax函数，而交叉熵误差层表示交叉熵误差。在这里，我们假设数据已经被分类为三类，并且有三个输入（得分），这些数据来自前一层。如图所示，Softmax层对输入(a1,
    a2, a3)进行归一化，并输出(y1, y2, y3)。交叉熵误差层接收Softmax的输出(y1, y2, y3)和标签(t1, t2, t3)，并根据这些数据输出损失L。
- en: Backward propagation from the Softmax layer returns (y1 t1, y2 t2, y3 t3), which
    is a "clean" result. Because (y1, y2, y3) is the output of the Softmax layer and
    (t1, t2, t3) is the label, (y1 − t1, y2 − t2, y3 − t3) is the difference between
    the output of the Softmax layer and the label. When backward propagating a neural
    network, an error, which is the difference, is passed to the previous layer. This
    characteristic is important when training a neural network.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从Softmax层的反向传播返回(y1 t1, y2 t2, y3 t3)，这是一个“干净”的结果。因为(y1, y2, y3)是Softmax层的输出，(t1,
    t2, t3)是标签，(y1 − t1, y2 − t2, y3 − t3)是Softmax层的输出与标签之间的差异。在神经网络反向传播时，这个差异作为误差传递给上一层。这个特性在训练神经网络时非常重要。
- en: Note that the purpose of neural network training is to adjust weight parameters
    so that the neural network's output (output of Softmax) approaches the label.
    To do that, we need to pass the error between the neural network's output and
    the label to the previous layer in an efficient manner. The previous result, (y1
    − t1, y2 − t2, y3 − t3), is exactly the difference between the output of the Softmax
    layer and the label, and clearly shows the current error between the neural network's
    output and the label.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，神经网络训练的目的是调整权重参数，使得神经网络的输出（Softmax的输出）接近标签。为了做到这一点，我们需要有效地将神经网络输出与标签之间的误差传递给上一层。前面的结果(y1
    − t1, y2 − t2, y3 − t3)正是Softmax层输出与标签之间的差异，并清楚地显示了神经网络输出与标签之间的当前误差。
- en: When we use a "cross-entropy error" as the loss function for the "softmax function,"
    backward propagation returns a "pretty" result, (y1 − t1, y2 − t2, y3 − t3). This
    "pretty" result is not accidental. A function that calls a cross-entropy error
    is designed to do this. In a regression problem, an "identity function" is used
    for the output layer, and the "sum of squared errors" is used as the loss function
    (refer to the *Designing the Output Layer* section of *Chapter 3*, *Neural Networks*)
    for the same reason. When we use the sum of squared errors as the loss function
    of an "identity function", backward propagation provides a "pretty" result, (y1
    − t1, y2 − t2, y3 − t3).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用“交叉熵误差”作为“softmax函数”的损失函数时，反向传播返回一个“漂亮”的结果，(y1 − t1, y2 − t2, y3 − t3)。这个“漂亮”的结果不是偶然的。调用交叉熵误差的函数就是为了做到这一点。在回归问题中，输出层使用“恒等函数”，损失函数使用“平方误差和”（参见*第3章*的*设计输出层*部分，*神经网络*）是出于同样的原因。当我们使用平方误差和作为“恒等函数”的损失函数时，反向传播也会提供一个“漂亮”的结果，(y1
    − t1, y2 − t2, y3 − t3)。
- en: Let's consider a specific example here. Say that, for one piece of data, the
    label is (0, 1, 0) and the output of the Softmax layer is (0.3, 0.2, 0.5). At
    this time, the neural network does not recognize it correctly because the probability
    of it being the correct label is 0.2 (20%). Here, backward propagating from the
    Softmax layer propagates a large error, (0.3, −0.8, 0.5). Because this large error
    propagates to the previous layers, the layers before the Softmax layer learn a
    lot from the large error.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里考虑一个具体的例子。假设对于一组数据，标签是（0，1，0），而 Softmax 层的输出是（0.3，0.2，0.5）。此时，神经网络没有正确识别，因为它是正确标签的概率只有
    0.2（20%）。在这种情况下，从 Softmax 层向后传播一个较大的误差（0.3，−0.8，0.5）。因为这个较大的误差会传播到前面的层，所以 Softmax
    层之前的层从这个大误差中学到了更多信息。
- en: As another example, let's assume that, for one piece of data, the label is (0,
    1, 0) and the output of the Softmax layer is (0.01, 0.99, 0) (this neural network
    recognizes quite accurately). In this case, backward propagating from the Softmax
    layer propagates a small error, (0.01, −0.01, 0). This small error propagates
    to the previous layers. The layers before the Softmax layer learn only small pieces
    of information because the error is small.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，假设对于一组数据，标签是（0，1，0），而 Softmax 层的输出是（0.01，0.99，0）（这个神经网络识别得相当准确）。在这种情况下，从
    Softmax 层向后传播一个小的误差（0.01，−0.01，0）。这个小误差会传播到前面的层。Softmax 层之前的层只学习到小部分信息，因为误差很小。
- en: 'Now, let''s implement the Softmax-with-Loss layer. You can implement the Softmax-with-Loss
    layer as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来实现 Softmax-with-Loss 层。你可以按照如下方式实现 Softmax-with-Loss 层：
- en: '[PRE12]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This implementation uses the `softmax()` and `cross_entropy_error()` functions.
    They were implemented in the sub-section, *Issues when Implementing the Softmax
    Function*, of *Chapter 3*, *Neural networks* and sub-section, *Implementing Cross-Entropy
    Error (Using Batches)*, of *Chapter 4*, *Neural Network Training*. Therefore,
    the implementation here is very easy. Note that the error per data propagates
    to the previous layers in backward propagation because the value of propagation
    is divided by the number of batches (`batch_size`).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本实现使用了 `softmax()` 和 `cross_entropy_error()` 函数。这些函数在《第三章 神经网络》的子章节 *实现 Softmax
    函数时的问题* 和《第四章 神经网络训练》的子章节 *实现交叉熵误差（使用小批量）* 中已经实现。因此，这里的实现非常简单。请注意，每个数据的误差会在反向传播时传播到前面的层，因为传播值会被批量大小（`batch_size`）除以。
- en: Implementing Backpropagation
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: You can build a neural network by combining the layers implemented in the previous
    sections as if you were assembling Lego blocks. Here, we will build a neural network
    by combining the layers we've implemented so far.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将之前实现的层像拼装乐高积木一样组合起来，构建一个神经网络。在这里，我们将通过组合目前为止实现的层来构建一个神经网络。
- en: Overall View of Neural Network Training
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络训练的整体视图
- en: Because my description was a little long, let's check the overall view of neural
    network training again before proceeding with its implementation. Now we will
    take a look at the procedure for neural network training.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我的描述有点长，在继续实现之前，我们先来回顾一下神经网络训练的整体过程。现在我们将查看神经网络训练的流程。
- en: Presupposition
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前提假设
- en: 'A neural network has adaptable weights and biases. Adjusting them so that they
    fit the training data is called "training." Neural network training consists of
    the following four steps:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络具有可调的权重和偏置。调整它们以使其适应训练数据的过程称为“训练”。神经网络训练包括以下四个步骤：
- en: '**Step 1 (mini-batch):**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1（小批量）：**'
- en: Select some data at random from the training data.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练数据中随机选择一些数据。
- en: '**Step 2 (calculating the gradients):**'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2（计算梯度）：**'
- en: Obtain the gradient of the loss function for each weight parameter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 获取每个权重参数的损失函数梯度。
- en: '**Step 3 (updating the parameters):**'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3（更新参数）：**'
- en: Update the parameters slightly in the gradient's direction.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度的方向上稍微更新参数。
- en: '**Step 4 (repeating):**'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4（重复）：**'
- en: Repeat steps 1, 2, and 3.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤 1、2 和 3。
- en: Backpropagation occurs in *Step 2,* *Calculating the gradients*. In the previous
    chapter, we used numerical differentiation to obtain a gradient. Numerical differentiation
    is easy to implement, but calculation takes a lot of time. If we use backpropagation,
    we can obtain a gradient much more quickly and efficiently.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播发生在*步骤 2*，*计算梯度*。在上一章中，我们通过数值微分获得梯度。数值微分容易实现，但计算时间较长。如果使用反向传播，我们可以更快速有效地获得梯度。
- en: Implementing a Neural Network That Supports Backpropagation
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个支持反向传播的神经网络
- en: In this section, we will implement a two-layer neural network called `TwoLayerNet`.
    First, we will look at the instance variables and methods of this class in *Tables
    5.1* and *5.2*.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个名为`TwoLayerNet`的两层神经网络。首先，我们将查看*表 5.1*和*表 5.2*中该类的实例变量和方法。
- en: 'The implementation of this class is a little long but has many sections in
    common with the implementation as in the *Implementing a Training Algorithm* section,
    of *Chapter 4*, *Neural Network Training*. A large change from the previous chapter
    is that layers are being used here. If you use layers, you can obtain recognition
    results (`predict( )`) and gradients (`gradient( )`) by propagating between the
    layers:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的实现稍微长一些，但有许多部分与*第4章*中*实现训练算法*部分的实现相似。与上一章的主要变化是，这里使用了层。如果使用层，你可以通过在层之间传播来获取识别结果（`predict(
    )`）和梯度（`gradient( )`）：
- en: '![Table 5.1: Instance variables in the TwoLayerNet class'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '![表 5.1：TwoLayerNet 类中的实例变量'
- en: '](img/Table_5.1.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Table_5.1.jpg)'
- en: 'Table 5.1: Instance variables in the TwoLayerNet class'
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 表 5.1：TwoLayerNet 类中的实例变量
- en: '![Table 5.2: Methods in the TwoLayerNet class'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '![表 5.2：TwoLayerNet 类中的方法'
- en: '](img/Table_5.2.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Table_5.2.jpg)'
- en: 'Table 5.2: Methods in the TwoLayerNet class'
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 表 5.2：TwoLayerNet 类中的方法
- en: 'Now, let''s implement `TwoLayerNet`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现`TwoLayerNet`：
- en: '[PRE13]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note the code in bold here. Retaining a neural network layer as `OrderedDict`
    (i,e, an ordered dictionary) is especially important, as it means that the dictionary
    can remember the order of the elements added to it. Therefore, in forward propagation
    in the neural network, you can complete processing by calling the `forward()`
    method of the layer in the order of addition. In backward propagation, you only
    have to call the layers in reverse order. The Affine and ReLU layers internally
    process forward propagation and backward propagation properly. So, all you have
    to do is combine the layers in the correct order and call them in order (or in
    reverse order).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的代码是加粗的。保持神经网络层为`OrderedDict`（即有序字典）是特别重要的，因为它意味着字典可以记住添加到其中的元素顺序。因此，在神经网络的前向传播中，你可以按添加顺序调用层的`forward()`方法完成处理。在反向传播中，你只需要按相反顺序调用这些层。Affine
    和 ReLU 层在内部会正确处理前向传播和反向传播。所以，你只需要按正确的顺序将层组合起来并按顺序调用它们（或反向顺序调用）。
- en: Thus, by implementing the components of the neural network as "layers," you
    can build the neural network easily. The advantage of modular implementation using
    "layers" is enormous. If you want to create a large network containing five, 10,
    or 20 layers, you can create it by adding the required layers (as if you were
    assembling Lego blocks). In this way, the gradients required for recognition and
    learning are obtained properly by forward propagation and backward propagation
    being implemented in each layer.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过将神经网络的组件实现为“层”，你可以轻松构建神经网络。使用“层”进行模块化实现的优势是巨大的。如果你想创建一个包含五层、十层或二十层的大型网络，你可以通过添加所需的层（就像组装乐高积木一样）来完成。通过这种方式，通过每一层实现的前向传播和反向传播，正确地获得了识别和学习所需的梯度。
- en: Gradient Check
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度检查
- en: So far, we have seen two methods for calculating a gradient. One of them uses
    numerical differentiation, while the other solves the equation analytically. The
    latter method enables efficient calculation by using backpropagation, even if
    many parameters exist. Therefore, we'll be using backpropagation instead of slow
    numerical differentiation to calculate a gradient from now on.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了计算梯度的两种方法。它们中的一种使用数值微分，另一种通过解析解来求解方程。后一种方法通过使用反向传播，使得即使有很多参数也能高效地计算。因此，从现在开始，我们将使用反向传播而不是缓慢的数值微分来计算梯度。
- en: Numerical differentiation takes time to calculate. If the correct implementation
    of backpropagation exists, we do not need the implementation of numerical differentiation.
    So, what is numerical differentiation useful for? The fact is that numerical differentiation
    is required to check whether the implementation of backpropagation is correct.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 数值微分需要时间来计算。如果存在正确的反向传播实现，我们就不需要数值微分的实现。那么，数值微分到底有什么用呢？事实上，数值微分是用来检查反向传播的实现是否正确的。
- en: 'The advantage of numerical differentiation is that it is easy to implement,
    making mistakes infrequent compared to the far more complicated backpropagation.
    So, the result of numerical differentiation is often compared with that of backpropagation
    to check whether the implementation of backpropagation is correct. The process
    of this verification is called a `ch05/gradient_check.py`):'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 数值微分的优点在于它易于实现，与更加复杂的反向传播相比，犯错的概率较低。因此，数值微分的结果常常与反向传播的结果进行比较，以检查反向传播的实现是否正确。这个验证过程被称为`ch05/gradient_check.py`。
- en: '[PRE14]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, the MNIST dataset is loaded as usual. Next, part of the training data
    is used to check the error between the gradient by numerical differentiation and
    that of backpropagation. As the error, the absolute values of the differences
    between the elements in the individual weight parameters are averaged. When the
    preceding code is executed, the following result is returned:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，MNIST 数据集像往常一样被加载。接下来，使用部分训练数据检查数值微分的梯度与反向传播的梯度之间的误差。误差是通过对各个权重参数中元素差异的绝对值求平均得出的。当执行上述代码时，将返回以下结果：
- en: '[PRE15]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The result shows that the differences between the gradients by numerical differentiation
    and those by backpropagation are quite small. Case in point, the error of the
    biases for layer 1 is `9.7e-13 (0.00000000000097)`. This indicates that the gradient
    by backpropagation is also correct and improves the reliability of its accuracy.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，数值微分和反向传播计算的梯度之间的差异非常小。例如，层 1 偏置的误差为 `9.7e-13 (0.00000000000097)`。这表明反向传播计算的梯度也是正确的，并提高了其准确性的可靠性。
- en: The error between the calculation result of numerical differentiation and that
    of backpropagation rarely becomes 0\. This is because the accuracy of the calculations
    that are performed by a computer is finite (for example, 32-bit floating-point
    numbers are used). Because the numerical precision is limited, the error is not
    usually 0\. However, if the implementation is correct, the error is expected to
    be a small value near 0\. If the value is large, the implementation of backpropagation
    is incorrect.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 数值微分的计算结果与反向传播的计算结果之间的误差很少为 0。原因在于计算机执行的计算精度是有限的（例如，使用的是 32 位浮动点数）。由于数值精度有限，误差通常不会为
    0。但是，如果实现正确，误差应该是一个接近 0 的小值。如果误差较大，则说明反向传播的实现有误。
- en: Training Using Backpropagation
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用反向传播进行训练
- en: 'Lastly, we will see how we can implement neural network training using backpropagation.
    The only difference is that gradients are calculated via backpropagation. We will
    only see the code and omit the description (the source code is located at `ch05/train_neuralnet.py`):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将看到如何使用反向传播实现神经网络训练。唯一的区别是梯度是通过反向传播计算的。我们将仅查看代码并省略描述（源代码位于 `ch05/train_neuralnet.py`）：
- en: '[PRE16]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about computational graphs, which show calculation
    processes visually. We looked at a computational graph that described backpropagation
    in a neural network and implemented processing in a neural network with layers,
    including the ReLU layer, Softmax-with-Loss layer, Affine layer, and Softmax layer.
    These layers have forward and backward methods and can calculate the gradients
    of weight parameters efficiently by propagating data both forward and backward
    in direction. By using layers as modules, you can combine them freely in a neural
    network so that you can build the desired network easily. The following points
    were covered in this chapter:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了计算图，它以直观的方式展示了计算过程。我们查看了一个描述神经网络反向传播的计算图，并实现了具有多个层的神经网络处理，包括 ReLU 层、Softmax-with-Loss
    层、仿射层和 Softmax 层。这些层具有前向和后向方法，可以通过前向和后向传播数据来高效地计算权重参数的梯度。通过将层作为模块，您可以在神经网络中自由组合它们，从而轻松构建所需的网络。本章涵盖了以下内容：
- en: We can use computational graphs to show calculation processes visually.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用计算图来直观地展示计算过程。
- en: A node in a computational graph consists of local calculations. Local calculations
    constitute the whole calculation.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图中的一个节点由局部计算组成，局部计算构成了整个计算。
- en: Performing forward propagation in a computational graph leads to a regular calculation.
    Meanwhile, performing backward propagation in a computational graph can calculate
    the differential of each node.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算图中执行前向传播会进行常规计算。与此同时，在计算图中执行反向传播可以计算每个节点的微分。
- en: By implementing components in a neural network as layers, you can calculate
    gradients efficiently (backpropagation).
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将神经网络中的组件实现为层，您可以高效地计算梯度（反向传播）。
- en: By comparing the results of numerical differentiation and backpropagation, you
    can check that the implementation of backpropagation is correct (gradient check).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过比较数值微分和反向传播的结果，您可以检查反向传播的实现是否正确（梯度检查）。
