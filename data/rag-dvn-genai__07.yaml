- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Building Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用维基百科API和LlamaIndex构建可扩展的知识图谱RAG
- en: Scaled datasets can rapidly become challenging to manage. In real-life projects,
    data management generates more headaches than AI! Project managers, consultants,
    and developers constantly struggle to obtain the necessary data to get any project
    running, let alone a RAG-driven generative AI application. Data is often unstructured
    before it becomes organized in one way or another through painful decision-making
    processes. Wikipedia is a good example of how scaling data leads to mostly reliable
    but sometimes incorrect information. Real-life projects often evolve the way Wikipedia
    does. Data keeps piling up in a company, challenging database administrators,
    project managers, and users.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展的数据集可能会迅速变得难以管理。在实际项目中，数据管理产生的麻烦比人工智能还要多！项目经理、顾问和开发者不断努力获取启动任何项目所需的数据，更不用说一个由RAG驱动的生成式AI应用了。数据在以某种方式组织之前通常是未结构化的。维基百科是数据扩展导致信息大多可靠但有时不正确的一个好例子。现实生活中的项目通常像维基百科一样发展。数据在公司中不断积累，挑战数据库管理员、项目经理和用户。
- en: 'One of the main problems is seeing how large amounts of data fit together,
    and **knowledge graphs** provide an effective way of visualizing the relationships
    between different types of data. This chapter begins by defining the architecture
    of a knowledge base ecosystem designed for RAG-driven generative AI. The ecosystem
    contains three pipelines: data collection, populating a vector store, and running
    a knowledge graph index-based RAG program. We will then build *Pipeline 1: Collecting
    and preparing the documents*, in which we will build an automated Wikipedia retrieval
    program with the Wikipedia API. We will simply choose a topic based on a Wikipedia
    page and then let the program retrieve the metadata we need to collect and prepare
    the data. The system will be flexible and allow you to choose any topic you wish.
    The use case to first run the program is a marketing knowledge base for students
    who want to upskill for a new job, for example. The next step is to build *Pipeline
    2: Creating and populating the Deep Lake vector store*. We will load the data
    in a vector store leveraging Deep Lake’s in-built automated chunking and OpenAI
    embedding functionality. We will peek into the dataset to explore how this marvel
    of technology does the job.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题之一是看到大量数据如何相互关联，**知识图谱**提供了一种有效的方式来可视化不同类型数据之间的关系。本章首先定义了一个为RAG驱动的生成式AI设计的知识库生态系统架构。该生态系统包含三个管道：数据收集、填充向量存储和运行基于知识图谱索引的RAG程序。然后我们将构建**管道1：收集和准备文档**，其中我们将使用维基百科API构建一个自动化的维基百科检索程序。我们将简单地根据维基百科页面选择一个主题，然后让程序检索我们需要收集和准备的数据的元数据。系统将是灵活的，允许你选择任何你希望的主题。首先运行程序的使用案例是一个为希望提升技能以适应新工作的学生设计的营销知识库。下一步是构建**管道2：创建和填充Deep
    Lake向量存储**。我们将利用Deep Lake内置的自动分块和OpenAI嵌入功能将数据加载到向量存储中。我们将深入了解数据集，探索这项技术奇迹是如何工作的。
- en: 'Finally, we will build *Pipeline 3: Knowledge graph index-based RAG*, where
    LlamaIndex will automatically build a knowledge graph index. It will be exciting
    to see how the index function churns through our data and produces a graph showing
    semantic relationships contained in our data. We will then query the graph with
    LlamaIndex’s in-built OpenAI functionality to automatically manage user inputs
    and produce a response. We will also see how re-ranking can be done and implement
    metrics to calculate and display the system’s performance.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将构建**管道3：基于知识图谱索引的RAG**，其中LlamaIndex将自动构建知识图谱索引。将非常有趣地看到索引功能如何处理我们的数据，并生成显示我们数据中包含的语义关系的图表。然后我们将使用LlamaIndex内置的OpenAI功能查询该图表，以自动管理用户输入并生成响应。我们还将看到如何进行重新排序，并实现用于计算和显示系统性能的指标。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Defining knowledge graphs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义知识图谱
- en: Implementing the Wikipedia API to prepare summaries and content
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现维基百科API以准备摘要和内容
- en: Citing Wikipedia sources in an ethical approach
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以道德方式引用维基百科来源
- en: Populating a Deep Lake vector store with Wikipedia data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用维基百科数据填充Deep Lake向量存储
- en: Building a knowledge graph index with LlamaIndex
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LlamaIndex构建知识图谱索引
- en: Displaying the LlamaIndex knowledge graph
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示LlamaIndex知识图谱
- en: Interacting with the knowledge graph
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与知识图谱交互
- en: Generating retrieval responses with the knowledge graph
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用知识图谱生成检索响应
- en: Re-ranking the order retrieval responses to choose a better output
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序检索响应的顺序以选择更好的输出
- en: Evaluating and measuring the outputs with metrics
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用指标评估和衡量输出
- en: Let’s begin by defining the architecture of RAG for knowledge-based semantic
    search.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义基于知识的语义搜索的 RAG 架构开始。
- en: The architecture of RAG for knowledge-graph-based semantic search
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于知识图谱的语义搜索的 RAG 架构
- en: 'As established, we will build a graph-based RAG program in this chapter. The
    graph will enable us to visually map out the relationships between the documents
    of a RAG dataset. It can be created automatically with LlamaIndex, as we will
    do in the *Pipeline 3: Knowledge graph index-based RAG* section of this chapter.
    The program in this chapter will be designed for any Wikipedia topic, as illustrated
    in the following figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将在本章中构建一个基于图的 RAG 程序。该图将使我们能够直观地映射出 RAG 数据集文档之间的关系。它可以通过 LlamaIndex 自动创建，正如我们在本章的
    *管道 3：基于知识图谱索引的 RAG* 部分中所做的那样。本章节的程序将针对任何维基百科主题进行设计，如图中所示：
- en: '![A diagram of a graph  Description automatically generated](img/B31169_07_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![一个图形的图表  自动生成的描述](img/B31169_07_01.png)'
- en: 'Figure 7.1: From a Wikipedia topic to interacting with a graph-based vector
    store index'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：从维基百科主题到与基于图的向量存储索引交互
- en: 'We will first implement a marketing agency for which a knowledge graph can
    visually map out the complex relationships between different marketing concepts.
    Then, you can go back and explore any topic you wish once you understand the process.
    In simpler words, we will implement the three pipelines seamlessly to:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实现一个营销机构，其中知识图谱可以直观地映射出不同营销概念之间的复杂关系。然后，一旦你理解了过程，你就可以回过头来探索任何你感兴趣的主题。用更简单的话说，我们将无缝实现三个管道，以：
- en: Select a Wikipedia topic related to *marketing*. Then, you can run the process
    with the topic of your choice to explore the ecosystem.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择与 *营销* 相关的维基百科主题。然后，你可以使用你选择的主题运行该过程以探索生态系统。
- en: Generate a corpus of Wikipedia pages with the Wikipedia API.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用维基百科 API 生成维基百科页面的语料库。
- en: Retrieve and store the citations for each page.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索并存储每个页面的引用。
- en: Retrieve and store the URLs for each page.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索并存储每个页面的 URL。
- en: Retrieve and upsert the content of the URLs in a Deep Lake vector store.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Deep Lake 向量存储中检索并更新 URL 的内容。
- en: Build a knowledge base index with LlamaIndex.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LlamaIndex 构建知识库索引。
- en: Define a user input prompt.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义用户输入提示。
- en: Query the knowledge base index.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询知识库索引。
- en: Let LlamaIndex’s in-built LLM functionality, based on OpenAI’s embedding models,
    produce a response based on the embedded data in the knowledge graph.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让 LlamaIndex 的内置 LLM 功能，基于 OpenAI 的嵌入模型，根据知识图谱中的嵌入数据生成响应。
- en: Evaluate the LLM’s response with a sentence transformer.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用句子转换器评估 LLM 的响应。
- en: Evaluate the LLM’s response with a human feedback score.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人类反馈评分评估 LLM 的响应。
- en: Provide time metrics for the key functions, which you can extend to other functions
    if necessary.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供关键函数的时间指标，如果需要，你可以将其扩展到其他函数。
- en: Run metric calculations and display the results.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行指标计算并显示结果。
- en: 'To attain our goal, we will implement three pipelines leveraging the components
    we have already built in the previous chapters, as illustrated in the following
    figure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的目标，我们将利用前几章中已经构建的组件实现三个管道，如图中所示：
- en: '![A diagram of a graph  Description automatically generated](img/B31169_07_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![一个图形的图表  自动生成的描述](img/B31169_07_02.png)'
- en: 'Figure 7.2: Knowledge graph ecosystem for index-based RAG'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：基于索引的 RAG 知识图谱生态系统
- en: '**Pipeline 1: Collecting and preparing the documents** will involve building
    a Wikipedia program using the Wikipedia API to retrieve links from a Wikipedia
    page and the metadata for all the pages (summary, URL, and citation data). Then,
    we will load and parse the URLs to prepare the data for upserting.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道 1：收集和准备文档** 将涉及使用维基百科 API 构建一个维基百科程序，以检索维基百科页面中的链接和所有页面的元数据（摘要、URL 和引用数据）。然后，我们将加载并解析
    URL 以准备数据以便更新插入。'
- en: '**Pipeline 2: Creating and populating the Deep Lake vector store** will embed
    and upsert parsed content of the Wikipedia pages prepared by *Pipeline 1* to a
    Deep Lake vector store.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道 2：创建和填充 Deep Lake 向量存储** 将将 *管道 1* 准备的维基百科页面解析内容嵌入并更新到 Deep Lake 向量存储中。'
- en: '**Pipeline 3: Knowledge graph index-based RAG** will build the knowledge graph
    index using embeddings with LlamaIndex and display it. Then, we will build the
    functionality to query the knowledge base index and let LlamaIndex’s in-built
    LLM generate the response based on the updated dataset.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pipeline 3: 基于知识图谱索引的RAG** 将使用LlamaIndex的嵌入构建知识图谱索引并展示它。然后，我们将构建查询知识库索引的功能，并让LlamaIndex内置的LLM根据更新的数据集生成响应。'
- en: In this chapter’s scenario, we are directly implementing an augmented retrieval
    system leveraging OpenAI’s embedding models more than we are augmenting inputs.
    This implementation shows the many ways we can improve real-time data retrieval
    with LLMs. There are no conventional rules. What works, works!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的场景中，我们直接实现了一个增强检索系统，利用OpenAI的嵌入模型比增强输入更多。这种实现展示了我们可以用LLM改进实时数据检索的多种方式。没有传统规则。什么有效，就做什么！
- en: The ecosystem of the three pipelines will be controlled by a scenario that will
    enable an administrator to either query the vector base or add new Wikipedia pages,
    as we will implement in this chapter. As such, the architecture of the ecosystem
    allows for indefinite scaling since it processes and populates the vector dataset
    one set of Wikipedia pages at a time. The system only uses a CPU and an optimized
    amount of memory. There are limits to this approach since the LlamaIndex knowledge
    graph index is loaded with the entire dataset. We can only load portions of the
    dataset as the vector store grows. Or, we can create one Deep Lake vector store
    per topic and run queries on multiple datasets. These are decisions to make in
    real-life projects that require careful decision-making and planning depending
    on the specific requirements of each project.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 三个管道的生态系统将由一个场景控制，该场景将允许管理员查询向量库或添加新的维基百科页面，正如我们将在本章中实现的那样。因此，生态系统的架构允许无限扩展，因为它一次处理和填充一组维基百科页面的向量数据集。系统仅使用CPU和优化的内存量。由于LlamaIndex知识图谱索引加载了整个数据集，这种方法有局限性。随着向量存储的增长，我们只能加载数据集的部分。或者，我们可以为每个主题创建一个Deep
    Lake向量存储，并在多个数据集上运行查询。这些是在实际项目中需要谨慎决策和计划的决定，具体取决于每个项目的特定要求。
- en: We will now dive into the code, beginning a tree-to-graph sandbox.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入代码，开始一个树到图的沙盒。
- en: Building graphs from trees
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从树构建图
- en: A graph is a collection of nodes (or vertices) connected by edges (or arcs).
    Nodes represent entities, and edges represent relationships or connections between
    these entities. For instance, in our chapter’s use case, nodes could represent
    various marketing strategies, and the edges could show how these strategies are
    interconnected. This helps new customers understand how different marketing tactics
    work together to achieve overall business goals, facilitating clearer communication
    and more effective strategy planning. You can play around with the tree-to-graph
    sandbox before building the pipelines in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图是由节点（或顶点）通过边（或弧）连接的集合。节点代表实体，边代表这些实体之间的关系或连接。例如，在我们本章的使用案例中，节点可以代表各种营销策略，边可以显示这些策略如何相互关联。这有助于新客户了解不同的营销策略如何协同工作以实现整体业务目标，促进更清晰的沟通和更有效的策略规划。在构建本章的管道之前，您可以在树到图沙盒中尝试操作。
- en: You may open `Tree-2-Graph.ipynb` on GitHub. The provided program is designed
    to visually represent relationships in a tree structure using NetworkX and Matplotlib
    in Python. It specifically creates a directed graph from given pairs, checks and
    marks friendships, and then displays this tree with customized visual attributes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上打开`Tree-2-Graph.ipynb`。提供的程序旨在使用Python中的NetworkX和Matplotlib在树结构中可视化关系。它特别创建了一个有向图，从给定的对中检查并标记友谊，然后以自定义的视觉属性显示此树。
- en: 'The program first defines the main functions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先定义了主要函数：
- en: '`build_tree_from_pairs(pairs)`: Constructs a directed graph (tree) from a list
    of node pairs, potentially identifying a root node'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build_tree_from_pairs(pairs)`: 从节点对列表构建一个有向图（树），可能识别一个根节点'
- en: '`check_relationships(pairs, friends)`: Checks and prints the friendship status
    for each pair'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`check_relationships(pairs, friends)`: 检查并打印每对的朋友关系状态'
- en: '`draw_tree(G, layout_choice, root, friends)`: Visualizes the tree using `matplotlib`,
    applying different styles to edges based on friendship status and different layout
    options for node positioning'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`draw_tree(G, layout_choice, root, friends)`: 使用`matplotlib`可视化树，根据友谊状态应用不同的边样式，并为节点定位提供不同的布局选项'
- en: 'Then, the program executes the process from tree to graph:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，程序执行从树到图的转换过程：
- en: Node pairs and friendship data are defined.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点对和友谊数据被定义。
- en: The tree is built from the pairs.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树从对中构建。
- en: Relationships are checked against the friendship data.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系与友谊数据进行了检查。
- en: The tree is drawn using a selected layout, with edges styled differently to
    denote friendship.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树使用选定的布局绘制，边以不同的样式绘制以表示友谊。
- en: 'For example, the program first defines a set of node pairs with their pairs
    of friends:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，程序首先定义了一组节点对及其朋友对：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that `('a', 'z')` are not friends because they are not on the `friends`
    list. Neither are `('b', 'q')`. You can imagine any type of relationship between
    the pairs, such as the same customer age, similar job, same country, or any other
    concept you wish to represent. For instance, the `friends` list could contain
    relationships between friends on social media, friends living in the same country,
    or anything else you can imagine or need!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`('a', 'z')`不是朋友，因为它们不在`friends`列表中。`('b', 'q')`也不是。您可以想象任何类型的对之间的关系，例如相同的客户年龄、相似的工作、相同的国籍或您希望表示的任何其他概念。例如，`friends`列表可以包含社交媒体上的朋友关系、生活在同一国家的朋友关系或您能想象或需要的任何其他东西！
- en: 'The program then builds the tree and checks the relationships:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 程序随后构建树并检查关系：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output shows which pairs are friends and which ones are not:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了哪些对是朋友，哪些不是：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output can be used to provide useful information for similarity searches.
    The program now draws the graph with the `''spring''` layout:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可以用于提供用于相似性搜索的有用信息。程序现在使用`'spring'`布局绘制图形：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `'spring'` layout attracts nodes attracted by edges, simulating the effect
    of springs. It also ensures that all nodes repel each other to avoid overlapping.
    You can dig into the `draw_tree` function to explore and select other layouts
    listed there. You can also modify the colors and line styles.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`''spring''`布局吸引由边吸引的节点，模拟弹簧的效果。它还确保所有节点相互排斥以避免重叠。您可以深入研究`draw_tree`函数以探索和选择那里列出的其他布局。您还可以修改颜色和线型。'
- en: 'In this case, the pairs of friends are represented with solid lines, and the
    pairs that are not friends are represented with dashes, as shown in the following
    graph:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，朋友对用实线表示，不是朋友的对用虚线表示，如下面的图中所示：
- en: '![A diagram of a tree  Description automatically generated](img/B31169_07_03.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![树图  自动生成的描述](img/B31169_07_03.png)'
- en: 'Figure 7.3: Example of a spring layout'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：弹簧布局的示例
- en: You can play with this sandbox graph with different pairs of nodes. If you imagine
    doing this with hundreds of nodes, you will begin to appreciate the automated
    functionality we will build in this chapter with LlamaIndex’s knowledge graph
    index!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用不同的节点对来玩这个沙盒图。如果您想象用数百个节点来做这件事，您将开始欣赏我们在本章中用LlamaIndex的知识图索引构建的自动化功能！
- en: Let’s go from the architecture to the code, starting by collecting and preparing
    the documents.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从架构转到代码，首先从收集和准备文档开始。
- en: 'Pipeline 1: Collecting and preparing the documents'
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道1：收集和准备文档
- en: 'The code in this section retrieves the metadata we need from Wikipedia, retrieves
    the documents, cleans them, and aggregates them to be ready for insertion into
    the Deep Lake vector store. This process is illustrated in the following figure:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分代码从维基百科检索我们需要的元数据，检索文档，清理它们，并将它们聚合起来以便插入到Deep Lake向量存储中。这个过程在以下图中说明：
- en: '![A diagram of a document  Description automatically generated](img/B31169_07_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![文档图  自动生成的描述](img/B31169_07_04.png)'
- en: 'Figure 7.4: Pipeline 1 flow chart'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：管道1流程图
- en: '*Pipeline 1* includes two notebooks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*管道1*包括两个笔记本：'
- en: '`Wikipedia_API.ipynb`, in which we will implement the Wikipedia API to retrieve
    the URLs of the pages related to the root page of the topic we selected, including
    the citations for each page. As mentioned, the topic is “marketing” in our case.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Wikipedia_API.ipynb`，在这个文件中，我们将实现Wikipedia API以检索与我们选择的主题根页相关的页面URL，包括每个页面的引用。如前所述，我们的主题是“营销”。'
- en: '`Knowledge_Graph_Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`, in which we will implement
    all three pipelines. In Pipeline 1, it will fetch the URLs provided by the `Wikipedia_API`
    notebook, clean them, and load and aggregate them for upserting.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Knowledge_Graph_Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`，在这个文件中，我们将实现所有三个管道。在管道1中，它将获取由`Wikipedia_API`笔记本提供的URL，对其进行清理，并加载和聚合它们以进行更新插入。'
- en: We will begin by implementing the Wikipedia API.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实现Wikipedia API。
- en: Retrieving Wikipedia data and metadata
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索 Wikipedia 数据和元数据
- en: Let’s begin by building a program to interact with the Wikipedia API to retrieve
    information about a specific topic, tokenize the retrieved text, and manage citations
    from Wikipedia articles. You may open `Wikipedia_API.ipynb` in the GitHub repository
    and follow along.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建一个程序，与 Wikipedia API 交互以检索特定主题的信息，对检索到的文本进行标记化，并管理来自 Wikipedia 文章的引用。你可以在
    GitHub 仓库中打开 `Wikipedia_API.ipynb` 并跟随操作。
- en: 'The program begins by installing the `wikipediaapi` library we need:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先安装我们需要的 `wikipediaapi` 库：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to define the tokenization function that will be called to
    count the number of tokens of a summary, as shown in the following excerpt:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义一个标记化函数，该函数将被调用来计算摘要的标记数量，如下面的摘录所示：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This function takes a string of text as input and returns the number of tokens
    in the text, using the NLTK library for sophisticated tokenization, including
    punctuation. Next, to start retrieving data, we need to set up an instance of
    the Wikipedia API with a specified language and user agent:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受一个文本字符串作为输入，并使用 NLTK 库进行复杂的标记化（包括标点符号）来返回文本中的标记数量。接下来，为了开始检索数据，我们需要设置一个具有指定语言和用户代理的
    Wikipedia API 实例：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this case, English was defined with `''en''`, and you must enter the user
    agent information, such as an email address, for example. We can now define the
    main topic and filename associated with the Wikipedia page of interest:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，英语被定义为 `'en'`，你必须输入用户代理信息，例如电子邮件地址。现在我们可以定义与感兴趣的主题的 Wikipedia 页面相关联的主要主题和文件名：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The three parameters defined are:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了三个参数：
- en: '`topic`: The topic of the retrieval process'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topic`: 检索过程的主题'
- en: '`filename`: The name of the topic that will customize the files we produce,
    which can be different from the topic'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filename`: 将定制我们生成的文件的主题名称，这可以与主题不同'
- en: '`maxl`: The maximum number of URL links of the pages we will retrieve'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxl`: 我们将检索的页面的最大 URL 链接数'
- en: 'We now need to retrieve the summary of the specified Wikipedia page, check
    if the page exists, and print its summary:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要检索指定 Wikipedia 页面的摘要，检查页面是否存在，并打印其摘要：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output provides the control information requested:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了请求的控制信息：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The information provided shows if we are on the right track or not before running
    a full search on the main page of the topic:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的信息显示在运行完整搜索主题主页之前我们是否在正确的轨道上：
- en: '`Page - Exists: True` confirms that the page exists. If not, the `print("Page
    does not exist")` message will be displayed.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Page - Exists: True` 确认页面存在。如果不存在，将显示 `print("Page does not exist")` 消息。'
- en: '`Number of tokens: 229` provides us with insights into the size of the content
    we are retrieving for project management assessments.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Number of tokens: 229` 为我们提供了关于我们为项目管理评估检索的内容大小的见解。'
- en: The output of `summary=page.summary` displays a summary of the page.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summary=page.summary` 的输出显示页面的摘要。'
- en: 'In this case, the page exists, fits our topic, and the summary makes sense.
    Before we continue, we check if we are working on the right page to be sure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，页面存在，符合我们的主题，摘要也是有意义的。在我们继续之前，我们检查我们是否在正确的页面上工作以确保：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is correct:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是正确的：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We are now ready to retrieve the URLs, links, and summaries on the target page:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好检索目标页面的 URL、链接和摘要：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The function is limited to `maxl`, defined at the beginning of the program.
    The function will retrieve URL links up to `maxl` links, or less if the page contains
    fewer links than the maximum requested. We then check the output before moving
    on to the next step and generating files:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 函数限制在程序开始时定义的 `maxl`。函数将检索最多 `maxl` 个链接的 URL，或者如果页面包含的链接少于请求的最大数量，则更少。然后我们在进行下一步并生成文件之前检查输出：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We observe that we have the information we need, and the summaries are acceptable:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到我们得到了所需的信息，摘要也是可接受的：
- en: '`Link 1`: The link counter'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Link 1`: 链接计数器'
- en: '`Link`: The actual link to the page retrieved from the main topic page'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Link`: 从主题页面检索到的实际页面链接'
- en: '`Summary`: A summary of the link to the page'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Summary`: 页面的链接摘要'
- en: 'The next step is to apply the function we just built to generate the text file
    containing citations for the links retrieved from a Wikipedia page and their URLs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将我们刚刚构建的函数应用于生成包含从 Wikipedia 页面检索的链接及其 URL 的引用的文本文件：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`urls = []` will be appended to have the full list of URLs we need for the
    final step. The output is a file containing the name of the topic, `datetime`,
    and the citations beginning with the citation text:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`urls = []`将被添加以获得最终步骤所需的完整URL列表。输出是一个包含主题名称、`datetime`和以引用文本开头的引用的文件：'
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The output, in this case, is a file named `Marketing_citations.txt`. The file
    was downloaded and uploaded to the `/citations` directory of this chapter’s directory
    in the GitHub repository.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出是一个名为`Marketing_citations.txt`的文件。该文件已下载并上传到GitHub仓库中该章节目录的`/citations`目录。
- en: 'With that, the citations page has been generated, displayed in this notebook,
    and also saved in the GitHub repository to respect Wikipedia’s citation terms.
    The final step is to generate the file containing the list of URLs we will use
    to fetch the content of the pages we need. We first display the URLs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，引用页面已生成，在本笔记本中显示，并已保存到GitHub仓库中，以遵守维基百科的引用条款。最后一步是生成包含我们将用于获取所需页面内容的URL列表的文件。我们首先显示URL：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output confirms we have the URLs required:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认我们已获得所需的URL：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The URLs are written in a file with the topic as a prefix:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: URL将写入一个以主题为前缀的文件中：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this case, the output is a file named `Marketing_urls.txt` that contains
    the URLs of the pages we need to fetch. The file was downloaded and uploaded to
    the `/citations` directory of the chapter’s directory in the GitHub repository.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出是一个名为`Marketing_urls.txt`的文件，其中包含我们需要获取的页面的URL。该文件已下载并上传到GitHub仓库中该章节目录的`/citations`目录。
- en: We are now ready to prepare the data for upsertion.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好为更新插入准备数据。
- en: Preparing the data for upsertion
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据以进行更新插入
- en: The URLs provided by the Wikipedia API in the `Wikipedia_API.ipynb` notebook
    will be processed in the `Knowledge_Graph_ Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`
    notebook you can find in the GitHub directory of the chapter. The *Installing
    the environment* section of this notebook is almost the same section as its equivalent
    section in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*,
    and *Chapter 3*, *Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI*.
    In this chapter, however, the list of URLs was generated by the `Wikipedia_API.ipynb`
    notebook, and we will retrieve it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub章节目录中可以找到的`Knowledge_Graph_ Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`笔记本中，将处理`Wikipedia_API.ipynb`笔记本中提供的维基百科API的URL。本笔记本的“安装环境”部分几乎与第2章、“使用Deep
    Lake和OpenAI构建RAG嵌入向量存储”、第3章、“使用LlamaIndex、Deep Lake和OpenAI构建基于索引的RAG”中相应的部分相同。然而，在本章中，URL列表是由`Wikipedia_API.ipynb`笔记本生成的，我们将检索它。
- en: 'First, go to the *Scenario* section of the notebook to define the strategy
    of the workflow:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，转到笔记本的“场景”部分以定义工作流程的策略：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The parameters will determine the behavior of the three pipelines in the notebook:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 参数将确定笔记本中三个管道的行为：
- en: '`graph_name="Marketing"`: The prefix (topic) of the files we will read and
    write.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`graph_name="Marketing"`: 我们将读取和写入的文件的（主题）前缀。'
- en: '`db="hub://denis76/marketing01"`: The name of the Deep Lake vector store. You
    can choose the name of the dataset you wish.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`db="hub://denis76/marketing01"`: 深湖向量存储的名称。您可以选择您希望的数据集名称。'
- en: '`vector_store_path = db`: The path to the vector store.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_store_path = db`: 向量存储的路径。'
- en: '`dataset_path = db`: The path to the dataset of the vector store.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_path = db`: 向量存储的数据集路径。'
- en: '`pop_vs=True`: Activates data insertion if `True` and deactivates it if `False`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pop_vs=True`: 如果为`True`，则激活数据插入；如果为`False`，则禁用。'
- en: '`ow=True`: Overwrites the existing dataset if `True` and appends it if `False`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ow=True`: 如果为`True`，则覆盖现有数据集；如果为`False`，则追加。'
- en: 'Then, we can launch the *Pipeline 1: Collecting and preparing the documents*
    section of the notebook. The program will download the URL list generated in the
    previous section of this chapter:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以启动笔记本的“管道1：收集和准备文档”部分。程序将下载本章前一部分生成的URL列表：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It will then read the file and store the URLs in a list named `urls`. The rest
    of the code in the *Pipeline 1: Collecting and preparing the documents* section
    of this notebook follows the same process as the `Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`
    notebook from *Chapter 3*. In *Chapter 3*, the URLs of the web pages were entered
    manually in a list.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它将读取文件并将URL存储在名为`urls`的列表中。本笔记本中“管道1：收集和准备文档”部分的其余代码遵循与第3章中`Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`笔记本相同的流程。在第3章中，网页的URL是手动输入到列表中的。
- en: The code will fetch the content in the list of URLs. The program then cleans
    and prepares the data to populate the Deep Lake vector store.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将从URL列表中获取内容。然后程序清理并准备数据以填充Deep Lake向量存储。
- en: 'Pipeline 2: Creating and populating the Deep Lake vector store'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道2：创建和填充Deep Lake向量存储
- en: The pipeline in this section of `Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb` was
    built with the code of *Pipeline 2* from *Chapter 3*. We can see that by creating
    pipelines as components, we can rapidly repurpose and adapt them to other applications.
    Also, Activeloop Deep Lake possesses in-built default chunking, embedding, and
    upserting functions, making it seamless to integrate various types of unstructured
    data, as in the case of the Wikipedia documents we are upserting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`这一节中的管道是用第3章中的*管道2*的代码构建的。我们可以看到，通过将管道作为组件创建，我们可以快速重新利用和适应它们到其他应用中。此外，Activeloop
    Deep Lake内置了默认的块处理、嵌入和更新功能，使得集成各种类型的不结构化数据变得无缝，正如我们在更新维基百科文档时所做的那样。
- en: 'The output of the `display_record(record_number)` function shows how seamless
    the process is. The output displays the ID and metadata such as the file information,
    the data collected, the text, and the embedded vector:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`display_record(record_number)`函数的输出显示了过程的无缝性。输出显示了ID和元数据，如文件信息、收集的数据、文本和嵌入向量：'
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: And with that, we have successfully repurposed the *Pipeline 2* component of
    *Chapter 3* and can now move on and build the graph knowledge index.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经成功重新利用了第3章的*管道2*组件，现在可以继续并构建图知识索引。
- en: 'Pipeline 3: Knowledge graph index-based RAG'
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道3：基于知识图谱索引的RAG
- en: 'It’s time to create a knowledge graph index-based RAG pipeline and interact
    with it. As illustrated in the following figure, we have a lot of work to do:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候创建一个基于知识图谱索引的RAG管道并与之交互了。如图所示，我们有很多工作要做：
- en: '![A diagram of a graph  Description automatically generated](img/B31169_07_05.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![一个图形的示意图  自动生成的描述](img/B31169_07_05.png)'
- en: 'Figure 7.5: Building knowledge graph-index RAG from scratch'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：从头开始构建知识图谱索引RAG
- en: 'In this section, we will:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将：
- en: Generate the knowledge graph index
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成知识图谱索引
- en: Display the graph
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示图形
- en: Define the user prompt
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义用户提示
- en: Define the hyperparameters of LlamaIndex’s in-built LLM model
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义LlamaIndex内置LLM模型的超参数
- en: Install the similarity score packages
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装相似度得分包
- en: Define the similarity score functions
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义相似度得分函数
- en: Run a sample similarity comparison between the similarity functions
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行相似度函数之间的样本相似度比较
- en: Re-rank the output vectors of an LLM response
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排序LLM响应的输出向量
- en: Run evaluation samples and apply metrics and human feedback scores
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行评估样本并应用指标和人工反馈得分
- en: Run metric calculations and display them
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行指标计算并显示它们
- en: Let’s go through these steps and begin by generating the knowledge graph index.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按步骤进行，并首先生成知识图谱索引。
- en: Generating the knowledge graph index
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成知识图谱索引
- en: We will create a knowledge graph index from a set of documents using the `KnowledgeGraphIndex`
    class from the `llama_index.core` module. We will also time the index creation
    process to evaluate performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`llama_index.core`模块中的`KnowledgeGraphIndex`类从一个文档集中创建知识图谱索引。我们还将计时索引创建过程以评估性能。
- en: 'The function begins by recording the start time with `time.time()`. In this
    case, measuring the time is important because it takes quite some time to create
    the index:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先使用`time.time()`记录开始时间。在这种情况下，测量时间很重要，因为创建索引需要相当长的时间：
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now create a `KnowledgeGraphIndex` with embeddings using the `from_documents`
    method. The function uses the following parameters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用`from_documents`方法创建一个基于嵌入的`KnowledgeGraphIndex`。该函数使用以下参数：
- en: '`documents` is the set of documents to index'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`documents`是要索引的文档集'
- en: '`max_triplets_per_chunk` is set to 2, limiting the number of triplets per chunk
    to optimize memory usage and processing time'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_triplets_per_chunk`设置为2，限制每个块中的三元组数量以优化内存使用和处理时间'
- en: '`include_embeddings` is set to `True`, indicating that embeddings should be
    included'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_embeddings`设置为`True`，表示应包含嵌入'
- en: 'The graph index is thus created in a few lines of code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，知识图谱索引仅用几行代码就创建完成了：
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The timer is stopped and the creation time is measured:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 停止计时器并测量创建时间：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output displays the time:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了时间：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The graph type is displayed:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 显示图形类型：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output confirms the knowledge graph index class:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认了知识图谱索引类：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will now set up a query engine for our knowledge graph index and configure
    it to manage similarity, response temperature, and output length parameters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将设置我们的知识图谱索引的查询引擎，并配置它来管理相似度、响应温度和输出长度参数：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The parameters will determine the behavior of the query engine:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数将决定查询引擎的行为：
- en: '`k=3` sets the number of top similar results to take into account.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k=3`设置考虑的前N个最相似结果的数目。'
- en: '`temp=0.1` sets the temperature parameter, controlling the randomness of the
    query engine’s response generation. The lower it is, the more precise it is; the
    higher it is, the more creative it is.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temp=0.1`设置温度参数，控制查询引擎响应生成的随机性。它越低，越精确；越高，越有创造性。'
- en: '`mt=1024` sets the maximum number of tokens for the output, defining the length
    of the generated responses.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mt=1024`设置输出中最大令牌数，定义生成响应的长度。'
- en: 'The query engine is then created with the parameters we defined:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用我们定义的参数创建查询引擎：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The graph index and query engine are ready. Let’s display the graph.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图索引和查询引擎已就绪。让我们显示图形。
- en: Displaying the graph
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显示图形
- en: 'We will create a graph instance, `g`, with `pyvis.network`, a Python library
    used for creating interactive network visualizations. The displayed parameters
    are similar to the ones we defined in the *Building graphs from trees* section
    of this chapter:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pyvis.network`创建一个图实例，`g`，这是一个用于创建交互式网络可视化的Python库。显示的参数与我们在这章的*从树构建图*部分定义的类似：
- en: '[PRE30]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'A directed graph has been created, and now we will save it in an HTML file
    to display it for further use:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 已创建一个有向图，现在我们将将其保存为HTML文件以供进一步使用：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `graph_name` was defined at the beginning of the notebook, in the *Scenario*
    section. We will now display the graph in the notebook as an HTML file:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph_name`在笔记本的*场景*部分定义。现在我们将以HTML文件的形式在笔记本中显示该图：'
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can now download the file to display it in your browser to interact with
    it. You can also visualize it in the notebook, as shown in the following figure:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以下载文件并在浏览器中显示它以与之交互。您还可以在笔记本中可视化它，如图下所示：
- en: '![A diagram of marketing strategy  Description automatically generated](img/B31169_07_06.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![营销策略图解 描述自动生成](img/B31169_07_06.png)'
- en: 'Figure 7.6: The knowledge graph'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：知识图谱
- en: We are all set to interact with the knowledge graph index.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好与知识图谱索引进行交互。
- en: Interacting with the knowledge graph index
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与知识图谱索引交互
- en: 'Let’s now define the functionality we need to execute the query, as we have
    done in *Chapter 3* in the *Pipeline 3: Index-based RAG* section:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在定义执行查询所需的功能，就像我们在*第3章*的*管道3：基于索引的RAG*部分中所做的那样：
- en: '`execute_query` is the function we created that will execute the query: `response
    = graph_query_engine.query(user_input)`. It also measures the time it takes.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`execute_query`是我们创建的执行查询的函数：`response = graph_query_engine.query(user_input)`。它还测量所需时间。'
- en: '`user_query="What is the primary goal of marketing for the consumer market?"`,
    which we will use to make the query.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_query="What is the primary goal of marketing for the consumer market?"`，我们将用它来执行查询。'
- en: '`response = execute_query(user_query)`, which is encapsulated in the request
    code and displays the response.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response = execute_query(user_query)`，这是封装在请求代码中并显示响应的。'
- en: 'The output provides the best vectors that we created with the Wikipedia data
    with the time measurement:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了我们使用维基百科数据创建的最佳向量，并附有时间测量：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We will now install similarity score packages and define the similarity calculation
    functions we need.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将安装相似度评分包并定义我们需要的相似度计算函数。
- en: Installing the similarity score packages and defining the functions
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装相似度评分包和定义函数
- en: 'We will first retrieve the Hugging Face token from the **Secrets** tab on Google
    Colab, where it was stored in the settings of the notebook:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从Google Colab上的**密钥**选项卡中检索Hugging Face令牌，它在笔记本的设置中被存储：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In August 2024, the token is optional for Hugging Face’s `sentence-transformers`.
    You can ignore the message and comment the code. Next, we install `sentence-transformers`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 到2024年8月，Hugging Face的`sentence-transformers`令牌是可选的。您可以忽略消息并注释代码。接下来，我们安装`sentence-transformers`：
- en: '[PRE35]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We then create a cosine similarity function with embeddings:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着创建一个使用嵌入的余弦相似度函数：
- en: '[PRE36]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We import the libraries we need:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入所需的库：
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We have a similarity function and can use it for re-ranking.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个相似度函数，并且可以使用它进行重新排序。
- en: Re-ranking
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新排序
- en: 'In this section, the program re-ranks the response of a query by reordering
    the top results to select other, possibly better, ones:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，程序通过重新排序顶级结果来重新排序查询的响应，以选择其他可能更好的结果：
- en: '`user_query=" Which experts are often associated with marketing theory?"` represents
    the query we are making.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_query=" Which experts are often associated with marketing theory?"` 表示我们正在进行的查询。'
- en: '`start_time = time.time()` records the start time for the query execution.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_time = time.time()` 记录查询执行的开始时间。'
- en: '`response = execute_query(user_query)` executes the query.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response = execute_query(user_query)` 执行查询。'
- en: '`end_time = time.time()` stops the timer, and the query execution time is displayed.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_time = time.time()` 停止计时器，并显示查询执行时间。'
- en: '`for idx, node_with_score in enumerate(response.source_nodes)` iterates through
    the response to retrieve all the nodes in the response.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`for idx, node_with_score in enumerate(response.source_nodes)` 遍历响应以检索响应中的所有节点。'
- en: '`similarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)`
    calculates the similarity score between the user query and the text in the nodes
    retrieved from the response. All the comparisons are displayed.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`similarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)`
    计算用户查询与从响应中检索到的节点中的文本之间的相似度得分。所有比较都显示出来。'
- en: '`best_score=similarity_score3` stores the best similarity score found.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_score=similarity_score3` 存储找到的最佳相似度得分。'
- en: '`print(textwrap.fill(str(best_text), 100))` displays the best re-ranked result.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print(textwrap.fill(str(best_text), 100))` 显示最佳重新排序结果。'
- en: 'The initial response for the `user_query` `"Which experts are often associated
    with marketing theory?"` was:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`user_query` `"Which experts are often associated with marketing theory?"`的初始响应是：
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The response is acceptable. However, the re-ranked response goes deeper and
    mentions the names of marketing experts (highlighted in bold font):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是可接受的。然而，重新排序的响应更深入，并提到了营销专家的名字（以粗体字体突出显示）：
- en: '[PRE39]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The re-ranked response is longer and contains raw document content instead
    of the summary provided by LlamaIndex’s LLM query engine. The original query engine
    response is better from an LLM perspective. However, it isn’t easy to estimate
    what an end-user will prefer. Some users like short answers, and some like long
    documents. We can imagine many other ways of re-ranking documents, such as modifying
    the prompt, adding documents, and deleting documents. We can even decide to fine-tune
    an LLM, as we will do in *Chapter 9*, *Empowering AI Models: Fine-Tuning RAG Data
    and Human Feedback*. We can also introduce human feedback scores as we did in
    *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*, because, in
    many cases, mathematical metrics will not capture the accuracy of a response (writing
    fiction, long answers versus short input, and other complex responses). But we
    need to try anyway!'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排序的响应更长，包含原始文档内容而不是LlamaIndex的LLM查询引擎提供的摘要。从LLM的角度来看，原始查询引擎的响应更好。然而，很难估计最终用户会喜欢什么。一些用户喜欢简短的答案，而一些用户喜欢长文档。我们可以想象许多其他重新排序文档的方式，例如修改提示、添加文档和删除文档。我们甚至可以决定微调一个LLM，就像我们在第9章*赋予AI模型能力：微调RAG数据和人类反馈*中所做的那样。我们还可以像在第5章*通过专家人类反馈提升RAG性能*中那样引入人类反馈评分，因为在许多情况下，数学指标无法捕捉到响应的准确性（写作小说、长答案与短输入以及其他复杂响应）。但无论如何，我们都需要尝试！
- en: Let’s perform some of the possible metrics for the examples we are going to
    run.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行将要运行的示例的一些可能的指标。
- en: Example metrics
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例指标
- en: 'To evaluate the knowledge graph index’s query engine, we will run ten examples
    and keep track of the scores. `rscores` keeps track of human feedback scores while
    `scores=[]` keeps track of similarity function scores:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估知识图谱索引的查询引擎，我们将运行十个示例并跟踪得分。`rscores`跟踪人类反馈得分，而`scores=[]`跟踪相似度函数得分：
- en: '[PRE40]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The number of examples can be increased as much as necessary depending on the
    needs of a project. Each of the ten examples has the same structure:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 根据项目的需求，可以增加尽可能多的示例。每个十个示例都有相同的结构：
- en: '`user_query`, which is the input text for the query engine'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_query`，这是查询引擎的输入文本'
- en: '`elapsed_time`, which is the result of the time measurement of the system’s
    response'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elapsed_time`，这是系统响应的时间测量结果'
- en: '`response = execute_query(user_query)` executes the query'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response = execute_query(user_query)` 执行查询'
- en: 'The user query and output are the same as in the example used for the re-ranking
    function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 用户查询和输出与用于重新排序函数的示例相同：
- en: '[PRE41]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'However, this time, we will run a similarity function and also ask a human
    for a score:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次，我们将运行一个相似度函数，并要求人类给出一个评分：
- en: '[PRE42]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this function:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中：
- en: '`text1` is the query engine’s response.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text1` 是查询引擎的响应。'
- en: '`text2` is the user query.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text2` 是用户查询。'
- en: '`similarity_score3` is the cosine similarity score.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`similarity_score3` 是余弦相似度得分。'
- en: '`scores.append(similarity_score3)` appends the similarity score to scores.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scores.append(similarity_score3)` 将相似度得分添加到 `scores` 中。'
- en: '`human_feedback` is the human similarity evaluation. We could replace this
    score with a document as we did in *Chapter 5*, *Boosting RAG Performance with
    Expert Human Feedback*, or we could replace the human score with a human text
    response, which will become the ground truth. In both cases, the similarity score
    is recalculated with human feedback content.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`human_feedback` 是人类相似度评估。我们可以像在 *第 5 章* 中所做的那样，用文档替换这个分数，或者我们可以用人类文本响应替换人类分数，这将成为事实。在两种情况下，相似度分数都会根据人类反馈内容重新计算。'
- en: '`rscores.append(human_feedback)` appends the human score to `rscores`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rscores.append(human_feedback)` 将人类分数添加到 `rscores` 中。'
- en: Let’s review a few of the ten examples’ outputs and add a comment at the end
    of each one.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下十个示例输出的几个，并在每个示例的末尾添加注释。
- en: LLMs are stochastic algorithms. As such, the responses and scores may vary from
    one run to another.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 是随机算法。因此，响应和得分可能因运行而异。
- en: '**Example 1**:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 1**:'
- en: '**User query**: `Which experts are often associated with marketing theory?`'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户查询**: `哪些专家通常与营销理论相关联？`'
- en: '**Response**: Psychologists, cultural anthropologists, and other experts in
    behavioral sciences are often associated with marketing theory.'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应**: 心理学家、文化人类学家以及其他行为科学专家通常与营销理论相关联。'
- en: '**Cosine similarity score**: `0.809`'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦相似度得分**: `0.809`'
- en: '**Human feedback**: `0.75`'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈**: `0.75`'
- en: '**Comment**: The response is acceptable, but it could be more specific and
    mention the names of experts. However, the prompt is ambiguous and only mentions
    experts in general.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注释**: 响应是可以接受的，但可以更加具体，并提及专家的名字。然而，提示是模糊的，只提到了专家的一般情况。'
- en: '**Example 3**:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 3**:'
- en: '**User query**: `What is the difference between B2B and B2C?`'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户查询**: `B2B和B2C之间的区别是什么？`'
- en: '**Response**: B2B businesses sell products and services to other companies,
    while B2C businesses sell directly to customers.'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应**: B2B 企业向其他公司销售产品和服务，而 B2C 企业直接向客户销售。'
- en: '**Cosine Similarity score**: `0.760`'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦相似度得分**: `0.760`'
- en: '**Human feedback**: `0.8`'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈**: `0.8`'
- en: '**Comment**: The response is precise, but in some cases, users like examples.'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注释**: 响应是精确的，但在某些情况下，用户喜欢示例。'
- en: '**Example 7**:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 7**:'
- en: '**User query**: `What commodity programs does the Agricultural Marketing Service
    (AMS) maintain?`'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户查询**: `农业营销服务局（AMS）维护哪些商品项目？`'
- en: '**Response**: The **Agricultural Marketing Service** (**AMS**) maintains programs
    in five commodity areas: cotton and tobacco, dairy, fruit and vegetable, livestock
    and seed, and poultry.'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应**: 农业营销服务局（**AMS**）在五个商品领域维护项目：棉花和烟草、乳制品、水果和蔬菜、牲畜和种子以及家禽。'
- en: '**Cosine Similarity score**: `0.904`'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦相似度得分**: `0.904`'
- en: '**Human feedback**: `0.9`'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈**: `0.9`'
- en: '**Comment**: This response is accurate and interesting because the information
    is contained in a page linked to the main page. Thus, this is information from
    a linked page to the main page. We could ask Wikipedia to search the links of
    all the linked pages to the main page and go down several levels. However, the
    main information we are looking for may be diluted in less relevant data. The
    decision on the scope of the depth of the data depends on the needs of each project.'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注释**: 这个响应是准确且有趣的，因为信息包含在链接到主页的页面中。因此，这是从链接页面到主页的信息。我们可以要求维基百科搜索所有链接到主页的链接页面，并深入几层。然而，我们主要寻找的信息可能被不太相关的数据稀释。关于数据深度范围的决定取决于每个项目的需求。'
- en: We will now perform metric calculations on the cosine similarity scores and
    the human feedback scores.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对余弦相似度得分和人类反馈得分进行度量计算。
- en: Metric calculation and display
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 度量计算和显示
- en: 'The cosine similarity scores of the examples are stored in `scores`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的余弦相似度得分存储在 `scores` 中：
- en: '[PRE43]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The ten scores are displayed:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了十个得分：
- en: '[PRE44]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We could expand the evaluations to as many other examples, depending on the
    needs of each project. The human feedback scores for the same examples are stored
    in `rscores`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据每个项目的需求将评估扩展到更多其他示例。相同示例的人类反馈得分存储在 `rscores` 中：
- en: '[PRE45]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The ten human feedback scores are displayed:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了十个人类反馈得分：
- en: '[PRE46]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We apply metrics to evaluate the responses:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用度量来评估响应：
- en: '[PRE47]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Each metric can provide several insights. Let’s go through each of them and
    the outputs obtained:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 每个指标都可以提供几个见解。让我们逐一分析它们和获得的输出：
- en: '**Central tendency (mean, median)** gives us an idea of what a typical score
    looks like.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中心趋势（平均值、中位数）**给我们一个典型分数的外观。'
- en: '**Variability (standard deviation, variance, range, IQR)** tells us how spread
    out the scores are, indicating the consistency or diversity of the data.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变异性（标准差、方差、范围、四分位距）**告诉我们分数的分布情况，表明数据的稳定性或多样性。'
- en: '**Extremes (minimum, maximum)** show the bounds of our dataset.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**极值（最小值、最大值）**显示了数据集的范围。'
- en: '**Distribution (percentiles)** provides insights into how scores are distributed
    across the range of values.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布（百分位数）**提供了关于分数在值范围内的分布的见解。'
- en: 'Let’s go through these metrics calculated from the cosine similarity scores
    and the human feedback scores and display their outputs:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一分析从余弦相似度分数和人工反馈分数计算出的这些指标，并显示它们的输出：
- en: '**Mean (average)**:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**平均值（平均）**:'
- en: '**Definition**: The mean is the sum of all the scores divided by the number
    of scores.'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 平均值是所有分数的总和除以分数的数量。'
- en: '**Purpose**: It gives us the central value of the data, providing an idea of
    the typical score.'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它给出了数据的中心值，提供了一个典型得分的概念。'
- en: '**Calculation**:'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**:'
- en: '![](img/B31169_07_001.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31169_07_001.png)'
- en: '**Output**: `Mean: 0.68`'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `平均值: 0.68`'
- en: '**Median**:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**中位数**:'
- en: '**Definition**: The median is the middle value when the scores are ordered
    from smallest to largest.'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 中位数是在分数按从小到大排序时中间的值。'
- en: '**Purpose**: It provides the central point of the dataset and is less affected
    by extreme values (outliers) compared to the mean.'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它提供了数据集的中心点，并且与平均值相比，受极端值（异常值）的影响较小。'
- en: '**Output**: `Median: 0.71`'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `中位数: 0.71`'
- en: '**Standard deviation**:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标准差**:'
- en: '**Definition**: The standard deviation measures the average amount by which
    each score differs from the mean.'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 标准差衡量每个分数与平均值差异的平均量。'
- en: '**Purpose**: It gives an idea of how spread out the scores are around the mean.
    A higher value indicates more variability.'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它给出了分数围绕平均值分布的概念。值越高，表示变异性越大。'
- en: '**Calculation**:'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**:'
- en: '![](img/B31169_07_002.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31169_07_002.png)'
- en: '**Output**: `Standard Deviation: 0.15`'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `标准差: 0.15`'
- en: '**Variance**:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**方差**:'
- en: '**Definition**: The variance is the square of the standard deviation.'
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 方差是标准差的平方。'
- en: '**Purpose**: It also measures the spread of the scores, showing how much they
    vary from the mean.'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它也衡量分数的分布，显示它们与平均值的差异程度。'
- en: '**Output**: `Variance: 0.02`'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `方差: 0.02`'
- en: '**Minimum**:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最小值**:'
- en: '**Definition**: The minimum is the smallest score in the dataset.'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 最小值是数据集中的最小分数。'
- en: '**Purpose**: It tells us the lowest value.'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它告诉我们最低值。'
- en: '**Output**: `Minimum: 0.45`'
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `最小值: 0.45`'
- en: 'Maximum:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最大值:'
- en: '**Definition**: The maximum is the largest score in the dataset.'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 最大值是数据集中的最大分数。'
- en: '**Purpose**: It tells us the highest value.'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它告诉我们最高值。'
- en: '**Output**: `Maximum: 0.90`'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `最大值: 0.90`'
- en: '**Range**:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**范围**:'
- en: '**Definition**: The range is the difference between the maximum and minimum
    scores.'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 范围是最大值和最小值之间的差异。'
- en: '**Purpose**: It shows the span of the dataset from the lowest to the highest
    value.'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它显示了数据集从最低到最高值的范围。'
- en: '**Calculation**:'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**:'
- en: '*Range* = *Maximum* - *Minimum*'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*范围* = *最大值* - *最小值*'
- en: '**Output**: `Range: 0.46`'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `范围: 0.46`'
- en: '**25**^(th) **Percentile (Q1)**:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**第25百分位数（Q1）**:'
- en: '**Definition**: The 25^(th) percentile is the value below which 25% of the
    scores fall.'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 第25百分位数是低于该值的25%的分数。'
- en: '**Purpose**: It provides a point below which a quarter of the data lies.'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它提供了一个数据四分之一以下的数据点。'
- en: '**Output**: `25th Percentile (Q1): 0.56`'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `第25百分位数（Q1）: 0.56`'
- en: '**75**^(th) **Percentile (Q3)**:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**第75百分位数（Q3）**:'
- en: '**Definition**: The 75^(th) percentile is the value below which 75% of the
    scores fall.'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 第75百分位数是低于该值的75%的分数。'
- en: '**Purpose**: It gives a point below which three-quarters of the data lies.'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**: 它给出了低于该值的数据占四分之三的点。'
- en: '**Output**: `75th Percentile (Q3): 0.80`'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `第75百分位数（Q3）: 0.80`'
- en: '**Interquartile Range (IQR)**:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**四分位距（IQR）**:'
- en: '**Definition**: The IQR is the range between the 25^(th) percentile (Q1) and
    the 75^(th) percentile (Q3).'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义**: 四分位距是第25百分位数（Q1）和第75百分位数（Q3）之间的范围。'
- en: '**Purpose**: It measures the middle 50% of the data, providing a sense of the
    data’s spread without being affected by extreme values.'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的**：它测量数据的中间 50%，提供对数据分布的感觉，而不会受到极端值的影响。'
- en: '**Calculation**:'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**：'
- en: '*IQR* = *Q3* – *Q1*'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*IQR* = *Q3* – *Q1*'
- en: '**Output**: `Interquartile Range (IQR): 0.24`'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：`四分位距 (IQR)：0.24`'
- en: We have built a knowledge-graph-based RAG system, interacted with it, and evaluated
    it with some examples and metrics. Let’s sum up our journey.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了一个基于知识图谱的 RAG 系统，与之互动，并使用一些示例和指标对其进行评估。让我们总结我们的旅程。
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**摘要**'
- en: In this chapter, we explored the creation of a scalable knowledge-graph-based
    RAG system using the Wikipedia API and LlamaIndex. The techniques and tools developed
    are applicable across various domains, including data management, marketing, and
    any field requiring organized and accessible data retrieval.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用维基百科 API 和 LlamaIndex 创建可扩展的知识图谱 RAG 系统的方法。开发的技术和工具适用于各个领域，包括数据管理、营销以及任何需要组织化和可访问数据检索的领域。
- en: Our journey began with data collection in *Pipeline 1*. This pipeline focused
    on automating the retrieval of Wikipedia content. Using the Wikipedia API, we
    built a program to collect metadata and URLs from Wikipedia pages based on a chosen
    topic, such as marketing. In *Pipeline 2*, we created and populated the Deep Lake
    vector store. The retrieved data from *Pipeline 1* was embedded and upserted into
    the Deep Lake vector store. This pipeline highlighted the ease of integrating
    vast amounts of data into a structured vector store, ready for further processing
    and querying. Finally, in *Pipeline 3*, we introduced knowledge graph index-based
    RAG. Using LlamaIndex, we automatically built a knowledge graph index from the
    embedded data. This index visually mapped out the relationships between different
    pieces of information, providing a semantic overview of the data. The knowledge
    graph was then queried using LlamaIndex’s built-in language model to generate
    optimal responses. We also implemented metrics to evaluate the system’s performance,
    ensuring accurate and efficient data retrieval.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程始于 *Pipeline 1* 中的数据收集。该管道专注于自动化检索维基百科内容。使用维基百科 API，我们构建了一个程序，根据选定的主题（如营销）从维基百科页面收集元数据和
    URL。在 *Pipeline 2* 中，我们创建并填充了 Deep Lake 向量存储。从 *Pipeline 1* 中检索的数据被嵌入并更新到 Deep
    Lake 向量存储中。该管道强调了将大量数据集成到结构化向量存储中的简便性，以便进行进一步的处理和查询。最后，在 *Pipeline 3* 中，我们引入了基于知识图谱索引的
    RAG。使用 LlamaIndex，我们自动从嵌入的数据中构建知识图谱索引。该索引以视觉方式映射出不同信息片段之间的关系，提供了数据的语义概述。然后使用 LlamaIndex
    内置的语言模型查询知识图谱以生成最佳响应。我们还实施了指标来评估系统的性能，确保准确高效的数据检索。
- en: By the end of this chapter, we had constructed a comprehensive, automated RAG-driven
    knowledge graph system capable of collecting, embedding, and querying vast amounts
    of Wikipedia data with minimal human intervention. This journey showed the power
    and potential of combining multiple AI tools and models to create an efficient
    pipeline for data management and retrieval. You are now all set to implement knowledge
    graph-based RAG systems in real-life projects. In the next chapter, we will learn
    how to implement dynamic RAG for short-term usage.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们已经构建了一个综合的、自动的 RAG 驱动的知识图谱系统，该系统能够以最小的人为干预收集、嵌入和查询大量维基百科数据。这段旅程展示了结合多个
    AI 工具和模型以创建高效数据管理和检索管道的强大和潜力。现在你们都已经准备好在现实项目中实施基于知识图谱的 RAG 系统。在下一章中，我们将学习如何实现用于短期使用的动态
    RAG。
- en: Questions
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions with yes or no:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 用是或否回答以下问题：
- en: Does the chapter focus on building a scalable knowledge-graph-based RAG system
    using the Wikipedia API and LlamaIndex?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 章节是否专注于使用维基百科 API 和 LlamaIndex 构建可扩展的知识图谱 RAG 系统？
- en: Is the primary use case discussed in the chapter related to healthcare data
    management?
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章讨论的主要用例是否与医疗数据管理相关？
- en: Does *Pipeline 1* involve collecting and preparing documents from Wikipedia
    using an API?
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Pipeline 1* 是否涉及使用 API 从维基百科收集和准备文档？'
- en: Is Deep Lake used for creating a relational database in *Pipeline 2*?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *Pipeline 2* 中是否使用 Deep Lake 创建关系数据库？
- en: Does *Pipeline 3* utilize LlamaIndex to build a knowledge graph index?
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Pipeline 3* 是否利用 LlamaIndex 构建知识图谱索引？'
- en: Is the system designed to only handle a single specific topic, such as marketing,
    without flexibility?
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统是否设计为仅处理单个特定主题，例如营销，而没有灵活性？
- en: Does the chapter describe how to retrieve URLs and metadata from Wikipedia pages?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 章节是否描述了如何从维基百科页面检索 URL 和元数据？
- en: Is a GPU required to run the pipelines described in the chapter?
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行章节中描述的管道需要 GPU 吗？
- en: Does the knowledge graph index visually map out relationships between pieces
    of data?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 知识图谱索引是否以视觉方式映射出数据片段之间的关系？
- en: Is human intervention required at every step to query the knowledge graph index?
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在查询知识图谱索引的每一步都需要人工干预吗？
- en: References
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Wikipedia API GitHub repository: [https://github.com/martin-majlis/Wikipedia-API](https://github.com/martin-majlis/Wikipedia-API)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科 API GitHub 仓库：[https://github.com/martin-majlis/Wikipedia-API](https://github.com/martin-majlis/Wikipedia-API)
- en: 'PyVis Network: *Interactive Network Visualization in Python*.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyVis 网络：*Python 中的交互式网络可视化*。
- en: Further reading
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Hogan, A., Blomqvist, E., Cochez, M., et al. *Knowledge Graphs*. `arXiv:2003.02320`
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hogan, A., Blomqvist, E., Cochez, M., et al. *知识图谱*。`arXiv:2003.02320`
- en: Join our community on Discord
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/rag](https://www.packt.link/rag)'
- en: '![](img/QR_Code50409000288080484.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code50409000288080484.png)'
