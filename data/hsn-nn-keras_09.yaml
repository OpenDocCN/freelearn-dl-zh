- en: Reinforcement Learning with Deep Q-Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络强化学习
- en: In the last chapter, we saw how recursive loops, information gates, and memory
    cells can be used to model complex time-dependent signals with neural networks.
    More specifically, we saw how the **Long Short-Term Memory** (**LSTM**) architecture
    leverages these mechanics to preserve prediction errors and backpropagate them
    over increasingly long time steps. This allowed our system to inform predictions
    using both short-term (that is, from information relating to the immediate environment)
    and long-term representations (that is, from information pertaining to the environment
    that was observed long ago).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了递归循环、信息门控和记忆单元如何被用来通过神经网络建模复杂的时间相关信号。更具体地说，我们看到**长短期记忆**（**LSTM**）架构如何利用这些机制来保留预测误差，并将其在越来越长的时间步长上反向传播。这使得我们的系统能够利用短期（即来自与即时环境相关的信息）和长期表示（即来自于很久以前观察到的环境信息）来提供预测。
- en: The beauty of the LSTM lies in the fact that it is able to learn and preserve
    useful representations over very large periods of time (up to a thousand time
    steps). By maintaining a constant error flow through the architecture, we can
    implement a mechanism that allows our network to learn complex cause-and-effect
    patterns, embedded in the reality we face everyday. Indeed, the problem of educating
    computers on matters of cause and effect has presented itself to be quite a challenge
    so far in the field of **Artificial Intelligence** (**AI**). As it happens, real-world
    environments are heavily populated with sparse and time-delayed rewards, with
    increasingly complex sets of actions corresponding to these rewards. Modeling
    optimal behavior in such circumstances involves discovering sufficient information
    about a given environment, along with the possible set of actions and respective
    rewards to make relevant predictions. As we know, encoding such complex cause-and-effect
    relations can be difficult, even for humans. We often succumb to our irrational
    desires without perusing some pretty beneficial cause and effect relations. Why?
    Simply put, the actual cause and effect relationship may not correspond to our
    internal valuation of the situation. We may be acting upon different reward signals,
    spread out through time, each influencing our aggregate decision.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的美妙之处在于，它能够在非常长的时间跨度内（最长可达一千个时间步）学习并保留有用的表示。通过在架构中保持恒定的误差流，我们可以实现一种机制，使得我们的网络能够学习复杂的因果模式，这些模式嵌入在我们日常面对的现实中。实际上，教育计算机理解因果关系的问题，至今仍是**人工智能**（**AI**）领域中的一个巨大挑战。现实世界的环境充斥着稀疏且具有时间延迟的奖励，且随着奖励对应的动作集的复杂度增加，问题变得更加复杂。在这种情况下，建模最佳行为涉及到发现关于给定环境的足够信息，以及可能的动作集合和相应的奖励，以便做出相关预测。正如我们所知，编码这种复杂的因果关系对于人类来说也是一项困难的任务。我们常常在没有考虑到一些相当有益的因果关系时，屈服于我们的非理性欲望。为什么？简单来说，实际的因果关系可能与我们对情况的内部评估不相符。我们可能是在不同的奖励信号驱动下行动，这些信号跨越时间影响着我们的整体决策。
- en: The degree to which we act upon certain reward signals is highly variant. This
    is based upon the specific individual and determined by a complex combination
    of genetic makeup and environmental factors that are faced by a given individual.
    In some ways, it is embedded in our nature. Some of us are just, inherently, a
    little more swayed by short-term rewards (such as delicious snacks or entertaining
    movies) over long-term rewards (such as having a healthy body or using our time
    efficiently). This isn't so bad, right? Well, not necessarily. Different environments
    require different balances of short and long-term considerations to be able to
    succeed. Given the diversity of environments a human may encounter (in individual
    sense, as well as the broad, species sense), it is not a surprise that we observe
    such a variety in the interpretation of reward signals among different individuals.
    On a grand scale, evolution is simply maximizing our chances to survive as many
    environments that this reality may impress upon us. However, as well will shortly
    see, this may have consequences for certain individuals (and perhaps some greedy
    machines) on the minute scale of events.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对某些奖励信号的反应程度差异很大。这是基于特定个体的情况，并由基因组成和环境因素的复杂组合决定的。在某些方面，这已经深深植根于我们的天性中。我们之中有些人天生更容易被短期奖励（如美味的小吃或娱乐电影）所吸引，而不是长期奖励（如保持健康的身体或高效利用时间）。这也不是特别糟糕，对吧？其实不一定。不同的环境需要不同的短期与长期考虑平衡才能成功。鉴于人类可能遇到的环境多种多样（无论是个体层面还是物种层面），我们在不同个体之间观察到如此多样的奖励信号解读方式也就不足为奇。从宏观角度来看，进化的目标只是最大化我们在各种环境中生存的机会，适应现实世界带来的挑战。然而，正如我们很快会看到的，这对某些个体（甚至是一些贪婪的机器）在微观层面的事件中可能会带来后果。
- en: On reward and gratification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于奖励与满足感
- en: Interestingly, a group of Stanford researchers showed (Marshmallow experiment, in
    the 1970s, led by psychologist Walter Mischel) how the capability for individuals
    to delay short term gratification was correlated with more successful outcomes
    in the long term. Essentially, these researchers called upon children and observed
    their behavior once they were presented with a set of choices. The children were
    given two choices that determined how many total marshmallows they could receive
    during an interaction. They could either choose to cash out one marshmallow on
    the spot, or cash out two marshmallows if they chose to wait it out for 15 minutes.
    This experiment gave keen insight into how interpreting reward signals are beneficial
    or detrimental for performing in a given environment as the subjects who chose
    two marshmallows turned out to be more successful on average over the span of
    their lives. It turns out that delaying gratification could be a paramount part
    of maximizing actions that are more beneficial over the long term. Many have even
    pointed out how concepts such as religion could be the collective manifestation
    of delaying short term gratification (that is, do not steal), for favorable, long-term
    consequences (such as eventually ascending to heaven).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，一组斯坦福大学的研究人员（由心理学家沃尔特·米歇尔领导，进行的棉花糖实验，发生在1970年代）展示了个体延迟短期满足感的能力与长期成功之间的相关性。简而言之，这些研究人员邀请了一些孩子，观察他们在面对一组选择时的行为。这些孩子有两个选择，决定他们在互动中能够获得多少棉花糖。他们可以选择立即拿到一个棉花糖，或者选择等待15分钟后拿到两个棉花糖。这个实验深入揭示了如何解读奖励信号对在特定环境中的表现是有益还是有害的，结果显示，选择等待两个棉花糖的孩子，平均一生中表现得更加成功。事实证明，延迟满足感可能是最大化长期更有益的行动的关键部分。许多人甚至指出，像宗教这样的概念可能就是延迟短期满足感的集体体现（即：不偷窃），以换取长期的好处（例如最终升天）。
- en: A new way of examining learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种新的学习检视方式
- en: So, it would seem that we develop an internal sense of what actions to take
    and how this may affect future outcomes. We have mechanisms that enable us to
    tune these senses through environmental interaction, observing what kind of rewards
    we get for different actions, over very long periods of time. This appears to
    be true for humans, as well as most living things that inhabit our planet, including
    not only fauna but also flora. Even plants are optimizing some sort of energy
    score throughout the day as they turn their leaves and branches to capture the
    sunlight that's required for them to live. So, what is this mechanism that permits
    these organisms to model optimal outcomes? How do these biological systems keep
    track of the environment and execute timely and precise maneuvers to favorable
    ends? Well, perhaps a branch of behavioral psychological, known as **reinforcement
    theory**, may shine some light on this topic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，似乎我们发展出了对应采取何种行动及其可能影响未来结果的内在感知。我们有机制使我们能够通过环境互动来调整这些感知，观察不同的行为会带来何种奖励，这一过程可能持续很长时间。这一点对于人类以及地球上大多数生物来说都是真实的，包括不仅是动物，甚至是植物。即使是植物，也在全天优化某种能量评分，它们通过转动叶子和枝条来捕捉光线，这是它们生存所必需的。那么，这种机制是什么，能让这些生物体建模出最佳结果？这些生物系统如何追踪环境并执行及时而精确的操作以达到有利的结果呢？或许，**强化理论**这一行为心理学分支能够为这个话题提供一些启示。
- en: Proposed by Harvard psychologist B.F. Skinner, this view defines reinforcement
    as a consequence of an observed interaction between an agent (human, animal, and
    now, computer program) and its environment. The encoding of information from this
    interaction may either strengthen or weaken the likelihood of the agent acting
    in the same way in similar future iterations. In simpler terms, if you walk on
    hot coals, the pain you feel will act as negative reinforcement, decreasing the
    likelihood of you choosing to step on hot coals in the future. Conversely, if
    you rob a bank and get away with it, the thrill and excitement may reinforce this
    action as a more likely one to consider in the future. In fact, Skinner showed
    how you can even train the common pigeon to recognize the difference between words
    of the English language and play games of ping-pong through a simple mechanism
    of designed reinforcement. He showed how exposing the pigeon to enough reward
    signals over a period of time was enough to incentivize the pigeon to pick up
    on the subtle variations between the words it was shown or the movements it was
    asked to perform. Since picking up on these variations represented the difference
    between a full and empty stomach for the pigeon, Skinner was able to influence
    its behavior by incrementally rewarding the pigeon for desirable outcomes. From
    these experiments, he coined the term *operant conditioning*, relating to breaking
    down tasks into increments and then rewarding favorable behavior iteratively.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由哈佛心理学家B.F. 斯金纳提出，这一观点将强化定义为代理（人类、动物，现在也包括计算机程序）与其环境之间观察到的互动的结果。从这种互动中编码的信息可能会增强或削弱代理在未来相似情境中以相同方式行动的可能性。简而言之，如果你走在炙热的煤炭上，你感受到的疼痛将作为负向强化，减少你未来选择再次踏上炙热煤炭的可能性。相反，如果你抢劫银行并成功逃脱，所体验到的刺激和兴奋可能会强化这一行为，使其在未来成为更可能的选择。事实上，斯金纳展示了如何通过简单的强化机制训练普通的鸽子，帮助它区分英语单词的差异，甚至让它玩乒乓球。他展示了，经过一段时间的奖励信号的暴露，足以激励鸽子识别它所看到的单词之间的微妙差异或它被要求执行的动作。由于识别这些差异意味着鸽子是否能够获得饱腹感，斯金纳通过逐步奖励鸽子达到期望的结果，从而影响其行为。从这些实验中，他创造了*操作性条件反射*一词，这一概念与将任务分解成小步骤并逐步奖励有利行为有关。
- en: Today, about half a century later, we operationalize these concepts in the realm
    of **machine learning** (**ML**) to reinforce favorable behavior that a simulated
    agent is to perform in a given environment. This notion is referred to as reinforcement
    learning, and can give rise to complex systems that parallel (and perhaps even
    surpass) our own intellect when performing tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '今天，约半个世纪后，我们在**机器学习**（**ML**）领域将这些概念应用于强化模拟代理在特定环境中执行有利行为。这一概念被称为强化学习，它可以产生复杂的系统，这些系统在执行任务时可能会与我们自己的智力相媲美（甚至超越）。 '
- en: Conditioning machines with reinforcement learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强化学习训练机器
- en: So far in our journey, we have been dealing with simple regression and classification
    tasks. We regressed observations against continuous values (that is, when predicting
    the stock market) and classified features into categorical labels (while conducting
    sentiment analysis). These are two cornerstone activities pertaining to supervised
    ML. We showed a specific target label for each observation our network comes across
    while training. Later on in this book, we will cover some unsupervised learning
    techniques with neural networks by using **Generative Adversarial Networks** (**GANs**)
    and autoencoders. Today, however, we employ neural networks to something quite
    different from these two caveats of learning. This caveat of learning can be named
    **reinforcement learning**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理简单的回归和分类任务。我们根据连续值对观察结果进行回归（例如预测股市）并将特征分类到不同的标签中（例如进行情感分析）。这两项任务是监督学习的基石活动。在训练过程中，我们为每个网络遇到的观察结果显示一个特定的目标标签。稍后在本书中，我们将介绍一些无监督学习技术，使用**生成对抗网络**（**GANs**）和自动编码器。不过，今天我们将神经网络应用于与这两种学习任务截然不同的领域。这种学习任务可以称为**强化学习**。
- en: Reinforcement learning is noticeably distinct from the aforementioned variations
    of ML. Here, we do not explicitly label all possible sequences of actions to all
    possible outcomes in an environment (as in supervised classification)—nor do we
    try and partition our data based on similarity-based distance measures (as in
    unsupervised clustering) to segment optimal actions. Rather, we let the machine
    monitor responses from actions it takes and cumulatively model the maximum possible
    reward as a function of actions over a period of time. In essence, we deal with
    goal-oriented algorithms that learn to achieve complex objectives over given time
    steps. The goal can be to beat the space invaders that are gradually moving down
    the screen, or for a dog-shaped robot in the real world to move from point A to
    B.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与前述的机器学习变种明显不同。在这里，我们并不会明确标注环境中所有可能的动作序列与可能的结果（如监督分类中那样），也不会基于相似性度量来划分数据（如无监督聚类中那样）以寻找最佳动作。相反，我们让机器监控它所采取的行动的反馈，并在一段时间内逐步建模最大可能奖励作为行动的函数。本质上，我们处理的是目标导向的算法，这些算法通过给定的时间步学习实现复杂目标。目标可以是击败逐渐向屏幕下方移动的太空侵略者，或者让一个形似狗的机器人从现实世界中的A点移动到B点。
- en: The credit assignment problem
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用分配问题
- en: Just as our parents reinforced our behavior with treats and rewards, so can
    we reinforce desirable machine actions for given states (or configurations) of
    our environment. This invokes more of a *trial-and-error* approach to learning,
    and recent events have shown how such an approach can produce extremely powerful
    learning systems, opening the door to some very interesting use cases. This considerably
    distinct yet powerful paradigm of learning does bring some complications of its
    own into the picture. Consider the credit assignment problem, for instance. That
    is to say, which of our previous actions are responsible for generating a reward,
    and to what degree? In an environment with sparse, time delayed rewards, many
    actions may occur between some action, which later generated the reward in question.
    It can become very difficult to properly assign due credit to respective actions.
    In the absence of proper credit assignment, our agent is left clueless while evaluating
    different strategies to use when trying to accomplish its goal.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像父母通过奖励和奖励来强化我们的行为一样，我们也可以在给定环境的状态（或配置）下，强化机器的期望行为。这更像是一种*试错*学习方法，近期的事件表明，这种方法可以产生极其强大的学习系统，打开了非常有趣的应用场景的大门。然而，这种明显不同但强大的学习范式也带来了一些自身的复杂性。例如，考虑信用分配问题。也就是说，我们之前的哪些行动为生成奖励负责，负责的程度如何？在奖励稀疏且时延的环境中，许多动作可能发生在某些行动之间，这些行动后来产生了所讨论的奖励。正确地将应得的信用分配给各个行动会变得非常困难。如果没有适当的信用分配，我们的代理在评估不同策略时会毫无头绪，无法达成其目标。
- en: The explore-exploit dilemma
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索-利用困境
- en: Let's assume that our agent even manages to figure out a consistent strategy
    that delivers rewards. What's next? Should they simply stick to that same strategy,
    generating the same reward for eternity? Or rather should they keep trying new
    things all the time? Perhaps, by not exploiting a known strategy, the agent can
    have a chance at a much bigger reward in the future? This is known as the **explore-exploit
    dilemma**, referring to the degree to which agents should explore new strategies
    or exploit known strategies.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的智能体甚至设法找到了一个稳定的策略来获取奖励。那么接下来怎么办？它是否应该一直坚持这个策略，永远生成相同的奖励？还是它应该一直尝试新的方法？也许，通过不利用已知的策略，智能体可以在未来获得更大的奖励？这就是所谓的**探索与利用困境**，指的是智能体在探索新策略或利用已知策略之间的权衡。
- en: At the extreme, we can better appreciate the explore-exploit dilemma by understanding
    how it can be detrimental to rely on known strategies for immediate reward in
    the long run. Experiments with rats, for example, have shown that these animals
    will starve themselves to death if given a mechanism to trigger the release of
    dopamine (a neurotransmitter that's responsible for regulating our reward system).
    Clearly, starving was not the right move in the long run, however ecstatic the
    finale may have been. Yet since the rat exploits a simple strategy that consistently
    triggers a reward signal, the prospect of long-term rewards (such as staying alive)
    were not explored. So, how can we compensate for the fact that our environment
    may present better opportunities later on by foregoing current ones? Somehow,
    we have to make our agent understand this notion of delayed gratification if we
    want it to aptly solve complex environments. Soon, we will see how deep reinforcement
    learning attempts to solve such problems, giving rise to even more complex and
    powerful systems that some may even call devilishly sharp.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 极端情况下，我们可以通过理解依赖已知策略来获得即时奖励的长期不利性，更好地理解探索与利用的困境。例如，通过对老鼠的实验表明，如果给这些动物一个触发多巴胺释放机制的方式，它们会饿死自己（多巴胺是一种负责调节我们奖励系统的神经递质）。显然，尽管结局可能让老鼠非常兴奋，但从长远来看，饿死自己并不是正确的选择。然而，由于老鼠利用了一种简单的策略，它不断触发奖励信号，因此它并没有去探索长期奖励的前景（例如活下去）。那么，如何弥补环境中可能存在更好机会的事实，而放弃当前的机会呢？如果我们希望智能体能够恰当地解决复杂环境，我们必须让它理解延迟满足的概念。很快，我们将看到深度强化学习如何尝试解决这些问题，从而催生出更复杂、更强大的系统，有些人甚至可能称其为异常敏锐。
- en: Path to artificial general intelligence
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通向人工通用智能的路径
- en: Take the example of the AlphaGo system, which was developed by UK-based start-up
    DeepMind, which leverages an acute flavor of deep reinforcement learning to inform
    its predictions. There is good reason behind Google's move to acquire it for a
    round sum of $500 million, since many claim that DeepMind has made first steps
    toward something called **Artificial General Intelligence** (**AGI**)—sort of
    the Holy Grail of AI, if you will. This notion refers to the capability of an
    artificially intelligent system to perform well on various tasks, instead of the
    narrow span of application our networks have taken so far. A system that learns
    through observing its own actions on an environment is similar in spirit (and
    potentially much faster) to how we humans learn ourselves.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以AlphaGo系统为例，该系统由总部位于英国的初创公司DeepMind开发，利用了一种深度强化学习的独特形式来为其预测提供依据。谷歌以5亿美元的价格收购DeepMind是有充分理由的，因为许多人认为DeepMind已经迈出了朝向**人工通用智能**（**AGI**）的第一步——如果你愿意的话，这可以看作是AI的圣杯。这一概念指的是人工智能系统在多种任务上表现良好的能力，而不是我们目前的网络在狭窄应用范围内的表现。一个通过观察自己在环境中行动来学习的系统，从精神上看（并且可能更快），与我们人类的学习方式相似。
- en: The networks we built in the previous chapters perform well at a narrow classification
    or regression task, but must be redesigned significantly and retrained to perform
    on any other task. DeepMind, however, demonstrated how they could train a single
    network to perform well at several different (albeit narrow) tasks, involving
    playing several old-school Atari 2600 games. While a bit dated, these games were
    initially designed to be challenging for humans, making the feat quite a remarkable
    achievement in the field of AI. In their research ([https://deepmind.com/research/dqn/](https://deepmind.com/research/dqn/)),
    DeepMind showed how their **Deep Q Networks** (**DQN**) may be used to make artificial
    agents play different games just by observing the pixels on the screen without
    any prior information about the game itself. Their work inspired a new wave of
    researchers, who set off to train deep learning networks using reinforcement learning-based
    algorithms, giving birth to deep reinforcement learning. Since then, researchers
    and entrepreneurs alike have tried leveraging such techniques for a cascade of
    use cases, including but not limited to making machines move like animals and
    humans, generating molecular compounds for medicine, and even making bots that
    can trade on the stock market.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章构建的网络在狭窄的分类或回归任务中表现良好，但如果要执行其他任务，则必须进行显著的重新设计和重新训练。然而，DeepMind展示了如何训练一个单一的网络，在多个不同（尽管狭窄）的任务上表现出色，这些任务包括玩几款经典的Atari
    2600游戏。虽然这些游戏有些过时，但它们最初设计时就是为了给人类带来挑战，这使得这一成就成为AI领域的一个相当了不起的突破。在他们的研究中（[https://deepmind.com/research/dqn/](https://deepmind.com/research/dqn/)），DeepMind展示了他们的**深度Q网络**（**DQN**）如何仅通过观察屏幕上的像素，而不需要任何关于游戏本身的先验信息，就能让人工代理玩不同的游戏。他们的工作激发了一波新研究人员，他们开始使用基于强化学习的算法训练深度学习网络，推动了深度强化学习的诞生。此后，研究人员和企业家们纷纷尝试利用这些技术应用于一系列场景，包括但不限于让机器像动物和人类一样移动、生成药物分子化合物，甚至创建能够在股市交易的机器人。
- en: Needless to say, such systems can be much more flexible at modeling real-world
    events and can be applied to an array of tasks, reducing the resources that are
    spent on training separate narrow systems. One day, they may even be able to uncover
    complex and high-dimensional cause and effect relations, leveraging training examples
    from several domains to encode synergistic representations, which in turn help
    us to solve more complex problems. Our own discoveries are often inspired by information
    from various scientific domains. This tends to enhance our understanding of these
    situations and the complex dynamics that govern them. So, why not let machines
    do this too? Given the right reward signals for possible actions in a given environment,
    it may even surpass our own intuitions! Perhaps you can help with this one day.
    For now, let's start by having a look at how we go about simulating a virtual
    agent and make it interact with an environment to solve simple problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不用多说，这种系统在模拟现实世界事件方面更具灵活性，并可以应用于多种任务，从而减少训练多个独立狭窄系统所需的资源。未来，它们甚至可能揭示复杂且高维的因果关系，通过利用来自多个领域的训练示例来编码协同表示，这些表示反过来帮助我们解决更复杂的问题。我们的发现往往受到来自不同科学领域的信息启发，这有助于加深我们对这些情况及其复杂动态的理解。那么，为什么不让机器也来做这些呢？只要给定合适的奖励信号，机器在特定环境下的可能行动甚至可能超越我们自己的直觉！也许有一天你能在这方面提供帮助。现在，让我们先来看看如何模拟一个虚拟代理，并让它与环境互动，解决简单的问题。
- en: Simulating environments
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟环境
- en: First things first, we will need a simulated environment. An environment is
    defined as the *interaction space for a learning agent*. For humans, an environment
    can be any play you go to in the course of a day. For an artificial agent, this
    will often be a simulated environment that we have engineered. Why is it simulated?
    Well, we could ask the agent to learn in real time, like ourselves, but it turns
    out that this is quite impractical. For one, we would have to design each agent
    a body and then precisely engineer its actions and the environments that they
    are to interact with. Moreover, an agent can train much faster in a simulation,
    without requiring it to be restricted to human time frames. By the time a machine
    completes a single task in reality, its simulated version could have completed
    the same task several times over, providing a better opportunity for it to learn
    from its mistakes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个模拟环境。环境被定义为*学习代理的互动空间*。对于人类而言，环境可以是你一天中去的任何地方。对于人工智能代理而言，这通常是我们精心设计的模拟环境。为什么是模拟的呢？我们可以要求代理像我们一样在实时中学习，但事实证明这是相当不切实际的。首先，我们必须为每个代理设计一个身体，然后精确地设计其行为以及它们要与之互动的环境。此外，代理在模拟中训练得更快，无需局限于人类的时间框架。当机器在现实中完成一个任务时，其模拟版本可能已经完成了同一个任务好几次，为其从错误中学习提供了更好的机会。
- en: Next, we will go over the basic terminology that's used to describe a game,
    which represents an environment where an agent is expected to perform certain
    tasks to receive rewards and solve the environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一些基本术语，这些术语用于描述游戏，而游戏代表了一个代理需要在其中执行特定任务以获得奖励并解决环境的环境。
- en: Understanding states, actions, and rewards
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解状态、行动和奖励
- en: The environment itself can be broken down into a collection of different states,
    all of which represent the different situations that the agent may find itself
    in. The agent can navigate through these states by trying out different combinations
    of actions it is allowed to perform (such as walking left or right and jumping,
    in respect to a 2D arcade game). The actions that are made by the agent effectively
    change the state of the environment, making available tools, alternate routes,
    enemies, or any other goodies the game makers may have hidden to make it more
    interesting for you. All of these objects and events represent different states
    that the learning environment may take as the agent navigates through it. A new
    state is generated by the environment as a result of the agent interacting with
    it at its previous state, or due to random events occurring in the environment.
    This is how a game essentially progresses until a terminal state is reached, meaning
    that the game can go no further (due to a win or a death).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 环境本身可以分解为一系列不同的状态，这些状态代表了代理可能遇到的不同情况。代理可以通过尝试不同的行动组合来在这些状态间导航（例如，在二维街机游戏中，向左或向右走、跳跃等）。代理所做的行动会有效地改变环境的状态，使得可用的工具、备用路线、敌人或游戏设计者可能隐藏的任何其他元素变得可用，从而使游戏更加有趣。这些对象和事件代表了学习环境在代理进行导航时可能经历的不同状态。每当代理在其先前的状态下与环境互动，或环境中发生随机事件时，环境就会生成一个新的状态。这就是游戏如何进行，直到达到终端状态，即游戏无法继续（因为胜利或死亡）。
- en: 'Essentially, we want the agent to pursue timely and favorable actions to solve
    its environment. These actions must change the environment''s state to bring the
    agent closer toward attaining a given goal (like moving from point A to B or maximizing
    a score). To be able to do this, we need to design reward signals that occur as
    a consequence of the agent''s interactions with different states of the environment.
    We can use the notion of reward as feedback that allows our agent to assess the
    degree of success that''s attained by its actions as it optimizes a given goal:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们希望代理采取及时且有利的行动来解决其环境。这些行动必须改变环境的状态，使代理更接近达成预定目标（如从A点到B点或最大化得分）。为了做到这一点，我们需要设计奖励信号，这些信号作为代理与环境中不同状态互动的结果发生。我们可以将奖励的概念视为反馈，允许我们的代理评估其行动所取得的成功程度，从而优化给定目标：
- en: '![](img/26d4b365-8425-4fac-9733-2ea36e1afcc1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26d4b365-8425-4fac-9733-2ea36e1afcc1.png)'
- en: For those of you who are familiar with classic arcade style video games, think
    of a game of Mario. Mario himself is the agent and is controlled by you. The environment
    refers to the map that Mario can move about in. The presence of coins and mushrooms
    represent different states of the game. Once Mario interacts with either of these
    states, a reward is triggered in the form of points and a new state is born as
    consequence, which in turn alters Mario's environment accordingly. Mario's goal
    can be either to move from point A to B (if you're in a hurry to complete the
    game) or to maximize his score (if you're more interested in unlocking achievements).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些熟悉经典街机风格视频游戏的人来说，可以想象一场《马里奥》的游戏。马里奥本身是代理角色，由你来控制。环境指的是马里奥可以在其中移动的地图。金币和蘑菇的存在代表着游戏中的不同状态。一旦马里奥与这些状态中的任何一个互动，就会触发奖励，形式为积分，新的状态也会随之产生，进而改变马里奥的环境。马里奥的目标可以是从A点移动到B点（如果你急于完成游戏），或者是最大化他的得分（如果你更感兴趣的是解锁成就）。
- en: A self-driving taxi cab
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一辆自动驾驶出租车
- en: Next, we will clarify the theoretical understandings we have gathered so far
    by observing how environments can be solved by artificial agents. We will see
    how this can be achieved even through randomly sampling actions from an agent's
    action space (possible actions an agent may perform). This will help us to understand
    the complexities involved in solving even the simplest of environments, and why
    we might want to call upon deep reinforcement learning shortly to help us to achieve
    our goals. The goal we are about to address is creating a self-driving taxi cab
    in a reduced, simulated environment. While the environment we will deal with is
    much simpler than the real world, this simulation will serve as an excellent stepping
    stone into the design architecture of reinforcement learning systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过观察人工智能代理如何解决环境问题，来澄清我们迄今为止所获得的理论理解。我们将看到，即使通过随机从代理的动作空间中采样动作（代理可能执行的动作），也能实现这一目标。这将帮助我们理解解决即使是最简单环境所涉及的复杂性，以及为什么我们可能很快需要调用深度强化学习来帮助我们实现目标。我们即将解决的目标是在一个简化的模拟环境中创建一辆自动驾驶出租车。尽管我们将处理的环境比现实世界简单得多，但这个模拟将作为深入强化学习系统设计架构的一个优秀跳板。
- en: 'To do this, we will be using OpenAI''s `gym`, an adequately named module that''s
    used to simulate artificial environments for training machines. You may install
    the OpenAI gym dependency by using the `pip` package manager. The following command
    will run on Jupyter Notebooks and initiate the installation of the module:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个目标，我们将使用OpenAI的`gym`，一个恰如其名的模块，用于模拟人工环境以训练机器。你可以使用`pip`包管理器来安装OpenAI gym依赖。以下命令将在Jupyter
    Notebooks中运行并启动该模块的安装：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `gym` module comes with a plethora of pre-installed environments (or test
    problems), spanning from simple to more complex simulations. The test problem
    we will be using for the following example comes from the `''TaxiCab-v2''` environment.
    We will begin our experiments with what is known as the **taxicab simulation**,
    which simply simulates a grid of roads for a taxi to navigate through so that
    they can pick up and drop off customers:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym`模块提供了大量预安装的环境（或测试问题），涵盖了从简单到复杂的各种模拟。我们将在以下示例中使用的测试问题来自`''TaxiCab-v2''`环境。我们将从所谓的**出租车模拟**开始实验，这个模拟简单地模拟了一张道路网格，出租车需要在其中穿行，以便接送顾客：'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Understanding the task
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解任务
- en: 'The taxi cab simulation was introduced in (Dietterich 2000) to demonstrate
    problems with applying reinforcement learning in a hierarchical manner. We will,
    however, use this simulation to solidify our understanding of agents, environments,
    rewards, and goals, before we continue to simulate and solve more complex problems.
    For now, the problem we face is relatively simple: pick up passengers and drop
    them off at a given location. These locations (four in total) are represented
    by letters. All our agent has to do is travel to these pickup locations, pick
    up a passenger, then travel to a designated drop-off location where the passenger
    may disembark. Given a successful disembark, the agent receives +20 points (simulating
    the money our virtual cabby gets). Each time step our cabby takes before it reaches
    its destination, is attributed a reward of -1 (intuitively, this is the penalty
    our cabby incurs for the cost of petrol they must replenish). Lastly, another
    penalty of -10 exists for pickups and drop-offs that are not scheduled. You could
    imagine the reason behind penalizing pickups as a taxi company trying to optimize
    its fleet deployment to cover all areas of the city, requiring our virtual cabby
    to only pick up assigned passengers. Unscheduled drop-offs, on the other hand,
    simply reflect disgruntled and baffled customers. Let''s have a look at what the
    taxi cab environment actually looks like.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车模拟最早由(Dietterich 2000)提出，用以展示在分层方式应用强化学习时所遇到的问题。然而，我们将使用此模拟来巩固我们对智能体、环境、奖励和目标的理解，然后再继续模拟和解决更复杂的问题。目前，我们面临的问题相对简单：接送乘客并将其送到指定地点。共有四个目的地，这些地点用字母表示。我们的智能体只需前往这些接载地点，接上乘客，然后前往指定的下车地点，乘客可以下车。成功下车后，智能体会获得+20点奖励（模拟我们虚拟出租车司机获得的报酬）。每经过一步，出租车司机在到达目的地前会被扣除-1点奖励（直观上，这代表了出租车司机为补充油料而产生的费用）。最后，对于未按计划接载或下车的情况，还有一个-10点的惩罚。你可以想象，惩罚接载的原因就像出租车公司试图优化其车队部署，以覆盖城市的各个区域，要求我们的虚拟出租车司机只接载指定乘客。而未按计划下车，则反映了顾客的不满和困惑。让我们来看一下出租车环境实际是什么样子的。
- en: Rendering the environment
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 渲染环境
- en: 'To visualize the environment we just loaded up, we must first initialize it
    by calling `reset()` on our environment object. Then, we can render the starting
    frame, corresponding to the position of our taxi (in yellow) and four different
    pickup locations (denoted by colored letters):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化我们刚加载的环境，我们必须首先通过在环境对象上调用`reset()`来初始化它。然后，我们可以渲染起始帧，显示出租车（黄色）的位置以及四个不同的接客地点（用不同颜色的字母表示）：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/96326554-b2e7-4610-bc85-8e8503b9bc64.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96326554-b2e7-4610-bc85-8e8503b9bc64.png)'
- en: Do note that the preceding screenshot depicts open roads using colons (`:`),
    and walls that cannot be traversed by the taxi using the symbol (`|`). While the
    position of these obstacles and routes remain permanent, the letters denoting
    the pickup points, as well as our yellow taxi, keep changing every time the environment
    is initialized. We can also notice that resetting the environment generates an
    integer. This refers to a specific state of the environment (that is, positioning
    of the taxi and the pickups) that's taken upon initialization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的截图中，开放的道路用冒号（`:`）表示，而出租车无法穿越的墙壁则用竖线（`|`）表示。虽然这些障碍物和道路的位置保持不变，但表示接载点的字母以及我们黄色出租车的位置在每次初始化环境时都会发生变化。我们还可以注意到，重置环境会生成一个整数。这表示环境的一个特定状态（即出租车和接载点的位置），这是在初始化时获得的。
- en: You can replace the `Taxi-v2` string with other environments in the registry
    (like `CartPole-v0` or `MountainCar-v0`), and render a few frames to get an idea
    of what we are dealing with. There's also a few other commands that let you better
    understand the environment you are dealing with. While the taxi cab environment
    is simple enough to be simulated using colored symbols, more complex environments
    may be rendered in a separate window, which opens upon execution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将`Taxi-v2`字符串替换为注册表中的其他环境（如`CartPole-v0`或`MountainCar-v0`），并渲染几帧来大致了解我们所处理的环境。还有一些其他命令可以帮助你更好地理解你所面对的环境。虽然出租车环境足够简单，可以使用彩色符号进行模拟，但更复杂的环境可能会在执行时在一个单独的窗口中渲染。
- en: Referencing observation space
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考观察空间
- en: 'Next, we will try to better understand our environment and action space. All
    states in the taxi cab environment are denoted by an integer ranging between 0
    to 499\. We can verify this by printing out the total number of possible states
    our environment has. Let''s have a look at the number of possible different states
    our environment may take:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试更好地理解我们的环境和动作空间。出租车环境中的所有状态通过一个介于0到499之间的整数来表示。我们可以通过打印出环境中所有可能状态的总数来验证这一点。让我们来看一下我们环境可能的不同状态的数量：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Referencing action space
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用动作空间
- en: 'In the taxi cab simulation, our cabby agent is given six distinct actions that
    it may perform at each time step. We can check the total number of possible actions
    by checking the environment''s action space, as shown here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在出租车模拟中，我们的司机智能体在每个时间步骤上有六个不同的动作可以执行。我们可以通过查看环境的动作空间来检查可能的动作总数，具体如下所示：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our cabby may, at any given time, do one of these six actions. These actions
    correspond to moving up, down, left, or right; picking someone up; or dropping
    them off.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的司机在任何给定时刻都可以执行六个动作之一。这些动作分别对应向上、向下、向左或向右移动；接乘客；或把乘客放下。
- en: Interacting with the environment
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与环境互动
- en: 'To make our agent do something, we can use the `step()` method on our environment
    object. The `step(i)` method takes an integer that refers to one out of the six
    possible actions that our agent is allowed to take. In this case, these actions
    were labeled as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要让我们的智能体执行某个动作，我们可以在环境对象上使用`step()`方法。`step(i)`方法接受一个整数，这个整数对应智能体允许执行的六个可能动作中的一个。在这个例子中，这些动作标记如下：
- en: (0) to move down
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (0) 表示向下移动
- en: (1) to move up
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) 表示向上移动
- en: (2) for a right turn
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) 表示右转
- en: (3) for a left turn
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) 表示左转
- en: (4) for picking up a passenger
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) 表示接乘客
- en: (5) for dropping the passenger off
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) 表示放下乘客
- en: 'Here''s how the code is represented:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码的表示方式：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We get the following output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/14ef2611-c6f5-4c9e-bbc7-93f19dd2688d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14ef2611-c6f5-4c9e-bbc7-93f19dd2688d.png)'
- en: 'As we can see here, we make our agent take a step downward. Now, we understand
    how to make our agent do all of the required steps to achieve its goal. In fact,
    calling `step(i)` on the environment object will return four specific variables,
    referring to what action (i) did to the environment, from the perspective of the
    agent. These variables are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们让智能体向下执行一个步骤。现在，我们理解了如何让智能体执行所有必要的步骤以达成目标。事实上，调用环境对象的`step(i)`会返回四个特定的变量，这些变量描述了从智能体的角度来看，动作(i)对环境的影响。这些变量如下：
- en: '`observation`: This is the observed state of the environment. This can be pixel
    data from game screenshots or another manner of representing the states of the
    environment to the learning agent.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`observation`：这是环境的观测状态。它可以是来自游戏截图的像素数据，或者是以其他方式表示环境状态的形式，以便学习智能体理解。'
- en: '`reward`: This is the compensation for our agent, due to actions taken on a
    given time step. We use the reward to set goals for our learning agent by simply
    asking it to maximize the reward it receives in a given environment. Note that
    the scale of the reward (float) values may differ per experimental setup.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`reward`：这是我们为智能体提供的奖励，用来表示智能体在某个时间步骤执行某个动作后的补偿。我们利用奖励为学习智能体设定目标，方法是要求其最大化在特定环境中获得的奖励。注意，奖励（浮动）值的尺度可能会根据实验设置有所不同。'
- en: '`done`: This Boolean (binary) value denotes whether a trial episode has terminated
    or not. In the case of the taxi-cab simulation, an episode is considered `done`
    when a passenger has been picked up and dropped off at a given location. For an
    Atari game, an episode can be defined as the life of the agent, which terminates
    once you get hit by a space invader.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`done`：这个布尔（布尔值）变量表示一个试验是否已结束。在出租车模拟中，当乘客被接上并在指定位置放下时，认为这一轮试验已完成。在一款雅达利游戏中，试验可以定义为智能体的生命，直到被外星人击中而结束。'
- en: '`info`: This dictionary item serves to store information that''s used to debug
    our agents'' actions and is usually not used in the learning process itself. It
    does store valuable information, such as the probabilities affecting the previous
    change of states, for a given step:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`info`：这个字典项用于存储调试智能体动作时所需要的信息，通常在学习过程中不使用。然而，它存储了有价值的信息，比如影响前一个状态变化的概率，在给定步骤中：'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Solving the environment randomly
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机解决环境
- en: 'Armed with the logic governing OpenAI gym environments, and how to make artificial
    agents interact therein, we can proceed to implement a random algorithm that allows
    the agent to (eventually) solve the taxi cab environment. First, we define a fixed
    state to begin our simulation with. This is helpful if you want to repeat the
    same experiment (that is, initiate the environment with the same state) while
    checking how many random steps the agent took in solving the environment each
    episode. We also define a `counter` variable that simply keeps track of the number
    of time steps our agent takes as the episode progresses. The reward variable is
    initialized as `None` and will be updated once the agent takes its first step.
    Then, we simply initiate a `while` loop that repeatedly samples possible actions
    that are random from our action space and updates the respective `state`, `reward`,
    and `done` variables for each sampled action. To randomly sample actions from
    the environment''s action space, we use the `.sample()` method on the `env.action_space`
    object. Finally, we increment our `counter` and render the environment for visualization:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了OpenAI gym环境的逻辑和如何让人工智能体在其中互动之后，我们可以继续实现一个随机算法，让智能体（最终）解决出租车环境。首先，我们定义一个固定状态来开始我们的仿真。如果你想重复同一个实验（即用相同的状态初始化环境），这将非常有帮助，同时可以检查智能体在每次任务中解决环境时采取了多少随机步骤。我们还定义了一个`counter`变量，用于简单地跟踪智能体在任务进展过程中所采取的时间步骤数。奖励变量初始化为`None`，并将在智能体采取第一步时更新。然后，我们简单地启动一个`while`循环，反复从我们的动作空间中随机选择可能的动作，并更新每个选定动作的`state`、`reward`和`done`变量。为了从环境的动作空间中随机选择动作，我们使用`.sample()`方法操作`env.action_space`对象。最后，我们增加`counter`，并渲染环境以便可视化：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/07002f1f-b2af-4e82-b9be-6a0dacef89ba.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07002f1f-b2af-4e82-b9be-6a0dacef89ba.png)'
- en: It wasn't until the 2,145^(th) attempt that our agent even got to the right
    passenger in the cab (as indicated by the cab turning green). That's quite long,
    even if you may not feel the time pass by. Random algorithms are helpful to benchmark
    our performance when calling upon more complex models as a measure of sanity.
    But surely we can do better than 6,011 steps (as taken by the agent running on
    the random algorithm) to solve this simple environment. How? Well, we reward it
    for being right. To do this, we must first define the notion of reward mathematically.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 直到第2,145次尝试，我们的智能体才找到了正确的乘客（如出租车变绿所示）。即便你可能没有感到时间的流逝，这也是相当长的。随机算法有助于在调用更复杂的模型时作为基准，确保结果的合理性。但显然，我们可以做得比智能体运行在随机算法上所用的6,011步（解决这个简单环境）要好。怎么做呢？我们奖励它做对的事情。为此，我们必须首先在数学上定义奖励的概念。
- en: Trade-off between immediate and future rewards
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 立刻奖励与未来奖励之间的权衡
- en: 'At first glance, this may appear quite simple. We already saw how the cabby
    can be incentivized by awarding it +20 points for a correct dropoff, -10 for a
    false one, and -1 for each time step that it takes to complete the episode. Logically,
    then, you can calculate the total reward collected by an agent for an episode
    as the cumulation of all the individual rewards for each time step that''s seen
    by the agent. We can denote this mathematically and represent the total reward
    in an episode as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这可能显得相当简单。我们已经看到，出租车司机通过正确送客获得+20分，错误送客扣-10分，每个时间步骤完成一轮任务时扣-1分。逻辑上，你可以将一个智能体在一次任务中的总奖励计算为该智能体在每个时间步骤所获得的所有个别奖励的累积。我们可以通过数学表示来表示这一点，并表示一次任务中的总奖励如下：
- en: '![](img/47449219-aeff-4a74-acb8-0e703c9695f5.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47449219-aeff-4a74-acb8-0e703c9695f5.png)'
- en: Here, *n* simply denotes the time step of the episode. This seems intuitive
    enough. We can now ask our agent to maximize the total reward in a given episode.
    But there's a problem. Just like our own reality, the environment that's faced
    by our agent may be governed by largely random events. Hence, there may be no
    guarantee that performing the same action will return the same reward in the similar
    future states. In fact, as we progress into the future, the rewards may diverge
    more and more from the corresponding actions that are taken at each state due
    to the inherent randomness that's present.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n*仅仅表示任务的时间步骤。这似乎是直观的。现在，我们可以要求智能体最大化给定任务中的总奖励。但问题来了。就像我们自己所处的现实一样，智能体所面临的环境可能主要由随机事件主导。因此，无法保证在相似的未来状态下执行相同的动作会返回相同的奖励。事实上，随着我们进入未来，由于固有的随机性，奖励可能会越来越偏离每个状态下采取的相应动作所带来的奖励。
- en: Discounting future rewards
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对未来奖励的折扣
- en: 'So, how can we compensate for this divergence? One way is through discounting
    future rewards, thereby amplifying the relevance of current rewards over rewards
    from future time steps. We can achieve this by adding a discount factor to the
    reward that''s generated at each time step while we calculate the total reward
    in a given episode. The purpose of this discount factor will be to dampen future
    rewards and amplify current ones. In the short term, we have more certainty of
    being able to collect rewards by using corresponding state action pairs. This
    cannot be said in the long run due to the cumulating effects of random events
    that populate the environment. Hence, to incentivize the agent to focus on relatively
    certain events, we can modify our earlier formulation for total reward to include
    this discount factor, like so:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何弥补这种差异呢？一种方法是通过对未来奖励进行折扣，从而增强当前奖励相对于未来时间步奖励的重要性。我们可以通过在每个时间步生成奖励时加入折扣因子来实现这一点，同时在计算给定回合的总奖励时应用它。折扣因子的目的是减弱未来奖励并增强当前奖励。从短期来看，通过使用相应的状态-行动对，我们更有把握获得奖励。然而，由于环境中随机事件的累积效应，从长期来看就无法做到这一点。因此，为了激励智能体集中于相对确定的事件，我们可以修改之前的总奖励公式，加入这个折扣因子，如下所示：
- en: '![](img/8491e1d1-370d-4734-a81c-d522f17ca20f.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8491e1d1-370d-4734-a81c-d522f17ca20f.png)'
- en: 'In our new total reward formulation, γ denotes a discount factor between 0
    and 1, and *t* denotes the present time step. As you may have noticed right away,
    the exponential decrease of the γ term allows for future rewards to be dampened
    over current ones. Intuitively, this just means that rewards that are far into
    the future are taken less into account compared to more contemporary ones while
    the agent considers its next action. And by how much? Well, that is still up to
    us. A discount factor nearing zero will produce short sighted strategies, hedonistically
    favoring immediate rewards over future ones. On the other hand, setting a discount
    factor too close to one will defeat the purpose of having it in the first place.
    In practice, a balancing value can be in the range of 0.75-0.9, depending on the
    degree of stochasticity in the environment. As a rule of thumb, you would want
    higher gamma (γ ) values for more deterministic environments, and lower (γ) values
    for stochastic environments. We can even simplify the total reward formula that
    was given prior like so:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们新的总奖励公式中，γ 表示一个介于 0 和 1 之间的折扣因子，*t* 表示当前的时间步。如你所见，γ 项的指数递减使得未来的奖励相对于当前奖励被减弱。直观地说，这意味着在智能体考虑下一步行动时，较远的未来奖励会比更接近的奖励被考虑得少。那么，减弱的程度如何呢？这仍然由我们决定。一个接近零的折扣因子会产生短视的策略，享乐主义地偏向即时奖励而非未来奖励。另一方面，如果折扣因子过于接近
    1，折扣因子的作用将被完全抵消。实际上，根据环境中的随机性程度，折扣因子的平衡值通常在 0.75 到 0.9 之间。作为经验法则，较为确定性环境中需要较高的
    γ 值，而较为随机的环境中需要较低的 γ 值。我们甚至可以像下面这样简化之前给出的总奖励公式：
- en: '![](img/5e7e1c95-2c44-4f03-9830-f1bffdd5502e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e7e1c95-2c44-4f03-9830-f1bffdd5502e.png)'
- en: Hence, we formalize the total reward in an episode as the cumulative discounted
    reward for each time step in the episode. By using the notion of discounted future
    reward, we can generate strategies for an agent, hence governing its actions.
    An agent carrying out a beneficial strategy will aim to select actions that maximize
    the discounted future rewards in a given episode. Now that we have a good idea
    of how to engineer a reward signal for our agent, it's time to move on and look
    at an overview of the entire process of learning.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将回合中的总奖励形式化为每个时间步的累计折扣奖励。通过使用折扣未来奖励的概念，我们可以为智能体生成策略，从而指导其行动。执行有利策略的智能体将会选择那些在给定回合中最大化折扣未来奖励的行动。现在，我们已经大致了解了如何为智能体设计奖励信号，是时候继续研究整个学习过程的概述了。
- en: Markov decision process
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In reinforcement learning, we are trying to solve the problem of correlating
    immediate actions with the delayed rewards they return. These rewards are simply
    sparse, time-delayed labels that are used to control the agent's behavior. So
    far, we have discussed how an agent may act upon different states of an environment.
    We also saw how interactions generate various rewards for the agent and unlock
    new states of the environment. From here, the agent can resume interacting with
    the environment until the end of an episode. It's about time we mathematically
    formalize these relations between an agent and environment for the purpose of
    goal optimization. To do this, we will call upon a framework proposed by Russian
    mathematician Andrey Markov, now known as the **Markov decision process** (**MDP**).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们试图解决将即时行动与其返回的延迟奖励相关联的问题。这些奖励仅仅是稀疏的、时间延迟的标签，用于控制智能体的行为。到目前为止，我们已经讨论了智能体如何根据环境的不同状态进行行动。我们还看到了交互如何为智能体生成不同的奖励，并解锁环境的新状态。从这里开始，智能体可以继续与环境进行交互，直到一回合结束。现在是时候数学上形式化这些智能体与环境之间的关系，以便进行目标优化了。为此，我们将借用由俄罗斯数学家安德烈·马尔可夫提出的框架，现在称为**马尔可夫决策过程**（**MDP**）。
- en: 'This mathematical framework allows us to model our agent''s decision-making
    process in an environment that is partially stochastic and partially controllable
    by the agent. This process relies on the Markov assumption, stating that the probability
    of future states (*st+1*) depends on the current state (*st*) only. This assumption
    means that all states and actions leading up to the current state have no influence
    on the probability of future states. A MDP is defined by the following five variables:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数学框架使我们能够建模智能体在部分随机且部分可控的环境中的决策过程。这个过程依赖于马尔可夫假设，假设未来状态的概率（*st+1*）仅依赖于当前状态（*st*）。这一假设意味着，所有导致当前状态的状态和动作对未来状态的概率没有影响。MDP由以下五个变量定义：
- en: '![](img/b9c1b7d8-197f-48a1-9192-f157ca72de81.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9c1b7d8-197f-48a1-9192-f157ca72de81.png)'
- en: While the first two variables are quite self-explanatory, the third one (**R**)
    refers to the probability distribution of a reward, given a state-action pair.
    Here, a state-action pair simply refers to the corresponding action to take for
    a given state of the environment. Next on the list is the transition probability
    (**P**), which denotes the probability of the new state given the chosen state-action
    pair at a time step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前两个变量相当直观，第三个变量（**R**）指的是给定一个状态-动作对下的奖励概率分布。这里，状态-动作对仅指给定环境状态下应采取的相应动作。接下来是转移概率（**P**），它表示在某一时间步下，给定选择的状态-动作对后得到新状态的概率。
- en: 'Finally, the discount factor refers to the degree to which we wish to discount
    future rewards for more immediate ones, which is elaborated on in the following
    diagram:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，折扣因子指的是我们希望将未来奖励折扣到更即时奖励的程度，以下图说明：
- en: '![](img/8ccdd66f-917a-41a7-b81c-f065858f093b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ccdd66f-917a-41a7-b81c-f065858f093b.png)'
- en: 'Left: Reinforcement learning problem. Right: Markov decision process'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 左：强化学习问题。右：马尔可夫决策过程
- en: 'Hence, we can describe interactions between our agent and the environment using
    the MDP. An MDP is composed of a collection of states and actions, together with
    rules that dictate the transition from one state to another. We can now mathematically
    define one episode as a finite sequence of states, actions, and rewards, like
    so:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用MDP描述智能体与环境之间的交互。一个MDP由一系列状态和动作组成，并包含规则，规定了从一个状态到另一个状态的转移。我们现在可以数学地定义一次回合为一个有限的状态、动作和奖励序列，如下所示：
- en: '![](img/8fe05707-fcf7-4973-b70d-aa6832a6f676.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fe05707-fcf7-4973-b70d-aa6832a6f676.png)'
- en: Here, (s[t]) and (a[t]) denote the state and corresponding action at time *t*.
    We can also denote the reward corresponding to this state-action pair as (r[t+1]).
    Hence, we begin an episode by sampling an initial state from the environment (s[o]).
    Then, until our goal is completed, we ask our agent to select an action for the
    corresponding state of the environment it finds itself in. Once the agent executes
    an action, the environment samples a reward for the action taken by the agent
    and the next state (s[t+1]) to follow. The agent then receives both the reward
    and the next state before repeating the process until it is able to solve the
    environment. Finally, a terminal state (s[n]) is reached at the end of an episode
    (that is, when our goal is completed, or we deplete our lives in a game). The
    rules that determine the actions of the agent at each state are collectively known
    as a **policy**, and are denoted by the Greek symbol (π).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，(s[t]) 和 (a[t]) 表示时间 *t* 时的状态和相应的动作。我们还可以将与该状态-动作对对应的奖励表示为 (r[t+1])。因此，我们通过从环境中采样一个初始状态
    (s[o]) 来开始一个回合。接下来，直到目标完成，我们要求智能体为其所处环境的相应状态选择一个动作。一旦智能体执行了一个动作，环境就会为智能体采取的动作以及接下来的状态
    (s[t+1]) 采样一个奖励。然后，智能体在接收到奖励和下一个状态后，会重复这个过程，直到它能够解决环境中的问题。最后，在回合结束时，会到达一个终止状态
    (s[n])（也就是说，当目标完成时，或者在游戏中用完所有生命时）。决定智能体在每个状态下采取动作的规则统称为**策略**，并用希腊字母 (π) 表示。
- en: Understanding policy functions
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解策略函数
- en: 'As we can see, the efficiency of our agent to solve an environment depends
    on what policy it uses to match state-action pairs at each time step. Hence, a
    function, known as a **policy function** (π), can specify the combination of state-action
    pairs for each time step the agent comes across. As the simulation runs, the policy
    is responsible for producing the trajectory, which is composed of game-states;
    actions that are taken by our agent as a response; and a reward that''s generated
    by the environment, as well as the next state of the game the agent receives.
    Intuitively, you can think of a policy as a heuristic that generates actions that
    respond to the generated states of an environment. A policy function itself can
    be a good or a bad one. If your policy is to shoot first and ask questions later,
    you may end up shooting a hostage. Hence, all we need to do now is evaluate different
    policies (that is, the trajectories they produce, including their sequences of
    states, actions, rewards, and next-states), and pick out the optimal policy (π
    *) that maximizes the cumulative discounted rewards for a given game. This can
    be illustrated mathematically as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，智能体解决一个环境的效率取决于它在每个时间步骤中使用什么策略来匹配状态-动作对。因此，一个被称为**策略函数**（π）的函数可以指定智能体在每个时间步骤上遇到的状态-动作对的组合。随着模拟的进行，策略负责生成轨迹，轨迹由游戏状态、智能体作为响应所采取的动作、环境生成的奖励以及智能体接收到的游戏下一个状态组成。直观地讲，你可以将策略看作是一个启发式方法，它生成响应环境状态的动作。策略函数本身可以是好的，也可以是坏的。如果你的策略是先开枪再问问题，最终你可能会误伤人质。因此，我们现在要做的就是评估不同的策略（即它们产生的轨迹，包括状态、动作、奖励和下一个状态的序列），并挑选出能最大化给定游戏的累计折扣奖励的最优策略
    (π *)。这可以通过以下数学公式来表示：
- en: '![](img/237dbf57-199c-4267-9521-853d92843d01.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/237dbf57-199c-4267-9521-853d92843d01.png)'
- en: 'Hence, if we are trying to move from point A to point B, our optimal policy
    will involve taking an action that allows us to move as close as possible to point
    B at each time step. Unfortunately, due to the randomness that''s present in such
    environments, we may not speak with such certainty as to claim that our policy
    absolutely maximizes the sum of discounted rewards. By definition, we cannot account
    for certain random events like earthquakes, for example, while traveling from
    point A to B (assuming you''re not a seismological expert). Hence, we can also
    not perfectly account for the rewards that arise as a result of the actions performed
    due to randomness in an environment. Instead, we can define the optimal policy
    ( π *) as a policy that lets our agent maximize the expected sum of discounted
    rewards. This can be denoted with a slight modification to the earlier equation,
    which can be portrayed as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们试图从A点移动到B点，我们的最优策略将包括采取一个使我们在每个时间步尽可能靠近B点的行动。不幸的是，由于在这样的环境中存在随机性，我们不能如此确定地声称我们的策略绝对最大化了折扣奖励的总和。根据定义，我们无法考虑在从A点到B点的过程中发生的某些随机事件，例如地震（假设你不是地震学专家）。因此，我们也无法完美地考虑因环境中的随机性而产生的动作所带来的奖励。相反，我们可以将最优策略（π*）定义为一种让我们的代理最大化预期的折扣奖励之和的策略。这可以通过对先前方程式进行轻微修改来表示，表示如下：
- en: '![](img/d253b709-88ec-4d0d-bcda-79f29ff914c4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d253b709-88ec-4d0d-bcda-79f29ff914c4.png)'
- en: Here, we use the MDP framework and sample the initial state (s[o]) from our
    state probability distribution p(s[o]). The actions of our agent (a[t]) are sampled
    from a policy, given a state. Then, a reward is sampled that corresponds to the
    utility of the action that's performed at the given state. Finally, the environment
    samples the next state (s[t+1]), from the transition probability distribution
    of the current state-action pair. Hence, at each time step, we aim to update our
    optimal policy so that we can maximize the expected sum of discounted rewards.
    Some of you may wonder at this point, how can we assess the utility of an action,
    given a state? Well, this is where the value and Q-value functions come in. To
    evaluate different policy functions, we need to be able to assess the value of
    different states, as well as the quality of actions corresponding to these states,
    for a given policy. For this, we need to define two additional functions, known
    as the value function and the Q-value function.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用MDP框架，并从我们的状态概率分布p(s[o])中采样初始状态（s[o]）。代理的动作（a[t]）根据给定状态从策略中采样。然后，采样一个奖励，对应于在给定状态下执行的动作的效用。最后，环境从当前状态-动作对的转移概率分布中采样下一个状态（s[t+1]）。因此，在每个时间步，我们的目标是更新最优策略，以最大化预期的折扣奖励之和。你们中的一些人可能会在此时想，如何在给定状态的情况下评估一个动作的效用呢？嗯，这就是值函数和Q值函数发挥作用的地方。为了评估不同的策略函数，我们需要能够评估不同状态的值，以及给定策略下与这些状态对应的动作质量。为此，我们需要定义两个额外的函数，分别是值函数和Q值函数。
- en: Assessing the value of a state
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估状态的值
- en: 'First, we need to estimate the value (*V*) of state (*s*) while following a
    specific policy (π). This tells you the expected cumulative reward at the terminal
    state of a game while following a policy (π) starting at state (*s*). Why is this
    useful? Well, imagine that our learning agent''s environment is populated by enemies
    that are continuously chasing the agent. It may have developed a policy dictating
    it to never stop running during the whole game. In this case, the agent should
    have enough flexibility to evaluate the value of game states (when it runs up
    to the edge of a cliff, for example, so as to not run off it and die). We can
    do this by defining the value function at a given state, *V* *π (s)*, as the expected
    cumulative (discounted) reward that the agent receives from following that policy,
    starting from the current state:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要估计在遵循特定策略（π）下，状态（*s*）的值（*V*）。这告诉你在遵循从状态（*s*）开始的策略（π）下，游戏结束时的预期累计奖励。为什么这个有用呢？想象一下，我们的学习代理环境中充满了不断追赶代理的敌人。它可能已经形成了一个策略，指示它在整个游戏过程中永远不停跑。在这种情况下，代理应该具有足够的灵活性来评估游戏状态的值（例如，当它跑到悬崖边缘时，以避免跑下去并死亡）。我们可以通过定义给定状态下的值函数，*V*
    *π (s)*，作为代理在遵循该策略时，从当前状态开始的预期累计（折扣）奖励来实现这一点：
- en: '![](img/8cc4b32c-c73b-408b-9a93-17d214b22a7f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cc4b32c-c73b-408b-9a93-17d214b22a7f.png)'
- en: Hence, we are able to use the value function to evaluate how good a state is
    while following a certain policy. However, this only tells us about the value
    of a state itself, given a policy. We also want our agent to be able to judge
    the value of an action in response to a given state. This is what really allows
    the agent to act dynamically in response to whatever the environment throws at
    it (be it an enemy or the edge of a cliff). We can operationalize this notion
    of *goodness* for given state action pairs for a given policy by using the Q-value
    function.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们能够使用价值函数来评估在遵循特定策略时，某个状态的优劣。然而，这仅仅告诉我们给定策略下一个状态本身的价值。我们还希望我们的智能体能够判断某个动作在特定状态下的价值。这正是允许智能体根据环境中的任何情况（无论是敌人还是悬崖边缘）动态做出反应的关键。我们可以通过使用Q值函数来实现这一概念，将给定状态-动作对在特定策略下的“优良”程度量化出来。
- en: Assessing the quality of an action
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估动作质量
- en: 'If you walk up to a wall, there are not many actions you can perform. You will
    likely respond to this state in your environment by choosing the action of turning
    around, followed by asking yourself why you walked up to a wall in the first place.
    Similarly, we would like our agent to leverage a sense of goodness for different
    actions with respect to the states they find themselves in while following a policy.
    We can achieve this using a Q-Value function. This function simply denotes the
    expected cumulative reward from taking a specific action, in a specific state,
    while following a policy. In other words, it denotes the quality of a state-action
    pairs for a given policy. Mathematically, we can denote the *Q* *π ( a , s)* relation
    as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你走到墙前，能执行的动作不多。你可能会通过选择掉头动作来回应这个状态，接着会问自己为什么一开始走到墙前。同样地，我们希望我们的智能体能根据它所处状态的不同，利用一个关于动作优劣的感知，同时遵循策略。我们可以通过使用Q值函数来实现这一目标。这个函数简单地表示在特定状态下采取特定动作时，遵循一个策略所期望的累积奖励。换句话说，它表示给定策略下，状态-动作对的质量。数学上，我们可以表示
    *Q* *π ( a , s)* 关系如下：
- en: '![](img/174dab38-76ed-457e-be41-f29e0daf7ea9.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/174dab38-76ed-457e-be41-f29e0daf7ea9.png)'
- en: 'The *Q* *π ( s , a)* function allows us to represent the expected cumulative
    reward from following a policy (π). Intuitively, this function helps us to quantify
    our total score at the end of a game, given the actions (that is, the different
    joystick control moves) taken at each state (game screen you observe) of the environment
    (a game of Mario, for example), while following a policy (move forward while jumping).
    Using this function, we can then define the best possible expected cumulative
    reward at the terminal state of a game, given the policy being followed. This
    can be represented as the maximum expected value that''s attainable by the Q-value
    function and is known as the optimal Q-value function. We can mathematically define
    this as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q* *π ( s , a)* 函数允许我们表示遵循一个策略（π）所获得的期望累积奖励。直观地说，这个函数帮助我们量化在游戏结束时的总得分，给定每个状态（即你观察到的游戏画面）下所采取的动作（即不同的摇杆控制操作），并遵循一个策略（例如：在跳跃时向前移动）。通过这个函数，我们可以定义在游戏的终止状态下，给定所遵循的策略时，最好的期望累积奖励。这可以表示为Q值函数能够达到的最大期望值，称为最优Q值函数。我们可以通过以下数学公式来定义它：'
- en: '![](img/7cef80af-3b08-4873-9144-6372f7b1fd09.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cef80af-3b08-4873-9144-6372f7b1fd09.png)'
- en: Now, we have a function that quantifies the expected optimal value of state
    action pairs, given a policy. We can use this function to predict the optimal
    action to follow, given a game state. However, how do we assess the true label
    of our prediction? We don't exactly have our game-screens labeled with corresponding
    target actions to assess how far off the mark our network is. This is where the
    Bellman equation comes in, which helps us asses the value of a given state action
    pair as a function of both the current reward generated, as well as the value
    of the following game state. We can then use this function to compare our network's
    predictions and back-propagate the error to update the model weights.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个函数，该函数量化了在给定策略下，状态-动作对的期望最优值。我们可以使用这个函数来预测在给定游戏状态下应该采取的最优动作。然而，我们如何评估预测的真实标签呢？我们并没有给游戏屏幕标注对应的目标动作，因此无法评估我们的网络偏差有多大。这时，贝尔曼方程派上用场了，它帮助我们评估给定状态-动作对的值，作为当前产生的奖励和随后的游戏状态值的函数。然后，我们可以利用这个函数来比较网络的预测结果，并通过反向传播误差来更新模型权重。
- en: Using the Bellman equation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用贝尔曼方程
- en: The Bellman equation, which was proposed by American mathematician Richard Bellman,
    is one of the main workhorse equations powering the chariot of deep Q-learning.
    It essentially allows us to solve the Markov decision process we formalized earlier.
    Intuitively, the Bellman equation makes one simple assumption. It states that
    the maximum future reward for a given action, performed at a state, is the immediate
    reward plus the maximum future reward for the next state. To draw a parallel to
    the marshmallow experiments, the maximum possible reward of two marshmallows is
    attained by the agents through the act of abstaining at the first time step (with
    a reward of 0 marshmallows) and then collecting (with a reward of two marshmallows)
    at the second time step.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是由美国数学家理查德·贝尔曼提出的，它是驱动深度Q学习的主要方程之一。它本质上让我们能够解决之前所定义的马尔可夫决策过程。直观地说，贝尔曼方程做了一个简单的假设。它声明，对于一个给定的状态下执行的动作，最大未来奖励是当前奖励加上下一个状态的最大未来奖励。为了类比棉花糖实验，两个棉花糖的最大可能奖励是通过在第一时刻自我克制（奖励为0个棉花糖）然后在第二时刻收集（奖励为两个棉花糖）来实现的。
- en: 'In other words, given any state-action pair, the quality (Q) of performing
    an action (a) at the given state (s) is equal to the reward to be received (r)
    , along with the value of the following state (s'') that the agent ends up in.
    Thus, we can calculate the optimal action for the current state as long as we
    can estimate the optimal state-action values, *Q*(s'',a'')*, for the next time
    step. As we just saw with the marshmallow example, our agent needs to be able
    to anticipate the maximum possible reward at a future time (of two marshmallows)
    to abstain from accepting just one marshmallow at the current time. Using the
    Bellman equation, we want our agent to take actions that maximize the immediate
    reward (r), as well as the optimal Q*-value for the next state-action pair to
    come, *Q*(s'',a'')* , dampened by discount factor gamma (y). In more simple terms,
    we want it to be able to calculate the maximum expected future rewards for actions
    at the current state. This translates to the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，给定任何状态-动作对，在给定状态（s）下执行动作（a）的质量（Q）等于将要收到的奖励（r），以及智能体最终到达的下一个状态（s'）的价值。因此，只要我们能够估计下一个时间步的最优状态-动作值，*Q*(s',a')*，就能计算当前状态的最优动作。正如我们在棉花糖实验中所看到的那样，智能体需要能够预见到未来某个时刻可能获得的最大奖励（两个棉花糖），以便在当前时刻拒绝只获得一个棉花糖。使用贝尔曼方程，我们希望智能体采取最大化当前奖励（r）以及下一个状态-动作对的最优Q*值的动作，*Q*(s',a')*，并且考虑折扣因子gamma（y）。用更简单的话来说，我们希望它能够计算当前状态下动作的最大预期未来奖励。这可以转化为以下公式：
- en: '![](img/c753435a-e314-4ca8-b636-8bd4d4dc4b67.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c753435a-e314-4ca8-b636-8bd4d4dc4b67.png)'
- en: 'Now, we know how to mathematically estimate the expected quality of an action
    at a given state. We also know how to estimate the maximum expected reward of
    state-action pairs when following a specific policy. From this, we can redefine
    our optimal policy (π*) at a given state, (s), as the maximum expected Q-values
    for actions at given states. This can be shown as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道如何在数学上估计给定状态下执行一个动作的预期质量。我们也知道如何估计遵循特定策略时，状态-动作对的最大预期奖励。从这里，我们可以重新定义给定状态（s）下的最优策略（π*），即给定状态下的动作最大预期Q值。这可以表示如下：
- en: '![](img/2dc686a1-049a-4431-ade7-2f936882a02d.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dc686a1-049a-4431-ade7-2f936882a02d.png)'
- en: 'Finally, we have all of the pieces of the puzzle to actually try and find an
    optimal policy (π *) to guide our agent. This policy will allow our agent to maximize
    the expected discounted rewards (incorporating environmental stochasticity) by
    choosing ideal actions for each state the environment generates. So, how do we
    actually go about doing this? A simple, non-deep learning solution is to use a
    value iteration algorithm to calculate the quality of actions at future time steps
    *( Qt+1 ( s , a ))* as a function of expected current reward (r) and maximum discounted
    reward at the following state of the game ( *γ max a Qt ( s'' , a''))*. Mathematically,
    we can formulate this as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经具备了所有拼图的部分，实际上可以尝试找到一个最优策略（π*）来引导我们的智能体。这个策略将允许我们的智能体通过为环境生成的每个状态选择理想的动作，从而最大化预期的折扣奖励（考虑环境的随机性）。那么，我们究竟该如何进行操作呢？一个简单的非深度学习解决方案是使用价值迭代算法来计算未来时间步的动作质量*(
    Qt+1 ( s , a ))*，作为期望当前奖励（r）和下一个游戏状态的最大折扣奖励（*γ max a Qt ( s' , a' )*）的函数。从数学上讲，我们可以将其表示如下：
- en: '![](img/e0d8f2b2-a7ea-43b1-9706-0e1853cdb2cb.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0d8f2b2-a7ea-43b1-9706-0e1853cdb2cb.png)'
- en: Here, we basically update the Bellman equation iteratively until Qt converges
    to Q*, as *t* increases, ad infinitum. We can actually test out the estimation
    of the Bellman equation by naively implementing it to solve the taxi cab simulation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们基本上会不断迭代更新贝尔曼方程，直到Qt收敛到Q*，随着*t*的增加，直到无限远。我们实际上可以通过直接实现贝尔曼方程来测试它在解决出租车模拟问题中的估计效果。
- en: Updating the Bellman equation iteratively
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代更新贝尔曼方程
- en: 'You may recall that a random approach to solving the taxi cab simulation took
    our agent about 6,000 time steps. Sometimes, out of sheer luck, you may be able
    to solve it under 2,000 time steps. However, we can further tip the odds in our
    favor by implementing a version of the Bellman equation. This approach will essentially
    allow our agent to remember its actions and corresponding rewards per state by
    using a Q-table. We can implement this Q-table on Python using a NumPy array,
    with dimensions corresponding to our observation space (the number of different
    possible states) and action space (the number of different possible actions our
    agent can make) in the taxi cab environment. Recall that the taxi cab simulation
    has an environment space of 500 and an action space of six, making our Q-table
    a matrix of 500 rows and six columns. We can also initialize a reward variable
    (`R`) and a value for our discount factor, gamma:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，采用随机方法解决出租车模拟时，我们的智能体大约需要6,000个时间步。有时，凭借纯粹的运气，你可能会在2,000个时间步以内解决它。然而，我们可以通过实现一个贝尔曼方程的版本进一步提升我们的成功概率。这种方法将基本上允许我们的智能体通过使用Q表记住每个状态下的动作及其相应的奖励。我们可以在Python中使用NumPy数组实现这个Q表，数组的维度对应于我们出租车模拟中的观察空间（可能的不同状态的数量）和动作空间（智能体可以执行的不同动作的数量）。回想一下，出租车模拟的环境空间是500，动作空间是六个，因此我们的Q表是一个500行六列的矩阵。我们还可以初始化一个奖励变量（`R`）和一个折扣因子（gamma）的值：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we simply loop through a thousand episodes. Per episode, we initialize
    the state of the environment, a counter to keep track of drop-offs that have been
    performed, and the reward variables (total: `R` and episode-wise: `r`). Within
    our first loop, we nest yet another loop, instructing our agent to pick an action
    with the highest Q-value, perform the action, and store the future state of the
    environment, along with the reward that''s received. This loop is instructed to
    run until the episode is considered terminated, as indicated by the Boolean variable
    done.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需循环执行一千个回合。在每个回合中，我们初始化环境的状态，设置一个计数器来跟踪已执行的下车操作，以及奖励变量（总奖励：`R`和每回合奖励：`r`）。在我们的第一个循环中，我们再次嵌套另一个循环，指示智能体选择具有最高Q值的动作，执行该动作，并存储环境的未来状态及获得的奖励。这个循环会一直执行，直到回合结束，由布尔变量done表示。
- en: Next, we update the state-action pairs in the Q-table, along with the global
    reward variable (which indicates how well our agent performed overall). The alpha
    term (α) in the algorithm denotes a learning rate, which helps to control the
    amount of change between the previous and newly generated Q-value while performing
    the update of the Q-table. Hence, our algorithm iteratively updates the quality
    of state action pairs (Q [state, action]) through approximating optimal Q-values
    for actions at each time step. As this process keeps repeating, our agent eventually
    converges to optimal state-action pairs, as denoted by Q*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在Q表中更新状态-动作对，并更新全局奖励变量（它表示我们的智能体整体表现如何）。算法中的alpha项（α）表示学习率，它有助于控制在更新Q表时前一个Q值与新生成的Q值之间的变化量。因此，我们的算法通过在每个时间步近似最优Q值来迭代更新状态-动作对的质量（Q[state,
    action]）。随着这个过程不断重复，我们的智能体最终会收敛到最优的状态-动作对，正如Q*所示。
- en: Finally, we update the state variable, redefining the current state with the
    new state variable. Then, the loop may begin anew, iteratively updating the Q-values,
    and ideally converging to optimal state-action pairs that are stored in the Q-table.
    We print out the overall rewards that are sampled by the environment as a result
    of our agent's actions every 50 episodes. We can see that our agent eventually
    converges to the optimal possible reward for the task at hand (that is, the optimal
    reward considering the traveling costs at each time step, as well as the reward
    for a correct drop-off), which is somewhere between 9 to 13 points for this task.
    You will also notice that, by the 50^(th) episode, our agent has performed 19
    successful drop-offs in 51 time steps! This approach turns out to perform much
    better than its stochastic counterpart we implemented before.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们更新状态变量，用新的状态变量重新定义当前状态。然后，循环可以重新开始，迭代地更新Q值，并理想情况下收敛到存储在Q表中的最优状态-动作对。我们每50个回合输出一次环境采样的整体奖励，作为代理动作的结果。我们可以看到，代理最终会收敛到任务的最优奖励（即考虑到每个时间步的旅行成本，以及正确的下车奖励的最优奖励），这个任务的最优奖励大约在9到13分之间。你还会注意到，到第50^(回合)时，我们的代理在51个时间步内成功完成了19次下车！这个方法的表现明显优于我们之前实现的随机方法。
- en: Why use neural networks?
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用神经网络？
- en: As we just saw, a basic value iteration approach can be used to update the Bellman
    equation and iteratively find ideal state-action pairs to optimally navigate a
    given environment. This approach actually stores new information at each time
    step, iteratively making our algorithm more *intelligent*. However, there is a
    problem with this method as well. It's simply not scalable! The taxi cab environment
    is simple enough, with 500 states and 6 actions, to be solved by iteratively updating
    the Q-values, thereby estimating the value of each individual state-action pair.
    However, more complex simulations, like a video game, may potentially have millions
    of states and hundreds of actions, which is why computing the quality of each
    state-action pair becomes computationally unfeasible and logically inefficient.
    The only option we are left with, in such circumstances, is to try approximating
    the function *Q(a,s)* using a network of weighted parameters.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才所看到的，基本的价值迭代方法可以用来更新贝尔曼方程，并通过迭代地寻找理想的状态-动作对，从而最优地导航给定的环境。这个方法实际上在每一个时间步都会存储新的信息，不断地让我们的算法变得更加*智能*。然而，这个方法也有一个问题，那就是它根本不可扩展！出租车环境足够简单，只有500个状态和6个动作，可以通过迭代更新Q值来解决，从而估计每个状态-动作对的价值。然而，更复杂的模拟环境，比如视频游戏，可能会有数百万个状态和数百个动作，这就是为什么计算每个状态-动作对的质量变得在计算上不可行且在逻辑上低效的原因。在这种情况下，我们唯一能做的就是尝试使用加权参数的网络来逼近函数*Q(a,s)*。
- en: 'And thus, we venture into the territories of neural networks, which, as we
    are well aware of by now, make excellent function approximators. The particular
    flavor of deep reinforcement learning that we will experience shortly is known
    as deep Q-learning, which naturally gets its name from its task of learning optimal
    Q-values for given state-action pairs in an environment. More formally, we will
    use a neural network to approximate the optimal function *Q*(s,a)* through simulating
    a sequence of states, actions, and rewards for our agent. By doing this, we can
    then iteratively update our model weights (theta) in the direction that best matches
    the optimal state-action pairs for a given environment:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们进入了神经网络的领域，正如我们现在已经很清楚的，神经网络是非常优秀的函数逼近器。我们即将体验的深度强化学习的特殊形式叫做深度Q学习（Deep
    Q-learning），它的名字来源于它的任务：学习给定状态-动作对的最优Q值。更正式地说，我们将使用神经网络通过模拟代理的状态、动作和奖励序列来逼近最优函数*Q*(s,a)*。通过这样做，我们可以迭代更新我们的模型权重（theta），朝着最匹配给定环境的最优状态-动作对的方向前进：
- en: '![](img/d8250fa4-145f-4881-b4f9-cff3a1fcf568.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8250fa4-145f-4881-b4f9-cff3a1fcf568.png)'
- en: Performing a forward pass in Q-learning
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Q学习中执行前向传递
- en: 'Now, you understand the intuition behind using a neural network to approximate
    the optimal function *Q*(s,a)*, finding the best possible actions at given states.
    It goes without saying that the optimal sequence of actions, for a sequence of
    states, will generate an optimal sequence of rewards. Hence, our neural network
    is trying to estimate a function that can map possible actions to states, generating
    an optimal reward for the overall episode. As you will also recall, the optimal
    quality function *Q*(s,a)* that we need to estimate must satisfy the Bellman equation.
    The Bellman equation simply models maximum possible future reward as the reward
    at the current time, plus the maximum possible reward, at the immediately following
    time step:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你理解了使用神经网络逼近最优函数*Q*(s,a)*的直觉，找到给定状态下的最佳动作。不言而喻，状态序列的最优动作序列将生成一个最优的奖励序列。因此，我们的神经网络试图估计一个函数，可以将可能的动作映射到状态，从而为整个剧集生成最优的奖励。正如你也会回忆起的，我们需要估计的最优质量函数*Q*(s,a)*必须满足贝尔曼方程。贝尔曼方程简单地将最大可能的未来奖励建模为当前时刻的奖励加上紧接着的下一个时间步骤的最大可能奖励：
- en: '![](img/352c3c39-06cb-42d5-8b18-419f59cb8c9d.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/352c3c39-06cb-42d5-8b18-419f59cb8c9d.png)'
- en: 'Hence, we need to ensure that the conditions set forth by the Bellman equation
    are maintained when we aim to predict the optimal Q-value at a given time. To
    do this, we can define the overall loss function of this model as one that minimizes
    the error in our Bellman equation and in-play predictions. In other words, at
    each forward pass, we compute how far the current state-action quality values
    *Q (s, a ; θ)* are from the ideal ones that have been denoted by the Bellman equation
    at that time (*Y[t]*). Since the ideal predictions denoted by the Bellman equation
    are being iteratively updated, we are actually computing our model''s loss using
    a moving target variable (Y[t]). This can be formulated mathematically as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要确保在预测给定时间的最优Q值时，贝尔曼方程中规定的条件得以保持。为此，我们可以将该模型的整体损失函数定义为最小化贝尔曼方程和实际预测中的误差。换句话说，在每次前向传播时，我们计算当前状态-动作质量值*Q
    (s, a ; θ)*与由贝尔曼方程在该时间（*Y[t]*）所表示的理想值之间的差距。由于由贝尔曼方程表示的理想预测正在被迭代更新，我们实际上是在使用一个移动目标变量（Y[t]）来计算我们模型的损失。这可以通过数学公式表示如下：
- en: '![](img/069e0250-a679-40a7-b27c-34686e709dc9.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/069e0250-a679-40a7-b27c-34686e709dc9.png)'
- en: 'Hence, our network will be trained by minimizing a sequence of loss functions,
    *L[t](θt)*, changing at each time step. Here, the term *y[t]* is the target label
    for our prediction, at time (*t*), and is continuously updated at each time step.
    Also note that the term *ρ(s, a)* simply denotes the internal probability distribution
    over sequences s and actions taken by our model, also known as its behavior distribution.
    As you can see, the model weights at the previous time (*t-1*) step are kept frozen
    when optimizing the loss function at a given time (*t*). While the implementation
    shown here uses the same network for two separate forward passes, later variations
    of Q-learning (Mihn et al., 2015) use two separate networks: one to predict the
    moving target variable satisfying the Bellman equation (named target network),
    and another to compute the model''s predictions at a given time. For now, let''s
    have a look at how the backward pass updates our model weights in deep Q-learning.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的网络将通过最小化一系列损失函数*L[t](θt)*来训练，在每个时间步骤中进行更新。这里，术语*y[t]*是我们在时间（*t*）进行预测时的目标标签，并在每个时间步骤中持续更新。另请注意，术语*ρ(s,
    a)*仅表示我们模型在序列s和执行的动作上的内部概率分布，也称为其行为分布。正如你所看到的，在优化给定时间（*t*）的损失函数时，前一时刻（*t-1*）的模型权重保持不变。虽然这里展示的实现使用相同的网络进行两个独立的前向传播，但后来的Q学习变体（Mihn等，2015年）使用两个独立的网络：一个用于预测满足贝尔曼方程的移动目标变量（称为目标网络），另一个用于计算给定时间的模型预测。现在，让我们看看反向传播是如何在深度Q学习中更新我们的模型权重的。
- en: Performing a backward pass in Q-Learning
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Q学习中执行反向传播
- en: 'Now, we have a defined loss metric, which computes the error between the optimal
    Q-function (derived from the Bellman equation) and the current Q-function at a
    given time. We can then propagate our prediction errors in Q-values, backwards
    through the model layers, as our network plays about the environment. As we are
    well aware of by now, this is achieved by taking the gradient of the loss function
    with respect to model weights, and then updating these weights in the opposite
    direction of the gradient per learning batch. Hence, we can iteratively update
    the model weights in the direction of the optimal Q-value function. We can formulate
    the backpropagation process and illustrate the change in model weights (theta)
    like so:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个定义好的损失度量，它计算给定时间点最优Q函数（由Bellman方程推导得出）与当前Q函数之间的误差。然后，我们可以将Q值中的预测误差反向传播，通过模型层进行反向传播，就像我们的网络在环境中进行探索一样。正如我们现在已经非常清楚的那样，这通过对损失函数关于模型权重的梯度进行求解，并根据每个学习批次在梯度的反方向上更新这些权重来实现。因此，我们可以在最优Q值函数的方向上迭代地更新模型权重。我们可以像这样表述反向传播过程，并说明模型权重（theta）的变化：
- en: '![](img/b02f39ee-9fea-4576-9a35-b898c0f5deb6.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b02f39ee-9fea-4576-9a35-b898c0f5deb6.png)'
- en: Eventually, as the model has seen enough state action pairs, it will sufficiently
    backpropagate its errors and learn optimal representations to help it navigate
    the given environment. In other words, a trained model will have the ideal configuration
    of layer weights, corresponding to the optimal Q-value function, mapping the agent's
    actions at given states of the environment.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，随着模型看到足够的状态-动作对，它将充分反向传播误差并学习最优的表示，从而帮助其在给定环境中导航。换句话说，一个经过训练的模型将具有理想的层权重配置，对应于最优的Q值函数，映射代理在环境给定状态下的动作。
- en: Long story short, these equations describe the process of estimating an optimal
    polity (π*) to solve a given environment. We use a neural network to learn the
    best Q-values for state action pairs in the given environment, which in turn can
    be used to calculate trajectories that generate optimal rewards, for our agent
    (that is, optimal policies). This is how we can use reinforcement learning to
    train anticipatory and reactive agents operating in a quasi-random simulation
    with sparse time delayed rewards. Now, we have all of the understanding that's
    required to go ahead and implement our very own deep reinforcement learning agent.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说，这些方程描述了估算最优策略（π*）以解决给定环境的过程。我们使用神经网络学习在给定环境中状态-动作对的最佳Q值，这反过来可以用来计算生成最优奖励的轨迹，为我们的代理（即最优策略）。这就是我们如何使用强化学习来训练在稀疏时间延迟奖励的准随机模拟中运行的预期性和反应性代理。现在，我们已经掌握了所有实施我们自己的深度强化学习代理所需的理解。
- en: Replacing iterative updates with deep learning
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用深度学习替代迭代更新
- en: 'Before we move on to the implementation, let''s clarify what we have learned
    in regards to deep Q-learning so far. As we saw with the iterative update approach,
    we can use the transitions (from initial state, action performed, reward generated,
    and new state sampled, < s, a, r, s'' >) to update the Q-table holding the value
    of these tuples at each time step. However, as we mentioned, this method is not
    computationally scalable. Instead, we will replace this iterative update performed
    on the Q-table and try to approximate the optimal Q-value function (*Q*(s,a)*),
    using a neural network, like so:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始实现之前，让我们澄清一下迄今为止我们在深度Q学习方面所学到的内容。正如我们在迭代更新方法中所看到的，我们可以使用转移（从初始状态、执行的动作、生成的奖励、以及采样的新状态，<
    s, a, r, s' >）来更新Q表，在每个时间步骤保存这些元组的值。然而，正如我们提到的，这种方法在计算上不可扩展。相反，我们将替代这种在Q表上进行的迭代更新，并尝试使用神经网络近似最优的Q值函数
    (*Q*(s,a)*)，如下所示：
- en: Execute a feedforward pass using current state (*s*) as input, and then predict
    the Q-values for all of the actions at this state.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前状态（*s*）作为输入执行前馈传递，然后预测该状态下所有动作的Q值。
- en: Execute a feedforward pass using the new state (*s'*) to compute the maximum
    overall outputs of our network at the next state, that is, *max a' Q(s', a')*.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的状态（*s'*）执行前馈传递，计算网络在下一个状态下的最大总体输出，即 *max a' Q(s', a')*。
- en: With the maximum overall outputs calculated in step 2, we set the target Q-value
    for the respective action to *r + γmax a' Q(s', a')*. We also set the target Q-value
    for all other actions to the same value that's returned by step 1 for each of
    the unselected action to only compute the prediction error for of the selected
    action. This effectively neutralizes (sets to zero) the effect of the errors from
    the predicted actions that were not taken by our agent at each time step.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在步骤2中计算出最大整体输出后，我们将目标Q值设置为所选动作的*r + γmax a' Q(s', a')*。我们还将所有其他动作的目标Q值设置为步骤1返回的相同值，针对每个未选择的动作，仅计算所选动作的预测误差。这有效地消除了（设为零）未被我们的智能体在每个时间步选择的预测动作带来的误差影响。
- en: Backpropagate the error to update the model weights
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播误差以更新模型权重
- en: All we did here is build a network that's capable of making predictions on a
    moving target. This is useful since our model iteratively comes across more information
    about the physics of the environment as it plays the game. This is reflected by
    the fact that our target outputs (what actions to perform, at a given state) also
    keep changing, unlike in supervised learning, where we have fixed outputs that
    we call labels. Hence, we are actually trying to learn a function *Q*(s,a) that *can
    learn the mapping between constantly changing inputs (game states) and outputs
    (corresponding actions to take).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的只是建立一个能够对动态目标进行预测的网络。这非常有用，因为随着模型在玩游戏的过程中不断获得更多的环境物理信息，它的目标输出（在给定状态下执行的动作）也不断变化，这不同于监督学习，在监督学习中我们有固定的输出，称之为标签。因此，我们实际上是在尝试学习一个可以学习**不断变化的输入（游戏状态）与输出（相应动作）之间映射关系的函数Q(s,a)**。
- en: Through this process, our model develops better intuitions on what actions to
    perform and gain a better idea of the correct Q-values for state-action pairs
    as it sees more of the environment. In theory, Q-learning allows us to address
    the credit assignment problem by correlating rewards to actions that have been
    taken at previous game states. The errors are backpropagated until our model is
    able to identify decisive state-action pairs that are responsible for generating
    a given reward. However, we will soon see that a lot of computational and mathematical
    tricks are employed to make deep Q-learning systems work as well as they do. Before
    we dive into these considerations, it may be helpful to further explore the forward
    and backward passes that occur in a deep Q-network.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程，我们的模型在执行动作时形成更好的直觉，并随着它看到更多的环境信息，获得关于状态-动作对正确Q值的更清晰认识。从理论上讲，Q学习通过将奖励与在先前游戏状态中所采取的动作相关联来解决信用分配问题。误差会被反向传播，直到我们的模型能够识别出决定性的状态-动作对，这些对负责生成给定的奖励。然而，我们很快会看到，为了让深度Q学习系统如预期那样工作，使用了许多计算和数学技巧。在我们深入研究这些考虑因素之前，了解深度Q网络中前向和反向传播的过程可能会有所帮助。
- en: Deep Q-learning in Keras
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的深度Q学习
- en: Now that we understand how to train an agent to select optimal state action
    pairs, let's try to solve a more complex environment than the taxi cab simulation
    we dealt with previously. Why not implement a learning agent to solve a problem
    that was originally crafted for humans themselves? Well, thanks to the wonders
    of the open source movement, that is exactly what we will do. Next on our task
    list, we will implement the methodologies of Mnih et al. (2013, and 2015) referring
    to the original DeepMind paper that implemented a Q-learning based agent. The
    researchers used the same methodology and neural architecture to play seven different
    Atari games. Notably, the researchers achieved remarkable results for six of the
    seven different games it was tested on. In three out of these six games, the agent
    was noted to outperform a human expert. This is why, today, we try and partially
    replicate these results and train a neural network to play some old-school games
    like Space Invaders and Ms. Pacman.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何训练智能体选择最佳的状态-动作对，接下来让我们尝试解决一个比之前的出租车模拟更复杂的环境。为什么不实现一个学习智能体来解决一个最初为人类设计的问题呢？好吧，感谢开源运动的奇迹，这正是我们将要做的。接下来，我们将实施Mnih等人（2013年和2015年）的方法，参考原始的DeepMind论文，该论文实现了基于Q学习的智能体。研究人员使用相同的方法和神经架构来玩七种不同的Atari游戏。值得注意的是，研究人员在七个测试的不同游戏中，有六个取得了显著的成绩。在这六个游戏中的三款，智能体的表现超过了人类专家。这就是为什么今天，我们试图部分复现这些结果，并训练一个神经网络来玩一些经典游戏，如《太空入侵者》和《吃豆人》。
- en: This is done by using a **convolutional neural network** (**CNN**), which takes
    video game screenshots as input and estimates the optimal Q values for actions
    given states of the game. To follow along, all you need to do is install the reinforcement
    learning package built on top of Keras, known as `keras-rl`. You will also require
    the Atari dependency for the OpenAI `gym` module, which we used previously. The
    Atari dependency is essentially an emulator for the Atari console that will generate
    our training environments. While the dependency was originally designed to run
    on the Ubuntu operating system, it has since been ported to be compatible Windows
    and Mac users alike. You can install both modules for the following experiments
    using the `pip` package manager.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用**卷积神经网络**（**CNN**）完成的，该网络将视频游戏截图作为输入，并估计给定游戏状态下动作的最优Q值。为了跟上进度，你只需要做的是安装建立在Keras基础上的强化学习包，名为`keras-rl`。你还需要为OpenAI
    `gym`模块安装Atari依赖，这是我们之前使用过的。Atari依赖本质上是一个为Atari主机设计的模拟器，将生成我们的训练环境。虽然该依赖最初是为Ubuntu操作系统设计的，但它已经被移植并与Windows和Mac用户兼容。你可以使用`pip`包管理器安装这两个模块，以便进行以下实验。
- en: 'You can install the Keras reinforcement learning package with the following
    command:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用以下命令安装Keras强化学习包：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can install the Atari dependency for Windows with the following command:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用以下命令为Windows安装Atari依赖：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Mnih et al (2015) [https://arxiv.org/pdf/1312.5602v1.pdf](https://arxiv.org/pdf/1312.5602v1.pdf)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih等人（2015）[https://arxiv.org/pdf/1312.5602v1.pdf](https://arxiv.org/pdf/1312.5602v1.pdf)
- en: Making some imports
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入一些库
- en: 'In the realm of machine intelligence, it has been a long-standing dream to
    achieve human-level control for tasks like gaming. The complexity involved in
    automating an agent, operating only on high-dimensional sensory inputs (like audio,
    images, and so on), has been quite a challenging to accomplish with reinforcement
    learning. Previous approaches heavily relied on hand-crafted features, combined
    with linear policy representations that relied too much on the quality of the
    engineered features, to perform well. Unlike the previous attempts, this technique
    does not require our agent to have any human-engineered knowledge about the game.
    It will solely rely on the pixel inputs it receives and encode representations
    to predict the optimal Q-value for each possible action at each state of the environment
    it traverses. Pretty cool, no? Let''s import the following libraries into our
    workspace to see how we can proceed with this task:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器智能领域，实现人类水平的控制一直是一个长期的梦想，尤其是在游戏等任务中。涉及到的复杂性包括自动化智能体的操作，该操作仅依赖于高维感知输入（如音频、图像等），这在强化学习中一直是一个非常具有挑战性的任务。之前的方法主要依赖于手工设计的特征，结合了过于依赖工程特征质量的线性策略表示，以便取得良好的表现。与以往的尝试不同，这种技术不需要我们的智能体拥有任何关于游戏的人工设计知识。它将完全依赖它所接收到的像素输入，并编码表示，以预测在其遍历的每个环境状态下每个可能动作的最优Q值。挺酷的，是吧？让我们将以下库导入到工作区，以便继续进行此任务：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Preprocessing techniques
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理技术
- en: As we mentioned previously, we will be using a CNN to encode representative
    visual features about each state that's shown to our agent. Our CNN will proceed
    to regress these higher-level representations against optimal Q-values, corresponding
    to optimal actions to be taken for each given state. Hence, we must show our network
    a sequence of inputs, corresponding to a sequence of screenshots you would see,
    when playing an Atari game.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们将使用卷积神经网络（CNN）对展示给我们智能体的每个状态进行编码，提取代表性的视觉特征。我们的CNN将继续回归这些高级表示与最优Q值，这些Q值对应于每个给定状态下应该采取的最优行动。因此，我们必须向我们的网络展示一系列输入，
    corresponding to the sequence of screenshots you would see, when playing an Atari
    game.
- en: 'Were we playing the game of Space Invaders (Atari 2600), these screenshots
    would look something like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在玩《太空侵略者》（Atari 2600），这些截图大致会是这样的：
- en: '![](img/ca81f847-74e0-4b70-8d0d-d60eea49f25a.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca81f847-74e0-4b70-8d0d-d60eea49f25a.png)'
- en: The original Atari 2600 screen frames, which were designed to be aesthetically
    pleasing to the human palate from the 70s era, exist in the dimensions of 210
    x 160 pixels, with a color scheme of 128\. While it may be computationally demanding
    to process these raw frames in sequence, note that there is a lot of opportunity
    for downsampling our training images from these frames to work with more manageable
    representations. Indeed, this follows the approach taken by Minh et al. to reduce
    the input dimensions to a more manageable size. This is achieved by downsampling
    the original RGB image to a greyscale image with 110 x 84 pixels, before cropping
    out the extremities of the image where nothing much happens. This leaves us with
    our final image size of 84 x 84 pixels. This reduction in dimensionality helps
    our CNN better encode representative visual features, following the theory we
    covered in Chapter 4, *Convolutional Neural Networks*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Atari 2600游戏画面帧，设计时旨在让人眼前一亮，符合70年代的美学，尺寸为210 x 160像素，色彩方案为128色。尽管按顺序处理这些原始帧可能在计算上要求较高，但请注意，我们可以从这些帧中对训练图像进行降采样，从而得到更易于处理的表示。实际上，这正是Minh等人所采用的方法，将输入维度降至更易管理的大小。这是通过将原始RGB图像降采样为110
    x 84像素的灰度图像实现的，然后裁剪掉图像的边缘部分，因这些部分没有太多变化。这使得我们最终得到了84 x 84像素的图像大小。这一维度的减少有助于我们的CNN更好地编码代表性的视觉特征，遵循我们在第四章《卷积神经网络》中讲解的理论。
- en: Defining input parameters
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义输入参数
- en: 'Finally, our convolutional network will also receive these cropped images in
    batches of four at a time. Using these four frames, the neural network will be
    asked to estimate the optimal Q-values for the given input frame. Hence, we define
    our input shape, referring to the size of the pre-processed 84 x 84 game screen
    frames. We also define a window length of `4`, which simply refers to the number
    of images our network sees at a time. For each image, the network will make a
    scalar prediction for the optimal Q-value, which maximizes the expected future
    rewards that are attainable by our agent:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们的卷积神经网络将按批次接收这些裁剪后的图像，每次四张。使用这四帧图像，神经网络将被要求估算给定输入帧的最优Q值。因此，我们定义了我们的输入形状，指的是经过预处理的84
    x 84游戏画面帧的大小。我们还定义了一个窗口长度为`4`，它仅仅指的是我们的网络每次看到的图像数量。对于每一张图像，网络将对最优Q值进行标量预测，该Q值最大化了我们智能体可以获得的预期未来奖励：
- en: '![](img/bf877265-f575-4d99-9b14-5a02378aa151.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf877265-f575-4d99-9b14-5a02378aa151.png)'
- en: Making an Atari game state processor
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Atari游戏状态处理器
- en: Since our network is only allowed to observe the state of the game through the
    input images, we must first construct a Python class that lets our **deep-Q learning
    agent** (**DQN**) processes the states and the rewards that are generated by the
    Atari emulator. This class will accept a processor object, which simply refers
    to the coupling mechanism between an agent and its environment, as implemented
    in the `keras-rl` library.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的网络只能通过输入图像观察游戏状态，我们必须首先构建一个Python类，允许我们的**深度Q学习智能体**（**DQN**）处理由Atari模拟器生成的状态和奖励。这个类将接受一个处理器对象，处理器对象指的是智能体与其环境之间的耦合机制，正如在`keras-rl`库中实现的那样。
- en: 'We are creating the `AtariProcessor` class as we want to use the same network
    to perform in different environments, each with different types of states, actions,
    and rewards. What''s the intuition behind this? Well, think of the difference
    in game screen and possible moves between a Space Invaders game versus a Pacman
    game. While the defender in the space invaders game can only scroll sideways and
    fire, Pacman can move up, down, left, and right to respond to the different states
    of its environment. A custom processor class helps us streamline the training
    process between different games, without performing too many modifications on
    the learning agent or on the observed environment. The processor class we will
    implement will allow us to simplify the processing of different game states and
    rewards that are generated through the agent acting upon the environment:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建`AtariProcessor`类，因为我们希望使用相同的网络在不同的环境中进行操作，每个环境具有不同类型的状态、动作和奖励。这背后的直觉是什么呢？嗯，想想太空入侵者游戏和吃豆人游戏之间的游戏画面和可能的操作差异。虽然太空入侵者游戏中的防御者只能左右移动并开火，但吃豆人可以上下左右移动，以应对环境的不同状态。自定义的处理器类帮助我们简化不同游戏之间的训练过程，而不必对学习智能体或观测环境做过多修改。我们将实现的处理器类将允许我们简化不同游戏状态和奖励的处理，这些状态和奖励是通过智能体对环境的作用生成的：
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Processing individual states
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理单独的状态
- en: Our processor class includes three simple functions. The first function (`process_observation`)
    takes an array representing a simulated game state and converts it into images.
    The images are then resized, converted back into an array, and returned as a manageable
    datatype to the experience memory (a concept we will elaborate upon shortly).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的处理器类包含三个简单的函数。第一个函数（`process_observation`）接收一个表示模拟游戏状态的数组，并将其转换为图像。然后，这些图像被调整大小、转换回数组，并作为可管理的数据类型返回给经验记忆（这是一个我们稍后将详细说明的概念）。
- en: Processing states in batch
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量处理状态
- en: Next, we have the (`process_state_batch`) function, which processes the images
    in batch and returns them as a `float32` array. While this step could also be
    achieved in the first function, the reason we do it separately is to achieve higher
    computational efficiency. As simple mathematics dictates, storing a `float32`
    array is four times more memory intensive than storing an 8-bit array. Since we
    want our observations to be stored in experience memory, we would rather store
    them in manageable representations. Doing so becomes especially important when
    processing the millions of states of a given environment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个（`process_state_batch`）函数，它批量处理图像并将其作为`float32`数组返回。虽然这个步骤也可以在第一个函数中实现，但我们单独处理的原因是为了提高计算效率。按照简单的数学规律，存储一个`float32`数组比存储一个8位数组需要更多四倍的内存。由于我们希望将观察结果存储在经验记忆中，因此我们宁愿将它们存储为可管理的表示形式。当处理给定环境中的数百万个状态时，这一点变得尤为重要。
- en: Processing rewards
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理奖励
- en: Finally, the last function in our class lets us clip the rewards that are generated
    from the environment by using the (`process_reward`) function. Why do this? Well,
    let's consider a little bit of background information. While we let our agent
    train on the real, unmodified game, this change to the reward structure is performed
    during training only. Instead of letting our agent use the actual score from the
    game screen, we can fix positive and negative rewards to +1 and -1, respectively.
    A reward of 0 is not influenced by this clipping operation. Doing so is practically
    useful as it lets us limit the scale of the derivatives as we backpropagate our
    network's errors. Moreover, it becomes easier to implement the same agent on a
    different learning environment since the agent does not have to learn a new scoring
    scheme for an entirely new type of game.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们类中的最后一个函数使用（`process_reward`）函数对从环境中生成的奖励进行裁剪。为什么这么做呢？让我们先考虑一下背景信息。当我们让智能体在真实且未经修改的游戏中进行训练时，这种奖励结构的变化仅在训练过程中进行。我们不会让智能体使用游戏画面中的实际得分，而是将正奖励和负奖励分别固定为+1和-1。奖励为0的部分不受此裁剪操作的影响。这样做在实践中非常有用，因为它可以限制我们在反向传播网络误差时的导数规模。此外，由于智能体不需要为全新的游戏类型学习新的评分方案，因此在不同学习环境中实现相同的智能体变得更加容易。
- en: Limitations of reward clipping
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励裁剪的局限性
- en: One clear downside of clipping rewards, as noted in the DeepMind paper (Minh
    et al, 2015), is that this operation prohibits our agent from being able to differentiate
    rewards of differing magnitude. Such a notion will certainly be relevant for even
    more complex simulations. Consider a real self-driving car, for instance. The
    artificial agent in control may need to assess the magnitudes of reward/penalty
    for dilemmatic actions it may have to take, given a state of the environment.
    Perhaps the agent faces actions like running over a pedestrian to avoid a more
    disastrous accident on the road. This limitation, however, does not seem to severely
    affect our agent's ability to conquer the simpler learning environment that's
    offered by Atari 2600 games.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如DeepMind论文（Minh等，2015）所指出，奖励裁剪的一个明显缺点是，这个操作使得我们的智能体无法区分不同大小的奖励。这一点对于更复杂的模拟环境来说肯定会有影响。例如，考虑一辆真实的自动驾驶汽车。控制的人工智能智能体可能需要评估在给定环境状态下，采取困境性行动时，奖励/惩罚的大小。也许该智能体面临的选择是为了避免更严重的事故，选择撞到行人。这种局限性，然而，对我们智能体在Atari
    2600游戏中应对简单学习环境的能力似乎并不会产生严重影响。
- en: Initializing the environment
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化环境
- en: 'Next, we simply initialize the space invaders environment using the Atari dependency
    (separate import not necessary) we added earlier to the available `gym` environments:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需使用之前添加到可用`gym`环境中的Atari依赖（无需单独导入），初始化太空入侵者环境：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We also generate a random seed to consistently initialize the state of the environment
    so that it has reproduceable experiments. Finally, we define a variable pertaining
    to the number of actions that can be taken by our agent at any given time.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还生成了一个随机种子来始终如一地初始化环境的状态，以便进行可重复的实验。最后，我们定义了一个变量，表示代理在任何给定时刻可以执行的动作数量。
- en: Building the network
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络
- en: Thinking about our problem intuitively, we are designing a neural network that
    takes in a sequence of game states that have been sampled from an environment.
    At each state of the sequence, we want our network to predict the action with
    the highest Q-value. Hence, the output of our network will refer to Q-values per
    action, for each possible game state. Hence, we first define a few convolutional
    layers with an increasing number of filters and decreasing stride-lengths as the
    layers progress. All of these convolutional layers are implemented with a **Rectified
    Linear Unit** (**ReLU**) activation function. Following these, we add a flatten
    layer to reduce the dimensions of the outputs from our convolutional layers to
    vector representations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上思考我们的这个问题，我们正在设计一个神经网络，该网络接收来自环境的游戏状态序列。在序列的每个状态下，我们希望我们的网络预测出具有最高 Q 值的动作。因此，我们网络的输出将指代每个可能游戏状态下的每个动作的
    Q 值。因此，我们首先定义了几个卷积层，随着层数的增加，滤波器数量逐渐增多，步幅长度逐渐减少。所有这些卷积层都使用**修正线性单元** (**ReLU**)
    激活函数。接下来，我们添加了一个展平层，将卷积层的输出维度缩减为向量表示。
- en: 'These representations are then fed to two densely connected layers that perform
    the regression of game states against Q-values for the actions that are available:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些表示会被输入到两个密集连接的层中，这些层执行游戏状态与动作的 Q 值回归：
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally, you will notice that our output layer is a densely connected one, with
    a number of neurons corresponding to our agent's action space (that is, the number
    of actions it may perform). This layer also has a linear activation function,
    just like the regression examples we saw previously. This is because our network
    is essentially performing a sort of a multi-variate regression, where it uses
    its feature representations to predict the highest Q-value for each action the
    agent may take at the given input state.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你会注意到我们的输出层是一个密集连接的层，包含与我们代理的动作空间相对应的神经元数量（即它可能执行的动作数）。这个层也采用了线性激活函数，就像我们之前看到的回归示例一样。这是因为我们的网络本质上在执行一种多变量回归，它利用其特征表示来预测代理在给定输入状态下每个动作的最高
    Q 值。
- en: Absence of pooling layers
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺乏池化层
- en: 'Another difference you may have noticed from previous CNN examples is the absence
    of pooling layers. Previously, we used pooling layers to downsample the activation
    maps produced by each convolutional layer. As you will recall from Chapter 4, *Convolutional
    Neural Networks*, these pooling layers helped us to implement the notion of spatial
    invariance to different types of inputs our CNN. However, when implementing a
    CNN for our particular use case, we may not want to discard information that''s
    specific to the spatial location of representations, as this may actually be an
    integral part of identifying the correct move for our agent:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还注意到与之前的 CNN 示例相比的另一个区别是缺少池化层。之前，我们使用池化层对每个卷积层生成的激活图进行下采样。正如你在第 4 章《卷积神经网络》中回忆的那样，这些池化层帮助我们实现了对不同类型输入的空间不变性。然而，在为我们特定的使用案例实现
    CNN 时，我们可能不希望丢弃特定于表示空间位置的信息，因为这实际上可能是识别代理正确动作的一个重要部分。
- en: '![](img/2cada956-3eb7-4acf-a749-4faddb16e3fe.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cada956-3eb7-4acf-a749-4faddb16e3fe.png)'
- en: As you can see in the two almost identical images, the location of the projectile,
    which is fired by the space invaders, significantly alters the game state for
    our agent. While the agent is far enough to avoid this projectile in the first
    image, it may meet its doom by making one wrong move (moving to its right) in
    the second image. Since we would like it to be able to significantly distinguish
    between these two states, we avoid the use of pooling layers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在这两张几乎相同的图片中所看到的，太空侵略者发射的弹丸的位置显著地改变了我们代理的游戏状态。当代理在第一张图中足够远以避开这个弹丸时，它可能在第二张图中因犯一个错误（向右移动）而迎来末日。由于我们希望它能够显著区分这两种状态，因此我们避免使用池化层。
- en: Problems with live learning
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时学习中的问题
- en: As we mentioned earlier, our neural network will process a sequence of four
    frames at a time and regress these inputs to actions with the highest Q-value
    for each individual state (that is, image) that's sampled from the Atari emulator.
    However, if we do not shuffle the order in which our network receives each batch
    of four images, then our network runs into some pretty vexing problems during
    the learning process.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们的神经网络将一次处理四帧图像，并将这些输入回归到每个个体状态（即从Atari模拟器中采样的图像）对应的最高Q值动作。然而，如果我们不打乱网络接收每个四张图像批次的顺序，那么在学习过程中，网络会遇到一些相当棘手的问题。
- en: The reason we do not want our network to learn from consecutive batches of samples
    is because these sequences are locally correlated. This is a problem since the
    network parameters, at any given time, will determine the next training examples
    that are generated by the emulator. Given the Markov assumption, the probability
    of future game states are dependent on the current game state. Hence, if the current
    maximizing action dictates our agent to move to the right, then the following
    training samples in the batch would be dominated by the agent moving right, causing
    bad and unnecessary feedback loops. Moreover, consecutive training samples are
    often too similar for the network to effectively learn from them. These issues
    will likely cause our network's loss to converge to a local (rather than global)
    minimum during the training process. So, how exactly do we counter this?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望网络从连续批次的样本中学习的原因是，这些序列是局部相关的。这是一个问题，因为网络的参数在任何给定时刻都会决定由模拟器生成的下一批训练样本。根据马尔可夫假设，未来的游戏状态概率依赖于当前的游戏状态。因此，如果当前的最大化动作要求我们的智能体向右移动，那么接下来批次中的训练样本将会主要是智能体向右移动，导致糟糕且不必要的反馈循环。而且，连续的训练样本通常过于相似，网络难以从中有效学习。这些问题可能会导致网络的损失在训练过程中收敛到局部（而不是全局）最小值。那么，我们到底该如何应对这个问题呢？
- en: Storing experience in replay memory
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储经验到重放记忆中
- en: The answer lies in the idea of forging a replay memory for our network. Essentially,
    the replay memory can act as a fixed length *experience* *que* of sorts. It can
    be used to store the sequential states of the game being played, along with the
    actions made, reward generated, and the state that's returned to the agent. These
    experience ques are continuously updated to maintain *n* most recent states of
    the game. Then, our network will use randomized batches of experience tuples (`state`,
    `action`, `reward`, and `next state`) that are saved in replay memory to perform
    gradient decent.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案在于为我们的网络构建重放记忆的概念。本质上，重放记忆可以充当一种固定长度的*经验* *队列*。它可以用来存储正在进行的游戏的连续状态，连同所采取的动作、生成的奖励和返回给智能体的状态。这些经验队列会不断更新，以保持*最近*的*n*个游戏状态。然后，我们的网络将使用从重放记忆中保存的随机批次经验元组（`state`，`action`，`reward`，和`next
    state`）来执行梯度下降。
- en: 'There are different types of replay memory implementations available in `rl.memory`,
    the `keras-rl` module. We use the `SequentialMemory` object to accomplish our
    purpose. This takes two parameters, as shown here:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在`rl.memory`，即`keras-rl`模块中，提供了不同类型的重放记忆实现。我们使用`SequentialMemory`对象来实现我们的目的。这个对象需要两个参数，如下所示：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The limit parameter denotes the number of entries to be held in memory. Once
    the limit is exceeded, newer entries will replace older ones. The `window_length`
    parameter simply refers to the number of training samples per batch.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`limit`参数表示要保存在记忆中的条目数。一旦超出此限制，新的条目将替换旧的条目。`window_length`参数仅指每个批次中的训练样本数量。'
- en: Due to the random order of the experience tuple batches, the network is less
    likely to get entrenched in a local minimum and will eventually converge to find
    optimal weights, representing the optimal policy for a given environment. Furthermore,
    using non-sequential batches to perform weight updates means that we achieve higher
    data efficiency, as the same individual image can be shuffled into different batches,
    contributing to multiple weight updates. Lastly, these experience tuples can even
    be collected from human gameplay data, rather than the previous moves that were
    executed by the network.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于经验元组批次的随机顺序，网络不容易陷入局部最小值，并最终会收敛找到最优权重，代表给定环境的最优策略。此外，使用非顺序批次进行权重更新意味着我们可以实现更高的数据效率，因为同一张图像可以被打乱到不同的批次中，从而贡献多次权重更新。最后，这些经验元组甚至可以从人类游戏数据中收集，而不是之前由网络执行的动作。
- en: Other approaches (Schaul et al., 2016: [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952))
    have implemented a prioritized version of experience replay memory by adding an
    additional data structure that keeps track of the priority of each transition
    (*state -> action -> reward -> next-state*) in order to replay important transitions
    more frequently. The intuition behind this is to make the network learn from its
    best and worst performances more often, rather than instances where not much learning
    can occur. While these are some clever approaches that help our model converge
    to relevant representations, we also want it to surprise us from time to time
    and explore opportunities it hasn't considered yet. This brings us back to, *The
    explore-exploit dilemma* that we discussed earlier.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法（Schaul等人，2016年：[https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)）通过添加一个额外的数据结构，记录每个转换（*状态
    -> 动作 -> 奖励 -> 下一状态*）的优先级，实现了优先级版本的经验重放记忆，以便更频繁地重放重要的转换。其背后的直觉是让网络更频繁地从其最佳和最差的表现中学习，而不是从那些无法产生多少学习的实例中学习。尽管这些是一些巧妙的方法，有助于我们的模型收敛到相关的表示，但我们也希望它时不时地给我们带来惊喜，并探索它尚未考虑过的机会。这让我们回到了之前讨论过的，*探索-开发困境*。
- en: Balancing exploration with exploitation
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡探索与开发
- en: How can we ensure that our agent relies on a good balance of old and new strategies?
    This problem is made worse through the random initialization of weights for our
    Q-network. Since the predicted Q-values are a result of these random weights,
    the model will generate sub-optimal predictions at the initial training epochs,
    which in turn results in poor Q-value learning. Naturally, we don't want our network
    to rely too much on strategies it generates at first for given state-action pairs.
    Just like the dopamine addicted rat, the agent cannot be expected to perform well
    in the long term if it doesn't explore new strategies and expand its horizons
    instead of exploiting known strategies. To address this problem, we must implement
    a mechanism that encourages the agent to try out new actions, ignoring the learned
    Q-values. Doing so basically allows our learning agent to try out new strategies
    that may potentially be more beneficial in the long run.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确保我们的代理在旧策略和新策略之间保持良好的平衡呢？这个问题在我们的Q网络随机初始化权重时变得更加复杂。由于预测的Q值是这些随机权重的结果，模型在初期训练时会生成次优的预测，进而导致Q值学习较差。自然，我们不希望我们的网络过于依赖它最初为给定状态-动作对生成的策略。就像多巴胺成瘾的老鼠一样，如果代理不探索新策略并扩展其视野，而只是利用已知的策略，它不可能在长期内表现良好。为了解决这个问题，我们必须实现一种机制，鼓励代理尝试新的动作，忽略已学得的Q值。这样做基本上允许我们的学习代理尝试新的策略，这些策略可能在长期内更有利。
- en: Epsilon-greedy exploration policy
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Epsilon-贪婪探索策略
- en: This can be achieved by algorithmically modifying the policy that's used by
    our learning agent to solve its environment. A common approach to this is using
    an epsilon-greedy exploration strategy. Here, we define a probability (ε). Then,
    our agent may ignore the learnt Q-values and try a random action with a probability
    of (1 - ε). Hence, if the epsilon value is set to 0.5, our network will, on average,
    ignore actions suggested by its learnt Q-Table and do something random. This is
    quite an exploratory agent. Conversely, a value of 0.001 for epsilon will make
    the network more consistently rely on the learned Q-values, picking random actions
    in only one out of a hundred time steps on average.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过算法修改我们学习代理解决环境问题时使用的策略来实现。一个常见的方法是使用epsilon-贪婪探索策略。在这里，我们定义一个概率（ε）。然后，我们的代理可能会忽略学习到的Q值，并以（1
    - ε）的概率尝试一个随机动作。因此，如果epsilon值设置为0.5，那么我们的网络平均将忽略其学习的Q表所建议的动作，做一些随机的事情。这是一个相当具有探索性的代理。相反，epsilon值为0.001时，网络将更一致地依赖学习到的Q值，平均每一百个时间步中才随机选择一次动作。
- en: 'A fixed ε value is rarely used as the degree of exploration versus exploitation
    to implement can differ based on many internal (for example, agents learning rate)
    and external factors (for example, the degree of randomness versus determinism
    in a given environment). In the DeepMind paper, the researchers implemented a
    decaying ε term over time, starting from 1 (that is not relying at all upon the
    initial random predictions) to 0.1 (relying on predicted Q-values 9 out of 10
    times):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 很少使用固定的 ε 值，因为探索与利用的程度可以基于许多内部（例如，代理的学习率）和外部因素（例如，给定环境中的随机性与确定性的程度）有所不同。在 DeepMind
    论文中，研究人员实现了一个随时间衰减的 ε 项，从 1（即完全不依赖初始随机预测）到 0.1（在 10 次中有 9 次依赖于预测的 Q 值）：
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Hence, a decaying epsilon ensures that our agent does not rely upon the random
    predictions at the initial training epochs, only to later on exploit its own predictions
    more aggressively as the Q-function converges to more consistent predictions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，衰减的 epsilon 确保我们的代理不会在初期训练阶段依赖于随机预测，而是在 Q 函数逐渐收敛到更一致的预测后，逐渐更积极地利用自身的预测。
- en: Initializing the deep Q-learning agent
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化深度 Q 学习代理
- en: 'Now, we have programmatically defined all of the individual components that
    are necessary to initialize our deep Q-learning agent. For this, we use the imported
    `DQNAgent` object from `rl.agents.dqn` and defined the appropriate parameters,
    as shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经通过编程定义了初始化深度 Q 学习代理所需的所有个体组件。为此，我们使用从 `rl.agents.dqn` 导入的 `DQNAgent`
    对象，并定义了相应的参数，如下所示：
- en: '[PRE17]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding parameters are initialized following the original DeepMind paper.
    Now, we are ready to finally compile our model and initiate the training process.
    To compile the model, we can simply call the compile method on our `dqn` model
    object:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 前述参数是根据原始 DeepMind 论文初始化的。现在，我们准备好最终编译我们的模型并启动训练过程了。为了编译模型，我们只需要在我们的 `dqn` 模型对象上调用
    compile 方法：
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `compile` method here takes an optimizer and the metric we want to track
    as arguments. In our case, we choose the `Adam` optimizer with a low learning
    rate of `0.00025` and track the **Mean Absolute Error** (**MAE**) metric, as shown
    here.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `compile` 方法需要传入一个优化器和我们希望跟踪的度量标准作为参数。在我们的例子中，我们选择了学习率为 `0.00025` 的 `Adam`
    优化器，并跟踪 **平均绝对误差** (**MAE**) 度量，具体如图所示。
- en: Training the model
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now, we can initiate the training session for our deep Q-learning network.
    We do this by calling the `fit` method on our compiled DQN network object. The
    `fit` parameter takes the environment being trained on (in our case, the SpaceInvaders-v0)
    and the number of total game steps (similar to epoch, denoting the total number
    of game states to sample from the environment) during this training session, as
    arguments. You may choose to define the optional parameter `visualize` as `True` if
    you wish to visualize how well your agent is doing as it trains. While this is
    quite fun—even a tad mesmerizing to observe—it significantly affects training
    speed, and hence is not practical to have as a default:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以启动我们的深度 Q 学习网络的训练过程。我们通过调用已编译的 DQN 网络对象上的 `fit` 方法来实现这一点。`fit` 参数需要传入正在训练的环境（在我们这个例子中是
    SpaceInvaders-v0）以及在此次训练过程中总的游戏步数（类似于 epoch，表示从环境中采样的游戏状态总数）作为参数。如果你希望可视化训练过程中代理的表现，可以选择将可选参数
    `visualize` 设置为 `True`。虽然这非常有趣——甚至有点让人着迷——但它会显著影响训练速度，因此不建议将其设置为默认选项：
- en: '[PRE19]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Testing the model
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'We test the model using the following code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码测试模型：
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Summarizing the Q-learning algorithm
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结 Q 学习算法
- en: 'Congratulations! You have now achieved a detailed understanding behind the
    concept of deep Q-learning and have applied these concepts to make a simulated
    agent incrementally learn to solve its environment. The following pseudocode is
    provided as a refresher to the whole deep Q-learning process we just implemented:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在已经深入理解了深度 Q 学习的概念，并将这些概念应用于使模拟代理逐步学习解决其环境。以下伪代码作为我们刚刚实现的整个深度 Q 学习过程的回顾：
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Double Q-learning
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重 Q 学习
- en: 'Another augmentation to the standard Q-learning model we just built is the
    idea of Double Q-learning, which was introduced by Hado van Hasselt (2010, and
    2015). The intuition behind this is quite simple. Recall that, so far, we were
    estimating our target values for each state-action pair using the Bellman equation
    and checking how far off the mark our predictions are at a given state, like so:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建的标准Q-learning模型的另一个扩展是Double Q-learning的思想，该方法由Hado van Hasselt（2010年，及2015年）提出。其背后的直觉非常简单。回顾一下，到目前为止，我们通过Bellman方程估计每个状态-动作对的目标值，并检查在给定状态下我们的预测偏差，如下所示：
- en: '![](img/560856e1-103a-48e8-a5a8-f305404472cc.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/560856e1-103a-48e8-a5a8-f305404472cc.png)'
- en: 'However, a problem arises from estimating the maximum expected future reward
    in this manner. As you may have noticed earlier, the max operator in the target
    equation (*y[t]*) uses the same Q-values to evaluate a given action as the ones
    that are used to predict a given action for a sampled state. This introduces a
    propensity for overestimation of Q-values, eventually even spiraling out of control.
    To compensate for such possibilities, Van Hasselt et al. (2016) implemented a
    model that decoupled the selection of actions from the evaluation thereof. This
    is achieved using two separate neural networks, each parametrized to estimate
    a subset of the entire equation. The first network is tasked with predicting the
    actions to take at given states, while a second network is used to generate the
    targets by which the first network''s predictions are evaluated as the loss is
    computed iteratively. Although the formulation of the loss at each iteration does
    not change, the target label for a given state can now be represented by the augmented
    Double DQN equation, as shown here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从这种方式估算最大期望未来奖励会出现一个问题。如你可能在之前注意到的那样，目标方程中的最大运算符（*y[t]*）使用相同的Q值来评估给定动作，这些Q值也被用于预测采样状态下的给定动作。这引入了Q值过度估计的倾向，最终可能失控。为了解决这种可能性，Van
    Hasselt等人（2016）实施了一个模型，将动作选择与其评估解耦。这是通过使用两个独立的神经网络来实现的，每个网络都被参数化以估算整个方程的一个子集。第一个网络负责预测给定状态下应该采取的动作，而第二个网络则用来生成目标，供第一个网络的预测在计算损失时迭代评估。尽管每次迭代时损失的公式没有变化，但给定状态的目标标签现在可以通过增强的Double
    DQN方程来表示，如下所示：
- en: '![](img/c0f29994-cacf-41c3-9c83-7154ab59cac1.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0f29994-cacf-41c3-9c83-7154ab59cac1.png)'
- en: As we can see, the target network has its own set of parameters to optimize,
    (θ-). This decoupling of action selection from evaluation has shown to compensate
    for the overoptimistic representations that are learned by the naïve DQN. As a
    consequence, we are able to converge our loss function faster while achieving
    a more stable learning.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，目标网络有自己的一组参数需要优化，（θ-）。这种将动作选择与评估解耦的做法已被证明可以弥补天真DQN所学习到的过度乐观的表示。因此，我们能够更快地收敛我们的损失函数，同时实现更稳定的学习。
- en: In practice, the target networks weights can also be fixed and slowly/periodically
    updated to avoid destabilizing the model with bad feedback loops (between the
    target and prediction). This technique was notably popularized by yet another
    DeepMind paper (Hunt, Pritzel, Heess et al. , 2016), where the approach was found
    to stabilize the training process.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，目标网络的权重也可以被固定，并且定期/缓慢地更新，以避免因目标与预测之间的反馈回路不良而使模型不稳定。这项技术被另一本DeepMind论文（Hunt、Pritzel、Heess等人，2016）广泛推广，该论文指出这种方法能够稳定训练过程。
- en: The DeepMind paper by Hunt, Pritzel, Heess et al., *Continuous Control with
    Deep Reinforcement **Learning*, 2016, can be accessed at [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Hunt、Pritzel、Heess等人的DeepMind论文，*Continuous Control with Deep Reinforcement **Learning*,
    2016，可以通过以下链接访问：[https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)。
- en: 'You may implement the Double DQN through the `keras-rl` module by using the
    same code we used earlier to train our Space Invaders agent, with a slight modification
    to the part that defines your DQN agent:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用之前训练我们《太空侵略者》智能体时的相同代码，并对定义DQN智能体的部分做些微小修改，来通过`keras-rl`模块实现Double DQN：
- en: '[PRE22]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'All we simply have to do is define the Boolean value for `enable_double_dqn`
    to `True`, and we are good to go! Optionally, you may also want to experiment
    with the number of warm up steps (that is, before the model starts learning) and
    the frequency with which the target model is updated. We can further refer the
    following paper:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将`enable_double_dqn`的布尔值设置为`True`，然后就可以开始了！如果需要，您还可以尝试调整预热步数（即在模型开始学习之前的步骤数）以及目标模型更新的频率。我们可以进一步参考以下论文：
- en: '**Deep Reinforcement Learning with Double Q-learning**: [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度强化学习与双Q学习**：[https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
- en: Dueling network architecture
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对战网络架构
- en: 'The last variation of Q-learning architecture that we shall implement is the
    Dueling network architecture ([https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)).
    As the name might suggest, here, we figuratively make a neural network duel with
    itself using two separate estimators for the value of a state and the value of
    a state-action pair. You will recall from earlier in this chapter that we estimated
    the quality of a state-action pairs using a single stream of convolutional and
    densely connected layers. However, we can actually split up the Q-value function
    into a sum of two separate terms. The reason behind this segregated architecture
    is to allow our model to separately learn states that may or may not be valuable,
    without having to specifically learn the effect of each action that''s performed
    at each state:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现的最后一种Q学习架构是对战网络架构（[https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)）。顾名思义，在这种架构中，我们通过使用两个独立的估计器来使神经网络与自己对战，一个估计器用于评估状态值，另一个用于评估状态-动作对的值。你可能还记得在本章前面部分，我们使用一个单一的卷积和密集连接层流来估计状态-动作对的质量。然而，我们实际上可以将Q值函数分解为两个独立项的和。这样做的原因是让我们的模型能够分别学习某些状态可能有价值，某些则没有价值，而不需要专门学习在每个状态下执行每个动作的影响：
- en: '![](img/76d2ea1a-a292-4b93-a3c0-a1962a7d8e76.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76d2ea1a-a292-4b93-a3c0-a1962a7d8e76.png)'
- en: At the top of the preceding diagram, we can see the standard DQN architecture.
    At the bottom, we can see how the Dueling DQN architecture bifurcates into two
    separate streams, where the state and state-action values are separately estimated
    without any extra supervision. Hence, Dueling DQNs use separate estimators (that
    is, densely connected layers) for both the value of being at a state, *V(s)*,
    as well as the advantage of performing one action over another, at a given state, *A(s,a)*.
    These two terms are then combined to predict Q-values for given state-action pair,
    ensuring that our agent chooses optimal actions in the long run. While the standard
    Q function, *Q(s,a)*, only allowed us to estimate the value of selecting actions
    for given states, we can now measure both value of states and relative advantage
    of actions separately. Doing so can be helpful in situations where performing
    an action does not alter the environment in a relevant enough manner.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表顶部，我们可以看到标准的DQN架构。底部则展示了对战DQN架构如何分成两个独立的流，其中状态值和状态-动作值被分别估计，而不需要额外的监督。因此，对战DQN使用独立的估计器（即密集连接层）来分别估计处于某一状态时的值*V(s)*以及在给定状态下执行某个动作相较于其他动作的优势*A(s,a)*。这两个项随后被结合起来，预测给定状态-动作对的Q值，从而确保我们的智能体在长期内选择最优动作。与标准的Q函数*Q(s,a)*只能让我们估计给定状态下选择动作的价值不同，使用这种方法我们可以分别衡量状态的价值和动作的相对优势。在执行某个动作不会显著改变环境的情况下，这种做法可能会有所帮助。
- en: 'Both the value and the advantage function are given in the following equation:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数和优势函数可以通过以下方程给出：
- en: '![](img/00e4055e-3390-4e93-a151-d36b8f657edf.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00e4055e-3390-4e93-a151-d36b8f657edf.png)'
- en: DeepMind researchers (Wang et al, 2016) tested such an architecture on an early
    car racing game (Atari Enduro), where the agent is instructed to drive on a road
    where obstacles may sometimes occur. Researchers noted how the state value stream
    learns to pay attention to the road and the on-screen score, whereas the action
    advantage stream would only learn to pay attention when specific obstacles would
    appear on the game screen. Naturally, it only becomes important for the agent
    to perform an action (move left or right) once an obstacle is in its path. Otherwise,
    moving left or right has no importance to the agent. On the other hand, it is
    always important for our agent to keep their eyes on the road and at the score,
    which is done by the state value stream of the network. Hence, in their experiments,
    the researchers show how this architecture can lead to better policy evaluation,
    especially when an agent is faced with many actions with similar consequences.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的研究人员（Wang等人，2016年）在一个早期的赛车游戏（Atari Enduro）中测试了这样的架构，游戏中要求代理在道路上行驶，途中可能会遇到障碍物。研究人员注意到，状态值流会学习关注道路和屏幕上的分数，而动作优势流则只有在游戏屏幕上出现特定障碍物时才会学会关注。自然，只有当障碍物出现在其路径上时，代理才需要执行一个动作（向左或向右移动）。否则，向左或向右移动对代理来说没有任何意义。另一方面，代理始终需要关注道路和分数，这是由网络的状态值流来完成的。因此，在他们的实验中，研究人员展示了这种架构如何在代理面对许多具有相似后果的动作时，提供更好的策略评估。
- en: 'We can implement Dueling DQNs using the `keras-rl` module for the very same
    Space Invaders problem we viewed earlier. All we need to do is redefine our agent,
    as shown here:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`keras-rl`模块实现对战DQN，解决之前提到的Space Invaders问题。我们需要做的就是重新定义我们的代理，如下所示：
- en: '[PRE23]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we simply have to define the Boolean argument `enabble_dueling_network`
    parameter to `True` and specify a dueling type.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只需将布尔参数`enable_dueling_network`设置为`True`并指定一个对战类型。
- en: For more information on the network architecture and potential benefits of usage,
    we encourage you to follow up on the full research paper, *Dueling Network Architectures
    for Deep Reinforcement Learning*, at[ https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于网络架构和使用潜在好处的信息，我们鼓励你参考完整的研究论文，《*深度强化学习中的对战网络架构*》，你可以在[https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)查看。
- en: Exercise
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Implement standard Q-learning with a different policy (Boltzman) on an Atari
    environment and examine the difference in performance metrics
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Atari环境中实现标准的Q学习并采用不同的策略（Boltzman），检查性能指标的差异
- en: Implement a Double DQN on the same problem and compare the difference in performance
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同问题上实现双重DQN并比较性能差异
- en: Implement a Dueling DQN for the same problem and compare the difference in performance
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个对战DQN并比较性能差异
- en: Limits of Q-learning
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习的局限性
- en: 'It is truly remarkable how a relatively simple algorithm as such can give rise
    to complex strategies that such agents can come up with, given enough training
    time. Notably, researchers (and now, you too) are able to show how expert strategies
    may be learned through enough interaction with the environment. In the classic
    game of breakout, for example (included as an environment in the Atari dependency),
    you are expected to move a plank at the bottom of the screen to bounce a ball
    back and break some bricks at the top of the screen. After enough hours of training,
    a DQN agent can even figure out intricate strategies such as getting the ball
    stuck on the top side of the screen, scoring the maximum amount of points possible:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 真是令人惊叹，像这样的相对简单的算法，经过足够的训练时间后，竟能产生复杂的策略，代理能够凭此做出决策。特别是，研究人员（现在你也可以）能够展示出如何通过与环境的充分互动，学习到专家策略。例如，在经典的打砖块游戏中（该游戏作为Atari依赖的环境之一），你需要移动屏幕底部的挡板，反弹一个球并击破屏幕上方的砖块。经过足够的训练时间，DQN代理甚至能想出复杂的策略，比如将球卡在屏幕的顶部，获得最大得分：
- en: '![](img/9687c498-c42c-491f-8410-6aa22222b726.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9687c498-c42c-491f-8410-6aa22222b726.png)'
- en: Such intuitive behavior naturally makes you wonder—how far can we take this
    methodology? What type of environments may we be able to master with this approach,
    and what are its limits?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直观的行为自然会让你产生疑问——我们能将这种方法应用到多远？我们能通过这种方法掌握哪些类型的环境，它的局限性又是什么？
- en: Indeed, the power of Q-learning algorithms lies in their ability to solve problems
    with high-dimensional observation spaces, like images from a game screen. We achieved
    this using a convolutional architecture, allowing us to correlate state-action
    pairs with optimal rewards. However, the action spaces that we've concerned ourselves
    with thus far were mostly discrete and low dimensional. Turning right or left,
    denotes a discrete action, as opposed to a continuous action, like turning left
    at an angle. The latter is an example of a continuous action space, as the agent's
    action of turning to the left depends on the variable denoted by a certain angle,
    which can take a continuous value. We also did not have that many executable actions
    to begin with (ranging between 4 and 18 for Atari 2600 games). Other candidate
    deep reinforcement learning problems, like robotic motion control or optimizing
    fleet deployment, may require the modeling of very high dimensional and continuous
    action spaces, where standard DQNs tend to perform poorly. This is simply because
    DQNs rely on finding actions that maximize the Q-value function, which would require
    iterative optimization at every step in the case continuous action spaces. Thankfully,
    other approaches exist for this problem.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，Q学习算法的优势在于它们能够解决具有高维观测空间的问题，比如来自游戏屏幕的图像。我们通过卷积架构实现了这一点，使我们能够将状态-动作对与最优奖励关联起来。然而，到目前为止，我们关注的动作空间大多是离散的和低维的。例如，向右或向左转是一个离散的动作，而不是像以角度转向左那样的连续动作。后者是一个连续动作空间的例子，因为智能体向左转的动作依赖于一个由特定角度表示的变量，该角度可以取连续值。我们最初也没有那么多可执行的动作（Atari
    2600游戏中的动作范围为4到18）。其他潜在的深度强化学习问题，如机器人运动控制或优化车队部署，可能需要建模非常高维和连续的动作空间，在这种情况下，标准的DQN表现较差。这是因为DQN依赖于找到最大化Q值函数的动作，而这在连续动作空间的情况下需要在每一步进行迭代优化。幸运的是，针对这个问题，已经有其他方法存在。
- en: Improving Q-learning with policy gradients
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用策略梯度改进Q学习
- en: 'In our approach so far, we have been iteratively updating our estimates of
    Q-values for state-action pairs, from which we inferred the optimal policies.
    However, this becomes an arduous learning task when dealing with continuous action
    spaces. In the case of robot motion control, for example, our action space is
    defined by continuous variables like joint positions and angles of the robot.
    In such cases, estimating the Q-value function becomes impractical as we can assume
    that the function itself is extremely complicated. So, instead of learning optimal
    Q-values for each joint position and angle, at each given state, we can try a
    different approach. What if we could learn a policy directly, without inferring
    it from iteratively updating our Q-values for state-action pairs? Recall that
    a policy is simply a trajectory of states, followed by actions performed, reward
    generated, and the states being returned to the agent. Hence, we can define a
    set of parameterized policies (parameterized by the weights (θ) of a neural network),
    where the value of each policy can be defined by the function given here:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的方法是通过迭代更新状态-动作对的Q值估计，从而推断出最优策略。然而，当面对连续动作空间时，这变成了一项繁重的学习任务。例如，在机器人运动控制的情况下，我们的动作空间由机器人关节位置和角度等连续变量定义。在这种情况下，估计Q值函数变得不切实际，因为我们可以假设这个函数本身非常复杂。因此，
    вместо学习每个关节位置和角度的最优Q值，我们可以尝试一种不同的方法。如果我们能够直接学习一个策略，而不是通过迭代更新状态-动作对的Q值来推断策略，那会怎么样呢？回想一下，策略仅仅是一个状态轨迹，之后是执行的动作、产生的奖励以及返回给智能体的状态。因此，我们可以定义一组参数化的策略（通过神经网络的权重(θ)参数化），其中每个策略的价值可以由此处给出的函数定义：
- en: '![](img/d9a73329-884e-4274-92e5-9013f462ef63.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9a73329-884e-4274-92e5-9013f462ef63.png)'
- en: Here, the value of a policy is represented by the function *J(θ)*, where theta
    represents our model weights. On the left-hand side, we can define the value of
    a given policy with the familiar term we saw before, denoting the expected sum
    of cumulated future rewards. Our objective under this new setup is to find model
    weights that return the maximum of the policy value function, *J(θ)*, corresponding
    to the best expected future reward for our agent.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，策略的价值由函数*J(θ)*表示，其中theta表示我们的模型权重。在左侧，我们可以用之前熟悉的术语来定义给定策略的价值，表示预期的累计未来奖励之和。在这个新设置下，我们的目标是找到能够使策略价值函数*J(θ)*最大化的模型权重，从而为我们的智能体带来最佳的预期未来奖励。
- en: Previously, to find the global minimum of a function, we performed an iterative
    optimization of the first order derivatives of that function, and took steps that
    were proportional to the negative of our gradient to update model weights. This
    is what we call gradient decent. However, since we want to find the maximum of
    our policy value function, *J(θ)*, we will perform gradient ascent, which iteratively
    updates model weights that are proportional to the positive of our gradient. Hence,
    we can get a deep neural network to converge on optimal policies by evaluating
    trajectories that are generated by a given policy, instead of individually evaluating
    the quality of state-action pairs. Following this, we can even make actions from
    favorable policies have a higher probability of being selected by our agent, whereas
    actions from unfavorable policies can be sampled less frequently, given game states.
    This is the main intuition behind policy gradient methods. Naturally, a whole
    new bag of tricks follow this approach, which we encourage you to read up on.
    One such example is *Continuous Control with Deep Reinforcement Learning,* by,
    which can be found at [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，为了找到函数的全局最小值，我们通过对该函数的一级导数进行迭代优化，采取与梯度负方向成比例的步骤来更新模型权重。这就是我们所说的梯度下降。然而，由于我们想要找到我们策略价值函数的最大值，*J(θ)*，我们将执行梯度上升，它会迭代地更新与梯度正方向成比例的模型权重。因此，我们可以通过评估由给定策略生成的轨迹来使深度神经网络收敛到最优策略，而不是单独评估状态-动作对的质量。接着，我们甚至可以让来自有利策略的动作在给定游戏状态下有更高的被选中概率，而来自不利策略的动作可以被较少地采样。这就是策略梯度方法背后的主要直觉。自然，跟随这一方法会有一整套新的技巧，我们鼓励你去阅读。例如，*深度强化学习中的连续控制*，可以在[https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)找到。
- en: An interesting policy gradient implementation to look into could be the Actor
    Critic model, which can be implemented in continuous action space to solve more
    complex problems involving high-dimensional action spaces, such as the ones we
    previously discussed. More information on the Actor Critic model can be found
    at [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得关注的有趣的策略梯度实现是演员-评论家模型，它可以在连续动作空间中实现，以解决更复杂的高维动作空间问题，例如我们之前讨论过的问题。有关演员-评论家模型的更多信息可以在[https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)中找到。
- en: 'This same actor critic concept has been used in different settings for a range
    of different tasks, such a natural language generation and dialogue modeling,
    and even playing complex real-time strategy games like StarCraft II, which interested
    readers are encouraged to explore:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的演员-评论家概念已经在不同的环境中应用于各种任务，如自然语言生成和对话建模，甚至是在像《星际争霸II》这样的复杂实时策略游戏中，感兴趣的读者可以进一步探索：
- en: '**Natural Language Generation and dialogue modeling**: [https://arxiv.org/pdf/1607.07086.pdf](https://arxiv.org/pdf/1607.07086.pdf)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言生成与对话建模**：[https://arxiv.org/pdf/1607.07086.pdf](https://arxiv.org/pdf/1607.07086.pdf)'
- en: '**Starcraft II: a new challenge for reinforcement learning**: [https://arxiv.org/pdf/1708.04782.pdf?fbclid=IwAR30QJE6Kw16pHA949pEf_VCTbrX582BDNnWG2OdmgqTIQpn4yPbtdV-xFs](https://arxiv.org/pdf/1708.04782.pdf?fbclid=IwAR30QJE6Kw16pHA949pEf_VCTbrX582BDNnWG2OdmgqTIQpn4yPbtdV-xFs)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**星际争霸 II：强化学习的新挑战**：[https://arxiv.org/pdf/1708.04782.pdf?fbclid=IwAR30QJE6Kw16pHA949pEf_VCTbrX582BDNnWG2OdmgqTIQpn4yPbtdV-xFs](https://arxiv.org/pdf/1708.04782.pdf?fbclid=IwAR30QJE6Kw16pHA949pEf_VCTbrX582BDNnWG2OdmgqTIQpn4yPbtdV-xFs)'
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered quite a lot. Not only did we explore a whole new
    branch of machine learning, that is, reinforcement learning, we also implemented
    some state-of-the-art algorithms that have shown to give rise to complex autonomous
    agents. We saw how we can model an environment using the Markov decision process
    and assess optimal rewards using the Bellman equation. We also saw how problems
    of credit assignment can be addressed by approximating a quality function using
    deep neural networks. While doing so, we explored a whole bag of tricks like reward
    discounting, clipping, and experience replay memory (to name a few) that contribute
    toward representing high dimensional inputs like game screen images to navigate
    simulated environments while optimizing a goal.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们涵盖了很多内容。我们不仅探索了机器学习的一个全新分支——强化学习，还实现了一些被证明能够培养复杂自主智能体的最先进算法。我们了解了如何使用马尔可夫决策过程来建模环境，并通过贝尔曼方程评估最优奖励。我们还看到，如何通过使用深度神经网络近似质量函数来解决信用分配问题。在这个过程中，我们探索了许多技巧，如奖励折扣、裁剪和经验回放等（仅举几例），它们有助于表示高维输入，比如游戏画面图像，以便在模拟环境中导航并优化目标。
- en: Finally, we explored some of the advances in the fiend of deep-Q learning, overviewing
    architectures like double DQNs and dueling DQNs. Finally, we reviewed some of
    the challenges that are present in making agents successfully navigate high-dimensional
    action spaces and saw how different approaches, such as policy gradients, may
    help address these considerations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探索了深度 Q 学习领域的一些进展，概述了双重 DQN 和对抗性 DQN 等架构。最后，我们回顾了一些在使智能体成功导航高维动作空间时面临的挑战，并了解了如策略梯度等不同方法如何帮助解决这些问题。
