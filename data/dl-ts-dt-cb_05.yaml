- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Global Forecasting Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球预测模型
- en: In this chapter, we explore various time series forecasting scenarios and learn
    how to handle them with deep learning. These scenarios include multi-step and
    multi-output forecasting tasks, and problems involving multiple time series. We’ll
    cover each of these cases, explaining how to prepare your data, train appropriate
    neural network models, and validate them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨各种时间序列预测场景，并学习如何使用深度学习处理这些场景。这些场景包括多步和多输出预测任务，以及涉及多个时间序列的问题。我们将涵盖这些案例，解释如何准备数据、训练适当的神经网络模型，并对其进行验证。
- en: By the end of this chapter, you should be able to build deep learning forecasting
    models for different time series datasets. This includes hyperparameter optimization,
    which is an important stage in model development.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你应该能够为不同的时间序列数据集构建深度学习预测模型。这包括超参数优化，这是模型开发中的重要阶段。
- en: 'This chapter will guide you through the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将引导你完成以下配方：
- en: Multi-step forecasting with multivariate time series
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量时间序列的多步预测
- en: Multi-step and multi-output forecasting with multivariate time series
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量时间序列的多步和多输出预测
- en: Preparing multiple time series for a global model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为全局模型准备多个时间序列
- en: Training a global LSTM with multiple time series
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个时间序列训练全局 LSTM
- en: Global forecasting models for seasonal time series
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 季节性时间序列的全球预测模型
- en: Hyperparameter optimization using Ray Tune
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Tune 进行超参数优化
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires the following Python libraries:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要以下 Python 库：
- en: '`numpy` (1.26.3)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`（1.26.3）'
- en: '`pandas` (2.0.3)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`（2.0.3）'
- en: '`scikit-learn` (1.4.0)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`（1.4.0）'
- en: '`sktime` (0.26.0)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sktime`（0.26.0）'
- en: '`torch` (2.2.0)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`（2.2.0）'
- en: '`pytorch-forecasting` (1.0.0)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch-forecasting`（1.0.0）'
- en: '`pytorch-lightning` (2.1.4)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch-lightning`（2.1.4）'
- en: '`gluonts` (0.14.2)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gluonts`（0.14.2）'
- en: '`ray` (2.9.2)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray`（2.9.2）'
- en: 'You can install these libraries in one go using `pip`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `pip` 一次性安装这些库：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The recipes in this chapter will follow a design philosophy based on PyTorch
    Lightning that provides a modular and flexible way of building and deploying PyTorch
    models. The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的配方将遵循基于 PyTorch Lightning 的设计理念，这种理念提供了一种模块化和灵活的方式来构建和部署 PyTorch 模型。有关本章代码，可以在以下
    GitHub URL 中找到：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。
- en: Multi-step forecasting with multivariate time series
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多变量时间序列的多步预测
- en: So far, we’ve been working on forecasting the next value of a single variable
    of a time series. Forecasting the value of the next observation is referred to
    as one-step-ahead forecasting. In this recipe, we’ll extend the models we developed
    in the previous chapter for multi-step-ahead forecasting.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理单一变量时间序列的下一个值预测。预测下一个观测值的值被称为一步预测。在本配方中，我们将扩展上一章中开发的模型，以进行多步预测。
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Multi-step ahead forecasting is the process of forecasting several observations
    in advance. This task is important for reducing the long-term uncertainty of time
    series.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多步预测是提前预测多个观测值的过程。这个任务对于减少时间序列的长期不确定性非常重要。
- en: It turns out that much of the work we did before is also applicable to multi-step
    forecasting settings. The `TimeSeriesDataSet` class makes it extremely simple
    to extend the one-step-ahead problem to the multi-step case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们之前所做的大部分工作也适用于多步预测的设置。`TimeSeriesDataSet` 类使得将一步预测问题扩展到多步预测变得非常简单。
- en: 'In this recipe, we’ll set the forecasting horizon to `7` and the number of
    lags to `14`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将预测范围设置为 `7`，并将滞后期数设置为 `14`：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In practice, this means the predictive task is to forecast the next 7 days of
    solar radiation based on the past 14 days of data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这意味着预测任务是基于过去 14 天的数据来预测未来 7 天的太阳辐射。
- en: How to do it…
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'For multi-step ahead forecasting problems, two things need to be changed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多步预测问题，需要改变两件事：
- en: One is the output dimension of the neural network model. Instead of `1` (which
    represents the next value), the output dimension needs to match the number of
    prediction steps. This is done in the `output_dim` variable of the model.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一项是神经网络模型的输出维度。与表示下一个值的`1`不同，输出维度需要与预测步数相匹配。这可以通过模型中的 `output_dim` 变量来实现。
- en: The prediction length of the data module needs to be set to the forecasting
    horizon. This is done in the `max_prediction_length` parameter of the `TimeSeriesDataSet`
    class.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据模块的预测长度需要设置为预测时段。这可以通过`TimeSeriesDataSet`类中的`max_prediction_length`参数来完成。
- en: 'These two inputs can be passed to the data and model modules as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个输入可以按如下方式传递给数据和模型模块：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, the training and testing of the model remain the same:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型的训练和测试保持不变：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We trained the model for 20 epochs and then evaluated it in the test set, which
    is retrieved using the data loader defined in the data module.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了模型20个周期，然后在测试集上评估了它，测试集通过数据模块中定义的数据加载器进行获取。
- en: How it works…
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Traditional supervised machine learning models usually learn from a one-dimensional
    target variable. In forecasting problems, this variable can be, for example, the
    value of the time series in the next period. However, multi-step-ahead forecasting
    problems require the prediction of several values at each time. Deep learning
    models are naturally multi-output algorithms. So, they can handle several target
    variables with a single model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的监督学习模型通常从一维目标变量中学习。在预测问题中，这个变量可以是，例如，下一个时间段时间序列的值。然而，多步预测问题需要在每个时间点预测多个值。深度学习模型天生就是多输出算法。因此，它们可以使用一个模型处理多个目标变量。
- en: 'Other approaches for multi-step-ahead forecasting often involve creating several
    models or reusing the same model for different horizons. However, a multi-output
    approach is preferable because it enables the capture of dependencies among different
    horizons. This can lead to better forecasting performance, as has been documented
    in articles such as the following: Taieb, Souhaib Ben, et al., *A review and comparison
    of strategies for multi-step ahead time series forecasting based on the NN5 forecasting
    competition*. Expert systems with applications 39.8 (2012): 7067-7083'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他针对多步预测的方法通常涉及创建多个模型或将相同的模型用于不同的预测时段。然而，多输出方法更为可取，因为它能够捕捉不同预测时段之间的依赖关系。这可能带来更好的预测性能，正如以下文章所记录的那样：Taieb,
    Souhaib Ben等，*基于NN5预测竞赛的多步时间序列预测策略回顾与比较*。《专家系统与应用》39.8（2012）：7067-7083
- en: There’s more…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'There are other ways we could use a deep learning neural network for multi-step-ahead
    forecasting. Three other popular methods are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用深度学习神经网络进行多步预测的其他方法有很多。以下是另外三种流行的方法：
- en: '`Recursive`: Training a neural network for one-step-ahead forecasting and using
    it recursively to get multi-step forecasts'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`递归`: 训练一个神经网络进行一步预测，并通过递归方式使用它进行多步预测'
- en: '`Direct`: Training one neural network for each forecasting horizon'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`直接`: 为每个预测时段训练一个神经网络'
- en: '`DirRec`: Training one neural network for each forecasting horizon and feeding
    the previous forecast as input to the next one'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DirRec`: 为每个预测时段训练一个神经网络，并将前一个预测结果作为输入传递给下一个预测'
- en: Multi-step and multi-output forecasting with multivariate time series
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多元时间序列进行多步和多输出预测
- en: In this recipe, we’ll extend the LSTM model to predict multiple steps of several
    variables of a multivariate time series.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们将扩展LSTM模型，以预测多元时间序列的多个变量的多个时间步。
- en: Getting ready
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: So far, in this chapter, we have built several models to forecast the future
    of one particular variable, solar radiation. We used the extra variables in the
    time series to improve the modeling of solar radiation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这一章节中，我们已经构建了多个模型来预测某一特定变量——太阳辐射的未来。我们利用时间序列中的额外变量来改善太阳辐射的建模。
- en: Yet, when working with multivariate time series, we’re often interested in forecasting
    several variables, not just one. A common example occurs when dealing with spatiotemporal
    data. A spatiotemporal dataset is a particular case of a multivariate time series
    where a real-world process is observed in different locations. In this type of
    dataset, the goal is to forecast the future values of all these locations. Again,
    we can leverage the fact that neural networks are multi-output algorithms to handle
    multiple target variables in a single model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理多元时间序列时，我们通常关心的是预测多个变量，而不仅仅是一个。一个常见的例子是在处理时空数据时出现的。时空数据集是多元时间序列的一个特例，其中在不同位置观察到一个现实世界的过程。在这种数据集中，目标是预测所有这些位置的未来值。同样，我们可以利用神经网络是多输出算法的特点，在一个模型中处理多个目标变量。
- en: 'In this recipe, we’ll work with the solar radiation dataset, as in previous
    ones. However, our goal is to forecast the future values of three variables—solar
    radiation, vapor pressure, and air temperature:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将继续使用太阳辐射数据集，和之前的例子一样。不过，我们的目标是预测三个变量的未来值——太阳辐射、蒸气压和气温：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Regarding data preparation, the process is similar to what we did before. The
    difference is that we set the target variable (`TARGET`) to the preceding list
    of variables instead of just solar radiation. The `TimeSeriesDataSet` class and
    the data module handle all the preprocessing and data sharing for us.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据准备，过程与我们之前所做的类似。不同之处在于，我们将目标变量（`TARGET`）设置为前面列出的变量，而不是仅仅设置为太阳辐射。`TimeSeriesDataSet`类和数据模块会处理所有的预处理和数据共享工作。
- en: How to do it…
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We start by tweaking the data module to handle multiple target variables. In
    the following code, we make the necessary changes. Let’s start by defining the
    constructor of the module:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调整数据模块，以处理多个目标变量。下面的代码展示了我们所做的必要更改。让我们从定义模块的构造函数开始：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The constructor contains a new argument, `target_variables`, which we use to
    pass the list of target variables. Besides that, we also make a small change to
    the `self.target_scaler` attribute, which is now a dictionary object that contains
    a scaler for each target variable. Then, we build the `setup()` method as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数包含了一个新的参数`target_variables`，我们用它来传递目标变量的列表。除此之外，我们还对`self.target_scaler`属性做了小改动，现在它是一个字典对象，包含了每个目标变量的缩放器。接着，我们构建了如下的`setup()`方法：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The main differences from the previous recipe are the following. We pass the
    list of target variables to the target input of the `TimeSeriesDataSet` class.
    The scaling process of the target variables is also changed to a `for` loop that
    iterates over each target variable.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的例子相比，主要的不同点如下。我们将目标变量的列表传递给`TimeSeriesDataSet`类的目标输入。目标变量的缩放过程也变更为一个`for`循环，遍历每个目标变量。
- en: 'We also update the model module to process multiple target variables. Let’s
    start with the constructor and `forward()` method:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还更新了模型模块，以处理多个目标变量。让我们从构造函数和`forward()`方法开始：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `forward()` method is the same as in the previous chapter. We store a few
    more elements in the constructor, such as the forecasting horizon (`self.horizon`),
    as they are necessary in the following steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward()`方法与上一章相同。我们在构造函数中存储了一些额外的元素，比如预测时间跨度（`self.horizon`），因为它们在后续步骤中是必要的：'
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s break down the preceding code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析一下前面的代码：
- en: We add an `n_output` parameter to the constructor, which details the number
    of target variables (in this example, `3`)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们向构造函数添加了一个`n_output`参数，详细说明了目标变量的数量（在本例中是`3`）
- en: The output dimension is set to the number of target variables times the forecasting
    horizon (`self.n_output *` `self.horizon`)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出维度设置为目标变量数乘以预测时间跨度（`self.n_output * self.horizon`）
- en: When processing the data in the training and testing steps, the predictions
    are reshaped into the appropriate format (batch size, horizon, and number of variables)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练和测试步骤中处理数据时，预测结果会被重新调整为合适的格式（批大小、时间跨度和变量数）
- en: We compute the MSE loss for each target variable, and then take the average
    across them using `torch.mean(torch.stack(loss))`
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为每个目标变量计算MSE损失，然后使用`torch.mean(torch.stack(loss))`对它们取平均。
- en: 'Then, the remaining processes are similar to what we did in previous recipes
    based on PyTorch Lightning:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，剩余的过程与我们在之前基于PyTorch Lightning的例子中所做的类似：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: The modeling approach used in this recipe follows the idea of **Vector Auto-Regression**
    (**VAR**). VAR works by modeling the future value of the variables of a multivariate
    time series as a function of the past values of all these variables. Predicting
    multiple variables may be relevant in several scenarios, such as spatiotemporal
    forecasting.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中使用的建模方法遵循了**向量自回归**（**VAR**）的思想。VAR通过将多元时间序列中各变量的未来值建模为这些变量过去值的函数来工作。预测多个变量在多个场景下可能很有意义，例如时空预测。
- en: In this recipe, we adapted the VAR principle to a deep learning context, specifically
    through the use of LSTM networks. Unlike traditional VAR models that linearly
    project future values based on past observations, our deep learning model captures
    nonlinear relationships and temporal dependencies across multiple time steps and
    variables.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将VAR原理应用于深度学习环境，特别是通过使用LSTM网络。与传统的VAR模型根据过去的观察线性预测未来值不同，我们的深度学习模型能够捕捉多时间步和多个变量之间的非线性关系和时间依赖性。
- en: To compute the `loss``()` function of our model—essential for training and evaluating
    its performance—we had to perform some changes in the `training_step()` and `test_step()`
    methods. After the network generates predictions, we segment the output by variable.
    This segmentation allows us to calculate the MSE loss for each variable separately.
    These individual losses are then aggregated to form a composite loss measure,
    which guides the optimization process of the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算我们模型的`loss`函数——这对于训练和评估模型性能至关重要——我们需要对`training_step()`和`test_step()`方法做一些修改。在网络生成预测后，我们按变量对输出进行分段。这种分段允许我们分别计算每个变量的MSE损失。然后，这些单独的损失会被聚合，形成一个复合损失度量，指导模型的优化过程。
- en: Preparing multiple time series for a global model
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为全球模型准备多个时间序列
- en: Now, it is time to move on to the type of time series problems that involve
    multiple time series. In this recipe, we will learn the fundamentals of global
    forecasting models and how they work. We’ll also explore how to prepare a dataset
    that contains multiple time series for forecasting. Again, we leverage the capabilities
    of the `TimeSeriesDataSet` and `DataModule` classes to help us do this.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候开始处理涉及多个时间序列的时间序列问题了。在本节中，我们将学习全球预测模型的基本原理及其工作方式。我们还将探索如何为预测准备包含多个时间序列的数据集。同样，我们利用`TimeSeriesDataSet`和`DataModule`类的功能来帮助我们完成这项任务。
- en: Getting ready
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'So far, we’ve been working with time series problems involving a single dataset.
    Now, we’ll learn about global forecasting models, including the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理涉及单一数据集的时间序列问题。现在，我们将学习全球预测模型，包括以下内容：
- en: '**Transitioning from local to global models**: Initially, our work with time
    series forecasting focused on single datasets, where models predict future values
    based on historical data of one series. These so-called local models are tailored
    to specific time series, whereas global models involve handling multiple related
    time series and capturing relevant information across them.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从本地模型到全球模型的过渡**：最初，我们在时间序列预测中只处理单一数据集，其中模型根据一个系列的历史数据预测未来值。这些所谓的本地模型是针对特定时间序列量身定制的，而全球模型则涉及处理多个相关的时间序列，并捕捉它们之间的相关信息。'
- en: '**Leveraging neural networks**: Neural networks excel in data-rich environments,
    making them ideal for global forecasting. This is particularly effective in domains
    such as retail, where understanding the relationships across different product
    sales can lead to more accurate forecasts.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用神经网络**：神经网络在数据丰富的环境中表现出色，使其成为全球预测的理想选择。这在零售等领域尤为有效，在这些领域中，了解不同产品销售之间的关系可以带来更准确的预测。'
- en: We’ll learn how to build a global forecasting model using a dataset concerning
    transportation called **NN5**. This dataset was used in a previous forecasting
    competition and includes 111 different time series.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用一个关于运输的数据集构建全球预测模型，名为**NN5**。这个数据集曾在一个先前的预测竞赛中使用，包括111个不同的时间序列。
- en: 'The data is available in the `gluonts` Python library and can be loaded as
    follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以通过`gluonts` Python库获得，并可以通过以下方式加载：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here’s a sample of five of the time series in the dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据集中五个时间序列的样本：
- en: '![Figure 5.1: Sample of the NN5 time series dataset](img/B21145_05_001.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1：NN5 时间序列数据集样本](img/B21145_05_001.jpg)'
- en: 'Figure 5.1: Sample of the NN5 time series dataset'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：NN5 时间序列数据集样本
- en: 'The original source of this dataset is at the following link: [https://zenodo.org/records/3889750](https://zenodo.org/records/3889750).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集的原始来源可以在以下链接找到：[https://zenodo.org/records/3889750](https://zenodo.org/records/3889750)。
- en: Now, let’s build a `DataModule` class to handle the data preprocessing steps.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个`DataModule`类来处理数据预处理步骤。
- en: How to do it…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'We’ll build a `LightningDataModule` class that handles a dataset with multiple
    time series and passes them to a model. Here’s what it looks like starting with
    the constructor:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个`LightningDataModule`类，处理包含多个时间序列的数据集，并将其传递给模型。以下是构造函数的样子：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Essentially, we store the necessary elements for training and using the model.
    This includes a `self.target_scaler` attribute based on a `LocalScaler` class.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们存储了训练和使用模型所需的元素。这包括基于`LocalScaler`类的`self.target_scaler`属性。
- en: 'The main method of the `LocalScaler` class is `transform()`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`LocalScaler`类的主要方法是`transform()`：'
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This method applies two preprocessing operations to the dataset:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法对数据集应用了两种预处理操作：
- en: A log transformation to stabilize the variance of the time series
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对时间序列进行对数转换以稳定方差
- en: Standardization of each time series in the dataset
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据集中每个时间序列进行标准化
- en: You can extend this class to include any transformation that you need to perform
    on your dataset. The complete implementation of the `LocalScaler` class is available
    on the GitHub repository.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以扩展此类，以包括你需要对数据集执行的任何转换。`LocalScaler`类的完整实现可以在GitHub仓库中找到。
- en: 'Then, we preprocess the data in the `setup()` function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们在`setup()`函数中对数据进行了预处理：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding code, we split the data into training, validation, testing,
    and prediction sets and set up the respective `TimeSeriesDataSet` instances. Finally,
    the data loaders are similar to what we’ve done in previous recipes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将数据分成了训练集、验证集、测试集和预测集，并设置了相应的`TimeSeriesDataSet`实例。最后，数据加载器与我们在之前的实例中做的类似：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can call the data module as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像下面这样调用数据模块：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Using this module, each individual series in the dataset will be processed in
    such a way as to use the last `N_LAGS` values to predict the next `HORIZON` observations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此模块，数据集中的每个独立时间序列都将以使用最后`N_LAGS`个值来预测下一个`HORIZON`个观察值的方式进行处理。
- en: How it works…
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Global methods are trained on multiple time series. The idea is that there are
    common patterns across the different time series. So, a neural network can use
    observations from these series to train better models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 全局方法在多个时间序列上进行训练。其思路是不同时间序列之间存在共同模式。因此，神经网络可以利用这些序列的观察值来训练更好的模型。
- en: In the preceding section, we retrieved a dataset involving several time series
    from the `gluonts` Python library via the `get_dataset``()` function. The process
    of preparing a dataset that contains multiple time series for supervised learning
    is similar to what we did before. The key input to the `TimeSeriesDataSet` instance
    is the `group_id` variable that details the entity to which each observation belongs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们通过`get_dataset()`函数从`gluonts` Python库中检索了一个包含多个时间序列的数据集。准备一个包含多个时间序列的监督学习数据集的过程与我们之前做的类似。`TimeSeriesDataSet`实例的关键输入是`group_id`变量，它详细说明了每个观察值所属的实体。
- en: 'The main work happens in the `setup()` method. First, we transform the dataset
    into a `pandas` DataFrame with a long format. Here’s a sample of this data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的工作发生在`setup()`方法中。首先，我们将数据集转换为具有长格式的`pandas` DataFrame。以下是该数据的示例：
- en: '![Figure 5.2: Sample of the NN5 time series dataset in a long format](img/B21145_05_002.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2：NN5时间序列数据集的长格式示例](img/B21145_05_002.jpg)'
- en: 'Figure 5.2: Sample of the NN5 time series dataset in a long format'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：NN5时间序列数据集的长格式示例
- en: In this case, the `group_id` column is not constant and details which time series
    the observation refers to. Since each time series is univariate, there’s a single
    numeric variable called `value`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`group_id`列不是常量，它详细说明了观察值所对应的时间序列。由于每个时间序列是单变量的，因此有一个名为`value`的数值变量。
- en: Training a global LSTM with multiple time series
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个时间序列训练全局LSTM
- en: In the previous recipe, we learned how to prepare datasets with multiple time
    series for supervised learning with a global forecasting model. In this recipe,
    we continue this topic and describe how to train a global LSTM neural network
    for forecasting.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个实例中，我们学习了如何为全局预测模型准备多个时间序列的监督学习数据集。在本实例中，我们将继续这一主题，描述如何训练一个全局LSTM神经网络进行预测。
- en: Getting ready
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We’ll continue with the same data module we used in the previous recipe:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用在前一个实例中使用的数据模块：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let’s see how to create an LSTM module to handle a data module with multiple
    time series.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建一个LSTM模块来处理包含多个时间序列的数据模块。
- en: How to do it…
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'We create a `LightningModule` class that contains the implementation of the
    LSTM. First, let’s look at the class constructor and the `forward()` method:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含LSTM实现的`LightningModule`类。首先，让我们看一下类的构造函数和`forward()`方法：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The logic of the neural network is similar to what we’ve done for a dataset
    with a single time series. This is also true for the remaining methods:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的逻辑与我们之前处理单一时间序列数据集时所做的相似。对于剩下的方法也是如此：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we can call the model and train it as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以调用模型并进行训练，如下所示：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Using the PyTorch Lightning design, the training, testing, and prediction steps
    are similar to what we did in other recipes based on this framework.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch Lightning设计，训练、测试和预测步骤与我们在其他基于该框架的食谱中所做的类似。
- en: How it works…
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As you can see, the `LightningModule` class that contains the LSTM is identical
    to the one we built for a single multivariate time series. This class only deals
    with the part of the model definition, so no change is necessary. The main work
    is done during the data preprocessing stage. So, we only need to change the `setup()`
    method in the data module to reflect the necessary changes, which were explained
    in the previous recipe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，包含LSTM的`LightningModule`类与我们为单一多变量时间序列构建的完全相同。这个类仅处理模型定义的部分，因此无需更改。主要工作是在数据预处理阶段完成的。因此，我们只需要修改数据模块中的`setup()`方法，以反映之前食谱中所解释的必要更改。
- en: We transitioned from a local LSTM model, designed for forecasting a single time
    series, to a global LSTM model capable of handling multiple time series simultaneously.
    The main difference lies in how the data is prepared and presented to the model
    than changes in the neural network architecture itself. Both local and global
    models utilize the same underlying LSTM structure, characterized by its ability
    to process sequences of data and predict future values.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个本地LSTM模型过渡到了一个全球LSTM模型，后者能够同时处理多个时间序列。主要的区别在于数据的准备和呈现方式，而不是神经网络架构本身的变化。无论是本地模型还是全球模型，都使用相同的LSTM结构，其特点是能够处理数据序列并预测未来值。
- en: In a local LSTM setup, the model’s input typically follows the structure [`batch_size`,
    `sequence_length`, `num_features`], with the output shaped to match the forecasting
    horizon, usually [`batch_size`, `horizon`]. This setup is straightforward as it
    deals with data from a single series.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地LSTM设置中，模型的输入通常遵循结构[`batch_size`，`sequence_length`，`num_features`]，输出的形状与预测的时间跨度匹配，通常为[`batch_size`，`horizon`]。此设置非常直观，因为它处理的是来自单一序列的数据。
- en: Shifting to a global LSTM model, the approach to input and output configuration
    remains fundamentally the same in terms of dimensionality. However, the input
    now aggregates information across multiple time series. It increases the ability
    of the neural network to learn new patterns and dependencies not just within a
    single series but across several. Consequently, the output of a global LSTM model
    is designed to produce forecasts for multiple time series simultaneously, reflecting
    predictions across the entire dataset.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 转向全球LSTM模型后，输入和输出配置在维度上保持基本一致。然而，现在输入聚合了多个时间序列的信息。它增强了神经网络学习新模式和依赖关系的能力，不仅限于单一序列，还跨越多个序列。因此，全球LSTM模型的输出旨在同时为多个时间序列生成预测，反映整个数据集的预测结果。
- en: Global forecasting models for seasonal time series
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球预测模型用于季节性时间序列
- en: This recipe shows how to extend a data module to include extra explanatory variables
    in a `TimeSeriesDataSet` class and a `DataModule` class. We’ll use a particular
    case about seasonal time series.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱展示了如何扩展数据模块，将额外的解释变量包含在`TimeSeriesDataSet`类和`DataModule`类中。我们将使用一个关于季节性时间序列的特定案例。
- en: Getting ready
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We load the dataset that we used in the previous recipe:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了在上一个食谱中使用的那个数据集：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This dataset contains time series with a daily granularity. Here, we’ll model
    weekly seasonality using the `Fourier` series. Unlike what we did in the previous
    chapter (in the *Handling seasonality: seasonal dummies and Fourier series* recipe),
    we’ll learn how to include these features using the `TimeSeriesDataSet` framework.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含了日粒度的时间序列。在这里，我们将使用`Fourier`级数建模周季节性。与我们在上一章中所做的不同（在*处理季节性：季节虚拟变量和Fourier级数*一节中），我们将学习如何使用`TimeSeriesDataSet`框架来包含这些特征。
- en: How to do it…
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here’s the updated `DataModule` that includes the `Fourier` series. We only
    describe part of the `setup()` method for brevity. The remaining methods stay
    the same, and you can check them in the GitHub repository:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是更新后的`DataModule`，包含了`Fourier`级数。为了简洁起见，我们只描述了`setup()`方法的一部分，其余方法保持不变，您可以在GitHub仓库中查看：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the `setup()` method, we compute the `Fourier` terms using the date and
    time information of the dataset. This leads to four deterministic variables: `sin_7_1`,
    `cos_7_1`, `sin_7_2`, and `cos_7_2`. These are `Fourier` series that we use to
    model seasonality. After adding them to the dataset using `pd.concat([tseries_long,
    fourier_features], axis=1)`, we use the `time_varying_known_reals` argument to
    tell that these features vary over time but in a predictable way.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在`setup()`方法中，我们使用数据集的日期和时间信息计算`Fourier`项。这样会生成四个确定性变量：`sin_7_1`、`cos_7_1`、`sin_7_2`和`cos_7_2`。这些是我们用来建模季节性的`Fourier`级数。将它们通过`pd.concat([tseries_long,
    fourier_features], axis=1)`加入数据集后，我们使用`time_varying_known_reals`参数来告知这些特征随时间变化，但变化是可预测的。
- en: 'In the LSTM, we need to update the input dimension to `5` to reflect the number
    of variables in the dataset (the target variable plus four `Fourier` series).
    This is done as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM中，我们需要将输入维度更新为`5`，以反映数据集中变量的数量（目标变量加上四个`Fourier`级数）。这一步骤如下所示：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Again, the training and inference stages are similar to the previous recipe
    since the only differences here are in the data preprocessing stage handled by
    the data module.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，训练和推理阶段与之前的步骤类似，因为这里唯一的不同是在数据模块处理的数据预处理阶段。
- en: How it works…
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Modeling seasonality with the `Fourier` series involves enriching the dataset
    with extra variables derived from the Fourier transformation. This approach was
    implemented in the `setup()` method of the `DataModule` instance, where these
    variables were incorporated into the `TimeSeriesDataSet` objects.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Fourier`级数建模季节性涉及通过傅里叶变换从数据集中提取额外变量来丰富数据集。这种方法在`DataModule`实例的`setup()`方法中实现，将这些变量并入到`TimeSeriesDataSet`对象中。
- en: '`Fourier` series decomposition allows us to capture seasonality by breaking
    down complex periodic patterns into simpler, sinusoidal waves. Each component
    of the `Fourier` series corresponds to a different frequency, capturing different
    seasonal cycles within the time series data. This is particularly beneficial for
    neural networks for several reasons:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`Fourier`级数分解使我们能够通过将复杂的周期性模式分解成更简单的正弦波来捕捉季节性。`Fourier`级数的每个组件对应于不同的频率，捕捉时间序列数据中的不同季节性周期。这对神经网络特别有益，原因有几点：'
- en: '`Fourier` series acts as automatic feature engineering, creating informative
    features that directly encode periodic behaviors. This can significantly improve
    the ability of the model to recognize and predict seasonal patterns, even in complex
    or noisy data. Since Fourier features are added to the input data, they can work
    with any neural network algorithm or architecture.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fourier`级数作为自动特征工程，创建了直接编码周期性行为的有信息特征。这可以显著提升模型识别和预测季节性模式的能力，哪怕在复杂或嘈杂的数据中也能发挥作用。由于`Fourier`特征是加入到输入数据中的，它们可以与任何神经网络算法或架构兼容。'
- en: '`Fourier` series can model these multiple seasonality levels simultaneously,
    providing a more nuanced representation of the data that can be difficult to achieve
    with traditional seasonal decomposition methods.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fourier`级数可以同时建模这些多重季节性水平，提供一种更为细致的数据表示，这种表示是传统季节性分解方法难以实现的。'
- en: '**Improved generalization**: By providing a clear, mathematical representation
    of seasonality, Fourier features help neural networks to generalize better from
    the observed data to unseen future periods. This reduces the risk of overfitting
    noise and anomalies in the data, focusing the model’s learning on the underlying
    periodic trends.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进泛化能力**：通过提供季节性的明确数学表示，`Fourier`特征帮助神经网络更好地从观测数据推断到未见的未来时期。这减少了过拟合噪声和数据异常的风险，使模型的学习更加专注于潜在的周期性趋势。'
- en: There’s more…
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'You can check the following URL to learn how to include extra categorical variables
    in the dataset (such as holidays): [https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问以下网址，学习如何将额外的类别变量（如节假日）包含到数据集中：[https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data)。
- en: Hyperparameter optimization using Ray Tune
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ray Tune进行超参数优化
- en: Neural networks have hyperparameters that define their structure and learning
    process. Hyperparameters include the learning rate or the number of hidden layers
    and units. Different hyperparameter values can affect the learning process and
    the accuracy of models. Incorrectly chosen values can result in underfitting or
    overfitting, which decreases the model’s performance. So, it’s important to optimize
    the value of hyperparameters to get the most out of deep learning models. In this
    recipe, we’ll explore how to do hyperparameter optimization using Ray Tune, including
    learning rate, regularization parameters, the number of hidden layers, and so
    on. The optimization of these parameters is very important to the performance
    of our models. More often than not, we face poor results in fitting neural network
    models simply due to poor selection of hyperparameters, which can lead to underfitting
    or overfitting unseen data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有一些超参数，这些超参数定义了其结构和学习过程。超参数包括学习率、隐藏层的数量和单元数等。不同的超参数值会影响学习过程和模型的准确性。不恰当的值可能导致欠拟合或过拟合，从而降低模型的性能。因此，优化超参数值以最大限度地发挥深度学习模型的作用非常重要。在本教程中，我们将探讨如何使用
    Ray Tune 进行超参数优化，包括学习率、正则化参数、隐藏层的数量等。这些参数的优化对于我们模型的表现至关重要。往往，由于超参数选择不当，我们在拟合神经网络模型时会得到较差的结果，这可能导致欠拟合或过拟合未见数据。
- en: Getting ready
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before we begin with hyperparameter optimization, we need to install Ray Tune,
    if it’s not already installed. This can be done using the following command:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始进行超参数优化之前，如果尚未安装 Ray Tune，我们需要先进行安装。可以使用以下命令：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will use the same data and LSTM model to optimize:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的数据和 LSTM 模型进行优化：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, we also made all the necessary imports for this recipe.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们还导入了所有本教程所需的库。
- en: How to do it…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Let’s discuss how we can implement hyperparameter optimization using Ray Tune:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下如何使用 Ray Tune 实现超参数优化：
- en: '**Define the search space**: First, define the hyperparameter space you want
    to explore.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义搜索空间**：首先，定义你想要探索的超参数空间。'
- en: '**Configure Ray Tune**: Initialize the Tune experiment with the desired settings,
    such as the number of trials, resources, and so on.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置 Ray Tune**：初始化 Tune 实验，设置所需的设置，例如试验次数、资源等。'
- en: '**Run the optimization**: Execute the experiment by passing the training function
    and the defined search space.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行优化**：通过传递训练函数和定义的搜索空间来执行实验。'
- en: '**Analyze the results**: Utilize Ray Tune’s tools to analyze the results and
    identify the best hyperparameters.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分析结果**：利用 Ray Tune 的工具分析结果，并确定最佳超参数。'
- en: 'Let’s start by defining the search space:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义搜索空间：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this example, we only optimize two parameters: the number of hidden units
    and the number of layers in the LSTM neural network.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们只优化两个参数：LSTM 神经网络中隐藏单元的数量和层数。
- en: 'Then, we define the training cycle within a function:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在一个函数中定义训练循环：
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After defining the training function, we pass it to a `TorchTrainer` class
    instance, along with the running configuration:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完训练函数后，我们将其传递给 `TorchTrainer` 类实例，并与运行配置一起使用：
- en: '[PRE27]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the `ScalingConfig` instance, we configured the computational environment,
    specifying whether the process should run on a GPU or CPU, the number of workers
    allocated, and the resources per worker. Meanwhile, the `RunConfig` instance is
    set to define the optimization process, including the metric that should be monitored
    throughout this process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `ScalingConfig` 实例中，我们配置了计算环境，指定了该过程是否应在 GPU 或 CPU 上运行、分配的工作节点数量以及每个工作节点的资源。同时，`RunConfig`
    实例用于定义优化过程，包括在此过程中应监控的指标。
- en: 'Then, we create a `Tuner` instance that combines this information:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 `Tuner` 实例，将这些信息结合在一起：
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `Tuner` instance requires a scheduler as one of its inputs. For this purpose,
    we utilize `ASHAScheduler`, which employs an **Asynchronous Successive Halving
    Algorithm** (**ASHA**) to efficiently allocate resources across various configurations.
    This method helps identify the most effective configuration by iteratively narrowing
    down the search space based on performance. Ultimately, by running this process,
    we can determine the optimal configuration:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuner` 实例需要一个调度器作为其输入之一。为此，我们使用 `ASHAScheduler`，它采用**异步成功折半算法**（**ASHA**）高效地在不同配置之间分配资源。这种方法通过根据性能反复缩小搜索空间来帮助识别最有效的配置。最终，通过运行这个过程，我们可以确定最佳配置：'
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code, we get the configuration that minimizes the validation
    loss.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们获得了最小化验证损失的配置。
- en: 'After selecting the best hyperparameters based on the validation loss, we can
    evaluate the model on the test set. Retrieve the model weights from the checkpoint
    and load the best hyperparameters from the tuning process. Then, use these parameters
    to load the model and evaluate it on the test data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在根据验证损失选择最佳超参数后，我们可以在测试集上评估模型。从检查点中获取模型权重，并加载调优过程中的最佳超参数。然后，使用这些参数加载模型并在测试数据上进行评估：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, we load the model with the best configuration and test
    it in the test set defined in the `DataModule` class.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们加载了具有最佳配置的模型，并在`DataModule`类中定义的测试集上进行测试。
- en: How it works…
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Our hyperparameter optimization process involves defining a search space, configuring
    and executing the optimization, and analyzing the results. The code snippets shared
    in this section provide a step-by-step guide to integrating Ray Tune into any
    machine learning workflow, allowing us to explore and find the best hyperparameters
    for our model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的超参数优化过程包括定义搜索空间、配置和执行优化，以及分析结果。本节共享的代码片段提供了一个逐步指南，说明如何将Ray Tune集成到任何机器学习工作流中，从而帮助我们探索并找到最适合模型的超参数：
- en: The `search_space` dictionary defines the hyperparameter search space
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`search_space`字典定义了超参数搜索空间。'
- en: The `train_tune()` function encapsulates the training process, including model
    configuration, data preparation, and fitting
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_tune()`函数封装了训练过程，包括模型配置、数据准备和拟合。'
- en: The `ScalingConfig` class defines the computational environment for the optimization
    process, such as whether to run it on GPU or CPU
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ScalingConfig`类定义了优化过程的计算环境，例如是否在GPU或CPU上运行。'
- en: The `RunConfig` class sets up how the optimization is done, such as the metric
    that should be tracked during this process
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RunConfig`类设置了优化的执行方式，例如在此过程中应跟踪的指标。'
- en: The `ASHAScheduler` class is a scheduler that defines how to select from among
    different possible configurations
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ASHAScheduler`类是一个调度器，定义了如何从不同的可能配置中进行选择。'
- en: Ray Tune efficiently explores the hyperparameter space using various algorithms
    such as Random Search, Grid Search, or more advanced methods such as ASHA. It
    parallelizes trials to utilize available resources effectively, hence speeding
    up the search process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Tune通过使用诸如随机搜索、网格搜索或更先进的算法（如ASHA）等多种算法，高效地探索超参数空间。它并行化试验以有效利用可用资源，从而加速搜索过程。
- en: There’s more…
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Ray Tune offers several additional features and advantages. It can integrate
    with other libraries, making it compatible with popular machine learning frameworks
    such as PyTorch, TensorFlow, and Scikit-Learn. Moreover, it provides advanced
    search algorithms such as Bayesian Optimization and Population-Based Training,
    giving users the flexibility to experiment with different optimization strategies.
    Lastly, Ray Tune supports visualization tools, allowing users to utilize TensorBoard
    or custom tools provided by Ray to effectively visualize and analyze the hyperparameter
    search process.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Tune提供了几个额外的功能和优势。它可以与其他库集成，使其与流行的机器学习框架（如PyTorch、TensorFlow和Scikit-Learn）兼容。此外，它还提供了先进的搜索算法，如贝叶斯优化和基于群体的训练，使用户能够灵活地尝试不同的优化策略。最后，Ray
    Tune支持可视化工具，允许用户利用TensorBoard或Ray提供的自定义工具有效地可视化和分析超参数搜索过程。
