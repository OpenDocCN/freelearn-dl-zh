- en: Learning Process in Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的学习过程
- en: Just as there are many different types of learning and approaches to human learning,
    so we can say about the machines as well. To ensure that a machine will be able
    to learn from experience, it is important to define the best available methodologies
    depending on the specific job requirements. This often means choosing techniques
    that work for the present case and evaluating them from time to time, to determine
    if we need to try something new.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如人类学习有许多不同的类型和方法一样，机器学习也可以有不同的方式。为了确保机器能够从经验中学习，定义最适合的学习方法非常重要，这取决于具体的工作要求。这通常意味着选择当前情况下有效的技术，并不时进行评估，以决定是否需要尝试新的方法。
- en: We have seen the basics of neural networks in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and also two simple implementations
    using R. In this chapter, we will deal with the learning process, that is how
    to train, test, and deploy a neural network machine learning model. The training
    phase is used for learning, to fit the parameters of the neural networks. The
    testing phase is used to assess the performance of fully-trained neural networks.
    Finally, in the deployment phase, actual data is passed through the model to get
    the prediction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)中学习了神经网络的基础知识，*神经网络与人工智能概念*，并通过R语言进行了两个简单的实现。在本章中，我们将探讨学习过程，也就是如何训练、测试和部署神经网络机器学习模型。训练阶段用于学习，以调整神经网络的参数。测试阶段用于评估完全训练好的神经网络的性能。最后，在部署阶段，实际数据会通过模型进行预测。
- en: 'The following is the list of concepts covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章涵盖的概念列表：
- en: Learning process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程
- en: Supervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Training, testing, and deploying a model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、测试和部署模型
- en: Evaluation metrics-error measurement and fine tuning; measuring accuracy of
    a model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标——误差测量与微调；衡量模型的准确性
- en: Supervised learning model using neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络的有监督学习模型
- en: Unsupervised learning model using neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络的无监督学习模型
- en: Backpropagation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: By the end of the chapter, we will understand the basic concepts of the learning
    process and how to implement it in the R environment. We will discover different
    types of algorithms to implement a neural network. We will learn how to train,
    test, and deploy a model. We will know how to perform a correct valuation procedure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将理解学习过程的基本概念，并学习如何在R环境中实现它。我们将发现实现神经网络的不同类型的算法。我们将学习如何训练、测试和部署模型。我们将知道如何执行正确的评估程序。
- en: What is machine learning?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习是什么？
- en: 'What do we mean by the term machine learning? The definition is quite difficult,
    to do so, we are asking large field of scientists to help. We can mention an artificial
    intelligence pioneer''s quote:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是什么意思？这个定义相当复杂，因此我们需要请广大科学家们帮助解答。我们可以提到一位人工智能先驱的名言：
- en: '*"Field of study that gives computers the ability to learn without being explicitly
    programmed."*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*“一个研究领域，赋予计算机无需明确编程即可学习的能力。”*'
- en: – Arthur Samuel
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: – 阿瑟·塞缪尔
- en: Machine learning is about training a model or an algorithm with data and then
    using the model to predict any new data. For example, a toddler is taught how
    to walk from his crawling phase. Initially, the toddler's parents hold the toddler's
    hand to help him up, and he is taught through the data that is given. On the basis
    of these procedures, if an obstacle presents itself in the toddler's path or if
    there is a turn somewhere, the toddler is able to navigate on his own after the
    training. The data used for training is the training data and the recipient continues
    to learn even after the formal training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是指通过数据训练模型或算法，然后使用该模型预测新的数据。例如，一个幼儿从爬行阶段被教会如何走路。最初，幼儿的父母扶着孩子的手帮助他站起来，并通过给定的数据进行教学。基于这些过程，如果在幼儿的路上出现障碍，或者某个地方需要转弯，在训练之后，幼儿能够自己导航。用于训练的数据称为训练数据，接受者在正式训练后仍然继续学习。
- en: Machines too can be taught like toddlers to do a task based on training. First,
    we feed enough data to tell the machine what needs to be done on what circumstances.
    After the training, the machine can perform automatically and can also learn to
    fine-tune itself. This type of training the machine is called **machine learning**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器也可以像教小孩子一样，基于训练来完成任务。首先，我们输入足够的数据来告诉机器在什么情况下需要做什么。经过训练后，机器可以自动执行任务，还能学会自我调整。这种训练机器的方式叫做**机器学习**。
- en: The main difference between machine learning and programming is that there is
    no coding/programming involved in machine learning, while programming is about
    giving the machine a set of instructions to perform. In machine learning, the
    data is the only input provided and the model is based on the algorithm we have
    decided to use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与编程的主要区别在于，机器学习不涉及编码/编程，而编程是给机器一组执行指令的过程。在机器学习中，数据是唯一的输入，而模型是基于我们决定使用的算法。
- en: 'The algorithm to be used is based on various factors of the data: the features
    (or independent variables), the type of dependent variable(s), the accuracy of
    the model, and the speed of training and prediction of the model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的算法是基于数据的各种因素：特征（或自变量）、因变量的类型、模型的准确性以及模型训练和预测的速度。
- en: 'Based on the independent variable(s) of the machine learning data, there are
    three different ways to train a model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习数据的自变量，有三种不同的方式来训练模型：
- en: Supervised learning
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The following figure shows the different algorithms to train a machine learning
    model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了用于训练机器学习模型的不同算法：
- en: '![](img/00035.gif)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.gif)'
- en: In the following sections, we will go through them on by one.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐一讲解这些内容。
- en: Supervised learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: '**Supervised learning** is a learning method where there is a part of the training
    data which acts as a teacher to the algorithm to determine the model. The machine
    is taught what to learn from the target data. The target data, or dependent or
    response variables, are the outcome of the collective action of the independent
    variables. The network training is done with the target data and its behavior
    with patterns of input data. The target labels are known in advance and the data
    is fed to the algorithm to derive the model.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是一种学习方法，其中一部分训练数据作为算法的“老师”，帮助确定模型。机器被教会如何从目标数据中学习。目标数据或因变量是自变量的集体作用结果。网络训练是通过目标数据及其与输入数据模式的关系进行的。目标标签是事先已知的，数据被输入算法以推导模型。'
- en: Most of neural network usage is done using supervised learning. The weights
    and biases are adjusted based on the output values. The output can be categorical
    (like true/false or 0/1/2) or continuous (like 1,2,3, and so on). The model is
    dependent on the type of output variables, and in the case of neural networks,
    the output layer is built on the type of target variable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数神经网络的使用是基于监督学习的。权重和偏差根据输出值进行调整。输出可以是分类的（如真/假或0/1/2）或连续的（如1、2、3，依此类推）。模型依赖于输出变量的类型，而在神经网络中，输出层是根据目标变量的类型来构建的。
- en: For neural networks, all the independent and dependent variables need to be
    numeric, as a neural network is based on mathematical models. It is up to the
    data scientist to convert the data to numbers to be fed into the model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，所有的自变量和因变量需要是数值型的，因为神经网络是基于数学模型的。数据科学家需要将数据转换为数值形式，以便输入模型。
- en: 'Supervised learning is depicted by the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通过下图来表示：
- en: '![](img/00036.gif)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.gif)'
- en: Unsupervised learning
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In unsupervised learning (or self organization), the output layer is trained
    to organize the input data into another set of data without the need of a target
    variable. The input data is analyzed and patterns are found in it to derive the
    output, as shown in the following figure. Since there is no teacher (or target
    variable), this type of learning is called **unsupervised learning**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习（或自组织）中，输出层被训练以将输入数据组织成另一组数据，而无需目标变量。输入数据会被分析，并从中找到模式以推导输出，如下图所示。由于没有教师（或目标变量），这种学习方式称为**无监督学习**。
- en: '![](img/00037.gif)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.gif)'
- en: 'The different techniques available for unsupervised learning are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习中可用的不同技术如下：
- en: Clustering (K-means, hierarchical)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类（K均值，层次聚类）
- en: Association techniques
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联技术
- en: Dimensionality reduction
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: '**Self Organizing Map **(**SOM**)/ Kohonen networks'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自组织映射**（**SOM**）/ Kohonen网络'
- en: 'To summarize, the two main types of machine learning are depicted in the following
    figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，以下图示了机器学习的两种主要类型：
- en: '![](img/00038.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00038.jpeg)'
- en: For neural networks, we have both the types available, using different ways
    available in R.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，我们有两种类型可用，使用R中不同的方式。
- en: Reinforcement learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is a type of machine learning where there is constant
    feedback given to the model to adapt to the environment. There is a performance
    evaluation at each step to improve the model. For neural networks, there is a
    special type called **Q-learning**, combined with neuron to implement reinforcement
    learning in the backpropagation feedback mechanism. The details are out of scope
    of this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种机器学习类型，在这种类型中，模型会不断收到反馈，以适应环境。在每个步骤中都会进行性能评估，以改进模型。对于神经网络，有一种特殊类型叫做**Q-learning**，它结合神经元来实现反向传播反馈机制中的强化学习。本书不涉及详细内容。
- en: 'The following are the three types of learnings we have covered so far:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们目前已经涵盖的三种学习类型：
- en: '![](img/00039.gif)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00039.gif)'
- en: Training and testing the model
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试模型
- en: Training and testing the model forms the basis for further usage of the model
    for prediction in predictive analytics. Given a dataset of *100* rows of data,
    which includes the predictor and response variables, we split the dataset into
    a convenient ratio (say *70:30*) and allocate *70* rows for training and *30*
    rows for testing. The rows are selected in random to reduce bias.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试模型是将该模型用于预测分析的基础。给定一个包含*100*行数据的数据集，其中包括预测变量和响应变量，我们将数据集分成一个合适的比例（例如*70:30*），并分配*70*行用于训练，*30*行用于测试。数据行是随机选择的，以减少偏差。
- en: Once the training data is available, the data is fed to the neural network to
    get the massive universal function in place. The training data determines the
    weights, biases, and activation functions to be used to get to output from input.
    Until recently, we could not say that a weight has a positive or a negative influence
    on the target variable. But now we've been able to shed some light inside the
    black box. For example, by plotting a trained neural network, we can discover
    trained synaptic weights and basic information about the training process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练数据可用，数据将被传入神经网络，以建立巨大的通用函数。训练数据决定了权重、偏置和激活函数的使用，以便从输入得到输出。直到最近，我们才不能说一个权重对目标变量有正向或负向影响。但现在我们已经能够揭示这个黑盒中的一些信息。例如，通过绘制一个训练过的神经网络，我们可以发现训练过的突触权重和有关训练过程的基本信息。
- en: Once the sufficient convergence is achieved, the model is stored in memory and
    the next step is testing the model. We pass the *30* rows of data to check if
    the actual output matches with the predicted output from the model. The evaluation
    is used to get various metrics which can validate the model. If the accuracy is
    too wary, the model has to be re-built with change in the training data and other
    parameters passed to the neural net function. We will cover more about the evaluation
    metrics later in this chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦足够的收敛性达到，模型将存储在内存中，下一步是测试模型。我们将*30*行数据传入模型，检查实际输出是否与模型的预测输出相符。评估用于获取各种指标，以验证模型。如果准确度差异过大，则需要通过更改训练数据和传递给神经网络函数的其他参数重新构建模型。我们将在本章稍后讨论更多关于评估指标的内容。
- en: After training and testing, the model is said to be deployed, where actual data
    is passed through the model to get the prediction. For example, the use case may
    be determining a fraud transaction or a home loan eligibility check based on various
    input parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和测试之后，模型被认为已部署，其中实际数据通过模型传递以获得预测。例如，使用案例可能是基于各种输入参数确定欺诈交易或房贷资格检查。
- en: 'The training, testing, and deployment is represented in the following figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练、测试和部署在下图中表示：
- en: '![](img/00040.gif)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00040.gif)'
- en: So far, we have focused on the various algorithms available; it is now time
    to dedicate ourselves to the data that represents the essential element of each
    analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关注的是各种可用的算法；现在是时候将注意力集中在代表每个分析基本元素的数据上。
- en: The data cycle
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据周期
- en: 'The data forms a key component for model building and the learning process.
    The data needs to be collected, cleaned, converted, and then fed to the model
    for learning. The overall data life cycle is shown as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是模型构建和学习过程中的关键组成部分。数据需要被收集、清洗、转换，然后输入模型进行学习。整体数据生命周期如下所示：
- en: '![](img/00041.gif)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00041.gif)'
- en: One of the critical requirements for modeling is having good and balanced data.
    This helps in higher accuracy models and better usage of the available algorithms.
    A data scientist's time is mostly spent on cleansing the data before building
    the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 建模的一个关键要求是拥有良好且平衡的数据。这有助于提高模型的准确度，并更好地利用现有的算法。数据科学家的大部分时间都花费在数据清洗上，然后才能开始建模。
- en: We have seen the training and testing before deployment of the model. For testing,
    the results are captured as evaluation metrics, which helps us decide if we should
    use a particular model or change it instead.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到模型部署前的训练和测试过程。在测试时，结果作为评估指标进行记录，帮助我们决定是否使用某个模型，或是否需要改变模型。
- en: We will see the evaluation metrics next.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看到评估指标。
- en: Evaluation metrics
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: Evaluating a model involves checking if the predicted value is equal to the
    actual value during the testing phase. There are various metrics available to
    check the model, and they depend on the state of the target variable.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个模型涉及在测试阶段检查预测值是否等于实际值。有多种评估指标可用，它们取决于目标变量的状态。
- en: 'For a binary classification problem, the predicted target variable and the
    actual target variable can be in any of the following four states:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，预测目标变量和实际目标变量可以处于以下四种状态之一：
- en: '| **Predicted** | **Actual** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **预测** | **实际** |'
- en: '| *Predicted = TRUE* | *Actual = TRUE* |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 真* | *实际 = 真* |'
- en: '| *Predicted = TRUE* | *Actual = FALSE* |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 真* | *实际 = 假* |'
- en: '| *Predicted = FALSE* | *Actual = TRUE* |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 假* | *实际 = 真* |'
- en: '| *Predicted = FALSE* | *Actual = FALSE* |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 假* | *实际 = 假* |'
- en: When we have the predicted and actual values as same values, we are said to
    be accurate. If all predicted and actual values are same (either all *TRUE* or
    all *FALSE*), the model is *100* percent accurate. But, this is never the case.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测值与实际值相同，我们认为模型是准确的。如果所有预测值和实际值都相同（无论是全为*真*还是全为*假*），则模型准确度为*100*%。但实际情况中永远不会如此。
- en: Since neural networks are approximation models, there is always a bit of error
    possible. All the four states mentioned in the previous table are possible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络是近似模型，总会有一些误差的可能性。前述表格中的四种状态都是可能的。
- en: 'We define the following terminology and metrics for a model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为模型定义以下术语和指标：
- en: '**True Positives** (**TP**):All cases where the predicted and actual are both
    *TRUE* (good accuracy).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性** (**TP**)：所有预测值与实际值均为*真*的情况（准确度良好）。'
- en: '**True Negative** (**TN**): All cases when predicted is *FALSE* and the actual
    is also *FALSE* (good accuracy).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例** (**TN**)：所有预测为*假*且实际也是*假*的情况（准确度良好）。'
- en: '**False Positive **(**FP**):This is a case when we predict something as positive
    (*TRUE*), but it is actually negative. It is like a false alarm or an FP error.
    An example is when a male is predicted to be pregnant by a pregnancy test kit.
    All cases when predicted is *TRUE*, while the actual is *FALSE*. This is also
    called **type 1 error**.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性** (**FP**)：这是当我们预测某个值为阳性（*真*），但实际为阴性时的情况。就像是误报或FP错误。例如，当预测男性为孕妇时。所有预测为*真*，而实际为*假*的情况。这也叫做**第一类错误**。'
- en: '**False Negative** (**FN**):When we predict something as *FALSE*, but in actuality
    it is *TRUE*, then the case is called FN. For example, when a pregnant female
    is predicted as not being pregnant by a pregnancy test kit, it is an FN case.
    All cases when predicted is *FALSE* and actual *TRUE*. This is also called **type
    2 error**.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例** (**FN**)：当我们预测某个值为*假*，但实际却为*真*时，该情况称为FN。例如，当孕妇被孕检试剂预测为未怀孕时，就是一个FN案例。所有预测为*假*且实际为*真*的情况。这也叫做**第二类错误**。'
- en: Confusion matrix
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'When the values of the classification are plotted in a *nxn* matrix (*2x2*
    in case of binary classification), the matrix is called the **confusion matrix**.
    All the evaluation metrics can be derived from the confusion matrix itself:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类值被绘制成一个*nxn*矩阵（在二分类情况下为*2x2*矩阵）时，该矩阵称为**混淆矩阵**。所有评估指标都可以从混淆矩阵本身得出：
- en: '|  | **Predicted value** | **Predicted value** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测值** | **预测值** |'
- en: '| *Actual values* | *TRUE* | *FALSE* |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| *实际值* | *真* | *假* |'
- en: '| *TRUE* | *TP* | *FN* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| *真* | *TP* | *FN* |'
- en: '| *FALSE* | *FP* | *TN* |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| *假* | *FP* | *TN* |'
- en: Now, let's look at some evaluation metrics in detail.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细看看一些评估指标。
- en: True Positive Rate
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真阳性率
- en: '**True Positive Rate** (**TPR**) or sensitivity or recall or hit rate is a
    measure of how many true positives were identified out of all the positives identified:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**真阳性率** (**TPR**) 或敏感性或召回率或命中率，是指从所有识别出的正样本中，正确识别的真阳性比例：'
- en: '![](img/00042.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: Ideally, the model is better if we have this closer to one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，如果模型的值接近1，那么模型的表现会更好。
- en: True Negative Rate
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真负率
- en: '**True Negative Rate** (**TNR**) or specificity is the ratio of true negatives
    and total number of negatives we have predicted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**真负率** (**TNR**) 或特异度是正确预测的负样本与我们预测的所有负样本总数之比：'
- en: '![](img/00043.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: If this ratio is closer to zero, the model is more accurate.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个比率接近零，模型就更准确。
- en: Accuracy
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: Accuracy is the measure of how good our model is. It is expected to be closer
    to 1, if our model is performing well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是衡量我们模型好坏的标准。如果模型表现良好，它应该接近1。
- en: 'Accuracy is the ratio of correct predictions and all the total predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是正确预测与所有总预测的比率：
- en: '![](img/00044.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: Precision and recall
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: Precision and recall are again ratios between the *TP* with (*TP+FP*) and *TP*
    with (*TP+FN*) respectively. These ratios determine how relevant our predictions
    are compared to the actual.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率分别是 *TP* 与 (*TP+FP*) 以及 *TP* 与 (*TP+FN*) 的比率。这些比率决定了我们的预测与实际的相关程度。
- en: Precision is defined as how many selected items are relevant. That is, how many
    of the predicted ones are actually correctly predicted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度定义为选定项目中相关项目的比例。也就是说，预测的结果中有多少是实际正确的。
- en: 'The equation is:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式是：
- en: '![](img/00045.gif)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.gif)'
- en: If precision is closer to one, we are more accurate in our predictions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果精确度接近1，我们的预测就会更准确。
- en: 'Recall, on the other hand, tells how many relevant items we selected. Mathematically,
    it is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率则告诉我们选择了多少相关项目。从数学上讲，它是：
- en: '![](img/00046.gif)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.gif)'
- en: 'The following diagram depicts clearly the discussion we have done so far:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表清晰地展示了我们迄今为止的讨论：
- en: '![](img/00047.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: F-score
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F分数
- en: 'F-score, or F1-score, is another measure of accuracy. Technically, it is the
    harmonic mean of precision and recall:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: F分数，或F1分数，是衡量准确度的另一种方式。从技术上讲，它是精确度和召回率的调和均值：
- en: '![](img/00048.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00048.jpeg)'
- en: Receiver Operating Characteristic curve
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线
- en: 'A **Receiver Operating Characteristic **(**ROC**) curve is a graphical visual
    that illustrates the predictive ability of a binary classifier system. The ROC
    curve is created by plotting a graph of the TPR against the **False Positive Rate**
    (**FPR**) at various threshold settings. This gives us **Sensitivity** versus
    (**1 - Specificity**). A ROC curve typically looks like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征** (**ROC**) 曲线是一种图形展示，说明二分类系统的预测能力。ROC曲线通过在各种阈值设置下绘制真阳性率（**TPR**）与**假阳性率**（**FPR**）的图表来创建。这给我们提供了**敏感性**与（**1
    - 特异度**）的关系。ROC曲线通常看起来是这样的：'
- en: '![](img/00049.gif)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.gif)'
- en: After acquiring the necessary skills, we are ready to analyze in detail the
    algorithms used for building the neural networks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 获得必要技能后，我们已经准备好详细分析用于构建神经网络的算法。
- en: Learning in neural networks
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的学习
- en: As we saw in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, neural networks is a machine
    learning algorithm that has the ability to learn from data and give us predictions
    using the model built. It is a universal function approximation, that is, any
    input, output data can be approximated to a mathematical function.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)《神经网络与人工智能概念》中看到的，神经网络是一种机器学习算法，具有从数据中学习并使用构建的模型进行预测的能力。它是一种通用的函数近似方法，也就是说，任何输入输出数据都可以逼近为数学函数。
- en: The forward propagation gives us an initial mathematical function to arrive
    at output(s) based on inputs by choosing random weights. The difference between
    the actual and predicted is called the error term. The learning process in a feed-forward
    neural network actually happens during the backpropagation stage. The model is
    fine tuned with the weights by reducing the error term in each iteration. Gradient
    descent is used in the backpropagation process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播通过选择随机权重为我们提供了一个初始的数学函数，用于根据输入得到输出。实际输出与预测输出之间的差异称为误差项。神经网络的学习过程实际上发生在反向传播阶段。通过减少每次迭代中的误差项，模型会对权重进行微调。在反向传播过程中使用梯度下降法。
- en: Let us cover the backpropagation in detail in this chapter, as it is an important
    machine learning aspect for neural networks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在本章详细讲解反向传播，因为它是神经网络中重要的机器学习方面。
- en: Back to backpropagation
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 返回到反向传播
- en: We have covered the forward propagation in detail in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and a little about backpropagation
    using gradient descent. Backpropagation is one of the important concepts for understanding
    neural networks and it relies on calculus to update the weights and biases in
    each layer. Backpropagation of errors is similar to *learning from mistakes*.
    We correct ourselves in our mistakes (errors) in every iteration, until we reach
    a point called **convergence***. *The goal of backpropagation is to correct the
    weights in each layer and minimize the overall error at the output layer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)中详细讨论了正向传播，*神经网络与人工智能概念*，以及使用梯度下降法的反向传播的一些内容。反向传播是理解神经网络的重要概念之一，它依赖于微积分来更新每一层的权重和偏置。误差的反向传播类似于*从错误中学习*。我们在每次迭代中纠正我们的错误（误差），直到达到一个称为**收敛**的点。*反向传播的目标是纠正每一层的权重，并最小化输出层的整体误差。
- en: 'Neural network learning heavily relies on backpropagation in feed-forward networks.
    The usual steps of forward propagation and error correction are explained as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习在前馈网络中极度依赖反向传播。正向传播和误差修正的常见步骤如下所示：
- en: Start the neural network forward propagation by assigning random weights and
    biases to each of the neurons in the hidden layer.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为隐藏层中每个神经元分配随机的权重和偏置，开始神经网络的正向传播。
- en: Get the sum of *sum(weight*input) + bias* at each neuron.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元上获取 *sum(weight*input) + bias* 的总和。
- en: Apply the activation function (*sigmoid*) at each neuron.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元上应用激活函数（*sigmoid*）。
- en: Take this output and pass it onto the next layer neuron.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此输出传递给下一个层的神经元。
- en: If the layer is the output layer, apply the weights and get the sum of *sum(weight*input)
    + bias* at each output layer neuron.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果该层是输出层，应用权重并获取每个输出层神经元的 *sum(weight*input) + bias* 的总和。
- en: Again, apply the activation function at the output layer neuron.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，在输出层神经元应用激活函数。
- en: This forms the output of the neural network at the output layer for one forward
    pass.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就形成了神经网络在一次正向传播中的输出层输出。
- en: Now, with the training data, we can identify the error term at each output neuron,
    by subtracting the actual output and the activation function output value.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用训练数据，我们可以通过减去实际输出和激活函数输出值来识别每个输出神经元的误差项。
- en: 'The total of the errors is arrived at by using the following formula:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 误差的总和通过以下公式计算得出：
- en: '![](img/00050.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpeg)'
- en: A factor of *1/2* is used to cancel the exponent when the error function *E*
    is subsequently differentiated.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当误差函数 *E* 随后进行微分时，使用一个 *1/2* 的因子来消除指数。
- en: 'The gradient descent technique requires calculation of the partial derivative
    of the error term (*E*) with respect to the weights of the network. Calculating
    the partial derivative of the full error with respect to the weight *w[ij] *is
    done using the **chain rule**of differentiation:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降技术要求计算误差项（*E*）相对于网络权重的偏导数。使用**链式法则**来计算误差项相对于权重 *w[ij]* 的全误差的偏导数：
- en: '![](img/00051.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00051.jpeg)'
- en: The derivative is defined as the rate of change of a value, the gradient descent
    uses the derivative (or slope) to minimize the error term and arrive at a correct
    set of weights.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 导数定义为值变化的速率，梯度下降使用导数（或斜率）来最小化误差项并得到一组正确的权重。
- en: 'The first factor is partial derivative of the error term with respect to the
    output at that particular neuron *j* and *o[j]* is equal to *y*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个因子是该特定神经元 *j* 对输出的误差项的偏导数，并且 *o[j]* 等于 *y*：
- en: '![](img/00052.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpeg)'
- en: 'The second factor in the chain rule is the partial derivative of the output
    of neuron *o[j ]* with respect to its input, and is the partial derivative of
    the activation function (the *sigmoid* function):'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链式法则中的第二个因子是神经元 *o[j]* 对其输入的输出的偏导数，并且是激活函数（*sigmoid* 函数）的偏导数：
- en: '![](img/00053.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpeg)'
- en: Here *net[j ]*is the input to the neuron.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *net[j]* 是输入到神经元的值。
- en: The third term in the chain rule is simply *o[i]*.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链式法则中的第三项只是 *o[i]*。
- en: '![](img/00054.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: 'Combining steps 11, 12, and 13, we get:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第 11 步、第 12 步和第 13 步结合，我们得到：
- en: '![](img/00055.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: The weight *w[ij]* at each neuron (any layer) is updated with this partial derivative,
    combined with the learning rate.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个神经元（任何层级）上的权重 *w[ij]* 会通过此偏导数与学习率结合进行更新。
- en: These steps are repeated until we have convergence of very low error term or
    a specified number of times.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会重复进行，直到我们获得非常低的误差项的收敛或达到指定的次数。
- en: All the steps are taken care of internally in the R packages available. We can
    supply the learning rate along with various other parameters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所有步骤都在可用的 R 包中内部处理。我们可以提供学习率以及其他各种参数。
- en: 'The backpropagation is illustrated as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播过程如下所示：
- en: '![](img/00056.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: As with all things in life, even an algorithm has further improvement margins.
    In the next section, we'll see how to do it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生活中的一切事物一样，即便是算法也有进一步改进的空间。在接下来的章节中，我们将看到如何实现这一点。
- en: Neural network learning algorithm optimization
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络学习算法优化
- en: The procedure used to carry out the learning process in a neural network is
    called the training algorithm. The learning algorithm is what the machine learning
    algorithm chooses as model with the best optimization. The aim is to minimize
    the loss function and provide more accuracy. Here we illustrate some of the optimization
    techniques, other than gradient descent.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中执行学习过程的程序称为训练算法。学习算法是机器学习算法选择的最佳优化模型。其目标是最小化损失函数并提高准确性。这里我们展示一些优化技术，除了梯度下降法。
- en: The **Particle Swarm Optimization** (**PSO**) method is inspired by observations
    of social and collective behavior on the movements of bird flocks in search of
    food or survival. It is similar to a fish school trying to move together. We know
    the position and velocity of the particles, and PSO aims at searching a solution
    set in a large space controlled by mathematical equations on position and velocity. It
    is bio-inspired from biological organism behavior for collective intelligence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**粒子群优化**（**PSO**）方法受到鸟群在寻找食物或生存过程中的集体行为观察的启发。它类似于鱼群试图一起移动。我们知道粒子的位置和速度，PSO
    的目标是在由位置和速度控制的数学方程的指导下，在大空间中搜索解决方案集。它从生物体行为中汲取灵感，用于集体智能。'
- en: '**Simulated annealing** is a method that works on a probabilistic approach
    to approximate the global optimum for the cost function. The method searches for
    a solution in large space with simulation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**模拟退火**是一种基于概率方法的技术，用来逼近代价函数的全局最优解。该方法在大空间中通过模拟搜索解决方案。'
- en: Evolutionary methods are derived from the evolutionary process in biology, and
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 进化方法源自生物学中的进化过程，且
- en: evolution can be in terms of reproduction, mutation, selection, and recombination.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 进化可以通过繁殖、变异、选择和重组来进行。
- en: A fitness function is used to determine the performance of a model, and based
    on this
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 适应度函数用于确定模型的性能，并基于此进行调整。
- en: function, we select our final model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 函数，我们选择最终模型。
- en: The **Expectation Maximization **(**EM**) methodis a statistical learning method
    that uses an iterative method to find maximum likelihood or maximum posterior
    estimate, thus minimizing the error.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**（**EM**）方法是一种统计学习方法，采用迭代方法找到最大似然或最大后验估计，从而最小化误差。'
- en: Supervised learning in neural networks
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的监督学习
- en: As previously mentioned, supervised learning is a learning method where there
    is a part of training data which acts as a teacher to the algorithm to determine
    the model. In the following section, an example of a regression predictive modeling
    problem is proposed to understand how to solve it with neural networks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，监督学习是一种学习方法，其中一部分训练数据作为算法的“老师”，帮助确定模型。在接下来的部分中，将提出一个回归预测建模问题示例，以便了解如何使用神经网络解决该问题。
- en: Boston dataset
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 波士顿数据集
- en: 'The dataset describes 13 numerical properties of houses in Boston suburbs,
    and is concerned with modeling the price of houses in those suburbs in thousands
    of dollars. As such, this is a regression predictive modeling problem. Input attributes
    include things like crime rate, proportion of non-retail business acres, chemical
    concentrations, and more. In the following list are shown all the variables followed
    by a brief description:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集描述了波士顿郊区住宅的13个数值属性，并涉及在这些郊区以千美元为单位建模住宅价格。因此，这是一个回归预测建模问题。输入属性包括犯罪率、非零售商业用地比例、化学浓度等。以下列表显示了所有变量及其简要描述：
- en: 'Number of instances: *506*'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例数量：*506*
- en: 'Number of attributes: *13* continuous attributes (including `class`'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性数量：*13* 个连续属性（包括 `class`）
- en: attribute `MEDV`), and one binary-valued attribute
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 属性`MEDV`，以及一个二进制值属性
- en: 'Each of the attributes is detailed as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 每个属性的详细描述如下：
- en: '`crim` per capita crime rate by town.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`crim` 按城镇划分的每人犯罪率。'
- en: '`zn` proportion of residential land zoned for lots over *25,000* square feet.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`zn` 划为住宅用地的、超过*25,000*平方英尺的地块比例。'
- en: '`indus` proportion of non-retail business acres per town.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`indus` 每个城镇的非零售商业用地比例。'
- en: '`chas` Charles River dummy variable (*= 1* if tract bounds river; *0* otherwise).'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`chas` 查尔斯河虚拟变量（如果区域边界是河流，*= 1*；否则为*0*）。'
- en: '`nox` nitric oxides concentration (parts per *10* million).'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nox` 氮氧化物浓度（单位：每*10*百万）。'
- en: '`rm` average number of rooms per dwelling.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`rm` 每个住宅的平均房间数。'
- en: '`age` proportion of owner-occupied units built prior to *1940*.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`age` 1940年前建成的自有住房比例。'
- en: '`dis` weighted distances to five Boston employment centres'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dis` 到波士顿五个就业中心的加权距离'
- en: '`rad` index of accessibility to radial highways.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`rad` 通往放射性高速公路的可达性指数。'
- en: '`tax` full-value property-tax rate per *$10,000*.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tax` 每*10,000*美元的全额财产税率。'
- en: '`ptratio` pupil-teacher ratio by town.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ptratio` 按城镇划分的师生比。'
- en: '`black` *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`black` *1000(Bk - 0.63)^2* 其中 *Bk* 是按城镇划分的黑人比例。'
- en: '`lstat` percent lower status of the population.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lstat` 低收入人群的百分比。'
- en: '`medv` median value of owner-occupied homes in *$1000''s*.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`medv` 自有住房的中位数价值（单位：*1000美元*）。'
- en: Of these, `medv` is the response variable, while the other thirteen variables
    are possible predictors. The goal of this analysis is to fit a regression model
    that best explains the variation in `medv`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些变量中，`medv` 是响应变量，而其他十三个变量是可能的预测变量。本分析的目标是拟合一个回归模型，最有效地解释 `medv` 的变化。
- en: There is a relation between the first thirteen columns and the `medv` response
    variable. We can predict the `medv` value based on the input thirteen columns.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 前十三列与 `medv` 响应变量之间存在关系。我们可以根据输入的十三列预测 `medv` 的值。
- en: This dataset is already provided with R libraries (`MASS`), as we will see later,
    so we do not have to worry about retrieving the data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已通过R库（`MASS`）提供，正如我们稍后将看到的那样，因此我们不必担心如何获取数据。
- en: Neural network regression with the Boston dataset
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用波士顿数据集进行神经网络回归
- en: 'In this section, we will run a regression neural network for the `Boston` dataset.
    The `medv` value is predicted for the test data. The train to test split is *70:30*.
    The `neuralnet`function is used to model the data with a neural network:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为 `Boston` 数据集运行回归神经网络。预测测试数据的 `medv` 值。训练和测试的划分比例为*70:30*。`neuralnet`函数用于通过神经网络建模数据：
- en: '[PRE0]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Don't worry, now we will explain in detail the whole code, line by line.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，我们将逐行详细解释整个代码。
- en: '[PRE1]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first two lines of the code are simple, as they load the libraries we will
    use for later calculations. Specifically, the `neuralnet` library will help us
    to build and train the network, while the `MASS` library will serve us to load
    the `Boston` dataset that we have previously introduced in detail.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的前两行很简单，因为它们加载了我们将用于后续计算的库。具体来说，`neuralnet` 库将帮助我们构建和训练神经网络，而 `MASS` 库将帮助我们加载之前已详细介绍过的
    `Boston` 数据集。
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，要安装 R 初始发行版中没有的库，必须使用 `install.package` 函数。这是安装软件包的主要函数。它接受一个包含名称的向量和一个目标库，从仓库下载软件包并进行安装。
- en: 'In our case, for example, to install the `neuralnet` package, we should write:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的例子中，要安装 `neuralnet` 包，我们应该写：
- en: '[PRE2]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, it should be emphasized that this function should be used only once
    and not every time you run the code. Instead, load the library through the following
    command and must be repeated every time you run the code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要强调的是，这个函数只应该使用一次，而不是每次运行代码时都使用。相反，通过以下命令加载库，并且每次运行代码时必须重复这一过程：
- en: '[PRE3]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function `set.seed` sets the seed of R''s random number generator, which
    is useful for creating simulations or random objects that can be reproduced:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`set.seed` 函数设置 R 随机数生成器的种子，这对创建可以复现的模拟或随机对象很有用：'
- en: '[PRE4]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You have to use this function every time you want to get a reproducible random
    result. In this case, the random numbers are the same, and they would continue
    to be the same no matter how far out in the sequence we go.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你想获得可复现的随机结果时，都必须使用这个函数。在这种情况下，随机数是相同的，无论我们在序列中走多远，它们都会保持不变。
- en: 'The following command loads the `Boston` dataset, which, as we anticipated,
    is contained in the `MASS` library and saves it in a given frame:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令加载 `Boston` 数据集，该数据集如我们预期，包含在 `MASS` 库中，并将其保存到给定的框架中：
- en: '[PRE5]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Use the `str` function to view a compactly display the structure of an arbitrary
    R object. In our case, using `str(data)`, we will obtain the following results:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `str` 函数查看任意 R 对象的紧凑结构。在我们的例子中，使用 `str(data)`，我们将获得如下结果：
- en: '[PRE6]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result obtained for the given object is shown in the following figure:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对给定对象获得的结果如下面的图所示：
- en: '![](img/00057.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00057.jpeg)'
- en: 'Let''s go back to parse the code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回过头来解析代码：
- en: '[PRE7]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We need this snippet of code to normalize the data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这段代码来规范化数据。
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在训练神经网络之前，规范化数据是一个好习惯。通过规范化，数据单位被消除，从而可以轻松地比较来自不同地点的数据。
- en: 'This is an extremely important procedure in building a neural network, as it
    avoids unnecessary results or very difficult training processes resulting in algorithm
    convergence problems. You can choose different methods for scaling the data (**z-normalization**,
    **min-max scale**, and so on). For this example, we will use the min-max method
    (usually called feature scaling) to get all the scaled data in the range *[0,1]*.
    The formula to achieve this is the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构建神经网络中一个极其重要的步骤，因为它可以避免不必要的结果或非常复杂的训练过程，从而导致算法收敛问题。你可以选择不同的数据缩放方法（**z-标准化**、**最小-最大缩放**等）。在这个示例中，我们将使用最小-最大方法（通常称为特征缩放）将所有数据缩放到
    *[0,1]* 范围内。实现这一点的公式如下：
- en: '![](img/00058.gif)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.gif)'
- en: Before applying the method chosen for normalization, you must calculate the
    minimum and maximum values of each database column. To do this, we use the `apply`
    function. This function returns a vector or an array or a list of values obtained
    by applying a function to margins of an array or matrix. Let's understand the
    meaning of the arguments used.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用所选的规范化方法之前，必须先计算每个数据库列的最小值和最大值。为此，我们使用 `apply` 函数。该函数返回一个通过将函数应用于数组或矩阵的边缘而获得的向量、数组或列表。让我们理解所使用参数的含义。
- en: '[PRE8]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first argument of the `apply` function specifies the dataset to apply the
    function, in our case, the dataset named `data`. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and `2` indicates columns. The third argument must contain
    the function to be applied; in our case, the `max` function.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply` 函数的第一个参数指定应用该函数的数据集，在我们的例子中是名为 `data` 的数据集。第二个参数必须包含一个向量，给出函数将应用于的下标。在我们的例子中，`1`
    表示行，`2` 表示列。第三个参数必须包含要应用的函数；在我们的例子中是 `max` 函数。'
- en: To normalize the data, we use the `scale` function, which is a generic function
    whose default method centers and/or scales the columns of a numeric matrix.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行标准化，我们使用了`scale`函数，这是一个通用函数，其默认方法是对数值矩阵的列进行居中和/或缩放。
- en: '[PRE9]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the first line of the code just suggested, the dataset is split into *70:30*, with
    the intention of using *70* percent of the data at our disposal to train the network
    and the remaining *30* percent to test the network. In the second and third lines,
    the data of the dataframe named `data` is subdivided into two new dataframes,
    called `train_data` and `test_data.`
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚才建议的代码的第一行中，数据集被分割为*70:30*，目的是使用*70*百分比的数据来训练网络，剩余的*30*百分比用于测试网络。在第二行和第三行中，名为`data`的数据框的内容被划分为两个新的数据框，分别称为`train_data`和`test_data`。
- en: '[PRE10]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Everything so far has only been used to prepare the data. It is now time to
    build the network. To do this, we first recover all the variable names using the
    `names` function. This function will get or set the name of an object.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有工作仅用于准备数据。现在是时候构建网络了。为此，我们首先通过`names`函数恢复所有变量名称。该函数可以获取或设置对象的名称。
- en: 'Next, we build `formula` that we will use to build the network, so we use the
    `neuralnet` function to build and train the network. In this case, we will create
    a network with only one hidden layer with `10` nodes. Finally, we plot the neural
    network,as shown in the following figure:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建了用于构建网络的`formula`，因此我们使用`neuralnet`函数来构建和训练网络。在这种情况下，我们将创建一个只有一个隐藏层，且有`10`个节点的网络。最后，我们绘制了神经网络，如下图所示：
- en: '![](img/00059.jpeg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00059.jpeg)'
- en: 'Now that we have the network, what do we do? Of course, we use it to make predictions.
    We had set aside *30* percent of the available data to do this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们有了网络，接下来该做什么呢？当然，我们用它来进行预测。我们已经预留了*30*百分比的可用数据来进行预测：
- en: '[PRE11]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In our case, we applied the function to the `test_data` dataset, using only
    the first `13` columns representing the input variables of the network:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将该函数应用于`test_data`数据集，仅使用前`13`列，代表网络的输入变量：
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: But how do we figure out whether the forecasts the network is able to perform
    are accurate? We can use the **Mean Squared Error** (**MSE**) as a measure of
    how far away our predictions are from the real data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何判断网络能够进行的预测是否准确呢？我们可以使用**均方误差**（**MSE**）作为衡量我们预测值与真实数据之间差距的标准。
- en: 'In this regard, it is worth remembering that before we built the network we
    had normalized the data. Now, in order to be able to compare, we need to step
    back and return to the starting position. Once the values of the dataset are restored,
    we can calculate the *MSE* through the following equation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，值得记住的是，在构建网络之前，我们已经对数据进行了标准化。现在，为了能够进行比较，我们需要退回一步，回到起始位置。一旦数据集的值恢复后，我们就可以通过以下公式计算*MSE*：
- en: '![](img/00060.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpeg)'
- en: 'Well, we have calculated *MSE* now with what do we compare it to? To get an
    idea of the accuracy of the network prediction, we can build a linear regression
    model:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们现在已经计算了*MSE*，那么该用什么来进行比较呢？为了了解网络预测的准确性，我们可以构建一个线性回归模型：
- en: '[PRE13]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We build a linear regression model using the `lm` function. This function is
    used to fit linear models. It can be used to perform regression, single stratum
    analysis of variance, and analysis of covariance. To produce result summaries
    of the results of model fitting obtained, we have used the `summary` function,
    which returns the following results:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`lm`函数构建了一个线性回归模型。该函数用于拟合线性模型，可以用于回归分析、单层方差分析和协方差分析。为了生成模型拟合结果的摘要，我们使用了`summary`函数，返回了以下结果：
- en: '[PRE14]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Also, for the regression model, we calculate the mean MSE. Finally, in order
    to assess the performance of the network, it is compared with a multiple linear
    regression model calculated with the same database as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于回归模型，我们计算了平均均方误差（MSE）。最后，为了评估网络的性能，我们将其与使用相同数据库计算的多元线性回归模型进行比较，具体如下：
- en: '[PRE15]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results are:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: From the analysis of the results, it is possible to note that the neural network
    has a lower `MSE` than the linear regression model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果分析中可以看出，神经网络的`MSE`低于线性回归模型。
- en: Unsupervised learning in neural networks
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的无监督学习
- en: In this section, we present unsupervised learning models in neural network, named
    competitive learning and Kohonen SOM. Kohonen SOM was invented by a professor
    named Teuvo Kohonen and is a way to represent multidimensional data in much lower
    dimensions: *1D* or *2D*. It can classify data without supervision. Unsupervised
    learning aims at finding hidden patterns within the dataset and clustering them
    into different classes of data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们介绍了神经网络中的无监督学习模型，分别是竞争学习和Kohonen自组织映射（SOM）。Kohonen SOM是由名为Teuvo Kohonen的教授发明的，它是一种将多维数据表示为更低维度的方法：*1D*或*2D*。它可以在没有监督的情况下对数据进行分类。无监督学习旨在发掘数据集中的隐藏模式，并将它们聚类成不同的数据类别。
- en: There are many unsupervised learning techniques, namely K-means clustering,
    dimensionality reduction, EM, and so on. The common feature is that there is no
    input-output mapping and we work only on the input values to create a group or
    set of outputs.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多无监督学习技术，如K均值聚类、降维、EM等。它们的共同特征是没有输入输出映射，我们仅根据输入值来创建一个输出的组或集合。
- en: For the case of neural networks, they can be used for unsupervised learning.
    They can group data into different buckets (clustering) or abstract original data
    into a different set of output data points (feature abstraction or dimensionality
    reduction). Unsupervised techniques require less processing power and memory than
    supervised technique.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，它们可以用于无监督学习。它们可以将数据分组到不同的桶中（聚类），或者将原始数据抽象成不同的一组输出数据点（特征抽象或降维）。无监督技术比有监督技术需要更少的处理能力和内存。
- en: 'In unsupervised neural networks, there is no target variable and we cannot
    do backpropagation. Instead, we keep adjusting the weights without the error measure
    and try to group similar data together. There are two methods we will see for
    unsupervised neural networks:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督神经网络中，没有目标变量，因此我们无法进行反向传播。相反，我们在没有误差度量的情况下持续调整权重，并尝试将相似的数据分组。我们将看到两种无监督神经网络方法：
- en: Competitive learning
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞争学习
- en: Kohonen SOMs
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohonen自组织映射（SOM）
- en: Competitive learning
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 竞争学习
- en: 'Here, the neural network nodes compete with each other for the right to respond
    to a subset of the input data. The hidden layer is called the **competitive layer**.
    Every competitive neuron has its own weight and we calculate the similarity measure
    between the individual input vector and the neuron weight. For each input vector,
    the hidden neurons compete with each other to see which one is the *most *similar
    to the particular input vector:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，神经网络节点之间相互竞争，以争夺对输入数据子集的响应权。隐藏层被称为**竞争层**。每个竞争神经元都有自己的权重，我们计算每个输入向量与神经元权重之间的相似度。对于每个输入向量，隐藏神经元之间相互竞争，看哪个最*类似*于该输入向量：
- en: '![](img/00061.gif)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00061.gif)'
- en: The output neurons are said to be in competition for input patterns.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输出神经元被认为是在为输入模式进行竞争。
- en: During training, the output neuron that provides the highest activation to a
    given input pattern is declared the weights of the winner and is moved closer
    to the input pattern, whereas the rest of the neurons are left unchanged
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，提供给定输入模式最高激活的输出神经元被声明为胜者，并且其权重被移向该输入模式，而其余神经元保持不变。
- en: 'This strategy is also called **winner-takes-all**, since only the winning neuron
    is updated:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种策略也叫做**胜者全得**，因为只有获胜的神经元会被更新：
- en: '![](img/00062.gif)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.gif)'
- en: 'Let us see a simple competitive learning algorithm example to find three neurons
    within the given input data:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的竞争学习算法示例，以在给定的输入数据中找到三个神经元：
- en: We will have three input neurons in the input layer. Each input to the neuron
    is a continuous variable and let the weight at each input neuron be a random number
    between *0.0* and *1.0*. The output of each node is the product of the three weights
    and its input.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在输入层中设置三个输入神经元。每个神经元的输入是一个连续变量，假设每个输入神经元的权重是一个介于*0.0*和*1.0*之间的随机数。每个节点的输出是三个权重与其输入的乘积。
- en: Each competitive layer neuron receives the sum of the product of weights and
    inputs.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个竞争层神经元接收权重和输入的乘积之和。
- en: The competitive layer node with the highest output is regarded as the winner.
    The input is then categorized as being within the cluster corresponding to that
    node.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出最大值的竞争层节点被视为胜者。然后，输入被分类为属于该节点对应的聚类。
- en: The winner updates each of its weights, moving the weight from the connections
    that gave it weaker signals to the ones that gave it stronger signals.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 胜者更新其每个权重，将权重从那些提供较弱信号的连接转移到那些提供较强信号的连接上。
- en: Thus, as we receive more data, each node converges on the center of the cluster
    that it has come to represent. It activates more strongly for inputs belonging
    to this cluster and more weakly for inputs that belong to other clusters.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着我们接收更多数据，每个节点都会收敛到它所代表的聚类中心。它对属于该聚类的输入激活得更强烈，而对属于其他聚类的输入激活得较弱。
- en: 'There are basically two stopping conditions of competitive learning:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争学习基本上有两个停止条件：
- en: '**Predefined number of epochs**: Only *N *epochs are run and this prevents
    the algorithm from running for a relatively long time without convergence'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预定义的周期数**：仅运行*N*个周期，这防止了算法在没有收敛的情况下运行过长时间。'
- en: '**Minimum value of weight update**: The algorithm is run until we have a minimum
    value of weight update'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小权重更新值**：算法会一直运行，直到我们得到最小的权重更新值。'
- en: Kohonen SOM
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kohonen SOM
- en: The concept of competitive learning combined with neighborhood neurons gives
    us Kohonen SOMs. Every neuron in the output layer has two neighbors. The neuron
    that fires the greatest value updates its weights in competitive learning, but
    in SOM, the neighboring neurons also update their weights at a relatively slow
    rate. The number of neighborhood neurons that the network updates the weights
    is based on the dimension of the problem.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争学习的概念与邻域神经元相结合，得到了Kohonen SOM。输出层中的每个神经元都有两个邻居。在竞争学习中，发出最大值的神经元会更新其权重，但在SOM中，邻域神经元也会以相对较慢的速度更新其权重。网络更新权重的邻域神经元数量取决于问题的维度。
- en: 'For a *2D* problem, the SOM is represented as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个*2D*问题，SOM的表示方式如下：
- en: '![](img/00063.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpeg)'
- en: 'Diagrammatically, this is how the SOM maps different colors into different
    clusters:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从图示来看，SOM是如何将不同的颜色映射到不同的聚类中的：
- en: '![](img/00064.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00064.jpeg)'
- en: 'Let us understand the working of Kohonen SOM step-by-step:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地理解Kohonen SOM的工作原理：
- en: The number of inputs and the clusters that define the SOM structure and each
    node's weights are initialized.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化定义SOM结构的输入数量、聚类数量以及每个节点的权重。
- en: A vector is chosen at random from the set of training data and is presented
    to the network.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中随机选择一个向量并将其呈现给网络。
- en: Every node in the network is examined to calculate which one's weights are most
    similar to the input vector. The winning node is commonly known as the **Best
    Matching Unit** (**BMU**).
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查网络中的每个节点，计算哪个节点的权重与输入向量最相似。获胜节点通常被称为**最佳匹配单元**（**BMU**）。
- en: The radius of the neighborhood of the BMU is calculated. This value starts large
    and is typically set to be the radius of the network, diminishing each time-step.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算BMU的邻域半径。这个值开始时较大，通常设置为网络的半径，并随着每个时间步逐渐减小。
- en: Any neurons found within the radius of the BMU, calculated in step 4, are adjusted
    to make them more like the input vector. The closer a neuron is to the BMU, the
    more its weights are altered.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何位于步骤4中计算的BMU半径范围内的神经元，都将进行调整，使其更加接近输入向量。神经元离BMU越近，其权重的改变越大。
- en: Repeat from step 2 for *N* iterations.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对*N*次迭代重复执行步骤2。
- en: The steps are repeated for a set of *N* epochs or until a minimum weight update
    is obtained.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤会重复执行*N*次周期，或者直到获得最小的权重更新。
- en: SOMs are used in the fields of clustering (grouping of data into different buckets),
    data abstraction (deriving output data from inputs), and dimensionality reduction
    (reducing the number of input features). SOMs handle the problem in a way similar
    to **Multi Dimensional Scaling** (**MDS**), but instead of minimizing the distances,
    they try regroup topology, or in other words, they try to keep the same neighbors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: SOM被应用于聚类（将数据分组到不同的桶中）、数据抽象（从输入中派生输出数据）和降维（减少输入特征的数量）等领域。SOM以类似于**多维尺度法**（**MDS**）的方式处理问题，但不是最小化距离，而是尝试重新分组拓扑，换句话说，它们尽量保持相同的邻居。
- en: Let us see an example of SOM implementation in R. The `kohonen` package is a
    package to be installed to use the functions offered in R for SOM.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看R中SOM实现的一个示例。`kohonen`包是一个需要安装的包，用于在R中使用SOM相关的函数。
- en: 'The following R program explains some functions from the `kohonen` package
    :'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 以下R程序说明了`kohonen`包中的一些函数：
- en: '[PRE17]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The code uses a wine dataset, which contains a data frame with `177` rows and
    `13` columns; the object `vintages` contains the class labels. This data is obtained
    from the chemical analyses of wines grown in the same region in Italy (Piemonte)
    but derived from three different cultivars, namely, the `Nebbiolo`, `Barberas`,
    and `Grignolino` grapes. The wine from the `Nebbiolo` grape is called **Barolo**.
    The data consists of the amounts of several constituents found in each of the
    three types of wines, as well as some spectroscopic variables.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用了一个葡萄酒数据集，该数据集包含一个有`177`行和`13`列的数据框；`vintages`对象包含类标签。这些数据来自于意大利（皮埃蒙特）同一地区葡萄酒的化学分析，这些葡萄酒源自三种不同的葡萄品种，即`Nebbiolo`、`Barberas`和`Grignolino`葡萄。`Nebbiolo`葡萄酿成的葡萄酒叫做**巴罗洛**。数据包括每种葡萄酒中多种成分的含量，以及一些光谱变量。
- en: Now, let's see the outputs at each section of the code.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看每个代码部分的输出。
- en: '[PRE18]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The first line of the code is simple, as it loads the library we will use for
    later calculations. Specifically, the `kohonen` library will help us to train
    SOMs. Also, interrogation of the maps and prediction using trained maps are supported.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一行很简单，它加载了我们将用于后续计算的库。具体来说，`kohonen`库将帮助我们训练 SOM。此外，支持对映射的查询和使用训练好的映射进行预测。
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，要安装 R 中初始分发版中没有的库，您必须使用`install.package`函数。这是安装包的主要函数。它接受一个名称向量和目标库，从仓库中下载包并安装它们。
- en: '[PRE19]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'These lines load the `wines` dataset, which, as we anticipated, is contained
    in the R distribution, and saves it in a dataframe named `data`. Then, we use
    the `str` function to view a compactly display the structure of the dataset. The
    function `head` is used to return the first or last parts of the dataframe. Finally,
    the `view` function is used to invoke a spreadsheet-style data viewer on the dataframe
    object, as shown in the following figure:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码行加载了`wines`数据集，正如我们所预期的，它包含在 R 的分发版中，并将其保存到一个名为`data`的数据框中。然后，我们使用`str`函数来紧凑显示数据集的结构。`head`函数用于返回数据框的前部分或后部分。最后，`view`函数用于调用数据框对象的电子表格样式数据查看器，如下图所示：
- en: '![](img/00065.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00065.jpeg)'
- en: 'We will continue to analyze the code:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续分析代码：
- en: '[PRE20]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After loading the wine data and setting `seed` for reproducibility, we call
    `som` to create a *5x5* matrix, in which the features have to be clustered. The
    function internally does the `kohonen` processing and the result can be seen by
    the clusters formed with the features. There are *25* clusters created, each of
    which has a combined set of features having common pattern, as shown in the following
    image:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 加载葡萄酒数据并设置`seed`以确保结果可重复后，我们调用`som`函数来创建一个*5x5*的矩阵，其中需要对特征进行聚类。该函数内部执行`kohonen`处理，结果可以通过特征聚类来看。共有*25*个聚类，每个聚类都有一组具有相似模式的特征，如下图所示：
- en: '![](img/00066.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: 'The next part of the code plots the mean distance to the closest unit versus
    the number of iterations done by `som`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分绘制了与`som`迭代次数相关的最近单元的平均距离：
- en: '[PRE21]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the following figure is shown mean distance to closest unit versus the number
    of iterations:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了与迭代次数相关的最近单元的平均距离：
- en: '![](img/00067.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpeg)'
- en: 'Next, we create a `training` dataset with `150` rows and `test` dataset with `27`
    rows. We run the SOM and predict with the test data. The `supersom`function is
    used here. Here, the model is supervised SOM:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建了一个`training`数据集，包含`150`行，和一个`test`数据集，包含`27`行。我们运行 SOM 并使用测试数据进行预测。这里使用了`supersom`函数。此处的模型是监督式
    SOM：
- en: '[PRE22]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we invoke the `table` function that uses the cross-classifying factors
    to build a contingency table of the counts at each combination of factor levels,
    as shown next:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用`table`函数，它使用交叉分类因素构建一个列出每个因素水平组合计数的列联表，如下所示：
- en: '[PRE23]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `kohonen` package features standard SOMs and two extensions: for classification
    and regression purposes, and for data mining. Also, it has extensive graphics
    capability for visualization.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`kohonen`包具有标准的 SOM（自组织映射）功能，并提供了两个扩展功能：用于分类和回归的扩展，以及用于数据挖掘的扩展。同时，它还具有广泛的图形功能用于可视化。'
- en: 'The following table lists the functions available in the `kohonen` package:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了`kohonen`包中可用的函数：
- en: '| **Function name** | **Description** |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| **函数名称** | **描述** |'
- en: '| `som` | Standard SOM |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| `som` | 标准 SOM |'
- en: '| `xyf`, `bdk` | Supervised SOM; two parallel maps |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| `xyf`, `bdk` | 监督式 SOM；两个并行映射 |'
- en: '| `supersom` | SOM with multiple parallel maps |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| `supersom` | 带有多个并行映射的 SOM |'
- en: '| `plot.kohonen` | Generic plotting function |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| `plot.kohonen` | 通用绘图函数 |'
- en: '| `summary.kohonen` | Generic summary function |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| `summary.kohonen` | 通用总结函数 |'
- en: '| `map.kohonen` | Map data to the most similar neuron |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| `map.kohonen` | 将数据映射到最相似的神经元 |'
- en: '| `predict.kohonen` | Generic function to predict properties |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| `predict.kohonen` | 通用预测属性函数 |'
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the machine learning field and we saw the learning
    process in a neural network. We learned to distinguish between supervised learning,
    unsupervised learning, and reinforcement learning. To understand in detail the
    necessary procedures, we also learned how to train and test the model.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了机器学习领域，并观察了神经网络中的学习过程。我们学会了区分监督学习、无监督学习和强化学习。为了详细了解必要的流程，我们还学习了如何训练和测试模型。
- en: Afterwards, we discovered the meaning of the data cycle and how the data must
    be collected, cleaned, converted, and then fed to the model for learning. So we
    went deeper into the evaluation model to see if the expected value is equal to
    the actual value during the test phase. We analyzed the different metrics available
    to control the model that depends on the status of the target variable.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们发现了数据周期的意义，以及数据必须如何收集、清理、转换，再输入模型进行学习。因此，我们深入研究了评估模型，查看测试阶段的预期值是否等于实际值。我们分析了可用于控制模型的不同度量标准，这些度量标准依赖于目标变量的状态。
- en: Then we discovered one of the concepts important for understanding the neural
    networks, the backpropagation algorithm, that is based on computing to update
    weights and bias ions at each level.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们发现了理解神经网络时一个重要的概念——反向传播算法，它基于计算来更新每个层级的权重和偏置。
- en: Finally, we covered two practical programs in R for the learning process, by
    applying the `neuralnet` and the `kohonen` libraries. We can systematically use
    these basics for further building of complex networks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了两个R中的实际程序，应用了`neuralnet`和`kohonen`库来实现学习过程。我们可以系统地使用这些基础知识进一步构建复杂的网络。
- en: In the next chapter, we will discover the **Deep Neural Network** (**DNN**).
    We will see some basics of the `H2O` package. Overall, `H2O` is a highly user-friendly
    package that can be used to train feed-forward networks or deep auto-encoders.
    It supports distributed computations and provides a web interface. By including
    the `H2O` package, like any other package in R, we can do all kind of modeling
    and processing of DNN.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索**深度神经网络**（**DNN**）。我们将了解`H2O`包的一些基础知识。总体而言，`H2O`是一个高度用户友好的包，可以用于训练前馈网络或深度自编码器。它支持分布式计算，并提供Web界面。通过引入`H2O`包，像使用任何其他R包一样，我们可以进行各种DNN建模和处理。
