- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Trust Region Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¿¡ä»»åŒºåŸŸæ–¹æ³•
- en: 'In this chapter, we will take a look at the approaches used to improve the
    stability of the stochastic policy gradient method. Some attempts have been made
    to make the policy improvement more stable, and in this chapter, we will focus
    on three methods:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€äº›æ”¹è¿›éšæœºç­–ç•¥æ¢¯åº¦æ–¹æ³•ç¨³å®šæ€§çš„ç­–ç•¥ã€‚å·²æœ‰ä¸€äº›å°è¯•ä½¿å¾—ç­–ç•¥æ”¹è¿›æ›´åŠ ç¨³å®šï¼Œæœ¬ç« æˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»ä¸‰ç§æ–¹æ³•ï¼š
- en: Proximal policy optimization (PPO)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰
- en: Trust region policy optimization (TRPO)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼ˆTRPOï¼‰
- en: Advantage actor-critic (A2C) using Kronecker-factored trust region (ACKTR) .
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Kronecker åˆ†è§£ä¿¡ä»»åŒºåŸŸçš„ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ï¼ˆA2Cï¼‰.
- en: In addition, we will compare these methods to a relatively new off-policy method
    called soft actor-critic (SAC), which is the evolution of the deep deterministic
    policy gradients (DDPG) method described in ChapterÂ [15](ch019.xhtml#x1-27200015).
    To compare them to the A2C baseline, we will use several environments from the
    so-called â€œlocomotion gym environmentsâ€ â€“ environments shipped with Farama Gymnasium
    (using MuJoCo and PyBullet). We also will do a head-to-head comparison between
    PyBullet and MuJoCo (which we discussed in Chapter 15).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†è¿™äº›æ–¹æ³•ä¸ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„ç¦»ç­–ç•¥æ–¹æ³•â€”â€”è½¯æ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ï¼ˆSACï¼‰è¿›è¡Œæ¯”è¾ƒï¼ŒSAC æ˜¯æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆDDPGï¼‰çš„æ¼”å˜ï¼ŒDDPG
    æ–¹æ³•åœ¨ç¬¬[15ç« ](ch019.xhtml#x1-27200015)ä¸­æœ‰è¯¦ç»†æè¿°ã€‚ä¸ºäº†ä¸ A2C åŸºå‡†æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ‰€è°“çš„â€œè¿åŠ¨è®­ç»ƒç¯å¢ƒâ€ä¸­çš„å‡ ä¸ªç¯å¢ƒâ€”â€”è¿™äº›ç¯å¢ƒä¸
    Farama Gymnasium ä¸€èµ·æä¾›ï¼ˆä½¿ç”¨ MuJoCo å’Œ PyBulletï¼‰ã€‚æˆ‘ä»¬è¿˜å°†å¯¹ PyBullet å’Œ MuJoCo è¿›è¡Œæ­£é¢æ¯”è¾ƒï¼ˆæˆ‘ä»¬åœ¨ç¬¬15ç« ä¸­è®¨è®ºäº†è¿™äº›å†…å®¹ï¼‰ã€‚
- en: 'The purpose of the methods that we will look at is to improve the stability
    of the policy update during training. There is a dilemma: on the one hand, weâ€™d
    like to train as fast as we can, making large steps during the stochastic gradient
    descent (SGD) update. On the other hand, a large update of the policy is usually
    a bad idea. The policy is a very nonlinear thing, so a large update could ruin
    the policy weâ€™ve just learned.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦è®¨è®ºçš„æ–¹æ³•çš„ç›®çš„æ˜¯æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§ã€‚è¿™é‡Œå­˜åœ¨ä¸€ä¸ªä¸¤éš¾å›°å¢ƒï¼šä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½å¿«åœ°è®­ç»ƒï¼Œåœ¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ›´æ–°è¿‡ç¨‹ä¸­é‡‡å–è¾ƒå¤§çš„æ­¥ä¼ã€‚å¦ä¸€æ–¹é¢ï¼Œç­–ç•¥çš„å¤§å¹…æ›´æ–°é€šå¸¸æ˜¯ä¸ªåä¸»æ„ã€‚ç­–ç•¥æ˜¯ä¸€ä¸ªé«˜åº¦éçº¿æ€§çš„äº‹ç‰©ï¼Œå› æ­¤å¤§å¹…æ›´æ–°å¯èƒ½ä¼šç ´åæˆ‘ä»¬åˆšåˆšå­¦ä¹ åˆ°çš„ç­–ç•¥ã€‚
- en: Things can become even worse in the reinforcement learning (RL) landscape because
    you canâ€™t recover from making a bad update to the policy by subsequent updates.
    Instead, the bad policy will provide bad experience samples that we will use in
    subsequent training steps, which could break our policy completely. Thus, we want
    to avoid making large updates by all means possible. One of the naÃ¯ve solutions
    would be to use a small learning rate to take baby steps during SGD, but this
    would significantly slow down the convergence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é¢†åŸŸï¼Œæƒ…å†µå¯èƒ½ä¼šå˜å¾—æ›´ç³Ÿï¼Œå› ä¸ºä½ æ— æ³•é€šè¿‡åç»­çš„æ›´æ–°ä»ä¸€ä¸ªä¸å¥½çš„ç­–ç•¥æ›´æ–°ä¸­æ¢å¤è¿‡æ¥ã€‚ç›¸åï¼Œç³Ÿç³•çš„ç­–ç•¥ä¼šæä¾›ä¸è‰¯çš„ç»éªŒæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬ä¼šåœ¨åç»­çš„è®­ç»ƒæ­¥éª¤ä¸­ä½¿ç”¨ï¼Œå¯èƒ½ä¼šå½»åº•ç ´åæˆ‘ä»¬çš„ç­–ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¦å°½ä¸€åˆ‡å¯èƒ½é¿å…è¿›è¡Œå¤§çš„æ›´æ–°ã€‚ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œåœ¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è¿‡ç¨‹ä¸­é‡‡å–å°æ­¥ä¼ï¼Œä½†è¿™ä¼šæ˜¾è‘—å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚
- en: To break this vicious cycle, several attempts have been made by researchers
    to estimate the effect that our policy update is going to have in terms of future
    outcomes. One of the popular approaches is the trust region optimization extension,
    which constrains the steps taken during the optimization to limit its effect on
    the policy. The main idea is to prevent a dramatic policy update during the loss
    optimization by checking the Kullback-Leibler (KL) divergence between the old
    and the new policy. Of course, this is an informal explanation, but it can help
    you understand the idea, especially as those methods are quite math-heavy (especially
    TRPO).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰“ç ´è¿™ä¸ªæ¶æ€§å¾ªç¯ï¼Œç ”ç©¶äººå‘˜å·²åšå‡ºå¤šæ¬¡å°è¯•ï¼Œè¯„ä¼°æˆ‘ä»¬çš„ç­–ç•¥æ›´æ–°å¯¹æœªæ¥ç»“æœçš„å½±å“ã€‚ä¸€ä¸ªæµè¡Œçš„æ–¹æ³•æ˜¯ä¿¡ä»»åŒºåŸŸä¼˜åŒ–æ‰©å±•ï¼Œå®ƒé™åˆ¶äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡‡å–çš„æ­¥ä¼ï¼Œä»è€Œé™åˆ¶å¯¹ç­–ç•¥çš„å½±å“ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯åœ¨æŸå¤±ä¼˜åŒ–è¿‡ç¨‹ä¸­é€šè¿‡æ£€æŸ¥æ—§ç­–ç•¥å’Œæ–°ç­–ç•¥ä¹‹é—´çš„
    Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦æ¥é˜²æ­¢å‰§çƒˆçš„ç­–ç•¥æ›´æ–°ã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯ä¸€ä¸ªéæ­£å¼çš„è§£é‡Šï¼Œä½†å®ƒå¯ä»¥å¸®åŠ©ä½ ç†è§£è¿™ä¸€æ€æƒ³ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºè¿™äº›æ–¹æ³•ç›¸å½“æ•°å­¦åŒ–ï¼ˆå°¤å…¶æ˜¯
    TRPOï¼‰ã€‚
- en: Environments
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¯å¢ƒ
- en: Previous editions of this book used the Roboschool library from OpenAI ([https://openai.com/index/roboschool](https://openai.com/index/roboschool))
    to illustrate trust region methods. But eventually, OpenAI deprecated Roboschool
    and stopped its support.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„æ—©æœŸç‰ˆæœ¬ä½¿ç”¨äº†æ¥è‡ª OpenAI çš„ Roboschool åº“ï¼ˆ[https://openai.com/index/roboschool](https://openai.com/index/roboschool)ï¼‰æ¥è¯´æ˜ä¿¡ä»»åŒºåŸŸæ–¹æ³•ã€‚ä½†æ˜¯æœ€ç»ˆï¼ŒOpenAI
    åœæ­¢äº†å¯¹ Roboschool çš„æ”¯æŒå¹¶å¼ƒç”¨äº†è¯¥åº“ã€‚
- en: 'But environments are still available in other sources:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å…¶ä»–æ¥æºä»ç„¶æä¾›è¿™äº›ç¯å¢ƒï¼š
- en: 'PyBullet: The physics simulator we experimented with in the previous chapter,
    which includes a wide variety of environments that support Gym. PyBullet may be
    a bit outdated (the latest release was in 2022), but it is still workable with
    a bit of hacking.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBulletï¼šæˆ‘ä»¬åœ¨å‰ä¸€ç« ä¸­å®éªŒè¿‡çš„ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼ŒåŒ…å«æ”¯æŒGymçš„å„ç§ç¯å¢ƒã€‚PyBulletå¯èƒ½æœ‰äº›è¿‡æ—¶ï¼ˆæœ€æ–°ç‰ˆæœ¬å‘å¸ƒäº2022å¹´ï¼‰ï¼Œä½†é€šè¿‡ä¸€äº›å°ä¿®æ”¹ï¼Œå®ƒä»ç„¶å¯ä»¥æ­£å¸¸å·¥ä½œã€‚
- en: 'Farama Gymnasium MuJoCo environments: MuJoCo is a physics simulator that we
    discussed in ChapterÂ [15](ch019.xhtml#x1-27200015). After it was made open source,
    MuJoCo was adopted in various products, including Gymnasium, which ships several
    environments: [https://gymnasium.farama.org/environments/mujoco/](https://gymnasium.farama.org/environments/mujoco/).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farama Gymnasium MuJoCoç¯å¢ƒï¼šMuJoCoæ˜¯æˆ‘ä»¬åœ¨ç¬¬[15](ch019.xhtml#x1-27200015)ç« ä¸­è®¨è®ºçš„ç‰©ç†æ¨¡æ‹Ÿå™¨ã€‚è‡ªä»å®ƒå¼€æºä»¥æ¥ï¼ŒMuJoCoå·²è¢«åº”ç”¨åˆ°å¤šä¸ªäº§å“ä¸­ï¼ŒåŒ…æ‹¬Gymnasiumï¼Œå®ƒæä¾›äº†å¤šä¸ªç¯å¢ƒï¼š[https://gymnasium.farama.org/environments/mujoco/](https://gymnasium.farama.org/environments/mujoco/)ã€‚
- en: 'In this chapter, we will explore two problems: HalfCheetah-v4, which models
    a two-legged creature, and Ant-v4, which has four legs. Their state and action
    spaces are very similar to the Minitaur environment that we saw in ChapterÂ [15](ch019.xhtml#x1-27200015):
    the state includes characteristics from joints, and the actions are activations
    of those joints. The goal for each problem is to move as far as possible, minimizing
    the energy spent. The following figure shows screenshots of the two environments:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸¤ä¸ªé—®é¢˜ï¼šHalfCheetah-v4ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªä¸¤æ¡è…¿çš„ç”Ÿç‰©ï¼Œå’ŒAnt-v4ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªå››æ¡è…¿çš„ç”Ÿç‰©ã€‚å®ƒä»¬çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¸æˆ‘ä»¬åœ¨ç¬¬[15](ch019.xhtml#x1-27200015)ç« ä¸­çœ‹åˆ°çš„Minitaurç¯å¢ƒéå¸¸ç›¸ä¼¼ï¼šçŠ¶æ€åŒ…æ‹¬å…³èŠ‚çš„ç‰¹å¾ï¼Œè€ŒåŠ¨ä½œæ˜¯è¿™äº›å…³èŠ‚çš„æ¿€æ´»ã€‚æ¯ä¸ªé—®é¢˜çš„ç›®æ ‡æ˜¯å°½å¯èƒ½åœ°ç§»åŠ¨ï¼ŒåŒæ—¶æœ€å°åŒ–èƒ½é‡æ¶ˆè€—ã€‚ä¸‹å›¾å±•ç¤ºäº†è¿™ä¸¤ä¸ªç¯å¢ƒçš„æˆªå›¾ï¼š
- en: '![PIC](img/file215.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file215.png)'
- en: 'FigureÂ 16.1: Screenshots of the cheetah and ant environments'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.1ï¼šçŒè±¹å’Œèš‚èšç¯å¢ƒçš„æˆªå›¾
- en: 'In our experiment, weâ€™ll use PyBullet and MuJoCo to do a comparison of both
    simulators in terms of speed and training dynamics (however, note that the internal
    structure of the PyBullet and MuJoCo environments might be different, and so the
    comparison of training dynamics may not always be reliable). To install the Gymnasium
    with MuJoCo extensions, you need to run the following command in your Python environment:
    pip install gymnasium[mujoco]==0.29.0.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨PyBulletå’ŒMuJoCoå¯¹è¿™ä¸¤ä¸ªæ¨¡æ‹Ÿå™¨è¿›è¡Œé€Ÿåº¦å’Œè®­ç»ƒåŠ¨æ€æ–¹é¢çš„æ¯”è¾ƒï¼ˆä½†è¯·æ³¨æ„ï¼ŒPyBulletå’ŒMuJoCoç¯å¢ƒçš„å†…éƒ¨ç»“æ„å¯èƒ½ä¸åŒï¼Œå› æ­¤è®­ç»ƒåŠ¨æ€çš„æ¯”è¾ƒå¯èƒ½å¹¶ä¸æ€»æ˜¯å¯é çš„ï¼‰ã€‚è¦å®‰è£…å¸¦æœ‰MuJoCoæ‰©å±•çš„Gymnasiumï¼Œä½ éœ€è¦åœ¨Pythonç¯å¢ƒä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š`pip
    install gymnasium[mujoco]==0.29.0`ã€‚
- en: The A2C baseline
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A2CåŸºå‡†
- en: 'To establish the baseline results, we will use the A2C method in a very similar
    way to the previous chapter. The complete source code is in the Chapter16/01_train_a2c.py
    and Chapter16/lib/model.py files. There are a few differences between this baseline
    and the version we used before:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å»ºç«‹åŸºå‡†ç»“æœï¼Œæˆ‘ä»¬å°†ä»¥ä¸å‰ä¸€ç« éå¸¸ç›¸ä¼¼çš„æ–¹å¼ä½¿ç”¨A2Cæ–¹æ³•ã€‚å®Œæ•´çš„æºä»£ç ä½äºChapter16/01_train_a2c.pyå’ŒChapter16/lib/model.pyæ–‡ä»¶ä¸­ã€‚è¿™ä¸ªåŸºå‡†ä¸æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„ç‰ˆæœ¬æœ‰ä¸€äº›åŒºåˆ«ï¼š
- en: 16 parallel environments are used to gather experience during the training.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨16ä¸ªå¹¶è¡Œç¯å¢ƒæ¥æ”¶é›†ç»éªŒã€‚
- en: They differ in model structure and the way that we perform exploration.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä»¬åœ¨æ¨¡å‹ç»“æ„å’Œæˆ‘ä»¬è¿›è¡Œæ¢ç´¢çš„æ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒã€‚
- en: Implementation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: To illustrate the differences between this baseline and the previously discussed
    version, letâ€™s look at the model and the agent classes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ä¸ªåŸºå‡†å’Œä¹‹å‰è®¨è®ºçš„ç‰ˆæœ¬ä¹‹é—´çš„åŒºåˆ«ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹æ¨¡å‹å’Œä»£ç†ç±»ã€‚
- en: The actor and critic are placed in separate networks without sharing weights.
    They follow the approach used in ChapterÂ [15](ch019.xhtml#x1-27200015), with our
    critic estimating the mean and the variance for the actions. However, now, variance
    is not a separate head of the base network; it is just a single parameter of the
    model. This parameter will be adjusted during the training by SGD, but it doesnâ€™t
    depend on the observation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Actorå’ŒCriticè¢«æ”¾ç½®åœ¨ä¸åŒçš„ç½‘ç»œä¸­ï¼Œä¸”ä¸å…±äº«æƒé‡ã€‚å®ƒä»¬éµå¾ªç¬¬[15](ch019.xhtml#x1-27200015)ç« ä¸­ä½¿ç”¨çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„Criticä¼°è®¡åŠ¨ä½œçš„å‡å€¼å’Œæ–¹å·®ã€‚ç„¶è€Œï¼Œç°åœ¨ï¼Œæ–¹å·®ä¸å†æ˜¯åŸºç¡€ç½‘ç»œçš„å•ç‹¬å¤´éƒ¨ï¼›å®ƒåªæ˜¯æ¨¡å‹çš„ä¸€ä¸ªå‚æ•°ã€‚è¿™ä¸ªå‚æ•°å°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡SGDè¿›è¡Œè°ƒæ•´ï¼Œä½†å®ƒä¸ä¾èµ–äºè§‚å¯Ÿç»“æœã€‚
- en: 'The actor network has two hidden layers of 64 neurons, each with tanh nonlinearity
    (to push the output in the âˆ’1â€¦1 range). The variance is modeled as a separate
    network parameter and is interpreted as a logarithm of the standard deviation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Actorç½‘ç»œæœ‰ä¸¤ä¸ª64ä¸ªç¥ç»å…ƒçš„éšè—å±‚ï¼Œæ¯ä¸ªå±‚éƒ½æœ‰tanhéçº¿æ€§ï¼ˆå°†è¾“å‡ºå‹ç¼©åˆ°âˆ’1â€¦1èŒƒå›´å†…ï¼‰ã€‚æ–¹å·®è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå•ç‹¬çš„ç½‘ç»œå‚æ•°ï¼Œå¹¶è¢«è§£é‡Šä¸ºæ ‡å‡†å·®çš„å¯¹æ•°ï¼š
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The critic network also has two hidden layers of the same size, with one single
    output value, which is the estimation of V (s), which is a discounted value of
    the state:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„è®ºç½‘ç»œä¹Ÿæœ‰ä¸¤ä¸ªç›¸åŒå¤§å°çš„éšè—å±‚ï¼Œå¹¶ä¸”åªæœ‰ä¸€ä¸ªè¾“å‡ºå€¼ï¼Œå³V(s)çš„ä¼°è®¡å€¼ï¼Œè¿™æ˜¯çŠ¶æ€çš„æŠ˜æ‰£å€¼ï¼š
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The agent that converts the state into the action also works by simply obtaining
    the predicted mean from the state and applying the noise with variance, dictated
    by the current value of the logstd parameter:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°†çŠ¶æ€è½¬æ¢ä¸ºåŠ¨ä½œçš„æ™ºèƒ½ä½“ä¹Ÿä»…é€šè¿‡ä»çŠ¶æ€ä¸­è·å¾—é¢„æµ‹å‡å€¼ï¼Œå¹¶æ ¹æ®å½“å‰logstdå‚æ•°çš„å€¼åº”ç”¨å…·æœ‰æ–¹å·®çš„å™ªå£°æ¥å·¥ä½œï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Results
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'The training utility 01_train_a2c.py could be started in two different modes:
    with PyBullet as the physics simulator (without any extra command-line options)
    or with MuJoCo (if the --mujoco parameter is given).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå·¥å…·01_train_a2c.pyå¯ä»¥ä»¥ä¸¤ç§ä¸åŒçš„æ¨¡å¼å¯åŠ¨ï¼šä½¿ç”¨PyBulletä½œä¸ºç‰©ç†æ¨¡æ‹Ÿå™¨ï¼ˆæ— éœ€é¢å¤–çš„å‘½ä»¤è¡Œé€‰é¡¹ï¼‰æˆ–ä½¿ç”¨MuJoCoï¼ˆå¦‚æœæä¾›äº†--mujocoå‚æ•°ï¼‰ã€‚
- en: By default, the HalfCheetah environment is used, which simulates a flat two-legged
    creature that can jump around on its legs. With -e ant, you can switch to the
    Ant environment, which is a 3-dimensional 4-legged spider. You can also experiment
    with other environments shipped with Gymnasium and PyBullet, but this will require
    tweaking the common.py module.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨HalfCheetahç¯å¢ƒï¼Œå®ƒæ¨¡æ‹Ÿäº†ä¸€ä¸ªå¯ä»¥ç”¨è…¿è·³è·ƒçš„å¹³åœ°åŒè¶³ç”Ÿç‰©ã€‚é€šè¿‡-e anté€‰é¡¹ï¼Œä½ å¯ä»¥åˆ‡æ¢åˆ°Antç¯å¢ƒï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰ç»´çš„å››è¶³èœ˜è››ã€‚ä½ è¿˜å¯ä»¥å°è¯•Gymnasiumå’ŒPyBulletéšé™„çš„å…¶ä»–ç¯å¢ƒï¼Œä½†è¿™éœ€è¦è°ƒæ•´common.pyæ¨¡å—ã€‚
- en: Results for HalfCheetah on PyBullet are shown in FigureÂ [16.2](#x1-294002r2).
    Performance on my machine (using the GPU) was about 1,600 frames per second during
    the training, so 100M training steps took 20 hours in total.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PyBulletä¸­HalfCheetahçš„ç»“æœå¦‚å›¾[16.2](#x1-294002r2)æ‰€ç¤ºã€‚æˆ‘æœºå™¨ä¸Šçš„è¡¨ç°ï¼ˆä½¿ç”¨GPUï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤§çº¦æ˜¯1,600å¸§æ¯ç§’ï¼Œå› æ­¤100Mçš„è®­ç»ƒæ­¥éª¤æ€»å…±èŠ±è´¹äº†20å°æ—¶ã€‚
- en: '![PIC](img/B22150_16_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_02.png)'
- en: 'FigureÂ 16.2: The reward during training (left) and test reward (right) for
    HalfCheetah on PyBullet'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.2ï¼šPyBulletä¸­HalfCheetahè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: The dynamics suggest that the policy could be further improved with more time
    given to optimization, but for our purpose of method comparison, it should be
    enough. Of course, if youâ€™re curious and have plenty of time, you can run this
    for longer and find the point when the policy stops improving. According to research
    papers, HalfCheetah has a maximum score of around 4,000-5,000.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨åŠ›å­¦è¡¨æ˜ï¼Œé€šè¿‡ç»™äºˆä¼˜åŒ–æ›´å¤šæ—¶é—´ï¼Œç­–ç•¥å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¹è¿›ï¼Œä½†å¯¹äºæˆ‘ä»¬è¿›è¡Œæ–¹æ³•æ¯”è¾ƒçš„ç›®çš„æ¥è¯´ï¼Œç°æœ‰çš„ç»“æœåº”è¯¥è¶³å¤Ÿã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ„Ÿå…´è¶£ä¸”æ—¶é—´å……è£•ï¼Œå¯ä»¥è¿è¡Œæ›´é•¿æ—¶é—´ï¼Œæ‰¾åˆ°ç­–ç•¥åœæ­¢æ”¹è¿›çš„ç‚¹ã€‚æ ¹æ®ç ”ç©¶è®ºæ–‡ï¼ŒHalfCheetahçš„æœ€é«˜åˆ†æ•°å¤§çº¦åœ¨4,000åˆ°5,000ä¹‹é—´ã€‚
- en: 'To use MuJoCo as a physics simulation engine, training has to be started with
    the --mujoco command-line option. MuJoCo has a performance of 5,100 frames per
    second, which is 3 times faster than PyBullet, which is really nice. In addition,
    the training has much better dynamics, so in 90M training steps (which took about
    5 hours) the model got a reward of 4,500\. Plots for MuJoCo are shown in FigureÂ [16.3](#x1-294004r3):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨MuJoCoä½œä¸ºç‰©ç†ä»¿çœŸå¼•æ“ï¼Œå¿…é¡»ä½¿ç”¨--mujocoå‘½ä»¤è¡Œé€‰é¡¹å¯åŠ¨è®­ç»ƒã€‚MuJoCoçš„æ€§èƒ½ä¸º5,100å¸§æ¯ç§’ï¼Œæ¯”PyBulletå¿«ä¸‰å€ï¼Œè¿™éå¸¸æ£’ã€‚æ­¤å¤–ï¼Œè®­ç»ƒçš„åŠ¨æ€æ€§æ›´å¥½ï¼Œå› æ­¤åœ¨90Mè®­ç»ƒæ­¥éª¤ï¼ˆå¤§çº¦éœ€è¦5å°æ—¶ï¼‰ä¸­ï¼Œæ¨¡å‹å¾—åˆ°äº†4,500çš„å¥–åŠ±ã€‚MuJoCoçš„å›¾å½¢æ˜¾ç¤ºåœ¨å›¾[16.3](#x1-294004r3)ä¸­ï¼š
- en: '![PIC](img/B22150_16_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_03.png)'
- en: 'FigureÂ 16.3: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.3ï¼šMuJoCoä¸­HalfCheetahè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: The difference could be explained by a more accurate simulation, but could also
    be attributed to the difference in the observation space and the underlying model
    differences. PyBulletâ€™s model has 26 parameters provided to the agent as observations,
    while MuJoCo has only 17, so those models are not identical.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å·®å¼‚å¯ä»¥é€šè¿‡æ›´å‡†ç¡®çš„ä»¿çœŸæ¥è§£é‡Šï¼Œä½†ä¹Ÿå¯ä»¥å½’å› äºè§‚å¯Ÿç©ºé—´çš„ä¸åŒå’Œåº•å±‚æ¨¡å‹çš„å·®å¼‚ã€‚PyBulletçš„æ¨¡å‹ä¸ºæ™ºèƒ½ä½“æä¾›äº†26ä¸ªè§‚å¯Ÿå‚æ•°ï¼Œè€ŒMuJoCoåªæœ‰17ä¸ªï¼Œå› æ­¤è¿™ä¸¤ä¸ªæ¨¡å‹å¹¶ä¸å®Œå…¨ç›¸åŒã€‚
- en: To test our model in the Ant environment, the -e ant command-line option has
    to be passed to the training process. This model is more complex (due to the 3D
    nature of the model and more joints being used), so the simulation is slower.
    On PyBullet, the speed is around 1,400 frames per second. On MuJoCo, the speed
    is 2,500.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨Antç¯å¢ƒä¸­æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¿…é¡»å°†-e antå‘½ä»¤è¡Œé€‰é¡¹ä¼ é€’ç»™è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ¨¡å‹æ›´ä¸ºå¤æ‚ï¼ˆç”±äºæ¨¡å‹çš„ä¸‰ç»´ç‰¹æ€§å’Œä½¿ç”¨äº†æ›´å¤šçš„å…³èŠ‚ï¼‰ï¼Œå› æ­¤ä»¿çœŸé€Ÿåº¦è¾ƒæ…¢ã€‚åœ¨PyBulletä¸Šï¼Œé€Ÿåº¦çº¦ä¸º1,400å¸§æ¯ç§’ã€‚åœ¨MuJoCoä¸Šï¼Œé€Ÿåº¦ä¸º2,500å¸§æ¯ç§’ã€‚
- en: The MuJoCo Ant environment also has an additional check for â€œhealthinessâ€ â€“
    if the simulated creature is inclined more than a certain degree, the episode
    is terminated. This check is enabled by default and has a very negative effect
    on the training â€“ in the early stage of the training, our method has no chance
    of figuring out how to make the ant stand on its legs. The reward in the environment
    is the distance traveled, but with this early termination, our training has no
    chance of discovering this. As a result, the training process got stuck forever
    in local minima without making progress. To overcome this, we need to disable
    this healthiness check by passing the --no-unhealthy command-line option (which
    only has to be done for MuJoCo training).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MuJoCo Antç¯å¢ƒè¿˜é¢å¤–æ£€æŸ¥â€œå¥åº·çŠ¶æ€â€â€”â€”å¦‚æœæ¨¡æ‹Ÿç”Ÿç‰©çš„å€¾æ–œè§’åº¦è¶…è¿‡æŸä¸ªç‰¹å®šè§’åº¦ï¼Œå›åˆå°†è¢«ç»ˆæ­¢ã€‚é»˜è®¤å¯ç”¨æ­¤æ£€æŸ¥ï¼Œå¹¶ä¸”å®ƒå¯¹è®­ç»ƒæœ‰éå¸¸è´Ÿé¢çš„å½±å“â€”â€”åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— æ³•å¼„æ¸…æ¥šå¦‚ä½•è®©èš‚èšç«™ç«‹èµ·æ¥ã€‚ç¯å¢ƒä¸­çš„å¥–åŠ±æ˜¯æ—…è¡Œçš„è·ç¦»ï¼Œä½†ç”±äºè¿™ä¸ªæå‰ç»ˆæ­¢ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ²¡æœ‰æœºä¼šå‘ç°è¿™ä¸€ç‚¹ã€‚ç»“æœï¼Œè®­ç»ƒè¿‡ç¨‹æ°¸è¿œåœæ»åœ¨å±€éƒ¨æœ€å°å€¼ä¸­ï¼Œæ— æ³•å–å¾—è¿›å±•ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä¼ é€’--no-unhealthyå‘½ä»¤è¡Œé€‰é¡¹æ¥ç¦ç”¨æ­¤å¥åº·æ£€æŸ¥ï¼ˆä»…åœ¨MuJoCoè®­ç»ƒä¸­éœ€è¦æ‰§è¡Œæ­¤æ“ä½œï¼‰ã€‚
- en: In principle, you can implement more advanced exploration methods, such as the
    OU process (discussed in ChapterÂ [15](ch019.xhtml#x1-27200015)) or other methods
    (covered in ChapterÂ [18](ch022.xhtml#x1-32800018)) to address the issue we just
    discussed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åŸåˆ™ä¸Šï¼Œæ‚¨å¯ä»¥å®ç°æ›´é«˜çº§çš„æ¢ç´¢æ–¹æ³•ï¼Œå¦‚OUè¿‡ç¨‹ï¼ˆåœ¨ç¬¬[15](ch019.xhtml#x1-27200015)ç« ä¸­è®¨è®ºï¼‰æˆ–å…¶ä»–æ–¹æ³•ï¼ˆåœ¨ç¬¬[18](ch022.xhtml#x1-32800018)ç« ä¸­è®¨è®ºï¼‰æ¥è§£å†³æˆ‘ä»¬åˆšåˆšè®¨è®ºçš„é—®é¢˜ã€‚
- en: The results of the training in the Ant environment are shown in FigureÂ [16.4](#x1-294006r4)
    and FigureÂ [16.5](#x1-294007r5).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Antç¯å¢ƒçš„è®­ç»ƒç»“æœå¦‚å›¾[16.4](#x1-294006r4)å’Œå›¾[16.5](#x1-294007r5)æ‰€ç¤ºã€‚
- en: '![PIC](img/B22150_16_04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_04.png)'
- en: 'FigureÂ 16.4: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.4ï¼šPyBulletä¸ŠAntè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰ä¸æµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_16_05.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_05.png)'
- en: 'FigureÂ 16.5: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.5ï¼šMuJoCoä¸ŠAntè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰ä¸æµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: 'As you can see from the MuJoCo plots in FigureÂ [16.5](#x1-294007r5), the testing
    reward had almost no increase for the first 100M steps of training, but then grew
    to a score of 5,000 (the best model got 5,380 on testing). This result is quite
    impressive. According to the [https://paperswithcode.com](https://paperswithcode.com)
    website, the state of the art for Ant MuJoCo environment is 4,362.9, obtained
    by IQ-Learn in 2021: [https://paperswithcode.com/sota/mujoco-games-on-ant](https://paperswithcode.com/sota/mujoco-games-on-ant).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨ä»å›¾[16.5](#x1-294007r5)ä¸­çš„MuJoCoå›¾è¡¨ä¸­çœ‹åˆ°çš„ï¼Œæµ‹è¯•å¥–åŠ±åœ¨è®­ç»ƒçš„å‰1äº¿æ­¥å‡ ä¹æ²¡æœ‰å¢åŠ ï¼Œä½†éšåå¢é•¿åˆ°5,000åˆ†ï¼ˆæœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•ä¸­å¾—åˆ°äº†5,380åˆ†ï¼‰ã€‚è¿™ä¸ªç»“æœç›¸å½“ä»¤äººå°è±¡æ·±åˆ»ã€‚æ ¹æ®[https://paperswithcode.com](https://paperswithcode.com)ç½‘ç«™çš„æ•°æ®ï¼ŒAntåœ¨MuJoCoç¯å¢ƒä¸­çš„æœ€æ–°æŠ€æœ¯æ°´å¹³æ˜¯4,362.9ï¼Œç”±IQ-Learnåœ¨2021å¹´è·å¾—ï¼š[https://paperswithcode.com/sota/mujoco-games-on-ant](https://paperswithcode.com/sota/mujoco-games-on-ant)ã€‚
- en: Video recording
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§†é¢‘è®°å½•
- en: 'As in the previous chapter, there is a utility that can benchmark the trained
    model and record a video of the agent in action. As all the methods in this chapter
    share the same actor network, the tool is universal for all the methods illustrated
    here: 02_play.py.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸Šä¸€ç« ä¸€æ ·ï¼Œå­˜åœ¨ä¸€ä¸ªå·¥å…·å¯ä»¥åŸºå‡†æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶å½•åˆ¶ä»£ç†çš„åŠ¨ä½œè§†é¢‘ã€‚ç”±äºæœ¬ç« ä¸­çš„æ‰€æœ‰æ–¹æ³•å…±äº«ç›¸åŒçš„æ¼”å‘˜ç½‘ç»œï¼Œå› æ­¤è¯¥å·¥å…·å¯¹æœ¬ç« æ‰€ç¤ºçš„æ‰€æœ‰æ–¹æ³•éƒ½æ˜¯é€šç”¨çš„ï¼š02_play.pyã€‚
- en: You need to pass the model file stored in the saves directory during training,
    change the environment using the -e ant command line, and enable the MuJoCo engine
    with the --mujoco parameter. This is important because the same environments in
    PyBullet and MuJoCo have different amounts of observations, and so the physics
    engine has to match to the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‚¨éœ€è¦ä¼ é€’å­˜å‚¨åœ¨`saves`ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶ï¼Œé€šè¿‡-e antå‘½ä»¤è¡Œæ›´æ”¹ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨--mujocoå‚æ•°å¯ç”¨MuJoCoå¼•æ“ã€‚è¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå› ä¸ºPyBulletå’ŒMuJoCoä¸­çš„ç›¸åŒç¯å¢ƒæœ‰ä¸åŒçš„è§‚å¯Ÿé‡ï¼Œå› æ­¤ç‰©ç†å¼•æ“å¿…é¡»ä¸æ¨¡å‹åŒ¹é…ã€‚
- en: 'You can find the individual videos for the best A2C models as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥æ‰¾åˆ°æœ€ä½³A2Cæ¨¡å‹çš„å•ç‹¬è§†é¢‘ï¼Œç½‘å€å¦‚ä¸‹ï¼š
- en: 'HalfCheetah on PyBullet (score 2,189): [https://youtu.be/f3ZhjnORQm0](https://youtu.be/f3ZhjnORQm0)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HalfCheetahåœ¨PyBulletä¸Šçš„è¡¨ç°ï¼ˆå¾—åˆ†2,189ï¼‰ï¼š[https://youtu.be/f3ZhjnORQm0](https://youtu.be/f3ZhjnORQm0)
- en: 'HalfCheetah on MuJoCo (score 4,718): [https://youtube.com/shorts/SpaWbS0hM8I](https://youtube.com/shorts/SpaWbS0hM8I)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HalfCheetahåœ¨MuJoCoä¸Šçš„è¡¨ç°ï¼ˆå¾—åˆ†4,718ï¼‰ï¼š[https://youtube.com/shorts/SpaWbS0hM8I](https://youtube.com/shorts/SpaWbS0hM8I)
- en: 'Ant on PyBullet (score 2,425): [https://youtu.be/SIUM_Q24zSk](https://youtu.be/SIUM_Q24zSk)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBulletä¸Šçš„Antï¼ˆå¾—åˆ†2,425ï¼‰ï¼š[https://youtu.be/SIUM_Q24zSk](https://youtu.be/SIUM_Q24zSk)
- en: 'Ant on MuJoCo (score 5,380): [https://youtube.com/shorts/mapOraGKtG0](https://youtube.com/shorts/mapOraGKtG0)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antåœ¨MuJoCoä¸Šçš„è¡¨ç°ï¼ˆå¾—åˆ†5380ï¼‰ï¼š[https://youtube.com/shorts/mapOraGKtG0](https://youtube.com/shorts/mapOraGKtG0)
- en: PPO
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO
- en: The PPO method came from the OpenAI team, and it was proposed after TRPO, which
    is from 2015\. However, we will start with PPO because it is much simpler than
    TRPO. It was first proposed in the 2017 paper named Proximal Policy Optimization
    Algorithms by Schulman et al. [[Sch+17](#)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PPOæ–¹æ³•æ¥è‡ªOpenAIå›¢é˜Ÿï¼Œåœ¨TRPOä¹‹åæå‡ºï¼ŒTRPOæ˜¯2015å¹´æå‡ºçš„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†ä»PPOå¼€å§‹ï¼Œå› ä¸ºå®ƒæ¯”TRPOç®€å•å¾—å¤šã€‚å®ƒæœ€æ—©æ˜¯åœ¨2017å¹´ç”±Schulmanç­‰äººæå‡ºçš„è®ºæ–‡ã€ŠProximal
    Policy Optimization Algorithmsã€‹ä¸­æå‡ºçš„[[Sch+17](#)]ã€‚
- en: 'The core improvement over the classic A2C method is changing the formula used
    to estimate the policy gradients. Instead of using the gradient of the logarithm
    probability of the action taken, the PPO method uses a different objective: the
    ratio between the new and the old policy scaled by the advantages.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å¯¹äºç»å…¸çš„A2Cæ–¹æ³•ï¼Œæ ¸å¿ƒçš„æ”¹è¿›åœ¨äºä¼°è®¡ç­–ç•¥æ¢¯åº¦æ—¶æ‰€ä½¿ç”¨çš„å…¬å¼å‘ç”Ÿäº†å˜åŒ–ã€‚PPOæ–¹æ³•å¹¶ä¸æ˜¯ä½¿ç”¨æ‰€é‡‡å–åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡çš„æ¢¯åº¦ï¼Œè€Œæ˜¯ä½¿ç”¨äº†ä¸€ä¸ªä¸åŒçš„ç›®æ ‡ï¼šæ–°æ—§ç­–ç•¥ä¹‹é—´çš„æ¯”ç‡ï¼ŒæŒ‰ä¼˜åŠ¿è¿›è¡Œç¼©æ”¾ã€‚
- en: In math form, the A2C objective could be written like this
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å­¦å½¢å¼ä¸­ï¼ŒA2Cçš„ç›®æ ‡å¯ä»¥å†™ä½œè¿™æ ·
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq60.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq60.png)'
- en: which means our gradient on model ğœƒ is estimated as the logarithm of the policy
    Ï€ multiplied by the advantage A.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬å¯¹æ¨¡å‹ğœƒçš„æ¢¯åº¦è¢«ä¼°è®¡ä¸ºç­–ç•¥Ï€çš„å¯¹æ•°ä¸ä¼˜åŠ¿Açš„ä¹˜ç§¯ã€‚
- en: 'The new objective proposed in PPO is the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PPOä¸­æå‡ºçš„æ–°ç›®æ ‡æ˜¯ä»¥ä¸‹å†…å®¹ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq55.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq55.png)'
- en: 'The reason for changing the objective is the same as with the cross-entropy
    method covered in ChapterÂ [4](ch008.xhtml#x1-740004): importance sampling. However,
    if we just start to blindly maximize this value, it may lead to a very large update
    to the policy weights. To limit the update, the clipped objective is used. If
    we write the ratio between the new and the old policy as ![-Ï€ğœƒ(at|st)- Ï€ğœƒold(at|st)](img/eq56.png),
    the clipped objective could be written as'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¹å˜ç›®æ ‡å‡½æ•°çš„åŸå› ä¸ç¬¬[4ç« ](ch008.xhtml#x1-740004)ä¸­æ¶‰åŠçš„äº¤å‰ç†µæ–¹æ³•ç›¸åŒï¼šé‡è¦æ€§é‡‡æ ·ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬åªæ˜¯ç›²ç›®åœ°å¼€å§‹æœ€å¤§åŒ–è¿™ä¸ªå€¼ï¼Œå®ƒå¯èƒ½ä¼šå¯¼è‡´ç­–ç•¥æƒé‡çš„éå¸¸å¤§æ›´æ–°ã€‚ä¸ºäº†é™åˆ¶æ›´æ–°ï¼Œé‡‡ç”¨äº†å‰ªåˆ‡ç›®æ ‡ã€‚å¦‚æœæˆ‘ä»¬å°†æ–°æ—§ç­–ç•¥ä¹‹é—´çš„æ¯”ç‡å†™ä¸º
    ![-Ï€ğœƒ(at|st)- Ï€ğœƒold(at|st)](img/eq56.png)ï¼Œé‚£ä¹ˆå‰ªåˆ‡ç›®æ ‡å¯ä»¥å†™ä¸º
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq57.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq57.png)'
- en: This objective limits the ratio between the old and the new policy to be in
    the interval [1 âˆ’ğœ–,1 + ğœ–], so by varying ğœ–, we can limit the size of the update.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç›®æ ‡å°†æ—§ç­–ç•¥å’Œæ–°ç­–ç•¥ä¹‹é—´çš„æ¯”ç‡é™åˆ¶åœ¨åŒºé—´[1 âˆ’ğœ–,1 + ğœ–]å†…ï¼Œå› æ­¤é€šè¿‡å˜åŒ–ğœ–ï¼Œæˆ‘ä»¬å¯ä»¥é™åˆ¶æ›´æ–°çš„å¤§å°ã€‚
- en: Another difference from the A2C method is the way that we estimate the advantage.
    In the A2C paper, the advantage obtained from the finite-horizon estimation of
    T steps is in the form
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸A2Cæ–¹æ³•çš„å¦ä¸€ä¸ªåŒºåˆ«åœ¨äºæˆ‘ä»¬å¦‚ä½•ä¼°è®¡ä¼˜åŠ¿ã€‚åœ¨A2Cè®ºæ–‡ä¸­ï¼Œä»Tæ­¥çš„æœ‰é™è§†é‡ä¼°è®¡ä¸­è·å¾—çš„ä¼˜åŠ¿æ˜¯ä»¥ä¸‹å½¢å¼
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq58.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq58.png)'
- en: In the PPO paper, the authors used a more general estimation
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PPOè®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†æ›´ä¸€èˆ¬çš„ä¼°è®¡æ–¹æ³•
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq59.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq59.png)'
- en: where Ïƒ[t] = r[t] + Î³V (s[t+1]) âˆ’V (s[t]).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­Ïƒ[t] = r[t] + Î³V (s[t+1]) âˆ’V (s[t])ã€‚
- en: 'The original A2C estimation is a special case of the proposed method with Î»
    = 1\. The PPO method also uses a slightly different training procedure: a long
    sequence of samples is obtained from the environment and then the advantage is
    estimated for the whole sequence before several epochs of training are performed.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹çš„A2Cä¼°è®¡æ˜¯æå‡ºçš„æ–¹æ³•çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå½“Î» = 1æ—¶ã€‚PPOæ–¹æ³•è¿˜ä½¿ç”¨äº†ç¨å¾®ä¸åŒçš„è®­ç»ƒç¨‹åºï¼šä»ç¯å¢ƒä¸­è·å–ä¸€é•¿ä¸²æ ·æœ¬ï¼Œç„¶ååœ¨æ‰§è¡Œå¤šä¸ªè®­ç»ƒå‘¨æœŸä¹‹å‰ï¼Œä¼°ç®—æ•´ä¸ªåºåˆ—çš„ä¼˜åŠ¿ã€‚
- en: Implementation
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'The code of the sample is placed in two source code files: Chapter16/04_train_ppo.py
    and Chapter16/lib/model.py. The actor, the critic, and the agent classes are exactly
    the same as we had in the A2C baseline.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹çš„ä»£ç è¢«åˆ†å¸ƒåœ¨ä¸¤ä¸ªæºä»£ç æ–‡ä»¶ä¸­ï¼šChapter16/04_train_ppo.py å’Œ Chapter16/lib/model.pyã€‚æ¼”å‘˜ã€è¯„è®ºå‘˜å’Œä»£ç†ç±»ä¸æˆ‘ä»¬åœ¨A2CåŸºçº¿ä¸­çš„ç±»å®Œå…¨ç›¸åŒã€‚
- en: 'The differences are in the training procedure and the way that we calculate
    advantages, but letâ€™s start with the hyperparameters:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åŒºåˆ«åœ¨äºè®­ç»ƒç¨‹åºå’Œæˆ‘ä»¬è®¡ç®—ä¼˜åŠ¿çš„æ–¹å¼ï¼Œä½†è®©æˆ‘ä»¬ä»è¶…å‚æ•°å¼€å§‹ï¼š
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The value of GAMMA is already familiar, but GAE_LAMBDA is the new constant that
    specifies the lambda factor in the advantage estimator. The authors chose to use
    a value of 0.95 in the PPO paper.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GAMMAçš„å€¼å·²ç»å¾ˆç†Ÿæ‚‰ï¼Œä½†GAE_LAMBDAæ˜¯ä¸€ä¸ªæ–°çš„å¸¸æ•°ï¼Œç”¨äºæŒ‡å®šä¼˜åŠ¿ä¼°è®¡ä¸­çš„Î»å› å­ã€‚ä½œè€…åœ¨PPOè®ºæ–‡ä¸­é€‰æ‹©äº†0.95çš„å€¼ã€‚
- en: The method assumes that a large number of transitions will be obtained from
    the environment for every subiteration. (As mentioned previously in this section,
    when describing PPO, during training, it performs several epochs over the sampled
    training batch.) We also use two different optimizers for the actor and the critic
    (as they have no shared weights).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•å‡è®¾åœ¨æ¯æ¬¡å­è¿­ä»£ä¸­å°†ä»ç¯å¢ƒä¸­è·å¾—å¤§é‡çš„è¿‡æ¸¡ã€‚ï¼ˆå¦‚æœ¬èŠ‚å‰é¢æ‰€è¿°ï¼Œæè¿°PPOæ—¶æåˆ°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒå¯¹é‡‡æ ·çš„è®­ç»ƒæ‰¹æ¬¡æ‰§è¡Œå¤šæ¬¡epochã€‚ï¼‰æˆ‘ä»¬è¿˜ä¸ºæ¼”å‘˜å’Œè¯„è®ºè€…ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„ä¼˜åŒ–å™¨ï¼ˆå› ä¸ºå®ƒä»¬æ²¡æœ‰å…±äº«æƒé‡ï¼‰ã€‚
- en: 'For every batch of TRAJECTORY_SIZE samples, we perform PPO_EPOCHES iterations
    of the PPO objective, with mini-batches of 64 samples. The value PPO_EPS specifies
    the clipping value for the ratio of the new and the old policy. The following
    function takes the trajectory with steps and calculates advantages for the actor
    and reference values for the critic training. Our trajectory is not a single episode,
    but could be several episodes concatenated together:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸€æ‰¹TRAJECTORY_SIZEæ ·æœ¬ï¼Œæˆ‘ä»¬æ‰§è¡ŒPPO_EPOCHESæ¬¡PPOç›®æ ‡çš„è¿­ä»£ï¼Œæ¯æ¬¡ä½¿ç”¨64ä¸ªæ ·æœ¬çš„å°æ‰¹é‡ã€‚å€¼PPO_EPSæŒ‡å®šæ–°æ—§ç­–ç•¥æ¯”ç‡çš„è£å‰ªå€¼ã€‚ä»¥ä¸‹å‡½æ•°æ¥å—å¸¦æœ‰æ­¥éª¤çš„è½¨è¿¹ï¼Œå¹¶è®¡ç®—æ¼”å‘˜çš„ä¼˜åŠ¿å€¼å’Œè¯„è®ºè€…è®­ç»ƒçš„å‚è€ƒå€¼ã€‚æˆ‘ä»¬çš„è½¨è¿¹ä¸æ˜¯å•ä¸ªå›åˆï¼Œè€Œæ˜¯å¯ä»¥ç”±å¤šä¸ªå›åˆè¿æ¥è€Œæˆï¼š
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As the first step, we ask the critic to convert states into values.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è¦æ±‚è¯„è®ºè€…å°†çŠ¶æ€è½¬æ¢ä¸ºå€¼ã€‚
- en: 'The next loop joins the values obtained and experience points:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå¾ªç¯å°†è·å¾—çš„å€¼ä¸ç»éªŒç‚¹ç»“åˆèµ·æ¥ï¼š
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For every trajectory step, we need the current value (obtained from the current
    state) and the value for the subsequent step (to perform the estimation using
    the Bellman equation). We also traverse the trajectory in reverse order in order
    to calculate more recent values of the advantage in one step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªè½¨è¿¹æ­¥éª¤ï¼Œæˆ‘ä»¬éœ€è¦å½“å‰å€¼ï¼ˆä»å½“å‰çŠ¶æ€è·å¾—ï¼‰å’Œåç»­æ­¥éª¤çš„å€¼ï¼ˆä»¥ä½¿ç”¨Bellmanæ–¹ç¨‹è¿›è¡Œä¼°è®¡ï¼‰ã€‚æˆ‘ä»¬è¿˜ä»¥åå‘é¡ºåºéå†è½¨è¿¹ï¼Œä»¥ä¾¿åœ¨ä¸€æ­¥ä¸­è®¡ç®—æ›´è¿‘æœŸçš„ä¼˜åŠ¿å€¼ã€‚
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In every step, our action depends on the done_trunc flag for this step. If this
    is the terminal step of the episode, we have no prior reward to take into account.
    (Remember, weâ€™re processing the trajectory in reverse order.) So, our value of
    delta in this step is just the immediate reward minus the value predicted for
    the step. If the current step is not terminal, delta will be equal to the immediate
    reward plus the discounted value from the subsequent step, minus the value for
    the current step. In the classic A2C method, this delta was used as an advantage
    estimation, but here, the smoothed version is used, so the advantage estimation
    (tracked in the last_gae variable) is calculated as the sum of deltas with the
    discount factor Î³^Î».
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬çš„è¡ŒåŠ¨ä¾èµ–äºè¯¥æ­¥éª¤çš„done_truncæ ‡å¿—ã€‚å¦‚æœè¿™æ˜¯å›åˆçš„ç»ˆæ­¢æ­¥éª¤ï¼Œæˆ‘ä»¬ä¸éœ€è¦è€ƒè™‘ä¹‹å‰çš„å¥–åŠ±ã€‚ï¼ˆè®°ä½ï¼Œæˆ‘ä»¬æ˜¯ä»¥åå‘é¡ºåºå¤„ç†è½¨è¿¹çš„ã€‚ï¼‰å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¯¥æ­¥éª¤çš„deltaå€¼åªæ˜¯å³æ—¶å¥–åŠ±å‡å»è¯¥æ­¥éª¤é¢„æµ‹çš„å€¼ã€‚å¦‚æœå½“å‰æ­¥éª¤ä¸æ˜¯ç»ˆæ­¢æ­¥éª¤ï¼Œdeltaå°†ç­‰äºå³æ—¶å¥–åŠ±åŠ ä¸Šåç»­æ­¥éª¤çš„æŠ˜æ‰£å€¼ï¼Œå†å‡å»å½“å‰æ­¥éª¤çš„å€¼ã€‚åœ¨ç»å…¸çš„A2Cæ–¹æ³•ä¸­ï¼Œè¿™ä¸ªdeltaç”¨ä½œä¼˜åŠ¿ä¼°è®¡ï¼Œä½†è¿™é‡Œä½¿ç”¨çš„æ˜¯å¹³æ»‘ç‰ˆæœ¬ï¼Œå› æ­¤ä¼˜åŠ¿ä¼°è®¡ï¼ˆåœ¨last_gaeå˜é‡ä¸­è·Ÿè¸ªï¼‰æ˜¯é€šè¿‡æŠ˜æ‰£å› å­Î³^Î»è®¡ç®—çš„æ‰€æœ‰deltaçš„æ€»å’Œã€‚
- en: 'The goal of the function is to calculate advantages and reference values for
    the critic, so we save them in lists:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°çš„ç›®æ ‡æ˜¯ä¸ºè¯„è®ºè€…è®¡ç®—ä¼˜åŠ¿å€¼å’Œå‚è€ƒå€¼ï¼Œå› æ­¤æˆ‘ä»¬å°†å®ƒä»¬ä¿å­˜åœ¨åˆ—è¡¨ä¸­ï¼š
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At the end of the function, we convert values to tensors and return them:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‡½æ•°çš„æœ€åï¼Œæˆ‘ä»¬å°†å€¼è½¬æ¢ä¸ºå¼ é‡å¹¶è¿”å›ï¼š
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the training loop, we gather a trajectory of the desired size using the
    ExperienceSource(steps_count=1) class from the PTAN library. This configuration
    provides us with individual steps from the environment in Experience dataclass
    instances, containing the state, action, reward, and termination flag. The following
    is the relevant part of the training loop:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨PTANåº“ä¸­çš„ExperienceSource(steps_count=1)ç±»æ”¶é›†æ‰€éœ€å¤§å°çš„è½¨è¿¹ã€‚æ­¤é…ç½®æä¾›æ¥è‡ªç¯å¢ƒçš„å•ä¸ªæ­¥éª¤ï¼Œå­˜å‚¨åœ¨Experienceæ•°æ®ç±»å®ä¾‹ä¸­ï¼Œå…¶ä¸­åŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œç»ˆæ­¢æ ‡å¿—ã€‚ä»¥ä¸‹æ˜¯è®­ç»ƒå¾ªç¯ä¸­çš„ç›¸å…³éƒ¨åˆ†ï¼š
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When weâ€™ve got a trajectory thatâ€™s large enough for training (which is given
    by the TRAJECTORY_SIZE hyperparameter), we convert states and actions taken into
    tensors and use the already-described function to obtain advantages and reference
    values. Although our trajectory is quite long, the observations of our environments
    are small enough, so itâ€™s fine to process our batch in one step. In the case of
    Atari frames, such a batch could cause a GPU memory error. In the next step, we
    calculate the logarithm of the probability of the actions taken. This value will
    be used as Ï€[ğœƒ[old]] in the objective of PPO. Additionally, we normalize the advantageâ€™s
    mean and variance to improve the training stability:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æ‹¥æœ‰è¶³å¤Ÿå¤§çš„è½¨è¿¹ç”¨äºè®­ç»ƒæ—¶ï¼ˆç”± TRAJECTORY_SIZE è¶…å‚æ•°ç»™å‡ºï¼‰ï¼Œæˆ‘ä»¬å°†çŠ¶æ€å’Œæ‰€é‡‡å–çš„åŠ¨ä½œè½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶ä½¿ç”¨å·²ç»æè¿°çš„å‡½æ•°æ¥è·å–ä¼˜åŠ¿å’Œå‚è€ƒå€¼ã€‚å°½ç®¡æˆ‘ä»¬çš„è½¨è¿¹ç›¸å½“é•¿ï¼Œä½†æˆ‘ä»¬çš„ç¯å¢ƒè§‚å¯Ÿè¶³å¤Ÿå°ï¼Œå› æ­¤å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†æˆ‘ä»¬çš„æ‰¹æ¬¡ã€‚åœ¨
    Atari å¸§çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå¯¼è‡´ GPU å†…å­˜é”™è¯¯ã€‚æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æ‰€é‡‡å–åŠ¨ä½œçš„æ¦‚ç‡çš„å¯¹æ•°ã€‚è¿™ä¸ªå€¼å°†ä½œä¸ºç›®æ ‡å‡½æ•°ä¸­ PPO çš„ Ï€[ğœƒ[old]]ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ä¼˜åŠ¿çš„å‡å€¼å’Œæ–¹å·®å½’ä¸€åŒ–ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼š
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The two subsequent lines drop the last entry from the trajectory to reflect
    the fact that our advantages and reference values are one step shorter than the
    trajectory length (as we shifted values in the loop inside the calc_adv_ref function):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„ä¸¤è¡Œåˆ é™¤äº†è½¨è¿¹ä¸­çš„æœ€åä¸€ä¸ªæ¡ç›®ï¼Œä»¥åæ˜ æˆ‘ä»¬çš„ä¼˜åŠ¿å’Œå‚è€ƒå€¼æ¯”è½¨è¿¹é•¿åº¦å°‘ä¸€æ­¥çš„äº‹å®ï¼ˆå› ä¸ºæˆ‘ä»¬åœ¨ `calc_adv_ref` å‡½æ•°å†…éƒ¨çš„å¾ªç¯ä¸­ç§»åŠ¨äº†å€¼ï¼‰ï¼š
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When all the preparations have been done, we perform several epochs of training
    on our trajectory. For every batch, we extract the portions from the corresponding
    arrays and do the critic and the actor training separately:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‰€æœ‰å‡†å¤‡å·¥ä½œå®Œæˆåï¼Œæˆ‘ä»¬å¯¹è½¨è¿¹è¿›è¡Œå¤šä¸ªè®­ç»ƒå‘¨æœŸã€‚å¯¹äºæ¯ä¸ªæ‰¹æ¬¡ï¼Œæˆ‘ä»¬ä»ç›¸åº”çš„æ•°ç»„ä¸­æå–éƒ¨åˆ†æ•°æ®ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œè¯„è®ºå‘˜å’Œæ¼”å‘˜è®­ç»ƒï¼š
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To train the critic, all we need to do is calculate the mean squared error
    (MSE) loss with the reference values calculated beforehand:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®­ç»ƒè¯„è®ºå‘˜ï¼Œæˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯è®¡ç®—ä¹‹å‰è®¡ç®—å¥½çš„å‚è€ƒå€¼çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ï¼š
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the actor training, we minimize the negated clipped objective:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¼”å‘˜è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬æœ€å°åŒ–äº†è´Ÿå‰ªåˆ‡ç›®æ ‡ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq61.png) ![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq62.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s]](img/eq61.png) ![Ï€ (a |s) = P[At = a|St = s]](img/eq62.png)'
- en: 'To achieve this, we use the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Results
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: After being trained in both our test environments, the PPO method has shown
    much faster convergence than the A2C method. On HalfCheetah using PyBullet, PPO
    reached an average training reward of 1,800 and 2,500 during the testing after
    8 hours of training and 25M training steps. A2C got lower results after 110M steps
    and 20 hours. FigureÂ [16.6](#x1-298002r6) shows the comparison plots.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ä¸¤ä¸ªæµ‹è¯•ç¯å¢ƒä¸­è®­ç»ƒåï¼ŒPPO æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äº A2C æ–¹æ³•ã€‚åœ¨ PyBullet ä¸Šä½¿ç”¨ HalfCheetah æ—¶ï¼ŒPPO åœ¨ 8 å°æ—¶çš„è®­ç»ƒå’Œ
    25M è®­ç»ƒæ­¥åï¼Œè¾¾åˆ°äº† 1,800 çš„å¹³å‡è®­ç»ƒå¥–åŠ±å’Œ 2,500 çš„æµ‹è¯•å¥–åŠ±ã€‚A2C åœ¨ 110M æ­¥å’Œ 20 å°æ—¶åå¾—åˆ°äº†è¾ƒä½çš„ç»“æœã€‚å›¾ [16.6](#x1-298002r6)
    æ˜¾ç¤ºäº†æ¯”è¾ƒå›¾è¡¨ã€‚
- en: '![PIC](img/B22150_16_06.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_06.png)'
- en: 'FigureÂ 16.6: The reward during training (left) and the test reward (right)
    for HalfCheetah on PyBullet'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 16.6ï¼šåœ¨ PyBullet ä¸Šè®­ç»ƒæœŸé—´çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰å¯¹äº HalfCheetah
- en: But on HalfCheetah using MuJoCo, the situation is the opposite â€“ PPO growth
    was much slower, and I stopped it after 50M training steps (12 hours). FigureÂ [16.7](#x1-298003r7)
    shows the plots.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨ä½¿ç”¨ MuJoCo çš„ HalfCheetah ä¸Šï¼Œæƒ…å†µæ­£å¥½ç›¸åâ€”â€”PPO çš„å¢é•¿é€Ÿåº¦è¦æ…¢å¾—å¤šï¼Œæˆ‘åœ¨ 50M è®­ç»ƒæ­¥ï¼ˆ12 å°æ—¶ï¼‰ååœæ­¢äº†è®­ç»ƒã€‚å›¾
    [16.7](#x1-298003r7) æ˜¾ç¤ºäº†è¿™äº›å›¾è¡¨ã€‚
- en: '![PIC](img/B22150_16_07.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_07.png)'
- en: 'FigureÂ 16.7: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 16.7ï¼šåœ¨ MuJoCo ä¸Šè®­ç»ƒæœŸé—´çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰å¯¹äº HalfCheetah
- en: After checking the video of the model (links are provided later in this section),
    we might guess the reason for the low score â€“ our agent learned how to flip the
    cheetah on its back and move forward in this position. During training, it wasnâ€™t
    able to get from this suboptimal â€œlocal maximum.â€ Most likely, running the training
    several times might yield a better policy. Another approach to solving this might
    be to optimize hyperparameters. Again, this is something you can try experimenting
    with.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸ¥çœ‹äº†æ¨¡å‹çš„è§†é¢‘åï¼ˆç¨åä¼šæä¾›é“¾æ¥ï¼‰ï¼Œæˆ‘ä»¬å¯èƒ½çŒœæµ‹ä½åˆ†çš„åŸå› â€”â€”æˆ‘ä»¬çš„æ™ºèƒ½ä½“å­¦ä¼šäº†å¦‚ä½•å°†çŒè±¹ç¿»è½¬åˆ°èƒŒéƒ¨å¹¶åœ¨è¿™ä¸ªä½ç½®å‰è¿›ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒæ— æ³•ä»è¿™ä¸ªæ¬¡ä¼˜çš„â€œå±€éƒ¨æœ€å¤§å€¼â€ä¸­è„±ç¦»å‡ºæ¥ã€‚å¾ˆå¯èƒ½å¤šæ¬¡è¿è¡Œè®­ç»ƒä¼šå¾—åˆ°æ›´å¥½çš„ç­–ç•¥ã€‚å¦ä¸€ç§è§£å†³æ–¹æ³•å¯èƒ½æ˜¯ä¼˜åŒ–è¶…å‚æ•°ã€‚åŒæ ·ï¼Œè¿™ä¹Ÿæ˜¯ä½ å¯ä»¥å°è¯•å®éªŒçš„å†…å®¹ã€‚
- en: 'In the Ant environment, PPO was better on both PyBullet and MuJoco and was
    able to reach the same level of reward almost twice as fast as A2C. This comparison
    is shown in the plots in FigureÂ [16.8](#x1-298005r8) and FigureÂ [16.9](#x1-298006r9):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Ant ç¯å¢ƒä¸­ï¼ŒPPO åœ¨ PyBullet å’Œ MuJoCo ä¸Šçš„è¡¨ç°éƒ½ä¼˜äº A2Cï¼Œå¹¶ä¸”èƒ½å¤Ÿå‡ ä¹æ˜¯ A2C çš„ä¸¤å€é€Ÿåº¦è¾¾åˆ°ç›¸åŒçš„å¥–åŠ±æ°´å¹³ã€‚è¿™ä¸ªå¯¹æ¯”å±•ç¤ºåœ¨å›¾
    [16.8](#x1-298005r8) å’Œå›¾ [16.9](#x1-298006r9) ä¸­ï¼š
- en: '![PIC](img/B22150_16_08.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_08.png)'
- en: 'FigureÂ 16.8: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 16.8ï¼šPyBullet ä¸Š Ant è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_16_09.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_09.png)'
- en: 'FigureÂ 16.9: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 16.9ï¼šMuJoCo ä¸Š Ant è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: 'As before, you can use the 02_play.py utility to benchmark saved models and
    record videos of the learned policy in action. This is the list of the best models
    for my training experiments:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¹‹å‰æ‰€è¿°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 02_play.py å·¥å…·æ¥åŸºå‡†æµ‹è¯•ä¿å­˜çš„æ¨¡å‹ï¼Œå¹¶å½•åˆ¶å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨å®é™…ä¸­çš„è¡¨ç°ã€‚è¿™æ˜¯æˆ‘è®­ç»ƒå®éªŒä¸­æœ€ä½³æ¨¡å‹çš„åˆ—è¡¨ï¼š
- en: 'HalfCheetah on PyBullet (score 2,567): [https://youtu.be/Rai-smyfyeE](https://youtu.be/Rai-smyfyeE).
    The agent learned how to do long jumps with the back leg.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBullet ä¸Šçš„ HalfCheetahï¼ˆå¾—åˆ† 2,567ï¼‰ï¼š[https://youtu.be/Rai-smyfyeE](https://youtu.be/Rai-smyfyeE)ã€‚ä»£ç†å­¦ä¼šäº†å¦‚ä½•ç”¨åè…¿åšè¿œè·³ã€‚
- en: 'HalfCheetah on MuJoCo (score 1,623): [https://youtube.com/shorts/VcyzNtbVzd4](https://youtube.com/shorts/VcyzNtbVzd4).
    Quite a funny video: the cheetah flips on its back and moves forward this way.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCo ä¸Šçš„ HalfCheetahï¼ˆå¾—åˆ† 1,623ï¼‰ï¼š[https://youtube.com/shorts/VcyzNtbVzd4](https://youtube.com/shorts/VcyzNtbVzd4)ã€‚è¿™æ˜¯ä¸€æ®µéå¸¸æœ‰è¶£çš„è§†é¢‘ï¼šçŒè±¹ç¿»èº«å¹¶ä»¥è¿™ç§æ–¹å¼å‘å‰ç§»åŠ¨ã€‚
- en: 'Ant on PyBullet (score 2,560): [https://youtu.be/8lty_Mdjnfs](https://youtu.be/8lty_Mdjnfs).
    The Ant policy is much better than A2C â€“ it steadily moves forward.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBullet ä¸Šçš„ Antï¼ˆå¾—åˆ† 2,560ï¼‰ï¼š[https://youtu.be/8lty_Mdjnfs](https://youtu.be/8lty_Mdjnfs)ã€‚Ant
    ç­–ç•¥æ¯” A2C å¥½å¾—å¤šâ€”â€”å®ƒèƒ½ç¨³å®šåœ°å‘å‰ç§»åŠ¨ã€‚
- en: 'Ant on MuJoCo (score 5,108): [https://youtube.com/shorts/AcXxH2f_KWs](https://youtube.com/shorts/AcXxH2f_KWs).
    This model is much faster; most likely, the weight of the ant in the MuJoCo model
    is lower than in PyBullet.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCo ä¸Šçš„ Antï¼ˆå¾—åˆ† 5,108ï¼‰ï¼š[https://youtube.com/shorts/AcXxH2f_KWs](https://youtube.com/shorts/AcXxH2f_KWs)ã€‚è¿™ä¸ªæ¨¡å‹æ›´å¿«ï¼›å¾ˆå¯èƒ½ï¼ŒMuJoCo
    æ¨¡å‹ä¸­çš„èš‚èšé‡é‡ä½äº PyBullet æ¨¡å‹ä¸­çš„èš‚èšã€‚
- en: TRPO
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO
- en: TRPO was proposed in 2015 by Berkeley researchers in a paper by Schulman et
    al., called Trust region policy optimization [[Sch15](#)]. This paper was a step
    towards improving the stability and consistency of stochastic policy gradient
    optimization and has shown good results on various control tasks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO ç”±ä¼¯å…‹åˆ©ç ”ç©¶äººå‘˜äº 2015 å¹´åœ¨ Schulman ç­‰äººçš„è®ºæ–‡ã€Šä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–ã€‹ï¼ˆTrust region policy optimizationï¼‰ä¸­æå‡º[[Sch15](#)]ã€‚è¿™ç¯‡è®ºæ–‡æ˜¯æå‡éšæœºç­–ç•¥æ¢¯åº¦ä¼˜åŒ–çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§çš„ä¸€ä¸ªæ­¥éª¤ï¼Œå¹¶åœ¨å„ç§æ§åˆ¶ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚
- en: Unfortunately, the paper and the method are quite math-heavy, so it can be hard
    to understand the details. The same could be said about the implementation, which
    uses the conjugate gradients method to efficiently solve the constrained optimization
    problem.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œè®ºæ–‡å’Œæ–¹æ³•ç›¸å½“æ•°å­¦åŒ–ï¼Œå› æ­¤ç†è§£ç»†èŠ‚å¯èƒ½æ¯”è¾ƒå›°éš¾ã€‚å®ç°éƒ¨åˆ†ä¹Ÿå­˜åœ¨åŒæ ·çš„é—®é¢˜ï¼Œå®ƒä½¿ç”¨å…±è½­æ¢¯åº¦æ³•æ¥é«˜æ•ˆåœ°è§£å†³çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚
- en: 'As the first step, the TRPO method defines the discounted visitation frequencies
    of the state as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬ä¸€æ­¥ï¼ŒTRPO æ–¹æ³•å°†çŠ¶æ€çš„æŠ˜æ‰£è®¿é—®é¢‘ç‡å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq63.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq63.png)'
- en: In this equation, P(s[i] = s) equals the sampled probability of state s to be
    met at position i of the sampled trajectories.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ–¹ç¨‹ä¸­ï¼ŒP(s[i] = s) ç­‰äºåœ¨é‡‡æ ·è½¨è¿¹çš„ç¬¬ i ä¸ªä½ç½®ä¸Šé‡åˆ°çŠ¶æ€ s çš„é‡‡æ ·æ¦‚ç‡ã€‚
- en: Then, TRPO defines the optimization objective as
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼ŒTRPO å°†ä¼˜åŒ–ç›®æ ‡å®šä¹‰ä¸º
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq64.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq64.png)'
- en: where
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq65.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq65.png)'
- en: is the expected discounted reward of the policy and Ï€Ìƒ = arg max[a]A[Ï€](s,a)
    defines the deterministic policy. To address the issue of large policy updates,
    TRPO defines the additional constraint on the policy update, which is expressed
    as the maximum KL divergence between the old and the new policies, which could
    be written as
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯ç­–ç•¥çš„æœŸæœ›æŠ˜æ‰£å¥–åŠ±ï¼ŒÏ€Ìƒ = arg max[a]A[Ï€](s,a) å®šä¹‰äº†ç¡®å®šæ€§ç­–ç•¥ã€‚ä¸ºäº†è§£å†³å¤§è§„æ¨¡ç­–ç•¥æ›´æ–°çš„é—®é¢˜ï¼ŒTRPO å¯¹ç­–ç•¥æ›´æ–°å®šä¹‰äº†é¢å¤–çš„çº¦æŸï¼Œè¯¥çº¦æŸè¡¨ç¤ºä¸ºæ—§ç­–ç•¥ä¸æ–°ç­–ç•¥ä¹‹é—´çš„æœ€å¤§
    KL æ•£åº¦ï¼Œå½¢å¼å¦‚ä¸‹ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq66.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq66.png)'
- en: 'As a reminder, KL divergence measures the similarity between probability distributions
    and is calculated as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼ŒKL æ•£åº¦è¡¡é‡çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq67.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq67.png)'
- en: We met KL divergence in ChapterÂ [4](ch008.xhtml#x1-740004) and ChapterÂ [11](ch015.xhtml#x1-18200011).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ç¬¬[4](ch008.xhtml#x1-740004)ç« å’Œç¬¬[11](ch015.xhtml#x1-18200011)ç« ä¸­é‡åˆ°è¿‡KLæ•£åº¦ã€‚
- en: Implementation
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'Most of the TRPO implementations available on GitHub, or in other open source
    repositories, are very similar to each other, probably because all of them grew
    from the original John Schulman TRPO implementation here: [https://github.com/joschu/modular_rl](https://github.com/joschu/modular_rl).
    My version of TRPO is also not very different and uses the core functions that
    implement the conjugate gradient method (used by TRPO to solve the constrained
    optimization problem) from this repository: [https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: GitHubæˆ–å…¶ä»–å¼€æºä»£ç åº“ä¸­å¤§å¤šæ•°å¯ç”¨çš„TRPOå®ç°éƒ½éå¸¸ç›¸ä¼¼ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå®ƒä»¬éƒ½æºè‡ªæœ€åˆçš„John Schulman TRPOå®ç°ï¼š[https://github.com/joschu/modular_rl](https://github.com/joschu/modular_rl)ã€‚æˆ‘ç‰ˆæœ¬çš„TRPOä¹Ÿæ²¡æœ‰å¤ªå¤§ä¸åŒï¼Œä½¿ç”¨äº†è¯¥ä»£ç åº“ä¸­çš„æ ¸å¿ƒå‡½æ•°ï¼Œè¿™äº›å‡½æ•°å®ç°äº†å…±è½­æ¢¯åº¦æ–¹æ³•ï¼ˆTRPOç”¨äºè§£å†³çº¦æŸä¼˜åŒ–é—®é¢˜ï¼‰ï¼š[https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo)ã€‚
- en: 'The complete example is in 03_train_trpo.py and lib/trpo.py, and the training
    loop is very similar to the PPO example: we sample the trajectory of transitions
    of the predefined length and calculate the advantage estimation using the smoothed
    formula discussed in the PPO section (historically, this estimator was proposed
    first in the TRPO paper.) Next, we do one training step of the critic using MSE
    loss with the calculated reference value, and one step of the TRPO update, which
    consists of finding the direction we should go in by using the conjugate gradients
    method and doing a linear search in this direction to find a step that preserves
    the desired KL divergence.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„ä¾‹å­å¯ä»¥åœ¨03_train_trpo.pyå’Œlib/trpo.pyä¸­æ‰¾åˆ°ï¼Œè®­ç»ƒå¾ªç¯ä¸PPOçš„ç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼šæˆ‘ä»¬é‡‡æ ·é¢„å®šä¹‰é•¿åº¦çš„è½¨è¿¹è½¬ç§»ï¼Œå¹¶ä½¿ç”¨PPOéƒ¨åˆ†è®¨è®ºçš„å¹³æ»‘å…¬å¼è®¡ç®—ä¼˜åŠ¿ä¼°è®¡ï¼ˆå†å²ä¸Šï¼Œè¿™ä¸ªä¼°è®¡æœ€æ—©æ˜¯åœ¨TRPOè®ºæ–‡ä¸­æå‡ºçš„ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç”¨è®¡ç®—å‡ºçš„å‚è€ƒå€¼è¿›è¡Œä¸€æ¬¡ä½¿ç”¨MSEæŸå¤±çš„è¯„è®ºè€…è®­ç»ƒæ­¥éª¤ï¼Œå¹¶æ‰§è¡Œä¸€æ¬¡TRPOæ›´æ–°æ­¥éª¤ï¼Œè¿™ä¸€æ­¥éª¤åŒ…æ‹¬ä½¿ç”¨å…±è½­æ¢¯åº¦æ–¹æ³•æ‰¾åˆ°æˆ‘ä»¬åº”èµ°çš„æ–¹å‘ï¼Œå¹¶åœ¨è¯¥æ–¹å‘ä¸Šè¿›è¡Œçº¿æ€§æœç´¢ï¼Œæ‰¾åˆ°ä¸€ä¸ªä¿æŒæ‰€éœ€KLæ•£åº¦çš„æ­¥é•¿ã€‚
- en: 'The following is the piece of the training loop that carries out both those
    steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ‰§è¡Œè¿™ä¸¤ä¸ªæ­¥éª¤çš„è®­ç»ƒå¾ªç¯éƒ¨åˆ†ï¼š
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To perform the TRPO step, we need to provide two functions: the first will
    calculate the loss of the current actor policy, which uses the same ratio as the
    PPO of the new and the old policies multiplied by the advantage estimation. The
    second function has to calculate KL divergence between the old and the current
    policy:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰§è¡ŒTRPOæ­¥éª¤ï¼Œæˆ‘ä»¬éœ€è¦æä¾›ä¸¤ä¸ªå‡½æ•°ï¼šç¬¬ä¸€ä¸ªå‡½æ•°è®¡ç®—å½“å‰æ¼”å‘˜ç­–ç•¥çš„æŸå¤±ï¼Œè¿™ä¸ªæŸå¤±ä½¿ç”¨æ–°çš„å’Œæ—§çš„ç­–ç•¥ä¹‹é—´çš„æ¯”ä¾‹ï¼Œä¹˜ä»¥ä¼˜åŠ¿ä¼°è®¡ã€‚ç¬¬äºŒä¸ªå‡½æ•°åˆ™è®¡ç®—æ—§ç­–ç•¥å’Œå½“å‰ç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦ï¼š
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In other words, the PPO method is TRPO that uses the simple clipping of the
    policy ratio to limit the policy update, instead of the complicated conjugate
    gradients and line search.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒPPOæ–¹æ³•å®é™…ä¸Šå°±æ˜¯TRPOï¼Œå®ƒä½¿ç”¨ç®€å•çš„ç­–ç•¥æ¯”ä¾‹å‰ªåˆ‡æ¥é™åˆ¶ç­–ç•¥æ›´æ–°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤æ‚çš„å…±è½­æ¢¯åº¦å’Œçº¿æ€§æœç´¢ã€‚
- en: Results
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'TRPO in the HalfCheetah environment was able to reach better rewards than PPO
    and A2C. In FigureÂ [16.10](#x1-301002r10), the results from PyBullet training
    is shown. On MuJoCo, the results are even more impressive â€“ the best reward was
    over 5,000\. The plots for MuJoCo are shown in FigureÂ [16.11](#x1-301003r11):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨HalfCheetahç¯å¢ƒä¸­ï¼ŒTRPOèƒ½å¤Ÿè·å¾—æ¯”PPOå’ŒA2Cæ›´å¥½çš„å¥–åŠ±ã€‚åœ¨å›¾[16.10](#x1-301002r10)ä¸­ï¼Œå±•ç¤ºäº†PyBulletè®­ç»ƒçš„ç»“æœã€‚åœ¨MuJoCoä¸Šï¼Œç»“æœæ›´åŠ ä»¤äººå°è±¡æ·±åˆ»â€”â€”æœ€ä½³å¥–åŠ±è¶…è¿‡äº†5000ã€‚MuJoCoçš„å›¾ç¤ºè§å›¾[16.11](#x1-301003r11)ï¼š
- en: '![PIC](img/B22150_16_10.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_10.png)'
- en: 'FigureÂ 16.10: The reward during training (left) and test reward (right) for
    HalfCheetah on PyBullet'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.10ï¼šPyBulletç¯å¢ƒä¸­HalfCheetahçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_16_11.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_11.png)'
- en: 'FigureÂ 16.11: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.11ï¼šMuJoCoç¯å¢ƒä¸­HalfCheetahçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: 'Unfortunately, the Ant environment shows much less stable convergence. The
    plots shown in FigureÂ [16.12](#x1-301005r12) and FigureÂ [16.13](#x1-301006r13)
    compare the train and test rewards on A2C and TRPO:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼ŒAntç¯å¢ƒæ˜¾ç¤ºå‡ºè¿œä¸å¦‚é¢„æœŸçš„ç¨³å®šæ”¶æ•›æ€§ã€‚å›¾[16.12](#x1-301005r12)å’Œå›¾[16.13](#x1-301006r13)æ¯”è¾ƒäº†A2Cå’ŒTRPOåœ¨è®­ç»ƒå’Œæµ‹è¯•å¥–åŠ±ä¸Šçš„è¡¨ç°ï¼š
- en: '![PIC](img/B22150_16_12.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_12.png)'
- en: 'FigureÂ 16.12: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.12ï¼šPyBulletç¯å¢ƒä¸­Antçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_16_13.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_13.png)'
- en: 'FigureÂ 16.13: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.13ï¼šMuJoCoç¯å¢ƒä¸­Antçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: 'Video recordings of the best actions could be done in the same way as before.
    Here are some videos for the best TRPO models:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³åŠ¨ä½œçš„è§†é¢‘è®°å½•å¯ä»¥åƒä»¥å‰ä¸€æ ·è¿›è¡Œã€‚è¿™é‡Œæœ‰ä¸€äº›æœ€ä½³TRPOæ¨¡å‹çš„è§†é¢‘ï¼š
- en: 'HalfCheetah on PyBullet (score 2,419): [https://youtu.be/NIfkt2lVT74](https://youtu.be/NIfkt2lVT74).
    Front leg joints are not used.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBulletä¸­çš„åŠè±¹ï¼ˆå¾—åˆ†2,419ï¼‰ï¼š[https://youtu.be/NIfkt2lVT74](https://youtu.be/NIfkt2lVT74)ã€‚å‰è…¿å…³èŠ‚æ²¡æœ‰ä½¿ç”¨ã€‚
- en: 'HalfCheetah on MuJoCo (score 5,753): [https://youtube.com/shorts/FLM2t-XWDLc?feature=share](https://youtube.com/shorts/FLM2t-XWDLc?feature=share).
    This is a really fast Cheetah!'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCoä¸­çš„åŠè±¹ï¼ˆå¾—åˆ†5,753ï¼‰ï¼š[https://youtube.com/shorts/FLM2t-XWDLc?feature=share](https://youtube.com/shorts/FLM2t-XWDLc?feature=share)ã€‚è¿™çœŸæ˜¯ä¸€åªé£å¿«çš„è±¹å­ï¼
- en: 'Ant on PyBullet (score 834): [https://youtu.be/Ny1WBPVluNQ](https://youtu.be/Ny1WBPVluNQ).
    The training got stuck in a â€œstand stillâ€ local minimum.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBulletä¸­çš„èš‚èšï¼ˆå¾—åˆ†834ï¼‰ï¼š[https://youtu.be/Ny1WBPVluNQ](https://youtu.be/Ny1WBPVluNQ)ã€‚è®­ç»ƒå¡åœ¨äº†ä¸€ä¸ªâ€œé™æ­¢ä¸åŠ¨â€çš„å±€éƒ¨æœ€å°å€¼ã€‚
- en: 'Ant on MuJoCo (score 993): [https://youtube.com/shorts/9sybZGvXQFs](https://youtube.com/shorts/9sybZGvXQFs).
    The same as PyBullet â€“ the agent just stands still and does not move anywhere.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCoä¸­çš„èš‚èšï¼ˆå¾—åˆ†993ï¼‰ï¼š[https://youtube.com/shorts/9sybZGvXQFs](https://youtube.com/shorts/9sybZGvXQFs)ã€‚ä¸PyBulletç›¸åŒâ€”â€”æ™ºèƒ½ä½“åªæ˜¯ç«™ç€ä¸åŠ¨ï¼Œå“ªé‡Œä¹Ÿä¸å»ã€‚
- en: ACKTR
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ACKTR
- en: The third method that we will compare, ACKTR, uses a different approach to address
    SGD stability. In the paper by Wu et al. called Scalable trust-region method for
    deep reinforcement learning using Kronecker-factored approximation, published
    in 2017 [[Wu+17](#)], the authors combined the second-order optimization methods
    and trust region approach.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¯”è¾ƒçš„ç¬¬ä¸‰ç§æ–¹æ³•ACKTRï¼Œé‡‡ç”¨äº†ä¸åŒçš„æ–¹å¼æ¥è§£å†³SGDç¨³å®šæ€§é—®é¢˜ã€‚åœ¨å´ç­‰äººäº2017å¹´å‘è¡¨çš„è®ºæ–‡ã€Šä¸€ç§ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•ä¿¡ä»»åŸŸæ–¹æ³•ï¼ŒåŸºäºå…‹ç½—å†…å…‹è¿‘ä¼¼ã€‹ï¼ˆScalable
    trust-region method for deep reinforcement learning using Kronecker-factored approximationï¼‰[[Wu+17](#)]ä¸­ï¼Œä½œè€…ç»“åˆäº†äºŒé˜¶ä¼˜åŒ–æ–¹æ³•å’Œä¿¡ä»»åŸŸæ–¹æ³•ã€‚
- en: The idea of the second-order methods is to improve the traditional SGD by taking
    the second-order derivatives of the optimized function (in other words, its curvature)
    to improve the convergence of the optimization process. To make things more complicated,
    working with the second-order derivatives usually requires you to build and invert
    a Hessian matrix, which can be prohibitively large, so the practical methods typically
    approximate it in some way. This area is currently very active in research because
    developing robust, scalable optimization methods is very important for the whole
    machine learning domain.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒé˜¶æ–¹æ³•çš„æ€æƒ³æ˜¯é€šè¿‡å¯¹ä¼˜åŒ–å‡½æ•°è¿›è¡ŒäºŒé˜¶å¯¼æ•°ï¼ˆæ¢å¥è¯è¯´ï¼Œå°±æ˜¯å®ƒçš„æ›²ç‡ï¼‰çš„è®¡ç®—ï¼Œæ¥æ”¹è¿›ä¼ ç»Ÿçš„SGDï¼Œä»è€Œæé«˜ä¼˜åŒ–è¿‡ç¨‹çš„æ”¶æ•›æ€§ã€‚ä¸ºäº†è®©äº‹æƒ…æ›´å¤æ‚ï¼Œå¤„ç†äºŒé˜¶å¯¼æ•°é€šå¸¸éœ€è¦æ„å»ºå¹¶åè½¬ä¸€ä¸ªHessiançŸ©é˜µï¼Œè€Œè¿™ä¸ªçŸ©é˜µå¯èƒ½éå¸¸åºå¤§ï¼Œå› æ­¤å®é™…çš„æ–¹æ³•é€šå¸¸ä¼šä»¥æŸç§æ–¹å¼å¯¹å…¶è¿›è¡Œè¿‘ä¼¼ã€‚è¿™ä¸ªé¢†åŸŸç›®å‰åœ¨ç ”ç©¶ä¸­éå¸¸æ´»è·ƒï¼Œå› ä¸ºå¼€å‘ç¨³å¥ä¸”å¯æ‰©å±•çš„ä¼˜åŒ–æ–¹æ³•å¯¹äºæ•´ä¸ªæœºå™¨å­¦ä¹ é¢†åŸŸè‡³å…³é‡è¦ã€‚
- en: One of the second-order methods is called Kronecker-factored approximate curvature
    (K-FAC), which was proposed by James Martens and Roger Grosse in their paper Optimizing
    neural networks with Kronecker-factored approximate curvature, published in 2015
    [[MG15](#)]. However, a detailed description of this method is well beyond the
    scope of this book.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§äºŒé˜¶æ–¹æ³•å«åšå…‹ç½—å†…å…‹è¿‘ä¼¼æ›²ç‡ï¼ˆK-FACï¼‰ï¼Œç”±James Martenså’ŒRoger Grosseåœ¨ä»–ä»¬2015å¹´å‘è¡¨çš„è®ºæ–‡ã€Šä½¿ç”¨å…‹ç½—å†…å…‹è¿‘ä¼¼æ›²ç‡ä¼˜åŒ–ç¥ç»ç½‘ç»œã€‹ï¼ˆOptimizing
    neural networks with Kronecker-factored approximate curvatureï¼‰[[MG15](#)]ä¸­æå‡ºã€‚ç„¶è€Œï¼Œè¯¦ç»†æè¿°è¿™ç§æ–¹æ³•è¿œè¿œè¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ã€‚
- en: Implementation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: There are not very many implementations of this method available, and none of
    them are part of PyTorch (unfortunately). As far as I know, there are two versions
    of the K-FAC optimizer that work with PyTorch; one from Ilya Kostrikov ([https://github.com/ikostrikov/pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr))
    and one from Nicholas Gao ([https://github.com/n-gao/pytorch-kfac](https://github.com/n-gao/pytorch-kfac)).
    Iâ€™ve experimented only with the first one; you can give the second one a try.
    There is a version of K-FAC available for TensorFlow, which comes with OpenAI
    Baselines, but porting and testing it on PyTorch can be difficult.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰è¿™ä¸ªæ–¹æ³•çš„å®ç°å¹¶ä¸å¤šï¼Œè€Œä¸”æ²¡æœ‰ä»»ä½•ä¸€ä¸ªæ˜¯PyTorchçš„å®˜æ–¹å®ç°ï¼ˆå¾ˆé—æ†¾ï¼‰ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œæœ‰ä¸¤ä¸ªç‰ˆæœ¬çš„K-FACä¼˜åŒ–å™¨å¯ä»¥ä¸PyTorchä¸€èµ·ä½¿ç”¨ï¼›ä¸€ä¸ªæ¥è‡ªIlya
    Kostrikovï¼ˆ[https://github.com/ikostrikov/pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)ï¼‰ï¼Œå¦ä¸€ä¸ªæ¥è‡ªNicholas
    Gaoï¼ˆ[https://github.com/n-gao/pytorch-kfac](https://github.com/n-gao/pytorch-kfac)ï¼‰ã€‚æˆ‘åªè¯•è¿‡ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼›ä½ å¯ä»¥å°è¯•ç¬¬äºŒä¸ªç‰ˆæœ¬ã€‚K-FACä¹Ÿæœ‰TensorFlowç‰ˆæœ¬ï¼ŒéšOpenAI
    Baselinesæä¾›ï¼Œä½†å°†å…¶ç§»æ¤å¹¶åœ¨PyTorchä¸Šæµ‹è¯•å¯èƒ½ä¼šæœ‰éš¾åº¦ã€‚
- en: For my experiments, Iâ€™ve taken the K-FAC implementation from Kostrikov and adapted
    it to the existing code, which required replacing the optimizer and doing an extra
    backward() call to gather Fisher information. The critic was trained in the same
    way as in A2C. The complete example is in 05_train_acktr.py and is not shown here,
    as itâ€™s basically the same as A2C. The only difference is that a different optimizer
    was used.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘çš„å®éªŒï¼Œæˆ‘é‡‡ç”¨äº†Kostrikovçš„K-FACå®ç°å¹¶å°†å…¶é€‚é…åˆ°ç°æœ‰ä»£ç ä¸­ï¼Œè¿™éœ€è¦æ›¿æ¢ä¼˜åŒ–å™¨å¹¶é¢å¤–è°ƒç”¨backward()æ¥æ”¶é›†Fisherä¿¡æ¯ã€‚è¯„è®ºå‘˜çš„è®­ç»ƒæ–¹å¼ä¸A2Cç›¸åŒã€‚å®Œæ•´çš„ç¤ºä¾‹ä»£ç ä½äº05_train_acktr.pyä¸­ï¼Œæœ¬æ–‡æœªå±•ç¤ºï¼Œå› ä¸ºå®ƒåŸºæœ¬ä¸Šä¸A2Cç›¸åŒã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯ä½¿ç”¨äº†ä¸åŒçš„ä¼˜åŒ–å™¨ã€‚
- en: Results
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: Overall, the ACKTR method was very unstable in both environments and physics
    engines. It could be due to a lack of fine-tuning of hyperparameters or some bugs
    in the implementation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“æ¥çœ‹ï¼ŒACKTRæ–¹æ³•åœ¨è¿™ä¸¤ç§ç¯å¢ƒå’Œç‰©ç†å¼•æ“ä¸­éƒ½éå¸¸ä¸ç¨³å®šã€‚è¿™å¯èƒ½æ˜¯ç”±äºè¶…å‚æ•°çš„è°ƒä¼˜ä¸è¶³ï¼Œæˆ–è€…å®ç°ä¸­å­˜åœ¨ä¸€äº›bugã€‚
- en: The results of experiments on HalfCheetah are shown in FigureÂ [16.14](#x1-304002r14)
    and FigureÂ [16.15](#x1-304003r15).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: HalfCheetahå®éªŒçš„ç»“æœå¦‚å›¾[16.14](#x1-304002r14)å’Œå›¾[16.15](#x1-304003r15)æ‰€ç¤ºã€‚
- en: '![PIC](img/B22150_16_14.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_14.png)'
- en: 'FigureÂ 16.14: The reward during training (left) and the test reward (right)
    for HalfCheetah on PyBullet'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.14ï¼šè®­ç»ƒæœŸé—´çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œåœ¨PyBulletä¸Šæµ‹è¯•çš„å¥–åŠ±ï¼ˆå³ï¼‰ï¼Œé’ˆå¯¹HalfCheetah
- en: '![PIC](img/B22150_16_15.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_15.png)'
- en: 'FigureÂ 16.15: The reward during training (left) and test reward (right) for
    HalfCheetah on MuJoCo'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.15ï¼šè®­ç»ƒæœŸé—´çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œåœ¨MuJoCoä¸Šæµ‹è¯•çš„å¥–åŠ±ï¼ˆå³ï¼‰ï¼Œé’ˆå¯¹HalfCheetah
- en: In the Ant environment, the ACKTR method shows bad results on PyBullet and no
    reward improvements compared to training on MuJoCo. FigureÂ [16.16](#x1-304005r16)
    shows plots for PyBullet.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Antç¯å¢ƒä¸­ï¼ŒACKTRæ–¹æ³•åœ¨PyBulletä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œä¸åœ¨MuJoCoä¸Šè®­ç»ƒç›¸æ¯”æ²¡æœ‰å¥–åŠ±æ”¹è¿›ã€‚å›¾[16.16](#x1-304005r16)å±•ç¤ºäº†PyBulletçš„å›¾è¡¨ã€‚
- en: '![PIC](img/B22150_16_16.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_16_16.png)'
- en: 'FigureÂ 16.16: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.16ï¼šè®­ç»ƒæœŸé—´çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œåœ¨PyBulletä¸Šæµ‹è¯•çš„å¥–åŠ±ï¼ˆå³ï¼‰ï¼Œé’ˆå¯¹Ant
- en: SAC
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SAC
- en: 'In the final section, we will check our environments on a relatively new method
    called SAC, which was proposed by a group of Berkeley researchers and introduced
    in the paper Soft actor-critic: Off-policy maximum entropy deep reinforcement
    learning, by Haarnoja et al., published in 2018 [[Haa+18](#)].'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœ€åä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºSACï¼Œè¯¥æ–¹æ³•ç”±ä¼¯å…‹åˆ©ç ”ç©¶äººå‘˜æå‡ºï¼Œå¹¶åœ¨2018å¹´Haarnojaç­‰äººå‘å¸ƒçš„è®ºæ–‡ã€ŠSoft actor-critic:
    Off-policy maximum entropy deep reinforcement learningã€‹ä¸­ä»‹ç»[[Haa+18](#)]ã€‚'
- en: At the moment, itâ€™s considered to be one of the best methods for continuous
    control problems and is very widely used. The core idea of the method is closer
    to the DDPG method than to A2C policy gradients. We will compare it directly with
    PPOâ€™s performance, which has been considered to be the standard in continuous
    control problems for a long time.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯è§£å†³è¿ç»­æ§åˆ¶é—®é¢˜çš„æœ€ä½³æ–¹æ³•ä¹‹ä¸€ï¼Œå¹¶ä¸”å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ä¸DDPGæ–¹æ³•æ›´æ¥è¿‘ï¼Œè€Œä¸æ˜¯A2Cç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚æˆ‘ä»¬å°†ä¸PPOçš„è¡¨ç°è¿›è¡Œç›´æ¥æ¯”è¾ƒï¼ŒPPOé•¿æœŸä»¥æ¥è¢«è®¤ä¸ºæ˜¯è¿ç»­æ§åˆ¶é—®é¢˜çš„æ ‡å‡†æ–¹æ³•ã€‚
- en: 'The central idea of the SAC method is entropy regularization, which adds a
    bonus reward at each timestamp that is proportional to the entropy of the policy
    at this timestamp. In mathematical notation, the policy weâ€™re looking for is the
    following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: SACæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ç†µæ­£åˆ™åŒ–ï¼Œå®ƒåœ¨æ¯ä¸ªæ—¶é—´æˆ³ä¸Šæ·»åŠ ä¸€ä¸ªä¸è¯¥æ—¶é—´æˆ³ç­–ç•¥ç†µæˆæ­£æ¯”çš„å¥–åŠ±ã€‚ä»æ•°å­¦ç¬¦å·è¡¨ç¤ºï¼Œæˆ‘ä»¬è¦å¯»æ‰¾çš„ç­–ç•¥æ˜¯ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq68.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq68.png)'
- en: Here, H(P) = ğ”¼ [xâˆ¼P] [âˆ’log P(x)] is the entropy of distribution P. In other
    words, we give the agent a bonus for getting into situations where the entropy
    is at its maximum, which is very similar to the advanced exploration methods covered
    in ChapterÂ [18](ch022.xhtml#x1-32800018). In addition, the SAC method incorporates
    the clipped double-Q trick, where, in addition to the value function, we learn
    two networks predicting Q-values, and choose the minimum of them for Bellman approximation.
    According to researchers, this helps with dealing with Q-value overestimation
    during training. This problem was discussed in ChapterÂ [8](ch012.xhtml#x1-1240008),
    but was addressed differently.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼ŒH(P) = ğ”¼ [xâˆ¼P] [âˆ’log P(x)] æ˜¯åˆ†å¸ƒPçš„ç†µã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¥–åŠ±æ™ºèƒ½ä½“è¿›å…¥ç†µå€¼æœ€å¤§åŒ–çš„çŠ¶æ€ï¼Œç±»ä¼¼äºç¬¬[18](ch022.xhtml#x1-32800018)ç« ä¸­æåˆ°çš„é«˜çº§æ¢ç´¢æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSACæ–¹æ³•èåˆäº†å‰ªåˆ‡åŒQæŠ€å·§ï¼Œåœ¨æ­¤æŠ€å·§ä¸­ï¼Œé™¤äº†å€¼å‡½æ•°å¤–ï¼Œæˆ‘ä»¬è¿˜å­¦ä¹ ä¸¤ä¸ªé¢„æµ‹Qå€¼çš„ç½‘ç»œï¼Œå¹¶é€‰æ‹©å…¶ä¸­çš„æœ€å°å€¼è¿›è¡ŒBellmanè¿‘ä¼¼ã€‚ç ”ç©¶äººå‘˜è®¤ä¸ºï¼Œè¿™æœ‰åŠ©äºè§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­Qå€¼è¿‡åº¦ä¼°è®¡çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« ä¸­å·²è®¨è®ºè¿‡ï¼Œä½†é‡‡ç”¨äº†ä¸åŒçš„æ–¹æ³•è¿›è¡Œå¤„ç†ã€‚
- en: 'So, in total, we train four networks: the policy, Ï€(s), value, V (s,a) and
    two Q-networks, Q[1](s,a) and Q[2](s,a). For the value network, V (s,a), the target
    network is used. So, in summary, SAC training looks like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ€»å…±æˆ‘ä»¬è®­ç»ƒå››ä¸ªç½‘ç»œï¼šç­–ç•¥ç½‘ç»œÏ€(s)ã€å€¼ç½‘ç»œV(s,a)å’Œä¸¤ä¸ªQç½‘ç»œQ[1](s,a)ä¸Q[2](s,a)ã€‚å¯¹äºå€¼ç½‘ç»œV(s,a)ï¼Œä½¿ç”¨ç›®æ ‡ç½‘ç»œã€‚å› æ­¤ï¼ŒSACè®­ç»ƒæµç¨‹æ€»ç»“å¦‚ä¸‹ï¼š
- en: 'Q-networks are trained using the MSE objective by doing Bellman approximation
    using the target value network: y[q](r,sâ€²) = r+Î³V [tgt](sâ€²) (for non-terminating
    steps)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qç½‘ç»œé€šè¿‡ä½¿ç”¨ç›®æ ‡å€¼ç½‘ç»œè¿›è¡ŒBellmanè¿‘ä¼¼ï¼Œä½¿ç”¨MSEç›®æ ‡è¿›è¡Œè®­ç»ƒï¼šy[q](r,sâ€²) = r+Î³V [tgt](sâ€²)ï¼ˆå¯¹äºéç»ˆæ­¢æ­¥éª¤ï¼‰
- en: The V-network is trained using the MSE objective with the following target,
    y[v](s) = min[i=1,2]Q[i](s,Ã£) âˆ’Î± log Ï€[ğœƒ](Ã£|s), where Ã£ is sampled from policy
    Ï€[ğœƒ](â‹…|s)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vç½‘ç»œé€šè¿‡ä½¿ç”¨MSEç›®æ ‡å’Œä»¥ä¸‹ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼šy[v](s) = min[i=1,2]Q[i](s,Ã£) âˆ’Î± log Ï€[ğœƒ](Ã£|s)ï¼Œå…¶ä¸­Ã£æ˜¯ä»ç­–ç•¥Ï€[ğœƒ](â‹…|s)ä¸­é‡‡æ ·çš„
- en: The policy network, Ï€[ğœƒ], is trained in DDPG style by maximizing the following
    objective, Q[1](s,Ã£[ğœƒ](s)) âˆ’Î± log Ï€[ğœƒ](Ã£[ğœƒ](s)|s), where Ã£[ğœƒ] is a sample from
    Ï€[ğœƒ](â‹…|s)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥ç½‘ç»œÏ€[ğœƒ]é€šè¿‡æœ€å¤§åŒ–ä»¥ä¸‹ç›®æ ‡æ¥ä»¥DDPGé£æ ¼è¿›è¡Œè®­ç»ƒï¼šQ[1](s,Ã£[ğœƒ](s)) âˆ’Î± log Ï€[ğœƒ](Ã£[ğœƒ](s)|s)ï¼Œå…¶ä¸­Ã£[ğœƒ]æ˜¯ä»Ï€[ğœƒ](â‹…|s)ä¸­é‡‡æ ·çš„
- en: Implementation
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'The implementation of the SAC method is in 06_train_sac.py. The model consists
    of the following networks, defined in lib/model.py:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: SACæ–¹æ³•çš„å®ç°ä½äº06_train_sac.pyä¸­ã€‚è¯¥æ¨¡å‹ç”±ä»¥ä¸‹ç½‘ç»œç»„æˆï¼Œè¿™äº›ç½‘ç»œåœ¨lib/model.pyä¸­å®šä¹‰ï¼š
- en: 'ModelActor: This is the same policy that we used in the previous examples in
    this chapter. As the policy variance is not parametrized by the state (the logstd
    field is not a network, but just a tensor), the training objective does not 100%
    comply with SAC. On the one hand, it might influence the convergence and performance,
    as the core idea of the SAC method is entropy regularization, which canâ€™t be implemented
    without parametrized variance. On the other hand, it decreases the number of parameters
    in the model. If youâ€™re curious, you can extend the example with the parametrized
    variance of the policy and implement a proper SAC method.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ModelActorï¼šè¿™æ˜¯æˆ‘ä»¬åœ¨æœ¬ç« å‰é¢ç¤ºä¾‹ä¸­ä½¿ç”¨çš„ç›¸åŒç­–ç•¥ã€‚ç”±äºç­–ç•¥æ–¹å·®æ²¡æœ‰é€šè¿‡çŠ¶æ€æ¥å‚æ•°åŒ–ï¼ˆlogstdå­—æ®µä¸æ˜¯ç½‘ç»œï¼Œåªæ˜¯ä¸€ä¸ªå¼ é‡ï¼‰ï¼Œå› æ­¤è®­ç»ƒç›®æ ‡å¹¶ä¸å®Œå…¨ç¬¦åˆSACæ–¹æ³•ã€‚ä»ä¸€æ–¹é¢æ¥è¯´ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ”¶æ•›æ€§å’Œæ€§èƒ½ï¼Œå› ä¸ºSACæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ç†µæ­£åˆ™åŒ–ï¼Œè€Œæ²¡æœ‰å‚æ•°åŒ–æ–¹å·®çš„è¯æ— æ³•å®ç°è¿™ä¸€ç‚¹ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒå‡å°‘äº†æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ã€‚å¦‚æœä½ æ„Ÿå…´è¶£çš„è¯ï¼Œå¯ä»¥é€šè¿‡å¯¹ç­–ç•¥è¿›è¡Œå‚æ•°åŒ–æ–¹å·®æ‰©å±•è¯¥ç¤ºä¾‹ï¼Œå¹¶å®ç°ä¸€ä¸ªå®Œæ•´çš„SACæ–¹æ³•ã€‚
- en: 'ModelCritic: This is the same value network as in the previous examples.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ModelCriticï¼šè¿™æ˜¯ä¸å‰é¢ç¤ºä¾‹ç›¸åŒçš„å€¼ç½‘ç»œã€‚
- en: 'ModelSACTwinQ: These two networks take the state and action as the input and
    predict Q-values.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ModelSACTwinQï¼šè¿™ä¸¤ä¸ªç½‘ç»œå°†çŠ¶æ€å’ŒåŠ¨ä½œä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹Qå€¼ã€‚
- en: 'The first function implementing the method is unpack_batch_sac(), and it is
    defined in lib/common.py. Its goal is to take the batch of trajectory steps and
    calculate target values for V-networks and twin Q-networks:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°è¯¥æ–¹æ³•çš„ç¬¬ä¸€ä¸ªå‡½æ•°æ˜¯unpack_batch_sac()ï¼Œå®ƒå®šä¹‰åœ¨lib/common.pyä¸­ã€‚å®ƒçš„ç›®æ ‡æ˜¯è·å–è½¨è¿¹æ­¥éª¤çš„æ‰¹æ¬¡å¹¶ä¸ºVç½‘ç»œå’ŒåŒQç½‘ç»œè®¡ç®—ç›®æ ‡å€¼ï¼š
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first step of the function uses the already defined unpack_batch_a2c() method,
    which unpacks the batch, converts states and actions into tensors, and calculates
    the reference for Q-networks using Bellman approximation. Once this is done, we
    need to calculate the reference for the V-network from the minimum of the twin
    Q-values minus the scaled entropy coefficient. The entropy is calculated from
    our current policy network. As was already mentioned, our policy has the parametrized
    mean value, but the variance is global and doesnâ€™t depend on the state.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„ç¬¬ä¸€æ­¥ä½¿ç”¨å·²ç»å®šä¹‰çš„unpack_batch_a2c()æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è§£åŒ…æ‰¹æ¬¡ï¼Œå°†çŠ¶æ€å’ŒåŠ¨ä½œè½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶é€šè¿‡Bellmanè¿‘ä¼¼è®¡ç®—Qç½‘ç»œçš„å‚è€ƒå€¼ã€‚å®Œæˆæ­¤æ­¥éª¤åï¼Œæˆ‘ä»¬éœ€è¦ä»åŒQå€¼çš„æœ€å°å€¼å‡å»ç¼©æ”¾çš„ç†µç³»æ•°æ¥è®¡ç®—Vç½‘ç»œçš„å‚è€ƒå€¼ã€‚ç†µæ˜¯é€šè¿‡å½“å‰ç­–ç•¥ç½‘ç»œè®¡ç®—çš„ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬çš„ç­–ç•¥å…·æœ‰å‚æ•°åŒ–çš„å‡å€¼ï¼Œä½†æ–¹å·®æ˜¯å…¨å±€çš„ï¼Œå¹¶ä¸ä¾èµ–äºçŠ¶æ€ã€‚
- en: 'In the main training loop, we use the function defined previously and do three
    different optimization steps: for V, for Q, and for the policy. The following
    is the relevant part of the training loop defined in 06_train_sac.py:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸»è¦è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å…ˆå‰å®šä¹‰çš„å‡½æ•°ï¼Œå¹¶è¿›è¡Œä¸‰ç§ä¸åŒçš„ä¼˜åŒ–æ­¥éª¤ï¼šVç½‘ç»œã€Qç½‘ç»œå’Œç­–ç•¥ç½‘ç»œã€‚ä»¥ä¸‹æ˜¯06_train_sac.pyä¸­å®šä¹‰çš„è®­ç»ƒå¾ªç¯çš„ç›¸å…³éƒ¨åˆ†ï¼š
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the beginning, we unpack the batch to get the tensors and targets for the
    Q- and V-networks.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬è§£åŒ…æ‰¹æ¬¡ä»¥è·å–Qå’ŒVç½‘ç»œçš„å¼ é‡å’Œç›®æ ‡ã€‚
- en: 'The twin Q-networks are optimized by the same target value:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: åŒQç½‘ç»œé€šè¿‡ç›¸åŒçš„ç›®æ ‡å€¼è¿›è¡Œä¼˜åŒ–ï¼š
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The critic network is also optimized with the trivial MSE objective using the
    already calculated target value:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„è®ºè€…ç½‘ç»œä¹Ÿé€šè¿‡ä½¿ç”¨å·²ç»è®¡ç®—çš„ç›®æ ‡å€¼å’Œç®€å•çš„MSEç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼š
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And finally, we optimize the actor network:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä¼˜åŒ–è¡Œä¸ºè€…ç½‘ç»œï¼š
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In comparison with the formulas given previously, the code is missing the entropy
    regularization term and corresponds to DDPG training. As our variance doesnâ€™t
    depend on the state, it can be omitted from the optimization objective.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰ç»™å‡ºçš„å…¬å¼ç›¸æ¯”ï¼Œä»£ç ç¼ºå°‘äº†ç†µæ­£åˆ™åŒ–é¡¹ï¼Œå®é™…ä¸Šå¯¹åº”çš„æ˜¯DDPGè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„æ–¹å·®ä¸ä¾èµ–äºçŠ¶æ€ï¼Œå®ƒå¯ä»¥ä»ä¼˜åŒ–ç›®æ ‡ä¸­çœç•¥ã€‚
- en: Results
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: I ran SAC training in the HalfCheetah and Ant environments for 9-13 hours, with
    5M observations. The results are a bit contradictory. On the one hand, the sample
    efficiency and reward growing dynamics of SAC were better than the PPO method.
    For example, SAC was able to reach a reward of 900 after just 0.5M observations
    on HalfCheetah. PPO required more than 1M observations to reach the same policy.
    In the MuJoCo environment, SAC was able to find the policy that got a reward of
    7,063, which is an absolute record (demonstrating state-of-the-art performance
    on this environment).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨HalfCheetahå’ŒAntç¯å¢ƒä¸­è¿›è¡Œäº†9åˆ°13å°æ—¶çš„SACè®­ç»ƒï¼Œè§‚å¯Ÿæ•°æ®ä¸º5Mã€‚ç»“æœæœ‰ç‚¹çŸ›ç›¾ã€‚ä¸€æ–¹é¢ï¼ŒSACçš„æ ·æœ¬æ•ˆç‡å’Œå¥–åŠ±å¢é•¿åŠ¨æ€ä¼˜äºPPOæ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒSACåªéœ€0.5Mè§‚å¯Ÿå°±èƒ½åœ¨HalfCheetahä¸Šè¾¾åˆ°900çš„å¥–åŠ±ï¼Œè€ŒPPOéœ€è¦è¶…è¿‡1Mè§‚å¯Ÿæ‰èƒ½è¾¾åˆ°ç›¸åŒçš„ç­–ç•¥ã€‚åœ¨MuJoCoç¯å¢ƒä¸­ï¼ŒSACæ‰¾åˆ°äº†è·å¾—7,063å¥–åŠ±çš„ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ä¸ªç»å¯¹çš„è®°å½•ï¼ˆå±•ç¤ºäº†è¯¥ç¯å¢ƒä¸Šçš„æœ€å…ˆè¿›è¡¨ç°ï¼‰ã€‚
- en: On the other hand, due to the off-policy nature of SAC, the training speed was
    much slower, as we did more calculations than with on-policy methods. On my machine,
    5M frames on HalfCheetah took 10 hours. As a reminder, A2C did 50M observations
    in the same time.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œç”±äºSACæ˜¯ç¦»ç­–ç•¥æ–¹æ³•ï¼Œè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå› ä¸ºæˆ‘ä»¬è¿›è¡Œäº†æ¯”ä¼ ç»Ÿçš„åœ¨ç­–ç•¥æ–¹æ³•æ›´å¤šçš„è®¡ç®—ã€‚åœ¨æˆ‘çš„æœºå™¨ä¸Šï¼Œ5Må¸§çš„HalfCheetahè®­ç»ƒèŠ±è´¹äº†10å°æ—¶ã€‚ä½œä¸ºæé†’ï¼ŒA2Cåœ¨åŒæ ·æ—¶é—´å†…å®Œæˆäº†50Mè§‚å¯Ÿã€‚
- en: 'This demonstrates the trade-offs between on-policy and off-policy methods,
    as you have seen many times in this book so far: if your environment is fast and
    observations are cheap to obtain, an on-policy method like PPO might be the best
    choice. But if your observations are hard to obtain, off-policy methods will do
    a better job, but require more calculations to be performed.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å±•ç¤ºäº†åœ¨æœ¬ä¹¦ä¸­ä½ å·²å¤šæ¬¡çœ‹åˆ°çš„åœ¨ç­–ç•¥å’Œç¦»ç­–ç•¥æ–¹æ³•ä¹‹é—´çš„æƒè¡¡ï¼šå¦‚æœä½ çš„ç¯å¢ƒååº”é€Ÿåº¦å¿«ï¼Œä¸”è§‚å¯Ÿæ•°æ®æ˜“å¾—ï¼Œé‚£ä¹ˆåƒPPOè¿™æ ·çš„åœ¨ç­–ç•¥æ–¹æ³•å¯èƒ½æ˜¯æœ€ä½³é€‰æ‹©ã€‚ä½†å¦‚æœä½ çš„è§‚å¯Ÿæ•°æ®éš¾ä»¥è·å¾—ï¼Œç¦»ç­–ç•¥æ–¹æ³•å°†æ›´æœ‰æ•ˆï¼Œä½†éœ€è¦æ›´å¤šçš„è®¡ç®—ã€‚
- en: 'FigureÂ [16.17](#x1-307002r17) and FigureÂ [16.18](#x1-307003r18) show the reward
    dynamics on HalfCheetah:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[16.17](#x1-307002r17)å’Œå›¾[16.18](#x1-307003r18)å±•ç¤ºäº†HalfCheetahä¸Šçš„å¥–åŠ±åŠ¨æ€ï¼š
- en: '![PIC](img/B22150_16_17.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_17.png)'
- en: 'FigureÂ 16.17: The reward during training (left) and the test reward (right)
    for HalfCheetah on PyBullet'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.17ï¼šPyBulletä¸­HalfCheetahçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_16_18.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_18.png)'
- en: 'FigureÂ 16.18: The reward during training (left) and the test reward (right)
    for HalfCheetah on MuJoCo'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.18ï¼šMuJoCoä¸­HalfCheetahçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: 'The results in the Ant environment are much worse â€“ according to the score,
    the learned policy can barely stand. The PyBullet plots are shown in FigureÂ [16.19](#x1-307005r19);
    MuJoCo plots are shown in FigureÂ [16.20](#x1-307006r20):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Antç¯å¢ƒä¸­çš„ç»“æœåˆ™å·®å¾—å¤šâ€”â€”æ ¹æ®å¾—åˆ†ï¼Œå­¦ä¹ åˆ°çš„ç­–ç•¥å‡ ä¹æ— æ³•ç»´æŒã€‚PyBulletçš„å›¾åƒè§å›¾[16.19](#x1-307005r19)ï¼›MuJoCoçš„å›¾åƒè§å›¾[16.20](#x1-307006r20)ï¼š
- en: '![PIC](img/B22150_16_19.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_19.png)'
- en: 'FigureÂ 16.19: The reward during training (left) and the test reward (right)
    for Ant on PyBullet'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.19ï¼šPyBulletä¸­Antçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_16_20.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_16_20.png)'
- en: 'FigureÂ 16.20: The reward during training (left) and the test reward (right)
    for Ant on MuJoCo'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾16.20ï¼šMuJoCoä¸­Antçš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæµ‹è¯•å¥–åŠ±ï¼ˆå³ï¼‰
- en: 'Here are the videos for the best SAC models:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æœ€ä½³SACæ¨¡å‹çš„è§†é¢‘ï¼š
- en: 'HalfCheetah on PyBullet (score 1,765): [https://youtu.be/80afu9OzQ5s](https://youtu.be/80afu9OzQ5s).
    Our creature is a bit clumsy here.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBulletä¸­çš„HalfCheetahï¼ˆå¾—åˆ†1,765ï¼‰ï¼š[https://youtu.be/80afu9OzQ5s](https://youtu.be/80afu9OzQ5s)ã€‚æˆ‘ä»¬çš„ç”Ÿç‰©åœ¨è¿™é‡Œæ˜¾å¾—æœ‰ç‚¹ç¬¨æ‹™ã€‚
- en: 'HalfCheetah on MuJoCo (score 7,063): [https://youtube.com/shorts/0Ywn3LTJxxs](https://youtube.com/shorts/0Ywn3LTJxxs).
    This result is really impressive â€“ a super-fast Cheetah.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCoä¸­çš„HalfCheetahï¼ˆå¾—åˆ†7,063ï¼‰ï¼š[https://youtube.com/shorts/0Ywn3LTJxxs](https://youtube.com/shorts/0Ywn3LTJxxs)ã€‚è¿™ä¸ªç»“æœéå¸¸ä»¤äººå°è±¡æ·±åˆ»â€”â€”ä¸€åªè¶…å¿«çš„çŒè±¹ã€‚
- en: 'Ant on PyBullet (score 630): [https://youtu.be/WHqXJ3VqX4k](https://youtu.be/WHqXJ3VqX4k).
    After a couple of steps, the ant got stuck for some reason.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBulletä¸­çš„Antï¼ˆå¾—åˆ†630ï¼‰ï¼š[https://youtu.be/WHqXJ3VqX4k](https://youtu.be/WHqXJ3VqX4k)ã€‚åœ¨å‡ æ­¥ä¹‹åï¼Œèš‚èšå› æŸç§åŸå› è¢«å¡ä½äº†ã€‚
- en: Overall results
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ä½“ç»“æœ
- en: 'To simplify the comparison of the methods, I put all the numbers related to
    the best rewards obtained in the following table:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–æ–¹æ³•çš„æ¯”è¾ƒï¼Œæˆ‘å°†æ‰€æœ‰ä¸æœ€ä½³å¥–åŠ±ç›¸å…³çš„æ•°æ®æ±‡æ€»åœ¨ä¸‹é¢çš„è¡¨æ ¼ä¸­ï¼š
- en: '| Method | HalfCheetah | Ant |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | HalfCheetah | Ant |'
- en: '|  | PyBullet | MuJoCo | PyBullet | MuJoCo |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | PyBullet | MuJoCo | PyBullet | MuJoCo |'
- en: '| A2C | 2,189 | 4,718 | 2,425 | 5,380 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| A2C | 2,189 | 4,718 | 2,425 | 5,380 |'
- en: '| PPO | 2,567 | 1,623 | 2,560 | 5,108 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| PPO | 2,567 | 1,623 | 2,560 | 5,108 |'
- en: '| TRPO | 2,419 | 5,753 | 834 | 993 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| TRPO | 2,419 | 5,753 | 834 | 993 |'
- en: '| ACKTR | 250 | 3,100 | 1,820 | â€” |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ACKTR | 250 | 3,100 | 1,820 | â€” |'
- en: '| SAC | 1,765 | 7,063 | 630 | â€” |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| SAC | 1,765 | 7,063 | 630 | â€” |'
- en: 'TableÂ 16.1: Summary table'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨16.1ï¼šæ€»ç»“è¡¨
- en: As you can see, there is no single winning method â€“ some do well in some environments
    but get worse results in others. In principle, we can call A2C and PPO as quite
    consistent methods because theyâ€™re getting good results everywhere (PPOâ€™s â€œbackflip
    cheetahâ€ on MuJoCo could be attributed to a bad starting seed, so rerunning the
    training might lead to a better policy).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæ²¡æœ‰å•ä¸€çš„è·èƒœæ–¹æ³•â€”â€”æŸäº›æ–¹æ³•åœ¨æŸäº›ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¶ä»–ç¯å¢ƒä¸­æ•ˆæœè¾ƒå·®ã€‚åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç§°A2Cå’ŒPPOä¸ºç›¸å½“ä¸€è‡´çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬åœ¨å„ä¸ªç¯å¢ƒä¸­éƒ½èƒ½å–å¾—ä¸é”™çš„ç»“æœï¼ˆPPOåœ¨MuJoCoä¸Šçš„â€œåç©ºç¿»çŒè±¹â€å¯èƒ½å½’å› äºä¸å¥½çš„åˆå§‹ç§å­ï¼Œå› æ­¤é‡æ–°è®­ç»ƒå¯èƒ½ä¼šäº§ç”Ÿæ›´å¥½çš„ç­–ç•¥ï¼‰ã€‚
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'In this chapter, we checked three different methods with the aim of improving
    the stability of the stochastic policy gradient and compared them to the A2C implementation
    on two continuous control problems. Along with the methods covered in the previous
    chapter (DDPG and D4PG), these methods are basic tools to work with a continuous
    control domain. Finally, we checked a relatively new off-policy method that is
    an extension of DDPG: SAC. Here, we have just scratched the surface of this topic,
    but it could be a good starting point to dive into it in more depth. These methods
    are widely used in robotics and related areas.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†ä¸‰ç§ä¸åŒçš„æ–¹æ³•ï¼Œç›®çš„æ˜¯æé«˜éšæœºç­–ç•¥æ¢¯åº¦çš„ç¨³å®šæ€§ï¼Œå¹¶å°†å®ƒä»¬ä¸A2Cåœ¨ä¸¤ä¸ªè¿ç»­æ§åˆ¶é—®é¢˜ä¸Šçš„å®ç°è¿›è¡Œæ¯”è¾ƒã€‚è¿åŒä¸Šä¸€ç« ä»‹ç»çš„æ–¹æ³•ï¼ˆDDPGå’ŒD4PGï¼‰ï¼Œè¿™äº›æ–¹æ³•æ˜¯å¤„ç†è¿ç»­æ§åˆ¶é¢†åŸŸçš„åŸºæœ¬å·¥å…·ã€‚æœ€åï¼Œæˆ‘ä»¬æ£€æŸ¥äº†ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„ç¦»ç­–ç•¥æ–¹æ³•ï¼Œå®ƒæ˜¯DDPGçš„æ‰©å±•ï¼šSACã€‚æˆ‘ä»¬åªæ˜¯è§¦åŠäº†è¿™ä¸ªè¯é¢˜çš„è¡¨é¢ï¼Œä½†è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶ã€‚è¿™äº›æ–¹æ³•åœ¨æœºå™¨äººæŠ€æœ¯åŠç›¸å…³é¢†åŸŸä¸­å¹¿æ³›åº”ç”¨ã€‚
- en: 'In the next chapter, we will switch to a different set of RL methods that have
    been becoming popular recently: black-box or gradient-free methods.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è½¬å‘æœ€è¿‘è¶Šæ¥è¶Šæµè¡Œçš„å¦ä¸€ç±»å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼šé»‘ç®±æˆ–æ— æ¢¯åº¦æ–¹æ³•ã€‚
- en: Join our community on Discord
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒº
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–ç”¨æˆ·ã€æ·±åº¦å­¦ä¹ ä¸“å®¶ä»¥åŠä½œè€…æœ¬äººä¸€èµ·é˜…è¯»æœ¬ä¹¦ã€‚æé—®ã€ä¸ºå…¶ä»–è¯»è€…æä¾›è§£å†³æ–¹æ¡ˆã€é€šè¿‡é—®æˆ‘ä»»ä½•é—®é¢˜ç¯èŠ‚ä¸ä½œè€…äº’åŠ¨ï¼Œç­‰ç­‰ã€‚æ‰«æäºŒç»´ç æˆ–è®¿é—®é“¾æ¥åŠ å…¥ç¤¾åŒºã€‚[https://packt.link/rl](https://packt.link/rl)
- en: '![PIC](img/file1.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
