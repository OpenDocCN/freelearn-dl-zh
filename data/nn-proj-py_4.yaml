- en: Cats Versus Dogs - Image Classification Using CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 猫与狗 - 使用 CNN 进行图像分类
- en: In this chapter, we will use **convolutional neural networks** (**CNNs**) to
    create a classifier that can predict whether a given image contains a cat or a
    dog.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用**卷积神经网络**（**CNNs**）创建一个分类器，预测给定图像中是否包含猫或狗。
- en: This project marks the first in a series of projects where we will use neural
    networks for image recognition and computer vision problems. As we shall see,
    neural networks have proven to be an extremely effective tool for solving problems
    in computer vision.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目是系列项目的第一部分，我们将在其中使用神经网络解决图像识别和计算机视觉问题。正如我们将看到的，神经网络已被证明是解决计算机视觉问题的极其有效工具。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: 'Motivation for the problem that we''re trying to tackle: image recognition'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们尝试解决的问题的动机：图像识别
- en: Neural networks and deep learning for computer vision
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉中的神经网络与深度学习
- en: Understanding convolution and max pooling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积和最大池化
- en: Architecture of CNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 的架构
- en: Training CNNs in Keras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中训练 CNN
- en: Using transfer learning to leverage on a state-of-the art neural network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习来利用最先进的神经网络
- en: Analysis of our results
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们结果的分析
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The key Python libraries required for this chapter are:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的关键 Python 库包括：
- en: matplotlib 3.0.2
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib 3.0.2
- en: Keras 2.2.4
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 2.2.4
- en: Numpy 1.15.2
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Numpy 1.15.2
- en: Piexif 1.1.2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piexif 1.1.2
- en: To download the dataset required for this project, please refer to the directions
    at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/Chapter04/how_to_download_the_dataset.txt](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/chapter4/how_to_download_the_dataset.txt).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载此项目所需的数据集，请参阅[https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/Chapter04/how_to_download_the_dataset.txt](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/chapter4/how_to_download_the_dataset.txt)中的说明。
- en: The code for this chapter can be found in the GitHub repository for the book
    at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python)。
- en: 'To download the code into your computer, you may run the following `git clone`
    command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要将代码下载到您的计算机，请运行以下`git clone`命令：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the process is complete, there will be a folder titled `Neural-Network-Projects-with-Python`.
    Enter the folder by running the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完成后，将会有一个名为`Neural-Network-Projects-with-Python`的文件夹。通过运行以下命令进入该文件夹：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要在虚拟环境中安装所需的 Python 库，请运行以下命令：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that you should have installed Anaconda in your computer first, before
    running this command. To enter the virtual environment, run the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在运行此命令之前，您应该先在计算机上安装 Anaconda。要进入虚拟环境，请运行以下命令：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Important
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This chapter requires an additional image processing library known as **`Piexif`**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要一个额外的图像处理库，名为**`Piexif`**。
- en: 'To download **`Piexif`**, please run the following command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载**`Piexif`**，请运行以下命令：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Navigate to the folder `Chapter04` by running the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令，导航到文件夹`Chapter04`：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following files are located in the folder:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文件位于该文件夹中：
- en: '`main_basic_cnn.py`: This is the main code for the basic CNN'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main_basic_cnn.py`：这是基本 CNN 的主要代码'
- en: '`main_vgg16.py`: This is the main code for the VGG16 network'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main_vgg16.py`：这是 VGG16 网络的主要代码'
- en: '`utils.py`: This file contains auxiliary utility code that will help us in
    the implementation of our neural network'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`utils.py`：此文件包含辅助工具代码，帮助我们实现神经网络'
- en: '`visualize_dataset.py`: This file contains the code for exploratory data analysis
    and data visualization'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`visualize_dataset.py`：此文件包含探索性数据分析和数据可视化的代码'
- en: '`image_augmentation.py`: This file contains sample code for image augmentation'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_augmentation.py`：此文件包含图像增强的示例代码'
- en: 'To run the code for the neural network, simply execute the `main_basic_cnn.py`
    and `main_vgg16.py` files:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行神经网络的代码，只需执行`main_basic_cnn.py`和`main_vgg16.py`文件：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Computer vision and object recognition
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉与物体识别
- en: Computer vision is an engineering field where the objective is to create programs
    that can extract meaning from images. According to an urban legend, computer vision
    first started in the 1960s when Professor Marvin Minsky from MIT assigned a summer
    project to a group of undergraduates, with the requirement that they should attach
    a camera to a computer and to have the computer describe everything that it sees.
    The project was expected to be completed in just one summer. Needless to say,
    it wasn't completed within that summer as computer vision is an extremely complex
    field that scientists are continuously working on even today.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是一个工程领域，目标是创建能够从图像中提取意义的程序。根据一个城市传说，计算机视觉的起源可以追溯到1960年代，当时麻省理工学院的马文·敏斯基教授给一群本科生布置了一个暑期项目，要求他们将相机连接到计算机上，并让计算机描述它所看到的一切。该项目预计只需一个夏天便能完成。毫无疑问，由于计算机视觉是一个极其复杂的领域，科学家们直到今天仍在不断研究，它并没有在那个夏天完成。
- en: Early progression in computer vision was modest. In the 1960s, researchers started
    by creating algorithms to detect shapes, lines, and edges in photographs. The
    following decades saw the evolution of computer vision into several subfields.
    Computer vision researchers worked on signal processing, image processing, computer
    photometry, object recognition, and so on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的早期发展是温和的。20世纪60年代，研究人员开始创建算法来检测照片中的形状、线条和边缘。接下来的几十年里，计算机视觉发展成多个子领域。计算机视觉研究人员致力于信号处理、图像处理、计算机光度学、物体识别等领域。
- en: Object recognition is perhaps one of the most ubiquitous applications in computer
    vision. Researchers had worked on object recognition for a long time. The challenge
    faced by early object recognition researchers was that the dynamic appearance
    of objects made it difficult to teach computers to recognize them. Early computer
    vision researchers focused on template matching for object recognition, but often
    faced difficulties due to variations in angle, lighting, and occlusions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 物体识别可能是计算机视觉中最普遍的应用之一。研究人员已经在物体识别方面工作了很长时间。早期的物体识别研究者面临的挑战是，物体的动态外观使得教计算机识别它们变得非常困难。早期的计算机视觉研究者专注于物体识别中的模板匹配，但由于角度、光照和遮挡的变化，往往遇到困难。
- en: The field of object recognition has grown exponentially in recent years, propelled
    by the advancements in neural networks and deep learning. In 2012, Alex Krizhevsky
    et al. won the **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**)
    by a significant margin over other contenders. The winning idea proposed by Alex
    Krizhevsky et al. was to use a CNN (an architecture termed the AlexNet) for object
    recognition. AlexNet was a significant breakthrough for object recognition. Since
    then, neural networks have become the number one technique for object recognition
    and computer vision related tasks. In this project, you will create a CNN similar
    to AlexNet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，得益于神经网络和深度学习的进展，物体识别领域得到了飞速发展。2012年，Alex Krizhevsky等人凭借**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）以显著优势战胜其他参赛者。Alex
    Krizhevsky等人提出的获胜方案是使用CNN（被称为AlexNet的架构）进行物体识别。AlexNet是物体识别领域的一个重要突破。从那时起，神经网络就成为物体识别和计算机视觉相关任务的首选技术。在本项目中，您将创建一个类似于AlexNet的CNN。
- en: The breakthrough in object recognition also led to the rise of AI that we know
    today. Facebook uses facial recognition to automatically tag and classify photos
    of you and your friends. Security systems use facial recognition to detect intrusions
    and persons of interest. Self-driving cars use object recognition to detect pedestrians,
    traffic signs, and other road objects. In many ways, society is starting to view
    object recognition, computer vision, and AI as one entity, even though their roots
    are very much different.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 物体识别的突破也促进了今天我们所知的AI的崛起。Facebook使用面部识别技术自动标记和分类你和你朋友的照片。安全系统使用面部识别来检测入侵和嫌疑人。自动驾驶汽车使用物体识别来检测行人、交通标志和其他道路物体。在许多方面，社会开始将物体识别、计算机视觉和AI视为一个整体，尽管它们的根源非常不同。
- en: Types of object recognition tasks
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体识别任务的类型
- en: 'It is important to understand the different kinds of object recognition tasks,
    as the required neural network architecture greatly depends on the task. Object
    recognition tasks can be broadly classified into three different types:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 了解不同种类的物体识别任务非常重要，因为所需的神经网络架构在很大程度上取决于任务的类型。物体识别任务可以大致分为三种类型：
- en: Image classification
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类
- en: Object detection
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体检测
- en: Instance segmentation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例分割
- en: 'The following diagram depicts the difference between each task:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了每个任务之间的区别：
- en: '![](img/ce20900f-6812-4db0-863f-02f28952210f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce20900f-6812-4db0-863f-02f28952210f.png)'
- en: In **Image Classification**, the input to the problem is an image and the required
    output is simply a prediction of the class that the image belongs to. This is
    analogous to our first project, where we constructed a classifier to predict whether
    a patient is at risk of diabetes. In image classification, the problem is applied
    on pixels as our input data (specifically, the intensity value of each pixel),
    instead of tabular data represented by pandas DataFrames. In this project, we
    will focus on image classification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图像分类**中，问题的输入是图像，所需的输出仅仅是对图像所属类别的预测。这类似于我们的第一个项目，在该项目中，我们构建了一个分类器来预测一个病人是否有糖尿病风险。在图像分类中，问题是应用于像素作为输入数据（具体来说，是每个像素的强度值），而不是由pandas
    DataFrame表示的表格数据。在这个项目中，我们将专注于图像分类。
- en: In **Object Detection**, the input to the problem is an image and the required
    output are bounding boxes surrounding the detected objects. You can think of this
    as a step up from the image classification task. The neural network can no longer
    assume that there is only one class present in the image, and must assume that
    the image contains multiple classes. The neural network must then identify the
    presence of each class in the image, and to draw a bounding box around each of
    them. As you can imagine, this task is not trivial and object detection was a
    really difficult problem before neural networks came about. Today, neural networks
    can perform object detection efficiently. In 2014, 2 years after AlexNet was first
    developed, Girshick et al. showed that the results in image classification can
    be generalized to **Object** **Detection**. The intuitive idea behind their approach
    is to propose multiple boxes where objects of interest may exist, and then to
    use a CNN to predict the most likely class inside each bounding box. This approach
    is known as Regions with CNN (R-CNN).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在**目标检测**中，问题的输入是图像，所需的输出是包围检测到的物体的边界框。你可以将这看作是图像分类任务的升级。神经网络不再假设图像中只有一个类别，而必须假设图像包含多个类别。神经网络接下来需要识别图像中每个类别的存在，并在每个类别周围画出边界框。正如你所想象的，这个任务并不简单，目标检测在神经网络出现之前是一个非常困难的问题。如今，神经网络可以高效地进行目标检测。2014年，在AlexNet首次开发两年后，Girshick等人展示了图像分类中的结果可以推广到**目标**
    **检测**。他们方法的直观想法是提出多个可能包含感兴趣物体的框，然后使用CNN预测每个边界框内最可能的类别。这种方法被称为区域卷积神经网络（R-CNN）。
- en: Lastly, in **Instance Segmentation**, the input to the problem is an image and
    the output are pixel groupings that correspond to each class. You can think of
    instance segmentation as a refinement of object detection. Instance segmentation
    is especially useful and prevalent in technology today. The portrait mode function
    in many smartphone cameras relies on instance segmentation to separate objects
    in the foreground from the background, creating a nice depth of field (bokeh)
    effect. Instance segmentation is also crucial in self-driving cars, as the location
    of each object around the car must be identified with pinpoint precision. In 2017,
    an adaption of R-CNN, known as Mask R-CNN, was shown to be extremely effective
    at instance segmentation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在**实例分割**中，问题的输入是图像，输出是与每个类别对应的像素分组。你可以将实例分割看作是目标检测的细化。实例分割在今天的技术中尤为重要且广泛应用。许多智能手机相机中的人像模式功能依赖于实例分割，将前景物体与背景分离，从而创造出良好的景深（虚化）效果。实例分割在自动驾驶汽车中也至关重要，因为必须精确定位汽车周围每个物体的位置。在2017年，R-CNN的改进版——Mask
    R-CNN，已被证明在实例分割中非常有效。
- en: As we can see, recent advancements in object recognition are driven by CNNs.
    In this project, we will gain an in-depth understanding of CNNs, and we will train
    and create one from scratch in Keras.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，最近在物体识别方面的进展是由CNN推动的。在这个项目中，我们将深入了解CNN，并将在Keras中从零开始训练并创建一个CNN。
- en: Digital images as neural network input
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数字图像作为神经网络输入
- en: Recall that in previous chapters, we made the distinction that neural networks
    require numerical inputs. We saw how we can encode categorical features, such
    as day of week, into numerical features using one-hot encoding. How then do we
    use an image as input for our neural network? Well, the short answer is that all
    digital images are numerical in nature!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下在前面的章节中，我们曾提到神经网络需要数值输入。我们看到如何使用独热编码将类别特征（如星期几）转换为数值特征。那么，如何将图像作为输入传递给神经网络呢？简短的回答是，所有数字图像本质上都是数值的！
- en: 'To see why this is so, consider a 28 x 28 image of a handwritten digit 3, as
    shown in the following screenshot. Let''s assume for now that the image is in
    grayscale (black and white). If we look at the intensity of each pixel that makes
    up the image, we can see that certain pixels are totally white, while some pixels
    are gray and black. In a computer, white pixels are represented with the value
    0 and black pixels are represented with a value of 255\. Everything else in between
    white and black (that is, shades of gray) has a value in between 0 and 255\. Therefore,
    digital images are essentially numerical data and neural networks are perfectly
    able to learn from them:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，考虑一张28 x 28的手写数字“3”图像，如下截图所示。假设此时图像是灰度图（黑白图）。如果我们查看组成图像的每个像素的强度，我们会看到某些像素是纯白的，而某些像素是灰色或黑色。在计算机中，白色像素用值0表示，黑色像素用值255表示。白色和黑色之间的所有其他灰度值（即灰色的不同层次）在0和255之间。因此，数字图像本质上是数值数据，神经网络可以从中学习：
- en: '![](img/74113975-2997-4039-88a7-b7556eaf7e58.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74113975-2997-4039-88a7-b7556eaf7e58.png)'
- en: What about color images? Color images are simply images with three channels—red,
    green, and blue (commonly known as RGB). The pixel values in each channel then
    represent the red intensity, green intensity, and blue intensity. Another way
    to think about it is, that for a pure red image, the pixels value will be 255
    in the red channel and 0 for the green and blue channels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么彩色图像呢？彩色图像仅仅是具有三个通道的图像——红色、绿色和蓝色（通常称为RGB）。每个通道中的像素值代表红色强度、绿色强度和蓝色强度。另一种理解方式是，对于一张纯红色的图像，红色通道的像素值为255，绿色和蓝色通道的像素值为0。
- en: 'The following depicts a color image, and the separation of the color image
    into its RGB channels. Notice how a color image is stacked in a three-dimensional
    manner. In contrast, a grayscale image has only two dimensions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一幅彩色图像，以及将彩色图像分解为RGB通道。请注意，彩色图像是以三维方式堆叠的。相比之下，灰度图像只有两个维度：
- en: '![](img/f72ca293-4530-4c09-8e83-9015b660530b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f72ca293-4530-4c09-8e83-9015b660530b.png)'
- en: Building blocks of CNNs
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的构建模块
- en: One of the challenges faced in image classification is that the appearance of
    objects is dynamic. Just as there are many different breeds of cats and dogs,
    there are an infinite number of ways cats and dogs can appear in images. This
    makes it difficult for rudimentary image classification techniques, as it is impossible
    to show an infinite number of photos of cats and dogs to a computer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类面临的挑战之一是物体的外观是动态的。正如猫和狗有许多不同的品种一样，猫和狗在图像中的呈现方式也是无穷无尽的。这使得基础的图像分类技术变得困难，因为不可能向计算机展示无数张猫狗的照片。
- en: However, this really shouldn't be a problem at all. Humans don't require an
    infinite number of photos of cats and dogs to differentiate between the two. A
    toddler can easily differentiate cats and dogs once he has seen just a few of
    them. If we think about how humans approach image classification, we notice that
    humans tend to look for landmark features while trying to identify an object.
    For example, we know that cats tend to be smaller in size compared to dogs, cats
    tend to have pointy ears, and cats have a shorter snout compared to dogs. Instinctively,
    humans look for these features while classifying an image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这根本不应该成为问题。人类并不需要无数张猫狗的照片来区分这两者。一个幼儿只需看到几只猫狗，就能轻松区分它们。如果我们思考人类如何进行图像分类，我们会发现人类通常会寻找一些标志性特征来识别物体。例如，我们知道猫的体型通常比狗小，猫通常有尖耳朵，猫的嘴巴比狗短。人类在分类图像时本能地会寻找这些特征。
- en: Can we then teach a computer to look for these features within the entire image?
    The answer is a resounding yes! and the key lies in **convolution**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以教会计算机在整个图像中寻找这些特征吗？答案是肯定的！关键在于**卷积**。
- en: Filtering and convolution
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滤波与卷积
- en: Before we can understand what convolution is, it is important to first understand
    filtering.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解卷积之前，首先要理解滤波的概念。
- en: Suppose we have a 9 x 9 image as our input, and we need to classify the image
    as an X or an O. The following diagram illustrates some sample input images.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个9 x 9的图像作为输入，并且需要将该图像分类为X或O。以下图示展示了一些示例输入图像。
- en: 'A perfectly drawn O is shown in the leftmost box in the following diagram,
    while the other two boxes show badly drawn Os:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完美绘制的O展示在以下图示的最左侧框中，而另外两个框展示了绘制不好的O：
- en: '![](img/0e4ec0ce-4faf-4f96-9a19-3086ace1506a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e4ec0ce-4faf-4f96-9a19-3086ace1506a.png)'
- en: 'A perfectly drawn X is shown in the leftmost box, while the other two boxes
    show badly drawn Xs:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 完美绘制的X展示在最左侧框中，而另外两个框展示了绘制不好的X：
- en: '![](img/10a3b27d-a4f2-4509-b602-e213a1b21405.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10a3b27d-a4f2-4509-b602-e213a1b21405.png)'
- en: In either case, we cannot expect the figures to be drawn perfectly. This is
    no problem for human beings, as we can all differentiate between Os and Xs even
    for the badly drawn cases.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种情况，我们都不能期望图形被完美地绘制出来。对于人类来说，这并不成问题，因为我们即使面对绘制得很差的情况，也能区分出O和X。
- en: Let's think about what makes it easy for human beings to differentiate between
    the two. What are the characteristic features in the images that allows us to
    differentiate them easily? Well, we know that Os tend to have flat horizontal
    edges, while Xs tend to have diagonal lines.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下，是什么让人类容易区分这两者的呢？在图像中，哪些特征使得我们能够轻松区分它们？嗯，我们知道O通常具有平坦的水平边缘，而X则通常具有对角线。
- en: 'The following diagram depicts one such characteristic feature for Os:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了O的一个特征：
- en: '![](img/69e1a3e5-4b5e-4e2d-bd9c-c8468ce9f09d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69e1a3e5-4b5e-4e2d-bd9c-c8468ce9f09d.png)'
- en: 'And the following diagram depicts one such characteristic feature for Xs:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了X的一个特征：
- en: '![](img/6bf69568-bdd4-4a90-b2ff-cefb5cc8d109.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bf69568-bdd4-4a90-b2ff-cefb5cc8d109.png)'
- en: In this case, the characteristic feature (also known as the filter) is of size
    3 × 3\. The presence of the characteristic feature in an image gives us a big
    hint on the class of the image. For example, if an image contains an horizontal
    edge, the characteristic feature for O, then the image is probably an O.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，特征（也称为滤波器）的大小为3 × 3。图像中存在特征时，会给我们提供关于图像类别的重要线索。例如，如果图像包含一个水平边缘，并且特征符合O的特征，那么这张图像很可能是O。
- en: How then do we search for the presence of the characteristic feature in an image?
    We can simply do a brute force search by taking the 3 x 3 filter, before sliding it
    through every single pixel in the image to look for a match.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何在图像中搜索这种特征呢？我们可以通过简单的暴力搜索来实现，方法是取一个3 x 3的滤波器，然后将它滑过图像中的每个像素，查找匹配项。
- en: Let's start from the top left-hand corner of the image. The mathematical function
    performed by the filter (known as filtering) is the element-wise multiplication
    of the sliding window with the filter. In the top left-hand corner, the output
    from the filter is 2 (notice that this is a perfect match since the window is
    identical to the filter).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从图像的左上角开始。滤波器执行的数学操作（称为滤波）是滑动窗口与滤波器进行逐元素乘法。在左上角，滤波器的输出是2（注意，这是一个完美匹配，因为窗口与滤波器是完全相同的）。
- en: 'The following diagram shows the filtering operation on the top left-hand corner
    of the image. Note that for simplicity, we assume that pixel intensity values
    are **0** or **1** (instead of 0-255 in real digital images):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了图像左上角的滤波操作。请注意，为了简化起见，我们假设像素强度值为**0**或**1**（而不是实际数字图像中的0-255）：
- en: '![](img/5a3148ca-59dd-4331-a23c-31a56289ba70.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a3148ca-59dd-4331-a23c-31a56289ba70.png)'
- en: 'Next, we slide the window toward the right to cover the next 3 x 3 section
    in the image. The following diagram shows the filtering operation on the next
    3 x 3 section:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将窗口向右滑动，覆盖图像中的下一个3 x 3区域。以下图示展示了对下一个3 x 3区域的滤波操作：
- en: '![](img/d3e52908-0a61-4b38-9c31-afa013b52b51.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3e52908-0a61-4b38-9c31-afa013b52b51.png)'
- en: The process of sliding the window through the entire image and calculating the
    filtered value is known as **convolution**. The layer in the neural network that
    performs convolution is known as the convolutional layer. Essentially, convolution
    provides us with a map to the areas where the characteristic feature is found
    in each image. This ensures that our neural network is able to perform intelligent,
    dynamic object recognition just like a human being!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将窗口滑过整个图像并计算滤波值的过程称为**卷积**。执行卷积操作的神经网络层称为卷积层。本质上，卷积为我们提供了每个图像中找到特征的区域的映射。这确保了我们的神经网络能够像人类一样执行智能、动态的物体识别！
- en: In the preceding example, we handcrafted the filter based on our own knowledge
    of Os and Xs. Note that, when we train a neural network, it will automatically
    learn the most appropriate filter to use. Recall that in previous chapters, the
    fully connected layer (dense layer) was used and the weights of the layers were
    tuned during training. Similarly, the weights of a convolutional layer will be
    tuned during training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们根据自己对O和X的了解手工制作了滤波器。请注意，当我们训练神经网络时，它会自动学习最适合的滤波器。回想一下，在前面的章节中，我们使用了全连接层（密集层），并且在训练过程中调整了层的权重。同样，卷积层的权重也将在训练过程中进行调整。
- en: 'Lastly, note that there are two main hyperparameters in a convolutional layer:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，注意卷积层有两个主要的超参数：
- en: '**Number of filters**: In the preceding example, we have used just one filter.
    We can increase the number of filters to find multiple characteristic features.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器数量**：在前面的例子中，我们只使用了一个滤波器。我们可以增加滤波器的数量，以找到多个特征。'
- en: '**Filter size**: In the preceding example, we have used a 3 x 3 filter size.
    We can tune the filter size to represent larger characteristic features.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器大小**：在前面的例子中，我们使用了一个3 x 3的滤波器大小。我们可以调整滤波器的大小，以表示更大的特征。'
- en: We will talk about these hyperparameters in further detail when we construct
    our neural network later on in the chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后的部分详细讨论这些超参数，当我们构建神经网络时。
- en: Max pooling
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化
- en: In CNNs, it is common to place a max pooling layer immediately after a convolution
    layer. The objective of the max pooling layer is to reduce the number of weights
    after each convolution layer, thereby reducing model complexity and avoiding overfitting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，通常将最大池化层紧跟在卷积层后面。最大池化层的目标是减少每个卷积层之后的权重数量，从而减少模型复杂性并避免过拟合。
- en: 'The max pooling layer does this simply by looking at each subset of the input
    passed to it, and throwing out all but the maximum value in the subset. Let''s
    take a look at an example to see what this means. Assume that our input to the
    max pooling layer is a 4 x 4 tensor (a tensor is just an n-dimensional array,
    such as those output by a convolutional layer), and we are using a 2 x 2 max pooling
    layer. The following diagram illustrates the **Max Pooling** operation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化层通过查看传递给它的每个输入子集，并丢弃子集中除最大值以外的所有值，来实现这一点。让我们通过一个例子来看看这意味着什么。假设我们输入到最大池化层的是一个4
    x 4的张量（张量只是一个n维数组，例如卷积层输出的那种），我们使用一个2 x 2的最大池化层。下面的图示展示了**最大池化**操作：
- en: '![](img/84ff5c56-a00e-4df8-a0ac-bf443474f5c9.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84ff5c56-a00e-4df8-a0ac-bf443474f5c9.png)'
- en: As we can see from the preceding diagram, **Max Pooling** simply looks at each
    2 x 2 region of the input, and discards all but the maximum value in that region
    (boxed up in the preceding diagram). This effectively halves the height and width
    of the original input, reducing the number of parameters before passing it to
    the next layer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从前面的图示中看到的，**最大池化**只是查看输入的每个2 x 2区域，并丢弃该区域中除了最大值以外的所有值（前面的图示中已框出）。这实际上将原始输入的高度和宽度减半，在传递到下一层之前减少了参数的数量。
- en: Basic architecture of CNNs
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的基本架构
- en: We have seen the basic building blocks of CNNs in the previous section. Now,
    we'll put these building blocks together and see what a complete CNN looks like.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中已经看到CNN的基本构建块。现在，我们将这些构建块组合在一起，看看完整的CNN是什么样子。
- en: CNNs are almost always stacked together in a block of convolution and pooling
    pattern. The activation function used for the convolution layer is usually ReLU,
    as discussed in the previous chapters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）通常将卷积层和池化层按块堆叠在一起。卷积层使用的激活函数通常是ReLU，正如前几章所讨论的那样。
- en: 'The following diagram shows the first few layers in a typical CNN, made up
    of a series of convolution and pooling layers:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了典型卷积神经网络（CNN）中的前几层，包含了一系列的卷积层和池化层：
- en: '![](img/033458e4-2f17-4317-8f90-2b3f9d981189.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/033458e4-2f17-4317-8f90-2b3f9d981189.png)'
- en: The final layers in a CNN will always be **Fully Connected** layers (dense layers)
    with a sigmoid or softmax activation function. Note that the sigmoid activation
    function is used for binary classification problems, whereas the softmax activation
    function is used for multiclass classification problems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的最终层将始终是**全连接**层（密集层），并采用sigmoid或softmax激活函数。请注意，sigmoid激活函数用于二分类问题，而softmax激活函数则用于多类分类问题。
- en: 'The **Fully Connected** layer is identical to those that we have seen in the
    first two chapters: [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101* and [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml),
    *Diabetes Prediction with Multilayer Perceptrons*. At this point, you might be
    wondering what is the rationale for placing the fully connected layer at the end
    of a CNN? In CNNs, the early layers learn and extract the characteristic features
    of the data they are trying to predict. For example, we have seen how a convolutional
    layer learns the characteristic spatial features of Os and Xs. The convolutional
    layers then pass this information on to the fully connected layers, which then
    learn how to make accurate predictions, just like in an MLP.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**全连接**层与我们在前两章中看到的相同：[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)，*机器学习与神经网络基础*，以及[第2章](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml)，*多层感知机预测糖尿病*。此时，你可能会想，为什么要将全连接层放在CNN的末尾呢？在CNN中，早期的层学习并提取它们试图预测的数据的特征。例如，我们已经看到卷积层如何学习Os和Xs的空间特征。卷积层将这些信息传递给全连接层，而全连接层则学习如何进行准确预测，就像在多层感知机（MLP）中一样。'
- en: Essentially, the early layers of a CNN are responsible for identifying the characteristic
    spatial features, and the fully connected layers at the end are responsible for
    making predictions. The implication of this is significant. Instead of handcrafting
    features (that is, day of week, distance, and so on) for the machine learning
    algorithm, as we did in the previous chapter, [Chapter 3](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml),
    *Predicting Taxi Fares with Deep Feedforward Nets*, we're simply providing all
    the data to the CNN as it is. The CNN then automatically learns the best characteristic
    features to differentiate the classes. This is true AI!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，CNN的早期层负责识别特征性的空间特征，而末端的全连接层则负责进行预测。这一点的含义非常重要。与我们在上一章中为机器学习算法手动创建特征（例如，星期几、距离等）不同，[第3章](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml)，*利用深度前馈神经网络预测出租车费用*，我们只需将所有数据原样提供给CNN。然后，CNN会自动学习最佳的特征，以区分不同的类别。这才是真正的人工智能！
- en: A review of modern CNNs
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代CNN的回顾
- en: Now that we've seen the basic architecture of CNNs, let's take a look at modern,
    state-of-the-art CNNs. We'll do a walk-through of the evolution of CNNs, and see
    how they have changed over the years. We'll not go into the technical and mathematical
    details behind the implementation. Instead, we'll provide an intuitive overview
    of some of the most important CNNs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了CNN的基本架构，让我们来看一下现代、最先进的CNN。我们将回顾CNN的发展历程，看看它们是如何随着时间的推移而变化的。我们不会深入讨论实现背后的技术和数学细节，而是提供一些最重要CNN的直观概述。
- en: LeNet (1998)
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LeNet（1998）
- en: The first CNN was developed by Yann LeCun in 1998, with the architecture known
    as LeNet. LeCun was the first to prove that CNNs were effective in image recognition,
    particularly in the domain of handwritten digits recognition. However, throughout
    the 2000s, few scientists managed to build on the work done by LeCun and there
    were few breakthroughs in CNNs (and AI in general).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一款CNN是由Yann LeCun于1998年开发的，架构被称为LeNet。LeCun是第一个证明CNN在图像识别中有效的人，特别是在手写数字识别领域。然而，在2000年代，少数科学家能够在LeCun的工作基础上进行拓展，且CNN（以及人工智能整体）并没有出现显著突破。
- en: AlexNet (2012)
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlexNet（2012）
- en: As we mentioned earlier, AlexNet was developed by Alex Krizhevsky et al. and
    it was used to win the ILSVRC in 2012\. AlexNet was built on the same principles
    as LeNet, although AlexNet used a much deeper architecture. The overall number
    of trainable parameters in AlexNet is around 60 million, over 1,000 times more
    than LeNet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，AlexNet是由Alex Krizhevsky等人开发的，并且它在2012年赢得了ILSVRC。AlexNet基于与LeNet相同的原理，尽管AlexNet采用了更深的架构。AlexNet中的可训练参数总数约为6000万个，是LeNet的1000倍以上。
- en: VGG16 (2014)
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VGG16（2014）
- en: VGG16 was developed by Oxford's **Visual Geometry Group** (**VGG**) and it was
    considered to be a very important neural network. VGG16 was one of the first CNNs
    to deviate from large filter sizes, instead using a convolution filter size of
    3 x 3.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16由牛津大学的**视觉几何组**（**VGG**）开发，并且被认为是非常重要的神经网络。VGG16是最早偏离大滤波器尺寸的CNN之一，它采用了3
    x 3的卷积滤波器尺寸。
- en: VGG16 finished second in the image recognition task in the ILSVRC in 2014\.
    A downside to VGG16 is that there are many more parameters to be trained, leading
    to a significant training time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16在2014年ILSVRC的图像识别任务中获得了第二名。VGG16的一个缺点是需要训练的参数更多，导致训练时间显著增加。
- en: Inception (2014)
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception（2014）
- en: The Inception network was developed by researchers from Google and it won the
    ILSVRC in 2014\. The guiding principle for the Inception network was to provide
    highly accurate predictions efficiently. Google's interest was to create a CNN
    that could be trained and deployed in real time across their network of servers.
    To do that, the researchers developed something known as the Inception module,
    that vastly improved training time while maintaining its accuracy. In fact, in
    the 2014 ILSVRC, the Inception network managed to achieve a higher accuracy than
    VGG16, despite having far fewer parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Inception网络由谷歌的研究人员开发，并且在2014年赢得了ILSVRC。Inception网络的指导原则是高效地提供高度准确的预测。谷歌的兴趣是创建一种能够实时训练并在其服务器网络上部署的CNN。为此，研究人员开发了一种叫做Inception模块的技术，大大提高了训练速度，同时保持了准确性。事实上，在2014年ILSVRC中，Inception网络凭借较少的参数实现了比VGG16更高的准确率。
- en: The Inception network has been continuously improved upon. At the time of writing,
    the latest Inception network is at its 4th version (commonly known as Inception-v4).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Inception网络已不断改进。到本文写作时，最新的Inception网络已是其第四版本（通常称为Inception-v4）。
- en: ResNet (2015)
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet（2015）
- en: The **residual neural network** (**ResNet**) was introduced by Kaiming He et
    al. at the 2015 ILSVRC (by now, you should notice that this competition is extremely
    important for neural networks and computer vision, and new state-of-the-art techniques
    are revealed during the annual competition).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差神经网络**（**ResNet**）是由Kaiming He等人于2015年ILSVRC上提出的（到现在，你应该注意到这个竞赛对神经网络和计算机视觉至关重要，每年竞赛期间都会揭示新的最先进技术）。'
- en: The salient feature of ResNet was the residual block technique, which allowed
    the neural network to be deeper while keeping the number of parameters moderate.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet的显著特点是残差块技术，它使神经网络能够更深，同时保持参数数量适中。
- en: Where we stand today
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们今天所处的位置
- en: As we have seen, CNNs have progressed and improved exponentially in the past
    few years. In fact, recent CNNs can outperform humans at certain image recognition
    tasks. The recurring theme in recent years is to use innovative techniques to
    improve model performance, while preserving the model complexity. Clearly, the
    speed of the neural network is just as important as the accuracy.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，近年来CNN已经取得了指数级的进展和改进。事实上，最近的CNN在某些图像识别任务上能够超过人类。近年来的一个反复主题是使用创新技术提升模型性能，同时保持模型的复杂性。显然，神经网络的速度与准确性同样重要。
- en: The cats and dogs dataset
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 猫狗数据集
- en: Now that we understand the theory behind CNNs, let's dive into data exploration.
    The cats and dogs dataset is provided by Microsoft. The instructions for the downloading
    and setting up of the dataset can be found in the *Technical requirements* section
    of this chapter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了CNN背后的理论，让我们深入探讨数据探索。猫狗数据集由微软提供。下载和设置数据集的说明可以在本章的*技术要求*部分找到。
- en: 'Let''s plot the images to better understand the kind of data we''re working
    with. To do that, we can simply run the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制图像，更好地理解我们正在处理的数据类型。为此，我们只需运行以下代码：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We''ll see the following output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/7fb5749e-b73c-4362-ae4f-32c2e59ad805.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fb5749e-b73c-4362-ae4f-32c2e59ad805.png)'
- en: 'We can make some observations about our data:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对数据做出一些观察：
- en: The images have different dimensions.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些图像具有不同的尺寸。
- en: The subjects (cat/dog) are mostly centered in the image.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些物体（猫/狗）大多数情况下位于图像的中心。
- en: The subjects (cat/dog) have different orientations, and they may be occluded
    in the image. In other words, there's no guarantee that we'll always see the tail
    of the cat in the image.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些物体（猫/狗）有不同的方向，而且它们可能在图像中被遮挡。换句话说，并不能保证我们总是能在图像中看到猫的尾巴。
- en: 'Now, let''s do the same for the dog images:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对狗的图像做同样的操作：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''ll see the following output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如下输出：
- en: '![](img/6506c687-6339-4c5a-ac97-2cb2ba0e5cea.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6506c687-6339-4c5a-ac97-2cb2ba0e5cea.png)'
- en: Managing image data for Keras
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理 Keras 的图像数据
- en: One common problem encountered in neural network projects for image classification
    is that most computers do not have sufficient RAM to load the entire set of data
    into memory. Even for relatively modern and powerful computers, it would be far
    too slow to load the entire set of images into memory and to train a CNN from
    there.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类的神经网络项目中，一个常见的问题是大多数计算机的内存不足，无法将整个数据集加载到内存中。即便是相对现代且强大的计算机，将整个图像集加载到内存并从中训练卷积神经网络（CNN）也会非常缓慢。
- en: To alleviate this problem, Keras provides a useful `flow_from_directory` method
    that takes as an input the path to the images, and generates batches of data as
    output. The batches of data are loaded into memory, as required before model training.
    This way, we can train a deep neural network on a huge number of images without
    worrying about memory issues. Furthermore, the `flow_from_directory` method allows
    us to perform image preprocessing steps such as resizing and other image augmentation
    techniques by simply passing an argument. The `flow_from_directory` method would
    then perform the necessary image preprocessing steps in real time before passing
    the data for model training.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，Keras 提供了一个有用的 `flow_from_directory` 方法，该方法接受图像路径作为输入，并生成数据批次作为输出。数据批次会在模型训练之前根据需要加载到内存中。这样，我们可以在大量图像上训练深度神经网络，而无需担心内存问题。此外，`flow_from_directory`
    方法通过简单地传递参数，允许我们执行图像预处理步骤，例如调整图像大小和其他图像增强技术。该方法随后会在实时处理图像数据之前，执行必要的图像预处理步骤。
- en: 'To do all these, there are certain schemas for file and folder management that
    we must abide by, in order for `flow_from_directory` to work. In particular, we
    are required to create subdirectories for training and testing data, and within
    the training and testing subdirectories, we need to further create one subdirectory
    per class. The following diagram illustrates the required folder structure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这些操作，我们必须遵循某些文件和文件夹管理的方案，以确保 `flow_from_directory` 能正常工作。特别是，我们需要为训练和测试数据创建子目录，并且在训练和测试子目录中，我们需要进一步为每个类别创建一个子目录。以下图示说明了所需的文件夹结构：
- en: '![](img/75e11cdf-ef7e-43d4-a7b7-015930412358.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75e11cdf-ef7e-43d4-a7b7-015930412358.png)'
- en: The `flow_from_directory` method would then infer the class of the images from
    the folder structure.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`flow_from_directory` 方法随后会根据文件夹结构推断图像的类别。'
- en: 'The raw data is provided in a `Cat` and `Dog` folder, without separation of
    training and testing data. Therefore, we need to split the data into a `Train`
    and `Test` folder as per the preceding schema. To do that, we need to perform
    the following steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据提供在 `Cat` 和 `Dog` 文件夹中，没有区分训练数据和测试数据。因此，我们需要按照之前的方案将数据拆分为 `Train` 和 `Test`
    文件夹。为此，我们需要执行以下步骤：
- en: Create `/Train/Cat`, `/Train/Dog`, `/Test/Cat`, and `/Test/Dog` folders.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `/Train/Cat`、`/Train/Dog`、`/Test/Cat` 和 `/Test/Dog` 文件夹。
- en: Randomly assign 80% of the the images as train images and 20% of the images
    as test images.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机将 80% 的图像分配为训练图像，20% 的图像分配为测试图像。
- en: Copy those images into the respective folders.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些图像复制到相应的文件夹中。
- en: 'We have provided a helper function in `utils.py` to do these steps. We simply
    need to invoke the function, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `utils.py` 中提供了一个辅助函数来执行这些步骤。我们只需要调用该函数，如下所示：
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you run into an error while executing this code block, with the error message
    ImportError: No Module Named Piexif, it means that you have not installed Piexif
    in your Python virtual environment. This chapter requires an additional library
    for image processing. To download Piexif, please follow the instructions in the
    *Technical requirements section* at the start of this chapter.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '如果在执行此代码块时遇到错误，错误信息为 ImportError: No Module Named Piexif，这意味着你尚未在 Python 虚拟环境中安装
    Piexif。本章节需要一个额外的图像处理库。要下载 Piexif，请遵循本章节开始时的*技术要求部分*中的说明。'
- en: Great! Our images are now placed in the appropriate folders for Keras.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们的图像现在已经放置在Keras所需的适当文件夹中了。
- en: Image augmentation
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像增强
- en: 'Before we start building our CNN, let''s take a look at image augmentation,
    which is an important technique in image classification projects. Image augmentation
    is the creation of additional training data by making minor alterations to images
    in certain ways in order to create new images. For example, we can do the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建CNN之前，先来看看图像增强，它是图像分类项目中一个重要的技术。图像增强是通过对图像进行某些方式的微小修改，创造出额外的训练数据，从而生成新图像。例如，我们可以做以下操作：
- en: Image rotation
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像旋转
- en: Image translation
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像平移
- en: Horizontal flip
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平翻转
- en: Zooming into the image
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像进行缩放
- en: The motivation for image augmentation is that CNNs require a huge amount of
    training data before they can generalize well. However, it is often difficult
    to collect data, more so for images. With image augmentation, we can artificially
    create new training data based on the existing images.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图像增强的动机是，CNN需要大量的训练数据才能很好地泛化。然而，收集数据常常很困难，尤其是对于图像来说。通过图像增强，我们可以基于现有图像人工地创造新的训练数据。
- en: 'As always, Keras provides a handy `ImageDataGenerator` class to help us easily
    perform image augmentation. Let''s create a new instance of the class:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，Keras提供了一个方便的`ImageDataGenerator`类，帮助我们轻松地执行图像增强。让我们创建这个类的一个新实例：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let''s use it to augment a randomly selected image from the `/Train/Dog/`
    folder. Then, we can plot it to compare the augmented images with the original
    image. We can do this by running the following code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们用它来增强从`/Train/Dog/`文件夹中随机选择的一张图像。然后，我们可以绘制它，将增强后的图像与原始图像进行比较。我们可以通过运行以下代码来实现：
- en: '[PRE11]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll see the following output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如下输出：
- en: '![](img/a1821f8c-9640-4c89-bfba-1d055c090322.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1821f8c-9640-4c89-bfba-1d055c090322.png)'
- en: As we can see, each augmented image is randomly shifted or rotated by a certain
    amount as controlled by the arguments passed into the `ImageDataGenerator` class.
    These augmented images will provide supplemental training data for our CNN, increasing
    the robustness of our model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，每个增强后的图像都会按照传递给`ImageDataGenerator`类的参数随机地平移或旋转。这些增强后的图像将为我们的CNN提供补充的训练数据，提高我们模型的鲁棒性。
- en: Model building
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: We're finally ready to start building our CNN in Keras. In this section, we'll
    take two different approaches to model building. First, we'll start by building
    a relatively simple CNN consisting of a few layers. We'll take a look at the performance
    of the simple model, and discuss its pros and cons. Next, we'll use a model that
    was considered state-of-the art just a few years ago—the VGG16 model. We'll see
    how we can leverage on the pre-trained weights to adapt the VGG16 model for cats
    versus dogs image classification.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好开始在Keras中构建CNN了。在这一部分，我们将采用两种不同的方法来构建模型。首先，我们从构建一个包含几层的相对简单的CNN开始。我们将查看简单模型的性能，并讨论它的优缺点。接下来，我们将使用一个几年前被认为是最先进的模型——VGG16模型。我们将看看如何利用预训练的权重将VGG16模型调整用于猫狗图像分类。
- en: Building a simple CNN
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个简单的CNN
- en: 'In an earlier section, we showed how the fundamental building blocks of a CNN
    consist of a series of convolutional and pooling layers. In this section, we''re
    going to build a basic CNN consisting of this repeating pattern, as shown in the
    following diagram:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们展示了CNN的基本构建模块，它由一系列卷积层和池化层组成。在这一部分，我们将构建一个由这些重复模式组成的基本CNN，如下图所示：
- en: '![](img/c38754ca-f2ea-425a-b7a6-1fe0f2f5074e.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c38754ca-f2ea-425a-b7a6-1fe0f2f5074e.png)'
- en: 'This basic CNN consists of two repeated blocks of **Convolution** and **Max**
    **Pooling**, following by two **Fully Connected** layers. As discussed in a previous
    section, the convolution and max pooling layers are responsible for learning the
    spatial characteristics of the classes (for example, identifying the ears of cats),
    whereas the **Fully Connected** layers learn to make predictions using these spatial
    characteristics. We can thus represent the architecture of our basic CNN in another
    manner (we shall see why it is useful to visualize our neural network in this
    manner in the next subsection):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基础CNN由两个重复的**卷积**和**最大池化**块组成，然后是两个**全连接**层。正如前面部分所讨论的，卷积和最大池化层负责学习类的空间特征（例如，识别猫的耳朵），而**全连接**层则使用这些空间特征进行预测。因此，我们可以以另一种方式表示我们基本CNN的架构（我们将在下一小节中看到以这种方式可视化神经网络的好处）：
- en: '![](img/79d1c8a6-e093-45eb-b873-5c137649b1af.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79d1c8a6-e093-45eb-b873-5c137649b1af.png)'
- en: 'Building a CNN is similar to building an MLP or a feedforward neural network,
    as we''ve done in the previous chapters. We''ll start off by declaring a new `Sequential`
    model instance:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 构建CNN类似于构建多层感知器（MLP）或前馈神经网络，就像我们在前几章中所做的那样。我们将通过声明一个新的`Sequential`模型实例开始：
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Before we add any convolutional layers, it is useful to think about the hyperparameters
    that we are going to use. For a CNN, there are several hyperparameters:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加任何卷积层之前，考虑我们将使用的超参数是非常有用的。对于卷积神经网络（CNN），有几个超参数：
- en: '**Convolutional layer filter size**: Most modern CNNs use a small filter size
    of `3` x `3`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层滤波器大小**：大多数现代CNN使用较小的滤波器大小`3` x `3`。'
- en: '**Number of filters**: Let''s use a filter number of `32`. This is a good balance
    between speed and performance.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器数量**：我们将使用`32`个滤波器。这个数量在速度和性能之间达到了良好的平衡。'
- en: '**Input size**: As we''ve seen in an earlier section, the input images have
    different sizes, with their width and height approximately 150 px. Let''s use
    an input size of `32` x `32` pixels. This compresses the original image, which
    can result in some information loss, but helps to speed up the training of our
    neural network.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入大小**：正如我们在前面部分看到的，输入图像的大小各不相同，宽度和高度大约为150像素。我们将使用`32` x `32`像素的输入大小。这会压缩原始图像，可能会导致一些信息丢失，但有助于加速神经网络的训练。'
- en: '**Max pooling size**: A common max pooling size is `2` x `2`. This will halve
    the input layer dimensions.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化大小**：常见的最大池化大小为`2` x `2`。这将把输入层的维度减半。'
- en: '**Batch size**: This corresponds to the number of training samples to use in
    each mini batch during gradient descent. A large batch size results in more accurate
    training but longer training time and memory usage. Let''s use a batch size of
    `16`.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：这对应于每个小批量中使用的训练样本数。在梯度下降中，较大的批量大小会导致更准确的训练，但训练时间更长，内存使用量也更大。我们将使用`16`的批量大小。'
- en: '**Steps per epoch**: This is the number of iterations in each training epoch.
    Typically, this is equal to the number of training samples divided by the batch
    size.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每轮步数**：这是每个训练周期中的迭代次数。通常，这个值等于训练样本数除以批量大小。'
- en: '**Epochs**: The number of epochs to train our data. Note that, in neural networks,
    the number of epochs refers to the number of times the model sees each training
    sample during training. Multiple epochs are usually needed, as gradient descent
    is an iterative optimization method. Let''s train our model for `10` epochs. This
    means that each training sample will be passed to the the model 10 times during
    training.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数**：用于训练我们数据的轮数。请注意，在神经网络中，轮数指的是模型在训练过程中每次看到每个训练样本的次数。通常需要多个轮次，因为梯度下降是一种迭代优化方法。我们将训练模型`10`轮。这意味着每个训练样本将在训练过程中传递给模型10次。'
- en: 'Let''s declare variables for these hyperparameters so that they are constant
    throughout our code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这些超参数声明变量，以便在代码中保持一致：
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now add the first convolutional layer, with `32` filters, each of size
    (`3` x `3`):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以添加第一层卷积层，使用`32`个滤波器，每个滤波器的大小为(`3` x `3`)：
- en: '[PRE14]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we add a max pooling layer:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个最大池化层：
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is the basic convolution-pooling pattern of our CNN. Let''s repeat this
    once more according to our model architecture:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们CNN的基本卷积-池化模式。根据我们的模型架构，我们再重复一遍：
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We are now done with the convolution and pooling layers. Before we move on to
    the fully connected layers, we need to flatten its input. `Flatten` is a function
    in Keras that transforms a multidimensional vector into a single dimensional vector.
    For example, if the vector is of shape (5,5,3) before passing to `Flatten`, the
    output vector will be of shape (75) after passing to `Flatten`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在完成了卷积层和池化层的设置。在继续添加全连接层之前，我们需要将输入进行展平。`Flatten`是Keras中的一个函数，它将一个多维向量转换为一个一维向量。例如，如果向量的形状是(5,5,3)，在传递给`Flatten`之前，输出向量将变成形状为(75)的向量。
- en: 'To add a `Flatten` layer, we simply run the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加一个`Flatten`层，我们只需运行以下代码：
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now add a fully connected layer with `128` nodes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以添加一个全连接层，节点数为`128`：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Before we add our last fully connected layer, it is a good practice to add
    a dropout layer. The dropout layer randomly sets a certain fraction of its input
    to 0\. This helps to reduce overfitting, by ensuring that the model does not place
    too much emphasis on certain weights:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加最后一层全连接层之前，添加一个Dropout层是一个好习惯。Dropout层会随机将输入的一部分设置为0，从而帮助减少过拟合，确保模型不会过度依赖某些权重：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We add one last fully connected layer to our model:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为模型添加了最后一个全连接层：
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that the last fully connected layer should have only one node, as we're
    doing binary classification (cat or dog) in this project.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最后一层全连接层应该只有一个节点，因为我们在这个项目中进行的是二分类（猫或狗）。
- en: 'We''ll compile our model using the `adam` optimizer. The `adam` optimizer is
    a generalization of the **stochastic gradient descent** (**SGD**) algorithm that
    we''ve seen in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101* and it is widely used to train CNNs. The loss
    function is `binary_crossentropy` since we''re doing a binary classification:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`adam`优化器编译我们的模型。`adam`优化器是**随机梯度下降（SGD）**算法的一个泛化，它在[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)《机器学习与神经网络101》中介绍过，*并且被广泛用于训练CNN模型*。损失函数是`binary_crossentropy`，因为我们进行的是二分类：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In general, we use `binary_crossentropy` for binary classification problems
    and `categorical_crossentropy` for multiclass classification problems.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们对于二分类问题使用`binary_crossentropy`，对于多分类问题使用`categorical_crossentropy`。
- en: 'We''re now ready to train our CNN. Notice that we have not loaded any of the
    data into memory. We''ll use the `ImageDataGenerator` and `flow_from_directory`
    method to train our model in real time, which loads batches of the dataset into
    memory only as required:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的CNN。请注意，我们并没有将任何数据加载到内存中。我们将使用`ImageDataGenerator`和`flow_from_directory`方法实时训练我们的模型，该方法只在需要时将数据集的批次加载到内存中：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will start the training and once it is complete, you will see the following
    output:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将开始训练，一旦完成，你将看到以下输出：
- en: '![](img/0b1c40fb-6e06-4ea1-9067-5e87e701013c.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b1c40fb-6e06-4ea1-9067-5e87e701013c.png)'
- en: We can clearly see that the loss decreases while the accuracy increases with
    each epoch.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，随着每个周期的进行，损失值在减少，而准确率在提高。
- en: 'Now that our model is trained, let''s evaluate it on the testing set. We''ll
    create a new `ImageDataGenerator` and call `flow_from_directory` on the images
    in the `test` folder:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经训练完成，接下来让我们在测试集上评估它。我们将创建一个新的`ImageDataGenerator`，并对`test`文件夹中的图像调用`flow_from_directory`：
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We''ll get the following output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下输出：
- en: '![](img/93b08d19-3f6c-493a-b10e-7aff0f2df243.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93b08d19-3f6c-493a-b10e-7aff0f2df243.png)'
- en: We obtained an accuracy of 80%! That's pretty impressive considering that we
    only used a basic CNN. This shows the power of CNNs; we obtained an accuracy close
    to human performance from just a few lines of code.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了80%的准确率！考虑到我们仅使用了一个基础的CNN，这相当令人印象深刻。这显示了CNN的强大功能；我们仅用几行代码就得到了接近人类表现的准确率。
- en: Leveraging on pre-trained models using transfer learning
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用迁移学习的预训练模型
- en: Can we take our model further? Can we achieve close to 90%, reaching human level
    performance? As we shall see in this section, we can obtain better performance
    by leveraging on transfer learning.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能将模型推得更远吗？我们能达到接近90%的准确率，接近人类水平的表现吗？正如我们在本节中将看到的那样，通过利用迁移学习，我们可以获得更好的性能。
- en: 'Transfer learning is a technique in machine learning where a model trained
    for a certain task is modified to make predictions for another task. For example,
    we may use a model trained to classify cars to classify trucks instead, since
    they are similar. In the context of CNN, transfer learning involves freezing the
    convolution-pooling layers, and only retraining the final fully connected layers.
    The following diagram illustrates this process:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是机器学习中的一种技术，它通过将为某个任务训练的模型修改为用于另一个任务的预测。例如，我们可以使用训练好的汽车分类模型来分类卡车，因为它们相似。在卷积神经网络（CNN）的背景下，迁移学习涉及冻结卷积池化层，只重新训练最后的全连接层。以下图示说明了这一过程：
- en: '![](img/bc182395-a155-4b17-bdd9-796bd417fb1c.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc182395-a155-4b17-bdd9-796bd417fb1c.png)'
- en: How does transfer learning work? Intuitively, the purpose of the convolution
    and pooling layers is to learn the spatial characteristics of the classes. We
    can therefore reuse these layers since the spatial characteristics are similar
    in both tasks. We just need to retrain the final fully connected layers to re-purpose
    the neural network to make predictions for the new class. Naturally, a crucial
    requirement for transfer learning is that tasks A and B must be similar to one
    another.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是如何工作的？直观地说，卷积层和池化层的目的是学习类别的空间特征。因此，我们可以重用这些层，因为这两个任务的空间特征是相似的。我们只需要重新训练最后的全连接层，将神经网络重新定向到新类别的预测。自然，迁移学习的一个关键要求是任务A和任务B必须相似。
- en: In this section, we're going to re-purpose the VGG16 model to make predictions
    on images of cats and dogs. The VGG16 model was originally developed for the ILSVRC,
    which required the model to make a 1,000 class multiclass classification. Among
    the 1,000 classes are specific breeds of cats and dogs. In other words, VGG16
    knows how to recognize specific breeds of cats and dogs, and not just cats and
    dogs in general. It is therefore a viable approach to use transfer learning using
    the VGG16 model for our cats and dogs image classification problem.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新利用 VGG16 模型来对猫狗图像进行预测。VGG16 模型最初是为 ILSVRC 开发的，需要模型进行 1,000 类的多类别分类。这
    1,000 类中包含了特定品种的猫和狗。换句话说，VGG16知道如何识别特定品种的猫和狗，而不仅仅是猫和狗。因此，利用 VGG16 模型进行迁移学习来解决我们的猫狗图像分类问题是一种可行的方法。
- en: 'The VGG16 model and its trained weights are provided directly in Keras. Let''s
    create a new `VGG16` model, as shown in the following code:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 模型及其训练权重在 Keras 中已直接提供。我们可以按照以下代码创建一个新的`VGG16`模型：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that we used `include_top=False` when we created a new VGG16 model. This
    argument tells Keras not to import the fully connected layers at the end of the
    VGG16 network.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在创建新的 VGG16 模型时使用了`include_top=False`。这个参数告诉 Keras 不要导入 VGG16 网络末尾的全连接层。
- en: 'We''re now going to freeze the rest of the layers in the VGG16 model, since
    we''re not going to retrain them from scratch. We can freeze the layers by running
    the following code snippet:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将冻结 VGG16 模型中其余的层，因为我们不打算从头开始重新训练它们。我们可以通过运行以下代码片段来冻结这些层：
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we''re going to add a fully connected layer with `1` node right at the
    end of the neural network. The syntax to do this is slightly different, since
    the VGG16 model is not a Keras `Sequential` model that we''re used to. In any
    case, we can add the layers by running the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在神经网络的最后添加一个带有`1`个节点的全连接层。实现这一点的语法稍有不同，因为 VGG16 模型不是我们习惯的 Keras `Sequential`
    模型。无论如何，我们可以通过运行以下代码来添加这些层：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is just a manual way of adding layers in Keras, which the `.``add()` function
    in `Sequential` model has simplified for us so far. The rest of the code is similar
    to what we have seen in the previous section. We declare a training data generator,
    and we train the model (only the newly added layers) by calling `flow_from_directory()`.
    Since we only need to train the final layer, we''ll just train the model for `3`
    epochs:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是手动在 Keras 中添加层的方式，而`.``add()`函数在`Sequential`模型中已经简化了这个过程。其余的代码与我们在前一节中看到的类似。我们声明一个训练数据生成器，并通过调用`flow_from_directory()`来训练模型（仅训练新添加的层）。由于我们只需要训练最后一层，我们将仅训练模型`3`个周期：
- en: Caution
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following code block takes around an hour to run if you are not running
    Keras on a GPU (graphics card). If the code takes too long to run on your computer,
    you may reduce the `INPUT_SIZE` parameter to speed up model training. However,
    note that this will lower the accuracy of your model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有在 GPU（显卡）上运行 Keras，以下代码块大约需要一个小时才能完成。如果代码在您的计算机上运行得太慢，您可以减少`INPUT_SIZE`参数来加速模型训练。然而，请注意，这样会降低模型的准确性。
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We''ll get the following output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/725f105b-08fe-45b7-b095-a9887fd3cf0e.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/725f105b-08fe-45b7-b095-a9887fd3cf0e.png)'
- en: 'The training accuracy doesn''t look much different to the basic CNN in the
    previous section. This is expected, since both neural networks do really well
    in the training set. However, the testing accuracy is ultimately the metric which
    we will use to evaluate the performance of our model. Let''s see how well it does
    on the testing set:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 训练准确度看起来与前一节中的基本 CNN 模型差别不大。这是预期的，因为两个神经网络在训练集上表现都非常好。然而，最终我们将使用测试准确度来评估模型的性能。让我们看看它在测试集上的表现如何：
- en: '[PRE28]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ll see the following output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/a0aabb03-fbb3-4ce4-a0b4-a43a08c6e82a.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0aabb03-fbb3-4ce4-a0b4-a43a08c6e82a.png)'
- en: That's amazing! By making use of transfer learning, we managed to obtain a testing
    accuracy of 90.5%. Note that the training time here is much shorter than training
    a VGG16 model from scratch (it would probably take days to train a VGG16 model
    from scratch, even with a powerful GPU!), since we are only training the last
    layer. This shows that we can leverage on a pre-trained state-of-the art model
    like VGG16 to make predictions for our own projects.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 真了不起！通过使用迁移学习，我们成功地获得了90.5%的测试准确率。请注意，这里的训练时间远远短于从头开始训练VGG16模型的时间（即使有强大的GPU，也可能需要几天才能从头训练一个VGG16模型！），因为我们只训练了最后一层。这表明，我们可以利用像VGG16这样的预训练的最先进模型来为我们自己的项目进行预测。
- en: Results analysis
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果分析
- en: Let's take a deeper look into our results. In particular, we would like to know
    what kind of images our CNN does well in, and what kind of images it gets wrong.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地分析我们的结果。特别是，我们希望了解我们的CNN在哪些图像上表现良好，哪些图像会出错。
- en: Recall that the output of the sigmoid activation function in the last layer
    of our CNN is a list of values between 0 and 1 (one value/prediction per image).
    If the output value is < `0.5`, then the prediction is class 0 (that is, cat)
    and if the output value is >= `0.5`, then the prediction is class 1 (that is,
    dog). Therefore, an output value close to `0.5` means that the model isn't so
    sure, while an output value very close to `0.0` or `1.0` means that the model
    is very sure about its predictions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的CNN最后一层的sigmoid激活函数的输出是一个0到1之间的值列表（每个图像一个值/预测）。如果输出值< `0.5`，那么预测为类别0（即猫），如果输出值>=
    `0.5`，则预测为类别1（即狗）。因此，接近`0.5`的输出值意味着模型不太确定，而非常接近`0.0`或`1.0`的输出值则表示模型对其预测非常确信。
- en: 'Let''s run through the images in the testing set one by one, using our model
    to make predictions on the class of the image, and classify the images according
    to three categories:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一运行测试集中的图像，使用我们的模型对图像的类别进行预测，并将图像按三类进行分类：
- en: '**Strongly right predictions**: The model predicted these images correctly,
    and the output value is `> 0.8` or `< 0.2`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强烈正确的预测**：模型正确预测了这些图像，输出值为`> 0.8`或`< 0.2`'
- en: '**Strongly wrong predictions**: The model predicted these images wrongly, and
    the output value is `> 0.8` or `< 0.2`'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强烈错误的预测**：模型错误地预测了这些图像，输出值为`> 0.8`或`< 0.2`'
- en: '**Weakly wrong predictions**: The model predicted these images wrongly, and
    the output value is between `0.4` and `0.6`'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弱错误的预测**：模型错误预测了这些图像，输出值介于`0.4`和`0.6`之间'
- en: 'The following code snippet will do this for us:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将为我们完成此操作：
- en: '[PRE29]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s visualize the images from these three groups by randomly selecting `9`
    of the images in each group, and plot them on a 3 × 3 grid. The following helper
    function allows us to do that:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过随机选择每组中的`9`张图像，并将它们绘制在一个3×3的网格中，来可视化这三组图像。以下辅助函数可以帮助我们做到这一点：
- en: '[PRE30]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can now plot `9` randomly selected images from the strongly right predictions
    group:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以绘制`9`张从强烈正确预测组中随机选出的图像：
- en: '[PRE31]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We''ll see the following output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/6c6063b9-1286-45bc-aa15-7a2b8105ff50.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c6063b9-1286-45bc-aa15-7a2b8105ff50.png)'
- en: Selected images that have strong predictions, and are correct
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 选中的图像具有强烈的预测，并且预测正确
- en: No surprises there! These are almost classical images of cats and dogs. Notice
    that the pointy ears of cats and the dark eyes of dogs can all be seen in the
    preceding images. These characteristic features allow our CNN to easily identify
    them.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么意外！这些几乎是经典的猫狗图像。请注意，猫的尖耳朵和狗的黑眼睛在前面的图像中都有体现。这些特征使我们的CNN能够轻松地识别它们。
- en: 'Let''s now take a look at the strongly wrong predictions group:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下强烈错误预测组：
- en: '[PRE32]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We''ll get the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![](img/5c9e0e1f-4199-471b-8381-526474ead5ed.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c9e0e1f-4199-471b-8381-526474ead5ed.png)'
- en: Selected images that have strong predictions, but are wrong
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 选中的图像具有强烈的预测，但预测错误
- en: We notice a few commonalities among these strongly wrong predictions. The first
    thing we notice is that certain dogs do resemble cats with their pointy ears.
    Perhaps our neural network placed too much emphasis on the pointy ears and classified
    these dogs as cats. Another thing we notice is that some of the subjects were
    not facing the camera, making it really difficult to identify them. No wonder
    our neural network got them wrong.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到这些强烈错误预测中有一些共同点。首先，我们发现某些狗确实像猫一样有尖耳朵。也许我们的神经网络过分关注尖耳朵，将这些狗误分类为猫。另一个发现是，某些主体没有正对相机，这使得识别变得异常困难。难怪我们的神经网络将它们分类错误。
- en: 'Finally, let''s take a look at the weakly wrong predictions group:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看一下预测弱错误的图像组：
- en: '[PRE33]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We''ll get the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![](img/7d65f062-df15-4c19-9741-77903ddf6738.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d65f062-df15-4c19-9741-77903ddf6738.png)'
- en: Selected images that have weak predictions, and are wrong
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 选择预测较弱且错误的图像
- en: These images are ones that our model is on the fence with. Perhaps there is
    an equal number of characteristics to suggest that the object could be a dog or
    a cat. This is perhaps the most obvious with the images in the first row, where
    the puppies in the first row have a small frame like a cat, which could have confused
    the neural network.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像是我们的模型在分类时存在不确定性的情况。可能有相同数量的特征暗示对象既可能是狗也可能是猫。最明显的例子出现在第一行的图像中，第一行的小狗具有像猫一样的小框架，这可能使神经网络感到困惑。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we built a classifier that can predict whether an image contains
    a cat or a dog by using two different CNNs. We first went through the theory behind
    CNNs, and we understood that the fundamental building blocks of a CNN are the
    convolution, pooling, and fully connected layers. In particular, the front of
    the CNN consists of a block of convolution-pooling layers, repeated an arbitrary
    number of times. This block is responsible for identifying spatial characteristics
    in the images, which can be used to classify the images. The back of the CNN consists
    of fully connected layers, similar to an MLP. This block is responsible for making
    the final predictions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了一个分类器，通过使用两种不同的卷积神经网络（CNN）来预测图像中是否包含猫或狗。我们首先学习了CNN背后的理论，并理解了CNN的基本构建模块是卷积层、池化层和全连接层。特别地，CNN的前端由一块卷积-池化层组成，重复若干次。这个模块负责识别图像中的空间特征，这些特征可用于对图像进行分类。CNN的后端由全连接层组成，类似于多层感知机（MLP）。这个模块负责做出最终的预测。
- en: In the first CNN, we used a basic architecture that achieved 80% accuracy on
    the testing set. This basic CNN consists of two convolutional-max pooling layers,
    followed by two fully connected layers. In the second CNN, we used transfer learning
    to leverage on the pre-trained VGG16 network for our classification. We removed
    the final fully connected layer with 1,000 nodes in the pre-trained network, and
    added our own fully connected layer with one node (for our binary classification
    task). We managed to obtain an accuracy of 90% using the fine-tuned VGG16 model.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个CNN中，我们使用了一个基础架构，在测试集上达到了80%的准确率。这个基础CNN由两层卷积-最大池化层组成，后面接着两层全连接层。在第二个CNN中，我们使用了迁移学习，借助预训练的VGG16网络进行分类。我们移除了预训练网络中具有1,000个节点的最后一层全连接层，并添加了我们自己的全连接层（仅一个节点，用于二分类任务）。我们通过微调VGG16模型成功地获得了90%的准确率。
- en: Lastly, we visualized the images that our model did well in, as well as the
    images that our model struggled with. We saw that our model could not be certain
    when the subject is not facing the camera or when the subject has characteristics
    resembling both a cat and a dog (for example, a small puppy with pointy ears).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可视化了模型表现良好的图像和模型表现不佳的图像。我们发现，当主体没有正对相机时，或当主体具有既像猫又像狗的特征时，我们的模型无法确定。这一点在第一行的图像中尤为明显，第一行的小狗耳朵尖尖，可能让神经网络感到困惑。
- en: That concludes the chapter on using CNNs for image recognition. In the next
    chapter, [Chapter 5](16d0775b-23ec-456a-a57b-cba2e9da7570.xhtml), *Removing Noise
    from Images Using Autoencoders*, we'll use an autoencoder neural network to remove
    noise from images.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容到此为止，关于使用CNN进行图像识别的部分已结束。在下一章，[第5章](16d0775b-23ec-456a-a57b-cba2e9da7570.xhtml)，*使用自编码器去除图像噪声*，我们将使用自编码器神经网络去除图像中的噪声。
- en: Questions
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How are images represented in computers?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像是如何在计算机中表示的？
- en: Images are represented in computers as a group of pixels, with each pixel having
    its own intensity (value between 0 and 255). Color images have three channels
    (red, green, and blue) while grayscale images have only one channel.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图像在计算机中表示为一组像素，每个像素有自己的强度（值在 0 到 255 之间）。彩色图像有三个通道（红色、绿色和蓝色），而灰度图像只有一个通道。
- en: What are the fundamental building blocks of a CNN?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN 的基本构建模块是什么？
- en: All convolutional neural network consists of convolution layers, pooling layers,
    and fully connected layers.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 所有卷积神经网络由卷积层、池化层和全连接层组成。
- en: What is the role of the convolutional and pooling layers?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层和池化层的作用是什么？
- en: The convolutional and pooling layers are responsible for extracting spatial
    characteristics from the images. For example, when training a CNN to identify
    images of cats, one such spatial characteristic would be the pointy ears of cats.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和池化层负责从图像中提取空间特征。例如，在训练 CNN 识别猫的图像时，其中一个空间特征可能是猫的尖耳朵。
- en: What is the role of the fully connected layers?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全连接层的作用是什么？
- en: The fully connected layers are similar to the those in MLPs and feedforward
    neural networks. Their role is to use the spatial characteristics as input, and
    to output predicted classes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层类似于多层感知机（MLPs）和前馈神经网络中的全连接层。它们的作用是使用空间特征作为输入，并输出预测的类别。
- en: What is transfer learning, and how is it useful?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是迁移学习，它如何发挥作用？
- en: Transfer learning is a technique in machine learning where a model trained for
    a certain task is modified to make predictions for another task. Transfer learning
    allows us to leverage on state-of-the art models, such as VGG16, for our own purposes,
    with minimal training time.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种机器学习技术，其中为某个任务训练的模型被修改以预测另一个任务。迁移学习使我们能够利用最先进的模型，如 VGG16，来为我们的目的提供支持，并且训练时间最小化。
