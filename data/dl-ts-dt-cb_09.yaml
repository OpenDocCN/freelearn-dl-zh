- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Deep Learning for Time Series Anomaly Detection
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在时间序列异常检测中的应用
- en: In this chapter, we’ll delve into anomaly detection problems using time series
    data. This task involves detecting rare observations that are significantly different
    from most samples in a dataset. We’ll explore different approaches to tackle this
    problem, such as prediction-based methods or reconstruction-based methods. This
    includes using powerful methods such as **autoencoders** (**AEs**), **variational
    AEs** (**VAEs**), or **generative adversarial** **networks** (**GANs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将深入探讨使用时间序列数据进行异常检测问题。这个任务涉及到检测与数据集中大部分样本显著不同的稀有观测值。我们将探索不同的方法来解决这个问题，比如基于预测的方法或基于重构的方法。包括使用强大的方法，如**自编码器**（**AEs**）、**变分自编码器**（**VAEs**）或**生成对抗网络**（**GANs**）。
- en: By the end of this chapter, you’ll be able to define time series anomaly detection
    problems using different approaches with Python.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够使用不同的方法在 Python 中定义时间序列异常检测问题。
- en: 'The chapter covers the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下内容：
- en: Time series anomaly detection with **Autoregressive Integrated Moving** **Average**
    (**ARIMA**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**自回归积分滑动平均**（**ARIMA**）进行时间序列异常检测
- en: Prediction-based anomaly detection using **deep** **learning** (**DL**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于预测的异常检测使用**深度学习**（**DL**）
- en: Anomaly detection using a **long short-term memory** (**LSTM**) AE
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**长短期记忆**（**LSTM**）自编码器进行异常检测
- en: Building an AE using PyOD
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyOD 构建自编码器
- en: Creating a VAE for time series anomaly detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为时间序列异常检测创建 VAE
- en: Using GANs for time series anomaly detection
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GANs 进行时间序列异常检测
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The models developed in this chapter are based on different frameworks. First,
    we show how to develop prediction-based methods using the `statsforecast` and
    `neuralforecast` libraries. Other methods, such as an LSTM AE, will be explored
    using the PyTorch Lightning ecosystem. Finally, we’ll also use the PyOD library
    to create anomaly detection models based on approaches such as GANs or VAEs. Of
    course, we also rely on typical data manipulation libraries such as `pandas or
    NumPy.` The following list contains all the required libraries for this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中开发的模型基于不同的框架。首先，我们展示如何使用 `statsforecast` 和 `neuralforecast` 库开发基于预测的方法。其他方法，如
    LSTM 自编码器，将使用 PyTorch Lightning 生态系统进行探索。最后，我们还将使用 PyOD 库创建基于 GANs 或 VAEs 等方法的异常检测模型。当然，我们也依赖于典型的数据处理库，如
    `pandas` 或 NumPy。以下列表包含本章所需的所有库：
- en: '`scikit-learn` (1.3.2)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn` (1.3.2)'
- en: '`pandas` (2.1.3)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` (2.1.3)'
- en: NumPy (1.26.2)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy (1.26.2)
- en: '`statsforecast` (1.6.0)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsforecast` (1.6.0)'
- en: '`datasetsforecast` (0.08)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasetsforecast` (0.08)'
- en: '`0neuralforecast` (1.6.4)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0neuralforecast` (1.6.4)'
- en: '`torch` (2.1.1)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch` (2.1.1)'
- en: PyTorch Lightning (2.1.2)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Lightning (2.1.2)
- en: PyTorch Forecasting (1.0.0)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Forecasting (1.0.0)
- en: PyOD (1.1.2)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyOD (1.1.2)
- en: 'The code and datasets used in this chapter can be found at the following GitHub
    URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码和数据集可以在以下 GitHub URL 中找到：[https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook)。
- en: Time series anomaly detection with ARIMA
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ARIMA 进行时间序列异常检测
- en: Time series anomaly detection is an important task in application domains such
    as healthcare or manufacturing, among many others. Anomaly detection methods aim
    to identify observations that do not conform to the typical behavior of a dataset.
    In practice, anomalies can represent phenomena such as faults in machinery or
    fraudulent activity. Anomaly detection is a common task in **machine learning**
    (**ML**), and it has a few dedicated methods when it involves time series data.
    This type of dataset and the patterns therein can evolve over time, which complicates
    the modeling process and the effectiveness of the detectors. Statistical learning
    methods for time series anomaly detection problems usually follow a prediction-based
    approach or a reconstruction-based approach. In this recipe, we describe how to
    use an ARIMA method to create a prediction-based anomaly detection system for
    univariate time series.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列异常检测是诸如医疗保健或制造业等应用领域中的一项重要任务。异常检测方法旨在识别不符合数据集典型行为的观测值。在实际操作中，异常可能代表机器故障或欺诈行为等现象。异常检测是**机器学习**（**ML**）中的一项常见任务，尤其是在涉及时间序列数据时，它有一些专门的方法。这种类型的数据集及其中的模式可能会随时间演变，这使得建模过程和检测器的有效性变得更加复杂。用于时间序列异常检测问题的统计学习方法通常采用基于预测的方法或基于重构的方法。在本食谱中，我们将描述如何使用ARIMA方法为单变量时间序列创建一个基于预测的异常检测系统。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We’ll focus on a univariate time series from the `M3` dataset, which is available
    in the `datasetsforecast` library. Here’s how to get this data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于来自`M3`数据集的单变量时间序列，该数据集可以在`datasetsforecast`库中找到。以下是获取此数据的方法：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we start by loading the `M3` dataset using the `load()`
    method. Then, we use the `query()` method to get the univariate time series with
    an identifier (`unique_id` column) equal to `Q1`. Now, let’s see how to detect
    anomalies in this dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先使用`load()`方法加载`M3`数据集。然后，我们使用`query()`方法获取标识符为`Q1`的单变量时间序列（`unique_id`列）。现在，让我们看看如何在这个数据集中检测异常。
- en: How to do it…
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: We’ll build a forecasting model and use the corresponding prediction intervals
    to detect anomalies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个预测模型，并使用相应的预测区间来检测异常。
- en: 'We start by creating a forecasting model. While any model would work, in this
    recipe, we focus on ARIMA. Here’s how to define this model using the `statsforecast`
    library:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从创建一个预测模型开始。虽然任何模型都能工作，但在这个例子中，我们专注于ARIMA。以下是如何使用`statsforecast`库定义该模型：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we are ready to fit the model and get the forecasts:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备好拟合模型并获取预测：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First, we use the `forecast()` method to get the predictions. In this example,
    we set the forecasting horizon to `8` (`h=8`). We also pass two additional parameters:
    `level=[99]`, which means that we also want the model to predict the intervals
    with a `99`% confidence level; `fitted=True`, which tells the model to compute
    the training forecasts. We use the `forecast_fitted_values()` method to get the
    forecasts from the training set.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们使用`forecast()`方法获取预测结果。在此示例中，我们将预测时间设定为`8`（`h=8`）。我们还传递了两个额外的参数：`level=[99]`，这意味着我们还希望模型预测具有`99`%置信度的区间；`fitted=True`，这告诉模型计算训练预测。我们使用`forecast_fitted_values()`方法从训练集获取预测结果。
- en: 'Then, we identify anomalies based on whether the point forecasts are within
    the prediction intervals made by the model. This is done as follows:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们根据预测点是否在模型所做的预测区间内来识别异常。具体操作如下：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code checks whether the training predictions (`insample_forecasts['y']`
    object) are within the `99`% prediction intervals. Any observation that does not
    pass this check is considered an anomaly.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码检查训练预测（`insample_forecasts['y']`对象）是否在`99`%预测区间内。任何未通过此检查的观测值都被视为异常。
- en: 'Finally, we use the `plot()` method from the `StatsForecast` class to plot
    the anomalies:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`StatsForecast`类中的`plot()`方法来绘制异常：
- en: '[PRE4]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s what the plot looks like:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是该图的样子：
- en: '![Figure 9.1: Example of an anomaly identified by ARIMA](img/B21145_09_001.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：ARIMA识别的异常示例](img/B21145_09_001.jpg)'
- en: 'Figure 9.1: Example of an anomaly identified by ARIMA'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：ARIMA识别的异常示例
- en: How it works…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其工作原理…
- en: We used the `AutoARIMA` implementation available in the `statsforecast` library
    to create the ARIMA model. This approach automatically selects the best parameters
    for the model. We set the seasonal length to `4` since the frequency of the data
    is quarterly. The fitting process is carried out by a `StatsForecast` class instance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`statsforecast`库中可用的`AutoARIMA`实现来创建ARIMA模型。这种方法会自动选择模型的最佳参数。由于数据是季度数据，我们将季节性长度设置为`4`。拟合过程通过一个`StatsForecast`类实例来执行。
- en: Prediction-based methods work by comparing the forecasts of a given model with
    the actual values of the series. In this case, we use an ARIMA model, but other
    methods can also be used. Moreover, we consider an approach based on prediction
    intervals. Specifically, an observation is considered an anomaly if its value
    is outside of the predicted interval. In the code shown in the previous section,
    we considered a prediction interval with a 99% level, but you can test a different
    value for your problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预测的方法通过将给定模型的预测值与时间序列的实际值进行比较来工作。在这种情况下，我们使用ARIMA模型，但也可以使用其他方法。此外，我们还考虑了基于预测区间的方法。具体来说，如果观察值的数值超出了预测区间，就认为该值是异常值。在前面一节中的代码中，我们考虑了99%置信度的预测区间，但你可以根据你的问题测试不同的值。
- en: There’s more…
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: In this recipe, we focus on ARIMA to get prediction intervals, but you can use
    any other model with such capabilities.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们专注于使用ARIMA来获取预测区间，但你也可以使用任何具有此功能的其他模型。
- en: 'You can check the following URL for more details about how to use the `statsforecast`
    library for prediction-based anomaly detection: [https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html](https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下网址查看更多关于如何使用`statsforecast`库进行基于预测的异常检测的详细信息：[https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html](https://nixtla.github.io/statsforecast/docs/tutorials/anomalydetection.html)。
- en: Prediction-based anomaly detection using DL
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于预测的异常检测方法，使用深度学习（DL）
- en: We continue to explore prediction-based methods in this recipe. This time, we’ll
    create a forecasting model based on DL. Besides, we’ll use the point forecasts’
    error as a reference for detecting anomalies.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们继续探索基于预测的方法。这一次，我们将创建一个基于深度学习（DL）的预测模型。此外，我们还将使用点预测的误差作为检测异常的参考。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We’ll use a time series dataset about the number of taxi trips in New York
    City. This dataset is considered a benchmark problem for time series anomaly detection
    tasks. You can check the source at the following link: [https://databank.illinois.edu/datasets/IDB-9610843](https://databank.illinois.edu/datasets/IDB-9610843).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用纽约市出租车行程数的时间序列数据集。这个数据集被认为是时间序列异常检测任务的基准问题。你可以通过以下链接查看源数据：[https://databank.illinois.edu/datasets/IDB-9610843](https://databank.illinois.edu/datasets/IDB-9610843)。
- en: 'Let’s start by loading the time series using `pandas`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用`pandas`加载时间序列：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code involves several steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码包含几个步骤：
- en: Loading the dataset and corresponding labels using the `pd.read_csv``()` function.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pd.read_csv()`函数加载数据集及其对应的标签。
- en: 'Processing this dataset into a tabular format with three main pieces of information:
    the time series identifier (`unique_id`), the timestamp (`ds`), and the value
    of the observation (`y`).'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集处理为一个表格格式，包含三个主要信息：时间序列标识符（`unique_id`）、时间戳（`ds`）和观察值的数值（`y`）。
- en: Processing the labels into a new Boolean column called `is_anomaly` that denotes
    whether the corresponding observation is an anomaly.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签处理成一个新的布尔型列`is_anomaly`，用于标记相应的观察值是否为异常。
- en: 'Here’s what the series looks like:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是该时间序列的样子：
- en: '![Figure 9.2: New York City dataset with marked anomalies](img/B21145_09_002.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2：标记异常的纽约市数据集](img/B21145_09_002.jpg)'
- en: 'Figure 9.2: New York City dataset with marked anomalies'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：标记异常的纽约市数据集
- en: How to do it…
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行…
- en: 'Now, we use the taxi trips dataset to train a forecasting model. In this recipe,
    we’ll resort to the `neuralforecast` library, which contains implementation for
    several DL algorithm:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用出租车行程数据集来训练一个预测模型。在本食谱中，我们将使用`neuralforecast`库，它包含了多个深度学习算法的实现：
- en: 'Let’s start by defining the model as follows:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始定义模型，如下所示：
- en: '[PRE6]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We use an input size (`n_lags`) of `144`, which corresponds to 3 days of data
    as the time series is collected every `30` minutes (`freq='30T'`).
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用`144`作为输入大小（`n_lags`），这对应于3天的数据，因为时间序列每`30`分钟收集一次数据（`freq='30T'`）。
- en: 'After defining the model, we can train it using the `fit()` method:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义模型之后，我们可以使用`fit()`方法来训练它：
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Before the fitting process, we drop the `is_anomaly` variable that contains
    anomaly information. Now, the idea is to use the model to forecast the values
    of the time series. Any significant deviation from the actual value is considered
    an anomaly. Let’s look at the training predictions.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在拟合过程之前，我们会删除包含异常信息的`is_anomaly`变量。现在的想法是使用该模型来预测时间序列的值。任何与实际值有显著偏差的情况都被视为异常。让我们看看训练预测结果。
- en: 'We can get the training (or insample) predictions by calling the `predict_insample()`
    method, like so:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`predict_insample()`方法，我们可以获取训练（或样本内）预测，如下所示：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code, we get the training sample and remove the initial `n_lag`
    observations to align the predictions with the actual data. Then, we measure the
    absolute error of the model by taking the absolute difference between the predictions
    and actual values.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上述代码中，我们获取训练样本，并移除最初的`n_lag`观察以将预测与实际数据对齐。然后，通过模型的绝对误差来衡量模型的性能。
- en: 'Visualize the absolute error in the training data along with the marked anomalies:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化训练数据中的绝对误差以及标记的异常：
- en: '[PRE9]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the interest of conciseness, the plotting functions are not shown. You can
    check them out in the GitHub repository. The plot is shown in the following figure:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了简洁起见，绘图功能未显示。您可以在GitHub存储库中查看它们。图表显示在以下图中：
- en: '![Figure 9.3: Absolute error by the Neural Hierarchical Implementation for
    Time Series (NHITS) model and marked anomalies](img/B21145_09_003.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3：神经分层时间序列实现（NHITS）模型的绝对误差和标记的异常](img/B21145_09_003.jpg)'
- en: 'Figure 9.3: Absolute error by the Neural Hierarchical Implementation for Time
    Series (NHITS) model and marked anomalies'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：神经分层时间序列实现（NHITS）模型的绝对误差和标记的异常
- en: Large errors occur during two of the anomalies, though the model also misses
    some anomalies.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个异常情况下会发生较大的误差，尽管模型也会错过一些异常情况。
- en: How it works…
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: As seen in the previous recipe, we use a forecasting model to identify anomalies
    in a time series. In this case, instead of using prediction intervals, we rely
    on the absolute error of the model. A large error indicates a potential anomaly
    in the time series.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一示例所示，我们使用预测模型来识别时间序列中的异常。在这种情况下，我们不使用预测区间，而是依赖模型的绝对误差。较大的误差表明时间序列中可能存在异常。
- en: We use the `neuralforecast` framework to build a DL forecasting model based
    on the NHITS method. NHITS is a model that extends **Neural Basis Expansion Analysis**
    (**NBEATS**) and is based on a **multilayer perceptron** (**MLP**) type of architecture.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`neuralforecast`框架基于NHITS方法构建DL预测模型。NHITS是一种扩展**神经基础扩展分析**（**NBEATS**）的模型，基于**多层感知器**（**MLP**）类型的架构。
- en: This involves transforming the data into an appropriate format and training
    the model using auto-regression.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及将数据转换为适当的格式，并使用自回归方法训练模型。
- en: There’s more…
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In this recipe, we focus on a univariate time series dataset and a particular
    forecasting method (NHITS). Yet, it’s important to note that the prediction-based
    approach for anomaly detection can be applied to different settings (such as multivariate
    time series) and with other forecasting methods.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们专注于单变量时间序列数据集和特定的预测方法（NHITS）。然而，重要的是注意，基于预测的异常检测方法可以应用于不同的设置（例如多变量时间序列）和其他预测方法。
- en: During the training phase, we need to define an error threshold above which
    we flag an observation as an anomaly. We will explore several implementations
    with this feature in subsequent recipes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，我们需要定义一个误差阈值，超过这个阈值的观测值将被标记为异常。我们将在后续的示例中探讨几种带有此功能的实现。
- en: Anomaly detection using an LSTM AE
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM AE进行异常检测
- en: In this recipe, we’ll build an AE to detect anomalies in time series. An AE
    is a type of **neural network** (**NN**) that tries to reconstruct the input data.
    The motivation to use this kind of model for anomaly detection is that the reconstruction
    process of anomalous data is more difficult than that of typical observations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将构建一个AE来检测时间序列中的异常。AE是一种**神经网络**（**NN**）类型，试图重构输入数据。使用此类模型进行异常检测的动机在于，异常数据的重构过程比典型观测更为困难。
- en: Getting ready
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We’ll continue with the New York City taxi time series in this recipe. In terms
    of framework, we’ll show how to build an AE using PyTorch Lightning. This means
    that we’ll build a data module to handle the data preprocessing and another module
    for handling the training and inference of the NN.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将继续使用纽约市出租车的时间序列数据。在框架方面，我们将展示如何使用PyTorch Lightning构建AE模型。这意味着我们将构建一个数据模块来处理数据预处理，另一个模块则用于处理神经网络的训练和推理。
- en: How to do it…
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'This recipe is split into three parts. First, we build the data module based
    on PyTorch. Then, we create an AE module. Finally, we combine the two parts to
    build an anomaly detection system:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例分为三部分。首先，我们基于PyTorch构建数据模块。然后，我们创建AE模块。最后，我们将这两个部分结合起来，构建一个异常检测系统：
- en: 'Let’s start by building the data module. We create a class called `TaxiDataModule`
    that extends `pl.LightningDataModule`. Here’s the constructor of the class:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从构建数据模块开始。我们创建一个名为`TaxiDataModule`的类，它继承自`pl.LightningDataModule`。以下是该类的构造函数：
- en: '[PRE10]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `TaxiDataModule` class takes two inputs besides the dataset: the number
    of lags (context length) and the batch size.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`TaxiDataModule`类除了数据集外，还接受两个输入：滞后数（上下文长度）和批次大小。'
- en: 'Next, we code the `setup()` method, where the data is prepared for training
    and testing the model:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们编写`setup()`方法，在该方法中准备训练和测试模型所需的数据：
- en: '[PRE11]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, we start by splitting the data into training, validation,
    and testing sets. Each of these is transformed into a `TimeSeriesDataSet` class
    instance.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先将数据拆分为训练集、验证集和测试集。每个数据集都被转换为`TimeSeriesDataSet`类的实例。
- en: 'The data loaders are implemented as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据加载器的实现如下：
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Essentially, the data loading process is similar to what we did before in the
    forecasting tasks. You can check, for example, the *Multi-step and multi-output
    forecasting with multivariate time series* recipe in [*Chapter 5*](B21145_05.xhtml#_idTextAnchor306).
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本质上，数据加载过程与我们之前在预测任务中所做的类似。例如，你可以查看[*第5章*](B21145_05.xhtml#_idTextAnchor306)中的*多步多输出的多变量时间序列预测*示例。
- en: 'Now, we focus on the AE model, which is split into two parts: an encoder and
    a decoder. Here’s an implementation of the encoder in a class called `Encoder`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们关注AE模型，它分为两部分：编码器和解码器。以下是名为`Encoder`的类中的编码器实现：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The decoder is implemented in a class called `Decoder` that also extends `nn.Module`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器在一个名为`Decoder`的类中实现，该类也继承自`nn.Module`：
- en: '[PRE14]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The two parts are combined in an `AutoencoderLSTM` class that extends `pl.LightningModule`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个部分被结合在一个名为`AutoencoderLSTM`的类中，该类继承自`pl.LightningModule`：
- en: '[PRE15]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the `forward()` method, the encoder part takes the original input (`self.encoder(x)`)
    and transforms it into a reduced dimension (`xh` object). Then, the decoder reconstructs
    the original input data based on `xh`.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`forward()`方法中，编码器部分接收原始输入（`self.encoder(x)`）并将其转换为降维后的表示（`xh`对象）。然后，解码器基于`xh`重建原始输入数据。
- en: 'Then, we implement the training, validation, and prediction steps:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们实现训练、验证和预测步骤：
- en: '[PRE16]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We train the NN using the `Trainer` class from PyTorch Lightning. We use `144`
    lags, which amounts to 3 days of data. We also apply early stopping to guide the
    training process:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用PyTorch Lightning中的`Trainer`类训练神经网络。我们使用`144`个滞后，这相当于3天的数据。同时，我们应用了提前停止机制来指导训练过程：
- en: '[PRE17]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After training, we can apply the model to the test data as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后，我们可以按如下方式将模型应用于测试数据：
- en: '[PRE18]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we transform the `test` object from the data module
    into a data loader. We use a batch size of `1` without shuffling to process each
    instance sequentially. Then, we use the `trainer` object to get the predictions.
    The following figure shows the reconstructed error in the test set:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将数据模块中的`test`对象转换为数据加载器。我们使用批次大小为`1`，不进行洗牌，以便按顺序处理每个实例。然后，我们使用`trainer`对象获取预测。以下图表展示了测试集中的重建误差：
- en: '![Figure 9.4: Reconstruction error by the AE and marked anomalies](img/B21145_09_004.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4：AE的重建误差和标记的异常点](img/B21145_09_004.jpg)'
- en: 'Figure 9.4: Reconstruction error by the AE and marked anomalies'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：AE的重建误差和标记的异常点
- en: In most cases, the peaks in reconstruction error coincide with the anomalies.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，重建误差的峰值与异常点重合。
- en: How it works…
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The workflow in the data module may be familiar because it follows the same
    ideas behind the forecasting models we’ve built in other chapters; for example,
    in the *Multi-step and multi-output forecasting with multivariate time series*
    recipe in [*Chapter 5*](B21145_05.xhtml#_idTextAnchor306). But, in this case,
    we’re not interested in predicting the future values of the series. Instead, at
    each time step, both the input and the output of the model are recent lags of
    the series.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模块中的工作流程可能很熟悉，因为它遵循了我们在其他章节构建的预测模型的相同思路；例如，在 *多步多输出的多变量时间序列预测* 食谱中 [*第 5 章*](B21145_05.xhtml#_idTextAnchor306)。但在这种情况下，我们并不关心预测序列的未来值。相反，在每个时间步，模型的输入和输出都是序列的最近滞后值。
- en: 'An AE is composed of two main parts: an encoder and a decoder. The encoder
    aims to compress the input data into a small dimension, which is referred to as
    the bottleneck. Turning the input data into a small dimension is important to
    make the NN focus on the most important patterns in the data, disregarding noise.
    Then, the decoder takes the data encoded in the reduced dimension and tries to
    reconstruct the original input data. Both the encoder and the decoder of the NN
    are based on a stacked LSTM AE. Yet, you can use different architectures for these
    components.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: AE 由两个主要部分组成：编码器和解码器。编码器的目标是将输入数据压缩成一个小维度，这个小维度被称为瓶颈。将输入数据转化为小维度对于使神经网络（NN）聚焦于数据中的最重要模式、忽略噪音至关重要。然后，解码器接收在压缩维度中编码的数据，并尝试重建原始输入数据。编码器和解码器的神经网络均基于堆叠
    LSTM AE。不过，你也可以使用不同的架构来构建这些组件。
- en: The `Encoder` class extends the `nn.Module` class from `torch`. This particular
    encoder consists of two LSTM layers. These layers stack on top of each other as
    detailed in the `forward()` method. The `Decoder` class also contains two stacked
    LSTM layers that are followed by a densely connected layer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`Encoder` 类继承自 `torch` 的 `nn.Module` 类。这个特定的编码器由两层 LSTM 组成，这些层像在 `forward()`
    方法中详细说明的那样堆叠在一起。`Decoder` 类也包含两层堆叠的 LSTM 层，后面接一个密集连接的层。'
- en: In the training step of the AE, we pass a batch of the lagged time series (`x['encoder_cont']`)
    to the model. It produces an object called `y_pred`, which is the reconstructed
    input. Then, we compute the `F.mse_loss`), which compares the original input with
    the reconstructed one.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AE 的训练步骤中，我们将一批滞后的时间序列（`x['encoder_cont']`）传递给模型。它会生成一个名为 `y_pred` 的对象，即重建后的输入数据。然后，我们计算
    `F.mse_loss`，它用于比较原始输入和重建后的输入。
- en: Building an AE using PyOD
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyOD 构建 AE
- en: PyOD is a Python library that is devoted to anomaly detection. It contains several
    reconstruction-based algorithms such as AEs. In this recipe, we’ll build an AE
    using PyOD to detect anomalies in time series.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD 是一个专门用于异常检测的 Python 库。它包含了多个基于重建的算法，如 AE。在本食谱中，我们将使用 PyOD 构建 AE 来检测时间序列中的异常。
- en: Getting ready
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'You can install PyOD using the following command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令安装 PyOD：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We’ll use the same dataset as in the previous recipe. So, we start with the
    dataset object created in the *Prediction-based anomaly detection using DL* recipe.
    Let’s see how to transform this data to build an AE with PyOD.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前一个食谱相同的数据集。因此，我们从在 *基于预测的异常检测使用深度学习* 食谱中创建的数据集对象开始。让我们看看如何转换这些数据来使用 PyOD
    构建 AE。
- en: How to do it…
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'The following steps show how to build an AE and predict the probability of
    anomalies:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了如何构建 AE 并预测异常的概率：
- en: 'We start by transforming the time series using a sliding window with the following
    code:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过以下代码使用滑动窗口转换时间序列：
- en: '[PRE20]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the preceding code:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中：
- en: We get the value column of the time series and store it in the series object.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们获取时间序列的值列，并将其存储在系列对象中。
- en: Then, we iterate over the dataset using a sliding window similar to an auto-regressive
    approach. This way, the time series is represented by its past number of lags(`N_LAGS`)
    at each time step.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们通过类似自回归方法的滑动窗口遍历数据集。这样，时间序列在每个时间步上都由其过去的滞后值（`N_LAGS`）表示。
- en: We standardize the data using `StandardScaler` from `scikit-learn`, which is
    an important step for training NNs such as AEs.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `scikit-learn` 中的 `StandardScaler` 来标准化数据，这是训练神经网络（如 AE）时非常重要的一步。
- en: 'After preprocessing the data, we define the AE based on PyOD and fit it using
    the dataset:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预处理数据后，我们根据 PyOD 定义 AE 并使用数据集进行拟合：
- en: '[PRE21]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here’s the distribution of the scores:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是分数的分布：
- en: '![Figure 9.5: Histogram with the anomaly scores produced by the AE](img/B21145_09_005.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5：由 AE 产生的异常分数直方图](img/B21145_09_005.jpg)'
- en: 'Figure 9.5: Histogram with the anomaly scores produced by the AE'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：AE 生成的异常分数直方图
- en: 'Regarding the inference step, we can use the `predict``()` and `predict_proba``()`
    methods. The `predict``()` method works as follows:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于推断步骤，我们可以使用 `predict()` 和 `predict_proba()` 方法。`predict()` 方法的工作原理如下：
- en: '[PRE22]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `predict_proba()`method works as follows:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`predict_proba()` 方法的工作原理如下：'
- en: '[PRE23]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The probabilities represent the probability of each observation being an anomaly.
    You can plot the probabilities using the following code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 概率表示每个观察值是异常值的概率。你可以使用以下代码绘制概率图：
- en: '[PRE24]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here’s what the probabilities look like along the training set:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是训练集中的概率分布：
- en: '![Figure 9.6: Anomaly probability scores produced by the AE](img/B21145_09_006.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6: AE 生成的异常概率分数](img/B21145_09_006.jpg)'
- en: 'Figure 9.6: Anomaly probability scores produced by the AE'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：AE 生成的异常概率分数
- en: Again, the anomaly probability score peaks coincide with some anomalies.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，异常概率分数的峰值与一些异常值重合。
- en: How it works…
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理…
- en: The PyOD library follows a design pattern similar to `scikit-learn`. So, each
    method, such as `AutoEncoder`, is trained using the `fit``()` method and produces
    predictions based on a `predict` or `predict_proba``()` method.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PyOD 库遵循与 `scikit-learn` 类似的设计模式。因此，每个方法，如 `AutoEncoder`，都使用 `fit()` 方法进行训练，并基于
    `predict` 或 `predict_proba()` 方法进行预测。
- en: 'We use the `AutoEncoder` class instance from the `auto_encoder_torch` module.
    The library also contains the equivalent method but with a TensorFlow backend.
    We create an instance of the model and set up a few parameters:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 `auto_encoder_torch` 模块的 `AutoEncoder` 类实例。该库还包含相应的方法，但具有 TensorFlow
    后端。我们创建模型的实例并设置一些参数：
- en: '`hidden_neurons=[144, 2, 2, 144]`: These parameters detail the number of hidden
    units per layer. The input and output layers have a number of units equal to the
    input size, which is the number of lags. The hidden layers of the AE typically
    have a low number of units to compress the input data before reconstruction.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_neurons=[144, 2, 2, 144]`：这些参数详细说明了每层的隐藏单元数。输入层和输出层的单元数与输入大小相等，即滞后数。AE
    的隐藏层通常具有较少的单元数，以便在重建之前压缩输入数据。'
- en: '`hidden_activation`: The activation function, which is set to the rectified
    linear function.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_activation`：激活函数，设置为修正线性函数。'
- en: '`batch_norm`: A Boolean input that represents whether batch normalization should
    be applied. You can learn more about this at the following link: [https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_norm`：一个布尔值，表示是否应用批量归一化。你可以通过以下链接了解更多信息：[https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)。'
- en: '`learning_rate`: The learning rate, which is set to `0.001`.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：学习率，设置为 `0.001`。'
- en: '`batch_size`: The batch size, which is set to `64`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：批处理大小，设置为 `64`。'
- en: '`dropout_rate`: A dropout rate between the layers for regularization, which
    is set to `0.3`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate`：层之间的 dropout 率，用于正则化，设置为 `0.3`。'
- en: In this recipe, we created another AE for anomaly detection. This involves transforming
    the time series using a sliding window, similar to what we did for building forecasting
    models for auto-regression. The model predicts whether each observation is an
    anomaly based on the outlier scores. The threshold is set automatically by the
    model, though you can pick your own as well.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建了另一个用于异常检测的 AE。这涉及使用滑动窗口转换时间序列，类似于我们为自回归模型构建预测模型时所做的。该模型基于异常值分数预测每个观察值是否为异常。阈值由模型自动设置，当然你也可以自定义阈值。
- en: There’s more…
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: Deciding whether an observation is an anomaly involves analyzing the anomaly
    scores of the model. You can use different approaches, such as percentiles or
    standard deviations. For example, consider an observation an anomaly if the reconstruction
    error is above some percentile (such as 95) or if the reconstruction error is
    above two standard deviations.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 判断一个观察值是否为异常涉及分析模型的异常分数。你可以使用不同的方法，如百分位数或标准差。例如，如果重建误差超过某个百分位数（例如 95），或者重建误差超过两个标准差，则可以认为该观察值为异常。
- en: Note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We used the prediction from the training data for illustration purposes. Working
    with a test follows a similar approach.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练数据的预测进行说明。使用测试数据的方法类似。
- en: Creating a VAE for time series anomaly detection
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于时间序列异常检测的 VAE
- en: Building on the foundation laid in the previous recipe, we now turn our attention
    to VAEs, a more sophisticated and probabilistic approach to anomaly detection
    in time series data. Unlike traditional AEs, VAEs introduce a probabilistic interpretation,
    making them more adept at handling inherent uncertainties in real-world data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个例子的基础上，我们现在将注意力转向 VAE，这是在时间序列数据中进行异常检测的一种更复杂且概率性的方式。与传统的自动编码器不同，VAE 引入了概率解释，使其更擅长处理现实世界数据中的固有不确定性。
- en: Getting ready
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This code in this recipe is based on PyOD. We also use the same dataset as
    in the previous recipe:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的代码基于 PyOD。我们还使用与前一个例子相同的数据集：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, let’s see how to create a VAE for time series anomaly detection.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何为时间序列异常检测创建 VAE。
- en: How to do it…
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We begin by preparing our dataset, as in the previous recipe:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先准备数据集，如同在前一个例子中所做的那样：
- en: 'The dataset is first transformed using a sliding window, a technique that helps
    the model understand temporal dependencies within the time series:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集首先使用滑动窗口进行转换，这是一种帮助模型理解时间序列内时间依赖性的技术：
- en: '[PRE26]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After transforming the dataset, we define and fit the VAE model using PyOD’s
    `VAE` class. The configuration of the `VAE` class includes specifying the architecture
    of the encoder and decoder networks and various training parameters:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换数据集后，我们使用 PyOD 的 `VAE` 类来定义和拟合 VAE 模型。`VAE` 类的配置包括指定编码器和解码器网络的架构以及各种训练参数：
- en: '[PRE27]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The fitted VAE model is then used to generate anomaly scores. These scores
    reflect how well each data point conforms to the pattern learned by the model.
    Points with higher scores are more likely to be anomalies:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过拟合的 VAE 模型随后被用来生成异常分数。这些分数反映了每个数据点与模型学习到的模式的一致性。分数较高的点更可能是异常值：
- en: '[PRE28]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works…
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理……
- en: A VAE is a NN model that stands out for its ability to handle data’s latent
    or hidden aspects. Unlike traditional AEs, which map inputs to a fixed point in
    a latent space, VAEs transform inputs into a probability distribution, usually
    a normal distribution, characterized by mean and variance. This way, every input
    is associated with a region in the latent space rather than a single point, introducing
    an element of randomness and variability.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 是一种神经网络模型，因其能够处理数据的潜在或隐藏方面而突出。与传统的自动编码器（AE）不同，后者将输入映射到潜在空间中的固定点，VAE 将输入转换为概率分布，通常是正态分布，特征包括均值和方差。通过这种方式，每个输入与潜在空间中的一个区域相关联，而不是一个单一的点，从而引入了随机性和变异性。
- en: 'The decoder network then samples points from these estimated distributions
    and attempts to reconstruct the original input data. The training process involves
    two key objectives:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器网络从这些估计的分布中采样点，并尝试重建原始输入数据。训练过程涉及两个关键目标：
- en: Minimizing the reconstruction error ensures that the decoder can accurately
    recreate the input data from latent representations.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化重建误差确保解码器能够准确地从潜在表示重建输入数据。
- en: Regularizing latent space distributions to be close to a standard normal distribution.
    This is typically achieved by minimizing the Kullback-Leibler divergence. The
    regularization process prevents overfitting and ensures a well-structured and
    continuous latent space.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化潜在空间分布，使其接近标准正态分布。通常通过最小化 Kullback-Leibler 散度来实现这一点。正则化过程可以防止过拟合，并确保潜在空间结构良好且连续。
- en: Once trained, the VAE can be employed for anomaly detection. The VAE should
    be able to reconstruct normal data (similar to what it was trained on) with relatively
    low error. Conversely, data points significantly different from the training set
    (potential anomalies) will likely be reconstructed with higher error. Therefore,
    the reconstruction error serves as an anomaly score.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，可以使用 VAE 进行异常检测。VAE 应该能够以较低的误差重建正常数据（类似于它训练时使用的数据）。相反，训练集之外的数据点（潜在异常值）可能会以较高的误差重建。因此，重建误差可以作为异常分数。
- en: 'A high reconstruction error suggests that the data point does not conform well
    to the learned data distribution, flagging it as an anomaly:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 高重建误差表明数据点与学习到的数据分布不太一致，将其标记为异常：
- en: '![Figure 9.7: True values, true anomalies, and the probability of anomalies
    predicted by the VAE](img/B21145_09_007.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7：真实值、真实异常值，以及 VAE 预测的异常概率](img/B21145_09_007.jpg)'
- en: 'Figure 9.7: True values, true anomalies, and the probability of anomalies predicted
    by the VAE'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：真实值、真实异常值，以及 VAE 预测的异常概率
- en: This comparison helps us evaluate the performance of our VAE in real-world scenarios.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比较帮助我们评估 VAE 在现实场景中的表现。
- en: There’s more…
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: One of the most interesting aspects of VAEs is their ability to generate new
    data points. By sampling from learned distributions in the latent space, we can
    generate new instances that are similar to the training data. This property can
    be particularly useful in scenarios where data augmentation is required.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 最有趣的方面之一是它们能够生成新的数据点。通过从潜在空间中学习到的分布进行采样，我们可以生成与训练数据相似的新实例。这个特性在需要数据增强的场景中特别有用。
- en: Moreover, the probabilistic nature of VAEs offers a natural way to quantify
    uncertainty. This can be particularly beneficial in settings where it’s relevant
    to assess the confidence of the model’s predictions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，VAE 的概率性质提供了一种自然的方式来量化不确定性。这在需要评估模型预测置信度的场景中尤为有用。
- en: Using GANs for time series anomaly detection
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 进行时间序列异常检测
- en: GANs have gained significant popularity in various fields of ML, particularly
    in image generation and modification. However, their application in time series
    data, especially for anomaly detection, is an emerging area of research and practice.
    In this recipe, we focus on utilizing GANs, specifically **Anomaly Detection with
    Generative Adversarial Networks** (**AnoGAN**), to detect time series data anomalies.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 在机器学习的各个领域中获得了显著的关注，特别是在图像生成和修改方面。然而，它们在时间序列数据中的应用，尤其是在异常检测方面，仍然是一个新兴的研究和实践领域。在这篇文档中，我们专注于利用
    GAN，特别是 **生成对抗网络进行异常检测** (**AnoGAN**)，来检测时间序列数据中的异常。
- en: Getting ready…
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正在准备中…
- en: Before diving into the implementation, ensure that you have the PyOD library
    installed. We will continue using the taxi trip dataset for this recipe, which
    provides a real-world context for time series anomaly detection.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实现之前，请确保已经安装了 PyOD 库。我们将继续使用出租车行程数据集，该数据集为时间序列异常检测提供了一个真实的背景。
- en: How to do it…
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'The implementation involves several steps: data preprocessing, defining and
    training the AnoGAN model, and finally, performing anomaly detection:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 实现涉及多个步骤：数据预处理、定义和训练 AnoGAN 模型，最后进行异常检测：
- en: 'We start by loading the dataset and preparing it for the AnoGAN model. The
    dataset is transformed in the same way as before using a sliding window approach:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载数据集并为 AnoGAN 模型做准备。数据集通过滑动窗口方法进行转换，与之前的方法相同：
- en: '[PRE29]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'AnoGAN is then defined with specific hyperparameters and trained on the preprocessed
    data:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，AnoGAN 在特定的超参数下进行定义，并在预处理后的数据上进行训练：
- en: '[PRE30]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once the model is trained, we use it to predict anomalies in the data:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就使用它来预测数据中的异常：
- en: '[PRE31]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we visualize the results to compare the model’s predictions with actual
    anomalies:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过可视化结果，将模型预测与实际异常进行比较：
- en: '![Figure 9.8: True values, true anomalies, and the probability of anomalies
    predicted by a GAN](img/B21145_09_008.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8：真实值、真实异常和 GAN 预测的异常概率](img/B21145_09_008.jpg)'
- en: 'Figure 9.8: True values, true anomalies, and the probability of anomalies predicted
    by a GAN'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：真实值、真实异常和 GAN 预测的异常概率
- en: How it works…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'AnoGAN is a model that employs the principles of GANs for the specific task
    of anomaly detection in time series data. The core idea behind AnoGAN is the interaction
    between two key components: the generator and the discriminator.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: AnoGAN 是一个利用 GAN 原理进行时间序列数据异常检测的模型。AnoGAN 的核心思想是生成器和判别器之间的相互作用。
- en: The Generator is tasked with creating synthetic data that resembles the true
    time series data it has been trained on. It learns to capture the underlying patterns
    and distributions of the input data, trying to generate outputs that are indistinguishable
    from the real data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的任务是创建与真实时间序列数据相似的合成数据。它学习捕捉输入数据的潜在模式和分布，努力生成与真实数据无法区分的输出。
- en: 'The Discriminator, on the other hand, acts as a critic. Its role is to discern
    whether the data it reviews are genuine (actual data points from the dataset)
    or fabricated (outputs generated by the Generator). During training, these two
    components engage in a continuous game: the Generator improves its ability to
    produce realistic data, while the Discriminator becomes better at detecting fakes.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器，另一方面，充当评论员。它的角色是辨别它审查的数据是否真实（来自数据集的实际数据点）或虚假（由生成器生成的输出）。在训练过程中，这两个组件进行着一场持续的博弈：生成器提高其生成真实数据的能力，而判别器则在识别伪造数据方面变得更加精确。
- en: The reconstruction error is once again used to identify anomalies. The Generator,
    being trained only on normal data, will struggle to reproduce outliers or anomalous
    instances. Thus, when the reconstructed version of a data point diverges significantly
    from the original, we find a potential anomaly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差再次被用来识别异常。生成器只在正常数据上进行训练，因此在重建异常值或离群点时会遇到困难。因此，当数据点的重建版本与原始数据显著偏离时，我们就能发现潜在的异常。
- en: In practice, the reconstruction error can be calculated using various methods,
    such as MSE or other distance metrics, depending on the nature of the data and
    the specific requirements of the task at hand.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，重建误差可以通过各种方法来计算，例如均方误差（MSE）或其他距离度量，具体取决于数据的性质和任务的具体要求。
- en: There’s more…
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: While AnoGAN provides a novel approach to time series anomaly detection, it
    is worth exploring variations and improvements. For instance, one might consider
    tuning the model’s architecture or experimenting with different types of GANs,
    such as **conditional GANs** (**CGANs**) or **Wasserstein** **GANs** (**WGANs**).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AnoGAN提供了一种新颖的时间序列异常检测方法，但值得探索其变种和改进。例如，可以考虑调整模型的架构，或尝试不同类型的GAN，如**条件生成对抗网络**（**CGANs**）或**Wasserstein生成对抗网络**（**WGANs**）。
