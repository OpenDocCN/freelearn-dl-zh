- en: Chapter 4
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章
- en: Introducing Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍贝叶斯深度学习
- en: In [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002),
    *Fundamentals of* *Bayesian Inference*, we saw how traditional methods for Bayesian
    inference can be used to produce model uncertainty estimates, and we introduced
    the properties of well-calibrated and well-principled methods for uncertainty
    estimation. While these traditional methods are powerful in many applications,
    [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002)
    also highlighted some of their limitations with respect to scaling. In *Chapter
    3,* *Fundamentals of Deep Learning*, we saw the impressive things DNNs are capable
    of given large amounts of data; but we also learned that they aren’t perfect.
    In particular, they often lack robustness for out-of-distribution data – a major
    concern when we consider the deployment of these methods in real-world applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)，*贝叶斯推断基础*中，我们看到传统的贝叶斯推断方法如何用来产生模型的不确定性估计，并介绍了良好校准和有原则的不确定性估计方法的特性。尽管这些传统方法在许多应用中非常强大，[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)也突出了它们在扩展性方面的一些局限性。在*第3章*，*深度学习基础*中，我们看到了DNNs在大量数据下所能展现的令人印象深刻的能力；但我们也了解到它们并不完美。特别是，它们往往缺乏对分布外数据的鲁棒性——这是我们考虑将这些方法部署到现实世界应用中的一个主要问题。
- en: '![PIC](img/file79.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file79.png)'
- en: 'Figure 4.1: BDL combines the strengths of both deep learning and traditional
    Bayesian inference'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：BDL结合了深度学习和传统贝叶斯推断的优势
- en: 'BDL looks to ameliorate the shortcomings of both traditional Bayesian inference
    and standard DNNs, using the strengths from one method to address the weaknesses
    of the other. The fundamental idea is pretty straightforward: our DNNs gain uncertainty
    estimates, and so can be implemented more robustly, and our Bayesian inference
    methods gain the scalability and high-dimensional non-linear representation learning
    of DNNs.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: BDL旨在改进传统贝叶斯推断和标准DNN的不足，利用一种方法的优势来弥补另一种方法的不足。基本思想相当直接：我们的DNN获得不确定性估计，因此可以更稳健地实施，而我们的贝叶斯推断方法则获得了DNN的可扩展性和高维非线性表示学习能力。
- en: While conceptually this is quite intuitive, practically it’s not a case of just
    gluing things together. As the model complexity increases, so does the computational
    cost of Bayesian inference – making certain methods for Bayesian inference (such
    as via sampling) intractable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从概念上讲，这相当直观，但实际上并不是简单地将两者拼接在一起。随着模型复杂性的增加，贝叶斯推断的计算成本也会增加——使得某些贝叶斯推断方法（例如通过采样）变得不可行。
- en: 'In this chapter, we’ll introduce the concept of an ideal **Bayesian Neural**
    **Network** (**BNN**) and discuss its limitations, and we’ll learn about how we
    can use BNNs to create more robust deep learning systems. In particular, we’ll
    be covering the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍理想**贝叶斯神经网络**（**BNN**）的概念，并讨论其局限性，我们还将学习如何使用BNN创建更稳健的深度学习系统。具体来说，我们将涵盖以下内容：
- en: The ideal BNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想的BNN
- en: BDL fundamentals
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDL基础
- en: Tools for BDL
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BDL工具
- en: 4.1 Technical requirements
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 技术要求
- en: 'To complete the practical tasks in this chapter, you will need a Python 3.8
    environment with the `SciPy` stack and the following additional Python packages
    installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章中的实际任务，您需要一个安装了`SciPy`堆栈的Python 3.8环境，并安装以下额外的Python软件包：
- en: TensorFlow 2.0
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0
- en: TensorFlow Probability
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow概率
- en: Seaborn plotting library
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seaborn绘图库
- en: 'All of the code for this book can be found in the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码可以在书籍的GitHub仓库中找到：[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)。
- en: 4.2 The ideal BNN
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 理想的BNN
- en: As we saw in the previous chapter, a standard neural network comprises multiple
    layers. Each of these layers comprises a number of perceptrons – and these perceptrons
    comprise a multiplicative component (weight) and an additive component (bias).
    Each weight and bias parameter comprises a single parameter – or point estimate
    – and, in combination, these parameters transform the input to the perceptron.
    As we’ve seen, multiple layers of perceptrons are capable of achieving impressive
    feats when trained via backpropagation. However, these point estimates contain
    very limited information – let’s take a look.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章所看到的，一个标准的神经网络由多个层组成。每一层由若干感知机组成——这些感知机包含乘法组件（权重）和加法组件（偏置）。每个权重和偏置参数都是单一的参数——或点估计——并且这些参数的组合将输入转换为感知机的输出。正如我们所见，通过反向传播训练的多个感知机层能够实现令人印象深刻的成就。然而，这些点估计包含的信息非常有限——我们来看看。
- en: 'Generally speaking, the goal of deep learning is to find (potentially very,
    very many) parameter values that best map a set of inputs onto a set of outputs.
    That is, given some data, for each parameter in our network, we’ll choose the
    parameter that best describes the data. This often boils down to taking the mean
    – or expectation – of the candidate parameter values. Let’s see what this may
    look like for a single parameter in a neural network:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，深度学习的目标是找到（可能非常非常多的）参数值，最好的将一组输入映射到一组输出。也就是说，给定某些数据，对于网络中的每个参数，我们将选择最能描述数据的参数。这通常归结为取候选参数值的均值——或期望值。让我们看看这对于神经网络中的单一参数来说可能是什么样的：
- en: '![PIC](img/file80.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file80.png)'
- en: 'Figure 4.2: A table of values illustrating how parameters are averaged in machine
    learning models'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：展示如何在机器学习模型中对参数进行平均的数值表
- en: To understand this better, we’ll use a table to illustrate the relationship
    between input values, model parameters, and output values. The table shows, for
    five example input values (first column), what the ideal parameter (second column)
    would be to obtain the target output value (fourth column). In this context, ideal
    here simply means that the input value multiplied by the ideal parameter will
    exactly equal the target output value. Because we need to find a single value
    that best maps our input data to our output data, we end up taking the expectation
    (or mean) of our ideal parameters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，我们将使用表格来说明输入值、模型参数和输出值之间的关系。该表格显示了对于五个示例输入值（第一列），获得目标输出值（第四列）所需的理想参数（第二列）。在这种情况下，理想的意思是输入值乘以理想参数将完全等于目标输出值。因为我们需要找到一个最佳映射输入数据到输出数据的单一值，所以我们最终取理想参数的期望（或均值）。
- en: As we see here, taking the mean of these parameters is the compromise our model
    needs to make in order to find a parameter value that best fits all five data
    points in the example. This is the compromise that is made with traditional deep
    learning – by using distributions, rather than point estimates, BDL can improve
    on this. If we look at our standard deviation (*σ*) values, we get an idea of
    how the variation in the *ideal* parameter values (and thus the variance in the
    input values) translates to a variation in the loss. So, what happens if we have
    a poor selection of parameter values?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，取这些参数的均值是我们模型需要做出的折衷，以找到一个最适合示例中五个数据点的参数值。这是传统深度学习所做的折衷——通过使用分布，而不是点估计，BDL能够在此基础上进行改进。如果我们查看标准差（*σ*）值，我们可以大致了解理想参数值的变化（从而输入值的方差）如何转化为损失的变化。那么，如果我们选择了不合适的参数值，会发生什么呢？
- en: '![PIC](img/file81.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file81.png)'
- en: 'Figure 4.3: A table of values illustrating how parameter *σ* increases for
    poor sets of parameters'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：展示如何在参数不理想的情况下，参数*σ*值增大的数值表
- en: 'If we compare *Figure* [*4.2*](#x1-51002r2) and *Figure* [*4.3*](#x1-51004r3),
    we see how a significant variance in parameter values can lead to poorer approximation
    from the model, and that larger *σ* can be indicative of an error (at least for
    well-calibrated models). While in practice things are a little more complicated,
    what we see here is essentially what’s happening in every parameter of a deep
    learning model: parameter distributions are distilled down to point estimates,
    losing information in the process. In BDL, we’re interested in harnessing the
    additional information from these parameter distributions, using it for more robust
    training and for the creation of uncertainty-aware models.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较[*图4.2*](#x1-51002r2)和[*图4.3*](#x1-51004r3)，我们会看到参数值的显著方差如何导致模型近似度降低，而较大的*σ*可能表明模型存在误差（至少对于经过良好校准的模型）。虽然在实际中事情要复杂一些，但我们在这里看到的本质上是在每个深度学习模型的参数中发生的事情：参数分布被压缩成点估计，过程中的信息丢失。在BDL中，我们关注的是从这些参数分布中获取额外信息，用于更强健的训练和创建具有不确定性意识的模型。
- en: BNNs look to achieve this by modeling the distribution over neural network parameters.
    In the ideal case, the BNN would be able to learn any arbitrary distribution for
    every parameter in the network. At inference time, we would sample from the NN
    to obtain a distribution of output values. Using the sampling methods introduced
    in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of* *Bayesian Inference*](CH2.xhtml#x1-250002),
    we would repeat this process until we have obtained a statistically sufficient
    number of samples from which we could assume a good approximation of our output
    distribution. We could then use this output distribution to infer something about
    our input data, whether that be classifying speech content or performing regression
    on house prices.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: BNNs通过对神经网络参数的分布建模来实现这一目标。在理想情况下，BNN能够学习每个网络参数的任意分布。在推理时，我们将从神经网络中采样，获得输出值的分布。利用[*第2章*](CH2.xhtml#x1-250002)中介绍的采样方法，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)，我们将重复这一过程，直到获得足够数量的样本，从而能够假设我们的输出分布已得到很好的近似。然后，我们可以利用这个输出分布推断输入数据的某些特征，无论是分类语音内容还是对房价进行回归分析。
- en: 'Because we’d have parameter distributions, rather than point estimates, our
    ideal BNN would produce precise uncertainty estimates. These would tell us how
    likely the parameter values are given the input data. In doing so, they would
    allow us to detect cases where our input data deviates from the data seen at training
    time, and to quantify the degree of this deviation by how far a given sample of
    values lies from the distribution learned at training time. With this information,
    we would be able to handle our neural network outputs more intelligently – for
    example, if they’re highly uncertain, then we could fall back to some safe, pre-defined
    behavior. This concept of interpreting model predictions based on uncertainties
    should be familiar: we saw this in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals
    of Bayesian Inference*](CH2.xhtml#x1-250002), where we learned that high uncertainties
    are indicative of erroneous model predictions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们会有参数分布，而不是点估计，所以我们的理想BNN将提供精确的不确定性估计。这些估计将告诉我们给定输入数据的情况下，参数值的可能性有多大。这样，它们可以帮助我们检测输入数据与训练时数据的偏离情况，并通过给定样本值与训练时学习到的分布之间的差异量化这种偏差的程度。有了这些信息，我们就能更智能地处理神经网络的输出——例如，如果输出的不确定性很高，我们可以回退到一些安全的、预定义的行为。这种基于不确定性来解读模型预测的概念应该很熟悉：我们在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)中学到，高不确定性表明模型预测存在误差。
- en: Looking back to [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian
    Inference*](CH2.xhtml#x1-250002) again, we saw that sampling quickly becomes computationally
    intensive. Now imagine sampling from a distribution for each parameter in an NN
    – even if we take a relatively small network such as MobileNet (an architecture
    specifically designed to be more computationally efficient), we’re still looking
    at an enormous 4.2 million parameters. Performing this kind of sampling-based
    inference on such a network would be incredibly computationally intensive, and
    this would be even worse for other network architectures (for example, AlexNet
    has 60 million parameters!).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[*第二章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)，我们看到采样很快变得计算上不可行。现在，假设从每个神经网络参数的分布中进行采样——即使我们选择一个相对较小的网络，如
    MobileNet（一种专门设计以提高计算效率的架构），我们仍然需要处理多达 420 万个参数。对这样的网络进行基于采样的推断将非常计算密集，而对于其他网络架构，这种情况会更加糟糕（例如，AlexNet
    有 6000 万个参数！）。
- en: Because of this intractability, BDL methods make use of various approximations
    in order to facilitate uncertainty quantification. In the next section, we’ll
    learn about some of the fundamental principles applied to make uncertainty estimates
    possible with DNNs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种不可行性，BDL 方法采用了各种近似方法，以促进不确定性量化。在下一节中，我们将了解一些基本原理，这些原理被应用于使得使用深度神经网络（DNN）进行不确定性估计成为可能。
- en: 4.3 BDL fundamentals
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 BDL 基础
- en: Throughout the rest of the book, we will introduce a range of methods necessary
    to make BDL possible. There are a number of common themes present through these
    methods. We’ll cover these here, so that we have a good understanding of these
    concepts when we encounter them later on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们将介绍使得 BDL 成为可能的一系列方法。这些方法中有许多共同的主题。我们将在这里覆盖这些内容，以便在稍后遇到时能够很好地理解这些概念。
- en: 'These concepts include the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念包括以下内容：
- en: '**Gaussian assumptions**: With many BDL methods, we use Gaussian assumptions
    to make things computationally tractable'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯假设**：许多 BDL 方法使用高斯假设来使计算变得可行。'
- en: '**Uncertainty sources**: We’ll take a look at the different sources of uncertainty,
    and how we can determine the contributions of these sources for some BDL methods'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不确定性来源**：我们将查看不同的不确定性来源，并了解如何确定这些来源在某些 BDL 方法中的贡献。'
- en: '**Likelihoods**: We were introduced to likelihoods in [*Chapter 2*](CH2.xhtml#x1-250002),
    [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002), and here we’ll learn
    more about the importance of likelihood as a metric for evaluating the calibration
    of probabilistic models'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**似然**：我们在[*第二章*](CH2.xhtml#x1-250002)和[*贝叶斯推断基础*](CH2.xhtml#x1-250002)中介绍了似然，在这里我们将进一步了解似然作为评估概率模型校准的度量标准的重要性。'
- en: Let’s look at each of these in the following subsections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将查看以下小节中的每个问题。
- en: 4.3.1 Gaussian assumptions
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 高斯假设
- en: 'In the ideal case described previously, we talked about learning distributions
    for each neural network parameter. While realistically each parameter would follow
    a specific non-Gaussian distribution, this would make an already difficult problem
    even *more* difficult. This is because, for a BNN, we’re interested in learning
    two key probabilities:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面描述的理想情况下，我们讨论了为每个神经网络参数学习分布。实际上，虽然每个参数将遵循特定的非高斯分布，但这将使本已困难的问题变得更加*复杂*。这是因为，对于贝叶斯神经网络（BNN），我们关注的是学习两个关键概率：
- en: 'The probability of the weights *W* given some data *D*:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定某些数据 *D*，权重 *W* 的概率：
- en: '![P (W |D ) ](img/file82.jpg)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![P (W |D)](img/file82.jpg)'
- en: 'The probability of some output *ŷ* given some input **x**:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定某些输入 **x**，输出 *ŷ* 的概率：
- en: '![P (yˆ|x) ](img/file83.jpg)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![P (yˆ|x)](img/file83.jpg)'
- en: Obtaining these probabilities for arbitrary probability distributions would
    involve solving intractable integrals. Gaussian integrals, on the other hand,
    have closed-form solutions – making them a very popular choice for approximating
    distributions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意概率分布，获得这些概率需要求解无法求解的积分。而高斯积分有封闭解——使得它们成为近似分布时的非常流行的选择。
- en: 'For this reason, it’s common in BDL to assume that we can closely approximate
    the true underlying distribution of our weights with Gaussian distributions (similarly
    to what we’ve seen in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian
    Inference*](CH2.xhtml#x1-250002)). Let’s see what this would look like – taking
    our typical linear perceptron model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在贝叶斯深度学习（BDL）中，假设我们可以用高斯分布近似我们权重的真实底层分布是很常见的（这与我们在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推理基础*](CH2.xhtml#x1-250002)中看到的类似）。让我们看看这会是什么样子——以我们典型的线性感知机模型为例：
- en: '![z = f(x) = βX + ξ ](img/file84.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![z = f(x) = βX + ξ ](img/file84.jpg)'
- en: 'Here, *x* is our input to the perceptron, *β* is our learned weight value,
    *ξ* is our learned bias value, and *z* is the value that is returned (typically
    passed to the next layer). With a Bayesian approach, we turn our parameters *β*
    and *ξ* into distributions, rather than point estimates, such that:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是我们输入到感知机的值，*β* 是我们学习到的权重值，*ξ* 是我们学习到的偏置值，而 *z* 是返回的值（通常传递给下一层）。采用贝叶斯方法，我们将
    *β* 和 *ξ* 转换为分布，而不是点估计，具体来说：
- en: '![β ≈ 𝒩 (μ β,σβ) ](img/file85.jpg)![ξ ≈ 𝒩 (μ ξ,σξ) ](img/file86.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![β ≈ 𝒩 (μ β,σβ) ](img/file85.jpg)![ξ ≈ 𝒩 (μ ξ,σξ) ](img/file86.jpg)'
- en: 'The learning process would now involve learning four parameters instead of
    two, as each Gaussian is described by two parameters: the mean (*μ*) and standard
    deviation (*σ*). Doing this for each perceptron in our neural network, we end
    up doubling the number of parameters we need to learn – we can see this illustrated,
    starting with *Figure* [*4.4*](#x1-53005r4):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在学习过程将涉及学习四个参数，而不是两个，因为每个高斯分布由两个参数描述：均值（*μ*）和标准差（*σ*）。对我们神经网络中的每个感知机执行此操作后，我们最终需要学习的参数数量翻倍——我们可以通过从*图
    4.4*（[*Figure*](#x1-53005r4)）开始看到这一点：
- en: '![PIC](img/DNN-standard.JPG)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/DNN-standard.JPG)'
- en: 'Figure 4.4: An illustration of a standard DNN'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：标准DNN的示意图
- en: 'Introducing one-dimensional Gaussian distributions for our weights, our network
    becomes as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 引入一维高斯分布作为我们的权重后，网络变为如下：
- en: '![PIC](img/DNN-bayesian.JPG)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/DNN-bayesian.JPG)'
- en: 'Figure 4.5: An illustration of a BNN with Gaussian priors over the weights'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：带有高斯先验的贝叶斯神经网络（BNN）示意图
- en: In *Chapter 5, Principled Approaches for Bayesian Deep Learning*, we’ll see
    methods that do exactly this. While this does increase the computational complexity
    and memory footprint of our network, it makes the process of Bayesian inference
    with NNs manageable – making it a very worthwhile trade-off.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章，贝叶斯深度学习的原则方法*中，我们将看到正是这些方法。虽然这确实增加了网络的计算复杂性和内存占用，但它使得通过神经网络进行贝叶斯推理成为可能——这使得它成为一个非常值得的权衡。
- en: So, what is it we’re actually trying to capture in these uncertainty estimates?
    In [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002),
    we saw how uncertainty varies according to the sample of data used for training
    – but what are the sources of this uncertainty, and why is it important in deep
    learning applications? Let’s continue on to the next section to find out.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们实际上想要在这些不确定性估计中捕获什么呢？在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推理基础*](CH2.xhtml#x1-250002)中，我们看到了不确定性是如何根据用于训练的数据样本而变化的——但是这种不确定性的来源是什么？为什么它在深度学习应用中很重要？让我们继续往下看，找出答案。
- en: 4.3.2 Sources of uncertainty
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 不确定性的来源
- en: 'As we saw in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian
    Inference*](CH2.xhtml#x1-250002), and as we’ll see later on in the book, we typically
    deal with uncertainties as scalar variables associated with a parameter or output.
    These variables represent the variation in the parameter or output of interest,
    but while they are just scalar variables, there are multiple sources contributing
    to their values. These sources of uncertainty fall into two categories:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推理基础*](CH2.xhtml#x1-250002)中看到的，正如我们将在本书后续章节中看到的，我们通常将不确定性视为与某个参数或输出相关的标量变量。这些变量表示参数或输出的变化，但尽管它们只是标量变量，但有多个来源对它们的值产生影响。这些不确定性的来源可以分为两类：
- en: '**Aleatoric uncertainty**, otherwise known as observational uncertainty or
    data uncertainty, is the uncertainty associated with our inputs. It describes
    the variation in our **observations**, and as such is **irreducible**.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偶然性不确定性**，也称为观测不确定性或数据不确定性，是与输入相关的不确定性。它描述了我们**观测值**的变化，因此是**不可约的**。'
- en: '**Epistemic uncertainty**, otherwise known as model uncertainty, is the uncertainty
    that stems from our model. In the case of machine learning, this is the variance
    associated with the parameters of our model that *does not* stem from the observations,
    and is instead a product of the model, or how the model is trained. For example,
    in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002),
    we saw how different priors affected the uncertainty produced by Gaussian processes.
    This is an example of how model parameters influence the epistemic uncertainty
    – in this case, because they explicitly modify how the model interprets the relationship
    between different data points.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**认知不确定性**，也称为模型不确定性，是源于我们模型的不确定性。在机器学习中，这指的是与我们模型的参数相关的方差，它*并非*来源于观察，而是模型本身或模型的训练方式的产物。例如，在[*第
    2 章*](CH2.xhtml#x1-250002)《[*贝叶斯推理基础*](CH2.xhtml#x1-250002)》中，我们看到不同的先验如何影响高斯过程产生的不确定性。这是模型参数如何影响认知不确定性的一个例子——在这种情况下，因为它们明确地修改了模型对不同数据点之间关系的解释。'
- en: 'We can build an intuition of these concepts through some simple examples. Let’s
    say we have a basket of fruit containing apples and bananas. If we measure the
    height and length of some apples and bananas, we’ll see that apples are generally
    round, and that bananas are generally long, as illustrated in *Figure* [*4.6*](#x1-54010r6).
    We know from our observations that the exact dimensions of each fruit varies:
    we accept that there is randomness, or stochasticity, associated with the measurements
    of any given distribution of apples, but we know that they will all be roughly
    similar. This is the **irreducible uncertainty**: the inherent uncertainty in
    the data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一些简单的例子来建立对这些概念的直觉。假设我们有一篮水果，其中包含苹果和香蕉。如果我们测量一些苹果和香蕉的高度和长度，我们会发现苹果通常是圆形的，而香蕉通常是长的，如*图*
    [*4.6*](#x1-54010r6)所示。通过观察我们知道，每种水果的具体尺寸会有所不同：我们接受与任何给定的苹果分布的测量相关的随机性或随机性，但我们知道它们大致相似。这就是**不可减少的不确定性**：数据中的固有不确定性。
- en: '![PIC](img/aleatoric-uncertainty-illustration.JPG)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/aleatoric-uncertainty-illustration.JPG)'
- en: 'Figure 4.6: An illustration of aleatoric uncertainty, using fruit shapes as
    an example'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：使用水果形状作为示例的偶然不确定性的插图
- en: We can make use of this information to build a model to classify fruit as either
    apples or bananas according to these input features. But what happens if we mainly
    train our model on apples, with only a few measurements for bananas? This is illustrated
    in *Figure* [*4.7*](#x1-54013r7).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这些信息来构建一个模型，根据这些输入特征将水果分类为苹果或香蕉。但如果我们主要基于苹果来训练模型，而只有少量香蕉的测量数据会发生什么呢？这在*图*
    [*4.7*](#x1-54013r7)中有示例。
- en: '![PIC](img/epistemic-uncertainty-illustration.JPG)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/epistemic-uncertainty-illustration.JPG)'
- en: 'Figure 4.7: An illustration of high epistemic uncertainty based on our fruit
    example'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：基于水果示例的高认知不确定性的插图
- en: 'Here, we see that – because of limited data – our model has incorrectly classified
    bananas as apples. While these data points fall within our model’s `apple` boundary,
    we also see that they lie very far from the other apples, meaning that, although
    they’re classified as apples, our model (if it’s Bayesian) will have a high predictive
    uncertainty associated with these data points. This epistemic uncertainty is very
    useful in practical applications: it gives us an indication of when we can trust
    our model, and when we should be cautious about our model’s predictions. Unlike
    aleatoric uncertainty, epistemic uncertainty is **reducible** – if we give our
    model more examples of bananas, its class boundaries will improve, and the epistemic
    uncertainty will approach the aleatoric uncertainty.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到——由于数据有限——我们的模型错误地将香蕉分类为苹果。虽然这些数据点落在我们模型的`苹果`边界内，但我们也看到它们离其他苹果非常远，这意味着，尽管它们被分类为苹果，但我们的模型（如果是贝叶斯模型）会对这些数据点具有较高的预测不确定性。这种认知不确定性在实际应用中非常有用：它能告诉我们何时可以信任模型，何时我们应该对模型的预测保持谨慎。与偶然不确定性不同，认知不确定性是**可减少**的——如果我们给模型更多的香蕉示例，它的分类边界会改善，认知不确定性将接近偶然不确定性。
- en: '![PIC](img/epistemic-uncertainty-illustration2.JPG)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/epistemic-uncertainty-illustration2.JPG)'
- en: 'Figure 4.8: Illustration of low epistemic uncertainty'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：低认知不确定性的插图
- en: In *Figure* [*4.8*](#x1-54016r8), we see that the epistemic uncertainty has
    reduced significantly now that our model has observed more data, and it’s looking
    a lot more like the aleatoric uncertainty illustrated in *Figure* [*4.6*](#x1-54010r6).
    Epistemic uncertainty is therefore incredibly useful, both for indicating how
    much we can trust our model, and as a means of improving our model’s performance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图*[*4.8*](#x1-54016r8)中，我们可以看到，随着我们的模型观察到更多数据，认识不确定性显著减少，它现在看起来更像是*图*[*4.6*](#x1-54010r6)中展示的随机不确定性。因此，认识不确定性在两方面都极其有用：它不仅能指示我们可以多大程度上信任模型，而且还能作为提高模型性能的一种手段。
- en: 'As deep learning approaches are increasingly applied in mission-critical and
    safety-critical applications, it’s crucial that the methods we use can estimate
    the degree of epistemic uncertainty associated with their predictions. To illustrate
    this, let’s change the domain of our example from *Figure* [*4.7*](#x1-54013r7):
    instead of classifying fruit, we’re now classifying whether a jet engine is operating
    within safe parameters, as shown in *Figure* [*4.9*](#x1-54019r9).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习方法越来越多地应用于任务关键和安全关键的应用，使用的方法能够估计与其预测相关的认识不确定性的程度变得至关重要。为了说明这一点，让我们将示例的领域从*图*[*4.7*](#x1-54013r7)中的水果分类，改为现在分类喷气引擎是否在安全参数范围内运行，如*图*[*4.9*](#x1-54019r9)所示。
- en: '![PIC](img/epistemic-uncertainty-illustration-engine-failure.JPG)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/epistemic-uncertainty-illustration-engine-failure.JPG)'
- en: 'Figure 4.9: An illustration of high epistemic uncertainty in a safety-critical
    application'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：高认识不确定性在安全关键应用中的示意图
- en: Here, we see that our epistemic uncertainty could be a life-saving indicator
    of engine failure. Without this uncertainty estimate, our model would assume that
    all is fine, even though the temperature of the engine is unusual given the other
    parameters – this could lead to catastrophic consequences. Fortunately, because
    of our uncertainty estimates, our model is able to tell us that something is wrong,
    despite the fact that it’s never encountered this situation before.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的认识不确定性可能是引擎故障的一个生死攸关的指示。如果没有这个不确定性估计，我们的模型会假设一切正常，尽管在其他参数的情况下引擎的温度异常——这可能导致灾难性的后果。幸运的是，由于我们有不确定性估计，尽管我们的模型从未遇到过这种情况，它依然能够告诉我们出了问题。
- en: Separating sourcing of uncertainty
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分离不确定性的来源
- en: 'In this section, we’ve been introduced to two sources of uncertainty, and we’ve
    seen how epistemic uncertainty can be very useful for understanding how to interpret
    our model’s outputs. So, you may be wondering: is it possible to separate our
    sources of uncertainty?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了两种不确定性的来源，并且我们看到认识不确定性对于理解如何解释模型输出非常有用。那么，你可能会想：我们能否将不确定性来源分离开来？
- en: Generally speaking, there are limited guarantees when trying to decompose uncertainty
    into epistemic and aleatoric components, but some models allow us to obtain a
    good approximation of this. Ensemble methods provide a particularly good illustrative
    example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在尝试将不确定性分解为认识性不确定性和随机性不确定性时，提供的保证有限，但有些模型允许我们获得较好的近似。集成方法提供了一个特别好的示例。
- en: 'Let’s say we have an ensemble of *M* models that produce the predictive posterior
    *P*(*y*|**x***,D*) for some input **x** and output *y* from data *D*. For a given
    input, our prediction will have entropy:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含*M*个模型的集合，它们为某些输入**x**和输出*y*从数据*D*中生成预测后验*P*（*y*|**x***,D*）。对于给定的输入，我们的预测将具有熵：
- en: '![ 1 ∑M m m H [P (y|x, D)] ≈ H [M- P (y|x,𝜃 )],𝜃 ∼ p(𝜃|D ) m=1 ](img/file93.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ∑M m m H [P (y|x, D)] ≈ H [M- P (y|x,𝜃 )],𝜃 ∼ p(𝜃|D ) m=1 ](img/file93.jpg)'
- en: 'Here, *H* denotes entropy, and *𝜃* denotes our model parameters. This is a
    formal exdivssion of concepts we’ve already covered, showing that the entropy
    (in other words, uncertainty) of our predictive posterior will be high when our
    aleatoric and/or epistemic uncertainty is high. This, therefore, represents our
    **total** **uncertainty**, which is the uncertainty we’ll be working with throughout
    this book. We can represent this in a manner more consistent with what we’ll be
    encountering in the book – in terms of our predictive standard deviation *σ*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*H*表示熵，*𝜃*表示我们的模型参数。这是我们已经讨论过的概念的正式表达，表明当我们的随机不确定性和/或认识不确定性较高时，预测后验的熵（换句话说，不确定性）将很高。因此，这代表了我们的**总**不确定性，这是本书中我们将处理的不确定性。我们可以用一种更符合本书内容的方式来表示这一点——以我们的预测标准差*σ*为单位：
- en: '![σ = σa + σe ](img/file94.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![σ = σa + σe ](img/file94.jpg)'
- en: Where *a* and *e* denote aleatoric and epistemic uncertainty, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*a*和*e*分别表示偶然不确定性和认知不确定性。
- en: 'Because we’re working with ensembles, we can go a step further than our total
    uncertainty. Ensembles are unique in that each model learns something slightly
    different from the data, due to different data or parameter initialization. As
    we get an uncertainty estimate for each model, we can take the expectation (in
    other words, the average) of these uncertainty estimates:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在使用集成方法，我们可以进一步超越总不确定性。集成方法的独特之处在于每个模型从数据中学习到的内容略有不同，原因在于不同的数据或参数初始化。由于我们为每个模型都获得了不确定性估计，我们可以对这些不确定性估计值进行期望（换句话说，即求平均）：
- en: '![ ∑M 𝔼 [H [P(y|x,𝜃)]] ≈ -1- H [P (y|x,𝜃m )],𝜃m ∼ p(𝜃|D ) p(𝜃|D) M m=1 ](img/file95.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑M 𝔼 [H [P(y|x,𝜃)]] ≈ -1- H [P (y|x,𝜃m )],𝜃m ∼ p(𝜃|D ) p(𝜃|D) M m=1 ](img/file95.jpg)'
- en: This gives us our **expected data uncertainty** – an estimate of our aleatoric
    uncertainty. This approximate measure of aleatoric uncertainty becomes more accurate
    as ensemble size increases. This is possible because of the way ensemble members
    learn from different subsets of data. If there is no epistemic uncertainty, then
    the models are consistent, meaning their outputs are identical, and the total
    uncertainty exclusively comprises the aleatoric uncertainty.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们**期望的数据不确定性**——对偶然不确定性的估计。随着集成规模的增加，这种偶然不确定性的近似度量变得更为准确。这是因为集成成员从不同数据子集学习的方式。如果没有认知不确定性，那么模型是一致的，意味着它们的输出是相同的，总不确定性完全由偶然不确定性构成。
- en: 'If, on the other hand, there is some epistemic uncertainty, then our total
    uncertainty comprises both aleatoric and epistemic uncertainty. We can use the
    expected data uncertainty to determine how much epistemic uncertainty is present
    in our total uncertainty. We do this using **mutual information**, which is given
    by:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果存在认知不确定性，那么我们的总不确定性包括偶然不确定性和认知不确定性。我们可以使用期望的数据不确定性来确定我们总不确定性中存在多少认知不确定性。我们通过使用**互信息**来做到这一点，公式如下：
- en: '![I[y,𝜃|x,D ] = H [P (y|x, D)]− 𝔼p (𝜃|D )[H [P(y|x,𝜃)]] ](img/file96.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![I[y,𝜃|x,D ] = H [P (y|x, D)]− 𝔼p (𝜃|D )[H [P(y|x,𝜃)]] ](img/file96.jpg)'
- en: 'We can also exdivss this in terms of equation [4.3.2](#x1-550002):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过方程[4.3.2](#x1-550002)来表示这个问题：
- en: '![I[y,𝜃|x,D ] = σe = σ − σa ](img/file97.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![I[y,𝜃|x,D ] = σe = σ − σa ](img/file97.jpg)'
- en: 'As we can see, the concept is pretty straightforward: simply subtract our aleatoric
    uncertainty from our total uncertainty! The ability to estimate the aleatoric
    uncertainty can make ensemble methods more attractive for uncertainty quantification,
    as it allows us to decompose uncertainty, thus providing additional information
    we don’t usually have access to. In *Chapter 6, Bayesian* *Inference with a Standard
    Deep Learning Toolbox*, we’ll learn more about ensemble techniques for BDL. For
    non-ensemble methods, we just have the general predictive uncertainty, *σ* (the
    combined aleatoric and epistemic uncertainty), which is suitable in most cases.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这个概念相当直接：简单地将我们的**偶然不确定性**从总不确定性中减去！能够估计偶然不确定性使得集成方法在不确定性量化中更具吸引力，因为它允许我们分解不确定性，从而提供通常无法获得的额外信息。在*第六章，贝叶斯推断与标准深度学习工具箱*中，我们将学习更多关于BDL的集成技术。对于非集成方法，我们只有一般的预测不确定性，*σ*（合并了偶然和认知不确定性），这在大多数情况下是合适的。
- en: In the next section, we’ll see how we can incorporate uncertainties in how we
    evaluate our models, and how they can be incorporated in the loss function to
    improve model training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何将不确定性纳入到模型评估中，并且如何将其纳入损失函数以改善模型训练。
- en: '4.3.3 Going beyond maximum likelihood: the importance of likelihoods'
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 超越最大似然估计：似然的重要性
- en: In the previous section, we saw how uncertainty quantification can help to avoid
    potentially hazardous scenarios in real-world applications of machine learning.
    Going back even further to [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals
    of Bayesian Inference*](CH2.xhtml#x1-250002) and [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003), we were introduced to
    the concept of calibration, and shown how well-calibrated methods’ uncertainties
    increase as data at inference deviates from training data – a concept illustrated
    in *Figure* [*4.7*](#x1-54013r7).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了不确定性量化如何帮助避免在机器学习的实际应用中出现潜在的危险场景。回顾更早之前的[*第2章*](CH2.xhtml#x1-250002)的[*贝叶斯推断基础*](CH2.xhtml#x1-250002)和[*第3章*](CH3.xhtml#x1-350003)的[*深度学习基础*](CH3.xhtml#x1-350003)，我们引入了校准的概念，并展示了校准良好的方法如何随着推理数据偏离训练数据而增加其不确定性——这一概念在*图*
    [*4.7*](#x1-54013r7)中得到了说明。
- en: While it’s easy to illustrate the concept of calibration with simple data –
    as we saw in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002)
    (through *Figure* [*2.21*](CH2.xhtml#x1-31029r21)) – unfortunately, it’s not easy
    or practical to do this in most applications. A much more practical approach to
    understanding how well-calibrated a given method would be to use a metric that
    incorporates its uncertainty – and this is exactly what we get with **likelihood**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然用简单数据来说明校准的概念很容易——正如我们在[*第2章*](CH2.xhtml#x1-250002)中的[*贝叶斯推断基础*](CH2.xhtml#x1-250002)（通过*图*
    [*2.21*](CH2.xhtml#x1-31029r21)）中看到的那样——不幸的是，在大多数应用中，做这个并不容易或实际。理解给定方法的校准程度的一个更实际的方法是使用一个包含其不确定性的度量——这正是**似然**所提供的。
- en: 'Likelihood is the probability that some parameters describe some data. As mentioned
    earlier, we typically work with Gaussian distributions to make things tractable
    – so we’re interested in Gaussian likelihood: the likelihood that the parameters
    of a Gaussian fit some observed data. The equation for Gaussian likelihood is
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 似然是某些参数描述某些数据的概率。如前所述，我们通常使用高斯分布来简化问题——因此我们对高斯似然感兴趣：即高斯分布的参数拟合一些观测数据的似然。高斯似然的公式如下：
- en: '![ 1 (y − μ)2 p(y) = √----exp {− ----2--} 2π σ 2σ ](img/file98.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 (y − μ)2 p(y) = √----exp {− ----2--} 2π σ 2σ ](img/file98.jpg)'
- en: 'Let’s see what these distributions would look like for the parameter values
    we saw earlier in *Figures* [*4.2*](#x1-51002r2) and [*4.3*](#x1-51004r3):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些分布在我们之前在*图* [*4.2*](#x1-51002r2) 和 [*4.3*](#x1-51004r3) 中看到的参数值下会是什么样子：
- en: '![PIC](img/file99.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file99.png)'
- en: 'Figure 4.10: The plot of Gaussian distributions corresponding to the parameter
    sets from Figures [4.2](#x1-51002r2) and [4.3](#x1-51004r3)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：与图[4.2](#x1-51002r2) 和 [4.3](#x1-51004r3) 中的参数集对应的高斯分布图
- en: 'Visualizing these two distributions highlights the difference in uncertainty
    between the two parameter sets: our first set of parameters has high probability
    (solid line), whereas our second set of parameters has low probability (dotted
    line). But what does this mean for the resulting likelihood values associated
    with our model’s outputs? To investigate these, we need to plug these values into
    equation [4.3.3](#x1-560003). To do this, we’ll need a value for *y*. We’ll use
    the mean of our target values: 24*.*03\. For our *μ* and *σ* values, we’ll take
    the means and standard deviations of the predicted output values, respectively:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化这两个分布突出了这两组参数的不确定性差异：我们的第一组参数具有高概率（实线），而我们的第二组参数具有低概率（虚线）。但是，这对与我们模型输出相关的似然值意味着什么呢？为了调查这些，我们需要将这些值代入方程[4.3.3](#x1-560003)。为此，我们需要一个*y*的值。我们将使用目标值的均值：24*.*03\.
    对于我们的*μ*和*σ*值，我们将分别取预测输出值的均值和标准差：
- en: '![ 1 (24.03 − 24.01)2 p(𝜃1) = √---------exp { − ----------2----} = 0.29 2π
    × 1.37 2 × 1.37 ](img/file100.jpg)![ -----1----- (24.03−--31.11)2 − 5 p(𝜃2) =
    √2-π-× 1.78 exp {− 2× 1.782 } = 7.88× 10 ](img/file101.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 (24.03 − 24.01)2 p(𝜃1) = √---------exp { − ----------2----} = 0.29 2π
    × 1.37 2 × 1.37 ](img/file100.jpg)![ -----1----- (24.03−--31.11)2 − 5 p(𝜃2) =
    √2-π-× 1.78 exp {− 2× 1.782 } = 7.88× 10 ](img/file101.jpg)'
- en: We see here that we have a much higher likelihood score for our first set of
    parameters (*𝜃*[1]) than for our second (*𝜃*[2]). This is consistent with *Figure*
    [*4.10*](#x1-56004r10), and indicates that, given the data, parameters *𝜃*[1]
    have a higher probability than parameters *𝜃*[2] – in other words, parameters
    *𝜃*[1] do a better job of mapping the inputs to the outputs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在参数集*𝜃*[1]与*𝜃*[2]之间，前者的似然得分显著高于后者。这与*图* [*4.10*](#x1-56004r10)一致，表明根据数据，参数*𝜃*[1]比参数*𝜃*[2]具有更高的概率——换句话说，参数*𝜃*[1]更好地将输入映射到输出。
- en: These examples illustrate the impact of incorporating uncertainty estimates,
    allowing us to compute the likelihood of the data. While our error has increased
    somewhat due to the poorer mean prediction, our likelihood has decreased more
    dramatically – falling by many orders of magnitude. This tells us that these parameters
    are doing a very poor job of describing the data, and it does so in a more principled
    way than simply computing the error between our outputs and our targets.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子展示了引入不确定性估计的影响，使我们能够计算数据的似然性。虽然由于平均预测较差，我们的误差有所增加，但我们的似然性下降得更为显著——下降了多个数量级。这告诉我们，这些参数在描述数据方面表现得非常糟糕，而且它比仅仅计算输出和目标之间的误差更具原则性。
- en: 'An important feature of likelihood is that it balances a model’s accuracy with
    its uncertainty. Models that are over-confident have low uncertainty on data for
    which they have incorrect predictions, and likelihood penalizes them for this
    overconfidence. Similarly, well-calibrated models are confident on data for which
    they have correct predictions, and uncertain on data for which they have incorrect
    predictions. While the models will still be penalized for the incorrect predictions,
    they will also be rewarded for being uncertain in the right places, and not being
    over-confident. To see this in practice, we can again use the target output value
    from the tables shown in *Figure* [*4.2*](#x1-51002r2) and *Figure* [*4.3*](#x1-51004r3):
    *y* = 24*.*03, but we’ll also use an incorrect prediction: *ŷ* = 5*.*00\. As we
    can see, this produces a pretty significant error of |*y* −*ŷ*| = |24*.*03 − 5*.*00|
    = 19*.*03\. Let’s take a look at what happens to our likelihood as we increase
    our *σ*² value associated with this prediction:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 似然性的重要特征之一是它平衡了模型的准确性和不确定性。过于自信的模型在数据的预测不正确时，表现出较低的不确定性，而似然性会因这种过度自信而惩罚它们。同样，校准良好的模型在预测正确的数据上表现出信心，而在预测错误的数据上表现出不确定性。虽然模型仍会因错误的预测而受到惩罚，但它们也会因在正确的地方表现出不确定性而获得奖励，而不会过度自信。为了实践这一点，我们可以再次使用*图*
    [*4.2*](#x1-51002r2)和*图* [*4.3*](#x1-51004r3)中显示的目标输出值：*y* = 24*.*03，但我们也会使用一个不正确的预测值：*ŷ*
    = 5*.*00。如我们所见，这产生了一个相当大的误差：|*y* −*ŷ*| = |24*.*03 − 5*.*00| = 19*.*03。让我们来看一下，当我们增加与此预测相关的*σ*²值时，似然性会发生什么变化：
- en: '![PIC](img/file102.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file102.png)'
- en: 'Figure 4.11: A plot of likelihood values with increasing variance'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：方差增加时似然值的变化图
- en: As we see here, our likelihood value is very small when *σ*² = 0*.*00, but increases
    as *σ*² increases to around 0*.*15, before falling off again. This demonstrates
    that, given an incorrect prediction, some uncertainty is better than none when
    it comes to likelihood values. Thus, using likelihoods allows us to train better
    calibrated models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当*σ*² = 0*.*00时，我们的似然值非常小，但随着*σ*²增加到约0*.*15时，它又开始上升，然后再次下降。这表明，在预测不正确的情况下，与没有不确定性相比，一定的不确定性对于似然值更有利。因此，使用似然性可以帮助我们训练出更好校准的模型。
- en: 'Similarly, we can see that if we fix our uncertainty, in this case to *σ*²
    = 0*.*1, and vary our predictions, our likelihood peaks at the correct value,
    falling off in either direction as our predictions *ŷ* become less accurate and
    our error |*y* −*ŷ*| grows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以看到，如果我们固定不确定性，这里设为*σ*² = 0*.*1，并改变预测值，似然性在正确值处达到峰值，当预测*ŷ*变得不准确时，似然性在任一方向上都会下降，同时我们的误差|*y*
    −*ŷ*|也在增大：
- en: '![PIC](img/likelihood-varying-predictions.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/likelihood-varying-predictions.png)'
- en: 'Figure 4.12: A plot of likelihood values with varying predictions'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：随着预测变化的似然值图
- en: 'Practically, we don’t usually use the likelihood, but instead use the **negative**
    **log-likelihood** (**NLL**). We make it negative because, with loss functions,
    we are interested in finding the minima, rather than the maxima. We use the log
    because this allows us to use addition, rather than multiplication, which makes
    things more computationally efficient (making use of the logarithmic identity
    *log*(*a* ∗ *b*) = *log*(*a*) + *log*(*b*)). The equation that we’ll typically
    be using is therefore:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通常不使用似然函数，而是使用**负对数似然**（**NLL**）。我们将其取负，因为在损失函数中，我们关心的是寻找最小值，而不是最大值。我们使用对数，因为这使得我们能够使用加法而非乘法，从而提高计算效率（利用对数恒等式*log*(*a*
    ∗ *b*) = *log*(*a*) + *log*(*b*)）。因此，我们通常使用的方程是：
- en: '![ 2 N LL (y) = − log{-1--}− (y-−-μ)- 2πσ 2 σ2 ](img/file103.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![ 2 N LL (y) = − log{-1--}− (y-−-μ)- 2πσ 2 σ2 ](img/file103.jpg)'
- en: Now that we’re familiar with the core concepts of uncertainty and likelihood,
    we’re ready for the next section, where we’ll learn how to work with probabilistic
    concepts in code using the TensorFlow Probability library.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了不确定性和似然性这两个核心概念，接下来我们准备进入下一部分，学习如何在代码中使用TensorFlow概率库处理概率概念。
- en: 4.4 Tools for BDL
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 BDL工具
- en: In this chapter, as well as in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals
    of Bayesian Inference*](CH2.xhtml#x1-250002), we’ve seen a lot of equations involving
    probability. While it’s possible to create BDL models without a probability library,
    having a library that supports some of the fundamental functions makes things
    much easier. As we’re using TensorFlow for the examples in this book, we’ll be
    using the **TensorFlow** **Probability** (**TFP**) library to help us with some
    of these probabilistic components. In this section, we’ll introduce TFP and show
    how it can be used to easily implement many of the concepts we’ve seen in [*Chapter 2*](CH2.xhtml#x1-250002),
    [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002) and [*Chapter 4*](#x1-490004),
    [*Introducing Bayesian Deep* *Learning*](#x1-490004).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，正如在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)中所见，我们已经看到了许多涉及概率的方程。虽然没有概率库也能创建BDL模型，但有一个支持基本函数的库会使事情变得更容易。由于本书中的示例使用了TensorFlow，我们将使用**TensorFlow**
    **概率**（**TFP**）库来帮助我们实现这些概率组件。在本节中，我们将介绍TFP，并展示如何使用它轻松实现我们在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)和[*第4章*](#x1-490004)，[*介绍贝叶斯深度*
    *学习*](#x1-490004)中看到的许多概念。
- en: 'Much of the content up to this point has been about introducing the concept
    of working with distributions. As such, the first TFP module we’ll learn about
    is the `distributions` module. Let’s take a look:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，很多内容都在介绍如何与分布进行工作。因此，我们将要学习的第一个TFP模块是`distributions`模块。让我们来看看：
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we have a simple example of initializing a Gaussian (or normal) distribution
    using the `distributions` module. We can now sample from this distribution – we’ll
    visualize the distribution of our samples using `seaborn` and `matplotlib`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个简单的例子，使用`distributions`模块初始化一个高斯（或正态）分布。我们现在可以从这个分布中进行采样——我们将使用`seaborn`和`matplotlib`可视化我们的样本分布：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This produces the following plot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![PIC](img/file104.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file104.png)'
- en: 'Figure 4.13: A probability distribution of samples drawn from a Gaussian distribution
    using TFP'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：使用TFP从高斯分布中抽样得到的样本的概率分布
- en: 'As we can see, the samples follow a Gaussian distribution defined by our parameters
    *μ* = 0 and *σ* = 1*.*5\. The TFD distribution classes also have methods for useful
    functions such as **Probability Density Function** (**PDF**) and **Cumulative
    Density Function** (**CDF**). Let’s take a look, starting with computing the PDF
    over a range of values:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，样本遵循由我们的参数*μ* = 0和*σ* = 1*.*5定义的高斯分布。TFD分布类还具有一些有用的函数方法，如**概率密度函数**（**PDF**）和**累积分布函数**（**CDF**）。让我们先从计算PDF在一系列值上的表现开始：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the divceding code, we’ll produce the following plot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下代码，我们将生成以下图表：
- en: '![PIC](img/file105.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file105.png)'
- en: 'Figure 4.14: A plot of probability density function values for a range of inputs
    spanning *x* = −4 to *x* = 4'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14：一系列输入（*x* = −4到*x* = 4）对应的概率密度函数值的图
- en: 'Similarly, we can also compute the CDF:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也可以计算CDF：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Compared with the PDF, the CDF produces cumulative probability values, from
    0 to 1, as we see in the following plot:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与PDF相比，CDF生成累积概率值，范围从0到1，正如我们在下面的图表中所看到的：
- en: '![PIC](img/file106.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file106.png)'
- en: 'Figure 4.15: Cumulative density function values for a range of inputs spanning
    *x* = −4 to *x* = 4'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15：针对范围内输入值的累积分布函数值，*x* = −4到*x* = 4
- en: 'The `tfp.distributions` classes also give us easy access to the parameters
    of the distributions, for example, we can recover the parameters of our Gaussian
    distribution via the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`tfp.distributions`类还为我们提供了轻松访问分布参数的方式，例如，我们可以通过以下方式恢复高斯分布的参数：'
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that these will return `tf.Tensor` objects, but the NumPy values can be
    accessed easily via the `.numpy()` function, for example:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些将返回`tf.Tensor`对象，但可以通过`.numpy()`函数轻松访问NumPy值，例如：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives us two NumPy scalars for our `mu` and `sigma` variables: 0*.*0 and
    1*.*5, respectively.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了我们的`mu`和`sigma`变量的两个NumPy标量值：分别为0*.*0和1*.*5。
- en: 'Just as we can compute the probability, and thus obtain the PDF, using the
    `prob()` function, we can also easily compute the log probability, or log likelihood,
    using the `log_prob()` function. This makes things a little easier than coding
    the full likelihood equation (for instance, equation [4.3.3](#x1-56010r3)) each
    time:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们可以使用`prob()`函数计算概率，从而得到PDF一样，我们也可以轻松地使用`log_prob()`函数计算对数概率或对数似然。这使得我们比每次都编写完整的似然方程（例如，方程[4.3.3](#x1-56010r3)）更简单一些：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we first obtain the log likelihood for some value *x* = 5, and then obtain
    the NLL, such as would be used in the context of gradient descent.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先获得某个值*x* = 5的对数似然值，然后获得NLL，这在梯度下降的上下文中会用到。
- en: As we continue through the book, we’ll learn more about what TFP has to offer
    – using the `distributions` module to sample from parameter distributions, and
    exploring the powerful `tfp.layers` module, which implements probabilistic versions
    of common neural network layers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续阅读本书，我们将更多地了解TFP的功能——使用`distributions`模块从参数分布中采样，并探索强大的`tfp.layers`模块，该模块实现了常见神经网络层的概率版本。
- en: 4.5 Summary
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 总结
- en: In this chapter, we were introduced to the fundamental concepts that we’ll need
    to progress through the book and learn how to implement and use BNNs. Most crucially,
    we learned about the ideal BNN, which introduced us to the core ideas underlying
    BDL, and the computational difficulties of achieving this in practice. We also
    covered the fundamental practical methods used in BDL, giving us a grounding in
    the concepts that allow us to implement computationally tractable BNNs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了实现并使用BNN所需的基本概念。最重要的是，我们了解了理想的BNN，这使我们接触到BDL的核心思想，以及在实践中实现这一点的计算困难。我们还介绍了BDL中使用的基本实践方法，为我们实现计算可行的BNN打下了基础。
- en: 'The chapter also introduced the concept of uncertainty sources, describing
    the difference between data and model uncertainty, how these contribute to total
    uncertainty, and how we can estimate the contributions of different types of uncertainty
    with various models. We also introduced one of the most fundamental components
    in probabilistic inference – the likelihood function – and learned about how it
    can help us to train better principled and better calibrated models. Lastly, we
    were introduced to TensorFlow Probability: a powerful library for probabilistic
    inference, and a crucial component of the practical examples later in the book.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还介绍了不确定性来源的概念，描述了数据不确定性和模型不确定性之间的区别，这些如何影响总不确定性，并且我们如何通过不同的模型估计各种不确定性类型的贡献。我们还介绍了概率推断中最基本的组成部分之一——似然函数，并了解了它如何帮助我们训练更好的、原则性更强且更为精确的模型。最后，我们介绍了TensorFlow概率：一个强大的概率推断库，并且是本书后面实践例子中的一个关键组成部分。
- en: Now that we’ve covered these fundamentals, we’re ready to see how the concepts
    we’ve encountered so far can be applied in the implementation of several key BDL
    models. We’ll learn about the advantages and disadvantages of these approaches,
    and how to apply them to a variety of real-world problems. Continue on to [*Chapter 5*](CH5.xhtml#x1-600005),
    [*Principled Approaches for Bayesian* *Deep Learning*](CH5.xhtml#x1-600005), where
    we’ll learn about two key principled approaches for BDL.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了这些基础知识，我们准备好看看我们迄今为止遇到的概念如何应用到多个关键BDL模型的实现中。我们将了解这些方法的优缺点，并学习如何将它们应用于各种实际问题。继续阅读[*第5章*](CH5.xhtml#x1-600005)，[*贝叶斯深度学习的原则性方法*](CH5.xhtml#x1-600005)，在这里我们将学习两种关键的BDL原则性方法。
- en: 4.6 Further reading
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 进一步阅读
- en: 'This chapter has introduced the material necessary to start working with BDL;
    however, there are many resources that go into more depth on the topics of uncertainty
    sources. The following are a few recommendations for readers interested in exploring
    the theory and code in more depth:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了开始使用 BDL 所需的材料；然而，还有许多资源可以更深入地探讨不确定性来源的相关主题。以下是一些推荐，供那些有兴趣更深入探索理论和代码的读者参考：
- en: '*Machine Learning: A Probabilistic Perspective, Murphy*: Kevin Murphy’s extremely
    popular book on machine learning has become a staple for students and researchers
    in the field. This book provides a detailed treatment of machine learning from
    a probabilistic standpoint, unifying concepts from statistics, machine learning,
    and Bayesian probability.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习：一种概率视角，Murphy*：凯文·墨菲（Kevin Murphy）关于机器学习的极为流行的书籍已成为该领域学生和研究人员的必备读物。本书从概率的角度详细介绍了机器学习，统一了统计学、机器学习和贝叶斯概率的概念。'
- en: '*TensorFlow Probability* *Tutorials*: in this book, we’ll see how TensorFlow
    Probability can be used to develop BNNs, but their website includes a wide array
    of tutorials addressing probabilistic programming more generally: [https://www.tensorflow.org/probability/overview](https://www.tensorflow.org/probability/overview)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TensorFlow Probability* *教程*：在本书中，我们将看到如何使用 TensorFlow Probability 开发 BNNs，但他们的网站提供了广泛的教程，更广泛地涉及概率编程：[https://www.tensorflow.org/probability/overview](https://www.tensorflow.org/probability/overview)'
- en: '*Pyro Tutorials*: Pyro is a PyTorch-based library for probabilistic programming
    – it’s another powerful tool for Bayesian inference, and the Pyro website has
    many excellent tutorials and examples of probabilistic inference: [https://pyro.ai/](https://pyro.ai/).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pyro教程*：Pyro 是一个基于 PyTorch 的概率编程库——这是一个强大的贝叶斯推断工具，Pyro 网站上有许多关于概率推断的优秀教程和示例：[https://pyro.ai/](https://pyro.ai/)'
