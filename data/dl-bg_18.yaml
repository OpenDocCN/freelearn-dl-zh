- en: Final Remarks on the Future of Deep Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习未来的最终思考
- en: We have been through a journey together, and if you have read this far you deserve
    to treat yourself with a nice meal. What you have accomplished deserves recognition.
    Tell your friends, share what you have learned, and remember to always keep on
    learning. Deep learning is a rapidly changing field; you cannot sit still. This
    concluding chapter will briefly present to you some of the new exciting topics
    and opportunities in deep learning. If you want to continue your learning, we
    will recommend other helpful resources from Packt that can help you move forward
    in this field. At the end of this chapter, you will know where to go from here
    after having learned the basics of deep learning; you will know what other resources
    Packt offers for you to continue your training in deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一起经历了一段旅程，如果你读到这里，你值得犒劳自己一顿美餐。你所取得的成就值得被认可。告诉你的朋友们，分享你所学到的内容，并记得始终保持学习。深度学习是一个快速发展的领域；你不能停滞不前。本章将简要介绍一些深度学习中新的、令人兴奋的主题和机会。如果你希望继续学习，我们将推荐一些来自
    Packt 的其他有用资源，帮助你在这个领域继续前进。在本章结束时，你将知道在学习了深度学习的基础知识后，接下来该往哪里去；你将知道 Packt 提供的其他资源，帮助你继续深度学习的训练。
- en: 'This chapter is organized into the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章组织成以下几个部分：
- en: Looking for advanced topics in deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找深度学习中的高级主题
- en: Learning with more resources from Packt
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Packt 的更多资源进行学习
- en: Looking for advanced topics in deep learning
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找深度学习中的高级主题
- en: The future of deep learning is hard to predict at the moment; things are changing
    rapidly. However, I believe that if you invest your time in the present advanced
    topics in deep learning, you might see these areas developing prosperously in
    the near future.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 目前很难预测深度学习的未来；事物发展得非常迅速。然而，我相信，如果你把时间投资在当前的深度学习高级主题上，你可能会在不久的将来看到这些领域繁荣发展。
- en: The following sub-sections discuss some of these advanced topics that have the
    potential of flourishing and being disruptive in our area.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将讨论一些具有潜力、可能在我们领域蓬勃发展并带来颠覆性影响的高级主题。
- en: Deep reinforcement learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: '**Deep reinforcement learning** (**DRL**) is an area that has gained a lot
    of attention recently given that deep convolutional networks, and other types
    of deep networks, have offered solutions to problems that were difficult to solve
    in the past. Many of the uses of DRL are in areas where we do not have the luxuryof
    having data on all possible conceivable cases, such as space exploration, playing
    video games, or self-driving cars.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度强化学习**（**DRL**）是一个近年来备受关注的领域，因为深度卷积网络及其他类型的深度网络已经为过去难以解决的问题提供了解决方案。DRL
    的许多应用场景是在我们无法拥有所有可能情境数据的领域，例如太空探索、电子游戏或自动驾驶汽车。'
- en: 'Let''s expand on the latter example. If we were using traditional supervised
    learning to make a self-driving car that can take us from point A to point B without
    crashing, we would not only want to have examples of the positive class with events
    of successful journeys, but we would also needexamples of the negative class with
    bad events such as crashes and terrible driving. Think about this: we would need
    to crash as many cars as we have successful events to keep the dataset balanced.
    This is not acceptable; however, reinforcement learning comes to the rescue.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展一下后面的例子。如果我们使用传统的监督学习来构建一辆自动驾驶汽车，让它能够从 A 点安全到达 B 点，我们不仅需要正面类的示例（成功的行程），还需要负面类的示例（例如车祸和糟糕的驾驶）。想想看：为了保持数据集的平衡，我们需要和成功事件一样多的车祸数据。这是不可接受的；然而，强化学习能够帮助我们解决这个问题。
- en: DRL aims to **reward** good driving aspects; the models learn that there is
    a reward to be gained, so we don't need negative examples. In contrast, traditional
    learning would need to crash cars in order to **penalize** bad outcomes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）旨在**奖励**良好的驾驶行为；模型会学习到可以获得奖励，因此我们不需要负面示例。相比之下，传统学习需要通过撞车来**惩罚**不良结果。
- en: When you use DRL to learn using a simulator, you can get AI that can beat pilots
    on simulated flights ([https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/](https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/)),
    or you can get AI that can win on a video game simulator. The gaming world is
    a perfect test scenario for DRL. Let's say that you want to make a DRL model to
    play the famous game *Space Invaders*, shown in *Figure 15.1*; you can make a
    model that rewards destroying space invaders.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用深度强化学习（DRL）通过模拟器进行学习时，你可以获得能够在模拟飞行中击败飞行员的AI（[https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/](https://fortune.com/2020/08/20/f-16-fighter-pilot-versus-artificial-intelligence-simulation-darpa/)），或者你可以获得能够在视频游戏模拟器中获胜的AI。游戏世界是DRL的一个完美测试场景。假设你想要制作一个DRL模型来玩著名的游戏*太空侵略者*，如*图15.1*所示；你可以创建一个奖励摧毁太空侵略者的模型。
- en: '![](img/192d5354-6c0a-4be2-b3c5-248a2160a925.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/192d5354-6c0a-4be2-b3c5-248a2160a925.png)'
- en: Figure 15.1 – Space invaders video game simulator
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – 太空侵略者视频游戏模拟器
- en: If you make a traditional model to teach the user to **not die**, for example,
    then you will still lose because you will eventually be invaded from space. So,
    the best strategy to prevent invasion is both not dying and destroying space invaders.
    In other words, you reward the actions that lead to survival, which are to destroy
    the space invaders quickly while avoiding being killed by their bombs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建一个传统的模型来教用户**不死**，例如，你最终还是会失败，因为你最终会被太空入侵。因此，防止入侵的最佳策略是既不死又摧毁太空入侵者。换句话说，你奖励那些导致生存的行动，也就是在避免被它们的炸弹击中的同时迅速摧毁太空入侵者。
- en: In 2018, a new DRL research tool was released, called **Dopamine** (Castro,
    P. S., et al. 2018). Dopamine ([https://github.com/google/dopamine](https://github.com/google/dopamine))
    is meant for fast prototyping of reinforcement learning algorithms. Back in [Chapter
    2](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml), *Setup and Introduction to Deep
    Learning Frameworks*, we asked you to install Dopamine for this moment. We simply
    want to give you an idea of how easy Dopamine is so that you can go ahead and
    experiment with it if you are interested. In the following lines of code, we will
    simply load a pre-trained model (agent) and let it play the game.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，发布了一个新的DRL研究工具，名为**Dopamine**（Castro, P. S., 等，2018）。Dopamine（[https://github.com/google/dopamine](https://github.com/google/dopamine)）用于快速原型开发强化学习算法。在[第二章](0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml)中，*深度学习框架的设置与介绍*，我们要求你为此时安装Dopamine。我们只是想给你一个Dopamine的使用概念，让你可以在感兴趣时继续进行实验。在接下来的几行代码中，我们将简单地加载一个预训练模型（智能体），并让它玩游戏。
- en: 'This will make sure the library is installed, then load the pre-trained agent:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保库已安装，然后加载预训练的智能体：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sample trained agent, which in this case is called `rainbow`, is provided
    by the authors of Dopamine, but you can also train your own if you want.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个经过训练的智能体在这里被称为`rainbow`，是Dopamine的作者提供的，但如果你愿意，也可以训练自己的智能体。
- en: 'The next step is to make the agent run (that is, decide to take actions based
    on the rewards) for a number of steps, say `1024`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是让智能体运行（即，根据奖励决定采取的行动）若干步，例如`1024`步：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This piece of code may take a while to run. Internally, it connects to PyGame,
    which is a resource of game simulators for the Python community. It makes several
    decisions and avoids space invasion (and dying). As shown in *Figure 15.2*, the
    model describes the cumulative rewards obtained across time steps and the estimated
    probability of return for every action taken, such as stop, move left, move right,
    and shoot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可能需要一段时间才能运行。内部，它连接到PyGame，这是Python社区的一个游戏模拟器资源。它做出几个决策，并避免太空入侵（以及死亡）。如*图15.2*所示，模型描述了在时间步长中的累积奖励，以及每个动作的回报估计概率，例如停止、向左移动、向右移动和射击：
- en: '![](img/06b1145f-dc49-487e-8d6a-2f1471fb009c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06b1145f-dc49-487e-8d6a-2f1471fb009c.png)'
- en: 'Figure 15.2 – Left: Calculated rewards of the model across time steps. Right:
    Estimated probability of returns for every action'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 – 左：模型在时间步长中的计算奖励。右：每个行动的回报估计概率
- en: 'One of the interesting things about this is that you can visualize the agent
    at any of the time steps (frames) and see what the agent was doing at that specific
    time step using the plot in *Figure 15.2* as a reference to decide which time
    step to visualize. Let''s say that you want to visualize steps 540 or 550; you
    can do that as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点有趣的地方在于，你可以在任何时间步骤（帧）上可视化代理，并查看代理在特定时间步骤上做了什么。你可以使用*图 15.2*中的图来参考决定要可视化哪个时间步骤。假设你想要可视化步骤540或550，你可以按如下方式进行：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You substitute `<path to current directory>` with the path to your current working
    directory. This is because we need an absolute path, otherwise we could have used
    a relative path with `./` instead.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将`<path to current directory>`替换为你当前工作目录的路径。这是因为我们需要绝对路径，否则我们本可以使用带有`./`的相对路径。
- en: 'From this, it is self-evident that all frames are saved as images in the `./agent_viz/SpaceInvaders/rainbow/images/`
    directory. You can display them individually or even make a video. The preceding
    code produces the images shown in *Figure 15.3*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以显而易见，所有的帧都作为图像保存在`./agent_viz/SpaceInvaders/rainbow/images/`目录下。你可以单独展示它们，甚至制作成视频。前面的代码生成了*图
    15.3*中所示的图像：
- en: '![](img/a661df7f-d0ab-4ab6-a4d6-7af0359e5025.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a661df7f-d0ab-4ab6-a4d6-7af0359e5025.png)'
- en: 'Figure 15.3 – Left: step 540\. Right: step 550'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3 – 左：步骤 540。右：步骤 550
- en: Dopamine is as simple as that. We hope that you will be inspired by reinforcement
    learning and investigate further.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 多巴胺就是这么简单。我们希望你能从强化学习中得到启发并进一步研究。
- en: Self-supervised learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自监督学习
- en: 'Yann LeCun, one of the 2018 ACM Turing Award winners, said at the AAAI conference
    in 2020: *"the future is self-supervised."* He was implying that this area is
    exciting and has a lot of potential.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年ACM图灵奖获得者之一的杨·勒昆在2020年AAAI会议上说道：*"未来是自监督的。"* 他暗示这个领域非常激动人心，并且具有巨大的潜力。
- en: Self-supervision is a relatively new term used to move away from the term *unsupervision*.
    The term "unsupervised learning" might give the impression that there is no supervision
    when, in fact, unsupervised learning algorithms and models typically use more
    supervisory data than supervised models. Take, for example, the classification
    of MNIST data. It uses 10 labels as supervisory signals. However, in an autoencoder
    whose purpose is perfect reconstruction, every single pixel is a supervisory signal,
    so there are 784 supervisory signals from a 28 x 28 image, for example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督是一个相对较新的术语，用来替代*无监督*这一术语。术语“无监督学习”可能会给人一种没有监督的印象，而实际上，无监督学习算法和模型通常使用比监督模型更多的监督数据。以MNIST数据集的分类为例，它使用10个标签作为监督信号。然而，在一个目标是完美重建的自编码器中，每一个像素都是一个监督信号，因此以28
    x 28的图像为例，便有784个监督信号。
- en: Self-supervision is also used to mean models that combine some of the stages
    of unsupervised and supervised learning. For example, if we pipeline a model that
    learns representations unsupervised, then we can attach a model downstream that
    will learn to classify something supervised.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督还用来指代结合了无监督学习和监督学习某些阶段的模型。例如，如果我们管道化一个学习无监督表示的模型，我们可以在其下游附加一个模型，该模型将学习进行有监督的分类。
- en: Many of the recent advances in deep learning have been in self-supervision.
    It will be a good investment of your time if you try to learn more about self-supervised
    learning algorithms and models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最近深度学习的许多进展都发生在自监督领域。如果你能进一步学习自监督学习算法和模型，将是非常值得投入的时间。
- en: System 2 algorithms
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统 2 算法
- en: The famous economist Daniel Kahneman made popular the theory of dual process
    with his book *Thinking Fast and Slow* (Kahneman, D. 2011). The main idea is that
    there are highly complex tasks that we, as humans, are good at developing relatively
    fast and often without thinking too much; for example, drinking water, eating
    food, or looking at an object and recognizing it. These processes are done by
    *System 1*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 著名经济学家丹尼尔·卡尼曼在他的书《*思考，快与慢*》（Kahneman, D. 2011）中推广了双过程理论。主要思想是，有一些高度复杂的任务，我们人类能够相对快速且常常不需要过多思考地完成；例如，喝水、吃饭或看一个物体并识别它。这些过程由*系统
    1*完成。
- en: However, there are tasks that are not quite simple for the human mind, tasks
    that require our fully devoted attention, such as driving on an unfamiliar road,
    looking at a strange object that does not belong within the assumed context, or
    understanding an abstract painting. These processes are done by *System 2*. Another
    winner of the 2018 ACM Turing Award, Yoshua Bengio, has made the remark that deep
    learning has been very good at System 1 tasks, meaning that existing models can
    recognize objects and perform highly complex tasks relatively easily. However,
    deep learning has not made much progress on System 2 tasks. That is, the future
    of deep learning will be in solving those tasks that are very complex for human
    beings, which will probably involve combining different models across different
    domains with different learning types. Capsule neural networks might be a good
    alternative solution to System 2 tasks (Sabour, S., et al. 2017*)*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些任务对于人类大脑来说并不简单，这些任务需要我们全身心的注意力，比如在陌生的道路上驾驶，观察不属于预期背景的奇异物体，或理解一幅抽象画。这些过程由**系统2**完成。2018年ACM图灵奖的另一个获奖者Yoshua
    Bengio曾指出，深度学习在**系统1**任务上表现得非常出色，这意味着现有模型能够相对轻松地识别物体并执行高度复杂的任务。然而，深度学习在**系统2**任务上进展不大。也就是说，深度学习的未来将是解决那些对人类非常复杂的任务，这可能涉及跨不同领域、不同学习类型结合多种模型。胶囊神经网络可能是应对**系统2**任务的一个良好替代方案（Sabour,
    S., 等，2017年）。
- en: For these reasons, System 2 algorithms will probably be the future of deep learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，**系统2**算法可能会成为深度学习的未来。
- en: Now, let's look at resources available from Packt that can help in further studying
    these ideas.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看来自Packt的资源，它们可以帮助进一步研究这些概念。
- en: Learning with more resources from Packt
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自Packt的更多学习资源
- en: The following lists of books is not meant to be exhaustive, but a starting point
    for your next endeavor. These titles have come out at a great time when there
    is a lot of interest in the field. Regardless of your choice, you will not be
    disappointed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的书单并不意在详尽无遗，而是为你的下一个探索提供一个起点。这些书籍的出版正值该领域引起广泛关注的好时机。无论你选择哪一本，你都不会失望。
- en: Reinforcement learning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: '*Deep Reinforcement Learning Hands-On - Second Edition*, by Maxim Lapan, 2020.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《深度强化学习实战（第二版）》**，Maxim Lapan著，2020年出版。'
- en: '*The Reinforcement Learning Workshop*, by Alessandro Palmas *et al.*, 2020.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《强化学习工作坊》**，Alessandro Palmas *等*著，2020年出版。'
- en: '*Hands-On Reinforcement Learning for Games*, by Micheal Lanham, 2020.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《游戏中的实践强化学习》**，Micheal Lanham著，2020年出版。'
- en: '*PyTorch 1.x Reinforcement Learning Cookbook*, by Yuxi Liu, 2019.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《PyTorch 1.x强化学习实战》**，Yuxi Liu著，2019年出版。'
- en: '*Python Reinforcement Learning*, by Sudharsan Ravichandiran, 2019.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《Python强化学习》**，Sudharsan Ravichandiran著，2019年出版。'
- en: '*Reinforcement Learning Algorithms with Python*, by Andrea Lonza, 2019.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《Python强化学习算法》**，Andrea Lonza著，2019年出版。'
- en: Self-supervised learning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自监督学习
- en: '*The Unsupervised Learning Workshop*, by Aaron Jones *et. al.*, 2020.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《无监督学习工作坊》**，Aaron Jones *等*著，2020年出版。'
- en: '*Applied Unsupervised Learning with Python*, by Benjamin Johnston *et. al.*,
    2019.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《Python应用无监督学习》**，Benjamin Johnston *等*著，2019年出版。'
- en: '*Hands-On Unsupervised Learning with Python*, by Giuseppe Bonaccorso, 2019.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《用Python实践无监督学习》**，Giuseppe Bonaccorso著，2019年出版。'
- en: Summary
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This final chapter briefly covered new exciting topics and opportunities in
    deep learning. We discussed reinforcement learning, self-supervised algorithms,
    and System 2 algorithms. We also recommended some further resources from Packt,
    hoping that you will want to continue your learning and move forward in this field.
    At this point, you should know where to go from here, and be inspired by the future
    of deep learning. You should be knowledgeable of other recommended books in the
    area to continue with your learning journey.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要讨论了深度学习中的新兴话题和机会。我们讨论了强化学习、自监督算法和**系统2**算法。我们还推荐了一些来自Packt的进一步学习资源，希望你能继续学习，并在这一领域不断前进。此时，你应该知道接下来要做什么，并为深度学习的未来而感到鼓舞。你应该了解该领域的其他推荐书籍，以继续你的学习旅程。
- en: You are the future of deep learning, and the future is today. Go ahead and make
    things happen.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你是深度学习的未来，而未来就是今天。勇敢迈出一步，去实现你的目标。
- en: References
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. (2018).
    Dopamine: A research framework for deep reinforcement learning. arXiv preprint
    arXiv:1812.06110.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Castro, P. S., Moitra, S., Gelada, C., Kumar, S., 和Bellemare, M. G.（2018）。Dopamine:
    A research framework for deep reinforcement learning. arXiv预印本 arXiv:1812.06110。'
- en: Kahneman, D. (2011). *Thinking, Fast and Slow*. *Macmillan*.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahneman, D. (2011). *思考，快与慢*。*麦克米兰出版社*。
- en: Sabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules.
    In *Advances in neural information processing systems* (pp. 3856-3866).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour, S., Frosst, N., 和 Hinton, G. E. (2017). 胶囊之间的动态路由。载于 *神经信息处理系统的进展*（第3856-3866页）。
