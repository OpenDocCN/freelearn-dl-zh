- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Graph Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络
- en: In this chapter, we will look at a relatively new class of neural networks,
    the **Graph Neural Network** (**GNN**), which is ideally suited for processing
    graph data. Many real-life problems in areas such as social media, biochemistry,
    academic literature, and many others are inherently “graph-shaped,” meaning that
    their inputs are composed of data that can best be represented as graphs. We will
    cover what graphs are from a mathematical point of view, then explain the intuition
    behind “graph convolutions,” the main idea behind GNNs. We will then describe
    a few popular GNN layers that are based on variations of the basic graph convolution
    technique. We will describe three major applications of GNNs, covering node classification,
    graph classification, and edge prediction, with examples using TensorFlow and
    the **Deep Graph Library** (**DGL**). DGL provides the GNN layers we have just
    mentioned plus many more. In addition, it also provides some standard graph datasets,
    which we will use in the examples. Following on, we will show how you could build
    a DGL-compatible dataset from your own data, as well as your own layer using DGL’s
    low-level message-passing API. Finally, we will look at some extensions of graphs,
    such as heterogeneous graphs and temporal graphs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一种相对较新的神经网络类别——**图神经网络**（**GNN**），它非常适合处理图数据。许多现实生活中的问题，如社交媒体、生物化学、学术文献等，天生就是“图形化”的，意味着它们的输入由可以最适合用图表示的数据组成。我们将从数学角度讲解什么是图，然后解释“图卷积”这一概念，这是GNN的核心思想。接着，我们将介绍一些基于基本图卷积技术变体的流行GNN层。我们将描述GNN的三个主要应用，涵盖节点分类、图分类和边预测，并通过使用TensorFlow和**深度图书馆**（**DGL**）的示例来说明。DGL提供了我们刚刚提到的GNN层以及更多的层。此外，它还提供了一些标准的图数据集，我们将在示例中使用这些数据集。随后，我们将展示如何从自己的数据构建一个与DGL兼容的数据集，以及如何使用DGL的低级消息传递API构建自己的层。最后，我们将探讨图的扩展，例如异构图和时间图。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Graph basics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的基础
- en: Graph machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图机器学习
- en: Graph convolutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图卷积
- en: Common graph layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的图层
- en: Common graph applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的图应用
- en: Graph customizations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的定制
- en: Future directions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来方向
- en: All the code files for this chapter can be found at https://packt.link/dltfchp17
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 https://packt.link/dltfchp17 找到
- en: Let’s begin with the basics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始。
- en: Graph basics
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的基础
- en: 'Mathematically speaking, a graph *G* is a data structure consisting of a set
    of vertices (also called nodes) *V*, connected to each other by a set of edges
    *E*, i.e:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，一个图*G*是一个数据结构，包含一组顶点（也叫节点）*V*，这些顶点通过一组边*E*相互连接，即：
- en: '![](img/B18331_17_001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_001.png)'
- en: A graph can be equivalently represented as an adjacency matrix *A* of size (*n*,
    *n*) where *n* is the number of vertices in the set *V*. The element *A[I, j]*
    of this adjacency matrix represents the edge between vertex *i* and vertex *j*.
    Thus the element *A[I, j] = 1* if there is an edge between vertex *i* and vertex
    *j*, and 0 otherwise. In the case of weighted graphs, the edges might have their
    own weights, and the adjacency matrix will reflect that by setting the edge weight
    to the element *A[i, j]*. Edges may be directed or undirected. For example, an
    edge representing the friendship between a pair of nodes *x* and *y* is undirected,
    since *x* is friends with *y* implies that *y* is friends with *x*. Conversely,
    a directed edge can be one in a follower network (social media), where *x* following
    *y* does not imply that *y* follows *x*. For undirected graphs, *A[I, j] = A[j,
    i]*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图可以等价地表示为一个邻接矩阵*A*，其大小为（*n*, *n*），其中*n*是集合*V*中顶点的数量。该邻接矩阵的元素*A[I, j]*表示顶点*i*和顶点*j*之间的边。因此，若顶点*i*和顶点*j*之间有一条边，则元素*A[I,
    j] = 1*，否则为0。在加权图的情况下，边可能有自己的权重，邻接矩阵会通过将边的权重设置为元素*A[i, j]*来反映这一点。边可以是有向的或无向的。例如，表示节点*x*和节点*y*之间友谊的边是无向的，因为*x*是*y*的朋友意味着*y*也是*x*的朋友。相反，有向边可以是社交媒体中的关注网络，在这种情况下，*x*关注*y*并不意味着*y*关注*x*。对于无向图，*A[I,
    j] = A[j, i]*。
- en: Another interesting property of the adjacency matrix *A* is that *A*^n, i.e.,
    the product of *A* taken *n* times, exposes *n*-hop connections between nodes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵*A*的另一个有趣特性是，*A*^n，即* A*的* n*次乘积，揭示了节点之间的*n*跳连接。
- en: The graph-to-matrix equivalence is bi-directional, meaning the adjacency matrix
    can be converted back to the graph representation without any loss of information.
    Since **Machine Learning** (**ML**) methods, including **Deep Learning** (**DL**)
    methods, consume input data in the form of tensors, this equivalence means that
    graphs can be efficiently represented as inputs to all kinds of machine learning
    algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图到矩阵的等价性是双向的，这意味着邻接矩阵可以无损地转换回图的表示。由于**机器学习**（**ML**）方法，包括**深度学习**（**DL**）方法，消耗的输入数据是张量形式，因此这种等价性意味着图形可以有效地作为各种机器学习算法的输入表示。
- en: Each node can also be associated with its own feature vector, much like records
    in tabular input. Assuming a feature vector of size *f*, the set of nodes *X*
    can be represented as *(n, f)*. It is also possible for edges to have their own
    feature vectors. Because of the equivalence between graphs and matrices, graphs
    are usually represented by libraries as efficient tensor-based structures. We
    will examine this in more detail later in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点还可以与其自己的特征向量关联，就像表格输入中的记录一样。假设特征向量的大小为*f*，那么节点集*X*可以表示为*(n, f)*。边也可以有自己的特征向量。由于图和矩阵之间的等价性，图通常由库表示为高效的基于张量的结构。我们将在本章后面详细讨论这一点。
- en: Graph machine learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形机器学习
- en: 'The goal of any ML exercise is to learn a mapping *F* from an input space *X*
    to an output space *y*. Early machine learning methods required feature engineering
    to define the appropriate features, whereas DL methods can infer the features
    from the training data itself. DL works by hypothesizing a model *M* with random
    weights ![](img/B18331_17_002.png), formulating the task as an optimization problem
    over the parameters ![](img/B18331_17_003.png):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习任务的目标都是学习从输入空间*X*到输出空间*y*的映射*F*。早期的机器学习方法需要特征工程来定义合适的特征，而深度学习方法则可以从训练数据本身推断特征。深度学习通过假设一个具有随机权重的模型*M*来工作！[](img/B18331_17_002.png)，并将任务表述为一个关于参数![](img/B18331_17_003.png)的优化问题：
- en: '![](img/B18331_17_004.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_004.png)'
- en: 'and using gradient descent to update the model weights over multiple iterations
    until the parameters converge:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用梯度下降法在多次迭代中更新模型权重，直到参数收敛：
- en: '![](img/B18331_17_005.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_005.png)'
- en: Not surprisingly, GNNs follow this basic model as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，图神经网络（**GNNs**）也遵循这一基本模型。
- en: However, as you have seen in previous chapters, ML and DL are often optimized
    for specific structures. For example, you might instinctively choose a simple
    **FeedForward Network** (**FFN**) or “dense” network when working with tabular
    data, a **Convolutional Neural Network** (**CNN**) when dealing with image data,
    and a **Recurrent Neural Network** (**RNN**) when dealing with sequence data like
    text or time series. Some inputs may reduce to simpler structures such as pixel
    lattices or token sequences, but not necessarily so. In their natural form, graphs
    are topologically complex structures of indeterminate size and are not permutation
    invariant (i.e., instances are not independent of each other).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你在前几章中所看到的，机器学习（**ML**）和深度学习（**DL**）通常是针对特定结构进行优化的。例如，在处理表格数据时，你可能会直观地选择一个简单的**前馈网络**（**FFN**）或“密集”网络，在处理图像数据时选择**卷积神经网络**（**CNN**），而在处理像文本或时间序列这样的序列数据时选择**递归神经网络**（**RNN**）。有些输入可能简化为像素格子或令牌序列这样的结构，但也不一定如此。在其自然形式下，图形是拓扑复杂、大小不确定的结构，并且不是置换不变的（即实例之间不是相互独立的）。
- en: For these reasons, we need special tooling to deal with graph data. We will
    introduce in this chapter the DGL, a cross-platform graph library that supports
    users of MX-Net, PyTorch, and TensorFlow through the use of a configurable backend
    and is widely considered one of the most powerful and easy-to-use graph libraries
    available.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，我们需要特殊的工具来处理图数据。本章将介绍DGL，它是一个跨平台的图形库，支持MX-Net、PyTorch和TensorFlow用户，通过使用可配置的后端，广泛被认为是最强大且易于使用的图形库之一。
- en: Graph convolutions – the intuition behind GNNs
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图卷积——图神经网络的直觉
- en: The convolution operator, which effectively allows values of neighboring pixels
    on a 2D plane to be aggregated in a specific way, has been successful in deep
    neural networks for computer vision. The 1-dimensional variant has seen similar
    success in natural language processing and audio processing as well. As you will
    recall from *Chapter 3*, *Convolutional Neural Networks*, a network applies convolution
    and pooling operations across successive layers and manages to learn enough global
    features across a sufficiently large number of input pixels to succeed at the
    task it is trained for.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积算子有效地允许在二维平面上将相邻像素的值以特定方式聚合，这在计算机视觉中的深度神经网络中取得了成功。其一维变体在自然语言处理和音频处理领域也取得了类似的成功。正如你在*第3章*《卷积神经网络》中回忆的那样，网络在连续的层之间应用卷积和池化操作，并能够学习到足够多的全局特征，从而在它所训练的任务中获得成功。
- en: Examining the analogy from the other end, an image (or each channel of an image)
    can be thought of as a lattice-shaped graph where neighboring pixels link to each
    other in a specific way. Similarly, a sequence of words or audio signals can be
    thought of as another linear graph where neighboring tokens are linked to each
    other. In both cases, the deep learning architecture progressively applies convolutions
    and pooling operations across neighboring vertices of the input graph until it
    learns to perform the task, which is generally classification. Each convolution
    step encompasses an additional level of neighbors. For example, the first convolution
    merges signals from distance 1 (immediate) neighbors of a node, the second merges
    signals from distance 2 neighbors, and so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，图像（或图像的每个通道）可以被视为一种网格形状的图，其中相邻的像素以特定的方式彼此连接。类似地，一串单词或音频信号也可以被看作是另一个线性图，其中相邻的词元彼此相连。在这两种情况下，深度学习架构会在输入图的相邻节点之间逐步应用卷积和池化操作，直到它学会执行任务，通常是分类任务。每一步卷积都涉及额外层次的邻居。例如，第一个卷积合并来自距离1（直接）邻居的信号，第二个合并来自距离2的邻居的信号，以此类推。
- en: '*Figure 17.1* shows the equivalence between a 3 x 3 convolution in a CNN and
    the corresponding “graph convolution” operation. The convolution operator applies
    the filter, essentially a set of nine learnable model parameters, to the input
    and combines them via a weighted sum. You can achieve the same effect by treating
    the pixel neighborhood as a graph of nine nodes centered around the middle pixel.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.1* 显示了CNN中的3 x 3卷积与对应的“图卷积”操作之间的等价性。卷积算子将滤波器（本质上是一组九个可学习的模型参数）应用于输入，并通过加权和将它们合并。通过将像素邻域视为一个以中心像素为核心的九个节点的图，你可以达到相同的效果。'
- en: 'A graph convolution on such a structure would just be a weighted sum of the
    node features, the same as the convolution operator in the CNN:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种结构上的图卷积将只是节点特征的加权和，这与CNN中的卷积算子相同：
- en: '![Diagram  Description automatically generated](img/B18331_17_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图示 自动生成的说明](img/B18331_17_01.png)'
- en: 'Figure 17.1: Parallels between convolutions in images and convolutions in graphs.
    Image source: CS-224W machine learning with Graphs, Stanford Univ.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：图像卷积和图卷积之间的相似之处。图像来源：CS-224W机器学习与图，斯坦福大学。
- en: 'The corresponding equations for the convolution operation on the CNN and the
    graph convolution are shown below. As you can see, on CNN, the convolution can
    be considered as a weighted linear combination of the input pixel and each of
    its neighbors. Each pixel brings its own weight in the form of the filter being
    applied. On the other hand, the graph convolution is also a weighted linear combination
    of the input pixel and an aggregate of all its neighbors. The aggregate effect
    of all neighbors is averaged into the convolution output:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的卷积操作和图卷积的相应方程如下所示。正如你所看到的，在CNN中，卷积可以视为输入像素和它的每个邻居的加权线性组合。每个像素都以所应用的滤波器的形式带来了自己的权重。另一方面，图卷积也是输入像素和所有邻居的聚合加权线性组合。所有邻居的聚合效应会平均到卷积输出中：
- en: '![](img/B18331_17_006.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_006.png)'
- en: '![](img/B18331_17_007.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_007.png)'
- en: Graph convolutions are thus a variation of convolutions that we are already
    familiar with. In the following section, we will see how these convolutions can
    be composed to build different kinds of GCN layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，图卷积是我们已经熟悉的卷积的一种变体。在接下来的部分中，我们将看到如何将这些卷积组合起来构建不同类型的GCN层。
- en: Common graph layers
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的图层
- en: All the graph layers that we discuss in this section use some variation of the
    graph convolution operation described above. Contributors to graph libraries such
    as DGL provide prebuilt versions of many of these layers within a short time of
    it being proposed in an academic paper, so you will realistically never have to
    implement one of these. The information here is mainly for understanding how things
    work under the hood.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的所有图层都使用了上述描述的图卷积操作的某种变体。像DGL这样的图库贡献者，在学术论文提出这些层之后不久，就会提供许多这些层的预构建版本，因此实际上你永远不需要自己实现其中的一个。这部分信息主要是为了帮助理解其底层工作原理。
- en: Graph convolution network
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图卷积网络
- en: The **Graph Convolution Network** (**GCN**) is the graph convolution layer proposed
    by Kipf and Welling [1]. It was originally presented as a scalable approach for
    semi-supervised learning on graph-structured data. They describe the GCN as an
    operation over the node feature vectors *X* and the adjacency matrix *A* of the
    underlying graph and point out that this can be exceptionally powerful when the
    information in *A* is not present in the data *X*, such as citation links between
    documents in a citation network, or relations in a knowledge graph.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**图卷积网络**（**GCN**）是由Kipf和Welling提出的图卷积层[1]。最初，它被提出作为一种可扩展的半监督学习方法，用于图结构数据上。他们将GCN描述为对节点特征向量*X*和底层图的邻接矩阵*A*的操作，并指出当*A*中的信息不包含在数据*X*中时，这种方法特别强大，例如在引文网络中，文档之间的引文链接，或在知识图谱中的关系。'
- en: 'GCNs combine the value of each node’s feature vector with those of its neighbors
    using some weights (initialized to random values). Thus, for every node, the sum
    of the neighboring node’s features is added. This operation can be represented
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GCN结合了每个节点特征向量与其邻居的特征向量，通过一些权重（初始化为随机值）进行加权。因此，对于每个节点，邻居节点的特征之和会被加到一起。这个操作可以表示为如下：
- en: '![](img/B18331_17_008.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_008.png)'
- en: Here the *update* and *aggregate* are different kinds of summation functions.
    This sort of projection on node features is called a message-passing mechanism.
    A single iteration of this message passing is equivalent to a graph convolution
    over each node’s immediate neighbors. If we wish to incorporate information from
    more distant nodes, we can repeat this operation several times.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*update*和*aggregate*是不同类型的求和函数。这种对节点特征的投影被称为消息传递机制。这个消息传递的单次迭代等同于对每个节点的直接邻居进行图卷积。如果我们希望结合来自更远节点的信息，可以多次重复这个操作。
- en: 'The following equation describes the output of the GCN at layer *(l+1)* at
    node *i*. Here, *N(i)* is the set of neighbors of node *I* (including itself),
    *c*[ij] is the product of the square root of node degrees, and sigma is an activation
    function. The *b(l)* term is an optional bias term:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程描述了GCN在第*(l+1)*层对节点*i*的输出。这里，*N(i)*是节点*i*的邻居集合（包括它本身），*c*[ij]是节点度数平方根的乘积，sigma是激活函数。*b(l)*项是一个可选的偏置项：
- en: '![](img/B18331_17_009.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_009.png)'
- en: Next up, we will look at the graph attention network, a variant of the GCN where
    the coefficients are learned via an attentional mechanism instead of being explicitly
    defined.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论图注意力网络（Graph Attention Network，简称GAT），它是GCN的一个变体，其中系数是通过注意力机制学习的，而不是显式定义的。
- en: Graph attention network
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图注意力网络
- en: The **Graph Attention Network** (**GAT**) layer was proposed by Velickovic,
    et al. [2]. Like the GCN, the GAT performs local averaging of its neighbors’ features.
    The difference is instead of explicitly specifying the normalization term *c*[ij],
    the GAT allows it to be learned using self-attention over the node features to
    do so. The corresponding normalization term is written as ![](img/B18331_11_021.png)
    for the GAT, which is computed based on the hidden features of the neighboring
    nodes and the learned attention vector. Essentially, the idea behind the GAT is
    to prioritize feature signals from similar neighbor nodes compared to dissimilar
    ones.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**图注意力网络**（**GAT**）层是Velickovic等人提出的[2]。与GCN类似，GAT也对其邻居的特征进行局部平均。不同之处在于，GAT不是显式指定归一化项*c*[ij]，而是通过自注意力机制在节点特征上学习它。对应的归一化项写作![](img/B18331_11_021.png)，它是基于邻居节点的隐藏特征和学习到的注意力向量计算出来的。本质上，GAT的理念是优先考虑来自相似邻居节点的特征信号，而非来自不相似邻居节点的信号。'
- en: 'Every neighbor ![](img/B18331_17_011.png) neighborhood *N*(*i*) of node *i*
    sends its own vector of attentional coefficients ![](img/B18331_17_012.png). The
    following set of equations describes the output of the GAT at layer (*i+1*) for
    node *i*. The attention ![](img/B18331_11_021.png) is computed using Bahdanau’s
    attention model using a feedforward network:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个邻居 ![](img/B18331_17_011.png) 的邻域 *N*(*i*) 向节点 *i* 发送其自身的注意力系数向量 ![](img/B18331_17_012.png)。以下方程组描述了
    GAT 在第 (*i+1*) 层对节点 *i* 的输出。注意力 ![](img/B18331_11_021.png) 是使用 Bahdanau 的注意力模型，通过前馈网络计算得到的：
- en: '![](img/B18331_17_014.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_014.png)'
- en: '![](img/B18331_17_015.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_015.png)'
- en: '![](img/B18331_17_016.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_016.png)'
- en: GCN and GAT architectures are suitable for small to medium-sized networks. The
    GraphSAGE architecture, described in the next section, is more suitable for larger
    networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GCN 和 GAT 架构适用于小型到中型网络。下一节中描述的 GraphSAGE 架构则更适合于较大的网络。
- en: GraphSAGE (sample and aggregate)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphSAGE（采样与聚合）
- en: So far, the convolutions we have considered require that all nodes in the graph
    be present during the training, and are therefore transductive and do not naturally
    generalize to unseen nodes. Hamilton, Ying, and Leskovec [3] proposed GraphSAGE,
    a general, inductive framework that can generate embeddings for previously unseen
    nodes. It does so by sampling and aggregating from a node’s local neighborhood.
    GraphSAGE has proved successful at node classification on temporally evolving
    networks such as citation graphs and Reddit post data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑的卷积要求图中的所有节点在训练时都必须出现，因此它们是传导式的，并且不能自然地推广到未见过的节点。Hamilton、Ying 和 Leskovec
    [3] 提出了 GraphSAGE，这是一个通用的、归纳式的框架，能够为之前未见过的节点生成嵌入。它通过从节点的本地邻域进行采样和聚合来实现这一点。GraphSAGE
    已在动态演化的网络（如引用图和 Reddit 帖子数据）中的节点分类中取得了成功。
- en: GraphSAGE samples a subset of neighbors instead of using them all. It can define
    a node neighborhood using random walks and sum up importance scores to determine
    the optimum sample. An aggregate function can be one of MEAN, GCN, POOL, and LSTM.
    Mean aggregation simply takes the element-wise mean of the neighbor vectors. The
    LSTM aggregation is more expressive but is inherently sequential and not symmetric;
    it is applied on an unordered set derived from a random permutation of the node’s
    neighbors. The POOL aggregation is both symmetric and trainable; here, each neighbor
    vector is independently fed through a fully connected neural network and max pooling
    is applied across the aggregate information across the neighbor set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE 通过采样一部分邻居而不是使用全部邻居。它可以通过随机游走定义节点邻域，并汇总重要性分数来确定最佳样本。聚合函数可以是 MEAN、GCN、POOL
    或 LSTM。均值聚合只是简单地取邻居向量的元素级平均值。LSTM 聚合更具表现力，但本质上是序列性的且不具对称性；它作用于从节点邻居的随机排列中得到的无序集合。POOL
    聚合既具对称性又可训练；在这里，每个邻居向量独立地通过一个全连接神经网络，并对邻居集合中的信息应用最大池化。
- en: 'This set of equations shows how the output for node *i* at layer *(l+1)* is
    generated from node *i* and its neighbors *N(i)* at layer *l*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程组展示了如何从节点 *i* 和它在第 *l* 层的邻居 *N(i)* 生成节点 *i* 在第 *(l+1)* 层的输出：
- en: '![](img/B18331_17_017.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_017.png)'
- en: '![](img/B18331_17_018.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_018.png)'
- en: '![](img/B18331_17_019.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_019.png)'
- en: Now that we have seen strategies for handling large networks using GNNs, we
    will look at strategies for maximizing the representational (and therefore the
    discriminative) power of GNNs, using the graph isomorphism network.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了使用 GNN 处理大规模网络的策略后，我们将探讨如何利用图同构网络最大化 GNN 的表示能力（也就是区分能力）。
- en: Graph isomorphism network
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图同构网络
- en: Xu, et al. [4] proposed the **Graph Isomorphism Network** (**GIN**) as a graph
    layer with more expressive power compared to the ones available. Graph layers
    with high expressive power should be able to distinguish between a pair of graphs
    that are topologically similar but not identical. They showed that GCNs and GraphSAGE
    are unable to distinguish certain graph structures. They also showed that SUM
    aggregation is better than MEAN and MAX aggregation in terms of distinguishing
    graph structures. The GIN layer thus provides a better way to represent neighbor’s
    aggregation compared to GCNs and GraphSAGE.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人[4]提出了**图同构网络**（**GIN**），作为一种比现有图层具有更强表达能力的图层。具有高表达能力的图层应能够区分一对拓扑结构相似但不完全相同的图。研究表明，GCN
    和 GraphSAGE 无法区分某些图结构。他们还展示了，在区分图结构方面，SUM 聚合优于 MEAN 和 MAX 聚合。因此，GIN 图层提供了一种比 GCN
    和 GraphSAGE 更好的邻居聚合表示方法。
- en: 'The following equation shows the output at node *i* and layer *(l+1)*. Here,
    the function *f*[θ] is a callable activation function, *aggregate* is an aggregation
    function such as SUM, MAX, or MEAN, and ![](img/B18331_10_003.png) is a learnable
    parameter that will be learned over the course of the training:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程显示了节点*i*和层*(l+1)*的输出。在这里，函数*f*[θ]是一个可调用的激活函数，*aggregate*是一个聚合函数，如SUM、MAX或MEAN，且![](img/B18331_10_003.png)是一个可学习的参数，它将在训练过程中学习：
- en: '![](img/B18331_17_021.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_021.png)'
- en: Having been introduced to several popular GNN architectures, let us now direct
    our attention to the kind of tasks we can do with GNNs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了几种流行的GNN架构后，让我们现在关注可以使用GNN完成的任务类型。
- en: Common graph applications
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见图应用
- en: 'We will now look at some common applications of GNNs. Typically, applications
    fall into one of the three major classes listed below. In this section, we will
    see code examples on how to build and train GNNs for each of these tasks, using
    TensorFlow and DGL:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下GNN的一些常见应用。通常，应用可以分为以下三个主要类别。在本节中，我们将展示如何使用TensorFlow和DGL构建和训练GNN以解决这些任务的代码示例：
- en: Node classification
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点分类
- en: Graph classification
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图分类
- en: Edge classification (or link prediction)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边分类（或链接预测）
- en: There are other applications of GNNs as well, such as graph clustering or generative
    graph models, but they are less common and we will not consider them here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GNN还有其他应用，如图聚类或生成图模型，但它们较为少见，在这里我们不予考虑。
- en: Node classification
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点分类
- en: Node classification is a popular task on graph data. Here, a model is trained
    to predict the node category. Non-graph classification methods can use the node
    feature vectors alone to do so, and some pre-GNN methods such as DeepWalk and
    node2vec can use the adjacency matrix alone, but GNNs are the first class of techniques
    that can use both the node feature vectors and the connectivity information together
    to do node classification.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 节点分类是图数据中一个流行的任务。在这里，模型被训练来预测节点类别。非图分类方法可以仅使用节点特征向量来实现，而一些预GNN方法如DeepWalk和node2vec可以仅使用邻接矩阵，但GNN是第一类能够将节点特征向量和连接信息一起使用来进行节点分类的技术。
- en: Essentially, the idea is to apply one or more graph convolutions (as described
    in the previous section) to all nodes of a graph, to project the feature vector
    of the node to a corresponding output category vector that can be used to predict
    the node category. Our node classification example will use the CORA dataset,
    a collection of 2,708 scientific papers classified into one of seven categories.
    The papers are organized into a citation network, which contains 5,429 links.
    Each paper is described by a word vector of size 1,433.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这个思想是将一个或多个图卷积（如前一节所述）应用于图中的所有节点，将节点的特征向量投影到一个对应的输出类别向量，以此来预测节点的类别。我们的节点分类示例将使用CORA数据集，这是一个包含2,708篇科学论文的数据集，每篇论文被分类为七个类别之一。这些论文被组织成一个引用网络，包含5,429个链接。每篇论文由一个大小为1,433的词向量描述。
- en: 'We first set up our imports. If you have not already done so, you will need
    to install the DGL library into your environment with `pip install dgl`. You will
    also need to set the environment variable `DGLBACKEND` to TensorFlow. On the command
    line, this is achieved by the command `export DGLBACKEND=tensorflow`, and in a
    notebook environment, you can try using the magic `%env DGLBACKEND=tensorflow`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设置导入。如果你还没有这样做，你需要通过`pip install dgl`将DGL库安装到你的环境中。你还需要将环境变量`DGLBACKEND`设置为TensorFlow。在命令行中，可以通过命令`export
    DGLBACKEND=tensorflow`来实现，在笔记本环境中，你可以尝试使用魔法命令`%env DGLBACKEND=tensorflow`：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The CORA dataset is pre-packaged as a DGL dataset, so we load the dataset into
    memory using the following call:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CORA数据集被预先包装为DGL数据集，因此我们使用以下调用将数据集加载到内存中：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first time this is called, it will log that it is downloading and extracting
    to a local file. Once done, it will print out some useful statistics about the
    CORA dataset. As you can see, there are 2,708 nodes and 10,566 edges in the graph.
    Each node has a feature vector of size 1,433 and a node is categorized as being
    in one of seven classes. In addition, we see that it has 140 training samples,
    500 validation samples, and 1,000 test samples:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次调用时，它将记录正在下载并提取到本地文件。一旦完成，它将打印出一些关于CORA数据集的有用统计信息。正如你所看到的，图中有2,708个节点和10,566条边。每个节点有一个大小为1,433的特征向量，每个节点被分类为七个类别之一。此外，我们还看到它有140个训练样本、500个验证样本和1,000个测试样本：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since this is a graph dataset, it is expected to contain data pertaining to
    a set of graphs. However, CORA is a single citation graph. You can verify this
    by `len(dataset)`, which will give you `1`. This also means that downstream code
    will work on the graph given by `dataset[0]` rather than on the complete dataset.
    The node features will be contained in the dictionary `dataset[0].ndata` as key-value
    pairs, and the edge features in `dataset[0].edata`. The `ndata` contains the keys
    `train_mask`, `val_mask`, and `test_mask`, which are Boolean masks signifying
    which nodes are part of the train, validation, and test splits, respectively,
    and a `feat` key, which contains the feature vector for each node in the graph.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个图数据集，它预计包含与一组图相关的数据。然而，CORA是一个单一的引用图。你可以通过`len(dataset)`验证这一点，它将返回`1`。这也意味着下游代码将作用于由`dataset[0]`提供的图，而不是整个数据集。节点特征将包含在字典`dataset[0].ndata`中作为键值对，边特征则包含在`dataset[0].edata`中。`ndata`包含`train_mask`、`val_mask`和`test_mask`键，这些布尔掩码表示哪些节点属于训练集、验证集和测试集，另有一个`feat`键，其中包含图中每个节点的特征向量。
- en: We will build a `NodeClassifier` network with two `GraphConv` layers. Each layer
    will compute a new node representation by aggregating neighbor information. `GraphConv`
    layers are just simple `tf.keras.layers.Layer` objects and can therefore be stacked.
    The first `GraphConv` layer projects the incoming feature size (1,433) to a hidden
    feature vector of size 16, and the second `GraphConv` layer projects the hidden
    feature vector to an output category vector of size 2, from which the category
    is read.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个包含两个`GraphConv`层的`NodeClassifier`网络。每一层将通过聚合邻居信息来计算新的节点表示。`GraphConv`层只是简单的`tf.keras.layers.Layer`对象，因此可以堆叠。第一层`GraphConv`将输入特征的大小（1,433）映射到一个大小为16的隐藏特征向量，第二层`GraphConv`将隐藏特征向量映射到一个大小为2的输出类别向量，从中可以读取类别。
- en: 'Note that `GraphConv` is just one of many graph layers that we can drop into
    the `NodeClassifier` model. DGL makes available a variety of graph convolution
    layers that can be used to replace `GraphConv` if needed:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`GraphConv`只是我们可以放入`NodeClassifier`模型中的众多图层之一。DGL提供了多种图卷积层，如果需要，可以用它们替换`GraphConv`：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will train this model with the CORA dataset using the code shown below. We
    will use the `AdamW` optimizer (a variation of the more popular `Adam` optimizer
    that results in models with better generalization capabilities), with a learning
    rate of *1e-2* and weight decay of *5e-4*. We will train for 200 epochs. Let us
    also detect if we have a GPU available, and if so, assign the graph to the GPU.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码在CORA数据集上训练该模型。我们将使用`AdamW`优化器（这是更流行的`Adam`优化器的变种，能够使模型具有更好的泛化能力），学习率设置为*1e-2*，权重衰减为*5e-4*。我们将训练200个周期。我们还将检测是否有可用的GPU，如果有，我们会将图分配给GPU。
- en: 'TensorFlow will automatically move the model to the GPU if the GPU is detected:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到GPU，TensorFlow会自动将模型移动到GPU上：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We also define a `do_eval()` method that computes the accuracy given the features
    and the Boolean mask for the split being evaluated:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个`do_eval()`方法，它通过给定特征和用于评估拆分的布尔掩码来计算准确度：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we are ready to set up and run our training loop as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好按照如下方式设置并运行我们的训练循环：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the training run shows the training loss decreasing from `1.9`
    to `0.02` and the validation accuracy increasing from `0.13` to `0.78`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 训练运行的输出显示训练损失从`1.9`下降到`0.02`，验证准确率从`0.13`上升到`0.78`：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now evaluate our trained node classifier against the hold-out test split:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以评估我们训练的节点分类器在保留测试集上的表现：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This prints out the overall accuracy of the model against the hold-out test
    split:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出模型在保留测试集上的总体准确度：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Graph classification
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图分类
- en: Graph classification is done by predicting some attribute of the entire graph
    by aggregating all node features and applying one or more graph convolutions to
    it. This could be useful, for example, when trying to classify molecules during
    drug discovery as having a particular therapeutic property. In this section, we
    will showcase graph classification using an example.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图分类是通过聚合所有节点特征并对其应用一个或多个图卷积来预测整个图的某个属性完成的。例如，在药物发现过程中，当试图将分子分类为具有某种特定治疗特性时，这可能会很有用。在本节中，我们将通过一个示例展示图分类。
- en: 'In order to run the example, please make sure DGL is installed and set to use
    the TensorFlow backend; refer to the previous section on node classification for
    information on how to do this. To begin the example, let us import the necessary
    libraries:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行该示例，请确保已经安装DGL并设置为使用TensorFlow后端；有关如何操作的信息，请参阅上一节中的节点分类部分。要开始示例，请导入必要的库：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will use the protein dataset from DGL. The dataset is a set of graphs, each
    with node features and a single label. Each graph represents a protein molecule
    and each node in the graph represents an atom in the molecule. Node features list
    the chemical properties of the atom. The label indicates if the protein molecule
    is an enzyme:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用DGL提供的蛋白质数据集。该数据集是一组图，每个图都有节点特征和一个标签。每个图表示一个蛋白质分子，图中的每个节点表示分子中的一个原子。节点特征列出了原子的化学性质。标签表示该蛋白质分子是否是酶：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The call above downloads the protein dataset locally and prints out some information
    about the dataset. As you can see, each node has a feature vector of size `3`,
    the number of graph categories is `2` (enzyme or not), and the number of graphs
    in the dataset is `1113`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的调用会将蛋白质数据集下载到本地，并打印出一些数据集的信息。如您所见，每个节点的特征向量大小为`3`，图的类别数量为`2`（酶或非酶），数据集中的图数量为`1113`：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will first split the dataset into training, validation, and test. We will
    use the training dataset to train our GNN, validate using the validation dataset,
    and publish the results of our final model against the test dataset:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先把数据集分为训练集、验证集和测试集。我们将使用训练集来训练我们的GNN，使用验证集进行验证，并在测试集上发布最终模型的结果：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This splits the dataset into a training, validation, and test split of 801,
    89, and 223 graphs, respectively. Since our datasets are large, we need to train
    our network using mini-batches so as not to overwhelm GPU memory. So, this example
    will also demonstrate mini-batch processing using our data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据集分成训练集、验证集和测试集，分别包含801、89和223个图。由于我们的数据集很大，我们需要使用小批量来训练网络，以免占满GPU内存。因此，本示例还将展示如何使用我们的数据进行小批量处理。
- en: 'Next, we define our GNN for graph classification. This consists of two `GraphConv`
    layers stacked together that will encode the nodes into their hidden representations.
    Since the objective is to predict a single category for each graph, we need to
    aggregate all the node representations into a graph-level representation, which
    we do by averaging the node representations using `dgl.mean_nodes()`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义用于图分类的GNN。它由两个`GraphConv`层堆叠而成，这些层将节点编码为它们的隐藏表示。由于目标是为每个图预测一个单一类别，我们需要将所有节点表示聚合为图级表示，我们通过使用`dgl.mean_nodes()`平均节点表示来实现：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For the training, we set the training parameters and the `do_eval()` function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们设置了训练参数和`do_eval()`函数：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we define and run our training loop to train our `GraphClassifier`
    model. We use the `Adam` optimizer with a learning rate of `1e-2` and the `SparseCategoricalCrossentropy`
    as the loss function, training, or `20` epochs:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义并运行我们的训练循环来训练`GraphClassifier`模型。我们使用`Adam`优化器，学习率为`1e-2`，损失函数为`SparseCategoricalCrossentropy`，进行`20`轮训练：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output shows that the loss decreases and validation accuracy increases
    as the `GraphClassifier` model is trained over 20 epochs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，随着`GraphClassifier`模型训练了20轮，损失逐渐下降，验证准确度逐渐提高：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we evaluate the trained model against our hold-out test dataset:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在保留的测试数据集上评估训练好的模型：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This prints out the accuracy of the trained `GraphClassifier` model against
    the held-out test split:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印出训练好的`GraphClassifier`模型在保留的测试集上的准确度：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The accuracy shows that the model can successfully identify a molecule as an
    enzyme or non-enzyme slightly less than 70% of the time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度显示该模型可以成功地识别一个分子是酶还是非酶的概率略低于70%。
- en: Link prediction
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接预测
- en: Link prediction is a type of edge classification problem, where the task is
    to predict if an edge exists between two given nodes in the graph.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 链接预测是一种边分类问题，任务是预测图中两个给定节点之间是否存在边。
- en: Many applications, such as social recommendation, knowledge graph completion,
    etc., can be formulated as link prediction, which predicts whether an edge exists
    between a pair of nodes. In this example, we will predict if a citation relationship,
    either citing or cited, exists between two papers in a citation network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用程序，例如社交推荐，知识图完成等，可以被归纳为链接预测，即预测两个节点之间是否存在边缘关系，无论是引用还是被引用，在引用网络中的两篇论文之间。
- en: The general approach would be to treat all edges in the graph as positive examples
    and sample a number of non-existent edges as negative examples and train the link
    prediction classifier for binary classification (edge exists or not) on these
    positive and negative examples.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的方法是将图中的所有边都视为正例，并采样一些不存在的边作为负例，并在这些正例和负例上训练链接预测分类器进行二元分类（边是否存在）。
- en: 'Before running the example, please make sure DGL is installed and set to use
    the TensorFlow backend; refer to the *Node classification* section for information
    on how to do this. Let us start by importing the necessary libraries:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行示例之前，请确保安装了DGL并设置为使用TensorFlow后端；请参考*节点分类*部分获取如何执行此操作的信息。让我们从导入必要的库开始：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For our data, we will reuse the CORA citation graph from the DGL datasets that
    we had used for our node classification example earlier. We already know what
    the dataset looks like, so we won’t dissect it again here. If you would like to
    refresh your memory, please refer to the node classification example for the relevant
    details:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，我们将重复使用我们之前用于节点分类示例的DGL数据集中的CORA引用图。我们已经知道数据集的样子，所以这里不会再详细解剖它。如果你想要刷新记忆，请参考节点分类示例获取相关细节：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let us prepare our data. For training our link prediction model, we need
    a set of positive edges and a set of negative edges. Positive edges are one of
    the 10,556 edges that already exist in the CORA citation graph, and negative edges
    are going to be 10,556 node pairs without connecting edges sampled from the rest
    of the graph. In addition, we need to split both the positive and negative edges
    into training, validation, and test splits:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们准备我们的数据。为了训练我们的链接预测模型，我们需要一组正边和一组负边。正边是CORA引用图中已经存在的10,556条边之一，负边将从图的其余部分中采样的10,556对节点对。此外，我们需要将正边和负边分割为训练、验证和测试集：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now construct a GNN that will compute the node representation using two
    `GraphSAGE` layers, each layer computing the node representation by averaging
    its neighbor information:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们构建一个GNN，它将使用两个`GraphSAGE`层计算节点表示，每个层通过平均其邻居信息来计算节点表示：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'However, link prediction requires us to compute representations of pairs of
    nodes, DGL recommends that you treat the pairs of nodes as another graph since
    you can define a pair of nodes as an edge. For link prediction, we will have a
    positive graph containing all the positive examples as edges, and a negative graph
    containing all the negative examples as edges. Both positive and negative graphs
    contain the same set of nodes as the original graph:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，链接预测要求我们计算节点对的表示，DGL建议您将节点对视为另一个图，因为您可以将节点对定义为一条边。对于链接预测，我们将有一个包含所有正例作为边的正图，以及一个包含所有负例作为边的负图。正图和负图都包含与原始图相同的节点集：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we will define a predictor class that will take the set of node representations
    from the `LinkPredictor` class and use the `DGLGraph.apply_edges` method to compute
    edge feature scores, which are the dot product of the source node features and
    the destination node features (both output together from the `LinkPredictor` in
    this case):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个预测器类，它将从`LinkPredictor`类中获取节点表示集，并使用`DGLGraph.apply_edges`方法计算边特征分数，这些分数是源节点特征和目标节点特征的点积（在这种情况下一起从`LinkPredictor`输出）：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can also build a custom predictor such as a multi-layer perceptron with
    two dense layers, as the following code shows. Note that the `apply_edges` method
    describes how the edge score is calculated:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以构建一个自定义预测器，例如具有两个密集层的多层感知器，如下面的代码所示。请注意，`apply_edges`方法描述了如何计算边缘分数：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We instantiate the `LinkPredictor` model we defined earlier, select the `Adam`
    optimizer, and declare our loss function to be `BinaryCrossEntropy` (since our
    task is binary classification). The predictor head that we will use in our example
    is the `DotProductPredictor`. However, the `MLPPredictor` can be used as a drop-in
    replacement instead; just replace the `pred` variable below to point to the `MLPPredictor`
    instead of the `DotProductPredictor`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了之前定义的`LinkPredictor`模型，选择了`Adam`优化器，并声明我们的损失函数为`BinaryCrossEntropy`（因为我们的任务是二分类）。在我们的示例中，将使用的预测头是`DotProductPredictor`。但是，`MLPPredictor`也可以作为替代品使用；只需将下面的`pred`变量替换为指向`MLPPredictor`，而不是`DotProductPredictor`：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We also define a couple of convenience functions for our training loop. The
    first one computes the loss between the scores returned from the positive graph
    and the negative graphs, and the second computes the **Area Under the Curve**
    (**AUC**) from the two scores. AUC is a popular metric to evaluate binary classification
    models:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为训练循环定义了一些便利函数。第一个函数计算从正图和负图返回的得分之间的损失，第二个函数根据这两个得分计算**曲线下面积**（**AUC**）。AUC是评估二分类模型的常用指标：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now train our `LinkPredictor` GNN for 100 epochs of training, using the
    following training loop:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在训练我们的`LinkPredictor` GNN，进行100个周期的训练，使用以下训练循环：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This returns the following training logs:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下训练日志：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can now evaluate the trained model against the hold-out test set:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将训练好的模型与保留的测试集进行评估：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This returns the following test AUC for our `LinkPredictor` GNN:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回我们`LinkPredictor` GNN的以下测试AUC：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is quite impressive as it implies that the link predictor can correctly
    predict 82% of the links presented as ground truths in the test set.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点相当令人印象深刻，因为这意味着链接预测器可以正确预测测试集中作为真实标签呈现的82%的链接。
- en: Graph customizations
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的自定义
- en: We have seen how to build and train GNNs for common graph ML tasks. However,
    for convenience, we have chosen to use prebuilt DGL graph convolution layers in
    our models. While unlikely, it is possible that you might need a layer that is
    not provided with the DGL package. DGL provides a message passing API to allow
    you to build custom graph layers easily. In the first part of this section, we
    will look at an example where we use the message-passing API to build a custom
    graph convolution layer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何为常见的图ML任务构建和训练GNN。然而，为了方便起见，我们选择在我们的模型中使用预构建的DGL图卷积层。虽然不太可能，但你可能需要一个DGL包中没有提供的图层。DGL提供了一个消息传递API，允许你轻松构建自定义图层。在本节的第一部分，我们将看一个示例，展示如何使用消息传递API构建一个自定义的图卷积层。
- en: We have also loaded datasets from the DGL data package for our examples. It
    is far more likely that we will need to use our own data instead. So, in the second
    part of this section, we will see how to convert our own data into a DGL dataset.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还加载了来自DGL数据包的数据集供我们的示例使用。但更有可能的是，我们需要使用自己的数据。因此，在本节的第二部分，我们将看到如何将自己的数据转换为DGL数据集。
- en: Custom layers and message passing
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义层和消息传递
- en: Although DGL provides many graph layers out of the box, there may be cases where
    the ones provided don’t meet our needs exactly and we need to build your own.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DGL提供了许多开箱即用的图层，但可能会有一些情况，现有的图层不完全满足我们的需求，我们需要构建自己的图层。
- en: 'Fortunately, all these graph layers are based on a common underlying concept
    of message passing between nodes in the graph. So, in order to build a custom
    GNN layer, you need to understand how the message-passing paradigm works. This
    paradigm is also known as the **Message Passing Neural Network** (**MPNN**) framework
    [5]:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，所有这些图层都基于图中节点之间消息传递的共同基本概念。因此，为了构建一个自定义的GNN层，你需要理解消息传递范式是如何工作的。这个范式也被称为**消息传递神经网络**（**MPNN**）框架[5]：
- en: '![](img/B18331_17_022.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_022.png)'
- en: '![](img/B18331_17_023.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_023.png)'
- en: '![](img/B18331_17_024.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_024.png)'
- en: Each node *u* in the graph has a hidden state (initially its feature vector)
    represented by *h*[u]. For each node *u* and *v*, where nodes *u* and *v* are
    neighbors, i.e., connected by an edge *e*[u->v], we apply some function *M* called
    the *message function*. The message function *M* is applied to every node on the
    graph. We then aggregate the output of *M* for all nodes with the output of all
    their neighboring nodes to produce the message *m*. Here ![](img/B18331_07_002.png)
    is called the *reduce function*. Note that even though we represent the reduce
    function by the summation symbol ![](img/B18331_07_002.png), it can be any aggregation
    function. Finally, we update the hidden state of node *v* using the obtained message
    and the previous state of the node. The function *U* applied at this step is called
    the *update function*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的每个节点*u*都有一个隐藏状态（最初是其特征向量），由*h*[u]表示。对于每对节点*u*和*v*，它们是邻居节点，即由边*e*[u->v]连接，我们应用一个称为*消息函数*的函数*M*。消息函数*M*会应用于图中的每个节点。然后，我们将*M*的输出与所有邻居节点的输出聚合，产生消息*m*。这里的![](img/B18331_07_002.png)被称为*reduce函数*。注意，尽管我们用求和符号![](img/B18331_07_002.png)表示reduce函数，但它可以是任何聚合函数。最后，我们使用获得的消息和节点的前一个状态更新节点*v*的隐藏状态。此步骤中应用的函数*U*被称为*更新函数*。
- en: The message-passing algorithm is repeated a specific number of times. After
    that, we reach the *readout phase* where we extract the feature vector from each
    node that represents the entire graph. For example, the final feature vector for
    a node might represent the node category in the case of node classification.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递算法会重复特定次数。之后，我们进入*读取阶段*，在该阶段我们从每个节点提取特征向量，表示整个图。例如，在节点分类的情况下，节点的最终特征向量可能代表节点的类别。
- en: 'In this section, we will use the MPNN framework to implement a GraphSAGE layer.
    Even though DGL provides the `dgl.nn.SAGEConv`, which implements this already,
    this is an example to illustrate the creation of custom graph layers using MPNN.
    The message-passing steps of a GraphSAGE layer are given by:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用MPNN框架实现一个GraphSAGE层。尽管DGL已经提供了实现这一功能的`dgl.nn.SAGEConv`，但本示例旨在展示如何使用MPNN创建自定义图层。GraphSAGE层的消息传递步骤如下：
- en: '![](img/B18331_17_027.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_027.png)'
- en: '![](img/B18331_17_028.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_028.png)'
- en: 'The code to implement our custom GraphSAGE layer using MPNN is shown below.
    The DGL function `update_all` call allows you to specify a `message_fn` and a
    `reduce_fn`, which are also DGL built-in functions, and the `tf.concat` and `Dense`
    layers represent the final update function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MPNN实现我们自定义GraphSAGE层的代码如下所示。DGL函数`update_all`的调用允许你指定`message_fn`和`reduce_fn`，这也是DGL内置的函数，而`tf.concat`和`Dense`层则表示最终的更新函数：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we see that the `update_all` function specifies a `message_func`, which
    just copies the node’s current feature vector to a message vector *m*, and then
    averages all the message vectors in the neighborhood of each node. As you can
    see, this faithfully follows the first GraphSAGE equation above. DGL provides
    many such built-in functions ([https://docs.dgl.ai/api/python/dgl.function.xhtml](https://docs.dgl.ai/api/python/dgl.function.xhtml)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到`update_all`函数指定了一个`message_func`，该函数只是将节点当前的特征向量复制到消息向量*m*，然后对每个节点的邻域内所有消息向量进行平均。如你所见，这忠实地遵循了上述第一个GraphSAGE方程。DGL提供了许多类似的内置函数（[https://docs.dgl.ai/api/python/dgl.function.xhtml](https://docs.dgl.ai/api/python/dgl.function.xhtml)）。
- en: Once the neighborhood vector *h_N* is computed in the first step, it is concatenated
    with the input feature vector *h*, and then passed through a `Dense` layer with
    a ReLU activation, as described by the second equation for GraphSAGE above. We
    have thus implemented the GraphSAGE layer with our `CustomGraphSAGE` object.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在第一步中计算出了邻域向量*h_N*，它将与输入特征向量*h*拼接在一起，然后通过带有ReLU激活的`Dense`层，如上面第二个GraphSAGE方程所述。我们已经通过`CustomGraphSAGE`对象实现了GraphSAGE层。
- en: 'The next step is to put it into a GNN to see how it works. The following code
    shows a `CustomGNN` model that uses two layers of our custom `SAGEConv` implementation:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将其放入GNN中以查看它的效果。以下代码展示了一个使用我们自定义`SAGEConv`实现的两层`CustomGNN`模型：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We will run it to do node classification against the CORA dataset, details of
    which should be familiar from previous examples.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行该模型对CORA数据集进行节点分类，具体细节应该从之前的示例中有所了解。
- en: The above code assumes an unweighted graph, i.e., edges between nodes have the
    same weight. This condition is true for the CORA dataset, where each edge represents
    a citation from one paper to another.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码假设是一个无权图，即节点之间的边具有相同的权重。这一条件适用于CORA数据集，其中每条边代表一篇论文对另一篇论文的引用。
- en: However, we can imagine scenarios where edges may be weighted based on how many
    times some edge has been invoked, for example, an edge that connects a product
    and a user for user recommendations.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以设想一些场景，其中边的权重可能基于某条边被触发的次数。例如，连接产品和用户的边可以用于用户推荐。
- en: 'The only change we need to make to handle weighted edges is to allow the weight
    to play a part in our message function. That is, if an edge between our node `u`
    and a neighbor node `v` occurs `k` times, we should consider that edge `k` times.
    The code below shows our custom GraphSAGE layer with the ability to handle weighted
    edges:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对代码进行的唯一修改是让权重在我们的消息函数中发挥作用。也就是说，如果节点`u`和邻居节点`v`之间的边发生了`k`次，我们应该将这条边考虑`k`次。以下代码展示了我们自定义的GraphSAGE层，它能够处理带权边：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This code expects an additional edge property *w*, which contains the edge
    weights, which you can simulate on the CORA dataset by:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码期望一个额外的边属性*w*，它包含边的权重。你可以通过以下方式在CORA数据集中模拟：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `message_func` in `CustomWeightedGraphSAGE` has changed from simply copying
    the feature vector *h* to the message vector *m*, to multiplying *h* and *w* to
    produce the message vector *m*. Everything else is the same as in `CustomGraphSAGE`.
    The new `CustomWeightedGraphSAGE` layer can now be simply dropped into the calling
    class `CustomGNN` where `CustomGraphSAGE` was originally being called.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`CustomWeightedGraphSAGE`中的`message_func`已经从简单地将特征向量*h*复制到消息向量*m*，变成了将*h*与*w*相乘以生成消息向量*m*。其他部分与`CustomGraphSAGE`中的代码相同。新的`CustomWeightedGraphSAGE`层现在可以简单地放入调用类`CustomGNN`中，替代最初调用的`CustomGraphSAGE`。'
- en: Custom graph dataset
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义图数据集
- en: A more common use case that you are likely to face is to use your own data to
    train a GNN model. Obviously, in such cases, you cannot use a DGL-provided dataset
    (as we have been using in all our examples so far) and you must wrap your data
    into a custom graph dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的使用案例是使用你自己的数据来训练GNN模型。显然，在这种情况下，你不能使用DGL提供的数据集（正如我们在之前的所有示例中所使用的），你必须将自己的数据包装成自定义图数据集。
- en: 'Your custom graph dataset should inherit from the `dgl.data.DGLDataset` object
    provided by DGL and implement the following methods:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你的自定义图数据集应继承自DGL提供的`dgl.data.DGLDataset`对象，并实现以下方法：
- en: '`__getitem__(self, i)` – retrieve the `i`-th example from the dataset. The
    retrieved example contains a single DGL graph and its label if applicable.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__getitem__(self, i)` – 从数据集中检索第`i`个示例。检索到的示例包含一个单一的DGL图及其标签（如果适用）。'
- en: '`__len__(self)` – the number of examples in the dataset.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__len__(self)` – 数据集中的示例数量。'
- en: '`process(self)` – defines how to load and process raw data from the disk.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process(self)` – 定义如何从磁盘加载和处理原始数据。'
- en: As we have seen before, node classification and link prediction operate on a
    single graph, and graph classification operates on a set of graphs. While the
    approach is largely identical for both cases, there are some concerns specific
    to either case, so we will provide an example to do each of these below.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，节点分类和链路预测操作在单一图上，而图分类则作用于一组图。虽然这两种情况的处理方法大致相同，但每种情况都有其特定的关注点，因此我们将在下文提供一个例子来分别演示这两种方法。
- en: Single graphs in datasets
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集中的单一图
- en: 'For our example, we will choose Zachary’s Karate Club graph, which represents
    the members of a Karate Club observed over three years. Over time, there was a
    disagreement between an administrator (Officer) and the instructor (Mr. Hi), and
    the club members split and reformed under the Officer and Mr. Hi (shown below
    as blue and red nodes, respectively). The Zachary Karate Club network is available
    for download from the NetworkX library:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将选择Zachary的空手道俱乐部图，它代表了一个空手道俱乐部的成员，观察了三年。随着时间的推移，管理员（Officer）和教练（Mr.
    Hi）之间发生了分歧，俱乐部成员在两人之间分裂并重新组合，分别在下图中以蓝色和红色节点表示。Zachary空手道俱乐部网络可从NetworkX库中下载：
- en: '![A picture containing plant, red  Description automatically generated](img/B18331_17_02.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing plant, red  Description automatically generated](img/B18331_17_02.png)'
- en: 'Figure 17.2: Graph representation of the Karate Club Network'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：空手道俱乐部网络的图表示
- en: 'The graph contains 34 nodes labeled with one of “Officer” or “Mr. Hi” depending
    on which group they ended up in after the split. It contains 78 edges, which are
    undirected and unweighted. An edge between a pair of members indicates that they
    interact with each other outside the club. To make this dataset more realistic
    for GNN usage, we will attach a 10-dimensional random feature vector to each node,
    and an edge weight as an edge feature. Here is the code to convert the Karate
    Club graph into a DGL dataset that you can then use for downstream node or edge
    classification tasks:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包含34个节点，每个节点被标记为“Officer”或“Mr. Hi”，取决于它们在拆分后的分组。图中包含78条无向、无权重的边。两名成员之间的边表示他们在俱乐部外相互互动。为了使这个数据集在GNN使用中更具现实性，我们将为每个节点附加一个10维的随机特征向量，并将边权作为边特征。以下是将空手道俱乐部图转换为DGL数据集的代码，您可以将其用于后续的节点或边分类任务：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Most of the logic is in the `process` method. We call the NetworkX method to
    get the Karate Club as a NetworkX graph, then convert it to a DGL graph object
    with node features and labels. Even though the Karate Club graph does not have
    node and edge features defined, we manufacture some random numbers and set them
    to these properties. Note that this is only for purposes of this example, to show
    where these features would need to be updated if your graph had node and edge
    features. Note that the dataset contains a single graph.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分逻辑在`process`方法中。我们调用NetworkX方法将空手道俱乐部图作为NetworkX图获取，然后将其转换为带有节点特征和标签的DGL图对象。尽管空手道俱乐部图没有定义节点和边特征，我们制造了一些随机数并将其设置为这些属性。请注意，这仅用于本示例，目的是展示如果您的图包含节点和边特征，这些特征需要更新的位置。需要注意的是，数据集中只包含一个图。
- en: In addition, we also want to split the graph into training, validation, and
    test splits for node classification purposes. For that, we assign masks indicating
    whether a node belongs to one of these splits. We do this rather simply by splitting
    the nodes in the graph 60/20/20 and assigning Boolean masks for each split.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还希望将图拆分为训练、验证和测试集，以用于节点分类任务。为此，我们为每个节点分配掩码，指示其是否属于这些拆分中的某一组。我们通过将图中的节点按60/20/20比例拆分，并为每个拆分分配布尔掩码来简单地完成这一操作。
- en: 'In order to instantiate this dataset from our code, we can say:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从我们的代码中实例化这个数据集，我们可以这样写：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will give us the following output (reformatted a little for readability).
    The two main structures are the `ndata_schemas` and `edata_schemas`, accessible
    as `g.ndata` and `g.edata`, respectively. Within `ndata_schemas`, we have keys
    that point to the node features (`feats`), node labels (`label`), and the masks
    to indicate the training, validation, and test splits (`train_mask`, `val_mask`,
    and `test_mask`), respectively. Under `edata_schemas`, there is the `weight` attribute
    that indicates the edge weights:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出（略作重新格式化以提高可读性）。主要的两个结构是`ndata_schemas`和`edata_schemas`，分别可以通过`g.ndata`和`g.edata`访问。在`ndata_schemas`中，我们有指向节点特征（`feats`）、节点标签（`label`）以及表示训练、验证和测试拆分的掩码（`train_mask`、`val_mask`和`test_mask`）的键。在`edata_schemas`下，有表示边权的`weight`属性：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Please refer to the examples on node classification and link prediction for
    information on how to use this kind of custom dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考节点分类和链路预测的示例，以了解如何使用这种自定义数据集。
- en: Set of multiple graphs in datasets
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集中多个图的集合
- en: Datasets that support the graph classification task will contain multiple graphs
    and their associated labels, one per graph. For our example, we will consider
    a hypothetical dataset of molecules represented as graphs, and the task would
    be to predict if the molecule is toxic or not (a binary prediction).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 支持图分类任务的数据集将包含多个图及其关联标签，每个图对应一个标签。对于我们的示例，我们将考虑一个假设的数据集，包含作为图表示的分子，任务是预测分子是否有毒（一个二元预测）。
- en: We will use the NetworkX method `random_regular_graph()` to generate synthetic
    graphs with a random number of nodes and node degree. To each node of each graph,
    we will attach a random 10-dimensional feature vector. Each node will have a label
    (0 or 1) indicating if the graph is toxic. Note that this is just a simulation
    of what real data might look like. With real data, the structure of each graph
    and the values of the node vectors, which are random in our case, will have a
    real impact on the target variable, i.e., the toxicity of the molecule.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 NetworkX 方法 `random_regular_graph()` 来生成具有随机节点数和节点度数的合成图。对于每个图的每个节点，我们将附加一个随机的
    10 维特征向量。每个节点将有一个标签（0 或 1），表示图是否有毒。请注意，这仅仅是对真实数据可能样貌的模拟。对于真实数据，每个图的结构以及节点向量的值（在我们的案例中是随机的）将对目标变量产生实际影响，即分子是否有毒。
- en: 'The figure below shows some examples of what the synthetic “molecules” might
    look like:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一些合成“分子”可能的样子：
- en: '![Chart, radar chart  Description automatically generated](img/B18331_17_03.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图表，雷达图，描述自动生成](img/B18331_17_03.png)'
- en: 'Figure 17.3: Some examples of random regular graphs generated using NetworkX'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3：一些使用 NetworkX 生成的随机规则图的例子
- en: 'Here is the code to convert a set of random NetworkX graphs into a DGL graph
    dataset for graph classification. We will generate 100 such graphs and store them
    in a list in the form of a DGL dataset:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将一组随机 NetworkX 图转换为 DGL 图数据集以进行图分类的代码。我们将生成 100 个这样的图，并将它们以 DGL 数据集的形式存储在一个列表中：
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once created, we can then call it from our code as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建完成，我们可以像这样在代码中调用它：
- en: '[PRE41]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This produces the following output for the first graph in the DGL dataset (reformatted
    slightly for readability). As you can see, the first graph in the dataset has
    `6` nodes and `15` edges and contains a feature vector (accessible using the `feats`
    key) of size `10`. The label is a `0`-dimensional tensor (i.e., a scalar) of type
    long (`int64`):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这为 DGL 数据集中的第一个图生成以下输出（稍作格式调整以便阅读）。如你所见，数据集中的第一个图有 `6` 个节点和 `15` 条边，并且包含一个大小为
    `10` 的特征向量（通过 `feats` 键访问）。标签是一个 `0` 维张量（即标量），类型为 long（`int64`）：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As before, in order to see how you would use this custom dataset for some task
    such as graph classification, please refer to the example on graph classification
    earlier in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了看到如何使用这个自定义数据集进行任务（如图分类），请参考本章早些时候关于图分类的示例。
- en: Future directions
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来方向
- en: Graph neural networks are a rapidly evolving discipline. We have covered working
    with static homogeneous graphs on various popular graph tasks so far, which covers
    many real-world use cases. However, it is likely that some graphs are neither
    homogeneous nor static, and neither can they be easily reduced to this form. In
    this section, we will look at our options for dealing with heterogenous and temporal
    graphs.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络是一个迅速发展的学科。到目前为止，我们已经介绍了如何在各种流行的图任务中处理静态同构图，这涵盖了许多现实世界的使用案例。然而，很可能一些图既不是同构的，也不是静态的，而且它们也不能轻易地简化成这种形式。在这一部分，我们将探讨如何处理异构和时序图。
- en: Heterogeneous graphs
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异构图
- en: Heterogeneous graphs [7], also called heterographs, differ from the graphs we
    have seen so far in that they may contain different kinds of nodes and edges.
    These different types of nodes and edges might also contain different types of
    attributes, including possible representations with different dimensions. Popular
    examples of heterogeneous graphs are citation graphs that contain authors and
    papers, recommendation graphs that contain users and products, and knowledge graphs
    that can contain many different types of entities.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 异构图 [7]，也叫做异构图，区别于我们迄今所见的图，它们可能包含不同种类的节点和边。这些不同类型的节点和边可能还包含不同类型的属性，包括具有不同维度的表示。异构图的流行例子包括包含作者和论文的引用图、包含用户和产品的推荐图，以及可以包含多种不同类型实体的知识图谱。
- en: You can use the MPNN framework on heterogeneous graphs by manually implementing
    message and update functions individually for each edge type. Each edge type is
    defined by the triple (source node type, edge type, and destination node type).
    However, DGL provides support for heterogeneous graphs using the `dgl.heterograph()`
    API, where a graph is specified as a series of graphs, one per edge type.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过手动为每种边类型分别实现消息和更新函数，使用 MPNN 框架处理异构图。每种边类型由三元组（源节点类型、边类型和目标节点类型）定义。然而，DGL
    提供了使用 `dgl.heterograph()` API 支持异构图，其中图是作为一系列图来指定的，每个边类型对应一个图。
- en: Typical learning tasks associated with heterogeneous graphs are similar to their
    homogeneous counterparts, namely node classification and regression, graph classification,
    and edge classification/link prediction. A popular graph layer for working with
    heterogeneous graphs is the **Relational GCN** or **R-GCN**, available as a built-in
    layer in DGL.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与异构图相关的典型学习任务与同构图的任务相似，即节点分类与回归、图分类和边分类/链接预测。用于处理异构图的一个流行图层是**关系GCN**或**R-GCN**，它作为DGL中的内置图层提供。
- en: Temporal Graphs
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时序图
- en: Temporal Graphs [8] is a framework developed at Twitter to handle dynamic graphs
    that change over time. While GNN models have primarily focused on static graphs
    that do not change over time, adding the time dimension allows us to model many
    interesting phenomena in social networks, financial transactions, and recommender
    systems, all of which are inherently dynamic. In such systems, it is the dynamic
    behavior that conveys the important insights.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 时序图[8]是Twitter开发的一个框架，用于处理随时间变化的动态图。虽然GNN模型主要关注不随时间变化的静态图，但加入时间维度使我们能够对社交网络、金融交易和推荐系统等许多本质上是动态的现象进行建模。在这些系统中，正是动态行为传递了重要的洞察。
- en: A dynamic graph can be represented as a stream of timed events, such as additions
    and deletions of nodes and edges. This stream of events is fed into an encoder
    network that learns a time-dependent encoding for each node in the graph. A decoder
    is trained on this encoding to support some specific task such as link prediction
    at a future point in time. There is currently no support in the DGL library for
    Temporal Graphs, mainly because it is a very rapidly evolving research area.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 动态图可以表示为一系列定时事件，例如节点和边的添加与删除。这些事件流输入到编码器网络中，编码器为图中的每个节点学习一个依赖于时间的编码。解码器基于该编码进行训练，以支持某些特定任务，如未来时刻的链接预测。目前，DGL库对时序图尚不支持，主要是因为这是一个快速发展的研究领域。
- en: At a high level, a **Temporal Graph Network** (**TGN**) encoder works by creating
    a compressed representation of the nodes based on their interaction and updates
    over time. The current state of each node is stored in TGN memory and acts as
    the hidden state *s*[t] of an RNN; however, we have a separate state vector *s*[t]*(t)*
    for each node *i* and time point *t*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，**时序图网络**（**TGN**）编码器通过基于节点的交互和时间更新创建节点的压缩表示。每个节点的当前状态存储在TGN内存中，作为RNN的隐藏状态*s*[t]；然而，对于每个节点*i*和时间点*t*，我们有一个单独的状态向量*s*[t]*(t)*。
- en: A message function similar to what we have seen in the MPNN framework computes
    two messages *m*[i] and *m*[j] for a pair of nodes *i* and *j* using the state
    vectors and their interaction as input. The message and state vectors are then
    combined using a memory updater, which is usually implemented as an RNN. TGNs
    have been found to outperform their static counterparts on the tasks of future
    edge prediction and dynamic node classification both in terms of accuracy and
    speed.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在MPNN框架中看到的消息函数，计算两节点*i*和*j*的两个消息*m*[i]和*m*[j]，输入为状态向量及其交互。然后，使用记忆更新器将消息和状态向量组合在一起，通常该记忆更新器实现为RNN。研究发现，TGNs在未来边预测和动态节点分类任务中，无论在准确度还是速度上，都优于静态图方法。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have covered graph neural networks, an exciting set of techniques
    to learn not only from node features but also from the interaction between nodes.
    We have covered the intuition behind why graph convolutions work and the parallels
    between them and convolutions in computer vision. We have described some common
    graph convolutions, which are provided as layers by DGL. We have demonstrated
    how to use the DGL for popular graph tasks of node classification, graph classification,
    and link prediction. In addition, in the unlikely event that our needs are not
    met by standard DGL graph layers, we have learned how to implement our own graph
    convolution layer using DGL’s message-passing framework. We have also seen how
    to build DGL datasets for our own graph data. Finally, we look at some emerging
    directions of graph neural networks, namely heterogeneous graphs and temporal
    graphs. This should equip you with skills to use GNNs to solve interesting problems
    in this area.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了图神经网络，这是一组令人兴奋的技术，可以从节点特征以及节点之间的交互中学习。我们讲解了图卷积为何有效的直觉，以及它们与计算机视觉中卷积的相似之处。我们描述了一些常见的图卷积，这些卷积作为层由DGL提供。我们展示了如何使用DGL处理常见的图任务，如节点分类、图分类和链接预测。此外，如果标准的DGL图层不能满足我们的需求，我们已经学习了如何利用DGL的消息传递框架实现自己的图卷积层。我们还介绍了如何构建适用于我们自己图数据的DGL数据集。最后，我们探讨了一些图神经网络的前沿方向，即异构图和时序图。这些内容应当能够为你提供使用GNNs解决该领域有趣问题的技能。
- en: In the next chapter, we will turn our attention to learning about some best
    ML practices associated with deep learning projects.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将关注学习与深度学习项目相关的一些最佳机器学习实践。
- en: References
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Kipf, T. and Welling, M. (2017). *Semi-supervised Classification with Graph
    Convolutional Networks*. Arxiv Preprint, arXiv: 1609.02907 [cs.LG]. Retrieved
    from [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kipf, T. 和 Welling, M. (2017). *使用图卷积网络进行半监督分类*。Arxiv预印本，arXiv: 1609.02907
    [cs.LG]。来自 [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)'
- en: Velickovic, P., et al. (2018). *Graph Attention Networks*. Arxiv Preprint, arXiv
    1710.10903 [stat.ML]. Retrieved from [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Velickovic, P., 等人 (2018). *图注意力网络*。Arxiv预印本，arXiv 1710.10903 [stat.ML]。来自 [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)
- en: 'Hamilton, W. L., Ying, R., and Leskovec, J. (2017). *Inductive Representation
    Learning on Large Graphs*. Arxiv Preprint, arXiv: 1706.02216 [cs.SI]. Retrieved
    from [https://arxiv.org/abs/1706.02216](https://arxiv.org/abs/1706.02216)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hamilton, W. L., Ying, R., 和 Leskovec, J. (2017). *大规模图上的归纳表示学习*。Arxiv预印本，arXiv:
    1706.02216 [cs.SI]。来自 [https://arxiv.org/abs/1706.02216](https://arxiv.org/abs/1706.02216)'
- en: 'Xu, K., et al. (2018). *How Powerful are Graph Neural Networks?*. Arxiv Preprint,
    arXiv: 1810.00826 [cs.LG]. Retrieved from [https://arxiv.org/abs/1810.00826](https://arxiv.org/abs/1810.00826)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Xu, K., 等人 (2018). *图神经网络有多强大？*。Arxiv预印本，arXiv: 1810.00826 [cs.LG]。来自 [https://arxiv.org/abs/1810.00826](https://arxiv.org/abs/1810.00826)'
- en: 'Gilmer, J., et al. (2017). *Neural Message Passing for Quantum Chemistry*.
    Arxiv Preprint, arXiv: 1704.01212 [cs.LG]. Retrieved from [https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gilmer, J., 等人 (2017). *量子化学中的神经信息传递*。Arxiv预印本，arXiv: 1704.01212 [cs.LG]。来自
    [https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212)'
- en: Zachary, W. W. (1977). *An Information Flow Model for Conflict and Fission in
    Small Groups*. Journal of Anthropological Research. Retrieved from [https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752](https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752)
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zachary, W. W. (1977). *小组冲突与分裂中的信息流模型*。人类学研究杂志。来自 [https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752](https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752)
- en: Pengfei, W. (2020). *Working with Heterogeneous Graphs in DGL*. Blog post. Retrieved
    from [https://www.jianshu.com/p/767950b560c4](https://www.jianshu.com/p/767950b560c4)
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pengfei, W. (2020). *在DGL中处理异构图*。博客文章。来自 [https://www.jianshu.com/p/767950b560c4](https://www.jianshu.com/p/767950b560c4)
- en: Bronstein, M. (2020). *Temporal Graph Networks*. Blog post. Retrieved from [https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe](https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe)
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bronstein, M. (2020). *时序图网络*。博客文章。来自 [https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe](https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe)
- en: Join our book’s Discord space
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的书籍Discord社区
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人相遇，并与超过2000名成员一起学习，网址：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
