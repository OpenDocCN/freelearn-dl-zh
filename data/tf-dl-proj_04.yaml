- en: Building GANs for Conditional Image Creation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于条件图像生成的GAN
- en: Yann LeCun, Director of Facebook AI, has recently stated that "*Generative Adversarial
    Networks is the most interesting idea in the last ten years in machine learning*",
    and that is certainly confirmed by the elevated interest in academia about this
    deep learning solution. If you look at recent papers on deep learning (but also
    look at the leading trends on LinkedIn or Medium posts on the topic), there has
    really been an overproduction of variants of GANs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook AI的总监Yann LeCun最近表示：“*生成对抗网络是过去十年机器学习领域最有趣的想法*”，这一观点无疑得到了学术界对这一深度学习解决方案日益关注的验证。如果你查看最近的深度学习论文（同时也可以看看LinkedIn或Medium上关于该话题的领先趋势），你会发现GAN的变种已经被大量生产出来。
- en: You can get an idea of what a *zoo* the world of GANs has become just by glancing
    the continuously updated reference table, created by Hindu Puravinash, which can
    be found at [https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv) or
    by studying the GAN timeline prepared by Zheng Liu, which can be found at [https://github.com/dongb5/GAN-Timeline](https://github.com/dongb5/GAN-Timeline) and
    can help you putting everything into time perspective.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过浏览Hindu Puravinash不断更新的参考表格，来了解GAN世界已经变成了一个什么样的*动物园*，该表格可以在[https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv)找到，或者通过研究Zheng
    Liu准备的GAN时间线，时间线可以在[https://github.com/dongb5/GAN-Timeline](https://github.com/dongb5/GAN-Timeline)找到，帮助你将一切放入时间框架中。
- en: 'GANs have the power to strike the imagination because they can demonstrate
    the creative power of AI, not just its computational strength. In this chapter,
    we are going to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GAN具备激发想象力的能力，因为它们不仅能展示AI的计算能力，还能展现AI的创造力。在本章中，我们将：
- en: Demystify the topic of GANs by providing you with all the necessary concepts
    to understand what GANs are, what they can do at the moment, and what they are
    expected to do
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供所有必要的概念来揭开GAN的神秘面纱，帮助你理解GAN是什么、它们目前能做什么以及未来能做什么
- en: Demonstrate how to generate images both based on the initial distribution of
    example images (the so-called unsupervised GANs)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何基于示例图像的初始分布生成图像（即所谓的无监督GAN）
- en: Explain how to condition the GAN to the kind of resulting image you expect them
    to generate for you
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释如何为GAN设置条件，以便它们生成你期望的图像类型
- en: Set up a basic yet complete project that can work with different datasets of
    handwritten characters and icons
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个基本但完整的项目，可以处理不同的数据集，例如手写字符和图标
- en: Provide you with basic instructions how to train your GANs in the Cloud (specifically
    on Amazon AWS)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供基本指导，教你如何在云端（特别是在Amazon AWS上）训练你的GAN
- en: The success of GANs much depends, besides the specific neural architecture you
    use, on the problem they face and the data you feed them with. The datasets we
    have chosen for this chapter should provide satisfactory results. We hope you
    will enjoy and be inspired by the creative power of GANs!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的成功除了取决于你所使用的特定神经网络架构外，还与它们面临的问题以及你提供的数据密切相关。我们为本章选择的数据集应该能够提供令人满意的结果。我们希望你能享受并从GAN的创造力中获得灵感！
- en: Introducing GANs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍GAN
- en: We'll start with some quite recent history because GANs are among the newest
    ideas you'll find around AI and deep learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一些相当近期的历史开始，因为GAN是你在AI和深度学习领域找到的最新想法之一。
- en: 'Everything started in 2014, when Ian Goodfellow and his colleagues (there is
    also Yoshua Bengio closing the list of contributors) at the *Departement d''informatique et
    de recherche opérationnelle* at Montreal University published a paper on **Generative
    Adversarial Nets** (**GANs**), a framework capable of generating new data based
    on a set of initial examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一切始于2014年，当时Ian Goodfellow和他的同事们（包括Yoshua Bengio也在贡献者名单上）在蒙特利尔大学的*计算机科学与运筹学系*发表了关于**生成对抗网络**（**GANs**）的论文，这是一种能够基于一组初始示例生成新数据的框架：
- en: '*GOODFELLOW*, Ian, et al. Generative Adversarial Nets. In: *Advances in Neural
    Information Processing Systems*. 2014\. p. 2672-2680: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*GOODFELLOW*，Ian等。生成对抗网络。在：*神经信息处理系统的进展*，2014年，第2672-2680页：[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)。'
- en: 'The initial images produced by such networks were astonishing, considering
    the previous attempts using Markov chains which were far from being credible.
    In the image, you can see some of the examples proposed in the paper, showing
    examples reproduced from MNIST, **Toronto Face Dataset** (**TFD**) a non-public
    dataset and CIFAR-10 datasets:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络生成的初始图像令人惊讶，考虑到之前使用马尔可夫链的尝试远未达到可信的程度。在图像中，你可以看到论文中提出的一些例子，展示了从 MNIST、**多伦多面部数据集**（**TFD**）——一个非公开数据集以及
    CIFAR-10 数据集中复制的示例：
- en: '![](img/5550d1dd-463a-469c-a3ef-f4f11b3c5386.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5550d1dd-463a-469c-a3ef-f4f11b3c5386.png)'
- en: 'Figure 1: Samples from the first paper on GANs using different datasets for
    learning to generate fresh images: a) MNIST b) TFD c) and d) CIFAR-10'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：第一篇关于 GAN 的论文中，使用不同数据集生成新图像的样本：a) MNIST b) TFD c) 和 d) CIFAR-10
- en: 'SOURCE: *GOODFELLOW*, Ian, et al. Generative Adversarial Nets. In: *Advances
    in Neural Information Processing Systems*. 2014\. p. 2672-2680'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：*GOODFELLOW*，Ian 等。生成对抗网络。在：*神经信息处理系统进展*。2014年，第 2672-2680 页
- en: The paper was deemed quite innovative because it put working together deep neural
    networks and game theory in a really smart architecture that didn't require much
    more than the usual back-propagation to train. GANs are generative models, models
    that can generate data because they have inscribed a model distribution (they
    learned it, for instance). Consequently when they generate something it is just
    like if they were sampling from that distribution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文被认为相当创新，因为它将深度神经网络和博弈论结合在一个非常聪明的架构中，而这个架构并不需要太多除了常规的反向传播来进行训练。GANs 是生成模型，这些模型能够生成数据，因为它们已经学习到并刻画了一个模型分布（例如它们已经学习了这个分布）。因此，当它们生成某些东西时，就像是从这个分布中进行采样一样。
- en: The key is in the adversarial approach
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键在于对抗性方法
- en: The key to understanding how GANs can be such successful generative models resides
    in the term adversarial. Actually, the GANs architecture is made up of two distinct
    networks that are optimized based on the pooling of respective errors, and that's
    called an **adversarial process**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 GANs 为什么能成为如此成功的生成模型的关键就在于“对抗”一词。实际上，GANs 的架构由两个独立的网络组成，这两个网络通过各自错误的聚合来优化，这个过程被称为**对抗过程**。
- en: You start with a real dataset, let's call it R, containing your images or your
    data of a different kind (GANs are not limited to images only, though they constitute
    the major application). You then set up a generator network, G, which tries to
    make fake data that looks like the genuine data, and you set up a discriminator,
    D, whose role is to compare the data produced by G mixed against the real data,
    R, and figures out which is genuine and which is not.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你从一个真实的数据集开始，假设它叫做 R，包含你的图像或其他类型的数据（尽管 GANs 主要应用于图像，但它们并不限于图像）。然后你设置一个生成器网络
    G，它尝试生成看起来像真实数据的假数据，并设置一个判别器 D，其作用是将 G 生成的数据与真实数据 R 混合，比较并判断哪个是原始数据，哪个是伪造的。
- en: Goodfellow used the art forgers metaphor to describe the process, being, the
    generator the forgers, and the discriminator the detective (or the art critic)
    that has to disclose their misdeed. There is a challenge between the forgers and
    the detective because while the forgers have to become more skillful in order
    not to be detected, the detective has to become better at detecting fakes. Everything
    turns into an endless fight between the forgers and the detective until the forged
    artifacts are completely similar to the originals. When GANs overfit, in fact,
    they just reproduce the originals. It really seems an explanation of a competitive
    market, and it really is, because the idea comes from competitive game theory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Goodfellow 用伪造艺术家的隐喻来描述这个过程，其中生成器是伪造者，判别器是侦探（或艺术评论家），必须揭露伪造行为。伪造者和侦探之间存在一种挑战，因为伪造者必须变得更加熟练，以避免被侦探发现，而侦探则必须提高识别伪造品的能力。一切都变成了伪造者和侦探之间的无休止斗争，直到伪造的物品与原物完全相似。当
    GANs 过拟合时，实际上它们只是复制了原始数据。这似乎是在解释一个竞争性市场，实际上也是如此，因为这个概念来源于竞争博弈理论。
- en: In GANs, the generator is incentivized to produce images that the discriminator
    cannot figure out if they are a fake or not. An obvious solution for the generator
    is simply to copy some training image or to just settle down for some produced
    image that seems successful with the discriminator. One solution is *one-sided
    label smoothing* a technique which we will be applying in our project. It is described
    in SALIMANS, Tim, et al. Improved techniques for training gans. In: <q>Advances
    in Neural Information Processing Systems</q><q>. 2016\. p. 2234-2242</q>: [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN中，生成器的目标是生成让判别器无法判断真伪的图像。生成器的一种显而易见的解决方案是简单地复制某些训练图像，或者选择一些看似能成功欺骗判别器的图像。一个解决方案是*单边标签平滑*，这是我们在项目中将应用的技术。该技术在 SALIMANS,
    Tim等人所著的《训练GAN的改进技术》中进行了描述。书中出现在<q>Advances in Neural Information Processing Systems</q><q>，2016年，2234-2242页</q>：
    [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498)。
- en: 'Let''s discuss how things actually work a little bit more. At first, the generator,
    *G*, is clueless and produces completely random data (it has actually never seen
    a piece of original data), it is therefore punished by the discriminator, *D--*an
    easy job figuring out the real versus the fake data. *G* takes full blame and
    starts trying something different to get better feedback from *D*. This is done
    completely randomly because the only data the generator sees is a random input
    called *Z*, it never touches the real data. After many trials and fails, hinted
    by the discriminator, the generator at last figures out what to do and starts
    to produce credible outputs. In the end, given enough time, the generator will
    exactly replicate all the original data without ever having seen a single example
    of it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论一下实际是如何工作的。最初，生成器*G*毫无头绪，生成完全随机的数据（实际上它从未见过任何原始数据），因此它会受到判别器*D*的惩罚——*D*非常容易区分真假数据。*G*承担了全部责任，开始尝试不同的方法，以获得来自*D*的更好反馈。这个过程是完全随机的，因为生成器看到的唯一数据是一个随机输入*Z*，它从未接触过真实数据。经过多次试错后，在判别器的提示下，生成器最终弄明白该做什么，并开始生成可信的输出。最终，随着时间的推移，生成器将完美复制所有原始数据，甚至从未见过任何一个原始样本：
- en: '![](img/6c2256ef-2d1e-4ded-a5ca-0ac8cd9f16b8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c2256ef-2d1e-4ded-a5ca-0ac8cd9f16b8.png)'
- en: Figure 2: Illustrative example of how a vanilla GAN architecture works
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：展示了一个简单的GAN架构如何工作
- en: A cambrian explosion
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次寒武纪大爆发
- en: As mentioned, there are new papers on GANs coming out every month (as you can
    check on the reference table made by Hindu Puravinash that we mentioned at the
    beginning of the chapter).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，每个月都会有关于GAN的新论文发布（你可以查看我们在本章开头提到的Hindu Puravinash制作的参考表）。
- en: Anyway, apart from the vanilla implementation described in the initial paper
    from Goodfellow and his colleagues, the most notable implementations to take notice
    of are **deep convolutional generative adversarial networks **(**DCGANs**) and
    **conditional GANs (**CGANs**)**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，除了Goodfellow及其同事最初论文中描述的基础实现外，最值得注意的实现是**深度卷积生成对抗网络（DCGANs）**和**条件生成对抗网络（CGANs）**。
- en: DCGANs are GANs based on CNN architecture (<q>RADFORD, Alec; METZ, Luke; CHINTALA,
    Soumith. Unsupervised representation learning with deep convolutional generative
    adversarial networks. arXiv preprint arXiv:1511.06434</q><q>, 2015</q>: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DCGAN是基于CNN架构的GAN（<q>RADFORD, Alec; METZ, Luke; CHINTALA, Soumith. 使用深度卷积生成对抗网络进行无监督表示学习.
    arXiv预印本arXiv:1511.06434</q><q>, 2015</q>: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)）。'
- en: CGANs are DCGANs which are conditioned on some input label so that you can obtain
    as a result an image with certain desired characteristics (<q>MIRZA, Mehdi; OSINDERO,
    Simon. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784</q><q>,
    2014</q>: [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)).
    Our project will be programming a `CGAN` class and training it on different datasets
    in order to prove its functioning.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CGAN是条件化的DCGAN，依赖于某些输入标签，因此可以生成具有特定所需特征的图像（<q>MIRZA, Mehdi; OSINDERO, Simon.
    条件生成对抗网络. arXiv预印本arXiv:1411.1784</q><q>, 2014</q>: [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)）。我们的项目将编写一个`CGAN`类，并在不同数据集上训练它，以验证其功能。'
- en: 'But there are also other interesting examples around (which are not covered
    by our project) offering practical solutions to problems related to image creation
    or improvement:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有其他有趣的例子（这些并未包含在我们的项目中），它们提供了与图像创建或改善相关的实际解决方案：
- en: 'A CycleGAN translates an image into another (the classic example is the horse
    that becomes a zebra: <q>ZHU, Jun-Yan, et al. Unpaired image-to-image translation
    using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593</q><q>,
    2017</q>: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593))'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN将一张图像转换为另一张图像（经典例子是马变成斑马：<q>ZHU, Jun-Yan, et al. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. arXiv预印本arXiv:1703.10593</q><q>,
    2017</q>: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)）
- en: 'A StackGAN creates a realistic image from a text describing the image (<q>ZHANG,
    Han, et al. Stackgan: Text to photo-realistic image synthesis with stacked generative
    adversarial networks. arXiv preprint arXiv:1612.03242</q><q>, 2016</q>: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'StackGAN通过描述图像的文本生成逼真的图像（<q>ZHANG, Han, et al. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. arXiv预印本arXiv:1612.03242</q><q>,
    2016</q>: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242)）'
- en: A Discovery GAN (DiscoGAN) transfers stylistic elements from one image to another,
    thus transferring texture and decoration from a fashion item such as a bag to
    another fashion item such as a pair of shoes (<q>KIM, Taeksoo, et al. Learning
    to discover cross-domain relations with generative adversarial networks. arXiv
    preprint arXiv:1703.05192</q><q>, 2017</q>: [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192))
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Discovery GAN（DiscoGAN）将一种图像的风格元素转移到另一张图像上，从而将纹理和装饰从一种时尚物品（例如包包）转移到另一种时尚物品（例如鞋子）上（<q>KIM,
    Taeksoo, et al. Learning to discover cross-domain relations with generative adversarial
    networks. arXiv预印本arXiv:1703.05192</q><q>, 2017</q>: [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192)）
- en: A SRGAN can convert low-quality images into high-resolution ones (<q>LEDIG,
    Christian, et al. Photo-realistic single image super-resolution using a generative
    adversarial network. arXiv preprint arXiv:1609.04802</q><q>, 2016</q>: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802))
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRGAN能够将低质量图像转换为高分辨率图像（<q>LEDIG, Christian, et al. Photo-realistic single image
    super-resolution using a generative adversarial network. arXiv预印本arXiv:1609.04802</q><q>,
    2016</q>: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802)）
- en: DCGANs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DCGAN
- en: 'DCGANs are the first relevant improvement on the GAN architecture. DCGANs always
    successfully complete their training phase and, given enough epochs and examples,
    they tend to generate satisfactory quality outputs. That soon made them the baseline
    for GANs and helped to produce some amazing achievements, such as generating new
    Pokemon from known ones: [https://www.youtube.com/watch?v=rs3aI7bACGc](https://www.youtube.com/watch?v=rs3aI7bACGc)
    or creating faces of celebrities that actually never existed but are incredibly
    realistic (nothing uncanny), just as NVIDIA did: [https://youtu.be/XOxxPcy5Gr4](https://youtu.be/XOxxPcy5Gr4) using
    a new training approach called **progressing growing**: [http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf).
    They have their root in using the same convolutions used in image classification
    by deep learning supervised networks, and they use some smart tricks:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN是GAN架构的首次相关改进。DCGAN始终能够成功完成训练阶段，并且在足够的训练周期和示例下，通常能够生成令人满意的输出。这使得它们很快成为了GAN的基准，并帮助产生了一些令人惊叹的成就，比如从已知的宝可梦生成新的宝可梦：[https://www.youtube.com/watch?v=rs3aI7bACGc](https://www.youtube.com/watch?v=rs3aI7bACGc)，或者创造出实际上从未存在过但极其真实的名人面孔（毫无诡异感），正如NVIDIA所做的：[https://youtu.be/XOxxPcy5Gr4](https://youtu.be/XOxxPcy5Gr4)，使用一种新的训练方法叫做**逐步生长**：[http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf)。它们的根基在于使用深度学习监督网络中用于图像分类的相同卷积操作，并且它们使用了一些聪明的技巧：
- en: Batch normalization in both networks
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个网络中都使用批量归一化
- en: No fully hidden connected layers
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无完全隐藏连接层
- en: No pooling, just stride-in convolutions
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无池化，仅使用步幅卷积
- en: ReLU activation functions
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU激活函数
- en: Conditional GANs
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件GAN
- en: 'In **conditional GANs** (**CGANs**), adding a vector of features controls the
    output and provides a better guide to the generator in figuring out what to do. Such
    a vector of features could encode the class the image should be derived be from
    (that is an image of a woman or a man if we are trying to create faces of imaginary
    actors) or even a set of specific characteristics we expect from the image (for
    imaginary actors, it could be the type of hair, eyes or complexion). The trick
    is done by incorporating the information into the images to be learned and into
    the *Z* input, which is not completely random anymore. The evaluation by the discriminator
    is done not only on the resemblance of fake data to the original data but also
    on the correspondence of the fake data image to its input label (or features):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在**条件GANs**（**CGANs**）中，添加特征向量可以控制输出，并为生成器提供更好的指导，帮助其弄清楚该做什么。这样的特征向量可以编码图像应来源于哪个类别（如果我们试图创建虚构演员的面部图像，可以是女性或男性的图像），甚至可以是我们期望图像具备的一组特定特征（对于虚构演员，可能是发型、眼睛或肤色等）。这一技巧是通过将信息融入到需要学习的图像和*Z*输入中实现的，而*Z*输入不再是完全随机的。判别器的评估不仅仅是基于假数据与真实数据的相似度，还会基于假数据图像与其输入标签（或特征）的一致性进行评估。
- en: '![](img/bf357ac9-ebea-4b71-b3d1-868b0738625d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf357ac9-ebea-4b71-b3d1-868b0738625d.png)'
- en: Figure 3: Combining Z input with Y input (a labeling feature vector) allows
    generating controlled images
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：将Z输入与Y输入（标签特征向量）结合，允许生成受控图像
- en: The project
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: 'Importing the right libraries is where we start. Apart from `tensorflow`, we
    will be using `numpy` and math for computations, `scipy`, `matplolib` for images
    and graphics, and `warnings`, `random`, and `distutils` for support in specific
    operations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 导入正确的库是我们的起点。除了`tensorflow`，我们还将使用`numpy`和math进行计算，`scipy`、`matplolib`用于图像和图形处理，`warnings`、`random`和`distutils`则为特定操作提供支持：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dataset class
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集类
- en: Our first step is to provide the data. We will rely on datasets that have already
    been preprocessed, but our readers could use different kinds of images for their
    own GAN implementation. The idea is to keep separate a `Dataset` class that will
    have the task of providing batches of normalized and reshaped images to the GANs
    class we will build later.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是提供数据。我们将依赖已预处理的数据集，但读者可以使用不同类型的图像进行他们自己的GAN实现。我们的想法是保持一个独立的`Dataset`类，该类负责为我们稍后构建的GAN类提供规范化和重塑后的图像批次。
- en: 'In the initialization, we will deal with both images and their labels (if available).
    Images are first reshaped (if their shape differs from the one defined when instantiating
    the class), then shuffled. Shuffling helps GANs learning better if any order,
    for instance by class, is initially inscribed into the dataset - and this is actually
    true for any machine learning algorithm based on stochastic gradient descent:
    <q>BOTTOU, Léon. Stochastic gradient descent tricks. In: Neural networks: Tricks
    of the trade</q><q>. Springer, Berlin, Heidelberg, 2012\. p. 421-436</q>: [https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf).
    Labels instead are encoded using one-hot encoding, that is, a binary variable
    is created for each one of the classes, which is set to one (whereas others are
    set to zero) to represent the label as a vector.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在初始化时，我们将处理图像及其标签（如果有的话）。首先，图像会被重塑（如果其形状与实例化类时定义的形状不同），然后进行打乱。打乱有助于GANs更好地学习，如果数据集一开始就按某种顺序（例如按类别）排列，打乱会更有效——这实际上对任何基于随机梯度下降的机器学习算法都是成立的：<q>BOTTOU,
    Léon. 随机梯度下降技巧. 在: Neural networks: Tricks of the trade</q><q>。Springer，柏林，海德堡，2012年，第421-436页</q>:
    [https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf)。标签则使用独热编码进行编码，即为每个类别创建一个二进制变量，该变量被设置为1（而其他变量为0），以向量的形式表示标签。'
- en: 'For instance, if our classes are `{dog:0, cat:1}`, we will have these two one-hot
    encoded vectors to represent them: `{dog:[1, 0], cat:[0, 1]}`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的类别是`{dog:0, cat:1}`，我们将使用这两个独热编码向量来表示它们：`{dog:[1, 0], cat:[0, 1]}`。
- en: 'In such a way, we can easily add the vector to our image, as a further channel,
    and inscribe into it some kind of visual characteristic to be replicated by our
    GAN. Moreover, we could arrange the vectors in order to inscribe even more complex
    classes with special characteristics. For instance, we could specify the code
    for a class we prefer to be generated, and we can also specify some of its characteristics:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以轻松地将向量添加到我们的图像中，作为一个额外的通道，并在其中铭刻某种视觉特征，供我们的 GAN 复制。此外，我们还可以排列这些向量，以便铭刻更复杂的类别，并赋予其特殊特征。例如，我们可以指定一个我们希望生成的类别的代码，也可以指定一些它的特征：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `get_batches` method will just release a batch subset of the dataset and
    normalize the data by dividing the pixel values by the maximum (256) and subtracting
    -0.5\. The resulting images will have float values in the interval [-0.5, +0.5]:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_batches` 方法将只释放数据集的一个批次子集，并通过将像素值除以最大值（256）并减去 -0.5 来规范化数据。结果图像的浮动值将在区间
    [-0.5, +0.5] 内：'
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: CGAN class
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CGAN 类
- en: 'The `CGAN` class contains all the functions necessary for running a conditional
    GAN based on the `CGAN` model. The deep convolutional generative adversarial networks
    proved to have the performance in generating photo-like quality outputs. We have
    previously introduced CGANs, so just to remind you, their reference paper is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`CGAN` 类包含了运行基于 `CGAN` 模型的条件 GAN 所需的所有函数。深度卷积生成对抗网络已被证明能够生成接近照片质量的输出。我们之前已经介绍了
    CGAN，因此提醒大家，它们的参考论文是：'
- en: RADFORD, Alec; METZ, Luke; CHINTALA, Soumith. Unsupervised representation learning
    with deep convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434,
    2015 at [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: RADFORD, Alec; METZ, Luke; CHINTALA, Soumith. 无监督表示学习与深度卷积生成对抗网络。arXiv 预印本 arXiv:1511.06434,
    2015，链接：[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)。
- en: In our project, we will then add the conditional form of the `CGAN` that uses
    label information as in a supervised learning task. Using labels and integrating
    them with images (this is the trick) will result in much better images and in
    the possibility of deciding the characteristics of the generated image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们将添加条件形式的 `CGAN`，它使用标签信息，就像在监督学习任务中一样。使用标签并将其与图像整合（这就是技巧）将生成更好的图像，并且可以决定生成图像的特征。
- en: 'The reference paper for conditional GANs is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 GAN 的参考论文是：
- en: MIRZA, Mehdi; OSINDERO, Simon. *Conditional Generative Adversarial Nets*. arXiv
    preprint arXiv:1411.1784, 2014, [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MIRZA, Mehdi; OSINDERO, Simon. *条件生成对抗网络*。arXiv 预印本 arXiv:1411.1784, 2014，链接：[https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)。
- en: Our `CGAN` class expects as input a dataset class object, the number of epochs,
    the image `batch_size`, the dimension of the random input used for the generator
    (`z_dim`), and a name for the GAN (for saving purposes). It also can be initialized
    with different values for alpha and smooth. We will discuss later what these two
    parameters can do for the GAN network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `CGAN` 类需要作为输入的一个数据集类对象、训练的 epoch 数量、图像 `batch_size`、用于生成器的随机输入维度 (`z_dim`)，以及一个
    GAN 的名称（用于保存）。它还可以使用不同的 alpha 和 smooth 值进行初始化。我们稍后会讨论这两个参数如何影响 GAN 网络。
- en: 'The instantiation sets all the internal variables and performs a performance
    check on the system, raising a warning if a GPU is not detected:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化设置了所有内部变量，并对系统进行性能检查，如果未检测到 GPU，则会发出警告：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `instantiate_inputs` function creates the TensorFlow placeholders for the
    inputs, both real and random. It also provides the labels (treated as images of
    the same shape of the original but for a channel depth equivalent to the number
    of classes), and for the learning rate of the training procedure:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`instantiate_inputs` 函数为输入（包括真实和随机输入）创建 TensorFlow 占位符。它还提供标签（作为与原图像形状相同但通道深度等于类别数量的图像进行处理），以及训练过程的学习率：'
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we pass to work on the architecture of the network, defining some basic
    functions such as the `leaky_ReLU_activation` function (that we will be using
    for both the generator and the discriminator, contrary to what is prescribed in
    the original paper on deep convolutional GANs):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始着手网络架构的工作，定义一些基本函数，如 `leaky_ReLU_activation` 函数（我们将在生成器和判别器中都使用它，这与原始深度卷积
    GAN 论文中所规定的相反）：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our next function represents a discriminator layer. It creates a convolution
    using Xavier initialization, operates batch normalization on the result, sets
    a `leaky_ReLU_activation`, and finally applies `dropout` for regularization:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的函数表示一个判别器层。它使用Xavier初始化创建卷积，对结果进行批量归一化，设置一个`leaky_ReLU_activation`，最后应用`dropout`进行正则化：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Xavier initialization assures that the initial weights of the convolution are
    not too small, nor too large, in order to allow a better transmission of the signals
    through the network since the initial epochs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier初始化确保卷积的初始权重既不太小，也不太大，以便从初期阶段开始，信号可以更好地在网络中传输。
- en: Xavier initialization provides a Gaussian distribution with a zero mean whose
    variance is given by 1.0 divided by the number of neurons feeding into a layer.
    It is because of this kind of initialization that deep learning moved away from
    pre-training techniques, previously used to set initial weights that could transmit
    back propagation even in the presence of many layers. You can read more about
    it and about the Glorot and Bengio's variant of the initialization in this post: [http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization.](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier初始化提供了一个均值为零的高斯分布，其方差由1.0除以输入层神经元的数量给出。正是由于这种初始化方式，深度学习摆脱了以前用于设定初始权重的预训练技术，这些技术能够即使在存在许多层的情况下，也能传递反向传播。你可以在这篇文章中了解更多内容，关于Glorot和Bengio的初始化变体：[http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization.](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
- en: 'Batch normalization is described by this paper:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化在这篇论文中有所描述：
- en: 'IOFFE, Sergey; SZEGEDY, Christian. Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. In: *International Conference on
    Machine Learning*. 2015\. p. 448-456.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: IOFFE, Sergey; SZEGEDY, Christian. 批量归一化：通过减少内部协变量偏移加速深度网络训练。来源：*国际机器学习会议*，2015年，p.
    448-456。
- en: As noted by the authors, the batch normalization algorithm for normalization
    deals with covariate shift ([http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html](http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html)),
    that is, changing distribution in the inputs which could cause the previously
    learned weights not to work properly anymore. In fact, as distributions are initially
    learned in the first input layers, they are transmitted to all the following layers,
    and shifting later because suddenly the input distribution has changed (for instance,
    initially you had more input photos of cats than dogs, now it's the contrary)
    could prove quite daunting unless you have set the learning rate very low.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者所指出的，批量归一化算法用于归一化，它处理协变量偏移（[http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html](http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html)），即输入数据的分布变化，可能导致之前学习到的权重无法正常工作。事实上，由于分布最初是在第一个输入层中学习的，它们会传递到所有后续层，而后续层的分布如果发生了变化（例如，最初你有更多的猫的照片而不是狗的照片，现在正好相反），则可能会很棘手，除非你已经将学习率设置得非常低。
- en: 'Batch normalization solves the problem of changing distribution in the inputs
    because it normalizes each batch by both mean and variance (using batch statistics),
    as illustrated by the paper <q>IOFFE, Sergey; SZEGEDY, Christian</q>. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. <q>In: International
    Conference on Machine Learning. 2015\. p. 448-456</q> (it can be found on the
    Internet at [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化解决了输入数据分布变化的问题，因为它通过均值和方差（使用批量统计量）对每个批次进行归一化，正如论文中所示 <q>IOFFE, Sergey;
    SZEGEDY, Christian</q>。批量归一化：通过减少内部协变量偏移加速深度网络训练。<q>来源：国际机器学习会议，2015年，p. 448-456</q>（可以通过[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)在网上找到）。
- en: '``g_reshaping `and` g_conv_transpose`` are two functions that are part of the
    generator. They operate by reshaping the input, no matter if it is a flat layer
    or a convolution. Practically, they just reverse the work done by convolutions,
    restoring back the convolution-derived features into the original ones:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '``g_reshaping `和` g_conv_transpose``是生成器的一部分。它们通过调整输入的形状来操作，无论输入是平坦层还是卷积层。实际上，它们只是反转卷积所做的工作，将卷积生成的特征恢复成原始特征：'
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The discriminator architecture operates by taking images as input and, by various
    convolutions, transforming them until the result is flattened and turned into
    logits and probabilities (using the sigmoid function). Practically, everything
    is the same as in an ordinal convolution:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器架构的工作原理是通过将图像作为输入，并通过各种卷积操作将其转换，直到结果被展平并转化为对数值和概率（使用sigmoid函数）。实际上，所有操作与常规卷积中的操作相同：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As for the generator, the architecture is exactly the opposite of the discriminator.
    Starting from an input vector, `z`, a dense layer is first created, then a series
    of transpositions aims to rebuild the inverse process of convolutions in the discriminator,
    ending in a tensor of the same shape of the input images, which undergoes a further
    transformation by a `tanh` activation function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 至于生成器，其架构与判别器完全相反。从输入向量`z`开始，首先创建一个密集层，然后通过一系列转置操作重建判别器中的卷积逆过程，最终得到一个与输入图像形状相同的张量，接着通过`tanh`激活函数进行进一步的转换：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The architecture is very similar to the one depicted in the paper introducing
    CGANs, depicting how to reconstruct a 64 x 64 x 3 image from an initial input
    of a vector of size 100:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构与CGAN论文中介绍的架构非常相似，展示了如何从一个大小为100的向量的初始输入重建一个64 x 64 x 3的图像：
- en: '![](img/a4918bff-eb00-41c2-aea4-ff729f556f37.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4918bff-eb00-41c2-aea4-ff729f556f37.png)'
- en: 'Figure 4: The DCGAN architecture of the generator.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：生成器的DCGAN架构。
- en: 'SOURCE: arXiv, 1511.06434,2015'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：arXiv, 1511.06434, 2015
- en: After defining the architecture, the loss function is the next important element
    to define. It uses two outputs, the output from the generator, which is pipelined
    into the discriminator outputting logits, and the output from the real images
    pipelined themselves into the discriminator. For both, a loss measure is then calculated.
    Here, the smooth parameter comes in handy because it helps to smooth the probabilities
    of the real images into something that is not 1.0, allowing a better, more probabilistic
    learning by the GAN network (with full penalization it could become more difficult
    for the fake images to have a chance against the real ones).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了架构之后，损失函数是下一个需要定义的重要元素。它使用两个输出，一个是来自生成器的输出，经过管道传递给判别器并输出对数值，另一个是来自真实图像的输出，直接传递给判别器。对于这两者，都需要计算损失度量。这里，平滑参数非常有用，因为它有助于将真实图像的概率平滑成不是1.0的值，从而使GAN网络能够更好、更具概率性地学习（如果完全惩罚，假图像可能更难有机会与真实图像竞争）。
- en: 'The final discriminator loss is simply the sum of the loss calculated on the
    fake and on the real images. The loss is calculated on the fake comparing the
    estimated logits against the probability of zero. The loss on the real images
    is calculated comparing the estimated logit against the smoothed probability (in
    our case it is 0.9), in order to prevent overfitting and having the discriminator
    learn simply to spot the real images because it memorized them. The generator
    loss is instead calculated from the logits estimated by the discriminator for
    the fake images against a probability of 1.0\. In this way, the generator should
    strive to produce fake images that are estimated by the discriminator as most
    likely true (thus using a high probability). Therefore, the loss simply transmits
    from the discriminator evaluation on fake images to the generator in a feedback
    loop:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的判别器损失是简单地将对假图像和真实图像计算的损失相加。假图像的损失通过比较估计的对数值与零的概率来计算。真实图像的损失通过将估计的对数值与平滑后的概率（在我们的例子中是0.9）进行比较来计算，目的是防止过拟合，并避免判别器仅仅因为记住了真实图像而学习识别它们。生成器的损失则是根据判别器对假图像估计的对数值与1.0的概率进行计算的。通过这种方式，生成器应该努力生成被判别器估计为最可能为真的假图像（因此使用较高的概率）。因此，损失从判别器对假图像的评估传递到生成器，形成一个反馈循环：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since the work of the GAN is visual, there are a few functions for visualizing
    a sample of the current production from the generator, as well as a specific set
    of images:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GAN的工作是视觉性的，因此有一些函数用于可视化生成器当前生成的样本，以及一组特定的图像：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using the Adam optimizer, both the discriminator loss and the generator one
    are reduced, starting first from the discriminator (establishing how good is the
    generator''s production against true images) and then propagating the feedback
    to the generator, based on the evaluation of the effect the fake images produced
    by the generator had on the discriminator:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Adam优化器，判别器损失和生成器损失都会被减少，首先从判别器开始（建立生成器的输出与真实图像的对比），然后将反馈传递给生成器，基于生成器所生成的假图像对判别器的影响进行评估：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At last, we have the complete training phase. In the training, there are two
    parts that require attention:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们完成了整个训练阶段。在训练过程中，有两个部分需要特别注意：
- en: 'How the optimization is done in two steps:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化是如何通过两个步骤进行的：
- en: Running the discriminator optimization
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行判别器优化
- en: Working on the generator's one
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成器部分工作
- en: How the random input and the real images are preprocessed by mixing them with
    labels in a way that creates further image layers containing the one-hot encoded
    information of the class relative to the image's label
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过将随机输入和真实图像与标签混合的方式预处理图像，从而创建包含图像标签的独热编码信息的额外图层
- en: 'In this way, the class is incorporated into the image, both in input and in
    output, conditioning the generator to take this information into account also,
    since it is penalized if it doesn''t produce realistic images, that is, images
    with the right label attached. Let''s say that our generator produces the image
    of a cat, but gives it the label of a dog. In this case, it will be penalized
    by the discriminator because the discriminator will notice how the generator cat
    is different from the real cats because of the different labels:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，类被整合进图像中，既作为输入也作为输出，迫使生成器在生成时也要考虑这些信息，因为如果它生成不真实的图像，即没有正确标签的图像，就会受到惩罚。比如说我们的生成器生成了一只猫的图像，但给它加上了狗的标签。在这种情况下，判别器会惩罚它，因为判别器会注意到生成的猫和真实的猫在标签上有差异：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'During the training, the network is constantly saved on disk. When it is necessary
    to generate new images, you don''t need to retrain, but just upload the network
    and specify the label you want the GAN to produce images for:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，网络会不断保存到磁盘上。当需要生成新图像时，你无需重新训练，只需加载网络并指定你希望GAN生成的标签即可：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The class is completed by the `fit` method, which accepts both the learning
    rate parameter and the beta1 (an Adam optimizer parameter, adapting the parameter
    learning rates based on the average first moment, that is, the mean), and plots
    the resulting losses from the discriminator and the generator after the training
    is completed:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类通过`fit`方法完成，该方法接受学习率参数和beta1（一个Adam优化器参数，根据平均一阶矩调整参数的学习率，即均值），并在训练完成后绘制判别器和生成器的损失曲线：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Putting CGAN to work on some examples
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将CGAN应用于一些示例
- en: 'Now that the `CGAN` class is completed, let''s go through some examples in
    order to provide you with fresh ideas on how to use this project. First of all,
    we will have to get everything ready for both downloading the necessary data and
    training our GAN. We start by importing the routine libraries:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`CGAN`类已经完成，让我们通过一些示例来提供新的思路，帮助你更好地使用这个项目。首先，我们需要准备好所有必要的资源，包括下载所需的数据和训练我们的GAN。我们从导入常用库开始：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then proceed by loading in the dataset and `CGAN` classes that we previously
    prepared:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载之前准备好的数据集和`CGAN`类：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The class `TqdmUpTo` is just a `tqdm` wrapper that enables the use of the progress
    display also for downloads. The class has been taken directly from the project''s
    page at [https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`TqdmUpTo`类只是一个`Tqdm`的封装器，它使得进度条显示器也可以用于下载。这个类是直接从项目页面[https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm)中提取的：'
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, if we are using a Jupyter notebook (warmly suggested for this roadshow),
    you have to enable the inline plotting of images:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们使用的是Jupyter Notebook（强烈建议在本次展示中使用），你需要启用图像的内联显示：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We are now ready to proceed with the first example.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好继续进行第一个示例了。
- en: MNIST
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST
- en: 'The `MNIST` database of handwritten digits was provided by Yann LeCun when
    he was at Courant Institute, NYU, and by Corinna Cortes (Google Labs) and Christopher
    J.C. Burges (Microsoft Research). It is considered the standard for learning from
    real-world image data with minimal effort in preprocessing and formatting. The
    database consists of handwritten digits, offering a training set of 60,000 examples
    and a test set of 10,000\. It is actually a subset of a larger set available from
    NIST. All the digits have been size-normalized and centered in a fixed-size image:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`手写数字数据库由Yann LeCun在纽约大学Courant研究所时提供，并由Corinna Cortes（谷歌实验室）和Christopher
    J.C. Burges（微软研究院）共同提供。它被认为是从现实世界图像数据中学习的标准数据库，且在预处理和格式化方面所需的努力最小。该数据库包含手写数字，提供了60,000个训练样本和10,000个测试样本。它实际上是从NIST的一个更大数据集中提取的子集。所有数字都已经大小归一化并居中在固定尺寸的图像中：'
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
- en: '![](img/ce2ca276-49f1-411a-be98-f281615230de.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce2ca276-49f1-411a-be98-f281615230de.png)'
- en: 'Figure 5: A sample of the original MNIST helps to understand the quality of
    the images to be reproduced by the CGAN.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：原始MNIST样本有助于理解CGAN重建图像的质量。
- en: 'As a first step, we upload the dataset from the Internet and store it locally:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从互联网上上传数据集并将其保存在本地：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In order to learn this set of handwritten numbers, we apply a batch of 32 images,
    a learning rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and `15`
    epochs for training:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习这一组手写数字，我们应用了32张图像的批量，学习率为`0.0002`，`beta1`为`0.35`，`z_dim`为`96`，并进行了`15`轮训练：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following image represents a sample of the numbers generated by the GAN
    at the second epoch and at the last one:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了GAN在第二轮和最后一轮生成的数字样本：
- en: '![](img/3fe5beef-20c5-45ab-8e30-18b7ab0c260f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fe5beef-20c5-45ab-8e30-18b7ab0c260f.png)'
- en: 'Figure 6: The GAN''s results as they appear epoch after epoch'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：GAN的结果随着训练轮次的变化
- en: After 16 epochs, the numbers appear to be well shaped and ready to be used.
    We then extract a sample of all the classes arranged by row.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 经过16轮训练后，数字看起来已经形成良好，准备使用。然后我们提取了按行排列的所有类别样本。
- en: Evaluating the performances of a GAN is still most often the matter of visual
    inspecting some of its results by a human judge, trying to figure out if the image
    could be a fake (like a discriminator) from its overall aspect or by precisely
    revealing details. GANs lack an objective function to help to evaluate and compare
    them, though there are some computational techniques that could be used as a metric
    such as the *log-likelihood*, as described by <q>THEIS, Lucas; OORD, Aäron van
    den; BETHGE, Matthias. A note on the evaluation of generative models. arXiv preprint
    arXiv:1511.01844</q><q>, 2015</q>: [https://arxiv.org/abs/1511.01844](https://arxiv.org/abs/1511.01844).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '评估GAN的性能通常依赖于人工评估其某些结果，通过观察图像的整体外观或细节来判断图像是否可能是伪造的（像判别器一样）。GAN缺乏一个客观函数来帮助评估和比较它们，尽管有一些计算技术可以作为评估指标，例如*对数似然*，如<q>THEIS,
    Lucas; OORD, Aäron van den; BETHGE, Matthias. 关于生成模型评估的笔记. arXiv预印本arXiv:1511.01844</q><q>,
    2015</q>: [https://arxiv.org/abs/1511.01844](https://arxiv.org/abs/1511.01844)所描述。'
- en: 'We will keep our evaluation simple and empirical and thus we will use a sample
    of images generated by the trained GAN in order to evaluate the performances of
    the network and we also try to inspect the training loss for both the generator
    and the discriminator in order to spot any particular trend:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持评价方法简单且经验性，因此我们将使用由训练过的GAN生成的图像样本来评估网络的表现，同时尝试检查生成器和判别器的训练损失，以便发现任何特殊趋势：
- en: '![](img/57ca84ee-b495-4231-a862-a24a0c50afcc.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57ca84ee-b495-4231-a862-a24a0c50afcc.png)'
- en: 'Figure 7: A sample of the final results after training on MNIST reveals it
    is an accessible task for a GAN network'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在MNIST上训练后的最终结果样本显示，GAN网络能够完成这一任务
- en: 'Observing the training fit chart, represented in the figure the following,
    we notice how the generator reached the lowest error when the training was complete.
    The discriminator, after a previous peak, is struggling to get back to its previous
    performance values, pointing out a possible generator''s breakthrough. We can
    expect that even more training epochs could improve the performance of this GAN
    network, but as you progress in the quality the output, it may take exponentially
    more time. In general, a good indicator of convergence of a GAN is having a downward
    trend of both the discriminator and generator, which is something that could be
    inferred by fitting a linear regression line to both loss vectors:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 观察训练拟合图（下图所示），我们可以看到生成器在训练完成时达到了最低误差。判别器在经历了一个先前的峰值后，正在努力恢复到之前的性能值，这表明生成器可能有了突破。我们可以预期，更多的训练周期可能会提高该
    GAN 网络的性能，但随着输出质量的提升，所需时间可能会呈指数增长。通常，GAN 收敛的良好指标是生成器和判别器的损失值都呈下降趋势，这一点可以通过对两个损失向量拟合一条线性回归线来推测：
- en: '![](img/246d5831-c0aa-4504-947f-1236b006eea9.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/246d5831-c0aa-4504-947f-1236b006eea9.png)'
- en: 'Figure 8: The training fit along the 16 epochs'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：16 个训练周期内的拟合情况
- en: Training an amazing GAN network may take a very long time and a lot of computational
    resources. By reading this recent article appeared in the New York Times, [https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html),
    you can find a chart from NVIDIA showing the progress in time for the training
    of a progressive GAN learning from photos of celebrities. Whereas it can take
    a few days to get a decent result, for an astonishing one you need at least a
    fortnight. In the same way, even with our examples, the more training epochs you
    put in, the better the results.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个出色的 GAN 网络可能需要很长时间和大量的计算资源。通过阅读《纽约时报》近期发布的文章，[https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html)，你可以看到来自
    NVIDIA 的图表，展示了一个渐进式 GAN 学习名人照片的训练进度。虽然获得一个不错的结果可能只需要几天时间，但要达到惊人的效果至少需要两周。同样地，即使是我们的例子，投入更多的训练周期，结果也会变得更好。
- en: Zalando MNIST
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Zalando MNIST
- en: Fashion `MNIST` is a dataset of Zalando's article images, composed of a training
    set of 60,000 examples and a test set of 10,000 examples. As with `MNIST`, each
    example is a 28x28 grayscale image, associated with a label from 10 classes. It
    was intended by authors from Zalando Research ([https://github.com/zalandoresearch/fashion-mnist/graphs/contributors](https://github.com/zalandoresearch/fashion-mnist/graphs/contributors))
    as a replacement for the original MNIST dataset in order to better benchmark machine
    learning algorithms since it is more challenging to learn and much more representative
    of deep learning in real-world tasks ([https://twitter.com/fchollet/status/852594987527045120](https://twitter.com/fchollet/status/852594987527045120)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion `MNIST` 是 Zalando 文章图像的数据集，由 60,000 个训练样本和 10,000 个测试样本组成。与 `MNIST`
    类似，每个样本都是一个 28x28 的灰度图像，带有 10 个类别中的一个标签。Zalando Research 的作者们意图将其作为原始 MNIST 数据集的替代品，以更好地评估机器学习算法，因为它比
    MNIST 更具挑战性，并且更能代表现实任务中的深度学习（[https://twitter.com/fchollet/status/852594987527045120](https://twitter.com/fchollet/status/852594987527045120)）。
- en: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
- en: '![](img/72d9410c-dc59-41ba-b547-13ba2893f256.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72d9410c-dc59-41ba-b547-13ba2893f256.png)'
- en: 'Figure 9: A sample of the original Zalando dataset'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：原始 Zalando 数据集的一个样本
- en: 'We download the images and their labels separately:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分别下载图像和它们的标签：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In order to learn this set of images, we apply a batch of 32 images, a learning
    rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and `10` epochs for
    training:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习这组图像，我们应用了一个包含 32 张图像的批次，学习率为`0.0002`，`beta1`为`0.35`，`z_dim`为`96`，并进行了`10`个周期的训练：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The training takes a long time to go through all the epochs, but the quality
    appears to soon stabilize, though some problems take more epochs to disappear
    (for instance holes in shirts):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要很长时间才能完成所有周期，但质量似乎很快会稳定下来，尽管有些问题需要更多周期才能消失（例如衬衫上的孔洞）：
- en: '![](img/9ef7ad88-3460-48b3-a62d-11ed0db43d9a.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ef7ad88-3460-48b3-a62d-11ed0db43d9a.png)'
- en: 'Figure 10: The evolution of the CGAN''s training through epochs'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：CGAN 训练过程随周期的演变
- en: 'Here is the result after 64 epochs:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 64 个 epoch 后的结果：
- en: '![](img/7dc822bf-59d2-48af-8183-da1d5bd141b7.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dc822bf-59d2-48af-8183-da1d5bd141b7.png)'
- en: 'Figure 11: An overview of the results achieved after 64 epochs on Zalando dataset'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：在 Zalando 数据集上进行 64 个 epoch 后的结果概览
- en: The result is fully satisfactory, especially for clothes and men's shoes. Women's
    shoes, however, seem more difficult to be learned because smaller and more detailed
    than the other images.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果完全令人满意，尤其是衣服和男鞋。然而，女鞋似乎更难学习，因为它们比其他图像更小且更精细。
- en: EMNIST
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EMNIST
- en: 'The `EMNIST` dataset is a set of handwritten character digits derived from
    the `NIST` Special Database and converted to a 28 x 28 pixel image format and
    dataset structure that directly matches the `MNIST` dataset. We will be using
    `EMNIST` Balanced, a set of characters with an equal number of samples per class,
    which consists of 131,600 characters spread over 47 balanced classes. You can
    find all the references to the dataset in:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`EMNIST` 数据集是一组手写字符数字，源自 `NIST` 特别数据库，并转换为 28 x 28 像素的图像格式和数据集结构，直接匹配 `MNIST`
    数据集。我们将使用 `EMNIST` Balanced，它是一个每个类别样本数相等的字符集，包含 131,600 个字符，分布在 47 个平衡类别中。你可以在以下位置找到所有与该数据集相关的参考文献：'
- en: 'Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension
    of MNIST to handwritten letters. Retrieved from [http://arxiv.org/abs/1702.05373](http://arxiv.org/abs/1702.05373).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017)。EMNIST：MNIST 的手写字母扩展。检索自
    [http://arxiv.org/abs/1702.05373](http://arxiv.org/abs/1702.05373)。
- en: 'You can also explore complete information about `EMNIST` by browsing the official
    page of the dataset: [https://www.nist.gov/itl/iad/image-group/emnist-dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset).
    Here is an extraction of the kind of characters that can be found in the EMNIST
    Balanced:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过浏览数据集的官方网站，探索关于`EMNIST`的完整信息：[https://www.nist.gov/itl/iad/image-group/emnist-dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset)。以下是
    EMNIST Balanced 中可以找到的字符类型的提取：
- en: '![](img/aff40bc1-76d6-4e47-b6ea-bb6e1b027db8.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aff40bc1-76d6-4e47-b6ea-bb6e1b027db8.png)'
- en: 'Figure 11: A sample of the original EMNIST dataset'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：原始 EMNIST 数据集的示例
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After downloading from the NIST website, we unzip the downloaded package:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从 NIST 网站下载后，我们解压了下载的包：
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We remove the unused ZIP file after checking that the unzipping was successful:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认解压成功后，我们删除未使用的 ZIP 文件：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order to learn this set of handwritten numbers, we apply a batch of 32 images,
    a learning rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and 10 epochs
    for training:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习这组手写数字，我们应用了一个包含 32 张图像的批次，学习率为 `0.0002`，`beta1` 为 `0.35`，`z_dim` 为 `96`，并进行了
    10 个 epoch 的训练：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is a sample of some handwritten letters when completing the training after
    32 epochs:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成 32 个 epoch 训练后的部分手写字母示例：
- en: '![](img/b9e19ccb-f85c-451d-837e-4c8b081285cc.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9e19ccb-f85c-451d-837e-4c8b081285cc.png)'
- en: 'Figure 12: An overview of the results obtained training a CGAN on the EMNIST
    dataset'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：在 EMNIST 数据集上训练 CGAN 后的结果概览
- en: As for MNIST, a GAN can learn in a reasonable time to replicate handwritten
    letters in an accurate and credible way.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MNIST，GAN 可以在合理的时间内学习以准确、可信的方式复制手写字母。
- en: Reusing the trained CGANs
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重用训练好的 CGAN
- en: After training a CGAN, you may find useful to use the produced images in other
    applications. The method `generate_new` can be used to extract single images as
    well as a set of images (in order to check the quality of results for a specific
    image class). It operates on a previously trained `CGan` class, so all you have
    to do is just to pickle it in order first to save it, then to restore it again
    when needed.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好一个 CGAN 后，你可能会发现将生成的图像用于其他应用非常有用。方法 `generate_new` 可以用来提取单张图像或一组图像（以便检查特定图像类别的结果质量）。它作用于先前训练好的
    `CGan` 类，因此你只需要先将其保存，然后在需要时恢复。
- en: 'When the training is complete, you can save your `CGan` class using `pickle`,
    as shown by these commands:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成后，你可以使用 `pickle` 保存你的 `CGan` 类，命令如下所示：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this case, we have saved the `CGAN` trained on the MNIST dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们已保存了在 MNIST 数据集上训练的 `CGAN`。
- en: 'After you have restarted the Python session and memory is clean of any variable,
    you can just `import` again all the classes and restore the pickled `CGan`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新启动 Python 会话并清理内存中的任何变量后，你可以再次 `import` 所有类，并恢复已保存的 `CGan`：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'When done, you set the target class you would like to be generated by the `CGan`
    (in the example we ask for the number `8` to be printed) and you can ask for a
    single example, a grid 5 x 5 of examples or a larger 10 x 10 grid:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，设置你希望`CGan`生成的目标类别（在本例中我们要求打印数字`8`），你可以请求生成单个示例、一个5 x 5的网格或一个更大的10 x 10网格：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you just want to obtain an overview of all the classes, just set the parameter
    `target_class` to -1.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是想获得所有类别的概览，只需将参数`target_class`设置为-1。
- en: 'After having set out target class to be represented, the `generate_new` is
    called three times and the last one the returned values are stored into the `images`
    variable, which is sized (100, 28, 28, 1) and contains a Numpy array of the produced
    images that can be reused for our purposes. Each time you call the method, a grid
    of results is plotted as shown in the following figure:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置目标类别之后，`generate_new`方法被调用三次，最后返回的值存储在`images`变量中，该变量的大小为(100, 28, 28, 1)，包含生成的图像的Numpy数组，可以用于我们的目的。每次调用该方法时，都会绘制一个结果网格，如下图所示：
- en: '![](img/f9f18203-dfc6-4fdc-8306-d108b47f9c45.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9f18203-dfc6-4fdc-8306-d108b47f9c45.png)'
- en: 'Figure 13: The plotted grid is a composition of the produced images, that is
    an image itself. From left to right, the plot of a'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：绘制的网格是生成图像的组合，实际上是一个图像。从左到右，图像的绘制过程是
- en: request for a 1 x 1, 5 x 5, 10 x 10 grid of results. The real images are returned
    by the method and can be reused.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请求1 x 1、5 x 5、10 x 10结果网格。方法返回的真实图像可以重复使用。
- en: If you don't need `generate_new` to plot the results, you simply set the `plot`
    parameter to False: `images = gan.generate_new(target_class=nclass, rows=10, cols=10,
    plot=False)`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要`generate_new`来绘制结果，只需将`plot`参数设置为False：`images = gan.generate_new(target_class=nclass,
    rows=10, cols=10, plot=False)`。
- en: Resorting to Amazon Web Service
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon Web Service
- en: As previously noticed, it is warmly suggested you use a GPU in order to train
    the examples proposed in this chapter. Managing to obtain results in a reasonable
    time using just a CPU is indeed impossible, and also using a GPU may turn into
    quite long hours waiting for the computer to complete the training. A solution,
    requiring the payment of a fee, could be to resort to Amazon Elastic Compute Cloud,
    also known as Amazon EC2 ([https://aws.amazon.com/it/ec2/](https://aws.amazon.com/it/ec2/)),
    part of the **Amazon Web Services** (**AWS**). On EC2 you can launch virtual servers
    that you can control from your computer using the Internet connection. You can
    require servers with powerful GPUs on EC2 and make your life with TensorFlow projects
    much easier.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，强烈建议你使用GPU来训练本章中提出的示例。仅使用CPU在合理的时间内获得结果几乎是不可能的，甚至使用GPU也可能需要长时间等待计算机完成训练。一个需要支付费用的解决方案是使用Amazon
    Elastic Compute Cloud，也就是Amazon EC2 ([https://aws.amazon.com/it/ec2/](https://aws.amazon.com/it/ec2/))，它是**Amazon
    Web Services**（**AWS**）的一部分。在EC2上，你可以启动虚拟服务器，并通过互联网连接从你的计算机进行控制。你可以在EC2上要求强大的GPU服务器，使得TensorFlow项目的工作变得更加轻松。
- en: Amazon EC2 is not the only cloud service around. We have suggested you this
    service because it is the one we used in order to test the code in this book.
    Actually, there are alternatives, such as Google Cloud Compute ([cloud.google.com](http://cloud.google.com)),
    Microsoft Azure (azure.microsoft.com) and many others.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EC2并不是唯一的云服务提供商。我们推荐这个服务，因为它是我们在本书中测试代码时使用的。实际上，也有其他替代服务，比如Google Cloud
    Compute ([cloud.google.com](http://cloud.google.com))、Microsoft Azure（azure.microsoft.com）以及许多其他服务。
- en: Running the chapter’s code on EC2 requires having an account in AWS. If you
    don’t have one, the first step is to register at [aws.amazon.com](https://aws.amazon.com/),
    complete all the necessary forms and start with a free Basic Support Plan.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在EC2上运行本章的代码需要在AWS中拥有一个账户。如果你还没有账户，第一步是注册到[aws.amazon.com](https://aws.amazon.com/)，填写所有必要的表格，并开始使用免费的基础支持计划。
- en: 'After you are registered on AWS, you just sign in and visit the EC2 page ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2)).
    There you will:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在你注册AWS之后，只需登录并访问EC2页面([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2))，在该页面你将会：
- en: Select a region which is both cheap and near to you which allows the kind of
    GPU instances we need, from EU (Ireland), Asia Pacific (Tokyo), US East (N. Virginia)
    and US West (Oregon).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个既便宜又靠近你的区域，并且该区域支持我们需要的GPU实例，包括欧盟（爱尔兰）、亚太地区（东京）、美国东部（北弗吉尼亚）和美国西部（俄勒冈）。
- en: 'Upgrade your EC2 Service Limit report at: [https://console.aws.amazon.com/ec2/v2/home?#Limits](https://console.aws.amazon.com/ec2/v2/home?#Limits).
    You will need to access a **p3.2xlarge** instance. Therefore if your actual limit
    is zero, that should be taken at least to one, using the *Request Limit Increase* form
    (this may take up to 24 hours, but before it''s complete, you won’t be able to
    access this kind of instance).'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下网址升级你的 EC2 服务限制报告：[https://console.aws.amazon.com/ec2/v2/home?#Limits](https://console.aws.amazon.com/ec2/v2/home?#Limits)。你将需要访问**p3.2xlarge**实例。因此，如果你当前的限制为零，至少应该通过*请求增加配额*表格将其提高到一个（这可能需要最多
    24 小时，完成之前你无法访问这种类型的实例）。
- en: Get some AWS credits (providing your credit card, for instance).
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一些 AWS 积分（例如提供你的信用卡信息）。
- en: 'After setting your region and having enough credit and request limit increase,
    you can start a **p3.2xlarge** server (a GPU compute server for deep learning
    applications) set up with an OS already containing all the software you need (thanks
    to an AMI, an image prepared by Amazon):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 设置好你的区域，并确保有足够的信用和请求配额增加后，你可以启动一个**p3.2xlarge**服务器（用于深度学习应用的 GPU 计算服务器），该服务器配有已经包含所有必要软件的操作系统（感谢
    Amazon 提供的 AMI，预先准备好的镜像）：
- en: Get to the EC2 Management Console, and click on the **Launch Instance** button.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 EC2 管理控制台，点击**启动实例**按钮。
- en: 'Click on AWS Marketplace, and search for **Deep Learning AMI with Source Code
    v2.0 (ami-bcce6ac4)** AMI. This AMI has everything pre-installed: CUDA, cuDNN
    ([https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)), Tensorflow.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 AWS Marketplace，搜索**Deep Learning AMI with Source Code v2.0 (ami-bcce6ac4)**
    AMI。这个 AMI 已经预安装了所有必要的东西：CUDA、cuDNN（[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)）、Tensorflow。
- en: Select the *GPU* compute **p3.2xlarge** instance. This instance has a powerful
    NVIDIA Tesla V100 GPU.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*GPU*计算**p3.2xlarge**实例。这个实例配备了强大的 NVIDIA Tesla V100 GPU。
- en: Configure a security group (which you may call **Jupyter**) by adding **Custom
    TCP Rule**, with TCP protocol, on `port 8888`, accessible from anywhere. This
    will allow you to run a Jupyter server on the machine and see the interface from
    any computer connected to the Internet.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置一个安全组（你可以称之为**Jupyter**），通过添加**自定义 TCP 规则**，使用 TCP 协议，指定`port 8888`，并允许任何地方访问。这将允许你在机器上运行
    Jupyter 服务器，并且从任何连接到互联网的计算机上查看界面。
- en: Create an **Authentication Key Pair**. You can call it `deeplearning_jupyter.pem`
    for instance. Save it on your computer in a directory you can easily access.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个**认证密钥对**。你可以将其命名为 `deeplearning_jupyter.pem` 例如。将其保存在一个你容易访问的目录中。
- en: Launch the instance. Remember that you will be paying since this moment unless
    you **stop** it from the AWS menu—you still will incur in some costs, but minor
    ones and you will have the instance available for you, with all your data—or simply
    **terminate** it and don’t pay any more for it.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动实例。记住，从此刻开始，你将开始付费，除非你在 AWS 菜单中选择**停止**它——这样你仍然会产生一些费用，但这些费用较小，并且你可以随时使用该实例，所有数据都在；或者你可以选择**终止**它，这样将不再产生任何费用。
- en: After everything is launched, you can access the server from your computer using
    ssh.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一切启动后，你可以通过 ssh 从你的计算机访问服务器。
- en: Take notice of the IP of the machine. Let’s say it is `xx.xx.xxx.xxx`, as an
    example.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意机器的 IP 地址。假设它是 `xx.xx.xxx.xxx`，作为示例。
- en: 'From a shell pointing to the directory where you `.pem` file is, type:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在指向 `.pem` 文件所在目录的 shell 中，输入：
- en: '`ssh -i deeplearning_jupyter.pem ubuntu@ xx.xx.xxx.xxx`'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`ssh -i deeplearning_jupyter.pem ubuntu@ xx.xx.xxx.xxx`'
- en: 'When you have accessed the server machine, configure its Jupyter server by
    typing these commands:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问到服务器机器后，通过输入以下命令来配置 Jupyter 服务器：
- en: '`jupyter notebook --generate-config`'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`jupyter notebook --generate-config`'
- en: '`sed -ie "s/#c.NotebookApp.ip = ''localhost''/#c.NotebookApp.ip = ''*''/g"
    ~/.jupyter/jupyter_notebook_config.py`'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sed -ie "s/#c.NotebookApp.ip = ''localhost''/#c.NotebookApp.ip = ''*''/g"
    ~/.jupyter/jupyter_notebook_config.py`'
- en: 'Operate on the server by copying the code (for instance by git cloning the
    code repository) and installing any library you may require. For instance, you
    could install these packages for this specific project:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务器上操作，复制代码（例如通过 git 克隆代码库）并安装任何你可能需要的库。例如，你可以为这个特定项目安装以下包：
- en: '`sudo pip3 install tqdm`'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sudo pip3 install tqdm`'
- en: '`sudo pip3 install conda`'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sudo pip3 install conda`'
- en: 'Launch the Jupyter server by running the command:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行命令启动 Jupyter 服务器：
- en: '`jupyter notebook --ip=0.0.0.0 --no-browser`'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`jupyter notebook --ip=0.0.0.0 --no-browser`'
- en: At this point, the server will run and your ssh shell will prompt you the logs
    from Jupyter. Among the logs, take note of the token (it is something like a sequence
    of numbers and letters).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此时，服务器将运行，并且您的ssh shell将提示您查看Jupyter的日志。在日志中，请注意token（类似于一串数字和字母的序列）。
- en: 'Open your browser and write in the address bar:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开您的浏览器并在地址栏中输入：
- en: '`http:// xx.xx.xxx.xxx:8888/`'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`http:// xx.xx.xxx.xxx:8888/`'
- en: When required type the token and you are ready to use the Jupiter notebook as
    you were on your local machine, but it is actually operating on the server. At
    this point, you will have a powerful server with GPU for running all your experiments
    with GANs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要时输入token，您就可以像在本地计算机上一样使用Jupyter笔记本，但实际上它是在服务器上运行。此时，您将拥有一个强大的带有GPU的服务器，用于运行所有与GAN相关的实验。
- en: Acknowledgements
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: In concluding this chapter, we would like to thank Udacity and Mat Leonard for
    their DCGAN tutorial, licensed under MIT ([https://github.com/udacity/deep-learning/blob/master/LICENSE](https://github.com/udacity/deep-learning/blob/master/LICENSE))
    which provided a good starting point and a benchmark for this project.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章时，我们要感谢Udacity和Mat Leonard的DCGAN教程，根据MIT许可证（[https://github.com/udacity/deep-learning/blob/master/LICENSE](https://github.com/udacity/deep-learning/blob/master/LICENSE)）提供了这个项目的良好起点和基准。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have discussed at length the topic of Generative Adversarial
    Networks, how they work, and how they can be trained and used for different purposes.
    As a project, we have created a conditional GAN, one that can generate different
    types of images, based on your input and we learned how to process some example
    datasets and train them in order to have a pickable class capable of creating
    new images on demand.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细讨论了生成对抗网络的主题，它们的工作原理以及如何训练和用于不同目的。作为一个项目，我们创建了一个条件GAN，它可以根据您的输入生成不同类型的图像，并且我们学习了如何处理一些示例数据集并训练它们，以便拥有一个可随需求生成新图像的可选类。
