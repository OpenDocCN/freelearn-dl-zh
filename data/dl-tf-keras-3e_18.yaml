- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Machine Learning Best Practices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: Machine learning is much more than building and training models. Till now in
    this book, we focused on different deep learning algorithms and introduced the
    latest algorithms, their power, and their limitations. In this chapter, we change
    our focus, from the ML/DL algorithms to the practices that can make us better
    machine learning engineers and scientists.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习不仅仅是构建和训练模型。到目前为止，在本书中，我们集中介绍了不同的深度学习算法，并介绍了最新的算法、它们的强大功能以及它们的局限性。在本章中，我们将焦点从机器学习/深度学习算法转向那些可以帮助我们成为更好的机器学习工程师和科学家的实践。
- en: 'The chapter will include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将包括：
- en: The need for best practices for AI/ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对AI/ML最佳实践的需求
- en: Data best practices
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据最佳实践
- en: Model best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型最佳实践
- en: The need for best practices
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对最佳实践的需求
- en: Today, deep learning algorithms are not just an active research area but part
    and parcel of many commercial systems and products. *Figure 18.1* shows the investment
    in AI start-ups in the last five years. You can see that the interest in AI start-ups
    is continuously increasing. From healthcare to virtual assistants, from room cleaning
    robots to self-driving cars, AI today is the driving force behind many of the
    recent important technological advances. AI is deciding whether a person should
    be hired, or should be given a loan. AI is creating the feeds you see on social
    media. There are **Natural Language Processing** (**NLP**) bots generating content,
    images, faces – anything you can think of – there is someone trying to put AI
    into it. Since most teams consist of multiple team members working cross-domain,
    it is important to build best practices. What should be the best practices? Well,
    there is no definitive answer to this question as best practices in ML depend
    on the specific problem domain and dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，深度学习算法不仅是一个活跃的研究领域，还成为许多商业系统和产品的重要组成部分。*图18.1*展示了过去五年人工智能初创企业的投资情况。你可以看到对AI初创企业的兴趣持续增长。从医疗保健到虚拟助手，从清洁机器人到自动驾驶汽车，今天的人工智能是许多近期重要技术进步的推动力。AI正在决定一个人是否应该被雇用，或者是否应该获得贷款。AI正在创建你在社交媒体上看到的动态。有**自然语言处理**（**NLP**）机器人生成内容、图像、面孔——任何你能想到的东西——有人在努力将AI应用其中。由于大多数团队由多个成员组成，跨领域合作，因此建立最佳实践至关重要。什么是最佳实践呢？嗯，这个问题没有定论，因为机器学习的最佳实践取决于具体的问题领域和数据集。
- en: 'However, in this chapter we will provide some general tips for best practices
    in machine learning:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本章中，我们将提供一些机器学习最佳实践的通用建议：
- en: '![Chart, bar chart  Description automatically generated](img/B18331_18_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图表，柱状图 说明自动生成](img/B18331_18_01.png)'
- en: 'Figure 18.1: Investment in AI start-ups in the last five years (2017–2022)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1：过去五年（2017–2022年）AI初创企业的投资情况
- en: 'Below are a few reasons why having best practices in machine learning is important:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些机器学习中实施最佳实践的重要原因：
- en: It can ensure that models are built in a way that is both effective and efficient.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以确保模型的构建既有效又高效。
- en: It can help to avoid issues such as overfitting, which can lead to poor performance
    on unseen data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有助于避免过拟合等问题，避免在未见过的数据上表现不佳。
- en: It can ensure that the models are interpretable and can be easily explained
    to non-technical audiences.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以确保模型是可解释的，并且可以轻松地向非技术受众解释。
- en: It can help to promote reproducibility in machine learning research.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有助于促进机器学习研究中的可重复性。
- en: In the coming sections, you will be introduced to some best practices as advocated
    by the **FAANG** (**Facebook**, **Amazon**, **Apple**, **Netflix**, and **Google**)
    companies and AI influencers. Following this advice can help you avoid common
    mistakes that can lead to inaccurate or poor results. These best practices will
    help ensure that your AI services are accurate and reliable. And finally, best
    practices can help you optimize your AI services for performance and efficiency.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，您将了解一些**FAANG**（**Facebook**，**Amazon**，**Apple**，**Netflix**，和**Google**）公司以及AI影响者提倡的最佳实践。遵循这些建议可以帮助你避免常见错误，避免得出不准确或糟糕的结果。这些最佳实践将帮助确保你的AI服务是准确和可靠的。最后，最佳实践还可以帮助你优化AI服务的性能和效率。
- en: Data best practices
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据最佳实践
- en: Data is becoming increasingly important in today’s world. Not just people in
    the field of AI but various world leaders are calling data “the new gold” or “the
    new oil”– basically the commodity that will drive the economy around the world.
    Data is helping in decision making processes, managing transport, dealing with
    supply chain issues, supporting healthcare, and so on. The insights derived from
    data can help businesses improve their efficiency and performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在当今世界变得越来越重要。不仅是人工智能领域的人，许多世界领导者也将数据称为“新黄金”或“新石油”——基本上是将推动全球经济的商品。数据在决策过程中、交通管理、供应链问题处理、支持医疗等方面都起着重要作用。通过数据获得的洞察能够帮助企业提升效率和表现。
- en: Most importantly, data can be used to create new knowledge. In business, for
    example, data can be used to identify new trends. In medicine, data can be used
    to uncover new relationships between diseases and to develop new treatments. However,
    our models are only as good as the data they are trained on. And therefore, the
    importance of data is likely to continue to increase in the future. As data becomes
    more accessible and easier to use, it will become increasingly important in a
    variety of fields. Let us now see some common bottlenecks and the best way to
    deal with them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，数据可以用来创造新的知识。例如，在商业领域，数据可以用来识别新的趋势。在医学领域，数据可以用来揭示疾病之间的新关系并开发新的治疗方法。然而，我们的模型只有在所训练数据的基础上才能发挥作用。因此，数据的重要性在未来可能会继续增加。随着数据变得更加易于访问和使用，它将在多个领域变得越来越重要。现在，让我们来看一些常见的瓶颈以及应对这些瓶颈的最佳方法。
- en: Feature selection
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'The first step when we start with any AI/ML problem is to propose a hypothesis:
    what are the input features that can help us in classifying or predicting our
    output? Choosing the right features is essential for any machine learning model,
    but it can be difficult to know which ones to choose. If you include too many
    irrelevant features in your model, your results will be inaccurate. If you include
    too few features, your model may not be able to learn from your data. Thus, feature
    selection is a critical step in machine learning that helps you reduce noise and
    improve the accuracy of your models:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始解决任何AI/ML问题时，第一步是提出假设：哪些输入特征可以帮助我们分类或预测输出？选择正确的特征对任何机器学习模型至关重要，但有时很难知道选择哪些特征。如果你在模型中加入过多无关的特征，结果将不准确。如果加入的特征太少，模型可能无法从数据中学习。因此，特征选择是机器学习中的一个关键步骤，它帮助你减少噪声并提高模型的准确性：
- en: As a rule, before using any feature engineering, one should start with directly
    observed and reported features instead of learned features. Learned features are
    the features generated either by an external system (like a clustering algorithm)
    or by employing a deep model itself. Simplifying can help you achieve a solid
    baseline performance, after which you can experiment with more esoteric strategies.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，在使用任何特征工程之前，应该从直接观察和报告的特征开始，而不是从学习到的特征开始。学习到的特征是通过外部系统（如聚类算法）或通过深度模型本身生成的特征。简化可以帮助你实现一个稳固的基准性能，之后你可以尝试更多复杂的策略。
- en: Remove the features that you are not using. Unused features create technical
    debt. They make your code more difficult to read and maintain and can also lead
    to unexpected bugs and security vulnerabilities. Of course, it can be difficult
    to keep track of which features are being used and which are not. However, do
    not drop the features arbitrarily; perform data analysis and exploration carefully
    – understand the features. A good way to do this would be to assign an owner to
    each feature. The feature owner would be responsible for maintaining the feature
    and documenting its rationale so that the knowledge can be shared across teams.
    This also means that whenever a feature owner leaves the team, ownership is transferred
    to other members. By taking the time to understand and remove unused features,
    you can keep your code clean and avoid accumulating technical debt.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除你未使用的特征。未使用的特征会增加技术债务，它们使代码更难阅读和维护，还可能导致意外的错误和安全漏洞。当然，跟踪哪些特征被使用、哪些没有被使用可能会很困难。然而，不要随意删除特征；要仔细进行数据分析和探索
    – 理解这些特征。一个好的方法是为每个特征指定一个负责人。特征负责人将负责维护该特征并记录其理由，以便知识能够在团队之间共享。这也意味着，每当特征负责人离开团队时，责任将转交给其他成员。通过花时间理解并移除未使用的特征，你可以保持代码清晰，避免积累技术债务。
- en: Often, we think more features equal a better model, but that is far from true.
    Instead of using millions of features you do not understand, it is better to work
    with specific features; you can use the method of regularization to remove the
    features that apply to too few examples.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们常常认为更多的特征意味着更好的模型，但事实远非如此。与其使用你不理解的数百万个特征，不如使用具体的特征；你可以通过正则化方法去除那些仅适用于少数样本的特征。
- en: You can also combine and modify the features to create new features. There are
    a variety of ways you can combine and modify. For example, you can discretize
    continuously valued features into many discrete features. You can also create
    synthetic new features by crossing (multiplying) two or more existing features.
    For example, if you have the features “height” and “weight,” you can create a
    new feature called “BMI” by combining those two features. Feature crossing can
    provide predictive abilities beyond what those features can provide individually.
    Two features that are each somewhat predictive of the desired outcome may be much
    more predictive when combined. This is because the combined feature captures information
    that is not captured by either individual feature. Feature crossing is a powerful
    tool that can help to improve the accuracy of predictive models.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以结合和修改特征来创建新的特征。你可以通过多种方式进行结合和修改。例如，你可以将连续值特征离散化为多个离散特征。你还可以通过交叉（乘法）两个或多个现有特征来创建新的合成特征。例如，如果你有“身高”和“体重”这两个特征，你可以通过将这两个特征结合起来，创建一个新的特征“BMI”。特征交叉能够提供比单个特征更强的预测能力。当两个特征各自对期望的结果具有一定的预测性时，结合后可能会显著提高预测性。这是因为合并后的特征捕捉到了单个特征无法捕捉的信息。特征交叉是一个强大的工具，有助于提高预测模型的准确性。
- en: Features and data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征和数据
- en: One of the problems when we move from learning data science to solving real
    problems is the lack of data. Despite the internet, mobile, and IoT devices generating
    loads of data, getting good-quality labeled data is a big hurdle. The cost of
    annotation is normally as high as it is time-consuming and requires subject matter
    expertise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从学习数据科学转向解决实际问题时，一个问题是缺乏数据。尽管互联网、移动设备和物联网设备生成了大量的数据，但获取高质量的标注数据仍然是一个大难题。标注的成本通常既高又费时，而且需要专业知识。
- en: 'Thus, we need to ensure we have sufficient data to train the model. As a rule
    of thumb, the number of input features (*n*) that a model can learn is roughly
    proportional to the amount of data (*N*) you have (*n* << *N*). A few tips that
    can be followed in such a situation are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要确保有足够的数据来训练模型。根据经验法则，一个模型可以学习的输入特征数量（*n*）大致与你拥有的数据量（*N*）成正比（*n* << *N*）。在这种情况下，以下是一些可以遵循的建议：
- en: Scale model learning to the size of the data. For example, if we have only 1,000
    labeled samples, then use highly human-engineered features. A good number would
    be to have a dozen well-selected features for 1,000 labeled samples. But if we
    have millions of examples, then we can afford to have about a hundred thousand
    features. And assuming we have billions of data samples, we can build a model
    with millions of features.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型学习的规模与数据量相匹配。例如，如果我们只有1,000个标注样本，那么就使用高度人工设计的特征。对于1,000个标注样本，一个好的选择是使用一打精心挑选的特征。但如果我们有数百万个样本，那么我们就可以使用大约十万个特征。如果我们有数十亿的数据样本，那么我们可以建立一个包含数百万个特征的模型。
- en: If we have too much data, we do not arbitrarily drop it; instead, we can use
    **Importance Weight Sampling** ([https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report247.pdf](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report247.pdf)).
    The idea is to assign a weight of importance to each sample based on some distributional
    feature that captures similarity to the specialized domain data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有过多的数据，我们不会随便丢弃它；相反，我们可以使用**重要性加权抽样**([https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report247.pdf](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/reports/final_reports/report247.pdf))。其思想是根据某些分布特征为每个样本分配一个重要性权重，这些特征能够捕捉与专业领域数据的相似性。
- en: 'Another way to deal with a lack of sufficient data is using data augmentation.
    Initially proposed for image data by H. S. Baird in his article, *Document image
    analysis* [7], it has proven to be a good way to increase image data by making
    use of simple image transformations, like horizontal flips, vertical flips, rotation,
    translation, etc. Most deep learning frameworks have data generators, which you
    can use to perform this augmentation on the go, as shown in *Figure 18.2*:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种应对数据不足的方法是使用数据增强。最初由 H. S. Baird 在他的文章《文档图像分析》中提出[7]，它已被证明是通过简单的图像变换（如水平翻转、垂直翻转、旋转、平移等）增加图像数据的好方法。大多数深度学习框架都提供了数据生成器，可以在运行时使用这些生成器进行数据增强，如
    *图 18.2* 所示：
- en: '![Background pattern  Description automatically generated](img/B18331_18_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![背景图案 描述自动生成](img/B18331_18_02.png)'
- en: 'Figure 18.2: Original and augmented images'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2：原始图像与增强图像
- en: While augmenting image data is readily available in all the major deep learning
    frameworks, augmenting textual data and audio data is not that straightforward.
    Next, we present some of the techniques you can use to augment textual and speech
    data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然增强图像数据在所有主要的深度学习框架中都有现成的工具，但增强文本数据和音频数据并不那么简单。接下来，我们将介绍一些可以用来增强文本和语音数据的技术。
- en: Augmenting textual data
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强文本数据
- en: 'Some of the simple ways that we can use to augment textual data are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来增强文本数据的一些简单方法包括：
- en: '**Synonym replacement**: In this, random words from the sentence are chosen
    and replaced by their synonyms using WordNet. For example, if we have the sentence
    “This book **focuses** on deep learning using TensorFlow and Keras and is meant
    for both **novices** and experts,” we can choose the two words in bold for synonym
    replacement, resulting in this sentence: “This book **centers** on deep learning
    using TensorFlow and Keras and is meant for both **beginners** and experts.”'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词替换**：在这种方法中，随机选择句子中的词语，并使用 WordNet 将它们替换为同义词。例如，如果我们有句子“这本书**专注**于使用 TensorFlow
    和 Keras 的深度学习，适合**新手**和专家”，我们可以选择两个加粗的词进行同义词替换，得到以下句子：“这本书**聚焦**于使用 TensorFlow
    和 Keras 的深度学习，适合**初学者**和专家。”'
- en: '**Back translation**: The method was proposed by Sennrich et al. in 2016\.
    The basic idea is that a sentence is translated into another language and then
    translated back to the original language. We can use language translation APIs
    or Python modules like `googletrans`. The following code snippet translates a
    sentence from English to German and back. For the code to work, we need to have
    `googletrans` installed:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向翻译**：该方法由 Sennrich 等人在 2016 年提出。其基本思想是将一句话翻译成另一种语言，然后再翻译回原语言。我们可以使用语言翻译
    API 或 Python 模块，如 `googletrans`。以下代码片段将一句话从英语翻译成德语并再翻译回英语。为了使代码能够工作，我们需要安装 `googletrans`：'
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we have two sentences “I am going to the market” and “I walk to the market”
    belonging to the same class. *Figure 18.3* details the process of data augmentation
    using back translation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个句子：“I am going to the market”和“I walk to the market”，它们属于同一类别。*图 18.3*
    详细说明了使用反向翻译进行数据增强的过程：
- en: '![Diagram  Description automatically generated](img/B18331_18_03.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18331_18_03.png)'
- en: 'Figure 18.3: Data augmentation using back translation'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3：使用反向翻译的数据增强
- en: In the review paper *A survey of Data Augmentation Approaches for NLP*, the
    authors provide an extensive list of many other augmentation methods. This paper
    provides an in-depth analysis of data augmentation for NLP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在综述论文《NLP 数据增强方法综述》中，作者列出了许多其他增强方法。这篇论文对 NLP 数据增强进行了深入分析。
- en: In recent years, with the success of large language models and transformers,
    people have experimented with using them for the task of data augmentation. In
    the paper entitled *Data augmentation using pre-trained transformers*, by the
    Amazon Alexa AI team, the authors demonstrate how by using only 10 training samples
    per class they can generate synthetic data using the pretrained transformers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着大规模语言模型和变换器的成功，研究人员尝试将它们应用于数据增强任务。在亚马逊 Alexa AI 团队的论文《使用预训练变换器进行数据增强》中，作者展示了如何仅使用每个类别
    10 个训练样本，通过预训练变换器生成合成数据。
- en: 'They experimented with three different pretrained models: an autoencoder LM
    BERT, an autoregressive LM GPT2, and the pretrained `seq2seq` model BART. *Figure
    18.4* shows their algorithm for generating synthetic data using pretrained models:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 他们实验了三种不同的预训练模型：自动编码器 LM BERT、自动回归 LM GPT2，以及预训练的`seq2seq`模型 BART。*图18.4*展示了他们使用预训练模型生成合成数据的算法：
- en: '![Diagram  Description automatically generated](img/B18331_18_04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图示说明自动生成](img/B18331_18_04.png)'
- en: 'Figure 18.4: Algorithm for generating synthetic textual data using pretrained
    transformers'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4：使用预训练变换器生成合成文本数据的算法
- en: 'Speech data can also be augmented using techniques like:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 语音数据也可以通过以下技术进行增强：
- en: '**Time warping**: Here a random point is selected and data is warped to either
    left or right with a distance *w*. The distance *w* is not fixed, and instead
    is chosen from a uniform distribution [0, *W*].'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间扭曲**：这里随机选择一个点，并将数据向左或向右扭曲，距离为*w*。距离*w*不是固定的，而是从均匀分布[0, *W*]中选择的。'
- en: '**Frequency Masking**: Here a range of frequency channels [f[0], f[0]+f)] are
    masked; the choice of frequency *f*[0] and *f* depends upon the number of frequency
    channels and frequency mask parameter *F*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率掩码**：在这里，一范围频率通道[f[0], f[0]+f)]会被掩码；频率*f*[0]和*f*的选择取决于频率通道的数量和频率掩码参数*F*。'
- en: '**Time Masking**: In this case, the consecutive time steps are masked.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间掩码**：在这种情况下，连续的时间步长会被掩码。'
- en: 'These techniques were proposed by the Google Team in 2019, in their paper *SpecAugment:
    A simple data augmentation method for Automatic Speech Recognition*.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术由谷歌团队在2019年提出，发表于他们的论文《SpecAugment：一种用于自动语音识别的简单数据增强方法》中。
- en: Model best practices
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型最佳实践
- en: Model accuracy and performance are critical to success for any machine learning
    and deep learning project. If a model is not accurate enough, the associated business
    use case will not be successful. Therefore, it is important to focus on model
    accuracy and performance to increase the chances of success. There are a number
    of factors that impact model accuracy and performance, so it is important to understand
    all of them in order to optimize accuracy and performance. Below we list some
    of the model best practices that can help us leverage best from our model development
    workflow.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确性和性能对任何机器学习和深度学习项目的成功至关重要。如果一个模型的准确性不足，相关的业务用例将无法成功。因此，关注模型的准确性和性能以提高成功的机会是非常重要的。影响模型准确性和性能的因素很多，因此理解所有这些因素对于优化准确性和性能至关重要。以下列出了一些模型最佳实践，可以帮助我们充分利用模型开发流程。
- en: Baseline models
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准模型
- en: A baseline model is a tool used in machine learning to evaluate other models.
    It is usually the simplest possible model, and acts as a comparison point for
    more complex models. The goal is to see if the more complex models are actually
    providing any improvements over the baseline model. If not, then there is no point
    in using the more complex model. Baseline models can also be used to help detect
    data leakage. Data leakage occurs when information from the test set bleeds into
    the training set, resulting in overfitting. By comparing the performance of the
    baseline model to other models, it is possible to detect when data leakage has
    occurred. Baseline models are an essential part of machine learning and provide
    a valuable perspective on the performance of more complex models. Thus, whenever
    we start working on a new problem, it is good to think of the simplest model that
    can fit the data and get a baseline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型是机器学习中用于评估其他模型的工具。它通常是最简单的模型，作为更复杂模型的比较基准。目标是查看更复杂的模型是否真的比基准模型提供了任何改进。如果没有，那么使用更复杂的模型就没有意义。基准模型还可以帮助检测数据泄露。数据泄露是指测试集中的信息渗透到训练集中，导致过拟合。通过将基准模型的表现与其他模型进行比较，可以检测到何时发生了数据泄露。基准模型是机器学习的重要组成部分，为更复杂模型的性能提供了有价值的视角。因此，每当我们开始处理一个新问题时，最好思考能够拟合数据并获得基准的最简单模型。
- en: Once we have built a satisfactory baseline model, we need to carefully review
    it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了一个令人满意的基准模型，我们需要仔细审查它。
- en: Review the initial hypothesis about the dataset and the choice of our initial
    algorithms. For example, maybe when we first began working with the data, we hypothesized
    that the patterns we are observing would be best explained by a **Gaussian Mixture
    Model** (**GMM**). However, after further exploration, we may find that the GMM
    is not able to capture the underlying structure of the data accurately. In that
    case we will need to rethink our strategy. Ultimately, our choice of algorithms
    is dictated by the nature of the data itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾关于数据集的初步假设和初步算法选择。例如，也许当我们第一次开始处理数据时，我们假设观察到的模式最适合用**高斯混合模型**（**GMM**）来解释。然而，在进一步探索后，我们可能会发现
    GMM 并不能准确捕捉数据的潜在结构。在这种情况下，我们需要重新考虑我们的策略。最终，算法的选择由数据本身的性质决定。
- en: Confirm if the model is overfitting or underfitting. If the model is overfitting,
    try more data, reduce model complexity, increase batch size, or include regularization
    methods like **ridge**, **lasso**, or **dropout**. If the model is underfitting,
    try increasing model complexity, adding more features, and training for more epochs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 确认模型是过拟合还是欠拟合。如果模型过拟合，可以尝试增加数据、减少模型复杂度、增加批处理大小，或者引入正则化方法，如**岭回归**、**套索回归**或**dropout**。如果模型欠拟合，可以尝试增加模型复杂度、添加更多特征，并训练更多的周期。
- en: Analyze the model based on its performance metrics. For example, if we have
    made a classification model, analyze its confusion metrics and its precision/recall
    as per the business use case. Identify which class model is not predicting correctly;
    this should give us an insight into the data for those classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型的性能指标分析模型。例如，如果我们构建了一个分类模型，就可以根据业务用例分析其混淆矩阵以及精度/召回率。识别出哪个类别模型预测不准确；这将帮助我们深入了解这些类别的数据。
- en: Perform hyperparameter tuning to get a strong baseline model. It is important
    that we establish a strong baseline model because it serves as a benchmark for
    future model improvements. The baseline should incorporate all the business and
    technical requirements, and test the data engineering and model deployment pipelines.
    By taking the time to develop a strong baseline, we can ensure that our machine
    learning project is on the right track from the start. Furthermore, a good baseline
    can help us identify potential areas of improvement as we iterate on our models.
    As such, it is well worth investing the time and effort to create a robust baseline
    model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 执行超参数调优，以获得一个强有力的基准模型。建立一个强基准模型非常重要，因为它为未来的模型改进提供了基准。基准应包括所有业务和技术需求，并测试数据工程和模型部署管道。花时间开发一个强有力的基准模型，可以确保我们的机器学习项目从一开始就走在正确的轨道上。此外，一个好的基准模型可以帮助我们在模型迭代过程中识别潜在的改进领域。因此，投入时间和精力创建一个强大的基准模型是非常值得的。
- en: Pretrained models, model APIs, and AutoML
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练模型、模型 API 和 AutoML
- en: When we want to launch a commercial product, time and energy are often two of
    the most important factors. When working on a new project, it can be very time-consuming
    to train a baseline model from scratch. However, there are now a number of sources
    where we can find pretrained models that can save us a lot of time and effort.
    These include GitHub, Kaggle, and various cloud-based APIs from companies like
    Amazon, Google, OpenAI, and Microsoft.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要推出商业产品时，时间和精力往往是最重要的两个因素之一。在进行新项目时，从零开始训练一个基准模型可能非常耗时。然而，现在有很多资源可以找到预训练模型，帮助我们节省大量时间和精力。这些资源包括
    GitHub、Kaggle，以及来自 Amazon、Google、OpenAI 和 Microsoft 等公司的各种基于云的 API。
- en: In addition, there are specialized start-ups like Scale AI and Hugging Face
    that offer pretrained models for a variety of different tasks. By taking advantage
    of these resources, we can quickly get our machine learning projects up and running
    without having to spend a lot of time training a model from scratch. So, if our
    problem is a standard classification or regression problem, or we have structured
    tabular data available, we can make use of either pretrained models, or APIs provided
    by companies like Amazon, Google, and Microsoft. Using these approaches can save
    us valuable time and energy and allow us to get started with our project quickly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些专业的初创公司，如 Scale AI 和 Hugging Face，提供适用于各种任务的预训练模型。通过利用这些资源，我们可以快速启动机器学习项目，而无需花费大量时间从头开始训练模型。因此，如果我们的问题是标准的分类或回归问题，或者我们拥有结构化的表格数据，我们可以使用预训练模型或由
    Amazon、Google 和 Microsoft 等公司提供的 API。采用这些方法可以节省宝贵的时间和精力，并帮助我们快速开始项目。
- en: Another solution that is evolving is using **AutoML**, or **Automatic Machine
    Learning**. Using AutoML, we can create custom models that are more tailored to
    a company’s specific needs. If you are limited in terms of organizational knowledge
    and resources, we can still take advantage of machine learning at scale by utilizing
    AutoML. This solution has already been helping companies large and small to meet
    their business goals in a more efficient and accurate manner. In the future, it
    is likely that AutoML will only become more prevalent and popular as awareness
    of its capabilities grows.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个正在发展的解决方案是使用**AutoML**，即**自动机器学习**。通过使用AutoML，我们可以创建更符合公司特定需求的定制模型。如果您的组织知识和资源有限，我们仍然可以通过利用AutoML在大规模上发挥机器学习的优势。这个解决方案已经帮助大小公司更高效、准确地实现其业务目标。未来，随着人们对其能力的认知不断提高，AutoML可能会变得更加普及和受欢迎。
- en: Model evaluation and validation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估与验证
- en: 'In this section, we talk about ways of evaluating our model. Here we are not
    talking about conventional machine learning metrics, but instead focusing on the
    experience of the end user:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了评估模型的方法。这里我们不是在讨论传统的机器学习指标，而是专注于最终用户的体验：
- en: '**User experience techniques**: When our model is near production, we should
    test it further. Crowdsourcing is a great way to get feedback from our audience
    before we release the product. We can either pay people or use live experiments
    on real users in order for them to give valuable opinions about what works best.
    We can create user personas early in a process, that is, create hypothetical users
    – for example, if we are a team with an age group lying between 19–40 and we build
    a recommender system, we can create a user persona for someone in their sixties.
    Later, we can perform usability testing by bringing in actual people and watching
    their reactions to our site.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户体验技巧**：当我们的模型接近生产时，我们应该进一步测试它。众包是一个在发布产品之前从观众那里获得反馈的好方法。我们可以支付给人们，或者使用真实用户进行现场实验，让他们提供关于最有效方案的宝贵意见。我们可以在过程初期创建用户角色，也就是创建假设用户——例如，如果我们是一个年龄介于19至40岁之间的团队，并且我们构建了一个推荐系统，我们可以为一个60多岁的人创建一个用户角色。随后，我们可以通过邀请实际用户并观察他们对我们网站的反应来进行可用性测试。'
- en: '**Use model deltas**: When we’re releasing a new model, one of the best ways
    to measure its success is by calculating how different it is from the one in production.
    For example, if our ranking algorithm has been giving better results than expected
    but not as much so that people would notice, or care, then we should run both
    models on samples through the entire system with weights given by position rank.
    If we find that the difference between the two queries is very small, then we
    know that there will be little change. However, if the difference is large, we
    should ensure that the change is good. In this case we should explore the queries
    where symmetric differences are high; this will enable you to understand the change
    qualitatively.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用模型差异**：当我们发布一个新模型时，衡量其成功的最佳方法之一就是计算它与生产中模型的差异。例如，如果我们的排序算法产生的结果比预期更好，但并不至于让人注意到或关心，那么我们应该让两个模型在整个系统中通过样本运行，并根据位置排名赋予权重。如果我们发现两个查询之间的差异非常小，那么我们知道变化不大。然而，如果差异较大，我们应该确保这种变化是积极的。在这种情况下，我们应当探索那些对称差异较大的查询；这将帮助您定性地理解变化。'
- en: '**Utilitarian power is more important than predictive power**: We may have
    a model with the highest accuracy and best prediction. But that is not the end;
    the question is what we do with that prediction. For example, if we build a model
    to semantically rank the documents, then the quality of the final ranking matters
    more than the prediction. Let us consider another example: let us say you built
    a spam filter, and our model predicts the probability whether the given message
    is spam or ham; we follow it with a cut-off on what text is blocked. In such a
    situation, what we allow to pass through matters most. So, it is possible that
    we get a model with better log loss, but still no improvement in overall performance.
    In such a case, we should look for other features to improve the performance.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功利性效果比预测性效果更重要**：我们可能有一个精度最高、预测最好的模型，但这还不是最终目的；问题是我们如何利用这个预测。例如，如果我们建立一个模型来对文档进行语义排序，那么最终排序的质量比预测本身更为重要。我们再举一个例子：假设你建立了一个垃圾邮件过滤器，模型预测给定信息是垃圾邮件还是正常邮件的概率；接下来我们会设定一个阻止哪些文本通过的截断值。在这种情况下，最重要的是我们允许哪些信息通过。所以，可能我们得到一个具有更好对数损失的模型，但整体性能却没有改善。在这种情况下，我们应该寻找其他特征来提高性能。'
- en: '**Look for patterns in measured errors**: In training samples, check the ones
    the model is not able to predict correctly. Explore features that we have not
    yet considered yet; can they improve the prediction for the incorrect samples?
    Do not be very specific with features; we can add a dozen of them, and let the
    model decide what can be done with them. To visualize errors in classification
    problems, we can use a confusion matrix, and in regression tasks, we can look
    for cases where the loss is high.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**寻找测量误差中的模式**：在训练样本中，检查模型无法正确预测的样本。探索我们尚未考虑的特征；它们能改善对不正确样本的预测吗？不要对特征过于具体；我们可以添加十几个特征，让模型决定如何使用它们。为了可视化分类问题中的错误，我们可以使用混淆矩阵，对于回归任务，我们可以寻找损失较高的情况。'
- en: '**Test on unseen data**: To measure the performance of our model, test it on
    the data that is gathered after the model has been trained; this way we will get
    an estimate of the performance in production. This may result in reduced performance,
    but the reduction should not be severe.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在未见数据上测试**：为了衡量模型的性能，可以在模型训练后收集的数据上进行测试；这样我们可以估算在生产环境中的性能。虽然这样可能会导致性能下降，但下降不应该过于严重。'
- en: Performance monitoring is a crucial part of model development. The performance
    between training and production data can vary drastically, which means that we
    must continuously monitor the behavior of deployed models to make sure they’re
    not doing anything unexpected in our system. We should build a monitoring pipeline
    that continuously monitors performance, quality and skew metrics, fairness metrics,
    model explanations, and user interactions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 性能监控是模型开发中至关重要的一部分。训练数据和生产数据之间的性能可能会有很大差异，这意味着我们必须持续监控已部署模型的行为，以确保它们在系统中没有发生任何异常。我们应该建立一个监控管道，持续监控性能、质量和偏差指标、公平性指标、模型解释以及用户交互。
- en: Model improvements
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型改进
- en: 'Once a reliable model is built and deployed, the work is far from over. The
    model may need to be changed for various reasons, such as data drift or concept
    drift. Data drift occurs when the distribution of data changes over time, and
    concept drift occurs when the properties of dependent (labeled) variables change
    over time. To account for these changes, the model must be retrained on new data
    and updated accordingly. This process can be time-consuming and expensive, but
    it is essential to maintaining a high-performing machine learning model. However,
    before we jump into model improvement, it is important to identify and measure
    the reasons for low performance – **“measure first, optimize second”**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建并部署了一个可靠的模型，工作并没有结束。由于数据漂移或概念漂移，模型可能需要进行更改。数据漂移是指数据的分布随时间变化，而概念漂移是指依赖（标记）变量的属性随时间发生变化。为了应对这些变化，模型必须在新数据上重新训练，并相应地进行更新。这个过程可能既耗时又昂贵，但对于维持高性能的机器学习模型至关重要。然而，在我们开始改进模型之前，识别并衡量低性能的原因非常重要——**“先测量，再优化”**：
- en: '**Data drift**: The performance of a machine learning model can vary depending
    on when it is trained and when it is deployed. This is because the data used during
    training and serving can be different. To avoid this problem, it is important
    to log the features at the time of deployment. This can allow us to monitor the
    variation in serving data (data in production). Once the data drift (the difference
    between training data and serving data) crosses a threshold, we should retrain
    the model with new data. This will ensure that the model is trained on the same
    data that it will be deployed on, and thus improve its performance.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据漂移**：机器学习模型的性能可能会因训练时间和部署时间的不同而有所变化。这是因为在训练和服务过程中使用的数据可能不同。为避免此问题，重要的是在部署时记录特征。这样可以帮助我们监控服务数据（生产数据）的变化。一旦数据漂移（训练数据与服务数据之间的差异）超过阈值，我们应该使用新数据重新训练模型。这将确保模型在与其部署时相同的数据上进行训练，从而提升其性能。'
- en: '**Training-serving skew**: Training-serving skew can be a major problem for
    machine learning models. If there is a discrepancy between how the model is trained
    and how it is used in the real world, this can lead to poor performance and inaccuracies.
    There are three main causes of training-serving skew: a discrepancy between the
    data used in training and serving, a change in the data between training and serving,
    and a feedback loop between the model and the algorithm. For example, if we have
    built a recommender system to recommend movies, we can then retrain the recommender
    later based on the movies users saw from the recommended list. The first two causes
    can be addressed by careful data management, while the third cause requires special
    attention when designing machine learning models.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练与服务偏差**：训练与服务偏差可能是机器学习模型中的一个主要问题。如果模型的训练方式与实际使用方式之间存在差异，可能会导致性能不佳和不准确。训练与服务偏差的主要原因有三：训练数据和服务数据之间的差异、训练与服务之间数据的变化以及模型与算法之间的反馈循环。例如，如果我们构建了一个推荐系统来推荐电影，我们可以根据用户从推荐列表中看到的电影重新训练推荐系统。前两种原因可以通过精确的数据管理来解决，而第三种原因则需要在设计机器学习模型时特别注意。'
- en: It is possible that even after sufficient experimentation, we find that with
    the present features we cannot improve the model performance any further. However,
    to stay in business, continuous growth is necessary. Thus, when we find that our
    model performance has plateaued, it is time to look for new sources for improvements,
    instead of working with the existing features.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 即使经过充分的实验，我们也可能发现，在现有特性下，模型的性能已经无法进一步提升。然而，为了保持业务的持续发展，持续增长是必要的。因此，当我们发现模型性能已趋于平稳时，是时候寻找新的改进来源，而不是继续依赖现有特性。
- en: The software development process is never really “done.” Even after a product
    is launched, there are always going to be new features that could be added or
    existing features that could be improved. The same is true for machine learning
    models. Even after a model is “finished” and deployed to production, there will
    always be new data that can be used to train a better model. And as data changes
    over time, the model will need to be retrained on new data to remain accurate.
    Therefore, it’s important to think of machine learning models as being in a constant
    state of flux. It’s never really “done” until you stop working on it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 软件开发过程从来不是“完成”的。即使产品已经发布，总会有新的特性可以添加，或者现有特性可以改进。机器学习模型也不例外。即使一个模型已经“完成”并部署到生产环境中，依然会有新的数据可以用于训练更好的模型。随着时间的推移，数据会发生变化，模型也需要在新数据上进行重新训练，以保持准确性。因此，重要的是要把机器学习模型看作是处于不断变化的状态。只有当你停止工作时，它才算“完成”。
- en: As we build our model, it is important to think about how easy it is to add
    or remove features. Can we easily create a fresh copy of the pipeline and verify
    its correctness? Is it possible to have two or three copies of the model running
    in parallel? These are all important considerations when building our model. By
    thinking about these things upfront, we can save ourselves a lot of time and effort
    down the line.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，重要的是要考虑添加或删除特征的难易程度。我们能否轻松创建新的管道副本并验证其正确性？是否可以让两个或三个模型副本并行运行？这些都是在构建模型时需要考虑的重要问题。通过提前思考这些问题，我们可以节省大量后续时间和精力。
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we focused on the strategies and rules to follow to get the
    best performance from your models. The list here is not exhaustive, and since
    AI technology is still maturing, in the years to come we may see more rules and
    heuristics emerge. Still, if you follow the advice in the chapter, you will be
    able to move from the alchemical nature of AI models to more reliable, robust,
    and reproducible behavior.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们重点介绍了获取模型最佳性能的策略和规则。这里的列表并非详尽无遗，由于人工智能技术仍在不断发展，未来几年可能会出现更多的规则和启发式方法。不过，如果你遵循本章中的建议，你将能够从人工智能模型的神秘性转向更可靠、稳健和可复现的行为。
- en: In the next chapter, we will explore the TensorFlow ecosystem and see how we
    can integrate all that is covered in this book into practical business applications.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索 TensorFlow 生态系统，并看看如何将本书中涉及的内容整合到实际的商业应用中。
- en: References
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Soni, N., Sharma, E. K., Singh, N., and Kapoor, A. (2020). *Artificial intelligence
    in business: from research and innovation to market deployment*. Procedia Computer
    Science, *167*, 2200–2210.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Soni, N., Sharma, E. K., Singh, N., 和 Kapoor, A. (2020). *人工智能在商业中的应用：从研究和创新到市场部署*。Procedia
    Computer Science, *167*, 2200–2210。
- en: Feng, S. Y., Gangal, V., Wei, J., Chandar, S., Vosoughi, S., Mitamura, T., and
    Hovy, E. (2021). *A survey of data augmentation approaches for NLP*. arXiv preprint
    arXiv:2105.03075.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Feng, S. Y., Gangal, V., Wei, J., Chandar, S., Vosoughi, S., Mitamura, T., 和
    Hovy, E. (2021). *自然语言处理中的数据增强方法综述*。arXiv 预印本 arXiv:2105.03075。
- en: 'Sennrich, R., Haddow, B., and Birch, A. (2016). *Improving Neural Machine Translation
    Models with Monolingual Data*. In Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), pages 86–96,
    Berlin, Germany. Association for Computational Linguistics.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sennrich, R., Haddow, B., 和 Birch, A. (2016). *通过单语数据改进神经机器翻译模型*。在《第54届计算语言学年会论文集（第1卷：长篇论文）》中，页面
    86-96，德国柏林。计算语言学协会。
- en: Kumar, V., Choudhary, A., and Cho, E. (2020). *Data augmentation using pre-trained
    transformer models*. arXiv preprint arXiv:2003.02245.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kumar, V., Choudhary, A., 和 Cho, E. (2020). *使用预训练的变换器模型进行数据增强*。arXiv 预印本 arXiv:2003.02245。
- en: 'Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D., and
    Le, Q. V. (2019). *SpecAugment: A Simple Data Augmentation Method for Automatic
    Speech Recognition*. arXiv preprint arXiv:1904.08779.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D., 和 Le,
    Q. V. (2019). *SpecAugment：一种用于自动语音识别的简单数据增强方法*。arXiv 预印本 arXiv:1904.08779。
- en: 'Rules of Machine Learning: Best practices for ML engineering. Martin Zinkewich.
    [https://developers.google.com/machine-learning/guides/rules-of-ml](https://developers.google.com/machine-learning/guides/rules-of-ml)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习规则：机器学习工程的最佳实践。Martin Zinkewich. [https://developers.google.com/machine-learning/guides/rules-of-ml](https://developers.google.com/machine-learning/guides/rules-of-ml)
- en: 'Baird, H. S. (1995). *Document image analysis*. Chapter: Document Image Defect
    Models, pages 315–325\. IEEE Computer Society Press, Los Alamitos, CA, USA.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Baird, H. S. (1995). *文档图像分析*。章节：文档图像缺陷模型，页码 315–325。IEEE 计算机学会出版社，美国洛杉矶，CA。
- en: Join our book’s Discord space
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的书籍 Discord 社区
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人们交流，并与超过 2000 名成员一起学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
