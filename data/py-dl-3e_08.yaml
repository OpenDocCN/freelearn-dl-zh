- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Exploring Large Language Models in Depth
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探索大型语言模型
- en: In recent years, interest in transformers has skyrocketed in the academic world,
    industry, and even the general public. The state-of-the-art transformer-based
    architectures today are called **large language models** (**LLMs**). The most
    captivating feature is their text-generation capabilities, and the most popular
    example is ChatGPT ([https://chat.openai.com/](https://chat.openai.com/)). But
    in their core lies the humble transformer we introduced in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
    Luckily, we already have a solid foundation of transformers. One remarkable aspect
    of this architecture is that it has changed little in the years since it was introduced.
    Instead, the capabilities of LLMs have grown with their size (the name gives it
    away), lending credibility to the phrase *quantitative change leads to* *qualitative
    change*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，学术界、工业界甚至大众对Transformer的兴趣急剧上升。当前最前沿的基于Transformer的架构被称为**大型语言模型**（**LLM**）。其最吸引人的特点是文本生成能力，而最著名的例子便是ChatGPT（[https://chat.openai.com/](https://chat.openai.com/)）。但在其核心，依旧是我们在[
    *第7章*](B19627_07.xhtml#_idTextAnchor202)中介绍的朴素Transformer。幸运的是，我们已经建立了坚实的Transformer基础。这一架构的一个显著特点是，自从引入以来，它几乎没有发生过太大变化。相反，LLM的能力随着模型规模的增大而增长（这一点从名称上就可以看出来），这也为“*量变导致质变*”这一说法提供了有力的佐证。
- en: The success of LLMs has further fueled the research in the area (or is it the
    other way around?). On the one hand, large industrial labs (such as Google, Meta,
    Microsoft, or OpenAI) invest heavily to push the boundaries for even larger LLMs.
    On the other hand, the agile, open source community finds creative ways to achieve
    a lot with limited resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的成功进一步推动了该领域的研究（或者是反过来？）。一方面，大型工业实验室（如Google、Meta、Microsoft或OpenAI）投入大量资金，以推动更大规模LLM的边界。另一方面，灵活的开源社区也以有限的资源找到创造性的方式，取得了大量进展。
- en: In this chapter, we’ll explore the current LLM landscape from theoretical and
    practical perspectives. We will survey many of the latest LLMs, their properties,
    and their training. Furthermore, we’ll see how to apply them for our purposes
    with the help of the Hugging Face Transformers library.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从理论和实践两个角度探索当前的LLM领域。我们将调查许多最新的LLM，它们的特性以及训练过程。此外，我们还将看到如何借助Hugging
    Face Transformers库将它们应用于我们的目标。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Introducing LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍LLM
- en: LLM architecture
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM架构
- en: Training LLMs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练LLM
- en: Emergent abilities of LLMs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的突现能力
- en: Introducing Hugging Face Transformers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Hugging Face Transformers
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and the
    Hugging Face Transformers library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    If you don’t have an environment with these tools, fret not—the example is available
    as a Jupyter notebook on Google Colab. The code examples are in the book’s GitHub
    repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python、PyTorch和Hugging Face Transformers库（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）实现本章中的示例。如果你没有安装这些工具的环境，也不必担心——示例可以在Google
    Colab的Jupyter notebook中找到。代码示例也在本书的GitHub仓库中： [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08)。
- en: Introducing LLMs
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍LLM
- en: 'In this section, we’ll take a more systematic approach and dive deeper into
    transformer-based architectures. As we mentioned in the introduction, the transformer
    block has changed remarkedly little since its introduction in 2017\. Instead,
    the main advances have come in terms of larger models and larger training sets.
    For example, the original GPT model (GPT-1) has 117M parameters, while GPT-3 (*Language
    Models are Few-Shot Learners*, [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165))
    has 175B, a thousandfold increase. We can distinguish two informal transformer
    model categories based on size:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将采用更系统化的方法，深入探讨基于Transformer的架构。正如我们在介绍中提到的，自2017年引入以来，Transformer模块的变化几乎可以说微乎其微。相反，主要的进展体现在模型规模和训练数据集的增大。例如，原始的GPT模型（GPT-1）有1.17亿个参数，而GPT-3（*语言模型是少样本学习者*，[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)）则有1750亿个参数，增加了1000倍。我们可以根据模型的规模区分两类非正式的Transformer模型：
- en: '**Pre-trained language models** (**PLMs**): Transformers with fewer parameters,
    such as **Bidirectional Encoder Representations from Transformers** (**BERT**)
    and **generative pre-trained transformers** (**GPT**), fall into this category.
    Starting with BERT, these transformers introduced the two-step pre-training/FT
    paradigm. The combination of the attention mechanism and unsupervised pre-training
    (**masked language modeling** (**MLM**) or **next-word prediction** (**NWP**)
    creates effective general-purpose semantic features, which we can use for a number
    of downstream tasks. Because of this, PLMs perform better than other **natural
    language processing** (**NLP**) algorithms, such as **recurrent neural networks**
    (**RNNs**). Combined with their highly parallelizable architecture, this has inspired
    a lot of follow-up work on transformers, which produced improved models and eventually
    led to the next category.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练语言模型**（**PLMs**）：具有较少参数的变换器，例如**双向编码器表示来自变换器**（**BERT**）和**生成式预训练变换器**（**GPT**）属于这一类别。从BERT开始，这些变换器引入了两步预训练/微调（FT）范式。注意力机制和无监督预训练（**掩蔽语言建模**（**MLM**）或**下一个词预测**（**NWP**）的结合创造了有效的通用语义特征，我们可以利用这些特征进行多个下游任务。因此，PLMs比其他**自然语言处理**（**NLP**）算法（如**递归神经网络**（**RNNs**））表现得更好。加上它们高度并行化的架构，这激发了很多基于变换器的后续研究，产生了改进的模型，并最终带来了下一个类别。'
- en: '**LLMs**: These are transformer models with billions of parameters. LLMs differ
    qualitatively from PLMs in the following ways:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMs**：这些是具有数十亿参数的变换器模型。LLMs与PLMs在以下几个方面有质的区别：'
- en: '**Emergent capabilities**: They can solve a series of complex tasks, which
    we will discuss in the *Emergent abilities of* *LLMs* section'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突现能力**：它们能够解决一系列复杂的任务，我们将在*LLMs的突现能力*部分进行讨论'
- en: '**Prompting interface**: LLMs can interact with humans with natural language
    instead of special APIs'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示接口**：LLMs可以通过自然语言与人类互动，而无需特定的API'
- en: '**Fusion of research and engineering**: The scale of LLMs requires researchers
    to have strong engineering skills in large-scale data processing and parallel
    training'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究与工程的融合**：LLM的规模要求研究人员具备强大的大规模数据处理和并行训练的工程能力'
- en: Today, LLMs are almost exclusively decoder-only models because the main applications
    of the current LLMs revolve around text generation (for example, chatbots such
    as ChatGPT). This has happened at the expense of encoder-only and encoder-decoder
    architectures. To better understand why, let’s see how a chatbot works. It starts
    with a user-generated message (known as a **prompt**). A prompt is the initial
    input sequence to the decoder-based model, which generates a response one token
    at a time. The response is added back to the input sequence. A special token separates
    the prompts and the responses. Once the LLM generates a response, the user may
    make another prompt. In this case, we concatenate the new prompt to the existing
    sequence and task the LLM to create a new response based on the extended sequence.
    The LLM has no mechanism for memorizing the existing chat session other than including
    it as part of the input sequence. This process can continue indefinitely. However,
    once it reaches the maximum length of the context window, it will start truncating
    the initial parts of the sequence (we can think of this as a sliding window).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，LLMs几乎完全是解码器-only模型，因为当前LLMs的主要应用集中在文本生成（例如，像ChatGPT这样的聊天机器人）。这种情况是以牺牲编码器-only和编码器-解码器架构为代价的。为了更好地理解为什么会这样，我们来看一下聊天机器人的工作原理。它以用户生成的消息（称为**提示**）开始。提示是解码器模型的初始输入序列，该模型每次生成一个标记作为响应。响应会被添加回输入序列。一个特殊的标记将提示和响应分开。一旦LLM生成了响应，用户可以再发出一个新的提示。在这种情况下，我们将新提示与现有序列连接，并要求LLM基于扩展的序列创建新的响应。LLM没有机制去记住现有的聊天会话，除了将其作为输入序列的一部分。这一过程可以无限继续。然而，一旦达到上下文窗口的最大长度，它将开始截断序列的最初部分（我们可以把它看作一个滑动窗口）。
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Parts of this chapter are based on the paper *A Survey of Large Language Models*
    ([https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223)). We’ll
    refer to it simply as *the survey*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章部分内容来源于论文《大规模语言模型概览》([https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223))。我们将简单地称之为*概览*。
- en: LLM architecture
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM架构
- en: In [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202), we introduced the **multi-head
    attention** (**MHA**) mechanism and the three major transformer variants—encoder-decoder,
    encoder-only, and decoder-only (we used BERT and GPT as prototypical encoder and
    decoder models). In this section, we’ll discuss various bits and pieces of the
    LLM architecture. Let’s start by focusing our attention (yes—it’s the same old
    joke) on the attention mechanism.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B19627_07.xhtml#_idTextAnchor202)中，我们介绍了**多头注意力**（**MHA**）机制以及三种主要的
    Transformer 变体——编码器-解码器、仅编码器和仅解码器（我们使用 BERT 和 GPT 作为典型的编码器和解码器模型）。在本节中，我们将讨论 LLM
    架构的各个方面。让我们首先集中注意力（是的——这又是那个老笑话）在注意力机制上。
- en: LLM attention variants
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 注意力变体
- en: 'The attention we discussed so far is known as **global attention**. The following
    diagram displays the **connectivity matrix** of a bidirectional global self-attention
    mechanism (context window with size *n=8*):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的注意力机制被称为**全局注意力**。下图展示了双向全局自注意力机制的**连接矩阵**（上下文窗口大小为 *n=8*）：
- en: '![Figure 8.1 – Global self-attention with a context window with size n=8](img/B19627_08_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 带有上下文窗口大小为 n=8 的全局自注意力](img/B19627_08_1.jpg)'
- en: Figure 8.1 – Global self-attention with a context window with size n=8
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 带有上下文窗口大小为 n=8 的全局自注意力
- en: Each row and column represent the full input token sequence, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/737.png).
    The dotted colored diagonal cells represent the current input token (query), ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/738.png).
    The uninterrupted colored cells of each column represent all tokens (keys) that
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/338.png)
    can attend to. For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/740.png)
    attends to all preceding tokens, [t 1…t 4],
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行和每一列代表完整的输入令牌序列，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/737.png)。虚线彩色对角单元格表示当前输入令牌（查询），![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/738.png)。每列中未中断的彩色单元格表示所有令牌（键），这些令牌是![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/338.png)可以关注的对象。例如，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/740.png)关注所有前面的令牌，[t
    1…t 4]，
- en: and all succeeding tokens, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/741.png).
    The term *global* implies that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/742.png)
    attends to all tokens. Hence all cells are colored. As we’ll see in the *Sparse
    attention* section, there are attention variants where not all tokens participate.
    We’ll denote these tokens with transparent cells. The diagram depicts bidirectional
    self-attention, as the query can attend to both preceding (down) and succeeding
    (up) elements. The query will only attend to the elements below the current input
    token in the unidirectional case. For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/740.png)
    will only attend to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/744.png).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以及所有后续的标记，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/741.png)。术语*全局*意味着![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/742.png)会关注所有的标记。因此，所有的单元格都会被着色。正如我们将在*稀疏注意力*部分看到的，存在一些注意力的变体，并非所有标记都会参与其中。我们将用透明单元格来表示这些标记。该图展示了双向自注意力机制，因为查询可以同时关注前面的（下方）和后面的（上方）元素。在单向情况下，查询只会关注当前输入标记下方的元素。例如，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/740.png)只会关注![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/744.png)。
- en: As we’ll see, one of the main challenges of the attention mechanism is its time
    and space complexity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，注意力机制的一个主要挑战是其时间和空间复杂度。
- en: Attention complexity
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力复杂度
- en: Despite its advantages, the attention mechanism (particularly global attention)
    has some drawbacks. One of them is that space and time complexity increase quadratically
    with the increase of the context window. That’s because the mechanism is implemented
    with the help of matrices and matrix multiplication.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力机制（特别是全局注意力）有其优点，但也存在一些缺点。其中之一是随着上下文窗口的增大，空间和时间复杂度会呈二次方增长。这是因为该机制是通过矩阵和矩阵乘法实现的。
- en: Matrix multiplication time complexity
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的时间复杂度
- en: The time complexity of the multiplication of two *n×n* matrices is ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/745.png)
    because the classic implementation uses three nested loops. In practice, the algorithm
    is optimized and is less complex. For the purposes of this section, we’ll use
    the complexity of the classic implementation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两个*n×n*矩阵相乘的时间复杂度是![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/745.png)，因为经典实现使用了三重嵌套循环。在实践中，该算法经过优化，复杂度较低。本节目的是使用经典实现的复杂度。
- en: For example, a context window with size *n=4* results in *n×n=4x4* **Q** and
    **V** matrices with 16 total cells each. But a context window of *n=8* results
    in *n×n=8x8* **Q** and **V** matrices with 64 total cells each. Therefore, a two-times-larger
    context window requires four times more memory. Since the time complexity of matrix
    multiplication is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/746.png),
    increasing the context window from *n=4* to *n=8* would increase the number of
    operations from ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:math>](img/747.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:math>](img/748.png).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，大小为*n=4*的上下文窗口会产生*n×n=4x4* **Q** 和 **V** 矩阵，每个矩阵有16个单元格。但是，大小为*n=8*的上下文窗口会产生*n×n=8x8*
    **Q** 和 **V** 矩阵，每个矩阵有64个单元格。因此，两倍大的上下文窗口需要四倍的内存。由于矩阵乘法的时间复杂度是![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/746.png)，将上下文窗口从*n=4*增加到*n=8*会将操作数从![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:math>](img/747.png)增加到![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:math>](img/748.png)。
- en: Next, let’s focus on the transformer block, where we have a **feed-forward network**
    (**FFN**),
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们专注于变压器块，其中包括一个**前馈网络**（**FFN**），
- en: 'multi-head self-attention, and four linear projections (**fully connected**
    (**FC**) layers)—three for the **Q**/**K**/**V** pre-attention split and one that
    combines the attention heads’ outputs. We’ll discuss each component’s relative
    weight in the block’s computational load. Let’s denote the embedding size with
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png),
    the key dimension with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png),
    the value dimension with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/695.png)
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:math>](img/752.png)),
    the context window size with *n*, the number of heads with *h*, and the size of
    the hidden layer in the FFN with *ffn* (the usual convention is *ffn=4*d*). The
    time complexity of the different components is shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力机制和四个线性投影（**全连接**（**FC**）层）——三个用于**Q**/**K**/**V**预注意力分离，一个用于合并注意力头的输出。我们将讨论每个组件在模块计算负载中的相对权重。我们用![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png)表示嵌入大小，用![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png)表示关键维度，用![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/695.png)表示值维度（![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:math>](img/752.png))，用上下文窗口大小*n*，头的数量*h*，以及FFN中隐藏层的大小*ffn*（通常约定为*ffn=4*d*）表示。不同组件的时间复杂度如下所示：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/753.png):
    The three input linear projections for all heads'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/753.png)：三个输入线性投影用于所有头'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/754.png):
    The *h* self-attention heads'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/754.png):
    *h* 自注意力头'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/755.png):
    The fourth output linear projection after the self-attention heads'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/755.png):
    第四个自注意力头后的输出线性投影'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>8</mml:mn><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/756.png):
    The FFN module'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>8</mml:mn><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/756.png):
    FFN 模块'
- en: The full combined complexity of the block is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/757.png).
    We can see that it depends on the ratio between the length of the context window,
    *n*, and the embedding size, *d*. If *d>>n*, then the computational time of the
    linear projections will overshadow the time of the attention heads and vice versa.
    In practice, *d>>n* is the most common scenario. But in either case, the attention
    mechanism has at least quadratic space and time complexity. Let’s see some solutions
    to this challenge.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该块的完整综合复杂度为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/757.png)。我们可以看到它依赖于上下文窗口长度
    *n* 与嵌入大小 *d* 之间的比例。如果 *d>>n*，则线性投影的计算时间将超过注意力头的时间，反之亦然。在实际应用中，*d>>n* 是最常见的情况。但无论如何，注意力机制至少具有二次空间和时间复杂度。我们来看看一些应对这一挑战的解决方案。
- en: Multi-query and grouped-query attention
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多查询和分组查询注意力
- en: 'MHA branches the input data to multiple heads using three linear projections
    per head. The following diagram shows two optimizations of this configuration:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MHA 将输入数据通过每个头的三个线性投影分支到多个头。下图展示了该配置的两种优化：
- en: '![Figure 8.2 – Left: MHA; center: multi-query attention (MQA); right: grouped-query
    attention (GQA) (inspired by https://arxiv.org/abs/2305.13245)](img/B19627_08_2.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 左侧：MHA；中间：多查询注意力（MQA）；右侧：分组查询注意力（GQA）（灵感来源于 https://arxiv.org/abs/2305.13245)](img/B19627_08_2.jpg)'
- en: 'Figure 8.2 – Left: MHA; center: multi-query attention (MQA); right: grouped-query
    attention (GQA) (inspired by [https://arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245))'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 左侧：MHA；中间：多查询注意力（MQA）；右侧：分组查询注意力（GQA）（灵感来源于 [https://arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245)）
- en: 'Let’s discuss them (apart from MHA, which we introduced in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202)):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来讨论它们（除了我们在 [*第 7 章*](B19627_07.xhtml#_idTextAnchor202) 中介绍的 MHA）。
- en: '**MQA** (*Fast transformer decoding: One write-head is all you need*, [https://arxiv.org/abs/1911.02150):](https://arxiv.org/abs/1911.02150):)
    The different heads share key and value projections, as opposed to unique projections
    in MHA. Since the input sequence is the same as well, all heads share the same
    key-value store and only differ in their queries. This optimization reduces both
    the memory and computational requirements, with little performance penalty.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MQA**（*快速转换器解码：只需一个写头*，[https://arxiv.org/abs/1911.02150](https://arxiv.org/abs/1911.02150)）：不同的头共享键和值投影，而不是
    MHA 中的独立投影。由于输入序列相同，所有头共享相同的键值存储，仅在查询上有所不同。这个优化减少了内存和计算需求，且几乎没有性能损失。'
- en: '**GQA** (*GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
    Checkpoints*, [https://arxiv.org/abs/2305.13245):](https://arxiv.org/abs/2305.13245):)
    A hybrid between MHA and MQA, which shares single key and value heads for a *subgroup*
    of query heads. The authors show that GQA is almost as fast as MQA and achieves
    quality close to MHA.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GQA**（*GQA：从多头检查点训练通用多查询转换器模型*，[https://arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245)）：MHA
    和 MQA 的混合体，为一组查询头共享单一的键和值头。作者显示，GQA 的速度几乎与 MQA 一致，且质量接近 MHA。'
- en: In the next section, we’ll discuss attention optimization, which takes into
    account the specifics of GPU memory management.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论注意力优化，它考虑了 GPU 内存管理的具体细节。
- en: FlashAttention
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FlashAttention
- en: 'In this section, we’ll introduce FlashAttention (*FlashAttention: Fast and
    Memory-Efficient Exact Attention with IO-Awareness*, [https://arxiv.org/abs/2205.14135;](https://arxiv.org/abs/2205.14135;)
    *FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning*,
    [https://arxiv.org/abs/2307.08691](https://arxiv.org/abs/2307.08691)). This is
    not a new attention mechanism but an implementation of global attention, which
    considers the specifics of the GPU hardware. A GPU has a large number of computational
    cores that can perform relatively simple but highly parallelizable operations
    (such as matrix multiplication). It has two memory levels: small but fast cache
    (L1 and L2) and large but relatively slow **high bandwidth memory** (**HBM**).
    To perform an operation, it transfers the necessary data from the HBM to the cache.
    The cores use the cache for their calculations. Once the operation is done, the
    result is stored back in the HBM. The main bottleneck in this pipeline is the
    data transfers rather than the actual computation (the fewer data transfers, the
    better).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍 FlashAttention（*FlashAttention: Fast and Memory-Efficient Exact
    Attention with IO-Awareness*，[https://arxiv.org/abs/2205.14135;](https://arxiv.org/abs/2205.14135;)
    *FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning*，[https://arxiv.org/abs/2307.08691](https://arxiv.org/abs/2307.08691)）。这不是一种新的注意力机制，而是全球注意力的一种实现，考虑了
    GPU 硬件的具体特点。GPU 拥有大量的计算核心，可以执行相对简单但高度可并行化的操作（如矩阵乘法）。它有两级内存：小但快速的缓存（L1 和 L2）和大但相对较慢的
    **高带宽内存** (**HBM**)。为了执行一个操作，它会将必要的数据从 HBM 转移到缓存。计算核心使用缓存进行计算。操作完成后，结果会存储回 HBM。在这个管道中，主要的瓶颈是数据传输，而不是实际的计算（数据传输越少越好）。'
- en: 'Next, let’s focus on the attention block, which has five operations: 1) matrix
    multiplication (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/758.png)),
    2) mask, 3) softmax, 4) dropout, and 5) matrix multiplication (**V**). The standard
    implementation performs the operations sequentially, starting with the first matrix
    multiplication. Once it’s done, it proceeds with the mask, and so on. Each operation
    involves two-way data transfer between the HBM and the cache. These transfers
    are unnecessary because the results of operation *i* are transferred from the
    cache to the HBM just to be sent back from the HBM to the cache for operation
    *i+1*. FlashAttention proposes a special **fused kernel** to solve the inefficiency.
    It splits the **Q**/**K**/**V** matrices into smaller blocks that can fit in the
    cache. Once these blocks are transferred there, the fused kernel performs all
    five operations without intermediate data transfers. Only the final result is
    sent back to the HBM. Splitting the matrices into blocks is possible because matrix
    multiplication is embarrassingly parallel. But the other innovation of FlashAttention
    is the ability to split the softmax operation, which isn’t as trivial (we won’t
    go into details about how it’s implemented). The operation is done once all matrix
    blocks pass through this pipeline.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注注意力模块，它包含五个操作：1）矩阵乘法 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/758.png))，2）掩码，3）softmax，4）dropout，和5）矩阵乘法（**V**）。标准实现顺序地执行这些操作，从第一个矩阵乘法开始。完成后，它会继续进行掩码操作，依此类推。每个操作涉及HBM和缓存之间的双向数据传输。这些传输是多余的，因为操作
    *i* 的结果从缓存传输到HBM后，又需要从HBM返回到缓存进行操作 *i+1*。FlashAttention 提出了一个特殊的 **融合内核** 来解决这种低效问题。它将
    **Q**/**K**/**V** 矩阵拆分成可以适配缓存的小块。一旦这些块被传输到缓存中，融合内核将执行所有五个操作，无需中间的数据传输。只有最终结果被发送回
    HBM。将矩阵拆分成小块是可行的，因为矩阵乘法是显式并行的。但 FlashAttention 的另一个创新是能够拆分 softmax 操作，这并不像矩阵乘法那样简单（我们不会详细介绍它是如何实现的）。当所有矩阵块通过这个管道时，操作就完成了。
- en: Splitting matrix multiplication
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法拆分
- en: 'Let’s say we want to multiply the matrices **A** and **B**. Because of the
    way matrix multiplication works, we can split **B** by column into two matrices,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/759.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/760.png).
    Then, we perform two matrix multiplications on each device: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/761.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/762.png).
    Finally, we concatenate the output of the two operations in a single matrix, equivalent
    to the matrix produced by the original multiplication, **AB**.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要对矩阵**A**和**B**进行乘法运算。由于矩阵乘法的运作方式，我们可以按列将**B**拆分为两个矩阵，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/759.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/760.png)。然后，我们在每个设备上执行两个矩阵乘法运算：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/761.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/762.png)。最后，我们将两个操作的输出合并到一个矩阵中，相当于原始乘法产生的矩阵，**AB**。
- en: In the next section, we’ll discuss solving the performance issue with new attention
    mechanisms.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论通过新的注意力机制解决性能问题。
- en: Sparse attention
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏注意力
- en: '**Sparse attention** is a class of methods where the output vector attends
    to a subset of all key vectors instead of the entire context window. For example,
    if we can attend to four vectors of interest from the entire eight-vector context,
    we could reduce the necessary computations twice.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏注意力**是一类方法，其中输出向量仅关注所有关键向量的一个子集，而不是整个上下文窗口。例如，如果我们可以从八个上下文向量中选择四个感兴趣的向量进行关注，那么我们可以将所需的计算量减少一半。'
- en: 'The following diagram displays three bidirectional sparse attention mechanisms:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了三种双向稀疏注意力机制：
- en: '![Figure 8.3 – Left: local attention; center: dilated local attention; right:
    random attention; context window size n=12](img/B19627_08_3.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 左：局部注意力；中：膨胀局部注意力；右：随机注意力；上下文窗口大小 n=12](img/B19627_08_3.jpg)'
- en: 'Figure 8.3 – Left: local attention; center: dilated local attention; right:
    random attention; context window size n=12'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 左：局部注意力；中：膨胀局部注意力；右：随机注意力；上下文窗口大小 n=12
- en: The mechanisms follow the same notation as the ones in *Figure 8**.2*, with
    one addition—the transparent cells represent tokens (keys), which the query doesn’t
    attend to.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制与*图 8.2*中的符号相同，唯一的区别是——透明单元格代表令牌（关键字），查询不会关注这些单元格。
- en: On the left, we have bidirectional **local attention** (or **sliding window
    attention**), first introduced in *Image Transformer*, [https://arxiv.org/abs/1802.05751](https://arxiv.org/abs/1802.05751)).
    The query attends to a limited context window of the nearest *w* keys around the
    current token (½*w* to the left and ½*w* to the right). The self-attention block
    still takes the full *n*-sized sequence as input, but each token attends to a
    limited *w*-sized local context. This way, the memory footprint is the same as
    global attention, but the time complexity is reduced to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/763.png)
    instead of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/764.png).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们有双向**局部注意力**（或**滑动窗口注意力**），这是在*图像Transformer*中首次提出的，[https://arxiv.org/abs/1802.05751](https://arxiv.org/abs/1802.05751)。查询仅关注当前标记周围最近的*w*个键的有限上下文窗口（左边½*w*，右边½*w*）。自注意力模块仍然将整个*n*大小的序列作为输入，但每个标记只关注有限的*w*大小的局部上下文。这样，内存占用与全局注意力相同，但时间复杂度减少为![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/763.png)，而不是![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/764.png)。
- en: To understand why local attention works, let’s return to **convolutional neural
    networks** (**CNNs**). Recall that the earlier layers of a CNN have small receptive
    fields and capture smaller, simpler features. Conversely, the deeper CNN layers
    have large receptive fields that capture larger and more complex features. We
    can apply the same principle to transformers. Research has shown that the initial
    transformer blocks learn simple token features and local syntax, while the deeper
    layers learn more complex context-dependent aspects of token semantics. Because
    of this, we can apply local attention to the earlier transformer blocks and reserve
    global attention for the deeper ones without sacrificing performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解局部注意力为何有效，我们回顾一下**卷积神经网络**（**CNN**）。回想一下，CNN的早期层具有较小的感受野，并捕获较小、更简单的特征。相反，CNN的更深层具有较大的感受野，能够捕获更大和更复杂的特征。我们可以将相同的原则应用于Transformer。研究表明，初始的Transformer模块学习简单的标记特征和局部语法，而更深层则学习标记语义中更复杂的上下文相关特征。因此，我们可以将局部注意力应用于较浅的Transformer模块，而将全局注意力保留给更深的模块，而不会牺牲性能。
- en: '**Dilated attention** (*Figure 8**.3*, center) is a modification of local attention,
    which works in a similar way to the dilated convolutions we introduced in [*Chapter
    4*](B19627_04.xhtml#_idTextAnchor107). Unlike local attention, here, the context
    window is not continuous. Instead, there is a gap of *g* cells (which could be
    more than one) between each context token. This makes it possible to attend to
    a wider context with the same *n* of computations.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展注意力**（*图 8.3*，中间）是局部注意力的一种修改，工作原理类似于我们在[*第4章*](B19627_04.xhtml#_idTextAnchor107)中介绍的扩展卷积。与局部注意力不同，这里上下文窗口不是连续的。相反，每个上下文标记之间有一个*间隔*（可以是多个单元格）。这使得在相同的计算*数量*下，可以关注更广泛的上下文。'
- en: Next, we have bidirectional **random attention** (*Figure 8**.3*, right), where
    the current query (token) attends to a subset of *r* keys (tokens) from the full
    context window. The time complexity is reduced to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/765.png)
    instead of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/766.png).
    The attention pattern can be viewed as a directed graph. In the case of random
    attention, this graph is also random. That is, the information can flow rapidly
    between any pair of nodes without considering the actual structure of the data,
    which might be biased.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有双向的 **随机注意力** (*图8.3*，右)，其中当前查询（标记）会关注来自完整上下文窗口的 *r* 个键（标记）的子集。时间复杂度减少为
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/765.png)
    而不是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/766.png)。注意力模式可以视为一个有向图。在随机注意力的情况下，这个图也是随机的。也就是说，信息可以在任何一对节点之间迅速流动，而不考虑数据的实际结构，这可能会带有偏见。
- en: 'It is also possible to combine global and local attention. One such example
    is **Longformer** (*Longformer: The Long-Document Transformer*, [https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150)),
    displayed in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以结合全局和局部注意力。其中一个例子是 **Longformer** (*Longformer：长文档变换器*，[https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150))，如下图所示：
- en: '![Figure 8.4 – Combined local and global attention; left: Longformer block;
    right: Big Bird block](img/B19627_08_4.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 结合了局部和全局注意力；左：Longformer 块；右：大鸟块](img/B19627_08_4.jpg)'
- en: 'Figure 8.4 – Combined local and global attention; left: Longformer block; right:
    Big Bird block'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 结合了局部和全局注意力；左：Longformer 块；右：大鸟块
- en: It introduces a drop-in replacement self-attention block in an otherwise unmodified
    transformer model. The block represents a combination of global and local (or
    dilated) attention. It applies local attention to most input tokens, but a few
    can use global attention. The left section of *Figure 8**.4* shows the combined
    self-attention block and one example of input tokens that apply local and global
    attention. More specifically, the authors use the Longformer block in a unidirectional
    BERT-style model to solve MLM and `[CLS]` in MLM tasks. As the diagram shows,
    global attention works in both directions. The special token can attend to all
    other tokens, but the other tokens can also attend to the special token in addition
    to their local attention context. In the case of autoregressive language modeling
    (unidirectional model), they apply only dilated local attention, as there are
    no tokens with special significance. The full Longformer model uses dilated attention
    with a larger context window and *g* in the deeper layers, leaving the earlier
    ones with only local attention.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它在一个未修改的变换器模型中引入了一个可以替代的自注意力块。该块表示全局和局部（或扩展）注意力的结合。它对大多数输入标记应用局部注意力，但少数标记可以使用全局注意力。*图8.4*
    的左侧显示了结合的自注意力块和一个应用局部和全局注意力的输入标记示例。更具体地说，作者在一个单向 BERT 风格的模型中使用 Longformer 块来解决
    MLM 和 `[CLS]` 在 MLM 任务中的问题。如图所示，全局注意力在两个方向上都有效。特殊标记可以关注所有其他标记，但其他标记除了其局部注意力上下文之外，还可以关注特殊标记。在自回归语言建模（单向模型）的情况下，他们仅应用扩展的局部注意力，因为没有具有特殊意义的标记。完整的
    Longformer 模型在较深的层使用扩展注意力和更大的上下文窗口，而较早的层仅使用局部注意力。
- en: '**Big Bird** (*Figure 8**.4*, right; *Big Bird: Transformers for Longer Sequences*,
    [https://arxiv.org/abs/2007.14062](https://arxiv.org/abs/2007.14062)) is similar
    to Longformer but adds random attention.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**大鸟** (*图8.4*，右；*大鸟：用于长序列的变换器*，[https://arxiv.org/abs/2007.14062](https://arxiv.org/abs/2007.14062))
    类似于 Longformer，但添加了随机注意力。'
- en: 'Next, let’s discuss the **sparse transformer** attention developed by OpenAI
    (*Generating Long Sequences with Sparse Transformers*, [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)).
    A sparse transformer introduces unidirectional strided and fixed attention schemes,
    displayed in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论由 OpenAI 开发的 **稀疏变换器** 注意力机制（*生成长序列的稀疏变换器*，[https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)）。稀疏变换器引入了单向步长和固定注意力机制，如下图所示：
- en: '![Figure 8.5 – Left: strided sparse attention with l=4; right: fixed sparse
    attention; input image size 4×4; sequence length n=12 (inspired by https://arxiv.org/abs/1904.10509)](img/B19627_08_5.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 左：步长稀疏注意力，l=4；右：固定稀疏注意力；输入图像大小 4×4；序列长度 n=12（灵感来自 https://arxiv.org/abs/1904.10509)](img/B19627_08_5.jpg)'
- en: 'Figure 8.5 – Left: strided sparse attention with l=4; right: fixed sparse attention;
    input image size 4×4; sequence length n=12 (inspired by [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 左：步长稀疏注意力，l=4；右：固定稀疏注意力；输入图像大小 4×4；序列长度 n=12（灵感来自 [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)）
- en: To understand how they work, let’s discuss the context of the paper. It proposes
    a unified decoder-only model to generate new images, text, or audio. Depending
    on the use case, the input and output data can be a two-dimensional image tensor
    (we’ll omit the color dimension for simplicity). However, the transformer accepts
    as input a one-dimensional sequence. We can solve this by concatenating the rows
    of the image in a single one-dimensional tensor. Once done, we can treat the image
    like a regular sequence and feed it to the model. *Figure 8**.5* displays a strided
    (left) and fixed attention (right) connectivity matrix for a two-dimensional image
    (top) and its equivalent concatenated one-dimensional sequence (bottom). Let’s
    note that the bottom expanded sequence doesn’t match the dimensions of the top
    image—it should be with length *n=16*, which reflects the 4×4 image, instead of
    *n=12* as it is now. Since this is a generative decoder-only model, it uses unidirectional
    attention, even though the concept of direction doesn’t exist in the same way
    in images as in text.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解它们是如何工作的，我们来讨论一下论文的背景。论文提出了一种统一的仅解码器模型，用于生成新的图像、文本或音频。根据使用场景，输入和输出数据可以是二维图像张量（为简化起见，我们省略颜色维度）。然而，变换器接受的是一维序列作为输入。我们可以通过将图像的行连接成一个一维张量来解决这个问题。完成后，我们可以将图像视为常规序列，并将其输入到模型中。*图
    8.5* 显示了一个二维图像（顶部）及其等效的一维连接序列（底部）的步长（左）和固定注意力（右）连接矩阵。需要注意的是，底部扩展的序列与顶部图像的尺寸不匹配——它的长度应为
    *n=16*，对应 4×4 图像，而不是现在的 *n=12*。由于这是一个生成式解码器模型，它使用单向注意力，尽管图像中的方向性并不像文本中那样明确。
- en: 'Next, let’s discuss the two attention schemes. We’ll start with strided attention,
    where the current token attends to the preceding row and column of the input image.
    These are two separate mechanisms split between different attention heads:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论这两种注意力机制。我们从步长注意力开始，其中当前词元关注输入图像的前一行和列。这是两个分别在不同注意力头之间分开的机制：
- en: '**Row head**: Equivalent to unidirectional local attention, which attends to
    the previous ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mo>≈</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/767.png)
    tokens, where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/768.png)
    is the length of one entire row of the 2D input image. Let’s denote the index
    of the current input token with *i* and the tokens it attends to with *j*. We
    can summarize the row mechanism in the following way:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行头**：等同于单向局部注意力，它关注前一个![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mo>≈</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/767.png)
    词元，其中![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/768.png)是二维输入图像每行的长度。我们可以用
    *i* 表示当前输入词元的索引，用 *j* 表示它关注的词元。我们可以用以下方式总结行机制：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo><</mml:mo><mml:mi>l</mml:mi></mml:math>](img/769.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo><</mml:mo><mml:mi>l</mml:mi></mml:math>](img/769.png)'
- en: '**Column head**: Equivalent to unidirectional dilated attention with a stride
    (gap) of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mo>≈</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/770.png)
    (the same as the row head). Assuming that the input image is square, the column
    head jumps the equivalent of one row (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/768.png))
    and attends to a location representing the previous cell in a virtual column of
    the one-dimensional sequence. We can summarize column strided attention in the
    following way:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列头**：等同于具有步幅（间隔）为![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mo>≈</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/770.png)的单向膨胀注意力（与行头相同）。假设输入图像是正方形的，列头会跳过相当于一行的间隔
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/768.png))，并关注一个位置，表示在一维序列虚拟列中的前一个单元格。我们可以用以下方式总结列跨步注意力：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfenced
    open="(" close=")"><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></mfenced><mtext>mod</mtext><mi>l</mi><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/772.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfenced
    open="(" close=")"><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></mfenced><mtext>mod</mtext><mi>l</mi><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/772.png)'
- en: This scheme performs best for 2D input data, such as images, because the row/column
    split reflects the underlying data structure. The time complexity of this scheme
    is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/773.png).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该方案在二维输入数据上表现最佳，例如图像，因为行/列划分反映了底层数据结构。该方案的时间复杂度为![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/773.png)。
- en: 'Next, we have **fixed attention**, which attends to a fixed column and the
    elements after the latest column element. It performs better on non-periodic data,
    such as text. Once again, this is a combination of two separate mechanisms split
    between different heads:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍**固定注意力**，它关注一个固定的列及其最新列元素之后的元素。对于非周期性数据（如文本），它表现更好。再次强调，这是两种独立机制在不同头之间的组合：
- en: '**Column head**: Attends to a fixed column, which doesn’t necessarily match
    the column of the current input token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/774.png).
    Multiple input tokens attend to the same column, which makes it possible to attend
    to the entire length of the sequence. We can summarize the column mechanism in
    the following way:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列头**：关注于一个固定的列，该列不一定与当前输入标记的列相匹配，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/774.png)。多个输入标记可以关注相同的列，这使得它能够关注整个序列的长度。我们可以用以下方式总结列机制：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>l</mi><mo>−</mo><mi>c</mi><mo>≤</mo><mi>j</mi><mtext>mod</mtext><mi>l</mi><mo>≤</mo><mi>l</mi></mrow></mrow></math>](img/775.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>l</mi><mo>−</mo><mi>c</mi><mo>≤</mo><mi>j</mi><mtext>mod</mtext><mi>l</mi><mo>≤</mo><mi>l</mi></mrow></mrow></math>](img/775.png)'
- en: Here, *c* is a parameter (8, 16, or 32). For example, if *l=64* and *c=16*,
    then all positions greater than 64 can attend to positions 48-64, all positions
    greater than 128 can attend to 112-128, and so on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*c* 是一个参数（8、16 或 32）。例如，如果 *l=64* 且 *c=16*，那么所有大于 64 的位置可以关注 48-64 的位置，所有大于
    128 的位置可以关注 112-128 的位置，以此类推。
- en: '**Row head**: The first head is similar to the row head in strided attention.
    But instead of attending to the length of one entire row, it only attends to the
    location of the current column head. The row head provides local context. We can
    summarize it in the following way:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行头**：第一个头与跨步注意力中的行头类似。但不同的是，它不是关注整个行的长度，而是只关注当前列头的位置。行头提供了局部上下文。我们可以将其总结如下：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/776.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/776.png)'
- en: Here, *floor* rounds down the result of the division to the nearest whole number.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*floor* 将除法结果向下取整到最接近的整数。
- en: Next, let’s focus our attention (I can’t stop myself) on a special case of decoder-only
    architecture and various aspects of LLM architecture.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将注意力集中在一种解码器架构的特殊案例及大语言模型架构的各个方面上（我简直停不下来）。
- en: Prefix decoder
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前缀解码器
- en: 'In this section, we’ll introduce **prefix** (or **non-causal**) **decoder**
    (*Unified Language Model Pre-training for Natural Language Understanding and Generation*,
    [https://arxiv.org/abs/1905.03197](https://arxiv.org/abs/1905.03197)). This is
    a decoder-only model that introduces a new type of attention pattern, displayed
    in the following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将介绍**前缀**（或**非因果**）**解码器**（*统一语言模型预训练用于自然语言理解与生成*，[https://arxiv.org/abs/1905.03197](https://arxiv.org/abs/1905.03197)）。这是一种仅包含解码器的模型，提出了一种新的注意力模式，如下图所示：
- en: '![Figure 8.6 – Prefix decoder self-attention pattern (inspired by https://arxiv.org/abs/1905.03197)](img/B19627_08_6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 前缀解码器自注意力模式（灵感来自 https://arxiv.org/abs/1905.03197）](img/B19627_08_6.png)'
- en: Figure 8.6 – Prefix decoder self-attention pattern (inspired by https://arxiv.org/abs/1905.03197)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 前缀解码器自注意力模式（灵感来自 https://arxiv.org/abs/1905.03197）
- en: We split the input sequence into two segments—![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/777.png)
    through ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math>](img/778.png)
    (**source** or **prefix**), and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/779.png)
    through ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math>](img/780.png)
    (**target**). The tokens of the source segment have bidirectional access to all
    other tokens of that segment. However, the target segment tokens have unidirectional
    access to the preceding tokens of the whole (source and target) input sequence.
    For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/781.png)
    is part of the source segment and can attend to
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入序列分成两个部分——![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/777.png)
    到 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math>](img/778.png)（**源**或**前缀**），以及
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/779.png)
    到 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math>](img/780.png)（**目标**）。源段落的词汇之间可以互相访问。而目标段落的词汇只能单向访问整个（源和目标）输入序列中前面的词汇。例如，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/781.png)
    是源段落的一部分，可以访问
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/782.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/783.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math>](img/784.png).
    Conversely, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:math>](img/785.png)
    is part of the target and can only attend to tokens ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/782.png)
    through ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:math>](img/787.png)
    (but not ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math>](img/788.png)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/782.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/783.png)，和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math>](img/784.png)。相反，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:math>](img/785.png)是目标的一部分，只能处理从![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/782.png)到![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:math>](img/787.png)（但不能处理![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math>](img/788.png)）。'
- en: The prefix decoder is a hybrid between encoder-decoder and decoder models. The
    source segment acts as an encoder, and the target acts as a decoder, yet the underlying
    architecture is decoder-based.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀解码器是编码器-解码器和解码器模型的混合体。源段充当编码器，目标段充当解码器，但其底层架构基于解码器。
- en: 'We can use the prefix decoder for `[SOS]`) and end-of-sequence (`[EOS]`) tokens.
    For example, let’s take the text summarization task. We represent the text sequence
    to summarize (`S1`) and its summarization (`S2`) as a single sequence: `[[SOS],S1,[EOS],S2,[EOS]]`.
    The source sequence, `[[SOS],S1,[EOS]]`, falls within the bidirectional part of
    the attention pattern, and the target sequence, `[S2,[EOS]]`, falls within the
    unidirectional one. We pre-train the model with the help of MLM, where we mask
    random tokens from the full sequence. We fine-tune the model by randomly masking
    some tokens in the target sequence and learning to recover the masked words. Let’s
    note that the `[EOS]` token can also participate in the masking. In this way,
    the model learns when to generate `[EOS]` tokens and terminate the generation
    of the target sequence.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前缀解码器来表示 `[SOS]` 和序列结束（`[EOS]`）标记。例如，我们来看一下文本摘要任务。我们将要总结的文本序列（`S1`）及其摘要（`S2`）表示为一个单一序列：`[[SOS],S1,[EOS],S2,[EOS]]`。源序列
    `[[SOS],S1,[EOS]]` 属于双向注意力模式，而目标序列 `[S2,[EOS]]` 则属于单向注意力模式。我们通过 MLM（Masked Language
    Model）预训练模型，其中我们从完整序列中随机遮蔽一些标记。我们通过随机遮蔽目标序列中的一些标记并学习恢复被遮蔽的词语来微调模型。需要注意的是，`[EOS]`
    标记也可以参与遮蔽。通过这种方式，模型学习何时生成 `[EOS]` 标记，并终止目标序列的生成。
- en: Next, let’s get into more details about various aspects of the LLM architecture.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更详细地了解 LLM 架构的各个方面。
- en: Transformer nuts and bolts
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 的基本构件
- en: 'The following table provides a detailed summary of the main transformer network
    configurations and their variants:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了主要 Transformer 网络配置及其变种的详细总结：
- en: '![Figure 8.7 – Different transformer configurations (source: https://arxiv.org/abs/2303.18223)](img/B19627_08_7.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 不同的 Transformer 配置（来源：https://arxiv.org/abs/2303.18223）](img/B19627_08_7.jpg)'
- en: 'Figure 8.7 – Different transformer configurations (source: https://arxiv.org/abs/2303.18223)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 不同的 Transformer 配置（来源：https://arxiv.org/abs/2303.18223）
- en: 'We’re already familiar with many of these—we introduced the three different
    normalization positions in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202). We
    also introduced two of the three normalization methods in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
    By default, most transformers use **layer normalization** (**LN**). However, some
    models use **RMSNorm** because of its superior training speed and performance.
    Last but not least, **DeepNorm** (*DeepNet: Scaling Transformers to 1,000 Layers*,
    [https://arxiv.org/abs/2203.00555](https://arxiv.org/abs/2203.00555)) is new to
    us. As the paper’s name suggests, this normalization helped build a 1,000-layer
    transformer. The authors argue that in pre-**layer normalization** (**pre-ln**)
    architectures, the gradients at the bottom layers tend to be larger than the ones
    at the top layers, degrading the performance compared to **post-layer normalization**
    (**post-ln**) models. On the other hand, post-ln models are unstable due to exploding
    gradients. To overcome this, they propose a simple yet effective normalization
    of the residual connections:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经熟悉了其中的许多内容——我们在[*第 7 章*](B19627_07.xhtml#_idTextAnchor202)介绍了三种不同的归一化位置。我们还在[*第
    3 章*](B19627_03.xhtml#_idTextAnchor079)介绍了三种归一化方法中的两种。默认情况下，大多数 Transformer 使用
    **层归一化** (**LN**)。然而，一些模型使用 **RMSNorm**，因为它在训练速度和性能上优于 LN。最后但同样重要的是，**DeepNorm**
    (*DeepNet: 扩展 Transformer 至 1000 层*, [https://arxiv.org/abs/2203.00555](https://arxiv.org/abs/2203.00555))
    对我们来说是新的。正如论文标题所示，这种归一化帮助构建了一个 1000 层的 Transformer。作者认为，在**层归一化** (**pre-ln**)
    架构中，底层的梯度往往大于顶部层的梯度，这导致与 **后层归一化** (**post-ln**) 模型相比性能下降。另一方面，后层归一化模型由于梯度爆炸而不稳定。为了克服这一问题，他们提出了一种简单而有效的残差连接归一化方法：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>α</mml:mi><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/789.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>α</mml:mi><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/789.png)'
- en: Here, *α* is a constant applied at the output of the residual connection. Its
    value depends on the transformer type (encoder or decoder) and the model depth
    (number of blocks). The theoretical justification of DeepNorm is that it bounds
    the model update by that constant.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*α* 是应用在残差连接输出处的常数。其值取决于变压器类型（编码器或解码器）和模型深度（块数）。DeepNorm 的理论基础在于通过这个常数限制模型更新。
- en: 'Next, let’s discuss the activation functions. More specifically, we’ll discuss
    the activation function (`ActivationFunc`) of the first layer of the **feed-forward
    network** (**FFN**) sublayer, as this is the only explicit activation in the transformer
    block. As a reminder, we can define the original FFN as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论激活函数。更具体地说，我们将讨论**前馈网络**（**FFN**）子层的第一层激活函数（`ActivationFunc`），因为这是变压器块中唯一显式的激活函数。作为提醒，我们可以定义原始的
    FFN 如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>FFN</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>Activation</mml:mtext><mml:mtext>Func</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/790.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>FFN</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>激活函数</mml:mtext><mml:mtext>Func</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/790.png)'
- en: 'We discussed most activations in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    except for **SwiGLU** and **GeGLU** (*GLU Variants Improve Transformer*, [https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)).
    They are variations of **Gated Linear Unit** (**GLU**, *Language Modeling with
    Gated Convolutional Networks*, [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)),
    which is more of a fusion between layer and activation function rather than pure
    activation. We can define GLU as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第三章*](B19627_03.xhtml#_idTextAnchor079) 中讨论了大多数激活函数，除了 **SwiGLU** 和 **GeGLU**（*GLU
    变体改进变压器*，[https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)）。它们是
    **门控线性单元**（**GLU**）的变体，GLU 更像是层和激活函数的融合，而不是纯粹的激活函数。我们可以定义 GLU 如下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext>GLU</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/791.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext>GLU</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/791.png)'
- en: 'Here, *ActivationFunc* is a specific activation function (Swish for *Swi*GLU
    and *Ge*LU for *Ge*GLU), ⊗ is the element-wise product of two vectors, and **W**
    and **V** are weight matrices, which represent linear projections (that is, FC
    layers). GLU introduces an additional linear projection, **V**, parallel to the
    original path of the network, **W**. Thanks to the element-wise product, the path
    with activation, **W**, acts as a gate to the signal coming from the **V** path.
    This is like **Long Short-Term Memory** (**LSTM**) gates. We can now define the
    FFN with GLU activation:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*ActivationFunc* 是一个特定的激活函数（*Swi*GLU 对应 *Swish*，*Ge*GLU 对应 *Ge*LU），⊗ 表示两个向量的逐元素乘积，**W**
    和 **V** 是权重矩阵，表示线性投影（即，全连接层）。GLU 引入了一个额外的线性投影 **V**，与原始网络路径 **W** 并行。由于逐元素乘积，带有激活的路径
    **W** 作为来自 **V** 路径信号的门控。这类似于 **长短时记忆**（**LSTM**）门。我们现在可以定义带 GLU 激活的前馈网络（FFN）：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mtext>FFN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Activation</mml:mtext><mml:mtext>GLU</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:mi
    mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/792.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mtext>FFN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Activation</mml:mtext><mml:mtext>GLU</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:mi
    mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/792.png)'
- en: 'Let’s note that the authors have excluded the bias from the modified FFN. This
    is also a good place to mention that different LLMs have different bias configurations,
    listed next:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们注意到，作者已经从修改后的FFN中排除了偏置。这也是提及不同LLMs具有不同偏置配置的一个好地方，下面列出：
- en: Use bias in both the linear projections and the attention blocks themselves
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线性投影和注意力块本身都使用偏置。
- en: Use bias in the linear projections but not in the attention blocks
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线性投影中使用偏置，但在注意力块中不使用。
- en: Don’t use bias in either the linear projections or the attention blocks
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要在线性投影或注意力块中使用偏置。
- en: According to some experiments, the lack of biases stabilizes the training.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一些实验，缺乏偏置可以稳定训练。
- en: Next, let’s focus on the various types of positional embeddings we haven’t mentioned
    so far. Unfortunately (or fortunately), discussing them in detail goes beyond
    the scope of this book. But the important thing to remember is that we have either
    absolute (static) or relative (dynamic) positional encodings. In the first case,
    we modify the input token embedding vectors. In the second case, we modify the
    **K**/**V** attention matrices relative to their position of the current input
    token.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们专注于迄今为止未提及的各种类型的位置嵌入。不幸的是（或者幸运的是），详细讨论它们超出了本书的范围。但要记住的重要一点是，我们有绝对（静态）或相对（动态）位置编码。在第一种情况下，我们修改输入令牌嵌入向量。在第二种情况下，我们修改与当前输入令牌位置相关的**K**/**V**注意力矩阵。
- en: The survey summarizes the suggestions from existing literature for detailed
    transformer configuration. For stronger generalization, it suggests using pre-RMSNorm
    normalization, and SwiGLU or GeGLU activation functions. In addition, using LN
    immediately after embedding layers is likely to incur performance degradation.
    As for position embeddings, **Rotary Positional Embedding** (**RoPE**) or **Attention
    with Linear Biases** (**AliBi**) perform better on long sequences than other methods.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 该调查总结了现有文献中有关详细变压器配置的建议。为了更强的泛化能力，建议使用预先的RMSNorm归一化，以及SwiGLU或GeGLU激活函数。此外，在嵌入层之后立即使用LN可能会导致性能下降。至于位置嵌入，**Rotary
    Positional Embedding**（**RoPE**）或**Attention with Linear Biases**（**AliBi**）在处理长序列时比其他方法表现更好。
- en: Now that we’re familiar with the architecture properties of LLMs, let’s discuss
    specific model instances.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对LLMs的架构属性已经很熟悉，让我们讨论具体的模型实例。
- en: Models
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'The following table represents a summary of some of the popular recent LLMs:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了一些流行的最近LLMs：
- en: '![Figure 8.8 – Model cards of recent LLMs with public configuration details
    (modified from https://arxiv.org/abs/2303.18223p)](img/B19627_08_8.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – 最近大型语言模型的模型卡，包含公开配置详情（修改自 https://arxiv.org/abs/2303.18223p）](img/B19627_08_8.jpg)'
- en: Figure 8.8 – Model cards of recent LLMs with public configuration details (modified
    from https://arxiv.org/abs/2303.18223p)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 最近大型语言模型的模型卡，包含公开配置详情（修改自 https://arxiv.org/abs/2303.18223p）
- en: Here, **PE** denotes position embedding, **#L** denotes the number of transformer
    layers, **#H** denotes the number of attention heads per layer, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/707.png)
    denotes the size of hidden states, and **MCL** denotes the maximum context length
    during training.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**PE** 表示位置嵌入，**#L** 表示变换器层数，**#H** 表示每层的注意力头数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/707.png)
    表示隐层状态的大小，**MCL** 表示训练期间的最大上下文长度。
- en: 'We’ll start with the GPT series of models (developed by OpenAI), which is outlined
    in the following diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 GPT 系列模型开始（由 OpenAI 开发），如以下图所示：
- en: '![Figure 8.9 – The evolution of the GPT series of models (inspired by https://arxiv.org/abs/2303.18223)](img/B19627_08_9.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – GPT 系列模型的演变（灵感来源于 https://arxiv.org/abs/2303.18223）](img/B19627_08_9.jpg)'
- en: Figure 8.9 – The evolution of the GPT series of models (inspired by https://arxiv.org/abs/2303.18223)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – GPT 系列模型的演变（灵感来源于 https://arxiv.org/abs/2303.18223）
- en: We’re already familiar with GPT-1, so let’s move on to `gpt-3.5-turbo` with
    a context length of 4,096 tokens and `gpt-3.5-turbo-16k` with 16,384 tokens. The
    current version of Copilot is based on GPT-3.5\. The newest model, GPT-4, accepts
    multimodal inputs (images and text) but outputs text only. It is also closed,
    but it might have more than 1T parameters. According to Sam Altman, CEO of OpenAI,
    training GPT-4 has cost more than $100 million (https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
    GPT-4 is also available through OpenAI’s API with two subvariants—`gpt-4` with
    a context length of 8,192 tokens and `gpt-4-32k` with 32,768 tokens.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉 GPT-1，因此让我们继续了解 `gpt-3.5-turbo`，其上下文长度为 4,096 个标记，`gpt-3.5-turbo-16k`
    的上下文长度为 16,384 个标记。目前的 Copilot 版本基于 GPT-3.5。最新的模型 GPT-4 接受多模态输入（图像和文本），但仅输出文本。它也是封闭的，但可能具有超过
    1T 的参数。根据 OpenAI 首席执行官 Sam Altman 的说法，训练 GPT-4 的成本已超过 1 亿美元（https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/）。GPT-4
    也通过 OpenAI 的 API 提供，有两个子变体——`gpt-4`，上下文长度为 8,192 个标记，以及 `gpt-4-32k`，上下文长度为 32,768
    个标记。
- en: 'Next, let’s discuss the **LlaMa** series of pre-trained (and not fine-tuned)
    models released by Meta. The first version (*LLaMA: Open and Efficient Foundation
    Language Models*, [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971))
    has four variants, ranging from 6B to 65B parameters. This is one of the most
    popular LLMs in the open source community because Meta has also released its weights
    (although they are not licensed for commercial use). This way, the company has
    done the heavy lifting of pre-training the model. The open source community uses
    it as a **foundation model** because it can be fine-tuned with relatively little
    compute. Recently, Meta released **Llama 2**—an updated version of Llama (*Llama
    2: Open Foundation and Fine-Tuned Chat Models*, [https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models)).
    It has three variants with 7B, 13B, and 70B parameters. Llama 2 uses GQA and 40%
    more pre-training data than Llama 1\. In addition, each variant also has a version
    fine-tuned using RLHF. The model’s license allows commercial use (with some limitations).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，让我们讨论Meta发布的**LlaMa**系列预训练（且未微调）模型。第一个版本（*LLaMA: Open and Efficient Foundation
    Language Models*， [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971)）有四个变体，参数量从6B到65B不等。由于Meta还发布了其权重（尽管不允许商业使用），这是开源社区中最受欢迎的LLM之一。这样，Meta完成了预训练模型的重担，开源社区则将其作为**基础模型**使用，因为它可以通过相对较少的计算资源进行微调。最近，Meta发布了**Llama
    2**——Llama的更新版（*Llama 2: Open Foundation and Fine-Tuned Chat Models*， [https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models)）。它有三个变体，分别为7B、13B和70B参数。Llama
    2使用比Llama 1多40%的预训练数据，并且每个变体还有一个使用RLHF微调的版本。该模型的许可证允许商业使用（有些限制）。'
- en: This concludes our survey on the architecture of LLMs. Next, let’s discuss their
    training.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对LLM架构的调查。接下来，让我们讨论它们的训练。
- en: Training LLMs
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练LLM
- en: 'Since most LLMs are decoder-only, the most common LLM pre-training task is
    NWP. The large number of model parameters (up to hundreds of billions) requires
    comparatively large training datasets to prevent overfitting and realize the full
    capabilities of the models. This requirement poses two significant challenges:
    ensuring training data quality and the ability to process large volumes of data.
    In the following sections, we’ll discuss various aspects of the LLM training pipeline,
    starting from the training datasets.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数LLM是仅解码器模型，最常见的LLM预训练任务是NWP。模型参数的庞大数量（可达数百亿个）需要相对较大的训练数据集来防止过拟合，并实现模型的全部能力。这一要求带来了两个重大挑战：确保训练数据的质量和处理大量数据的能力。在接下来的部分中，我们将讨论LLM训练流水线的各个方面，从训练数据集开始。
- en: Training datasets
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据集
- en: 'We can categorize the training data into two broad categories:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将训练数据分为两大类：
- en: '**General**: Examples include web pages, books, or conversational text. LLMs
    almost always train on general data because it’s widely available and diverse,
    improving the language modeling and generalization capabilities of LLMs.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用**：例如网页、书籍或对话文本。LLM几乎总是基于通用数据进行训练，因为这些数据广泛可用且多样化，能够提升LLM的语言建模和泛化能力。'
- en: '**Specialized**: Code, scientific articles, textbooks, or multilingual data
    for providing LLMs with task-specific capabilities.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专业**：代码、科学文章、教科书或多语言数据，旨在为LLM提供特定任务的能力。'
- en: 'The following table lists the most popular language modeling datasets:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了最受欢迎的语言模型数据集：
- en: '![Figure 8.10 – Language modeling datasets (modified from https://arxiv.org/abs/2303.18223)](img/B19627_08_10.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 语言建模数据集（修改自 https://arxiv.org/abs/2303.18223）](img/B19627_08_10.jpg)'
- en: Figure 8.10 – Language modeling datasets (modified from https://arxiv.org/abs/2303.18223)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 语言建模数据集（修改自 https://arxiv.org/abs/2303.18223）
- en: 'Let’s discuss them:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论一下：
- en: '**Books**: We’ll focus on two datasets:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**书籍**：我们将专注于两个数据集：'
- en: '**BookCorpus** (*Aligning Books and Movies: Towards Story-like Visual Explanations
    by Watching Movies and Reading Books*, [https://arxiv.org/abs/1506.06724](https://arxiv.org/abs/1506.06724)):
    Includes 11,000 fictional books with close to 1B words (released in 2015).'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BookCorpus**（*Aligning Books and Movies: Towards Story-like Visual Explanations
    by Watching Movies and Reading Books*， [https://arxiv.org/abs/1506.06724](https://arxiv.org/abs/1506.06724)）：包含了11,000本虚构书籍，约有10亿个词（2015年发布）。'
- en: '**Project Gutenberg** ([https://www.gutenberg.org/](https://www.gutenberg.org/)):
    Includes 70,000 fictional books.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**古腾堡计划** ([https://www.gutenberg.org/](https://www.gutenberg.org/))：包括 70,000
    本小说类书籍。'
- en: '**Common Crawl** ([https://commoncrawl.org/](https://commoncrawl.org/)): Petabyte-sized
    web crawling database. The data is split by the date obtained, starting from 2008\.
    The latest archive contains 3.1B web pages (390 TiB of uncompressed content),
    scraped from 44 million hosts or 35 million registered domains. It contains a
    lot of low-quality data, but there are multiple subsets with higher-quality data:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Common Crawl** ([https://commoncrawl.org/](https://commoncrawl.org/))：PB
    级别的网络抓取数据库。数据按照获取日期进行分割，从 2008 年开始。最新的档案包含 31 亿网页（390 TiB 的未压缩内容），这些数据来源于 4,400
    万个主机或 3,500 万个注册域名。虽然包含大量低质量数据，但也有多个子集包含更高质量的数据：'
- en: '**Colossal, cleaned version of Common Crawl** (**C4**): An 800 GiB dataset
    developed by Google. The original dataset is unavailable for download, but Google
    has published the tools to recreate it from the Common Crawl database. In 2019,
    the **Allen Institute for AI** (**AI2**, https://allenai.org/) released a recreation,
    available at [https://huggingface.co/datasets/allenai/c4](https://huggingface.co/datasets/allenai/c4).
    Its most popular sub-variant is the *en* version, which removes all documents
    that contain words from the so-called *badwords filter* (a list of bad words is
    available at [https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words)).'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**庞大且清理过的 Common Crawl 版本**（**C4**）：由 Google 开发的 800 GiB 数据集。原始数据集不可下载，但 Google
    已发布工具，以便从 Common Crawl 数据库中重建该数据集。2019 年，**艾伦人工智能研究所**（**AI2**，https://allenai.org/）发布了该数据集的重建版本，可通过
    [https://huggingface.co/datasets/allenai/c4](https://huggingface.co/datasets/allenai/c4)
    获取。其最受欢迎的子版本是 *en* 版本，它移除了所有包含“坏词”列表中单词的文档（“坏词”列表可通过 [https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words)
    查看）。'
- en: '**CC-News**: Articles from news sites all over the world.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CC-News**：来自全球各大新闻网站的文章。'
- en: '**RealNews**: News articles extracted from the 5,000 news domains indexed by
    *Google News*.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RealNews**：从 *Google News* 索引的 5,000 个新闻域名中提取的新闻文章。'
- en: '**CC-Stories-R**: A dataset for common sense reasoning and language modeling.
    It consists of Common Crawl documents with the most overlapping *n*-grams with
    the questions in common sense reasoning tasks. The new training corpus represents
    the top 1.0% of the highest-ranked documents.'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CC-Stories-R**：一个用于常识推理和语言建模的数据集。它由与常识推理任务中的问题具有最大重叠的 *n*-gram 的 Common Crawl
    文档组成。新的训练语料库代表了排名前 1.0% 的高质量文档。'
- en: '**Reddit links**: One way to overcome the low signal-to-noise ratio of Common
    Crawl is to rely on human-curated content. Enter Reddit, where users can post
    textual content or links, and other users can upvote these submissions (the upvotes
    are known as *karma*). We’ll mention two Reddit-based datasets:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reddit 链接**：解决 Common Crawl 低信噪比的一种方法是依赖人工策划的内容。Reddit 就是一个典型平台，用户可以发布文本内容或链接，其他用户可以对这些提交进行点赞（点赞称为
    *karma*）。我们将提到两个基于 Reddit 的数据集：'
- en: '**WebText** (released alongside the GPT-2 model): Contains a subset of 45 million
    Reddit-submitted links with a karma of three or more. The documents behind these
    links form the LLM training data. WebText is not publicly available, but there
    is an open source version called **OpenWebText** ([https://github.com/jcpeterson/openwebtext](https://github.com/jcpeterson/openwebtext)).'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WebText**（与 GPT-2 模型一起发布）：包含 4,500 万个 Reddit 提交链接，其中 karma 为三次或以上。这些链接背后的文档构成了
    LLM 训练数据。WebText 并未公开发布，但有一个开源版本，名为 **OpenWebText** ([https://github.com/jcpeterson/openwebtext](https://github.com/jcpeterson/openwebtext))。'
- en: '**Pushshift** ([https://arxiv.org/abs/2001.08435](https://arxiv.org/abs/2001.08435)):
    Contains all link submissions and comments posted on Reddit.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pushshift** ([https://arxiv.org/abs/2001.08435](https://arxiv.org/abs/2001.08435))：包含所有在
    Reddit 上提交的链接和评论。'
- en: Reddit API pricing controversy
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit API 定价争议
- en: The rise of LLMs has made Reddit data much more valuable than before. Because
    of this, the company has decided to introduce fees for access to its previously
    free API. This measure mainly targets AI companies that plan to train their LLMs
    using the data. However, the proposal has led many of the site’s voluntary moderators
    (Reddit relies on them) to announce a strike by temporarily closing the previously
    open communities they moderate. At the time of writing, the disagreement is still
    ongoing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的兴起使得Reddit的数据比以往更有价值。基于这一点，公司决定对其原本免费的API引入费用。这项措施主要针对那些计划利用这些数据训练LLM的AI公司。然而，这一提案导致许多该网站的志愿版主（Reddit依赖他们）宣布通过暂时关闭他们所管理的原本开放的社区来进行罢工。截至写作时，双方的争议仍在持续。
- en: '**The Pile** (*An 800GB Dataset of Diverse Text for Language Modeling*, [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)):
    Composed of 22 diverse and high-quality datasets derived from various sources,
    including PubMed, arXiv, GitHub, Stack Exchange, Hacker News, YouTube, and others.
    The Pile also introduces the OpenWebText2 and BookCorpus2 extensions of the original
    OpenWebText and BookCorpus datasets.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**The Pile** (*An 800GB Dataset of Diverse Text for Language Modeling*, [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)):
    由22个多样且高质量的数据集组成，来源包括PubMed、arXiv、GitHub、Stack Exchange、Hacker News、YouTube等。The
    Pile还引入了原始OpenWebText和BookCorpus数据集的扩展版本OpenWebText2和BookCorpus2。'
- en: '**ROOTS** (*The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset*,
    [https://arxiv.org/abs/2303.03915](https://arxiv.org/abs/2303.03915)): A web-scale
    curated dataset covering 46 natural languages and 13 programming languages.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROOTS** (*The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset*,
    [https://arxiv.org/abs/2303.03915](https://arxiv.org/abs/2303.03915)): 一个规模庞大的精心策划的数据集，涵盖46种自然语言和13种编程语言。'
- en: '**Wikimedia** ([https://dumps.wikimedia.org/](https://dumps.wikimedia.org/)):
    Because of its high-quality content, this is an excellent source of training data.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wikimedia** ([https://dumps.wikimedia.org/](https://dumps.wikimedia.org/)):
    因其高质量的内容，这是一个优秀的训练数据来源。'
- en: '**Stack Exchange** ([https://archive.org/details/stackexchange](https://archive.org/details/stackexchange)):
    A network of QA topic sites with a rating system. The most popular representative
    is **Stack Overflow**. It releases a tri-monthly anonymized data dump with all
    user-contributed content.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stack Exchange** ([https://archive.org/details/stackexchange](https://archive.org/details/stackexchange)):
    一个拥有评分系统的QA主题网站网络。最具代表性的站点是**Stack Overflow**。它每三个月发布一次匿名化数据转储，包含所有用户贡献的内容。'
- en: '**arXiv** (https://www.kaggle.com/datasets/Cornell-University/arxiv): The primary
    scientific data source, which contains more than 2.2B scientific articles.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**arXiv** (https://www.kaggle.com/datasets/Cornell-University/arxiv): 主要的科学数据来源，包含超过22亿篇科学文章。'
- en: '**GitHub**: The GH Archive project ([https://www.gharchive.org/](https://www.gharchive.org/))
    records, archives, and provides access to the public GitHub timeline.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub**: GH Archive项目 ([https://www.gharchive.org/](https://www.gharchive.org/))
    记录、归档并提供公共GitHub时间线的访问。'
- en: 'In practice, the LLM pre-training step uses a mix of several datasets. The
    following screenshot shows the distribution of the sources of pre-training data
    for several representative LLMs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，LLM的预训练步骤使用的是多个数据集的混合。以下截图展示了几个代表性LLM的预训练数据来源分布：
- en: '![Figure 8.11 – Ratios of various data sources in the pre-training data for
    existing LLMs (source: https://arxiv.org/abs/2303.18223)](img/B19627_08_11.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 现有LLM预训练数据中各种数据源的比例（来源：https://arxiv.org/abs/2303.18223)](img/B19627_08_11.jpg)'
- en: 'Figure 8.11 – Ratios of various data sources in the pre-training data for existing
    LLMs (source: https://arxiv.org/abs/2303.18223)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 现有LLM预训练数据中各种数据源的比例（来源：https://arxiv.org/abs/2303.18223）
- en: 'Mixing datasets is not a trivial process and requires several processing steps.
    Let’s discuss them:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集混合并非一项简单的过程，需要多个处理步骤。我们来讨论一下这些步骤：
- en: '**Remove low-quality or irrelevant data**: For example, web pages contain large
    amounts of HTML tags, JavaScript, or **cascading style sheets** (**CSS**). Yet,
    we’re only interested in'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除低质量或无关的数据**：例如，网页中包含大量HTML标签、JavaScript或**层叠样式表**（**CSS**）。然而，我们只对'
- en: human-readable text (except when we want to train the model explicitly to understand
    HTML). In this case, we’ll have to remove the HTML and JavaScript and only leave
    the text.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人类可读文本（除非我们明确希望训练模型理解HTML）。在这种情况下，我们必须去除HTML和JavaScript，只保留文本。
- en: '**Remove personally identifiable information (PII)**: Data is often extracted
    from web pages, which might contain personal information. This step aims to remove
    such data from the training set.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除个人可识别信息（PII）**：数据通常从网页中提取，而网页中可能包含个人信息。此步骤旨在从训练集中删除此类数据。'
- en: '**Tokenization**: We discussed tokenization in depth in [*Chapter 6*](B19627_06.xhtml#_idTextAnchor185),
    and we won’t discuss it here.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：我们在[*第六章*](B19627_06.xhtml#_idTextAnchor185)中深入讨论了分词，本文不再赘述。'
- en: 'Finally, let’s introduce a practical transformer scaling law (*Scaling Laws
    for Neural Language Models*, https://arxiv.org/abs/2001.08361). Because of their
    scale, training LLMs can be expensive. Therefore, it is important not to train
    the model more (or less) than necessary. Based on empirical experiments, the scaling
    law proposes an optimal ratio between the amount of training compute (expressed
    in **floating-point operations per second**, or **FLOPS**), *C*, the model size
    (number of parameters), *N*, and the training dataset size (number of tokens),
    *D*:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们介绍一个实际的变压器缩放法则（*神经语言模型的缩放法则*， https://arxiv.org/abs/2001.08361）。由于其规模，训练LLM可能非常昂贵。因此，避免过度训练或训练不足至关重要。根据经验实验，缩放法则提出了训练计算量（以**浮动点操作每秒**，或**FLOPS**表示）、*C*，模型大小（参数数量）、*N*，以及训练数据集大小（令牌数量）之间的最佳比例：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>C</mml:mi><mml:mo>≈</mml:mo><mml:mn>6</mml:mn><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:math>](img/794.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>C</mml:mi><mml:mo>≈</mml:mo><mml:mn>6</mml:mn><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:math>](img/794.png)'
- en: Now that we know the steps to build the training set, let’s focus on the actual
    pre-training.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了构建训练集的步骤，接下来让我们专注于实际的预训练。
- en: Pre-training properties
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练的特性
- en: Similar to other **neural networks** (**NNs**), pre-training of LLMs works with
    gradient descent and backpropagation. But because of their size, the training
    has specific properties, which we’ll discuss in this section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他**神经网络**（**NNs**）类似，LLM的预训练通过梯度下降和反向传播进行。但由于其规模庞大，训练具有一些特定的特性，我们将在本节中讨论这些特性。
- en: Adam optimizer
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adam优化器
- en: 'Most LLMs use Adam (*Adam: A Method for Stochastic Optimization*, [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980))
    or one of its modifications. Although we’ve used it in many examples so far, we
    haven’t discussed it in detail. Time to remedy this omission.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数LLM使用Adam（*Adam：一种随机优化方法*， [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)）或其某些变体。尽管我们在许多示例中使用了它，但至今我们尚未详细讨论它。现在是时候弥补这个遗漏了。
- en: A reminder of the weight update formula
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新公式的回顾
- en: 'In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047), we learned that we use
    backpropagation to compute the gradient (first derivative) of the loss function,
    *J(θ)*, with respect to every parameter, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/795.png):
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/796.png).
    Once we have the gradient, we can perform the weight update with the formula ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/797.png),
    where *η* is the learning rate. We can add momentum (or velocity) to that formula.
    To do so, we’ll assume we are at step *t* of the training process. Then, we can
    calculate the momentum of the current update based on the momentum of the update
    at step *t-1*: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/798.png),
    where *µ* is a momentum rate in the [0:1] range. In addition, we can add L2 regularization
    (or weight decay; see [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079)): ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/799.png),
    where *λ* is the weight decay coefficient. Finally, we can perform the weight
    update: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/800.png).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B19627_02.xhtml#_idTextAnchor047)，我们学习到使用反向传播来计算损失函数*J(θ)*关于每个参数![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>的梯度（一阶导数）：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>（img/795.png）.
    一旦我们得到梯度，我们可以使用公式![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>（img/796.png），其中*η*是学习率。我们可以在该公式中加入动量（或速度）。为此，我们假设我们处于训练过程的第*t*步。然后，我们可以根据第*t-1*步的更新动量来计算当前更新的动量：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>（img/797.png），其中*µ*是[0:1]范围内的动量率。此外，我们还可以添加L2正则化（或权重衰减；见[*第3章*](B19627_03.xhtml#_idTextAnchor079)）：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>（img/799.png），其中*λ*是权重衰减系数。最后，我们可以执行权重更新：![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>（img/800.png）.
- en: 'Adam calculates individual and adaptive learning rates for every weight based
    on previous weight updates (momentum). Let’s see how that works:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 根据先前的权重更新（动量）为每个权重计算个体和自适应学习率。让我们看看是如何工作的：
- en: 'Compute the first moment (or mean) and the second moment (or variance) of the
    gradient:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度的一阶矩（或均值）和二阶矩（或方差）：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>←</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>m</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>1</mn></msub></mrow></mfenced><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></math>](img/801.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>←</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>m</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>1</mn></msub></mrow></mfenced><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></math>](img/801.png)'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/802.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/802.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/803.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/804.png)
    are hyperparameters with default values of 0.9 and 0.95, respectively. The two
    formulas are very similar to the momentum one. The relationship between ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/805.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/806.png)**)*
    and
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/803.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/804.png)
    是具有默认值 0.9 和 0.95 的超参数。 这两个公式与动量公式非常相似。![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/805.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/806.png)**)*
    和
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/807.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/808.png)**)*
    acts as a simulation of a moving average. But instead of averaging across multiple
    previous values, we take the latest previous value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/809.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/810.png)**)*,
    and assign it a weight coefficient, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/803.png)
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/804.png)).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/807.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/808.png)**)*
    作为一种移动平均的模拟。但不同于跨多个前值进行平均，我们只取最新的前一个值，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/809.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/810.png)**)*，并为其分配一个权重系数，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/803.png)
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/804.png))。'
- en: 2. The initial values of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png)
    are 0, so they will have a bias toward 0 in the initial phase of the training.
    To understand why this could be a problem, let’s assume that at *t=1*, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math>](img/815.png)
    and
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 2. ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png)
    的初始值为 0，因此在训练的初期阶段，它们会对 0 产生偏差。为了理解为什么这可能是个问题，假设在 *t=1* 时，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math>](img/815.png)
    且
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math>](img/816.png).
    Then, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mi>*</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mfenced><mml:mi>*</mml:mi><mml:mn>5</mml:mn><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/817.png)
    is much less than the actual gradient of 5\. We can compensate for this bias with
    bias-corrected versions of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math>](img/816.png)。然后，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mi>*</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mfenced><mml:mi>*</mml:mi><mml:mn>5</mml:mn><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/817.png)
    远小于实际梯度 5。我们可以通过使用偏差校正版本的 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mrow><mml:mrow><mml:mi>t</mml:mrow></mml:msub></mml:math>](img/813.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mrow><mml:mrow><mml:mi>t</mml:mrow></mml:msub></mml:math>](img/187.png)
    来补偿这个偏差：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>m</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mfrac><msub><mi>m</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></mrow></math>](img/820.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>m</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mfrac><msub><mi>m</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></mrow></math>](img/820.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>v</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mfrac><msub><mi>v</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></mrow></math>](img/821.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>v</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mfrac><msub><mi>v</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></mrow></math>](img/821.png)'
- en: '3. Perform the weight update with the following formula:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 使用以下公式进行权重更新：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>η</mi><mfrac><mover><msub><mi>m</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><msqrt><mrow><mover><msub><mi>v</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mrow></mrow></math>](img/822.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>η</mi><mfrac><mover><msub><mi>m</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><msqrt><mrow><mover><msub><mi>v</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mrow></mrow></math>](img/822.png)'
- en: Here, *ε* is some small value to prevent division by 0.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*ε* 是一个小值，用于防止除以 0。
- en: '**AdamW** (*Decoupled Weight Decay Regularization*, [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101))
    improves Adam with decoupled weight decay:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdamW**（*解耦权重衰减正则化*， [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)）通过解耦的权重衰减改进了
    Adam 算法：'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msqrt><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/823.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msqrt><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/823.png)'
- en: Recall that the L2 regularization participates in the loss function and then,
    through the derivative process, is transferred (as weight decay) to the weight
    update formula. In this case, the regularization will pass through all the transformations
    of the cost function and will be subject to them. As the name suggests, decoupled
    weight decay bypasses all these transformations and participates directly in the
    preceding formula.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，L2 正则化参与损失函数，并通过导数过程被转移（作为权重衰减）到权重更新公式中。在这种情况下，正则化会经过损失函数的所有变换，并且会受到这些变换的影响。正如名字所示，解耦权重衰减绕过了所有这些变换，直接参与到前面的公式中。
- en: One issue with Adam and AdamW is the increased memory consumption—the optimizer
    stores at least two additional values (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png))
    for every model parameter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 和 AdamW 的一个问题是增加的内存消耗——优化器为每个模型参数存储至少两个额外的值 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png))。
- en: Parallel processing
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行处理
- en: 'The scale of LLMs necessitates special steps for efficient training. First,
    we’ll discuss how to train LLMs across multiple devices. More specifically, we’ll
    discuss a combination of three different types of parallelism (also referred to
    as 3D parallelism):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的规模需要特别的步骤来进行高效训练。首先，我们将讨论如何在多个设备上训练 LLMs。更具体地，我们将讨论三种不同类型的并行性组合（也称为 3D
    并行性）：
- en: '**Data parallelism**: It works when the model is small enough to fit on a single
    device:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据并行性**：当模型足够小，可以适配到单个设备上时，它有效：'
- en: Create identical copies of the entire model and its optimizer states (including
    the random seeds) across all devices.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有设备上创建整个模型及其优化器状态（包括随机种子）的相同副本。
- en: Split each batch of the training set into unique subsets (shards) and distribute
    them across all devices.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每批训练集拆分成唯一的子集（分片），并分配到所有设备上。
- en: Each device computes its gradient based on its unique subset of the input batch.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个设备根据其唯一的输入批次子集计算其梯度。
- en: Aggregate the gradients of all devices into a single gradient update.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有设备的梯度汇总为一个单一的梯度更新。
- en: Distribute the aggregated updates across the devices and perform weight updates
    on each device. This way, we start and end each training step with identical models.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将聚合的更新分发到各个设备，并在每个设备上执行权重更新。通过这种方式，我们在每个训练步骤开始和结束时，都会使用相同的模型。
- en: '**Model (or pipeline)**: Split the model across multiple devices on an operation
    (layer) level. For example, if our model has 9 layers, we can send layers 1 through
    6 to one device and layers 7 through 9 to another. In this way, we can train models
    that don’t fit in the memory of a single device. Not only that, but we can apply
    this method even on a single device. In this case, we’ll load the first set of
    operations (1-6) and compute their output. Then, we’ll unload them and load the
    following subset (7-9). The output of the first set will serve as input for the
    second. Backpropagation works in the same way but in the opposite direction. One
    issue with model parallelism is that if we use multiple devices, the second one
    will idle until the first produces output.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型（或流水线）**：在操作（层）级别将模型分布到多个设备上。例如，如果我们的模型有9层，我们可以将第1到第6层发送到一台设备，将第7到第9层发送到另一台设备。通过这种方式，我们可以训练无法在单个设备内存中容纳的模型。不仅如此，我们即使在单个设备上也可以应用这种方法。在这种情况下，我们将加载第一组操作（1-6）并计算它们的输出。然后，我们会卸载这些操作并加载接下来的子集（7-9）。第一组的输出将作为第二组的输入。反向传播以相同的方式进行，但方向相反。模型并行的一个问题是，如果使用多个设备，第二个设备会在第一个设备产生输出之前处于空闲状态。'
- en: '**Tensor (or horizontal)**: Split the model across different devices on the
    tensor level, which solves the idling problem of model parallelism. To understand
    how this works, let’s recall that matrix multiplication is the most computationally
    intensive operation of contemporary NNs. But, as we discussed in the *FlashAttention*
    section, it is also embarrassingly parallel. Therefore, we can split it across
    devices.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量（或水平）**：在张量级别将模型分布到不同的设备上，从而解决了模型并行中的空闲问题。为了理解这一点，我们回顾一下矩阵乘法是当代神经网络中最计算密集的操作。但正如我们在*FlashAttention*部分讨论的，它也是极其容易并行化的。因此，我们可以将它分配到不同的设备上。'
- en: Zero redundancy optimizer
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零冗余优化器
- en: '**Zero redundancy optimizer** (**ZeRO**) (*ZeRO: Memory Optimizations Toward
    Training Trillion Parameter Models*, https://arxiv.org/abs/1910.02054) is a hybrid
    between data and model parallelism. The following diagram illustrates the three
    stages of ZeRO:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**零冗余优化器**（**ZeRO**）(*ZeRO: 面向训练万亿参数模型的内存优化*, https://arxiv.org/abs/1910.02054)
    是数据并行和模型并行的混合体。下图展示了ZeRO的三个阶段：'
- en: '![Figure 8.12 – ZeRO (inspired by https://arxiv.org/abs/1910.02054)](img/B19627_08_12.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – ZeRO（灵感来源于 https://arxiv.org/abs/1910.02054）](img/B19627_08_12.jpg)'
- en: Figure 8.12 – ZeRO (inspired by https://arxiv.org/abs/1910.02054)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – ZeRO（灵感来源于 https://arxiv.org/abs/1910.02054）
- en: 'The first line represents a case of a data-parallel system. Each GPU receives
    a unique shard of the input mini-batch. It also holds an identical copy of the
    model parameters (first colored rectangle of the GPUi block), gradients (second
    rectangle), and optimizer states (third rectangle). The size of the optimizer
    states that they take up most of the memory during training (for example, Adam
    stores multiple values per model parameter). The following three lines represent
    the three stages of ZeRO:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行表示数据并行系统的情况。每个GPU接收输入小批量数据的一个独特分片。它还保存模型参数的副本（GPUi块的第一个彩色矩形）、梯度（第二个矩形）和优化器状态（第三个矩形）。优化器状态占用的内存最大（例如，Adam为每个模型参数存储多个值），因此它们在训练过程中占用大部分内存。接下来的三行表示ZeRO的三个阶段：
- en: '**Optimizer state partitioning** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os</mml:mtext></mml:mrow></mml:msub></mml:math>](img/826.png)):
    Each GPU holds an identical copy of the entire model parameters and its gradients,
    but the optimizer states are partitioned across the GPUs, and each holds only
    a portion.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化器状态分区** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os</mml:mtext></mml:mrow></mml:msub></mml:math>](img/826.png))：每个GPU保存整个模型参数及其梯度的副本，但优化器状态在GPU之间进行分区，每个GPU只保存其中一部分。'
- en: '**Add gradient partitioning** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g</mml:mtext></mml:mrow></mml:msub></mml:math>](img/827.png)):
    Each GPU holds an identical copy of the entire model parameters, but the gradients
    and the optimizer states are partitioned.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加梯度分区** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g</mml:mtext></mml:mrow></mml:msub></mml:math>](img/827.png))：每个GPU持有整个模型参数的副本，但梯度和优化器状态是分区的。'
- en: '**Add model parameters** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g+p</mml:mtext></mml:mrow></mml:msub></mml:math>](img/828.png)):
    Each GPU holds a portion of all components.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加模型参数** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g+p</mml:mtext></mml:mrow></mml:msub></mml:math>](img/828.png))：每个GPU保存所有组件的一部分。'
- en: To understand how the algorithm works, we’ll assume we use ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g+p</mml:mtext></mml:mrow></mml:msub></mml:math>](img/829.png),
    a model with *N* layers and *N* GPUs. Each layer is distributed on one GPU—the
    first layer is on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/830.png),
    the second layer is on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/831.png),
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解算法的工作原理，我们假设使用![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g+p</mml:mtext></mml:mrow></mml:msub></mml:math>](img/829.png)，一个包含*N*层和*N*个GPU的模型。每一层都分布在一个GPU上——第一层在![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/830.png)，第二层在![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/831.png)。
- en: and so on. Let’s start with the forward phase. First, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/832.png)
    receives ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/833.png).
    Since it holds the first layer of the model, it can feed it the input and calculate
    its activations independently. At the same time, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/834.png)
    broadcasts the parameters of the first layer to all other GPUs. Each GPU now holds
    the first layer parameters in addition to its own portion of the model parameters.
    In this way, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:math>](img/835.png)
    can process its own input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:math>](img/836.png),
    through the first layer, just as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/832.png)
    did. Once a GPU computes the activations of the first layer, it deletes its parameters
    from its memory (except ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/838.png),
    which preserves them). We repeat the same steps, this time with the second layer.
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/839.png)
    broadcasts its parameters so that all GPUs can continue with the forward phase.
    After this, all but ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/840.png)
    delete the second layer parameters. This process continues until all GPUs produce
    model output. Then, the loss function is aggregated across all GPUs. Next, the
    backward phase starts, which works in the same way as the forward one, but this
    time the GPUs broadcast both the gradients and the optimizer states.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。让我们从前向传播阶段开始。首先，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/832.png)
    接收 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/833.png)。由于它持有模型的第一层，它可以独立地将输入喂入并计算其激活值。同时，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/834.png)
    将第一层的参数广播到所有其他GPU。现在，每个GPU除了持有自己的部分模型参数外，还持有第一层的参数。通过这种方式，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:math>](img/835.png)
    可以处理自己的输入，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:math>](img/836.png)，通过第一层，就像![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/832.png)
    做的那样。一旦GPU计算完第一层的激活值，它会从内存中删除其参数（除了![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/838.png)
    保留的参数）。我们重复相同的步骤，这次处理第二层。![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/839.png)
    广播其参数，以便所有GPU可以继续前向传播阶段。之后，除了![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/840.png)之外，所有其他GPU都会删除第二层的参数。这个过程会一直持续，直到所有GPU输出模型结果。然后，所有GPU的损失函数会被汇总。接下来，反向传播阶段开始，它的工作方式与前向传播相同，但这次GPU会同时广播梯度和优化器状态。
- en: Mixed precision training
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: '**Mixed precision training** ([https://arxiv.org/abs/1710.03740](https://arxiv.org/abs/1710.03740))
    is the idea that not all values have to be stored with 32-bit (double or full)
    floating-point precision (**FP32** or **Float32** data format). Research has shown
    that storing some values into lower 16-bit (single or half) floating-point precision
    (**FP16** of **Float16**) doesn’t degrade the model performance. The weights,
    activations, gradients, and optimizer states are stored as FP16\. In addition,
    the model retains an FP32 master copy of the weights. The forward and backward
    passes use FP16 weights, but the results are optimal when the weight update operation
    uses the FP32 master copy. One possible explanation is that the weight update
    formula uses the gradients multiplied by the learning rate, and the result might
    become too small to be represented in FP16\. Another explanation is that the ratio
    of the weight value to the weight update is very large, which could lead to the
    weight update becoming zero.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合精度训练** ([https://arxiv.org/abs/1710.03740](https://arxiv.org/abs/1710.03740))的核心思想是，并非所有的值都必须以
    32 位（双精度或全精度）浮动点精度（**FP32** 或 **Float32** 数据格式）来存储。研究表明，将部分值存储为 16 位（单精度或半精度）浮动点精度（**FP16**
    或 **Float16**）不会降低模型的性能。权重、激活、梯度和优化器状态都以 FP16 格式存储。此外，模型保留一个 FP32 的主副本作为权重。前向和反向传播使用的是
    FP16 权重，但当进行权重更新操作时，使用的是 FP32 主副本，这样结果是最优的。一个可能的解释是，权重更新公式使用了乘以学习率的梯度，而结果可能会变得太小，无法在
    FP16 中表示。另一个解释是，权重值和权重更新的比例非常大，这可能导致权重更新变为零。'
- en: Bfloat16 and TensorFloat32
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Bfloat16 和 TensorFloat32
- en: The Google Brain division developed the **brain floating-point** format (hence
    the name, **bfloat**) for **machine learning** (**ML**) applications. The standard
    FP16 format has one sign bit, five exponent bits, and ten mantissa bits. In contrast,
    bfloat16 has eight exponent and seven mantissa bits. The exponent bits are the
    same as FP32\. Bfloat16 comes close to FP32 in terms of performance on ML tasks.
    We also have **TensorFloat-32** (**TF32**)—a 19-bit format developed by NVIDIA
    for ML purposes with 8 exponent and 10 mantissa bits.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Google Brain 部门为 **机器学习**（**ML**）应用开发了 **brain floating-point** 格式（因此得名 **bfloat**）。标准的
    FP16 格式有一个符号位、五个位的指数部分和十个位的尾数部分。与之不同，bfloat16 具有八个位的指数部分和七个位的尾数部分。其指数部分与 FP32
    相同。就 ML 任务的性能而言，bfloat16 与 FP32 相差无几。我们还可以找到 **TensorFloat-32**（**TF32**）格式——这是
    NVIDIA 为 ML 目的开发的 19 位格式，具有 8 位的指数部分和 10 位的尾数部分。
- en: Pre-training peculiarities and summary
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练特殊性和总结
- en: In this section, we’ll discuss some LLM pre-training peculiarities. Let’s start
    with the mini-batch size. Ideally, we would compute the gradients over the entire
    training dataset and only then perform one weight update. However, large datasets
    and models make this computationally infeasible. The opposite extreme is to perform
    one weight update per training sample. But then, the training would be susceptible
    to outlier samples, which might steer the loss function to suboptimal local minima.
    Mini-batch training is a compromise that makes it possible to fit within the available
    computational resources and avoid the influence of outlier samples. But in theory,
    the larger the mini-batch size, the better. LLM training is distributed across
    multiple devices, which makes it possible (and even desirable) to use large batch
    sizes. The batch size can vary between 32K to 8.25M tokens depending on the model.
    In addition, it can be dynamic and increase as the training progresses. Empirical
    experiments have demonstrated that this technique stabilizes the training.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将讨论一些 LLM 预训练的特殊性。首先从小批量大小开始。理想情况下，我们会在整个训练数据集上计算梯度，然后进行一次权重更新。然而，庞大的数据集和模型使得这种计算方式在实际操作中不可行。另一个极端是对每个训练样本进行一次权重更新，但这样训练就容易受到离群样本的影响，这可能会将损失函数引导到次优的局部最小值。小批量训练是一种折衷方法，使得在有限的计算资源内可以进行训练，并避免离群样本的影响。但从理论上讲，小批量大小越大越好。LLM
    训练是分布式在多个设备上进行的，这使得使用大批量大小成为可能（甚至是理想的）。根据模型的不同，批量大小可以在 32K 到 8.25M 个 token 之间变化。此外，批量大小还可以是动态的，并随着训练的进行逐渐增大。实验证明，这种技术可以稳定训练过程。
- en: Next, let’s focus on the learning rate, *η*. Although Adam implements an adaptive
    learning rate, most LLMs start with a **warmup phase** to stabilize the training.
    More specifically, during the first 0.1% to 0.5% of the training steps, the learning
    rate gradually increases from around ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:math>](img/841.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/842.png).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们关注学习率，*η*。虽然 Adam 实现了自适应学习率，但大多数 LLM 从 **预热阶段** 开始，以稳定训练。更具体来说，在训练的前 0.1%
    到 0.5% 步骤中，学习率从大约 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:math>](img/841.png)
    增加到 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/842.png)。
- en: Then, the learning rate gradually decreases to around 10% of its maximum value
    following a cosine (or linear) decay strategy.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，学习率将按余弦（或线性）衰减策略逐渐降低到最大值的 10% 左右。
- en: 'LLM training also uses **gradient clipping**—a technique that prevents the
    exploding gradients problem. One way to implement it is to clip by value:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 训练还使用了 **梯度裁剪**——一种防止梯度爆炸问题的技术。一种实现方法是通过值裁剪：
- en: If l**g**l ≥ *max_threshold* or l**g**l ≤ *min_threshold* then **g** ← *relevant_threshold*
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 l**g**l ≥ *max_threshold* 或 l**g**l ≤ *min_threshold*，则 **g** ← *relevant_threshold*
- en: Here, **g** is a vector with all gradient values (l**g**l is the norm or the
    vector or its absolute value). First, we select *min_threshold* and *max_threshold*
    values. Then, we clip the value of the gradient to the threshold in the weight
    update formula if it exceeds these boundaries.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**g** 是一个包含所有梯度值的向量（l**g**l 是向量的范数或其绝对值）。首先，我们选择 *min_threshold* 和 *max_threshold*
    值。然后，如果梯度值超出这些边界，我们将在权重更新公式中将其裁剪至阈值。
- en: 'Another option is to clip by norm:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是通过范数裁剪：
- en: If l**g**l ≥ *threshold*, then **g** ← *threshold* * **g**/l**g**l
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 l**g**l ≥ *threshold*，则 **g** ← *threshold* * **g**/l**g**l
- en: 'Here, **g**/l**g**l is a unit vector. It has the same direction as the original,
    but its length is 1\. The value of every element in the unit vector is in the
    [0:1] range. By multiplying it by the *threshold*, every element lies within the
    [0: threshold] range. In this way, norm clipping scales the gradients within the
    pre-defined threshold.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '这里，**g**/l**g**l 是一个单位向量。它的方向与原向量相同，但长度为 1。单位向量中每个元素的值都在 [0:1] 范围内。通过将其乘以 *threshold*，每个元素都落在
    [0: threshold] 范围内。这样，范数裁剪将梯度限制在预定义的阈值范围内。'
- en: 'The following table summarizes the training properties of some popular LLMs:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了一些流行 LLM 的训练特性：
- en: '![Figure 8.13 – LLM training properties (modified from https://arxiv.org/abs/2303.18223)](img/B19627_08_13.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13 – LLM 训练特性（修改自 https://arxiv.org/abs/2303.18223）](img/B19627_08_13.jpg)'
- en: Figure 8.13 – LLM training properties (modified from https://arxiv.org/abs/2303.18223)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – LLM 训练特性（修改自 https://arxiv.org/abs/2303.18223）
- en: This concludes our introduction to pre-training LLMs. Next, let’s focus on the
    FT phase.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对 LLM 预训练的介绍。接下来，我们将关注 FT 阶段。
- en: FT with RLHF
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有 RLHF 的 FT
- en: So far, we have focused on the pre-training phase of LLMs. The pre-training
    objective of an LLM is to predict the next token based on (primarily) a web page
    training dataset. However, pre-trained models can express undesirable behaviors.
    They can often make up facts, generate biased or toxic text, or simply not follow
    user instructions. Yet, their purpose is to interact with humans in a *helpful*,
    *honest*, and *harmless* way. In this section, we’ll discuss the technique of
    RLHF, which makes it possible to fine-tune the LLM for better alignment with human
    values (also known as **alignment tuning**). More specifically, we’ll focus on
    the technique described in *Training language models to follow instructions with
    human feedback* ([https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155))
    by OpenAI. They apply RLHF on a GPT-3 model to produce the GPT-3.5 family of models.
    This is part of the secret sauce that makes ChatGPT so good at interacting with
    users.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经关注了LLM的预训练阶段。LLM的预训练目标是基于（主要是）网页训练数据集预测下一个标记。然而，预训练模型可能表现出不良行为。它们经常编造事实，生成偏见或有毒的文本，或者根本不遵循用户指令。然而，它们的目的是以*有帮助*、*诚实*和*无害*的方式与人类互动。在本节中，我们将讨论RLHF技术，这使得微调LLM以更好地与人类价值观对齐成为可能（也称为**对齐调优**）。更具体地，我们将关注OpenAI在《用人类反馈训练语言模型以遵循指令》（[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)）中描述的技术。他们在GPT-3模型上应用RLHF，进而推出GPT-3.5系列模型。这是使得ChatGPT如此擅长与用户互动的秘密之一。
- en: 'The FT starts where the pre-training ends—with the pre-trained LLM. The following
    diagram shows the three steps of the RLHF process:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: FT从预训练结束的地方开始——即使用预训练的LLM。以下图表展示了RLHF过程的三步：
- en: '![Figure 8.14 – Left: supervised FT; middle: reward model training; right:
    LLM RLHF (inspired by https://arxiv.org/abs/2203.02155)](img/B19627_08_14.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – 左侧：监督式FT；中间：奖励模型训练；右侧：LLM RLHF（灵感来源于 https://arxiv.org/abs/2203.02155）](img/B19627_08_14.jpg)'
- en: 'Figure 8.14 – Left: supervised FT; middle: reward model training; right: LLM
    RLHF (inspired by https://arxiv.org/abs/2203.02155)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – 左侧：监督式FT；中间：奖励模型训练；右侧：LLM RLHF（灵感来源于 https://arxiv.org/abs/2203.02155）
- en: 'First is `[prompt: response]` samples, where `prompt` and `response` are source
    and target token sequences, respectively. This dataset serves to fine-tune the
    LLM using the same target as pre-training—to predict the next token of the response,
    given the prompt. The fine-tuned LLM serves as a base for the next two steps.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步是`[prompt: response]`样本，其中`prompt`和`response`分别是源和目标的标记序列。该数据集用于微调LLM，使用与预训练相同的目标——即在给定提示的情况下预测响应的下一个标记。微调后的LLM作为下一步的基础。'
- en: On the need for pre-training and three-step FT
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于预训练和三步FT的必要性
- en: The SFT step implicitly answers an unasked question—why do we need pre-training
    and three-step FT to train our model? The reason is that generating a human-labeled
    training dataset is not scalable and represents a major bottleneck. For example,
    the pre-training dataset can have over a trillion tokens; generating prompts and
    their respective responses of such magnitude with human labelers is infeasible.
    Therefore, we need pre-training to provide the LLM with a solid foundation, which
    we can fine-tune with a much smaller dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: SFT步骤隐含地回答了一个未被提问的问题——为什么我们需要预训练和三步FT来训练我们的模型？原因在于，生成一个人工标注的训练数据集是不可扩展的，并且是一个主要的瓶颈。例如，预训练数据集可以包含超过一万亿个标记；使用人工标注员生成如此规模的提示及其相应的响应是不现实的。因此，我们需要预训练为LLM提供一个坚实的基础，而我们可以使用一个较小的数据集来进行微调。
- en: The second step is `[(A, B), (A, C), (B, C)]`. This dataset trains the RM, which
    is based on the fine-tuned LLM. Its output next-token classifier is replaced with
    a randomly initialized regression layer, which outputs the predicted scalar score
    of a given response. The RM computes the scores of both responses of each pair.
    The difference between them participates in the loss function. This is an example
    of **transfer learning** (**TL**), which aims to train the new regression layer
    on top of the original LLM.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是`[(A, B), (A, C), (B, C)]`。该数据集训练RM，基于微调后的LLM。其输出的下一个标记分类器被替换为一个随机初始化的回归层，该层输出给定响应的预测标量分数。RM计算每对响应的分数，它们之间的差异参与损失函数的计算。这是**迁移学习**（**TL**）的一个例子，旨在在原始LLM的基础上训练新的回归层。
- en: The third step is to train the LLM using RL with the RM and **proximal policy
    optimization** (**PPO**) (*Figure 8**.14*—right).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是使用RL通过RM和**近端策略优化**（**PPO**）来训练LLM（*图 8.14*—右侧）。
- en: A recap of RL
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: RL的回顾
- en: To understand the third step, let’s recap our introduction to RL from [*Chapter
    1*](B19627_01.xhtml#_idTextAnchor016). We have a system of environment and an
    agent. The agent can take one of a number of actions that change the state of
    the environment. The environment reacts to the agent’s actions and provides its
    modified state and reward (or penalty) signals that help the agent to decide its
    next action. The decision-making algorithm of the agent is called a policy. The
    agent’s goal is to maximize the total rewards received throughout the training
    episodes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解第三步，我们来回顾一下在[*第1章*](B19627_01.xhtml#_idTextAnchor016)中的强化学习介绍。我们有一个环境系统和一个智能体。智能体可以采取多种行动之一，这些行动会改变环境的状态。环境会对智能体的行动作出反应，并提供修改后的状态和奖励（或惩罚）信号，帮助智能体决定下一步的行动。智能体的决策算法称为策略。智能体的目标是最大化在整个训练过程中所获得的总奖励。
- en: In this scenario, the policy of the agent is represented by the fine-tuned LLM.
    The token vocabulary represents the actions it can take—that is, the agent’s action
    is to select the next token in the sequence. The environment presents the LLM
    with a random prompt, and the agent (LLM) generates a response. Then, the RM,
    part of the environment, scores the generated response. The RM score is the reward
    sent to the agent and serves to update its parameters.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，智能体的策略由经过微调的LLM表示。令牌词汇表表示智能体可以采取的行动——也就是说，智能体的行动是选择序列中的下一个令牌。环境向LLM提供一个随机提示，智能体（LLM）生成一个响应。然后，环境的一部分RM对生成的响应进行评分。RM分数是发送给智能体的奖励，并用于更新其参数。
- en: In the next section, we’ll discuss what makes LLMs different from other models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论LLM与其他模型的不同之处。
- en: Emergent abilities of LLMs
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的涌现能力
- en: 'In this section, we’ll discuss the phenomenon of **emergent abilities** of
    LLMs, first summarized in [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682).
    The paper defines emergent abilities as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论LLM的**涌现能力**现象，该现象首次在[https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)中总结。该论文将涌现能力定义如下：
- en: An ability is emergent if it is not present in smaller models but is present
    in larger models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个能力在较小的模型中不存在，但在较大的模型中存在，则该能力为涌现能力。
- en: These abilities represent a qualitative difference between large and small language
    models, which cannot be predicted by extrapolation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些能力代表了大语言模型与小语言模型之间的质的差异，这种差异无法通过外推预测。
- en: 'We’ll start with the ability known as **few-shot prompting** (or **in-context
    learning**), popularized by GPT-3\. Here, the initial user prompt is an instruction
    the LLM has to follow through its response without any additional training. The
    prompt itself may describe with natural text one or more training examples (hence,
    the term *few-shot*). This is the only context that the LLM can use for training
    before generating its response. The following diagram shows an example of a few-shot
    prompt:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从被称为**少量示例提示**（或**上下文学习**）的能力开始，这种能力由GPT-3普及。在这里，初始用户提示是LLM必须通过响应遵循的指令，且无需任何额外的训练。提示本身可能用自然语言描述一个或多个训练示例（因此，称为*少量示例*）。这是LLM在生成响应之前可以用来进行训练的唯一上下文。以下图表展示了一个少量示例提示的例子：
- en: '![Figure 8.15 – An example of a few-shot prompt (inspired by https://arxiv.org/abs/2206.07682)](img/B19627_08_15.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.15 – 一个少量示例提示的例子（灵感来自 https://arxiv.org/abs/2206.07682）](img/B19627_08_15.jpg)'
- en: Figure 8.15 – An example of a few-shot prompt (inspired by https://arxiv.org/abs/2206.07682)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – 一个少量示例提示的例子（灵感来自 [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)）
- en: 'Next, let’s discuss the ability of LLMs to solve complex multi-step reasoning
    tasks with the help of а **chain-of-thought** (**CoT**) prompting strategy (*Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models*, https://arxiv.org/abs/2201.11903).
    This type of prompt presents the LLM with a series of intermediate steps that
    can guide the model to reach the final task answer. The following diagram shows
    a comparison between regular and CoT prompts:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论大型语言模型（LLM）在**思维链**（**CoT**）提示策略的帮助下解决复杂多步骤推理任务的能力（*思维链提示引发大型语言模型的推理能力*，https://arxiv.org/abs/2201.11903）。这种提示为LLM提供了一系列中间步骤，可以引导模型达到最终任务答案。以下图表展示了常规提示和思维链提示的比较：
- en: '![Figure 8.16 – Left: regular one-shot prompt; right: CoT one-shot prompt (inspired
    by https://arxiv.org/abs/2201.11903)](img/B19627_08_16.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16 – 左：常规单次提示；右：CoT单次提示（灵感来自 https://arxiv.org/abs/2201.11903）](img/B19627_08_16.jpg)'
- en: 'Figure 8.16 – Left: regular one-shot prompt; right: CoT one-shot prompt (inspired
    by https://arxiv.org/abs/2201.11903)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 左：常规的一次性提示；右：链式思维一次性提示（灵感来自 [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)）
- en: It is speculated that this ability is obtained by including source code in the
    training data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有人推测，这种能力是通过将源代码包括在训练数据中获得的。
- en: Let’s also note that the alignment tuning we discussed in the *FT with RLHF*
    section is also an emergent ability, as it only improves the performance of large
    models.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，我们在 *FT with RLHF* 部分讨论的对齐微调也是一种紧急能力，因为它仅仅提升了大模型的性能。
- en: 'The following diagram shows how the performance on various tasks significantly
    improves with the scale of the model:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了随着模型规模的增加，模型在各种任务上的性能如何显著提升：
- en: '![Figure 8.17 – Emergent abilities are only present in large-scale models (source:
    https://arxiv.org/abs/2206.07682)](img/B19627_08_17.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17 – 紧急能力仅出现在大规模模型中 (来源：[https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682))](img/B19627_08_17.jpg)'
- en: 'Figure 8.17 – Emergent abilities are only present in large-scale models (source:
    [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682))'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 – 紧急能力仅出现在大规模模型中 (来源：[https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682))
- en: 'The *x* axis shows the training computational time for each model (measured
    in FLOPS), and the *y* axis shows the model accuracy. The graphs show the model
    accuracy on three different benchmarks:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* 轴显示每个模型的训练计算时间（以 FLOPS 计），*y* 轴显示模型的准确性。图表显示了模型在三个不同基准测试上的准确性：'
- en: An arithmetic benchmark that tests multiplication of 2-digit numbers, as well
    as the addition and subtraction of 3-digit numbers
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个算术基准，测试 2 位数乘法，以及 3 位数的加法和减法
- en: 57 college-level tests covering a range of topics, including math, history,
    law, and more
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涵盖多个主题的 57 项大学级测试，包括数学、历史、法律等
- en: Chain-of-thought versus regular prompt on math word problems, like the one described
    in *Figure 8**.16*
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数学应用题上，链式思维与常规提示的比较，例如 *图 8.16* 所描述的那种
- en: This concludes our theoretical introduction to LLMs. Next, let’s see how to
    use them in practice.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对大语言模型（LLM）的理论介绍。接下来，让我们看看如何在实践中使用它们。
- en: Introducing Hugging Face Transformers
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Hugging Face Transformers
- en: So far, we have discussed in depth the architecture and training properties
    of LLMs. But the sad truth is that these models are so large it is unlikely that
    you or I would build one from scratch. Instead, we’ll probably use a pre-trained
    model. In this section, we’ll see how to do that with the Hugging Face Transformers
    library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    As the name suggests, its focus is the transformer architecture. It supports three
    different backends—PyTorch, TensorFlow, and JAX (as usual, we’ll focus on PyTorch).
    It is open source and available for commercial use. The company behind it, Hugging
    Face, also develops the Hugging Face Hub—a complementary service to the library
    cloud-based platform. It supports hosting and/or running Git repositories (such
    as GitHub), transformer models, datasets, and web applications (intended for **proof-of-concept**
    (**POC**) demos of ML applications). With that, let’s proceed with our first example.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经深入讨论了 LLM 的架构和训练特性。但可悲的事实是，这些模型如此庞大，以至于你或我都不太可能从零开始构建一个。相反，我们很可能会使用一个预训练模型。在本节中，我们将看到如何使用
    Hugging Face Transformers 库（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）。顾名思义，它的重点是
    transformer 架构。它支持三种不同的后端——PyTorch、TensorFlow 和 JAX（像往常一样，我们将重点讨论 PyTorch）。它是开源的，可以用于商业用途。背后的公司
    Hugging Face 还开发了 Hugging Face Hub——这是一个与库配套的云平台服务。它支持托管和/或运行 Git 仓库（如 GitHub）、transformer
    模型、数据集和 Web 应用程序（用于 **概念验证** (**POC**) 的机器学习应用演示）。好了，让我们继续我们的第一个示例。
- en: 'We’ll start with a basic use case—we’ll load a pre-trained Llama 2 chat 7B
    model and use it to generate a response to the user’s prompt:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个基本的使用案例开始——加载一个预训练的 Llama 2 chat 7B 模型，并使用它来生成对用户提示的回答：
- en: 'First, we add in the `import` statements:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加入 `import` 语句：
- en: '[PRE0]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we define the model’s name in a variable:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在一个变量中定义模型的名称：
- en: '[PRE1]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Every transformer model has a unique identifier, which works for the Hugging
    Face model hub. The Hub hosts all models, and the library can automatically download
    the model weights behind the scenes. In this case, we use the smallest Llama 2
    7B RLHF-optimized model to preserve computational resources.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个 transformer 模型都有一个唯一的标识符，这对于 Hugging Face 模型中心（Hub）有效。该 Hub 托管所有模型，库可以在后台自动下载模型权重。在这种情况下，我们使用的是最小的
    Llama 2 7B RLHF 优化模型，以节省计算资源。
- en: 'Next, let’s load the model tokenizer:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们加载模型分词器：
- en: '[PRE2]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The various LLM models use different tokenizers. The `AutoTokenizer` instance
    can select the right one based on the model identifier.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 各种 LLM 模型使用不同的分词器。`AutoTokenizer` 实例可以根据模型标识符选择正确的分词器。
- en: 'Let’s see the `tokenizer` properties by printing it with `print(tokenizer)`:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过 `print(tokenizer)` 打印分词器来查看其属性：
- en: '[PRE3]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The tokenizer is based on a byte-level **byte-pair encoding** (**BPE**). This
    output gives us useful information about the token vocabulary size, special tokens,
    and other properties.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分词器基于字节级**字节对编码**（**BPE**）。该输出提供了有关标记词汇大小、特殊标记和其他属性的有用信息。
- en: 'Then, we create a `pipeline` instance:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 `pipeline` 实例：
- en: '[PRE4]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The pipeline abstraction makes it easy to use the models for inference. The
    `task` parameter determines the type of task to solve. The library supports multiple
    tasks, also covering images and audio. `pipeline` will return different objects,
    depending on the task. It also takes care of downloading and initializing the
    model. In addition, we set the datatype to `torch.bfloat16` to reduce the memory
    footprint. The `device_map='auto'` parameter allows the Accelerate library ([https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate))
    to run the model across any distributed configuration automatically.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流水线抽象使得使用模型进行推理变得简单。`task` 参数决定了解决任务的类型。该库支持多种任务，还涵盖了图像和音频。`pipeline` 会根据任务返回不同的对象。它还负责下载和初始化模型。此外，我们将数据类型设置为
    `torch.bfloat16` 以减少内存占用。`device_map='auto'` 参数允许 Accelerate 库（[https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate)）自动在任何分布式配置中运行模型。
- en: 'We can see the model definition with the following command: `print(text_gen_pipeline.model)`.
    For example, the command output for the largest 70B Llama 2 model, `Llama-2-70b-hf`,
    is this:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令查看模型定义：`print(text_gen_pipeline.model)`。例如，最大 70B Llama 2 模型 `Llama-2-70b-hf`
    的命令输出如下：
- en: '[PRE5]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To fit the page line length, I have modified the original output: `in` stands
    for `in_features`, `out` stands for `out_features`, and all linear layers have
    an additional `bias=False` parameter. The token vocabulary size is 32,000, and
    the embedding size (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/844.png))
    is 8,192\. The model has 80 identical decoder blocks (`LlamaDecoderLayer`). Each
    block contains a'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了适应页面行长，我修改了原始输出：`in` 代表 `in_features`，`out` 代表 `out_features`，所有线性层都有一个额外的
    `bias=False` 参数。标记词汇大小为 32,000，嵌入大小 (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/844.png))
    为 8,192。模型有 80 个相同的解码器块（`LlamaDecoderLayer`）。每个块包含一个
- en: self-attention sublayer (`*_proj` are the projections), a FFN with a single
    hidden layer (`LlamaMLP`), rotary embeddings (`LlamaRotaryEmbedding`), `LlamaRMSNorm`),
    and SiLU activation (`SiLUActivation`). Let’s note that the activation differs
    from the SwiGLU activation defined in the paper.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力子层（`*_proj` 为投影）、一个具有单隐藏层的 FFN（`LlamaMLP`）、旋转嵌入（`LlamaRotaryEmbedding`）、`LlamaRMSNorm`）以及
    SiLU 激活（`SiLUActivation`）。需要注意的是，该激活函数与论文中定义的 SwiGLU 激活函数有所不同。
- en: 'Then, we run the inference:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们运行推理：
- en: '[PRE6]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, `text_inputs` is the user prompt, which serves as the initial input sequence.
    The `num_return_sequences=2` parameter indicates that model will generate two
    separate responses (more on that later). Here’s the first response:'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`text_inputs` 是用户的提示，作为初始输入序列。`num_return_sequences=2` 参数表示模型将生成两个独立的响应（稍后会详细介绍）。这是第一个响应：
- en: '[PRE7]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s analyze the rest of the arguments of the `text_gen_pipeline` call, as
    they all relate to the strategy of generating new tokens. The LLM ends with a
    softmax operation, which outputs a probability distribution over all tokens of
    the vocabulary. The simplest way to select the next token is a greedy strategy,
    which always takes the one with the highest probability. However, this is often
    suboptimal because it can hide high-probability words behind low-probability ones.
    To clarify, a token might be assigned a low probability at the current state of
    the generated sequence, and another token would be selected in its place. This
    means that the potential sequence, which includes the current low-probability
    token, will not exist. Therefore, even if it had high-probability tokens down
    the line, we would never know because the low-probability token blocks it from
    further exploration. One way to solve this is with a `do_sample=True`. In this
    case, the algorithm takes the probability of the entire current sequence rather
    than just the probability of the latest token. Therefore, the new token will be
    the one that maximizes the overall probability of the sequence instead of its
    local probability. The `num_beams=2` parameter indicates that the algorithm always
    keeps the two sequences (beams) with the highest probability. Since we can have
    more than one output sequence, the `num_return_sequences=2` parameter indicates
    the number of sequences to return. For example, if `num_beams=5` and `num_return_sequences=3`,
    the algorithm will return the three highest-probability sequences out of all five
    available (`num_return_sequences > num_beams` are invalid arguments). The `early_stopping=True`
    parameter indicates that the generation is finished when all beam hypotheses reach
    the end-of-sequence (`[EOS]`) token. The `top_k=10` parameter indicates that the
    algorithm will only sample the top 10 highest-probability tokens, regardless of
    their sequence probabilities. `top_p=0.9` is like `top_k`, but instead of sampling
    only from the most likely *k* tokens, it selects from the smallest possible set
    of tokens whose combined probability exceeds the probability *p*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下`text_gen_pipeline`调用中的其余参数，因为它们都与生成新标记的策略相关。LLM的输出以softmax操作结束，该操作对词汇表中的所有标记输出一个概率分布。选择下一个标记的最简单方法是贪心策略，它总是选择概率最高的那个。然而，这通常不是最优的，因为它可能会把高概率的单词隐藏在低概率的单词后面。为了说明这一点，某个标记在当前生成序列的状态下可能会被分配一个低概率，然后会选择另一个标记替代它。这意味着包含当前低概率标记的潜在序列将不会存在。因此，即使它在后续有高概率的标记，我们也永远无法知道，因为低概率标记阻止了它的进一步探索。解决这个问题的一种方法是使用`do_sample=True`。在这种情况下，算法会考虑整个当前序列的概率，而不仅仅是最新标记的概率。因此，新标记将是最大化整个序列概率的那个，而不是局部概率的最大值。`num_beams=2`参数表示算法始终保留两个具有最高概率的序列（beams）。因为我们可以有多个输出序列，`num_return_sequences=2`参数表示返回的序列数量。例如，如果`num_beams=5`并且`num_return_sequences=3`，算法将返回五个可用序列中三个最高概率的序列（`num_return_sequences
    > num_beams`是无效的参数）。`early_stopping=True`参数表示当所有beams的假设都到达序列结束标记（`[EOS]`）时，生成过程结束。`top_k=10`参数表示算法只会从概率最高的前10个标记中进行采样，而不考虑它们的序列概率。`top_p=0.9`类似于`top_k`，但它不是只从最可能的*k*个标记中进行采样，而是从一组最小的标记中选择，这些标记的总概率超过*p*。
- en: This concludes our introduction to the Transformers library and the whole chapter.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对Transformers库的介绍以及整个章节的内容。
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: LLMs are very large transformers with various modifications to accommodate the
    large size. In this chapter, we discussed these modifications, as well as the
    qualitative differences between LLMs and regular transformers. First, we focused
    on their architecture, including more efficient attention mechanisms such as sparse
    attention and prefix decoders. We also discussed the nuts and bolts of the LLM
    architecture. Next, we surveyed the latest LLM architectures with special attention
    given to the GPT and LlaMa series of models. Then, we discussed LLM training,
    including training datasets, the Adam optimization algorithm, and various performance
    improvements. We also discussed the RLHF technique and the emergent abilities
    of LLMs. Finally, we introduced the Hugging Face Transformers library.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: LLM（大型语言模型）是非常庞大的变压器模型，具有各种修改以适应其庞大的规模。在这一章中，我们讨论了这些修改，以及LLM和普通变压器之间的质的差异。首先，我们重点讲解了它们的架构，包括更高效的注意力机制，如稀疏注意力和前缀解码器。我们还讨论了LLM架构的细节。接下来，我们回顾了最新的LLM架构，特别关注了GPT和LlaMa系列模型。然后，我们讨论了LLM的训练过程，包括训练数据集、Adam优化算法以及各种性能提升。我们还讨论了RLHF技术和LLM的突现能力。最后，我们介绍了Hugging
    Face Transformers库。
- en: In the next chapter, we’ll discuss transformers for **computer vision** (**CV**),
    multimodal transformers, and we’ll continue our introduction to the Transformers
    library.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论**计算机视觉**（**CV**）的变压器模型、多模态变压器，并继续介绍Transformers库。
