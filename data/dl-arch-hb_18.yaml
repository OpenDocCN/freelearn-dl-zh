- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Exploring the DataRobot AI Platform
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 DataRobot AI 平台
- en: In this chapter, we will turn our focus to the DataRobot AI platform, a paid
    software platform that provides a powerful toolkit for deep learning use cases.
    DataRobot allows its users to streamline the complex stages of the machine learning
    life cycle. It presents an intuitive interface for data scientists, engineers,
    and researchers who wish to harness the power of machine learning for their projects
    and businesses. As we delve into the workings of DataRobot, you will learn how
    it simplifies and accelerates the creation, training, deployment, and government
    of intricate deep learning models. Thanks to features designed for automation
    and ease of use, it empowers users to focus on what truly matters—extracting significant
    value from their machine learning applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将重点讨论 DataRobot AI 平台，这是一款付费软件平台，提供强大的工具集来支持深度学习应用案例。DataRobot 允许用户简化机器学习生命周期中的复杂阶段。它为数据科学家、工程师和研究人员提供了一个直观的界面，帮助他们在项目和业务中利用机器学习的强大功能。当我们深入了解
    DataRobot 的工作原理时，你将学习到它如何简化并加速复杂深度学习模型的创建、训练、部署和管理。凭借为自动化和易用性设计的功能，它使用户能够专注于真正重要的事情——从他们的机器学习应用中提取重要的价值。
- en: Our exploration will highlight the key functionalities of DataRobot, underlining
    its potential as a catalyst in the evolution of deep learning solutions. DataRobot
    aspires to offer a combination of automation, collaboration, and scalability for
    machine learning use cases, making it also a noteworthy tool in the deep learning
    domain.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索将重点介绍 DataRobot 的关键功能，强调其在深度学习解决方案演进中的潜力。DataRobot 致力于为机器学习应用案例提供自动化、协作和可扩展性的结合，使其在深度学习领域也成为一个值得关注的工具。
- en: 'Specifically, we will cover the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下内容：
- en: A high-level look into what the DataRobot AI platform provides
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级概述：DataRobot AI 平台提供的功能
- en: Preparing data with DataRobot
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataRobot 准备数据
- en: Executing modeling experiments with DataRobot
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataRobot 执行建模实验
- en: Deploying a deep learning blueprint
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署深度学习蓝图
- en: Governing a deployed deep learning blueprint
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理已部署的深度学习蓝图
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will have a practical topic in this chapter to make predictions using a
    DataRobot deployed model. We will be using Python 3.10 and we will require the
    following Python libraries to be installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨一个实用的话题，通过使用 DataRobot 部署的模型进行预测。我们将使用 Python 3.10，并需要安装以下 Python 库：
- en: '`datarobotx==0.1.17`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datarobotx==0.1.17`'
- en: '`pandas==2.0.3`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas==2.0.3`'
- en: The code files are available on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_18](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_18),
    and the dataset can be downloaded from [https://www.kaggle.com/datasets/dicksonchin93/datarobot-compatible-house-pricing-dataset](https://www.kaggle.com/datasets/dicksonchin93/datarobot-compatible-house-pricing-dataset).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可以在 GitHub 上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_18](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_18)，数据集可以从
    [https://www.kaggle.com/datasets/dicksonchin93/datarobot-compatible-house-pricing-dataset](https://www.kaggle.com/datasets/dicksonchin93/datarobot-compatible-house-pricing-dataset)
    下载。
- en: 'Additionally, a paid or free trial account is needed to access DataRobot. To
    subscribe for a trial account, do the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，需要一个付费或免费试用账户才能访问 DataRobot。订阅试用账户的步骤如下：
- en: Visit the DataRobot website at [https://www.datarobot.com/trial/](https://www.datarobot.com/trial/).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 DataRobot 网站：[https://www.datarobot.com/trial/](https://www.datarobot.com/trial/)。
- en: Fill up your credentials under the **Start For Free** interface on the right
    side of the web page and click on the **Submit** button.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网页右侧的 **Start For Free** 界面中填写你的凭证，并点击 **Submit** 按钮。
- en: A high-level look into what the DataRobot AI platform provides
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级概述：DataRobot AI 平台提供的功能
- en: The DataRobot AI platform provides data ingestion, data preparation, data insights,
    model development, model evaluation, model insights and analysis, model deployment,
    and model governance through model monitoring and model maintenance tools that
    work seamlessly with each other. While DataRobot streamlines the deep learning
    life cycle, it is important to note that the planning stage still requires human
    input to define the goals and scope of the project. Additionally, you are still
    required to consume the insights, reports, and results made easy for you to obtain.
    Ultimately, this means that such a platform is a tool that can assist any machine
    learning practitioner instead of being a replacement for data scientists, machine
    learning engineers, machine learning researchers, or data analysts. Think of AI
    platforms such as DataRobot as being powerful calculators that can help you solve
    complex math problems quickly and accurately. But just like a calculator can’t
    think for you, DataRobot can’t replace the expertise and creativity of a data
    scientist, engineer, analyst, or researcher.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot AI 平台通过模型监控和模型维护工具提供数据摄取、数据准备、数据洞察、模型开发、模型评估、模型洞察与分析、模型部署和模型治理，这些工具相互无缝配合。虽然
    DataRobot 简化了深度学习生命周期，但需要注意的是，规划阶段仍然需要人工输入，以定义项目的目标和范围。此外，你仍然需要利用易于获得的洞察、报告和结果。最终，这意味着这样的平台是一个可以协助任何机器学习从业者的工具，而不是替代数据科学家、机器学习工程师、机器学习研究人员或数据分析师。可以把像
    DataRobot 这样的 AI 平台看作是强大的计算器，可以帮助你快速而准确地解决复杂的数学问题。但就像计算器不能代替你的思考一样，DataRobot 也无法替代数据科学家、工程师、分析师或研究人员的专业知识和创造力。
- en: Some of the tools DataRobot offers are built to be extensible, composable, and
    flexible to add your own code or components, and they don’t tie you into the only
    things that the platform provides. Additionally, some components cover a wide
    range of methods, so you don’t need to worry about doing any customization. Effectively,
    this means you get the benefit of executing projects reliably fast while still
    holding customization power.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 提供的某些工具被设计为可扩展、可组合和灵活，允许你添加自己的代码或组件，它们不会将你束缚在平台仅提供的功能上。此外，一些组件覆盖了广泛的方法，因此你不需要担心进行任何定制化。实际上，这意味着你可以在可靠的速度下执行项目，同时仍然保持定制化的能力。
- en: 'Before we dive into the components DataRobot offers, there’s more key information
    that can help you understand what the platform is capable of:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解 DataRobot 提供的组件之前，还有一些关键信息可以帮助你了解该平台的功能：
- en: '**Platform hosting options**: The DataRobot AI platform offers a cloud-hosted
    application. If your business has data privacy and security concerns, DataRobot
    also offers the option to host the AI platform as a privately hosted instance
    in the confines of your own infrastructure.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台托管选项**：DataRobot AI 平台提供云托管应用。如果你的企业有数据隐私和安全方面的顾虑，DataRobot 还提供将 AI 平台作为私有托管实例，在你自己的基础设施中进行托管的选项。'
- en: '**Scalability and collaborative nature**: Whether you’re a single user or a
    team, DataRobot is designed to scale with your needs. Most of the tool components
    provided by DataRobot can be shared with multiple users, which enables collaboration
    between different users.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和协作性**：无论你是单个用户还是团队，DataRobot 都设计为可以根据你的需求进行扩展。DataRobot 提供的大多数工具组件都可以与多个用户共享，这使得不同用户之间的协作成为可能。'
- en: '`datarobot`, which is installable through `PyPI`. Almost everything you see
    in the web UI is available in the API interface and the Python API client. Additionally,
    the Notebooks feature provides a flexible and interactive environment where data
    scientists can manually perform complex data analysis, create machine learning
    models, and prototype data manipulations alongside the other DataRobot features
    through the Python API client easily. It enhances the user experience by allowing
    the flexibility of defaulting to using traditional Python code.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datarobot`，可以通过 `PyPI` 安装。几乎你在 Web UI 中看到的所有内容都可以通过 API 接口和 Python API 客户端访问。此外，Notebooks
    功能提供了一个灵活和互动的环境，在这个环境中，数据科学家可以手动进行复杂的数据分析、创建机器学习模型，并通过 Python API 客户端轻松地与 DataRobot
    其他功能一起原型化数据操作。它通过允许默认使用传统的 Python 代码来增强用户体验。'
- en: '**The transition to a use case-focused asset management**: The ML or DL life
    cycle introduced in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning
    Life Cycle*, is an iterative process. This means a DL use case will involve a
    lot of data, data versions, model development experimentations, applications that
    utilize a deployed model, and notebooks being created. The **Workbench** feature
    in DataRobot is meant to support this process by managing many use case-related
    assets mentioned in a single interface, making it easier to realize value through
    the use case. However, at the time of writing this book, Workbench does not comprehensively
    support all the features provided by the traditional, separately managed DataRobot
    projects. It will be updated to support the full suite of features in time. You
    can manage projects separately with DataRobot Classic features. *Figure 18**.1*
    shows an example flow of how a typical machine learning practitioner would navigate
    the platform:![](img/B18187_18_01.jpg)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过渡到以用例为中心的资产管理**：在[*第1章*](B18187_01.xhtml#_idTextAnchor015)中介绍的机器学习（ML）或深度学习（DL）生命周期是一个迭代过程。也就是说，一个深度学习用例会涉及大量的数据、数据版本、模型开发实验、使用已部署模型的应用程序和创建的笔记本。DataRobot
    中的**工作台**功能旨在通过在单一界面中管理许多与用例相关的资产，支持这一过程，使得通过用例实现价值变得更加容易。然而，在编写本书时，工作台并未全面支持传统的、独立管理的
    DataRobot 项目提供的所有功能。未来将会更新以支持完整的功能套件。你可以使用 DataRobot Classic 功能单独管理项目。*图 18.1*
    展示了典型机器学习从业者如何导航平台的示例流程：![](img/B18187_18_01.jpg)'
- en: 'To PD: This image has been sent for redraw.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给 PD：该图片已发送进行重绘。
- en: Figure 18.1 – Example Workbench workflow
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1 – 示例工作台工作流
- en: Now, we are ready to dive into the relevant supported features for deep learning
    use cases, starting with the data preparation component.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好深入了解深度学习用例相关的支持功能，首先从数据准备组件开始。
- en: Preparing data with DataRobot
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataRobot 准备数据
- en: The first part of what the platform offers is the data preparation component.
    DataRobot simplifies the data preparation process by offering a range of features
    to streamline data ingest, cleaning, transformation, and integration. Let’s dive
    into these features in detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 平台提供的第一个部分是数据准备组件。DataRobot 通过提供一系列功能来简化数据摄取、清洗、转换和集成过程。让我们详细了解这些功能。
- en: Ingesting data for deep learning model development
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型开发的数据摄取
- en: The development of deep learning models in DataRobot begins with the pivotal
    step of data ingestion. This process allows you to directly import your data from
    various sources, including cloud storage (such as AWS S3), Google Cloud Storage,
    local files, or databases such as PostgreSQL, Oracle, and SQL Server. The platform
    accepts diverse file formats, including CSV, XLSX, and ZIP files. Additionally,
    the platform supports image, text, document, geospatial, numerical, categorical,
    and summarized categorical data through secondary datasets as input data types.
    For the target data types, the platform supports numerical, categorical, and multilabel
    data types along with data with no targets for unsupervised learning. This sets
    the stage for regression, binary classification, multiclass classification, multilabel
    classification, unsupervised anomaly detection, and unsupervised clustering.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataRobot 中，深度学习模型的开发始于数据摄取这一关键步骤。这个过程允许你直接从各种来源导入数据，包括云存储（如 AWS S3）、Google
    Cloud Storage、本地文件或数据库（如 PostgreSQL、Oracle 和 SQL Server）。该平台接受多种文件格式，包括 CSV、XLSX
    和 ZIP 文件。此外，平台支持通过二级数据集作为输入数据类型的图像、文本、文档、地理空间、数值、类别和汇总类别数据。对于目标数据类型，平台支持数值型、类别型和多标签数据类型，以及无目标数据的无监督学习。这为回归、二元分类、多类分类、多标签分类、无监督异常检测和无监督聚类奠定了基础。
- en: Notably, image data type, text data type, and document data type are the core
    unstructured input data types for building deep learning use cases in DataRobot.
    Image data and document data are supported by being encoded as a base64 string
    under the hood. However, the dataset to be ingested itself can be structured to
    be zipped folders with images or documents, where the folder names are the classes
    or target names. Text data can naturally exist under tabular data in a column,
    encoded in formats such as CSV. Additionally, DataRobot automatically creates
    useful features if there are secondary datasets or datetime feature columns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，图像数据类型、文本数据类型和文档数据类型是DataRobot中构建深度学习用例的核心非结构化输入数据类型。图像数据和文档数据通过底层编码为base64字符串来支持。然而，待导入的数据集本身可以是已压缩的图像或文档文件夹，其中文件夹名称为类别或目标名称。文本数据自然可以以表格数据的形式存在于某一列中，并以CSV等格式编码。此外，如果存在二级数据集或日期时间特征列，DataRobot会自动创建有用的特征。
- en: 'There are two paths you can take to ingest data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种路径可以导入数据：
- en: Through project creation, tied closely to the model development process.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过项目创建，这与模型开发过程紧密相连。
- en: Through the **AI Catalog** feature, which allows you to share the dataset independently.
    This can be subsequently used to create an experiment in a use case or a project
    independently.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**AI目录**功能，你可以独立共享数据集，之后可以用于在用例或项目中独立创建实验。
- en: Let’s explore the second approach, as it is a more responsible and reliable
    way of managing data used for model development.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索第二种方法，因为它是一种更负责任且可靠的数据管理方式，用于模型开发。
- en: Using DataRobot to ingest an image and text dataset
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用DataRobot导入图像和文本数据集
- en: 'We will be tackling the use case of predicting housing prices with multimodal
    data that consists of image data, text data, date data, categorical data, and
    numerical data. Let’s start the step-by-step tutorial:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理一个使用多模态数据预测房价的案例，数据包括图像数据、文本数据、日期数据、分类数据和数值数据。让我们开始一步步的教程：
- en: 'Start by creating a use case by clicking on the **+ Create a new Use Case**
    button shown in *Figure 18**.2*. The page shown in *Figure 18**.2* will also be
    the landing page after you enter the DataRobot web app and log in through [app.datarobot.com](http://app.datarobot.com):'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击**+ 创建新用例**按钮开始创建用例，按钮如*图 18.2*所示。*图 18.2*中的页面也是你进入DataRobot web应用并通过[app.datarobot.com](http://app.datarobot.com)登录后的着陆页面：
- en: '![Figure 18.2 – Creating a use case screen in Workbench](img/B18187_18_2.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.2 – 在Workbench中创建用例的界面](img/B18187_18_2.jpg)'
- en: Figure 18.2 – Creating a use case screen in Workbench
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2 – 在Workbench中创建用例的界面
- en: 'Now, click on **DataRobot Classic** in the top-right and then on the **AI Catalog**
    tab in the top-left of the interface. Then, click on the **Add to catalog** button
    in the top-left of the interface and then **Local File**, as depicted in *Figure
    18**.3*:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，点击右上角的**DataRobot Classic**，然后点击界面左上角的**AI目录**选项卡。接着，点击界面左上角的**添加到目录**按钮，再点击**本地文件**，如*图
    18.3*所示：
- en: '![Figure 18.3 – Adding the dataset to the catalog interface in DataRobot](img/B18187_18_3.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.3 – 在DataRobot中将数据集添加到目录的界面](img/B18187_18_3.jpg)'
- en: Figure 18.3 – Adding the dataset to the catalog interface in DataRobot
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3 – 在DataRobot中将数据集添加到目录的界面
- en: Upload the `trulia_pricing_dataset.zip` file provided in the code repo. This
    dataset is a ZIP file that consists of a single CSV file, which contains the raw
    data, and additionally, raw image files that are mapped to one of the columns
    in the CSV through its relative path in the zipped file. From here, an uploaded
    dataset in the AI Catalog can be managed independently and shared separately.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传代码仓库中提供的`trulia_pricing_dataset.zip`文件。该数据集是一个ZIP文件，包含一个CSV文件，其中包含原始数据，此外还有原始图像文件，这些图像文件通过其在ZIP文件中的相对路径映射到CSV中的某一列。从这里，AI目录中的上传数据集可以独立管理并单独共享。
- en: 'Once it is created, go into Workbench in the top-right again, go into the use
    case we created, and rename the use case to a suitable name, such as `Pricing
    Prediction`. Then, click on the **Add new** button dropdown in the top-right of
    the interface and click on **Add datasets**, as depicted in *Figure 18**.4*:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建完成后，再次进入右上角的Workbench，进入我们创建的用例，并将用例重命名为合适的名称，如`房价预测`。然后，点击界面右上角的**添加新**按钮下拉菜单，点击**添加数据集**，如*图
    18.4*所示：
- en: '![Figure 18.4 – Pricing prediction use case in the Workbench interface](img/B18187_18_4.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.4 – 在Workbench界面中的房价预测用例](img/B18187_18_4.jpg)'
- en: Figure 18.4 – Pricing prediction use case in the Workbench interface
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4 – 工作台界面中的定价预测用例
- en: 'Choose `trulia_pricing_dataset` from the **Data Registry** page and click on
    **Add to Use case**, as depicted in *Figure 18**.5*:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**数据注册表**页面选择`trulia_pricing_dataset`并点击**添加到用例**，如*图18.5*所示：
- en: '![Figure 18.5 – Adding the dataset to the use case](img/B18187_18_5.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图18.5 – 将数据集添加到用例](img/B18187_18_5.jpg)'
- en: Figure 18.5 – Adding the dataset to the use case
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.5 – 将数据集添加到用例
- en: Now, we are ready to move into the EDA part of DataRobot.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好进入DataRobot的EDA部分。
- en: Exploratory analysis of the data
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的探索性分析
- en: DataRobot provides a two-step experience in performing **exploratory data analysis**
    (**EDA**) on an at most 500MB subset of the dataset. The first step is EDA, which
    is executed before a project or experiment type is determined, and the second
    step is done after. Standard EDA techniques are provided. These include histograms
    for numerical and categorical data, frequency distribution for the top 50 items
    for categorical data, duplicate counts, missing value counts, disguised missing
    value detection, excessive zero value detection, target leakage, numerical value
    aggregates, outlier rows, univariate feature correlation to the target, and a
    feature association matrix that measures mutual information. For images and text
    data specifically, DataRobot Classic shows a general sample of the data in *step
    1* and sorted by the target values or value ranges group in *step 2*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot提供了一个两步体验，用于执行最多500MB数据子集的**探索性数据分析**（**EDA**）。第一步是EDA，在确定项目或实验类型之前执行，第二步则在之后进行。提供了标准的EDA技术，包括数值和分类数据的直方图，分类数据前50项的频率分布，重复计数、缺失值计数、伪缺失值检测、过度零值检测、目标泄漏、数值汇总、离群行、单变量特征与目标的相关性，以及一个衡量互信息的特征关联矩阵。特别是对于图片和文本数据，DataRobot
    Classic在*第1步*显示数据的总体样本，并在*第2步*按目标值或值范围组进行排序。
- en: '*Figure 18**.6* shows the EDA 2 visualizations from the house pricing prediction
    dataset in DataRobot Classic, where the left image shows the image samples by
    binned targets and the right image shows the duplicate feature interface:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.6*展示了DataRobot Classic中的房价预测数据集的EDA 2可视化，左侧图片显示了按目标分组的图片样本，右侧图片则显示了重复特征的界面：'
- en: '![Figure 18.6 – EDA 1 and EDA 2 of images](img/B18187_18_6.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图18.6 – 图片的EDA 1和EDA 2](img/B18187_18_6.jpg)'
- en: Figure 18.6 – EDA 1 and EDA 2 of images
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.6 – 图片的EDA 1和EDA 2
- en: Now, let’s continue the tutorial with the EDA of the house pricing dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行房价数据集的EDA教程。
- en: Practically performing EDA in DataRobot
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在DataRobot中实际执行EDA
- en: 'We will continue the EDA tutorial in a step-by-step manner:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按步骤继续进行EDA教程：
- en: 'Click on the `trulia_pricing_dataset` entity under the **Data** tab in the
    use case and you will be presented with the view in *Figure 18**.7*, which can
    be scrolled horizontally and vertically:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在用例中的**数据**选项卡下点击`trulia_pricing_dataset`实体，你将看到*图18.7*所示的视图，界面可以水平和垂直滚动：
- en: '![￼Figure 18.7 – Sample data preview EDA interface of the housing dataset in
    Workbench](img/B18187_18_7.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![￼图18.7 – 工作台中房价数据集的样本数据预览EDA界面](img/B18187_18_7.jpg)'
- en: Figure 18.7 – Sample data preview EDA interface of the housing dataset in Workbench
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7 – 工作台中房价数据集的样本数据预览EDA界面
- en: If you scroll through the entire table, you will find that the dataset consists
    of 68 features, where 52 features are identified to be informative. This most
    notably consists of an image column that is valid, along with 23 other columns
    that are supposed to be images too but are just links that are not included properly.
    It also has five useful text columns that consist of facts, short and full addresses,
    features of the houses, and general descriptions of the houses.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你滚动查看整个表格，你会发现数据集由68个特征组成，其中52个特征被认为是有用的。最显著的是一个有效的图片列，以及另外23个原本应该是图片的列，但由于链接没有正确包含，实际上只是一些链接。它还包含五个有用的文本列，分别为事实、简短和完整的地址、房屋特征以及房屋的一般描述。
- en: 'If you click on the **Features** button in the top-left, you will see the interface
    in *Figure 18**.8*, which shows simple statistics of the dataset:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你点击左上角的**特征**按钮，你将看到*图18.8*所示的界面，其中展示了数据集的简单统计信息：
- en: '![Figure 18.8 – Features of the housing dataset](img/B18187_18_8.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图18.8 – 房价数据集的特征](img/B18187_18_8.jpg)'
- en: Figure 18.8 – Features of the housing dataset
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8 – 房价数据集的特征
- en: 'Now, we need to set the prediction target. The use case is to predict the price
    of a house here, so click on the `Price` in the **Target Feature** box. You will
    then see the interface shown in *Figure 18**.9*:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要设置预测目标。这里的用例是预测房价，因此点击**目标特征**框中的`价格`。然后你会看到如*图18.9*所示的界面：
- en: '![Figure 18.9 – Choosing a target in Workbench](img/B18187_18_9.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图18.9 – 在工作台中选择目标](img/B18187_18_9.jpg)'
- en: Figure 18.9 – Choosing a target in Workbench
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.9 – 在工作台中选择目标
- en: 'Then, click on the **Start modeling** button to start the quick modeling process.
    If you navigate to the **Data** tab from the DataRobot Classic interface, you
    will be able to see the univariate importance computed under the hood. This ranks
    features by their univariate informativeness with regard to the chosen target.
    This is depicted by the green bars in *Figure 18**.10*:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击**开始建模**按钮，开始快速建模过程。如果你从DataRobot Classic界面导航到**数据**标签，你将能够看到在后台计算的单变量重要性。这会根据所选目标对特征进行单变量信息量排名。这个过程在*图18.10*中用绿色条形图表示：
- en: '![Figure 18.10 – DataRobot Classic Data tab showing univariate importance](img/B18187_18_10.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图18.10 – DataRobot Classic 数据标签显示单变量重要性](img/B18187_18_10.jpg)'
- en: Figure 18.10 – DataRobot Classic Data tab showing univariate importance
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.10 – DataRobot Classic 数据标签显示单变量重要性
- en: Next, we will discover how DataRobot allows data wrangling after connecting
    to a dataset source and performing EDA.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索DataRobot在连接数据集源并执行EDA后，如何支持数据预处理。
- en: Wrangling data for deep learning model development
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理用于深度学习模型开发
- en: DataRobot allows you to transform your data with its **Wrangle** feature. It
    is supported for the case where the dataset is added through a data connection
    to a source, such as Snowflake. You can craft a set of data transformations you
    intend to apply to the entire dataset, which are called **recipes**. These transformations
    are initially tested on the live sample to ensure accuracy. Once your recipe is
    finalized, it’s sent to the data source and executed to create the final output
    dataset. The feature allows you to optionally save the transformed dataset right
    into the source of the data and get the output under the data registry, the AI
    Catalog.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot允许你使用其**预处理**功能对数据进行转换。此功能支持通过数据连接将数据集添加到源（如Snowflake）。你可以创建一组打算应用于整个数据集的转换，这些转换被称为**食谱**。这些转换首先会在实时样本上进行测试，以确保其准确性。一旦食谱完成，它将被发送到数据源并执行，生成最终的输出数据集。此功能还允许你选择性地将转换后的数据集直接保存到数据源中，并通过数据注册表（AI
    Catalog）获取输出。
- en: 'DataRobot supports a wide range of transformation operations, including the
    following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot支持广泛的转换操作，包括以下内容：
- en: Joining datasets from the same connection instance
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从同一连接实例合并数据集
- en: Applying mathematical aggregations to dataset features
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据集特征应用数学聚合
- en: Computing new features using scalar subqueries, scalar functions, or window
    functions
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标量子查询、标量函数或窗口函数计算新特征
- en: Filtering rows based on specified values and conditions
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据指定的值和条件过滤行
- en: De-duplicating rows to remove duplicates
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去重行以移除重复项
- en: Finding and replacing specific feature values
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找并替换特定特征值
- en: Renaming features within the dataset
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重命名数据集中的特征
- en: Removing selected features from the dataset
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中删除选定的特征
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: DataRobot doesn’t provide data labeling tools to create labels from scratch
    and depends on the idea that raw business data is all already recorded and saved
    somewhere. Use external labeling tools such as LabelBox to label data collaboratively
    and reliably for deep learning use cases.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot不提供数据标注工具来从零开始创建标签，它依赖于一个假设：原始的业务数据已经被记录并保存在某个地方。使用外部标注工具，如LabelBox，来协作和可靠地为深度学习用例标注数据。
- en: As the dataset we are using leverages a local dataset, we won’t be practically
    exploring the data wrangling component here. Dive into [https://docs.datarobot.com/en/docs/workbench/wb-dataprep/wb-wrangle-data/wb-add-operation.html](https://docs.datarobot.com/en/docs/workbench/wb-dataprep/wb-wrangle-data/wb-add-operation.html)
    to explore more on this topic.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的数据集是本地数据集，因此这里不会实际探讨数据预处理部分。请访问[https://docs.datarobot.com/en/docs/workbench/wb-dataprep/wb-wrangle-data/wb-add-operation.html](https://docs.datarobot.com/en/docs/workbench/wb-dataprep/wb-wrangle-data/wb-add-operation.html)进一步了解此话题。
- en: Next, we will explore how DataRobot executes modeling experiments or projects.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨DataRobot如何执行建模实验或项目。
- en: Executing modeling experiments with DataRobot
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataRobot执行建模实验
- en: 'DataRobot currently provides two ways to execute modeling experiments: DataRobot
    Classic and Workbench. Workbench is where an experiment will be managed under
    a use case, focusing on extracting value from a use case more seamlessly, and
    DataRobot Classic is the original AutoML experience where a modeling experiment
    is called a project. A project, or a modeling experiment here, encompasses the
    same components, which include modeling machine learning, gathering model insights
    and prediction insights, and making one-off batch predictions. We will dive deeper
    into these three components.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 目前提供两种方式来执行建模实验：DataRobot Classic 和 Workbench。Workbench 是一个在用例下管理实验的地方，重点是更加无缝地从用例中提取价值，而
    DataRobot Classic 是原始的 AutoML 体验，其中建模实验被称为项目。在这里，项目或建模实验包含相同的组件，这些组件包括建模机器学习、收集模型洞察与预测洞察以及进行一次性批量预测。我们将深入探讨这三个组件。
- en: Deep learning modeling
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习建模
- en: 'DataRobot provides modeling configurations and tasks in the form of **directed
    acyclic graphs** (**DAG**) called **blueprints**. The individual nodes in the
    graph are grouped up into the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 提供以 **有向无环图**（**DAG**）形式呈现的建模配置和任务，这些图被称为 **蓝图**。图中的每个节点分组为以下内容：
- en: '**Input data**: The input nodes can be any of the supported input data types.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据**：输入节点可以是任何支持的输入数据类型。'
- en: '**Data preprocessing tasks**: They consist of data regularization, normalization,
    missing value filling, and just any data preprocessing logic. You can also have
    tasks that choose the exact column to operate on. Additionally, techniques to
    perform predictions post-processing are also grouped here. Pre-trained networks
    that serve as feature transforms are also grouped here.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理任务**：它们包括数据规范化、标准化、缺失值填充以及任何数据预处理逻辑。你也可以执行选择具体列进行操作的任务。此外，执行预测后处理的技术也在这里归类。作为特征转换的预训练网络也在这里归类。'
- en: '**Modeling tasks**: They consist of any model that produces predictions in
    all formats. You can also make a model task part of an intermediate node, where
    the predictions from the model can then be used in subsequent modeling tasks through
    the stacking method. The stacking method outputs the combined out-of-fold features
    from the *k*-fold cross-validation strategy introduced in the *Partitioning the
    data for deep learning training* section in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*. This can be useful for training a neural
    network and using it to provide new features in the inferencing stage in a non-overfitting
    manner.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建模任务**：它们包括所有格式的任何模型，模型会生成预测。你还可以将一个模型任务作为中间节点的一部分，在这里，模型的预测可以通过堆叠方法用于后续的建模任务。堆叠方法输出来自
    *k*-折交叉验证策略的组合折外特征，该策略在[*第8章*](B18187_08.xhtml#_idTextAnchor125)，《探索监督式深度学习》的*深度学习训练数据划分*部分中介绍。这对于训练神经网络并以非过拟合方式在推理阶段提供新特征非常有用。'
- en: 'Comprehensively, the supported types of deep learning-specific tasks grouped
    by data type are as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从全面来看，根据数据类型，支持的深度学习特定任务类型如下：
- en: '**Image tasks**: The Visual AI feature, which is the product feature name that
    encapsulates everything related to images in DataRobot, supports both pre-trained
    featurizers, fine-tuning featurizers, and predictors with the following networks:
    Darknet, EfficientNet-B0, EfficientNet-B4, Preresnet10, Resnet50, Squeezenet,
    mobilenet-v3-small, and EfficientNetV2-S. For pre-trained featurizers specifically,
    pruned variants of the networks mentioned are offered, which offer no accuracy
    degradation with improved inference speeds. For featurizers, DataRobot provides
    an out-of-the-box way to extract low-, medium-, high-, and highest-level features
    from the pre-trained network, which can be tuned according to the use case. Additionally,image
    augmentation tasks are supported, which can be configured before an experiment
    has been executed and after a blueprint has been trained through the **advanced
    tuning** feature, where a trained blueprint can be retrained with new parameters.
    During the configuration of image augmentation, insights into how the augmented
    images will appear are provided, which will be demonstrated in the coming practical
    section.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像任务**：视觉 AI 特性是封装与 DataRobot 中图像相关的所有内容的产品功能，支持预训练特征提取器、微调特征提取器和预测器，涉及以下网络：Darknet、EfficientNet-B0、EfficientNet-B4、Preresnet10、Resnet50、Squeezenet、mobilenet-v3-small
    和 EfficientNetV2-S。特别是对于预训练特征提取器，提供了上述网络的修剪变体，这些变体在提高推理速度的同时不会降低准确性。对于特征提取器，DataRobot
    提供了一种开箱即用的方式，可以从预训练网络中提取低、中、高及最高级别的特征，并可以根据使用案例进行调整。此外，支持图像增强任务，这些任务可以在实验执行前和蓝图训练后通过**高级调优**功能进行配置，其中训练过的蓝图可以使用新参数重新训练。在配置图像增强过程中，将提供增强图像的显示预览，这将在接下来的实际操作部分中进行展示。'
- en: '**Types of models for text and document**: DataRobot supports various models
    for text and document processing, including lemmatizer, pre-trained part of speech
    tagger, Stemmer, FastText embeddings, TFIDF with stopwords, pre-trained TinyBERT
    featurizer for the English language, pre-trained Roberta featurizer for the English
    language, and pre-trained MiniLM for multiple languages. Note that the strategy
    DataRobot made for text is that the stopwords and pre-trained model used will
    depend on the language detected in the EDA sample.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本和文档的模型类型**：DataRobot 支持多种文本和文档处理模型，包括词形还原器、预训练的词性标注器、词干提取器、FastText 嵌入、带停用词的
    TFIDF、针对英语语言的预训练 TinyBERT 特征提取器、针对英语语言的预训练 Roberta 特征提取器，以及支持多语言的预训练 MiniLM。请注意，DataRobot
    为文本所做的策略是，所使用的停用词和预训练模型将取决于在 EDA 样本中检测到的语言。'
- en: '**General models**: DataRobot offers various general models, such as MLP with
    and without residuals, **Automatic Feature Interaction Learning** (**AutoInt**),
    Neural Architecture Search with Hyperband for MLP, Self-Normalizing Residual MLP
    with Training Schedule, and Adaptive Training Schedule.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用模型**：DataRobot 提供了多种通用模型，如带残差的 MLP 和不带残差的 MLP、**自动特征交互学习**（**AutoInt**）、使用
    Hyperband 进行 MLP 的神经架构搜索、自归一化残差 MLP 与训练计划以及自适应训练计划。'
- en: DataRobot automatically determines the blueprints that will be included in an
    experiment based on the dataset characteristics based on the modeling strategy
    of autopilot, quick, manual, or comprehensive mode. Manual mode doesn’t run any
    blueprints and leaves it to you to decide which blueprint to run. Quick, autopilot,
    and comprehensive modes can be viewed as modes that will take the fastest, medium
    fast, and slowest to complete. The comprehensive mode that is slowest to complete
    will run either different blueprints or additional blueprints that can be long-running,
    such as large deep learning models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 会根据数据集的特点、自动驾驶、快速、手动或综合模式的建模策略，自动确定将包含在实验中的蓝图。手动模式不会运行任何蓝图，而是由你决定运行哪个蓝图。快速模式、自动驾驶模式和综合模式可以视为分别需要最快、中等速度和最慢时间完成的模式。综合模式作为完成最慢的模式，会运行不同的蓝图或额外的蓝图，这些蓝图可能会长时间运行，如大型深度学习模型。
- en: DataRobot uses a modeling strategy that gradually eliminates models to find
    the best balance between exploration and runtime required to build the best model.
    This involves creating a set of blueprints with smaller sample sizes and removing
    weaker models. The top blueprints are then trained with a higher sample size in
    succession. The process continues by identifying a second reduced feature list
    with only the most informative features. Finally, the best model is trained with
    this feature list. For images, a pre-trained CNN model is used as the base model
    across all blueprints. When the final blueprints are built with the final sample
    size and reduced feature list, the best model is retrained with a larger and more
    time-consuming pre-trained network. This approach helps identify the most effective
    features and ensures the final model is optimized for accuracy and efficiency.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot使用一种建模策略，逐渐淘汰模型，以找到最佳的探索和运行时平衡，从而构建最佳模型。这涉及创建一组样本量较小的蓝图，并移除较弱的模型。然后，顶级蓝图会使用更大的样本量依次进行训练。这个过程通过识别第二个精简的特征列表，仅保留最具信息量的特征，继续进行。最终，使用这个特征列表对最佳模型进行训练。对于图像，所有蓝图都使用一个预训练的CNN模型作为基础模型。当最终的蓝图使用最终的样本量和精简的特征列表构建完成时，最佳模型将在一个更大且更耗时的预训练网络上进行重训练。这种方法有助于识别最有效的特征，并确保最终模型在准确性和效率上得到优化。
- en: Another key modeling functionality is DataRobot’s bias and fairness functionality.
    It enables users to build and evaluate fair AI models by defining protected attributes,
    assessing various fairness metrics, and comparing model performance across different
    subpopulations. The protected attributes have to be categorical values at the
    time of writing this book. If enabled through the **Show Advanced Options** option,
    the platform automatically detects potential biases, offers mitigation strategies,
    and allows users to monitor fairness throughout the model development process.
    By incorporating these features, DataRobot promotes responsible AI deployment,
    ensuring models comply with ethical guidelines and deliver equitable results for
    all users and subgroups.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键的建模功能是DataRobot的偏差和公平性功能。它使用户能够通过定义受保护的属性、评估各种公平性指标并比较不同子群体的模型表现来构建和评估公平的AI模型。在撰写本书时，受保护的属性必须是分类值。如果通过**显示高级选项**启用该功能，平台会自动检测潜在的偏见，提供缓解策略，并允许用户在整个模型开发过程中监控公平性。通过纳入这些功能，DataRobot促进了负责任的AI部署，确保模型符合伦理指南并为所有用户和子群体提供公平的结果。
- en: If you want to try out tasks that are not part of the out-of-the-box deep learning
    tasks, you can leverage **custom tasks** that you can share and use in a modeling
    experiment or project. Custom tasks allow you to define custom logic to either
    preprocess data or do modeling logic. On top of this feature, the Composable ML
    feature allows you to restructure and rearrange the blueprint DAGs flexibly. These
    features allow you to leverage more commonly used methods out-of-the-box and leverage
    any custom model that you might want to try out in your experiments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试一些不属于开箱即用的深度学习任务的工作，可以利用**自定义任务**，这些任务可以在建模实验或项目中共享和使用。自定义任务允许你定义自定义逻辑来进行数据预处理或建模逻辑。基于此功能，组合式机器学习（Composable
    ML）功能允许你灵活地重构和重新排列蓝图DAG。这些功能使你能够利用更多开箱即用的常用方法，并能够在实验中尝试任何自定义模型。
- en: DataRobot executes any tasks, such as training a blueprint, computing model
    insights, and computing predictions, through an on-demand worker queue. Each user
    will get their own assigned number of workers. Training and predicting with deep
    learning models can take a long time. Fortunately, DataRobot has both CPU and
    GPU workers, and deep learning models can be run on GPU workers to speed up runtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot通过按需工作队列执行所有任务，例如训练蓝图、计算模型洞察和计算预测。每个用户将获得自己分配的工作者数量。使用深度学习模型进行训练和预测可能需要较长时间。幸运的是，DataRobot既有CPU工作者也有GPU工作者，深度学习模型可以在GPU工作者上运行，以加速运行时间。
- en: On the topic of evaluation, DataRobot uses nested cross-validation and never
    uses test data for in-training validation. The evaluation metrics supported by
    DataRobot are comprehensive and can be referred to at [https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html](https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html).
    The metrics and trained blueprints will then be presented in a leaderboard interface
    where blueprints are ranked by the chosen metric.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估方面，DataRobot 使用嵌套交叉验证，并且绝不使用测试数据进行训练过程中的验证。DataRobot 支持的评估指标非常全面，可以参考 [https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html](https://docs.datarobot.com/en/docs/modeling/reference/model-detail/opt-metric.html)。这些指标和训练好的蓝图随后会以排行榜形式呈现，蓝图会根据所选指标进行排名。
- en: Comparisons between blueprints, however, are much broader than comparing blueprints
    trained on the same dataset. Datasets can be different, experiment settings can
    be different, and associated insights can also be different. This is where the
    **Model Comparison** feature helps to bridge this gap and allows the comparison
    of many blueprint setups managed under a single use case.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，蓝图之间的比较远不止是比较在同一数据集上训练的蓝图。数据集可以不同，实验设置也可以不同，关联的洞察也会有所不同。此时，**模型比较**功能有助于弥补这个差距，并允许在单一用例下比较许多蓝图设置。
- en: As a final note here, most modeling-related settings, such as the weights column,
    partitioning strategy, and metric to optimize against, can be configured under
    the **Advanced Options** feature. We will now continue the tutorial to execute
    a model experiment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要提到的是，大多数与建模相关的设置，如权重列、分区策略和优化的评估指标，都可以在**高级选项**功能下进行配置。接下来我们将继续教程，执行一个模型实验。
- en: Practically executing modeling experiments in DataRobot
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 DataRobot 中实际执行建模实验
- en: 'Let’s dive into practical modeling with DataRobot through a step-by-step process,
    continuing on from the previous tutorial:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过逐步过程深入了解 DataRobot 的实际建模，从上一个教程继续：
- en: 'After you start the modeling process, in Workbench, you will see the following
    interface, where DataRobot shows you what the platform is doing while waiting
    for blueprints to start populating:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你启动建模过程后，在工作台中你将看到以下界面，DataRobot 会显示平台在等待蓝图加载时的操作情况：
- en: '![Figure 18.11 – Waiting for blueprints to populate in Workbench](img/B18187_18_11.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.11 – 等待蓝图在工作台中加载](img/B18187_18_11.png)'
- en: Figure 18.11 – Waiting for blueprints to populate in Workbench
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.11 – 等待蓝图在工作台中加载
- en: 'After waiting for the blueprints to be generated and complete their training,
    in Workbench, you will be able to see the sorted trained blueprints on the left,
    as shown in *Figure 18**.12 (a)*, where we can **star** two models to compare
    them more comprehensively:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在等待蓝图生成并完成训练后，在工作台中，你将能够看到左侧排序好的训练蓝图，如*图 18**.12 (a)*所示，我们可以**星标**两个模型，以便更全面地比较它们：
- en: '![Figure 18.12 – (a) Showing the ranked blueprints with scores and (b) showing
    the dataset Comparison feature](img/B18187_18_12.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.12 – (a) 显示带有分数的排名蓝图，(b) 显示数据集对比功能](img/B18187_18_12.jpg)'
- en: Figure 18.12 – (a) Showing the ranked blueprints with scores and (b) showing
    the dataset Comparison feature
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.12 – (a) 显示带有分数的排名蓝图，(b) 显示数据集对比功能
- en: By clicking on the **Comparison** tab, you will be able to compare the two starred
    models in terms of evaluation metrics, datasets, blueprint type, and many more
    insights across different experiments, as depicted in *Figure* *18**.12 (b)*.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击**对比**标签，你将能够在多个实验中，按评估指标、数据集、蓝图类型等方面比较两个星标模型的表现，正如*图* *18**.12 (b)*所示。
- en: By clicking on the best-performing model in the **Gamma Deviance** metric, we
    can investigate the blueprint structure of the model under the **Blueprint** dropdown
    depicted in *Figure 18**.13*. The blueprint is a multimodal blueprint with an
    XGBoost final modeler that takes in transformed input from categorical, geospatial,
    numerical, image, and text variables.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击在**Gamma Deviance**指标下表现最好的模型，我们可以在**蓝图**下拉菜单中查看该模型的蓝图结构，如*图 18**.13*所示。该蓝图是一个多模态蓝图，最终模型使用
    XGBoost，并接受来自分类、地理空间、数值、图像和文本变量的转化输入。
- en: '![Figure 18.13 – Best model blueprint diagram](img/B18187_18_13.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.13 – 最佳模型蓝图图示](img/B18187_18_13.jpg)'
- en: Figure 18.13 – Best model blueprint diagram
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.13 – 最佳模型蓝图图示
- en: 'Now, let’s see if we can make manual improvements to the metric score, which
    you can get for the **Validation**, **Cross Validation**, or **Holdout** partitions.
    As the default experiment modeling mode is a quick pilot and you can’t rerun another
    modeling mode in Workbench as of writing, let’s manually select blueprints available
    in the repository by clicking on **View experiment info** in the top-left of the
    experiment interface, which will bring you to the interface in *Figure 18**.14*:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看是否可以对**验证**、**交叉验证**或**留置**分区的指标得分进行手动改进。由于默认实验建模模式是快速试点，并且在撰写时不能在Workbench中重新运行另一种建模模式，让我们通过点击实验界面左上角的**查看实验信息**来手动选择存储库中可用的蓝图，这将带您到*图
    18**.14*中的界面：
- en: '![Figure 18.14 – The Blueprint repository tab in Experiment information](img/B18187_18_14.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.14 – 实验信息中的蓝图存储库选项卡](img/B18187_18_14.jpg)'
- en: Figure 18.14 – The Blueprint repository tab in Experiment information
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.14 – 实验信息中的蓝图存储库选项卡
- en: Now, search for all Keras models and fine-tuned image models, check the checkbox,
    and click on the **Train model** button on the right to train more blueprints
    that can take much longer to execute.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，搜索所有Keras模型和精调图像模型，勾选复选框，并点击右侧的**训练模型**按钮，以训练更多需要更长时间执行的蓝图。
- en: 'As an additional modeling step, let’s add image augmentation to the existing
    best model starred earlier. We can do that by navigating into the **DataRobot
    Classic Models** tab and clicking on the starred best model on the Validation
    partition. Click on the **Advanced Tuning** sub-tab under the **Evaluate** tab
    under the blueprint. This will bring you to the interface shown in *Figure 18**.15*:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为额外的建模步骤，让我们将图像增强添加到先前标记为最佳模型的现有模型中。我们可以通过导航到**DataRobot经典模型**选项卡，并点击验证分区上最佳模型上的星号。在**蓝图**下的**评估**选项卡中点击**高级调整**子选项卡。这将带您到*图
    18**.15*中显示的界面：
- en: '![Figure 18.15 – The Advanced Tuning interface under the best model’s Evaluate
    tab](img/B18187_18_15.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.15 – 最佳模型的评估选项卡下的高级调整界面](img/B18187_18_15.jpg)'
- en: Figure 18.15 – The Advanced Tuning interface under the best model’s Evaluate
    tab
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.15 – 最佳模型的评估选项卡下的高级调整界面
- en: 'Now, scroll down to the **Image Augmentation List** tab and click on **Create
    new list**, as shown in *Figure 18**.16 (a)*. Configure **Blur**, **Cutout**,
    **Horizontal flip**, **Vertical flip**, **New images per original**, **Probability**,
    **Shift**, **Scale**, and **Rotate** to the default settings. Click on the **Preview
    augmentation** button and you will see image previews like in *Figure 18**.16
    (b)*. Now click on **Save as new list**, set your name, and click on **Create
    Augmentation List**. Finally, click on **Begin Tuning**, which is also shown in
    *Figure* *18**.16 (a)*:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，滚动到**Image Augmentation List**选项卡，点击**创建新列表**，如*图 18**.16 (a)*所示。将**模糊**、**遮挡**、**水平翻转**、**垂直翻转**、**每个原始图像的新图像**、**概率**、**位移**、**缩放**和**旋转**配置为默认设置。点击**预览增强**按钮，你将看到像*图
    18**.16 (b)*中那样的图像预览。现在点击**保存为新列表**，设置你的名称，并点击**创建增强列表**。最后，点击**开始调整**，这也显示在*图
    18**.16 (a)*中：
- en: '![Figure 18.16 – Image augmentation configuration in Advanced Tuning](img/B18187_18_16.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.16 – 高级调整中的图像增强配置](img/B18187_18_16.jpg)'
- en: Figure 18.16 – Image augmentation configuration in Advanced Tuning
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.16 – 高级调整中的图像增强配置
- en: Wait until the newly tuned blueprint completes its training, and you will be
    blessed with a better-performing blueprint!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 等到新调整的蓝图完成训练，你将会得到一个表现更好的蓝图！
- en: The steps done here only cover a small part of the modeling process that DataRobot
    supports. Be sure to explore features such as bias, fairness mitigation and evaluation,
    Composable ML, custom tasks, advanced tuning of many other parameters, and time-series
    modeling.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里完成的步骤仅涵盖DataRobot支持的建模过程的一小部分。请务必探索功能，如偏差、公平性缓解和评估、可组合ML、自定义任务、许多其他参数的高级调整以及时间序列建模。
- en: Notice that up to this stage, we were using the evaluation metric as the only
    form of model comparison feedback. Comparing blueprints by only using the metric
    is not enough in most critical use cases. In the next section, we will discover
    how we can gather model and prediction insights to compare blueprints comprehensively.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个阶段，我们仅使用评估指标作为模型比较的唯一反馈形式。在大多数关键使用情况下，仅使用指标比较蓝图是不够的。在接下来的部分中，我们将发现如何收集模型和预测见解，全面比较蓝图。
- en: Gathering model and prediction insights
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集模型和预测见解
- en: 'Notably, DataRobot provides the following insights, which are relevant to blueprints
    that have deep learning model tasks:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，DataRobot 提供了以下洞察，这些洞察与包含深度学习模型任务的蓝图相关：
- en: '**Feature impact**: This is a multivariate analysis that helps determine the
    importance of different features within a dataset, revealing which variables have
    the strongest influence on model predictions.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征影响**：这是一种多变量分析，帮助确定数据集中不同特征的重要性，揭示哪些变量对模型预测有最强的影响。'
- en: '**Feature Effects**: This is a tool that helps you understand how each feature
    in your dataset affects the model’s predictions. It provides you with a clear
    and easy-to-interpret visual representation of the relationship between each feature
    and the model’s output.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征效果**：这是一个工具，帮助你理解数据集中每个特征如何影响模型的预测。它提供了一个清晰、易于解释的可视化，展示每个特征与模型输出之间的关系。'
- en: '**Activation maps**: These are visualizations that display the regions in an
    input image that are most responsible for making the final blueprint predictions.
    It covers neural networks as predictors, intermediate featurizers, and even intermediate
    modelers.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活图**：这些是显示输入图像中最有责任做出最终蓝图预测的区域的可视化。它涵盖了作为预测器的神经网络、中间特征提取器，甚至是中间建模器。'
- en: '**Prediction explanations**: These are techniques used to explain the output
    of a model, highlighting the contribution of each input feature to a particular
    prediction. **SHAP** (**SHapley Additive exPlanations**) and **XEMP** (**eXtended
    Example-based Model explanations through Perturbations**) are two popular methods
    that are supported. For images, image activation maps are used. For text explanations,
    a model-agnostic method is used to provide word-based importance scores.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测解释**：这些是用于解释模型输出的技术，突出显示每个输入特征对特定预测的贡献。**SHAP**（**Shapley 加性解释**）和 **XEMP**（**基于扰动的扩展示例模型解释**）是两种受支持的流行方法。对于图像，使用图像激活图；对于文本解释，使用与模型无关的方法提供基于单词的重要性分数。'
- en: '**Word cloud**: This is a visualization technique that represents the frequency
    of words or phrases within a text dataset, where the size of the word indicates
    its importance or frequency.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词云**：这是一种可视化技术，用于表示文本数据集中单词或短语的频率，其中单词的大小表示其重要性或频率。'
- en: '**Image embeddings**: This is a visualization of images in a lower-dimensional
    space that captures essential features. It is derived from the output of the supported
    CNN models.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像嵌入**：这是在低维空间中对图像的可视化，捕捉了图像的基本特征。它来源于支持的 CNN 模型的输出。'
- en: '**ROC curve**: This is a plot that illustrates the diagnostic ability of a
    binary classifier, showing the true positive rate against the false positive rate
    at various threshold settings, which helps in selecting an optimal threshold.
    Along with the curve, the confusion matrix and an optional profit curve feature
    are added. The profit curve is a tool that helps optimize the threshold for classification
    models by plotting the profit (or other performance metrics) against different
    threshold values.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROC 曲线**：这是一个图表，展示了二分类器的诊断能力，显示不同阈值设置下的真实正例率与假正例率的关系，帮助选择最佳阈值。除了曲线，还增加了混淆矩阵和一个可选的利润曲线功能。利润曲线是一种工具，通过绘制不同阈值下的利润（或其他性能指标），帮助优化分类模型的阈值。'
- en: '**Neural network visualizer**: This is a tool that allows users to visualize
    the architecture of a neural network, displaying the layers, neurons, and connections
    between them.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络可视化工具**：这是一个工具，允许用户可视化神经网络的架构，展示各层、神经元以及它们之间的连接。'
- en: '**Training dashboard**: This is a tool that provides you with an easy-to-use
    interface that shows you important loss curves and any metric by epochs or iterations
    for a neural network model.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练仪表盘**：这是一个工具，提供一个易于使用的界面，展示了重要的损失曲线以及神经网络模型的每个训练周期或迭代的各种度量。'
- en: '**Blueprint visualizer**: This is a feature that provides a comprehensive view
    of the overall blueprint, displaying the data processing, feature engineering,
    and modeling steps involved in creating a machine learning model.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝图可视化工具**：这是一个功能，提供了整个蓝图的综合视图，展示了创建机器学习模型时所涉及的数据处理、特征工程和建模步骤。'
- en: Now, let’s explore some of these functionalities by continuing the earlier tutorial.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续之前的教程，探索其中的一些功能。
- en: Practically gathering insights in DataRobot
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际上在 DataRobot 中收集洞察
- en: 'Under each blueprint in the DataRobot Classic experience, you can explore all
    the insight functionalities. In Workbench, work is being done to add the comprehensive
    insights experience, and so far, feature impact, lift charts, and residuals insights
    are available. Let’s dive into the insights part of the tutorial in a step-by-step
    manner:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataRobot Classic体验中的每个蓝图下，您可以探索所有的洞察功能。在Workbench中，正在进行工作以增加全面的洞察体验，迄今为止，特征影响、提升图和残差洞察已可用。让我们以逐步方式深入讲解教程中的洞察部分：
- en: 'Start by clicking on the `Image 4` column being used, even when it’s just URLs.
    The next thing is the `Home Id` column, which should’ve been removed:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从点击`图像4`栏开始使用，即使它只是URLs。接下来的`Home Id`栏应该被移除：
- en: '![ Figure 18.17 – Feature impact on the best model](img/B18187_18_17.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图18.17 – 特征对最佳模型的影响](img/B18187_18_17.jpg)'
- en: Figure 18.17 – Feature impact on the best model
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.17 – 特征对最佳模型的影响
- en: 'Next, we will explore the feature effects of the best blueprint by clicking
    on the `Bath`. The effect graph shows that with increasing bath numbers, the price
    generally increases, which makes sense:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过点击`浴室`来探索最佳蓝图的特征效果。效果图显示，随着浴室数量的增加，价格通常会增加，这很有道理：
- en: '![Figure 18.18 – Feature effects for the best blueprint](img/B18187_18_18.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图18.18 – 最佳蓝图的特征效果](img/B18187_18_18.jpg)'
- en: Figure 18.18 – Feature effects for the best blueprint
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.18 – 最佳蓝图的特征效果
- en: 'Next, click on the `Description` is also one of the top contributing features.
    Clicking on the symbol under the **Value** column open a pop-up modal window will
    allow you to check the text explanations for the feature, as shown in *Figure
    18**.19*. Both the explanations look good and make good sense here:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击`描述`，它也是一个重要的贡献特征。点击**值**栏下的符号打开弹出模态窗口，您可以查看该特征的文本说明，如*图18.19*所示。两个说明都很清晰且合理：
- en: '![Figure 18.19 – Prediction explanations for the best blueprint](img/B18187_18_19.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图18.19 – 最佳蓝图的预测说明](img/B18187_18_19.jpg)'
- en: Figure 18.19 – Prediction explanations for the best blueprint
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.19 – 最佳蓝图的预测说明
- en: Now, let’s click on the `nice` being attributed negatively. However, `cozy`
    makes sense to be negatively attributed, as it usually refers to small units.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们点击被负面归因的`nice`。然而，`cozy`被负面归因是有道理的，因为它通常指代小型单元。
- en: '![Figure 18.20 – Word cloud importance attribution of the best blueprint](img/B18187_18_20.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图18.20 – 最佳蓝图的词云重要性归因](img/B18187_18_20.jpg)'
- en: Figure 18.20 – Word cloud importance attribution of the best blueprint
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.20 – 最佳蓝图的词云重要性归因
- en: 'Next, click on the **Activation Maps** sub-tab to see the interface shown in
    *Figure 18**.21*:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击**激活图**子标签页，查看*图18.21*所示的界面：
- en: '![Figure 18.21 – Activation maps of the best-performing blueprint](img/B18187_18_21.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图18.21 – 最佳蓝图的激活图](img/B18187_18_21.jpg)'
- en: Figure 18.21 – Activation maps of the best-performing blueprint
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.21 – 最佳蓝图的激活图
- en: For image activation maps, you want to look out for areas that don’t make sense
    logically. Ask questions such as, “Why is the model looking at the sky?” or “Why
    is the model looking at the grass?” Also, ask follow-up questions such as “Is
    a well-trimmed lawn connected to price?” For this use case, there are a lot of
    visual components at play that make it hard to say what doesn’t make sense. Focusing
    on grass can still be meaningful, but It’s a relief that the model is at least
    not looking at the sky, which contributes to nothing. Additionally, the predicted
    and actual filter can allow you to pinpoint the successful example’s behavior
    vs the failed example’s behavior, which can be useful to form a mental picture
    of what patterns the model is identifying. Another issue is that the image column
    isn’t standardized with regard to which part of the house it is representing.
    Standardizing can help you achieve a better prediction performance.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像激活图，您需要注意逻辑上不合常理的区域。可以提出像“为什么模型在看天空？”或“为什么模型在看草地？”这样的问题。同时，可以进一步提问“修剪整齐的草坪和价格有关系吗？”对于这个使用场景，涉及到很多视觉组件，使得很难判断什么不合常理。聚焦于草地仍然可能是有意义的，但至少可以放心的是，模型至少没有在看没有任何贡献的天空。此外，预测和实际过滤器可以帮助您辨别成功示例的行为与失败示例的行为，这对于形成模型识别的模式的心理图像非常有帮助。另一个问题是，图像列在表示房屋的哪一部分时没有标准化。标准化有助于您获得更好的预测性能。
- en: Finally, you can export the insights individually, and better yet, download
    the compliance report documentation that provides an offline one-stop document
    with all the insights. You can do this by clicking on the **Compliance** tab and
    clicking on the **Create Report** button shown in *Figure 18**.22*. There is a
    default structure of the document, but you can craft the exact structure that
    you want to have in your report. An example document is provided in the code repository.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以单独导出洞察报告，甚至更好的是，下载合规报告文档，它提供了一个离线的一站式文档，包含所有洞察。你可以通过点击**合规性**标签页并点击**创建报告**按钮（如*图
    18.22*所示）来完成。文档有一个默认结构，但你可以根据需要设计报告的确切结构。代码库中提供了一个示例文档。
- en: '![Figure 18.22 – The Model Compliance Documentation interface under the blueprint](img/B18187_18_22.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.22 – 蓝图下的模型合规文档界面](img/B18187_18_22.jpg)'
- en: Figure 18.22 – The Model Compliance Documentation interface under the blueprint
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.22 – 蓝图下的模型合规文档界面
- en: With that, we are done with the tutorial on gathering insights. However, note
    that this is not a comprehensive take on gathering insights for this use case.
    So, be sure to test out more insight types such as lift charts and image embeddings
    and iterate through more model improvements that you identify through gathering
    insights.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了关于收集洞察的教程。不过，请注意，这并不是一个全面的关于此用例的洞察收集方法。因此，务必测试更多类型的洞察，例如提升图和图像嵌入，并在收集洞察时进行更多模型改进的迭代。
- en: Before we move on to the DLOps side of things, to deploy and govern a deep learning
    model, let’s explore how batch predictions can be made without deploying a model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入 DLOps 部分之前，先来探索一下如何在不部署模型的情况下进行批量预测。
- en: Making batch predictions
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量预测
- en: There are use cases where a deployment is not needed, as predictions can be
    made asynchronously in a regular cadence via a custom trigger or a one-time event.
    This is where the **batch predictions** feature from DataRobot comes in. Batch
    predictions simply allow you to upload your data, compute predictions optionally
    with prediction explanations, and, once this is done, download the results.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些用例中，部署不是必须的，因为可以通过自定义触发器或一次性事件以定期的方式异步进行预测。这时，**批量预测**功能就派上用场了。批量预测允许你上传数据，选择性地计算预测结果（并可附加预测解释），完成后可以下载结果。
- en: 'This step can be done in both DataRobot Classic and Workbench. For the DataRobot
    Classic graphical UI experience, navigate to the **Make Predictions** sub-tab
    under the **Predict** tab of a leaderboard model. You will then be able to upload
    the dataset you want to generate one-off predictions with. This interface is shown
    in *Figure 18**.23*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可以在 DataRobot Classic 和 Workbench 中完成。对于 DataRobot Classic 的图形界面体验，导航到**预测**标签下的**进行预测**子标签页。然后，你就可以上传你希望用来生成一次性预测的数据集。该界面如*图
    18.23*所示：
- en: '![Figure 18.23 – Batch predictions functionality interface](img/B18187_18_23.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.23 – 批量预测功能界面](img/B18187_18_23.jpg)'
- en: Figure 18.23 – Batch predictions functionality interface
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.23 – 批量预测功能界面
- en: There will still be use cases where real-time predictions are needed by single
    samples. This brings us to the next section—discussing how DataRobot manages the
    deployment of a blueprint.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然会有一些用例需要通过单个样本进行实时预测。这引出了下一部分——讨论 DataRobot 如何管理蓝图的部署。
- en: Deploying a deep learning blueprint
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署深度学习蓝图
- en: DataRobot allows the deployment of a model directly through a trained blueprint
    in an experiment or a project, which we will explore in the next practical section.
    However, for more advanced users, the platform also allows the deployment of custom
    models through the `requirements.txt` file. Once uploaded, users can create, test,
    and deploy custom inference models to DataRobot’s centralized deployment hub.
    These custom models support different model types, which include regression, classification,
    and unstructured types where the input and output can be of various types.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 允许通过实验或项目中的训练蓝图直接部署模型，我们将在下一个实际操作部分中详细探讨。然而，对于更高级的用户，平台还允许通过 `requirements.txt`
    文件部署自定义模型。一旦上传，用户就可以创建、测试并将自定义推理模型部署到 DataRobot 的集中部署中心。这些自定义模型支持不同类型的模型，包括回归、分类和非结构化类型，其中输入和输出可以是各种类型。
- en: 'To ensure the reliability and compatibility of your custom models, DataRobot
    provides a comprehensive testing suite in the Custom Model Workshop. The custom
    model testing suite encompasses a comprehensive range of evaluations, including
    the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保自定义模型的可靠性和兼容性，DataRobot提供了一个全面的测试套件，供自定义模型工作坊使用。自定义模型测试套件涵盖了广泛的评估，包括以下内容：
- en: '**Startup check**: This ensures that the custom model can be built and the
    custom model service can be launched without errors'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启动检查**：确保自定义模型能够构建且自定义模型服务能够无误启动'
- en: '**Null imputation check**: This validates the handling of missing values'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空值插补检查**：这验证了对缺失值的处理'
- en: '**Side effects check**: This makes sure that a row of data predicted as part
    of a batch of data produces the same predictions as the same row of data predicted
    with a single row of data'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副作用检查**：确保在批量数据中预测的一行数据与单独预测同一行数据时产生相同的预测结果'
- en: 'P**rediction verification**: This confirms the correctness of predictions'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测验证**：这确认了预测的正确性'
- en: '**Performance check**: This gauges the efficiency and speed of the model'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能检查**：这评估了模型的效率和速度'
- en: '**Stability check**: This evaluates the model’s consistency and reliability'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定性检查**：这评估了模型的一致性和可靠性'
- en: '**Duration check**: This measures the time taken for various tasks'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续时间检查**：这衡量了完成各种任务所需的时间'
- en: 'By running these tests, you can verify the performance, stability, and prediction
    accuracy of your custom models before deployment. A bonus here with a supervised
    custom model is that it can be linked to training data, which will allow prediction
    explanations to be computed and data drift to be measured and monitored. Once
    your custom model is assembled and tested, you can deploy it alongside other blueprints
    in DataRobot, making it a versatile and powerful tool for advanced users. To deploy
    a model, either a custom or a local DataRobot model, a prerequisite is that you’d
    need to choose the reliability of the deployment that you want. Levels of reliability
    include the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行这些测试，你可以在部署前验证自定义模型的性能、稳定性和预测准确性。使用监督式自定义模型的一个额外好处是，它可以与训练数据关联，这样就可以计算预测解释并衡量和监控数据漂移。一旦你的自定义模型完成并经过测试，就可以将其与DataRobot中的其他蓝图一起部署，使其成为高级用户的多功能且强大的工具。要部署模型（无论是自定义模型还是本地DataRobot模型），前提是你需要选择你想要的部署可靠性。可靠性级别包括以下几种：
- en: '**Low**: This is suitable for non-critical, experimental, or low-priority use
    cases where occasional downtime or reduced performance is acceptable. This option
    provides minimal resources and infrastructure redundancy.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低**：适用于非关键、实验性或低优先级的用例，在这些情况下，偶尔的停机或性能下降是可以接受的。此选项提供最少的资源和基础设施冗余。'
- en: '**Medium**: This is ideal for moderately important use cases that require a
    balance between cost and performance. This level offers better resources and redundancy
    than the low option, but you may still experience some downtime or reduced performance
    during peak loads.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中等**：适用于需要在成本和性能之间平衡的中等重要的用例。此级别提供比低级选项更好的资源和冗余，但在高峰期负载时，仍可能会经历一些停机或性能下降。'
- en: '**High**: This is recommended for important use cases that demand high availability
    and performance. This level provides increased resources, infrastructure redundancy,
    and faster response times to ensure consistent performance, even during heavy
    loads.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高**：推荐用于要求高可用性和高性能的重要用例。此级别提供增加的资源、基础设施冗余和更快的响应时间，以确保在重负载下也能保持一致的性能。'
- en: '**Critical**: This is designed for mission-critical applications where maximum
    availability and performance are essential. This option offers the highest level
    of resources, redundancy, and response times to ensure near-zero downtime and
    optimal performance under any conditions.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键**：专为任务关键型应用设计，最大化可用性和性能至关重要。此选项提供最高级别的资源、冗余和响应时间，以确保在任何条件下几乎无停机时间并保持最佳性能。'
- en: Choosing an appropriate reliability level allows DataRobot to configure an appropriate
    server machine type and infrastructure to host your model according to your requirements.
    As there are limitations to the deployment your organization signed up with, choosing
    an appropriate reliability level will make sure you don’t overpay after passing
    the organization deployment limits, or it just makes sure you stay under the deployment
    limits. In other words, you must manage costs incurred when it comes to deployment.
    Let’s continue through the previous tutorial and deploy the best-performing blueprint
    that wasn’t trained into the validation or holdout partition.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的可靠性级别可以让 DataRobot 根据你的需求配置适当的服务器机器类型和基础设施来托管你的模型。由于你的组织签订的部署协议有一定限制，选择适当的可靠性级别可以确保你不会在超出组织部署限制后支付过多费用，或者确保你不会超出部署限制。换句话说，你必须管理部署过程中产生的成本。让我们继续之前的教程，部署表现最佳的蓝图，该蓝图没有经过验证或保留分区训练。
- en: Practically deploying a blueprint in DataRobot
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际在 DataRobot 中部署蓝图
- en: 'Deploying a blueprint is as simple as going to the **Deploy** sub-tab under
    the **Predict** tab under a blueprint and then clicking the **Deploy model** button.
    You then need to choose the deployment reliabilitythe default is low. Then, click
    on the **Deploy model** button again in the same location. The interface for the
    first **Deploy model** button is shown in *Figure 18**.24*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 部署蓝图就像去蓝图下的 **Predict** 标签下的 **Deploy** 子标签，然后点击 **Deploy model** 按钮一样简单。接着，你需要选择部署可靠性，默认值为低。然后，在相同位置再次点击
    **Deploy model** 按钮。第一个 **Deploy model** 按钮的界面如 *图 18.24* 所示：
- en: '![Figure 18.24 – Deploying a blueprint](img/B18187_18_24.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.24 – 部署蓝图](img/B18187_18_24.jpg)'
- en: Figure 18.24 – Deploying a blueprint
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.24 – 部署蓝图
- en: And that’s it! We have successfully deployed a model in DataRobot with the click
    of two buttons. Next, we will discuss how DataRobot governs its deployed blueprint.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经成功通过点击两个按钮在 DataRobot 中部署了一个模型。接下来，我们将讨论 DataRobot 如何管理已部署的蓝图。
- en: Governing a deployed deep learning blueprint
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理已部署的深度学习蓝图
- en: In this section, we will discuss how DataRobot enables users to govern their
    deep-learning models effectively by providing comprehensive tools for model utilization,
    monitoring, and maintenance. With a focus on seamless integration, DataRobot allows
    users to deploy AI applications on cloud-based or on-premises infrastructure,
    manage prediction outputs, and monitor model performance using custom metrics
    and alerts. Furthermore, the platform supports data drift detection and offers
    retraining capabilities for continuous model improvement. We will explore these
    features in detail, demonstrating how DataRobot empowers users to efficiently
    manage their deep learning models and ensure optimal performance throughout their
    life cycle.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 DataRobot 如何通过提供全面的模型利用、监控和维护工具，帮助用户有效地管理他们的深度学习模型。DataRobot 关注无缝集成，允许用户在云端或本地基础设施上部署
    AI 应用，管理预测输出，并使用自定义指标和警报监控模型性能。此外，平台支持数据漂移检测，并提供再训练功能，以持续改进模型。我们将详细探讨这些功能，展示 DataRobot
    如何帮助用户高效管理深度学习模型，并确保其在整个生命周期中的最佳性能。
- en: Governing through model utilization in DataRobot
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 DataRobot 中的模型利用进行管理
- en: Users can access their models through various means, such as API calls, Python
    interfaces, or DataRobot-made applications called `base64` format. DataRobot also
    facilitates the direct storage of predictions into databases, streamlining the
    process of incorporating model outputs into existing workflows or applications.
    Additionally, DataRobot allows for the scheduling of batch predictions with a
    deployed model to be executed regularly to the specified frequency and time.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过多种方式访问他们的模型，如 API 调用、Python 接口，或 DataRobot 制作的名为 `base64` 格式的应用程序。DataRobot
    还方便将预测结果直接存储到数据库中，从而简化了将模型输出纳入现有工作流程或应用程序的过程。此外，DataRobot 允许安排定期执行批量预测，以便按指定的频率和时间自动执行。
- en: Overall, the model utilization component in DataRobot simplifies the process
    of leveraging deep learning models and machine learning models in general, making
    it more accessible and efficient for users across various domains.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，DataRobot 中的模型利用组件简化了深度学习模型和机器学习模型的一般应用流程，使其对各个领域的用户更具可及性和效率。
- en: Now, let’s continue on from the previous tutorial to get predictions using the
    DataRobot HTTP client library.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续之前的教程，使用 DataRobot HTTP 客户端库获取预测。
- en: Practically consuming predictions from a deployed blueprint in DataRobot
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际使用DataRobot中部署蓝图的预测
- en: 'In this section, we will continue on from the previous tutorial and use the
    `datarobotx` Python client library to generate predictions with the deployed model.
    Under each deployment, DataRobot includes low-level example code to make real-time
    `prediction-api` requests to a deployment. However, in this tutorial, we will
    utilize an easy-to-use, high-level library called `datarobotx` that simplifies
    making `prediction-api` requests. Let’s start the step-by-step process:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将继续上一个教程，并使用`datarobotx` Python客户端库通过已部署的模型生成预测。在每个部署下，DataRobot都包含了低级示例代码，可以向部署发送实时`prediction-api`请求。然而，在本教程中，我们将使用一个简单易用的高级库`datarobotx`，该库简化了`prediction-api`请求的发送。让我们开始逐步操作：
- en: 'Let’s start by importing the libraries:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入所需的库：
- en: '[PRE0]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will initialize the deployment instance based on the deployment ID.
    You’d have to replace `deployment_id` with your own deployment ID here:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将基于部署ID初始化部署实例。你需要在此处将`deployment_id`替换为自己的部署ID：
- en: '[PRE1]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will set the API token and the endpoint URL in a DataRobot context
    class. You’d have to set your own token here:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在DataRobot上下文类中设置API令牌和端点URL。你需要在此处设置自己的令牌：
- en: '[PRE2]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we will load the `trulia_one_row.csv` house pricing DataFrame provided
    in the code repository and make a prediction using the initialized deployment
    instance. Finally, we display the predictions:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将加载代码仓库中提供的`trulia_one_row.csv`房价数据框，并使用初始化的部署实例进行预测。最后，我们展示预测结果：
- en: '[PRE3]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will produce the following results:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE4]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that concludes the tutorial. We will now dive into how DataRobot implements
    model monitoring for a deployed model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程到此结束。接下来，我们将深入了解DataRobot如何为已部署的模型实现模型监控。
- en: Governing through model monitoring in DataRobot
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过在DataRobot中进行模型监控进行治理
- en: 'Model monitoring in DataRobot is an essential component that allows users to
    track the performance and health of their deployed deep learning models. The platform
    provides several features to ensure models maintain optimal performance over time:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataRobot中，模型监控是一个重要组成部分，允许用户跟踪其已部署的深度学习模型的性能和健康状况。平台提供了多项功能，确保模型随着时间的推移保持最佳性能：
- en: '**Data drift detection**: DataRobot continuously monitors changes in the distribution
    of input data, identifying any deviations from the original training data. This
    feature helps users understand when their models might be at risk of becoming
    less accurate due to shifts in the underlying data.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移检测**：DataRobot持续监控输入数据分布的变化，识别任何与原始训练数据的偏差。此功能帮助用户了解模型是否因基础数据发生变化而可能面临准确度下降的风险。'
- en: '**Model performance monitoring**: Users can track the performance of their
    models over time. This includes the following:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能监控**：用户可以跟踪其模型随时间变化的性能。包括以下内容：'
- en: '**Accuracy**: This can be monitored by comparing the actual target values with
    the predicted values. Actual target values can be sent to the deployment any time
    after a prediction has been made with the prerequisite that an association ID
    has to be set and returned to connect the actual targets to the historical prediction
    requests.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：可以通过将实际目标值与预测值进行比较来进行监控。实际目标值可以在做出预测后随时发送到部署，但前提是必须设置关联ID并返回，以将实际目标与历史预测请求连接起来。'
- en: '**Fairness**: This ensures that AI models continue to provide fair and unbiased
    predictions in a production environment. An association ID is similarly required
    here, as it is tied to the accuracy-related performance metric. Key aspects of
    the fairness functionality for deployed models include the following:'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性**：确保AI模型在生产环境中继续提供公平和无偏的预测。这里同样需要一个关联ID，因为它与准确性相关的性能指标相关。部署模型的公平性功能的关键方面包括：'
- en: '**Fairness metrics tracking**: The platform tracks various fairness metrics,
    such as disparate impact, demographic parity, and equal opportunity, enabling
    users to assess the fairness of their models across different subgroups within
    the protected attributes.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性指标追踪**：平台追踪各种公平性指标，如差异影响、人口统计平衡和机会平等，使用户能够评估其模型在受保护属性不同子群体中的公平性。'
- en: '**Alerts and notifications**: DataRobot can be configured to send alerts and
    notifications if biases or disparities are detected, ensuring that users are promptly
    informed about any fairness issues that may arise during the model’s life cycle.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报和通知**：如果检测到偏差或差异，DataRobot可以配置为发送警报和通知，确保用户及时了解在模型生命周期中可能出现的任何公平性问题。'
- en: '**Humility rules**: Users can set actions to execute based on undesirable conditions.
    Supported conditions are defined as uncertainties in predictions, outlier input
    values or ranges, and low observation regions. Supported actions are recording
    the trigger, overriding the prediction with a defined prediction, and throwing
    an error. This enhances the user’s overall confidence in the model’s predictions
    and mitigates the risk of it making incorrect decisions based on low-confidence
    predictions, out-of-distribution data, or low observation data points.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谦逊规则**：用户可以设置基于不良条件执行的操作。支持的条件包括预测中的不确定性、异常输入值或范围，以及低观察区域。支持的操作包括记录触发器、用定义的预测覆盖预测结果，以及抛出错误。这增强了用户对模型预测的整体信心，减少了模型基于低信心水平的预测、分布外数据或低观察数据点做出错误决策的风险。'
- en: '**Deployment service health monitoring**: This includes total predictions made,
    total requests made, requests made over a defined time interval, aggregated response
    time (such as median), aggregated execution time (median), median peak load at
    calls per minute, data error rate, system error rate, number of consumers, and
    cache hit rate.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署服务健康监控**：包括总预测次数、总请求次数、在定义的时间间隔内发出的请求数、聚合响应时间（例如中位数）、聚合执行时间（中位数）、每分钟调用的中位数峰值负载、数据错误率、系统错误率、消费者数量以及缓存命中率。'
- en: '**Deployment notifications**: Be notified about changes in the deployment,
    either for all changes or just critical changes.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署通知**：有关部署变化的通知，可以选择通知所有变化或仅通知关键变化。'
- en: '**Custom metrics tracking and alerting**: DataRobot enables users to define
    and monitor custom performance metrics specific to their use cases. Users can
    set up alerts to notify them when certain thresholds are reached, ensuring prompt
    response to any performance-related issues.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义指标追踪和警报**：DataRobot允许用户定义和监控特定于其使用案例的自定义性能指标。用户可以设置警报，当某些阈值被达到时通知他们，从而确保及时响应任何与性能相关的问题。'
- en: Now, let’s practically explore the interface that DataRobot provides for model
    monitoring.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实际探索一下DataRobot提供的模型监控界面。
- en: Practically monitoring a deployed blueprint in DataRobot
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际监控在DataRobot中已部署的蓝图
- en: By clicking on the **Service Health** tab of the deployed model, you will be
    able to see the general service health monitoring. This is shown in *Figure 18**.25*.
    Additionally, you can check out the dashboards for data drift, accuracy, humility,
    fairness, and custom metrics each in their own tab under the deployed model. Notifications,
    on the other hand, are by default configured to be sent to all deployment activities
    and can be configured based on preferences.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 点击已部署模型的**服务健康**标签，你将能够查看整体服务健康监控。如*图18.25*所示。此外，你可以在已部署模型下的各自标签中查看数据漂移、准确性、谦逊、公平性和自定义指标的仪表盘。而通知则默认配置为发送至所有部署活动，并可以根据偏好进行配置。
- en: '![Figure 18.25 –The service health of the deployed model](img/B18187_18_25.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图18.25 – 已部署模型的服务健康状况](img/B18187_18_25.jpg)'
- en: Figure 18.25 –The service health of the deployed model
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.25 – 已部署模型的服务健康状况
- en: Next, we will discuss how a user can maintain the performance of the deployed
    model and ensure that the model can continuously deliver value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论用户如何维护已部署模型的性能，并确保模型能够持续提供价值。
- en: Governing through model maintenance in DataRobot
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过DataRobot进行模型维护的管理
- en: 'Model maintenance in DataRobot is a crucial aspect of managing deep learning
    models, ensuring that they continue to deliver accurate and reliable results throughout
    their life cycle. The platform provides several features to facilitate effective
    model maintenance:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot中的模型维护是管理深度学习模型的一个关键方面，确保它们在整个生命周期内持续提供准确和可靠的结果。平台提供了多个功能来促进有效的模型维护：
- en: '**Challenger models and model replacement**: DataRobot allows users to create
    and compare alternative models, known as challengers, against the currently deployed
    model. By evaluating the performance of these challenger models, users can identify
    potential improvements and decide if it’s necessary to replace the existing model
    with a better-performing alternative. Once a better model has been identified,
    it is referred to as the **Champion model**. It will then replace the previous
    model for the existing deployment. This maintains the same deployment ID and ensures
    a seamless transition to a better model.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挑战者模型和模型替换**：DataRobot允许用户创建并比较替代模型（称为挑战者模型）与当前已部署模型的表现。通过评估这些挑战者模型的表现，用户可以识别潜在的改进，并决定是否需要用性能更好的替代模型替换现有模型。一旦确定了更好的模型，它将被称为**冠军模型**，并替换掉之前的模型，保持相同的部署ID，确保顺利过渡到更好的模型。'
- en: '**Model versioning**: DataRobot tracks and manages different versions of a
    model, allowing users to easily revert to previous versions if needed. This feature
    ensures that users can maintain a history of their models and compare their performance
    across different versions.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型版本控制**：DataRobot跟踪并管理模型的不同版本，允许用户在需要时轻松恢复到之前的版本。此功能确保用户可以保留模型历史记录，并在不同版本之间进行性能对比。'
- en: '**Retraining and retraining policies**: Users can register data received by
    the deployed model, including input data and delayed target values, into the AI
    Catalog. This enables the models to be retrained with the most recent data. Additionally,
    DataRobot’s retraining policies provide a way to manage and automate the model
    updating process, ensuring that deployed models stay relevant and maintain optimal
    performance. Key aspects of retraining policies include the following:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**再训练和再训练策略**：用户可以将通过已部署模型接收到的数据，包括输入数据和延迟的目标值，注册到AI目录中。这使得模型能够使用最新的数据进行再训练。此外，DataRobot的再训练策略提供了一种管理和自动化模型更新过程的方法，确保已部署的模型保持相关性并维持最佳性能。再训练策略的关键方面包括以下内容：'
- en: '**Customizable triggers**: Users can define specific triggers for retraining,
    such as data drift, performance degradation, or scheduled intervals, to initiate
    the retraining process automatically when certain conditions are met.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可定制触发器**：用户可以定义用于再训练的特定触发器，例如数据漂移、性能下降或预定的时间间隔，以便在满足特定条件时自动启动再训练过程。'
- en: '**Data integration**: Retraining policies facilitate the seamless integration
    of new data into the model updating process, ensuring that models are trained
    on the most recent and relevant information.'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集成**：再训练策略促进了新数据无缝集成到模型更新过程中，确保模型训练时使用最新和最相关的信息。'
- en: '**Automated retraining and deployment workflow**: DataRobot automates the entire
    retraining process, from data ingestion and preprocessing to model building and
    validation, streamlining the model update workflow and reducing manual effort.
    You can choose to either maintain the same model with the same parameters, maintain
    the same model with hyperparameter optimization, or just choose the best model
    from autopilot. Retrained models are automatically evaluated against the new data
    using performance metrics, enabling users to assess the updated model’s performance
    and determine if it’s ready for redeployment. Once a retrained model meets the
    desired performance criteria you‘d wish to achieve, DataRobot facilitates its
    seamless deployment, replacing the existing model with minimal interruption. By
    employing retraining policies, DataRobot simplifies and automates the model updating
    process, helping users ensure that their deployed AI models remain accurate, relevant,
    and high-performing as new data and insights become available.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化再训练和部署工作流**：DataRobot自动化了整个再训练过程，从数据摄取和预处理到模型构建和验证，简化了模型更新工作流，并减少了人工工作量。用户可以选择保持相同的模型及其参数，保持相同的模型并进行超参数优化，或者从自动驾驶模式中选择最佳模型。再训练后的模型会使用性能指标自动与新数据进行评估，帮助用户评估更新后的模型性能，并确定其是否准备好重新部署。一旦再训练后的模型达到您希望实现的性能标准，DataRobot将帮助其无缝部署，最小化中断，替换掉现有模型。通过采用再训练策略，DataRobot简化并自动化了模型更新过程，帮助用户确保他们部署的AI模型在新数据和洞察可用时保持准确、相关和高性能。'
- en: By delving into the features supported by DataRobot and going through hands-on
    tutorials, we’ve gained significant insights into how DataRobot employs deep learning
    methods to process and analyze data, including unstructured and structured data.
    Now, let’s examine some real-world examples that demonstrate the capabilities
    of this technology, as shared by customers who were enthusiastic about their experiences
    with DataRobot.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入了解DataRobot支持的功能，并通过实践教程，我们获得了关于DataRobot如何利用深度学习方法处理和分析数据，包括非结构化数据和结构化数据的重大见解。现在，让我们来看看一些实际的案例，这些案例展示了该技术的能力，并由对DataRobot充满热情的客户分享了他们的经验。
- en: Exploring some customer success stories
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索一些客户成功案例
- en: 'DataRobot has empowered numerous organizations to achieve remarkable success
    through the implementation of deep learning solutions, particularly in handling
    unstructured data such as text and images. While most of these success stories
    remain confidential, we are fortunate to have a few customers who have enthusiastically
    shared their inspiring experiences, showcasing the transformative potential of
    deep learning in various industries. Some of these notable successes include the
    following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot使众多组织通过实施深度学习解决方案取得了显著成功，特别是在处理非结构化数据（如文本和图像）方面。虽然大多数成功案例仍然保密，但我们很幸运能有一些客户热情地分享了他们鼓舞人心的经验，展示了深度学习在各个行业中具有的变革潜力。其中一些显著的成功案例包括：
- en: Lenovo, a leading technology company, successfully implemented DataRobot’s Visual
    AI in its Brazilian laptop manufacturing facility to improve quality control and
    increase productivity. The Visual AI system helped increase label verification
    accuracy from 93% to 98% by automating the comparison of identification labels
    on laptops with their respective bill of materials. This implementation not only
    reduced errors in the manual labeling process but also had a positive impact on
    delivery times, customer satisfaction, and legal risk reduction for the manufacturer.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联想，这家领先的科技公司，成功在其巴西笔记本电脑制造厂实施了DataRobot的视觉AI，以改善质量控制并提高生产力。该视觉AI系统通过自动化比对笔记本电脑上的识别标签与其相应的物料清单，将标签验证的准确率从93%提高到了98%。此实施不仅减少了人工标签过程中的错误，还对交货时间、客户满意度以及制造商的法律风险产生了积极影响。
- en: OYAK Cement, a leading Turkish cement maker, successfully utilized DataRobot’s
    AI solutions to optimize their manufacturing processes, resulting in reduced costs
    and CO2 emissions. By implementing AI-assisted process control, OYAK increased
    alternative fuel usage by seven times, cutting almost 2% of total CO2 emissions
    and reducing costs by approximately $39 million. The company was also able to
    predict and prevent mechanical failures more efficiently, improving overall operational
    efficiency and environmental sustainability.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OYAK水泥，土耳其领先的水泥制造商，成功利用DataRobot的AI解决方案优化了其制造流程，从而降低了成本和CO2排放。通过实施AI辅助的过程控制，OYAK将替代燃料的使用量提高了七倍，减少了近2%的总CO2排放，并节省了约3900万美元的成本。该公司还能够更高效地预测和防止机械故障，提高了整体运营效率和环境可持续性。
- en: AUTOproff successfully implemented the DataRobot AI Platform, which included
    visual AI capabilities for processing image data of vehicles, to develop their
    Pricing Robot to make automated car value estimations. The AI-driven solution
    automated 55–60% of all estimates, leading to improved pricing accuracy and a
    significant reduction in the time required to generate quotes. As a result, the
    estimators could focus on rarer vehicles, enhancing their efficiency. The Pricing
    Robot’s success has played a pivotal role in AUTOproff’s European expansion, enabling
    the company to swiftly adapt to new markets. This has ultimately led to increased
    customer satisfaction and streamlined business operations.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUTOproff成功实施了DataRobot AI平台，其中包括处理车辆图像数据的视觉AI能力，开发了其定价机器人来进行自动化的汽车价值估算。这个AI驱动的解决方案自动化了55%–60%的所有估算，从而提高了定价准确性，并显著缩短了生成报价所需的时间。因此，评估人员可以专注于更为稀有的车辆，提高了他们的效率。定价机器人的成功在AUTOproff的欧洲扩展中发挥了关键作用，使公司能够迅速适应新市场。这最终提高了客户满意度，并优化了业务运作。
- en: For more information on the latest success stories, check out [https://www.datarobot.com/customers/](https://www.datarobot.com/customers/).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 获取更多关于最新成功案例的信息，请访问[https://www.datarobot.com/customers/](https://www.datarobot.com/customers/)。
- en: As we reach the end of this chapter, if you are interested in trying out the
    DataRobot AI Platform for yourself and don’t already have access, you can subscribe
    for a free trial. And that’s it! This will allow you to experience first-hand
    the powerful tools and automation features that DataRobot offers for 30 days (as
    of 28 September 2023), enabling you to focus on extracting significant value from
    your deep learning applications.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们走到本章的尾声时，如果您有兴趣亲自试用DataRobot AI平台，并且尚未拥有访问权限，您可以订阅免费的试用版。这将使您能够亲身体验DataRobot所提供的强大工具和自动化功能，试用期为30天（截至2023年9月28日），帮助您专注于从深度学习应用中提取显著价值。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter explored the DataRobot AI Platform and showcased the benefits an
    AI platform can provide to you in general. DataRobot streamlines the complex stages
    of the machine learning life cycle, providing an intuitive interface for data
    scientists, engineers, and researchers. By harnessing the potential of AI platforms
    such as DataRobot, users can accelerate the creation, training, deployment, and
    governance of intricate deep learning models, focusing on extracting significant
    value from their machine learning applications.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了DataRobot AI平台，并展示了AI平台能为您提供的普遍好处。DataRobot简化了机器学习生命周期中的复杂阶段，为数据科学家、工程师和研究人员提供了直观的界面。通过利用像DataRobot这样的AI平台的潜力，用户可以加速创建、训练、部署和管理复杂的深度学习模型，专注于从机器学习应用中提取显著价值。
- en: DataRobot offers automation, collaboration, and scalability for machine learning
    use cases. DataRobot provides support for various data types and advanced features
    such as bias and fairness mitigation, Composable ML, custom tasks, advanced tuning,
    and time-series modeling. DataRobot also enables users to deploy AI applications
    on cloud-based or on-premises infrastructure, manage prediction outputs, monitor
    model performance, and maintain models implemented in features such as **Challenger
    Models**, **Model Versioning**, **Retraining**, and **Retraining policies**.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot为机器学习应用场景提供自动化、协作和可扩展性。DataRobot支持多种数据类型和先进功能，如偏差与公平性缓解、可组合机器学习、定制任务、先进调优和时间序列建模等。DataRobot还使用户能够在基于云的或本地的基础设施上部署AI应用程序，管理预测输出，监控模型性能，并维护在**挑战者模型**、**模型版本控制**、**再训练**和**再训练策略**等功能中实现的模型。
- en: While this chapter showcased the various features and capabilities of the DataRobot
    AI platform, it is not a comprehensive coverage of what the platform provides.
    Additionally, the company constantly evolves to attend to real-world data science
    needs, so any unsupported features may be added in the future. For a more detailed
    understanding, you can refer to the official documentation at [https://docs.datarobot.com/](https://docs.datarobot.com/).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了DataRobot AI平台的各种功能和能力，但并未全面覆盖该平台提供的所有内容。此外，随着公司不断发展以满足现实世界数据科学的需求，未来可能会添加任何尚未支持的功能。欲了解更详细的信息，可以参考官方文档
    [https://docs.datarobot.com/](https://docs.datarobot.com/)。
- en: In summary, AI platforms such as DataRobot offer a powerful solution for deep
    learning applications, streamlining and accelerating the deep learning life cycle.
    However, they are not a replacement for the expertise and creativity of data scientists,
    engineers, analysts, or researchers; instead, they serve as tools to assist practitioners
    in solving complex problems quickly and accurately.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，像DataRobot这样的AI平台为深度学习应用提供了强大的解决方案，简化并加速了深度学习生命周期。然而，它们并不是数据科学家、工程师、分析师或研究人员的专业知识和创造力的替代品；相反，它们是帮助从业人员快速且准确地解决复杂问题的工具。
- en: As we move forward to the next chapter, we will delve deeper into the world
    of large language models, exploring their potential, challenges, and ways to create
    effective solutions. Building upon the foundation from all the previous chapters,
    we’ll uncover how to harness the power of LLMs to tackle complex language-related
    tasks and create advanced, contextually-aware applications.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进入下一章时，我们将深入探讨大语言模型的世界，探索它们的潜力、挑战以及如何创造有效的解决方案。在前面所有章节的基础上，我们将揭示如何利用LLM的强大功能来处理复杂的语言任务，并创建先进的、具备上下文意识的应用程序。
