- en: 6\. Neural Networks and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 神经网络与深度学习
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you will be introduced to the final topic on neural networks
    and deep learning. You will be learning about TensorFlow, Convolutional Neural
    Networks (CNNs), and Recurrent Neural Networks (RNNs). You will use key deep learning
    concepts to determine creditworthiness of individuals and predict housing prices
    in a neighborhood. Later on, you will also implement an image classification program
    using the skills you learned. By the end of this chapter, you will have a firm
    grasp on the concepts of neural networks and deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解神经网络和深度学习的最终主题。你将学习 TensorFlow、卷积神经网络（CNNs）和递归神经网络（RNNs）。你将利用深度学习的核心概念来评估个人的信用worthiness并预测社区的房价。稍后，你还将使用学到的技能实现一个图像分类程序。到本章结束时，你将牢牢掌握神经网络和深度学习的概念。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we learned about what clustering problems are and saw
    several algorithms, such as k-means, that can automatically group data points
    on their own. In this chapter, we will learn about neural networks and deep learning networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了聚类问题的概念，并了解了几种算法，如 k-means，它们可以自动将数据点分组。在本章中，我们将学习神经网络和深度学习网络。
- en: The difference between neural networks and deep learning networks is the complexity
    and depth of the networks. Traditionally, neural networks have only one hidden
    layer, while deep learning networks have more than that.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络与深度学习网络的区别在于网络的复杂性和深度。传统的神经网络只有一个隐藏层，而深度学习网络则有多个隐藏层。
- en: Although we will use neural networks and deep learning for supervised learning,
    note that neural networks can also model unsupervised learning techniques. This
    kind of model was actually quite popular in the 1980s, but because the computation
    power required was limited at the time, it's only recently that this model has
    been widely adopted. With the democratization of Graphics Processing Units (GPUs)
    and cloud computing, we now have access to a tremendous amount of computation
    power. This is the main reason why neural networks and especially deep learning
    are hot topics again.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将使用神经网络和深度学习进行监督学习，但请注意，神经网络也可以建模无监督学习技术。这种模型在1980年代其实非常流行，但由于当时计算能力有限，直到最近，这种模型才被广泛采用。随着图形处理单元（GPU）和云计算的普及，我们现在可以获得巨大的计算能力。这正是神经网络，尤其是深度学习，重新成为热门话题的主要原因。
- en: Deep learning can model more complex patterns than traditional neural networks,
    and so deep learning is more widely used nowadays in computer vision (in applications
    such as face detection and image recognition) and natural language processing
    (in applications such as chatbots and text generation).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习能够建模比传统神经网络更复杂的模式，因此如今在计算机视觉（如面部检测和图像识别）和自然语言处理（如聊天机器人和文本生成）中得到了广泛应用。
- en: Artificial Neurons
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: '**Artificial Neural Networks** (**ANNs**), as the name implies, try to replicate
    how a human brain works, and more specifically how neurons work.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**），顾名思义，试图模仿人类大脑的工作方式，特别是神经元的工作方式。'
- en: A neuron is a cell in the brain that communicates with other cells via electrical
    signals. Neurons can respond to stimuli such as sound, light, and touch. They
    can also trigger actions such as muscle contractions. On average, a human brain
    contains 10 to 20 billion neurons. That's a pretty huge network, right? This is
    the reason why humans can achieve so many amazing things. This is also why researchers
    have tried to emulate how the brain operates and in doing so created ANNs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是大脑中的一种细胞，通过电信号与其他细胞进行通信。神经元能够响应声音、光线、触摸等刺激。它们也能引发动作，比如肌肉收缩。平均而言，人脑包含约100亿到200亿个神经元。这是一个相当庞大的网络，对吧？这也是人类能够实现如此多惊人事物的原因。这也是为什么研究人员试图模仿大脑的运作，并由此创造了人工神经网络。
- en: 'ANNs are composed of multiple artificial neurons that connect to each other
    and form a network. An artificial neuron is simply a processing unit that performs
    mathematical operations on some inputs (`x1`, `x2`, …, `xn`) and returns the final
    results (`y`) to the next unit, as shown here:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络由多个相互连接的人工神经元组成，形成一个网络。人工神经元简单来说是一个处理单元，它对一些输入（`x1`、`x2`、……、`xn`）进行数学运算，并将最终结果（`y`）返回给下一个单元，如下图所示：
- en: '![Figure 6.1: Representation of an artificial neuron'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1：人工神经元的表示'
- en: '](img/B16060_06_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_01.jpg)'
- en: 'Figure 6.1: Representation of an artificial neuron'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：人工神经元的表示
- en: We will see how an artificial neuron works more in detail in the coming sections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中更详细地了解人工神经元的工作原理。
- en: Neurons in TensorFlow
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 中的神经元
- en: TensorFlow is currently the most popular neural network and deep learning framework.
    It was created and is maintained by Google. TensorFlow is used for voice recognition
    and voice search, and it is also the brain behind [translate.google.com](http://translate.google.com).
    Later in this chapter, we will use TensorFlow to recognize written characters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 目前是最流行的神经网络和深度学习框架。它由 Google 创建并维护。TensorFlow 用于语音识别和语音搜索，也是 [translate.google.com](http://translate.google.com)
    背后的大脑。稍后在本章中，我们将使用 TensorFlow 来识别书写字符。
- en: The TensorFlow API is available in many languages, including Python, JavaScript,
    Java, and C. TensorFlow works with **tensors**. You can think of a tensor as a
    container composed of a matrix (usually with high dimensions) and additional information related
    to the operations it will perform (such as weights and biases, which you will
    be looking at later in this chapter). A tensor with no dimensions (with no rank)
    is a scalar. A tensor of rank 1 is a vector, rank 2 tensors are matrices, and
    a rank 3 tensor is a three-dimensional matrix. The rank indicates the dimensions
    of a tensor. In this chapter, we will be looking at tensors of ranks 2 and 3.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow API 支持多种编程语言，包括 Python、JavaScript、Java 和 C。TensorFlow 与 **张量** 一起工作。你可以将张量视为一个容器，它由一个矩阵（通常是高维的）和与其将执行的操作相关的附加信息组成（如权重和偏差，你将在本章稍后看到）。没有维度（即无秩）的张量是标量。秩为
    1 的张量是向量，秩为 2 的张量是矩阵，秩为 3 的张量是三维矩阵。秩表示张量的维度。在本章中，我们将讨论秩为 2 和 3 的张量。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Mathematicians use the terms matrix and dimension, whereas deep learning programmers
    use tensor and rank instead.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家使用矩阵和维度的术语，而深度学习程序员则使用张量和秩。
- en: 'TensorFlow also comes with mathematical functions to transform tensors, such
    as the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 还提供了数学函数来变换张量，例子包括以下内容：
- en: '`add` and `multiply`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add` 和 `multiply`'
- en: '`exp` and `log`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exp` 和 `log`'
- en: '`greater`, `less`, and `equal`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`greater`、`less` 和 `equal`'
- en: '`concat`, `slice`, and `split`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concat`、`slice` 和 `split`'
- en: '`matrix_inverse`, `matrix_determinant`, and `matmul`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_inverse`、`matrix_determinant` 和 `matmul`'
- en: '`sigmoid`, `relu`, and `softmax`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sigmoid`、`relu` 和 `softmax`'
- en: We will go through them in more detail later in this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后详细讲解这些内容。
- en: In the next exercise, we will be using TensorFlow to compute an artificial neuron.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将使用 TensorFlow 来计算一个人工神经元。
- en: 'Exercise 6.01: Using Basic Operations and TensorFlow Constants'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.01：使用基本操作和 TensorFlow 常量
- en: In this exercise, we will be using arithmetic operations in TensorFlow to emulate
    an artificial neuron by performing a matrix multiplication and addition, and applying
    a non-linear function, `sigmoid`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 TensorFlow 中的算术操作，通过执行矩阵乘法和加法，并应用非线性函数 `sigmoid` 来模拟一个人工神经元。
- en: 'The following steps will help you complete the exercise:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件。
- en: 'Import the `tensorflow` package as `tf`:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `tensorflow` 包并将其命名为 `tf`：
- en: '[PRE0]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a tensor called `W` of shape `[1,6]` (that is, with 1 row and 6 columns),
    using `tf.constant()`, that contains the matrix `[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]`.
    Print its value:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `W` 的形状为 `[1,6]`（即 1 行 6 列）的张量，使用 `tf.constant()`，并使其包含矩阵 `[1.0, 2.0,
    3.0, 4.0, 5.0, 6.0]`。打印其值：
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The expected output is this:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE2]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a tensor called `X` of shape `[6,1]` (that is, with 6 rows and 1 column),
    using `tf.constant()`, that contains `[7.0, 8.0, 9.0, 10.0, 11.0, 12.0]`. Print
    its value:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `X` 的形状为 `[6,1]`（即 6 行 1 列）的张量，使用 `tf.constant()`，并使其包含 `[7.0, 8.0, 9.0,
    10.0, 11.0, 12.0]`。打印其值：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The expected output is this:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, create a tensor called `b`, using `tf.constant()`, that contains `-88`.
    Print its value:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个名为 `b` 的张量，使用 `tf.constant()`，并使其包含 `-88`。打印其值：
- en: '[PRE5]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The expected output is this:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE6]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Perform a matrix multiplication between `W` and `X` using `tf.matmul`, save
    its results in the `mult` variable, and print its value:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `W` 和 `X` 之间执行矩阵乘法，使用 `tf.matmul`，将其结果保存在 `mult` 变量中，并打印其值：
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The expected output is this:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE8]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Perform a matrix addition between `mult` and `b`, save its results in a variable
    called `Z`, and print its value:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `mult` 和 `b` 之间执行矩阵加法，并将结果保存在名为 `Z` 的变量中，打印其值：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The expected output is this:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Apply the `sigmoid` function to `Z` using `tf.math.sigmoid`, save its results
    in a variable called `a`, and print its value. The `sigmoid` function transforms
    any numerical value within the range **0** to **1** (we will learn more about
    this in the following sections):'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tf.math.sigmoid` 对 `Z` 应用 `sigmoid` 函数，将其结果保存在名为 `a` 的变量中，并打印其值。`sigmoid`
    函数将任何数值转化到 **0** 到 **1** 的范围内（我们将在后续章节中深入了解这一点）：
- en: '[PRE11]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The expected output is this:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出是这样的：
- en: '[PRE12]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `sigmoid` function has transformed the original value of `Z`, which was
    `129`, to `1`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 函数将 `Z` 的原始值 `129` 转换为 `1`。'
- en: Note
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ekGLM](https://packt.live/31ekGLM).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问本节的源代码，请参考 [https://packt.live/31ekGLM](https://packt.live/31ekGLM)。
- en: You can also run this example online at [https://packt.live/3evuKnC](https://packt.live/3evuKnC).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/3evuKnC](https://packt.live/3evuKnC) 在线运行此示例。你必须执行整个笔记本才能获得预期的结果。
- en: In this exercise, you successfully implemented an artificial neuron using TensorFlow.
    This is the base of any neural network model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，你成功地使用 TensorFlow 实现了一个人工神经元。这是任何神经网络模型的基础。
- en: In the next section, we will be looking at the architecture of neural networks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究神经网络的架构。
- en: Neural Network Architecture
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: Neural networks are the newest branch of **Artificial Intelligence** (**AI**).
    Neural networks are inspired by how the human brain works. They were invented
    in the 1940s by Warren McCulloch and Walter Pitts. The neural network was a mathematical
    model that was used to describe how the human brain can solve problems.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是**人工智能**（**AI**）最新的分支。神经网络的灵感来源于人类大脑的工作方式。它们由 Warren McCulloch 和 Walter
    Pitts 在 1940 年代发明。神经网络是一个数学模型，用于描述人类大脑如何解决问题。
- en: We will use ANN to refer to both the mathematical model, and the biological
    neural network when talking about the human brain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论人类大脑时，使用 ANN 来指代数学模型和生物神经网络。
- en: The way a neural network learns is more complex compared to other classification
    or regression models. The neural network model has a lot of internal variables,
    and the relationship between the input and output variables may involve multiple
    internal layers. Neural networks have higher accuracy than other supervised learning algorithms.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习方式比其他分类或回归模型更为复杂。神经网络模型具有许多内部变量，输入和输出变量之间的关系可能涉及多个内部层次。神经网络的准确性通常高于其他监督学习算法。
- en: Note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Mastering neural networks with TensorFlow is a complex process. The purpose
    of this section is to provide you with an introductory resource to get started.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 精通使用 TensorFlow 构建神经网络是一个复杂的过程。本节的目的是为你提供一个入门资源，帮助你开始学习。
- en: In this chapter, the main example we are going to use is the recognition of
    digits from an image. We are considering this format since each image is small,
    and we have around 70,000 images available. The processing power required to process
    these images is similar to that of a regular computer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用的主要示例是从图像中识别数字。我们采用这种格式是因为每张图像较小，并且我们有约 70,000 张图像可供使用。处理这些图像所需的计算能力类似于普通计算机的能力。
- en: ANNs work similarly to how the human brain works. A dendroid in a human brain
    is connected to a nucleus, and the nucleus is connected to an axon. In an ANN,
    the input is the dendroid, where the calculations occur is the nucleus, and the
    output is the axon.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（ANN）工作原理与人类大脑相似。人类大脑中的树突与细胞核相连，细胞核与轴突相连。在神经网络中，输入相当于树突，计算发生的地方是细胞核，输出则是轴突。
- en: 'An artificial neuron is designed to replicate how a nucleus works. It will
    transform an input signal by calculating a matrix multiplication followed by an
    activation function. If this function determines that a neuron has to fire, a
    signal appears in the output. This signal can be the input of other neurons in
    the network:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元被设计成模拟细胞核的工作方式。它将通过矩阵乘法计算和激活函数来转化输入信号。如果激活函数判定神经元必须激发，那么一个信号将出现在输出端。这个信号可以是网络中其他神经元的输入：
- en: '![Figure 6.2: Figure showing how an ANN works'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2：显示神经网络如何工作的图'
- en: '](img/B16060_06_02.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_02.jpg)'
- en: 'Figure 6.2: Figure showing how an ANN works'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：显示神经网络如何工作的图
- en: 'Let''s understand the preceding figure further by taking the example of `n=4`.
    In this case, the following applies:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以 `n=4` 为例，我们可以进一步理解前面的图形。在这种情况下，适用的公式为：
- en: '`X` is the input matrix, which is composed of `x1`, `x2`, `x3`, and `x4`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X` 是输入矩阵，由 `x1`、`x2`、`x3` 和 `x4` 组成。'
- en: '`W`, the weight matrix, will be composed of `w1`, `w2`, `w3`, and `w4`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`W`，权重矩阵，将由 `w1`、`w2`、`w3` 和 `w4` 组成。'
- en: '`b` is the bias.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b` 是偏置。'
- en: '`f` is the activation function.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f` 是激活函数。'
- en: 'We will first calculate `Z` (the left-hand side of the neuron) with matrix
    multiplication and bias:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过矩阵乘法和偏置计算 `Z`（神经元的左侧）：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then the output, `y`, will be calculated by applying a function, `f`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输出 `y` 将通过应用函数 `f` 来计算：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Great – this is how an artificial neuron works under the hood. It is two matrix
    operations, a product followed by a sum, and a function transformation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 很好——这就是人工神经元在幕后工作的方式。它是两个矩阵运算，先乘积后求和，再经过函数变换。
- en: We now move on to the next section – weights.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入下一部分——权重。
- en: Weights
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重
- en: '`y`.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`y`。'
- en: 'A single neuron is the combination of the weighted sum and the activation function
    and can be referred to as a hidden layer. A neural network with one hidden layer
    is called a **regular neural network**:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单一的神经元是加权和与激活函数的组合，可以称之为隐藏层。具有一个隐藏层的神经网络称为**常规神经网络**：
- en: '![Figure 6.3: Neurons 1, 2, and 3 form the hidden layer of this sample network'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3：神经元1、2和3构成了这个示例网络的隐藏层](img/B16060_06_03.jpg)'
- en: '](img/B16060_06_03.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_03.jpg)'
- en: 'Figure 6.3: Neurons 1, 2, and 3 form the hidden layer of this sample network'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：神经元1、2和3构成了这个示例网络的隐藏层
- en: When connecting inputs and outputs, we may have multiple hidden layers. A neural
    network with multiple layers is called a **deep neural network**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接输入和输出时，我们可能会有多个隐藏层。具有多个层的神经网络称为**深度神经网络**。
- en: The term deep learning comes from the presence of multiple layers. When creating
    an **Artificial Neural Network** (**ANN**), we can specify the number of hidden
    layers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习”一词来自于多个层次的存在。在创建**人工神经网络**（**ANN**）时，我们可以指定隐藏层的数量。
- en: Biases
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏置
- en: 'Previously, we saw that the equation for a neuron is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到神经元的方程如下：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The problem with this equation is that there is no constant factor that depends
    on the inputs `x1`, `x2`, `x3`, and `x4`. The preceding equation can model any
    linear function that will go through the point 0: if all `w` values are equal
    to 0 then `y` will also equal to 0\. But what about other functions that don''t
    go through the point 0? For example, imagine that we are predicting the probability
    of churn for an employee by their month of tenure. Even if they haven''t worked
    for the full month yet, the probability of churn is not zero.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程的问题在于，缺少一个依赖于输入 `x1`、`x2`、`x3` 和 `x4` 的常数因子。前面的方程可以表示任何经过原点的线性函数：如果所有 `w`
    值都等于0，那么 `y` 也会等于0。但对于那些不经过原点的其他函数呢？例如，假设我们正在预测某员工的流失概率，按其在职月份来算。即使他们还没有工作满一个月，流失概率也不可能为零。
- en: To accommodate this situation, we need to introduce a new parameter called `b`
    can equal to 0.5 and therefore the churn probability for a new employer during
    the first month will be 50%.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应这种情况，我们需要引入一个新参数 `b`，它可以等于 0.5，因此新雇员在第一个月的流失概率将是 50%。
- en: 'Therefore, we add bias to the equation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在方程中加入偏置：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first equation is the verbose form, describing the role of each coordinate,
    weight coefficient, and bias. The second equation is the vector form, where `x
    = (x1, x2, x3, x4)` and `w = (w1, w2, w3, w4)`. The dot operator between the vectors
    symbolizes the dot or scalar product of the two vectors. The two equations are
    equivalent. We will use the second form in practice because it is easier to define
    a vector of variables using TensorFlow than to define each variable one by one.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程是详细形式，描述了每个坐标、权重系数和偏置的作用。第二个方程是向量形式，其中 `x = (x1, x2, x3, x4)` 和 `w = (w1,
    w2, w3, w4)`。向量之间的点运算符表示这两个向量的点积或标量积。这两个方程是等效的。我们在实践中将使用第二种形式，因为使用 TensorFlow
    定义变量向量比逐一定义每个变量更为简便。
- en: Similarly, for `w1`, `w2`, `w3`, and `w4`, the bias, `b`, is a variable, meaning
    that its value can change during the learning process.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于 `w1`、`w2`、`w3` 和 `w4`，偏置 `b` 是一个变量，意味着它的值在学习过程中可以发生变化。
- en: With this constant factor built into each neuron, a neural network model becomes
    more flexible in terms of fitting a specific training dataset better.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在每个神经元中内置这个常数因子，神经网络模型变得更加灵活，能够更好地拟合特定的训练数据集。
- en: Note
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It may happen that the product `p = x1*w1 + x2*w2 + x3*w3 + x4*w4` is negative
    due to the presence of a few negative weights. We may still want to give the model
    the flexibility to execute (*or fire*) a neuron with values above a given negative
    number. Therefore, adding a constant bias, `b = 5`, for instance, can ensure that
    the neuron fires for values between `-5` and `0` as well.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某些负权重的存在，可能会发生产品`p = x1*w1 + x2*w2 + x3*w3 + x4*w4`为负的情况。我们可能仍然希望给予模型一定的灵活性，使其在值大于某个负数时能够激活（*或触发*）神经元。因此，添加一个常数偏置，例如`b
    = 5`，可以确保神经元在`-5`到`0`之间的值也能触发。
- en: 'TensorFlow provides the `Dense()` class to model the hidden layer of a neural
    network (*also called the fully connected layer*):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了`Dense()`类来建模神经网络的隐藏层（*也叫做全连接层*）：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this example, we have created a fully connected layer of `128` neurons that
    takes as input a tensor of shape `200`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建了一个包含`128`个神经元的全连接层，该层的输入为形状为`200`的张量。
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information on this TensorFlow class at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)找到有关此TensorFlow类的更多信息。
- en: 'The `Dense()` class is expected to have a flattened input (only one row). For
    instance, if your input is of shape `28` by `28`, you will have to flatten it
    beforehand with the `Flatten()` class in order to get a single row with 784 neurons
    (`28 * 28`):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense()`类要求输入是一个展平的数组（只有一行）。例如，如果输入的形状是`28`×`28`，你需要先使用`Flatten()`类将其展平，才能得到一个包含784个神经元（`28
    * 28`）的单行数据：'
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information on this TensorFlow class at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)找到有关此TensorFlow类的更多信息。
- en: In the following sections, we will learn about how we can extend this layer
    of neurons with additional parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习如何使用额外的参数扩展这一神经元层。
- en: Use Cases for ANNs
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）的应用场景
- en: 'ANNs have their place among supervised learning techniques. They can model
    both classification and regression problems. A classifier neural network seeks
    a relationship between features and labels. The features are the input variables,
    while each class the classifier can choose as a return value is a separate output.
    In the case of regression, the input variables are the features, while there is
    one single output: the predicted value. While traditional classification and regression
    techniques have their use cases in AI, ANNs are generally better at finding complex
    relationships between inputs and outputs.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）在监督学习技术中占有一席之地。它们可以处理分类和回归问题。分类器神经网络试图找到特征与标签之间的关系。特征是输入变量，而分类器可以选择作为返回值的每个类别是一个独立的输出。在回归的情况下，输入变量是特征，而只有一个输出：预测值。尽管传统的分类和回归技术在人工智能中有其应用场景，但人工神经网络通常更擅长发现输入与输出之间复杂的关系。
- en: In the next section, we will be looking at activation functions and their different
    types.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨激活函数及其不同类型。
- en: Activation Functions
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: As seen previously, a single neuron needs to perform a transformation by applying
    an activation function. Different activation functions can be used in neural networks.
    Without these functions, a neural network would simply be a linear model that
    could easily be described using matrix multiplication.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一个神经元需要通过应用激活函数来执行变换。神经网络中可以使用不同的激活函数。如果没有这些函数，神经网络将仅仅是一个线性模型，可以通过矩阵乘法轻松描述。
- en: The activation function of a neural network provides non-linearity and therefore
    can model more complex patterns. Two very common activation functions are `sigmoid`
    and `tanh` (the hyperbolic tangent function).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的激活函数提供了非线性，因此能够建模更复杂的模式。两种非常常见的激活函数是`sigmoid`和`tanh`（双曲正切函数）。
- en: Sigmoid
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The formula of `sigmoid` is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`的公式如下：'
- en: '![Figure 6.4: The sigmoid formula'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4：Sigmoid公式'
- en: '](img/B16060_06_04.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_04.jpg)'
- en: 'Figure 6.4: The sigmoid formula'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：Sigmoid公式
- en: The output values of a `sigmoid` function range from **0** to **1**. This activation
    function is usually used at the last layer of a neural network for a binary classification
    problem.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数的输出值范围是**0**到**1**。这种激活函数通常用于神经网络的最后一层，用于二分类问题。'
- en: Tanh
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tanh
- en: 'The formula of the hyperbolic tangent is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切的公式如下：
- en: '![Figure 6.5: The tanh formula'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：tanh 公式'
- en: '](img/B16060_06_05.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_05.jpg)'
- en: 'Figure 6.5: The tanh formula'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：tanh 公式
- en: The `tanh` activation function is very similar to the `sigmoid` function and
    was quite popular until recently. It is usually used in the hidden layers of a
    neural network. Its values range between **-1** and **1**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`tanh` 激活函数与 `sigmoid` 函数非常相似，并且直到最近都非常流行。它通常用于神经网络的隐藏层。它的值范围在 **-1** 和 **1**
    之间。'
- en: ReLU
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU
- en: 'Another important activation function is `relu`. **ReLU** stands for **Rectified
    Linear Unit**. It is currently the most widely used activation function for hidden
    layers. Its formula is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的激活函数是 `relu`。**ReLU** 代表 **Rectified Linear Unit**，目前是最广泛使用的隐藏层激活函数。其公式如下：
- en: '![Figure 6.6: The ReLU formula'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.6：ReLU 公式'
- en: '](img/B16060_06_06.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_06.jpg)'
- en: 'Figure 6.6: The ReLU formula'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：ReLU 公式
- en: There are now different variants of `relu` functions, such as `leaky ReLU` and `PReLU`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有不同版本的 `relu` 函数，例如 `leaky ReLU` 和 `PReLU`。
- en: Softmax
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax
- en: 'The function shrinks the values of a list to be between `softmax` function
    is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将列表中的值缩放到 `softmax` 函数如下所示的范围：
- en: '![Figure 6.7: The softmax formula'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7：softmax 公式'
- en: '](img/B16060_06_07.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_07.jpg)'
- en: 'Figure 6.7: The softmax formula'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：softmax 公式
- en: The `softmax` function is usually used as the last layer of a neural network
    for multi-class classification problems as it can generate probabilities for each
    of the different output classes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`softmax` 函数通常作为神经网络的最后一层，用于多类分类问题，因为它能够为每个不同的输出类别生成概率。'
- en: 'Remember, in TensorFlow, we can extend a `Dense()` layer with an activation
    function; we just need to set the `activation` parameter. In the following example,
    we will add the `relu` activation function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在 TensorFlow 中，我们可以通过激活函数扩展 `Dense()` 层；只需要设置 `activation` 参数即可。在以下示例中，我们将添加
    `relu` 激活函数：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let's use these different activation functions and observe how these functions
    dampen the weighted inputs by solving the following exercise.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这些不同的激活函数，观察它们如何通过解决以下练习来抑制加权输入。
- en: 'Exercise 6.02: Activation Functions'
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.02：激活函数
- en: 'In this exercise, we will be implementing the following activation functions
    using the `numpy` package: `sigmoid`, `tanh`, `relu`, and `softmax`.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用 `numpy` 包实现以下激活函数：`sigmoid`、`tanh`、`relu` 和 `softmax`。
- en: 'The following steps will help you complete the exercise:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件。
- en: 'Import the `numpy` package as `np`:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy` 包并命名为 `np`：
- en: '[PRE20]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create a `sigmoid` function, as shown in the following code snippet, that implements
    the sigmoid formula (shown in the previous section) using the `np.exp()` method:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `sigmoid` 函数，如下所示的代码片段，使用 `np.exp()` 方法实现 sigmoid 公式（如前所述）：
- en: '[PRE21]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Calculate the result of `sigmoid` function on the value `-1`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 `sigmoid` 函数在值 `-1` 上的结果：
- en: '[PRE22]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The expected output is this:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE23]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is the result of performing a sigmoid transformation on the value `-1`.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是对值 `-1` 执行 sigmoid 变换后的结果。
- en: 'Import the `matplotlib.pyplot` package as `plt`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `matplotlib.pyplot` 包并命名为 `plt`：
- en: '[PRE24]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create a `numpy` array called `x` that contains values from `-10` to `10` evenly
    spaced by an increment of `0.1`, using the `np.arange()` method. Print its value:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `x` 的 `numpy` 数组，包含从 `-10` 到 `10` 的均匀间隔的值，增量为 `0.1`，使用 `np.arange()`
    方法。打印其值：
- en: '[PRE25]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The expected output is this:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE26]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Great – we generated a `numpy` array containing values between `-10` and `10`.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 很好——我们生成了一个包含从 `-10` 到 `10` 的值的 `numpy` 数组。
- en: Note
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding output is truncated.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的输出已被截断。
- en: 'Plot a line chart with `x` and `sigmoid(x)` using `plt.plot()` and `plt.show()`:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plt.plot()` 和 `plt.show()` 绘制 `x` 和 `sigmoid(x)` 的折线图：
- en: '[PRE27]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The expected output is this:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '![Figure 6.8: Line chart using the sigmoid function'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.8：使用 sigmoid 函数的折线图'
- en: '](img/B16060_06_08.jpg)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_08.jpg)'
- en: 'Figure 6.8: Line chart using the sigmoid function'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.8：使用 sigmoid 函数的折线图
- en: We can see here that the output of the `sigmoid` function ranges between `0`
    and `1`. The slope is quite steep for values around `0`.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，`sigmoid` 函数的输出范围在 `0` 和 `1` 之间。对于接近 `0` 的值，斜率非常陡峭。
- en: 'Create a `tanh()` function that implements the Tanh formula (shown in the previous
    section) using the `np.exp()` method:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `tanh()` 函数，使用 `np.exp()` 方法实现 Tanh 公式（如前所述）：
- en: '[PRE28]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Plot a line chart with `x` and `tanh(x)` using `plt.plot()` and `plt.show()`:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plt.plot()`和`plt.show()`绘制`x`和`tanh(x)`的折线图：
- en: '[PRE29]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The expected output is this:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '![Figure 6.9: Line chart using the tanh function'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.9：使用tanh函数的折线图'
- en: '](img/B16060_06_09.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_09.jpg)'
- en: 'Figure 6.9: Line chart using the tanh function'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.9：使用tanh函数的折线图
- en: The shape of the `tanh` function is very similar to `sigmoid` but its slope
    is steeper for values close to `0`. Remember, its range is between **-1** and
    **1**.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tanh`函数的形状与`sigmoid`非常相似，但在接近`0`的值时，它的斜率更陡。记住，它的值域介于**-1**和**1**之间。'
- en: 'Create a `relu` function that implements the ReLU formula (shown in the previous
    section) using the `np.maximum()` method:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`relu`函数，使用`np.maximum()`方法实现ReLU公式（如上一节所示）：
- en: '[PRE30]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Plot a line chart with `x` and `relu(x)` using `plt.plot()` and `plt.show()`:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plt.plot()`和`plt.show()`绘制`x`和`relu(x)`的折线图：
- en: '[PRE31]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The expected output is this:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '![Figure 6.10: Line chart using the relu function'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.10：使用relu函数的折线图'
- en: '](img/B16060_06_10.jpg)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_10.jpg)'
- en: 'Figure 6.10: Line chart using the relu function'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.10：使用relu函数的折线图
- en: The ReLU function equals `0` when values are negative, and equals the identity
    function, `f(x)=x`, for positive values.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当值为负数时，ReLU函数等于`0`，当值为正数时，ReLU函数等于恒等函数`f(x)=x`。
- en: 'Create a `softmax` function that implements the softmax formula (shown in the
    previous section) using the `np.exp()` method:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`softmax`函数，使用`np.exp()`方法实现softmax公式（如上一节所示）：
- en: '[PRE32]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Calculate the output of `softmax` on the list of values, `[0, 1, 168, 8, 2]`:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算列表`[0, 1, 168, 8, 2]`上`softmax`的输出：
- en: '[PRE33]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The expected output is this:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '[PRE34]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As expected, the item at the third position has the highest softmax probabilities
    as its original value was the highest.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，第三个位置的项具有最高的softmax概率，因为它的原始值是最高的。
- en: Note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fJzoOU](https://packt.live/3fJzoOU).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/3fJzoOU](https://packt.live/3fJzoOU)。
- en: You can also run this example online at [https://packt.live/3188pZi](https://packt.live/3188pZi).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，网址是 [https://packt.live/3188pZi](https://packt.live/3188pZi)。你必须执行整个Notebook，才能获得预期的结果。
- en: By completing this exercise, we have implemented some of the most important
    activation functions for neural networks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此练习后，我们实现了神经网络中一些最重要的激活函数。
- en: Forward Propagation and the Loss Function
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播和损失函数
- en: So far, we have seen how a neuron can take an input and perform some mathematical
    operations on it and get an output. We learned that a neural network is a combination
    of multiple layers of neurons.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到一个神经元如何接收输入并对其进行一些数学运算以得到输出。我们了解到，神经网络是由多个神经元层组合而成的。
- en: 'The process of transforming the inputs of a neural network into a result is
    called **forward propagation** (or the forward pass). What we are asking the neural
    network to do is to make a prediction (the final output of the neural network)
    by applying multiple neurons to the input data:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络的输入转化为结果的过程称为**前向传播**（或前向传递）。我们要求神经网络做的事情是，通过将多个神经元应用于输入数据，来进行预测（神经网络的最终输出）：
- en: '![Figure 6.11: Figure showing forward propagation'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11：展示前向传播的图示'
- en: '](img/B16060_06_11.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_11.jpg)'
- en: 'Figure 6.11: Figure showing forward propagation'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：展示前向传播的图示
- en: The neural network relies on the weights matrices, biases, and activation function
    of each neuron to calculate the predicted output value, ![b](img/B16060_06_11a.png).
    For now, let's assume the values of the weight matrices and biases are set in
    advance. The activation functions are defined when you design the architecture
    of the neural networks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络依赖于每个神经元的权重矩阵、偏置和激活函数来计算预测的输出值，![b](img/B16060_06_11a.png)。目前，我们假设权重矩阵和偏置的值已预设。激活函数在设计神经网络架构时定义。
- en: 'As for any supervised machine learning algorithm, the goal is to make accurate
    predictions. This implies that we need to assess how accurate the predictions
    are compared to the true values. For traditional machine learning algorithms,
    we used scoring metrics such as mean squared error, accuracy, or the F1 score.
    This can also be applied to neural networks, but the only difference is that such
    scores are used in two different ways:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何监督式机器学习算法，目标都是做出准确的预测。这意味着我们需要评估预测与真实值之间的准确度。对于传统的机器学习算法，我们使用诸如均方误差、准确率或
    F1 分数等评分指标。这同样适用于神经网络，但唯一的区别是，这些分数有两种不同的使用方式：
- en: They are used by data scientists to assess the performance of a model on training
    and testing sets and then tune hyperparameters if needed. This also applies to
    neural networks, so nothing new here.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家使用它们来评估模型在训练集和测试集上的表现，然后根据需要调整超参数。这同样适用于神经网络，所以这里没有什么新鲜的东西。
- en: They are used by neural networks to automatically learn from mistakes and update
    weight matrices and biases. This will be explained in more detail in the next
    section, which is about backpropagation. So, the neural network will use a metric
    (also called a **loss function**) to compare its predicted values, ![38](img/B16060_06_11b.png)
    to the true label, (y), and then learn how to make better predictions automatically.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络利用它们自动从错误中学习，并更新权重矩阵和偏差。这个过程将在下一节详细解释，那个部分讲的是反向传播。因此，神经网络将使用一个度量标准（也叫做**损失函数**）来比较预测值与真实标签（y），并学习如何自动进行更好的预测。
- en: 'The loss function is critical to a neural network learning to make good predictions.
    This is a hyperparameter that needs to be defined by data scientists while designing
    the architecture of a neural network. The choice of which loss function to use
    is totally arbitrary and depending on the dataset or the problem you want to solve,
    you will pick one or another. Luckily for us, though, there are some basic rules
    of thumb that work in most cases:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数对神经网络学习做出良好预测至关重要。这是一个超参数，需要数据科学家在设计神经网络架构时定义。选择使用哪种损失函数是完全任意的，取决于你想解决的数据集或问题，你会选择一种或另一种。幸运的是，有一些基本的经验法则在大多数情况下都有效：
- en: If you are working on a regression problem, you can use mean squared error.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在处理回归问题，可以使用均方误差。
- en: If it is a binary classification, the loss function should be binary cross-entropy.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是二分类问题，损失函数应该使用二元交叉熵。
- en: If it is a multi-class classification, then categorical cross-entropy should
    be your go-to choice.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是多类分类问题，那么你应该选择类别交叉熵作为损失函数。
- en: As a final note, the choice of loss function will also define which activation
    function you will have to use on the last layer of the neural network. Each loss
    function expects a certain type of data in order to properly assess prediction
    performance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要注意的是，损失函数的选择也将决定你在神经网络最后一层使用哪种激活函数。每个损失函数都需要特定类型的数据来正确评估预测性能。
- en: 'Here is the list of activation functions according to the loss function and
    type of project/problem:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是根据损失函数和项目/问题类型列出的激活函数：
- en: '![Figure 6.12: Overview of the different activation functions and their applications'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.12：不同激活函数及其应用概述'
- en: '](img/B16060_06_12.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_12.jpg)'
- en: 'Figure 6.12: Overview of the different activation functions and their applications'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12：不同激活函数及其应用概述
- en: 'With TensorFlow, in order to build your custom architecture, you can instantiate
    the `Sequential()` class and add your layers of fully connected neurons as shown
    in the following code snippet:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 时，为了构建自定义架构，你可以实例化 `Sequential()` 类，并按如下代码片段添加完全连接的神经元层：
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now it is time to have a look at how a neural network improves its predictions
    with backpropagation.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看神经网络是如何通过反向传播改进预测的。
- en: Backpropagation
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'Previously, we learned how a neural network makes predictions by using weight
    matrices and biases (we can combine them into a single matrix) from its neurons.
    Using the loss function, a network determines how good or bad the predictions
    are. It would be great if it could use this information and update the parameters
    accordingly. This is exactly what backpropagation is about: optimizing a neural
    network''s parameters.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们学习了神经网络如何通过使用其神经元的权重矩阵和偏置（我们可以将它们合并成一个单一的矩阵）来进行预测。通过使用损失函数，网络可以确定预测结果的好坏。如果它能利用这些信息并相应地更新参数，那就太好了。这正是反向传播的目的：优化神经网络的参数。
- en: Training a neural network involves executing forward propagation and backpropagation
    multiple times in order to make predictions and update the parameters from the
    errors. During the first pass (or propagation), we start by initializing all the
    weights of the neural network. Then, we apply forward propagation, followed by
    backpropagation, which updates the weights.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及多次执行前向传播和反向传播，以便进行预测并根据误差更新参数。在第一次传播中，我们首先初始化神经网络的所有权重。然后，进行前向传播，接着进行反向传播，后者会更新权重。
- en: We apply this process several times and the neural network will optimize its
    parameters iteratively. You can decide to stop this learning process by setting
    the maximum number of times the neural networks will go through the entire dataset
    (also called epochs) or define an early stop threshold if the neural network's
    score is not improving anymore after few epochs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次应用这个过程，神经网络会逐步优化其参数。你可以通过设置神经网络遍历整个数据集的最大次数（也叫做epoch）来决定是否停止这个学习过程，或者如果神经网络在几个epoch后得分不再提升，则定义一个提前停止的阈值。
- en: Optimizers and the Learning Rate
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器和学习率
- en: In the previous section, we saw that a neural network follows an iterative process
    to find the best solution for any input dataset. Its learning process is an optimization
    process. You can use different optimization algorithms (also called `Adam`, `SGD`,
    and `RMSprop`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到神经网络遵循一个迭代过程，以找到任何输入数据集的最佳解决方案。它的学习过程是一个优化过程。你可以使用不同的优化算法（也叫做`Adam`、`SGD`和`RMSprop`）。
- en: One important parameter for the neural networks optimizer is the learning rate.
    This value defines how quickly the neural network will update its weights. Defining
    a too-low learning rate will slow down the learning process and the neural network
    will take a long time before finding the right parameters. On the other hand,
    having too-high a learning rate can make the neural network not learn a solution
    as it is making bigger weight changes than required. A good practice is to start
    with a not-too-small learning rate (such as **0.01** or **0.001**), then stop
    the neural network training once its score starts to plateau or get worse, and
    lower the learning rate (by an order of magnitude, for instance) and keep training
    the network.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络优化器的一个重要参数是学习率。这个值定义了神经网络更新权重的速度。设置过低的学习率会减慢学习过程，神经网络需要很长时间才能找到合适的参数。另一方面，学习率过高会导致神经网络无法学习到合适的解决方案，因为它在每次更新时都会做出比需要更大的权重变化。一个好的做法是从一个不是很小的学习率开始（例如**0.01**或**0.001**），然后当神经网络的得分开始平稳或变差时停止训练，接着降低学习率（例如降低一个数量级）并继续训练网络。
- en: 'With TensorFlow, you can instantiate an optimizer from `tf.keras.optimizers`.
    For instance, the following code snippet shows us how to create an `Adam` optimizer
    with `0.001` as the learning rate and then compile our neural network by specifying
    the loss function (`''sparse_categorical_crossentropy''`) and metrics to be displayed
    (`''accuracy''`):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow，你可以从`tf.keras.optimizers`实例化一个优化器。例如，下面的代码片段展示了如何创建一个学习率为`0.001`的`Adam`优化器，然后通过指定损失函数（`'sparse_categorical_crossentropy'`）和要显示的指标（`'accuracy'`）来编译我们的神经网络：
- en: '[PRE36]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once the model is compiled, we can then train the neural network with the `.fit()`
    method like this:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型编译完成，我们就可以像这样使用`.fit()`方法训练神经网络：
- en: '[PRE37]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here we trained the neural network on the training set for `5` epochs. Once
    trained, we can use the model on the testing set and assess its performance with
    the `.evaluate()` method:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对训练集进行了`5`个epoch的神经网络训练。训练完成后，我们可以使用该模型在测试集上进行评估，并通过`.evaluate()`方法评估其性能：
- en: '[PRE38]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information on this TensorFlow optimizers at [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)上找到更多关于TensorFlow优化器的信息。
- en: In the next exercise, we will be training a neural network on a dataset.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将基于数据集训练一个神经网络。
- en: 'Exercise 6.03: Classifying Credit Approval'
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.03：分类信用批准
- en: In this exercise, we will be using the German credit approval dataset, and train
    a neural network to classify whether an individual is creditworthy or not.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用德国信用批准数据集，并训练一个神经网络来分类个人是否具备信用。
- en: Note
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset file can also be found in our GitHub repository:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件也可以在我们的GitHub仓库中找到：
- en: '[https://packt.live/2V7uiV5](https://packt.live/2V7uiV5).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.live/2V7uiV5](https://packt.live/2V7uiV5)。'
- en: 'The following steps will help you complete the exercise:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: 'Import the `loadtxt` method from `numpy`:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`numpy`中导入`loadtxt`方法：
- en: '[PRE39]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create a variable called `file_url` containing the link to the raw dataset:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`file_url`的变量，包含指向原始数据集的链接：
- en: '[PRE40]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Load the data into a variable called `data` using `loadtxt()` and specify the
    `delimiter='',''` parameter. Print its content:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`loadtxt()`加载数据到一个名为`data`的变量中，并指定`delimiter=','`参数。打印其内容：
- en: '[PRE41]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The expected output is this:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望的输出是这样的：
- en: '[PRE42]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Create a variable called `label` that contains the data only from the first
    column (this will be our response variable):'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`label`的变量，包含来自第一列的数据（这将是我们的响应变量）：
- en: '[PRE43]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create a variable called `features` that contains all the data except for the
    first column (which corresponds to the response variable):'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`features`的变量，包含除第一列外的所有数据（第一列对应于响应变量）：
- en: '[PRE44]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Import the `train_test_split` method from `sklearn.model_selection`:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.model_selection`中导入`train_test_split`方法：
- en: '[PRE45]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Split the data into training and testing sets and save the results into four
    variables called `features_train`, `features_test`, `label_train`, and `label_test`.
    Use 20% of the data for testing and specify `random_state=7`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集，并将结果保存到四个变量中，分别为`features_train`，`features_test`，`label_train`，`label_test`。使用20%的数据作为测试集，并指定`random_state=7`：
- en: '[PRE46]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`为`np`，`tensorflow`为`tf`，并从`tensorflow.keras`中导入`layers`：
- en: '[PRE47]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Set `1` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.random_seed()`和`tf.random.set_seed()`分别将`1`设置为`numpy`和`tensorflow`的种子：
- en: '[PRE48]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Instantantiate a `tf.keras.Sequential()` class and save it into a variable
    called `model`:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.Sequential()`类，并将其保存到一个名为`model`的变量中：
- en: '[PRE49]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Instantantiate a `layers.Dense()` class with `16` neurons, `activation=''relu''`,
    and `input_shape=[19]`, then save it into a variable called `layer1`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`layers.Dense()`类，包含`16`个神经元，`activation='relu'`，`input_shape=[19]`，然后将其保存到一个名为`layer1`的变量中：
- en: '[PRE50]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Instantantiate a second `layers.Dense()` class with `1` neuron and `activation=''sigmoid''`,
    then save it into a variable called `final_layer`:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化第二个`layers.Dense()`类，包含`1`个神经元，`activation='sigmoid'`，然后将其保存到一个名为`final_layer`的变量中：
- en: '[PRE51]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Add the two layers you just defined to the model using `.add()`:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.add()`将你刚才定义的两个层添加到模型中：
- en: '[PRE52]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Instantantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it into a variable called `optimizer`:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.optimizers.Adam()`类，学习率为`0.001`，并将其保存到名为`optimizer`的变量中：
- en: '[PRE53]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Compile the neural network using `.compile()` with `loss=''binary_crossentropy''`,
    `optimizer=optimizer, metrics=[''accuracy'']` as shown in the following code snippet:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.compile()`编译神经网络，`loss='binary_crossentropy'`，`optimizer=optimizer`，`metrics=['accuracy']`，如下代码片段所示：
- en: '[PRE54]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print a summary of the model using `.summary()`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.summary()`打印模型的摘要：
- en: '[PRE55]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The expected output is this:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望的输出是这样的：
- en: '![Figure 6.13: Summary of the sequential model'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.13：顺序模型的摘要](img/B16060_06_13.jpg)'
- en: '](img/B16060_06_13.jpg)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_13.jpg)'
- en: 'Figure 6.13: Summary of the sequential model'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.13：顺序模型的摘要
- en: This output summarizes the architecture of our neural networks. We can see it
    is composed of three layers, as expected, and we know each layer's output size
    and number of parameters, which corresponds to the weights and biases. For instance,
    the first layer has `16` neurons and `320` parameters to be learned (weights and
    biases).
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个输出总结了我们神经网络的架构。我们可以看到它由三层组成，符合预期，并且知道每一层的输出大小和参数数量，这些参数对应于权重和偏置。例如，第一层有`16`个神经元和`320`个需要学习的参数（权重和偏置）。
- en: 'Next, fit the neural networks with the training set and specify `epochs=10`:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用训练集拟合神经网络并指定`epochs=10`：
- en: '[PRE56]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The expected output is this:'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望的输出是这样的：
- en: '![Figure 6.14: Fitting the neural network with the training set'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.14：使用训练集拟合神经网络'
- en: '](img/B16060_06_14.jpg)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_14.jpg)'
- en: 'Figure 6.14: Fitting the neural network with the training set'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：使用训练集拟合神经网络
- en: 'The output provides a lot of information about the training of the neural network.
    The first line tells us the training set was composed of `800` observations. Then
    we can see the results of each epoch:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了大量关于神经网络训练的信息。第一行告诉我们训练集由`800`个观测值组成。然后我们可以看到每一轮训练的结果：
- en: Total processing time in seconds
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 总处理时间（秒）
- en: Processing time by data sample in us/sample
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据样本的处理时间（微秒/样本）
- en: Loss value and accuracy score
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值和准确度得分
- en: 'The final result of this neural network is the last epoch (`epoch=10`), where
    we achieved an accuracy score of `0.6888`. But we can see that the trend was improving:
    the accuracy score was still increasing after each epoch. So, we may get better
    results if we train the neural network for longer by increasing the number of
    epochs or lowering the learning rate.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络的最终结果是最后一轮训练（`epoch=10`），我们在此达到了`0.6888`的准确度得分。但我们可以看到趋势在持续改善：每一轮训练后准确度得分都在增加。因此，如果我们通过增加训练轮数或降低学习率来延长训练时间，可能会得到更好的结果。
- en: Note
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fMhyLk](https://packt.live/3fMhyLk).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/3fMhyLk](https://packt.live/3fMhyLk)。
- en: You can also run this example online at [https://packt.live/2Njghza](https://packt.live/2Njghza).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/2Njghza](https://packt.live/2Njghza) 在线运行这个示例。你必须执行整个
    Notebook，才能得到预期的结果。
- en: By completing this exercise, you just trained your first classifier. In traditional
    machine learning algorithms, you would need to use more lines of code to achieve
    this, as you would have to define the entire architecture of the neural network.
    Here the neural network got `0.6888` after `10` epochs, but it could still improve
    if we let it train for longer. You can try this on your own.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个练习后，你就训练了你的第一个分类器。在传统的机器学习算法中，你需要编写更多的代码才能实现这一点，因为你需要定义神经网络的整个架构。在这里，神经网络在`10`轮训练后达到了`0.6888`，但如果让它训练得更长一些，性能仍然有提升空间。你可以尝试自己做一下。
- en: Next, we will be looking at regularization.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨正则化。
- en: Regularization
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: As with any machine learning algorithm, neural networks can face the problem
    of overfitting when they learn patterns that are only relevant to the training
    set. In such a case, the model will not be able to generalize the unseen data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何机器学习算法一样，神经网络在学习仅对训练集相关的模式时，可能会面临过拟合的问题。在这种情况下，模型无法对未见过的数据进行泛化。
- en: 'Luckily, there are multiple techniques that can help reduce the risk of overfitting:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有多种技术可以帮助减少过拟合的风险：
- en: L1 regularization, which adds a penalty parameter (absolute value of the weights)
    to the loss function
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 正则化，它向损失函数中添加了一个惩罚参数（权重的绝对值）
- en: L2 regularization, which adds a penalty parameter (squared value of the weights)
    to the loss function
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化，它向损失函数中添加了一个惩罚参数（权重的平方值）
- en: Early stopping, which stops the training if the error for the validation set
    increases while the error decreases for the training set
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早停法，它会在验证集的误差增加而训练集的误差减少时停止训练
- en: Dropout, which will randomly remove some neurons during training
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout，它会在训练过程中随机移除一些神经元
- en: All these techniques can be added at each layer of a neural network we create.
    We will be looking at this in the next exercise.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术都可以添加到我们创建的神经网络的每一层。在下一个练习中，我们将会深入探讨这一点。
- en: 'Exercise 6.04: Predicting Boston House Prices with Regularization'
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.04：使用正则化预测波士顿房价
- en: In this exercise, you will build a neural network that will predict the median
    house price for a suburb in Boston and see how to add regularizers to a network.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将构建一个神经网络，用来预测波士顿某个郊区的房价中位数，并了解如何为网络添加正则化方法。
- en: Note
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/2V9kRUU](https://packt.live/2V9kRUU).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件也可以在我们的 GitHub 仓库中找到：[https://packt.live/2V9kRUU](https://packt.live/2V9kRUU)。
- en: 'Citation: The data was originally published by *Harrison, D. and Rubinfeld,
    D.L. ''Hedonic prices and the demand for clean air'', J. Environ. Economics &
    Management, vol.5, 81-102, 1978*.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：数据最初由*Harrison, D. 和 Rubinfeld, D.L. 'Hedonic prices and the demand for clean
    air', J. Environ. Economics & Management, vol.5, 81-102, 1978*发布。
- en: The dataset is composed of `12` different features that provide information
    about the suburb and a target variable (`MEDV`). The target variable is numeric
    and represents the median value of owner-occupied homes in units of $1,000.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含`12`个不同的特征，提供有关郊区的信息和一个目标变量（`MEDV`）。目标变量是数字型，表示以$1,000为单位的自有住房的中位数价值。
- en: 'The following steps will help you complete the exercise:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: 'Import the `pandas` package as `pd`:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`pandas`包导入为`pd`：
- en: '[PRE57]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Create a `file_url` variable containing a link to the raw dataset:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含原始数据集链接的`file_url`变量：
- en: '[PRE58]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Load the dataset into a variable called `df` using `pd.read_csv()`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pd.read_csv()`将数据集加载到一个名为`df`的变量中：
- en: '[PRE59]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Display the first five rows using `.head()`:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.head()`显示前五行：
- en: '[PRE60]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The expected output is this:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '![Figure 6.15: Output showing the first five rows of the dataset'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.15：输出显示数据集的前五行'
- en: '](img/B16060_06_15.jpg)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_15.jpg)'
- en: 'Figure 6.15: Output showing the first five rows of the dataset'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.15：输出显示数据集的前五行
- en: 'Extract the target variable using `.pop()` and save it into a variable called `label`:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.pop()`提取目标变量，并将其保存到一个名为`label`的变量中：
- en: '[PRE61]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Import the `scale` function from `sklearn.preprocessing`:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.preprocessing`导入`scale`函数：
- en: '[PRE62]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Scale the DataFrame, `df`, and save the results into a variable called `scaled_features`.
    Print its content:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对DataFrame `df`进行缩放，并将结果保存到一个名为`scaled_features`的变量中。打印其内容：
- en: '[PRE63]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The expected output is this:'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '[PRE64]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In the output, you can see that all our features are now standardized.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出中可以看到，我们的所有特征现在已经标准化。
- en: 'Import `train_test_split` from `sklearn.model_selection`:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.model_selection`导入`train_test_split`：
- en: '[PRE65]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Split the data into training and testing sets and save the results into four
    variables called `features_train`, `features_test`, `label_train`, and `label_test`.
    Use 10% of the data for testing and specify `random_state=8`:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集，并将结果保存到四个变量中，分别为`features_train`，`features_test`，`label_train`和`label_test`。使用10%的数据作为测试集，并指定`random_state=8`：
- en: '[PRE66]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`numpy`导入为`np`，`tensorflow`导入为`tf`，并从`tensorflow.keras`导入`layers`：
- en: '[PRE67]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Set `8` as the seed for NumPy and TensorFlow using `np.random_seed()` and `tf.random.set_seed()`:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.random_seed()`和`tf.random.set_seed()`设置NumPy和TensorFlow的种子为`8`：
- en: '[PRE68]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Instantantiate a `tf.keras.Sequential()` class and save it into a variable
    called `model`:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.Sequential()`类，并将其保存到一个名为`model`的变量中：
- en: '[PRE69]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, create a combined `l1` and `l2` regularizer using `tf.keras.regularizers.l1_l2`
    with `l1=0.01` and `l2=0.01`. Save it into a variable called `regularizer`:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`tf.keras.regularizers.l1_l2`创建一个结合了`l1`和`l2`正则化器，`l1=0.01`和`l2=0.01`。将其保存到一个名为`regularizer`的变量中：
- en: '[PRE70]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Instantantiate a `layers.Dense()` class with `10` neurons, `activation=''relu''`,
    `input_shape=[12]`, and `kernel_regularizer=regularizer`, and save it into a variable
    called `layer1`:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`layers.Dense()`类，使用`10`个神经元，`activation='relu'`，`input_shape=[12]`，以及`kernel_regularizer=regularizer`，并将其保存到一个名为`layer1`的变量中：
- en: '[PRE71]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Instantantiate a second `layers.Dense()` class with `1` neuron and save it
    into a variable called `final_layer`:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化第二个`layers.Dense()`类，使用`1`个神经元，并将其保存到一个名为`final_layer`的变量中：
- en: '[PRE72]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Add the two layers you just defined to the model using `.add()` and add a layer
    in between each of them with `layers.Dropout(0.25)`:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.add()`将刚定义的两个层添加到模型中，并在它们之间添加一个`layers.Dropout(0.25)`层：
- en: '[PRE73]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We added a dropout layer in between each dense layer that will randomly remove
    25% of the neurons.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在每个全连接层之间添加了一个dropout层，它会随机移除25%的神经元。
- en: 'Instantantiate a `tf.keras.optimizers.SGD()` class with `0.001` as the learning
    rate and save it into a variable called `optimizer`:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.optimizers.SGD()`类，学习率为`0.001`，并将其保存到一个名为`optimizer`的变量中：
- en: '[PRE74]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Compile the neural network using `.compile()` with `loss=''mse'', optimizer=optimizer,
    metrics=[''mse'']`:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.compile()`编译神经网络，配置`loss='mse'`，`optimizer=optimizer`，`metrics=['mse']`：
- en: '[PRE75]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Print a summary of the model using `.summary()`:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.summary()`打印模型的摘要：
- en: '[PRE76]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The expected output is this:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出是这样的：
- en: '![Figure 6.16: Summary of the model'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.16：模型摘要'
- en: '](img/B16060_06_16.jpg)'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_16.jpg)'
- en: 'Figure 6.16: Summary of the model'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.16：模型摘要
- en: This output summarizes the architecture of our neural networks. We can see it
    is composed of three layers with two dense layers and one dropout layer.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此输出总结了我们神经网络的架构。我们可以看到它由三层组成，其中有两层密集层和一层丢弃层。
- en: 'Instantiate a `tf.keras.callbacks.EarlyStopping()` class with `monitor=''val_loss''`
    and `patience=2` as the learning rate and save it into a variable called `callback`:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.callbacks.EarlyStopping()`类，设置`monitor='val_loss'`和`patience=2`作为学习率，并将其保存到名为`callback`的变量中：
- en: '[PRE77]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We just defined a callback stating the neural network will stop its training
    if the validation loss (`monitor='val_loss'`) does not improve after `2` epochs
    (`patience=2`).
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们刚刚定义了一个回调，声明神经网络将在验证损失（`monitor='val_loss'`）在`2`个epochs（`patience=2`）内未改善时停止训练。
- en: 'Fit the neural networks with the training set and specify `epochs=50`, `validation_split=0.2`,
    `callbacks=[callback]`, and `verbose=2`:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集拟合神经网络，并指定`epochs=50`，`validation_split=0.2`，`callbacks=[callback]`和`verbose=2`：
- en: '[PRE78]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The expected output is this:'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '![Figure 6.17: Fitting the neural network with the training set'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.17：使用训练集拟合神经网络'
- en: '](img/B16060_06_17.jpg)'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16060_06_17.jpg)'
- en: 'Figure 6.17: Fitting the neural network with the training set'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：使用训练集拟合神经网络
- en: 'In the output, we see that the neural network stopped its training after the
    22nd epoch. It stopped well before the maximum number of epochs, `50`. This is
    due to the callback we set earlier: if the validation loss does not improve after
    two epochs, the training should stop.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们看到神经网络在第22个epoch后停止了训练。它在最大epochs数`50`之前停止了训练。这是因为我们之前设置的回调：如果验证损失在两个epoch后没有改善，训练应该停止。
- en: Note
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yobbba](https://packt.live/2Yobbba).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见[https://packt.live/2Yobbba](https://packt.live/2Yobbba)。
- en: You can also run this example online at [https://packt.live/37SVSu6](https://packt.live/37SVSu6).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/37SVSu6](https://packt.live/37SVSu6)在线运行此示例。你必须执行整个Notebook才能获得预期的结果。
- en: You just applied multiple regularization techniques and trained a neural network
    to predict the median value of housing in Boston suburbs.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚应用了多种正则化技术，并训练了一个神经网络来预测波士顿郊区的住房中位数值。
- en: 'Activity 6.01: Finding the Best Accuracy Score for the Digits Dataset'
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.01：为数字数据集找到最佳准确度得分
- en: In this activity, you will be training and evaluating a neural network that
    will be recognizing handwritten digits from the images provided by the MNIST dataset.
    You will be focusing on achieving an optimal accuracy score.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，你将训练和评估一个神经网络，该网络将识别由MNIST数据集提供的手写数字图像。你将专注于实现最佳的准确度得分。
- en: Note
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more about this dataset on TensorFlow's website at [https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在TensorFlow官网上阅读更多关于该数据集的信息：[https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)。
- en: 'Citation: This dataset was originally shared by *Yann Lecun*.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：该数据集最初由*Yann Lecun*分享。
- en: 'The following steps will help you complete the activity:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该活动：
- en: Import the MNIST dataset.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入MNIST数据集。
- en: Standardize the data by applying a division by `255`.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将数据除以`255`来标准化数据。
- en: 'Create a neural network architecture with the following layers:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有以下层的神经网络架构：
- en: A flatten input layer using `layers.Flatten(input_shape=(28,28))`
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个扁平化的输入层，使用`layers.Flatten(input_shape=(28,28))`
- en: A fully connected layer with `layers.Dense(128, activation='relu')`
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个完全连接的层，使用`layers.Dense(128, activation='relu')`
- en: A dropout layer with `layers.Dropout(0.25)`
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个丢弃层，使用`layers.Dropout(0.25)`
- en: A fully connected layer with `layers.Dense(10, activation='softmax')`
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个完全连接的层，使用`layers.Dense(10, activation='softmax')`
- en: Specify an `Adam` optimizer with a learning rate of `0.001`.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个学习率为`0.001`的`Adam`优化器。
- en: Define an early stopping on the validation loss and patience of `5`.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证损失上定义一个早停机制，`patience=5`。
- en: Train the model.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Evaluate the model and find the accuracy score.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型并找到准确度得分。
- en: 'The expected output is this:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '![Figure 6.18: Expected accuracy score'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.18：预期的准确度得分'
- en: '](img/B16060_06_18.jpg)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_18.jpg)'
- en: 'Figure 6.18: Expected accuracy score'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18：预期的准确度得分
- en: Note
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 378
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第378页找到
- en: In the next part, we will dive into deep learning topics.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将深入探讨深度学习的主题。
- en: Deep Learning
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: Now that we are comfortable in building and training a neural network with one
    hidden layer, we can look at more complex architecture with deep learning.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够构建和训练一个具有一个隐藏层的神经网络，我们可以研究更复杂的深度学习架构。
- en: Deep learning is just an extension of traditional neural networks but with deeper
    and more complex architecture. Deep learning can model very complex patterns,
    be applied in tasks such as detecting objects in images and translating text into
    a different language.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习只是传统神经网络的扩展，但具有更深和更复杂的架构。深度学习可以建模非常复杂的模式，应用于检测图像中的物体、将文本翻译成不同语言等任务。
- en: Shallow versus Deep Networks
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浅层与深层网络
- en: Now that we are comfortable in building and training a neural network with one
    hidden layer, we can look at more complex architecture with deep learning.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够构建和训练一个具有一个隐藏层的神经网络，我们可以研究更复杂的深度学习架构。
- en: 'As mentioned earlier, we can add more hidden layers to a neural network. This
    will increase the number of parameters to be learned but can potentially help
    to model more complex patterns. This is what deep learning is about: increasing
    the depth of a neural network to tackle more complex problems.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以在神经网络中添加更多的隐藏层。这将增加需要学习的参数数量，但有可能帮助建模更复杂的模式。这就是深度学习的核心：增加神经网络的深度，以解决更复杂的问题。
- en: 'For instance, we can add a second layer to the neural network we presented
    earlier in the section on forward propagation and loss functions:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在前向传播和损失函数部分中呈现的神经网络中添加第二层：
- en: '![Figure 6.19: Figure showing two hidden layers in a neural network'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.19：显示神经网络中两个隐藏层的图'
- en: '](img/B16060_06_19.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_19.jpg)'
- en: 'Figure 6.19: Figure showing two hidden layers in a neural network'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19：显示神经网络中两个隐藏层的图
- en: In theory, we can add an infinite number of hidden layers. But there is a drawback
    with deeper networks. Increasing the depth will also increase the number of parameters
    to be optimized. So, the neural network will have to train for longer. So, as
    good practice, it is better to start with a simpler architecture and then steadily
    increase its depth.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们可以添加无限多个隐藏层。但深层网络有一个缺点。增加深度还会增加需要优化的参数数量。因此，神经网络需要训练更长时间。所以，作为良好的实践，最好从更简单的架构开始，然后逐步增加其深度。
- en: Computer Vision and Image Classification
  id: totrans-426
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉与图像分类
- en: Deep learning has achieved amazing results in computer vision and natural language
    processing. Computer vision is a field that involves analyzing digital images.
    A digital image is a matrix composed of **pixels**. Each pixel has a value between
    **0** and **255** and this value represents the intensity of the pixel. An image
    can be black and white and have only one channel. But it can also have colors,
    and in that case, it will have three channels for the colors red, green, and blue.
    This digital version of an image that can be fed to a deep learning model.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉和自然语言处理方面取得了惊人的成果。计算机视觉是一个涉及分析数字图像的领域。数字图像是由**像素**组成的矩阵。每个像素的值在**0**到**255**之间，这个值表示像素的强度。一张图像可以是黑白的，并且只有一个通道。但它也可以是彩色的，在这种情况下，它将有三个通道，分别代表红色、绿色和蓝色。这种数字版本的图像可以输入到深度学习模型中。
- en: There are multiple applications of computer vision, such as image classification
    (recognizing the main object in an image), object detection (localizing different
    objects in an image), and image segmentation (finding the edges of objects in
    an image). In this book, we will only look at image classification.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉有多种应用，例如图像分类（识别图像中的主要物体）、物体检测（定位图像中的不同物体）和图像分割（寻找图像中物体的边缘）。本书将只关注图像分类。
- en: 'In the next section, we will look at a specific type of architecture: CNNs.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一种特定类型的架构：CNNs。
- en: Convolutional Neural Networks (CNNs)
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）
- en: '**CNNs** are ANNs that are optimized for image-related pattern recognition.
    CNNs are based on convolutional layers instead of fully connected layers.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络（CNNs）**是针对图像相关模式识别优化的人工神经网络。CNNs基于卷积层，而不是全连接层。'
- en: 'A convolutional layer is used to detect patterns in an image with a filter.
    A filter is just a matrix that is applied to a portion of an input image through
    a convolutional operation and the output will be another image (also called a
    feature map) with the highlighted patterns found by the filter. For instance,
    a simple filter can be one that recognizes vertical lines on a flower, such as
    for the following image:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层用于通过滤波器检测图像中的模式。滤波器只是一个矩阵，通过卷积操作应用到输入图像的某个部分，输出将是另一张图像（也称为特征图），其中突出显示了滤波器找到的模式。例如，一个简单的滤波器可以识别花朵上的垂直线条，如下图所示：
- en: '![Figure 6.20: Convolution detecting patterns in an image'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.20：卷积在图像中检测模式'
- en: '](img/B16060_06_20.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_20.jpg)'
- en: 'Figure 6.20: Convolution detecting patterns in an image'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20：卷积在图像中检测模式
- en: These filters are not set in advance but learned by CNNs automatically. After
    the training is over, a CNN can recognize different shapes in an image. These
    shapes can be anywhere on the image, and the convolutional operator recognizes
    similar image information regardless of its exact position and orientation.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 这些滤波器不是事先设定好的，而是通过卷积神经网络（CNN）自动学习的。训练结束后，CNN可以识别图像中的不同形状。这些形状可以出现在图像的任何地方，卷积操作符无论图像的精确位置和方向如何，都能识别类似的信息。
- en: Convolutional Operations
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'A convolution is a specific type of matrix operation. For an input image, a
    filter of size `n*n` will go through a specific area of an image and apply an
    element-wise product and a sum and return the calculated value:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一种特定类型的矩阵运算。对于输入图像，大小为`n*n`的滤波器将遍历图像的特定区域，执行逐元素相乘和求和，并返回计算结果：
- en: '![Figure 6.21: Convolutional operations'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.21：卷积操作'
- en: '](img/B16060_06_21.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_21.jpg)'
- en: 'Figure 6.21: Convolutional operations'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21：卷积操作
- en: 'In the preceding example, we applied a filter to the top-left part of the image.
    Then we applied an element-wise product that just multiplied an element from the
    input image to the corresponding value on the filter. In the example, we calculated
    the following:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们将滤波器应用到图像的左上部分。然后，我们执行逐元素相乘，将输入图像中的一个元素与滤波器上对应的值相乘。在这个例子中，我们计算了以下内容：
- en: '1st row, 1st column: `5` * `2` = `10`'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1行，第1列：`5` * `2` = `10`
- en: '1st row, 2nd column: `10` * `0` = `0`'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1行，第2列：`10` * `0` = `0`
- en: '1st row, 3rd column: `15` * `(-1)` = `-15`'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1行，第3列：`15` * `(-1)` = `-15`
- en: '2nd row, 1st column: `10` * `2` = `20`'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2行，第1列：`10` * `2` = `20`
- en: '2nd row, 2nd column: `20` * `0` = `0`'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2行，第2列：`20` * `0` = `0`
- en: '2nd row, 3rd column: `30` * `(-1)` = `-30`'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2行，第3列：`30` * `(-1)` = `-30`
- en: '3rd row, 1st column: `100` * `2` = `200`'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3行，第1列：`100` * `2` = `200`
- en: '3rd row, 2nd column: `150` * `0` = `0`'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3行，第2列：`150` * `0` = `0`
- en: '3rd row, 3rd column: `200` * `(-1)` = `-200`'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3行，第3列：`200` * `(-1)` = `-200`
- en: 'Finally, we perform the sum of these values: `10` + `0` -`15` + `20` + `0`
    - `30` + `200` + `0` - `200` = `-15`.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对这些值进行求和：`10` + `0` -`15` + `20` + `0` - `30` + `200` + `0` - `200` = `-15`。
- en: 'Then we will perform the same operation by sliding the filter to the right
    by one column from the input image. We keep sliding the filter until we have covered
    the entire image:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过将滤波器向右滑动一个列来执行相同的操作。我们会一直滑动滤波器，直到覆盖整个图像：
- en: '![Figure 6.22: Convolutional operations on different rows and columns'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.22：在不同的行和列上进行卷积操作'
- en: '](img/B16060_06_22.jpg)'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_22.jpg)'
- en: 'Figure 6.22: Convolutional operations on different rows and columns'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22：在不同的行和列上进行卷积操作
- en: Rather than sliding column by column, we can also slide by two, three, or more
    columns. The parameter defining the length of this sliding operation is called
    the **stride**.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以按列滑动，也可以按两列、三列或更多列滑动。定义滑动操作长度的参数称为**步幅**。
- en: You may have noticed that the result of the convolutional operation is an image
    (or feature map) with smaller dimensions than the input image. If you want to
    keep the exact same dimensions, you can add additional rows and columns with the
    value 0 around the border of the input image. This operation is called **padding**.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，卷积操作的结果是一个尺寸比输入图像小的图像（或特征图）。如果你想保持相同的尺寸，可以在输入图像的边界周围添加额外的行和列，值为0。这种操作称为**填充**。
- en: This is what is behind a convolutional operation. A convolutional layer is just
    the application of this operation with multiple filters.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是卷积操作背后的原理。卷积层就是应用这种操作并使用多个滤波器。
- en: 'We can declare a convolutional layer in TensorFlow with the following code
    snippet:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 TensorFlow 中用以下代码片段声明一个卷积层：
- en: '[PRE79]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In the preceding example, we have instantiated a convolutional layer with `32`
    filters (also called `(3, 3)` with stride of `1` (sliding window by 1 column or
    row at a time) and no padding (`padding="valid"`).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们已经实例化了一个具有 `32` 个滤波器的卷积层（也叫做 `(3, 3)` 卷积，步幅为 `1`（每次滑动窗口按 1 列或 1 行移动），并且没有填充（`padding="valid"`）。
- en: Note
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more about this Conv2D class on TensorFlow's website, at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 TensorFlow 的官方网站上阅读更多关于 Conv2D 类的信息，网址为 [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)。
- en: 'In TensorFlow, convolutional layers expect the input to be tensors with the
    following format: (**rows**, **height**, **width**, **channel**). Depending on
    the dataset, you may have to reshape the images to conform to this requirement.
    TensorFlow provides a function for this, shown in the following code snippet:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，卷积层期望输入为具有以下格式的张量：（**行数**，**高度**，**宽度**，**通道数**）。根据数据集的不同，您可能需要重塑图像以符合此要求。TensorFlow
    提供了一个函数来执行此操作，如下所示：
- en: '[PRE80]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Pooling Layer
  id: totrans-467
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: Another frequent layer in a CNN's architecture is the pooling layer. We have
    seen previously that the convolutional layer reduces the size of the image if
    no padding is added. Is this behavior expected? Why don't we keep the exact same
    size as for the input image? In general, with CNNs, we tend to reduce the size
    of the feature maps as we progress through different layers. The main reason for
    this is that we want to have more and more specific pattern detectors closer to
    the end of the network.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 架构中另一个常见的层是池化层。我们之前已经看到，如果没有添加填充，卷积层会减小图像的大小。这种行为是预期的吗？为什么我们不保持输入图像的完全相同的大小？通常，在
    CNN 中，我们倾向于随着不同层的推进，逐渐减少特征图的大小。这样做的主要原因是，我们希望在网络的末端有更多和更多特定的模式检测器。
- en: Closer to the beginning of the network, a CNN will tend to have more generic
    filters, such as vertical or horizontal line detectors, but as it goes deeper,
    we would, for example, have filters that can detect a dog's tail or a cat's whiskers
    if we were training a CNN to recognize cats versus dogs, or the texture of objects
    if we were classifying images of fruits. Also, having smaller feature maps reduces
    the risk of false patterns being detected.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的前端，CNN 更倾向于具有更通用的滤波器，例如垂直或水平线检测器；但随着网络的加深，例如，如果我们训练一个 CNN 来区分猫和狗，它可能会有可以检测狗尾巴或猫胡须的滤波器；或者如果我们分类水果图像，滤波器可以检测物体的纹理。此外，较小的特征图能降低检测到错误模式的风险。
- en: 'By increasing the stride, we can further reduce the size of the output feature
    map. But there is another way to do this: adding a pooling layer after a convolutional
    layer. A pooling layer is a matrix of a given size and will apply an aggregation
    function to each area of the feature map. The most frequent aggregation method
    is finding the maximum value of a group of pixels:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加步幅，我们可以进一步减小输出特征图的大小。但还有另一种方法可以做到这一点：在卷积层后添加池化层。池化层是一个给定大小的矩阵，它会对特征图的每个区域应用聚合函数。最常见的聚合方法是找到一组像素中的最大值：
- en: '![Figure 6.23: Workings of the pooling layer'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.23：池化层的工作原理'
- en: '](img/B16060_06_23.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_23.jpg)'
- en: 'Figure 6.23: Workings of the pooling layer'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23：池化层的工作原理
- en: In the preceding example, we use a max pooling of size (`2, 2`) and `stride=2`.
    We look at the top-left corner of the feature map and find the maximum value among
    the pixels `6`, `8`, `1`, and `2` and get the result, `8`. Then we slide the max
    pooling by a stride of `2` and perform the same operation on the pixels `6`, `1`,
    `7`, and `4`. We repeat the same operation on the bottom groups and get a new
    feature map of size (`2,2`).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们使用了大小为 (`2, 2`) 且步幅为 `2` 的最大池化。我们查看特征图的左上角，找到像素 `6`、`8`、`1` 和 `2`
    中的最大值，得到结果 `8`。然后，我们通过步幅 `2` 滑动最大池化，对像素 `6`、`1`、`7` 和 `4` 执行相同的操作。我们对底部的区域重复相同的操作，得到一个大小为
    (`2,2`) 的新特征图。
- en: 'In TensorFlow, we can use the `MaxPool2D()` class to declare a max-pooling
    layer:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们可以使用 `MaxPool2D()` 类来声明一个最大池化层：
- en: '[PRE81]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Note
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more about this Conv2D class on TensorFlow's website at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D).
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 TensorFlow 的官方网站上阅读更多关于 Conv2D 类的信息，网址为 [https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)。
- en: CNN Architecture
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN架构
- en: As you saw earlier, you can define your own custom CNN architecture by specifying
    the type and number of hidden layers, the activation functions to be used, and
    so on. But this may be a bit daunting for beginners. How do we know how many filters
    need to be added at each layer or what the right stride will be? We will have
    to try multiple combinations and see which ones work.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前看到的，你可以通过指定隐藏层的类型和数量、激活函数等，定义你自己的自定义CNN架构。但对于初学者来说，这可能有些令人畏惧。我们如何知道每一层需要添加多少个滤波器，或者适当的步幅是多少呢？我们必须尝试多种组合，看看哪种效果最好。
- en: 'Luckily, a lot of researchers in deep learning have already done such exploratory
    work and have published the architecture they designed. Currently, the most famous
    ones are these:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，深度学习领域的许多研究人员已经做了这样的探索性工作，并公开了他们设计的架构。目前，最著名的架构包括：
- en: AlexNet
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlexNet
- en: VGG
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG
- en: ResNet
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: Inception
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception
- en: Note
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We will not go through the details of each architecture as it is not in the
    scope of this book, but you can read more about the different CNN architectures
    implemented on TensorFlow at [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications).
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们不会详细介绍每种架构，因为它超出了本书的范围，但你可以在TensorFlow的官方网站上阅读更多关于不同CNN架构的信息，网址为[https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)。
- en: 'Activity 6.02: Evaluating a Fashion Image Recognition Model Using CNNs'
  id: totrans-488
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.02：使用CNN评估时尚图像识别模型
- en: In this activity, we will be training a CNN to recognize clothing images that
    belong to 10 different classes from the Fashion MNIST dataset. We will be finding
    the accuracy of this CNN model.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将训练一个CNN来识别属于Fashion MNIST数据集中的10个不同类别的服装图像。我们将计算这个CNN模型的准确性。
- en: Note
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can read more about this dataset on TensorFlow's website at [https://www.tensorflow.org/datasets/catalog/fashion_mnist](https://www.tensorflow.org/datasets/catalog/fashion_mnist).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在TensorFlow的网站上阅读更多关于这个数据集的信息，网址为[https://www.tensorflow.org/datasets/catalog/fashion_mnist](https://www.tensorflow.org/datasets/catalog/fashion_mnist)。
- en: The original dataset was shared by *Han Xiao*.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集由*Han Xiao*分享。
- en: 'The following steps will help you complete the activity:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个活动：
- en: Import the Fashion MNIST dataset.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Fashion MNIST数据集。
- en: Reshape the training and testing set.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整训练集和测试集的形状。
- en: Standardize the data by applying a division by `255`.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对数据进行`255`除法标准化。
- en: 'Create a neural network architecture with the following layers:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有以下层的神经网络架构：
- en: Three convolutional layers with `Conv2D(64, (3,3), activation='relu')` followed
    by `MaxPooling2D(2,2)`
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 三个卷积层，使用`Conv2D(64, (3,3), activation='relu')`，后跟`MaxPooling2D(2,2)`
- en: A flatten layer
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个Flatten层
- en: A fully connected layer with `Dense(128, activation=relu)`
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个全连接层，使用`Dense(128, activation=relu)`
- en: A fully connected layer with `Dense(10, activation='softmax')`
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个全连接层，使用`Dense(10, activation='softmax')`
- en: Specify an `Adam` optimizer with a learning rate of `0.001`.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个学习率为`0.001`的`Adam`优化器。
- en: Train the model.
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Evaluate the model on the testing set.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估模型。
- en: 'The expected output is this:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE82]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Note
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 382.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第382页找到。
- en: 'In the following section, we will learn about a different type of deep learning
    architecture: the RNN.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习另一种深度学习架构：RNN。
- en: Recurrent Neural Networks (RNNs)
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）
- en: In the last section, we learned how we can use CNNs for computer vision tasks
    such as classifying images. With deep learning, computers are now capable of achieving
    and sometimes surpassing human performance. Another field that is attracting a
    lot of interest from researchers is natural language processing. This is a field
    where RNNs excel.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们学习了如何使用CNN进行计算机视觉任务，如图像分类。随着深度学习的发展，计算机现在能够实现甚至超越人类的表现。另一个正在吸引研究人员大量关注的领域是自然语言处理，这是一个RNN表现突出的领域。
- en: In the last few years, we have seen a lot of different applications of RNN technology,
    such as speech recognition, chatbots, and text translation applications. But RNNs
    are also quite performant in predicting time series patterns, something that's
    used for forecasting stock markets.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，我们见证了RNN技术的许多不同应用，如语音识别、聊天机器人和文本翻译应用。但RNN在预测时间序列模式方面也表现得相当出色，这一点在股票市场预测中得到了广泛应用。
- en: RNN Layers
  id: totrans-513
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN层
- en: The common point with all the applications mentioned earlier is that the inputs
    are sequential. There is a time component with the input. For instance, a sentence
    is a sequence of words, and the order of words matters; stock market data consists
    of a sequence of dates with corresponding stock prices.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前提到的应用程序的共同点是它们的输入是顺序的。输入中有一个时间组件。例如，句子是单词的序列，单词的顺序很重要；股票市场数据由一系列日期和相应的股价组成。
- en: 'To accommodate such input, we need neural networks to be able to handle sequences
    of inputs and be able to maintain an understanding of the relationships between
    them. One way to do this is to create memory where the network can take into account
    previous inputs. This is exactly how a basic RNN works:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应这样的输入，我们需要神经网络能够处理输入序列，并能够保持对它们之间关系的理解。一种方法是创建记忆，让网络考虑先前的输入。这正是基本循环神经网络的工作方式：
- en: '![Figure 6.24: Overview of a single RNN'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.24：单个循环神经网络概述'
- en: '](img/B16060_06_24.jpg)'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_24.jpg)'
- en: 'Figure 6.24: Overview of a single RNN'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24：单个循环神经网络概述
- en: In the preceding figure, we can see a neural network that takes an input called
    `X`t and performs some transformations and gives the output results, ![a](img/B16060_06_24a.png).
    Nothing new so far.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到一个神经网络接收称为 `X`t 的输入，并进行一些转换，并给出输出结果，![a](img/B16060_06_24a.png)。到目前为止没有新东西。
- en: 'But you may have noticed that there is an additional output called Ht-1 that
    is an output but also an input to the neural network. This is how RNN simulates
    memory – by considering its previous results and taking them in as an additional
    input. Therefore, the result ![b](img/B16060_06_24b.png) will depend on the input
    xt but also Ht-1\. Now, we can represent a sequence of four inputs that get fed
    into the same neural network:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 但您可能已经注意到，还有一个名为 Ht-1 的附加输出，它是一个输出，但也是神经网络的输入。这就是 RNN 模拟记忆的方式 - 通过考虑其先前的结果并将其作为额外输入。因此，结果
    ![b](img/B16060_06_24b.png) 将取决于输入 xt，但也取决于 Ht-1。现在，我们可以表示四个输入的序列，这些输入被馈送到同一个神经网络中。
- en: '![Figure 6.25: Overview of an RNN'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.25：循环神经网络概述'
- en: '](img/B16060_06_25.jpg)'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_25.jpg)'
- en: 'Figure 6.25: Overview of an RNN'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.25：循环神经网络概述
- en: We can see the neural network is taking an input (`x`) and generating an output
    (`y`) at each time step (`t`, `t+1`, …, `t+3`) but also another output (`h`),
    which is feeding the next iteration.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到神经网络在每个时间步（`t`、`t+1`、…、`t+3`）中接收一个输入（`x`）并生成一个输出（`y`），同时生成另一个输出（`h`），该输出将传递到下一次迭代。
- en: Note
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding figure may be a bit misleading – there is actually only one RNN
    here (all the RNN boxes in the middle form one neural network), but it is easier
    to see how the sequencing works in this format.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图可能有点误导性 - 这里实际上只有一个循环神经网络（中间的所有循环神经网络框形成一个神经网络），但从这种格式更容易看出顺序如何工作。
- en: 'An RNN cell looks like this on the inside:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 单元内部如下所示：
- en: '![Figure 6.26: Internal workings of an RNN using tanh'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.26：使用 tanh 的 RNN 的内部工作方式'
- en: '](img/B16060_06_26.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_26.jpg)'
- en: 'Figure 6.26: Internal workings of an RNN using tanh'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.26：使用 tanh 的 RNN 的内部工作方式
- en: It is very similar to a simple neuron, but it takes more inputs and uses `tanh`
    as the activation function.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 它与简单神经元非常相似，但接受更多的输入，并使用 `tanh` 作为激活函数。
- en: Note
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can use any activation function in an RNN cell. The default value in TensorFlow
    is `tanh`.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RNN 单元中，您可以使用任何激活函数。在 TensorFlow 中，默认值是 `tanh`。
- en: 'This is the basic logic of RNNs. In TensorFlow, we can instantiate an RNN layer
    with `layers.SimpleRNN`:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 RNN 的基本逻辑。在 TensorFlow 中，我们可以使用 `layers.SimpleRNN` 实例化一个 RNN 层：
- en: '[PRE83]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: In the code snippet, we created an RNN layer with `4` outputs and the `tanh`
    activation function (which is the most widely used activation function for RNNs).
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段中，我们创建了一个具有 `4` 个输出和 `tanh` 激活函数（这是用于 RNN 的最常用的激活函数）的 RNN 层。
- en: The GRU Layer
  id: totrans-537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GRU 层
- en: One drawback with the previous type of layer is that the final output takes
    into consideration all the previous outputs. If you have a sequence of 1,000 input
    units, the final output, `y`, is influenced by every single previous result. If
    this sequence was composed of 1,000 words and we were trying to predict the next
    word, it would really be overkill to have to memorize all of the 1,000 words before
    making a prediction. Probably, you only need to look at the previous 100 words
    from the final output.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 前一类型层的一个缺点是最终输出考虑了所有先前的输出。如果您有 1,000 个输入单元的序列，则最终输出 `y` 受每一个先前结果的影响。如果此序列由 1,000
    个单词组成，而我们试图预测下一个单词，则必须记住所有 1,000 个单词真的太多了。可能只需查看最终输出之前的前 100 个单词。
- en: 'This is exactly what **Gated Recurrent Unit** (**GRU**) cells are for. Let''s
    look at what is inside them:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 **门控循环单元**（**GRU**）单元的作用。我们来看看它们的内部结构：
- en: '![Figure 6.27: Internal workings of an RNN using tanh and sigmoid'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.27：使用 tanh 和 sigmoid 的 RNN 内部工作原理'
- en: '](img/B16060_06_27.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_27.jpg)'
- en: 'Figure 6.27: Internal workings of an RNN using tanh and sigmoid'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.27：使用 tanh 和 sigmoid 的 RNN 内部工作原理
- en: 'Compared to a simple RNN cell, a GRU cell has a few more elements:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的 RNN 单元相比，GRU 单元有更多的元素：
- en: A second activation function, which is `sigmoid`
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个激活函数，即 `sigmoid`
- en: A multiplier operation performed before generating the outputs ![39](img/B16060_06_27a.png)
    and Ht
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成输出之前执行的乘法运算 ![39](img/B16060_06_27a.png) 和 Ht
- en: The usual path with `tanh` is still responsible for making a prediction, but
    this time we will call it the "candidate." The sigmoid path acts as an "update"
    gate. This will tell the GRU cell whether it needs to discard the use of this
    candidate or not. Remember that the output ranges between **0** and **1**. If
    close to 0, the update gate (that is, the sigmoid path) will say we should not
    consider this candidate.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用的 `tanh` 路径仍然负责进行预测，但这次我们称之为“候选”。sigmoid 路径充当“更新”门。它将告诉 GRU 单元是否需要丢弃此候选值。记住，输出范围在
    **0** 到 **1** 之间。如果接近 0，则更新门（即 sigmoid 路径）会表示我们不应考虑这个候选。
- en: On the other hand, if it is closer to 1, we should definitely use the result
    of this candidate.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果它更接近 1，则我们肯定应该使用这个候选的结果。
- en: Remember that the output Ht is related to Ht-1, which is related to Ht-2, and
    so on. So, this update gate will also define how much "memory" we should keep.
    It tends to prioritize previous outputs closer to the current one.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，输出 Ht 与 Ht-1 相关，Ht-1 又与 Ht-2 相关，依此类推。因此，更新门还将定义我们应保留多少“记忆”。它倾向于优先考虑与当前输出更接近的先前输出。
- en: 'This is the basic logic of GRU (note that the GRU cell has one more component,
    the reset gate, but for the purpose of simplicity, we will not look at it). In
    TensorFlow, we can instantiate such a layer with `layers.GRU`:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 GRU 的基本逻辑（请注意，GRU 单元还有一个额外的组件，即重置门，但为了简化，我们将不讨论它）。在 TensorFlow 中，我们可以通过
    `layers.GRU` 实例化这样的层：
- en: '[PRE84]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: In the code snippet, we have created a GRU layer with `4` output units and the
    `tanh` activation function for the candidate prediction and sigmoid for the update
    gate.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段中，我们创建了一个具有 `4` 个输出单元的 GRU 层，候选预测使用 `tanh` 激活函数，更新门使用 sigmoid 激活函数。
- en: The LSTM Layer
  id: totrans-552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM 层
- en: 'There is another very popular type of cell for RNN architecture called the
    LSTM cell. **LSTM** stands for **Long Short-Term Memory**. LSTM came before GRU,
    but the latter is much simpler, and this is the reason why we presented it first.
    Here is what is under the hood of LSTM:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种非常流行的 RNN 架构单元，叫做 LSTM 单元。**LSTM** 代表 **长短期记忆**。LSTM 比 GRU 先出现，但后者更简单，这就是我们首先介绍
    GRU 的原因。LSTM 的内部结构如下：
- en: '![Figure 6.28: Overview of LSTM'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.28：LSTM 概述'
- en: '](img/B16060_06_28.jpg)'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_06_28.jpg)'
- en: 'Figure 6.28: Overview of LSTM'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.28：LSTM 概述
- en: 'At first, this looks very complicated. It is composed of several elements:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，这看起来非常复杂。它由几个元素组成：
- en: '`Cell state`: This is the concatenation of all the previous outputs. It is
    the "memory" of the LSTM cell.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`细胞状态`：这是所有先前输出的拼接。它是 LSTM 单元的“记忆”。'
- en: '`Forget gate`: This is responsible for defining whether we should keep or forget
    a given memory.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`遗忘门`：它负责定义是否应该保留或忘记给定的记忆。'
- en: '`Input gate`: This is responsible for defining whether the new memory candidate
    needs to be updated or not. This new memory candidate is then added to the previous
    memory.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`输入门`：它负责定义是否需要更新新的记忆候选。然后，将此新记忆候选加入到先前的记忆中。'
- en: '`Output gate`: This is responsible for making the prediction based on the previous
    output (Ht-1), the current input (xt), and the memory.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`输出门`：它负责根据先前的输出（Ht-1）、当前输入（xt）和记忆进行预测。'
- en: An LSTM cell can consider previous results but also past memory, and this is
    the reason why it is so powerful.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元可以考虑先前的结果，也可以考虑过去的记忆，这就是它如此强大的原因。
- en: 'In TensorFlow, we can instantiate such a layer with `layers.SimpleRNN`:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们可以通过 `layers.SimpleRNN` 实例化这样的层：
- en: '[PRE85]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: In the code snippet, we have created an LSTM layer with `4` output units and
    the `tanh` activation function for the candidate prediction and sigmoid for the
    update gate.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段中，我们创建了一个具有 `4` 个输出单元的 LSTM 层，候选预测使用 `tanh` 激活函数，更新门使用 sigmoid 激活函数。
- en: Note
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about SimpleRNN implementation in TensorFlow here: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN).'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以在这里阅读更多关于 TensorFlow 中 SimpleRNN 实现的内容：[https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)。  '
- en: 'Activity 6.03: Evaluating a Yahoo Stock Model with an RNN'
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '活动 6.03：评估基于 RNN 的 Yahoo 股票模型  '
- en: In this activity, we will be training an RNN model with LSTM to predict the
    stock price of Yahoo! based on the data of the past `30` days. We will be finding
    the optimal mean squared error value and checking whether the model overfits.
    We will be using the same Yahoo Stock dataset that we saw in *Chapter 2*, *An
    Introduction to Regression*.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '在本活动中，我们将使用 LSTM 训练一个 RNN 模型，以预测基于过去 `30` 天数据的 Yahoo! 股票价格。我们将寻找最佳的均方误差值，并检查模型是否发生过拟合。我们将使用在*第
    2 章*《回归简介》中看到的相同的 Yahoo 股票数据集。  '
- en: Note
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '注意  '
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/3fRI5Hk](https://packt.live/3fRI5Hk).'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集文件也可以在我们的 GitHub 仓库中找到：[https://packt.live/3fRI5Hk](https://packt.live/3fRI5Hk)。  '
- en: 'The following steps will help you to complete this activity:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '以下步骤将帮助你完成此活动：  '
- en: Import the Yahoo Stock dataset.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '导入 Yahoo 股票数据集。  '
- en: Extract the `close price` column.
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '提取 `close price` 列。  '
- en: Standardize the dataset.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '对数据集进行标准化。  '
- en: Create the previous `30` days' stock price features.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建过去`30`天的股票价格特征。  '
- en: Reshape the training and testing sets.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '重塑训练集和测试集。  '
- en: 'Create the neural network architecture with the following layers:'
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建神经网络架构，包含以下层：  '
- en: Five LSTM layers with `LSTM(50, (3,3), activation='relu') followed by Dropout(0.2)`
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '五个 LSTM 层，使用 `LSTM(50, (3,3), activation=''relu'')` 后跟 `Dropout(0.2)`  '
- en: A fully connected layer with `Dense(1)`
  id: totrans-580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个全连接层，使用 `Dense(1)`
- en: Specify an `Adam` optimizer with a learning rate of `0.001`.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '指定学习率为 `0.001` 的 `Adam` 优化器。  '
- en: Train the model.
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '训练模型。  '
- en: Evaluate the model on the testing set.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在测试集上评估模型。  '
- en: 'The expected output is this:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '期望的输出结果是：  '
- en: '[PRE86]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Note
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '注意  '
- en: The solution for this activity can be found on page 387.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '该活动的解决方案可以在第 387 页找到。  '
- en: In the next section, we will be looking at the hardware needed for deep learning.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的部分，我们将讨论深度学习所需的硬件。  '
- en: Hardware for Deep Learning
  id: totrans-589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '深度学习硬件  '
- en: As you may have noticed, training deep learning models takes longer than traditional
    machine learning algorithms. This is due to the number of calculations required
    for the forward pass and backpropagation. In this book, we trained very simple
    models with just a few layers. But there are architectures with hundreds of layers,
    and some with even more than that. That kind of network can take days or even
    weeks to train.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '如你所见，训练深度学习模型比传统机器学习算法需要更长的时间。这是因为前向传播和反向传播过程中需要进行大量计算。在本书中，我们训练了只有少数几层的简单模型。但也有一些架构有数百层，甚至更多。那种网络的训练可能需要几天，甚至几周。  '
- en: To speed up the training process, it is recommended to use a specific piece
    of hardware called a GPU. GPUs specialize in performing mathematical operations
    and therefore are perfect for deep learning. Compared to a **Central Processing
    Unit** (**CPU**), a GPU can be up to 10X faster at training a deep learning model.
    You can personally buy a GPU and set up your own deep learning computer. You just
    need to get one that is CUDA-compliant (currently only NVIDIA GPUs are).
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '为了加速训练过程，建议使用一种名为 GPU 的特定硬件。GPU 擅长执行数学运算，因此非常适合深度学习。与**中央处理单元**（**CPU**）相比，GPU
    在训练深度学习模型时速度可以快到 10 倍。你可以亲自购买 GPU 并搭建自己的深度学习计算机，只需要确保所购买的 GPU 支持 CUDA（目前只有 NVIDIA
    的 GPU 支持）。  '
- en: Another possibility is to use cloud providers such as AWS or Google Cloud Platform
    and train your models in the cloud. You will pay only for what you use and can
    switch them off as soon as you are done. The benefit is that you can scale the
    configuration up or down depending on the needs of your projects – but be mindful
    of the cost. You will be charged for the time your instance is up even if you
    are not training a model. So, don't forget to switch things off if you're not
    using them.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种选择是使用像 AWS 或 Google Cloud Platform 这样的云服务提供商，并在云端训练模型。你只需为实际使用的部分付费，且在完成后可以随时关闭它们。好处是，你可以根据项目的需求调整配置的规模，但要注意费用。如果你的实例一直开启，即使没有训练模型，也会产生费用。所以，如果不使用时，记得关闭实例。  '
- en: 'Finally, Google recently released some new hardware dedicated to deep learning:
    **Tensor Processing Unit** (**TPUs**). They are much faster than GPUs, but they
    are quite costly. Currently, only Google Cloud Platform provides such hardware
    in their cloud instances.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，谷歌最近发布了一些专门用于深度学习的新硬件：**张量处理单元**（**TPUs**）。它们比GPU快得多，但成本也相当高。目前，只有谷歌云平台在其云实例中提供这种硬件。
- en: Challenges and Future Trends
  id: totrans-594
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战与未来趋势
- en: As with any new technology, deep learning comes with challenges. One of them
    is the big barrier to entry. To become a deep learning practitioner, you used
    to have to know all the mathematical theory behind deep learning very well and
    be a confirmed programmer. On top of this, you had to learn the specifics of the
    deep learning framework you chose to use (be it TensorFlow, PyTorch, Caffe, or
    anything else). For a while, deep learning couldn't reach a broad audience and
    was mainly limited to researchers. This situation has changed, though it is not
    perfect. For instance, TensorFlow now comes with a higher-level API called Keras
    (this is the one you saw in this chapter) that is much easier to use than the
    core API. Hopefully, this trend will keep going and make deep learning frameworks
    more accessible to anyone interested in this field.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何新技术一样，深度学习也面临着挑战。其中之一是巨大的入门门槛。要成为一名深度学习从业者，你曾经需要非常了解深度学习背后的所有数学理论，并且是一名熟练的程序员。此外，你还需要学习你选择使用的深度学习框架的具体知识（无论是TensorFlow、PyTorch、Caffe，还是其他任何框架）。一段时间以来，深度学习无法接触到广泛的受众，主要局限于研究人员。但这种情况已经发生了变化，尽管仍不完美。例如，TensorFlow现在有了一个更高级的API，叫做Keras（这就是你在本章看到的），它比核心API更容易使用。希望这种趋势会继续下去，让深度学习框架对任何有兴趣的人都更加易于接触。
- en: 'The second challenge was that deep learning models require a lot of computation
    power, as mentioned in the previous section. This was again a major blocker for
    anyone who wanted to have a go at it. Even though the cost of GPUs has gone down,
    deep learning still requires some upfront investment. Luckily for us, there is
    now a free option to train deep learning models with GPUs: Google Colab. It is
    an initiative from Google to promote research by providing temporary cloud computing
    for free. The only thing you need is a Google account. Once signed up, you can
    create Notebooks (similar to Jupyter Notebooks) and choose a kernel to be run
    on a CPU, GPU (limited to 10 hours per day), or even a TPU (limited to ½ hour
    per day). So, before investing in purchasing or renting out GPU, you can first
    practice with Google Colab.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个挑战是，深度学习模型需要大量的计算能力，正如前面一节所提到的。这再次成为任何想要尝试的人面临的主要障碍。尽管GPU的成本已经下降，深度学习仍然需要一些前期投资。幸运的是，现在我们有了一个免费的选择来使用GPU训练深度学习模型：Google
    Colab。这是谷歌推出的一项倡议，旨在通过提供免费的临时云计算来促进研究。你需要的唯一条件是一个Google账号。注册后，你可以创建笔记本（类似于Jupyter笔记本），并选择一个内核，在CPU、GPU（每天限制10小时）甚至TPU（每天限制½小时）上运行。因此，在投资购买或租赁GPU之前，你可以先在Google
    Colab上进行练习。
- en: Note
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information about Google Colab at [https://colab.research.google.com/](https://colab.research.google.com/).
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://colab.research.google.com/](https://colab.research.google.com/)上找到更多关于Google
    Colab的信息。
- en: More advanced deep learning models can be very deep and require weeks of training.
    So, it is hard for basic practitioners to use such architecture. But thankfully,
    a lot of researchers have embraced the open source movement and have shared not
    only the architectures they have designed but also the weights of the networks.
    This means you can now access state-of-the-art pre-trained models and fine-tune
    them to fit your own projects. This is called transfer learning (which is out
    of the scope of this book). It is very popular in computer vision, where you can
    find pre-trained models on ImageNet or MS-Coco, for instance, which are large
    datasets of pictures. Transfer learning is also happening in natural language
    processing, but it is not as developed as it is for computer vision.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 更先进的深度学习模型可能非常深，训练可能需要数周。因此，基础的从业者很难使用这种架构。但幸运的是，许多研究人员已经接受了开源运动，并不仅分享了他们设计的架构，还分享了网络的权重。这意味着你现在可以访问最先进的预训练模型，并对其进行微调，以适应你自己的项目。这被称为迁移学习（此书不涉及）。它在计算机视觉领域非常流行，你可以在ImageNet或MS-Coco上找到预训练模型，例如，这些是包含大量图片的数据集。迁移学习也在自然语言处理领域发生，但其发展程度不如计算机视觉领域。
- en: Note
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information about these datasets at [http://www.image-net.org/](http://www.image-net.org/)
    and [http://cocodataset.org/](http://cocodataset.org/).
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://www.image-net.org/](http://www.image-net.org/)和[http://cocodataset.org/](http://cocodataset.org/)找到有关这些数据集的更多信息。
- en: Another very important topic related to deep learning is the increasing need
    to be able to interpret model results. Soon, these kinds of algorithms may be
    regulated, and deep learning practitioners will have to be able to explain why
    a model is making a given decision. Currently, deep learning models are more like
    black boxes due to the complexity of the networks. There are already some initiatives
    from researchers to find ways to interpret and understand deep neural networks,
    such as *Zeiler and Fergus*, "*Visualizing and Understanding Convolutional Networks*",
    *ECCV 2014*. However, more work needs to be done in this field with the democratization
    of such technologies in our day-to-day lives. For instance, we will need to make
    sure that these algorithms are not biased and are not making unfair decisions
    affecting specific groups of people.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与深度学习相关的重要话题是对模型结果进行解释的需求日益增加。很快，这些算法可能会受到监管，深度学习从业者将必须能够解释模型为何做出某个特定决策。目前，由于网络的复杂性，深度学习模型更像是黑盒。研究人员已经提出了一些倡议，寻找解释和理解深度神经网络的方法，例如*Zeiler和Fergus*的《*Visualizing
    and Understanding Convolutional Networks*》，*ECCV 2014*。然而，随着这些技术在我们日常生活中的民主化，仍然需要在该领域做更多的工作。例如，我们需要确保这些算法不会存在偏见，并且不会做出影响特定群体的、不公平的决策。
- en: Summary
  id: totrans-603
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have just completed the entire book of *The Applied Artificial Intelligence
    Workshop, Second Edition*. In this workshop, we have learned about the fundamentals
    of AI and its applications. We wrote a Python program to play tic-tac-toe. We
    learned about search techniques such as breadth-first search and depth-first search
    and how they can help us solve the tic-tac-toe game.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚完成了整本*《应用人工智能工作坊（第二版）》*。在这个工作坊中，我们学习了AI的基本原理及其应用。我们写了一个Python程序来玩井字游戏。我们学习了广度优先搜索和深度优先搜索等搜索技术，并了解了它们如何帮助我们解决井字游戏。
- en: 'In the next couple of chapters after that, we learned about supervised learning
    using regression and classification. These chapters included data preprocessing,
    train-test splitting, and models that were used in several real-life scenarios.
    Linear regression, polynomial regression, and support vector machines all came
    in handy when it came to predicting stock data. Classification was performed using
    k-nearest neighbor and support vector classifiers. Several activities helped you
    to apply the basics of classification in an interesting real-life use case: credit
    scoring.'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们学习了使用回归和分类的监督学习。这些章节包括数据预处理、训练-测试拆分以及在多个实际场景中使用的模型。线性回归、多项式回归和支持向量机在预测股票数据时都发挥了重要作用。分类是通过k-最近邻和支持向量分类器进行的。几个活动帮助你将分类的基础应用于一个有趣的实际用例：信用评分。
- en: In *Chapter 4*, *An Introduction to Decision Trees*, you were introduced to
    decision trees, random forests, and extremely randomized trees. This chapter introduced
    different means of evaluating the utility of models. We learned how to calculate
    the accuracy, precision, recall, and F1 score of models. We also learned how to
    create the confusion matrix of a model. The models of this chapter were put into
    practice through the evaluation of car data.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*，*决策树简介*中，你学习了决策树、随机森林和极端随机树。本章介绍了评估模型效用的不同方法。我们学习了如何计算模型的准确率、精确度、召回率和F1分数。我们还学会了如何创建模型的混淆矩阵。本章的模型通过对汽车数据的评估得以实践。
- en: 'Unsupervised learning was introduced in *Chapter 5*, *Artificial Intelligence:
    Clustering*, along with the k-means and hierarchical clustering algorithms. One
    interesting aspect of these algorithms is that the labels are not given in advance,
    but they are detected during the clustering process.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '*第5章*，*人工智能：聚类*中介绍了无监督学习，以及k-均值和层次聚类算法。一个有趣的方面是，这些算法在聚类过程中没有预先给定标签，而是通过聚类过程来检测标签。'
- en: This workshop concluded with *Chapter 6*, *Neural Networks and Deep Learning*,
    where neural networks and deep learning using TensorFlow was presented. We used
    these techniques to achieve the best accuracy in real-life applications, such
    as the detection of written digits, image classification, and time series forecasting.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 本次研讨会以*第六章*，*神经网络与深度学习*为结束，其中介绍了如何使用 TensorFlow 进行神经网络和深度学习。我们运用了这些技术，在实际应用中取得了最佳准确度，例如手写数字检测、图像分类和时间序列预测。
