- en: '*Chapter 8:* Neural Machine Translation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章:* 神经机器翻译'
- en: In the previous chapter, [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*, we introduced several text encoding techniques
    and used them in three **Natural Language Processing** (**NLP**) applications.
    One of the applications was for free text generation. The result showed that it
    is possible for a network to learn the structure of a language, so as to generate
    text in a certain style.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，[*第7章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)，*实现NLP应用程序*，我们介绍了几种文本编码技术，并将其应用于三个**自然语言处理**（**NLP**）应用程序。其中一个应用是自由文本生成。结果表明，网络有可能学习语言的结构，从而生成某种风格的文本。
- en: In this chapter, we will build on top of this case study for free text generation
    and train a neural network to automatically translate sentences from a source
    language into a target language. To do that, we will use concepts learned from
    the free text generation network, as well as from the autoencoder introduced in
    [*Chapter 5*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152), *Autoencoder for
    Fraud Detection*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在自由文本生成的案例研究基础上，训练一个神经网络来自动将句子从源语言翻译成目标语言。为此，我们将使用从自由文本生成网络以及在[*第5章*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152)，*用于欺诈检测的自编码器*中学到的概念。
- en: We will start by describing the general concept of machine translation, followed
    by an introduction to the encoder-decoder neural architectures that will be used
    for neural machine translation. Next, we will discuss all the steps involved in
    the implementation of the application, from preprocessing to defining the network
    structure to training and applying the network.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先描述机器翻译的一般概念，然后介绍将用于神经机器翻译的编码器-解码器神经架构。接下来，我们将讨论实现该应用程序的所有步骤，从预处理到定义网络结构，再到训练和应用网络。
- en: 'The chapter is organized into the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构分为以下几个部分：
- en: Idea of Neural Machine Translation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经机器翻译的理念
- en: Encoder-Decoder Architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: Preparing the Data for the Two Languages
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为两种语言准备数据
- en: Building and Training an Encoder-Decoder Architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练编码器-解码器架构
- en: Idea of Neural Machine Translation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经机器翻译的理念
- en: '**Automatic translation** has been a popular and challenging task for a long
    time now. The flexibility and ambiguity of the human language make it still one
    of the most difficult tasks to implement. The same word or phrase can have different
    meanings depending on the context and, often, there might not be just one correct
    translation, but many possible ways to translate the same sentence. So, how can
    a computer learn to translate text from one language into another? Different approaches
    have been introduced over the years, all with the same goal: to automatically
    translate sentences or text from a source language into a target language.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动翻译**一直是一个热门且富有挑战性的任务。人类语言的灵活性和模糊性使其至今仍然是最难实现的任务之一。同一个单词或短语根据上下文可以有不同的含义，而且通常并非只有一个正确的翻译，而是存在多种可能的翻译方式。那么，计算机如何学会将文本从一种语言翻译成另一种语言呢？多年来，已经提出了不同的方法，所有方法的目标都是相同的：将源语言的句子或文本自动翻译成目标语言。'
- en: The development of automatic translation systems started in the early 1970s
    with **Rule-Based Machine Translation** (**RBMT**). Here, automatic translation
    was implemented through hand-developed rules and dictionaries by specialized linguists
    at the lexical, syntactic, and semantic levels of sentences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自动翻译系统的发展始于1970年代初期，采用**基于规则的机器翻译**（**RBMT**）。在这里，自动翻译是通过专门的语言学家在句子的词汇、句法和语义层面上手工开发的规则和词典来实现的。
- en: 'In the 1990s, **statistical machine translation** models became state of the
    art, even though the first concepts for statistical machine translation were introduced
    in 1949 by Warren Weaver. Instead of using dictionaries and handwritten rules,
    the idea became to use a vast corpus of examples to train statistical models.
    This task can be described as modeling the probability distribution, ![](img/Formula_B16391_08_001.png),
    that a string, ![](img/Formula_B16391_07_001.png), in the target language (for
    example, German) is the translation of a string, ![](img/Formula_B16391_08_003.png),
    in the source language (for example, English). Different approaches have been
    introduced to model this ![](img/Formula_B16391_08_004.png) probability distribution,
    the most popular of which came from the Bayes theorem and modeled ![](img/Formula_B16391_08_005.png)
    as ![](img/Formula_B16391_08_006.png). Thus, in this approach, the task is split
    into two subtasks: training a language model, ![](img/Formula_B16391_08_007.png),
    and modeling the probability, ![](img/Formula_B16391_08_008.png) More generally,
    several subtasks can be defined, and several models are trained and tuned for
    each subtask.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年代，**统计机器翻译**模型成为了当时的先进技术，尽管统计机器翻译的第一个概念是在1949年由沃伦·韦弗提出的。与使用词典和手写规则不同，新的思路是使用大量的示例语料库来训练统计模型。这个任务可以描述为建模概率分布，![](img/Formula_B16391_08_001.png)，即目标语言（例如德语）中的一个字符串，![](img/Formula_B16391_07_001.png)，是源语言（例如英语）中的一个字符串，![](img/Formula_B16391_08_003.png)，的翻译。为建模这个![](img/Formula_B16391_08_004.png)概率分布，提出了不同的方法，其中最流行的来自贝叶斯定理，并将![](img/Formula_B16391_08_005.png)建模为![](img/Formula_B16391_08_006.png)。因此，在这种方法中，任务被分为两个子任务：训练一个语言模型，![](img/Formula_B16391_08_007.png)，以及建模概率，![](img/Formula_B16391_08_008.png)。更一般来说，可以定义多个子任务，并为每个子任务训练和调整多个模型。
- en: 'More recently, neural machine translation gained quite some popularity in the
    task of automatic translation. Also, here, a vast corpus of example sentences
    in a source and target language is required to train the translation model. The
    difference between classical statistical-based models and neural machine translation
    is in the definition of the task: instead of training many small sub-components
    and tuning them separately, one single network is trained in an end-to-end fashion.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，神经机器翻译在自动翻译任务中获得了相当大的关注。此外，在这里，训练翻译模型需要大量的源语言和目标语言的示例句子语料库。经典的基于统计的模型和神经机器翻译的区别在于任务的定义：不是训练许多小的子组件并分别调整它们，而是训练一个单一的网络，以端到端的方式进行训练。
- en: One network architecture that can be used for neural machine translations is
    an encoder-decoder network. Let's find out what this is.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可以用于神经机器翻译的网络架构是编码器-解码器网络。让我们来了解一下这是什么。
- en: Encoder-Decoder Architecture
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: In this section, we will first introduce the general concept of an encoder-decoder
    architecture. Afterward, we will focus on how the encoder is used in neural machine
    translation. In the last two subsections, we will concentrate on how the decoder
    is applied during training and deployment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍编码器-解码器架构的基本概念。之后，我们将重点讨论编码器在神经机器翻译中的应用。在最后两个子节中，我们将集中讨论解码器在训练和部署过程中的应用。
- en: One of the possible structures for neural machine translation is the **encoder-decoder**
    network. In [*Chapter 5*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152), *Autoencoder
    for Fraud Detection*, we introduced the concept of a neural network consisting
    of an encoder and a decoder component. Remember, in the case of an autoencoder,
    the task of the encoder component is to extract a dense representation of the
    input, while the task of the decoder component is to recreate the input based
    on the dense representation given by the encoder.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 神经机器翻译的一种可能结构是**编码器-解码器**网络。在[*第5章*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152)《用于欺诈检测的自编码器》中，我们介绍了由编码器和解码器组成的神经网络的概念。记住，在自编码器的情况下，编码器的任务是提取输入的密集表示，而解码器的任务是根据编码器提供的密集表示重建输入。
- en: In the case of encoder-decoder networks for neural machine translation, the
    task of the encoder is to extract the context of the sentence in the source language
    (the input sentence) into a dense representation, while the task of the decoder
    is to create the corresponding translation in the target language from the dense
    representation of the encoder.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经机器翻译的编码器-解码器网络中，编码器的任务是将源语言（输入句子）的上下文提取为密集表示，而解码器的任务是根据编码器的密集表示在目标语言中生成相应的翻译。
- en: '*Figure 8.1* visualizes this process:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.1* 可视化了这个过程：'
- en: '![Figure 8.1 – The general structure of an encoder-decoder network for neural
    machine translation](img/B16391_08_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 神经机器翻译中编码器-解码器网络的通用结构](img/B16391_08_001.jpg)'
- en: Figure 8.1 – The general structure of an encoder-decoder network for neural
    machine translation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 神经机器翻译中编码器-解码器网络的通用结构
- en: Here, the source language is English, and the target language is German. The
    goal is to translate the sentence `I am a student` from English into German, where
    one correct translation could be `Ich bin ein Student`. The encoder consumes the
    `I am a student` sentence and produces as output a dense vector representation
    of the content of the sentence. This dense vector representation is fed into the
    decoder, which then outputs the translation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，源语言是英语，目标语言是德语。目标是将句子 `I am a student` 从英语翻译成德语，其中一种正确的翻译可能是 `Ich bin ein
    Student`。编码器接收句子 `I am a student` 作为输入，并输出句子内容的密集向量表示。这个密集向量表示被输入到解码器，解码器随后输出翻译结果。
- en: In this case study, the input and the output of the network are sequences. Therefore,
    **Recurrent Neural Network** (**RNN**) layers are commonly used in the encoder
    and decoder parts, to capture the context information and to handle input and
    output sequences of variable length.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，网络的输入和输出都是序列。因此，**循环神经网络**（**RNN**）层通常用于编码器和解码器部分，以捕获上下文信息并处理可变长度的输入和输出序列。
- en: In general, encoder-decoder RNN-based architectures are used for all kinds of
    sequence-to-sequence analysis tasks – for example, question-and-answer systems.
    Here, the question is first processed by the encoder, which creates a dense numerical
    representation of it, then the decoder generates the answer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，基于编码器-解码器 RNN 的架构用于各种序列到序列的分析任务——例如问答系统。这里，问题首先由编码器处理，编码器为问题生成一个密集的数值表示，然后解码器生成答案。
- en: Let's focus now on the encoder part of the neural translation network, before
    we move on to the decoder, to understand what kind of data preparation is needed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们专注于神经翻译网络的编码器部分，在我们继续讲解解码器之前，理解需要哪些数据准备。
- en: Applying the Encoder
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用编码器
- en: The goal of the encoder is to extract a dense vector representation of the context
    from the input sentence. This can be achieved by using a **Long Short-Term Memory**
    (**LSTM**) layer where the encoder reads the input sentence (in English) either
    word by word or character by character.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的目标是从输入句子中提取上下文的密集向量表示。这可以通过使用 **长短期记忆**（**LSTM**）层来实现，其中编码器逐字或逐字符地读取输入句子（英语）。
- en: Tip
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent
    Neural Networks for Demand Prediction*, we introduced LSTM layers. Remember that
    an LSTM layer has two hidden states, one being the cell state and the other being
    a filtered version of it. The cell state contains a summary of all previous inputs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第六章*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181)，《需求预测的循环神经网络》中，我们介绍了
    LSTM 层。请记住，LSTM 层有两个隐藏状态，一个是单元状态，另一个是它的过滤版本。单元状态包含了所有先前输入的摘要。
- en: 'In a classic encoder-decoder network architecture, the vectors of the hidden
    states of the LSTM layer are used to store the dense representation. *Figure 8.2*
    shows how the LSTM-based encoder processes the input sentence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的编码器-解码器网络架构中，LSTM 层的隐藏状态向量用于存储密集表示。*图 8.2* 展示了基于 LSTM 的编码器如何处理输入句子：
- en: '![Figure 8.2 – Example of how the encoder processes the input sentence](img/B16391_08_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 编码器如何处理输入句子的示例](img/B16391_08_002.jpg)'
- en: Figure 8.2 – Example of how the encoder processes the input sentence
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 编码器如何处理输入句子的示例
- en: The encoder starts with some initialized hidden state vectors. At each step,
    the next word in the sequence is fed into the LSTM unit and the hidden state vectors
    are updated. The final hidden state vectors, after processing the whole input
    sequence in the source language, contain the context representation and become
    the input for the hidden state vectors in the decoder.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器从一些初始化的隐藏状态向量开始。在每一步，序列中的下一个单词被输入到 LSTM 单元，隐藏状态向量得到更新。处理完源语言的整个输入序列后，最终的隐藏状态向量包含了上下文表示，并成为解码器中隐藏状态向量的输入。
- en: The intermediate output hidden states of the encoder are not used.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的中间输出隐藏状态不会被使用。
- en: Now that we have a dense representation of the context, we can use it to feed
    the decoder. While the way the encoder works during training and deployment stays
    the same, the way the decoder works is a bit different during training and deployment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了编码器的密集上下文表示，可以使用它来馈送给解码器。虽然编码器在训练和部署过程中的工作方式保持不变，但解码器在训练和部署过程中的工作方式有所不同。
- en: Let's first concentrate on the training phase.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先集中讨论训练阶段。
- en: Applying the Decoder during Training
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在训练过程中应用解码器
- en: The task of the decoder is to generate the translation in the target sequence
    from the dense context representation, either word by word or character by character,
    using again an RNN with an LSTM layer. This means that, in theory, each predicted
    word/character should be fed back into the network as the next input. However,
    during training, we can skip the theory and apply the concept of **teacher forcing**.
    Here, the actual word/character is fed back into the LSTM unit instead of the
    predicted word/character, which greatly benefits the training procedure.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的任务是从密集的上下文表示中生成目标序列的翻译，可以按单词或字符逐个生成，再次使用带有 LSTM 层的 RNN。这意味着，理论上，每个预测的单词/字符都应该被反馈到网络中作为下一次输入。然而，在训练过程中，我们可以跳过理论，应用**教师强制**的概念。在这里，实际的单词/字符会被反馈到
    LSTM 单元，而不是预测的单词/字符，这极大地有利于训练过程。
- en: '*Figure 8.3* shows an example of teacher forcing during the training phase
    of the decoder:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.3* 显示了解码器训练阶段的教师强制示例：'
- en: '![Figure 8.3 – Example of teacher forcing while training of the decoder](img/B16391_08_003.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 解码器训练阶段的教师强制示例](img/B16391_08_003.jpg)'
- en: Figure 8.3 – Example of teacher forcing while training of the decoder
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 解码器训练阶段的教师强制示例
- en: 'The dense context representation of the encoder is used to initialize the hidden
    states of the decoder''s LSTM layer. Next, two sequences are used by the LSTM
    layer to train the decoder: the input sequence with the true word/character values,
    starting with a **start token**, and the target sequence, also with the true word/character
    values.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的密集上下文表示用于初始化解码器 LSTM 层的隐藏状态。接下来，LSTM 层使用两个序列来训练解码器：输入序列，包含真实的单词/字符值，从**开始标记**开始，和目标序列，也包含真实的单词/字符值。
- en: Important note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The target sequence, in this case, is the input sequence shifted by one character
    and with an end token at the end.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，目标序列是输入序列向右移动一个字符，并在末尾添加一个结束标记。
- en: 'To summarize, three sequences of words/characters are needed during training:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，训练过程中需要三组单词/字符序列：
- en: The input sequence for the encoder
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的输入序列
- en: The input sequence for the decoder
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的输入序列
- en: The output sequence for the decoder
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器的输出序列
- en: During deployment, we don't have the input and output sequence for the decoder.
    So, let's find out how the trained decoder can be used during deployment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署期间，我们没有解码器的输入和输出序列。因此，让我们来看看训练好的解码器如何在部署过程中使用。
- en: Applying the Decoder during Deployment
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在部署过程中应用解码器
- en: 'When we apply the trained network, we don''t know the true values of the translation
    sequence. So, we feed only the dense context representation from the encoder and
    a start token into the decoder. Then, the decoder applies the LSTM unit multiple
    times, always feeding the last predicted word/character back into the LSTM unit
    as input for the next step. *Figure 8.4* visualizes the usage of the decoder during
    deployment:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们应用训练好的网络时，我们并不知道翻译序列的真实值。因此，我们只将来自编码器的密集上下文表示和一个开始标记输入到解码器中。然后，解码器多次应用 LSTM
    单元，每次都将上一个预测的单词/字符反馈到 LSTM 单元作为下一步的输入。*图 8.4* 展示了在部署期间解码器的使用：
- en: '![Figure 8.4 – Usage of the decoder during deployment](img/B16391_08_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 解码器在部署期间的使用](img/B16391_08_004.jpg)'
- en: Figure 8.4 – Usage of the decoder during deployment
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 解码器在部署期间的使用
- en: In the first step, the dense context representation from the encoder forms the
    input hidden state vectors and the **start token** forms the input value for the
    decoder. Based on this, the first word is predicted, and the hidden state vectors
    are updated. In the next steps, the updated hidden state vectors and the last
    predicted word are fed back into the LSTM unit, to predict the next word. This
    means that if a wrong word has been predicted once; the error accumulates in this
    kind of sequential prediction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，编码器的密集上下文表示形成输入的隐藏状态向量，而**起始标记**作为解码器的输入值。基于此，预测第一个单词，并更新隐藏状态向量。在接下来的步骤中，更新后的隐藏状态向量和最后预测的单词将被反馈到LSTM单元，以预测下一个单词。这意味着，如果某个单词第一次被预测错误，这种错误会在这种顺序预测中累积。
- en: In this section, you learned what encoder-decoder neural networks are and how
    they can be used for neural machine translation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了什么是编码器-解码器神经网络以及它们如何应用于神经机器翻译。
- en: In the next sections, we will go through the steps required to train a neural
    machine translation network to translate sentences from English into German. As
    usual, the first step is data preparation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将介绍训练一个神经机器翻译网络的步骤，目的是将英文句子翻译成德语。像往常一样，第一步是数据准备。
- en: So, let's start by creating the three sequences required to train a neural machine
    translation network using an encoder-decoder structure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们从创建训练神经机器翻译网络所需的三个序列开始，使用编码器-解码器结构。
- en: Preparing the Data for the Two Languages
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备两种语言的数据
- en: In [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), *Implementing
    NLP Applications*, we talked about the advantages and disadvantages of training
    neural networks at the character and word levels. As we already have some experience
    with the character level, we decided to also train this network for automatic
    translation at the character level.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)《实现NLP应用》中，我们讨论了在字符级别和词汇级别训练神经网络的优缺点。由于我们已经有了一些字符级别的经验，因此决定也在字符级别训练这个自动翻译网络。
- en: 'To train a neural machine translation network, we need a dataset with bilingual
    sentence pairs for the two languages. Datasets for different language combinations
    can be downloaded for free at [www.manythings.org/anki/](http://www.manythings.org/anki/).
    From there, we can download a dataset containing a number of sentences in English
    and German that are commonly used in everyday life. The dataset consists of two
    columns only: the original short text in English and the corresponding translation
    in German.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个神经机器翻译网络，我们需要一个包含两种语言的双语句子对的数据集。不同语言组合的数据集可以从[www.manythings.org/anki/](http://www.manythings.org/anki/)免费下载。在那里，我们可以下载一个包含英语和德语常用句子的数据集。该数据集仅由两列组成：英文原文和相应的德语翻译。
- en: '*Figure 8.5* shows you a subset of this dataset to be used as the training
    set:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.5*展示了一个子集数据集，作为训练集使用：'
- en: '![Figure 8.5 – Subset of the training set with English and German sentences](img/B16391_08_005.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 包含英语和德语句子的训练集子集](img/B16391_08_005.jpg)'
- en: Figure 8.5 – Subset of the training set with English and German sentences
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 包含英语和德语句子的训练集子集
- en: As you can see, for some English sentences, there is more than one possible
    translation. For example, the sentence `Hug Tom` can be translated to `Umarmt
    Tom`, `Umarmen Sie Tom`, or `Drücken Sie Tom`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于一些英文句子，可能有多个翻译。例如，句子`Hug Tom`可以翻译成`Umarmt Tom`、`Umarmen Sie Tom`或`Drücken
    Sie Tom`。
- en: Remember that a network doesn't understand characters, only numerical values.
    Thus, character input sequences need to be transformed into numerical input sequences.
    In the first part of the previous chapter, we introduced several encoding techniques.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，网络并不理解字符，只有数字值。因此，字符输入序列需要转换为数字输入序列。在上一章的第一部分中，我们介绍了几种编码技术。
- en: As for the free text generation case study, we adopted **one-hot encoding**
    as the encoding scheme, which will be implemented in two steps. First, an **index-based
    encoding** is produced; then, this index-based encoding is converted into a one-hot
    encoding inside the **Keras Network Learner** node during training and the **Keras
    Network Executor** node when applying the trained network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与自由文本生成的案例研究一样，我们采用了**独热编码**作为编码方案，这将分两步实现。首先，产生**基于索引的编码**；然后，在训练时通过**Keras网络学习器**节点将其转换为独热编码，在应用训练好的网络时通过**Keras网络执行器**节点进行转换。
- en: In addition, a dictionary mapping for the English and German characters with
    their index is also needed. In the previous chapter, for product name generation,
    we resorted to the **KNIME Text Processing Extension** to generate the index-based
    encoding for the character sequences. We will do the same here.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还需要一个字典，用于映射英语和德语字符及其索引。在上一章中，为了生成产品名称，我们使用了**KNIME文本处理扩展**来生成基于索引的字符序列编码。我们将在此处做同样的操作。
- en: 'For the training of the neural machine translation, three index-encoded character
    sequences must be created:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练神经机器翻译，必须创建三种索引编码的字符序列：
- en: The input sequence to feed the encoder. This is the index-encoded input character
    sequence from the source language – in our case, English.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入序列，用于输入编码器。这是来自源语言的索引编码字符序列——在我们的例子中是英语。
- en: The input sequence to feed the decoder. This is the index-encoded character
    sequence for the target language, starting with a start token.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入序列，用于输入解码器。这是目标语言的索引编码字符序列，以起始标记开头。
- en: The target sequence to train the decoder, which is the input sequence to the
    decoder shifted by one step in the past and ending with an end token.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练解码器的目标序列，是解码器的输入序列向后偏移一步，并以结束标记结尾。
- en: 'The workflow in *Figure 8.6* reads the bilingual sentence pairs, extracts the
    first 10,000 sentences, performs the index-encoding for the sentences in English
    and German separately, and finally, partitions the dataset into a training and
    a test set:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.6* 中的工作流读取双语句子对，提取前 10,000 个句子，分别对英语和德语句子进行索引编码，最后将数据集划分为训练集和测试集：'
- en: '![Figure 8.6 – Preprocessing workflow snippet to prepare the data to train
    the network for neural machine translation](img/B16391_08_006.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 用于准备训练神经机器翻译网络的数据的预处理工作流片段](img/B16391_08_006.jpg)'
- en: Figure 8.6 – Preprocessing workflow snippet to prepare the data to train the
    network for neural machine translation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 用于准备训练神经机器翻译网络的数据的预处理工作流片段
- en: 'Most of the work of the preprocessing happens inside the component named **Index
    encoding and sequence creation**. *Figure 8.7* shows its content:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的主要工作发生在名为**索引编码和序列创建**的组件内。*图 8.7*展示了该组件的内容：
- en: '![Figure 8.7 – Workflow snippet inside the component named Index encoding and
    sequence creation](img/B16391_08_007.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 组件内名为索引编码和序列创建的工作流片段](img/B16391_08_007.jpg)'
- en: Figure 8.7 – Workflow snippet inside the component named Index encoding and
    sequence creation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 组件内名为索引编码和序列创建的工作流片段
- en: The workflow snippet inside the component first separates the English text from
    the German text, then produces the index-encoding for the sentences – in the upper
    part for the German sentences and the lower part for the English sentences. Then,
    finally, for each language, a dictionary is created, applied, and saved.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 组件内的工作流片段首先将英语文本与德语文本分离，然后为句子生成索引编码——上部分为德语句子，下部分为英语句子。然后，最终，为每种语言创建字典并应用并保存。
- en: 'After the index-encoding of the German sentences, the two sequences for the
    decoder are created: in the upper branch by adding a start token at the beginning
    and in the lower branch by adding an end token at the end of the sequence.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在对德语句子进行索引编码后，将为解码器创建两个序列：在上支路通过在序列开头添加一个起始标记，在下支路通过在序列末尾添加一个结束标记来创建。
- en: All sequences from the German and English languages are then transformed into
    collection cells so that they can be converted to one-hot encoding before training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 德语和英语的所有序列随后会转换为集合单元格，以便在训练前将其转换为独热编码。
- en: Building and Training the Encoder-Decoder Architecture
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和训练编码器-解码器架构
- en: Now that the three sequences are available, we can start defining the network
    structure within a workflow. In this section, you will learn how to define and
    train an encoder-decoder structure in KNIME Analytics Platform. Once the network
    is trained, you will learn how the encoder and decoder can be extracted into two
    networks. In the last section, we will discuss how the extracted networks can
    be used in a deployment workflow to translate English sentences into German.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，三个序列已准备好，我们可以开始在工作流中定义网络结构。在本节中，您将学习如何在KNIME分析平台中定义和训练一个编码器-解码器结构。一旦网络训练完成，您将学习如何将编码器和解码器提取成两个网络。在最后一节中，我们将讨论如何在部署工作流中使用提取的网络将英语句子翻译成德语。
- en: Defining the Network Structure
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义网络结构
- en: 'In the encoder-decoder architecture, we want to have both the encoder and the
    decoder as LSTM networks. The encoder and the decoder have different input sequences.
    The English one-hot-encoded sentences are the input for the encoder and the German
    one-hot-encoded sentences are the input for the decoder. This means two input
    layers are needed: one for the encoder and one for the decoder.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器架构中，我们希望将编码器和解码器都设置为LSTM网络。编码器和解码器有不同的输入序列。英语的one-hot编码句子作为编码器的输入，而德语的one-hot编码句子作为解码器的输入。这意味着需要两个输入层：一个用于编码器，一个用于解码器。
- en: 'The **encoder** network is made up of two layers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**网络由两层组成：'
- en: 'An input layer implemented via the **Keras Input Layer** node: The shape of
    the input tensor is ![](img/Formula_B16391_08_009.png), where ![](img/Formula_B16391_08_010.png)
    is the dictionary size for the source language. *?* in the input tensor shape
    represents variable length sequences, while *n* indicates one-hot vectors with
    ![](img/Formula_B16391_08_011.png) components. In our example, ![](img/Formula_B16391_08_012.png)
    for the English language, and the shape of the input tensor is [?,71].'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**Keras输入层**节点实现的输入层：输入张量的形状为 ![](img/Formula_B16391_08_009.png)，其中 ![](img/Formula_B16391_08_010.png)
    是源语言的词典大小。输入张量形状中的*?*表示可变长度的序列，而*n*表示具有 ![](img/Formula_B16391_08_011.png) 个组件的one-hot向量。在我们的示例中，英语语言的形状为
    ![](img/Formula_B16391_08_012.png)，输入张量的形状是[?,71]。
- en: 'An LSTM Layer via a **Keras LSTM Layer** node: In this node, we use 256 units
    and enable the *return state* checkbox to pass the hidden states to the upcoming
    decoder network.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**Keras LSTM层**节点实现的LSTM层：在这个节点中，我们使用256个单元并启用*return state*复选框，将隐藏状态传递到即将到来的解码器网络。
- en: 'The **decoder network** is made of three layers:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器网络**由三层组成：'
- en: First, a **Keras Input Layer** node to define the input shape. Again, the input
    shape ![](img/Formula_B16391_08_013.png) is a tuple, where ![](img/Formula_B16391_08_014.png)
    represents a variable length and ![](img/Formula_B16391_08_015.png) the size of
    each vector in the input sequence – that is, the dictionary size of the target
    language (German). In our example, the input tensor for German has a shape of
    ![](img/Formula_B16391_08_016.png).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，使用**Keras输入层**节点来定义输入形状。同样，输入形状 ![](img/Formula_B16391_08_013.png) 是一个元组，其中
    ![](img/Formula_B16391_08_014.png) 代表变量长度，![](img/Formula_B16391_08_015.png) 代表输入序列中每个向量的大小——即目标语言（德语）的词汇表大小。在我们的示例中，德语的输入张量形状为
    ![](img/Formula_B16391_08_016.png)。
- en: An LSTM layer via a Keras **LSTM Layer** node. This time, the optional input
    ports are used to feed the hidden states from the encoder into the decoder. This
    means the output port of the first LSTM layer in the encoder network is connected
    to both optional input ports in the decoder network. In addition, the output port
    of the Keras Input Layer node for the German input sequences is connected to the
    top input port. In its configuration window, it is important to select the correct
    input tensors as well as the hidden tensors. The *return sequence* and *return
    state* checkboxes must be activated to return the intermediate output hidden states,
    which are used in the next layer to extract the probability distribution for the
    next predicted character. As in the encoder LSTM, 256 units are used.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Keras **LSTM层**节点实现LSTM层。这一次，可选的输入端口用于将编码器中的隐藏状态传递到解码器。这意味着编码器网络中第一个LSTM层的输出端口连接到解码器网络中的两个可选输入端口。此外，Keras输入层节点的输出端口用于连接德语输入序列到顶部输入端口。在其配置窗口中，选择正确的输入张量和隐藏张量是非常重要的。必须激活*return
    sequence*和*return state*复选框，以返回中间输出的隐藏状态，这些状态在下一层中用于提取下一个预测字符的概率分布。与编码器LSTM一样，使用256个单元。
- en: Last, a softmax layer is added via a **Keras Dense Layer** node to produce the
    probability vector of the characters in the dictionary in the target language
    (German). In the configuration window, the softmax activation function is selected
    to have 85 units, which is the size of the dictionary of the target language.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，通过**Keras Dense层**节点添加了一个softmax层，以生成目标语言（德语）词典中字符的概率向量。在配置窗口中，选择softmax激活函数，设定85个单元，这是目标语言词典的大小。
- en: 'The workflow in *Figure 8.8* defines this encoder-decoder network structure:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.8*中的工作流定义了这个编码器-解码器网络结构：'
- en: '![Figure 8.8 – The workflow snippet that defines the encoder-decoder network](img/B16391_08_008.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 定义编码器-解码器网络的工作流片段](img/B16391_08_008.jpg)'
- en: Figure 8.8 – The workflow snippet that defines the encoder-decoder network
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 定义编码器-解码器网络的工作流片段
- en: The upper part of the workflow defines the encoder with a **Keras Input Layer**
    and **Keras LSTM Layer** node. In the lower part, the decoder is defined as described
    previously.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流的上半部分定义了带有 **Keras 输入层** 和 **Keras LSTM 层** 节点的编码器。在下半部分，解码器按之前描述的方式定义。
- en: Now that we have defined the encoder-decoder architecture, we can train the
    network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了编码器-解码器架构，可以开始训练网络了。
- en: Training the Network
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练网络
- en: As in all other examples in this book, the **Keras Network Learner** node is
    used to train the network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书中的所有其他示例一样，**Keras 网络学习器** 节点被用来训练网络。
- en: 'In the first tab of the configuration window, named **Input Data**, the input
    columns for both input layers are selected: in the upper part for the source language,
    which means the input for the encoder, and in the lower part for the target language,
    which means the input for the decoder. To convert the index-encoded sequences
    into one-hot-encoded sequences, the **From Collection of Number (integer) to One-Hot
    Tensor** conversion type is used for both columns.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置窗口的第一个标签页，名为 **输入数据** 中，选择了两个输入层的输入列：上半部分是源语言，即编码器的输入，下半部分是目标语言，即解码器的输入。为了将索引编码的序列转换为
    one-hot 编码序列，两个列都使用 **从数字集合（整数）到 One-Hot 张量** 的转换类型。
- en: In the next tab of the configuration window, named **Target Data**, the column
    with the target sequence for the decoder is selected and the **From Collection
    of Number (integer) to One-Hot Tensor** conversion type is enabled again. Characters
    are again considered like classes in a multi-class classification problem; therefore,
    the Categorical Cross Entropy loss function is adopted for the training process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置窗口的下一个标签页，名为 **目标数据** 中，选择了解码器的目标序列列，并再次启用 **从数字集合（整数）到 One-Hot 张量** 的转换类型。字符再次被视为多类分类问题中的类别；因此，训练过程中采用了分类交叉熵损失函数。
- en: In the third tab, **Options**, the training phase is set to run for a maximum
    of 120 epochs, with a batch size of 128 data rows, shuffling the data before each
    epoch and using Adam as the optimizer algorithm with the default settings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个标签页 **Options** 中，训练阶段设置为最多运行 120 个周期，批量大小为 128 数据行，每个周期前对数据进行洗牌，并使用 Adam
    作为优化算法，采用默认设置。
- en: During training, we monitor the performance using the **Learner Monitor** view
    of the Keras Network Learner node and decide to stop the learning process when
    an accuracy of 94% has been reached.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们使用 **Learner Monitor** 视图来监控 Keras 网络学习器节点的性能，并在准确率达到 94% 时决定停止学习过程。
- en: Extracting the Trained Encoder and Decoder
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取训练好的编码器和解码器
- en: To apply the trained model to translate new sentences, we need to split the
    encoder and decoder apart. To do so, each part is extracted from the complete
    network using a few lines of Python code in a **DL Python Network Editor** node.
    This node allows us to edit and modify the network structure using the **Python
    libraries** directly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将训练好的模型应用于翻译新的句子，我们需要将编码器和解码器分开。为此，每个部分都可以通过在 **DL Python 网络编辑器** 节点中使用几行
    Python 代码从完整的网络中提取出来。该节点允许我们直接使用 **Python 库** 编辑和修改网络结构。
- en: Remember that the output of the decoder is the probability distribution across
    all characters in the target language. In [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*, we introduced two approaches for the prediction
    of the next character based on this output probability distribution. Option one
    picks the character with the highest probability as the next character. Option
    two picks the next character randomly according to the given probability distribution.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，解码器的输出是目标语言中所有字符的概率分布。在 [*第七章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)，*实现自然语言处理应用*
    中，我们介绍了基于此输出概率分布预测下一个字符的两种方法。选项一是选择具有最高概率的字符作为下一个字符。选项二是根据给定的概率分布随机选择下一个字符。
- en: 'In this case study, we use option one and implement it directly in the decoder
    via an additional **lambda layer**. To summarize, when postprocessing, we need
    to perform the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们使用选项一，并通过额外的 **lambda 层** 直接在解码器中实现它。总结来说，在后处理时，我们需要执行以下步骤：
- en: Separate the encoder and decoder parts of the network.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分离网络中的编码器和解码器部分。
- en: Introduce a lambda layer with an argmax function that selects the character
    with the highest probability in the softmax layer.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入一个带有 argmax 函数的 lambda 层，它选择 softmax 层中概率最高的字符。
- en: Important note
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Lamba layers allow you to use arbitrary TensorFlow functions when constructing
    sequential and functional API models using TensorFlow as the backend. Lambda layers
    are best suited for simple operations or quick experimentation.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Lambda 层允许你在使用 TensorFlow 作为后端构建顺序模型和功能 API 模型时，使用任意的 TensorFlow 函数。Lambda 层最适合用于简单操作或快速实验。
- en: Let's start with extracting the encoder.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从提取编码器开始。
- en: Extracting the Encoder
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取编码器：
- en: 'In the following code, you can see the Python code used to extract the encoder:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，你可以看到用于提取编码器的 Python 代码：
- en: 'Load packages:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载包：
- en: '[PRE0]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define input:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入：
- en: '[PRE1]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Extract trained encoder LSTM and define model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取训练好的编码器 LSTM 并定义模型：
- en: '[PRE2]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It starts with defining the input, feeding it into the encoder's LSTM layer,
    and then defining the output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 它从定义输入开始，将其输入到编码器的 LSTM 层，然后定义输出。
- en: In more detail, in the first two lines, the required packages are loaded. Next,
    an input layer is defined; then, the `-3` layer – the trained LSTM layer of the
    encoder – is extracted. Finally, the network output is defined as the output of
    the trained encoder LSTM layer
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，在前两行中，加载了所需的包。接下来，定义了输入层；然后，提取了`-3`层——编码器的训练 LSTM 层。最后，网络输出被定义为训练好的编码器
    LSTM 层的输出。
- en: Now that we have extracted the encoder, let's see how we can extract the decoder.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取了编码器，接下来看看如何提取解码器。
- en: Extracting the Decoder and Adding a Lambda Layer
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取解码器并添加 lambda 层：
- en: 'In the following code snippet, you can see the code used in the **DL Python
    Network Editor** node to extract the decoder part and add the lambda layer to
    it:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，你可以看到在**DL Python 网络编辑器**节点中使用的代码，用于提取解码器部分并向其中添加 lambda 层：
- en: 'Load the packages:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载包：
- en: '[PRE3]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define the inputs:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入：
- en: '[PRE4]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Extract the trained decoder LSTM layer and softmax layer:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取训练好的解码器 LSTM 层和 softmax 层：
- en: '[PRE5]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Apply the LSTM and dense layer:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 LSTM 和全连接层：
- en: '[PRE6]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Add the lambda layer and define the output:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 lambda 层并定义输出：
- en: '[PRE7]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The code again first loads the necessary packages, then defines three inputs
    – two for the input hidden states and one for the one-hot-encoded character vector.
    Next, it extracts the trained LSTM layer and the softmax layer in the decoder.
    Finally, it introduces the lambda layer with the `argmax` function and defines
    the output.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先加载了必要的包，然后定义了三个输入——两个是输入的隐藏状态，一个是单热编码的字符向量。接着，它提取了解码器中的训练 LSTM 层和 softmax
    层。最后，它引入了带有 `argmax` 函数的 lambda 层并定义了输出。
- en: For faster execution during deployment, the encoder and the decoder are converted
    into TensorFlow networks using the **Keras to TensorFlow Network Converter** node.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在部署过程中加速执行，编码器和解码器会使用**Keras 到 TensorFlow 网络转换器**节点转换为 TensorFlow 网络。
- en: Now that we have trained the neural machine translation network and we have
    separated the encoder and the decoder, we want to apply them to the sentences
    in the test set.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了神经机器翻译网络，并且分离了编码器和解码器，我们希望将它们应用到测试集中的句子上。
- en: 'The full training workflow is available on the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/.](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/.)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的训练工作流可以在 KNIME Hub 上找到：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/.](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/.)
- en: Applying the Trained Network for Neural Machine Translation
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用训练好的网络进行神经机器翻译：
- en: To apply the encoder and decoder networks to the test data, we need a workflow
    that first applies the encoder to the index-encoded English sentences to extract
    the context information, and then applies the decoder to produce the translation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将编码器和解码器网络应用到测试数据中，我们需要一个工作流，首先将编码器应用到索引编码的英文句子中以提取上下文信息，然后应用解码器生成翻译。
- en: 'The decoder should be initialized with the first hidden states from the encoder
    and with the start token from the input sequence, to trigger the translation character
    by character in the recursive loop. *Figure 8.9* visualizes the process:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器应该使用来自编码器的第一个隐藏状态和输入序列中的起始符号进行初始化，以便在递归循环中逐字符地触发翻译。*图 8.9* 展示了这一过程：
- en: '![Figure 8.9 – The idea of applying the encoder and decoder model during deployment](img/B16391_08_009.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 部署过程中应用编码器和解码器模型的思路](img/B16391_08_009.jpg)'
- en: Figure 8.9 – The idea of applying the encoder and decoder model during deployment
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 部署过程中应用编码器和解码器模型的思路
- en: 'The workflow snippet in *Figure 8.10* performs exactly these steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.10*中的工作流片段正好执行了这些步骤：'
- en: '![Figure 8.10 – This workflow snippet applies the trained encoder-decoder neural
    architecture to translate English sentences into German sentences](img/B16391_08_010.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 此工作流片段应用训练好的编码器-解码器神经架构将英文句子翻译成德语句子](img/B16391_08_010.jpg)'
- en: Figure 8.10 – This workflow snippet applies the trained encoder-decoder neural
    architecture to translate English sentences into German sentences
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 此工作流片段应用训练好的编码器-解码器神经架构将英文句子翻译成德语句子
- en: It starts with a **TensorFlow Network Executor** node (the first one on the
    left in *Figure 8.10*). This node takes the encoder and the index-encoded English
    sentences as input. In its configuration window, two outputs are defined from
    the LSTM hidden states.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 它从**TensorFlow网络执行器**节点开始（位于*图8.10*的最左侧）。此节点将编码器和索引编码的英文句子作为输入。在其配置窗口中，定义了来自LSTM隐藏状态的两个输出。
- en: Next, we create a start token and transform it into a collection cell. To this
    start token, we apply the decoder network using another **TensorFlow Network Executor**
    node (the second one from the left). In the configuration window, we make sure
    that the hidden states from the encoder produced in the previous **TensorFlow
    Network Executor** node are used as input. As output, we again set the hidden
    states, as well as the next predicted character – that is, the first character
    of the translated sentence.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个起始标记，并将其转换为一个集合单元。对这个起始标记，我们应用解码器网络，使用另一个**TensorFlow网络执行器**节点（从左侧数第二个）。在配置窗口中，我们确保使用上一个**TensorFlow网络执行器**节点中生成的编码器隐藏状态作为输入。作为输出，我们再次设置隐藏状态，以及下一个预测字符——即翻译句子的第一个字符。
- en: Now, we enter the recursive loop, where this process is repeated multiple times
    using the updated hidden states from the last iteration and the last predicted
    character as input.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入递归循环，在此过程中，使用来自上次迭代的更新隐藏状态和最后预测的字符作为输入，重复此过程多次。
- en: 'Finally, the German dictionary is applied to the index-encoded predicted characters
    and the final translation is obtained. The following is an excerpt of the translation
    results:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，德语词典被应用于索引编码的预测字符，最终翻译结果得以获得。以下是翻译结果的摘录：
- en: '![Figure 8.11 – Final results of the deployed translation network on new English
    sentences](img/B16391_08_011.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11 – 部署的翻译网络在新英文句子上的最终结果](img/B16391_08_011.jpg)'
- en: Figure 8.11 – Final results of the deployed translation network on new English
    sentences
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 部署的翻译网络在新英文句子上的最终结果
- en: In the first column, we have the new English sentences, in the second column,
    the correct translations, and in the last column, the translation generated by
    the network. Most of these translations are actually correct, even though they
    don't match the sentences in column two, as the same sentence can have different
    translations. On the other hand, the translation of the `Talk to Tom` sentence
    is not correct as `rune` is not a German word.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一列，我们有新的英文句子，在第二列，有正确的翻译，在最后一列，有网络生成的翻译。尽管这些翻译大多数实际上是正确的，尽管它们与第二列的句子不匹配，因为相同的句子可以有不同的翻译。另一方面，`Talk
    to Tom`句子的翻译是不正确的，因为`rune`不是德语单词。
- en: 'The described deployment workflow is available on the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的部署工作流可以在KNIME Hub上找到：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%208/)。
- en: In this section, you have learned how you can define, train, and apply encoder-decoder
    architectures based on the example of neural machine translation at the character
    level.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你已经学会了如何根据字符级别的神经机器翻译示例，定义、训练并应用编码器-解码器架构。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the topic of neural machine translation and trained
    a network to produce English-to-German translations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了神经机器翻译的主题，并训练了一个网络来生成英译德翻译。
- en: We started with an introduction to automatic machine translation, covering its
    history from rule-based machine translation to neural machine translation. Next,
    we introduced the concept of encoder-decoder RNN-based architectures, which can
    be used for neural machine translation. In general, encoder-decoder architectures
    can be used for sequence-to-sequence prediction tasks or question-answer systems.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从自动机器翻译的介绍开始，讲解了其历史，从基于规则的机器翻译到神经机器翻译。接着，我们介绍了基于编码器-解码器 RNN 架构的概念，这些架构可以用于神经机器翻译。一般来说，编码器-解码器架构可以用于序列到序列的预测任务或问答系统。
- en: After that, we covered all the steps needed to train and apply a neural machine
    translation model at the character level, using a simple network structure with
    only one LSTM unit for both the encoder and decoder. The joint network, derived
    from the combination of the encoder and decoder, was trained using a **teacher
    forcing** paradigm.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们讲解了训练和应用一个基于字符级的神经机器翻译模型所需的所有步骤，使用一个简单的网络结构，其中编码器和解码器都只有一个 LSTM 单元。由编码器和解码器结合而成的联合网络，采用了**教师强制**范式进行训练。
- en: At the end of the training phase and before deployment, a **lambda layer** was
    inserted in the decoder part to predict the character with the highest probability.
    In order to do that, the structure of the trained network was modified after the
    training process with a few lines of Python code in a DL Python Network Editor
    node. The Python code split the decoder and the encoder networks and added the
    lambda layer. This was the only part involving a short, simple snippet of Python
    code.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段结束并在部署之前，**lambda 层**被插入到解码器部分，用于预测概率最高的字符。为此，训练后的网络结构在训练过程后被用几行 Python
    代码修改。Python 代码将解码器和编码器网络分开，并添加了 lambda 层。这是唯一涉及一小段简单 Python 代码的部分。
- en: Of course, this network could be further improved in many ways – for example,
    by stacking multiple LSTM layers or by training the model at the word level using
    additional embeddings.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个网络可以通过许多方式进一步改进——例如，通过堆叠多个 LSTM 层，或者使用额外的嵌入层在词汇级别上训练模型。
- en: This is the last chapter on RNNs. In the next chapter, we want to move on to
    another class of neural networks, **Convolutional Neural Networks** (**CNNs**),
    which have proven to be very successful for image processing.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于 RNN 的最后一章。在下一章中，我们将介绍另一类神经网络——**卷积神经网络**（**CNNs**），这种网络在图像处理方面已经证明了其非常成功的应用。
- en: Questions and Exercises
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与练习
- en: 'An encoder-decoder model is a:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器-解码器模型是一个：
- en: a.) Many-to-one architecture
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a.) 多对一架构
- en: b.) Many-to-many architecture
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b.) 多对多架构
- en: c.) One-to-many architecture
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c.) 一对多架构
- en: d.) CNN architecture
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d.) CNN 架构
- en: What is the task of the encoder in neural machine translation?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器在神经机器翻译中的任务是什么？
- en: a.) To encode the characters
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a.) 编码字符
- en: b.) To generate the translation
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b.) 生成翻译
- en: c.) To extract a dense representation of the content in the target language
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c.) 提取目标语言内容的密集表示
- en: d.) To extract a dense representation of the content in the source language
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d.) 提取源语言内容的密集表示
- en: What is another application for encoder-decoder LSTM networks?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器-解码器 LSTM 网络的另一个应用是什么？
- en: a.) Text classification
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a.) 文本分类
- en: b.) Question-answer systems
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b.) 问答系统
- en: c.) Language detection
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c.) 语言检测
- en: d.) Anomaly detection
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d.) 异常检测
