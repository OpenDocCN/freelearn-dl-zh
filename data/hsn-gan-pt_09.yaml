- en: Image Restoration with GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 进行图像修复
- en: Have you ever stumbled upon an image (or meme) you really love from the internet
    that has poor quality and is blurry, and even Google couldn't help you to find
    a high-resolution version of it? Unless you are one of the few who have spent
    years learning math and coding, knowing exactly which fractional-order regularization
    term in your objective equation can be solved by which numerical method, we might
    as well give GANs a shot!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经遇到过那种你非常喜欢的、质量差且模糊的图像（或表情包），即使是 Google 也无法帮助你找到更高分辨率的版本？除非你是那少数几位已经花费多年时间学习数学和编程的人，知道在目标方程中哪个分数阶正则化项可以通过哪种数值方法来求解，不然我们不妨尝试一下
    GAN！
- en: This chapter will help you to perform image super-resolution with SRGAN to generate
    high-resolution images from low-resolution ones and use a data prefetcher to speed
    up data loading and increase your GPU's efficiency during training. You will also
    learn how to implement your own convolution with several methods, including the
    direct approach, the FFT-based method, and the im2col method. Later on, we will
    get to see the disadvantages of vanilla GAN loss functions and how to improve
    them by using Wasserstein loss (the Wasserstein GAN). By the end of this chapter,
    you will have learned how to train a GAN model to perform image inpainting and
    fill in the missing parts of an image.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将帮助你使用 SRGAN 实现图像超分辨率，将低分辨率图像生成高分辨率图像，并使用数据预取器加速数据加载，提高训练过程中 GPU 的效率。你还将学习如何通过几种方法实现自己的卷积操作，包括直接方法、基于
    FFT 的方法和 im2col 方法。稍后，我们将了解原始 GAN 损失函数的缺点，并通过使用 Wasserstein 损失（即 Wasserstein GAN）来改进它们。在本章结束时，你将学会如何训练
    GAN 模型来执行图像修复，填补图像中的缺失部分。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Image super-resolution with SRGAN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SRGAN 进行图像超分辨率
- en: Generative image inpainting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式图像修复
- en: Image super-resolution with SRGAN
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SRGAN 进行图像超分辨率
- en: 'Image restoration is a vast field. There are three main processes involved
    in image restoration:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复是一个广泛的领域，涉及到图像修复的三大主要过程：
- en: 'Image super-resolution: Expanding an image to a higher resolution'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像超分辨率：将图像扩展至更高的分辨率
- en: 'Image deblur: Turning a blurry image into a sharp one'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像去模糊：将模糊的图像转化为清晰图像
- en: 'Image inpainting: Filling in holes or removing watermarks in an image'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像修复：填补图像中的空洞或去除水印
- en: 'All of these processes involve estimating pixel information from existing pixels.
    The term **restoration** of the pixels actually refers to estimating the way they
    should have looked. Take image super-resolution, for example: to expand the image
    size by 2, we need to estimate 3 additional pixels to form a 2 x 2 region with
    the current pixel. Image restoration has been studied by researchers and organizations
    for decades and many profound mathematical methods have been developed, which
    kind of discourages non-mathematicians from having fun with it. Now, intriguingly
    enough, GANs are starting to gain popularity.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些过程都涉及从现有像素中估计像素信息。**修复**像素一词实际上是指估计它们应该呈现的样子。例如，在图像超分辨率中：为了将图像大小扩展一倍，我们需要估算三个额外的像素，以便与当前像素一起形成一个
    2 x 2 区域。图像修复已经被研究人员和组织研究了几十年，开发了许多深奥的数学方法，这使得非数学背景的人很难轻松上手。现在，令人感兴趣的是，GAN 正在逐渐受到关注。
- en: In this section, we will introduce another member of the GAN family, SRGAN,
    to upscale our images to a higher resolution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将介绍 GAN 家族的另一位成员 SRGAN，用于将图像上采样至更高的分辨率。
- en: SRGAN (Super-Resolution Generative Adversarial Network) was proposed by Christian
    Ledig, Lucas Theis, Ferenc Huszar, et al. in their paper, *Photo-Realistic Single
    Image Super-Resolution Using a Generative Adversarial Network*. It is considered
    the first method to successfully upscale images by four. Its structure is very
    straightforward. Like many other GANs, it consists of one generator network and
    one discriminator network. Their architectures are shown in the following sections.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN（超分辨率生成对抗网络）由 Christian Ledig、Lucas Theis、Ferenc Huszar 等人提出，发表在他们的论文 *Photo-Realistic
    Single Image Super-Resolution Using a Generative Adversarial Network* 中。它被认为是首个成功将图像分辨率提高四倍的方法。它的结构非常简单。与许多其他
    GAN 一样，它由一个生成器网络和一个判别器网络组成。它们的架构将在以下部分展示。
- en: Creating a generator
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建生成器
- en: 'Let''s take a look at the components of the generator network:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下生成器网络的组成部分：
- en: '![](img/3db178a0-e043-429c-b455-760cdb9fec3d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3db178a0-e043-429c-b455-760cdb9fec3d.png)'
- en: Generator architecture of SRGAN (2X)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN 的生成器架构（2X）
- en: In the preceding diagram, we upscale a 512*512 image by 2x (to 1,024*1,024)
    as an example. The size of the input image is rather arbitrary since the design
    of each component in the generator network is independent of the size of feature
    maps. The upsampling block is responsible for expanding the image size by two.
    If we want to upscale by four, we simply need to append another upsampling block
    to the end of the existing one. Using three upsampling blocks will, of course,
    expand the image size by eight.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们以2倍（到1,024*1,024）放大一个512*512的图像作为例子。输入图像的大小是相当随意的，因为生成器网络中每个组件的设计与特征图的大小无关。上采样块负责将图像大小扩大一倍。如果我们想放大四倍，只需要在现有的上采样块后再添加一个上采样块。使用三个上采样块，当然可以将图像大小扩大八倍。
- en: In the generator network, the high-level features are extracted by the five
    residual blocks, which are combined with the less-processed detail information
    from a raw image (via the long skip-connection crossing over the residual blocks).
    The combined feature map is expanded to ![](img/81499972-4db1-4ada-88ff-f8000c4ade9b.png) channels
    (in which ![](img/1ca46ed6-0d0c-4e42-9ef6-47a8ade74cb3.png) stands for scale factor
    and ![](img/9ef4e851-f96d-448c-b8f4-a5059b456f83.png) stands for the number of
    channels in the residual blocks) with the size of ![](img/59d14244-4a4f-43fc-875b-5dcb8ce7c586.png).
    The upsampling block transforms this ![](img/654d971b-e31f-47fa-ace9-83b19ffe87d1.png) `Tensor`
    (![](img/5e6bfa9e-638f-4e34-8add-45fdb836d404.png) stands for batch size) into
    ![](img/939adbda-2fba-488f-93a1-96ec6dd93a77.png). This is done by **sub**-**pixel
    convolution**, which was proposed by Wenzhe Shi, Jose Caballero, Ferenc Huszár,
    et al. in their paper, *Real-Time Single Image and Video Super-Resolution Using
    an Efficient Sub-Pixel Convolutional Neural Network*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器网络中，高级特征通过五个残差块提取，这些特征与来自原始图像的较少处理的细节信息（通过跨越残差块的长跳跃连接）结合。结合后的特征图被扩展为 ![](img/81499972-4db1-4ada-88ff-f8000c4ade9b.png) 通道（其中 ![](img/1ca46ed6-0d0c-4e42-9ef6-47a8ade74cb3.png) 代表尺度因子， ![](img/9ef4e851-f96d-448c-b8f4-a5059b456f83.png) 代表残差块中的通道数），其大小为 ![](img/59d14244-4a4f-43fc-875b-5dcb8ce7c586.png)。上采样块将此 ![](img/654d971b-e31f-47fa-ace9-83b19ffe87d1.png) `Tensor`（ ![](img/5e6bfa9e-638f-4e34-8add-45fdb836d404.png) 代表批量大小）转换为 ![](img/939adbda-2fba-488f-93a1-96ec6dd93a77.png)。这一过程通过**子像素卷积**实现，这一方法由Wenzhe
    Shi、Jose Caballero、Ferenc Huszár等人在其论文《*使用高效子像素卷积神经网络进行实时单图像和视频超分辨率*》中提出。
- en: An example of sub-pixel convolution is shown in the following. For every ![](img/8505f9ca-374c-4a1c-acbd-4e19dceadf6e.png) channel
    in the low-resolution feature map, each channel is only responsible for one pixel
    inside the ![](img/4304736c-ad4d-4c86-8d77-d06fdba0c801.png) block in the high-resolution
    output. A big advantage of this approach is that it only performs ![](img/97145156-a1da-44aa-8a56-28ded7205372.png) of
    the convolution operations compared to the vanilla convolution layer, which makes
    it easier and faster to train.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是子像素卷积的示例。在低分辨率特征图中的每个 ![](img/8505f9ca-374c-4a1c-acbd-4e19dceadf6e.png) 通道中，每个通道只负责高分辨率输出中 ![](img/4304736c-ad4d-4c86-8d77-d06fdba0c801.png) 块内的一个像素。这种方法的一个大优点是，相较于普通卷积层，它只执行 ![](img/97145156-a1da-44aa-8a56-28ded7205372.png) 的卷积操作，从而使训练更容易且更快速。
- en: In PyTorch, the upscaling step in sub-pixel convolution can be done by the `nn.PixelShuffle`
    layer, which is essentially reshaping the input tensor. You can check out the
    source code in C++ here, [pytorch/aten/src/ATen/native/PixelShuffle.cpp](https://github.com/pytorch/pytorch/blob/517c7c98610402e2746586c78987c64c28e024aa/aten/src/ATen/native/PixelShuffle.cpp),
    to see how the reshaping is performed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，子像素卷积中的上采样步骤可以通过 `nn.PixelShuffle` 层完成，实际上它是重新调整输入张量的形状。你可以在这里查看C++的源代码，[pytorch/aten/src/ATen/native/PixelShuffle.cpp](https://github.com/pytorch/pytorch/blob/517c7c98610402e2746586c78987c64c28e024aa/aten/src/ATen/native/PixelShuffle.cpp)，看看形状是如何调整的。
- en: How do we check out the source code of a PyTorch operation? It is easy when
    using VS Code. We can just keep repeatedly double-clicking the class name and
    press *F12* until we reach the class definition inside the source tree of the `torch` module
    under the Python environment. We then look for what other method is called inside
    this class (normally, we can find it in `self.forward`), which will lead us to
    its C++ implementation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如何查看PyTorch操作的源代码？在使用VS Code时很容易。我们只需不断双击类名，并按 *F12* 直到我们到达 `torch` 模块下源代码树中的类定义。然后，我们查找该类中调用了哪些其他方法（通常可以在 `self.forward` 中找到），这将引导我们找到其C++实现。
- en: 'Here are the steps to reach the C++ source code for implementation of `nn.PixelShuffle`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现`nn.PixelShuffle`的C++源代码步骤：
- en: Double-click the name, `PixelShuffle`, and press *F12*. It leads us to this
    line in `site-packages/torch/nn/modules/__init__.py`*:*
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击名称`PixelShuffle`，然后按*F12*。这会把我们带到`site-packages/torch/nn/modules/__init__.py`文件中的这一行：*
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Double-clicking and pressing *F12* on `PixelShuffle` inside this line brings
    us to the class definition of `PixelShuffle` in `site-packages/torch/nn/modules/pixelshuffle.py`.
    Inside its `forward` method, we can see that `F.pixel_shuffle` is called.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击并按*F12*在这一行内的`PixelShuffle`，会将我们带到`site-packages/torch/nn/modules/pixelshuffle.py`中的`PixelShuffle`类定义。在它的`forward`方法中，我们可以看到调用了`F.pixel_shuffle`。
- en: 'Again, double-click and press *F12* on `pixel_shuffle`. We reach a snippet
    like this in `site-packages/torch/nn/functional.py`:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次双击并按*F12*在`pixel_shuffle`上。我们会到达`site-packages/torch/nn/functional.py`中的类似片段：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is where the C++ part of the code is registered as a Python object in
    PyTorch. The C++ counterpart of a PyTorch operation is sometimes also called from
    the `torch._C._nn` module. Hovering the mouse over `torch.pixel_shuffle` will
    show us `pixel_shuffle(self: Tensor, upscale_factor: int) -> Tensor`, depending
    on what extensions are used in VS Code. Unfortunately, we cannot find anything
    useful by pressing *F12* on it.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '这是代码中的C++部分作为Python对象注册到PyTorch的地方。PyTorch操作的C++对应物有时也会从`torch._C._nn`模块调用。在VS
    Code中，将鼠标悬停在`torch.pixel_shuffle`上会显示`pixel_shuffle(self: Tensor, upscale_factor:
    int) -> Tensor`，具体取决于使用的扩展。不幸的是，按*F12*没有找到任何有用的内容。'
- en: 'To find the C++ implementation of this `pixel_shuffle` function, we can simply
    search for the `pixel_shuffle` keyword inside the PyTorch repository on GitHub.
    If you have cloned the source code of PyTorch locally, you can type in the following
    command in the Terminal to search for a keyword in the `*.cpp` files:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查找此`pixel_shuffle`函数的C++实现，我们只需在GitHub上的PyTorch仓库中搜索`pixel_shuffle`关键字。如果你已将PyTorch源代码克隆到本地，可以在终端中输入以下命令来在`*.cpp`文件中搜索关键字：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Hence, we can find the function definition, `Tensor pixel_shuffle(const Tensor&
    self, int64_t upscale_factor)`, inside `pytorch/aten/src/ATen/native/PixelShuffle.cpp`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以在`pytorch/aten/src/ATen/native/PixelShuffle.cpp`中找到函数定义`Tensor pixel_shuffle(const
    Tensor& self, int64_t upscale_factor)`。
- en: 'If you are interested in how PyTorch was made and how C++ and Python are working
    together (on CPU and GPU) to deliver such a flexible and easy-to-use interface,
    you can check out this lone essay written by one of the developers of PyTorch,
    Edward Z. Yang: [http://blog.ezyang.com/2019/05/pytorch-internals](http://blog.ezyang.com/2019/05/pytorch-internals).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对PyTorch是如何开发的，以及C++和Python如何在CPU和GPU上协同工作，提供如此灵活且易于使用的接口感兴趣，可以查看PyTorch开发者之一Edward
    Z. Yang写的这篇长文：[http://blog.ezyang.com/2019/05/pytorch-internals](http://blog.ezyang.com/2019/05/pytorch-internals)。
- en: 'Now, let''s take a look at the code for defining the generator network. Our
    implementation of SRGAN is mostly based on this repository: [https://github.com/leftthomas/SRGAN](https://github.com/leftthomas/SRGAN).
    The full working source code for PyTorch 1.3 is also available under the code
    repository for this chapter. We''ll start by creating a new Python file. We''ll
    call it `srgan.py`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看定义生成器网络的代码。我们实现的SRGAN大部分基于这个仓库：[https://github.com/leftthomas/SRGAN](https://github.com/leftthomas/SRGAN)。本章的代码仓库中也提供了PyTorch
    1.3的完整工作源代码。我们将从创建一个新的Python文件开始，命名为`srgan.py`：
- en: Define the residual block (after, of course, importing the necessary modules).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义残差块（当然，首先需要导入必要的模块）。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here,** Parametric ReLU** (**PReLU**) is used as an activation function. PReLU
    is very similar to LeakyReLU, except that the slope factor for negative values
    is a learnable parameter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**参数化ReLU**（**PReLU**）被用作激活函数。PReLU与LeakyReLU非常相似，不同之处在于负值的斜率因子是一个可学习的参数。
- en: 'Define the upsampling block:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义上采样块：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we use one `nn.Conv2d` layer and one `nn.PixelShuffle` layer to perform
    the sub-pixel convolution, for reshaping the low-resolution feature map to high-resolution.
    It is a recommended method by the PyTorch official example: [https://github.com/pytorch/examples/blob/master/super_resolution/model.py](https://github.com/pytorch/examples/blob/master/super_resolution/model.py).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一个`nn.Conv2d`层和一个`nn.PixelShuffle`层来执行子像素卷积，将低分辨率特征图重塑为高分辨率。这是PyTorch官方示例推荐的方法：[https://github.com/pytorch/examples/blob/master/super_resolution/model.py](https://github.com/pytorch/examples/blob/master/super_resolution/model.py)。
- en: 'Define the generator network with the residual and upsampling blocks:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义包含残差和上采样块的生成器网络：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Don't forget the long skip-connection at the end (`self.block8(block1 + block7)`).
    Finally, the output of the generator network is scaled to [0,1] from the range
    of [-1,1] by a tanh activation function. It is because the pixel values of the
    training images lie within the range of [0,1] and we should make it comfortable
    for the discriminator network to distinguish the differences between real and
    fake images when we put their values in the same range.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了最后的长跳跃连接（`self.block8(block1 + block7)`）。最后，生成器网络的输出通过tanh激活函数被缩放到[0,1]的范围，原始范围是[-1,1]。这是因为训练图像的像素值位于[0,1]范围内，我们需要将其调整为有利于判别网络区分真实和伪造图像的相同范围。
- en: We haven't talked about how we should watch out for the trap of value range
    when training GANs. In the previous chapters, we pretty much always scale the
    input images to [-1,1] with `transforms.Normalize((0.5,), (0.5,))` during the
    pre-processing of training data. Since the output of `torch.tanh` is also [-1,1],
    there's no need to rescale the generated samples before feeding them to the discriminator
    network or loss function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有谈到在训练GAN时如何注意值范围的陷阱。在之前的章节中，我们几乎总是通过`transforms.Normalize((0.5,), (0.5,))`将输入图像缩放到[-1,1]范围，进行训练数据的预处理。由于`torch.tanh`的输出也是[-1,1]，因此在将生成的样本输入到判别器网络或损失函数之前，不需要对其进行重新缩放。
- en: Creating the discriminator
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建判别器
- en: 'The architecture of the discriminator network is shown in the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 判别网络的架构如下所示：
- en: '![](img/01f2476b-d501-4e74-9b1a-f3b6ded184da.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01f2476b-d501-4e74-9b1a-f3b6ded184da.png)'
- en: Discriminator architecture of SRGAN
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN的判别器架构
- en: The discriminator of SRGAN takes a VGG-like structure that gradually decreases
    the sizes of feature maps and expands the depth channel, in the hope that each
    layer contains a similar amount of information. Unlike in the vanilla VGG networks,
    the discriminator uses a pooling layer to transform the last VGG's feature map
    to 1 x 1\. The final output of the discriminator network is a single value, which
    indicates whether the input image is high-resolution or low-resolution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN的判别器采用类似VGG的结构，逐步减少特征图的尺寸，并扩展深度通道，希望每一层包含相似量的信息。与传统VGG网络不同，判别器使用池化层将最后一层VGG的特征图转换为1
    x 1。判别器网络的最终输出是一个单一值，表示输入图像是高分辨率还是低分辨率。
- en: 'Here, we give the definition code of the discriminator network of SRGAN:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们给出了SRGAN判别器网络的定义代码：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Defining training loss
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义训练损失
- en: 'The loss of SRGAN consists of 4 parts. Here, we let ![](img/6cab3c8c-90a2-4b60-899e-3b63ac27047d.png) denote
    the low-resolution (**LR**) image, ![](img/eff90a60-1fc9-4b54-a0ad-661704d6fd6d.png) denote
    the super-resolution (**SR**) image given by the generator, and ![](img/962d1574-8041-4f04-9ddf-87bc2c41a8e8.png) denote
    the real high-resolution (**HR**) image:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN的损失由4部分组成。这里，我们用 ![](img/6cab3c8c-90a2-4b60-899e-3b63ac27047d.png) 表示低分辨率（**LR**）图像， ![](img/eff90a60-1fc9-4b54-a0ad-661704d6fd6d.png) 表示生成器给出的超分辨率（**SR**）图像， ![](img/962d1574-8041-4f04-9ddf-87bc2c41a8e8.png) 表示真实的高分辨率（**HR**）图像：
- en: Adversarial loss ![](img/f14c42ee-3811-45da-9c6d-0191f7138677.png), as similar
    to previous GAN models
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗损失 ![](img/f14c42ee-3811-45da-9c6d-0191f7138677.png)，与之前的GAN模型类似
- en: Pixel-wise content loss ![](img/44fab683-47ff-459e-b8cb-d3041cfbe389.png), which
    is the MSE loss between the SR and HR images
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素级内容损失 ![](img/44fab683-47ff-459e-b8cb-d3041cfbe389.png)，即超分辨率（SR）图像与高分辨率（HR）图像之间的均方误差（MSE）损失
- en: VGG loss ![](img/a052e9a9-9576-4702-a1bd-78d0be41095b.png), which is the MSE
    loss between the last feature maps of a pre-trained VGG network from the SR and
    HR images
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG损失 ![](img/a052e9a9-9576-4702-a1bd-78d0be41095b.png)，即超分辨率（SR）图像和高分辨率（HR）图像最后特征图之间的均方误差（MSE）损失
- en: Regularization loss ![](img/ab9576be-b21c-418e-a2d4-3be8b7662689.png), which
    is the sum of average L2-norm of pixel gradients in horizontal and vertical directions
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化损失 ![](img/ab9576be-b21c-418e-a2d4-3be8b7662689.png)，即像素梯度在水平方向和垂直方向上的平均L2范数之和
- en: 'The final training loss is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最终训练损失如下：
- en: '![](img/ac5a827b-7e94-48fa-a3c3-31d0fe977824.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac5a827b-7e94-48fa-a3c3-31d0fe977824.png)'
- en: It is called **perceptual loss**, which means that it takes both pixel-wise
    similarities and high-level features into consideration when judging the quality
    of the SR images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为**感知损失**，意味着在判断超分辨率图像质量时，它会同时考虑像素级别的相似性和高层特征。
- en: Note that the L2-norm regularization term in the perceptual loss will actually
    make images blurry since it adds strong restraints to the pixel gradients. If
    you feel puzzled by the assertion, imagine a normal distribution in your head,
    in which the x axis represents the pixel gradient and the *y* axis tells us how
    likely a pixel gradient value would appear in the image. In a normal distribution, ![](img/26c2a7f5-c88b-45d3-ab6a-28d043c2f699.png),
    most of the elements are very close to the *y* axis, which means that most of
    the pixels have very small gradients. It indicates that the changes between the
    neighboring pixels are mostly smooth. Therefore, we don't want the regularization
    term to dominate the final loss. In fact, the regularization term is deleted from
    the updated version of the SRGAN paper. You can safely get rid of it as well.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，感知损失中的L2范数正则化项实际上会使图像变得模糊，因为它对像素梯度施加了强烈的约束。如果你对这一断言感到困惑，可以在脑海中想象一个正态分布，其中x轴表示像素梯度，*y*轴告诉我们像素梯度值出现在图像中的可能性。在正态分布中，![](img/26c2a7f5-c88b-45d3-ab6a-28d043c2f699.png)，大部分元素都非常接近*y*轴，这意味着大多数像素的梯度非常小。这表明相邻像素之间的变化通常是平滑的。因此，我们不希望正则化项主导最终的损失。实际上，正则化项已从SRGAN论文的更新版本中删除，你也可以放心地将其去除。
- en: 'Here is the definition code of the perceptual `loss` function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是感知`loss`函数的定义代码：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And the regularization term is calculated as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项计算如下：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we need to modify the existing `train.py` file to support our new functions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要修改现有的`train.py`文件，以支持我们的新功能：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The training script provided by [https://github.com/leftthomas/SRGAN](https://github.com/leftthomas/SRGAN)
    works fine with a few other minor fixes by replacing every `.data[0]` instance
    with `.item()`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由[https://github.com/leftthomas/SRGAN](https://github.com/leftthomas/SRGAN)提供的训练脚本在进行一些小修正后运行良好，修正方法是将每个`.data[0]`实例替换为`.item()`。
- en: Training SRGAN to generate high-resolution images
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练SRGAN以生成高分辨率图像
- en: Of course, we need to have some data to work with. We simply need to download
    the training images from the links in the `README.md` file. You can always use
    any image collection you like since the training of SRGAN only requires low-resolution
    images (which can be easily acquired by resizing to smaller scales) besides the
    original images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们需要一些数据来进行操作。我们只需从`README.md`文件中的链接下载训练图像。你可以使用任何你喜欢的图像集合，因为SRGAN的训练只需要低分辨率图像（这些可以通过调整大小轻松获得）以及原始图像。
- en: Create a folder named `data` and place the training images into a folder called
    `DIV2K_train_HR` and the valid images into `DIV2K_valid_HR`. Next, create a folder
    named `epochs` to hold the epoch data. Finally, create a folder named `training_results`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`data`的文件夹，并将训练图像放入名为`DIV2K_train_HR`的文件夹中，将验证图像放入`DIV2K_valid_HR`文件夹中。接下来，创建一个名为`epochs`的文件夹来保存周期数据。最后，创建一个名为`training_results`的文件夹。
- en: 'To train SRGAN, execute the following command in a Terminal:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练SRGAN，在终端中执行以下命令：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The image collection provided by `leftthomas` is sampled from the VOC2012 dataset
    and contains 16,700 images. With a batch size of 64, it takes about 6.6 hours
    to train for 100 epochs on a GTX 1080Ti graphics card. The GPU memory usage is
    about 6433 MB with a batch size of 88 and 7509 MB when the batch size is 96.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`leftthomas`提供的图像集合从VOC2012数据集采样，共包含16,700张图像。在GTX 1080Ti显卡上，批处理大小为64时，训练100个周期大约需要6.6小时。批处理大小为88时，GPU内存使用量大约为6433MB，批处理大小为96时，内存使用量为7509MB。'
- en: 'However, during the training of SRGAN, the GPU usage lies below 10% most of
    the time (observed via `nvtop`), which indicates that the loading and pre-processing
    of data take up too much time. This issue can be solved by two different solutions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在训练SRGAN期间，GPU使用率大多数时间都低于10%（通过`nvtop`观察到），这表明数据的加载和预处理占用了过多时间。这个问题可以通过两种不同的解决方案来解决：
- en: Putting the dataset on an SSD (preferably, via an NVMe interface)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集放置在SSD上（最好通过NVMe接口）
- en: Using a data prefetcher to preload the data into GPU memory before the next
    iteration begins
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据预取器在下一次迭代开始之前将数据预加载到GPU内存中
- en: 'Here, we will talk about how to carry out the second solution. The code for
    a data prefetcher is borrowed from the ImageNet example of NVIDIA''s [apex](https://github.com/NVIDIA/apex)
    project: [https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py](https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py).
    Follow these steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论如何执行第二种解决方案。数据预取器的代码来自于NVIDIA的[apex](https://github.com/NVIDIA/apex)项目中的ImageNet示例：[https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py](https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py)。请按照以下步骤操作：
- en: 'Define the data prefetcher somewhere in your source tree (for example, the `data_utils.py`
    file in SRGAN):'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在源代码树中的某个位置定义数据预取器（例如，SRGAN中的`data_utils.py`文件）：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use the data `prefetcher` to load samples during training:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据`prefetcher`在训练过程中加载样本：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, the `tqdm` module is for printing the progress bar in the Terminal during
    training and can be treated as its original iterable object. In the training of
    SRGAN, the data `prefetcher` makes a huge difference in GPU efficiency, as shown
    here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`tqdm`模块用于在训练过程中在终端打印进度条，并且可以当作其原始的可迭代对象。在SRGAN的训练中，数据`prefetcher`对GPU效率有着巨大的提升，如下所示：
- en: '![](img/4a89ff61-0dc8-460e-8157-c23db2873be4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a89ff61-0dc8-460e-8157-c23db2873be4.png)'
- en: GPU usage before and after using prefetcher to load images into GPU memory
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预取器加载图像到GPU内存之前和之后的GPU使用情况
- en: The data prefetcher can be adjusted to another form of data, which is also included
    in the source code under the repository for this chapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预取器可以调整为另一种数据形式，这些内容也包含在本章对应仓库的源代码中。
- en: 'Some super-resolution results are shown in the following. We can see that SRGAN
    is doing a good job sharpening the low-resolution images. But we can also notice
    that it has its limits when dealing with sharp edges between large color blobs
    (for example, the rocks in the first image and the trees in the third image):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了一些超分辨率的结果。我们可以看到SRGAN在锐化低分辨率图像方面表现良好。但我们也能注意到，在处理大色块之间的锐利边缘时，它会有一定的局限性（例如，第一张图中的岩石和第三张图中的树木）：
- en: '![](img/6c70cff2-7640-4692-afbf-cae3f13a02b6.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c70cff2-7640-4692-afbf-cae3f13a02b6.png)'
- en: Super-resolution results by SRGAN
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN的超分辨率结果
- en: Generative image inpainting
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成图像修复
- en: We know that GANs, if trained properly, are capable of learning the latent distribution
    of data and using that information to create new samples. This extraordinary ability
    of GANs makes them perfect for applications such as image inpainting, which is
    filling the missing part in images with plausible pixels.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，如果训练得当，GAN能够学习数据的潜在分布，并利用这些信息生成新的样本。GAN的这一非凡能力使其非常适合应用于图像修复等任务，即用合理的像素填补图像中缺失的部分。
- en: In this section, we will learn how to train a GAN model to perform image inpainting,
    based on the work of Jiahui Yu, Zhe Lin, Jimei Yang, et. al. in their paper, *Generative
    Image Inpainting with Contextual Attention*. Although an updated version of their
    project has been published ([http://jiahuiyu.com/deepfill2](http://jiahuiyu.com/deepfill2)),
    the source code is not yet open source at the time of writing. Therefore, we should
    try to implement the model in PyTorch based on the source code of its previous
    version for TensorFlow ([https://github.com/JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何训练一个GAN模型来执行图像修复，基于Jiahui Yu、Zhe Lin、Jimei Yang等人在其论文《*Generative
    Image Inpainting with Contextual Attention*》中的工作。尽管他们的项目已有更新版本发布（[http://jiahuiyu.com/deepfill2](http://jiahuiyu.com/deepfill2)），但在撰写时，源代码尚未开源。因此，我们应该尝试根据其先前版本在TensorFlow上的源代码（[https://github.com/JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting)）在PyTorch中实现该模型。
- en: Before we starting working on addressing image inpainting with GANs, there are
    a few fundamental concepts to understand as they are crucial to comprehend the
    method.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始处理使用GAN进行图像修复之前，有几个基本概念需要理解，因为这些概念对理解该方法至关重要。
- en: Efficient convolution – from im2col to nn.Unfold
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效卷积 – 从im2col到nn.Unfold
- en: If you have previously been curious enough to try implementing convolutional
    neural networks on your own (either with Python or C/C++), you must know the most
    painful part of work is the backpropagation of gradients, and the most time-consuming
    part is the convolutions (assuming that you are implementing a plain CNN such
    as LeNet).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经有足够的好奇心，尝试自己实现卷积神经网络（无论是使用Python还是C/C++），你一定知道最痛苦的部分是梯度反向传播，而最耗时的部分是卷积操作（假设你实现的是像LeNet这样的简单CNN）。
- en: 'There are several ways to perform the convolution in your code (apart from
    directly using deep learning tools such as PyTorch):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中执行卷积的方法有很多种（除了直接使用深度学习工具，如PyTorch）：
- en: Calculate the convolution directly as per definition, which is usually the slowest
    way.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照定义直接计算卷积，这通常是最慢的方法。
- en: Use **Fast Fourier Transform** (**FFT**), which is not ideal for CNNs, since
    the sizes of kernels are often way too small compared to the images.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**快速傅里叶变换**（**FFT**），这对于卷积神经网络（CNN）来说并不理想，因为卷积核的大小通常相对于图像来说太小。
- en: Treat the convolution as matrix multiplication (in other words, **General Matrix
    Multiply** or **GeMM**) using **im2col**. This is the most common method used
    by numerous software and tools and is a lot faster.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将卷积视为矩阵乘法（换句话说，**一般矩阵乘法**或**GeMM**）使用**im2col**。这是许多软件和工具中最常用的方法，速度也快得多。
- en: Use the **Winograd** method, which is faster than GeMM under certain circumstances.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**Winograd**方法，这在某些情况下比GeMM更快。
- en: In this section, we will only talk about the first three methods. If you want
    to learn more about the Winograd method, feel free to check out this project, [https://github.com/andravin/wincnn](https://github.com/andravin/wincnn),
    and this paper, *Fast Algorithms for Convolutional Neural Networks,* by Andrew
    Lavin and Scott Gray. Here, we will give Python code for 2D convolution with different
    methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将仅讨论前三种方法。如果你想了解更多关于Winograd方法的内容，可以查看这个项目，[https://github.com/andravin/wincnn](https://github.com/andravin/wincnn)，以及Andrew
    Lavin和Scott Gray的论文《*卷积神经网络的快速算法*》。在这里，我们将提供使用不同方法进行二维卷积的Python代码。
- en: 'Before proceeding, make sure you have installed the prerequisites by typing
    the following command in the Terminal:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保通过在终端输入以下命令来安装必要的依赖项：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let''s follow these steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照这些步骤进行：
- en: 'Directly calculate the convolution. Note that all of the following convolution
    implementations have a stride size of `1` and a padding size of `0`, which means
    that the output size is ![](img/96be635b-77a1-4776-ac2a-76adbed443f3.png):'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接计算卷积。请注意，以下所有卷积实现的步幅大小为`1`，填充大小为`0`，这意味着输出大小为![](img/96be635b-77a1-4776-ac2a-76adbed443f3.png)：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As we said before, directly calculating the convolution as per definition is
    extremely slow. Here is the elapsed time when convolving a 512 x 512 image with
    a 5 x 5 kernel:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，按照定义直接计算卷积是极其慢的。以下是使用5x5卷积核对512 x 512图像进行卷积时的消耗时间：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We also need to compare its result against a baseline (for example, `scipy.signal.convolve2d`)
    so that we''ll know the computation is correct:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将结果与基准进行比较（例如，`scipy.signal.convolve2d`），以确保计算正确：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we know our calculation is correct, the problem is how to do it faster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们的计算是正确的，问题在于如何更快地执行。
- en: 'Calculate the convolution with FFT:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用FFT计算卷积：
- en: 'According to this formula, we can get the result of convolution by performing
    two Fourier transforms and one inverse Fourier transform:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个公式，我们可以通过执行两次傅里叶变换和一次逆傅里叶变换来获得卷积结果：
- en: '![](img/5bdfc020-1d58-4ed2-be5e-0ace6eb3951f.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bdfc020-1d58-4ed2-be5e-0ace6eb3951f.png)'
- en: 'Since we are dealing with digital images, we need to perform a **Discrete Fourier
    Transform** (**DFT**), which can be calculated extremely fast with a **Fast Fourier
    Transform** (**FFT**) method provided by NumPy:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是数字图像，我们需要执行**离散傅里叶变换**（**DFT**），可以通过NumPy提供的**快速傅里叶变换**（**FFT**）方法极其快速地计算：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here are the elapsed time and calculation error of an FFT-based convolution:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是基于FFT的卷积的消耗时间和计算误差：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see that convolution by FFT is a lot faster than the direct approach
    and costs almost the same amount of time as `scipy.signal.convolve2d`. Can we
    do it even faster?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用FFT进行卷积比直接方法要快得多，且几乎与`scipy.signal.convolve2d`消耗的时间相同。我们能否做得更快？
- en: Calculate the convolution with im2col.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用im2col计算卷积。
- en: Let's take a pause and think about the first 2 methods. The direct approach
    involves 4 `for` loops and a lot of random access to the matrix elements. The
    FFT approach turns convolution into matrix multiplication but it requires 2 FFTs
    and 1 inverse FFT. We know low-level computational tools such as BLAS are very
    good at matrix multiplication. How about we treat the original convolution as
    matrix multiplication?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来思考一下前两种方法。直接方法涉及4个`for`循环和大量随机访问矩阵元素。FFT方法将卷积转化为矩阵乘法，但它需要进行2次FFT和1次逆FFT。我们知道，低级计算工具如BLAS在矩阵乘法方面非常高效。那么，我们能否将原始卷积视为矩阵乘法呢？
- en: 'Take the convolution between a 3 x 3 image and a 2 x 2 kernel, for example
    (with a stride size of 1 and padding size of 0):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个 3 x 3 的图像和 2 x 2 的卷积核为例（步长为 1，填充为 0）：
- en: '![](img/92a2131f-b25c-4907-95e2-64ef1c12efbb.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92a2131f-b25c-4907-95e2-64ef1c12efbb.png)'
- en: Convolution between image and 2 x 2 kernel
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图像与 2 x 2 卷积核之间的卷积
- en: 'We can stretch the input image into a very long vector (1 x 9), and transform
    the convolution kernel into a very big matrix (9 x 4) so that our output will
    have the size of 1 x 4 as expected. Of course, we also need to arrange the elements
    in the big matrix according to the computational process within the convolution
    (for example, ![](img/3376f0ae-927c-4b6e-8b39-895aeb02cffc.png)), as shown here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将输入图像拉伸成一个非常长的向量（1 x 9），并将卷积核转换成一个非常大的矩阵（9 x 4），这样我们的输出将具有预期的 1 x 4 大小。当然，我们还需要根据卷积中的计算过程安排大矩阵中的元素（例如，![](img/3376f0ae-927c-4b6e-8b39-895aeb02cffc.png)），如下所示：
- en: '![](img/0bb7902b-7f1c-4709-9e81-7f68195cb15f.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bb7902b-7f1c-4709-9e81-7f68195cb15f.png)'
- en: Convolution via sparse matrix multiplication
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过稀疏矩阵乘法进行卷积
- en: This way, we need to calculate the matrix multiplication between a very long
    vector and a large sparse matrix (in which many elements are zeros). Direct multiplication
    can be very inefficient (both in terms of time and memory). Even though we can
    speed up sparse matrix multiplication with some numerical algorithms, we won't
    go into the details of this approach as there is a more efficient way to turn
    the convolution into matrix multiplication.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们需要计算一个非常长的向量和一个大型稀疏矩阵之间的矩阵乘法（其中很多元素为零）。直接进行矩阵乘法可能非常低效（无论是从时间还是内存的角度）。虽然我们可以通过一些数值算法加速稀疏矩阵乘法，但我们不会深入探讨这种方法，因为有一种更高效的方式将卷积转换为矩阵乘法。
- en: Comparing sparse matrix multiplication to a fully-connected layer (`nn.Linear`)
    with the same input and output dimensions (also with the same size weight matrix),
    we can see that the convolution requires much fewer parameters than a fully connected
    layer (because there are many zeros in the weight matrix and the elements are
    mostly reusable). This makes CNNs easier to train and more robust to overfitting
    than MLP, which is also one of the reasons why CNNs have become more popular in
    recent years.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将稀疏矩阵乘法与具有相同输入和输出维度（且权重矩阵大小相同）的全连接层（`nn.Linear`）进行比较，我们可以看到，卷积所需的参数远少于全连接层（因为权重矩阵中有许多零，并且很多元素是可重用的）。这使得
    CNN 比 MLP 更容易训练且对过拟合更具鲁棒性，这也是近年来 CNN 更受欢迎的原因之一。
- en: 'Considering the size of the kernel is often much smaller than the image, we
    will try stretching the kernel into a vector and rearranging the elements from
    the input image to match the kernel vector''s dimensions, as shown here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到卷积核的大小通常远小于图像，我们将尝试将卷积核拉伸成一个向量，并重新排列输入图像中的元素以匹配卷积核向量的维度，如下所示：
- en: '![](img/5bcb163f-2a30-48ae-af02-f158021d4d80.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bcb163f-2a30-48ae-af02-f158021d4d80.png)'
- en: Convolution via im2col
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 im2col 进行卷积
- en: 'Now, we can see that we only need to perform dense matrix multiplication with
    much smaller dimensions. The transformation we perform on the input image is called
    **im2col**. The result of im2col is easy to comprehend: the elements in one row
    represent the elements of the input image needed to perform a convolution at a
    given location (this is known as the **sliding window**) and the ![](img/5188407c-7ab5-49b5-800c-803653717f3e.png)^(th)
    row corresponds to the ![](img/e453fb88-6f44-4e79-81cc-a039ee1ae3c3.png)^(th)
    output element (![](img/c51ff68a-833c-4f38-8a94-d4df04ed42f1.png)).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到，我们只需要进行具有较小维度的密集矩阵乘法。我们对输入图像进行的转换叫做**im2col**。im2col 的结果很容易理解：一行中的元素表示在给定位置执行卷积所需的输入图像元素（这被称为**滑动窗口**），而第
    ![](img/5188407c-7ab5-49b5-800c-803653717f3e.png)^(th) 行对应于第 ![](img/e453fb88-6f44-4e79-81cc-a039ee1ae3c3.png)^(th)
    输出元素（![](img/c51ff68a-833c-4f38-8a94-d4df04ed42f1.png)）。
- en: 'Here is the Python implementation of `im2col`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `im2col` 的 Python 实现：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here are the elapsed time and the calculation error:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是经过的时间和计算误差：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Treating the convolution as matrix multiplication gains the fastest computational
    speed among all three methods. It achieves a more than 260x speedup in calculation
    time compared to the direct approach. Another advantage of im2col is that it is
    completely compatible with CNNs. In CNNs, the convolutions are often performed
    in channels, which means that we need to calculate the sum of a group of individual
    convolutions. For example, say our input feature map has the size of ![](img/e3cc76c2-39b1-42ba-8862-9e8226738e85.png) and
    the weight tensor is ![](img/11319710-4744-479e-ba7f-6e454293fb93.png). For each
    neuron in the ![](img/9b613ad3-62bf-4e90-a1e7-e24db54f10a6.png) channels, it is
    the sum of ![](img/9ef11c64-d2f1-4d0d-be06-2f933d34d3c9.png) times the convolution
    operations between the image ![](img/5d62c5f4-0362-4056-a614-e1d205fa4e2f.png) and
    kernel ![](img/ce4965b8-e86a-438a-bccb-46d81a8283f8.png). With im2col, the convolution
    result of a sliding window at a given location is represented by the multiplication
    of two vectors (because the convolution itself is the summation of element-wise
    multiplication). We can apply this pattern by filling all elements inside the
    same sliding window from all ![](img/107a2bc9-3f69-4c16-bf4e-c39ce6c8d217.png) channels
    into one long vector so that the output pixel value in one of the ![](img/5b943155-3a29-48f3-be2b-f46718b744e8.png) channels
    can be obtained via a single vector multiplication. If you wish to learn more
    about how channel-wise convolution can be performed in Python, check out this
    Stack Overflow post: [https://stackoverflow.com/q/30109068/3829845](https://stackoverflow.com/q/30109068/3829845).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将卷积视为矩阵乘法在所有三种方法中获得了最快的计算速度。与直接方法相比，它在计算时间上实现了超过 260 倍的加速。im2col 的另一个优势是它与 CNN
    完全兼容。在 CNN 中，卷积通常是在通道内执行的，这意味着我们需要计算一组独立卷积的和。例如，假设我们的输入特征图大小为 ![](img/e3cc76c2-39b1-42ba-8862-9e8226738e85.png)，权重张量是
    ![](img/11319710-4744-479e-ba7f-6e454293fb93.png)。对于每个 ![](img/9b613ad3-62bf-4e90-a1e7-e24db54f10a6.png)
    通道中的神经元，它是 ![](img/9ef11c64-d2f1-4d0d-be06-2f933d34d3c9.png) 乘以图像 ![](img/5d62c5f4-0362-4056-a614-e1d205fa4e2f.png)
    和卷积核 ![](img/ce4965b8-e86a-438a-bccb-46d81a8283f8.png) 之间的卷积操作的和。使用 im2col，给定位置的滑动窗口的卷积结果由两个向量的乘法表示（因为卷积本身是元素逐项相乘的和）。我们可以通过将所有
    ![](img/107a2bc9-3f69-4c16-bf4e-c39ce6c8d217.png) 通道中的滑动窗口的所有元素填充到一个长向量中，从而应用这种模式，使得可以通过一次向量乘法获得一个
    ![](img/5b943155-3a29-48f3-be2b-f46718b744e8.png) 通道中的输出像素值。如果你想了解更多关于如何在 Python
    中执行通道-wise 卷积的信息，可以查看这个 Stack Overflow 帖子：[https://stackoverflow.com/q/30109068/3829845](https://stackoverflow.com/q/30109068/3829845)。
- en: 'Turning 4D tensor convolution into 3D tensor multiplication is where `nn.Unfold`
    comes in handy. Here is a code snippet showing how to explicitly turning convolution
    into matrix multiplication with PyTorch (based on the official document at [https://pytorch.org/docs/stable/nn.html?highlight=unfold#torch.nn.Unfold](https://pytorch.org/docs/stable/nn.html?highlight=unfold#torch.nn.Unfold)):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将 4D 张量卷积转化为 3D 张量乘法是 `nn.Unfold` 派上用场的地方。以下是一个代码片段，展示了如何使用 PyTorch 显式地将卷积转换为矩阵乘法（基于官方文档：[https://pytorch.org/docs/stable/nn.html?highlight=unfold#torch.nn.Unfold](https://pytorch.org/docs/stable/nn.html?highlight=unfold#torch.nn.Unfold)）：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output messages are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出消息如下：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It is delightful to see that our Python im2col implementation is even faster
    than PyTorch. We hope this will encourage you to build your own deep learning
    toolbox!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴看到我们的 Python im2col 实现甚至比 PyTorch 更快。我们希望这能鼓励你构建自己的深度学习工具箱！
- en: WGAN – understanding the Wasserstein distance
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WGAN – 理解 Wasserstein 距离
- en: GANs have been known to be hard to train, especially if you have tried to build
    one from scratch. (Of course, we hope that, after reading this book, training
    GANs can be a much easier job for you!) Over the past chapters, we have learned
    several different model design and training techniques that come from many excellent
    researchers' experience. In this section, we will talk about how to use a better
    distance measure to improve the training of GANs, namely, the Wasserstein GAN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 一直以来被认为难以训练，特别是如果你曾尝试从零开始构建一个 GAN。（当然，我们希望，在阅读本书后，训练 GAN 对你来说会变得更容易！）在过去的章节中，我们已经学习了许多不同的模型设计和训练技巧，这些技巧源自许多优秀研究者的经验。在这一部分，我们将讨论如何使用更好的距离度量来改进
    GAN 的训练，即 Wasserstein GAN。
- en: The **Wasserstein GAN** (**WGAN**) was proposed by Martin Arjovsky, Soumith
    Chintala, and Léon Bottou in their paper, *Wasserstein GAN*. Martin Arjovsky and
    Léon Bottou also laid the groundwork in an earlier paper, *Towards Principled
    Methods for Training Generative Adversarial Networks*. To fully comprehend these
    papers, you are expected to have fundamental mathematical knowledge in probability
    theory, measure theory, and functional analysis. We will try our best to keep
    the mathematical formulae to a minimum and help you to understand the concept
    of WGAN.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**Wasserstein GAN**（**WGAN**）由Martin Arjovsky、Soumith Chintala和Léon Bottou在他们的论文《*Wasserstein
    GAN*》中提出。Martin Arjovsky和Léon Bottou也在早期的论文《*Towards Principled Methods for Training
    Generative Adversarial Networks*》中奠定了基础。为了充分理解这些论文，你需要具备概率论、测度论和泛函分析的基础数学知识。我们将尽力简化数学公式，帮助你理解WGAN的概念。'
- en: Analyzing the problems with vanilla GAN loss
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析原始GAN损失函数的问题
- en: 'Let''s go over the commonly used loss functions for GANs (which have already
    appeared in previous chapters):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下GAN中常用的损失函数（这些已经出现在前几章中）：
- en: '![](img/52f2ad13-22c3-4712-a70a-da8eb1f517a3.png), which is the vanilla form
    of GAN loss'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/52f2ad13-22c3-4712-a70a-da8eb1f517a3.png)，这是GAN损失的原始形式'
- en: '![](img/22b30b86-c6a2-4074-a518-dcd903743882.png)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/22b30b86-c6a2-4074-a518-dcd903743882.png)'
- en: '![](img/cf42caa2-8878-411f-b1c6-4bb9d0df426a.png)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/cf42caa2-8878-411f-b1c6-4bb9d0df426a.png)'
- en: 'The experimental results in previous chapters have already shown that these
    loss functions work well in several applications. However, let''s dig deep into
    these functions and see what could go wrong when they don''t work so well:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章的实验结果已经表明这些损失函数在多个应用中表现良好。然而，让我们深入研究这些函数，看看当它们效果不好时可能出什么问题：
- en: '**Step 1: **Problems with the first loss function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步：** 第一个损失函数的问题：'
- en: 'Assume that the generator network is trained and we need to find an optimal
    discriminator network D. We have the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设生成器网络已经训练完成，我们需要找到一个最优的判别器网络D。我们有以下公式：
- en: '![](img/bf59d642-7d90-4bd3-8154-63bc4616c575.png).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/bf59d642-7d90-4bd3-8154-63bc4616c575.png)。'
- en: In this formula, ![](img/fc3813e8-466e-4df5-910a-9f447a29a343.png) represents
    the distribution of real data and ![](img/f1e2f683-c676-4904-b6ef-48dcb7f914d7.png) represents
    the distribution of fake (generated) data. ![](img/16424b9a-9c6a-4b19-b478-91904b8a1d54.png) is
    the real data when calculating ![](img/904c8fdb-9e94-4e7b-b4e2-bba4d7282bba.png) and
    the fake data when calculating ![](img/a200b222-ed26-43e5-8245-1ea9d09e835e.png).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/fc3813e8-466e-4df5-910a-9f447a29a343.png) 代表真实数据的分布，![](img/f1e2f683-c676-4904-b6ef-48dcb7f914d7.png)
    代表伪造（生成）数据的分布。![](img/16424b9a-9c6a-4b19-b478-91904b8a1d54.png) 是计算 ![](img/904c8fdb-9e94-4e7b-b4e2-bba4d7282bba.png)
    时的真实数据，计算 ![](img/a200b222-ed26-43e5-8245-1ea9d09e835e.png) 时的伪造数据。
- en: We admit that the notation of ![](img/3f39680e-19c7-47a1-bb81-706542f02142.png) here
    is a little bit confusing. However, if we consider that all kinds of data exists
    in the same data space (for example, all possible 256 x 256 images with three
    8-bit channels), and some part of the space belongs to the real data while some
    part belonging to the generated data. The training of GANs is essentially making
    the *fake* part overlap with the *real* part, hopefully, to become the same as
    the *real* part.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承认这里的![](img/3f39680e-19c7-47a1-bb81-706542f02142.png) 表示法有点混乱。然而，如果我们考虑所有种类的数据存在于同一个数据空间中（例如，所有可能的256
    x 256的三通道8位图像），并且其中一部分空间属于真实数据，另一部分属于生成数据。GAN的训练本质上是让*伪造*部分与*真实*部分重叠，希望最终能与*真实*部分相同。
- en: 'To find the minimum of the formula, we let its derivatives regarding *D* to
    be zero and get the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到公式的最小值，我们令关于*D*的导数为零，得到以下结果：
- en: '![](img/528f1a1a-352f-4f82-aef0-3840bc58007b.png).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/528f1a1a-352f-4f82-aef0-3840bc58007b.png)。'
- en: 'Therefore, the first loss function becomes (when D is optimal) as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当D最优时，第一个损失函数变为如下形式：
- en: '![](img/e4d427ab-a2b3-4b3d-a8f8-d96d64432e2e.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4d427ab-a2b3-4b3d-a8f8-d96d64432e2e.png)'
- en: 'Here, ![](img/30baa216-3b31-460a-b017-87314a55ca54.png) is the** Jensen–Shannon
    divergence** (**JS divergence**), which is the symmetric version of the **Kullback–Leibler
    divergence** (**KL divergence**):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/30baa216-3b31-460a-b017-87314a55ca54.png) 是**Jensen–Shannon散度**（**JS散度**），它是**Kullback–Leibler散度**（**KL散度**）的对称版本：
- en: '![](img/cf591939-e5e5-4731-a80d-7f0520c68671.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf591939-e5e5-4731-a80d-7f0520c68671.png)'
- en: The Kullback–Leibler divergence is usually used to describe the distance between
    two distributions. It equals the **cross entropy** of ![](img/5eadcab3-c338-471a-9abb-04e9f716d208.png) and
    ![](img/3fed48e0-f327-4a28-baae-fb1edef2468b.png) minus the **entropy** of ![](img/7a5901b0-0b49-4116-88c0-2babbb749b24.png),
    which is why KL divergence is also called **relative entropy**. Keep in mind that
    KL divergence is not symmetric, because ![](img/3f81b8f7-6978-45ff-9842-54859dc1c28c.png) and
    ![](img/21024c0c-5704-4fc9-8a66-b522d620a1dd.png) makes ![](img/5fc6767f-3065-4ba7-9c7e-64e9e6b5c8c4.png) but
    ![](img/d2acf37c-455a-41e7-bfe6-9c838059e4dd.png) and ![](img/cac29847-b9df-423d-8625-3c1dde691a94.png) makes
    ![](img/601f879d-e4eb-4179-ba9d-0b60517a6edc.png). Therefore, KL divergence is
    strictly not a distance metric. However, the Jensen–Shannon divergence is symmetric
    and can be used as a distance metric.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度通常用于描述两个分布之间的距离。它等于 **交叉熵** ![](img/5eadcab3-c338-471a-9abb-04e9f716d208.png) 与
    ![](img/3fed48e0-f327-4a28-baae-fb1edef2468b.png) 之差减去 ![](img/7a5901b0-0b49-4116-88c0-2babbb749b24.png)
    的 **熵**，这也是为什么 KL 散度有时被称为 **相对熵**。请记住，KL 散度是非对称的，因为 ![](img/3f81b8f7-6978-45ff-9842-54859dc1c28c.png) 与
    ![](img/21024c0c-5704-4fc9-8a66-b522d620a1dd.png) 会得到 ![](img/5fc6767f-3065-4ba7-9c7e-64e9e6b5c8c4.png)，但
    ![](img/d2acf37c-455a-41e7-bfe6-9c838059e4dd.png) 与 ![](img/cac29847-b9df-423d-8625-3c1dde691a94.png) 会得到 ![](img/601f879d-e4eb-4179-ba9d-0b60517a6edc.png)。因此，KL
    散度严格来说不是一个距离度量。然而，Jensen-Shannon 散度是对称的，可以作为一个距离度量使用。
- en: If you have used TensorBoard to visualize the embedding space learned by a neural
    network, you may have found a useful technique called **t-SNE** that can wonderfully
    illustrate high-dimensional data in a 2- or 3-dimensional graph (in a much clearer
    way than PCA). In t-SNE, a revised version of KL divergence is used to map the
    high-dimension data to low-dimension. You may check out this blog to learn more
    about t-SNE: [https://distill.pub/2016/misread-tsne](https://distill.pub/2016/misread-tsne).
    Also, this Google Techtalk video can be very helpful to understand KL divergence
    and t-SNE: [https://www.youtube.com/watch?v=RJVL80Gg3lA](https://www.youtube.com/watch?v=RJVL80Gg3lA).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用过 TensorBoard 来可视化神经网络学习到的嵌入空间，你可能会发现一种叫做 **t-SNE** 的有用技术，它可以非常清晰地在二维或三维图中展示高维数据（比
    PCA 更清晰）。在 t-SNE 中，KL 散度的修正版被用来将高维数据映射到低维。你可以查看这个博客来了解更多关于 t-SNE 的信息：[https://distill.pub/2016/misread-tsne](https://distill.pub/2016/misread-tsne)。此外，这个
    Google Techtalk 视频对于理解 KL 散度和 t-SNE 非常有帮助：[https://www.youtube.com/watch?v=RJVL80Gg3lA](https://www.youtube.com/watch?v=RJVL80Gg3lA)。
- en: A problem with JS divergence is that when ![](img/d0c11a95-82c0-4d34-be8a-3197a0b3b3a4.png) and ![](img/5de18367-97c1-4207-893a-0e45f84da8a9.png) are
    apart from each other (with no or little overlapping part), its value remains ![](img/b23d43ea-ff17-436a-8ff1-9fc0c23c5024.png) no
    matter how far away ![](img/98127593-c0b8-4550-89e4-219ea61bfcc7.png) and ![](img/81bc279b-7b52-430c-80a6-78186a911420.png) are
    from each other. It's rather reasonable to assume that ![](img/0378a6d1-cbb9-48e8-9196-2b1d39473fff.png) and ![](img/8382d783-08c9-4306-b399-22d02229f4f2.png) are
    no way near each other at the beginning of training (since the generator is randomly
    initialized and ![](img/98ddb0b2-65d2-44a8-b3ed-e058fa7d237a.png) could be anywhere
    in the data space). A nearly constant loss is hardly giving useful information
    to the derivatives when the discriminator is optimal. Therefore, when using the
    first form of loss in GANs, a well-trained discriminator will stop the generator
    from improving itself (**gradient vanishing**).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度的一个问题是，当 ![](img/d0c11a95-82c0-4d34-be8a-3197a0b3b3a4.png) 与 ![](img/5de18367-97c1-4207-893a-0e45f84da8a9.png) 相距很远时（几乎没有或只有很少的重叠部分），其值保持为 ![](img/b23d43ea-ff17-436a-8ff1-9fc0c23c5024.png)，无论 ![](img/98127593-c0b8-4550-89e4-219ea61bfcc7.png) 与 ![](img/81bc279b-7b52-430c-80a6-78186a911420.png) 之间相距多远。在训练的开始阶段，假设 ![](img/0378a6d1-cbb9-48e8-9196-2b1d39473fff.png) 与 ![](img/8382d783-08c9-4306-b399-22d02229f4f2.png) 之间的距离极大是合理的（因为生成器是随机初始化的，且 ![](img/98ddb0b2-65d2-44a8-b3ed-e058fa7d237a.png) 可能在数据空间的任何位置）。当判别器最优时，几乎常数的损失值对导数提供的信息非常有限。因此，在
    GAN 中使用第一种形式的损失时，一个训练良好的判别器将会阻止生成器自我改善（**梯度消失**）。
- en: The gradient vanishing problem in GANs can sometimes be solved by adding annealing
    noises to the inputs of the discriminator during training. But we will talk about
    a more principled method later.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 中的梯度消失问题有时可以通过在训练过程中向判别器的输入添加退火噪声来解决。但我们稍后会讨论一种更为有原则的方法。
- en: '**Step 2:** The problems with the other two loss functions:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：** 其他两个损失函数的问题：'
- en: 'Let''s take the third loss for example. It can be written as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以第三种损失为例，它可以写成如下形式：
- en: '![](img/63d3f953-fbc9-4a72-8a7c-63b4842b8ed8.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63d3f953-fbc9-4a72-8a7c-63b4842b8ed8.png)'
- en: In this formula, the last two terms are irrelevant to the generator. The first
    two terms are, however, aiming for totally opposite objectives (minimizing the
    KL divergence while maximizing the JS divergence). This causes the training to
    be very unstable. On the other hand, the employment of KL divergence can lead
    to **mode collapse**. Failing to generate realistic samples is severely penalized
    (![](img/36851d77-b8bf-400a-846b-3a8e2688945e.png) when ![](img/89ed9c93-32c3-45bf-8812-6a64a2f9ad7d.png) and
    ![](img/012144d4-5761-4862-875b-cb1bf1e3e47d.png)) but generating only a few kinds
    of realistic samples is not penalized (![](img/45c42b2b-3469-4c54-9268-1f09bfe047d7.png) when
    ![](img/31903b68-5617-4344-b713-4ffb0b16e60f.png) and ![](img/9ce2fd9b-df3c-499b-8f6c-139c0c71b15a.png)).
    This makes the generator more prone to generate samples with less variety.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，最后两个项与生成器无关。然而，前两个项的目标完全相反（最小化KL散度的同时最大化JS散度）。这导致训练非常不稳定。另一方面，使用KL散度可能会导致**模式崩溃**。未能生成真实的样本会受到严重惩罚（![](img/36851d77-b8bf-400a-846b-3a8e2688945e.png)当![](img/89ed9c93-32c3-45bf-8812-6a64a2f9ad7d.png)和![](img/012144d4-5761-4862-875b-cb1bf1e3e47d.png)）但仅生成少数几种真实样本则不会受到惩罚（![](img/45c42b2b-3469-4c54-9268-1f09bfe047d7.png)当![](img/31903b68-5617-4344-b713-4ffb0b16e60f.png)和![](img/9ce2fd9b-df3c-499b-8f6c-139c0c71b15a.png)）。这使得生成器更容易生成样本的多样性较少。
- en: The advantages of Wasserstein distance
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wasserstein 距离的优点
- en: 'Wasserstein distance (also called **Earth Mover''s Distance** or **EMD**) is
    defined as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein 距离（也叫**地球搬运距离**或**EMD**）定义如下：
- en: '![](img/3cab7073-2d57-4c28-a044-e06f1d2bd345.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cab7073-2d57-4c28-a044-e06f1d2bd345.png)'
- en: 'Don''t worry about the preceding equation if you find it hard to understand.
    It essentially describes the least distance between two variables sampled from
    all possible joint distributions. In plain words, it is the minimum cost of moving
    one pile of dirt (in a shape of certain distribution) to form a different pile
    (another distribution), as shown in the following screenshot:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现前面的公式难以理解，不用担心。它本质上描述了从所有可能的联合分布中采样的两个变量之间的最小距离。用通俗的话说，它是将一堆土（以某种分布的形式）移动到形成另一堆土（另一种分布）所需的最小成本，如下图所示：
- en: '![](img/04dbf900-a69e-42d4-98a9-808d2c8f1d01.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04dbf900-a69e-42d4-98a9-808d2c8f1d01.png)'
- en: 'The Wasserstein distance: optimal transportation between two piles (image retrieved
    from https://vincentherrmann.github.io/blog/wasserstein)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein 距离：两个堆之间的最优运输（图片来自 [https://vincentherrmann.github.io/blog/wasserstein](https://vincentherrmann.github.io/blog/wasserstein)）
- en: Compared to JS divergence, the Wasserstein distance can properly describe the
    distance between real data and fake data even when they are far apart from each
    other. Therefore, the derivatives can be correctly calculated to update the generator
    network when the discriminator is good.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与JS散度相比，Wasserstein 距离能够正确描述真实数据与假数据之间的距离，即使它们相隔较远。因此，当判别器表现良好时，可以正确计算导数来更新生成器网络。
- en: 'To find the most suitable function, *f*, we can simply train a neural network
    to estimate it (luckily, we are already training a discriminator network). An
    important condition for the second line of the equation to hold is that all functions, *f*, are **Lipschitz
    continuous**:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最合适的函数，*f*，我们可以简单地训练一个神经网络来估计它（幸运的是，我们已经在训练一个判别网络）。为了确保方程的第二行成立，一个重要的条件是所有函数
    *f* 都是**Lipschitz 连续**的：
- en: '![](img/f80f94be-bdbf-4083-90d7-e232f219ffd8.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f80f94be-bdbf-4083-90d7-e232f219ffd8.png)'
- en: Lipschitz continuity is easy to achieve in neural networks by clipping any gradient
    value that's larger than *K* to be *K* (**gradient clipping**), or simply clipping
    the weight values to a constant value (**weight clipping**).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，Lipschitz 连续性很容易实现，通过将任何大于*K*的梯度值裁剪为*K*（**梯度裁剪**），或者简单地将权重值裁剪为常数值（**权重裁剪**）。
- en: Remember the simple GAN written in Python in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml),
    *Generative Adversarial Networks Fundamentals*? We applied both gradient clipping
    and weight clipping to ensure stable training. If anyone asks why are you clipping
    (clamping) the tensors in your GANs, you can give a better answer than *gradient
    explosion* now.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在[第1章](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml)中编写的简单GAN吗？我们应用了梯度裁剪和权重裁剪来确保训练的稳定性。如果有人问你为什么要裁剪（钳制）GAN中的张量，你现在可以给出比*梯度爆炸*更好的答案。
- en: 'Finally, the Wasserstein loss is written as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Wasserstein 损失写作如下：
- en: '![](img/dd4107f0-5c0d-427a-b250-4f83ab6b8bf0.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd4107f0-5c0d-427a-b250-4f83ab6b8bf0.png)'
- en: 'However, there are also some issues with gradient clipping when training a
    very deep neural network. First, if the gradients/weights are clamped to [-c,
    c] too often, they tend to stick with -c or c by the end of training while only
    a few parameters have values between the two ends. Second, clamping the gradients
    to a larger or smaller range could cause "invisible" gradient vanishing or explosion.
    We call it "invisible" because even though the gradient values are extremely large,
    they are eventually clamped to [-c, c]. But it will be a complete waste of computational
    resources. Therefore, Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, et. al.
    proposed to add a penalty term to the discriminator loss, namely, **gradient penalty**,
    in their paper, *Improved Training of Wasserstein GANs*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在训练非常深的神经网络时，也会出现一些梯度剪切的问题。首先，如果梯度/权重被频繁限制在[-c, c]之间，它们会在训练结束时倾向于固定在-c或c，而只有少数参数的值在这两个端点之间。其次，限制梯度到更大或更小的范围可能会导致“看不见”的梯度消失或爆炸。我们称之为“看不见”，因为即使梯度值非常大，它们最终仍会被限制在[-c,
    c]之间。但这将是完全浪费计算资源。因此，Ishaan Gulrajani、Faruk Ahmed、Martin Arjovsky等人在他们的论文*Improved
    Training of Wasserstein GANs*中提出，向判别器损失中添加惩罚项——**梯度惩罚**。
- en: '![](img/5d40bb64-e05e-4b6c-b766-fef866757e64.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d40bb64-e05e-4b6c-b766-fef866757e64.png)'
- en: The penalty gradient is calculated with regards to a random interpolation between
    a pair of real data and fake data.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚梯度是根据真实数据和假数据之间的一对随机插值来计算的。
- en: 'In a nutshell, to use Wasserstein loss, you''ll need to do the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，要使用Wasserstein损失，你需要做以下几步：
- en: Get rid of the `Sigmoid` function at the last layer of the discriminator network.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去掉判别器网络最后一层的`Sigmoid`函数。
- en: Don't apply the `log` function to the results when calculating the loss.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失时不要对结果应用`log`函数。
- en: Use the gradient penalty (or simply clip the weights in shallow neural networks).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度惩罚（或者直接剪切浅层神经网络中的权重）。
- en: Use RMSprop instead of Momentum or Adam to train your networks.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RMSprop代替Momentum或Adam来训练你的网络。
- en: Training GAN for image inpainting
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练图像修复的GAN
- en: Now, it's finally time to train a new GAN model for image inpainting. You can
    get the code for the original PyTorch implementation that comes from [https://github.com/DAA233/generative-inpainting-pytorch](https://github.com/DAA233/generative-inpainting-pytorch).
    This will be a challenge for you to modify the original code to implement your
    own. Since you already have the `CelebA` dataset, use it as a training dataset
    for the experiment in this section.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，终于到了训练一个新的图像修复GAN模型的时刻。你可以从[https://github.com/DAA233/generative-inpainting-pytorch](https://github.com/DAA233/generative-inpainting-pytorch)获得原始PyTorch实现的代码。修改原始代码以实现你自己的模型将是一项挑战。由于你已经有了`CelebA`数据集，可以将其作为本节实验的训练数据集。
- en: Model design for image inpainting
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像修复的模型设计
- en: 'The GAN model for image inpainting consists of two generator networks (a coarse
    generator and a refinement generator) and two discriminator networks (a local
    discriminator and a global discriminator), as shown here:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复的GAN模型由两个生成器网络（粗生成器和精细生成器）和两个判别器网络（局部判别器和全局判别器）组成，如下所示：
- en: '![](img/d99696e9-4708-4efc-a8e5-705eaa9e1db1.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d99696e9-4708-4efc-a8e5-705eaa9e1db1.png)'
- en: 'GAN model for image inpainting: Image x represents the input image; x[1] and x[2]
    represent generated images by coarse and refinement generators, respectively; x[r ]represents
    the original complete image; and m represents the mask for missing part in the
    image.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复的GAN模型：图像x表示输入图像；x[1]和x[2]分别表示粗生成器和精细生成器生成的图像；x[r]表示原始完整图像；m表示图像中缺失部分的掩膜。
- en: The generator model uses two-stage coarse-to-fine architecture. The coarse generator
    is a 17-layer encoder-decoder CNN and dilated convolutions are used in the middle
    to expand the receptive fields. Assume that the size of the input image (*x*)
    is 3 x2 56 x 256, then the output (*x[1]*) of the coarse generator is also 3 x
    256 x 256.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器模型采用了两阶段的粗到细架构。粗生成器是一个17层的编码器-解码器CNN，在中间使用扩张卷积来扩展感受野。假设输入图像(*x*)的大小为3 x 256
    x 256，那么粗生成器的输出(*x[1]*)也是3 x 256 x 256。
- en: The refinement generator has two branches. One is a 10-layer CNN and the other
    is called a **Contextual Attention** branch, which is responsible for finding
    proper reference location in another part of the image to generate the correct
    pixels for filling the hole. The initial input image, *x*; the coarse output, *x[1]*;
    and the binary mask that marks which pixels are missing in *x* are fed into the
    refinement generator together and mapped to a [128, 64, 64] tensor (through 6
    convolution layers) before entering the Contextual Attention branch.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化生成器有两个分支。一个是10层CNN，另一个被称为**上下文注意力**分支，负责在图像的另一部分找到适当的参考位置，以生成正确的像素来填充孔洞。初始输入图像*x*、粗略输出*x[1]*以及标记*x*中缺失像素的二进制掩模一起输入到精细化生成器，并通过6个卷积层映射到[128,64,64]的张量，然后进入上下文注意力分支。
- en: 'The calculation process within the Contextual Attention branch is shown here:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了上下文注意力分支中的计算过程：
- en: '![](img/eacf6e76-16cc-41f1-9f44-a7a0a45beed3.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eacf6e76-16cc-41f1-9f44-a7a0a45beed3.png)'
- en: 'The calculation of Contextual Attention: Image b is the background, f is the
    foreground, and m is the mask.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文注意力的计算：图像b为背景，f为前景，m为掩模。
- en: We are not going into details about Contextual Attention due to the limited
    length of content. The important steps are as illustrated in previous diagram.
    Since we need to find the most relevant parts between the foreground (the pixels
    to be filled) and the background (the remaining pixels outside the masked hole),
    a pixel-wise similarity between every pair of image patches from the foreground
    and background images is calculated. Calculating all possible pairs one-by-one
    is apparently inefficient. Therefore, `nn.Unfold` is used to create a sliding-window
    (with a window size of 3 x 3) versions of the foreground and background images
    (*x[i]* and *w[i]*). To reduce the GPU memory costs, the images are resized to
    [128,32,32]. Therefore, there are *32*32=1,024* sliding windows in both images,
    and the convolution between *x**[i]* and *w**[i]* will tell us the pixel similarity
    in each pair of image patches. The location pair with the highest similarity indicates
    where the attention is focused when reconstructing the foreground patch.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内容长度的限制，我们不会深入讨论上下文注意力。重要步骤如前面图示所示。由于我们需要找到前景（待填充的像素）和背景（掩模孔外的剩余像素）之间最相关的部分，因此计算前景图像和背景图像中每一对图像块之间的逐像素相似度。逐一计算所有可能的对比显然效率低下。因此，使用`nn.Unfold`来创建前景和背景图像的滑动窗口版本（窗口大小为3x3）（*x[i]*和*w[i]*）。为了减少GPU内存消耗，图像被调整为[128,32,32]的大小。因此，两幅图像中有*32*32=1,024*个滑动窗口，*x**[i]*和*w**[i]*之间的卷积将告诉我们每对图像块中的像素相似度。具有最高相似度的位置信息对表明在重建前景块时注意力集中在哪个位置。
- en: 'To ensure the robustness against slight shifts of attention, the attention
    value of each pixel is averaged along horizontal and vertical axes, which is why
    *y[i]* is convolved with identity matrices twice. The attention scores are calculated
    via a scaled softmax function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保对轻微注意力偏移的鲁棒性，每个像素的注意力值沿水平和垂直轴进行了平均，这也是为什么*y[i]*与单位矩阵进行两次卷积的原因。注意力得分通过缩放的softmax函数计算：
- en: '![](img/f15d378f-e45d-46cb-b243-2ff434be0eb7.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f15d378f-e45d-46cb-b243-2ff434be0eb7.png)'
- en: Finally, a transposed convolution is performed on *y[i]* with the unfold form
    of the original background as kernel to reconstruct the missing pixels.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用原始背景的展开形式作为卷积核，对*y[i]*执行转置卷积，以重建缺失的像素。
- en: Both of the outputs of the CNN branch and CA branch have a size of [128,64,64],
    which are concatenated into one wide tensor of [256,64,64]. And another 7 convolution
    layers are used to gradually map the reconstructed feature maps to the [3,256,256]
    image.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: CNN分支和CA分支的两个输出大小均为[128,64,64]，这些输出被连接成一个[256,64,64]的宽张量。然后使用另外7个卷积层将重建的特征图逐渐映射到[3,256,256]的图像。
- en: The pixels values in the output images from both coarse and refinement generators
    are clamped to [-1,1] to suit the discriminator networks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略和精细化生成器输出图像中的像素值被限制在[-1,1]范围内，以适应判别网络。
- en: Two discriminator networks (local discriminator and global discriminator) with
    similar structures are used to evaluate the quality of generated images. They
    both have 4 convolution layers and 1 fully-connected layer. The only difference
    is that the local discriminator is used to evaluate the cropped image patches
    (in other words, the missing pixels in the original images, with a size of 3 x
    128 x 128) and the global discriminator is used to evaluate the whole images (3
    x 256 x 256).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个结构相似的判别器网络（局部判别器和全局判别器）来评估生成图像的质量。它们都有4层卷积层和1层全连接层。唯一的区别是局部判别器用于评估裁剪图像块（换句话说，就是原始图像中缺失的像素，大小为3
    x 128 x 128），而全局判别器用于评估整个图像（3 x 256 x 256）。
- en: Implementation of Wasserstein loss
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wasserstein损失的实现
- en: 'Here, we let  ![](img/dc14ad97-5552-4f01-9340-8e8075f7fae8.png) and ![](img/0c63a321-e277-4d4f-8867-65223199e7e2.png) (outputs
    of local discriminator) represent the fidelity confidence of the cropped images ![](img/3a0998c5-91fb-4991-84d2-a900ffcb29c3.png) and
    ![](img/1150b1d1-c918-4418-b4e3-0d5615c53361.png), respectively. We let ![](img/4dc1ae97-893c-45f6-b786-81f01a41edfb.png) and
    ![](img/636512fe-b458-4fb3-9313-25a12e31c261.png)(outputs of global discriminator) represent
    the fidelity confidence of whole images ![](img/f57afb66-578d-46b8-8102-1b236a062c3c.png) and
    ![](img/6ef0df8f-9696-4fbc-a390-a4c7e8cad28e.png), respectively. Then, the discriminator''s Wasserstein
    loss is defined as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们让 ![](img/dc14ad97-5552-4f01-9340-8e8075f7fae8.png) 和 ![](img/0c63a321-e277-4d4f-8867-65223199e7e2.png) （局部判别器的输出）分别表示裁剪图像 ![](img/3a0998c5-91fb-4991-84d2-a900ffcb29c3.png) 和
    ![](img/1150b1d1-c918-4418-b4e3-0d5615c53361.png)的保真度置信度。我们让 ![](img/4dc1ae97-893c-45f6-b786-81f01a41edfb.png) 和
    ![](img/636512fe-b458-4fb3-9313-25a12e31c261.png)（全局判别器的输出）分别表示整个图像 ![](img/f57afb66-578d-46b8-8102-1b236a062c3c.png) 和
    ![](img/6ef0df8f-9696-4fbc-a390-a4c7e8cad28e.png)的保真度置信度。然后，判别器的Wasserstein损失定义如下：
- en: '![](img/094b69de-2fb7-471f-a887-f92455e13dd9.png).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/094b69de-2fb7-471f-a887-f92455e13dd9.png)。'
- en: 'The gradient penalty term for the discriminator is given as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的梯度惩罚项定义如下：
- en: '![](img/abf3027d-9c6f-4a78-afca-22e07d6284fc.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abf3027d-9c6f-4a78-afca-22e07d6284fc.png)'
- en: 'The generator''s Wasserstein loss is defined as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的Wasserstein损失定义如下：
- en: '![](img/ba2610ff-05a1-41b7-b56f-3b5bf23db3a1.png).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/ba2610ff-05a1-41b7-b56f-3b5bf23db3a1.png)。'
- en: 'The L1 reconstruction loss for the missing pixels is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失像素的L1重建损失定义如下：
- en: '![](img/020acbfc-bafb-4a26-95bb-a44671373184.png).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/020acbfc-bafb-4a26-95bb-a44671373184.png)。'
- en: 'The L1 reconstruction loss for the remaining pixels is as follows (apparently,
    we don''t want to change these pixels):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余像素的L1重建损失定义如下（显然，我们不希望改变这些像素）：
- en: '![](img/2f63ae6d-3b03-45af-bccd-e6c5cb0728a0.png).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/2f63ae6d-3b03-45af-bccd-e6c5cb0728a0.png)。'
- en: 'Finally, the discriminator loss is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，判别器损失如下：
- en: '![](img/7a6c1484-f311-4560-87ee-2e00edef0635.png).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7a6c1484-f311-4560-87ee-2e00edef0635.png)。'
- en: 'The generator loss is as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失定义如下：
- en: '![](img/61261909-dd9c-4a93-98d4-b34fc9843c7c.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61261909-dd9c-4a93-98d4-b34fc9843c7c.png)'
- en: With a batch size of 24, the training of the inpainting GAN consumes about 10,097
    MB GPU memory and costs about 64 hours of training (180k iterations) before generating
    some decent results. Here are some of the inpainting results.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量大小为24的情况下，图像修复GAN的训练消耗约10,097MB的GPU内存，并且在生成一些不错的结果之前，需要大约64小时的训练（180k次迭代）。以下是一些修复结果。
- en: '![](img/b3cebe1c-228b-432d-bc6b-2c7c6f2ab2b2.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3cebe1c-228b-432d-bc6b-2c7c6f2ab2b2.png)'
- en: Image inpainting results by GAN
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: GAN生成的图像修复结果
- en: Now, we have pretty much learned most of the stuff we need to generate images
    with GANs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经学习了大部分需要用GAN生成图像的知识。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We've gotten a tremendous amount of practical and theoretical knowledge in this
    chapter, from learning about image deblurring and image resolution enhancement,
    and from FFA algorithms to implementing the Wasserstein loss function.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们获得了大量的实践和理论知识，包括学习图像去模糊和图像分辨率增强，从FFA算法到实现Wasserstein损失函数。
- en: In the next chapter, we will work on training our GANs to break other models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将训练我们的GAN来突破其他模型。
- en: Useful reading list and references
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的阅读列表和参考文献
- en: Ledig C, Theis L, Huszar F, et. al. (2017). *Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network*. CVPR.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ledig C, Theis L, Huszar F, 等人（2017）。*基于生成对抗网络的真实单图像超分辨率重建*。CVPR。
- en: Shi W, Caballero J, Huszár F, et. al. (2016). *Real-Time Single Image and Video
    Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network*. CVPR.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi W, Caballero J, Huszár F, 等. (2016). *实时单图像和视频超分辨率使用高效子像素卷积神经网络*。CVPR。
- en: Yang E. (May, 2019). *PyTorch internals*. Retrieved from [http://blog.ezyang.com/2019/05/pytorch-internals](http://blog.ezyang.com/2019/05/pytorch-internals).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang E. (2019年5月). *PyTorch内部机制*。取自 [http://blog.ezyang.com/2019/05/pytorch-internals](http://blog.ezyang.com/2019/05/pytorch-internals)。
- en: Yu J, Lin Z, Yang J, et, al.. (2018). *Generative Image Inpainting with Contextual
    Attention*. CVPR.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu J, Lin Z, Yang J, 等. (2018). *基于上下文注意力的生成图像修复*。CVPR。
- en: Lavin A, Gray S. (2016). *Fast Algorithms for Convolutional Neural Networks*.
    CVPR.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lavin A, Gray S. (2016). *卷积神经网络的快速算法*。CVPR。
- en: Warden P. (April 20, 2015). *Why GEMM is at the heart of deep learning*. Retrieved
    from [https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning).
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warden P. (2015年4月20日). *为什么GEMM是深度学习的核心*。取自 [https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning)。
- en: Arjovsky M, Bottou L. (2017). *Towards Principled Methods for Training Generative
    Adversarial Networks*. ICLR.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky M, Bottou L. (2017). *面向生成对抗网络的原则性训练方法*。ICLR。
- en: Arjovsky M, Chintala S, Bottou L. (2017). *Wasserstein GAN*. ICML.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky M, Chintala S, Bottou L. (2017). *Wasserstein GAN*。ICML。
- en: Distill. (2016). *How to Use t-SNE Effectively*. Retrieved from [https://distill.pub/2016/misread-tsne](https://distill.pub/2016/misread-tsne).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Distill. (2016). *如何有效使用t-SNE*。取自 [https://distill.pub/2016/misread-tsne](https://distill.pub/2016/misread-tsne)。
- en: Hui J. (Jun 22, 2018). *GAN — Why it is so hard to train Generative Adversarial
    Networks!*. Retrieved from [https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hui J. (2018年6月22日). *GAN——为什么训练生成对抗网络如此困难！*。取自 [https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)。
- en: Weng L. (Aug 20, 2017). *From GAN to WGAN*. Retrieved from [https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng L. (2017年8月20日). *从GAN到WGAN*。取自 [https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)。
- en: Herrmann V. (Feb 24, 2017). *Wasserstein GAN and the Kantorovich-Rubinstein
    Duality*. Retrieved from [https://vincentherrmann.github.io/blog/wasserstein](https://vincentherrmann.github.io/blog/wasserstein).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Herrmann V. (2017年2月24日). *Wasserstein GAN与Kantorovich-Rubinstein对偶性*。取自 [https://vincentherrmann.github.io/blog/wasserstein](https://vincentherrmann.github.io/blog/wasserstein)。
- en: Gulrajani I, Ahmed F, Arjovsky M, et. al. (2017). *Improved Training of Wasserstein
    GANs*. NIPS.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulrajani I, Ahmed F, Arjovsky M, 等. (2017). *改进的Wasserstein GAN训练方法*。NIPS。
