- en: Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理
- en: In the previous chapter, we learned how to ingest and transform data to train
    or evaluate a model using a batch ETL approach. You would use this approach in
    the training or evaluation phases in most cases, but when running a model, streaming
    ingestion is needed. This chapter covers setting up streaming ingestion strategies
    for DL models using a combination of the Apache Spark, DL4J, DataVec, and Apache
    Kafka frameworks. Streaming data ingestion frameworks don't simply move data from
    source to destination such as in the traditional ETL approach. With streaming
    ingestion, any incoming data in any format can be simultaneously ingested, transformed,
    and/or enriched with other structured and previously stored data for DL purposes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用批量ETL方法摄取和转换数据，以训练或评估模型。在大多数情况下，你会在训练或评估阶段使用这种方法，但在运行模型时，需要使用流式摄取。本章将介绍使用Apache
    Spark、DL4J、DataVec和Apache Kafka框架组合来设置流式摄取策略。与传统ETL方法不同，流式摄取框架不仅仅是将数据从源移动到目标。通过流式摄取，任何格式的进入数据都可以被同时摄取、转换和/或与其他结构化数据和先前存储的数据一起丰富，以供深度学习使用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Streaming data with Apache Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行流数据处理
- en: Streaming data with Kafka and Apache Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka和Apache Spark进行流数据处理
- en: Streaming data with DL4J and Apache Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DL4J和Apache Spark进行流数据处理
- en: Streaming data with Apache Spark
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行流数据处理
- en: In [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml), *The Apache Spark
    Ecosystem*, the details about Spark Streaming and DStreams were covered. A new
    and different implementation of streaming, Structured Streaming, was introduced
    as an alpha release in Apache Spark 2.0.0\. It finally became stable starting
    from Spark 2.2.0.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml)《Apache Spark生态系统》中，详细介绍了Spark
    Streaming和DStreams。结构化流处理作为Apache Spark 2.0.0的Alpha版本首次推出，它最终从Spark 2.2.0开始稳定。
- en: Structured Streaming (which has been built on top of the Spark SQL engine) is
    a fault-tolerant, scalable stream-processing engine. Streaming can be done in
    the same way batch computation is done, that is, on static data, which we presented
    in [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml), *The Apache Spark
    Ecosystem*. It is the Spark SQL engine that's responsible for incrementally and
    continuously running the computation and for finally updating the results as data
    continues to stream. In this scenario, end-to-end, exactly-once, and fault-tolerance
    guarantees are ensured through **Write Ahead Logs** (**WAL**) and check-pointing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理（基于Spark SQL引擎构建）是一个容错、可扩展的流处理引擎。流处理可以像批量计算一样进行，也就是说，在静态数据上进行计算，我们在[第1章](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml)《Apache
    Spark生态系统》中已经介绍过。正是Spark SQL引擎负责增量地和持续地运行计算，并在数据持续流入时最终更新结果。在这种情况下，端到端、精确一次和容错的保证是通过**预写日志**（**WAL**）和检查点实现的。
- en: 'The difference between the traditional Spark Streaming and the Structured Streaming
    programming models is sometimes not easy to grasp, especially so for experienced
    Spark developers who are approaching this concept for the first time. The best
    way to describe it is like this: you can think of it as a way of handling a live
    data stream as a table (where the table is thought of as an RDBMS) that is being
    continuously appended. The streaming computation is expressed as a standard batch-like
    query (in the same way it happens on a static table), but Spark runs it incrementally
    on the unbounded table.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的Spark Streaming和结构化流处理编程模型之间的差异，有时不容易理解，尤其是对于第一次接触这个概念的有经验的Spark开发者来说。描述这种差异的最好方式是：你可以把它当作一种处理实时数据流的方式，将其看作一个持续追加的表（表可以被视为一个RDBMS）。流计算被表达为一个标准的类批量查询（就像在静态表上发生的那样），但是Spark对这个无界表进行增量计算。
- en: 'Here''s how it works. The input data stream can be considered the input table.
    Every data item arriving in the stream is like a new row being appended to it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理如下：输入数据流可以看作是输入表。每个到达数据流的数据项就像是向表中追加了一行新数据：
- en: '![](img/8d9851de-66a9-49c9-8371-c0c8fd522cda.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d9851de-66a9-49c9-8371-c0c8fd522cda.png)'
- en: 'Figure 4.1: A data stream as an unbounded table'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：作为无界表的数据流
- en: 'A query on the input generates the result table. With every trigger, new interval
    rows are appended to the input table, which then update the result table (as shown
    in the following diagram). Any time the result table gets updated, the changed
    result rows can be written to an external sink. There are different modes for
    the output that is written to external storage:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 针对输入的查询会生成结果表。每次触发时，新的间隔行会追加到输入表中，然后更新结果表（如下图所示）。每当结果表更新时，更改后的结果行可以写入外部接收器。写入外部存储的输出有不同的模式：
- en: '**Complete mode**: In this mode, it is the entire updated result table being
    written to the external storage. How writing to the storage system of the entire
    table happens depends on the specific connector configuration or implementation.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整模式**：在这种模式下，整个更新后的结果表会被写入外部存储。如何将整个表写入存储系统取决于特定的连接器配置或实现。'
- en: '**Append mode**: Only the new rows that are appended in the result table will
    be written to the external storage system. This means that it is possible to apply
    this mode in situations where the existing rows in the result table aren''t expected
    to change.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追加模式**：只有追加到结果表中的新行会被写入外部存储系统。这意味着可以在结果表中的现有行不期望更改的情况下应用此模式。'
- en: '**Update mode**: Only the rows that have been updated in the result table are
    written to the external storage system. The difference between this mode and the
    complete mode is that this one sends out only those rows that have changed since
    the last trigger:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新模式**：只有在结果表中更新过的行会被写入外部存储系统。这种模式与完整模式的区别在于，它仅发送自上次触发以来发生变化的行：'
- en: '![](img/973dd2e8-1e6e-4759-b3d3-f7b574106ee2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/973dd2e8-1e6e-4759-b3d3-f7b574106ee2.png)'
- en: 'Figure 4.2: Programming model for Structured Streaming'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：结构化流处理的编程模型
- en: 'Now, let''s implement a simple Scala example – a streaming word count self-contained
    application, which is the same use case that we used in [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml),
    *The Apache Spark Ecosystem*, but for Structured Streaming instead. The code that''s
    used for this class can be found among the examples that are bundled with a Spark
    distribution. The first thing we need to do is initialize a `SparkSession`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个简单的Scala示例——一个流式单词计数自包含应用程序，这是我们在[第1章](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml)中使用的相同用例，*Apache
    Spark生态系统*，但这次是针对结构化流处理。用于此类的代码可以在与Spark发行版捆绑的示例中找到。我们首先需要做的是初始化一个`SparkSession`：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We must then create a DataFrame representing the stream of input lines from
    the connection to `host:port`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须创建一个表示从连接到`host:port`的输入行流的DataFrame：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `lines` DataFrame represents the unbounded table. It contains the streaming
    text data. The content of that table is a value, that is, a single column of strings.
    Each incoming line in the streaming text becomes a row.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`lines` DataFrame表示无界表格。它包含流式文本数据。该表的内容是一个值，即一个包含字符串的单列。每一行流入的文本都会成为一行数据。'
- en: 'Let''s split the lines into words:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将行拆分为单词：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we need to count the words:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要统计单词数量：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we can start running the query that prints the running counts to the
    console:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以开始运行查询，将运行计数打印到控制台：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We continue running until a termination signal is received:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会继续运行，直到接收到终止信号：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Before running this example, first, you need to run netcat as a data server
    (or the data server that we implemented in Scala in [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml),
    *The Apache Spark Ecosystem*):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此示例之前，首先需要运行netcat作为数据服务器（或者我们在[第1章](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml)中用Scala实现的数据服务器，*Apache
    Spark生态系统*）：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, in a different Terminal, you can start the example by passing the following
    as arguments:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在另一个终端中，你可以通过传递以下参数来启动示例：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Any line typed in the Terminal when running the netcat server will be counted
    and printed to the application screen. An output such as the following will occur:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行netcat服务器时，终端中输入的任何一行都会被计数并打印到应用程序屏幕上。将会出现如下输出：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will produce the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The event time is defined as the time that's embedded in the data itself. In
    many applications, such as those in an IoT context, when the number of events
    generated by devices every minute needs to be retrieved, the time the data was
    generated has to be used rather than the time Spark receives it. Event-time is
    naturally expressed in this programming model—each event from the device is a
    row in the table, and event-time is a column value in that row. This paradigm
    makes window-based aggregations simply a special type of aggregation on that event-time
    column. This grants consistency, because event-time and window-based aggregation
    queries can be defined in the same way on both static datasets (for example, events
    logs from devices) and streaming data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 事件时间定义为数据本身所嵌入的时间。在许多应用场景中，例如物联网（IoT）环境，当每分钟设备生成的事件数量需要被检索时，必须使用数据生成的时间，而不是
    Spark 接收到它的时间。在这种编程模型中，事件时间自然地被表达——每个来自设备的事件就是表中的一行，而事件时间是该行中的列值。这种范式使得基于窗口的聚合成为对事件时间列的特殊聚合类型。这样可以保证一致性，因为基于事件时间和基于窗口的聚合查询可以在静态数据集（例如设备事件日志）和流式数据上以相同方式进行定义。
- en: Following the previous consideration, it is evident that this programming model
    naturally handles data that has arrived later than expected based on its event-time.
    Since it is Spark itself that updates the result table, it has full control over
    updating old aggregates when there is late data, as well as limiting the size
    of intermediate data by cleaning up old aggregates. Starting from Spark 2.1, there
    is also support for watermarking, which allows you to specify the threshold of
    late data and allows the underlying engine to clean up old states accordingly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的考虑，显而易见，这种编程模型自然地处理了基于事件时间的数据，这些数据可能比预期的到达时间晚。由于是 Spark 本身更新结果表，因此它可以完全控制在有迟到数据时如何更新旧的聚合，以及通过清理旧的聚合来限制中间数据的大小。从
    Spark 2.1 开始，还支持水印（watermarking），它允许你指定迟到数据的阈值，并允许底层引擎相应地清理旧状态。
- en: Streaming data with Kafka and Spark
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kafka 和 Spark 进行流式数据处理
- en: Spark Streaming with Kafka is a common combination of technologies in data pipelines.
    This section will present some examples of streaming Kafka with Spark.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark Streaming 与 Kafka 是数据管道中常见的技术组合。本节将展示一些使用 Spark 流式处理 Kafka 的示例。
- en: Apache Kakfa
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: Apache Kafka ([http://kafka.apache.org/](http://kafka.apache.org/)) is an open
    source message broker written in Scala. Originally, it was developed by LinkedIn,
    but it was then released as open source in 2011 and is currently maintained by
    the Apache Software Foundation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka ([http://kafka.apache.org/](http://kafka.apache.org/)) 是一个用 Scala
    编写的开源消息代理。最初由 LinkedIn 开发，但它于 2011 年作为开源发布，目前由 Apache 软件基金会维护。
- en: 'Here are some of the reasons why you might prefer Kafka to a traditional JMS
    message broker:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你可能更倾向于使用 Kafka 而不是传统 JMS 消息代理的一些原因：
- en: '**It''s fast**: A single Kafka broker running on commodity hardware can handle
    hundreds of megabytes of reads and writes per second from thousands of clients'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它很快**：单个运行在普通硬件上的 Kafka 代理能够处理来自成千上万客户端的每秒数百兆字节的读写操作'
- en: '**Great scalability**: It can be easily and transparently expanded without
    downtime'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**出色的可扩展性**：可以轻松且透明地进行扩展，且不会产生停机时间'
- en: '**Durability and replication**: Messages are persisted on disk and replicated
    within the cluster to prevent data loss (by setting a proper configuration using
    the high number of available configuration parameters, you could achieve zero
    data loss)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久性与复制**：消息会被持久化存储在磁盘上，并在集群内进行复制，以防止数据丢失（通过设置适当的配置参数，你可以实现零数据丢失）'
- en: '**Performance**: Each broker can handle terabytes of messages without performance
    impact'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：每个代理能够处理数 TB 的消息而不会影响性能'
- en: It allows real-time stream processing
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持实时流处理
- en: It can be easily integrated with other popular open source systems for big data
    architectures such as Hadoop, Spark, and Storm
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以轻松与其他流行的开源大数据架构系统（如 Hadoop、Spark 和 Storm）进行集成
- en: 'The following are the core concepts of Kafka that you should become familiar
    with:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你应该熟悉的 Kafka 核心概念：
- en: '**Topics**: These are categories or feed names to which upcoming messages are
    published'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题**：这些是发布即将到来的消息的类别或源名称'
- en: '**Producers**: Any entity that publishes messages to a topic'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者**：任何发布消息到主题的实体'
- en: '**Consumers**: Any entity that subscribes to topics and consumes messages from
    them'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**：任何订阅主题并从中消费消息的实体'
- en: '**Brokers**: Services that handle read and write operations'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Broker**：处理读写操作的服务'
- en: 'The following diagram shows a typical Kafka cluster architecture:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了典型的 Kafka 集群架构：
- en: '![](img/de224ee0-9bff-44c9-a960-f34697e9e593.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de224ee0-9bff-44c9-a960-f34697e9e593.png)'
- en: 'Figure 4.3: Kafka architecture'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：Kafka 架构
- en: Kafka uses ZooKeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
    behind the scenes to keep its nodes in sync. The Kafka binaries provide it, so
    if hosting machines don't have ZooKeeper on board, you can use the one that comes
    bundled with Kafka. The communication between clients and servers happens using
    a highly performant and language-agnostic TCP protocol.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 在后台使用 ZooKeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
    来保持其节点的同步。Kafka 提供了 ZooKeeper，因此如果主机没有安装 ZooKeeper，可以使用随 Kafka 捆绑提供的 ZooKeeper。客户端和服务器之间的通信通过一种高性能、语言无关的
    TCP 协议进行。
- en: 'Typical use cases for Kafka are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 的典型使用场景如下：
- en: Messaging
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息传递
- en: Stream processing
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理
- en: Log aggregation
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志聚合
- en: Metrics
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标
- en: Web activity tracking
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站活动跟踪
- en: Event sourcing
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件溯源
- en: Spark Streaming and Kafka
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming 和 Kafka
- en: 'To use Spark Streaming with Kafka, you can do two things: either use a receiver
    or be direct. The first option is similar to streaming from other sources such
    as text files and sockets – data received from Kafka is stored in Spark executors
    and processed by jobs that are launched by a Spark Streaming context. This is
    not the best approach – it can cause data loss in the event of failures. This
    means that the direct approach (introduced in Spark 1.3) is better. Instead of
    using receivers to receive data, it periodically queries Kafka for the latest
    offsets in each topic and partitions, and accordingly defines, the offset ranges
    to process for each batch. When the jobs to process the data are executed, Kafka''s
    simple consumer API is used to read the defined ranges of offsets (almost in the
    same way as for reading files from a filesystem). The direct approach brings the
    following advantages:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Spark Streaming 与 Kafka 配合使用，您可以做两件事：要么使用接收器，要么直接操作。第一种选择类似于从其他来源（如文本文件和套接字）进行流式传输——从
    Kafka 接收到的数据会存储在 Spark 执行器中，并通过 Spark Streaming 上下文启动的作业进行处理。这不是最佳方法——如果发生故障，可能会导致数据丢失。这意味着，直接方式（在
    Spark 1.3 中引入）更好。它不是使用接收器来接收数据，而是定期查询 Kafka 以获取每个主题和分区的最新偏移量，并相应地定义每个批次处理的偏移量范围。当处理数据的作业执行时，Kafka
    的简单消费者 API 会被用来读取定义的偏移量范围（几乎与从文件系统读取文件的方式相同）。直接方式带来了以下优点：
- en: '**Simplified parallelism**: There''s no need to create multiple input Kafka
    streams and then struggle trying to unify them. Spark Streaming creates as many
    RDD partitions as there are Kafka partitions to consume, which read data from
    Kafka in parallel. This means that there is 1:1 mapping between Kafka and RDD
    partitions that is easier to understand and tune.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化的并行性**：不需要创建多个输入 Kafka 流然后努力将它们统一起来。Spark Streaming 会根据 Kafka 分区的数量创建相应数量的
    RDD 分区，这些分区会并行地从 Kafka 读取数据。这意味着 Kafka 和 RDD 分区之间是 1:1 映射，易于理解和调整。'
- en: '**Improved efficiency**: Following the receiver approach, to achieve zero-data
    loss, we need the data to be stored in a WAL. However, this strategy is inefficient,
    as the data effectively gets replicated twice, by Kafka first and then by the
    WAL. In the direct approach, there is no receiver, and subsequently no need for
    WALs—messages can be recovered from Kafka, assuming there is sufficient Kafka
    retention.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高效率**：按照接收器方式，为了实现零数据丢失，我们需要将数据存储在 WAL 中。然而，这种策略效率低，因为数据实际上被 Kafka 和 WAL
    各自复制了一次。在直接方式中，没有接收器，因此也不需要 WAL——消息可以从 Kafka 中恢复，只要 Kafka 保留足够的时间。'
- en: '**Exactly-once semantics**: The receiver approach uses Kafka''s high-level
    API to store consumed offsets in ZooKeeper. While this approach (combined with
    WALs) can ensure zero data loss, there is a remote possibility that some records
    will get consumed twice when a failure happens. Inconsistencies between data being
    reliably received by Spark Streaming and offsets tracked by ZooKeeper lead to
    this. With the direct approach, the simple Kafka API involved doesn''t use ZooKeeper—the
    offsets are tracked by Spark Streaming itself within its checkpoints. This ensures
    that each record is received by Spark Streaming effectively exactly once, even
    when a failure happens.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确一次语义**：接收器方法使用 Kafka 的高级 API 将消费的偏移量存储在 ZooKeeper 中。尽管这种方法（结合 WAL）可以确保零数据丢失，但在发生故障时，某些记录可能会被重复消费，这有一定的可能性。数据被
    Spark Streaming 可靠接收和 ZooKeeper 跟踪的偏移量之间的不一致导致了这一点。采用直接方法时，简单的 Kafka API 不使用 ZooKeeper——偏移量由
    Spark Streaming 本身在其检查点内进行跟踪。这确保了即使在发生故障时，每条记录也能被 Spark Streaming 确切地接收一次。'
- en: One disadvantage of the direct approach is that it doesn't update the offsets
    in ZooKeeper—this means that the ZooKeeper-based Kafka monitoring tools will not
    show any progress.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 直接方法的一个缺点是它不更新 ZooKeeper 中的偏移量——这意味着基于 ZooKeeper 的 Kafka 监控工具将不会显示任何进度。
- en: 'Now, let''s implement a simple Scala example – a Kafka direct word count. The
    example that''s presented in this section works with Kafka release 0.10.0.0 or
    later. The first thing to do is to add the required dependencies (Spark Core,
    Spark Streaming, and Spark Streaming Kafka) to your project:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个简单的 Scala 示例——一个 Kafka 直接词频统计。该示例适用于 Kafka 版本 0.10.0.0 或更高版本。首先要做的是将所需的依赖项（Spark
    Core、Spark Streaming 和 Spark Streaming Kafka）添加到项目中：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This application expects two arguments:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序需要两个参数：
- en: A comma-separated list of one or more Kafka brokers
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个以逗号分隔的 Kafka 经纪人列表
- en: 'A comma-separated list of one or more Kafka topics to consume from:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个以逗号分隔的 Kafka 主题列表，用于消费：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We need to create the Spark Streaming context. Let''s choose a `5`-second batch
    interval:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建 Spark Streaming 上下文。让我们选择一个 `5` 秒的批次间隔：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s create a direct Kafka stream with the given brokers and topics:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个包含给定经纪人和主题的直接 Kafka 流：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can implement the word count now, that is, get the lines from the stream,
    split them into words, count the words, and then print:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以实现词频统计，也就是从流中获取行，将其拆分成单词，统计单词数量，然后打印：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, let''s start the computation and keep it alive, waiting for a termination
    signal:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们启动计算并保持其运行，等待终止信号：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To run this example, we first need to start a Kafka cluster and create a topic.
    The Kafka binaries can be downloaded from the official website ([http://kafka.apache.org/downloads](http://kafka.apache.org/downloads)).
    Once it has been downloaded, we can follow the following instructions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，首先需要启动一个 Kafka 集群并创建一个主题。Kafka 的二进制文件可以从官方网站下载（[http://kafka.apache.org/downloads](http://kafka.apache.org/downloads)）。下载完成后，我们可以按照以下指示操作。
- en: 'Start a `zookeeper` node first:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先启动一个 `zookeeper` 节点：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It will start listening to the default port, `2181`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 它将开始监听默认端口，`2181`。
- en: 'Then, start a Kafka broker:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，启动一个 Kafka 经纪人：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It will start listening to the default port, `9092`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它将开始监听默认端口，`9092`。
- en: 'Create a topic called `packttopic`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 `packttopic` 的主题：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Check that the topic has been successfully created:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 检查主题是否已成功创建：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The topic name, `packttopic`, should be in the list that was printed to the
    console output.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 主题名称 `packttopic` 应该出现在打印到控制台输出的列表中。
- en: 'We can now start to produce messages for the new topic. Let''s start a command-line
    producer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始为新主题生成消息了。让我们启动一个命令行生产者：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we can write some messages to the producer console:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以向生产者控制台写入一些消息：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let's build the Spark application and execute it through the `$SPARK_HOME/bin/spark-submit`
    command, specifying the JAR filename, the Spark master URL, the job name, the
    main class name, the maximum memory to be used by each executor, and the job arguments
    (`localhost:9092` and `packttopic`).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建 Spark 应用程序，并通过 `$SPARK_HOME/bin/spark-submit` 命令执行，指定 JAR 文件名、Spark 主
    URL、作业名称、主类名称、每个执行器使用的最大内存和作业参数（`localhost:9092` 和 `packttopic`）。
- en: 'The output printed by the Spark job for each consumed message line will be
    something like the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每条被 Spark 作业消费的消息行输出将类似于以下内容：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Streaming data with DL4J and Spark
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DL4J 和 Spark 流式处理数据
- en: In this section, we are going to apply data streaming with Kafka and Spark to
    a use case scenario of a DL4J application. The DL4J module we are going to use
    is DataVec.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用Kafka和Spark进行数据流处理，以DL4J应用程序的使用情况场景为例。我们将使用的DL4J模块是DataVec。
- en: Let's consider the example that we presented in the *Spark Streaming and Kafka* section.
    What we want to achieve is direct Kafka streaming with Spark, then apply DataVec
    transformations on the incoming data as soon as it arrives, before using it downstream.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们在*Spark Streaming和Kafka*部分中提出的示例。我们想要实现的是使用Spark进行直接Kafka流，并在数据到达后立即对其应用DataVec转换，然后在下游使用它。
- en: 'Let''s define the input schema first. This is the schema we expect for the
    messages that are consumed from a Kafka topic. The schema structure is the same
    as for the classic `Iris` dataset ([https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义输入模式。这是我们从Kafka主题消费的消息所期望的模式。该模式结构与经典的`Iris`数据集（[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)）相同：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s define a transformation on it (we are going to remove the petal fields
    because we are going to do some analysis based on the sepal features only):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对其进行转换（我们将删除花瓣字段，因为我们将基于萼片特征进行一些分析）：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we can generate the new schema (after applying the transformation to the
    data):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以生成新的模式（在对数据应用转换之后）：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The next part of this Scala application is exactly the same as for the example
    in the *Spark Streaming and Kafka* section. Here, create a streaming context with
    a `5`-second batch interval and a direct Kafka stream:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此Scala应用程序的下一部分与*Spark Streaming和Kafka*部分中的示例完全相同。在这里，创建一个流上下文，使用`5`秒的批处理间隔和直接的Kafka流：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s get the input lines:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取输入行：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`lines` is a `DStream[String]`. We need to iterate for each RDD there, convert
    it to `javaRdd` (required by the DataVec reader), use a DataVec `CSVRecordReader`,
    parse the incoming comma-separated messages, apply the schema transformation,
    and print the result data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`lines`是一个`DStream[String]`。我们需要对每个RDD进行迭代，将其转换为`javaRdd`（DataVec读取器所需），使用DataVec的`CSVRecordReader`，解析传入的逗号分隔消息，应用模式转换，并打印结果数据：'
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we start the streaming context and keep it alive, waiting for a termination
    signal:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们启动流上下文并保持其活动状态，等待终止信号：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To run this example, we need to start a Kafka cluster and create a new topic
    called `csvtopic`. The steps are the same as for the example described in the *Spark
    Streaming and Kafka* section. Once the topic has been created, we can start to
    produce comma-separated messages on it. Let''s start a command-line producer:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，我们需要启动一个Kafka集群，并创建一个名为`csvtopic`的新主题。步骤与*Spark Streaming和Kafka*部分描述的示例相同。主题创建完成后，我们可以开始在其上生产逗号分隔的消息。让我们启动一个命令行生产者：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we can write some messages to the producer console:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将一些消息写入生产者控制台：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let's build the Spark application and execute it through the `$SPARK_HOME/bin/spark-submit`
    command, specifying the JAR filename, the Spark master URL, the job name, the
    main class name, the maximum memory to be used by each executor, and the job arguments
    (`localhost:9092` and `csvtopic`).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建Spark应用程序，并通过`$SPARK_HOME/bin/spark-submit`命令执行它，指定JAR文件名、Spark主URL、作业名称、主类名、每个执行程序可使用的最大内存以及作业参数（`localhost:9092`和`csvtopic`）。
- en: 'The output printed by the Spark job for each consumed message line will be
    something like the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每消费一行消息后，Spark作业打印的输出将类似于以下内容：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The full code for this example can be found among the source code that's bundled
    with this book at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark](https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的完整代码可以在本书捆绑的源代码中找到，链接为[https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark](https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark)。
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: To complete our overview of data ingestion possibilities when training, evaluating,
    and running DL models after exploring them in [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml),
    *Extract, Transform, Load*, in this chapter, we explored the different options
    that are available to us when we perform data streaming.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们在[第三章](44fab060-12c9-4eec-9e15-103da589a510.xhtml)中探索之后在训练、评估和运行深度学习模型时的数据摄入可能性的概述，*提取、转换、加载*，在本章中，我们探讨了在执行数据流处理时可用的不同选项。
- en: This chapter concludes the exploration of Apache Spark features. Starting from
    the next chapter, the focus will be on DL4J and some other deep learning framework
    features. These will be used in different use case scenarios, where they will
    be implemented on top of Spark.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了对 Apache Spark 特性的探讨。从下一章开始，重点将转向 DL4J 和其他一些深度学习框架的特性。这些特性将在不同的应用场景中使用，并将在
    Spark 上进行实现。
