- en: Training a Single Neuron
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个单神经元
- en: 'After revising the concepts around learning from data, we will now pay close
    attention to an algorithm that trains one of the most fundamental neural-based
    models: the **perceptron**. We will look at the steps required for the algorithm
    to function, and the stopping conditions. This chapter will present the perceptron
    model as the first model that represents a neuron, which aims to learn from data
    in a simple manner. The perceptron model is key to understanding basic and advanced
    neural models that learn from data. In this chapter, we will also cover the problems
    and considerations associated with non-linearly separable data.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了数据学习的相关概念后，我们现在将重点关注一个训练最基本神经网络模型的算法：**感知机**。我们将探讨使该算法运作所需的步骤及停止条件。本章将呈现感知机模型，作为第一个代表神经元的模型，旨在以简单的方式从数据中学习。感知机模型对于理解从数据中学习的基础和高级神经网络模型至关重要。本章还将讨论与非线性可分数据相关的问题和考虑因素。
- en: Upon completion of the chapter, you should feel comfortable discussing the perceptron
    model, and applying its learning algorithm. You will be able to implement the
    algorithm over both linearly and non-linearly separable data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你应该能够自如地讨论感知机模型，并应用其学习算法。你将能够在数据是线性可分或非线性可分的情况下实现该算法。
- en: 'Specifically, the following topics are covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章涵盖以下主题：
- en: The perceptron model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机模型
- en: The perceptron learning algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机学习算法
- en: A perceptron over non-linearly separable data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机与非线性可分数据
- en: The perceptron model
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机模型
- en: Back in [Chapter 1](e3181710-1bb7-4069-825a-a235355bc116.xhtml), *Introduction
    to Machine Learning*, we briefly introduced the basic model of a neuron and the
    **perceptron learning algorithm** (**PLA**). Here, in this chapter, we will now
    revisit and expand the concept and show how that is coded in Python. We will begin
    with the basic definition.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第1章](e3181710-1bb7-4069-825a-a235355bc116.xhtml)，*机器学习简介*，我们简要介绍了神经元的基本模型和**感知机学习算法**（**PLA**）。在本章中，我们将重新审视并扩展这一概念，并展示如何用Python编码实现它。我们将从基本定义开始。
- en: The visual concept
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉概念
- en: 'The perceptron is an analogy of a human-inspired information processing unit,
    originally conceived by F. Rosenblatt and depicted in *Figure 5.1* (Rosenblatt,
    F. (1958)). In the model, the input is represented with the vector ![](img/dd2553bf-c673-4ade-b77f-c9edb436ed12.png),
    the activation of the neuron is given by the function ![](img/018df1a9-86c5-40ef-afd0-12da768026e0.png),
    and the output is ![](img/e9f481bc-0639-4987-8be9-75dd4d377193.png). The parameters
    of the neuron are ![](img/d16c70d8-9f05-4c25-91f9-61fa60a89974.png) and ![](img/430e33f2-b6a0-44d0-b21a-c16e32da02b8.png):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是一个类比于人类信息处理单元的模型，最初由F. Rosenblatt提出，并在*图5.1*中描述（Rosenblatt, F. (1958)）。在该模型中，输入由向量表示 ![](img/dd2553bf-c673-4ade-b77f-c9edb436ed12.png)，神经元的激活由函数 ![](img/018df1a9-86c5-40ef-afd0-12da768026e0.png)给出，输出为 ![](img/e9f481bc-0639-4987-8be9-75dd4d377193.png)。神经元的参数为 ![](img/d16c70d8-9f05-4c25-91f9-61fa60a89974.png) 和 ![](img/430e33f2-b6a0-44d0-b21a-c16e32da02b8.png)：
- en: '![](img/bac392e3-087b-485d-aa22-287a161238ce.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bac392e3-087b-485d-aa22-287a161238ce.png)'
- en: Figure 5.1 – The basic model of a perceptron
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 感知机的基本模型
- en: 'The *trainable* parameters of a perceptron are ![](img/b7dce5c4-c06a-4c49-99ac-44a00c3dde4a.png),
    and they are unknown. Thus, we can use input training data ![](img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png) to
    determine these parameters using the PLA. From *Figure 5.1*, ![](img/5ebb08ed-23c4-4a97-ad97-192adaf9fa16.png)multiplies ![](img/590e73b6-a479-4c5c-9285-4551930c7b91.png),
    then ![](img/8b81f157-95ad-4b92-9977-3c2f3500a544.png)multiplies ![](img/67beef13-3471-40ef-869e-0c5009fa2a91.png),
    and ![](img/d0b9f56d-d74f-40fe-9160-7c65b95b390b.png) is multiplied by 1; all
    these products are added and then passed into the *sign* activation function,
    which in the perceptron operates as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的*可训练*参数为 ![](img/b7dce5c4-c06a-4c49-99ac-44a00c3dde4a.png)，这些参数是未知的。因此，我们可以使用输入训练数据 ![](img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png)来使用PLA确定这些参数。从*图5.1*可以看出， ![](img/5ebb08ed-23c4-4a97-ad97-192adaf9fa16.png)乘以 ![](img/590e73b6-a479-4c5c-9285-4551930c7b91.png)，然后 ![](img/8b81f157-95ad-4b92-9977-3c2f3500a544.png)乘以 ![](img/67beef13-3471-40ef-869e-0c5009fa2a91.png)，并且 ![](img/d0b9f56d-d74f-40fe-9160-7c65b95b390b.png)乘以1；所有这些乘积相加后，再输入*符号*激活函数，在感知机中，其操作如下：
- en: '![](img/25c4e4bc-29d9-4ee4-8ff0-0d8ada66ccb1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25c4e4bc-29d9-4ee4-8ff0-0d8ada66ccb1.png)'
- en: The main purpose of the activation sign is to map any response of the model
    to a binary output: [![](img/c2b8debd-da54-4f91-82aa-f673eacf1584.png)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的主要目的是将模型的任何响应映射到二进制输出：[![](img/c2b8debd-da54-4f91-82aa-f673eacf1584.png)]。
- en: Now let's talk about tensors in a general sense.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们一般性地讨论一下张量。
- en: Tensor operations
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量操作
- en: 'In Python, the implementation of the perceptron requires a few simple tensor
    (vector) operations that can be performed through standard NumPy functionalities.
    First, we can assume that we are given data [![](img/0d85ebf4-d57f-4fd1-88ee-e155345e4c6f.png)] in
    the form of a vector containing multiple vectors ![](img/59afade2-2430-4eba-b0c8-28c69956ec56.png)
    (a matrix), represented as [![](img/585f7ba2-8841-4a53-8431-2d1be8d31af2.png)],
    and multiple individual targets represented as a vector [![](img/b7e3a027-88db-4c9b-ad29-639468ea172f.png)].
    However, notice that for easier implementation of the perceptron it will be necessary
    to include ![](img/3b0fa764-dbef-4a77-a092-47b93447c9ed.png) in ![](img/7d70cc37-fb85-4d34-99b5-37b13204746e.png),
    as suggested in *Figure 5.1*, so that the products and additions in [![](img/55992dcb-f61a-4702-9339-fb13e815fdec.png)] can
    be simplified if we modify ![](img/5c464c60-6998-4884-9f1d-38fbca822376.png) to
    be ![](img/5fc1e66a-27b1-4b71-a1f9-e19233e0ffd6.png), and ![](img/71f4dc57-8177-4f50-b003-612501ff1d00.png) to
    be [![](img/c6fb2cba-f4dc-4519-8728-4d7bfb626352.png)]. In this way, the perceptron
    response for an input [![](img/e8377d7f-1b36-43b3-882d-751ea830c783.png)] could
    be simplified to be as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，感知机的实现需要一些简单的张量（向量）操作，可以通过标准的NumPy功能来完成。首先，我们可以假设给定的数据[![](img/0d85ebf4-d57f-4fd1-88ee-e155345e4c6f.png)]是一个包含多个向量![](img/59afade2-2430-4eba-b0c8-28c69956ec56.png)（矩阵）的向量，表示为[![](img/585f7ba2-8841-4a53-8431-2d1be8d31af2.png)]，以及表示多个单独目标的向量[![](img/b7e3a027-88db-4c9b-ad29-639468ea172f.png)]。然而，注意到为了便于感知机的实现，需要将![](img/3b0fa764-dbef-4a77-a092-47b93447c9ed.png)包含在![](img/7d70cc37-fb85-4d34-99b5-37b13204746e.png)中，正如*图5.1*中所建议的那样，这样可以简化[![](img/55992dcb-f61a-4702-9339-fb13e815fdec.png)]中的乘法和加法，如果我们将![](img/5c464c60-6998-4884-9f1d-38fbca822376.png)修改为![](img/5fc1e66a-27b1-4b71-a1f9-e19233e0ffd6.png)，并将![](img/71f4dc57-8177-4f50-b003-612501ff1d00.png)修改为[![](img/c6fb2cba-f4dc-4519-8728-4d7bfb626352.png)]，则可以简化输入[![](img/e8377d7f-1b36-43b3-882d-751ea830c783.png)]的感知机响应，具体如下：
- en: '![](img/e56c3f72-ce08-4071-9aac-2104e2d42362.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e56c3f72-ce08-4071-9aac-2104e2d42362.png)'
- en: Notice that **![](img/6d483edb-3362-4650-a2ef-4ad86675a0ae.png) **is now implicit
    in ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**![](img/6d483edb-3362-4650-a2ef-4ad86675a0ae.png)**现在在![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png)中是隐式存在的。
- en: 'Say that we want to have training data `X`, which we need to prepare for the
    perceptron; we can do that with a simple linearly separable dataset that can be
    generated through scikit-learn''s dataset method called `make_classification`
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望获得用于训练的`X`数据集，我们需要为感知机准备这些数据；我们可以通过scikit-learn的`make_classification`方法生成一个简单的线性可分数据集，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we use the `make_classification` constructor to produce 100 data points
    (`n_samples`) for two classes (`n_classes`) and with enough separation (`class_sep`)
    to make data that is linearly separable. However, the dataset produced binary
    values in `y` in the set [![](img/5fbae48a-781e-4d49-8899-6cefba685f19.png)],
    and we need to convert it to the values in the set [![](img/db866a8e-aee2-419a-9953-74d93d142c8d.png)].
    This can be easily achieved by replacing the zero targets with the negative targets
    by simply doing the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`make_classification`构造函数生成100个数据点（`n_samples`），这100个数据点有两个类别（`n_classes`），并且通过足够的分隔度（`class_sep`）使得数据具有线性可分性。然而，生成的数据集在`y`中产生了[![](img/5fbae48a-781e-4d49-8899-6cefba685f19.png)]中的二进制值，我们需要将其转换为[![](img/db866a8e-aee2-419a-9953-74d93d142c8d.png)]中的值。这可以通过以下简单操作实现：用负目标值替换零目标值：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The dataset produced looks as depicted in *Figure 5.2*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集如*图5.2*所示：
- en: '![](img/7fe6eb0e-e3ec-42a1-a4f3-7e9aa99ab418.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fe6eb0e-e3ec-42a1-a4f3-7e9aa99ab418.png)'
- en: Figure 5.2– Sample two-dimensional data for perceptron testing
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 - 用于感知机测试的二维样本数据
- en: 'Next, we can add the number 1 to each input vector by adding a vector of ones
    to `X` with length `N=100` as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过将一个长度为`N=100`的全1向量添加到`X`中，为每个输入向量添加数字1，具体操作如下：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The new data in `X` now contains a vector of ones. This will allow easier calculation
    of the tensor operation [![](img/f401a345-a15c-485c-8636-1fefe31ec8d7.png)] for
    all ![](img/f58f01a4-07d4-4577-aabe-6f1b9e92ce78.png). This common tensor operation
    can be performed in one single step considering the matrix ![](img/3964917c-2a69-4948-b032-f55e473de016.png) simply
    as ![](img/caa93965-f1c5-4721-8829-197f116857b1.png). We can even combine this
    operation and the sign activation function in one single step as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`X`中的新数据包含一个全为1的向量。这将简化张量操作的计算[![](img/f401a345-a15c-485c-8636-1fefe31ec8d7.png)]，适用于所有![](img/f58f01a4-07d4-4577-aabe-6f1b9e92ce78.png)。考虑矩阵![](img/3964917c-2a69-4948-b032-f55e473de016.png)，这个常见的张量操作可以通过一步完成，简单地将其视为![](img/caa93965-f1c5-4721-8829-197f116857b1.png)。我们甚至可以将这个操作与符号激活函数合并成一步，具体如下：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the equivalent of the mathematical tensor operation [![](img/3ac75eb8-972d-46c9-9647-b8cda8237618.png)].
    With this in mind, let's review the PLA in more detail using the dataset introduced
    previously, and the operations just described.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数学张量操作[![](img/3ac75eb8-972d-46c9-9647-b8cda8237618.png)]的等效形式。有了这个概念，让我们使用前面介绍的
    dataset 和刚才描述的操作，更详细地回顾 PLA。
- en: The perceptron learning algorithm
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器学习算法
- en: 'The **perceptron learning algorithm** (**PLA**) is the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知器学习算法**（**PLA**）如下：'
- en: '**Input**: Binary class dataset [![](img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png)]'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：二分类数据集[![](img/d22b2bbd-59f3-4f40-a4ab-6df6e95bf1ff.png)]'
- en: Initialize ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png) to zeros, and iteration counter ![](img/467437b4-17a7-42a3-af49-cd3cc8b8816e.png)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png)初始化为零，迭代计数器![](img/467437b4-17a7-42a3-af49-cd3cc8b8816e.png)
- en: 'While there are any incorrectly classified examples:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要存在任何错误分类的样本：
- en: Pick an incorrectly classified example, call it ![](img/01a87dc1-7f59-4c55-9c52-a1803ae95db7.png),
    whose true label is ![](img/16eb7743-58d1-4037-ae7f-67757849756f.png)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选取一个被错误分类的样本，记作![](img/01a87dc1-7f59-4c55-9c52-a1803ae95db7.png)，其真实标签是![](img/16eb7743-58d1-4037-ae7f-67757849756f.png)
- en: Update ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png) as follows: ![](img/cfa7a1f4-17cd-4f28-b07e-f17fbac38691.png)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按如下方式更新![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png)：![](img/cfa7a1f4-17cd-4f28-b07e-f17fbac38691.png)
- en: Increase iteration counter, [![](img/99f0af99-2238-4552-9903-2580096b8378.png)], and
    repeat
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加迭代计数器，[![](img/99f0af99-2238-4552-9903-2580096b8378.png)]，并重复
- en: '**Return**: ![](img/8ca08b66-f0e6-466a-bd50-d4d584c2cf06.png)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：![](img/8ca08b66-f0e6-466a-bd50-d4d584c2cf06.png)'
- en: Now, let's see how this takes form in Python.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这在 Python 中是如何实现的。
- en: PLA in Python
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的PLA
- en: 'Here is an implementation in Python that we will discuss part by part, while
    some of it has already been discussed:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个 Python 实现，我们将逐部分讨论，部分内容已被讨论过：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first few lines have been discussed previously in the *Tensor operations*
    section of this chapter. The initialization of ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png) to
    zeros is done with `w = np.zeros(X_train.shape[1])`. The size of this vector depends
    on the dimensionality of the input. Then, `it` is merely an iteration counter
    to keep track of the number of iterations that are performed until the PLA converges.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行在本章的*张量操作*部分已讨论过。将![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png)初始化为零，通过`w
    = np.zeros(X_train.shape[1])`完成。该向量的大小取决于输入的维度。然后，`it`只是一个迭代计数器，用于跟踪执行的迭代次数，直到PLA收敛为止。
- en: 'The `classification_error()` method is a helper method that takes as arguments
    the current vector of parameters `w`, the input data `X_train`, and corresponding
    target data `y`. The purpose of this method is to determine the number of misclassified
    points at the present state ![](img/e75323b6-725a-475c-9fe5-29dd9a0a45dc.png),
    if there are any, and return the total count of errors. The method can be defined
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification_error()`方法是一个辅助方法，接受当前的参数向量`w`、输入数据`X_train`和相应的目标数据`y`作为参数。该方法的目的是确定当前状态下误分类的点的数量（如果有的话），并返回错误的总数。该方法可以定义如下：'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This method could be simplified as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法可以简化如下：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: However, while this is a nice optimization for small datasets, for large datasets
    it may not be necessary to calculate all points for error. Thus, the first (and
    longer) method can be used and modified according to the type of data that is
    expected, and if we know that we will be dealing with large datasets, we could
    break out of the method at the first sign of error.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然这种优化方法对于小型数据集很有用，但对于大型数据集来说，可能没有必要计算所有点的误差。因此，第一种（且较长的）方法可以根据预期的数据类型进行使用和修改，如果我们知道将处理大型数据集，可以在第一次遇到误差时就跳出该方法。
- en: 'The second helper method in our code is `choose_miscl_point()`. The main purpose
    of this method is to select, at random, one of the misclassified points, if there
    are any. It takes as arguments the current vector of parameters `w`, the input
    data `X_train`, and corresponding target data `y`. It returns a misclassified
    point, `x`, and what the corresponding target sign should be, `s`. The method
    could be implemented as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代码中的第二个辅助方法是`choose_miscl_point()`。该方法的主要目的是随机选择一个被误分类的点（如果有的话）。它的参数包括当前的参数向量`w`、输入数据`X_train`和对应的目标数据`y`。它返回一个误分类点`x`以及该点对应的目标符号`s`。该方法可以按如下方式实现：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Similarly, this could be optimized for speed by randomizing a list of indices,
    iterating over them, and returning the first one found, as shown here:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，也可以通过随机化索引列表、遍历它们并返回第一个找到的点来优化速度，如下所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: However, the first implementation can be useful for absolute beginners or for
    those who would like to do some additional analysis of the misclassified points,
    which could be conveniently available in the list `mispts`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，第一种实现方式对绝对初学者或者那些想要对误分类点进行额外分析的人来说非常有用，这些误分类点可以方便地保存在`mispts`列表中。
- en: The crucial point, no matter the implementation, is to randomize the selection
    of the misclassified point.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 无论实现方式如何，关键点是要随机选择误分类的点。
- en: Finally, the update happens using the current parameters, the misclassified
    point, and the corresponding target on the line that executes `w = w + s*x`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，更新是通过使用当前参数、误分类点和对应的目标，通过执行`w = w + s*x`的方式完成的。
- en: 'If you run the complete program, it should output something like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行完整的程序，它应该输出类似这样的内容：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The total number of iterations may vary depending on the type of data and the
    random nature of the selection of the misclassified points. For the particular
    dataset we are using, the decision boundary could look as shown in *Figure 5.3*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总迭代次数可能会根据数据类型和误分类点选择的随机性质而有所不同。对于我们使用的特定数据集，决策边界可能看起来像*图 5.3*所示：
- en: '![](img/61a2413b-086d-4458-bab7-0433588aa938.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61a2413b-086d-4458-bab7-0433588aa938.png)'
- en: Figure 5.3 – Decision boundary found with PLA
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 使用 PLA 找到的决策边界
- en: The number of iterations will also depend on the separation or gap between data
    points in the feature space. The larger the gap is, the easier it is to find a
    solution, and the converse is also true. The worst-case scenario is when the data
    is non-linearly separable, which we'll address next.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数还将取决于特征空间中数据点之间的分隔或间隙。间隙越大，越容易找到解决方案，反之亦然。最坏的情况是当数据是非线性可分时，我们将在接下来讨论这一点。
- en: A perceptron over non-linearly separable data
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在非线性可分数据上的感知器
- en: As we have discussed before, a perceptron will find a solution in finite time
    if the data is separable. However, how many iterations it will take to find a
    solution depends on how close the groups are to each other in the feature space.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当数据是可分离时，感知器将在有限时间内找到解决方案。然而，找到解决方案所需的迭代次数取决于数据组在特征空间中彼此的距离。
- en: '**Convergence** is when the learning algorithm finds a solution or reaches
    a steady state that is acceptable to the designer of the learning model.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**收敛**是指学习算法找到一个解决方案，或者达到一个被学习模型设计者接受的稳定状态。'
- en: The following paragraphs will deal with convergence on different types of data: linearly separable
    and non-linearly separable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的段落将讨论在不同类型数据上的收敛性：线性可分和非线性可分。
- en: Convergence on linearly separable data
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对线性可分数据的收敛性
- en: 'For the particular dataset that we have been studying in this chapter, the
    separation between the two groups of data is a parameter that can be varied (this
    is usually a problem with real data). The parameter is `class_sep` and can take
    on a real number; for example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在本章中研究的特定数据集，两个数据组之间的分离度是一个可变参数（这通常是实际数据中的一个问题）。该参数是`class_sep`，可以取任意实数；例如：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This allows us to study how many iterations it takes, on average, for the perceptron
    algorithm to converge if we vary the separation parameter. The experiment can
    be designed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够研究当改变分隔参数时，感知机算法收敛所需的平均迭代次数。实验设计如下：
- en: 'We will vary the separation coefficient from large to small, recording the
    number of iterations it takes to converge: 2.0, 1.9, ..., 1.2, 1.1.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将分隔系数从大到小变化，记录其收敛所需的迭代次数：2.0、1.9、...、1.2、1.1。
- en: We will repeat this 1,000 times and record the average number of iterations
    and the corresponding standard deviation.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将重复进行此实验1000次，并记录平均迭代次数及其对应的标准差。
- en: 'Notice that we decided to run this experiment down to 1.1, since 1.0 already
    produces a non-linearly separable dataset. If we perform the experiment, we can
    record the results in a table and it will look like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们决定将实验运行到1.1，因为1.0已经生成了一个非线性可分的数据集。如果我们进行实验，我们可以将结果记录在表格中，结果如下：
- en: '| **Run** | **2.0** | **1.9** | **1.8** | **1.7** | **1.6** | **1.5** | **1.4**
    | **1.3** | **1.2** | **1.1** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **运行** | **2.0** | **1.9** | **1.8** | **1.7** | **1.6** | **1.5** | **1.4**
    | **1.3** | **1.2** | **1.1** |'
- en: '| 1 | 2 | 2 | 2 | 2 | 7 | 10 | 4 | 15 | 13 | 86 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 2 | 2 | 2 | 7 | 10 | 4 | 15 | 13 | 86 |'
- en: '| 2 | 5 | 1 | 2 | 2 | 4 | 8 | 6 | 26 | 62 | 169 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5 | 1 | 2 | 2 | 4 | 8 | 6 | 26 | 62 | 169 |'
- en: '| 3 | 4 | 4 | 5 | 6 | 6 | 10 | 11 | 29 | 27 | 293 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 4 | 5 | 6 | 6 | 10 | 11 | 29 | 27 | 293 |'
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
- en: '| 998 | 2 | 5 | 3 | 1 | 9 | 3 | 11 | 9 | 35 | 198 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 998 | 2 | 5 | 3 | 1 | 9 | 3 | 11 | 9 | 35 | 198 |'
- en: '| 999 | 2 | 2 | 4 | 7 | 6 | 8 | 2 | 4 | 14 | 135 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 999 | 2 | 2 | 4 | 7 | 6 | 8 | 2 | 4 | 14 | 135 |'
- en: '| 1000 | 2 | 1 | 2 | 2 | 2 | 8 | 13 | 25 | 27 | 36 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 2 | 1 | 2 | 2 | 2 | 8 | 13 | 25 | 27 | 36 |'
- en: '| **Avg.** | **2.79** | **3.05** | **3.34** | **3.67** | **4.13** | **4.90**
    | **6.67** | **10.32** | **24.22** | **184.41** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **平均值** | **2.79** | **3.05** | **3.34** | **3.67** | **4.13** | **4.90**
    | **6.67** | **10.32** | **24.22** | **184.41** |'
- en: '| **Std.** | **1.2** | **1.3** | **1.6** | **1.9** | **2.4** | **3.0** | **4.7**
    | **7.8** | **15.9** | **75.5** |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | **1.2** | **1.3** | **1.6** | **1.9** | **2.4** | **3.0** | **4.7**
    | **7.8** | **15.9** | **75.5** |'
- en: 'This table shows that the average number of iterations taken is fairly stable
    when the data is nicely separated; however, as the separation gap is reduced,
    the number of iterations increases dramatically. To put this in a visual perspective,
    the same data from the table is now shown in *Figure 5.4* on a logarithmic scale:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该表格显示，当数据分隔得很好时，迭代次数的平均值相对稳定；然而，当分隔间隔减小时，迭代次数显著增加。为了更直观地展示这一点，表格中的相同数据现在以*图5.4*的对数刻度形式展示：
- en: '![](img/6f74dcd4-9806-4fd8-b92b-035c1de18d94.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f74dcd4-9806-4fd8-b92b-035c1de18d94.png)'
- en: Figure 5.4 – The growth of the number of PLA iterations as the data groups are
    closer together
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 - 当数据组靠得越来越近时，PLA迭代次数的增长
- en: 'It is very clear that the number of iterations can grow exponentially as the
    separation gap is closing. *Figure 5.5* depicts the largest separation gap, 2.0,
    and indicates that the PLA found a solution after four iterations:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，当分隔间隔变小的时候，迭代次数会呈指数级增长。*图5.5*显示了最大的分隔间隔2.0，并且表明PLA在四次迭代后找到了一个解：
- en: '![](img/5364997e-8957-4e62-9ba5-d183b96c0dec.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5364997e-8957-4e62-9ba5-d183b96c0dec.png)'
- en: Figure 5.5 – The perceptron found a solution in four iterations for a separation
    gap of 2.0
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 - 感知机在分隔间隔为2.0时，找到了解，迭代了四次。
- en: 'Similarly, *Figure 5.6* shows that for the largest gap, 1.1, the PLA takes
    183 iterations; a close inspection of the figure reveals that the solution for
    the latter case is difficult to find because the data groups are too close to
    each other:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，*图5.6*显示，当分隔间隔最大时，1.1，PLA需要183次迭代；仔细查看图形可以发现，后者的解比较难找到，因为数据组之间的距离太近：
- en: '![](img/cd93c7a8-7a43-491c-b7d6-f61bcbc8b8ef.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd93c7a8-7a43-491c-b7d6-f61bcbc8b8ef.png)'
- en: Figure 5.6 – The perceptron found a solution in 183 iterations for a separation
    gap of 1.1
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 - 感知机在分隔间隔为1.1时，找到了一个解，迭代了183次。
- en: As noted before, data that is not linearly separable can be produced with a
    gap of 1.0 and the PLA will run in an infinite loop since there will always be
    a data point that will be incorrectly classified and the `classification_error()`
    method will never return a zero value. For those cases, we can modify the PLA
    to allow finding solutions on non-linearly separable data, as we'll cover in the
    next section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，非线性可分的数据可以通过设置 1.0 的间隔来生成，PLA 将进入无限循环，因为总会有一个数据点被错误分类，而 `classification_error()`
    方法将永远不会返回零值。对于这种情况，我们可以修改 PLA 以允许在非线性可分的数据上找到解决方案，接下来的部分将讨论这一点。
- en: Convergence on non-linearly separable data
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对非线性可分数据的收敛
- en: 'The modifications to the original PLA are rather simple, but are good enough
    to allow finding an acceptable solution in most cases. The main two things that
    we need to add to the PLA are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始 PLA 的修改相当简单，但足以在大多数情况下找到一个可接受的解决方案。我们需要向 PLA 添加的主要两项内容如下：
- en: A mechanism to prevent the algorithm from running forever
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止算法永远运行下去的机制
- en: A mechanism to store the best solution ever found
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储最佳解决方案的机制
- en: With respect to the first point, we can simply specify a number of iterations
    at which the algorithm can stop. With respect to the second point, we can simply
    keep a solution in storage, and compare it to the one in the current iteration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第一点，我们可以简单地指定一个算法停止的迭代次数。关于第二点，我们可以简单地将一个解决方案保存在存储中，并将其与当前迭代的解决方案进行比较。
- en: 'The relevant portion of the PLA is shown here and the new changes have been
    marked with bold font and will be discussed in detail:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了 PLA 的相关部分，新的变化已用粗体标出，稍后将进行详细讨论：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this code, `bestW` is a dictionary for keeping track of the best results
    so far, and it is initialized to reasonable values. Notice first that the loop
    is now bounded by the number 1,000, which is the maximum number of iterations
    you currently allow and you can change it to anything you desire to be the maximum
    number of allowed iterations. It would be reasonable to reduce this number for
    large datasets or high-dimensional datasets where every iteration is costly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，`bestW` 是一个字典，用来追踪到目前为止的最佳结果，并且它被初始化为合理的值。首先需要注意的是，循环现在被数字 1,000 限制，这是你当前允许的最大迭代次数，你可以根据需要将其更改为任何你想要的最大迭代次数。对于大数据集或高维数据集，每次迭代的代价较高时，减少这个数字是合理的。
- en: The next changes are the inclusion of the conditional statement, `if err < bestW['err']`,
    which determines whether we should store a new set of parameters. Every time the
    error, as determined by the total number of misclassified examples, is lower than
    the error of the stored parameters, then an update is made. And just for completion,
    we have to still check that there are no errors, which indicates that the data
    is linearly separable, a solution has been found, and the loop needs to terminate.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的变化是加入了条件语句 `if err < bestW['err']`，它决定了是否应该存储一组新的参数。每当通过总误分类样本数确定的误差低于存储参数的误差时，便会进行更新。为了完整性，我们还需要检查是否存在误差，这意味着数据是线性可分的，已找到解决方案，循环需要终止。
- en: 'The last few `print` statements will simply inform the iteration and error
    obtained when the best solution was recorded. The output may look as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几个 `print` 语句将仅仅输出记录最佳解决方案时的迭代次数和误差。输出可能如下所示：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This output was produced by running the updated PLA over the dataset with a
    separation of 1.0, which is depicted in *Figure 5.7*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出是通过在数据集上运行更新后的 PLA 生成的，数据集的分隔度为 1.0，如*图 5.7*所示：
- en: '![](img/2e1b5cd4-6720-4c62-bd35-ec5bdf23c214.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e1b5cd4-6720-4c62-bd35-ec5bdf23c214.png)'
- en: Figure 5.7 – The updated PLA finds a solution with only one misclassified point
    after 95 iterations
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 更新后的 PLA 在 95 次迭代后找到一个解决方案，只有一个误分类点
- en: From the figure, it can be seen that there is one sample from the positive class
    that is incorrectly classified. Knowing that in this example there is a total
    of 100 data points, we can determine that the accuracy is 99/100.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看到，正类中有一个样本被错误分类。知道在这个例子中总共有 100 个数据点，我们可以确定准确率为 99/100。
- en: This type of algorithm, which stores the *best solution so far*, is usually
    known as a **pocket algorithm** (Muselli, M. 1997). And the idea of the early
    termination of a learning algorithm is inspired by well-known numerical optimization
    methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种存储*迄今为止最佳解决方案*的算法，通常被称为**口袋算法**（Muselli，M. 1997）。而学习算法的提前终止思想则受到著名数值优化方法的启发。
- en: One of the general limitations is that the perceptron can only produce solutions
    that are based on a line in two dimensions, or a linear hyperplane in multiple
    dimensions. However, this limitation can be easily solved by putting several perceptrons
    together and in multiple layers to produce highly complex non-linear solutions
    for separable and non-separable problems. This will be the topic of the next chapter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个普遍的限制是感知器只能生成基于二维空间中的一条直线或多维空间中的一个线性超平面的解决方案。然而，这个限制可以通过将多个感知器组合在一起，并放置在多个层中，轻松解决，从而为可分离和不可分离的问题生成高度复杂的非线性解决方案。这将是下一章的主题。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented an overview of the classic perceptron model. We covered
    the theoretical model and its implementation in Python for both linearly and non-linearly
    separable datasets. At this point, you should feel confident that you know enough
    about the perceptron that you can implement it yourself. You should be able to
    recognize the perceptron model in the context of a neuron. Also, you should now
    be able to implement a pocket algorithm and early termination strategies in a
    perceptron, or any other learning algorithm in general.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了经典感知器模型的概述。我们讨论了理论模型及其在Python中的实现，适用于线性和非线性可分数据集。到此为止，你应该足够自信，能够自己实现感知器。你应该能够在神经元的上下文中识别感知器模型。此外，你现在应该能够在感知器或任何其他学习算法中实现口袋算法和提前终止策略。
- en: Since the perceptron is the most essential element that paved the way for deep
    neural networks, after we have covered it here, the next step is to go to [Chapter
    6](a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml), *Training Multiple Layers of Neurons.* In
    that chapter, you will be exposed to the challenges of deep learning using the
    multi-layer perceptron algorithm, such as gradient descent techniques for error
    minimization, and hyperparameter optimization to achieve generalization. But before
    you go there, please try to quiz yourself with the following questions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于感知器是为深度神经网络铺平道路的最基本元素，在我们在此介绍了它之后，下一步是进入[第六章](a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml)，*训练多层神经元*。在这一章中，你将接触到使用多层感知器算法进行深度学习的挑战，比如用于误差最小化的梯度下降技术，以及超参数优化以实现泛化。但在你前往那里之前，请尝试通过以下问题进行自我测试。
- en: Questions and answers
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问答
- en: '**What is the relationship between the separability of the data and the number
    of iterations of the PLA?**'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据的可分性与PLA迭代次数之间有什么关系？**'
- en: The number of iterations can grow exponentially as the data groups get close
    to one another.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据组之间的接近，迭代次数可能呈指数增长。
- en: '**Will the PLA always converge? **'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PLA会一直收敛吗？**'
- en: Not always, only for linearly separable data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 并不总是，只有在数据是线性可分的情况下。
- en: '**Can the PLA converge on non-linearly separable data?**'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PLA可以在非线性可分数据上收敛吗？**'
- en: No. However, you can find an acceptable solution by modifying it with the pocket
    algorithm, for example.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不行。然而，你可以通过修改它，使用例如口袋算法，来找到一个可接受的解决方案。
- en: '**Why is the perceptron important?**'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么感知器如此重要？**'
- en: Because it is one of the most fundamental learning strategies that has helped
    conceive the possibility of learning. Without the perceptron, it could have taken
    longer for the scientific community to realize the potential of computer-based
    automatic learning algorithms.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它是最基本的学习策略之一，帮助我们构思了学习的可能性。如果没有感知器，科学界可能需要更长时间才能意识到基于计算机的自动学习算法的潜力。
- en: References
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological review*, 65(6), 386.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological review*, 65(6), 386.'
- en: Muselli, M. (1997). On convergence properties of the pocket algorithm. *IEEE
    Transactions on Neural Networks*, 8(3), 623-629.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muselli, M. (1997). On convergence properties of the pocket algorithm. *IEEE
    Transactions on Neural Networks*, 8(3), 623-629.
