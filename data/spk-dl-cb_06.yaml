- en: Using LSTMs in Generative Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生成网络中使用LSTMs
- en: 'After reading this chapter, you will be able to accomplish the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将能够完成以下任务：
- en: Downloading novels/books that will be used as input text
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载将作为输入文本使用的小说/书籍
- en: Preparing and cleansing data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备和清理数据
- en: Tokenizing sentences
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子标记化
- en: Training and saving the LSTM model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练并保存LSTM模型
- en: Generating similar text using the model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型生成相似的文本
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Due to the drawbacks of **recurrent neural networks** (**RNNs**) when it comes
    to backpropagation, **Long Short-Term Memory Units** (**LSTMs**) and **Gated Recurrent
    Units** (**GRUs**) have been gaining popularity in recent times when it comes
    to learning sequential input data as they are better suited to tackle problems
    of vanishing and exploding gradients.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于**循环神经网络**（**RNNs**）在反向传播方面的缺点，**长短期记忆单元**（**LSTMs**）和**门控循环单元**（**GRUs**）近年来在学习序列输入数据时越来越受欢迎，因为它们更适合解决梯度消失和梯度爆炸的问题。
- en: Downloading novels/books that will be used as input text
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载将作为输入文本使用的小说/书籍
- en: In this recipe, we will go the steps that we need to download the novels/books
    which we will use as input text for the execution of this recipe.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将按步骤进行操作，下载我们将作为输入文本使用的小说/书籍，以执行本食谱。
- en: Getting ready
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Place the input data in the form of a `.txt` file in the working directory.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入数据以`.txt`文件的形式放入工作目录。
- en: The input may be any kind of text, such as song lyrics, novels, magazine articles,
    and source code.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入可以是任何类型的文本，如歌词、小说、杂志文章或源代码。
- en: Most of the classical texts are no longer protected by copyright and may be
    downloaded for free and used in experiments. The best place to get access to free
    books is Project [Gutenberg](http://www.gutenberg.org/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数经典文本不再受版权保护，可以免费下载安装并用于实验。获取免费书籍的最佳地方是[古腾堡计划](http://www.gutenberg.org/)。
- en: 'In this chapter, we will be using *The Jungle book* by Rudyard Kipling as the
    input to train our model and generate statistically similar text as output. The
    following screenshot shows you how to download the necessary file in `.txt` format:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用**鲁德亚德·吉卜林**的《丛林之书》作为输入，训练我们的模型并生成统计上相似的文本作为输出。以下截图显示了如何下载必要的`.txt`格式文件：
- en: '![](img/cefd5b72-ded3-45fc-8617-3ac54c0ca5e8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cefd5b72-ded3-45fc-8617-3ac54c0ca5e8.png)'
- en: After visiting the website and searching for the required book, click on Plain
    Text UTF-8 and download it. UTF-8 basically specifies the type of encoding. The
    text may be copied and pasted or saved directly to the working directory by clicking
    on the link.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问网站并搜索所需的书籍后，点击Plain Text UTF-8并下载。UTF-8基本上指定了编码类型。文本可以复制粘贴或直接保存到工作目录，方法是点击链接。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Before beginning, it always helps to take a look at the data and analyze it.
    After looking at the data, we can see that there are a lot of punctuation marks,
    blank spaces, quotes, and uppercase as well as lowercase letters. We need to prepare
    the data first before performing any kind of analysis on it or feeding it into
    the LSTM network. We require a number of libraries that will make handling data
    easier :'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，查看并分析数据总是有帮助的。通过查看数据，我们可以看到数据中有很多标点符号、空格、引号、大写字母和小写字母。在对数据进行任何分析或将其输入LSTM网络之前，我们需要先准备数据。我们需要一些库来简化数据的处理：
- en: 'Import the necessary libraries by issuing the following commands:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令导入必要的库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output to the preceding commands looks like the following screenshot:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出如下截图所示：
- en: '![](img/7cf320aa-709e-4ec0-9387-3072025116e8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cf320aa-709e-4ec0-9387-3072025116e8.png)'
- en: 'It is always a good idea to double check the current working directory and
    choose the required folder as the working directory. In our case, the `.txt` file
    is named `junglebook.txt` and is held in the folder named `Chapter 8`. So, we
    will select that folder as the working directory for the whole chapter. This may
    be done as shown in the following screenshot:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总是检查当前工作目录并选择所需的文件夹作为工作目录是个好主意。在我们的例子中，`.txt`文件名为`junglebook.txt`，它位于名为`Chapter
    8`的文件夹中。因此，我们将选择该文件夹作为整个章节的工作目录。这可以通过以下截图所示的方式完成：
- en: '![](img/213ed09c-fcfd-4047-984d-dfd893747cc1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/213ed09c-fcfd-4047-984d-dfd893747cc1.png)'
- en: 'Next, load the file into the program''s memory by defining a function named
    `load_document`, which can be done by issuing the following commands:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过定义名为`load_document`的函数将文件加载到程序内存中，可以通过执行以下命令完成此操作：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the previously defined function to load the document into memory and print
    the first `2000` characters of the text file using the following script:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用先前定义的函数将文档加载到内存中，并使用以下脚本打印文本文件的前`2000`个字符：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Running the preceding function as well as the commands produces the output 
    shown in the following screenshots:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前述函数以及命令会产生以下截图所示的输出：
- en: '![](img/5d383a5a-8337-473b-ad24-362275e51ff0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d383a5a-8337-473b-ad24-362275e51ff0.png)'
- en: 'The output to the above code is shown in the screenshot here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出显示在这里的截图中：
- en: '![](img/0437d675-32ce-47d2-8cd6-638188e83b29.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0437d675-32ce-47d2-8cd6-638188e83b29.png)'
- en: 'The following screenshot is a continuation of the previous output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是之前输出的继续：
- en: '![](img/0b7dafab-2a29-4a29-8081-00fb977e0a41.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b7dafab-2a29-4a29-8081-00fb977e0a41.png)'
- en: As seen in the preceding screenshots, the first `2000` characters from the `.txt`
    file are printed. It is always a good idea to analyze the data by looking at it
    before performing any preprocessing on it. It will give a better idea of how to
    approach the preprocessing steps.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前面截图所示，`.txt`文件中的前`2000`个字符被打印出来。通常，分析数据时先查看它的内容是一个好主意，这样可以更好地了解如何进行后续的预处理步骤。
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: The `array` function will be used to handle data in the form of arrays. The
    `numpy` library provides this function readily.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`array`函数将用于处理以数组形式表示的数据。`numpy`库提供了这个函数。'
- en: Since our data is only text data, we will require the string library to handle
    all input data as strings before encoding the words as integers, which can be
    fed.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的数据仅为文本数据，我们将需要字符串库来处理所有输入数据为字符串形式，然后将单词编码为整数，便于输入。
- en: The `tokenizer` function will be used to split all the sentences into tokens,
    where each token represents a word.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenizer`函数将用于将所有句子拆分为标记，其中每个标记表示一个单词。'
- en: The pickle library will be required in order to save the dictionary into a pickle
    file by using the `dump` function.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将字典保存到 pickle 文件中，需要使用`dump`函数，因此需要 pickle 库。
- en: The `to_categorical` function from the `keras` library converts a class vector
    (integers) to a binary class matrix, for example, for use with `categorical_crossentropy`,
    which we will require at a later stage in order to map tokens to unique integers
    and vice versa.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`keras`库中的`to_categorical`函数将类向量（整数）转换为二进制类矩阵，例如，用于`categorical_crossentropy`，我们在后面的步骤中将需要它，以便将标记映射到唯一的整数，并反向操作。'
- en: Some of the other Keras layers required in this chapter are the LSTM layer,
    dense layer, dropout layer, and the embedding layer. The model will be defined
    sequentially, for which we require the sequential model from the `keras` library.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章中所需的其他 Keras 层包括 LSTM 层、全连接层、丢弃层和嵌入层。模型将按顺序定义，因此我们需要从`keras`库中导入顺序模型。
- en: There's more...
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You may also use the same model with different types of texts, such as customer
    reviews on websites, tweets, structured text such as source code, mathematics
    theories, and so on.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以使用相同的模型处理不同类型的文本，如网站上的客户评价、推文、结构化文本（如源代码）、数学理论等。
- en: The idea of this chapter to understand how LSTMs learn long-term dependencies
    and how they perform better at processing sequential data when compared to recurrent
    neural networks.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的目的是理解 LSTM 如何学习长期依赖性，以及与循环神经网络相比，它们在处理顺序数据时如何表现得更好。
- en: Another good idea would be to input *Pokémon* names into the model and try to
    generate your own Pokémon names.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个好主意是将*宝可梦*的名称输入模型，并尝试生成你自己的宝可梦名称。
- en: See also
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'More information about the different libraries used can be found at the following
    links:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用的不同库的更多信息，请访问以下链接：
- en: '[https://www.scipy-lectures.org/intro/numpy/array_object.html](https://www.scipy-lectures.org/intro/numpy/array_object.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.scipy-lectures.org/intro/numpy/array_object.html](https://www.scipy-lectures.org/intro/numpy/array_object.html)'
- en: '[https://docs.python.org/2/library/string.html](https://docs.python.org/2/library/string.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/string.html](https://docs.python.org/2/library/string.html)'
- en: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
- en: '[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
- en: '[https://keras.io/layers/core/](https://keras.io/layers/core/)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/layers/core/](https://keras.io/layers/core/)'
- en: '[https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/layers/recurrent/](https://keras.io/layers/recurrent/)'
- en: Preparing and cleansing data
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的准备和清理
- en: This section of this chapter will discuss the various data preparation and text
    preprocessing steps involved before feeding it into the model as input. The specific
    way we prepare the data really depends on how we intend to model it, which in
    turn depends on how we intend to use it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节的这一部分将讨论在将数据输入模型之前的各种数据准备和文本预处理步骤。我们如何准备数据，实际上取决于我们打算如何建模，进而决定了我们如何使用它。
- en: Getting ready
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: The language model will be based on statistics and predict the probability of
    each word given an input sequence of text. The predicted word will be fed in as
    input to the model, to, in turn, generate the next word.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型将基于统计数据，预测给定输入文本序列的每个单词的概率。预测出的单词将作为输入反馈给模型，从而生成下一个单词。
- en: A key decision is how long the input sequences should be. They need to be long
    enough to allow the model to learn the context for the words to predict. This
    input length will also define the length of the seed text used to generate new
    sequences when we use the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键决策是输入序列的长度应该是多少。它们需要足够长，以便模型能够学习单词预测的上下文。这个输入长度还将决定在我们使用模型时，用于生成新序列的种子文本的长度。
- en: For the purpose of simplicity, we will arbitrarily pick a length of 50 words
    for the length of the input sequences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们将随意选择一个50个单词的长度作为输入序列的长度。
- en: How to do it...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Based on reviewing the text (which we did previously), the following are some
    operations that could be performed to clean and preprocess the text in the input
    file. We have presented a few options regarding text preprocessing. However, you
    may want to explore more cleaning operations as an exercise:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对文本的回顾（我们之前已进行过），以下是可以对输入文件中的文本进行清理和预处理的一些操作。我们已呈现了一些关于文本预处理的选项。然而，作为练习，你可能想要探索更多清理操作：
- en: Replace dashes `–` with whitespaces so you can split words better
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将破折号 `–` 替换为空格，以便更好地拆分单词
- en: Split words based on whitespaces
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据空格拆分单词
- en: Remove all punctuation from the input text in order to reduce the number of
    unique characters in the text that is fed into the model (for example, Why? becomes
    Why)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除输入文本中的所有标点符号，以减少输入模型的文本中唯一字符的数量（例如，Why? 变为 Why）
- en: Remove all words that are not alphabetic to remove standalone punctuation tokens
    and emoticons
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除所有非字母单词，以去除独立的标点符号标记和表情符号
- en: Convert all words from uppercase to lowercase in order to reduce the size of
    the total number of tokens further and remove any discrepancies and data redundancy
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有单词从大写转换为小写，以进一步减少标记总数的大小，并消除任何不一致和数据冗余
- en: 'Vocabulary size is a decisive factor in language modeling and deciding the
    training time for the model. A smaller vocabulary results in a more efficient
    model that trains faster. While it is good to have a small vocabulary in some
    cases, it helps to have a larger vocabulary in other cases in order to prevent
    overfitting. In order to preprocess the data, we are going to need a function
    that takes in the entire input text, splits it up based on white spaces, removes
    all punctuation, normalizes all cases, and returns a sequence of tokens. For this
    purpose, define the `clean_document` function by issuing the following commands:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表的大小是语言建模中的决定性因素，并且会影响模型的训练时间。较小的词汇表能使模型更高效，训练更快。虽然在某些情况下较小的词汇表更好，但在其他情况下，为了防止过拟合，较大的词汇表有其作用。为了预处理数据，我们需要一个函数，它接受整个输入文本，基于空格将其拆分，删除所有标点符号，规范化所有字母大小写，并返回一个标记序列。为此，定义
    `clean_document` 函数，执行以下命令：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The previously defined function will basically take the loaded document/file
    as its argument and return an array of clean tokens, as shown in the following
    screenshot:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前定义的函数基本上会将加载的文档/文件作为参数，并返回一个干净的标记数组，如以下截图所示：
- en: '![](img/7fc71d44-55e1-47bf-aa15-a10245e53dc9.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fc71d44-55e1-47bf-aa15-a10245e53dc9.png)'
- en: 'Next, print out some of the tokens and statistics just to develop a better
    understanding of what the `clean_document` function is doing. This step is done
    by issuing the following commands:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打印出一些标记和统计数据，以便更好地理解 `clean_document` 函数的作用。此步骤通过执行以下命令完成：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the preceding set of commands prints the first two hundred tokens
    and is as shown in the following screenshots:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令组的输出会打印前两百个标记，具体内容如以下截图所示：
- en: '![](img/70c89890-2b03-4307-8469-ab2ce81fb726.png)![](img/7ac2d64a-0cb2-4175-812d-a516e1c03953.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70c89890-2b03-4307-8469-ab2ce81fb726.png)![](img/7ac2d64a-0cb2-4175-812d-a516e1c03953.png)'
- en: 'Next, organize all these tokens into sequences, with each sequence containing
    50 words (chosen arbitrarily) using the following commands:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令将所有这些标记组织成序列，每个序列包含50个单词（随意选择）。
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The total number of sequences formed from the document may be viewed by printing
    them out, as shown in the following screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过打印文档生成的所有序列来查看总数，如下图所示：
- en: '![](img/0a5eb950-c902-485f-849e-adc3fd169a64.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a5eb950-c902-485f-849e-adc3fd169a64.png)'
- en: 'Save all the generated tokens as well as sequences into a file in the working
    directory by defining the `save_doc` function using the following commands:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，定义`save_doc`函数，将所有生成的标记以及序列保存到工作目录中的文件：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To save the sequences, use the following two commands:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存序列，请使用以下两个命令：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This process is illustrated in the following screenshot:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程在下图中有所示例：
- en: '![](img/a93679f6-9abc-479d-9718-1634ceb2092c.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a93679f6-9abc-479d-9718-1634ceb2092c.png)'
- en: 'Next, load the saved document, which contains all the saved tokens and sequences,
    into the memory using the `load_document` function, which is defined as follows:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`load_document`函数将保存的文档（包含所有保存的标记和序列）加载到内存中，`load_document`函数定义如下：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/b3dc8275-2494-4658-8aa6-5651f8259036.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3dc8275-2494-4658-8aa6-5651f8259036.png)'
- en: How it works...
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: The `clean_document` function removes all whitespaces, punctuation, uppercase
    text, and quotation marks, and splits the entire document into tokens, where each
    token is a word.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clean_document`函数删除所有空格、标点符号、大写字母和引号，并将整个文档分割成标记，每个标记是一个单词。'
- en: By printing the total number of tokens and total unique tokens in the document,
    we will note that the `clean_document` function generated 51,473 tokens, out of
    which 5,027 tokens (or words) are unique.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印文档中标记的总数和唯一标记的总数，我们会发现`clean_document`函数生成了51,473个标记，其中5,027个标记（或单词）是唯一的。
- en: The `save_document` function then saves all of these tokens as well as unique
    tokens which are required to generate our sequences of 50 words each. Note how,
    by looping through all the generated tokens, we are able to generate a long list
    of 51,422 sequences. These are the same sequences that will be used as input to
    train the language model.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save_document`函数将所有这些标记以及生成每个包含50个单词的序列所需的唯一标记进行保存。请注意，通过循环遍历所有生成的标记，我们能够生成一个包含51,422个序列的长列表。这些序列将作为输入用于训练语言模型。'
- en: Before training the model on all 51,422 sequences, it is always a good practice
    to save the tokens as well as sequences to file. Once saved, the file can be loaded
    back into the memory using the defined `load_document` function.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练所有51,422个序列之前，最好先将标记和序列保存到文件中。保存后，可以使用定义的`load_document`函数将文件重新加载到内存中。
- en: The sequences are organized as 50 input tokens and one output token (which means
    that there are 51 tokens per sequence). For predicting each output token, the
    previous 50 tokens will be used as the input to the model. We can do this by iterating
    over the list of tokens from token 51 onwards and taking the previous 50 tokens
    as a sequence, then repeating this process until the end of the list of all tokens.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列被组织为50个输入标记和一个输出标记（这意味着每个序列有51个标记）。为了预测每个输出标记，前50个标记将作为模型的输入。我们可以通过从第51个标记开始遍历标记列表，并将前50个标记作为一个序列来实现这一点，然后重复这个过程，直到所有标记的列表结束。
- en: See also
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Visit the following links for a better understanding of data preparation using
    various functions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下链接，了解使用各种函数进行数据准备的更好方法：
- en: '[https://docs.python.org/3/library/tokenize.html](https://docs.python.org/3/library/tokenize.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/3/library/tokenize.html](https://docs.python.org/3/library/tokenize.html)'
- en: '[https://keras.io/utils/](https://keras.io/utils/)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/utils/](https://keras.io/utils/)'
- en: '[http://www.pythonforbeginners.com/dictionary/python-split](http://www.pythonforbeginners.com/dictionary/python-split)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.pythonforbeginners.com/dictionary/python-split](http://www.pythonforbeginners.com/dictionary/python-split)'
- en: '[https://www.tutorialspoint.com/python/string_join.htm](https://www.tutorialspoint.com/python/string_join.htm)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tutorialspoint.com/python/string_join.htm](https://www.tutorialspoint.com/python/string_join.htm)'
- en: '[https://www.tutorialspoint.com/python/string_lower.htm](https://www.tutorialspoint.com/python/string_lower.htm)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tutorialspoint.com/python/string_lower.htm](https://www.tutorialspoint.com/python/string_lower.htm)'
- en: Tokenizing sentences
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对句子进行标记化
- en: Before defining and feeding data into an LSTM network it is important that the
    data is converted into a form which can be understood by the neural network. Computers
    understand everything in binary code (0s and 1s) and therefore, the textual or
    data in string format needs to be converted into one hot encoded variables.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义并将数据输入到 LSTM 网络之前，重要的是将数据转换为神经网络能够理解的形式。计算机理解所有内容都是二进制代码（0 和 1），因此文本或字符串格式的数据需要转换为一热编码变量。
- en: Getting ready
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'For understanding how one hot encoding works, visit the following links:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解一热编码如何工作，请访问以下链接：
- en: '[https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)'
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)'
- en: '[https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python](https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python](https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python)'
- en: '[https://www.ritchieng.com/machinelearning-one-hot-encoding/](https://www.ritchieng.com/machinelearning-one-hot-encoding/)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.ritchieng.com/machinelearning-one-hot-encoding/](https://www.ritchieng.com/machinelearning-one-hot-encoding/)'
- en: '[https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)'
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'After the going through the previous section you should be able to clean the
    entire corpus and split up sentences. The next steps which involve one hot encoding
    and tokenizing sentences can be done in the following manner:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的部分后，你应该能够清理整个语料库并分割句子。接下来的步骤，涉及一热编码和句子标记化，可以按照以下方式进行：
- en: Once the tokens and sequences are saved to a file and loaded into memory, they
    have to be encoded as integers since the word embedding layer in the model expects
    input sequences to be comprised of integers and not strings.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦标记和序列被保存到文件并加载到内存中，它们必须被编码为整数，因为模型中的词嵌入层期望输入序列由整数而非字符串组成。
- en: This is done by mapping each word in the vocabulary to a unique integer and
    encoding the input sequences. Later, while making predictions, the predictions
    can be converted (or mapped) back to numbers to look up their associated words
    in the same mapping and reverse map back from integers to words.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是通过将词汇表中的每个单词映射到一个唯一的整数并对输入序列进行编码来实现的。稍后，在进行预测时，可以将预测结果转换（或映射）回数字，以便在相同的映射中查找相关单词，并将其从整数反向映射回单词。
- en: 'To perform this encoding, utilize the `Tokenizer` class in the Keras API. Before
    encoding, the tokenizer must be trained on the entire dataset so it finds all
    the unique tokens and assigns each token a unique integer. The commands to do
    so as are  follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行这种编码，利用 Keras API 中的`Tokenizer`类。在编码之前，必须先对整个数据集进行训练，以便它能找到所有独特的标记，并为每个标记分配一个独特的整数。执行这些命令的方法如下：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You also need to calculate the size of the vocabulary before defining the embedding
    layer later. This is determined by calculating the size of the mapping dictionary.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还需要在稍后定义嵌入层之前计算词汇表的大小。这是通过计算映射字典的大小来确定的。
- en: 'Therefore, when specifying the vocabulary size to the Embedding layer, specify
    it as 1 larger than the actual vocabulary. The vocabulary size is therefore defined
    as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，在指定嵌入层的词汇表大小时，应该将其指定为比实际词汇表大 1。词汇表大小因此定义如下：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that once the input sequences have been encoded, they need to be separated
    into input and output elements, which can be done by array slicing.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦输入序列被编码，它们需要被分成输入和输出元素，这可以通过数组切片来实现。
- en: After separating, one hot encode the output word. This means converting it from
    an integer to an n-dimensional vector of 0 values, one for each word in the vocabulary,
    with a 1 to indicate the specific word at the index of the word's integer value. Keras
    provides the `to_categorical()` function, which can be used to one hot encode
    the output words for each input-output sequence pair.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离后，将输出单词进行独热编码。这意味着将其从整数转换为一个n维向量，其中每个维度对应词汇表中的一个单词，并且用1表示该单词在词汇表中整数值对应的索引。Keras提供了`to_categorical()`函数，可以用来为每个输入-输出序列对对输出单词进行独热编码。
- en: Finally, specify to the Embedding layer how long input sequences are. We know
    that there are 50 words because the model was designed by specifying the sequence
    length as 50, but a good generic way to specify the sequence length is to use
    the second dimension (number of columns) of the input data’s shape.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，指定给嵌入层输入序列的长度。我们知道有50个单词，因为模型是通过指定序列长度为50来设计的，但一种好的通用方式是使用输入数据形状的第二维（列数）来指定序列长度。
- en: 'This can be done by issuing the following commands:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可以通过执行以下命令来完成：
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This section will describe the outputs you must see on executing the commands
    in the previous section:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述您在执行前一节命令时必须看到的输出：
- en: 'After running the commands for tokenizing the sentences and calculating vocabulary
    length you must see an output as shown in the following screenshot:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行分词命令并计算词汇表长度后，您必须看到如下所示的输出：
- en: '![](img/fc6fb70e-e203-4bbe-9352-b1f710c3cb26.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc6fb70e-e203-4bbe-9352-b1f710c3cb26.png)'
- en: Words are assigned values starting from 1 up to the total number of words (for
    example, 5,027 in this case). The Embedding layer needs to allocate a vector representation
    for each word in this vocabulary from index 1 to the largest index. The index
    of the word at the end of the vocabulary will be 5,027; that means the array must
    be 5,027 + 1 in length.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词从1开始分配值，直到总词数（例如，本例中为5,027）。嵌入层需要为词汇表中从索引1到最大索引的每个单词分配一个向量表示。词汇表末尾的单词的索引将是5,027；这意味着数组的长度必须是5,027
    + 1。
- en: 'The output after array slicing and separating sentences into sequences of 50
    words per sequence must look like the following screenshot:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数组切片和将句子分割成每个序列50个单词的输出应如下所示：
- en: '![](img/a23d1d44-9eef-43c6-aed7-84cd516d9229.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a23d1d44-9eef-43c6-aed7-84cd516d9229.png)'
- en: The `to_categorical()` function is used so that the model learns to predict
    the probability distribution for the next word.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`to_categorical()`函数用于让模型学习预测下一个单词的概率分布。'
- en: There's more...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'More information on reshaping arrays in Python can be found at the following
    links:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在Python中重塑数组的更多信息，请参阅以下链接：
- en: '[https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)'
- en: '[https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/](https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/](https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/)'
- en: Training and saving the LSTM model
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练并保存LSTM模型
- en: You can now train a statistical language model from the prepared data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以从准备好的数据中训练一个统计语言模型。
- en: 'The model that will be trained is a neural language model. It has a few unique
    characteristics:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将要训练的模型是一个神经语言模型。它有一些独特的特点：
- en: It uses a distributed representation for words so that different words with
    similar meanings will have a similar representation
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用分布式表示法来表示单词，这样具有相似含义的不同单词将具有相似的表示。
- en: It learns the representation at the same time as learning the model
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在学习模型的同时学习表示
- en: It learns to predict the probability for the next word using the context of
    the previous 50 words
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它学会使用前50个单词的上下文来预测下一个单词的概率
- en: Specifically, you will use an Embedding Layer to learn the representation of
    words, and a **Long Short-Term Memory** (**LSTM**) recurrent neural network to
    learn to predict words based on their context.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，您将使用一个嵌入层来学习单词的表示，并使用**长短期记忆**（**LSTM**）递归神经网络来学习基于上下文预测单词。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在准备
- en: The learned embedding needs to know the size of the vocabulary and the length
    of input sequences as previously discussed. It also has a parameter to specify
    how many dimensions will be used to represent each word. That is the size of the
    embedding vector space.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的嵌入需要知道词汇表的大小和输入序列的长度，如前面所述。它还有一个参数，用来指定将使用多少维度来表示每个词。即嵌入向量空间的大小。
- en: Common values are 50, 100, and 300\. We will use 100 here, but consider testing
    smaller or larger values and evaluating metrics for those values.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的值为50、100和300。我们这里使用100，但可以考虑测试更小或更大的值，并评估这些值的指标。
- en: 'The network will be comprised of the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将包含以下内容：
- en: Two LSTM hidden layers with 200 memory cells each. More memory cells and a deeper
    network may achieve better results.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个LSTM隐藏层，每个隐藏层有200个记忆单元。更多的记忆单元和更深的网络可能会获得更好的结果。
- en: A dropout layer with a dropout of 0.3 or 30%, which will aid the network to
    depend less on each neuron/unit and reduce overfitting the data.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个丢弃层（dropout），丢弃率为0.3或30%，有助于网络减少对每个神经元/单元的依赖，减少过拟合数据。
- en: A dense fully connected layer with 200 neurons connects to the LSTM hidden layers
    to interpret the features extracted from the sequence.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含200个神经元的密集全连接层连接到LSTM隐藏层，用于解释从序列中提取的特征。
- en: The output layer, which predicts the next word as a single vector of the size
    of the vocabulary with a probability for each word in the vocabulary.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层，它预测下一个词作为一个大小为词汇表的向量，并为词汇表中的每个词分配一个概率。
- en: A softmax classifier is used in the second dense or fully connected layer to
    ensure the outputs have the characteristics of normalized probabilities (such
    as between 0 and 1).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个密集或全连接层使用softmax分类器，以确保输出具有归一化概率的特征（例如在0和1之间）。
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The model is defined using the following commands and is also illustrated in
    the following screenshot:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型通过以下命令定义，并在下方截图中进行说明：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/89b30c21-36b6-4d4e-803a-eb80c3dcb512.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89b30c21-36b6-4d4e-803a-eb80c3dcb512.png)'
- en: Print the model summary just to ensure that the model is constructed as intended.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型摘要，仅确保模型按照预期构建。
- en: 'Compile the model, specifying the categorical cross entropy loss needed to
    fit the model. The number of epochs is set to 75 and the model is trained in mini
    batches with a batch size of 250\. This is done using the following commands:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型，指定需要的类别交叉熵损失以适配模型。将训练的周期数设为75，并且使用批量大小为250的小批量训练。这是通过以下命令实现的：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding commands is illustrated in the following screenshot:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面命令的输出结果如以下截图所示：
- en: '![](img/a495a034-ddc3-4bb8-a358-30d6a7f422f9.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a495a034-ddc3-4bb8-a358-30d6a7f422f9.png)'
- en: 'Once the model is done compiling, it is saved using the following commands:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型编译完成，使用以下命令将其保存：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/78e7b73d-812f-46db-8f49-10f95283d075.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78e7b73d-812f-46db-8f49-10f95283d075.png)'
- en: How it works...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The model is built using the `Sequential()` function in the Keras framework.
    The first layer in the model is an embedding layer that takes in the vocabulary
    size, vector dimension, and the input sequence length as its arguments.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型是在Keras框架中使用`Sequential()`函数构建的。模型中的第一层是一个嵌入层，它将词汇表的大小、向量维度和输入序列长度作为参数。
- en: The next two layers are LSTM layers with 200 memory cells each. More memory
    cells and a deeper network can be experimented with to check if it improves accuracy.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的两层是LSTM层，每层有200个记忆单元。可以尝试更多的记忆单元和更深的网络，检查是否能够提高准确性。
- en: The next layer is a dropout layer with a dropout probability of 30%, which means
    that there is a 30% chance a certain memory unit is not used during training.
    This prevents overfitting of data. Again, the dropout probabilities can be played
    with and tuned accordingly.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个层是一个丢弃层，丢弃概率为30%，这意味着在训练过程中有30%的机会某个记忆单元不被使用。这样可以防止数据过拟合。同样，丢弃概率可以进行调整和优化。
- en: The final two layers are two fully connected layers. The first one has a `relu`
    activation function and the second has a softmax classifier. The model summary
    is printed to check whether the model is built according to requirements.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的两层是两个全连接层。第一层具有`relu`激活函数，第二层是softmax分类器。打印模型摘要以检查模型是否按照要求构建。
- en: Notice that in this case, the total number of trainable parameters are 2,115,228\.
    The model summary also shows the number of parameters that will be trained by
    each layer in the model.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，总的可训练参数数量为2,115,228。模型摘要还显示了每一层将在模型中训练的参数数量。
- en: The model is trained in mini batches of 250 over 75 epochs, in our case, to
    minimize training time. Increasing the number of epochs to over 100 and utilizing
    smaller batches while training greatly improves the model's accuracy while simultaneously
    reducing loss.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型以250的迷你批次，在75个周期内进行训练，以最小化训练时间。在我们的案例中，将周期数增加到100以上，并在训练时使用较小的批次，能显著提高模型的准确度，同时减少损失。
- en: During training, you will see a summary of performance, including the loss and
    accuracy evaluated from the training data at the end of each batch update. In
    our case, after running the model for 75 epochs, we obtained an accuracy of close
    to 40%.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，你将看到性能摘要，包括每个批次更新结束时根据训练数据评估的损失和准确度。在我们的案例中，经过75个周期后，我们获得了接近40%的准确率。
- en: The aim of the model is not to remember the text with 100% accuracy, but rather
    to capture the properties of the input text, such as long-term dependencies and
    structures that exist in natural language and sentences.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的目标不是以100%的准确率记住文本，而是捕捉输入文本的特性，比如在自然语言和句子中存在的长期依赖关系和结构。
- en: The model, after it is done training, is saved in the working directory named
    `junglebook_trained.h5`.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，模型会保存在名为`junglebook_trained.h5`的工作目录中。
- en: We also require the mapping of words to integers when the model is later loaded
    into memory to make predictions. This is present in the `Tokenizer` object, which
    is also saved using the `dump ()` function in the `Pickle` library.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要在以后将模型加载到内存中进行预测时，将单词映射到整数。这些信息存储在`Tokenizer`对象中，并通过`Pickle`库中的`dump()`函数进行保存。
- en: There's more...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Jason Brownlee''s blogs on Machine Learning Mastery have a lot of useful information
    on developing, training, and tuning machine learning models for natural language
    processing. They can be found at the following links:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Jason Brownlee在Machine Learning Mastery网站上的博客包含了大量有关开发、训练和调整用于自然语言处理的机器学习模型的有用信息。你可以通过以下链接访问它们：
- en: '[https://machinelearningmastery.com/deep-learning-for-nlp/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/deep-learning-for-nlp/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
- en: '[https://machinelearningmastery.com/lstms-with-python/](https://machinelearningmastery.com/lstms-with-python/)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/lstms-with-python/](https://machinelearningmastery.com/lstms-with-python/)'
- en: '[https://machinelearningmastery.com/blog/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/blog/](https://machinelearningmastery.com/deep-learning-for-nlp/)'
- en: See also
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Further information about different keras layers and other functions used in
    this section can be found at the following links:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的信息，关于不同的keras层和本节中使用的其他函数，可以通过以下链接找到：
- en: '[https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
- en: '[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)'
- en: '[https://keras.io/optimizers/](https://keras.io/optimizers/)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/optimizers/](https://keras.io/optimizers/)'
- en: '[https://keras.io/models/model/](https://keras.io/models/model/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/models/model/](https://keras.io/models/model/)'
- en: Generating similar text using the model
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型生成相似文本
- en: Now that you have a trained language model, it can be used. In this case, you
    can use it to generate new sequences of text that have the same statistical properties
    as the source text. This is not practical, at least not for this example, but
    it gives a concrete example of what the language model has learned.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了一个训练好的语言模型，可以开始使用它。在这个例子中，你可以用它生成具有与源文本相同统计特性的全新文本序列。虽然这在实际应用中并不实用（至少对于这个示例而言），但它提供了一个具体的例子，展示了语言模型学到的内容。
- en: Getting ready
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Begin by loading the training sequences again. You may do so by using the `load_document()`
    function, which we developed initially. This is done by using the following code:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新加载训练序列。你可以通过使用我们最初开发的`load_document()`函数来实现。代码如下：
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding code is illustrated in the following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下截图所示：
- en: '![](img/c88a273c-a0d6-4be1-8cbb-1b15be720983.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c88a273c-a0d6-4be1-8cbb-1b15be720983.png)'
- en: Note that the input filename is now `'junglebook_sequences.txt'`, which will
    load the saved training sequences into the memory. We need the text so that we
    can choose a source sequence as input to the model for generating a new sequence
    of text.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，输入文件名现在是`'junglebook_sequences.txt'`，它将把保存的训练序列加载到内存中。我们需要这些文本，以便选择一个源序列作为输入，供模型生成新的文本序列。
- en: The model will require 50 words as input.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将需要50个单词作为输入。
- en: 'Later, the expected length of input needs to be specified. This can be determined
    from the input sequences by calculating the length of one line of the loaded data
    and subtracting 1 for the expected output word that is also on the same line,
    as follows:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后续，需要指定期望的输入长度。这可以通过计算加载数据的一行的长度来确定，并减去1作为期望的输出单词（该单词也位于同一行），如下面所示：
- en: '`sequence_length = len(lines[0].split()) - 1`'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sequence_length = len(lines[0].split()) - 1`'
- en: 'Next, load the trained and saved model into memory by executing the following
    commands:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过执行以下命令将训练好的模型加载到内存中：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first step in generating text is preparing a seed input. Select a random
    line of text from the input text for this purpose. Once selected, print it so
    that you have some idea of what was used. This is done as follows:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成文本的第一步是准备一个种子输入。为此，从输入文本中选择一行随机文本。选择后，打印它，以便你了解所使用的内容。此操作如下所示：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/7b4a194c-a290-49eb-8e61-2d4f94174bdd.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b4a194c-a290-49eb-8e61-2d4f94174bdd.png)'
- en: How to do it...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'You are now ready to generate new words, one at a time. First, encode the seed
    text to integers using the same tokenizer that was used when training the model,
    which is done using the following code:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你已经准备好逐个生成新单词。首先，使用训练模型时所用的相同分词器将种子文本编码为整数，可以通过以下代码完成：
- en: '`encoded = tokenizer.texts_to_sequences([seed_text])[0]`'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`encoded = tokenizer.texts_to_sequences([seed_text])[0]`'
- en: '![](img/8eb637cf-8e57-41ef-a90f-bf076e3f1cdf.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8eb637cf-8e57-41ef-a90f-bf076e3f1cdf.png)'
- en: 'The model can predict the next word directly by calling `model.predict_classes()`,
    which will return the index of the word with the highest probability:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型可以通过调用`model.predict_classes()`直接预测下一个单词，该方法将返回具有最高概率的单词索引：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Look up the index in the Tokenizers mapping to get the associated word, as
    shown in the following code:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找 Tokenizers 映射中的索引，以获取相关的单词，如下面的代码所示：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Append this word to the seed text and repeat the process. Importantly, the
    input sequence is going to get too long. We can truncate it to the desired length
    after the input sequence has been encoded to integers. Keras provides the `pad_sequences()` function
    which we can use to perform this truncation, as follows:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此单词附加到种子文本并重复此过程。重要的是，输入序列将变得太长。我们可以在输入序列被编码为整数后，对其进行截断至所需长度。Keras 提供了`pad_sequences()`函数，可以用来执行此截断，如下所示：
- en: '[PRE20]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Wrap all of this into a function called `generate_sequence()` that takes as
    input the model, the tokenizer, the input sequence length, the seed text, and
    the number of words to generate. It then returns a sequence of words generated
    by the model. You may use the following code to do so:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有这些内容封装成一个名为`generate_sequence()`的函数，该函数接受模型、分词器、输入序列长度、种子文本和要生成的单词数作为输入。然后，它返回由模型生成的单词序列。你可以使用以下代码来实现：
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/e7d8b604-cebd-4406-bef0-97f6aed4fe00.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7d8b604-cebd-4406-bef0-97f6aed4fe00.png)'
- en: How it works...
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'We are now ready to generate a sequence of new words, given that we have some
    seed text :'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好生成新的单词序列，前提是我们有一些种子文本：
- en: 'Start by loading the model into memory again using the following command:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令再次将模型加载到内存中：
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, load the tokenizer by typing the following command:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过输入以下命令加载分词器：
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Select a seed text randomly by using the following command:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令随机选择一段种子文本：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, a new sequence is generated by using the following command:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过以下命令生成新的序列：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'On printing the generated sequence, you will see an output similar to the one
    shown in the following screenshot:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印生成的序列时，你将看到类似于以下截图的输出：
- en: '![](img/3d641bd0-01a3-47a7-aa6b-d66f83034217.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d641bd0-01a3-47a7-aa6b-d66f83034217.png)'
- en: 'The model first prints 50 words of the random seed text followed by 50 words
    of the generated text. In this case, the random seed text is as follows:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型首先打印出50个随机种子文本单词，然后是50个生成的文本单词。在这种情况下，随机种子文本如下所示：
- en: '*Baskets of dried grass and put grasshoppers in them or catch two praying mantises
    and make them fight or string a necklace of red and black jungle nuts or watch
    a lizard basking on a rock or a snake hunting a frog near the wallows then they
    sing long long songs*'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*篮子里装满了干草，并将蚂蚱放进去，或者抓住两只螳螂让它们打斗，或者串起一条红黑相间的丛林坚果项链，或者看着蜥蜴在岩石上晒太阳，或者看蛇在泥坑附近捕捉青蛙，然后他们唱着长长的歌。*'
- en: 'The 50 words of text generated by the model, in this case, are as follows:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型生成的50个单词的文本如下：
- en: '*with odd native quavers at the end of the review and the hyaena whom he had
    seen the truth they feel twitched to the noises round him for a picture of the
    end of the ravine and snuffing bitten and best of the bulls at the dawn is a native*'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*在评论的结尾带有奇怪的本地音调，以及他曾见证过真相的鬣狗，它们因周围的噪音而感到不安，仿佛看到峡谷尽头的画面，闻着被咬伤的味道，最好的公牛在黎明时分是本地的。*'
- en: Note how the model outputs a sequence of random words it generated based on
    what it learned from the input text. You will also notice that the model does
    a reasonably good job of mimicking the input text and generating its own stories.
    Though the text does not make much sense, it gives valuable insight into how the
    model learns to place statistically similar words next to each other.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意模型输出了它根据从输入文本中学到的内容生成的随机单词序列。你还会注意到，模型在模仿输入文本和生成自己的故事方面做得相当不错。尽管文本并不完全有意义，但它为我们提供了有价值的见解，说明了模型如何学习将统计上相似的单词排列在一起。
- en: There's more...
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Upon changing the random seed that was set, the output generated by the network
    also changes. You may not get the exact same output text as the preceding example,
    but it will be very similar to the input used to train the model.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改设置的随机种子后，网络生成的输出也会发生变化。你可能不会得到与前面示例完全相同的输出文本，但它会与用于训练模型的输入非常相似。
- en: 'The following are some screenshots of different results that were obtained
    by running the generated text piece multiple times:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是通过多次运行生成文本片段获得的不同结果的一些截图：
- en: '![](img/18257086-668e-4abb-9f15-4abb5826d268.png)![](img/8d7ad7ba-8b9b-4bd6-9b5b-cc3b54a721a9.png)![](img/beedf83f-1eb8-47cb-972f-fdc105c84b59.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18257086-668e-4abb-9f15-4abb5826d268.png)![](img/8d7ad7ba-8b9b-4bd6-9b5b-cc3b54a721a9.png)![](img/beedf83f-1eb8-47cb-972f-fdc105c84b59.png)'
- en: 'The model even generates its own version of the project Gutenberg license,
    as can be seen in the following screenshot:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型甚至生成了它自己版本的《古腾堡计划许可证》，如下所示的截图：
- en: '![](img/c954fc4c-15a4-4076-9d34-2e6512aa3827.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c954fc4c-15a4-4076-9d34-2e6512aa3827.png)'
- en: The model's accuracy can be improved to about 60% by increasing the number of
    epochs from about 100 to 200\. Another method to increase the learning is by training
    the model in mini batches of about 50 and 100\. Try to play around with the different
    hyperparameters and activation functions to see what affects the results in the
    best possible way.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将训练周期从大约100次增加到200次，模型的准确性可以提高到大约60%。另一种提高学习效果的方法是通过以大约50到100的迷你批次进行训练。尝试调整不同的超参数和激活函数，看看哪些对结果的影响最好。
- en: The model may also be made denser by including more LSTM and dropout layers
    while defining the model. However, know that it will only increase the training
    time if the model is more complex and runs over more epochs.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在定义模型时包含更多的LSTM层和丢弃层，模型也可以变得更密集。然而，要知道，如果模型更复杂并且训练周期更长，它将只会增加训练时间。
- en: After much experimentation, the ideal batch size was found to be between 50
    to 100, and the ideal number of epochs to train the model was determined to be
    between 100 and 200.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过多次实验，理想的批量大小被确定为50到100之间，训练模型的理想周期次数被确定为100到200之间。
- en: There is no definitive way of performing the preceding task. You can also experiment
    with different text inputs to the model such as tweets, customer reviews, or HTML
    code.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行上述任务没有确定的方式。你还可以尝试不同的文本输入，如推文、客户评论或HTML代码。
- en: Some of the other tasks that can be performed include using a simplified vocabulary
    (such as with all the stopwords removed) to further enhance the unique words in
    the dictionary; tuning the size of the embedding layer and the number of memory
    cells in the hidden layers; and extending the model to use a pre-trained model
    such as Google's Word2Vec (pre-trained word model) to see whether it results in
    a better model.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他可以执行的任务包括使用简化的词汇表（例如去除所有停用词）来进一步增强字典中的独特词汇；调节嵌入层的大小和隐藏层中记忆单元的数量；并扩展模型以使用预训练模型，如Google的Word2Vec（预训练词汇模型），以查看它是否能产生更好的模型。
- en: See also
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'More information about the various functions and libraries used in the final
    section of the chapter can be found by visiting the following links:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章最后部分使用的各种函数和库的更多信息，可以通过访问以下链接找到：
- en: '[https://keras.io/preprocessing/sequence/](https://keras.io/preprocessing/sequence/)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/preprocessing/sequence/](https://keras.io/preprocessing/sequence/)'
- en: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://wiki.python.org/moin/UsingPickle](https://wiki.python.org/moin/UsingPickle)'
- en: '[https://docs.python.org/2/library/random.html](https://docs.python.org/2/library/random.html)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.python.org/2/library/random.html](https://docs.python.org/2/library/random.html)'
- en: '[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)'
