- en: 3\. Deep Learning in Practice with TensorFlow 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 使用 TensorFlow 2 的深度学习实践
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will introduce you to TensorFlow and Keras and provide an overview
    of their key features and applications, as well as how they work in synergy. You
    will be able to implement a deep neural network with TensorFlow by addressing
    the main topics, from model creation, training, and validation, to testing. You
    will perform a regression task and solve a classification problem, thereby gaining
    hands-on experience with the frameworks. Finally, you will build and train a model
    to classify clothes images with high accuracy. By the end of this chapter, you
    will be able to design, build, and train deep learning models using the most advanced
    machine learning frameworks available.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 TensorFlow 和 Keras，并提供它们的主要功能和应用的概述，及其如何协同工作。通过本章，你将能够使用 TensorFlow 实现深度神经网络，涉及的主要主题包括模型创建、训练、验证和测试。你将完成一个回归任务并解决一个分类问题，从而获得使用这些框架的实践经验。最后，你将构建并训练一个模型，以高准确率分类衣物图像。到本章结束时，你将能够设计、构建并训练使用最先进的机器学习框架的深度学习模型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'In the previous chapter, we covered the theory behind Reinforcement Learning
    (RL), explaining topics such as Markov chains and Markov Decision Processes (MDPs),
    Bellman equations, and a number of techniques we can use to solve MDPs. In this
    chapter, we will be looking at deep learning methods, all of which will play a
    primary role in building approximate functions for reinforcement learning. Specifically,
    we will look at different families of deep neural networks: fully connected, convolutional,
    and recurrent networks. These algorithms have the key capability of encoding knowledge
    that''s been learned through examples in a compact and effective representation.
    In RL, they are typically used to approximate the so-called policy functions and
    value functions, which encode how the RL agent chooses its action, given the current
    state and the value associated with the current state, respectively. We will study
    the policy and value functions in the upcoming chapters.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讲解了强化学习（RL）背后的理论，涉及了马尔可夫链和马尔可夫决策过程（MDPs）、贝尔曼方程以及一些可以用来求解MDPs的技术。在本章中，我们将探讨深度学习方法，这些方法在构建强化学习的近似函数中起着关键作用。具体来说，我们将研究不同类型的深度神经网络：全连接网络、卷积网络和递归网络。这些算法具备一个关键能力，即通过实例学习编码知识，并以紧凑和有效的方式进行表示。在强化学习中，它们通常用于近似所谓的策略函数和值函数，分别用于表示强化学习代理如何在给定当前状态及与之相关的状态价值的情况下选择行动。我们将在接下来的章节中深入研究策略函数和值函数。
- en: '*Data is the new oil*: This famous quote is being heard more and more frequently
    these days, especially in tech and economic industries. With the great amount
    of data available today, techniques to leverage such enormous quantities of information,
    thereby creating value and opportunities, are becoming key competitive factors
    and skills to have. All products and platforms that are provided to users for
    free (from social networks to apps related to wearable devices) use data that
    is provided by the users to generate revenues: think about the huge quantity of
    information they collect every day relating to our habits, preferences, or even
    body weight trends. These provide high-value insights that can be leveraged by
    advertisers, insurance companies, and local businesses to improve their offers
    so that they fit the market.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据是新的石油*：这句名言如今越来越频繁地出现在科技和经济领域，特别是在技术和经济行业。随着今天可用的大量数据，如何利用这些庞大的信息量，从中创造价值和机会，已成为关键的竞争因素和必须掌握的技能。所有免费提供给用户的产品和平台（从社交网络到与可穿戴设备相关的应用）都利用用户提供的数据来创造收入：想想他们每天收集的关于我们的习惯、偏好，甚至体重趋势的庞大信息量。这些数据提供了高价值的见解，广告商、保险公司和本地企业可以利用这些数据来改进他们的产品和服务，使其更符合市场需求。'
- en: 'Thanks to the relevant increase in computational power availability and theory
    breakthroughs such as backpropagation-based training, deep learning has seen an
    explosion in the last 10 years, achieving unprecedented results in many fields,
    from image processing to speech recognition to natural language processing and
    understanding. In fact, it is now possible to successfully train large and deep
    neural networks by leveraging huge amounts of data and overcoming practical roadblocks
    that impeded their adoption in past decades. These models demonstrated the capability
    to exceed human performances in terms of both speed and accuracy. This chapter
    will teach you how to adopt deep learning to solve real-world problems by taking
    advantage of the top machine learning frameworks. TensorFlow and Keras, are the
    de facto production standards in the industry. Their success is mainly related
    to two aspects: TensorFlow''s unrivaled performance in production environments
    in terms of both speed and scalability, and Keras'' ease of use, which provides
    a very powerful, high-level interface that can be used to create deep learning
    models.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算能力的显著提升以及基于反向传播的训练方法等理论突破，深度学习在过去10年里经历了爆炸式的发展，在许多领域取得了前所未有的成果，从图像处理到语音识别，再到自然语言处理与理解。实际上，现在可以通过利用海量数据并克服过去几十年阻碍其普及的实际障碍，成功地训练大规模和深度的神经网络。这些模型展现了在速度和准确性方面超越人类的能力。本章将教你如何利用深度学习框架解决实际问题。TensorFlow和Keras是行业中事实上的生产标准。它们的成功主要与两个方面有关：TensorFlow在生产环境中的无与伦比的性能，特别是在速度和可扩展性方面，以及Keras的易用性，它提供了一个非常强大、高级的接口，可以用来创建深度学习模型。
- en: Now, let's take a look at the frameworks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看这些框架。
- en: An Introduction to TensorFlow and Keras
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow和Keras简介
- en: In this section, both frameworks will be presented, thus providing you with
    a general overview of their architecture, the fundamental elements they are composed
    of, and listing some of their typical applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍这两个框架，为你提供它们的架构概览、组成的基本元素，并列出一些典型的应用场景。
- en: TensorFlow
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow
- en: 'TensorFlow is an open source numerical computation software library that leverages
    data flow computational graphs. Its architecture allows users to run it on a wide
    variety of hardware: from CPUs to **Tensor Processing Units** (**TPUs**), including
    GPUs as well as mobile and embedded platforms. The main difference between the
    three is the speed and the type of data they are able to perform computations
    with (multiplications and additions), which, of course, is of primary importance
    when aiming for maximum performance.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个开源的数值计算软件库，利用数据流计算图进行运算。其架构使得用户能够在各种硬件平台上运行，包括从CPU到**张量处理单元**（**TPUs**），以及GPU、移动设备和嵌入式平台。三者之间的主要区别在于计算速度和它们能够执行的计算类型（如乘法和加法），这些差异在追求最大性能时至关重要。
- en: Note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will be looking at various code implementation examples for TensorFlow in
    the *Keras* section of this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的*Keras*部分，我们将查看一些针对TensorFlow的代码实现示例。
- en: 'You can refer to the official documentation of TensorFlow for more information
    here: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下链接参考TensorFlow的官方文档，获取更多信息：[https://www.tensorflow.org/](https://www.tensorflow.org/)
- en: 'The following article is a very good reference if you wish to find out more
    about the differences between GPUs and TPUs: [https://iq.opengenus.org/cpu-vs-gpu-vs-tpu/](https://iq.opengenus.org/cpu-vs-gpu-vs-tpu/)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望了解更多关于GPU与TPU之间差异的内容，以下文章是一个非常好的参考：[https://iq.opengenus.org/cpu-vs-gpu-vs-tpu/](https://iq.opengenus.org/cpu-vs-gpu-vs-tpu/)
- en: TensorFlow is based on a high-performance core implemented in C++ that's provided
    by a distributed execution engine that works as an abstraction toward the many
    devices it supports. We will be using TensorFlow 2, which has recently been released.
    It represents a major milestone for TensorFlow. Its main differences with respect
    to version 1 are related to its greater ease of use, in particular for model building.
    In fact, Keras has become the lead tool that's used to easily create models and
    experiment with them. TensorFlow 2 uses eager execution by default. This allowed
    the creators of TensorFlow to eliminate the previous complex workflow, which was
    based on the construction of a computational graph that's then run in a session.
    With eager execution, this is no longer required. Finally, the data pipeline has
    been simplified by means of the TensorFlow dataset, which is a common interface
    that's used to ingest standard or custom datasets with no need to define placeholders.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 基于一个高性能的核心，该核心用 C++ 实现，并由一个分布式执行引擎提供支持，该引擎作为对其支持的众多设备的抽象。我们将使用最近发布的
    TensorFlow 2 版本，它代表了 TensorFlow 的一个重要里程碑。与版本 1 相比，它的主要区别在于更高的易用性，特别是在模型构建方面。事实上，Keras
    已成为用来轻松创建模型并进行实验的主要工具。TensorFlow 2 默认使用即时执行（eager execution）。这使得 TensorFlow 的创建者能够消除以前基于构建计算图并在会话中执行的复杂工作流程。通过即时执行，这一步骤不再是必须的。最后，数据管道通过
    TensorFlow 数据集得到了简化，这是一个常见的接口，用于引入标准或自定义数据集，无需定义占位符。
- en: The execution engine is then interfaced with Python and C++ frontends, which,
    in turn, are the basis for the Layers API, which provides a simple interface for
    common layers in deep learning models. This hierarchical structure continues with
    higher-level APIs, including Keras (which we will describe later in this section).
    Finally, a set of common models are provided and can be used out of the box.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎接着与 Python 和 C++ 前端接口，这些前端是深度学习模型常见层的 API 接口——层 API 的基础。这个层次结构继续向更高级的 API
    发展，包括 Keras（我们将在本节后面描述）。最后，还提供了一些常见的模型，可以开箱即用。
- en: 'The following diagram provides an overview of how different TensorFlow modules
    are hierarchically organized, starting from the low level (bottom) up to the highest
    level (top):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表概述了不同 TensorFlow 模块的层次结构，从最低层（底部）到最高层（顶部）：
- en: '![Figure 3.1: TensorFlow architecture'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1：TensorFlow 架构'
- en: '](img/B16182_03_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_01.jpg)'
- en: 'Figure 3.1: TensorFlow architecture'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：TensorFlow 架构
- en: The historical execution model of TensorFlow was based on computational graphs.
    Using this approach, the first step when building a model is to create a computation
    graph that fully describes the calculations we want to perform. The second step
    is to execute it. This approach has the drawback of being less intuitive with
    respect to common implementations, where the graph doesn't have to be completed
    before it can be executed. At the same time, it provides several advantages, making
    the algorithm highly portable, deployable on different types of hardware platforms,
    and capable of running in parallel on multiple instances.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的历史执行模型基于计算图。使用这种方法，构建模型的第一步是创建一个完整描述我们要执行的计算的计算图。第二步是执行计算图。此方法的缺点是，相比常见的实现，它不够直观，因为在执行之前，图必须是完整的。与此同时，这种方法也有许多优点，使得算法具有高度的可移植性，能够部署到不同类型的硬件平台上，并且可以在多个实例上并行运行。
- en: 'In the latest version of TensorFlow (starting with v. 1.7), a new execution
    model called "eager execution" has been introduced. This is an imperative style
    for writing code. With eager execution enabled, all algorithmic operations can
    be run immediately, with no need to build a graph first and then execute it. This
    new approach has been greeted with enthusiasm and has some very important pros:
    first, it is much simpler to inspect and debug algorithms and access intermediate
    values; it is possible to directly use a Python control flow inside TensorFlow
    APIs; and it makes building and training complex algorithms very easy.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 的最新版本中（从 v.1.7 开始），引入了一种新的执行模型，称为“即时执行”（eager execution）。这是一种命令式编程风格，允许编写代码时直接执行所有算法操作，而不需要首先构建计算图再执行。这个新方法受到了热烈的欢迎，并且有几个非常重要的优点：首先，它使得检查和调试算法、更容易访问中间值变得非常简单；其次，可以直接在
    TensorFlow API 中使用 Python 控制流；最后，它使得构建和训练复杂算法变得非常容易。
- en: In addition, once the model that has been created using eager execution satisfies
    requirements, it is possible to automatically convert it into a graph, which makes
    it possible to leverage all the advantages we looked at previously, such as saving,
    porting, and distributing models optimally.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦使用即时执行（eager execution）创建的模型满足要求，就可以将其自动转换为图形，这样就能够利用我们之前讨论的所有优点，如模型保存、迁移和最优分发等。
- en: Like other machine learning frameworks, TensorFlow provides a large number of
    ready-to-use models and for many of them, it also provides trained model weights
    along with the model graph, meaning we can run such models out of the box, and
    even tune them for a specific use case to take advantage of techniques such as
    transfer learning with fine tuning. We will cover these in the following sections.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他机器学习框架一样，TensorFlow 提供了大量现成的模型，并且对于许多模型，它还提供了训练好的模型权重和模型图，这意味着我们可以直接运行这些模型，甚至为特定的应用场景进行调整，利用诸如迁移学习和微调等技术。我们将在接下来的章节中介绍这些内容。
- en: 'The models provided cover a wide range of different applications, for example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的模型涵盖了广泛的不同应用，例如：
- en: '**Image classification**: Able to classify images into categories.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类**：能够将图像分类到不同类别中。'
- en: '**Object detection**: Capable of detecting and localizing multiple objects
    in images.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物体检测**：能够在图像中检测和定位多个物体。'
- en: '**Language understanding and translation**: Performing natural language processing
    for tasks such as word prediction and translation.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言理解与翻译**：执行自然语言处理任务，如单词预测和翻译。'
- en: '**Patch harmonization and style transfer**: The algorithm is able to apply
    a given style (represented, for example, through a painting) to a given photo
    (refer to the following example).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**补丁协调与风格迁移**：该算法能够将特定的风格（例如通过画作表现的风格）应用到给定的照片上（参见以下示例）。'
- en: As we mentioned previously, many of the models include trained weights and examples
    explaining how to use them. Thus, it is very straightforward to adopt "transfer
    learning," that is, to take advantage of these pretrained models by creating new
    ones, retraining only a part of the network on a new dataset. This can be significantly
    smaller with respect to the one used to train the entire network from scratch.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，许多模型都包括训练好的权重和使用说明示例。因此，采用“迁移学习”变得非常简单，也就是通过创建新的模型并仅在新数据集上对网络的一部分进行再训练，从而利用这些预训练的模型。这相比于从零开始训练整个网络要小得多。
- en: TensorFlow models can also be deployed on mobile devices. After being trained
    on large systems, they are optimized to reduce their footprint, which cannot be
    too big to meet platform limitations. For example, the TensorFlow project known
    as **MobileNet** is developing a set of computer vision models specifically designed
    with optimal speed/accuracy trade-offs in mind. These are typically considered
    for embedded devices and mobile applications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 模型也可以部署到移动设备上。在大型系统上进行训练后，它们经过优化，以减小其占用空间，确保不会超出平台的限制。例如，TensorFlow
    项目中的 **MobileNet** 正在开发一套专为优化速度/准确度权衡的计算机视觉模型。这些模型通常用于嵌入式设备和移动应用。
- en: 'The following image represents a typical example of an object detection application
    where the input image is processed and three objects have been detected, localized,
    and classified:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了一个典型的物体检测应用示例，其中输入图像被处理，检测到了三个物体，并进行了定位和分类：
- en: '![Figure 3.2: Object detection'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2：物体检测'
- en: '](img/B16182_03_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_02.jpg)'
- en: 'Figure 3.2: Object detection'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：物体检测
- en: 'The following image shows how style transfer works: the style of the famous
    painting "*The Great Wave off Kanagawa*" has been applied to a photo of the Seattle
    skyline. The results keep the key parts of the picture (the majority of the buildings
    are there, mountains, and so on), but it is represented through stylistic elements
    that have been extrapolated from the reference image:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了风格迁移的工作原理：著名画作《神奈川冲浪里》的风格被应用到了西雅图天际线的照片上。结果保持了图像的关键部分（大部分建筑物、山脉等），但通过从参考图像中提取的风格元素进行了呈现：
- en: '![Figure 3.3: Style transfer'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3：风格迁移'
- en: '](img/B16182_03_03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_03.jpg)'
- en: 'Figure 3.3: Style transfer'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：风格迁移
- en: Now, let's learn about Keras.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来了解一下 Keras。
- en: Keras
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras
- en: Building deep learning models is quite complex, especially when we have to deal
    with all the typical low-level aspects of major frameworks, and this is one of
    the most relevant barriers for newcomers in the machine learning field. As an
    example, the following code shows how to create a simple neural network (one hidden
    layer with an input size of `100` and an output size of `10`) with a low-level
    TensorFlow API.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建深度学习模型相当复杂，特别是当我们需要处理所有主要框架的典型底层细节时，这也是机器学习领域新手面临的最相关障碍之一。例如，以下代码展示了如何使用低级
    TensorFlow API 创建一个简单的神经网络（一个隐藏层，输入大小为`100`，输出大小为`10`）。
- en: 'In the following code snippet, two functions are being defined. The first builds
    the weights matrix of a network layer, while the second one creates the bias vector:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，定义了两个函数。第一个构建了一个网络层的权重矩阵，而第二个创建了偏置向量：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, the placeholders for the input (`X`) and labels (`y`) are created. They
    will contain the training samples that will be used to fit the model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建输入（`X`）和标签（`y`）的占位符。它们将包含用于拟合模型的训练样本：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Two matrices and two vectors are created, one couple for each of the two hidden
    layers of the network to be created, with the functions previously defined. These
    will contain trainable parameters (network weights):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 创建两个矩阵和两个向量，每对分别对应网络中要创建的两个隐藏层，使用之前定义的函数。这些将包含可训练参数（网络权重）：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The two network layers are defined via their mathematical definition: matrix
    multiplication, plus the bias sum and activation function applied to the result:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过它们的数学定义来定义两个网络层：矩阵乘法，加上偏置和应用于结果的激活函数：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `loss` function is defined, the optimizer is initialized, and the training
    metrics are chosen. Finally, the graph is run to perform training:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss` 函数已经定义，优化器已初始化，训练指标已选择。最后，执行图形以进行训练：'
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see, we need to manually manage many different aspects: variable
    declaration, weights initialization, layer creation, layer-related mathematical
    operations, and the definition of the loss function, optimizers, and metrics.
    For comparison, the same neural network will be created using Keras later in this
    section.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们需要手动管理许多不同的方面：变量声明、权重初始化、层创建、层相关的数学操作，以及损失函数、优化器和指标的定义。为了比较，本节后面将使用 Keras
    创建相同的神经网络。
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Among many different proposals, Keras has become one of the main references
    for high-level APIs, especially the context of those targeted at creating neural
    networks. It is written in Python and can be interfaced with different backend
    computation engines, one of which is, of course, TensorFlow.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多不同的提议中，Keras 已经成为高层 API 的主要参考之一，尤其是在创建神经网络的上下文中。它是用 Python 编写的，可以与不同的后端计算引擎进行接口，其中一个引擎当然是
    TensorFlow。
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can refer to the official documentation for further reading on Keras here:
    [https://keras.io/](https://keras.io/).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考 Keras 的官方文档进行进一步阅读：[https://keras.io/](https://keras.io/)。
- en: Keras' conception has been driven by some clear principles, in particular, modularity,
    user friendliness, easy extendibility, and its straightforward integration with
    Python. Its aim is to favor adoption by newcomers and non-experienced users, and
    it presents a very gentle learning curve. It provides many different standalone
    modules, ranging from neural network layers to optimizers, from initialization
    schemes to cost functions. These can be easily created to create deep learning
    models quickly and to code them directly in Python, with no need to use separate
    configuration files. Given these features, its wide adoption, the fact that it
    can be interfaced with a large number of different backend engines (for example,
    TensorFlow, CNTK, Theano, MXNet, and PlaidML) and its wide choice of deployment
    options, it has risen to become the standard choice in the field.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的设计理念遵循了一些明确的原则，特别是模块化、用户友好、易于扩展，并且与 Python 的直接集成。它的目标是促进新手和非经验用户的采用，并且具有非常平缓的学习曲线。它提供了许多不同的独立模块，从神经网络层到优化器，从初始化方案到成本函数。这些模块可以轻松创建，以便快速构建深度学习模型，并直接用
    Python 编码，无需使用单独的配置文件。鉴于这些特点，Keras 的广泛应用，以及它能够与大量不同的后端引擎（例如 TensorFlow、CNTK、Theano、MXNet
    和 PlaidML）进行接口，并提供多种部署选项，它已经成为该领域的标准选择。
- en: Since it doesn't have its own low-level implementation, Keras needs to rely
    on an external element. This can be easily modified by editing (for Linux users)
    the `$HOME/.keras/keras.json` file, where it is possible to specify the backend
    name. It is also possible to specify it by means of the `KERAS_BACKEND` environment
    variable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它没有自己的低级实现，Keras需要依赖外部元素。这可以通过编辑（对于Linux用户）`$HOME/.keras/keras.json`文件轻松修改，在该文件中可以指定后端名称。也可以通过`KERAS_BACKEND`环境变量指定。
- en: 'Keras'' fundamental class is `Model`. There are two different types of model
    available: The sequential model (which we will use extensively), and the `Model`
    class, which is used with the functional API.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的基础类是`Model`。有两种不同类型的模型可供选择：顺序模型（我们将广泛使用）和`Model`类，它与功能性API一起使用。
- en: The sequential model can be seen as a linear stack of layers, piled one after
    the other in a very simple way, and these layers can be described very easily.
    The following exercise shows how short a Python script in Keras that builds a
    deep neural network using `model.add()` can be in order to define two dense layers
    in a sequential model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模型可以看作是层的线性堆叠，层与层之间按非常简单的方式一层接一层堆叠，且这些层可以非常容易地描述。以下练习展示了如何通过Python脚本在Keras中使用`model.add()`构建一个深度神经网络，以定义一个顺序模型中的两个密集层。
- en: 'Exercise 3.01: Building a Sequential Model with the Keras High-Level API'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习3.01：使用Keras高级API构建顺序模型
- en: 'This exercise shows how to easily build a sequential model, composed of two
    dense layers, with the Keras high-level API, step by step:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习演示了如何一步步使用Keras高级API轻松构建一个包含两个密集层的顺序模型：
- en: 'Import the TensorFlow module and print its version:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入TensorFlow模块并打印其版本：
- en: '[PRE6]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This outputs the following line:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE7]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Build the model using Keras'' `sequential` and `add` methods and print a network
    summary. To continue in parallel with a low-level API, the same activation functions
    are used. We are using `ReLu` here, which is a typical activation function that''s
    used for hidden layers. It is a key element that provides nonlinearity to the
    model thanks to its nonlinear shape. We also use `Softmax`, which is the activation
    function typically used for output layers in classification problems. It receives
    the output values (so-called "logits") from the previous layer and performs a
    weighting of them, defining all the probabilities of the output classes. The `input_dim`
    is the dimension of the input feature vector; it is assumed to have a dimension
    of `100`:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras的`sequential`和`add`方法构建模型并打印网络摘要。为了与低级API并行使用，使用了相同的激活函数。这里我们使用`ReLu`，它是典型的用于隐藏层的激活函数。它是一个关键元素，通过其非线性形状为模型提供了非线性特性。我们还使用`Softmax`，它是典型用于分类问题中输出层的激活函数。它接收来自前一层的输出值（所谓的“logits”），并对其进行加权，定义所有输出类别的概率。`input_dim`是输入特征向量的维度，假设其维度为`100`：
- en: '[PRE8]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Print the standard model architecture:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印标准的模型架构：
- en: '[PRE9]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In our case, the network model summary is as follows:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的案例中，网络模型的摘要如下：
- en: '[PRE10]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding output is a useful visualization that gives us a clear understanding
    of layers, their type and shape, and the number of network parameters.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述输出是一个有用的可视化，帮助我们清楚地了解各个层次、它们的类型和形状，以及网络参数的数量。
- en: Note
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/30A9Dw9](https://packt.live/30A9Dw9).
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/30A9Dw9](https://packt.live/30A9Dw9)。
- en: You can also run this example online at [https://packt.live/3cT0cKL](https://packt.live/3cT0cKL).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以在[https://packt.live/3cT0cKL](https://packt.live/3cT0cKL)在线运行这个示例。
- en: As anticipated, this exercise showed us how to create a sequential model and
    how to add two layers to it in a very straightforward way.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，这个练习向我们展示了如何创建一个顺序模型，并如何以非常简单的方式向其中添加两个层。
- en: 'We will deal with the remaining aspects later on, but it is still worth noting
    that training the model we just created and performing inference only requires
    very few lines of code, as presented in the following snippet, which needs to
    be appended to the snippet of *Exercise 3.01, Building a Sequential Model with
    the Keras High-Level API*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续处理中解决其余方面，但仍值得注意的是，训练我们刚创建的模型并进行推理只需要非常少的代码行，如以下代码片段所示，代码片段需要附加到*练习3.01，使用Keras高级API构建顺序模型*的代码片段中：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If more complex models are required, the sequential API is too limited. For
    these needs, Keras provides the functional API, which allows us to create models
    that are able to manage complex networks graphs, such as networks with multiple
    inputs and/or multiple outputs, recurrent neural networks where data processing
    is not sequential but instead is cyclic, and context, where layers' weights are
    shared among different parts of the network. For this purpose, Keras allows us
    to leverage the same set of layers as the sequential model, but provides more
    flexibility in putting them together. First, we have to define the layers and
    put them together. An example is presented in the following snippet.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要更复杂的模型，顺序API就显得过于有限。针对这些需求，Keras提供了功能性API，它允许我们创建能够管理复杂网络图的模型，例如具有多个输入和/或多个输出的网络、数据处理不是顺序的而是循环的递归神经网络（RNN），以及上下文，其中层的权重在网络的不同部分之间共享。为此，Keras允许我们使用与顺序模型相同的层集，但在组合层时提供了更多的灵活性。首先，我们必须定义层并将它们组合在一起。以下代码片段展示了一个例子。
- en: 'First, after importing TensorFlow, an input layer of dimension `784` is created:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在导入TensorFlow后，创建一个维度为`784`的输入层：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Inputs are processed by the first hidden layer. They go through the ReLu activation
    function and are returned as output. This output then becomes the input for the
    second hidden layer, which is exactly the same as the first one, and returns another
    output, again stored in the `x` variable:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输入由第一个隐藏层处理。它们通过ReLu激活函数，并作为输出返回。这个输出然后成为第二个隐藏层的输入，第二个隐藏层与第一个完全相同，并返回另一个输出，依然存储在`x`变量中：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, the `x` variable goes as input to the final output layer, which has
    a `softmax` activation function, and returns predictions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，`x`变量作为输入进入最终的输出层，该层具有`softmax`激活函数，并返回预测值：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once all the passages have been completed, the model can be created by telling
    Keras where it starts (input variable) and where it ends (predictions variable):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成所有步骤，模型就可以通过告诉Keras模型的起始点（输入变量）和终止点（预测变量）来创建：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After the model has been built, it is compiled by specifying the optimizer,
    the loss, and the metrics. Finally, it is fitted onto the training data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建完成后，通过指定优化器、损失函数和评估指标来编译模型。最后，它会拟合到训练数据上：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Keras provides a large number of predefined layers, as well as the possibility
    to code custom ones. Among those, the following are the already available layers:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了大量的预定义层，并且可以编写自定义层。在这些层中，以下是已提供的层：
- en: Dense layers, which are typically used for fully connected neural networks.
    They consist of a matrix of weights and a bias.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层，通常用于全连接神经网络。它们由一个权重矩阵和一个偏置组成。
- en: Convolution layers are filters that are defined by specific kernels, which are
    then convolved with the inputs they are applied to. There are layers available
    for different input dimensions, from 1D to 3D, including the possibility to embed
    in them complex operations, such as cropping or transposition.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层是由特定内核定义的滤波器，然后与应用的输入进行卷积。它们适用于不同输入维度，从1D到3D，包括可以在其中嵌入复杂操作的选项，如裁剪或转置。
- en: Locally connected layers are similar to convolution layers in the sense that
    they act only on a subgroup of the input features, but, unlike convolution layers,
    they don't share weights.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部连接层与卷积层类似，因为它们仅作用于输入特征的子集，但与卷积层不同，它们不共享权重。
- en: Pooling layers are layers that are used to downscale the input. As convolutional
    layers, they are available for inputs with dimensionality ranging from 1D to 3D.
    They include most of the common variants, such as max and average pooling.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层是用于降低输入尺寸的层。作为卷积层，它们适用于维度从1D到3D的输入。它们包括大多数常见的变种，例如最大池化和平均池化。
- en: Recurrent layers are used for recurrent neural networks, where the output of
    a layer is also fed backward in the network. They support state-of-the-art units
    such as **Gated Recurrent Units** (**GRUs**), **Long Short-Term Memory** (**LSTM**)
    units, and others.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环层用于递归神经网络，其中一个层的输出也会被反向传递到网络中。它们支持先进的单元，如**门控递归单元**（**GRU**）、**长短期记忆**（**LSTM**）单元等。
- en: Activation functions are also available in the form of layers. These are functions
    that are applied to layer outputs, such as `ReLu`, `Elu`, `Linear`, `Tanh`, and
    `Softmax`.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数也可以作为层的形式存在。这些函数应用于层的输出，如`ReLu`、`Elu`、`Linear`、`Tanh`和`Softmax`。
- en: Lambda layers are layers for embedding arbitrary, user-defined expressions.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambda 层是用于嵌入任意用户定义表达式的层。
- en: Dropout layers are special objects that randomly set a fraction of the input
    units to `0` at each training update to avoid overfitting (more on this later).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 层是特殊对象，它会在每次训练更新时随机将一部分输入单元设为 `0`，以避免过拟合（稍后会详细介绍）。
- en: Noise layers are additional layers, such as dropout, that are used to avoid
    overfitting.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声层是额外的层，例如 dropout，旨在避免过拟合。
- en: Keras also provides common datasets, as well as famous models. For image-related
    applications, many networks are available, such as Xception, VGG16, VGG19, ResNet50,
    InceptionV3, InceptionResNetV2, MobileNet, DenseNet, NASNet, and MobileNetV2TK,
    all of which are pretrained on ImageNet. Keras also provides text and sequences
    and generative models, making a total of more than 40 algorithms.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 还提供了常见的数据集和著名的模型。对于与图像相关的应用，许多网络是可用的，例如 Xception、VGG16、VGG19、ResNet50、InceptionV3、InceptionResNetV2、MobileNet、DenseNet、NASNet
    和 MobileNetV2TK，它们都在 ImageNet 上进行了预训练。Keras 还提供文本和序列及生成模型，总共超过 40 种算法。
- en: As we saw for TensorFlow, Keras models have a vast choice of deployment platforms,
    including iOS, via CoreML (supported by Apple); Android, via the TensorFlow Android
    runtime; in a browser, via Keras.js and WebDNN; on Google Cloud, via TensorFlow-Serving;
    in a Python webapp backend; on the JVM, via DL4J model import; and on a Raspberry
    Pi.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们看到的，对于 TensorFlow，Keras 模型有广泛的部署平台选择，包括通过 CoreML（Apple 支持）在 iOS 上；通过 TensorFlow
    Android 运行时在 Android 上；通过 Keras.js 和 WebDNN 在浏览器中；通过 TensorFlow-Serving 在 Google
    Cloud 上；在 Python Web 应用后端；通过 DL4J 模型导入在 JVM 上；以及在 Raspberry Pi 上。
- en: Now that we've looked at both TensorFlow and Keras, from the next section onward,
    our main focus will be on how to use them in combination to create deep neural
    networks. Keras will be used as a high-level API, given its user-friendliness,
    including TensorFlow, which will be the backend.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 TensorFlow 和 Keras，从下一部分开始，我们的主要焦点将是如何将它们结合使用来创建深度神经网络。Keras 将作为高级
    API 使用，因其用户友好性，而 TensorFlow 将作为后端。
- en: How to Implement a Neural Network Using TensorFlow
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 TensorFlow 实现神经网络
- en: In this section, we will look at the most important aspects to consider when
    implementing a deep neural network. Starting with the very basic concepts, we
    will go through all the steps that lead up to the creation of a state-of-the-art
    deep learning model. We will cover the network architecture's definition, training
    strategies, and performance improvement techniques, understanding how they work,
    and preparing you so that you can tackle the next section's exercises, where these
    concepts will be applied to solve real-world problems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论实现深度神经网络时需要考虑的最重要的方面。从最基本的概念开始，我们将经历所有步骤，直到创建出最先进的深度学习模型。我们将涵盖网络架构的定义、训练策略和性能提升技术，理解它们如何工作，并为你准备好，帮助你完成接下来的练习，在那里这些概念将被应用来解决现实世界的问题。
- en: 'To successfully implement a deep neural network in TensorFlow, we have to complete
    a given number of steps. These can be summarized and grouped as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功实现 TensorFlow 中的深度神经网络，我们必须完成一定数量的步骤。这些步骤可以总结并分组如下：
- en: '**Model creation**: Network architecture definition, input features encoding,
    embeddings, output layers'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型创建**：网络架构定义、输入特征编码、嵌入、输出层'
- en: '**Model training**: Loss function definition, optimizer choice, features normalization,
    backpropagation'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练**：损失函数定义、优化器选择、特征标准化、反向传播'
- en: '**Model validation**: Strategies and key elements'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型验证**：策略和关键元素'
- en: '**Model improvement**: Overfitting countermeasures'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型优化**：防止过拟合的对策'
- en: '**Model test and inference**: Performance evaluation and online predictions'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型测试和推理**：性能评估和在线预测'
- en: Let's look at each of these steps in detail.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每一个步骤。
- en: Model Creation
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型创建
- en: The very first step is to create a model. Choosing an architecture is hardly
    something that can be done *a priori* on paper. It is a typical process that requires
    experimentation, going back and forth between model design and field validation
    and testing. This is the phase where all network layers are created and properly
    linked to generate a complete processing operation set that goes from inputs to
    outputs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个模型。选择架构几乎不能 *先验* 在纸面上完成。这是一个典型的过程，需要实验，反复在模型设计和领域验证、测试之间进行调整。这是所有网络层创建并正确链接的阶段，生成一个完整的处理操作集，从输入到输出。
- en: The very first layer is the one that is interfaced with input data, specifically,
    the so-called "input features." In the case of images, for example, input features
    are image pixels. Depending on the nature of the layer, the input features' dimensionality
    needs to be taken into account. You will learn how to choose layer dimensions,
    depending on the layer's nature, in the upcoming sections.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最底层是与输入数据接口的层，特别是所谓的“输入特征”。例如，在图像的情况下，输入特征就是图像像素。根据层的性质，输入特征的维度需要被考虑。在接下来的章节中，你将学习如何根据层的性质选择层的维度。
- en: The very last layer is called the output layer. It generates model predictions,
    so its dimensions depend on the nature of the problem. For example, in classification
    problems, where the model has to predict in which of the, say, 10 classes a given
    instance falls, the model will have 10 neurons in the output layer providing 10
    scores (one per class). In the upcoming sections, we will illustrate how to create
    output layers with the correct dimensions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层叫做输出层。它生成模型的预测，因此它的维度取决于问题的性质。例如，在分类问题中，模型必须预测一个给定实例属于哪个类别（假设是10个类别中的一个），输出层将有10个神经元，每个神经元提供一个得分（每个类别一个）。在接下来的章节中，我们将说明如何创建具有正确维度的输出层。
- en: Between the first and last layers, there are intermediate layers, called hidden
    layers. These layers constitute the network architecture, and they are responsible
    for the core processing capabilities of the model. At the time of writing, a rule
    that can be used to choose the best network architecture doesn't exist; this is
    a process that requires a lot of experimentation, under the guidance of some general
    principles.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层和最后一层之间，是中间层，称为隐藏层。这些层构成了网络架构，并负责模型的核心处理能力。目前为止，还没有可以用来选择最佳网络架构的规则；这个过程需要大量的实验，并且需要遵循一些通用原则的指导。
- en: A very powerful and common approach is to leverage proven models from academic
    papers, using them as a starting point, and then adjusting the architecture appropriately
    to fit and fine-tune it to the custom problem. When pretrained literature models
    are used and fine-tuned, the procedure is called "transfer learning," meaning
    we are leveraging an already trained model and transferring its knowledge to the
    new model, which then won't start from scratch.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常强大且常见的方法是利用来自学术论文的经过验证的模型，作为起点，然后根据具体问题适当调整架构并进行微调。当使用预训练的文献模型并进行微调时，这个过程被称为“迁移学习”，意味着我们利用已经训练好的模型并将其知识转移到新模型中，后者就不需要从头开始训练了。
- en: Once the model has been created, all its parameters (weights/biases) must be
    initialized (for all non-pretrained layers). You might be tempted to set them
    all equal to zero, but this is hardly a good choice. There are many different
    initialization schemes available, and again, which one to choose requires experience
    and experimentation. This aspect will become clearer in the following sections.
    Implementation will rely on default initialization to be performed by Keras/TensorFlow,
    which is usually a good and safe starting point.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被创建，所有参数（权重/偏置）必须初始化（对于所有未预训练的层）。你可能会想将它们都设为零，但这并不是一个好选择。有许多不同的初始化方案可以使用，而选择哪一种需要经验和实验。在接下来的章节中，这一点会变得更清楚。实现将依赖于Keras/TensorFlow执行的默认初始化，通常这是一个好的且安全的起点。
- en: 'A typical code example for model creation can be seen in the following snippet,
    which we studied in the previous section:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型创建的典型代码示例可以在下面的代码片段中看到，这是我们在前一节中学习过的：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Model Training
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'When a model is initialized and applied to input data without undergoing a
    training phase, it outputs random values. In order to improve its performance,
    we need to adjust its parameters (weights) to minimize its errors. This is the
    aim of the model training stage, which requires the following steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型被初始化并应用于输入数据而没有经过训练阶段时，它输出的是随机值。为了提高性能，我们需要调整其参数（权重），以最小化误差。这是模型训练阶段的目标，包含以下步骤：
- en: First, we have to evaluate how "wrong" the model is with a given parameter configuration
    by computing a so-called "loss," which is a measure of model prediction error.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须评估模型在给定参数配置下的“错误”程度，通过计算所谓的“损失”，即模型预测误差的度量。
- en: Second, a hyperdimensional gradient is computed, which tells us how (in which
    direction) the model needs to change its parameters in order to improve current
    performance, thereby minimizing the loss function (it is indeed an optimization
    process).
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步，计算一个高维梯度，告诉我们模型需要在哪个方向上调整参数，以提高当前性能，从而最小化损失函数（这确实是一个优化过程）。
- en: Finally, the model parameters are updated by taking a "step" in the negative
    gradient direction (following some precise rules) and the whole process restarts
    from the loss evaluation stage.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过沿负梯度方向“步进”（遵循一些精确规则），更新模型参数，并且整个过程从损失评估阶段重新开始。
- en: This procedure is repeated as many times as needed until the system converges
    and the model reaches its maximum performance (minimum loss).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，直到系统收敛并且模型达到最佳性能（最小损失）。
- en: 'A typical code example for model training is shown in the following snippet,
    which we studied in the previous sections:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个典型的模型训练代码示例，我们在之前的章节中已经学习过：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Loss Function Definition
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数定义
- en: 'Model error can be measured by means of different loss functions. How to choose
    the best one requires experience. For complex applications, we often need to carefully
    adapt the loss function in order to drive training in directions we are interested
    in. As an example, let''s look at how to define a typical loss that''s used for
    classification problems: the sparse categorical cross entropy. To create it in
    Keras, we can use the following instruction:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 模型错误可以通过不同的损失函数来度量。如何选择最好的损失函数需要经验。对于复杂的应用，通常需要仔细调整损失函数，以引导训练朝着我们感兴趣的方向进行。举个例子，让我们看看如何定义一个常用于分类问题的典型损失：稀疏类别交叉熵。在Keras中创建它，我们可以使用以下指令：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This function operates on two inputs: true labels and predicted labels. Based
    on their values, it computes the loss associated with the model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数操作于两个输入：真实标签和预测标签。根据它们的值，计算与模型相关的损失：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Optimizer Choice
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器选择
- en: 'The second and third steps, estimating the gradient and updating the parameters,
    respectively, are addressed by optimizers. These objects calculate gradients and
    perform update steps in the gradient''s direction to minimize model loss. There
    are many optimizers available, from the simplest ones to the most advanced (refer
    to the following diagram). They provide different performances, and which one
    to select is, again, a matter of experience and a trial-and-error process. As
    an example, the following code selects the `Adam` optimizer, assigning a specific
    learning rate of `0.01`. This parameter regulates how "large" the step taken will
    be along the gradient direction:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步和第三步，分别是估算梯度和更新参数，这由优化器处理。这些对象计算梯度并执行沿梯度方向的更新步骤，以最小化模型损失。可以选择许多优化器，从最简单的到最先进的（参见下图）。它们提供了不同的性能，选择哪一个，仍然是经验和试错的过程。举个例子，下面的代码选择了`Adam`优化器，并为其指定了`0.01`的学习率。该参数调节沿梯度方向“步进”的大小：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following diagram is an instantaneous snapshot comparing different optimizers.
    It shows how *quickly* they move toward the minimum, starting all at the same
    time. We can see how some of them are faster than others:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是一个即时快照，比较了不同优化器的表现。它显示了它们是如何*快速*地朝着最小值移动的，所有优化器同时开始。我们可以看到一些优化器比其他的更快：
- en: '![Figure 3.4: Comparison of optimizer minimization steps'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4：优化器最小化步骤比较'
- en: '](img/B16182_03_04.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_04.jpg)'
- en: 'Figure 3.4: Comparison of optimizer minimization steps'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：优化器最小化步骤比较
- en: Note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding diagram was created by Alec Radford ([https://twitter.com/alecrad](https://twitter.com/alecrad)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表由Alec Radford创建（[https://twitter.com/alecrad](https://twitter.com/alecrad)）。
- en: Learning Rate Scheduling
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率调度
- en: 'In most cases, and for most deep learning models, the best results are achieved
    if the learning rate is gradually reduced during training. The reason for this
    can be seen in the following diagram:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，对于大多数深度学习模型，最佳效果通常是在训练过程中逐渐减小学习率。这个原因可以从以下图表中看到：
- en: '![Figure 3.5: Optimization behavior when using different learning rate values'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5：使用不同学习率值时的优化行为'
- en: '](img/B16182_03_05.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_05.jpg)'
- en: 'Figure 3.5: Optimization behavior when using different learning rate values'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：使用不同学习率值时的优化行为
- en: When approaching the minimum of the loss function, we want to take smaller and
    smaller steps to efficiently reach the very bottom of the hyperdimensional concavity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当接近损失函数的最小值时，我们希望采取越来越小的步伐，以高效地达到超维凹形的最底部。
- en: 'With Keras, it is possible to prescribe many different decreasing functions
    for the learning rate trend over epochs by means of a scheduler. One common choice
    is `InverseTimeDecay`. This can be implemented as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras，可以通过调度器为学习率的趋势在各个 epoch 中指定许多不同的递减函数。一种常见的选择是 `InverseTimeDecay`。它可以通过以下方式实现：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code sets a decreasing function through `InverseTimeDecay` to
    hyperbolically decrease the learning rate to 1/2 of the base rate at 1,000 epochs,
    1/3 at 2,000 epochs, and so on. This can be seen in the following graph:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码通过 `InverseTimeDecay` 设置了一个递减函数，采用双曲线方式使学习率在 1,000 个 epoch 时降至基础学习率的 1/2，在
    2,000 个 epoch 时降至 1/3，依此类推。以下图表展示了这一变化：
- en: '![Figure 3.6: Inverse time decay learning rate scheduling'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6：反时间衰减学习率调度'
- en: '](img/B16182_03_06.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_06.jpg)'
- en: 'Figure 3.6: Inverse time decay learning rate scheduling'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：反时间衰减学习率调度
- en: 'Then, it is applied to an optimizer as an argument, as shown in the following
    snippet for the `Adam` optimizer:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它被作为参数应用于优化器，如下所示的 `Adam` 优化器代码片段所示：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Each optimization step makes the loss drop, thereby improving the model. It
    is then possible to repeat the same process over and over until convergence is
    reached and the loss stops decreasing. The number of optimization steps performed
    is usually called the number of epochs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 每次优化步骤都会使损失减少，从而改进模型。然后可以重复相同的过程，直到收敛并且损失停止减少。执行的优化步骤次数通常被称为 epoch 数。
- en: Feature Normalization
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征归一化
- en: 'The broad applications for deep neural networks favor their usage on very different
    types of inputs, from image pixels to credit card transaction history, from social
    account profile habits to audio recordings. From this, it is clear that raw input
    features cover very different numerical scales. As mentioned previously, training
    these models requires solving an optimization problem using a loss gradient calculation.
    For this reason, numerical aspects are of paramount importance, resulting in a
    speeding up of the process, as well as making it more robust. One of the most
    important practices, in this context, is feature normalization or standardization.
    The most common approach consists of performing the following steps for each feature:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的广泛应用使得它们能够处理各种不同类型的输入，从图像像素到信用卡交易历史，从社交账号的个人资料习惯到音频记录。从这些可以看出，原始输入特征的数值范围差异很大。如前所述，训练这些模型需要通过损失梯度计算来解决优化问题。因此，数值方面至关重要，它不仅加速了过程，还使其更具鲁棒性。在这种背景下，最重要的实践之一就是特征归一化或标准化。最常见的方法包括对每个特征执行以下步骤：
- en: Calculating the mean and standard deviation using all the training set instances.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有训练集实例计算均值和标准差。
- en: Subtracting the mean and dividing by standard deviation. Values calculated on
    the training set must be applied to the training, validation, and test sets.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减去均值并除以标准差。计算得出的值必须应用于训练集、验证集和测试集。
- en: This way, all the features will have zero mean and standard deviation equal
    to `1`. Different, but similar, approaches scale feature values between a user-defined
    minimum-maximum range (for example, between –1 and 1) or apply similar transformations
    (for example, log scaling). As usual, in the field, which approach works better
    is hardly predictable and requires experience and a trial-and-error approach.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，所有特征将具有零均值和标准差为 `1`。不同但相似的方法会将特征值缩放到用户定义的最小-最大范围（例如，从 -1 到 1），或应用类似的转换（例如，对数缩放）。如同往常一样，在实际应用中，哪种方法更有效是很难预测的，需要经验和反复试验。
- en: 'The following code snippet shows how data normalization is performed, wherein
    the mean and standard deviation of the original values are calculated, the mean
    is then subtracted from the original values, and the result is then divided by
    the standard deviation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何进行数据归一化，其中计算了原始值的均值和标准差，然后从原始值中减去均值，最后将结果除以标准差：
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Model Validation
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型验证
- en: As stated in the previous subsections, a large portion of choices require experimentation,
    meaning we have to select a given configuration and evaluate how the corresponding
    model performs. In order to compute this performance measure, the candidate model
    must be applied to a set of instances and its output compared against ground truth
    values. This step can be repeated many times, depending on how many alternative
    configurations we want to compare. In the long run, these configuration choices
    can suffer an excessive influence of the set of instances used to measure model
    performance. For this reason, in order to have a final accurate performance measure
    of the model of choice, it has to be tested on a new set of instances that have
    never been seen before. The first set of instances is called a "validation set,"
    while the final one is called a "test set."
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述各节所述，大多数选择需要实验，这意味着我们必须选择一个特定的配置并评估相应模型的表现。为了计算该性能度量，必须将候选模型应用于一组实例，并将其输出与真实值进行比较。根据我们希望比较的不同配置数量，这一过程可能会重复多次。从长远来看，这些配置选择可能会受到用于衡量模型性能的实例集的过度影响。出于这个原因，为了得到最终准确的模型性能度量，必须在一个从未见过的新实例集上进行测试。第一个实例集被称为“验证集”，而最后一个则被称为“测试集”。
- en: 'There are different choices we can adopt when defining training, validation,
    and test sets, such as the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义训练集、验证集和测试集时，我们可以采取不同的选择，如下所示：
- en: '70:20:10: The initial dataset is decomposed into three chunks, that is, the
    training, validation, and test sets, with the proportion 70:20:10, respectively.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 70:20:10：初始数据集被分解为三个部分，即训练集、验证集和测试集，比例分别为 70:20:10。
- en: '80:20 + k-Folding: The initial dataset is decomposed into two chunks, 80% training
    and 20% testing, respectively. Validation is performed using k-Folding on the
    training dataset: it is divided into ''k'' folds and, in turn, training is carried
    out in ''k-1'' folds, while validation is performed on the k-th piece. ''K'' varies
    from 1 to k and metrics are averaged to obtain a global measure.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 80:20 + k-折叠：初始数据集被分解为两个部分，分别是 80% 的训练集和 20% 的测试集。通过在训练数据集上使用 k-折叠进行验证：它被分为“k”个折叠，接着在“k-1”个折叠上进行训练，而在第
    k 个折叠上进行验证。'K' 的值从 1 到 k 不等，度量标准被平均以获得全局指标。
- en: Many variants of the preceding methods can be used. The choices are strictly
    related to the problem and the available dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用许多前述方法的变种。选择严格与问题和可用的数据集相关。
- en: 'The following code snippet shows how to prescribe an 80:20 split for validation
    when fitting a model on a training dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在训练数据集上拟合模型时，规定 80:20 的验证集划分：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Performance Metrics
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能度量
- en: 'In order to measure performances, beside the loss functions, other metrics
    are usually adopted. There is a very wide set of metrics available, and the question
    as to which you should use depends on many factors, including the type of problem,
    dataset characteristics, and so on. The following is a list of the most common
    ones:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了损失函数外，通常还会采用其他度量标准来衡量性能。可用的度量标准种类繁多，应该使用哪一种取决于许多因素，包括问题类型、数据集特征等。以下是最常用的一些：
- en: '**Mean Squared Error** (**MSE**): Used for regression problems.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差** (**MSE**)：用于回归问题。'
- en: '**Mean Absolute Error** (**MAE**): Used for regression problems.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方绝对误差** (**MAE**)：用于回归问题。'
- en: 'Accuracy: Number of correct predictions divided by the number of total tested
    instances. This is used for classification problems.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率：正确预测的数量除以总测试实例的数量。这用于分类问题。
- en: '**Receiver Operating Characteristic Area Under Curve** (**ROC** **AUC**): Used
    for binary classification, especially in the presence of highly unbalanced data.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收操作特征曲线下的面积** (**ROC** **AUC**)：用于二分类，特别是在数据高度不平衡的情况下。'
- en: 'Others: Fβ score, precision, and recall.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他：Fβ 分数、精准度和召回率。
- en: Model Improvement
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型改进
- en: In this section, we will look at a few techniques that can be used to improve
    the performance of a model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些可以用于提高模型性能的技术。
- en: Overfitting
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'A common problem we may typically encounter when training deep neural networks
    is a critical drop in model performance (measured, of course, on the validation
    or test set) when the number of training epochs passes a given threshold, even
    if, at the same time, the training loss continues to decrease. This phenomenon
    is called **overfitting**. It can be defined as follows: a highly representative
    model, a model with the relevant number of degrees of freedom (for example, a
    neural network with many layers and neurons), if trained "*too much*," bends itself
    to adhere to the training data, with the intent to minimize the training loss.
    This results in poor generalization performances, making validation and/or test
    errors higher. Deep learning models, thanks to their high-dimensional parameter
    space, are usually very good at fitting the training data, but the actual aim
    of building a machine learning model is being able to generalize what has been
    learned, not merely fit a dataset.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，我们常常遇到的一个问题是，模型性能（当然是通过验证集或测试集来衡量）会在训练轮次超过某一阈值后急剧下降，即使在此时训练损失仍在减少。这个现象被称为**过拟合**。可以这样定义：一个非常具代表性的模型，即拥有相关自由度数量的模型（例如，具有多个层和神经元的神经网络），如果被“*过度训练*”，会趋向于紧贴训练数据，试图最小化训练损失。这会导致较差的泛化性能，从而使验证和/或测试错误增高。深度学习模型由于其高维参数空间，通常非常擅长拟合训练数据，但构建机器学习模型的实际目标是能够泛化已学到的知识，而不仅仅是拟合数据集。
- en: At this point, we might be tempted to significantly reduce the number of model
    parameters to avoid overfitting. But this would cause different problems. In fact,
    a model with an insufficient number of parameters would incur **underfitting**.
    Basically, it would not be able to properly fit the data, again resulting in poor
    performance, this time on both the training and validation/test sets.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，我们可能会倾向于显著减少模型参数的数量，以避免过拟合。但这会引发不同的问题。实际上，参数数量不足的模型会导致**欠拟合**。基本上，它将无法正确拟合数据，结果会导致性能差，训练集和验证/测试集的表现都会不理想。
- en: The correct solution is the one that finds a proper balance between having a
    large number of parameters that would perfectly fit training data and having too
    small a number of model degrees of freedom, resulting in it being able to capture
    important information from data. It is currently not possible to identify the
    right size for a model so that it won't face overfitting or underfitting problems.
    Experimentation is a key element in this regard, thereby requiring the data engineer
    to build and test different architectures. A good rule is to start with models
    with a relatively small number of parameters and then increase them until generalization
    performance grows.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的解决方案是在训练数据完全拟合的大量参数和具有过小模型自由度、导致无法捕捉数据中的重要信息之间找到适当的平衡。目前无法确定模型的最佳规模，以避免过拟合或欠拟合问题。实验是解决这个问题的关键因素，因此数据工程师需要构建并测试不同的架构。一个好的规则是从参数相对较少的模型开始，然后逐步增加它们，直到泛化性能提升。
- en: The best solution against overfitting is to enrich the training dataset with
    new data. Aim for complete coverage of the full range of inputs that are supported
    and expected by the model. New data should also contain additional information
    with respect to starting the dataset in order to effectively contrast overfitting
    and to result in a better generalization error. When collecting additional data
    is not possible or too expensive, it is necessary to adopt specific, very powerful
    techniques. The most important ones will be described here.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗过拟合的最佳解决方案是通过新数据丰富训练数据集。目标是完全覆盖模型所支持并期望的输入范围。新数据还应包含额外的信息，以便有效地对比过拟合，并实现更好的泛化误差。当无法收集额外数据或成本过高时，必须采用特定的、非常强大的技术。最重要的技术将在此描述。
- en: Regularization
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: 'Regularization is one of the most powerful tools used to contrast overfitting.
    Given a network architecture and a set of training data, there is an entire space
    of possible weights that produce the same results. Every combination of weights
    in this space defines a specific model. As we saw in the preceding section, we
    have to prefer, as a general principle, simple models over complex ones. A common
    way to reach this goal is to force network weights to assume small values, thereby
    regularizing the distribution of weights. This can be achieved through "weight
    regularization". This consists of shaping the loss function so that it can take
    weight values into consideration, adding a new term to it that is directly proportional
    to their magnitude. Two approaches are usually encountered:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是用来对抗过拟合的最强大工具之一。给定一个网络架构和一组训练数据，有一个包含所有可能权重的空间，它们会产生相同的结果。这个空间中每一个权重组合定义了一个特定的模型。正如我们在前面的章节中看到的，作为一般原则，我们需要偏向简单模型而非复杂模型。实现这一目标的常用方法是强制网络权重采用较小的值，从而对权重的分布进行正则化。这可以通过“权重正则化”来实现。权重正则化通过调整损失函数，使其考虑权重的值，并添加一个与权重大小成正比的新项。常见的两种方法是：
- en: '**L1 regularization**: The term that''s added to the loss function is proportional
    to the absolute value of the weight coefficients, commonly referred to as the
    "L1 norm" of the weights.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1 正则化**：添加到损失函数中的项与权重系数的绝对值成正比，通常被称为权重的“L1 范数”。'
- en: '**L2 regularization**: The term that''s added to the loss function is proportional
    to the square of the value of the weight coefficients, commonly referred to as
    the "L2 norm" of the weights.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 正则化**：添加到损失函数中的项与权重系数值的平方成正比，通常被称为权重的“L2 范数”。'
- en: Both of these have the effect of limiting the magnitude of the weights, but
    while L1 regularization tends to drive weights toward exactly zero, L2 regularization
    penalizes weights with a less strict constraint since the additional loss term
    grows at a higher rate. L2 is, in general, more common.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都能限制权重的大小，但 L1 正则化往往会使权重趋向于零，而 L2 正则化则对权重施加较宽松的约束，因为附加的损失项增长得更快。通常情况下，L2
    正则化更为常见。
- en: 'Keras contains pre-built L1 and L2 regularization objects. The user has to
    pass them as arguments to the network layers that they want to apply the technique
    to. The following code shows how to apply it to a common dense layer:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 包含了预构建的 L1 和 L2 正则化对象。用户需要将它们作为参数传递给希望应用该技术的网络层。下面的代码展示了如何将其应用于一个常见的全连接层：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The parameter that was passed to the L2 regularizer (`0.001`) shows that an
    additional loss term equal to `0.001 * weight_coefficient_value**2` will be added
    to the total loss of the network for every coefficient in the weight matrix.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 L2 正则化器的参数（`0.001`）表明，网络中的每个权重系数都会额外添加一个损失项 `0.001 * weight_coefficient_value**2`，以此来增加网络的总损失。
- en: Early Stopping
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早停
- en: Early stopping is a specific form of regularization. The idea is to keep track
    of both training and validation errors during training and to continue training
    the model until both training and validation losses decrease. This allows us to
    spot the epochs threshold, after which the training loss' decrease would come
    as an expense of increased generalization error, so that we can stop training
    when validation/test performances have reached their maximum. One typical parameter
    the user has to choose when adopting this technique is the number of epochs the
    system should wait for and monitor before stopping the iterations if no improvement
    in the validation error is shown. This parameter is commonly named "patience."
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 早停是正则化的一种特定形式。其思想是在训练过程中同时跟踪训练和验证误差，并继续训练模型，直到训练和验证损失都减少为止。这样我们可以找到训练损失下降后的阈值，此时继续训练会以增加泛化误差为代价，因此当验证/测试性能达到最大时，我们可以停止训练。采用此技术时，用户需要选择的一个典型参数是系统在停止迭代前应等待和监控的轮次，如果验证误差没有改善。这个参数通常被称为“耐心”。
- en: Dropout
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: One of the most popular and effective reliable regularization techniques for
    neural networks is Dropout. It was developed at the University of Toronto by Prof.
    Hinton and his research group.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中最流行且有效的正则化技术之一是 Dropout。它是由多伦多大学的 Hinton 教授及其研究小组开发的。
- en: When Dropout is applied to a layer, a certain percentage of the layer output
    features during training are randomly set to zero (they drop out). For example,
    if the output of a given layer would normally have been [0.3, 0.4, 1.2, 0.1, 1.5]
    for a given set of input features during training, when dropout is applied, the
    same output vector will have some zero entries randomly distributed; for example,
    [0.3, 0, 1.2, 0.1, 0].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Dropout 应用到一层时，在训练过程中，该层输出特征的某个百分比会被随机设置为零（它们被丢弃）。例如，如果给定一组输入特征，在训练时某层的输出通常为
    [0.3, 0.4, 1.2, 0.1, 1.5]，应用 dropout 后，相同的输出向量会有一些零项随机分布，例如 [0.3, 0, 1.2, 0.1,
    0]。
- en: The idea behind dropout is to encourage each node to output values that are
    highly informative and meaningful on their own, without relying on its neighboring
    ones.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: dropout 背后的理念是鼓励每个节点输出具有高度信息量和独立意义的值，而不依赖于其邻近的节点。
- en: 'The parameter to be set when inserting a dropout layer is called the `0.2`
    and `0.5`. When performing inference, dropout is deactivated, and an additional
    operation needs to be executed to take into account the fact that more units will
    be active with respect to training time. To re-establish a balance between these
    two situations, the layer''s output values are multiplied by a factor equal to
    the dropout rate, resulting in a scaling-down operation. In Keras, dropout can
    be introduced in a network using the dropout layer, which is applied to the output
    of the layer immediately before it. Consider the following code snippet:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 插入 dropout 层时需要设置的参数为 `0.2` 和 `0.5`。在进行推理时，dropout 被停用，需要执行额外的操作，以考虑到相对于训练时会有更多的单元处于激活状态。为了在这两种情况之间重新建立平衡，层的输出值会乘以一个与
    dropout 率相等的因子，形成缩放操作。在 Keras 中，可以通过 dropout 层将 dropout 引入网络，它会应用于紧接其前面的层的输出。考虑以下代码片段：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, dropout is applied to the layer with `512` neurons, setting
    50% of their values to 0.0 at training time, and multiplying their values by 0.5
    at inference time.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，dropout 被应用于 `512` 个神经元的层，在训练时将其50%的值设为 0.0，在推理时将其值乘以 0.5。
- en: Data Augmentation
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'Data augmentation is particularly useful when the number of instances available
    for training is limited. It is super easy to understand how it is implemented
    and works in the context of image processing. Suppose we want to train a network
    to classify images of different breeds of a specific species and we only have
    a limited number of examples for each breed. How can we enlarge the dataset to
    help the model generalize better? Data augmentation plays a major role in this
    context: the idea is to create new training instances, starting from those we
    already have and tweaking them appropriately. In the case of images, we can act
    on them by doing the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在训练实例有限的情况下尤其有用。在图像处理的背景下，理解其实现和工作原理非常简单。假设我们想训练一个网络来分类不同品种的特定物种的图像，而每个品种的示例数量有限。那么，我们如何扩大数据集以帮助模型更好地泛化呢？在这种情况下，数据增强起着重要作用：其理念是从我们已有的数据出发，适当地调整它们，从而生成新的训练实例。在图像的情况下，我们可以通过以下方式对其进行处理：
- en: Random rotations with respect to a point in the vicinity of the center
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对于中心附近的某一点进行随机旋转
- en: Random crops
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机裁剪
- en: Random affine transformations (shear, resize, and so on)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机仿射变换（剪切、缩放等）
- en: Random horizontal/vertical flips
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机水平/垂直翻转
- en: White noise superimposition
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白噪声叠加
- en: Salt and pepper noise superimposition
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盐和胡椒噪声叠加
- en: These are a few examples of data augmentation techniques that can be used for
    images, which, of course, have counterparts in other domains. This approach makes
    the model way more robust and improves its generalization performance, allowing
    it to abstract notions and knowledge about the specific problem it is facing in
    a more general way by giving privilege to the most informative input features.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些可以用于图像的数据增强技术的示例，当然，在其他领域也有对应的方法。这种方法使得模型更加健壮，并改善其泛化能力，通过赋予最具信息量的输入特征优先权，使其能够以更一般的方式抽象出有关其所面临的特定问题的概念和知识。
- en: Batch Normalization
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: 'Batch normalization is a technique that consists of applying a normalization
    transform to every batch of data. For example, in the context of training a deep
    network with a batch size of 128, meaning the system will process 128 training
    samples at a time, the batch normalization layer works this way:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是一种技术，涉及对每个数据批次应用归一化转换。例如，在训练一个批次大小为 128 的深度网络时，意味着系统将一次处理 128 个训练样本，批量归一化层按以下方式工作：
- en: It calculates the mean and variance for each feature using all the samples of
    the given batch.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用给定批次的所有样本计算每个特征的均值和方差。
- en: It subtracts the corresponding feature mean that was previously calculated from
    each feature of every batch sample.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从每个批次样本的每个特征中减去先前计算的相应特征均值。
- en: It divides each feature of every batch sample by the square root of the corresponding
    feature variance.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将每个批次样本的每个特征除以相应特征方差的平方根。
- en: Batch normalization has many benefits. It was initially proposed to solve *internal
    covariate shift*. While training deep networks, the layer's parameters continuously
    change, causing internal layers to constantly adapt and readjust to new distributions
    they see as inputs coming from the preceding layers. This is particularly critical
    for deep networks, where small changes in the first layers are amplified through
    the network. Normalizing the layer's output helps in bounding these shifts, speeding
    up training and generating more reliable models.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化有许多好处。它最初是为了解决*内部协变量偏移*问题而提出的。在训练深度网络时，层的参数不断变化，导致内部层必须不断适应和重新调整，以适应来自前一层的新分布。对于深度网络来说，这是特别关键的，因为第一层的小变化会通过网络被放大。对层输出进行归一化有助于限制这些变化，加速训练并生成更可靠的模型。
- en: 'In addition, using batch normalization, we can do the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过使用批量归一化，我们可以做到以下几点：
- en: We can adopt a higher learning rate without the risk of incurring the problem
    of vanishing or exploding gradients.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以采用更高的学习率，而不必担心出现梯度消失或爆炸的问题。
- en: We can favor network regularization by making its generalization better and
    mitigating overfitting.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过改善网络的泛化能力来有利于网络正则化，从而减轻过拟合问题。
- en: We can make the model become more robust to different initialization schemes
    and learning rates.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使模型对不同的初始化方案和学习率变得更加稳健。
- en: Model Testing and Inference
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型测试与推理
- en: 'Once the model has been trained and its validation performances are satisfactory,
    we can move on to the final stage. As already stated, a final, accurate, model
    performance estimation requires that we test the model on a set of instances it
    has never seen before: the test set. After performance has been confirmed, the
    model can be moved to production for online inference, where it will serve as
    designed: new instances will be provided to the model and it will output predictions,
    leveraging the knowledge it has been designed and trained to have.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成并且验证性能令人满意，我们可以进入最终阶段。如前所述，准确的最终模型性能评估要求我们在从未见过的实例集上测试模型：测试集。性能确认后，模型可以投入生产，用于在线推理，此时它将按设计提供服务：新实例将提供给模型，模型将根据它所设计和训练的知识输出预测。
- en: In the following subsections, three types of neural networks with specific elements/layers
    will be described. They will provide straightforward examples of different technologies
    that are widely encountered in the field.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，将描述三种具有特定元素/层的神经网络。它们将提供一些简单的示例，展示在该领域广泛应用的不同技术。
- en: Standard Fully Connected Neural Networks
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准全连接神经网络
- en: 'The term *fully connected neural network* is commonly used to indicate deep
    neural networks that are only composed of fully connected layers. Fully connected
    layers are the layers whose neurons are connected to all the neurons of the previous
    layer, as well as all the neurons of the next one, as shown in the following diagram:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*全连接神经网络*一词通常用于表示仅由全连接层组成的深度神经网络。全连接层是指神经元与上一层所有神经元以及下一层所有神经元相连的层，如下图所示：'
- en: '![Figure 3.7: A fully connected neural network'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7：一个全连接神经网络'
- en: '](img/B16182_03_07.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_07.jpg)'
- en: 'Figure 3.7: A fully connected neural network'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：一个全连接神经网络
- en: This chapter will mainly deal with fully connected networks. They map inputs
    to outputs through a series of intermediate hidden layers. These architectures
    are capable of handling a wide variety of problems, but they are limited in terms
    of the input dimensions they can handle, as well as the number of layers and number
    of neurons, due to the rapid growth of the number of parameters, which is strictly
    dependent on these variables.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将主要讨论全连接网络。它们通过一系列中间隐藏层将输入映射到输出。这些架构能够处理各种问题，但在输入维度、层数和神经元数目方面有一定的限制，因为参数数量会随着这些变量的增加而迅速增长。
- en: 'An example of a fully connected neural network that will be encountered later
    on is the one presented as follows, built with the Keras API. It connects an input
    layer who dimension is equal to `len(train_dataset.keys())` to an output layer
    of dimension `1`, by means of two hidden layers with `64` neurons each:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将在稍后遇到的全连接神经网络示例如下所示，该网络使用 Keras API 构建。它通过两个隐藏层（每层包含`64`个神经元）将输入层（维度等于`len(train_dataset.keys())`）连接到输出层（维度为`1`）：
- en: '[PRE28]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, let's quickly solve an exercise in order to aid our understanding of fully
    connected neural networks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速完成一个练习，以帮助理解全连接神经网络。
- en: 'Exercise 3.02: Building a Fully Connected Neural Network Model with the Keras
    High-Level API'
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.02：使用 Keras 高级 API 构建全连接神经网络模型
- en: 'In this exercise, we will build a fully connected neural network with an input
    dimension of `100`, 2 hidden layers, and an output layer of `10` neurons. The
    following are the steps to complete this exercise:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建一个全连接神经网络，输入维度为`100`，包含2个隐藏层，输出层为`10`个神经元。完成此练习的步骤如下：
- en: 'Import the `TensorFlow` module and print its version:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`TensorFlow`模块并打印其版本：
- en: '[PRE29]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This prints out the following line:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下行：
- en: '[PRE30]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create the network using the Keras `sequential` module. This allows us to build
    a model by stacking a series of layers, one after the other. In this specific
    case, we''re using two hidden layers and an output layer:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras `sequential` 模块创建网络。这允许我们通过将一系列层按顺序堆叠来构建模型。在此特定案例中，我们使用了两个隐藏层和一个输出层：
- en: '[PRE31]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Print the summary to look at the model description:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印摘要以查看模型描述：
- en: '[PRE32]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be as follows:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE33]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see, the model has been created and the summary provides us with
    a clear understanding of the layers, their types and shapes, and the number of
    parameters of the network, which is very useful when building neural networks
    in real life.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型已经创建，摘要为我们提供了对各层、层类型和形状以及网络参数数量的清晰理解，这在实际构建神经网络时非常有用。
- en: Note
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37s1M5w](https://packt.live/37s1M5w).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/37s1M5w](https://packt.live/37s1M5w)。
- en: You can also run this example online at [https://packt.live/3f9WzSq](https://packt.live/3f9WzSq).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，访问[https://packt.live/3f9WzSq](https://packt.live/3f9WzSq)。
- en: Now, let's move on and understand convolutional neural networks.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续深入理解卷积神经网络。
- en: Convolutional Neural Networks
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'The term **Convolutional Neural Network** (**CNN**) usually identifies a deep
    neural network composed of a combination of the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）一词通常指代由以下组成部分组合而成的深度神经网络：'
- en: Convolutional layers
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully connected layers
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: One of the most successful applications of CNNs is in image and video processing
    tasks. In fact, they are way more capable, with respect to fully connected ones,
    of handling high-dimensional inputs such as images. They are also widely used
    for anomaly detection tasks, being used in autoencoders, as well as encoders for
    reinforcement learning algorithms, specifically for policy and value networks.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）最成功的应用之一是在图像和视频处理任务中。实际上，卷积神经网络相比于全连接神经网络，在处理高维输入（如图像）时更加高效。它们也广泛应用于异常检测任务中，常用于自编码器，以及强化学习算法的编码器，特别是策略和值网络。
- en: Convolutional layers can be thought of as a series of filters applied (convolved)
    to layer inputs to generate layer outputs. The main parameters of these layers
    are the number of filters they have and the dimension of the convolution kernel.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层可以被认为是对输入层应用（卷积）的一系列滤波器，以生成层的输出。这些层的主要参数是滤波器的数量和卷积核的维度。
- en: Pooling layers reduce the dimensions of the data; they combine the outputs of
    neuron clusters at one layer into a single neuron in the next layer. Pooling layers
    may compute a max (**MaxPooling**), which uses the maximum value from each cluster
    of neurons at the prior layer, or an average (**AveragePooling**), which uses
    the average value from each cluster of neurons at the prior layer.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层减少数据的维度；它们将一层中的神经元群体输出合并为下一层的一个神经元。池化层可以计算最大值（**MaxPooling**），即从前一层每个神经元群体中选取最大值，或者计算平均值（**AveragePooling**），即从前一层每个神经元群体中计算平均值。
- en: 'These convolution/pooling operations encode input information in a compressed
    representation, up to a point where these new deep features, also called embeddings,
    are typically provided as inputs to standard fully connected layers at the very
    end of the network. A classic convolutional neural network schematization is represented
    in the following figure:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这些卷积/池化操作将输入信息编码成压缩的表示，直到这些新的深度特征，也称为嵌入，通常作为标准全连接层的输入，出现在网络的最后。经典的卷积神经网络示意图如下所示：
- en: '![Figure 3.8: Convolutional neural network scheme'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8：卷积神经网络示意图'
- en: '](img/B16182_03_08.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_08.jpg)'
- en: 'Figure 3.8: Convolutional neural network scheme'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：卷积神经网络示意图
- en: The following exercise shows how to create a convolutional neural network using
    the Keras high-level API.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习展示了如何使用Keras高级API创建一个卷积神经网络。
- en: 'Exercise 3.03: Building a Convolutional Neural Network Model with the Keras
    High-Level API'
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.03：使用Keras高级API构建卷积神经网络模型
- en: 'This exercise will show you how to build a convolutional neural network with
    three convolutional layers (number of filters equal to `16`, `32`, and `64`, respectively,
    and a kernel size of `3`), alternated with three `MaxPooling` layers, and, at
    the end, two fully connected layers with `512` and `1` neurons, respectively.
    Here is the step-by-step procedure:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习将向你展示如何构建一个具有三层卷积层（每层的滤波器数量分别为`16`、`32`和`64`，卷积核大小为`3`）的卷积神经网络，卷积层与三层`MaxPooling`层交替，最后是两个全连接层，分别具有`512`和`1`个神经元。以下是逐步过程：
- en: 'Import the `TensorFlow` module and print its version:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`TensorFlow`模块并打印其版本：
- en: '[PRE34]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This prints out the following line:'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下行：
- en: '[PRE35]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create the network using the Keras sequential module:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras的顺序模块创建网络：
- en: '[PRE36]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code allows us to build a model by stacking a series of layers,
    one after the other. In this specific case, three series of convolutional layers
    and max pooling layers are followed by a flattening layer and two dense layers.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码让我们通过一系列层逐个堆叠来构建模型。在这个特定的案例中，三组卷积层和最大池化层之后接着一个展平层和两个全连接层。
- en: 'This outputs the following model description:'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下模型描述：
- en: '[PRE37]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Thus, we have successfully created a CNN using Keras. The preceding summary
    gives us significant information about the layers and the different parameters
    of the network.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们成功地使用Keras创建了一个CNN。前面的总结为我们提供了有关网络层和不同参数的关键信息。
- en: Note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AZJqwn](https://packt.live/2AZJqwn).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/2AZJqwn](https://packt.live/2AZJqwn)。
- en: You can also run this example online at [https://packt.live/37p1OuX](https://packt.live/37p1OuX).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在[https://packt.live/37p1OuX](https://packt.live/37p1OuX)上在线运行此示例。
- en: 'Now that we''ve dealt with convolutional neural networks, let''s focus on another
    important architecture family: recurrent neural networks.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了卷积神经网络，让我们关注另一个重要的架构家族：循环神经网络。
- en: Recurrent Neural Networks
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Recurrent neural networks are models composed of particular units that, in the
    same way as feedforward networks, are able to process data from input to output,
    but, unlike them, are also able to process data in the opposite direction using
    feedback loops. They are basically designed so that the output of a layer is redirected
    and becomes the input of the same layer using specific internal states capable
    of "remembering" previous states.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是由特定单元组成的模型，它们与前馈网络类似，能够处理从输入到输出的数据，但与前馈网络不同的是，它们还能够通过反馈回路处理反向数据流。它们的基本设计是使一层的输出被重定向并成为该层的输入，利用特定的内部状态来“记住”先前的状态。
- en: This specific feature makes them particularly suited for solving tasks characterized
    by temporal/sequential development. It can be useful to compare CNNs and RNNs
    to understand which problems one is more suited to than the other. CNNs are the
    best fit for problems where local coherence is strongly enhanced and is particularly
    the case for images/video. Local coherence is exploited to drastically reduce
    the number of weights needed to process high-dimensional inputs. RNNs, on the
    other hand, perform best on problems characterized by temporal development, which
    means tasks where data can be represented by time series. This is the case for
    natural language processing or speech recognition, where words and sounds are
    meaningful if they're considered in a specific sequence.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这一特性使得它们特别适合解决具有时间/序列发展的任务。比较 CNN 和 RNN 可以帮助理解它们各自更适合解决哪些问题。CNN 最适合解决局部一致性较强的问题，尤其是在图像/视频处理方面。局部一致性被利用来大幅减少处理高维输入所需的权重数目。而
    RNN 则在处理具有时间序列数据的问题时表现最好，这意味着任务可以通过时间序列来表示。这对于自然语言处理或语音识别尤为重要，因为单词和声音只有在特定顺序中才有意义。
- en: 'Recurrent architectures can be thought of as sequences of operations, and they
    are perfectly designed to keep track of historical data:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 递归架构可以被看作是一系列操作，它们非常适合追踪历史数据：
- en: '![Figure 3.9: Recurrent neural network block diagram'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9：递归神经网络框图'
- en: '](img/B16182_03_09.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_09.jpg)'
- en: 'Figure 3.9: Recurrent neural network block diagram'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：递归神经网络框图
- en: The most important components they are based on are GRUs and LSTMs. These blocks
    have internal elements and states explicitly dedicated to keeping track of important
    information for the task they aim to solve. They both address the issue of learning
    long-term dependencies successfully when training machine learning algorithms
    on temporal data. They tackle this problem by storing "memory" from data seen
    in the past in order to help the network make predictions in the future.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 它们最重要的组成部分是 GRU 和 LSTM。这些模块包含专门用于追踪解决任务时重要信息的内部元素和状态。它们都成功地解决了在训练机器学习算法时学习长期依赖性的问题，尤其是在时间数据上。它们通过存储过去数据中的“记忆”来帮助网络对未来进行预测。
- en: The main differences between GRUs and LSTMs are the number of gates, the inputs
    the unit has, and the cell states, which are the internal elements the make up
    the unit's memory. GRUs have one gate, while LSTMs have three gates, called the
    input, forget, and output gates. LSTMs are more flexible than GRUs since they
    have more parameters, which, on the other hand, makes them less efficient in terms
    of both memory and time.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 和 LSTM 之间的主要区别在于门的数量、单元的输入和单元状态，后者是构成单元记忆的内部元素。GRU 只有一个门，而 LSTM 有三个门，分别称为输入门、遗忘门和输出门。由于
    LSTM 拥有更多的参数，它们比 GRU 更加灵活，但这也使得 LSTM 在内存和时间效率上不如 GRU。
- en: These networks have been responsible for the great advancements in fields such
    as speech recognition, natural language processing, text-to-speech, machine translation,
    language modeling, and many other similar tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络已经在语音识别、自然语言处理、文本转语音、机器翻译、语言建模以及许多其他类似任务的领域中取得了巨大的进展。
- en: 'The following is a block diagram of a typical GRU:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是典型 GRU 的框图：
- en: '![Figure 3.10: Block diagram of a GRU'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.10：GRU 的框图'
- en: '](img/B16182_03_10.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_10.jpg)'
- en: 'Figure 3.10: Block diagram of a GRU'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：GRU 的框图
- en: 'The following is a block diagram of a typical LSTM:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是典型 LSTM 的框图：
- en: '![Figure 3.11: Block diagram of an LSTM'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.11：LSTM 的框图'
- en: '](img/B16182_03_11.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_11.jpg)'
- en: 'Figure 3.11: Block diagram of an LSTM'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11：LSTM 的框图
- en: The following exercise shows how a recurrent network with LSTM units can be
    created using the Keras API.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习展示了如何使用 Keras API 创建一个包含 LSTM 单元的递归网络。
- en: 'Exercise 3.04: Building a Recurrent Neural Network Model with the Keras High-Level
    API'
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.04：使用 Keras 高级 API 构建一个递归神经网络模型
- en: 'In this exercise, we will create a recurrent neural network using the Keras
    high-level API. It will have the following architecture: the very first layer
    is simply a layer that encodes, using certain rules, the input features, thereby
    producing a given set of embeddings. The second layer is a layer where `64` LSTM
    units are added to it. They are added inside a bidirectional wrapper, which is
    a specific layer that''s used to improve and speed up learning by doubling the
    units it acts on and training the first ones with the input as-is, and the second
    ones with the input reversed (for example, words in a sentence read from right
    to left). Then, the outputs are concatenated. This technique has been proven to
    generate faster and better learning. Finally, two dense layers are added that
    have `64` and `1` neurons, respectively. Perform the following steps to complete
    this exercise:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用Keras高级API创建一个递归神经网络。它将具有以下架构：第一层只是一个编码层，使用特定规则对输入特征进行编码，从而生成一组嵌入向量。第二层是一个包含`64`个LSTM单元的层。它们被添加到一个双向包装器中，这个特定的层用于通过将其作用于的单元加倍来加速学习，第一个单元直接使用输入数据，第二个单元则使用反向输入（例如，按从右到左的顺序读取句子中的单词）。然后，输出会被拼接起来。证明这种技术能够生成更快、更好的学习效果。最后，添加了两个全连接层，分别包含`64`和`1`个神经元。请按照以下步骤完成本练习：
- en: 'Import the `TensorFlow` module and print its version:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`TensorFlow`模块并打印其版本：
- en: '[PRE38]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This outputs the following line:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE39]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Build the model using the Keras `sequential` method and print the network summary:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras的`sequential`方法构建模型并打印网络摘要：
- en: '[PRE40]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the preceding code, the model is simply built by stacking up consecutive
    layers. First, there is the embedding layer, then the bidirectional one, which
    operates on the LSTM layer, and finally two dense layers at the end of the model.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，模型通过堆叠连续的层来构建。首先是嵌入层，然后是双向层，它作用于LSTM层，最后是模型末尾的两个全连接层。
- en: 'The model summary will be as follows:'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型摘要将如下所示：
- en: '[PRE41]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3cX01OO](https://packt.live/3cX01OO).
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3cX01OO](https://packt.live/3cX01OO)。
- en: You can also run this example online at [https://packt.live/37nw1ud](https://packt.live/37nw1ud).
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/37nw1ud](https://packt.live/37nw1ud)上在线运行这个例子。
- en: With this overview of how to implement a neural network using TensorFlow, the
    following sections will show you how to combine all these notions to tackle typical
    machine learning problems, including regression and classification problems.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解如何使用TensorFlow实现神经网络，接下来的章节将展示如何将所有这些概念结合起来，解决典型的机器学习问题，包括回归和分类问题。
- en: Simple Regression Using TensorFlow
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行简单回归
- en: This section will explain, step by step, how to successfully tackle a regression
    problem. You will learn how to take a preliminary look at the dataset to understand
    its most important properties, as well as how to prepare it to be used during
    training, validation, and inference. Then, a deep neural network will be built
    from a clean sheet using TensorFlow via the Keras API. This model will then be
    trained and its performance will be evaluated.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将逐步解释如何成功解决回归问题。你将学习如何初步查看数据集，以了解其最重要的属性，并了解如何为训练、验证和推理做好准备。接下来，将使用TensorFlow通过Keras
    API从零开始构建深度神经网络。然后，训练该模型并评估其性能。
- en: 'In a regression problem, the aim is to predict the output of a continuous value,
    such as a price or a probability. In this exercise, the classic Auto MPG dataset
    will be used and a deep neural network will be trained on it to accurately predict
    car fuel efficiency, using no more than the following seven features: Cylinders,
    Displacement, Horsepower, Weight, Acceleration, Model Year, and Origin.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，目标是预测一个连续值的输出，例如价格或概率。在本练习中，将使用经典的Auto MPG数据集，并在其上训练一个深度神经网络，以准确预测汽车的燃油效率，使用的特征仅限于以下七个：气缸数、排量、马力、重量、加速度、模型年份和原产地。
- en: 'The dataset can be thought of as a table with eight columns (seven features,
    plus one target value) and as many rows as instances the dataset has. As per the
    best practices we looked at in the previous sections, it will be divided as follows:
    20% of the total number of instances will create the test set, while the remaining
    ones will be split again into training and validation sets with an 80:20 ratio.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以看作是一个具有八列（七个特征和一个目标值）的表格，并且有与数据集实例数量相同的行数。根据我们在前面的章节中讨论的最佳实践，它将按如下方式划分：20%的实例将用作测试集，剩余的部分将再次按
    80:20 的比例划分为训练集和验证集。
- en: As a first step, the training set will be inspected for missing values, and
    cleaned if needed. Then, a chart showing variable correlation will be plotted.
    The only categorical variable present will be converted into numerical form via
    one-hot encoding. Finally, all the features will be normalized.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，将检查训练集中的缺失值，并在需要时进行清理。接下来，将绘制一个展示变量相关性的图表。唯一存在的类别变量将通过独热编码转换为数值形式。最后，所有特征将进行标准化。
- en: 'The deep learning model will then be created. A three-layered fully connected
    architecture will be used: the first and the second layer will have 64 nodes,
    while the last one, being the output layer of a regression problem, will have
    only one node.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将创建深度学习模型。使用三层全连接架构：第一层和第二层各有64个节点，而最后一层作为回归问题的输出层，只有一个节点。
- en: Standard choices for the loss function (mean squared error) and optimizer (RMSprop)
    will be applied. Training will then be performed with and without early stopping
    to highlight the different effects they have on training and validation loss.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的损失函数（均方误差）和优化器（RMSprop）将被应用。接下来，将进行训练，分别带有和不带有早停机制，以突出它们对训练和验证损失的不同影响。
- en: Finally, the model will be applied to the test set to evaluate performances
    and make predictions.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型将应用于测试集，以评估性能并进行预测。
- en: 'Exercise 3.05: Creating a Deep Neural Network to Predict the Fuel Efficiency
    of Cars'
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.05：创建一个深度神经网络来预测汽车的燃油效率
- en: 'In this exercise, we will build, train, and measure performances of a deep
    neural network model that predicts car fuel efficiency using only seven car features:
    `Cylinders`, `Displacement`, `Horsepower`, `Weight`, `Acceleration`, `Model Year`,
    and `Origin`.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建、训练并评估一个深度神经网络模型，利用七个汽车特征预测汽车的燃油效率：`Cylinders`、`Displacement`、`Horsepower`、`Weight`、`Acceleration`、`Model
    Year` 和 `Origin`。
- en: 'The step-by-step procedure for this is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的步骤如下：
- en: 'Import all the required modules and print the versions of the most important
    ones:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有所需模块，并打印出最重要模块的版本：
- en: '[PRE42]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output will be as follows:'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE43]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Import the Auto MPG dataset, read it with pandas, and show the last five rows:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 Auto MPG 数据集，使用 pandas 读取并显示最后五行：
- en: '[PRE44]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Watch out for the slashes in the string below. Remember that the backslashes
    ( `\` ) are used to split the code across multiple lines, while the forward slashes
    ( `/` ) are part of the URL.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意下面字符串中的斜杠。记住，反斜杠（`\`）用于将代码拆分到多行，而正斜杠（`/`）是 URL 的一部分。
- en: 'The output will be as follows:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.12: Last five rows of the dataset imported in pandas'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.12：数据集导入 pandas 后的最后五行'
- en: '](img/B16182_03_12.jpg)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_12.jpg)'
- en: 'Figure 3.12: Last five rows of the dataset imported in pandas'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.12：数据集导入 pandas 后的最后五行
- en: 'Let''s clean the data from unknown values. Check how much `Not available` data
    is present and where:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理数据中的未知值。检查有多少`Not available`数据以及其所在位置：
- en: '[PRE45]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This produces the following output:'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE46]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Given the small number of rows with unknown values, simply drop them:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于未知值的行数较少，只需将其删除：
- en: '[PRE47]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Use one-hot encoding for the `Origin` variable, which is categorical:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 `Origin` 变量使用独热编码，它是类别型变量：
- en: '[PRE48]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be as follows:'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.13: Last five rows of the dataset imported into pandas using one-hot
    encoding'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.13：使用独热编码将数据集导入 pandas 后的最后五行'
- en: '](img/B16182_03_13.jpg)'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_13.jpg)'
- en: 'Figure 3.13: Last five rows of the dataset imported into pandas using one-hot
    encoding'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.13：使用独热编码将数据集导入 pandas 后的最后五行
- en: 'Split the data into training and test sets with an 80:20 ratio:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据按 80:20 的比例分为训练集和测试集：
- en: '[PRE49]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let''s take a look at some training data statistics, that is, the joint
    distributions of some pairs of features from the training set, using the `seaborn`
    module. The `pairplot` command takes in the features of the dataset as input to
    evaluate them, couple by couple. Along the diagonal (where the couple is composed
    of two instances of the same feature), it shows the distribution of the variable,
    while in the off-diagonal terms, it shows the scatterplot of the two features.
    This is useful if we wish to highlight correlations:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看一下训练数据统计，即使用`seaborn`模块展示训练集中的一些特征对的联合分布。`pairplot`命令将数据集的特征作为输入进行评估，逐对处理。在对角线上（其中一对由相同特征的两个实例组成），显示变量的分布，而在非对角项中，显示这两个特征的散点图。如果我们希望突出显示相关性，这会非常有用：
- en: '[PRE50]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This generates the following image:'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下图像：
- en: '![Figure 3.14: Joint distributions of some pairs of features from the training
    set'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.14：训练集中的一些特征对的联合分布'
- en: '](img/B16182_03_14.jpg)'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_14.jpg)'
- en: 'Figure 3.14: Joint distributions of some pairs of features from the training
    set'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.14：训练集中的一些特征对的联合分布
- en: 'Let''s now take a look at the overall statistics:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们来看一下整体统计数据：
- en: '[PRE51]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output will be as follows:'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 3.15: Overall training set statistics'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.15：整体训练集统计'
- en: '](img/B16182_03_15.jpg)'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_15.jpg)'
- en: 'Figure 3.15: Overall training set statistics'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.15：整体训练集统计
- en: 'Split the features from the labels and normalize the data:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征与标签分开并对数据进行归一化：
- en: '[PRE52]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, let''s look at the model''s creation and a summary of the same:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们查看模型的创建及其摘要：
- en: '[PRE53]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This generates the following output:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 3.16: Model summary'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.16：模型摘要'
- en: '](img/B16182_03_16.jpg)'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_16.jpg)'
- en: 'Figure 3.16: Model summary'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.16：模型摘要
- en: 'Use the `fit` model function to train the network for 1,000 epochs by using
    a validation set of 20%:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit`模型函数，通过使用20%的验证集来训练网络1000个周期：
- en: '[PRE54]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will produce a very long output. We will only report the last few lines
    here:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生非常长的输出。我们这里只报告最后几行：
- en: '[PRE55]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Visualize the training and validation metrics by plotting the MAE and MSE.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制平均绝对误差（MAE）和均方误差（MSE）来可视化训练和验证指标。
- en: 'The following snippet plots the MAE:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码段绘制了平均绝对误差（MAE）：
- en: '[PRE56]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output will be as follows:'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 3.17: Mean absolute error over the plot of epochs'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.17：每个周期图中的平均绝对误差'
- en: '](img/B16182_03_17.jpg)'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_17.jpg)'
- en: 'Figure 3.17: Mean absolute error over the plot of epochs'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.17：每个周期图中的平均绝对误差
- en: The preceding figure shows how increasing the training epochs causes the validation
    error to grow, meaning the system is experiencing an overfitting problem.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的图表显示了增加训练周期数如何导致验证误差增加，这意味着系统正经历过拟合问题。
- en: 'Now, let''s visualize the MSE using a plot:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过绘制图表来可视化均方误差（MSE）：
- en: '[PRE57]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output will be as follows:'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 3.18: Mean squared error over the plot of epochs'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.18：每个周期图中的均方误差'
- en: '](img/B16182_03_18.jpg)'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_18.jpg)'
- en: 'Figure 3.18: Mean squared error over the plot of epochs'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.18：每个周期图中的均方误差
- en: Also, in this case, the figure shows how increasing the training epochs causes
    the validation error to grow, meaning the system is experiencing an overfitting
    problem.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，图表显示了增加训练周期数如何导致验证误差增加，这意味着系统正经历过拟合问题。
- en: 'Use Keras callbacks to add early stopping (with the patience parameter equal
    to 10 epochs) to avoid overfitting. First of all, build the model:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras回调来添加早停（耐心参数设置为10个周期）以避免过拟合。首先，构建模型：
- en: '[PRE58]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Then, define an early stopping callback. This entity will be passed to the
    `model.fit` function and will be called every fit step to check whether the validation
    error stops decreasing for more than `10` consecutive epochs:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，定义一个早停回调。这个实体将被传递到`model.fit`函数中，并在每次拟合步骤中被调用，以检查验证误差是否在超过`10`个连续周期后停止下降：
- en: '[PRE59]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, call the `fit` method with the early stop callback:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，调用带有早停回调的`fit`方法：
- en: '[PRE60]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The last few lines of the output are as follows:'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE61]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Visualize the train and validation metrics for early stopping. Firstly, collect
    all the training history data and put it into a pandas DataFrame, for both the
    metric and epoch values:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化训练和验证指标以进行早停。首先，收集所有的训练历史数据，并将其放入一个pandas DataFrame中，包括指标和周期值：
- en: '[PRE62]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, plot the training and validation MAE against the epochs, limiting the
    max `y` values to `10`:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，绘制训练和验证的平均绝对误差（MAE）与周期的关系，并将最大`y`值限制为`10`：
- en: '[PRE63]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The preceding code will produce the following output:'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将产生以下输出：
- en: '![Figure 3.19: Mean absolute error over the plot of epochs (early stopping)'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.19：在训练轮次图中的平均绝对误差（早停法）'
- en: '](img/B16182_03_19.jpg)'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_19.jpg)'
- en: 'Figure 3.19: Mean absolute error over the plot of epochs (early stopping)'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.19：在训练轮次图中的平均绝对误差（早停法）
- en: As demonstrated by the preceding figure, training is stopped as soon as the
    validation error stops decreasing, thereby avoiding overfitting.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前图所示，训练会在验证误差停止下降时停止，从而避免过拟合。
- en: 'Evaluate the model accuracy on the test set:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估模型的准确性：
- en: '[PRE64]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output will be as follows:'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE65]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Note
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于随机抽样并使用可变的随机种子，准确度可能会显示略有不同的值。
- en: 'Finally, perform model inference by predicting all the MPG values for all test
    instances. Then, plot these values with respect to their true values so that you
    have a visual estimation of the model error:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过预测所有测试实例的 MPG 值来执行模型推断。然后，将这些值与它们的真实值进行比较，从而得到模型误差的视觉估计：
- en: '[PRE66]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output will be as follows:'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 3.20: Predictions versus ground truth scatterplot'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.20：预测值与真实值的散点图'
- en: '](img/B16182_03_20.jpg)'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_20.jpg)'
- en: 'Figure 3.20: Predictions versus ground truth scatterplot'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20：预测值与真实值的散点图
- en: The scatterplot puts predicted values versus true values in correspondence with
    one another, which means that the closer the points are to the diagonal line,
    the more accurate the predictions will be. It is evident how clustered the points
    are, meaning predictions are fairly accurate.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图将预测值与真实值进行对应，这意味着点越接近对角线，预测越准确。可以明显看出点的聚集程度，说明预测非常准确。
- en: Note
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3feCLNN](https://packt.live/3feCLNN).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/3feCLNN](https://packt.live/3feCLNN)。
- en: You can also run this example online at [https://packt.live/37n5WeM](https://packt.live/37n5WeM).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，访问 [https://packt.live/37n5WeM](https://packt.live/37n5WeM)。
- en: This section has shown how to successfully tackle a regression problem. The
    selected dataset has been imported, cleaned, and subdivided into training, validation,
    and test sets. Then, a brief exploratory data analysis was carried out before
    a three-layered fully connected deep neural network was created. The network has
    been successfully trained and its performance has been evaluated on the test set.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何成功地解决回归问题。所选的数据集已经导入、清洗并细分为训练集、验证集和测试集。然后，进行了简要的探索性数据分析，接着创建了一个三层全连接的深度神经网络。该网络已经成功训练，并且在测试集上评估了其表现。
- en: Now, let's study classification problems using TensorFlow.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 TensorFlow 来研究分类问题。
- en: Simple Classification Using TensorFlow
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行简单分类
- en: This section will help you understand and solve a typical supervised learning
    problem that falls under the category conventionally named **classification**.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将帮助你理解并解决一个典型的监督学习问题，这个问题属于传统上称为**分类**的类别。
- en: Classification tasks, in their simplest generic form, aim to associate one category,
    among a predefined set, with instances. An intuitive example of a classification
    task that's often used for introductory courses is classifying the images of domestic
    pets in the correct category they belong to, such as "cat" or "dog." Classification
    plays a fundamental role in many everyday activities and can easily be encountered
    in different contexts. The previous example is a specific case of classification
    called **image classification**, and many similar applications can be found in
    this category.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务在其最简单的通用形式中，旨在将一个类别与一组预定义的实例关联起来。一个常用于入门课程的直观分类任务示例是将家庭宠物的图片分类到它们所属的正确类别中，例如“猫”或“狗”。分类在许多日常活动中发挥着基础性作用，且在不同的情境中很容易遇到。前面的例子是一个特定的分类任务，称为**图像分类**，在这一类别中可以找到许多类似的应用。
- en: 'However, classification extends beyond images. The following are some examples:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分类不仅限于图像。以下是一些例子：
- en: Customer classification for video recommendation systems (answering the question,
    "In which market segment this user falls?")
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频推荐系统的客户分类（回答问题：“该用户属于哪个市场细分？”）
- en: Spam filters ("What are the chances this email is spam?")
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤器（“这封邮件是垃圾邮件的可能性有多大？”）
- en: Malware detection ("Is this program a cyber threat?")
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件检测（"这个程序是网络威胁吗？"）
- en: Medical diagnosis ("Is this patient sick?")
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学诊断（"这个病人有病吗？"）
- en: For image classification tasks, images are fed to the classification algorithm
    as inputs, and it returns the class they belong to as output. Images are three-dimensional
    arrays of numbers representing per-pixel brightness (height x width x number of
    channels, where color images have three channels – red, green, blue (RGB) – and
    grayscale images only have one), and these numbers are the features that the algorithm
    uses to determine the class images belong to.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类任务，图像作为输入传递给分类算法，算法返回它们所属的类别作为输出。图像是三维数组，表示每个像素的亮度（高度 x 宽度 x 通道数，其中彩色图像有三个通道——红色、绿色、蓝色（RGB），而灰度图像只有一个），这些数字是算法用来确定图像所属类别的特征。
- en: When dealing with other types of inputs, features can be different. For example,
    in the case of a medical diagnosis classification system, blood test parameters,
    age, sex, and suchlike can be features that are used by the algorithm to identify
    the class the instance belongs to, that is, "sick" or "not sick."
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 处理其他类型的输入时，特征可能有所不同。例如，在医学诊断分类系统中，血液检查参数、年龄、性别等可以作为特征，供算法用来识别实例所属的类别，即“生病”或“未生病”。
- en: 'In the following exercise, we will create a deep neural network by building
    upon what we described in the previous sections. This will be able to achieve
    an accuracy of around 70% when classifying signals that have been detected inside
    a simulated ATLAS experiment, distinguishing between background noise and Higgs
    Boson Tau-Tau decay using a set of 28 features: yes, machine learning applied
    to particle physics!'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，我们将基于前面部分的内容创建一个深度神经网络。它将在对ATLAS实验中检测到的信号进行分类时达到约70%的准确率，区分背景噪声与希格斯玻色子τ-τ衰变，使用28个特征：没错，机器学习也可以应用于粒子物理学！
- en: Note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'For additional information on the dataset, visit the official website: [http://archive.ics.uci.edu/ml/datasets/HIGGS](http://archive.ics.uci.edu/ml/datasets/HIGGS).'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据集的更多信息，请访问官方网站：[http://archive.ics.uci.edu/ml/datasets/HIGGS](http://archive.ics.uci.edu/ml/datasets/HIGGS)。
- en: 'Given the huge size of the dataset, to keep the exercise easy to run and still
    meaningful, it will be subsampled: 10,000 rows will be used for training and 1,000
    rows each for validation and test. Three different models will be trained: a small
    model that will be a reference (two layers with 16 and 1 neurons each), a large
    model with no overfit countermeasures (five layers; four with 512 neurons and
    the last one with 1 neuron) to demonstrate problems that may be encountered in
    this scenario, and then regularization and dropout will be added to the large
    model, effectively limiting overfitting and improving performance.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数据集的巨大规模，为了使练习便于运行并且仍然有意义，我们将对数据进行子采样：训练集将使用10,000行，验证集和测试集各使用1,000行。将训练三种不同的模型：一个小模型作为参考（两层，每层分别为16和1个神经元），一个不带防止过拟合措施的大模型（五层；四层有512个神经元，最后一层有1个神经元），用以展示在这种情况下可能遇到的问题，随后将向大模型添加正则化和dropout，有效限制过拟合并提高性能。
- en: 'Exercise 3.06: Creating a Deep Neural Network to Classify Events Generated
    by the ATLAS Experiment in the Quest for Higgs Boson'
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.06：创建一个深度神经网络，分类ATLAS实验中为寻找希格斯玻色子而生成的事件
- en: 'In this exercise, we will build, train, and measure the performance of a deep
    neural network in order to improve the discovery significance of the ATLAS experiment
    by using simulated data with features for characterizing events. The task is to
    classify events into two categories: "tau decay of a Higgs Boson" versus "background."'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建、训练并测量深度神经网络的性能，以通过使用带有特征的模拟数据来提高ATLAS实验的发现显著性，从而对事件进行分类。任务是将事件分类为两类：“希格斯玻色子的τ衰变”与“背景”。
- en: This dataset can be found in the TensorFlow dataset ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)),
    which is a collection of ready-to-use datasets. It is available to download and
    interface via the processing pipeline. In our case, the original dataset is too
    big for our purposes, so we will postpone dataset usage until we get to this chapter's
    activity. For now, we will use a subgroup of the dataset that's directly available
    through the repository.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集可以在TensorFlow数据集（[https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)）中找到，它是一个现成的可用数据集集合。可以通过处理管道进行下载和接口。由于我们当前的用途，原始数据集太大，因此我们将在本章活动中使用该数据集时再使用它。现在，我们将使用通过仓库直接提供的数据集子集。
- en: Note
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the dataset in this book''s GitHub repository here: [https://packt.live/3dUfYq8](https://packt.live/3dUfYq8).'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的 GitHub 仓库中找到数据集，链接地址是：[https://packt.live/3dUfYq8](https://packt.live/3dUfYq8)。
- en: 'The step-by-step procedure is described in detail as follows:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤逐一描述如下：
- en: 'Import all the required modules and print the versions of the most important
    ones:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的模块并打印最重要模块的版本：
- en: '[PRE67]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output will be as follows:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE68]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Import the dataset and prepare the data for preprocessing.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集并为预处理准备数据。
- en: 'For this exercise, we will download a custom-made smaller subset that''s been
    pulled from the original dataset:'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于本次练习，我们将下载一个从原始数据集中提取的小型自定义子集：
- en: '[PRE69]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Read the CSV dataset into a TensorFlow dataset class and repack it so that
    it has tuples (`features`, `labels`):'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV数据集读取为TensorFlow数据集类，并重新打包成包含元组（`features`，`labels`）的形式：
- en: '[PRE70]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Take a look at the value distribution of the features:'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看特征的值分布：
- en: '[PRE71]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output will be as follows:'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE72]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The plot will be as follows:'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图将如下所示：
- en: '![Figure 3.21: First feature value distribution'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.21：第一特征值分布'
- en: '](img/B16182_03_21.jpg)'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_21.jpg)'
- en: 'Figure 3.21: First feature value distribution'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.21：第一特征值分布
- en: In the preceding graph, the *x* axis represents the number of training samples
    for a given value, while the *y* axis denotes the first feature's numerical value.
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的图中，*x*轴表示给定值的训练样本数量，而*y*轴表示第一个特征的数值。
- en: 'Create training, validation, and test sets:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练集、验证集和测试集：
- en: '[PRE73]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define feature, label, and class names:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义特征、标签和类别名称：
- en: '[PRE74]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output will be as follows:'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE75]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Show a sample of a training instance for features and labels:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示一个训练实例的特征和标签示例：
- en: '[PRE76]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The output will be as follows:'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE77]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Assign a batch size to the datasets:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给数据集分配批次大小：
- en: '[PRE78]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Now, let''s start creating the model and training it. Create a decaying learning
    rate:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始创建模型并进行训练。创建一个递减学习率：
- en: '[PRE79]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Define a function that will compile a model with an `Adam` optimizer, use binary
    cross entropy as the `loss` function, and fit it on training data by using early
    stopping on the validation dataset.
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将使用`Adam`优化器编译模型，使用二元交叉熵作为`loss`函数，并通过在验证数据集上使用早停法对训练数据进行拟合。
- en: 'The function takes in the model as input, chooses the `Adam` optimizer, and
    compiles the model with it, as well as with the binary cross entropy loss and
    the accuracy metrics:'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数以模型作为输入，选择`Adam`优化器，并使用二元交叉熵损失和准确度指标对模型进行编译：
- en: '[PRE80]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'A summary of the model is then printed, as follows:'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后打印模型的摘要，如下所示：
- en: '[PRE81]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The model is then fitted on the training dataset using a validation dataset
    and the early stopping callback. The training `history` is saved and returned
    as output:'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用验证数据集和早停回调在训练数据集上拟合模型。训练的`history`被保存并作为输出返回：
- en: '[PRE82]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Create a small model with just two layers with 16 and 1 neurons, respectively,
    and compile it and fit it on the dataset:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个仅有两层的小型模型，分别为16个和1个神经元，并对其进行编译并在数据集上进行拟合：
- en: '[PRE83]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'This will produce a long output, where the last two lines will be similar to
    the following:'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生一个较长的输出，其中最后两行将类似于以下内容：
- en: '[PRE84]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Check the model''s performance on the test set:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型在测试集上的表现：
- en: '[PRE85]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The output will be as follows:'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE86]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Note
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于使用了具有可变随机种子的随机抽样，准确率可能会显示略有不同的值。
- en: 'Create a large model with five layers – four with `512` neurons and the last
    one with `1` neuron, respectively – and compile and fit it:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有五层的大型模型——前四层分别为`512`个神经元，最后一层为`1`个神经元——并对其进行编译和拟合：
- en: '[PRE87]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'This will produce a long output, where the last two lines will be similar to
    the following:'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生一个较长的输出，最后两行将类似于以下内容：
- en: '[PRE88]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Check the model''s performance on the test set:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型在测试集上的表现：
- en: '[PRE89]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The output will be as follows:'
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE90]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Note
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于使用带有可变随机种子的随机抽样，准确度可能会显示出略有不同的值。
- en: 'Create the same large model as before, but add regularization items such as
    L2 regularization and dropout. Then, compile it and fit the model to the set:'
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建与之前相同的大型模型，但添加正则化项，如L2正则化和dropout。然后，编译并将模型拟合到数据集上：
- en: '[PRE91]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This will produce a long output, where the last two lines will be similar to
    the following:'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生一个较长的输出，最后两行将类似于以下内容：
- en: '[PRE92]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Check the model''s performance on the test set:'
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型在测试集上的表现：
- en: '[PRE93]'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output will be as follows:'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE94]'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Note
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于使用带有可变随机种子的随机抽样，准确度可能会显示出略有不同的值。
- en: 'Compare the binary cross entropy trend of the three models over epochs:'
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较三种模型在训练轮次中的二元交叉熵趋势：
- en: '[PRE95]'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'This will produce the following graph:'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 3.22: Binary cross entropy comparison'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.22：二元交叉熵比较'
- en: '](img/B16182_03_22.jpg)'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_22.jpg)'
- en: 'Figure 3.22: Binary cross entropy comparison'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.22：二元交叉熵比较
- en: The preceding graph shows a comparison of the different models, in terms of
    both training and validation errors, to demonstrate how overfitting works. The
    training error goes down for each of them as the number of training epochs increases.
    The validation error for the large model, on the other hand, rapidly increases
    after a certain number of epochs. In the small model, it goes down, following
    the training error closely and reaching a final performance that is worse than
    the one obtained by the model with regularization, which avoids overfitting and
    has the best performance among the three.
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述图表展示了不同模型在训练和验证误差方面的比较，以演示过拟合的工作原理。每个模型的训练误差随着训练轮次的增加而下降。而大型模型的验证误差则在经过一定轮次后迅速增加。在小型模型中，验证误差下降，紧跟着训练误差，并最终表现出比带有正则化的模型更差的结果，后者避免了过拟合，并在三者中表现最佳。
- en: 'Compare the accuracy trend of the three models over epochs:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较三种模型在训练轮次中的准确度趋势：
- en: '[PRE96]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'This will produce the following graph:'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 3.23: Accuracy comparison'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.23：准确度比较'
- en: '](img/B16182_03_23.jpg)'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_23.jpg)'
- en: 'Figure 3.23: Accuracy comparison'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23：准确度比较
- en: In a specular way with respect to the previous one, this graph shows, once again,
    a comparison of the different models, but in terms of accuracy. The training accuracy
    grows for each model when the number of training epochs increases. The validation
    accuracy for the large model, on the other hand, stops growing after a certain
    number of epochs. In the small model, it goes up, following the training one closely
    and reaching a final performance that is worse than the one obtained by the model
    with regularization, which avoids overfitting and attains the best performance
    among the three.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的图表以镜像方式类似，这个图表再次展示了不同模型的比较，但从准确度的角度来看。当训练的轮次增加时，每个模型的训练准确度都会提高。另一方面，大型模型的验证准确度在经过若干轮次后停止增长。而在小型模型中，验证准确度上升，并紧跟着训练准确度，最终的表现差于带有正则化的模型，后者避免了过拟合，并在三者中达到了最佳表现。
- en: Note
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37m9huu](https://packt.live/37m9huu).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取此特定部分的源代码，请参考[https://packt.live/37m9huu](https://packt.live/37m9huu)。
- en: You can also run this example online at [https://packt.live/3hhIDaZ](https://packt.live/3hhIDaZ).
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3hhIDaZ](https://packt.live/3hhIDaZ)在线运行此示例。
- en: In this section, we solved a fancy classification problem, resulting in the
    creation of a deep learning model able to achieve about 70% accuracy when classifying
    Higgs boson-related signals using simulated ATLAS experiment data. After a first
    general overview of the dataset, where we understood how it is arranged and the
    nature of its features and labels, a set of three deep fully connected neural
    networks were created using the Keras API. These models were trained and tested,
    and their performances in terms of loss and accuracy over epochs have been compared,
    thereby giving us a firm grasp of the overfitting problem and which techniques
    help in solving it.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们解决了一个复杂的分类问题，从而创建了一个深度学习模型，能够在使用模拟的 ATLAS 实验数据对希格斯玻色子相关信号进行分类时达到约 70%
    的准确率。经过对数据集的初步概览，了解了它的组织方式以及特征和标签的性质后，使用 Keras API 创建了三层深度全连接神经网络。这些模型经过训练和测试，并比较了它们在各个周期中的损失和准确率，从而使我们牢牢掌握了过拟合问题，并知道哪些技术有助于解决该问题。
- en: TensorBoard – How to Visualize Data Using TensorBoard
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard - 如何使用 TensorBoard 可视化数据
- en: 'TensorBoard is a web-based tool embedded in TensorFlow. It provides a suite
    of methods that we can use to get insights into TensorFlow sessions and graphs,
    thus allowing the user to inspect, visualize, and understand them deeply. It provides
    access to many functionalities in a straightforward way, as follows:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是一个嵌入在 TensorFlow 中的基于 Web 的工具。它提供了一套方法，我们可以用来深入了解 TensorFlow 会话和图，从而使用户能够检查、可视化并深刻理解它们。它以直观的方式提供许多功能，如下所示：
- en: It allows us to explore the details of TensorFlow model graphs, making the user
    able to zoom in to specific blocks and subsections.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许我们探索 TensorFlow 模型图的详细信息，使用户能够缩放到特定的块和子部分。
- en: It can generate plots of typical quantities of interest that we can take a look
    at during training, such as loss and accuracy.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以生成我们在训练过程中可以查看的典型量的图表，如损失和准确率。
- en: It gives us access to histogram visualizations that show tensors changing over
    time.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了直方图可视化，展示张量随时间变化的情况。
- en: It provides trends of layer weights and bias over epochs.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了层权重和偏置在各个周期中的变化趋势。
- en: It stores runtime metadata for a run, such as total memory usage.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它存储运行时元数据，例如总内存使用情况。
- en: It visualizes embeddings.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可视化嵌入。
- en: TensorBoard reads TensorFlow log files containing summary information about
    the training process at hand. These are generated with the appropriate callbacks,
    which are then passed to TensorFlow jobs.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 读取包含有关当前训练过程的摘要信息的 TensorFlow 日志文件。这些信息是通过适当的回调生成的，然后传递给 TensorFlow
    作业。
- en: 'The following screenshot shows some typical visualizations that are provided
    by TensorBoard. The first one is the "Scalars" section, which shows scalar quantities
    associated with the training stage. In this example, accuracy and binary cross
    entropy are being represented:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 TensorBoard 提供的一些典型可视化内容。第一个是“标量”部分，展示了与训练阶段相关的标量量。在这个例子中，准确率和二进制交叉熵被表示出来：
- en: '![Figure 3.24: TensorBoard scalars'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.24：TensorBoard 标量'
- en: '](img/B16182_03_24.jpg)'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_24.jpg)'
- en: 'Figure 3.24: TensorBoard scalars'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24：TensorBoard 标量
- en: 'The second view provides a block diagram visualization of the computational
    graph, where all the layers are reported together with their relations, as shown
    in the following screenshot:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种视图提供了计算图的框图可视化，所有层及其关系都被一起呈现，如下图所示：
- en: '![Figure 3.25: TensorBoard graph'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.25：TensorBoard 图'
- en: '](img/B16182_03_25.jpg)'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_25.jpg)'
- en: 'Figure 3.25: TensorBoard graph'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25：TensorBoard 图
- en: 'The `DISTRIBUTIONS` tab provides an overview of how the model parameters are
    distributed across epochs, as shown in the following figure:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '`DISTRIBUTIONS` 标签提供了模型参数在各个周期中的分布概览，如下图所示：'
- en: '![Figure 3.26: TensorBoard distributions'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.26：TensorBoard 分布'
- en: '](img/B16182_03_26.jpg)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_26.jpg)'
- en: 'Figure 3.26: TensorBoard distributions'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26：TensorBoard 分布
- en: 'Finally, the `HISTOGRAMS` tab provides similar information to the `DISTRIBUTIONS`
    tab, but is unfolded in 3D, as shown in the following screenshot:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`HISTOGRAMS` 标签提供与 `DISTRIBUTIONS` 标签类似的信息，但以 3D 展示，如下图所示：
- en: '![Figure 3.27: TensorBoard histograms'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.27：TensorBoard 直方图'
- en: '](img/B16182_03_27.jpg)'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_27.jpg)'
- en: 'Figure 3.27: TensorBoard histograms'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27：TensorBoard 直方图
- en: In this section and, in particular, in the following exercise, TensorBoard will
    be leveraged to easily visualize metrics in terms of trends, tensor graphs, distributions,
    and histograms.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，特别是在接下来的练习中，将利用 TensorBoard 轻松地可视化指标，如趋势、张量图、分布和直方图。
- en: In order to focus only on TensorBoard, the very same classification exercise
    we performed in the previous section will be used. Only the large model will be
    trained. All we need is to import TensorBoard and activate it, as well as a definition
    of the log file directory.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 为了专注于 TensorBoard，我们将使用在上一部分中执行的相同分类练习。只会训练大型模型。我们需要做的就是导入 TensorBoard 并激活它，同时定义日志文件目录。
- en: A TensorBoard callback is then created and passed to the `fit` method of the
    model. This will generate all TensorBoard files inside the log directory. Once
    training is complete, this log directory path is passed to TensorBoard as an argument.
    This will open a web-based visualization where the user is able to gain deep insights
    into its model and training-related aspects.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个 TensorBoard 回调并将其传递给模型的 `fit` 方法。这将生成所有 TensorBoard 文件并保存在日志目录中。一旦训练完成，日志目录路径将作为参数传递给
    TensorBoard。这将打开一个基于 Web 的可视化工具，用户可以深入了解模型及其训练相关的各个方面。
- en: 'Exercise 3.07: Creating a Deep Neural Network to Classify Events Generated
    by the ATLAS Experiment in the Quest for the Higgs Boson Using TensorBoard for
    Visualization'
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.07：创建一个深度神经网络，用于分类 ATLAS 实验中生成的事件，以寻找希格斯玻色子，并使用 TensorBoard 进行可视化
- en: In this exercise, we will build, train, and measure the performance of a deep
    neural network with the same goal of *Exercise 3.06, Creating a Deep Neural Network
    to Classify Events Generated by the ATLAS Experiment in the Quest for Higgs Boson*
    in mind, but instead, we will leverage TensorBoard so that we can gain additional
    training insights.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建、训练并测量一个深度神经网络的表现，目标与*练习 3.06，创建一个深度神经网络，用于分类 ATLAS 实验中生成的事件，以寻找希格斯玻色子*相同，但这次我们将利用
    TensorBoard，从中获得更多的训练洞察。
- en: 'The following steps need to be implemented in order to complete this exercise:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个练习，需要实现以下步骤：
- en: 'Import all the required modules:'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的模块：
- en: '[PRE97]'
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Download the custom smaller subset of the original dataset:'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载原始数据集的定制小子集：
- en: '[PRE98]'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Read the CSV dataset into the TensorFlow dataset class and repack it so that
    it has tuples (`features`, `labels`):'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 CSV 数据集读入 TensorFlow 数据集类，并重新打包，以便它具有元组（`features`，`labels`）：
- en: '[PRE99]'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Create training, validation, and test sets and assign them the `BATCH_SIZE`
    parameter:'
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练集、验证集和测试集，并为它们分配`BATCH_SIZE`参数：
- en: '[PRE100]'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Now, let''s start creating the model and training it. Create a decaying learning
    rate:'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始创建模型并进行训练。创建一个衰减学习率：
- en: '[PRE101]'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Define a function that will compile a model with the `Adam` optimizer and use
    binary cross entropy as the `loss` function. Then, fit it on the training data
    using early stopping by using the validation dataset, as well as a TensorBoard
    callback:'
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将使用 `Adam` 优化器编译模型，并使用二元交叉熵作为 `loss` 函数。然后，使用验证数据集通过早停法拟合训练数据，并使用
    TensorBoard 回调：
- en: '[PRE102]'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Create the same large model as before with regularization items such as L2
    regularization and dropout, and then compile it and fit it on the dataset:'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建与之前相同的大型模型，并加入正则化项，如 L2 正则化和丢弃法，然后对其进行编译，并在数据集上拟合：
- en: '[PRE103]'
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The last output line will be as follows:'
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后一行输出将如下所示：
- en: '[PRE104]'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Check the model''s performances on the test set:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型在测试集上的表现：
- en: '[PRE105]'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The output will be as follows:'
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE106]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Note
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于随机抽样和可变的随机种子，准确度可能会显示略微不同的值。
- en: 'Visualize the variables with TensorBoard:'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 可视化变量：
- en: '[PRE107]'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'This command starts the web-based visualization tool. Four main windows are
    represented in the following figure, displaying information about loss and accuracy,
    model graphs, histograms, and distributions in a clockwise order, starting from
    the top left:'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令启动基于 Web 的可视化工具。下图表示四个主要窗口，按顺时针顺序从左上角开始，显示有关损失和准确度、模型图、直方图和分布的信息：
- en: '![Figure 3.28: TensorBoard visualization'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.28：TensorBoard 可视化'
- en: '](img/B16182_03_28.jpg)'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_28.jpg)'
- en: 'Figure 3.28: TensorBoard visualization'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.28：TensorBoard 可视化
- en: 'The advantages of using TensorBoard are quite evident: all the training information
    is collected in a single place, allowing the user to easily navigate through it.
    The top-left panel, the `SCALARS` tab, allows the user to monitor loss and accuracy
    so that they are able to check the same chart we saw previously in an easier way.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 的优点非常明显：所有训练信息都集中在一个地方，方便用户轻松浏览。左上角的`SCALARS`标签允许用户监控损失和准确度，从而能够以更简便的方式查看我们之前看到的相同图表。
- en: In the top right, the model graph is shown, so it is possible to visualize how
    input data flows into the computational graph by going through each block.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在右上角，显示了模型图，因此可以通过经过每个模块来可视化输入数据如何流入计算图。
- en: 'The two views at the bottom show the same information in two different representations:
    all the model parameter (networks weights and biases) distributions are shown
    across training epochs. On the left, the `DISTRIBUTIONS` tab displays the parameters
    in 2D, while the `HISTOGRAMS` tab unfolds the parameters in 3D. They both allow
    the user to monitor how trainable parameters vary during the training step.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的两个视图以两种不同的表示方式显示相同的信息：所有模型参数（网络权重和偏差）的分布在训练周期中得以展示。左侧的`DISTRIBUTIONS`标签以
    2D 展示参数，而`HISTOGRAMS`标签则以 3D 展开参数。两者都允许用户监控训练过程中可训练参数的变化。
- en: Note
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AWGjFv](https://packt.live/2AWGjFv).
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考[https://packt.live/2AWGjFv](https://packt.live/2AWGjFv)。
- en: You can also run this example online at [https://packt.live/2YrWl2d](https://packt.live/2YrWl2d).
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在线运行此示例，访问[https://packt.live/2YrWl2d](https://packt.live/2YrWl2d)。
- en: In this section, we focused on providing some insights into how to use TensorBoard
    to visualize training-related model parameters. We saw how, starting with an already
    familiar problem, it is super easy to add the TensorBoard web-based visualization
    tool and navigate through all of its plugins directly inside a Python notebook.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们主要讨论了如何使用 TensorBoard 可视化与训练相关的模型参数。我们看到，从一个已经熟悉的问题出发，加入 TensorBoard
    的基于 Web 的可视化工具并直接在 Python 笔记本内浏览所有插件变得非常简单。
- en: Now, let's complete an activity to put all our knowledge to the test.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个活动来检验我们所有的知识。
- en: 'Activity 3.01: Classifying Fashion Clothes Using a TensorFlow Dataset and TensorFlow
    2'
  id: totrans-604
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.01：使用 TensorFlow 数据集和 TensorFlow 2 对时尚服装进行分类
- en: Suppose you need to code an image processing algorithm for a company that owns
    a clothes warehouse. They want to autonomously classify clothes based on a camera
    output, thereby allowing them to group clothes together with no human intervention.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要为一个拥有服装仓库的公司编写图像处理算法。公司希望根据摄像头输出自动分类服装，从而实现无人工干预地将服装分组。
- en: In this activity, we will create a deep fully connected neural network capable
    of doing such a task, meaning that it will accurately classify images of clothes
    by assigning them to the class they belong to.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将创建一个深度全连接神经网络，能够完成此类任务，即通过将图像分配到它们所属的类别来准确分类服装。
- en: 'The following steps will help you to complete this activity:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此活动：
- en: Import all the required modules, such as `numpy`, `matplotlib.pyplot`, `tensorflow`,
    and `tensorflow_datasets`, and print out their main module versions.
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的模块，如`numpy`、`matplotlib.pyplot`、`tensorflow` 和 `tensorflow_datasets`，并打印出它们的主模块版本。
- en: Import the Fashion MNIST dataset using TensorFlow datasets and split it into
    train and test sets.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 数据集导入 Fashion MNIST 数据集，并将其拆分为训练集和测试集。
- en: Explore the dataset to get familiar with the input features, that is, shapes,
    labels, and classes.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索数据集，熟悉输入特征，即形状、标签和类别。
- en: Visualize some instances of the training set.
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化一些训练集的实例。
- en: Perform data normalization by building the classification model.
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过构建分类模型进行数据归一化。
- en: Train the deep neural network.
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练深度神经网络。
- en: Test the model's accuracy. You should obtain an accuracy in excess of 88%.
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型的准确性。你应该获得超过 88% 的准确率。
- en: Perform inference and check the predictions against the ground truth.
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行推理并检查预测结果与实际标签的对比。
- en: 'By the end of this activity, the trained model should be able to classify all
    the fashion items (clothes, shoes, bags, and so on) with an accuracy greater than
    88%, thus producing a result similar to the one shown in the following image:'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到本活动结束时，训练好的模型应该能够以超过 88% 的准确率分类所有时尚物品（服装、鞋子、包包等），从而生成类似于以下图像所示的结果：
- en: '![Figure 3.29: Clothes classification with a deep neural network output'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.29：使用深度神经网络输出进行衣物分类'
- en: '](img/B16182_03_29.jpg)'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_03_29.jpg)'
- en: 'Figure 3.29: Clothes classification with a deep neural network output'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.29：使用深度神经网络输出进行衣物分类
- en: Note
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 696.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第696页找到。
- en: Summary
  id: totrans-622
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we were introduced to practical deep learning with TensorFlow
    2 and Keras, their key features and applications, and how they work together.
    We became familiar with the differences between low- and high-level APIs, as well
    as how to leverage the most advanced modules to ease the creation of deep models.
    Then, we discussed how to implement a deep neural network with TensorFlow and
    addressed some major topics: from model creation, training, validation, and testing,
    we highlighted the most important aspects to consider so as to avoid pitfalls.
    We saw how to build different types of deep learning models, such as fully connected,
    convolutional, and recurrent neural networks, via the Keras API. We solved a regression
    task and a classification problem, which gave us hands-on experience with this.
    We learned how to leverage TensorBoard to visualize many different training trends
    regarding metrics and model parameters. Finally, we built and trained a model
    that is able to classify fashion item images with high accuracy, an activity that
    shows that a possible real-world problem can be solved with the help of the most
    advanced deep learning techniques.'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了使用TensorFlow 2和Keras进行实用深度学习的内容，讨论了它们的关键特性和应用，以及它们如何协同工作。我们熟悉了低级API和高级API之间的区别，并了解了如何利用最先进的模块简化深度模型的创建。接着，我们讨论了如何使用TensorFlow实现深度神经网络，并涵盖了一些主要话题：从模型创建、训练、验证到测试，我们强调了避免陷阱时需要考虑的最重要方面。我们展示了如何通过Keras
    API构建不同类型的深度学习模型，如全连接、卷积和递归神经网络。我们解决了回归任务和分类问题，从中获得了实践经验。我们还学习了如何利用TensorBoard可视化与训练趋势相关的多种指标和模型参数。最后，我们构建并训练了一个能够高准确率地分类时尚物品图像的模型，这项活动展示了如何借助最先进的深度学习技术解决一个可能的现实世界问题。
- en: In the next chapter, we will be studying the OpenAI Gym environment and how
    to use TensorFlow 2 for reinforcement learning.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究OpenAI Gym环境以及如何使用TensorFlow 2进行强化学习。
