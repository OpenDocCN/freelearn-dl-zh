- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Exploring Bias and Fairness
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索偏见与公平性
- en: 'A biased machine learning model produces and amplifies unfair or discriminatory
    predictions against certain groups. Such models can produce biased predictions
    that lead to negative consequences such as social or economic inequality. Fortunately,
    some countries have discrimination and equality laws that protect minority groups
    against unfavorable treatment. One of the worst scenarios a machine learning practitioner
    or anyone who deploys a biased model could face is either receiving a legal notice
    imposing a heavy fine or receiving a lawyer letter from being sued and forced
    to shut down their deployed model. Here are a few examples of such situations:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有偏见的机器学习模型会产生并放大对某些群体的不公平或歧视性预测。这些模型可能会产生偏见的预测，导致如社会或经济不平等等负面后果。幸运的是，一些国家有反歧视和平等法律，保护少数群体免受不利待遇。机器学习从业者或任何部署有偏见模型的人可能面临的最糟糕情况是收到法律通知，要求支付重罚款，或收到律师函被起诉并被迫关闭其已部署的模型。以下是一些此类情况的例子：
- en: The ride-hailing app Uber faced legal action from two unions in the UK for its
    facial verification system, which showed racial bias against dark-skinned people
    by displaying more frequent verification errors. This impeded their work as Uber
    drivers ([https://www.bbc.com/news/technology-58831373](https://www.bbc.com/news/technology-58831373)).
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叫车应用Uber因其面部验证系统在英国受到两个工会的法律诉讼，该系统对深色皮肤人群显示出种族偏见，表现为更频繁的验证错误。这阻碍了他们作为Uber司机的工作（[https://www.bbc.com/news/technology-58831373](https://www.bbc.com/news/technology-58831373)）。
- en: Creators filed a lawsuit against YouTube for its racial and other minority group
    discrimination against them as YouTube’s algorithm automatically removed their
    videos without proper explanation, removing their capability to earn ad revenue
    ([https://www.washingtonpost.com/technology/2020/06/18/black-creators-sue-youtube-alleged-race-discrimination/](https://www.washingtonpost.com/technology/2020/06/18/black-creators-sue-youtube-alleged-race-discrimination/)).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创作者因YouTube对他们的种族及其他少数群体歧视而提起诉讼，因为YouTube的算法自动删除了他们的视频，并没有给出合理解释，导致他们无法通过广告收入获利（[https://www.washingtonpost.com/technology/2020/06/18/black-creators-sue-youtube-alleged-race-discrimination/](https://www.washingtonpost.com/technology/2020/06/18/black-creators-sue-youtube-alleged-race-discrimination/)）。
- en: Facebook was charged with racial, gender, religion, familial status, and disability
    discrimination for its housing ads by the housing department of the US and had
    to pay under 5 million US dollars ([https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge](https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge)).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook因其住房广告涉嫌种族、性别、宗教、家庭状态和残疾歧视，遭到美国住房部门起诉并需支付不到500万美元的罚款（[https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge](https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge)）。
- en: 'Thus, utmost care must be taken to prevent bias from being perpetuated against
    sensitive and protected attributes of the underlying data and environment the
    machine learning model will be exposed to. In this chapter, we will approach this
    topic step by step, starting with an exploration of the types of bias, learning
    to detect and evaluate methods needed to identify bias and fairness, and finally
    exploring ways to mitigate bias. The concepts and techniques that will be presented
    in this chapter are relevant to all machine learning models. However, bias mitigation
    is an exception; there, we will explore a neural network-specific method that
    can reliably mitigate bias. More formally, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，必须格外小心，避免对机器学习模型所接触的基础数据和环境中的敏感和受保护属性产生偏见。在本章中，我们将逐步探讨这个话题，从探索偏见的类型开始，学习检测和评估识别偏见与公平性的方法，最后探讨减轻偏见的途径。本章将介绍的概念和技术适用于所有机器学习模型。然而，偏见缓解是一个例外；我们将在其中探讨一种特定于神经网络的方法，该方法能够可靠地缓解偏见。更正式地说，我们将涵盖以下主题：
- en: Exploring the types of bias
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索偏见的类型
- en: Understanding the source of AI bias
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解AI偏见的来源
- en: Discovering bias and fairness evaluation methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现偏见与公平性评估方法
- en: Evaluating the bias and fairness of a deep learning model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估深度学习模型的偏见与公平性
- en: Tailoring bias and fairness measures across use cases
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对不同使用案例量身定制偏见与公平性措施
- en: Mitigating AI bias
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少AI偏见
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含一些 Python 编程语言中的实际实现。要完成这些内容，您需要在计算机上安装以下库：
- en: '`pandas`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`matplotlib`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: '`scikit-learn`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: '`numpy`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`pytorch`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`transformers==4.28.0`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers==4.28.0`'
- en: '`accelerate==0.6.0`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accelerate==0.6.0`'
- en: '`captum`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`captum`'
- en: '`catalyst`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`catalyst`'
- en: The code files are available on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_13](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_13).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可以在 GitHub 上获取，链接为[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_13](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_13)。
- en: Exploring the types of bias
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索偏差的类型
- en: Bias can be described as a natural tendency or inclination toward a specific
    viewpoint, opinion, or belief system, regardless of whether it is treated as positive,
    neutral, or negative. AI bias, on the other hand, specifically occurs when mathematical
    models perpetuate the biases embedded by their creators or underlying data. Be
    aware that not all information is treated as biases, as some information can also
    be knowledge. Bias is a type of subjective information, and knowledge refers to
    factual information, understanding, or awareness acquired through learning, experience,
    or research. In other words, knowledge is the truth without bias.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差可以被描述为对某个特定观点、意见或信仰系统的自然倾向或倾向，无论其是否被视为正面、中立或负面。另一方面，AI 偏差特指当数学模型延续其创建者或基础数据中固有的偏差时发生的情况。需要注意的是，并非所有信息都被视为偏差，有些信息也可以被视为知识。偏差是一种主观信息，而知识指的是通过学习、经验或研究获得的事实性信息、理解或意识。换句话说，知识是不带偏见的真理。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Do not confuse bias in this book with the bias from the infamous “bias versus
    variance” concept in machine learning. Bias in this concept refers to the specific
    bias on how simple a machine learning model is concerning a certain task to learn.
    For completeness, variance specifies the sensitivity of the model toward the change
    in the data features. Here, a high bias would correspond to a low variance and
    underfitting behavior and indicate that a machine learning model is too simple.
    A low bias in this concept would correspond to high variance and overfitting behavior.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将本书中的偏差与机器学习中臭名昭著的“偏差与方差”概念中的偏差混淆。在这个概念中，偏差指的是机器学习模型在执行某个任务时的简单性。为了完整性，方差指定了模型对数据特征变化的敏感性。在这里，高偏差对应于低方差和欠拟合行为，表明机器学习模型过于简单。低偏差则对应于高方差和过拟合行为。
- en: 'In this book, we will use the term bias more colloquially. More well-known
    attributes of bias, such as race, gender, and age, are considered social bias,
    or stereotyping. However, the scope of bias is much broader. Here are some interesting
    examples of other types of bias:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将以更口语化的方式使用偏差一词。诸如种族、性别和年龄等更为人知的偏差特征被视为社会偏差或刻板印象。然而，偏差的范围要广得多。以下是一些其他类型偏差的有趣例子：
- en: '**Cultural bias**: The influence cultural perspectives, values, and norms have
    on judgments, decisions, and behaviors. It can manifest in machine learning models
    through biased data collection, skewed training data, or algorithms that reflect
    cultural prejudices.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文化偏差**：文化视角、价值观和规范对判断、决策和行为的影响。它可以通过偏倚的数据收集、失衡的训练数据或反映文化偏见的算法在机器学习模型中表现出来。'
- en: '**Cognitive bias**: Systematic patterns of deviation from rationality in thinking
    or decision-making. Let’s look at a few examples of cognitive bias along with
    possible phenomena that portray the bias in a machine learning project:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**认知偏差**：在思维或决策过程中系统性地偏离理性思考的模式。让我们看几个认知偏差的例子，并探讨这些偏差在机器学习项目中可能表现出的现象：'
- en: '**Confirmation bias**: Favoring data that aligns with existing beliefs'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确认偏差**：偏向于支持现有信念的数据。'
- en: '**Anchoring bias**: Overemphasizing certain features or variables during model
    training'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锚定偏差**：在模型训练过程中过度强调某些特征或变量。'
- en: '**Availability bias**: Relying on easily accessible data sources, potentially
    overlooking relevant but less accessible ones'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性偏差**：依赖于易于获取的数据源，可能忽视相关但不易获取的数据源。'
- en: '**Overconfidence bias**: Overestimating model capabilities or accuracy'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度自信偏差**：高估模型的能力或准确性。'
- en: '**Hindsight bias**: Believing that model predictions were more predictable
    after observing them'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事后偏差**：在观察到模型预测结果后，认为这些预测结果是可以预见的。'
- en: '**Automation bias**: Placing excessive trust in the model’s outputs without
    critical evaluation'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化偏差**：过度依赖模型输出而不进行批判性评估'
- en: '**Framing bias**: Influencing the model’s learning process through biased data
    presentation'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架偏差**：通过有偏的数据呈现影响模型的学习过程'
- en: '**Selection bias**: Non-random sampling leading to unrepresentative data'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择偏差**：非随机抽样导致的不具代表性数据'
- en: '**Sampling bias**: Skewed data due to an unrepresentative sample'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样偏差**：由于样本不具代表性导致的数据偏倚'
- en: '**Reporting bias**: Misrepresentation due to individuals’ preferences or beliefs'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**报告偏差**：由于个人的偏好或信仰导致的错误陈述'
- en: '**Algorithmic bias**: Presence of any biases in algorithms such as machine
    learning algorithms. Another example of such bias is **aggregation bias**, which
    has skewed predictions due to data grouping or aggregation methods.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法偏差**：算法中存在的任何偏差，如机器学习算法。另一个偏差的例子是**聚合偏差**，它是由于数据分组或聚合方法导致的预测偏倚。'
- en: '**Measurement bias**: Inaccuracies or errors in data collection and measurement.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量偏差**：数据收集和测量中的不准确或错误。'
- en: These bias groups are just the tip of the iceberg and can span to very niche
    bias groups such as political bias, industry bias, media bias, and so on. Now
    that we understand what bias is and what it covers, let’s discover the source
    of AI bias.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些偏差类别仅仅是冰山一角，可能延伸到非常特定的偏差类别，例如政治偏差、行业偏差、媒体偏差等。现在我们了解了偏差的概念及其涵盖的范围，接下来让我们一起探讨AI偏差的来源。
- en: Understanding the source of AI bias
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解AI偏差的来源
- en: 'AI bias can happen at any point in the deep learning life cycle. Let’s go through
    bias at those stages one by one:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AI偏差可能发生在深度学习生命周期的任何阶段。让我们逐个了解这些阶段中的偏差：
- en: '**Planning**: During the planning stage of the machine learning life cycle,
    biases can emerge as decisions are made regarding project objectives, data collection
    methods, and model design. Bias may arise from subjective choices, assumptions,
    or the use of unrepresentative data sources. Project planners need to maintain
    a critical perspective, actively consider potential biases, engage diverse perspectives,
    and prioritize fairness and ethical considerations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划**：在机器学习生命周期的规划阶段，随着项目目标、数据收集方法和模型设计的决定，偏差可能会出现。偏差可能源于主观选择、假设或使用不具代表性的数据源。项目规划者需要保持批判性的视角，积极考虑潜在的偏差，吸纳多元观点，并优先考虑公平性和伦理问题。'
- en: '**Data preparation**: This stage involves the following phases:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：这一阶段包括以下几个阶段：'
- en: '**Data collection**: During the data collection phase, bias can creep in if
    the collected data fails to represent the target population accurately. Several
    factors can contribute to this bias, including sampling bias, selection bias,
    or the underrepresentation of specific groups. These issues can lead to the creation
    of an imbalanced dataset that does not reflect the true diversity of the intended
    population.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集**：在数据收集阶段，如果收集的数据未能准确代表目标群体，偏差可能会悄然出现。多个因素可能导致这种偏差，包括抽样偏差、选择偏差，或者某些群体的代表性不足。这些问题可能导致创建不平衡的数据集，无法反映目标群体的真实多样性。'
- en: '**Data labeling**: Bias can also infiltrate the data labeling process. Each
    labeler may possess their own inherent biases, consciously or subconsciously,
    which can influence their decision-making when assigning labels to the data. If
    the labelers lack diversity or comprehensive training, their biases may seep into
    the annotations, ultimately leading to the development of biased models that perpetuate
    unfairness and discrimination. As a result, the final combined data can contain
    multiple biases which may even conflict with each other, causing difficulties
    in the learning process.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据标注**：偏差也可能渗透到数据标注过程中。每个标注员可能有自己固有的偏见，无论是有意识还是无意识，这些偏见可能会影响他们在给数据标注时的决策。如果标注员缺乏多样性或全面的培训，他们的偏见可能会渗入到标注中，最终导致开发出带有偏见的模型，进而延续不公平和歧视。因此，最终的综合数据可能包含多种偏见，甚至可能彼此冲突，从而造成学习过程中的困难。'
- en: '**Model development**: Bias can be introduced in two ways in a deep learning
    model:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型开发**：偏差可以通过两种方式在深度学习模型中引入：'
- en: '**Feature selection**: Biases can arise from the features selected for model
    training. If certain features are correlated with protected attributes (such as
    race or gender), the model may inadvertently learn and reinforce those biases,
    leading to discriminatory outcomes.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：偏差可能来源于选择用于模型训练的特征。如果某些特征与受保护属性（如种族或性别）相关，模型可能会无意中学习并强化这些偏见，导致歧视性结果。'
- en: '**Pretrained models**: A pretrained deep learning model might be a biased model.
    For example, if the model has been trained on biased data, it may learn and perpetuate
    those biases in its predictions. Even if fine-tuning was done, the bias is not
    likely to go away.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练模型**：一个预训练的深度学习模型可能是一个有偏的模型。例如，如果模型在有偏的数据上进行训练，它可能会在预测中学习并延续这些偏见。即使进行了微调，偏见也不太可能消失。'
- en: '**Deliver model insights**: Bias can happen particularly when interpreting
    explanations of model behavior. The process of understanding and explaining the
    inner workings of a model involves subjective reasoning and is susceptible to
    bias. The interpretation of model insights heavily relies on the perspective and
    preconceptions of the individuals involved, which can introduce biases based on
    their own beliefs, experiences, or implicit biases. It is essential to approach
    the interpretation of model explanations with awareness of these potential biases
    and strive for objectivity and fairness to avoid misinterpretation or reinforcing
    existing biases. Critical thinking and diverse perspectives are vital to ensuring
    that the insights delivered accurately reflect the model’s behavior without introducing
    additional bias.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传递模型洞察**：偏见特别容易出现在解释模型行为时。理解和解释模型内部工作原理的过程涉及主观推理，并且容易受到偏见的影响。模型洞察的解释在很大程度上依赖于相关人员的视角和先入为主的观念，这可能会根据他们自己的信仰、经验或隐性偏见引入偏见。必须意识到这些潜在的偏见，并力求客观公正，以避免误解或加剧现有偏见。批判性思维和多元视角对确保传递的洞察准确反映模型行为、避免引入额外偏见至关重要。'
- en: '**Model deployment**: This stage covers bias that can happen when a model is
    deployed, which includes the following components:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署**：此阶段涵盖了模型部署时可能出现的偏见，包括以下几个组件：'
- en: '**User interactions**: Bias can arise during model deployment when users provide
    feedback or responses and be introduced if the feedback is biased or if the system
    responds differently based on user characteristics. For example, the chat history
    mechanism in the ChatGPT UI allows a user to provide biased input.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户互动**：在模型部署过程中，当用户提供反馈或响应时，可能会引入偏见，特别是在反馈本身存在偏见，或系统根据用户特征做出不同响应时。例如，ChatGPT界面中的聊天历史机制允许用户提供有偏的输入。'
- en: '**Human-in-the-loop bias**: Biases can be introduced when human reviewers or
    operators make decisions based on the model’s predictions, exhibiting their own
    biases or interpreting outputs unfairly. This can impact the perceived fairness
    of the decision-making process.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类参与偏见**：当人工审阅者或操作员根据模型预测做出决策时，可能会引入他们自己的偏见或不公正地解读输出结果。这会影响决策过程的公正性。'
- en: '**Environment bias**: Some features might be treated and perceived differently
    in unseen areas, leading to data drift. Models were evaluated to be unbiased during
    the development stage, but with the new data, it could still produce biased predictions.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境偏差**：某些特征在未见过的领域可能会被不同地处理和感知，从而导致数据漂移。模型在开发阶段被评估为无偏，但随着新数据的加入，它仍然可能产生有偏的预测。'
- en: '**New data source for retraining bias**: New data can be collected and labeled
    for retraining, which can be a source of bias.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新训练的偏见数据源**：新数据可以被收集并标注用于重新训练，这可能成为偏见的来源。'
- en: '**Model governance**: Bias can emerge when the person responsible for monitoring
    the deployed model needs to establish thresholds for various types of drift (which
    will be introduced in [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238), *Governing
    Deep Learning Models*), analyze prediction summaries, and examine data summaries.
    Setting these thresholds introduces the potential for bias based on subjective
    decisions or assumptions. Additionally, when analyzing prediction and data summaries,
    there is a risk of overlooking certain biases or unintentionally reinforcing existing
    biases if not approached with diligence and a critical mindset. It is crucial
    to maintain awareness of these biases and ensure that monitoring and analysis
    processes are conducted rigorously and with a focus on fairness and accuracy.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型治理**：偏见可能出现在负责监控已部署模型的人员需要为各种类型的漂移设定阈值（将在[*第16章*](B18187_16.xhtml#_idTextAnchor238)中介绍，*治理深度学习模型*）、分析预测摘要和检查数据摘要时。设定这些阈值可能会基于主观决策或假设引入偏见。此外，在分析预测和数据摘要时，如果没有以谨慎和批判的思维方式进行，可能会忽视某些偏见，或不小心强化现有偏见。必须保持对这些偏见的敏感性，确保监控和分析过程严格进行，注重公正性和准确性。'
- en: Now that we’ve discovered some of the possible sources of bias in each stage
    of the deep learning life cycle, we are ready to dive into discovering bias detection
    and fairness evaluation methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经发现了深度学习生命周期中各阶段可能的偏差来源，我们准备深入探索偏差检测和公平评估方法。
- en: Discovering bias and fairness evaluation methods
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现偏差和公平评估方法
- en: Fairness and bias are opposing concepts. Fairness seeks to ensure fair and equal
    treatment in decision-making for all individuals or groups, while bias refers
    to unfair or unequal treatment. Mitigating bias is a crucial step in achieving
    fairness. Bias can exist in different forms and addressing all potential biases
    is complicated. Additionally, it’s important to understand that achieving fairness
    in one aspect doesn’t guarantee the complete absence of bias in general.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 公平和偏差是对立的概念。公平旨在确保所有个体或群体在决策中得到公平和平等的对待，而偏差则指不公平或不平等的对待。减少偏差是实现公平的关键步骤。偏差可能以不同形式存在，解决所有潜在偏差是复杂的。此外，理解在某个方面实现公平并不意味着完全消除一般性的偏差也很重要。
- en: 'To understand both how much bias and how fair our data and model are, what
    we need is a set of bias and fairness metrics to objectively measure and evaluate.
    This will then enable a feedback mechanism to iteratively and objectively mitigate
    bias and achieve fairness. Let’s go through a few robust bias and fairness metrics
    that you need to have in your arsenal of tools to achieve fairness:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们的数据和模型在多大程度上存在偏差以及多么公平，我们需要一组偏差和公平度量来客观地衡量和评估。这将启用一个反馈机制，迭代且客观地减少偏差并实现公平。接下来，让我们来看一些你必须掌握的强健的偏差和公平度量工具，以实现公平：
- en: '**Equal representation-based metrics**: This set of metrics focuses on the
    equal proportions of either the data or the decision outcomes without considering
    the errors:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于平等代表性的度量**：这一组度量关注的是数据或决策结果的平等比例，而不考虑错误：'
- en: '**Disparate impact**: Disparate impact examines whether the model treats different
    groups fairly or if there are significant relative disparities in the outcomes
    they receive by taking a ratio of the proportion of favorable outcomes between
    groups. Disparate impact for a chosen group in a chosen attribute can be computed
    using the following formula:'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同影响**：不同影响考察的是模型是否公平地对待不同群体，或者它们在所获得的结果中是否存在显著的相对差异，通过计算群体之间有利结果比例的比值来进行。选定群体在选定属性下的不同影响可以使用以下公式计算：'
- en: Disparate Impact (Group A) =
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 不同影响（组A） =
- en: (  Proportion of Positive Predictions for Group A     ___________________________________________________________      Proportion
    of Positive Predictions for Reference Group or Aggregate of Other Groups  )
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: （组A的正预测比例　　 ___________________________________________________________　　 参考组或其他群体的正预测比例）
- en: Disparate impact across groups can be averaged to obtain a single global representative
    value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 跨群体的不同影响可以求平均，以获得单一的全局代表值。
- en: '**Statistical parity difference**: This extends similar benefits as disparate
    impact but provides an absolute disparity measure instead of relative by using
    a difference instead of a ratio. The absolute difference is useful when you can
    and need to translate the values into tangible impacts, such as the number of
    individuals who were discriminated against based on a new sample size. It can
    be computed using the following formula:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计平等差异**：这一方法与不同影响具有相似的益处，但通过使用差值而非比值，提供了绝对差异度量。绝对差异在你可以且需要将值转化为实际影响时很有用，比如基于新样本大小的被歧视个体数量。它可以使用以下公式计算：'
- en: Statistical Parity Difference = |(Proportion of Positive Predictions for Privileged
    Group) − (Proportion of Positive Predictions for Unprivileged Group)|
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 统计平等差异 = |（特权组的正预测比例） − （非特权组的正预测比例）|
- en: '**Equal error-based metrics**: These are the metrics that consider the bias
    in error rates between groups.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于平等错误的度量**：这些度量考虑的是不同群体之间错误率的偏差。'
- en: '**Average Odd Difference** (**AOD**): This measures the average discrepancy
    in the odds of true positive and false positive outcomes across groups. AOD is
    computed by taking the average of the odds differences across different groups.
    The odds difference for a specific group is calculated as the difference between
    the odds of positive prediction for that group and the odds of positive prediction
    for a reference group using the following formula:'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均赔率差异 (AOD)**：这衡量了群体之间真实正例和假正例结果的平均差异。AOD 是通过计算不同群体间赔率差异的平均值来得出的。某个特定群体的赔率差异是通过以下公式计算的：该群体的正预测赔率与参考群体的正预测赔率之间的差异：'
- en: AOD = ( 1 _ n ) * Σ[(TPR _ ref − TPR _ i) + (FPR _ ref − FPR _ i)]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: AOD = (1 / n) * Σ[(TPR _ ref − TPR _ i) + (FPR _ ref − FPR _ i)]
- en: Here, n is the total number of groups, TPR _ i is the true positive rate (sensitivity)
    for group i, FPR _ i is the false positive rate (fallout) for group i, TPR _ ref
    is the true positive rate for the reference group, and FPR _ ref is the false
    positive rate for the reference group.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，n 是群体的总数，TPR _ i 是群体 i 的真实正例率（灵敏度），FPR _ i 是群体 i 的假正例率（误报率），TPR _ ref 是参考群体的真实正例率，FPR
    _ ref 是参考群体的假正例率。
- en: '**Average Absolute Odds Difference** (**AAOD**): This extends similar benefits
    to AOD but adds an absolute term in the individual group computations, as follows:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对赔率差异 (AAOD)**：这与 AOD 类似，但在各个群体的计算中增加了绝对项，计算方式如下：'
- en: AOD = ( 1 _ n ) * Σ[|(TPR _ ref − TPR _ i) + (FPR _ ref − FPR _ i)|]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: AOD = (1 / n) * Σ[|(TPR _ ref − TPR _ i) + (FPR _ ref − FPR _ i)|]
- en: This should be used over AOD when you care about the discrepancies in general
    and not only whether the group discrepancy is favored or non-favored.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你关心一般性差异而不仅仅是群体差异是否被偏好时，应该使用这个方法，而不是 AOD。
- en: '**Distributional fairness through g****eneralized entropy index** (**GEI**):
    This is designed to measure the level of inequality based on distribution across
    individuals of an entire population using only the numerical outcome. The formula
    for GEI is as follows:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过广义熵指数 (GEI) 衡量分配公平性**：这是通过仅使用数值结果，设计来衡量基于整个群体中个体分布的贫富差距。GEI 的公式如下：'
- en: GE(α) =  1 _ na(a − 1)  ∑ i=1 n (n a w a− n), a ≠ 0,1,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: GE(α) = 1 / na(a − 1) ∑ i=1 n (na w a− n)，a ≠ 0,1，
- en: GE(α) = log(n) + ∑ i=1 n w i log(w i), a = 1,
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GE(α) = log(n) + ∑ i=1 n w i log(w i)，a = 1，
- en: GE(α) = − log(n) −  1 _ n  ∑ i=1 n log(w i), a = 0
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: GE(α) = − log(n) − 1 / n ∑ i=1 n log(w i)，a = 0
- en: Here, E T = ∑ i=1 n  E i and w i =  E i _ E T
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，E T = ∑ i=1 n E i，w i = E i _ E T
- en: 'E iis the value of the chosen attribute of a specific entity, E T is the total
    summed value of all, and n is the total number of individuals or entities. Two
    foundational notions of inequality can be configured through the α parameter of
    GEI:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: E 是特定实体的选定属性值，E T 是所有实体的总和值，n 是实体或个体的总数。通过 GEI 的 α 参数可以配置两种基本的不平等概念：
- en: '**Theil index**: The general inequality of all individuals across all groups.
    It has an α value of 1.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泰尔指数**：衡量所有群体之间所有个体的总体不平等。其 α 值为 1。'
- en: '**Coefficient of variation**: The inequality that’s measured by computing the
    variability of individuals in a population group. The more varied the population
    group is, the more biased the population group is. It has an α value of 2.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变异系数**：通过计算群体个体的变异性来衡量不平等。群体的差异越大，该群体的不平等也越大。其 α 值为 2。'
- en: Use the Theil index as your main option if you want an overall inequality and
    switch to the coefficient of variation when you want to understand inequality
    by group.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解整体的不平等情况，可以使用泰尔指数作为主要选择，而当你想按群体理解不平等时，可以切换到变异系数。
- en: '**Individual fairness metric**: Disparity or similarity of outcome based on
    similar individuals is a single individual-based metric. Proximity algorithms
    such as KNN allow you to consider the multiple associating features of an individual
    and compute fairness metrics based on similar examples. You must perform the following
    steps:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个体公平性度量**：基于相似个体的结果差异或相似性是一个单一的基于个体的度量。像 KNN 这样的邻近算法可以让你考虑个体的多个关联特征，并基于相似的例子计算公平性度量。你需要执行以下步骤：'
- en: Find a chosen number of similar examples with associative features, excluding
    the protected attribute with KNN for the individual.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 KNN 方法为每个个体找到具有相似特征的若干个例子，排除受保护属性。
- en: Compute the inequality of the outcome using a chosen fairness metric. The most
    common metric that’s used here is the average similarity of the outcomes of similar
    examples to the individual.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用选择的公平性度量计算结果的不平等性。这里最常用的度量是相似示例与个体结果的平均相似度。
- en: '**Fair accuracy-based performance metric through Balanced accuracy**: This
    provides a balanced evaluation of a classification model’s performance, especially
    when dealing with imbalanced datasets. Balanced accuracy is computed by calculating
    the average of the class-wise accuracies.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过平衡准确度的公平性准确度性能度量**：这为分类模型的性能提供了平衡评估，特别是在处理不平衡数据集时。平衡准确度是通过计算每个类别的准确度的平均值来得到的。'
- en: Even though the metrics we’ve introduced here only cover a partial set of bias
    and fairness metrics available in the field, it is general enough to satisfy most
    machine learning use cases. Now, let’s explore how to use these metrics practically
    in a deep learning project to measure bias and fairness.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里介绍的度量指标仅涵盖了该领域中部分可用的偏差和公平性度量，但它足够通用，可以满足大多数机器学习的应用场景。现在，让我们探讨如何在深度学习项目中实际使用这些度量指标来衡量偏差和公平性。
- en: Evaluating the bias and fairness of a deep learning model
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估深度学习模型的偏差和公平性
- en: In this practical example, we will be exploring the infamous real-world use
    case of face recognition. This practical example will be leveraged for the practical
    implementation of bias mitigation in the next section. The basis of face recognition
    is to generate feature vectors that can be used to carry out KNN-based classification
    so that new faces don’t need to undergo additional network training. In this example,
    we will be training a classification model and evaluating it using traditional
    classification accuracy-based metrics; we won’t be demonstrating the recognition
    part of the use case, which allows us to handle unknown facial identity classes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实际示例中，我们将探讨广为人知的面部识别的现实应用场景。这个实际示例将用于接下来章节中偏差缓解的实际实施。面部识别的基础是生成特征向量，这些特征向量可以用来执行基于KNN的分类，这样新面孔就不需要经过额外的网络训练。在这个示例中，我们将训练一个分类模型，并使用传统的基于分类准确度的度量进行评估；我们不会演示该应用场景的识别部分，后者可以让我们处理未知的面部身份类别。
- en: The goal here is to ensure that the resulting facial classification model has
    low gender bias. We will be using a publicly available facial dataset called **BUPT-CBFace-50**,
    which has a diverse coverage of facial images that have different facial expressions,
    poses, lighting conditions, and occlusions. The dataset consists of 500,000 images
    of 10,000 facial identity classes. In this practical example, you will require
    a GPU with at least 12 GB of RAM so that training can be done in a reasonable
    time. Before starting the example, download the dataset from the official source
    ([https://buptzyb.github.io/CBFace/?reload=true](https://buptzyb.github.io/CBFace/?reload=true)).
    You can find it in the same directory as your project.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是确保结果的面部分类模型具有较低的性别偏差。我们将使用一个公开可用的面部数据集，名为**BUPT-CBFace-50**，它涵盖了各种不同的面部表情、姿势、光照条件和遮挡的面部图像。该数据集由10,000个面部身份类别的500,000张图像组成。在这个实际示例中，您将需要至少12
    GB内存的GPU，以便在合理的时间内完成训练。在开始示例之前，请从官方来源下载数据集（[https://buptzyb.github.io/CBFace/?reload=true](https://buptzyb.github.io/CBFace/?reload=true)）。您可以在与项目相同的目录中找到它。
- en: 'Let’s start with the step-by-step code walkthrough by using Python and the
    `pytorch` library with `catalyst` again:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一步一步的代码演示开始，再次使用 Python 和 `pytorch` 库，以及 `catalyst`：
- en: 'First, let’s import the necessary libraries, which include `pytorch` as the
    deep learning framework, `mlflow` for tracking and comparison, `torchvision` for
    the pretrained ResNet50 model, `catalyst` for efficient PyTorch model handling,
    and `albumentations` for simple image data processing:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库，包括作为深度学习框架的`pytorch`、用于跟踪和比较的`mlflow`、用于预训练ResNet50模型的`torchvision`、用于高效处理PyTorch模型的`catalyst`以及用于简单图像数据处理的`albumentations`：
- en: '[PRE0]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The last line sets the first GPU of your machine to be visible to CUDA, the
    computing interface for the GPU to be used.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后一行将设置您的机器的第一个GPU为CUDA可见，CUDA是GPU的计算接口，用于执行计算。
- en: 'Next, we will define the configuration that will be used for different components.
    This will include training process-specific parameters such as the batch sizes,
    learning rate, number of epochs, the number of early stopping epochs before stopping
    the training process, and the number of epochs to wait for validation metric improvements
    before reducing the learning:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义用于不同组件的配置。这将包括训练过程的特定参数，如批次大小、学习率、训练轮数、在训练过程中停止前的早期停止轮数，以及等待验证指标改进的轮数后才会减少学习率：
- en: '[PRE1]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Additionally, it will include the specification of the privileged and unprivileged
    groups that we will choose for computing the gender-based bias and fairness metrics.
    There are approximately two times more males than females in the dataset, so the
    privileged group here is expected to be `Male`:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，它还将包括我们为计算基于性别的偏差和公平性指标所选择的特权和非特权群体的规定。数据集中男性数量大约是女性的两倍，因此这里的特权群体预计为`Male`：
- en: '[PRE2]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we will set some names for the `mlflow` experiment name, the directory
    to save the models, and the extra parameters that we won’t be enabling for now:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将为`mlflow`实验名称、模型保存目录和一些暂时不启用的额外参数设置名称：
- en: '[PRE3]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we’ll proceed to load the CSV file that contains the dataset metadata,
    primarily consisting of the image paths for the downloaded and unzipped `BUPT_CBFace`
    dataset:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载包含数据集元数据的CSV文件，主要包括已下载并解压的`BUPT_CBFace`数据集的图像路径：
- en: '[PRE4]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Additionally, we will set up the `name2class` mapper, along with the class
    ID targets array and the number of classes:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们还将设置`name2class`映射器，以及类ID目标数组和类别数量：
- en: '[PRE5]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we will perform stratified splitting on this data to put it into training
    and validation sets for the facial identity classes so that both validation and
    training will have all the available facial identity classes:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将对数据进行分层切分，将其划分为训练集和验证集，确保两个数据集都包含所有可用的人脸身份类别：
- en: '[PRE6]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The logic of the last `if` clause will be covered in the next section on bias
    mitigation. For the subsequent steps, treat the usage of `pass` as an indicator.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后的`if`语句逻辑将在下一节关于偏差缓解的内容中讲解。对于后续步骤，将`pass`的使用视为一个指示符。
- en: 'Next, we will define the model we want to use based on the ResNet50 model.
    We will use the ARCFace layer here with the ResNet50 model base, a type of metric
    learning algorithm. It utilizes angular margin loss to enhance the discriminative
    power of the learned face embeddings, enabling more accurate and robust face recognition
    across varying poses, illuminations, and identities:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义基于ResNet50模型的模型。我们将在这里使用ARCFace层，并以ResNet50为基础，这是一种度量学习算法。它利用角度边距损失来增强学习到的人脸嵌入的区分能力，从而实现更精确、更鲁棒的人脸识别，能够应对不同的姿势、光照和身份变化：
- en: '[PRE7]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we will initialize the model, assign it to use GPU, define the cross-entropy
    loss variable, define the SGD optimizer variable, and define the reduced learning
    rate engine on validation degradation:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化模型，分配给GPU使用，定义交叉熵损失变量，定义SGD优化器变量，并定义在验证性能下降时减少学习率的引擎：
- en: '[PRE8]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will need to define the PyTorch dataset class to take in the image
    file paths and the sensitive attributes, which are gender data, the target, and
    the specified albumentation transform. The last variable will be utilized in the
    next section:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义PyTorch数据集类，以接收图像文件路径和敏感属性，这些敏感属性包括性别数据、目标和指定的albumentation变换。最后一个变量将在下一节中使用：
- en: '[PRE9]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we want to apply a simple set of transform operations from the `albumentation`
    library. Let’s define the method to return a transform instance with augmentation
    for training and without augmentation for validation purposes. Both require the
    transform instance to convert the image values into PyTorch tensors:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们要应用来自`albumentation`库的一组简单变换操作。我们将定义一个方法，返回一个用于训练的带增广的变换实例，以及用于验证的无增广的变换实例。两者都需要变换实例将图像值转换为PyTorch张量：
- en: '[PRE10]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let’s initialize the dataset and the subsequent dataset loader:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们初始化数据集并随后加载数据集：
- en: '[PRE11]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we will define the helper methods to help compute the multiclass bias
    and fairness metrics, which consist of performing safe division handling and zero
    division to prevent NaN values and computing false positives, true negatives,
    total positives, total negatives, and total data. Since this is a multiclass problem,
    we have to either choose macro-averaged or micro-averaged stats by class. Micro-averaged
    treats all samples equally, while macro-averaged treats all classes equally. Macro
    has an underlying issue where if the performance concerning a minority class is
    good, it will give a fake perception that the model is good in general. So, we
    will use micro-averaged here:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义辅助方法来帮助计算多类偏差和公平性指标，这些方法包括执行安全除法处理和零除法，防止NaN值，并计算假正例、真负例、总正例、总负例和总数据。由于这是一个多类问题，我们必须根据类别选择宏平均或微平均统计量。微平均方法将所有样本平等对待，而宏平均方法将所有类别平等对待。宏平均方法有一个潜在问题：如果某个少数类别的表现很好，它会给人一种模型总体表现很好的虚假印象。因此，我们将在这里使用微平均：
- en: '[PRE12]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we will utilize these helper methods to define the method that will
    compute four bias and fairness metrics using common computed results – that is,
    disparate impact, statistical parity difference, AOD, and AAOD:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将利用这些辅助方法定义一个方法，使用常见的计算结果来计算四个偏差和公平性指标——即，差异影响、统计平等差异、AOD和AAOD：
- en: '[PRE13]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will start by obtaining the false positives, true negatives, total positives,
    total negatives, and total data for the two groups, which are the privileged group
    and the non-privileged group, using the helper method:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先使用辅助方法获得特权组和非特权组的假正例、真负例、总正例、总负例和总数据：
- en: '[PRE14]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will compute the true positive ratio of both the privileged and non-privileged
    groups using the computed group stats so that it can be used directly to compute
    disparate impact and statistical parity difference:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用计算出的组统计数据计算特权组和非特权组的真实正例比例，以便直接用于计算差异影响和统计平等差异：
- en: '[PRE15]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will compute the two mentioned metrics using the true positive ratios:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用真实正例比例计算前面提到的两个指标：
- en: '[PRE16]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we will compute AOD and AAOD using the true positive rates, `tpr`,
    and false positive rates, `fpr`:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用真实正例率` tpr `和假正例率` fpr `来计算AOD和AAOD：
- en: '[PRE17]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will be computing these four metrics during training and will be able to
    monitor and track the metrics as they’re being trained. To track the experiment,
    we will record the parameters and monitor their performance by iteration and epoch.
    We will use MLflow to do this. Let’s define the `mlflow` logger and log the parameters
    we defined earlier in *step 2*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在训练过程中计算这四个指标，并能够在训练时监控和追踪这些指标。为了追踪实验，我们将记录参数并监控它们在每次迭代和每个epoch的表现。我们将使用MLflow来完成这个操作。让我们定义`mlflow`日志记录器并记录我们在*步骤2*中定义的参数：
- en: '[PRE18]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we will require a slightly specialized flow to be able to compute custom
    metrics and perform bias mitigation methods later on, we will define a custom
    runner using `catalyst` that will be used to train the ResNet50 model. We will
    need to define three custom logic for four methods: `on_loader_start` (to initialize
    the metric aggregator functionality), `handle_batch` (to obtain the loss), `on_loader_end`
    (to finalize the aggregated batch metrics and update the learning rate scheduler),
    and `get_loggers` (to log data into MLflow). Let’s start with defining `on_loader_start`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们稍后需要一个稍微专业化的流程来计算自定义指标并执行偏差缓解方法，我们将使用`catalyst`定义一个自定义的runner，用于训练ResNet50模型。我们需要定义四个方法的三条自定义逻辑：`on_loader_start`（初始化指标聚合器功能）、`handle_batch`（获取损失）、`on_loader_end`（最终汇总批次指标并更新学习率调度器）以及`get_loggers`（将数据记录到MLflow）。让我们从定义`on_loader_start`开始：
- en: '[PRE19]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we will define the batch handler logic, which will load the data from
    the batch data loader in a custom way, perform forward propagation using the model
    initialized in the runner, and then compute loss, accuracy, and multiclass fairness
    metrics:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义批次处理逻辑，它将以自定义方式从批次数据加载器加载数据，使用在runner中初始化的模型执行前向传播，然后计算损失、准确率和多类公平性指标：
- en: '[PRE20]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the same method, we will also be required to update the current batch metric
    and the aggregated batch metrics so that they can be logged properly and finally
    perform backpropagation if it is training mode instead of validation mode:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一方法中，我们还需要更新当前批次指标和聚合批次指标，以便它们能够正确地记录，并在训练模式下执行反向传播，而不是验证模式：
- en: '[PRE21]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we must define the final two straightforward methods:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须定义最后两个简单的方法：
- en: '[PRE22]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we will initialize the runner and train the model:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将初始化运行器并训练模型：
- en: '[PRE23]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*Figure 13**.1* shows the `mlflow` plotted performance graph of the accuracy,
    AOD, disparate impact, and statistical parity difference at every epoch of both
    the train and validation partitions:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图13.1* 显示了 `mlflow` 绘制的训练和验证集每个轮次的准确度、AOD、差异影响和统计平等差异的性能图：'
- en: '![Figure 13.1 – Performance graph of ResNet50 by epochs](img/B18187_13_1.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 – ResNet50 模型按训练轮次的性能图](img/B18187_13_1.jpg)'
- en: Figure 13.1 – Performance graph of ResNet50 by epochs
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – ResNet50 模型按训练轮次的性能图
- en: The validation scores ended up with 0.424 for both AAOD and AOD, 0.841 accuracy,
    0.398 disparate impact, and 0.051 statistical parity.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集得分为 AAOD 和 AOD 各 0.424，准确度 0.841，差异影响 0.398，统计平等 0.051。
- en: The graph shows that while accuracy increased epoch after epoch, the bias and
    fairness metrics gradually became worse but stagnated at a value. The lowest bias
    model is at the 0th epoch, where everything was close to zero, including the accuracy.
    Even though the model is not biased, the model is not useful at all. Another interesting
    observation is at the 16th epoch mark – the model managed to get a better validation
    accuracy performance on the male samples as the AOD value became higher at the
    same point. Depending on the circumstances, you can opt to choose to take the
    model at the 15th epoch, which has a somewhat good accuracy score but not the
    best and a lower bias score.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图表显示，尽管准确度随着每一轮训练不断提高，但偏差和公平性度量逐渐变差，但停滞在某个值。最低偏差的模型出现在第 0 轮训练时，那时一切几乎为零，包括准确度。尽管该模型没有偏差，但它根本没有任何用处。另一个有趣的观察是在第
    16 轮训练时，模型成功地在男性样本上提高了验证准确度，而 AOD 值在同一时刻变得更高。根据具体情况，你可以选择第 15 轮训练的模型，该模型的准确度得分较好，但不是最佳的，且偏差得分较低。
- en: 'To make a deeper analysis, let’s take a look at where the model is focusing
    when it’s making predictions using the integrated gradients technique from Captum.
    Let’s visualize images that are mostly frontal facing. To do this, we must define
    the necessary transform method:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进行更深的分析，让我们看看模型在使用 Captum 的集成梯度技术进行预测时关注的重点。我们将可视化大多数面向正面的图像。为了做到这一点，我们必须定义必要的转换方法：
- en: '[PRE24]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let’s define the frontal faces to visualize:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义要可视化的正面人脸：
- en: '[PRE25]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, let’s visualize the focus area:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们可视化关注区域：
- en: '[PRE26]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Figure 13**.2* shows the original images and focus areas of the model:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图13.2* 显示了原始图像和模型的关注区域：'
- en: '![Figure 13.2 – Saliency explanations results of the trained ResNet50 model](img/B18187_13_2.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图13.2 – 训练的 ResNet50 模型的显著性解释结果](img/B18187_13_2.jpg)'
- en: Figure 13.2 – Saliency explanations results of the trained ResNet50 model
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 训练的 ResNet50 模型的显著性解释结果
- en: These visuals show that the model exhibits bias by focusing incorrectly on the
    hair of female faces. For males, the model did not focus on the hair. The model
    also focuses on the white background a little. We’ll learn how to remove this
    bias in the next section.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些可视化结果表明，模型在处理女性面孔时存在偏差，错误地将注意力集中在头发上。对于男性，模型并没有关注头发。模型也稍微关注了白色背景。我们将在下一节学习如何去除这种偏差。
- en: Now that we understand some popular bias and fairness metrics, we need to know
    which metrics to use in different use cases.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了一些常见的偏差和公平性度量标准，我们需要知道在不同的使用案例中应使用哪些度量标准。
- en: Tailoring bias and fairness measures across use cases
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在不同使用案例中定制偏差和公平性度量
- en: 'The process of figuring out bias and fairness metrics to use for our use case
    can flow similarly to the process of figuring out general model performance evaluation
    metrics, as introduced in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161), *Exploring
    Model Evaluation Methods*, in the *Engineering the base model evaluation metric*
    section. So, be sure to check that topic out! However, bias and fairness have
    unique aspects that require additional heuristical recommendations. Earlier, recommendations
    for metrics that belong to the same metric group were explored. Now, let’s explore
    general recommendations on the four metric groups:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 确定偏差和公平性度量标准以适应我们使用案例的过程，和确定一般模型性能评估度量标准的过程类似，正如在 [*第10章*](B18187_10.xhtml#_idTextAnchor161)《探索模型评估方法》的*工程化基础模型评估度量标准*部分中所介绍的那样。所以，一定要去查看那个话题！然而，偏差和公平性有独特的方面，需要额外的启发式推荐。之前，我们探索了属于同一度量组的度量推荐。现在，让我们探索四个度量组的通用推荐：
- en: Equal representation is always desired when there is a sensitive and protected
    attribute. So, when you see these attributes, be sure to use equal representation-based
    metrics on both your data and the model. Examples include race, gender, religion,
    sexual orientation, disability, age, socioeconomic status, political affiliations,
    and criminal history.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当存在敏感和受保护属性时，平等代表性始终是期望的。因此，当你看到这些属性时，务必在数据和模型中使用基于平等代表性的度量。示例包括种族、性别、宗教、性取向、残疾、年龄、社会经济地位、政治倾向和犯罪历史。
- en: Predictive performance consistency is another desired trait of a machine learning
    model that deals with sensitive and protected attributes. So, when you see these
    attributes, be sure to use equal error-based metrics on your model.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测性能一致性是处理敏感和受保护属性的机器学习模型的另一个期望特性。因此，当你看到这些属性时，务必在模型中使用基于错误的平等度量。
- en: Both distributional fairness metrics and equal representation metrics measure
    inequality. However, distributional fairness works on continuous variables directly
    while equal representation metrics work on categorical variables. Binning can
    be done on continuous variables to transform them into categories but it’s not
    straightforward to decide on the proper binning strategy needed. So, use distributional
    fairness metrics when the variable to measure bias and fairness is a continuous
    variable.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布公平性度量和平等代表性度量都衡量不平等。然而，分布公平性直接作用于连续变量，而平等代表性度量作用于分类变量。可以对连续变量进行分箱处理，将其转化为类别，但如何选择合适的分箱策略并不简单。因此，当衡量偏差和公平性的变量是连续变量时，应使用分布公平性度量。
- en: 'Consider all potential aspects of bias and fairness and measure them separately
    with a chosen bias and fairness metrics:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑所有潜在的偏差和公平性方面，并使用选择的偏差和公平性度量分别进行衡量。
- en: Let’s consider a scenario where a machine learning model is used to predict
    loan approvals. One aspect of fairness is to ensure that loan approvals are granted
    fairly across different demographic groups, such as race or gender. Ensuring equal
    representation of the data and the resulting model can help you accomplish that.
    However, solely focusing on equal representation may not capture the complete
    picture. For example, even though the overall loan approval rates are equal across
    groups, there could be a significant disparity in the interest rates assigned
    to different groups. This disparity in interest rates could lead to unfair and
    inequitable outcomes, as certain groups may be charged higher interest rates,
    resulting in financial disadvantages. Any additional evaluation and monitoring
    that uses distributional fairness metrics can help you understand the impact and
    assist in targeted bias mitigation of unfavored groups.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设有一个机器学习模型用于预测贷款审批。公平性的一个方面是确保贷款审批在不同的群体之间（如种族或性别）是公平的。确保数据的平等代表性以及由此产生的模型可以帮助你实现这一目标。然而，仅仅关注平等的代表性可能无法捕捉到完整的情况。例如，即使不同群体之间的整体贷款批准率是平等的，不同群体的利率差异可能非常显著。这种利率的差异可能导致不公平和不平等的结果，因为某些群体可能会被收取更高的利率，从而导致财务上的不利影响。使用分布公平性度量进行额外评估和监控可以帮助你理解影响，并帮助针对不利群体的偏差缓解。
- en: Evaluating the individual fairness of the outcome is useful when you deploy
    the machine learning model and receive individual data during model inferencing.
    A threshold can be set here to create an alert when the individual fairness score
    is too high and requires a human reviewer to evaluate and make a manual decision.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署机器学习模型并在模型推断过程中接收个体数据时，评估结果的个体公平性非常有用。这里可以设置一个阈值，当个体公平性得分过高时，触发警报，并需要人工审查员进行评估并做出手动决策。
- en: Compute these metrics separately by sensitive groups and compare them visually
    to get a sense of fairness across groups. This can help you craft targeted bias
    mitigation responses to vulnerable groups.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分别计算这些度量并通过敏感群体进行可视化对比，以感受群体间的公平性。这可以帮助你制定针对易受影响群体的偏差缓解响应。
- en: Prediction explanations may help in understanding the reasons for bias and fairness.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测解释可能有助于理解偏差和公平性的原因。
- en: Bias and fairness measures can conflict with accuracy-based performance measures.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差和公平性度量可能与基于准确性的性能度量发生冲突。
- en: 'Additionally, two global opposing views that will affect how you see fairness
    are worth mentioning:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有两个全球性对立的观点值得一提，它们会影响你如何看待公平性：
- en: '**We’re All Equal** (**WAE**): The notion that data may not accurately represent
    reality due to the presence of inherent biases within it'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们都平等** (**WAE**)：这一概念认为，由于数据中固有偏见的存在，数据可能无法准确地代表现实。'
- en: '**What You See Is What You Get** (**WYSIWYG**): The notion is that the information
    presented by the data reflects an unbiased representation of reality, even if
    it reveals inequalities'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所见即所得** (**WYSIWYG**)：这一观点认为，数据所呈现的信息反映了一个不带偏见的现实表现，即便它揭示了不平等现象。'
- en: An extreme of either view will either remove any chances of building a model
    (WAE) or be too ignorant to care about fairness biases that will eventually lead
    to negative consequences, as mentioned earlier in this chapter (WYSIWYG). To create
    a successful machine learning model in our use case, we will need to balance the
    two views strategically so that both accuracy performance and fairness requirements
    can be satisfied. A good strategy here to employ is to apply common sense and
    accept what seems logical for a causal relationship (WYSIWYG), and practice fair
    processes that help us understand and mitigate the partial aspect of bias (WAE)
    even when true fairness can’t be achieved.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一方极端的观点都将导致无法构建模型（WAE）或对公平偏见缺乏足够的重视，最终导致负面后果，正如本章前面提到的（WYSIWYG）。为了在我们的应用场景中创建一个成功的机器学习模型，我们需要战略性地平衡这两种观点，以便同时满足准确性表现和公平性要求。这里采用的一个好策略是运用常识，接受因果关系中看似合逻辑的部分（WYSIWYG），并采取公平的流程帮助我们理解和缓解偏见的局部方面（WAE），即使不能实现真正的公平。
- en: 'Here are some examples with views to adopt that will make the most sense:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些值得采用的、最有意义的观点示例：
- en: '**Adopting WYSIWYG**: In the domain of personalized advertising using machine
    learning, the WYSIWYG view would involve tailoring advertisements based on observed
    user behavior and preferences. For example, if a user frequently engages with
    content related to fitness, the system will present ads for fitness equipment
    or gym memberships. The goal is to provide a personalized user experience that
    aligns with their interests and needs. In this use case, any notion of the WAE
    view will fail the project as it contradicts the goal of personalization.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采用WYSIWYG**：在个性化广告领域，使用机器学习的WYSIWYG观点将包括根据观察到的用户行为和偏好定制广告。例如，如果一个用户频繁与健身相关的内容互动，系统将展示健身设备或健身会员卡的广告。目标是提供与用户兴趣和需求相符的个性化体验。在这个应用场景中，任何关于WAE观点的内容都会使项目失败，因为它与个性化的目标相冲突。'
- en: '**Adopting something like 75% WYSIWYG and 25% WAE**: In the context of making
    hiring decisions using machine learning, the WAE view would help advocate for
    equal consideration of all candidates without any bias related to gender, race,
    or other protected attributes. The WYSIWYG view will then allow a machine learning
    model to be built based solely on their qualifications and skills. The strategy
    to create a fair and unbiased selection process will help promote equal opportunities
    for all applicants while still allowing a functional machine learning model to
    be built successfully.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采用类似于75% WYSIWYG和25% WAE的策略**：在使用机器学习做出招聘决策的情境中，WAE观点有助于倡导对所有候选人的平等考虑，避免性别、种族或其他受保护属性相关的偏见。WYSIWYG观点则会允许仅根据候选人的资历和技能来构建机器学习模型。创建一个公平且无偏的选拔过程的策略将有助于为所有应聘者提供平等机会，同时仍能成功地构建一个有效的机器学习模型。'
- en: In terms of metrics, the equal representation-based metric group is associated
    with the WAE view while the equal error-based metric group is associated with
    the WYSIWYG view. So, again, choose these two metric groups if the two views are
    involved in your use case.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在度量标准方面，基于平等代表性的度量标准组与WAE观点相关，而基于平等错误的度量标准组与WYSIWYG观点相关。因此，如果在应用场景中涉及这两种观点，请选择这两组度量标准。
- en: Next, we’ll discover ways we can mitigate bias in our machine learning models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何减轻机器学习模型中的偏见。
- en: Mitigating AI bias
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减轻AI偏见
- en: AI bias is an algorithmic bias that either comes from the model itself through
    its learning process or the data it used to learn from. The most obvious solution
    to mitigate bias is not programmatic mitigation methods but ensuring fair processes
    when collecting data. A data collection and preparation process is only truly
    fair when it not only ensures the resulting data is balanced by sensitive attributes
    but also ensures all inherent and systematic biases are not included.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: AI 偏见是一种算法偏见，可能来自模型本身的学习过程，或者来自用于学习的数据。缓解偏见的最明显解决方案不是程序化的缓解方法，而是确保在收集数据时采用公平的流程。数据收集和准备过程只有在确保结果数据不仅通过敏感属性平衡，而且确保不包含所有固有和系统性偏见时，才能算是完全公平的。
- en: Unfortunately, a balanced dataset based on the sensitive attribute does not
    guarantee a fair model. There can be differences in appearance among subgroups
    under the hood or associative groups of the data concerning multiple factors,
    which can potentially cause a biased system. Bias, however, can be mitigated partially
    when the dataset is balanced compared to without concerning the observable sensitive
    groups. But what are all these attributes? It might be easier to identify data
    attributes in tabular structured data with defined column names, but for unstructured
    data meant for deep learning, it’s impossible to cover all the possible attributes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，基于敏感属性的平衡数据集并不能保证模型的公平性。不同子组之间，或数据中与多种因素相关的联动组，可能存在外观上的差异，这可能导致系统偏见。然而，当数据集相较于未考虑可观察敏感群体时是平衡的，偏见可以在一定程度上得到缓解。但这些属性到底是什么呢？在具有明确定义列名的表格结构数据中，识别数据属性可能更容易，但对于深度学习用的非结构化数据，几乎不可能涵盖所有可能的属性。
- en: To dive deeper into actual examples, text data can contain a ton of attributes
    with examples, such as language, genre, topic, length, style, tone, time period,
    authorship, geographic origin, and cultural perspective. Image data can also contain
    a ton of attributes with examples such as subject/content, perspective, lighting,
    composition, color, texture, resolution, orientation, context, and cultural relevance.
    Finally, audio data can also contain a ton of attributes, such as genre, language,
    duration, sound quality, instrumentation, vocal style, tempo, mood, cultural influence,
    and recording environment.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地探讨实际示例，文本数据可以包含大量属性及其示例，例如语言、类型、主题、长度、风格、语气、时间段、作者身份、地理来源和文化视角。图像数据也可以包含大量属性及其示例，例如主题/内容、视角、光照、构图、颜色、纹理、分辨率、方向、背景和文化相关性。最后，音频数据也可以包含大量属性，例如类型、语言、时长、音质、乐器、声乐风格、节奏、情绪、文化影响和录音环境。
- en: It’s hard to ensure equal representation in all facets - more so when data is
    readily available and is already originally highly imbalanced with only a few
    examples for certain categories and plenty for others. Ideally, bias mitigation
    should always be executed right from the data preparation stage. However, if that's
    not possible, programmatic bias mitigation methods can be applied after data collection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在所有方面的平等代表性很困难——尤其是当数据轻松获得且原始数据本身已经高度不平衡，仅在某些类别中有少量示例，而其他类别有大量示例时。理想情况下，偏见缓解应始终从数据准备阶段开始执行。然而，如果无法做到这一点，程序化的偏见缓解方法可以在数据收集后应用。
- en: 'To get a clearer idea of these methods, have a look at *Figure 13**.3*, which
    presents an overview of various bias-reducing approaches:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地了解这些方法，请查看*图 13.3*，它展示了各种减少偏见的 approaches：
- en: '![Figure 13.3 – The four steps of bias mitigation under two machine learning
    life cycle stages](img/B18187_13_3.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3 – 在两个机器学习生命周期阶段下，偏见缓解的四个步骤](img/B18187_13_3.jpg)'
- en: Figure 13.3 – The four steps of bias mitigation under two machine learning life
    cycle stages
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 在两个机器学习生命周期阶段下，偏见缓解的四个步骤
- en: 'Programmatic bias mitigation provides methods that can be applied in three
    different stages of the model-building process:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 程序化的偏见缓解提供了可以在模型构建过程的三个不同阶段应用的方法：
- en: '**Pre-processing**: This is part of the data preparation ML life cycle stage.
    Here are some examples of bias mitigation methods that belong to this group:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：这是数据准备阶段中的一部分。以下是属于此组的一些偏见缓解方法示例：'
- en: Eliminating protected attributes from being used in the model. However, information
    about the protected attribute could still present itself in other associative
    attributes. Additionally, some attributes are deeply interconnected with other
    key information that’s required to predict the desired label, and unfortunately
    can’t be removed easily. Examples include gender in facial images.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除受保护属性在模型中的使用。然而，关于受保护属性的信息可能仍然会通过其他相关属性呈现。此外，某些属性与预测所需标签的其他关键信息紧密相连，且无法轻易移除。例如，面部图像中的性别。
- en: Disparate Impact Remover, available in the AIF360 open source library. It balances
    the dataset group proportions by dropping data rows to achieve a better disparate
    score.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平衡影响移除器，可在 AIF360 开源库中使用。它通过删除数据行来平衡数据集群体的比例，从而实现更好的不平衡得分。
- en: Weigh privileged class loss so that it’s lower than unprivileged class loss
    during training so that errors between the classes are more equal.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，对特权类别损失加权，使其低于弱势类别的损失，从而使各类别之间的误差更加平等。
- en: 'Targeted data augmentation for unprivileged classes. Augmentation adds more
    data variations and can be applied in two ways:'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对弱势群体的数据增强。增强技术可以通过两种方式增加数据变化：
- en: Adding more data variations in unprivileged groups with augmentation will help
    increase accuracy performance there and contribute to the balancing of error rates,
    especially when the unprivileged class is underrepresented.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在弱势群体中添加更多数据变化进行增强，有助于提高该群体的准确性，并有助于平衡错误率，特别是当弱势群体的样本不足时。
- en: 'Counterfactual role reversal augmentation inverts the privileged group into
    the unprivileged group and vice versa and allows for equal representation. Here
    are some augmentations that can be used based on variable type:'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反事实角色反转增强技术将特权群体反转为弱势群体，反之亦然，允许实现平等代表性。以下是根据变量类型可以使用的一些增强技术：
- en: '• **Text**: Use word swap'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • **文本**：使用词语交换
- en: '• **All variable types**: Use style transfer techniques. Example techniques
    are as follows:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: • **所有变量类型**：使用风格迁移技术。以下是一些示例技术：
- en: Using Generative AI techniques from a trained StarGAN, which is an image generator
    that can invert a person’s gender, change a person’s age, and much more.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练过的 StarGAN 生成式 AI 技术，这是一种图像生成器，能够反转一个人的性别、改变一个人的年龄等等。
- en: Optimize the desired image to mimic the style of another image by reducing the
    distance between a chosen intermediate layer between the images. This method is
    called transfer by input optimization and is based on the neural interpretation
    technique mentioned in the previous chapter.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少选定中间层与图像之间的距离，优化期望图像以模仿另一图像的风格。这种方法被称为输入优化传递，基于上一章提到的神经解释技术。
- en: Targeted counterfactual role reversal not through additional augmentation but
    a permanent input replacement for a deployed model. This allows you to explicitly
    control the equality of the results during deployment.
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对已部署模型，通过替换输入进行针对性的反事实角色反转，而不是额外的增强。这允许您在部署过程中显式地控制结果的平等性。
- en: OpenAI changed “male” to “female” in their Dall-E text-to-image prompts randomly
    as the model is biased to depict males for roles such as professor and CEO.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 在 Dall-E 的文本到图像提示中随机将“male”更改为“female”，因为该模型有偏向性，倾向于将男性描绘为教授和 CEO 等角色。
- en: '**In-processing**: This is part of the model development machine learning life
    cycle stage and is the process of training a model. Here are some examples of
    bias mitigation methods that belong to this group:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理过程中**：这是模型开发机器学习生命周期阶段的一部分，是训练模型的过程。以下是属于这一组的偏差缓解方法的示例：'
- en: '**Knowledge distillation**: This was introduced in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*. The idea is that a teacher model that has
    been trained with a much bigger and more representative dataset will be less biased
    compared to a student model that is being trained on a much smaller custom dataset.
    Ideally, the fairness from the teacher model will be distilled into the student
    model. However, knowledge distillation can also cause an increased bias in the
    resulting student model when you distill it with a biased teacher model.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识蒸馏**：这一概念在[*第8章*](B18187_08.xhtml#_idTextAnchor125)《探索监督深度学习》中介绍。其思路是，使用更大且更具代表性数据集训练的教师模型，相比于用较小定制数据集训练的学生模型，更不易产生偏见。理想情况下，教师模型中的公平性将被蒸馏到学生模型中。然而，当用有偏的教师模型进行蒸馏时，知识蒸馏也可能会导致生成的学生模型产生更强的偏见。'
- en: '**Adversarial debiasing**: This method iteratively trains a classifier to optimize
    prediction accuracy while simultaneously minimizing an adversary model’s ability
    to infer the protected attribute from the predictions. The classifier aims to
    make accurate predictions on the target variable, while the adversary tries to
    discern the sensitive attribute associated with bias. This simultaneous training
    process creates a competitive environment where the classifier learns to encode
    important information about the target variable while reducing the influence of
    potentially biased features. By doing so, adversarial debiasing promotes fairness
    by mitigating the impact of sensitive attributes and enhancing the overall equity
    of the model’s predictions.'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗性去偏**：该方法通过迭代训练一个分类器，以优化预测准确性，同时最小化对抗性模型从预测中推断受保护属性的能力。分类器旨在对目标变量进行准确预测，而对抗模型则试图辨别与偏差相关的敏感属性。这一同时训练的过程创造了一个竞争环境，使得分类器能够学习编码目标变量的重要信息，同时减少潜在偏见特征的影响。通过这种方式，对抗性去偏通过减轻敏感属性的影响并增强模型预测的整体公平性来促进公平性。'
- en: '**Regularization**: In deep learning, regularization involves any addition
    or modification to the neural network, data, or training process that is used
    to increase the generalization of the model to external data. This can indirectly
    contribute to reducing bias in the model. Some common regularization methods include
    dropout layers, L1/L2 regularization, batch normalization, group normalization,
    weight standardization, stochastic depth, label smoothing, and data augmentation.
    By improving generalization, regularization methods can help the model learn more
    general patterns in the data instead of fitting too closely to the training set,
    which might contain biased features. This method was explored more extensively
    in [*Chapter 2*](B18187_02.xhtml#_idTextAnchor040), *Designing Deep* *Learning
    Architectures*.'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：在深度学习中，正则化指的是对神经网络、数据或训练过程进行的任何添加或修改，旨在增加模型对外部数据的泛化能力。这可以间接帮助减少模型中的偏差。一些常见的正则化方法包括
    dropout 层、L1/L2 正则化、批量归一化、组归一化、权重标准化、随机深度、标签平滑和数据增强。通过改善泛化能力，正则化方法有助于模型学习数据中的更一般性模式，而不是过度拟合训练集中的偏见特征。该方法在[*第二章*](B18187_02.xhtml#_idTextAnchor040)中有更为详细的探讨，*深度学习架构设计*。'
- en: '**Post-processing**: This is part of the model development machine learning
    life cycle stage but only covers processing the trained model’s outputs to mitigate
    bias. Here are some examples of bias mitigation methods that belong to this group:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**：这是模型开发机器学习生命周期阶段的一部分，但仅涵盖处理已训练模型输出以减轻偏差。以下是属于该组的一些偏差缓解方法示例：'
- en: '**Test-time counterfactual role reversal augmentation ensemble**: This method
    involves performing two predictions each using opposite roles and performing an
    ensemble of the predictions. A max or exponential mean operation for ensembling
    performs much better than an average operation, as shown in [https://www.amazon.science/publications/mitigating-gender-bias-in-distilled-language-models-via-counterfactual-role-reversal](https://www.amazon.science/publications/mitigating-gender-bias-in-distilled-language-models-via-counterfactual-role-reversal).'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试时反事实角色逆转增强集成**：该方法涉及执行两个预测，每个使用相反的角色，并对预测结果进行集成。使用最大值或指数均值操作进行集成比平均操作效果更好，具体见[https://www.amazon.science/publications/mitigating-gender-bias-in-distilled-language-models-via-counterfactual-role-reversal](https://www.amazon.science/publications/mitigating-gender-bias-in-distilled-language-models-via-counterfactual-role-reversal)。'
- en: '**Equalized odds postprocessing**: This method involves modifying the output
    predictions of a classifier to ensure equal false positive and false negative
    rates across different groups through threshold optimization by groups. Specifically,
    prediction thresholds for each protected group are determined separately.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均衡机会后处理**：该方法通过通过阈值优化来调整分类器的输出预测，以确保不同组之间的假阳性和假阴性率相等，具体来说，分别为每个受保护组单独确定预测阈值。'
- en: It is important to note that a fairer model obtained after bias mitigation would
    likely cause a reduction in accuracy-based metrics. If the loss in an accuracy-based
    metric is minor enough, a fair model is highly desired as there would not be an
    issue in using bias mitigation methods. To that end, always start by creating
    a baseline model and evaluate fairness with the chosen bias and fairness metrics
    to ensure that bias even exists in your model before using any bias mitigation
    methods. Bias mitigation methods always cause a substantial increase in training
    time needed, so make sure it’s worth it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，通过偏差缓解方法得到的更公平的模型可能会导致基于准确率的指标下降。如果在准确率指标上的损失足够小，那么公平的模型是非常值得追求的，因为使用偏差缓解方法不会带来问题。为此，始终从创建基线模型开始，并使用所选择的偏差和公平性指标评估公平性，以确保在使用任何偏差缓解方法之前，模型中确实存在偏差。偏差缓解方法总是会显著增加训练时间，因此确保它是值得的。
- en: 'Additionally, there are a few behaviors related to bias that are useful to
    know about:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些与偏差相关的行为是有用的，需要了解：
- en: Decision-making algorithms tend to be biased toward more common occurrences,
    so balanced data can help reduce this bias
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策算法往往偏向于更常见的事件，因此平衡的数据有助于减少这种偏差。
- en: Pruned models increase bias (more at [https://arxiv.org/pdf/2106.07849.pdf](https://arxiv.org/pdf/2106.07849.pdf))
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝模型增加了偏差（更多内容请见[https://arxiv.org/pdf/2106.07849.pdf](https://arxiv.org/pdf/2106.07849.pdf)）。
- en: Models with limited capacity (smaller models) tend to exploit the biases in
    the dataset ([https://aclanthology.org/2022.gebnlp-1.27.pdf](https://aclanthology.org/2022.gebnlp-1.27.pdf)),
    so be wary when you’re performing knowledge distillation on a smaller model and
    evaluate bias and fairness
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量有限的模型（较小的模型）往往会利用数据集中的偏差（[https://aclanthology.org/2022.gebnlp-1.27.pdf](https://aclanthology.org/2022.gebnlp-1.27.pdf)），因此在对较小模型进行知识蒸馏时需要小心，并评估偏差和公平性。
- en: 'When it comes to mitigating bias, you might be wondering which of the four
    groups of methods to choose from. Here’s a suggestion: it’s best to begin by addressing
    bias as early as possible in the process, where you have the most control and
    flexibility. You don’t want to be stuck with limited options to mitigate bias
    and end up not being able to satisfactorily mitigate bias. If you don’t have access
    to the data collection stage, you can utilize data preprocessing techniques. On
    the other hand, if you don’t have access to the data itself but have a trained
    model, postprocessing techniques should be used. Techniques can be combined, so
    be sure to measure bias using the metrics introduced to ensure that each technique
    that’s applied improves the fairness of the resulting model. Also, consider using
    multiple methods to mitigate more bias.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在缓解偏差时，您可能会想选择哪四类方法中的哪一类。以下是一个建议：最好在流程的尽早阶段解决偏差问题，这样您可以获得更多的控制权和灵活性。您不希望在缓解偏差时受到有限选项的困扰，最终无法令人满意地解决偏差问题。如果您无法访问数据收集阶段，您可以利用数据预处理技术。另一方面，如果您无法访问数据本身，但拥有已训练的模型，那么应该使用后处理技术。可以将多种技术结合使用，因此请务必使用引入的度量标准来衡量偏差，确保所应用的每种技术都能提高最终模型的公平性。同时，考虑使用多种方法来缓解更多偏差。
- en: 'The programmatic bias mitigation methods we’ve introduced so far are separated
    into three groups. However, a robust bias mitigation method exists for deep learning
    models that lies in the intersection of all three of them. The method is a fusion
    between counterfactual augmentation, mix-up augmentation, knowledge distillation,
    and counterfactual test time augmentation. The idea is to do the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们介绍的程序化偏差缓解方法分为三类。然而，深度学习模型中存在一种强有力的偏差缓解方法，它位于这三类方法的交集处。该方法融合了反事实数据增强、混合增强、知识蒸馏和反事实测试时增强。其思想如下：
- en: Use counterfactual augmentation to train both the teacher model and student
    on the entire dataset.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反事实数据增强来训练教师模型和学生模型，涵盖整个数据集。
- en: Distill the counterfactual ensembled chosen layer features of the teacher model
    with exponential max to the student model. This is similar to mix-up augmentation
    but on the feature layer.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指数最大值对教师模型的反事实集成选择层特征进行蒸馏到学生模型。这类似于混合增强，但是在特征层面进行的。
- en: 'However, counterfactual role reversal remains to be a technique that’s more
    accessible to text. Let’s discover how to use the generic knowledge distillation
    method to reduce bias on the same use case we explored in the previous section
    on face classification. The method we will be introducing here is from [https://arxiv.org/pdf/2112.09786.pdf](https://arxiv.org/pdf/2112.09786.pdf),
    which provides two methods:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，反事实角色互换仍然是更易于应用于文本的技术。让我们探索如何使用通用的知识蒸馏方法，减少在上一节中我们探索的人脸分类的同一用例中的偏差。我们在这里将介绍的方法来自[https://arxiv.org/pdf/2112.09786.pdf](https://arxiv.org/pdf/2112.09786.pdf)，该方法提供了两种方法：
- en: '**(Distill and debias) D&D++**: First, train the teacher model using only privileged
    group data. Then, initialize the student model with the teacher model weights
    and train the student model with normal loss and the knowledge distillation loss
    using the cosine similarity of the chosen feature layer. Lastly, initialize a
    final student network using the previously trained student model and train on
    the entire dataset with knowledge distillation from the previous student model
    as the teacher model.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(蒸馏与去偏) D&D++**：首先，使用仅限特权群体数据训练教师模型。然后，用教师模型的权重初始化学生模型，并使用正常的损失和基于所选特征层余弦相似度的知识蒸馏损失训练学生模型。最后，使用之前训练的学生模型初始化最终的学生网络，并在整个数据集上进行训练，以教师模型为来源，通过知识蒸馏从先前的学生模型中学习。'
- en: '**One-step distillation D&D**: To make it simpler, but still immediately effective,
    the steps can be simplified. First, train the teacher model using only privileged
    group data. Then, train the student model on the entire dataset with knowledge
    distillation from the teacher model.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一步法蒸馏 D&D**：为了简化流程，但仍然立即有效，步骤可以简化。首先，使用仅限特权群体数据训练教师模型。然后，使用教师模型的知识蒸馏，在整个数据集上训练学生模型。'
- en: 'Let’s explore the one-step distillation D&D method practically by using the
    use case we experimented with in the previous section:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用上一节中实验的用例，实际探索一步法蒸馏 D&D 方法：
- en: 'First, we will be training a teacher model using only the privileged group
    data, which only consists of male facial identities. Let’s define the specifically
    different configurations for this example:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用仅限特权群体数据训练教师模型，该数据仅包含男性面部身份。让我们为此示例定义具体的不同配置：
- en: '[PRE27]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will be using the same code base we introduced in the previous section but
    with some additional custom code. The first is an addition to the code in *step
    6* of the *Evaluating the bias and fairness of a deep learning model* section,
    where we will be removing the female data in both the training and validation
    data:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用上一节中介绍的相同代码库，但会添加一些自定义代码。第一个是对*第6步*中*评估深度学习模型的偏差与公平性*部分代码的扩展，我们将移除训练和验证数据中的女性数据：
- en: '[PRE28]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Second is *step 7*, where we will define a special forward method and the method
    to get the last conv features. The idea is that we want to make sure the focus
    areas are the same and not the probabilities of the facial identity themselves,
    which will be useless when the model is used as a facial recognition featurizer.
    The special forward method is designed to return both the output logits and the
    last convolutional features in one forward pass, reducing latency:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二是*第7步*，我们将定义一个特殊的前向方法和获取最后卷积特征的方法。我们的想法是确保关注的区域相同，而不是面部身份的概率，因为在模型作为人脸识别特征提取器时，这些概率是无用的。这个特殊的前向方法旨在一次前向传递中返回输出对数值和最后的卷积特征，从而减少延迟：
- en: '[PRE29]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next is *step 8*, where we will extract the teacher model features once before
    starting training. This will reduce the time and resources needed to train the
    student model as the teacher model’s features will remain fixed on the same data
    without augmentation:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是*第8步*，在开始训练之前，我们将提取一次教师模型特征。这将减少训练学生模型所需的时间和资源，因为教师模型的特征将在相同的数据上保持固定，不做增强处理：
- en: '[PRE30]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will define the training and validation dataset that takes in the
    training and validation teacher model features separately from *step 10*:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义训练和验证数据集，将训练和验证的教师模型特征分别输入到*第10步*中：
- en: '[PRE31]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The final change is to add handling of the batch loading by taking the extra
    teacher model features and loss using both cross entropy loss and the cosine similarity
    loss of the student and teacher model’s last convolutional layer features from
    *step 14*:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的修改是通过处理批加载来添加额外的教师模型特征和损失，使用交叉熵损失和学生与教师模型最后一层卷积特征的余弦相似度损失，来自*第14步*：
- en: '[PRE32]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, run through all the steps from the previous section with the changes; you’ll
    get an approximate validation metric score of 0.774 accuracy with all other scores
    of 0 as there are no females in this first step.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照前一节的步骤运行，进行更改后，你将获得一个大约为0.774的验证指标分数，所有其他分数为0，因为在这第一步中没有女性。
- en: 'To execute the next step of one-step distillation, execute the previous code
    once more but with the following configuration changes:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行一步蒸馏的下一步骤，请再次执行之前的代码，但进行以下配置更改：
- en: '[PRE33]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The validation scores for the one-step distillation D&D approach end up with
    0.3838 for both AAOD and AOD, 0.76 accuracy, 0.395 disparate impact, and 0.04627
    statistical parity. For a fair comparison, the performance of basic training in
    terms of accuracy is an accuracy of 0.76333, 0.38856 for both AAOD and AOD, a
    0.3885 disparate impact, and a 0.04758 statistical parity. This means that the
    improvements are mainly from the AOD with a 0.0047 difference and statistical
    parity with a 0.00131 difference.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一步蒸馏D&D方法的验证得分为：AAOD和AOD均为0.3838，准确率为0.76，差异影响为0.395，统计公平性为0.04627。为了公平比较，基础训练的准确率为0.76333，AAOD和AOD均为0.38856，差异影响为0.3885，统计公平性为0.04758。这意味着改进主要来源于AOD，差异为0.0047，统计公平性差异为0.00131。
- en: 'By using the integrated gradients code from *step 19* again on the model we
    trained with bias mitigation methods, the following results can be obtained:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过再次使用*第19步*中的集成梯度代码，并应用于我们使用偏见缓解方法训练的模型，可以得到以下结果：
- en: '![Figure 13.4: Explanations of the one-step distillation D&D model](img/B18187_13_4.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图13.4：一步蒸馏D&D模型的解释](img/B18187_13_4.jpg)'
- en: 'Figure 13.4: Explanations of the one-step distillation D&D model'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：一步蒸馏D&D模型的解释
- en: The model now focuses on the right features – that is, facial features without
    backgrounds or hair regardless of gender.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型现在专注于正确的特征——即无论性别如何，面部特征而没有背景或头发。
- en: Now, experiment with D&D++ for yourself and see what improvements you can get!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，自己尝试使用D&D++，看看你能得到什么样的改进！
- en: In this section, we discovered a variety of mitigation techniques that apply
    to neural network models and also indirectly showed that metrics are not the only
    indicators of bias.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们发现了多种适用于神经网络模型的缓解技术，并间接地表明，评估指标并不是唯一的偏见指示器。
- en: While this chapter predominantly focused on deep learning models and unstructured
    data, it is important to note that the concepts, techniques, and metrics we discussed
    also apply to structured data. Bias can exist in structured data in the form of
    imbalanced classes, skewed attribute distributions, or unrepresentative samples,
    and can manifest in the model’s predictions, leading to unfair outcomes. A notable
    difference is that for structured data-based use cases, biases are usually more
    directly perpetuated by the input features. The bias and fairness evaluation methods,
    such as equal representation-based metrics, equal error-based metrics, and distributional
    fairness metrics, can be used to assess the fairness of machine learning models
    trained on structured data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章主要关注深度学习模型和非结构化数据，但值得注意的是，我们讨论的概念、技术和指标同样适用于结构化数据。结构化数据中的偏见可能以类别不平衡、属性分布偏斜或样本不代表性等形式存在，并可能在模型的预测中体现出来，导致不公平的结果。一个显著的区别是，对于基于结构化数据的用例，偏见通常是由输入特征直接延续的。偏见和公平性评估方法，如基于均等表现的指标、基于均等错误的指标和分布公平性指标，可以用来评估在结构化数据上训练的机器学习模型的公平性。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we focused on the critical issue of bias and fairness in machine
    learning models. The potential negative consequences of deploying biased models,
    such as legal actions and fines, were emphasized. We covered various types of
    biases and identified stages in the deep learning life cycle where bias can emerge,
    including planning, data preparation, model development, and deployment.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于机器学习模型中的偏见和公平性这一关键问题。我们强调了部署偏见模型可能带来的负面后果，如法律诉讼和罚款。我们讨论了不同类型的偏见，并确定了在深度学习生命周期中偏见可能出现的阶段，包括规划、数据准备、模型开发和部署。
- en: Several metrics for detecting and evaluating bias and fairness were also introduced,
    including equal representation-based metrics, equal error-based metrics, distributional
    fairness metrics, and individual fairness metrics. This chapter provided recommendations
    on selecting the right metrics for specific use cases and highlighted the importance
    of balancing opposing views, such as WAE and WYSIWYG, when evaluating fairness.
    This chapter also discussed programmatic bias mitigation methods that can be applied
    during the pre-processing, in-processing, and post-processing stages of model
    building. Examples of these methods include eliminating protected attributes,
    disparate impact remover, adversarial debiasing, and equalized odds post-processing.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还介绍了几种用于检测和评估偏见与公平性的指标，包括基于平等代表性的指标、基于平等错误的指标、分布公平性指标和个体公平性指标。章节中提供了选择适当指标的建议，针对具体用例，并强调了在评估公平性时平衡不同观点的重要性，例如WAE和WYSIWYG。本章还讨论了可以在模型构建的预处理、处理中和后处理阶段应用的程序性偏见缓解方法。这些方法的示例包括消除受保护属性、差异影响移除器、对抗性去偏和等化机会后处理。
- en: Finally, this chapter presented a comprehensive bias mitigation approach for
    deep learning models, combining counterfactual augmentation, mix-up augmentation,
    knowledge distillation, and counterfactual test-time augmentation. This approach
    aims to balance accuracy performance and fairness requirements in the model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章介绍了一种综合的深度学习模型偏见缓解方法，结合了反事实增强、混合增强、知识蒸馏和反事实测试时增强。这种方法旨在平衡模型的准确性性能和公平性要求。
- en: With this, you have learned about the importance and techniques of addressing
    bias and fairness in machine learning models and their potential negative consequences
    if not properly addressed. This knowledge will help you create machine learning
    systems that not only perform well on accuracy-based metrics but also consider
    the ethical implications and fairness aspects, ensuring a responsible and effective
    deployment of AI solutions, ultimately leading to better and more equitable outcomes
    in real-world applications.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，你已经了解了在机器学习模型中解决偏见与公平性的重要性和技术，以及如果不妥善处理可能带来的负面后果。这些知识将帮助你构建不仅在准确性指标上表现优异，而且考虑到伦理影响和公平性方面的机器学习系统，从而确保AI解决方案的负责任和有效部署，最终在现实应用中带来更好、更公平的结果。
- en: As we move forward, the next chapter will shift focus and analyze adversarial
    performance, a crucial aspect of ensuring robust and reliable machine learning
    models in production.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们前进，下一章将转向分析对抗性性能，这是确保生产中强健和可靠的机器学习模型的关键方面。
