- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Sequence-to-Sequence Learning – Neural Machine Translation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到序列学习 – 神经机器翻译
- en: Sequence-to-sequence learning is the term used for tasks that require mapping
    an arbitrary-length sequence to another arbitrary-length sequence. This is one
    of the most sophisticated tasks in NLP, which involves learning many-to-many mappings.
    Examples of this task include **Neural Machine Translation** (**NMT**) and creating
    chatbots. NMT is where we translate a sentence from one language (source language)
    to another (target language). Google Translate is an example of an NMT system.
    Chatbots (that is, software that can communicate with/answer a person) are able
    to converse with humans in a realistic manner. This is especially useful for various
    service providers, as chatbots can be used to find answers to easily solvable
    questions that customers might have, instead of redirecting them to human operators.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列学习是用于需要将任意长度序列映射到另一个任意长度序列的任务的术语。这是自然语言处理（NLP）中最复杂的任务之一，涉及学习多对多的映射。该任务的例子包括**神经机器翻译**（**NMT**）和创建聊天机器人。NMT是指我们将一个语言（源语言）的句子翻译成另一种语言（目标语言）。谷歌翻译就是一个NMT系统的例子。聊天机器人（即能够与人类沟通/回答问题的软件）能够以现实的方式与人类对话。这对于各种服务提供商尤其有用，因为聊天机器人可以用来解答顾客可能遇到的易于解决的问题，而不是将他们转接给人工客服。
- en: In this chapter, we will learn how to implement an NMT system. However, before
    diving directly into such recent advances, we will first briefly visit some **Statistical
    Machine Translation** (**SMT**) methods, which preceded NMT and were the state-of-the-art
    systems until NMT caught up. Next, we will walk through the steps required for
    building an NMT. Finally, we will learn how to implement a real NMT system that
    translates from German to English, step by step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何实现一个NMT系统。然而，在深入探讨这些最新进展之前，我们首先会简要回顾一些**统计机器翻译**（**SMT**）方法，这些方法是NMT之前的技术，并且在NMT赶超之前是当时的先进系统。接下来，我们将逐步讲解构建NMT所需的步骤。最后，我们将学习如何实现一个实际的NMT系统，从德语翻译到英语，逐步进行。
- en: 'Specifically, this chapter will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主要主题：
- en: Machine translation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: A brief historical tour of machine translation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译的简短历史回顾
- en: Understanding neural machine translation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解神经机器翻译
- en: Preparing data for the NMT system
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备NMT系统的数据
- en: Defining the model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型
- en: Training the NMT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练NMT
- en: The BLEU score – evaluating the machine translation systems
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLEU分数 – 评估机器翻译系统
- en: Visualizing attention patterns
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化注意力模式
- en: Inference with NMT
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NMT进行推理
- en: Other applications of Seq2Seq models – chatbots
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seq2Seq模型的其他应用 – 聊天机器人
- en: Machine translation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Humans often communicate with each other by means of a language, compared to
    other communication methods (for example, gesturing). Currently, more than 6,000
    languages are spoken worldwide. Furthermore, learning a language to a level where
    it is easily understandable to a native speaker of that language is a difficult
    task to master. However, communication is essential for sharing knowledge, socializing,
    and expanding your network. Therefore, language acts as a barrier to communicating
    with people in different parts of the world. This is where **Machine Translation**
    (**MT**) comes in. MT systems allow the user to input a sentence in their own
    tongue (known as the source language) and output a sentence in a desired target
    language.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人类常常通过语言彼此交流，相较于其他交流方式（例如，手势）。目前，全球有超过6,000种语言在使用。此外，要将一门语言学到能够被该语言的母语者轻松理解的水平，是一项难以掌握的任务。然而，交流对于分享知识、社交和扩大人际网络至关重要。因此，语言成为与世界其他地方的人进行交流的障碍。这就是**机器翻译**（**MT**）发挥作用的地方。MT系统允许用户输入他们自己的语言（称为源语言）的句子，并输出所需目标语言的句子。
- en: 'The problem with MT can be formulated as follows. Say we are given a sentence
    (or a sequence of words) *W*[s] belonging to a source language *S*, defined by
    the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MT的问题可以这样表述：假设我们给定一个句子（或一系列单词）*W*[s]，它属于源语言*S*，由以下公式定义：
- en: '![](img/B14070_09_001.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_001.png)'
- en: Here, ![](img/B14070_09_002.png).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_09_002.png)。
- en: 'The source language would be translated to a sentence ![](img/B14070_09_003.png),
    where *T* is the target language and is given by the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 源语言将被翻译成一个句子！[](img/B14070_09_003.png)，其中*T*是目标语言，并由以下公式给出：
- en: '![](img/B14070_09_004.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_004.png)'
- en: Here, ![](img/B14070_09_005.png).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_09_005.png)。
- en: '![](img/B14070_09_006.png) is obtained through the MT system, which outputs
    the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B14070_09_006.png) 通过机器翻译系统得到的输出如下：'
- en: '![](img/B14070_09_007.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_007.png)'
- en: 'Here, ![](img/B14070_09_008.png) is the pool of possible translation candidates
    found by the algorithm for the source sentence. Also, the best candidate from
    the pool of candidates is given by the following equation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_09_008.png) 是算法为源句子找到的可能翻译候选池。此外，从候选池中选出的最佳候选翻译由以下方程给出：
- en: '![](img/B14070_09_009.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_009.png)'
- en: Here, ![](img/B14070_09_073.png) is the model parameters. During training, we
    optimize the model to maximize the probability of some known target translations
    for a set of corresponding source translations (that is, training data).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_09_073.png) 是模型参数。在训练过程中，我们优化模型以最大化一组已知目标翻译的概率，这些目标翻译与对应的源语言翻译（即训练数据）相对应。
- en: So far, we have discussed the formal setup of the language translation problem
    that we’re interested in solving. Next, we will walk through the history of MT
    to get a feel of how people tried solving this in the early days.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了我们感兴趣的语言翻译问题的正式设置。接下来，我们将回顾机器翻译的历史，了解早期人们是如何尝试解决这一问题的。
- en: A brief historical tour of machine translation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译的简史
- en: Here, we will discuss the history of MT. The inception of MT involved rule-based
    systems. Then, more statistically sound MT systems emerged. **Statistical Machine
    Translation** (**SMT**) used various measures of statistics of a language to produce
    translations to another language. Then came the era of NMT. NMT currently holds
    state-of-the-art performance in most machine learning tasks compared with other
    methods.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论机器翻译的历史。机器翻译的起源涉及基于规则的系统。随后，出现了更多统计学上可靠的机器翻译系统。**统计机器翻译**（**SMT**）利用语言的各种统计量来生成目标语言的翻译。随后进入了神经机器翻译（NMT）时代。与其他方法相比，NMT在大多数机器学习任务中目前保持着最先进的性能。
- en: Rule-based translation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的翻译
- en: NMT came long after statistical machine learning, and statistical machine learning
    has been around for more than half a century now. The inception of SMT methods
    dates back to 1950-60, when during one of the first recorded projects, the *Georgetown-IBM
    experiment*, more than 60 Russian sentences were translated to English. o give
    some perspective, this attempt is almost as old as the invention of the transistor.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 神经机器翻译（NMT）是在统计机器学习之后很久才出现的，而统计机器学习已经存在超过半个世纪了。统计机器翻译方法的起源可以追溯到1950-60年，当时在第一次有记录的项目之一——*乔治敦-IBM实验*中，超过60个俄语句子被翻译成了英语。为了提供一些背景，这一尝试几乎和晶体管的发明一样久远。
- en: 'One of the initial techniques for MT was word-based machine translation. This
    system performed word-to-word translations using bilingual dictionaries. However,
    as you can imagine, this method has serious limitations. The obvious limitation
    is that word-to-word translation is not a one-to-one mapping between different
    languages. In addition, word-to-word translation may lead to incorrect results
    as it does not consider the context of a given word. The translation of a given
    word in the source language can change depending on the context in which it is
    used. To understand this with a concrete example, let’s look at the translation
    example from English to French in *Figure 9.1*. You can see that in the given
    two English sentences, a single word changes. However, this creates drastic changes
    in the translation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译的初期技术之一是基于词汇的机器翻译。该系统通过使用双语词典进行逐词翻译。然而，正如你所想的，这种方法有着严重的局限性。显而易见的局限性是，逐词翻译并不是不同语言之间的逐一映射。此外，逐词翻译可能导致不正确的结果，因为它没有考虑到给定单词的上下文。源语言中给定单词的翻译可以根据其使用的上下文而变化。为了通过一个具体的例子来理解这一点，我们来看一下*图9.1*中的英法翻译示例。你可以看到，在给定的两个英语句子中，一个单词发生了变化。然而，这种变化导致了翻译的显著不同：
- en: '![Rule-based translation](img/B14070_09_01.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![基于规则的翻译](img/B14070_09_01.png)'
- en: 'Figure 9.1: Translations (English to French) between languages are not one-to-one
    mappings between words'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：语言之间的翻译（英法）不是逐词映射
- en: 'In the 1960s, the **Automatic Language Processing Advisory Committee** (**ALPAC**)
    released a report, *Languages and machines: computers in translation and linguistics,*
    *National Academy of the Sciences (1966)*, on MT’s prospects. The conclusion was
    this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在1960年代，**自动语言处理咨询委员会**（**ALPAC**）发布了一份报告，*《语言与机器：计算机在翻译与语言学中的应用》*，*美国国家科学院（1966）*，讨论了机器翻译（MT）的前景。结论是：
- en: '*There is no immediate or predictable prospect of useful machine translation.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有直接或可预测的前景表明机器翻译会变得有用。*'
- en: This was because MT was slower, less accurate, and more expensive than human
    translation at the time. This delivered a huge blow to MT advancements, and almost
    a decade passed in silence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为机器翻译（MT）当时比人工翻译更慢、更不准确且更昂贵。这对机器翻译的进展造成了巨大打击，几乎有十年的时间处于沉寂状态。
- en: 'Next came corpora-based MT, where an algorithm was trained using tuples of
    source sentences, and the corresponding target sentence was obtained through a
    parallel corpus, that is, the parallel corpus would be of the format `[(<source_sentence_1>,
    <target_sentence_1>), (<source_sentence_2>, <target_sentence_2>), …]`. The parallel
    corpus is a large text corpus formed as tuples, consisting of text from the source
    language and the corresponding translation of that text. An illustration of this
    is shown in *Table 9.1*. It should be noted that building a parallel corpus is
    much easier than building bilingual dictionaries and they are more accurate because
    the training data is richer than word-to-word training data. Furthermore, instead
    of directly relying on manually created bilingual dictionaries, a bilingual dictionary
    (that is, the transition models) of two languages can be built using the parallel
    corpus. A transition model shows how likely a target word/phrase is to be the
    correct translation, given the current source word/phrase. In addition to learning
    the transition model, corpora-based MT also learns the word alignment models.
    A word alignment model can represent how words in a phrase from the source language
    correspond to the translation of that phrase. An example of parallel corpora and
    a word alignment model is depicted in *Figure 9.2*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是基于语料库的机器翻译（MT），其中一个算法通过使用源句子的元组进行训练，并通过平行语料库获得对应的目标句子，即平行语料库的格式为`[(<source_sentence_1>,
    <target_sentence_1>), (<source_sentence_2>, <target_sentence_2>), …]`。平行语料库是一个由源语言文本及其对应的翻译组成的元组形式的大型文本语料库。这个示例如*表
    9.1*所示。需要注意的是，构建平行语料库比构建双语词典更容易，而且它们更准确，因为训练数据比逐词训练数据更丰富。此外，基于平行语料库的机器翻译可以建立双语词典（即转移模型），而不直接依赖于人工创建的双语词典。转移模型展示了给定当前源词或短语时，目标词或短语是正确翻译的可能性。除了学习转移模型外，基于语料库的机器翻译还学习了词对齐模型。词对齐模型可以表示源语言中的短语的单词如何与该短语的翻译对应。平行语料库和词对齐模型的示例如*图
    9.2*所示：
- en: '![Rule-based translation](img/B14070_09_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![基于规则的翻译](img/B14070_09_02.png)'
- en: 'Figure 9.2: Word alignment between two different languages'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：两种不同语言之间的词对齐
- en: 'An illustration of an example parallel corpora is shown in *Table 9.1*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个平行语料库的示例如*表 9.1*所示：
- en: '| **Source language sentences (English)** | **Target language sentences (French)**
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **源语言句子（英语）** | **目标语言句子（法语）** |'
- en: '| --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| I went home | Je suis allé à la maison |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| I went home | Je suis allé à la maison |'
- en: '| John likes to play guitar | John aime jouer de la guitare |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| John likes to play guitar | John aime jouer de la guitare |'
- en: '| He is from England | Il est d’Angleterre |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| He is from England | Il est d’Angleterre |'
- en: '| … | …. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| … | …. |'
- en: 'Table 9.1: Parallel corpora for English and French sentences'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1：英语和法语句子的平行语料库
- en: Another approach was interlingual machine translation, which involved translating
    the source sentence into a language-neutral **interlingua** (that is, a metalanguage),
    and then generating the translated sentence out of the interlingua. More specifically,
    an interlingual machine translation system consists of two important components,
    an analyzer and a synthesizer. The analyzer will take the source sentence and
    identify agents (for example, nouns), actions (for example, verbs), and so on,
    and also how they interact with each other. Next, these identified elements are
    represented by means of an interlingual lexicon. An example of an interlingual
    lexicon can be made with the synsets (that is, the group of synonyms sharing a
    common meaning) available in WordNet. Then, from this interlingual representation,
    the synthesizer will create the translation. Since the synthesizer knows the nouns,
    verbs, and so on through the interlingual representation, it can generate the
    translation in the target language by incorporating language-specific grammar
    rules.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是跨语言机器翻译，它涉及将源语言句子翻译成一种语言中立的**中介语**（即元语言），然后从中介语生成翻译后的句子。更具体地说，跨语言机器翻译系统由两个重要组件组成，一个是分析器，另一个是合成器。分析器将获取源语言句子并识别出代理（例如名词）、动作（例如动词）等元素，以及它们之间的相互关系。接下来，这些识别出的元素通过跨语言词汇表进行表示。跨语言词汇表的一个例子可以通过WordNet中的同义词集（即共享共同意义的同义词组）来构建。然后，合成器将从这种跨语言表示中生成翻译。由于合成器通过跨语言表示了解名词、动词等，它可以通过结合特定语言的语法规则在目标语言中生成翻译。
- en: Statistical Machine Translation (SMT)
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计机器翻译（SMT）
- en: Next, more statistically sound systems started emerging. One of the pioneering
    models of this era was IBM Models 1-5, which did word-based translation. However,
    as we discussed earlier, word translations do not match one to one from the source
    language to a target language (for example, compound words and morphology). Eventually,
    researchers started experimenting with phrase-based translation systems, which
    made some notable advances in machine translation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，更多统计上更为合理的系统开始出现。这个时代的先锋模型之一是IBM模型1-5，它进行的是基于单词的翻译。然而，正如我们之前讨论的，单词翻译并不是一一对应的（例如复合词和形态学）。最终，研究人员开始尝试基于短语的翻译系统，这在机器翻译领域取得了一些显著的进展。
- en: Phrase-based translation works in a similar way to word-based translation, except
    that it uses phrases of a language as the atomic units of translation instead
    of individual words. This is a more sensible approach as it makes modeling the
    one-to-many, many-to-one, or many-to-many relationships between words easier.
    The main goal of phrase-based translation is to learn a phrase-translation model
    that contains a probability distribution of different candidate target phrases
    for a given source phrase. As you can imagine, this method involves maintaining
    huge databases of various phrases in two languages. A reordering step for phrases
    is also performed as there is no monotonic ordering of words between a sentence
    from one language and one in another.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于短语的翻译与基于单词的翻译类似，不同之处在于它使用语言的短语作为翻译的基本单位，而不是单个单词。这是一种更合理的方法，因为它使得建模单词之间的多对一、多对多或一对多关系变得更容易。基于短语的翻译的主要目标是学习一个短语翻译模型，其中包含不同候选目标短语对于给定源短语的概率分布。如你所想，这种方法需要维护两种语言之间大量短语的数据库。由于不同语言之间的句子没有单调的词序，因此还需要对短语进行重新排序。
- en: An example of this is shown in *Figure 9.2*; if the words were monotonically
    ordered between languages, there would not be crosses between word mappings.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点的例子如*图 9.2*所示；如果单词在语言之间是单调排序的，单词映射之间就不会有交叉。
- en: One of the limitations of this approach is that the decoding process (finding
    the best target phrase for a given source phrase) is expensive. This is due to
    the size of the phrase database, as well as the fact that a source phrase often
    contains multiple target language phrases. To alleviate the burden, syntax-based
    translations arose.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个局限性是解码过程（为给定源短语找到最佳目标短语）代价高昂。这是因为短语数据库的庞大，以及一个源短语通常包含多个目标语言短语。为了减轻这一负担，基于语法的翻译应运而生。
- en: 'In syntax-based translation, the source sentence is represented by a syntax
    tree. In *Figure 9.3*, **NP** represents a noun phrase, **VP** a verb phrase,
    and **S** a sentence. Then a **reordering phase** takes place, where the tree
    nodes are reordered to change the order of subject, verb, and object, depending
    on the target language. This is because the sentence structure can change depending
    on the language (for example, in English it is *subject-verb-object*, whereas
    in Japanese it is *subject-object-verb*). The reordering is decided according
    to something known as the **r-table**. The r-table contains the likelihood probabilities
    for the tree nodes to be changed to some other order:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于语法的翻译中，源句子通过语法树来表示。在*图9.3*中，**NP**表示名词短语，**VP**表示动词短语，**S**表示句子。然后进入**重排序阶段**，在这个阶段，树节点会根据目标语言的需要重新排序，以改变主语、动词和宾语的顺序。这是因为句子结构会根据语言的不同而变化（例如，英语是*主语-动词-宾语*，而日语是*主语-宾语-动词*）。重排序是根据一种叫做**r表**的东西来决定的。r表包含了树节点按照某种顺序重排的可能性概率：
- en: '![Statistical Machine Translation (SMT)](img/B14070_09_03.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![统计机器翻译（SMT）](img/B14070_09_03.png)'
- en: 'Figure 9.3: Syntax tree for a sentence'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：一个句子的语法树
- en: An **insertion phase** then takes place. In the insertion phase, we stochastically
    insert a word into each node of the tree. This is due to the assumption that there
    is an invisible `NULL` word, and it generates target words at random positions
    of the tree. Also, the probability of inserting a word is determined by something
    called the **n-table**, which is a table that contains probabilities of inserting
    a particular word into the tree.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后进入**插入阶段**。在插入阶段，我们随机地将一个词插入到树的每个节点中。这是由于假设存在一个不可见的`NULL`词，它会在树的随机位置生成目标词汇。此外，插入一个词的概率由一种叫做**n表**的东西决定，它是一个包含将特定词插入到树中的概率的表格。
- en: Next, the **translation phase** occurs, where each leaf node is translated to
    the target word in a word-by-word manner. Finally, the translated sentence is
    read off the syntax tree, to construct the target sentence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，进入**翻译阶段**，在这个阶段，每个叶子节点都按逐词的方式被翻译成目标词。最后，通过读取语法树中的翻译句子，构建目标句子。
- en: Neural Machine Translation (NMT)
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经机器翻译（NMT）
- en: Finally, around the year 2014, NMT systems were introduced. NMT is an end-to-end
    system that takes a full sentence as an input, performs certain transformations,
    and then outputs the translated sentence for the corresponding source sentence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在2014年左右，NMT系统被引入。NMT是一种端到端的系统，它将整个句子作为输入，进行某些转换，然后输出与源句子对应的翻译句子。
- en: 'Therefore, NMT eliminates the need for the feature engineering required for
    machine translation, such as building phrase translation models and building syntax
    trees, which is a big win for the NLP community. Also, NMT has outperformed all
    the other popular MT techniques in a very short period, just two to three years.
    In *Figure 9.4*, we depict the results of various MT systems reported in the MT
    literature. For example, 2016 results are obtained from Sennrich and others in
    their paper *Edinburgh Neural Machine Translation Systems for WMT 16, Association
    for Computational Linguistics, Proceedings of the First Conference on Machine
    Translation, August 2016: 371-376*, and from Williams and others in their paper
    *Edinburgh’s Statistical Machine Translation Systems for WMT16, Association for
    Computational Linguistics, Proceedings of the First Conference on Machine Translation,
    August 2016: 399-410*. All the MT systems are evaluated with the BLEU score. The
    BLEU score denotes the number of n-grams (for example, unigrams and bigrams) of
    candidate translation that matched in the reference translation. So the higher
    the BLEU score, the better the MT system. We’ll discuss the BLEU metric in detail
    later in the chapter. There is no need to highlight that NMT is a clear-cut winner:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，NMT消除了机器翻译所需的特征工程，例如构建短语翻译模型和构建语法树，这对NLP社区来说是一个巨大的胜利。此外，NMT在非常短的时间内（仅两到三年）超越了所有其他流行的MT技术。在*图9.4*中，我们展示了MT文献中报告的各种MT系统的结果。例如，2016年的结果来自Sennrich等人在他们的论文*《爱丁堡神经机器翻译系统（WMT
    16），计算语言学协会，第一个机器翻译会议论文集，2016年8月：371-376》*中的报告，也来自Williams等人在他们的论文*《爱丁堡统计机器翻译系统（WMT16），计算语言学协会，第一个机器翻译会议论文集，2016年8月：399-410》*中的报告。所有MT系统都通过BLEU分数进行了评估。BLEU分数表示候选翻译与参考翻译匹配的n-grams数量（例如，单字和双字组合）。因此，BLEU分数越高，MT系统越好。我们将在本章后面详细讨论BLEU指标。不言而喻，NMT无疑是赢家：
- en: '![Neural Machine Translation (NMT)](img/B14070_09_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![神经机器翻译（NMT）](img/B14070_09_04.png)'
- en: 'Figure 9.4: Comparison of statistical machine translation system to NMT systems.
    Courtesy of Rico Sennrich'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：统计机器翻译系统与NMT系统的比较。感谢Rico Sennrich提供。
- en: A case study assessing the potential of NMT systems is available in *Is Neural
    Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions,
    Junczys-Dowmunt, Hoang and Dwojak, Proceedings of the Ninth International Workshop
    on Spoken Language Translation*, *Seattle (2016)*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个评估NMT系统潜力的案例研究可以在*《神经机器翻译准备好部署了吗？30种翻译方向的案例研究》*中找到，作者为Junczys-Dowmunt、Hoang和Dwojak，发表于*第九届国际口语语言翻译研讨会*，*西雅图（2016）*。
- en: The study looks at the performance of different systems on several translation
    tasks between various languages (English, Arabic, French, Russian, and Chinese).
    The results also support that NMT systems (NMT 1.2M and NMT 2.4M) perform better
    than SMT systems (PB-SMT and Hiero).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究探讨了不同系统在多种语言之间（英语、阿拉伯语、法语、俄语和中文）的翻译任务中的表现。结果还表明，NMT系统（NMT 1.2M和NMT 2.4M）的表现优于SMT系统（PB-SMT和Hiero）。
- en: '*Figure 9.5* shows several statistics for a set from a 2017 state-of-the-art
    machine translator. This is from a presentation, *State of the Machine Translation,
    Intento, Inc, 2017*, produced by Konstantin Savenkov, cofounder and CEO of Intento.
    We can see that the performance of the MT produced by DeepL ([https://www.deepl.com](https://www.deepl.com))
    appears to be competing closely with other MT giants, including Google. The comparison
    includes MT systems such as DeepL (NMT), Google (NMT), Yandex (NMT-SMT hybrid),
    Microsoft (has both SMT and NMT), IBM (SMT), Prompt (rule-based), and SYSTRAN
    (rule-based/SMT hybrid). The graph clearly shows that NMT systems are leading
    the current MT advancements. The LEPOR score is used to assess different systems.
    LEPOR is a more advanced metric than BLEU, and it attempts to solve the **language
    bias problem**. The language bias problem refers to the phenomenon that some evaluation
    metrics (such as BLEU) perform well for certain languages, but perform poorly
    for others.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.5*显示了2017年最先进的机器翻译系统的一些统计数据。这些数据来自Konstantin Savenkov（Intento公司联合创始人兼CEO）制作的演示文稿《机器翻译现状，Intento公司，2017》。我们可以看到，DeepL（[https://www.deepl.com](https://www.deepl.com)）所生成的机器翻译性能与其他大型机器翻译系统，包括Google，表现得非常接近。比较包括了DeepL（NMT）、Google（NMT）、Yandex（NMT-SMT混合）、Microsoft（同时拥有SMT和NMT）、IBM（SMT）、Prompt（基于规则）和SYSTRAN（基于规则/SMT混合）等机器翻译系统。图表清晰地显示了NMT系统目前在机器翻译技术进展中处于领先地位。LEPOR得分用于评估不同的系统。LEPOR是一种比BLEU更先进的评估指标，它尝试解决**语言偏差问题**。语言偏差问题指的是一些评估指标（如BLEU）在某些语言上表现良好，但在其他语言上表现较差。'
- en: 'However, it should also be noted that the results do contain some bias due
    to the averaging mechanism used in this comparison. For example, Google Translate
    has been averaged over a larger set of languages (including difficult translation
    tasks), whereas DeepL has been averaged over a smaller and relatively easier subset
    of languages. Therefore, we should not conclude that the DeepL MT system is always
    better than the Google MT system. Nevertheless, the overall results provide a
    general comparison of the performance of the current NMT and SMT systems:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也应注意，由于在这次比较中使用了平均机制，结果确实存在一定的偏差。例如，Google翻译是基于一个更大范围的语言集合（包括较难的翻译任务）进行平均的，而DeepL则是基于一个较小且相对容易的语言子集进行平均的。因此，我们不应得出结论认为DeepL的机器翻译系统总是优于Google的机器翻译系统。尽管如此，整体结果仍然为当前的NMT和SMT系统提供了一个大致的性能对比：
- en: '![Neural Machine Translation (NMT)](img/B14070_09_05.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![神经机器翻译（NMT）](img/B14070_09_05.png)'
- en: 'Figure 9.5: Performance of various MT systems. Courtesy of Intento, Inc.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：各种机器翻译系统的表现。感谢Intento公司提供
- en: We saw that NMT has already outperformed SMT systems in very few years, and
    is the current state of the art. We will now move on to discussing the details
    and architecture of an NMT system. Finally, we will be implementing an NMT system
    from scratch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到NMT在短短几年内已经超过了SMT系统，成为当前的最先进技术。接下来，我们将讨论NMT系统的细节和架构。最后，我们将从头开始实现一个NMT系统。
- en: Understanding neural machine translation
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经机器翻译
- en: Now that we have an appreciation for how machine translation has evolved over
    time, let’s try to understand how state-of-the-art NMT works. First, we will take
    a look at the model architecture used by neural machine translators and then move
    on to understanding the actual training algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了机器翻译如何随着时间的推移而发展，让我们尝试理解最先进的NMT是如何工作的。首先，我们将看看神经机器翻译模型的架构，然后再深入了解实际的训练算法。
- en: Intuition behind NMT systems
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NMT系统背后的直觉
- en: 'First, let’s understand the intuition underlying an NMT system’s design. Say
    you are a fluent English and German speaker and were asked to translate the following
    sentence into German:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解NMT系统设计背后的直觉。假设你是一个流利的英语和德语使用者，并且被要求将以下句子翻译成德语：
- en: '*I went home*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*我回家了*'
- en: 'This sentence translates to the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该句的翻译如下：
- en: '*Ich ging nach Hause*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ich ging nach Hause*'
- en: 'Although it might not have taken more than a few seconds for a fluent person
    to translate this, there is a certain process that produces the translation. First,
    you read the English sentence, and then you create a thought or concept about
    what this sentence represents or implies, in your mind. And finally, you translate
    the sentence into German. The same idea is used for building NMT systems (see
    *Figure 9.6*). The encoder reads the source sentence (that is, similar to you
    reading the English sentence). Then the encoder outputs a context vector (the
    context vector corresponds to the thought/concept you imagined after reading the
    sentence). Finally, the decoder takes in the context vectors and outputs the translation
    in German:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对流利的人来说，翻译这个句子可能只需要几秒钟，但翻译是有一定过程的。首先，你阅读英文句子，然后在脑海中形成一个关于这个句子的思想或概念。最后，你将句子翻译成德语。构建NMT系统时使用了相同的思路（见*图9.6*）。编码器读取源句子（类似于你阅读英文句子的过程）。然后，编码器输出一个上下文向量（该上下文向量对应你在阅读句子后想象的思想/概念）。最后，解码器接收上下文向量并输出德语翻译：
- en: '![Intuition behind NMT](img/B14070_09_06.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![NMT直觉](img/B14070_09_06.png)'
- en: 'Figure 9.6: Conceptual architecture of an NMT system'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：NMT系统的概念架构
- en: NMT architecture
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NMT架构
- en: 'Now we will look at the architecture in more detail. The sequence-to-sequence
    approach was originally proposed by Sutskever, Vinyals, and Le in their paper
    *Sequence to Sequence Learning with Neural Networks, Proceedings of the 27th International
    Conference on Neural Information Processing Systems - Volume 2: 3104-3112.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们将更详细地看一下架构。序列到序列的方法最初是由Sutskever、Vinyals和Le在他们的论文《Sequence to Sequence
    Learning with Neural Networks, Proceedings of the 27th International Conference
    on Neural Information Processing Systems - Volume 2: 3104-3112.》中提出的。'
- en: 'From the diagram in *Figure 9.6*, we can see that there are two major components
    in the NMT architecture. These are called the encoder and decoder. In other words,
    NMT can be seen as an encoder-decoder architecture. The **encoder** converts a
    sentence from a given source language into a thought vector (i.e. a contextualized
    representation), and the **decoder** decodes or translates the thought into a
    target language. As you can see, this shares some features with the interlingual
    machine translation method we briefly talked about. This explanation is illustrated
    in *Figure 9.7*. The left-hand side of the context vector denotes the encoder
    (which takes a source sentence in word by word to train a time-series model).
    The right-hand side denotes the decoder, which outputs word by word (while using
    the previous word as the current input) the corresponding translation of the source
    sentence. We will also use embedding layers (for both the source and target languages)
    where the semantics of the individual tokens will be learned and fed as inputs
    to the models:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图9.6*的示意图中，我们可以看到NMT架构中有两个主要组件。它们被称为编码器和解码器。换句话说，NMT可以看作是一个编码器-解码器架构。**编码器**将源语言的句子转换为思想向量（即上下文化的表示），而**解码器**将思想向量解码或翻译为目标语言。正如你所看到的，这与我们简要讨论过的中介语言机器翻译方法有些相似。这个解释在*图9.7*中得到了说明。上下文向量的左侧表示编码器（它逐字读取源句子以训练时间序列模型）。右侧表示解码器，它逐字输出（同时使用前一个词作为当前输入）源句子的相应翻译。我们还将使用嵌入层（对于源语言和目标语言），在这些层中，单个词元的语义将被学习并作为输入传递给模型：
- en: '![NMT architecture](img/B14070_09_07.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![NMT架构](img/B14070_09_07.png)'
- en: 'Figure 9.7: Unrolling the source and target sentences over time'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：源句子和目标句子随时间展开
- en: 'With a basic understanding of what NMT looks like, let’s formally define the
    objective of the NMT. The ultimate objective of an NMT system is to maximize the
    log likelihood, given a source sentence *x*[s] and its corresponding *y*[t]. That
    is, to maximize the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在对NMT的基本理解之后，我们来正式定义NMT的目标。NMT系统的最终目标是最大化对给定源句子 *x*[s] 及其对应的 *y*[t] 的对数似然。即，最大化以下内容：
- en: '![](img/B14070_09_010.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_010.png)'
- en: Here, *N* refers to the number of source and target sentence inputs we have
    as training data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N* 指的是我们作为训练数据拥有的源句子和目标句子输入的数量。
- en: 'Then, during inference, for a given source sentence, ![](img/B14070_09_011.png),
    we will find the ![](img/B14070_09_012.png) translation using the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在推理过程中，对于给定的源句子， ![](img/B14070_09_011.png)，我们将使用以下方法找到 ![](img/B14070_09_012.png)
    翻译：
- en: '![](img/B14070_09_013.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_013.png)'
- en: Here, ![](img/B14070_09_014.png) is the predicted token at the *i*^(th) time
    step and ![](img/B14070_09_015.png) is the set of possible candidate sentences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_09_014.png) 是 *i*^(th) 时刻的预测标记，![](img/B14070_09_015.png)
    是可能的候选句子集合。
- en: Before we examine each part of the NMT architecture, let’s define the mathematical
    notation to understand the system more concretely. As our sequential model, we
    will choose a **Gated Recurrent Unit** (**GRU**), as it is simpler than an LSTM
    and performs comparatively well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究 NMT 架构的每个部分之前，让我们先定义一些数学符号，以便更具体地理解这个系统。作为我们的序列模型，我们将选择 **门控循环单元** (**GRU**)，因为它比
    LSTM 更简单，且表现相对较好。
- en: 'Let’s define the encoder GRU as ![](img/B14070_09_016.png) and the decoder
    GRU as ![](img/B14070_09_017.png). At the time step ![](img/B14070_09_018.png),
    let’s define the output state of a general GRU as *h*[t]. That is, feeding the
    input *x*[t] into the GRU produces *h*[t]:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义编码器 GRU 为 ![](img/B14070_09_016.png)，解码器 GRU 为 ![](img/B14070_09_017.png)。在时间步长
    ![](img/B14070_09_018.png) 处，定义一般 GRU 的输出状态为 *h*[t]。也就是说，将输入 *x*[t] 输入到 GRU 中会得到
    *h*[t]：
- en: '![](img/B14070_09_020.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_020.png)'
- en: Now, we will talk about the embedding layer, the encoder, the context vector,
    and finally, the decoder.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论嵌入层、编码器、上下文向量，最后是解码器。
- en: The embedding layer
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入层
- en: We have already seen the power of word embeddings. Here, we can also leverage
    embeddings to improve model performance. We will be using two-word embedding layers,
    ![](img/B14070_09_021.png), for the source language and ![](img/B14070_09_022.png)
    for the target language. So, instead of feeding *x*[t] directly into the GRU,
    we will be getting ![](img/B14070_09_023.png). However, to avoid excessive notation,
    we will assume ![](img/B14070_09_024.png).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到词嵌入的强大功能。在这里，我们也可以利用嵌入来提高模型性能。我们将使用两个词嵌入层，![](img/B14070_09_021.png) 用于源语言，![](img/B14070_09_022.png)
    用于目标语言。所以，我们将不直接将 *x*[t] 输入到 GRU 中，而是得到 ![](img/B14070_09_023.png)。然而，为了避免过多的符号表示，我们假设
    ![](img/B14070_09_024.png)。
- en: The encoder
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: 'As mentioned earlier, the encoder is responsible for generating a thought vector
    or a context vector that represents what is meant by the source language. For
    this, we will use a GRU-based network (see *Figure 9.8*):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，编码器负责生成一个思维向量或上下文向量，表示源语言的含义。为此，我们将使用基于 GRU 的网络（见 *图 9.8*）：
- en: '![](img/B14070_09_08.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_08.png)'
- en: 'Figure 9.8: A GRU cell'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：一个 GRU 单元
- en: 'The encoder is initialized with *h* at time step 0 (*h*[0]) with a zero vector
    by default. The encoder takes a sequence of words, ![](img/B14070_09_025.png),
    as the input and calculates a context vector, ![](img/B14070_09_026.png), where
    *v* is the final external hidden state obtained after processing the final element
    ![](img/B14070_09_027.png), of the sequence *x*[s]. We represent this as the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器在时间步长 0 (*h*[0]) 处用零向量初始化。编码器接受一个词序列，![](img/B14070_09_025.png)，作为输入，并计算一个上下文向量，![](img/B14070_09_026.png)，其中
    *v* 是处理序列 *x*[s] 的最后一个元素 ![](img/B14070_09_027.png) 后得到的最终外部隐藏状态。我们表示为以下内容：
- en: '![](img/B14070_09_028.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_028.png)'
- en: '![](img/B14070_09_029.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_029.png)'
- en: The context vector
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文向量
- en: The idea of the context vector (*v*) is to represent a sentence of a source
    language concisely. Also, in contrast to how the encoder’s state is initialized
    (that is, it is initialized with zeros), the context vector becomes the initial
    state for the decoder GRU. In other words, the decoder GRU doesn’t start with
    an initial state of zeros, but with the context vector as its initial state. This
    creates a linkage between the encoder and the decoder and makes the whole model
    end-to-end differentiable. We will talk about this in more detail next.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文向量 (*v*) 的概念是简洁地表示源语言的句子。此外，与编码器的状态初始化方式（即初始化为零）相对，上下文向量成为解码器 GRU 的初始状态。换句话说，解码器
    GRU 并非以零向量作为初始状态，而是以上下文向量作为初始状态。这在编码器和解码器之间创建了联系，使整个模型成为端到端可微分的。我们将在接下来详细讨论这一点。
- en: The decoder
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder is responsible for decoding the context vector into the desired
    translation. Our decoder is an RNN as well. Though it is possible for the encoder
    and decoder to share the same set of weights, it is usually better to use two
    different networks for the encoder and the decoder. This increases the number
    of parameters in our model, allowing us to learn the translations more effectively.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器负责将上下文向量解码为所需的翻译。我们的解码器也是一个RNN。虽然编码器和解码器可以共享相同的权重集，但通常使用两个不同的网络分别作为编码器和解码器会更好。这增加了我们模型中的参数数量，使我们能够更有效地学习翻译。
- en: 'First, the decoder’s states are initialized with the context vector, i.e. ![](img/B14070_09_030.png),
    as shown here: ![](img/B14070_09_031.png).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，解码器的状态通过上下文向量进行初始化，即![](img/B14070_09_030.png)，如图所示：![](img/B14070_09_031.png)。
- en: Here, ![](img/B14070_09_032.png) is the initial state vector of the decoder
    (![](img/B14070_09_033.png)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_09_032.png)是解码器的初始状态向量（![](img/B14070_09_033.png)）。
- en: This (*v*) is the crucial link that connects the encoder with the decoder to
    form an end-to-end computational chain (see in *Figure 9.6* that the only thing
    shared by the encoder and decoder is *v*). Also, this is the only piece of information
    that is available to the decoder about the source sentence.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个(*v*)是连接编码器和解码器，形成端到端计算链的关键链接（见*图9.6*，编码器和解码器共享的唯一内容是*v*）。此外，这是解码器获取源句子的唯一信息。
- en: 'Then we will compute the *m*^(th) prediction of the translated sentence with
    the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过以下公式计算翻译句子的*m*^(th)个预测结果：
- en: '![](img/B14070_09_034.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_034.png)'
- en: '![](img/B14070_09_035.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_035.png)'
- en: 'The full NMT system with the details of how the GRU cell in the encoder connects
    to the GRU cell in the decoder, and how the softmax layer is used to output predictions,
    is shown in *Figure 9.9*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 带有GRU单元在编码器和解码器之间连接细节，并且使用softmax层输出预测结果的完整NMT系统，如*图9.9*所示：
- en: '![](img/B14070_09_09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_09.png)'
- en: 'Figure 9.9: The encoder-decoder architecture with the GRUs. Both the encoder
    and the decoder have a separate GRU component. Additionally, the decoder has a
    fully-connected (dense) layer and a softmax layer that produce the final predictions.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：带有GRU的编码器-解码器架构。编码器和解码器都有一个独立的GRU组件。此外，解码器还具有一个全连接（密集）层和一个softmax层，用于生成最终的预测结果。
- en: In the next section, we will go through the steps required to prepare data for
    our model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍为模型准备数据所需的步骤。
- en: Preparing data for the NMT system
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为NMT系统准备数据
- en: In this section, we will understand the data and learn about the process for
    preparing data for training and predicting from the NMT system. First, we will
    talk about how to prepare training data (that is, the source sentence and target
    sentence pairs) to train the NMT system, followed by inputting a given source
    sentence to produce the translation of the source sentence.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解数据，并学习如何准备数据以进行NMT系统的训练和预测。首先，我们将讨论如何准备训练数据（即源句子和目标句子对），以训练NMT系统，然后输入给定的源句子以生成该源句子的翻译。
- en: The dataset
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'The dataset we’ll be using for this chapter is the WMT-14 English-German translation
    data from [https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/).
    There are ~4.5 million sentence pairs available. However, we will use only 250,000
    sentence pairs due to computational feasibility. The vocabulary consists of the
    50,000 most common English words and the 50,000 most common German words, and
    words not found in the vocabulary will be replaced with a special token, `<unk>`.
    You will need to download the following files:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将使用的数据集是来自[https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/)的WMT-14英德翻译数据。大约有450万个句子对可用。然而，由于计算可行性，我们只会使用25万个句子对。词汇表由最常见的50,000个英语单词和最常见的50,000个德语单词组成，词汇表中未找到的单词将被特殊标记`<unk>`替代。你需要下载以下文件：
- en: '`train.de` – File containing German sentences'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.de` – 包含德语句子的文件'
- en: '`train.en` – File containing English sentences'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.en` – 包含英语句子的文件'
- en: '`vocab.50K.de` – File containing German vocabulary'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab.50K.de` – 包含德语词汇的文件'
- en: '`vocab.50K.en` – File containing English vocabulary'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab.50K.en` – 包含英语词汇的文件'
- en: '`train.de` and `train.en` contain parallel sentences in German and English,
    respectively. Once downloaded we will load the sentences as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.de`和`train.en`分别包含德语和英语的平行句子。一旦下载，我们将按照以下方式加载这些句子：'
- en: '[PRE0]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you print the data you just loaded, for the two languages, you would have
    sentences like the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印刚刚加载的数据，对于这两种语言，你会看到如下的句子：
- en: '[PRE1]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Adding special tokens
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加特殊标记
- en: 'The next step is to add a few special tokens to the start and end of our sentences.
    We will add `<s>` to mark the start of a sentence and `</s>` to mark the end of
    a sentence. We can easily achieve this using the following list comprehension:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是向我们的句子开始和结束添加一些特殊标记。我们将添加`<s>`来标记句子的开始，添加`</s>`来标记句子的结束。我们可以通过以下列表推导轻松实现这一点：
- en: '[PRE2]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give us:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们带来：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is a very important step for Seq2Seq models. `<s>` and `</s>` tokens serve
    an extremely important role during model inference. As you will see, at inference
    time, we will be using the decoder to predict one word at a time, by using the
    output of the previous time step as an input. This way we can predict for an arbitrary
    number of time steps. Using `<s>` as the starting token gives us a way to signal
    to the decoder that it should start predicting tokens from the target language.
    Next, if we do not use the `</s>` token to mark the end of a sentence, we cannot
    signal the decoder to end a sentence. This can lead the model to enter an infinite
    loop of predictions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Seq2Seq模型中非常重要的一步。`<s>`和`</s>`标记在模型推理过程中起着极其重要的作用。正如你将看到的，在推理时，我们将使用解码器逐步预测一个单词，通过使用上一步的输出作为输入。这样，我们就可以预测任意数量的时间步。使用`<s>`作为起始标记使我们能够向解码器发出信号，指示它应开始预测目标语言的标记。接下来，如果我们不使用`</s>`标记来标记句子的结束，我们就无法向解码器发出结束句子的信号。这可能会导致模型进入无限预测循环。
- en: Splitting training, validation, and testing datasets
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 划分训练、验证和测试数据集
- en: 'We need to split our dataset into three parts: a training set, a validation
    set, and a testing set. Specifically, let’s use 80% of sentences to train the
    model, 10% as validation data, and the remaining 10% as testing data:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据集拆分成三个部分：训练集、验证集和测试集。具体来说，我们将使用80%的句子来训练模型，10%作为验证数据，剩下的10%作为测试数据：
- en: '[PRE4]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining sequence lengths for the two languages
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为两种语言定义序列长度
- en: 'A key statistic we have to understand at this point is how long, generally,
    the sentences in our corpus are. It is quite likely that the two languages will
    have different sentence lengths. To learn the statistics of this, we’ll be using
    the pandas library in the following way:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须理解的一个关键统计数据是，我们的语料库中的句子通常有多长。两种语言的句子长度很可能会有所不同。为了学习这个统计数据，我们将使用pandas库，具体方法如下：
- en: '[PRE5]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we are first converting the `train_en_sentences` to a `pd.Series` object.
    `pd.Series` is an indexed series of values (an array). Here, each value is a list
    of tokens belonging to each sentence. Calling `.str.len()` will give us the length
    of each list of tokens. Finally, the `describe` method will give important statistics
    such as mean, standard deviation, and percentiles. Here. we are specifically asking
    for 5%, 50%, and 95% percentiles.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先将`train_en_sentences`转换为一个`pd.Series`对象。`pd.Series`是一个带索引的值序列（数组）。在这里，每个值是属于每个句子的标记列表。调用`.str.len()`将给我们每个标记列表的长度。最后，`describe`方法将提供重要的统计数据，如均值、标准差和百分位数。在这里，我们特别请求5%、50%和95%的百分位数。
- en: Note that we are only using the training data for this calculation. If you include
    validation or test datasets in these calculations, we may be leaking data about
    validation and test data. Therefore, it’s best to only use the training dataset
    for these calculations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们仅使用训练数据进行此计算。如果将验证或测试数据集包括在计算中，我们可能会泄露有关验证和测试数据的信息。因此，最好仅使用训练数据集进行这些计算。
- en: 'The result from the previous code gives us:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码结果给我们带来了：
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can get the same for the German sentences the following way:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式获得德语句子的相同信息：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This gives us:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here we can see that 95% of English sentences have 53 tokens, where 95% of German
    sentences have 47 tokens.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到，95%的英语句子有53个标记，而95%的德语句子有47个标记。
- en: Padding the sentences
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充句子
- en: 'Next, we need to pad our sentences. For this, we will use the `pad_sequences()`
    function provided in Keras. This function takes in values for the following arguments:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要填充我们的句子。为此，我们将使用Keras提供的`pad_sequences()`函数。该函数接受以下参数的值：
- en: '`sequences` – A list of strings/IDs representing the text corpus. Each document
    can either be a list of strings or a list of integers'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` – 一个字符串/ID的列表，表示文本语料库。每个文档可以是一个字符串列表或一个整数列表。'
- en: '`maxlen` – The maximum length to pad for (defaults to `None`)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxlen` – 要填充的最大长度（默认为`None`）'
- en: '`dtype` – The type of data (defaults to `''int32''`)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` – 数据类型（默认为`''int32''`）'
- en: '`padding` – The side to pad short sequences (defaults to `''pre''`)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` – 填充短序列的方向（默认为`''pre''`）'
- en: '`truncating` – The side to truncate long sequences from (defaults to `''pre''`)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncating` – 截断长序列的方向（默认为`''pre''`）'
- en: '`value` – The values to pad with (defaults to `0.0`)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value` – 用于填充的值（默认为`0.0`）'
- en: 'We will use this function as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按如下方式使用这个函数：
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are padding all of the training, validation, and test sentences in both English
    and German. We will use the recently found sequence lengths as the padding/truncating
    length.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在对所有的训练、验证和测试句子进行填充处理，无论是英文还是德文。我们将使用最近找到的序列长度作为填充/截断长度。
- en: '**Reversing the source sentence**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**反转源语言句子**'
- en: We can also perform a special trick on the source sentences. Say we have the
    sentence *ABC* in the source language, which we want to translate to ![](img/B14070_09_036.png)
    in the target language. We will first reverse the source sentences so that the
    sentence *ABC* is read as *CBA*. This means that in order to translate *ABC* to
    ![](img/B14070_09_036.png), we need to feed in *CBA*. This improves the performance
    of our model significantly, especially when the source and target languages share
    the same sentence structure (for example, *subject-verb-object*).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对源语言句子执行一个特殊的技巧。假设我们有一个句子*ABC*，我们想将其翻译成目标语言中的![](img/B14070_09_036.png)。我们将首先反转源语言句子，使得句子*ABC*被读取为*CBA*。这意味着，为了将*ABC*翻译为![](img/B14070_09_036.png)，我们需要输入*CBA*。这种方法显著提高了模型的性能，尤其是当源语言和目标语言共享相同句子结构时（例如，*主语-动词-宾语*）。
- en: 'Let’s try to understand why this helps. Mainly, it helps to build good *communication*
    between the encoder and the decoder. Let’s start from the previous example. We
    will concatenate the source and target sentences:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着理解为什么这有帮助。主要是，它有助于在编码器和解码器之间建立良好的*沟通*。让我们从前面的例子开始。我们将把源语言句子和目标语言句子连接起来：
- en: '![](img/B14070_09_038.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_038.png)'
- en: 'If you calculate the distance (that is, the number of words separating two
    words) from *A* to ![](img/B14070_09_039.png) or *B* to ![](img/B14070_09_040.png),
    they will be the same. However, consider this when you reverse the source sentence,
    as shown here:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算从*A*到![](img/B14070_09_039.png)或从*B*到![](img/B14070_09_040.png)的距离（即，两个词之间的单词数），它们将是相同的。然而，考虑到反转源句子时的情况，如此处所示：
- en: '![](img/B14070_09_041.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_041.png)'
- en: Here, *A* is very close to ![](img/B14070_09_039.png) and so on. Also, to build
    good translations, building good communications at the very start is important.
    This simple trick can possibly help NMT systems to improve their performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*A*与![](img/B14070_09_039.png)非常接近，以此类推。另外，为了构建好的翻译，开始时建立良好的交流非常重要。这个简单的技巧可能有助于NMT系统提升其性能。
- en: Note that the source sentence reversing step is a subjective preprocessing step.
    This might not be necessary for some translational tasks. For example, if your
    translation task is to translate from Japanese (which is often written in *subject-object-verb*
    format) to Filipino (often written *verb-subject-object*), then reversing the
    source sentence might actually cause harm rather than helping. This is because
    by reversing the text in Japanese, you are increasing the distance between the
    starting element of the target sentence (that is, the verb (Japanese)) and the
    corresponding source language entity (that is, the verb (Filipino)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，反转源句子的步骤是一个主观的预处理步骤。对于某些翻译任务，这可能并不是必要的。例如，如果你的翻译任务是从日语（通常是*主语-宾语-动词*格式）翻译到菲律宾语（通常是*动词-主语-宾语*格式），那么反转源句子可能会适得其反，反而带来不利影响。这是因为通过反转日语文本，你会增加目标句子中起始元素（即动词（在日语中））与对应的源语言实体（即动词（在菲律宾语中））之间的距离。
- en: Next let’s define our encoder-decoder model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来定义我们的编码器-解码器模型。
- en: Defining the model
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义模型
- en: In this section, we will define the model from end to end.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将定义一个端到端的模型。
- en: We are going to implement an encoder-decoder based NMT model equipped with additional
    techniques to boost performance. Let’s start off by converting our string tokens
    to IDs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个基于编码器-解码器的NMT模型，并配备附加技术来提升性能。让我们从将字符串标记转换为ID开始。
- en: Converting tokens to IDs
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将标记转换为ID
- en: 'Before we jump to the model, we have one more text processing operation remaining,
    that is, converting the processed text tokens into numerical IDs. We are going
    to use a `tf.keras.layers.Layer` to do this. Particularly, we’ll be using the
    `StringLookup` layer to create a layer in our model that converts each token into
    a numerical ID. As the first step, let us load the vocabulary files provided in
    the data. Before doing so, we will define the variable `n_vocab` to denote the
    size of the vocabulary for each language:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入模型之前，还有一个文本处理操作剩下，那就是将处理过的文本标记转换为数字 ID。我们将使用`tf.keras.layers.Layer`来实现这一点。具体来说，我们将使用`StringLookup`层在模型中创建一个层，将每个标记转换为数字
    ID。第一步，让我们加载数据中提供的词汇表文件。在此之前，我们将定义变量`n_vocab`来表示每种语言词汇表的大小：
- en: '[PRE10]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Originally, each vocabulary contains 50,000 tokens. However, we’ll take only
    half of this to reduce the memory requirement. Note that we allow one extra token
    as there’s a special token `<unk>` to denote **out-of-vocabulary** (**OOV**) words.
    With a 50,000-token vocabulary, it is quite easy to run out of memory due to the
    size of the final prediction layer we’ll build. While cutting back on the size
    of the vocabulary, we have to make sure that we preserve the most common 25,000
    words. Fortunately, each vocabulary file is organized such that words are ordered
    by their frequency of occurrence (high to low). Therefore, we just need to read
    the first 25,001 lines of text from the file:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，每个词汇表包含50,000个标记。然而，我们将只取其中的一半，以减少内存需求。请注意，我们允许额外的一个标记，因为有一个特殊的标记`<unk>`表示**超出词汇表**（**OOV**）的单词。使用50,000个标记的词汇表，由于我们最终要构建的预测层的大小，内存很容易就会耗尽。在减少词汇表大小的同时，我们必须确保保留最常见的25,000个单词。幸运的是，每个词汇表文件的组织方式是按单词出现的频率排序（从高到低）。因此，我们只需从文件中读取前25,001行文本：
- en: '[PRE11]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we do the same for the German vocabulary:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对德语的词汇表做相同的操作：
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each of the vocabularies contain the special OOV token `<unk>` as the first
    line. We’ll pop that out of the `en_vocabulary` and `de_vocabulary` lists as we
    need this for the next step:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词汇表的第一行都包含特殊的OOV标记`<unk>`。我们将从`en_vocabulary`和`de_vocabulary`列表中移除它，因为在下一步中我们需要它：
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here’s how we can define our English `StringLookup` layer:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何定义英语的`StringLookup`层：
- en: '[PRE14]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s understand the arguments provided to this layer:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解传递给这个层的参数：
- en: '`vocabulary` – Contains a list of words that are found in the corpus (except
    certain special tokens that will be discussed below)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocabulary` – 包含在语料库中找到的单词列表（除了以下将讨论的某些特殊标记）'
- en: '`oov_token` – A special out-of-vocabulary token that will be used to replace
    tokens not listed in the vocabulary'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oov_token` – 一个特殊的超出词汇表（out-of-vocabulary）标记，用于替换词汇表中没有列出的标记'
- en: '`mask_token` – A special token that will be used to mask inputs (e.g. uninformative
    padded tokens)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` – 一个特殊的标记，用于遮蔽输入（例如，不含信息的填充标记）'
- en: '`pad_to_max_tokens` – If padding should occur to bring arbitrary-length sequences
    in a batch of data to the same length'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_max_tokens` – 如果需要进行填充，将任意长度的序列调整为数据批次中的相同长度'
- en: 'Similarly, we define a lookup layer for the German language:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们为德语定义一个查找层：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With the groundwork laid out, we can start building the encoder.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在打好基础后，我们可以开始构建编码器。
- en: Defining the encoder
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义编码器
- en: 'We start the encoder with an input layer. The input layer will take in a batch
    of sequences of tokens. Each sequence of tokens is `n_en_seq_length` elements
    long. Remember that we padded or truncated the sentences to make sure all of them
    have a fixed length of `n_en_seq_length`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从输入层开始构建编码器。输入层将接受一个包含标记序列的批次。每个标记序列的长度为`n_en_seq_length`个元素。记住，我们已经填充或截断了句子，确保它们的固定长度为`n_en_seq_length`：
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next we use the previously defined `StringLookup` layer to convert the string
    tokens into word IDs. As we saw, the `StringLookup` layer can take a list of unique
    words (i.e. a vocabulary) and create a lookup operation to convert a given token
    into a numerical ID:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前定义的`StringLookup`层将字符串标记转换为词 ID。如我们所见，`StringLookup`层可以接受一个独特单词的列表（即词汇表），并创建一个查找操作，将给定的标记转换为数字
    ID：
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With the tokens converted into IDs, we route the generated word IDs to a token
    embedding layer. We pass in the size of the vocabulary (derived from the `en_lookup_layer`''s
    `get_vocabulary()` method) and the embedding size (128) and finally we ask the
    layer to mask any zero-valued inputs as they don’t contain any information:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 将词元转换为ID后，我们将生成的单词ID传递给词元嵌入层。我们传入词汇表的大小（从`en_lookup_layer`的`get_vocabulary()`方法中获取）和嵌入大小（128），最后我们要求该层对任何零值输入进行掩蔽，因为它们不包含任何信息：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the embedding layer is stored in `encoder_emb_out`. Next we define
    a GRU layer to process the sequence of English token embeddings:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输出存储在 `encoder_emb_out` 中。接下来，我们定义一个GRU层来处理英文词元嵌入序列：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note how we are setting both the `return_sequences` and `return_state` arguments
    to `True`. To recap, `return_sequences` returns the full sequence of hidden states
    as the output (instead of returning only the last), where `return_state` returns
    the last state of the model as an additional output. We need both these outputs
    to build the rest of our model. For example, we need to pass the last state of
    the encoder to the decoder as the initial state. For that, we need the last state
    of the encoder (stored in `encoder_gru_last_state`). We will discuss the purpose
    of this in more detail as we go. We now have everything to define the encoder
    part of our model. It takes in a batch of sequences of string tokens and returns
    the full sequence of GRU hidden states as the output.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将 `return_sequences` 和 `return_state` 参数都设置为 `True`。总结一下，`return_sequences`
    返回完整的隐藏状态序列作为输出（而不是仅返回最后一个状态），而 `return_state` 返回模型的最后状态作为额外的输出。我们需要这两个输出才能构建模型的其余部分。例如，我们需要将编码器的最后状态传递给解码器作为初始状态。为此，我们需要编码器的最后状态（存储在
    `encoder_gru_last_state` 中）。我们将在后续详细讨论这个目的。现在我们已经准备好定义模型的编码器部分。它接收一批字符串词元序列，并返回完整的GRU隐藏状态序列作为输出。
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: With the encoder defined, let’s build the decoder.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好编码器后，让我们来构建解码器。
- en: Defining the decoder
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义解码器
- en: 'Our decoder will be more complex than the encoder. The objective of the decoder
    is, given the last encoder state and the previous token the decoder predicted,
    predict the next token. For example, for the German sentence:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器将比编码器更复杂。解码器的目标是，给定最后一个编码器状态和解码器预测的前一个词，预测下一个词。例如，对于德语句子：
- en: '*<s> ich ging zum Laden </s>*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*<s> ich ging zum Laden </s>*'
- en: 'We define:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义：
- en: '| **Input** | <s> | ich | ging | zum | Laden |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | <s> | ich | ging | zum | Laden |'
- en: '| **Output** | ich | ging | zum | Laden | </s> |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **输出** | ich | ging | zum | Laden | </s> |'
- en: 'This technique is known as **teacher forcing**. In other words, the decoder
    is leveraging previous tokens of the target itself to predict the next token.
    This makes the translation task easier for the model. We can understand this phenomenon
    as follows. Say a teacher asks a kindergarten student to complete the following
    sentence, given just the first word:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被称为 **教师强制**。换句话说，解码器利用目标语言的前一个词元来预测下一个词元。这使得翻译任务对模型来说变得更容易。我们可以通过以下方式理解这一现象。假设老师让幼儿园的学生完成以下句子，只给出第一个词：
- en: '*I ___ ____ ___ ___ ____ ____*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*I ___ ____ ___ ___ ____ ____*'
- en: This means that the child needs to pick a subject, verb, and object; know the
    syntax of the language; understand the grammar rules of the language; and so on.
    Therefore, the likelihood of the child producing an incorrect sentence is high.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着孩子需要选择主语、动词和宾语；了解语言的语法结构；理解语言的语法规则；等等。因此，孩子生成不正确句子的可能性很高。
- en: 'However, if we ask the child to produce it word by word, they might do a better
    job at coming up with a sentence. In other words, we ask the child to produce
    the next word given the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们要求孩子逐个单词地生成句子，他们可能更擅长于构造一个完整的句子。换句话说，我们要求孩子在给定以下条件的情况下生成下一个单词：
- en: '*I ____*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*I ____*'
- en: 'Then we ask them to fill in the blank given:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要求他们在给定的情况下填空：
- en: '*I like ____*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*I like ____*'
- en: 'And continue in the same fashion:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后继续以相同的方式进行：
- en: '*I like to ___, I like to fly ____, I like to fly kites ____*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*I like to ___, I like to fly ____, I like to fly kites ____*'
- en: 'This way, the child can do a better job at producing a correct and meaningful
    sentence. We can adopt the same approach to alleviate the difficulty of the translation
    task, as shown in *Figure 9.10*:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，孩子可以更好地生成一个正确且有意义的句子。我们可以采用相同的方法来减轻翻译任务的难度，如 *图 9.10* 所示：
- en: '![](img/B14070_09_10.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_10.png)'
- en: 'Figure 9.10: The teacher forcing mechanism. The darker arrows in the inputs
    depict newly introduced input connections to the decoder. The right-hand side
    figure shows how the decoder GRU cell changes.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：教师强制机制。输入中的深色箭头表示新引入的输入连接到解码器。右侧的图显示了解码器GRU单元如何变化。
- en: 'To feed in previous tokens predicted by the decoder, we need an input layer
    for the decoder. When formulating the decoder inputs and outputs this way, for
    a sequence of tokens with length *n*, the input and output are *n-1* tokens long:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解码器输入先前由解码器预测的标记，我们需要为解码器提供一个输入层。当以这种方式构造解码器的输入和输出时，对于长度为 *n* 的标记序列，输入和输出的长度是
    *n-1* 个标记：
- en: '[PRE21]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we use the `de_lookup_layer` defined earlier to convert tokens to IDs:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前定义的`de_lookup_layer`将标记转换为ID：
- en: '[PRE22]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Similar to the encoder, let’s define an embedding layer for the German language:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于编码器，让我们为德语定义一个嵌入层：
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We define a GRU layer in the decoder that will take the token embeddings and
    produce hidden outputs:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在解码器中定义一个GRU层，它将接受标记嵌入并生成隐藏输出：
- en: '[PRE24]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that we are passing the encoder’s last state to a special argument called
    `initial_state` in the GRU’s `call()` method. This ensures that the decoder uses
    the encoder’s last state to initialize its memory.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将编码器的最后状态传递给GRU的`call()`方法中的一个特殊参数`initial_state`。这确保了解码器使用编码器的最后状态来初始化其内存。
- en: The next step of our journey takes us to one of the most important concepts
    in machine learning, ‘attention.’ So far, the decoder had to rely on the encoder’s
    last state as the ‘only’ input/signal about the source language. This is like
    asking to summarize a sentence using a single word. Generally, when doing so,
    you lose a lot of the meaning and message in this conversion. Attention alleviates
    this problem.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旅程的下一步将带我们走向机器学习中最重要的概念之一——“注意力”。到目前为止，解码器必须依赖编码器的最后状态作为关于源语言的“唯一”输入/信号。这就像要求用一个单词总结一个句子。通常，在这样做时，你会失去很多转换中的意义和信息。注意力缓解了这个问题。
- en: 'Attention: Analyzing the encoder states'
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力：分析编码器状态
- en: Instead of relying just on the encoder’s last state, attention enables the decoder
    to analyze the complete history of state outputs. The decoder does this at every
    step of the prediction and creates a weighted average of all the state outputs
    depending on what it needs to produce at that step. For example, in the translation
    *I went to the shop -> ich ging zum Laden*, when predicting the word *ging*, the
    decoder will pay more attention to the first part of the English sentence than
    the latter.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅依赖编码器的最后状态，注意力使解码器能够分析整个状态输出历史。解码器在每一步的预测中都会这样做，并根据它在该步骤需要生成的内容创建所有状态输出的加权平均值。例如，在翻译
    *I went to the shop -> ich ging zum Laden* 时，在预测单词 *ging* 时，解码器会更多地关注英文句子的前半部分，而不是后半部分。
- en: 'There have been many different implementations of attention over the years.
    It’s important to properly emphasize the need for attention in NMT systems. As
    you have learned previously, the context, or thought vector, that resides between
    the encoder and the decoder is a performance bottleneck (see *Figure 9.11*):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，注意力机制有许多不同的实现。正确强调注意力在神经机器翻译（NMT）系统中的重要性是非常重要的。正如你之前所学到的，位于编码器和解码器之间的上下文向量或思想向量是一个性能瓶颈（见*图
    9.11*）：
- en: '![Breaking the context vector bottleneck](img/B14070_09_11.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![打破上下文向量瓶颈](img/B14070_09_11.png)'
- en: 'Figure 9.11: The encoder-decoder architecture'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：编码器-解码器架构
- en: 'To understand why this is a bottleneck, let’s imagine translating the following
    English sentence:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么这是一个瓶颈，让我们想象一下翻译下面的英文句子：
- en: '*I went to the flower market to buy some flowers*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*我去花市买花*'
- en: 'This translates to the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这转换为以下内容：
- en: '*Ich ging zum Blumenmarkt, um Blumen zu kaufen*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ich ging zum Blumenmarkt, um Blumen zu kaufen*'
- en: 'If we are to compress this into a fixed-length vector, the resulting vector
    needs to contain these:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要将其压缩成一个固定长度的向量，结果向量需要包含以下内容：
- en: Information about the subject (*I*)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于主语的信息（*我*）
- en: Information about the verbs (*buy* and *went*)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于动词的信息（*买* 和 *去*）
- en: Information about the objects (*flowers* and *flower market*)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于宾语的信息（*花* 和 *花市*）
- en: Interaction of the subjects, verbs, and objects with each other in the sentence
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子中主语、动词和宾语相互作用
- en: Generally, the context vector has a size of 128 or 256 elements. Reliance on
    the context vector to store all this information with a small-sized vector is
    very impractical and an extremely difficult requirement for the system. Therefore,
    most of the time, the context vector fails to provide the complete information
    required to make a good translation. This results in an underperforming decoder
    that suboptimally translates a sentence.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，上下文向量的大小为 128 或 256 元素。依赖上下文向量来存储所有这些信息，而只使用一个小尺寸的向量是非常不切实际的，而且对于系统来说是一个极其困难的要求。因此，大多数时候，上下文向量未能提供进行良好翻译所需的完整信息。这导致解码器性能不佳，无法以最优方式翻译句子。
- en: To make the problem worse, during decoding the context vector is observed only
    in the beginning. Thereafter, the decoder GRU must memorize the context vector
    until the end of the translation. This becomes more and more difficult for long
    sentences.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，在解码过程中，上下文向量只能在开始时观察到。此后，解码器 GRU 必须记住上下文向量，直到翻译结束。对于长句子来说，这变得越来越困难。
- en: Attention sidesteps this issue. With attention, the decoder will have access
    to the full state history of the encoder for each decoding time step. This allows
    the decoder to access a very rich representation of the source sentence. Furthermore,
    the attention mechanism introduces a softmax layer that allows the decoder to
    calculate a weighted mean of the past observed encoder states, which will be used
    as the context vector for the decoder. This allows the decoder to pay different
    amounts of attention to different words at different decoding steps.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力解决了这个问题。通过注意力机制，解码器将在每个解码时间步获得编码器的完整状态历史。这使得解码器能够访问源句子的丰富表示。此外，注意力机制引入了一个
    softmax 层，允许解码器计算过去观察到的编码器状态的加权平均值，并将其作为解码器的上下文向量。这样，解码器就可以在不同的解码步骤中对不同的单词赋予不同的关注权重。
- en: '*Figure 9.12* shows a conceptual breakdown of the attention mechanism:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.12* 展示了注意力机制的概念性分解：'
- en: '![The attention mechanism in detail](img/B14070_09_12.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![注意力机制详细图](img/B14070_09_12.png)'
- en: 'Figure 9.12: Conceptual attention mechanism in NMT'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：NMT 中的概念性注意力机制
- en: Next, let’s look at how we can compute attention.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何计算注意力。
- en: Computing Attention
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算注意力
- en: 'Now let’s investigate the actual implementation of the attention mechanism
    in detail. For this, we will use the Bahdanau attention mechanism introduced in
    the paper *Neural Machine Translation by Learning to Jointly Align and Translate*,
    by Bahdanau et al. We will discuss the original attention mechanism here. However,
    we’ll be implementing a slightly different version of it, due to the limitations
    of TensorFlow. For consistency with the paper, we will use the following notations:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细探讨注意力机制的实际实现。为此，我们将使用 Bahdanau 等人论文《通过学习联合对齐和翻译的神经机器翻译》中的 Bahdanau 注意力机制。我们将讨论原始的注意力机制。然而，由于
    TensorFlow 的限制，我们将实现一个略有不同的版本。为了与论文保持一致，我们将使用以下符号：
- en: 'Encoder’s *j*^(th) hidden state: *h*[j]'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器的第 *j*^(th) 个隐藏状态：*h*[j]
- en: '*i*^(th) target token: *y*[i]'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 *i*^(th) 个目标词：*y*[i]
- en: '*i*^(th) decode hidden state in the *i*^(th) time step: *s*[i]'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 *i*^(th) 时间步的解码隐藏状态：*s*[i]
- en: 'Context vector: *c*[i]'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文向量：*c*[i]
- en: 'Our decoder GRU is a function of an input *y*[i] and a previous step’s hidden
    state ![](img/B14070_09_043.png). This can be represented as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器 GRU 是输入 *y*[i] 和上一步隐藏状态 ![](img/B14070_09_043.png) 的函数。这可以表示如下：
- en: '![](img/B14070_09_044.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_044.png)'
- en: 'Here, *f* represents the actual update rules used to calculate *y*[i] and *s*[i-1].
    With the attention mechanism, we are introducing a new time-dependent context
    vector *c*[i] for the *i*^(th) decoding step. The *c*[i] vector is a weighted
    mean of the hidden states of all the unrolled encoder steps. A higher weight will
    be given to the *j*^(th) hidden state of the encoder if the *j*^(th) word is more
    important for translating the *i*^(th) word in the target language. This means
    the model can learn which words are important at which time step, regardless of
    the directionality of the two languages or alignment mismatches. Now the decoder
    GRU becomes this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f* 代表用于计算 *y*[i] 和 *s*[i-1] 的实际更新规则。通过引入注意力机制，我们为第 *i*^(th) 解码步骤引入了一个新的时间相关的上下文向量
    *c*[i]。这个 *c*[i] 向量是所有展开的编码器步骤的隐藏状态的加权平均值。如果第 *j*^(th) 个单词在翻译第 *i*^(th) 个目标语言单词时更为重要，那么编码器的第
    *j*^(th) 个隐藏状态将赋予更高的权重。这意味着模型可以学习在什么时间步，哪些单词更为重要，而不考虑两种语言的方向性或对齐不匹配的问题。现在，解码器的
    GRU 模型变成了这样：
- en: '![](img/B14070_09_045.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_045.png)'
- en: Conceptually, the attention mechanism can be thought of as a separate layer
    and illustrated as in *Figure 9.13*. As shown, attention functions as a layer.
    The attention layer is responsible for producing *c*[i] for the *i*^(th) time
    step of the decoding process.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，注意力机制可以被看作是一个独立的层，如*图 9.13*所示。正如所示，注意力作为一个层运作。注意力层负责生成解码过程中的第*i*^(th)时间步的*c*[i]。
- en: 'Let’s now see how to calculate *c*[i]:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看如何计算*c*[i]：
- en: '![](img/B14070_09_046.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_046.png)'
- en: 'Here, *L* is the number of words in the source sentence, and ![](img/B14070_09_047.png)
    is a normalized weight representing the importance of the *j*^(th) encoder hidden
    state for calculating the *i*^(th) decoder prediction. This is calculated using
    what is known as an energy value. We represent *e*[ij] as the energy of the encoder’s
    *j*^(th) position for predicting the decoder’s *i*^(th) position. *e*[ij] is computed
    using a small fully connected network as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*L*是源句子中的单词数量，![](img/B14070_09_047.png)是一个标准化权重，表示在计算第*i*^(th)解码器预测时，第*j*^(th)编码器隐藏状态的重要性。这个值是通过所谓的能量值计算的。我们将*e*[ij]表示为编码器在第*j*^(th)位置的能量，用于预测解码器的第*i*^(th)位置。*e*[ij]通过一个小型全连接网络计算，如下所示：
- en: '![](img/B14070_09_048.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_048.png)'
- en: 'In other words, ![](img/B14070_09_049.png) is calculated with a multilayer
    perceptron whose weights are *v*[a], *W*[a], and *U*[a], and ![](img/B14070_09_050.png)
    (decoder’s previous hidden state from (*i-1*)^(th) time step) and *h*[j] (encoder’s
    *j*^(th) hidden output) are the inputs to the network. Finally, we compute the
    normalized energy values (i.e. weights) using softmax normalization over all encoder
    timesteps:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，![](img/B14070_09_049.png)是通过一个多层感知机计算的，该网络的权重是*v*[a]、*W*[a]和*U*[a]，而![](img/B14070_09_050.png)（解码器的前一个隐藏状态，来自第*(i-1)*^(th)时间步）和*h*[j]（编码器的第*j*^(th)隐藏输出）是网络的输入。最后，我们使用
    softmax 标准化对所有编码器时间步的能量值（即权重）进行标准化计算：
- en: '![](img/B14070_09_051.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_051.png)'
- en: 'The attention mechanism is shown in *Figure 9.13*:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制如*图 9.13*所示：
- en: '![The attention mechanism in detail](img/B14070_09_13.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![注意力机制的详细说明](img/B14070_09_13.png)'
- en: 'Figure 9.13: The attention mechanism'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：注意力机制
- en: Implementing Attention
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现注意力机制
- en: 'We said above that we’ll be implementing a slightly different variation of
    Bahdanau attention. This is because TensorFlow currently does not support an attention
    mechanism that can be iteratively computed for each time step, similar to how
    an RNN works. Therefore, we are going to decouple the attention mechanism from
    the GRU model and have it computed separately. We will concatenate the attention
    output with the hidden output of the GRU layer and feed it to the final prediction
    layer. In other words, we are not feeding attention output to the GRU model, but
    directly to the prediction layer. This is depicted in *Figure 9.14*:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们将实现 Bahdanau 注意力的一个稍微不同的变体。这是因为 TensorFlow 目前不支持可以在每个时间步骤上迭代计算的注意力机制，类似于
    RNN 的工作方式。因此，我们将把注意力机制与 GRU 模型解耦，并单独计算。我们将把注意力输出与 GRU 层的隐藏输出拼接，并将其输入到最终的预测层。换句话说，我们不是将注意力输出输入到
    GRU 模型，而是直接输入到预测层。这在*图 9.14*中有所示意：
- en: '![](img/B14070_09_14.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_14.png)'
- en: 'Figure 9.14: The attention mechanism employed in this chapter'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：本章中采用的注意力机制
- en: 'To implement attention, we are going to use the sub-classing API of Keras.
    We’ll define a class called `BahdanauAttention` (which inherits from the `Layer`
    class) and override two functions in that:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现注意力机制，我们将使用 Keras 的子类化 API。我们将定义一个名为`BahdanauAttention`的类（该类继承自`Layer`类），并重写其中的两个函数：
- en: '`__init__()` – Defines the layer’s initialization logic'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__()` – 定义层的初始化逻辑'
- en: '`call()` – Defines the computational logic of the layer'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call()` – 定义层的计算逻辑'
- en: 'Our defined class would look like this. But don’t worry, we’ll be going through
    those two functions in detail below:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的类将如下所示。但不用担心，下面我们将详细讲解这两个函数：
- en: '[PRE25]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: First, we’ll be looking at the `__init__()` function.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看`__init__()`函数。
- en: 'Here, you can see that we are defining three layers: weight matrix `W_a`, weight
    matrix `U_a`, and finally the `AdditiveAttention` layer, which contains the attention
    computation logic we discussed above. The `AdditiveAttention` layer takes in a
    query, value and a key. The query is the decoder states, and the value and key
    are all of encoder states produced.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到我们定义了三个层：权重矩阵 `W_a`，权重矩阵 `U_a`，以及最终的 `AdditiveAttention` 层，其中包含了我们之前讨论的注意力计算逻辑。`AdditiveAttention`
    层接受查询、值和键。查询是解码器的状态，值和键是所有由编码器产生的状态。
- en: 'We will discuss this layer in more detail soon. We’ll discuss the details of
    this layer below. Next let’s look at the computations defined in the `call()`
    function:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快会更详细地讨论这一层。接下来让我们来看一下`call()`函数中定义的计算：
- en: '[PRE26]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first thing to note is that this function takes a query, a key, and a value.
    These three elements will drive the attention computation. In Bahdanau attention,
    you can think of the key and value as being the same thing. The query will represent
    each decoder GRU’s hidden states for each time step, and the value (or key) will
    represent each encoder GRU’s hidden states for each time step. In other words,
    we are querying an output for each decoder position based on values provided by
    the encoder’s hidden states.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，这个函数接受查询、键和值这三个输入。这三个元素将驱动注意力计算。在Bahdanau注意力中，你可以将键和值看作是相同的东西。查询将代表每个解码器GRU在每个时间步的隐藏状态，值（或键）将代表每个编码器GRU在每个时间步的隐藏状态。换句话说，我们正在根据编码器的隐藏状态提供的值，为每个解码器位置查询一个输出。
- en: 'Let’s recap the computations we have to perform:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们需要执行的计算：
- en: '![](img/B14070_09_048.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_048.png)'
- en: '![](img/B14070_09_051.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_051.png)'
- en: '![](img/B14070_09_046.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_046.png)'
- en: 'First we compute `wa_query` (represents ![](img/B14070_09_055.png)) and `ua_key`
    (represents ![](img/B14070_09_056.png)). Next, we propagate these values to the
    attention layer. The `AdditiveAttention` layer ([https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention))
    performs the following steps:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们计算 `wa_query`（代表 ![](img/B14070_09_055.png)）和 `ua_key`（代表 ![](img/B14070_09_056.png)）。接着，我们将这些值传递到注意力层。`AdditiveAttention`
    层（[https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention)）执行以下步骤：
- en: Reshapes `wa_query` from `[batch_size, Tq, dim]` to shape`[batch_size, Tq, 1,
    dim]` and `ua_key` from `[batch_size, Tv, dim]` shape to `[batch_size, 1, Tv,
    dim]`.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `wa_query` 从 `[batch_size, Tq, dim]` 形状重塑为 `[batch_size, Tq, 1, dim]`，并将 `ua_key`
    从 `[batch_size, Tv, dim]` 形状重塑为 `[batch_size, 1, Tv, dim]`。
- en: 'Calculates scores with shape `[batch_size, Tq, Tv]` as: `scores = tf.reduce_sum(tf.tanh(query
    + key), axis=-1)`.'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算形状为 `[batch_size, Tq, Tv]` 的分数：`scores = tf.reduce_sum(tf.tanh(query + key),
    axis=-1)`。
- en: 'Uses scores to calculate a distribution with shape `[batch_size, Tq, Tv]` using
    softmax activation: `distribution = tf.nn.softmax(scores)`.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分数来计算一个形状为 `[batch_size, Tq, Tv]` 的分布，并通过 softmax 激活函数计算：`distribution = tf.nn.softmax(scores)`。
- en: Uses `distribution` to create a linear combination of `value` with shape `[batch_size,
    Tq, dim]`.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `distribution` 来创建一个形状为 `[batch_size, Tq, dim]` 的值的线性组合。
- en: Returns `tf.matmul(distribution, value)`, which represents a weighted average
    of all encoder states (i.e. `value`)
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 `tf.matmul(distribution, value)`，它代表了所有编码器状态（即 `value`）的加权平均值。
- en: 'Here, you can see that *step 2* performs the first equation, *step 3* performs
    the second equation, and finally *step 4* performs the third equation. Another
    thing worth noting is that *step 2* does not mention ![](img/B14070_09_057.png)
    from the first equation. ![](img/B14070_09_058.png) is essentially a weight matrix
    with which we compute the dot product. We can introduce this weight matrix by
    setting `use_scale=True` when defining the `AdditiveAttention` layer:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到*步骤 2* 执行了第一个方程，*步骤 3* 执行了第二个方程，最后*步骤 4* 执行了第三个方程。另一个值得注意的事项是，*步骤 2*
    并没有提到来自第一个方程的 ![](img/B14070_09_057.png)。![](img/B14070_09_058.png) 本质上是一个权重矩阵，我们用它来计算点积。我们可以通过在定义
    `AdditiveAttention` 层时设置 `use_scale=True` 来引入这个权重矩阵：
- en: '[PRE27]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Another important argument is the `return_attention_scores` argument when calling
    the `AdditiveAttention` layer. This gives us the distribution weight matrix defined
    in *step 3*. We will use this to visualize where the model was paying attention
    when decoding the translation.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的参数是 `return_attention_scores`，当调用 `AdditiveAttention` 层时，该参数给我们提供了*步骤
    3*中定义的分布权重矩阵。我们将使用它来可视化模型在解码翻译时关注的部分。
- en: Defining the final model
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义最终模型
- en: With the attention mechanism understood and implemented, let’s continue our
    implementation of the decoder. We will get the attention output sequence, with
    one attended output for each time step.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解并实现了注意力机制后，让我们继续实现解码器。我们将获得每个时间步的注意力输出序列，每个时间步有一个被关注的输出。
- en: 'Moreover, we’ll get the attention weights distribution matrix, which we’ll
    use to visualize attention patterns against inputs and outputs:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还会得到注意力权重分布矩阵，用于可视化注意力模式在输入和输出之间的分布：
- en: '[PRE28]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'When defining attention, we’ll also pass a mask that denotes which tokens need
    to be ignored when computing outputs (e.g. padded tokens). Combine the attention
    output and the decoder’s GRU output to create a single concatenated input for
    the prediction layer:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义注意力时，我们还会传递一个掩码，表示在计算输出时需要忽略哪些标记（例如，填充的标记）。将注意力输出与解码器的GRU输出结合，创建一个单一的拼接输入供预测层使用：
- en: '[PRE29]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, the prediction layer takes the concatenated attention’s context vector
    and the GRU output to produce probability distributions over the German tokens
    for each timestep:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，预测层将拼接后的注意力上下文向量和GRU输出结合起来，生成每个时间步长的德语标记的概率分布：
- en: '[PRE30]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With the encoder and the decoder fully defined, let’s define the end-to-end
    model:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全定义编码器和解码器后，我们来定义端到端模型：
- en: '[PRE31]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We are also going to define a secondary model called the `attention_visualizer`:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将定义一个名为`attention_visualizer`的辅助模型：
- en: '[PRE32]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `attention_visualizer` can generate attention patterns for a given set of
    inputs. This is a handy way to know if the model is paying attention to the correct
    words during the decoding process. This visualizer model will be used once the
    full model is trained. We will now look at how we can train our model.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`attention_visualizer`可以为给定的输入集生成注意力模式。这是一种便捷的方式，能够判断模型在解码过程中是否关注了正确的词语。此可视化模型将在完整模型训练后使用。接下来，我们将探讨如何训练我们的模型。'
- en: Training the NMT
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练NMT
- en: 'Now that we have defined the NMT architecture and preprocessed the training
    data, it is quite straightforward to train the model. Here, we will define and
    illustrate (see *Figure 9.15*) the exact process used for training:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了NMT架构并预处理了训练数据，训练模型变得相当直接。在这里，我们将定义并展示（见*图 9.15*）用于训练的确切过程：
- en: '![](img/B14070_09_15.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_15.png)'
- en: 'Figure 9.15: The training procedure for NMT'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：NMT的训练过程
- en: 'For the model training, we’re going to define a custom training loop, as there
    is a special metric we’d like to track. Unfortunately, this metric is not a readily
    available TensorFlow metric. But before that, there are several utility functions
    we need to define:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型训练，我们将定义一个自定义的训练循环，因为有一个特殊的度量我们想要跟踪。不幸的是，这个度量并不是一个现成的TensorFlow度量。但是在此之前，我们需要定义几个工具函数：
- en: '[PRE33]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `prepare_data()` function takes the source sentence and target sentence
    pairs and generates encoder and decoder inputs and decoder labels. Let’s understand
    the arguments:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`prepare_data()`函数接受源句子和目标句子对，并生成编码器输入、解码器输入和解码器标签。让我们了解一下这些参数：'
- en: '`de_lookup_layer` – The `StringLookup` layer of the German language'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`de_lookup_layer` – 德语语言的`StringLookup`层'
- en: '`train_xy` – A tuple containing tokenized English sentences and tokenized German
    sentences in the training set, respectively'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_xy` – 包含训练集中标记化的英语句子和标记化的德语句子的元组'
- en: '`valid_xy` – Similar to `train_xy` but for validation data'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`valid_xy` – 与`train_xy`类似，但用于验证数据'
- en: '`test_xy` – Similar to `train_xy` but for test data'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_xy` – 与`train_xy`类似，但用于测试数据'
- en: 'For each training, validation, and test dataset, this function generates the
    following:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个训练、验证和测试数据集，此函数会生成以下内容：
- en: '`encoder_inputs` – Tokenized English sentences as in the preprocessed dataset'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_inputs` – 经过分词处理的英语句子，来自预处理的数据集'
- en: '`decoder_inputs` – All tokens except the last of each German sentence'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs` – 每个德语句子的所有标记，除去最后一个标记'
- en: '`decoder_labels` – All token IDs except the first of each German sentence,
    where token IDs are generated by the `de_lookup_layer`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_labels` – 每个德语句子的所有标记ID，除去第一个标记ID，标记ID由`de_lookup_layer`生成'
- en: 'So, you can see that `decoder_labels` will be `decoder_inputs` shifted one
    token to the left. Next we define the `shuffle_data()` function, which will shuffle
    a provided set of data:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可以看到`decoder_labels`将是`decoder_inputs`向左移动一个标记。接下来，我们定义`shuffle_data()`函数，用于打乱提供的数据集：
- en: '[PRE34]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The logic here is quite straightforward. We take the `encoder_inputs`, `decoder_inputs`,
    and `decoder_labels` (generated by the `prepare_data()` step) with `shuffle_inds`.
    If `shuffle_inds` is `None`, we generate a random permutation of the indices.
    Otherwise, we generate a random permutation of the `shuffle_inds` provided. Finally,
    we index all of the data according to the shuffled index. We can then train the
    model:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的逻辑非常简单。我们使用`encoder_inputs`、`decoder_inputs`和`decoder_labels`（由`prepare_data()`步骤生成）以及`shuffle_inds`。如果`shuffle_inds`为`None`，则生成索引的随机排列。否则，我们生成提供的`shuffle_inds`的随机排列。最后，我们根据洗牌后的索引对所有数据进行索引。然后我们就可以训练模型：
- en: '[PRE35]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'During model training, we do the following:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，我们执行以下操作：
- en: Prepare encoder and decoder inputs and decoder outputs using the `prepare_data()`
    function
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`prepare_data()`函数准备编码器和解码器输入以及解码器输出
- en: 'For each epoch:'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个周期：
- en: Shuffle the data if the flag `shuffle` is set to `True`
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标志`shuffle`设置为`True`，则需要对数据进行洗牌
- en: 'For each iteration:'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每次迭代：
- en: Get a batch of data from prepared inputs and outputs
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从准备好的输入和输出中获取一个批次的数据
- en: Evaluate that batch using `model.evaluate` to get the loss and accuracy
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`model.evaluate`评估该批次，以获取损失和准确率
- en: Check if any of the samples are giving `nan` values (useful as a debugging step)
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查是否有样本返回`nan`值（这对于调试很有用）
- en: Train on the batch of data
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在批次数据上进行训练
- en: Compute the BLEU score if the flag `predict_bleu_at_training` is set to `True`
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标志`predict_bleu_at_training`设置为`True`，则计算 BLEU 分数
- en: Evaluate the model on validation data to get validation loss and accuracy
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证数据上评估模型，以获取验证损失和准确率
- en: Compute the validation BLEU score
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算验证数据集的 BLEU 分数
- en: Compute the loss, accuracy, and BLEU score on test data
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算测试数据上的损失、准确率和 BLEU 分数
- en: You can see that we are computing a new metric called the BLEU metric. BLEU
    is a special metric used to measure performance in sequence-to-sequence problems.
    It tries to maximize the correctness of n-grams of tokens, rather than measuring
    it on individual tokens (e.g. accuracy). The higher the BLEU score, the better.
    You will learn more about how the BLEU score is calculated in the next section.
    You can see the logic defined in the `BLEUMetric` object in the code.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，我们正在计算一个新的度量标准，称为 BLEU 分数。BLEU 是一种专门用于衡量序列到序列问题表现的指标。它试图最大化 n-gram 标记的正确性，而不是单独标记的准确性（例如，准确率）。BLEU
    分数越高，效果越好。您将在下一部分了解更多关于 BLEU 分数如何计算的信息。您可以在代码中查看`BLEUMetric`对象定义的逻辑。
- en: In this, we are mostly doing the preprocessing of text to remove uninformative
    tokens, so that the BLEU score is not overestimated. For example, if we include
    the `<pad>` token, you will see high BLEU scores, as there are long sequences
    of `<pad>` tokens for short sentences. To compute the BLEU score, we’ll be using
    a third-party implementation available at [https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们主要进行文本预处理，去除无意义的标记，以避免 BLEU 分数被高估。例如，如果我们包括`<pad>`标记，您会看到高的 BLEU 分数，因为短句中会有长的`<pad>`标记序列。为了计算
    BLEU 分数，我们将使用一个第三方实现，链接地址为：[https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)。
- en: '**Note**'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'If you have a large batch size, you may see TensorFlow throwing an exception
    starting out as:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果批次大小较大，您可能会看到 TensorFlow 抛出如下异常：
- en: '[PRE36]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In this case, you may need to restart the notebook kernel, reduce the batch
    size, and rerun the code.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可能需要重启笔记本内核、减少批次大小，并重新运行代码。
- en: Another thing we do, but haven’t discussed, is check for `NaN` (i.e. not-a-number)
    values. It can be very frustrating to see your loss value being `NaN` at the end
    of a training cycle. This is done by using the `check_for_nan()` function. This
    function will print out any specific data points that caused `NaN` values, so
    you have a much better idea of what caused it. You can find the implementation
    of the `check_for_nan()` function in the code.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们还会做一件事，但尚未讨论，那就是检查是否存在`NaN`（即不是数字）值。看到损失值在训练周期结束时变为`NaN`是非常令人沮丧的。这是通过使用`check_for_nan()`函数完成的。该函数会打印出导致`NaN`值的具体数据点，这样您就能更清楚地了解原因。您可以在代码中找到`check_for_nan()`函数的实现。
- en: '**Note**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: In 2021, the current state-of-the-art BLEU score for German to English translation
    is 35.14 ([https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2021年，德语到英语的当前最先进的BLEU分数为35.14 ([https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german))。
- en: Once the model is fully trained, you should see a BLEU score of around 15 for
    validation and test data. This is quite good, given that we used a very small
    proportion of the data (i.e. 250,000 sentences from more than 4 million) and a
    relatively simpler model compared to the state-of-the-art models.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型完全训练完成，你应该会看到验证和测试数据的BLEU分数大约为15。考虑到我们使用的数据比例非常小（即250,000个句子，来自400多万句子），而且与最先进的模型相比我们使用的模型较为简单，这个分数已经相当不错了。
- en: '**Improving NMT performance with deep GRUs**'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**提高NMT性能的深度GRU**'
- en: 'One obvious improvement we can do is to increase the number of layers by stacking
    GRUs on top of each other, thereby creating a deep GRUs. For example, the Google
    NMT system uses eight LSTM layers stacked upon each other (*Google’s Neural Machine
    Translation System: Bridging the Gap between Human and Machine Translation, Wu
    and others, Technical Report (2016)*). Though this hampers the computational efficiency,
    having more layers greatly improves the neural network’s ability to learn the
    syntax and other linguistic characteristics of the two languages.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显而易见的改进是增加层数，通过将GRU层堆叠起来，从而创建一个深度GRU。例如，Google的NMT系统使用了八层堆叠的LSTM层 (*Google的神经机器翻译系统：弥合人类与机器翻译之间的差距，Wu等人，技术报告（2016年）*)。尽管这会影响计算效率，但更多的层大大提高了神经网络学习两种语言语法和其他语言特征的能力。
- en: Next, let’s understand how the BLEU score is calculated in detail.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们详细了解如何计算 BLEU 分数。
- en: The BLEU score – evaluating the machine translation systems
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BLEU分数 – 评估机器翻译系统
- en: '**BLEU** stands for **Bilingual Evaluation Understudy** and is a way of automatically
    evaluating machine translation systems. This metric was first introduced in the
    paper *BLEU: A Method for Automatic Evaluation of Machine Translation, Papineni
    and others, Proceedings of the 40th Annual Meeting of the Association for Computational
    Linguistics (ACL), Philadelphia, July 2002: 311-318*. We will be using an implementation
    of the BLEU score found at [https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py).
    Let’s understand how this is calculated in the context of machine translation.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**BLEU** 代表 **Bilingual Evaluation Understudy**，是一种自动评估机器翻译系统的方法。该指标最早在论文 *BLEU:
    A Method for Automatic Evaluation of Machine Translation, Papineni 等人, 第40届计算语言学协会年会论文集
    (ACL)，费城，2002年7月: 311-318* 中提出。我们将使用在 [https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)
    上找到的 BLEU 分数实现。让我们了解在机器翻译的上下文中如何计算这个分数。'
- en: 'Let’s consider an example to learn the calculations of the BLEU score. Say
    we have two candidate sentences (that is, a sentence predicted by our MT system)
    and a reference sentence (that is, the corresponding actual translation) for some
    given source sentence:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来学习如何计算 BLEU 分数。假设我们有两个候选句子（即由我们的机器翻译系统预测的句子）和一个参考句子（即对应的实际翻译），用于某个给定的源句子：
- en: 'Reference 1: *The cat sat on the mat*'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考1：*猫坐在垫子上*
- en: 'Candidate 1: *The cat is on the mat*'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选句子1：*猫在垫子上*
- en: 'To see how good the translation is, we can use one measure, **precision**.
    Precision is a measure of how many words in the candidate are actually present
    in the reference. In general, if you consider a classification problem with two
    classes (denoted by negative and positive), precision is given by the following
    formula:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估翻译的质量，我们可以使用一个度量标准，**精确度**。精确度是指候选翻译中有多少词语实际上出现在参考翻译中。通常情况下，如果你考虑一个有两个类别（分别为负类和正类）的分类问题，精确度的计算公式如下：
- en: '![](img/B14070_09_059.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_059.png)'
- en: 'Let’s now calculate the precision for candidate 1:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算候选句子1的精确度：
- en: '*Precision = # of times each word of candidate appeared in reference/# of words
    in candidate*'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度 = 候选词语在参考中出现的次数 / 候选中的词语总数*'
- en: 'Mathematically, this can be given by the following formula:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，这可以通过以下公式表示：
- en: '![](img/B14070_09_060.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_060.png)'
- en: '*Precision for candidate 1 = 5/6*'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '*候选句子1的精确度 = 5/6*'
- en: This is also known as 1-gram precision since we consider a single word at a
    time.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为1-gram精确度，因为我们一次只考虑一个单词。
- en: 'Now let’s introduce a new candidate:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们引入一个新的候选项：
- en: 'Candidate 2: *The the the cat cat cat*'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选项2：*The the the cat cat cat*
- en: 'It is not hard for a human to see that candidate 1 is far better than candidate
    2\. Let’s calculate the precision:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 人类不难看出候选项1比候选项2要好得多。让我们计算精度：
- en: '*Precision for candidate 2 = 6/6 = 1*'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '*候选项2的精度 = 6/6 = 1*'
- en: As we can see, the precision score disagrees with the judgment we made. Therefore,
    precision alone cannot be trusted to be a good measure of the quality of a translation.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，精度分数与我们做出的判断不一致。因此，仅依靠精度不能作为翻译质量的一个可靠衡量标准。
- en: Modified precision
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改后的精度
- en: 'To address the precision limitation, we can use a modified 1-gram precision.
    The modified precision clips the number of occurrences of each unique word in
    the candidate by the number of times that word appeared in the reference:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决精度的局限性，我们可以使用修改后的1-gram精度。修改后的精度通过参考中该单词出现的次数来裁剪候选句子中每个唯一单词的出现次数：
- en: '![](img/B14070_09_061.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_061.png)'
- en: 'Therefore, for candidates 1 and 2, the modified precision would be as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于候选项1和2，修改后的精度如下：
- en: '*Mod-1-gram-Precision Candidate 1 = (1 + 1 + 1 + 1 + 1)/ 6 = 5/6*'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mod-1-gram-Precision 候选项1 = (1 + 1 + 1 + 1 + 1)/ 6 = 5/6*'
- en: '*Mod-1-gram-Precision Candidate 2 = (2 + 1) / 6 = 3/6*'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mod-1-gram-Precision 候选项2 = (2 + 1) / 6 = 3/6*'
- en: We can already see that this is a good modification as the precision of candidate
    2 is reduced. This can be extended to any n-gram by considering *n* words at a
    time instead of a single word.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到，这是一个很好的修改，因为候选项2的精度已经降低。这可以扩展到任何n-gram，通过考虑一次*n*个单词，而不是单个单词。
- en: Brevity penalty
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简洁度惩罚
- en: 'Precision naturally prefers small sentences. This raises a question in evaluation,
    as the MT system might generate small sentences for longer references and still
    have higher precision. Therefore, a **brevity penalty** is introduced to avoid
    this. The brevity penalty is calculated by the following:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 精度自然偏向短句子。这在评估中引发了一个问题，因为机器翻译系统可能会为较长的参考句子生成较短的句子，并且仍然具有更高的精度。因此，引入了**简洁度惩罚**来避免这种情况。简洁度惩罚按以下方式计算：
- en: '![](img/B14070_09_074.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_074.png)'
- en: 'Here, *c* is the candidate sentence length and *r* is the reference sentence
    length. In our example, we calculate it as shown here:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*c*是候选句子的长度，*r*是参考句子的长度。在我们的例子中，我们按如下方式进行计算：
- en: BP for candidate 1 = ![](img/B14070_09_062.png)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选项1的简洁度惩罚 = ![](img/B14070_09_062.png)
- en: BP for candidate 2 = ![](img/B14070_09_062.png)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选项2的简洁度惩罚 = ![](img/B14070_09_062.png)
- en: The final BLEU score
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终的BLEU得分
- en: 'Next, to calculate the BLEU score, we first calculate several different modified
    n-gram precisions for a bunch of different *n=1,2,…,N* values. We will then calculate
    the weighted geometric mean of the n-gram precisions:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了计算BLEU得分，我们首先计算不同*n=1,2,…,N*值的几个修改后的n-gram精度。然后，我们将计算n-gram精度的加权几何平均值：
- en: '![](img/B14070_09_065.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_065.png)'
- en: Here, *w*[n] is the weight for the modified n-gram precision *p*[n]. By default,
    equal weights are used for all n-gram values. In conclusion, BLEU calculates a
    modified n-gram precision and penalizes the modified-n-gram precision with a brevity
    penalty. The modified n-gram precision avoids potential high precision values
    given to meaningless sentences (for example, candidate 2).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w*[n]是修改后的n-gram精度*p*[n]的权重。默认情况下，所有n-gram值使用相等的权重。总之，BLEU计算修改后的n-gram精度，并通过简洁度惩罚来惩罚修改后的n-gram精度。修改后的n-gram精度避免了给无意义句子（例如候选项2）赋予潜在的高精度值。
- en: Visualizing Attention patterns
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化注意力模式
- en: 'Remember that we specifically defined a model called `attention_visualizer`
    to generate attention matrices? With the model trained, we can now look at these
    attention patterns by feeding data to the model. Here’s how the model was defined:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们专门定义了一个叫做`attention_visualizer`的模型来生成注意力矩阵吗？在模型训练完成后，我们现在可以通过向模型输入数据来查看这些注意力模式。下面是模型的定义：
- en: '[PRE37]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We’ll also define a function to get the processed attention matrix along with
    label data that we can use directly for visualization purposes:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将定义一个函数，以获取处理后的注意力矩阵以及标签数据，方便我们直接用于可视化：
- en: '[PRE38]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This function does the following:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数执行以下操作：
- en: Randomly samples `n_samples` indices from the test data.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机从测试数据中抽取`n_samples`个索引。
- en: 'For each random index:'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个随机索引：
- en: Gets the inputs of the data point at that index (`en_input` and `de_input`)
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取该索引处数据点的输入（`en_input`和`de_input`）
- en: Gets the predicted words by feeding `en_input` and `de_input` to the `attention_visualizer`
    (stored in `predicted_words`)
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将`en_input`和`de_input`输入到`attention_visualizer`中（存储在`predicted_words`中），获取预测词
- en: Cleans `en_input` by removing any uninformative tokens (e.g. `<pad>`) and assigns
    to `clean_en_input`
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理`en_input`，移除任何无信息的标记（例如`<pad>`），并将其分配给`clean_en_input`
- en: Cleans `predicted_words` by removing tokens after the `</s>` token (stored in
    `clean_predicted_words`)
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理`predicted_words`，通过移除`</s>`标记之后的词（存储在`clean_predicted_words`中）
- en: Gets the attention weights only corresponding to the words left in the clean
    inputs and predicted words from `attn_weights`
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅获取与清理后的输入和预测词对应的注意力权重，来源于`attn_weights`
- en: Appends the `clean_en_input`, `clean_predicted_words`, and attention weights
    matrix to results
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`clean_en_input`、`clean_predicted_words`和注意力权重矩阵附加到结果中
- en: The results contain all the information we need to visualize attention patterns.
    You can see the actual code used to create the following visualizations in the
    notebook `Ch09-Seq2seq-Models/ch09_seq2seq.ipynb`.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 结果包含了我们需要的所有信息，以便可视化注意力模式。你可以在笔记本`Ch09-Seq2seq-Models/ch09_seq2seq.ipynb`中看到用于创建以下可视化效果的实际代码。
- en: 'Let’s take a few samples from our test dataset and visualize attention patterns
    exhibited by the model (*Figure 9.16*):'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从测试数据集中选取几个样本，并可视化模型展现的注意力模式（*图9.16*）：
- en: '![](img/B14070_09_16.png)![](img/B14070_09_16.1.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_16.png)![](img/B14070_09_16.1.png)'
- en: 'Figure 9.16: Visualizing attention patterns for a few test inputs'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16：可视化几个测试输入的注意力模式
- en: Overall, we’d like to see a heat map that has a roughly diagonal activation
    of energy. This is because both languages have a similar construct in terms of
    the direction of the language. And we can clearly see that in both examples.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们希望看到一个热图，具有大致对角线方向的能量激活。这是因为两种语言在语言结构的方向上具有相似之处。我们可以清楚地看到这一点，在这两个示例中都能看到。
- en: Looking at specific words in the first example, you can see the model focuses
    heavily on *evening* to predict *Abends*, *atmosphere* to predict *Ambiente*,
    and so on. In the second, you see that the model is focusing on the word *free*
    to predict *kostenlosen*, which is German for *free*.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 看第一个示例中的特定单词，你可以看到模型在预测*Abends*时，特别关注*evening*，在预测*Ambiente*时，特别关注*atmosphere*，依此类推。第二个示例中，模型特别关注单词*free*来预测*kostenlosen*，这是德语中*free*的意思。
- en: Next, we discuss how to infer translations from the trained model.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论如何从训练模型中推断翻译。
- en: Inference with NMT
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经机器翻译（NMT）进行推理
- en: 'Inferencing is slightly different from the training process for NMT (*Figure
    9.17*). As we do not have a target sentence at the inference time, we need a way
    to trigger the decoder at the end of the encoding phase. It’s not difficult as
    we have already done the groundwork for this in the data we have. We simply kick
    off the decoder by using `<s>` as the first input to the decoder. Then we recursively
    call the decoder using the predicted word as the input for the next timestep.
    We continue this way until the model:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程与NMT的训练过程略有不同（*图9.17*）。由于在推理时没有目标句子，我们需要一种方法在编码阶段结束时触发解码器。这个过程并不复杂，因为我们已经在数据中为此做好了准备。我们只需通过使用`<s>`作为解码器的第一个输入来启动解码器。然后，我们通过使用预测词作为下一时间步的输入，递归地调用解码器。我们以这种方式继续，直到模型：
- en: Outputs `</s>` as the predicted token or
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出`</s>`作为预测的标记，或者
- en: Reaches a pre-defined sentence length
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到预定义的句子长度
- en: 'To do this, we have to define a new model using the existing weights of the
    training model. This is because our trained model is designed to consume a sequence
    of decoder inputs at once. We need a mechanism to recursively call the decoder.
    Here’s how we can define the inference model:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们必须使用训练模型的现有权重定义一个新的模型。这是因为我们的训练模型设计为一次处理一系列解码器输入。我们需要一个机制来递归地调用解码器。以下是如何定义推理模型：
- en: Define an encoder model that outputs the encoder’s hidden state sequence and
    the last encoder state.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个编码器模型，该模型输出编码器的隐藏状态序列和最后的编码器状态。
- en: Define a new decoder that takes a decoder input having a time dimension of 1
    and a new input, to which we will input the previous hidden state value of the
    decoder (initialized with the encoder’s last state).
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个新的解码器，接受具有时间维度为1的解码器输入，并输入一个新的值，我们将把解码器的上一个隐藏状态值（由编码器的最后状态初始化）输入到其中。
- en: 'With that, we can start feeding data to generate predictions as follows:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以开始输入数据，生成如下预测：
- en: Preprocess *x*[s] as in data processing
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理*x*[s]，就像在数据处理中的做法
- en: Feed *x*[s] into ![](img/B14070_09_066.png) and calculate the encoder’s state
    sequence and the last state *h* conditioned on *x*[s]
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将*x*[s]输入![](img/B14070_09_066.png)，并计算编码器的状态序列和最后的状态*h*，条件化在*x*[s]上。
- en: Initialize ![](img/B14070_09_067.png) with *h*
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*h*初始化![](img/B14070_09_067.png)
- en: For the initial prediction step, predict ![](img/B14070_09_068.png) by conditioning
    the prediction on ![](img/B14070_09_069.png) as the first word and *h*
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于初始预测步骤，通过将预测条件化在![](img/B14070_09_069.png)作为第一个词和*h*上来预测![](img/B14070_09_068.png)
- en: For subsequent time steps, while ![](img/B14070_09_070.png) and predictions
    haven’t reached a pre-defined length threshold, predict ![](img/B14070_09_071.png)
    by conditioning the prediction on ![](img/B14070_09_072.png) and *h*
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于后续时间步，当![](img/B14070_09_070.png)和预测尚未达到预定义的长度阈值时，通过将预测条件化在![](img/B14070_09_072.png)和*h*上来预测![](img/B14070_09_071.png)
- en: 'This produces the translation given an input sequence of text:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成给定输入文本序列的翻译：
- en: '![](img/B14070_09_17.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_09_17.png)'
- en: 'Figure 9.17: Inferring from an NMT'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17：从NMT推断
- en: The actual code can be found in the notebook `Ch09-Seq2seq-Models/ch09_seq2seq.ipynb`.
    We will leave it for the reader to study the code and understand the implementation.
    We conclude our discussion about machine translation here. Now, let’s briefly
    examine another application of sequence-to-sequence learning.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 实际代码可以在笔记本`Ch09-Seq2seq-Models/ch09_seq2seq.ipynb`中找到。我们将留给读者去研究代码并理解实现。我们将在这里结束关于机器翻译的讨论。接下来，我们简要地看看序列到序列学习的另一个应用。
- en: Other applications of Seq2Seq models – chatbots
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Seq2Seq模型的其他应用 – 聊天机器人
- en: One other popular application of sequence-to-sequence models is in creating
    chatbots. A chatbot is a computer program that is able to have a realistic conversation
    with a human. Such applications are very useful for companies with a huge customer
    base. Responding to customers asking basic questions for which answers are obvious
    accounts for a significant portion of customer support requests. A chatbot can
    serve customers with basic concerns when it is able to find an answer. Also, if
    the chatbot is unable to answer a question, the request gets redirected to a human
    operator. Chatbots can save a lot of the time that human operators spend answering
    basic concerns and let them attend to more difficult tasks.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的序列到序列模型应用是创建聊天机器人。聊天机器人是一种能够与人类进行真实对话的计算机程序。这类应用对于拥有庞大客户群体的公司非常有用。应对客户提出的一些显而易见的基础问题，占据了客户支持请求的很大一部分。当聊天机器人能够找到答案时，它可以为客户解答这些基础问题。此外，如果聊天机器人无法回答某个问题，用户的请求会被转发给人工操作员。聊天机器人可以节省人工操作员回答基础问题的时间，让他们专注于更难处理的任务。
- en: Training a chatbot
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练聊天机器人
- en: So, how can we use a sequence-to-sequence model to train a chatbot? The answer
    is quite straightforward as we have already learned about the machine translation
    model. The only difference would be how the source and target sentence pairs are
    formed.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何使用序列到序列（sequence-to-sequence）模型来训练一个聊天机器人呢？答案其实非常直接，因为我们已经学习过机器翻译模型。唯一的区别在于源句子和目标句子对的形成方式。
- en: In the NMT system, the sentence pairs consist of a source sentence and the corresponding
    translation in a target language for that sentence. However, in training a chatbot,
    the data is extracted from the dialogue between two people. The source sentences
    would be the sentences/phrases uttered by person A, and the target sentences would
    be the replies to person A made by person B. One dataset that can be used for
    this purpose consists of movie dialogues between people and is found at [https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在NMT系统中，句子对由源句子和该句子在目标语言中的相应翻译组成。然而，在训练聊天机器人时，数据是从两个人之间的对话中提取的。源句子将是A人物所说的句子/短语，目标句子则是B人物对A人物所作出的回复。可以用于此目的的一个数据集是电影对白数据集，包含了人们之间的对话，可以在[https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)找到。
- en: 'Here are links to several other datasets for training conversational chatbots:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些其他数据集的链接，用于训练对话型聊天机器人：
- en: 'Reddit comments dataset: [https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddit评论数据集：[https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)
- en: 'Maluuba dialogue dataset: [https://datasets.maluuba.com/Frames](https://datasets.maluuba.com/Frames)'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maluuba 对话数据集: [https://datasets.maluuba.com/Frames](https://datasets.maluuba.com/Frames)'
- en: 'Ubuntu dialogue corpus: [http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/)'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ubuntu 对话语料库: [http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/)'
- en: 'NIPS conversational intelligence challenge: [http://convai.io/](http://convai.io/)'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NIPS 对话智能挑战: [http://convai.io/](http://convai.io/)'
- en: 'Microsoft Research social media text corpus: [https://tinyurl.com/y7ha9rc5](https://tinyurl.com/y7ha9rc5)'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Microsoft Research 社交媒体文本语料库: [https://tinyurl.com/y7ha9rc5](https://tinyurl.com/y7ha9rc5)'
- en: '*Figure 9.18* shows the similarity of a chatbot system to an NMT system. For
    example, we train a chatbot with a dataset consisting of dialogues between two
    people. The encoder takes in the sentences/phrases spoken by one person, where
    the decoder is trained to predict the other person’s response. After training
    in such a way, we can use the chatbot to provide a response to a given question:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.18* 显示了聊天机器人系统与神经机器翻译系统的相似性。例如，我们使用由两人对话组成的数据集来训练聊天机器人。编码器接收一个人说的句子/短语，解码器被训练以预测另一个人的回应。通过这种方式训练后，我们可以使用聊天机器人来回答给定的问题：'
- en: '![Training a chatbot](img/B14070_09_18.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![训练聊天机器人](img/B14070_09_18.png)'
- en: 'Figure 9.18: Illustration of a chatbot'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.18: 聊天机器人示意图'
- en: Evaluating chatbots – the Turing test
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估聊天机器人 – 图灵测试
- en: After building a chatbot, one way to evaluate its effectiveness is using the
    Turing test. The Turing test was invented by Alan Turing in the 1950s as a way
    of measuring the intelligence of a machine. The experiment settings are well suited
    for evaluating chatbots. The experiment is set up as follows.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 构建聊天机器人后，评估其效果的一种方法是使用图灵测试。图灵测试由艾伦·图灵在上世纪50年代发明，用于衡量机器的智能程度。实验设置非常适合评估聊天机器人。实验设置如下：
- en: 'There are three parties involved: an evaluator (that is, a human) (**A**),
    another human (**B**), and a machine (**C**). The three of them sit in three different
    rooms so that none of them can see the others. The only communication medium is
    text, which is typed into a computer by one party, and the receiver sees the text
    on a computer on their side. The evaluator communicates with both the human and
    the machine. And at the end of the conversation, the evaluator is to distinguish
    the machine from the human. If the evaluator cannot make the distinction, the
    machine is said to have passed the Turing test. This setup is illustrated in *Figure
    9.19*:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '有三个参与方: 一个评估者（即人类）(**A**), 另一个人类(**B**), 和一个机器人(**C**). 他们三个坐在三个不同的房间里，以便彼此互不见面。他们唯一的交流媒介是文本，一方将文本输入计算机，接收方在自己的计算机上看到文本。评估者与人类和机器人进行交流。在对话结束时，评估者需要区分机器人和人类。如果评估者无法做出区分，机器人被视为通过了图灵测试。这个设置如图
    *9.19* 所示：'
- en: '![Evaluating chatbots – Turing test](img/B14070_09_19.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![评估聊天机器人 – 图灵测试](img/B14070_09_19.png)'
- en: 'Figure 9.19: The Turing test'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.19: 图灵测试'
- en: This concludes the section on other applications of Seq2Seq models. We briefly
    discussed the application of creating chatbots, which is a popular use for sequential
    models.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分介绍了 Seq2Seq 模型的其他应用。我们简要讨论了创建聊天机器人的应用，这是序列模型的一种流行用途。
- en: Summary
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we talked in detail about NMT systems. Machine translation
    is the task of translating a given text corpus from a source language to a target
    language. First, we talked about the history of machine translation briefly to
    build a sense of appreciation for what has gone into machine translation for it
    to become what it is today. We saw that today, the highest-performing machine
    translation systems are actually NMT systems. Next, we solved the NMT task of
    generating English to German translations. We talked about the dataset preprocessing
    that needs to be done, and extracting important statistics about the data (e.g.
    sequence lengths). We then talked about the fundamental concept of these systems
    and decomposed the model into the embedding layer, the encoder, the context vector,
    and the decoder. We also introduced techniques like teacher forcing and Bahdanau
    attention, which are aimed at improving model performance. Then we discussed how
    training and inference work in NMT systems. We also discussed a new metric called
    BLEU and how it is used to measure performance on sequence-to-sequence problems
    like machine translation.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们详细讨论了 NMT 系统。机器翻译是将给定的文本语料从源语言翻译到目标语言的任务。首先，我们简要回顾了机器翻译的历史，以便培养对机器翻译发展的理解，帮助我们认识到它今天的成就。我们看到，今天表现最好的机器翻译系统实际上是
    NMT 系统。接下来，我们解决了从英语到德语翻译的 NMT 任务。我们讨论了数据预处理工作，包括提取数据的重要统计信息（例如序列长度）。然后我们讲解了这些系统的基本概念，并将模型分解为嵌入层、编码器、上下文向量和解码器。我们还介绍了像教师强制（teacher
    forcing）和巴赫达诺注意力（Bahdanau attention）等技术，旨在提高模型性能。接着我们讨论了 NMT 系统的训练和推理过程。我们还讨论了一种名为
    BLEU 的新指标，以及它是如何用来衡量机器翻译等序列到序列问题的表现的。
- en: 'Finally, we briefly talked about another popular application of sequence-to-sequence
    learning: chatbots. Chatbots are machine learning applications that are able to
    have realistic conversations with a human and even answer questions. We saw that
    NMT systems and chatbots work similarly, and only the training data is different.
    We also discussed the Turing test, which is a qualitative test that can be used
    to evaluate chatbots.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要讨论了序列到序列学习的另一个热门应用：聊天机器人。聊天机器人是能够与人类进行真实对话甚至回答问题的机器学习应用。我们看到，NMT 系统和聊天机器人工作原理相似，唯一不同的是训练数据。我们还讨论了图灵测试，这是一种可以用来评估聊天机器人的定性测试。
- en: 'In the next chapter, we will look at a new type of model that came out in 2016
    and is leading both the NLP and computer vision worlds: the Transformer.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一种在 2016 年推出的新型模型，它在 NLP 和计算机视觉领域都处于领先地位：Transformer。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过 1000 名成员一起学习：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
