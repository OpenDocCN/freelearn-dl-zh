- en: Reinforcement Learning in Game Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 博弈应用中的强化学习
- en: Games have always been a phenomenon of human culture where people manifest intelligence,
    interaction, and competition. But games are also an important theoretical paradigm
    in logic, **artificial intelligence** (**AI**), computer science, linguistics,
    biology, and lately, increasingly in the social sciences and in psychology. Games,
    especially strategy games, offer reinforcement learning algorithms an ideal and
    privileged environment for testing, as they can act as models for real problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏一直是人类文化中的一种现象，人们通过它展现智慧、互动和竞争。但游戏也是逻辑学、**人工智能**（**AI**）、计算机科学、语言学、生物学以及最近越来越多地出现在社会科学和心理学中的一个重要理论范式。游戏，尤其是战略游戏，为强化学习算法提供了理想的测试环境，因为它们可以作为实际问题的模型。
- en: In this chapter, we will learn how to use reinforcement learning algorithms
    to address a problem in game theory. By the end of the chapter, we will have learned
    the fundamental concepts of game theory. We will also learn how to install and
    configure the OpenAI Gym library, understand how the OpenAI Gym library works,
    and learn how to use Q-learning to solve game problems. Apart from that, we will
    understand how to make a learning and testing phase, and learn how to develop
    OpenAI Gym applications using R.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用强化学习算法解决博弈论中的问题。到本章结束时，我们将掌握博弈论的基本概念。我们还将学习如何安装和配置 OpenAI Gym
    库，了解 OpenAI Gym 库的工作原理，并学习如何使用 Q 学习解决游戏问题。除此之外，我们还将了解如何进行学习和测试阶段，并学习如何使用 R 开发
    OpenAI Gym 应用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding game theory essentials
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解博弈论的基础
- en: Exploring game theory applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索博弈论的应用
- en: Playing the tic-tac-toe game
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩井字棋游戏
- en: Introducing the OpenAI Gym library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 OpenAI Gym 库
- en: Robot control system using the FrozenLake environment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FrozenLake 环境的机器人控制系统
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码在实际应用中的表现：
- en: '[http://bit.ly/2tffzMD](http://bit.ly/2tffzMD)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2tffzMD](http://bit.ly/2tffzMD)'
- en: Understanding game theory essentials
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解博弈论的基础
- en: Game theory is a mathematical science that studies and analyzes the individual
    decisions of a subject in situations of conflict, or strategic interaction, with
    other rival subjects aimed at the maximum profit of each subject. In such situations,
    the decisions of one can influence the results achieved by the other(*s*), and
    vice versa, according to a feedback mechanism, by seeking competitive and or cooperative
    solutions through models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论是一门数学科学，研究和分析主体在冲突或战略互动情况下的个体决策，这些决策涉及与其他竞争主体的互动，目的是最大化每个主体的利润。在这种情况下，一个主体的决策可能会影响另一个（或多个）主体的结果，反之亦然，依据反馈机制，通过模型寻求竞争性和/或合作性解决方案。
- en: The theory of games has its distant origins in 1654, following correspondence
    between Blaise Pascal and Pierre de Fermat on the calculation of probabilities
    for gambling. The expression **game theory** was first used by Emil Borel in the
    1920s. Borel developed the **Théorie des jeux**, a zero-sum game with two players,
    and tried to find a solution known as Von Neumann's concept of solving a zero-sum
    game. It is generally acknowledged that the release of the book *Theory of Games
    and Economic Behavior*, by John von Neumann and Oskar Morgenstern in 1944, marked
    the birth of modern game theory, although other authors (such as Ernst Zermelo
    and Armand Borel) had written about game theory.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论的理论起源可以追溯到1654年，那时布莱兹·帕斯卡（Blaise Pascal）与皮埃尔·德·费马（Pierre de Fermat）就赌博概率的计算进行书信往来。**博弈论**一词最早由埃米尔·博雷尔（Emil
    Borel）在1920年代使用。博雷尔发展了**博弈理论**，提出了一个包含两名玩家的零和博弈，并试图寻找一种解决方案，即冯·诺依曼的零和博弈解决概念。普遍认为，约翰·冯·诺依曼（John
    von Neumann）和奥斯卡·摩根斯坦（Oskar Morgenstern）于1944年发布的《博弈论与经济行为》一书标志着现代博弈论的诞生，尽管其他作者（如恩斯特·泽梅洛（Ernst
    Zermelo）和阿尔芒·博雷尔（Armand Borel））也曾讨论过博弈论。
- en: The idea of these two scholars can be described informally as an attempt to
    describe human behavior mathematically in the cases in which the interaction between
    men involves the winning, or dividing, of some kind of resource. The most famous
    scholar to have subsequently dealt with the theory of games, in particular with
    regard to **non-cooperative games**, is the mathematician John Forbes Nash jr.,
    to whom the Ron Howard film *A Beautiful Mind *is dedicated.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这两位学者的思想可以非正式地描述为试图在涉及资源的赢得或分配的情境下，用数学描述人类行为。后来的著名学者，特别是在**非合作博弈**方面有深入研究的，是数学家约翰·福布斯·纳什（John
    Forbes Nash jr.），他的事迹被荣·霍华德的电影《美丽心灵》所呈现。
- en: Eight Nobel prizes in economics were awarded to scholars who dealt with game
    theory. A Crafoord Prize has also been awarded to John Maynard Smith, a long-time
    distinguished biologist, geneticist and professor at the University of Sussex,
    for his contribution to this field.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 曾有八位诺贝尔经济学奖获得者处理过博弈论问题。约翰·梅纳德·史密斯（John Maynard Smith），一位长期卓越的生物学家、遗传学家以及萨塞克斯大学教授，也因其在这一领域的贡献获得了克拉福德奖。
- en: In the following sections, we will introduce the concepts underlying game theory
    and then analyze the main types of games faced by researchers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍博弈论的基本概念，然后分析研究人员所面临的主要博弈类型。
- en: Basic concepts of game theory
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 博弈论的基本概念
- en: Game theory has victory as its main objective. Everyone must be aware of the
    rules of the game and be aware of the consequences of every single move. The set
    of moves that an individual intends to do is called a strategy. Depending on the
    strategies adopted by all the players, each one receives a pay-off according to
    an adequate unit of measurement. The reward can be positive, negative, or null.
    A game is called a **constant sum** if, for each player's payout, there is a corresponding
    loss for others. A **zero-sum** game between two players represents a situation
    in which the reward is paid from one player to another. The strategy to follow
    is satisfactory for all players; otherwise, it is necessary to calculate and maximize
    the player's mathematical hope or expected value, which is the weighted average
    of the possible rewards, each weighed for the respective probabilities of the
    event.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论的主要目标是胜利。每个人必须了解游戏规则并意识到每一步的后果。个人打算做出的所有行动构成了一种策略。根据所有玩家采取的策略，每个玩家根据适当的衡量单位获得报酬。报酬可以是正面、负面或无。若每个玩家的支付与其他玩家的损失相对应，则该游戏称为**常数和**游戏。两名玩家之间的**零和**游戏代表了一个奖励从一方支付给另一方的情况。若策略对所有玩家都是满意的，则称为均衡策略；否则，就需要计算并最大化玩家的数学期望或预期值，即可能奖励的加权平均值，每个奖励根据该事件的概率进行加权。
- en: In a game, there are one or more contenders who try to win the game, that is,
    to maximize their winnings. The winnings are defined by a rule that establishes
    quantitatively what the contenders win according to their behavior. This function
    is called a function of payments. Each player can undertake a finite-infinite
    number of actions or decisions that determine a strategy. Each strategy is characterized
    by a consequence for the player who has adopted it and which can be a reward (positive/negative).
    The result of the game is completely determined by the sequence of their strategies
    and the strategies adopted by the other players.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个游戏中，有一个或多个竞争者试图赢得比赛，也就是最大化他们的收益。收益由一个规则定义，量化地规定了竞争者根据他们的行为获得的奖励。这种函数叫做支付函数。每个玩家可以采取有限或无限的行动或决策，从而确定一种策略。每种策略都有一个结果，表现为采纳该策略的玩家所获得的后果，可能是奖励（正面/负面）。游戏的结果完全由他们策略的顺序以及其他玩家所采取的策略决定。
- en: 'How do you characterize the result of the game for each player? If you measure
    the consequence of a strategy in terms of reward, each strategy can be matched
    with a value: a negative value will indicate a payment to the opponent, such as
    a penalty; while a positive value will indicate winnings, that is, the collection
    of a prize. The gain or loss due to the generic player associated with his or
    her strategy and the strategies taken at a given moment by all the remaining players
    is expressed by the monetary value indicated by the payment function.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如何为每个玩家表征游戏的结果呢？如果你以奖励来衡量策略的后果，那么每个策略都可以匹配一个数值：负值表示支付给对手，如罚款；而正值则表示收益，即获得奖励。与玩家所采取的策略以及其他所有玩家在某一时刻所采取的策略相关的增益或损失，通常通过支付函数所指示的货币值来表示。
- en: The decisions taken by a player naturally collide, or are in accordance with
    the decisions made by the other players, and from such situations derive various
    types of games.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家所做的决策自然会相互碰撞，或者与其他玩家所做的决策相一致，从而衍生出各种类型的博弈。
- en: A useful tool to represent the interactions between two players, two companies,
    or two individuals is a double-entry decision matrix or table. This decision table
    shows the strategies and winnings of a game conducted by two players. The decision
    matrix is therefore a representation through which we catalog all the possible
    results of the interactions between players, and we assign the value of the reward
    that in each situation competes to each player. Another form of representation
    concerns the sequence with which each decision is taken, or the actions are conducted.
    This characteristic of each game can be described by means of a tree graph, representing
    every possible combination of how the contenders play from the initial state to
    the final states where the winnings are distributed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的工具来表示两个玩家、两家公司或两个个人之间的互动是一个双重决策矩阵或表格。这个决策表显示了由两个玩家进行的博弈的策略和获胜情况。因此，决策矩阵是一个表示工具，通过它我们可以列出玩家之间互动的所有可能结果，并为每种情形分配奖励值，该奖励值在每种情况下竞争给予每个玩家。另一种表示形式涉及到每个决策的采取顺序，或者行动的执行方式。游戏的这一特征可以通过树形图来描述，树形图表示从初始状态到最终状态之间，竞争者的每一个可能组合，最终在这些状态中分配奖励。
- en: 'To describe a strategic situation, four basic elements are required:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 描述一个战略情境需要四个基本要素：
- en: '**Players**: The decision makers in the game (who is involved?)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家**： 游戏中的决策者（谁参与？）'
- en: '**Actions**: The possible actions, or moves, that players can choose from (what
    can they do?)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动**： 玩家可以选择的可能行动或动作（他们能做什么？）'
- en: '**Strategies**: The action plans of the players (what are they going to do?)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**： 玩家们的行动计划（他们打算做什么？）'
- en: '**Winnings**: The possible gains that players get (what do they earn?)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收益**： 玩家获得的可能收益（他们得到了什么？）'
- en: A strategy is therefore a complete and contingent plan, or decision-making decision,
    that specifies how the player must act in any possible circumstances in which
    he may be called upon to decide. Being a complete contingent plan, a strategy
    often defines which action a player must choose in circumstances that may not
    be achieved during the game. In the following section, the games will be classified
    and a short description of each topic will be proposed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，策略是一个完整且有条件的计划，或决策决策，明确规定了玩家在任何可能需要做出决策的情况下必须采取的行动。作为一个完整的有条件计划，策略通常定义了玩家在游戏中可能无法实现的情境下应该选择哪种行动。在接下来的部分中，游戏将被分类，并且每个主题会有简短的描述。
- en: Game types
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏类型
- en: 'Games can be classified according to different paradigms, such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏可以根据不同的范式进行分类，以下是其中几种：
- en: Cooperation
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合作
- en: Symmetry
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称性
- en: Sum
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总和
- en: Sequencing
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序
- en: In the following sections, we will see a short description of these topics.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将简要描述这些主题。
- en: Cooperative game
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合作博弈
- en: A cooperative game is presented when the interests of the players are not in
    direct opposition to each other, but there is a commonality of interests. Players
    pursue a common goal, at least for the duration of the game; some of them may
    tend to associate to improve their **pay-off**. The guarantee is given by the
    binding agreements. What is the mathematical representation of a shared interest?
    The concept of the union of individual interests in a coalition or alliance is
    expressed by the definition of essential play; while the value of a generic coalition
    is measured by a function called a characteristic function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当玩家的利益不直接对立，而是具有共同利益时，便呈现出合作博弈。玩家们追求一个共同的目标，至少在游戏进行期间如此；其中一些玩家可能会倾向于联合起来以提高他们的**回报**。保证由绑定协议提供。那么，如何用数学来表示共同利益呢？个人利益在联盟或联合中的结合体的概念通过定义本质性博弈来表达；而一个普通联盟的价值则通过一个称为特征函数的函数来衡量。
- en: In contrast, in non-cooperative games, also called **competitive games**, players
    cannot enter into binding agreements (even by regulation), regardless of their
    objectives. The solution given by John Nash, with his *Equilibrium of Nash*, applies
    to this category, and it is probably the most famous notion of the whole theory,
    thanks to its vast field of applicability. The criterion of rational behavior
    adopted in non-cooperative games is individual and is called the maximum strategy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在非合作博弈中，也称为**竞争性博弈**，玩家不能达成具有约束力的协议（即使是通过规定）。约翰·纳什提出的*纳什均衡*适用于这一类博弈，它可能是整个理论中最著名的概念，因为它有广泛的应用领域。在非合作博弈中采用的理性行为标准是个体化的，称为最大策略。
- en: Symmetric game
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对称博弈
- en: In a symmetrical game, the profits deriving from the adoption of a strategy
    depend only on the other strategies employed, not by those who are playing them.
    If players' identities can be changed without changing the payoff, then a game
    is symmetrical.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在对称博弈中，采用某一策略所获得的利润仅取决于其他玩家所采用的策略，而与采用该策略的玩家无关。如果玩家的身份可以改变而不改变支付结果，则该博弈是对称的。
- en: In contrast, in asymmetric games there are no identical series of strategies
    for both players. It is possible, however, that a game has identical strategies
    for both players, but that it is asymmetric.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在不对称博弈中，两名玩家没有相同的策略系列。然而，也有可能一个博弈对于两名玩家来说具有相同的策略，但它依然是不对称的。
- en: Zero-sum game
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零和博弈
- en: 'Zero-sum games are a special case of constant-sum games in which the constant
    is zero. Zero-sum games model all the conflicting situations in which the contrast
    of the two players is total: the winning of a player coincides exactly with the
    loss of the other. In other words, the sum of the winnings of the two contenders
    according to the strategies used is always zero. In chess, for example, this means
    that the only three possible results are: victory, defeat, and draw (reward: +1,
    -1, and 0).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 零和博弈是常数和博弈的一个特例，其中常数为零。零和博弈建模了所有对立的情境，其中两名玩家的对抗是完全的：一名玩家的胜利恰好与另一名玩家的失败相对应。换句话说，两个竞争者根据所使用策略所获得的总利润始终为零。例如，在象棋中，这意味着唯一的三种可能结果是：胜利、失败和平局（奖励：+1，-1和0）。
- en: Sequential games
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序博弈
- en: In sequential games, subsequent players retain some knowledge of previous actions.
    This does not mean that they know every action of previous players. For example,
    a player may know that a previous player has not performed a certain action, while
    he does not know which other available actions the first player performed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在顺序博弈中，后续的玩家保留了一些关于前一玩家行为的知识。这并不意味着他们知道前一玩家的所有行动。例如，一名玩家可能知道前一名玩家没有进行某个动作，但不知道第一名玩家做了哪些其他可选动作。
- en: Now we have learned to classify games based on some paradigms. Why is analyzing
    games so important? This is due to the fact that many real-life problems can be
    tackled by deriving the solutions obtained from game theory. In the next section,
    we will see examples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了根据一些范式来分类游戏。为什么分析博弈如此重要？这是因为许多现实生活中的问题可以通过博弈论得出的解决方案来应对。在下一节中，我们将看到一些例子。
- en: Exploring game theory applications
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索博弈论的应用
- en: 'Game theory has always interested many researchers because of its usefulness
    in the practical field and in all fields of human work, such as the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈论一直吸引着许多研究者，因为它在实际领域和人类各个工作领域中的应用非常有价值，例如以下几个方面：
- en: '**Philosophy**: This has always analyzed game theory because it provides a
    way to clarify the logical difficulty of some philosophers, such as Kant, Rousseau,
    Hobbes, and other social and political theorists.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哲学**：哲学一直在分析博弈论，因为它提供了一种方法来澄清一些哲学家的逻辑难题，例如康德、卢梭、霍布斯以及其他社会政治理论家的难题。'
- en: '**Economy**: Many of the speculations in the business world can be modeled
    using the methodology of game theory. A famous example is that of the similarity
    between the setting of oligopoly prices and the prisoner''s dilemma.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经济学**：商业世界中的许多投机行为可以通过博弈论的方法来建模。一个著名的例子是寡头定价与囚徒困境之间的相似性。'
- en: '**Biology**: Although nature is often considered brutal, there is cooperation
    between many different species. The reason for this coexistence can be modeled
    using game theory.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生物学**：尽管大自然常被认为是残酷的，但许多不同物种之间存在合作。这种共存的原因可以通过博弈论来建模。'
- en: '**AI**: The human being can make decisions based on the environmental stimuli
    he receives. Instead, machines can plan only if programmed with decision lists
    based on several conditions. This limit can be overcome by artificial intelligence
    that can give the machine the ability to make new unplanned decisions from their
    creators. To do this, programs must generate new payoff matrices based on observed
    stimuli and experience.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI**：人类可以根据其所接收的环境刺激做出决策。而机器则只能在编程时根据多种条件的决策列表进行规划。这个限制可以通过人工智能来克服，它赋予机器从创造者那里获取新决策的能力，而不是仅依赖预先计划的规则。为此，程序必须根据观察到的刺激和经验生成新的回报矩阵。'
- en: In the following section, we will analyze a widespread game and see how to deal
    with reinforcement learning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将分析一个广泛流行的游戏，并学习如何使用强化学习进行处理。
- en: Playing the tic-tac-toe game
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩井字游戏
- en: 'The game of tic-tac-toe is perfect as a first example of a game solved with
    the use of reinforcement learning: In fact, compared to other strategy games,
    it has a few simple rules. Furthermore, it is relatively easy to program and,
    since a game can last up to nine moves, the training of an evaluation function
    is extremely rapid. The tic-tac-toe is a game with perfect information for two
    players, where each player is assigned a symbol to play with. The symbols usually
    used are the cross and the circle. The game is started by the player who uses
    the cross.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 井字游戏是用强化学习解决的第一个游戏的完美例子：实际上，与其他策略游戏相比，它有一些简单的规则。此外，它相对容易编程，而且因为游戏最多只进行九轮，评估函数的训练非常快速。井字游戏是一个完美信息的双人游戏，每个玩家被分配一个符号来进行游戏。通常使用的符号是叉和圈。游戏由使用叉号的玩家开始。
- en: 'The game grid has a 3x3 structure and presents nine initially empty cells as
    follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏网格具有3x3的结构，初始时呈现九个空单元格，如下所示：
- en: '![](img/d3cf2eff-9c0b-47a8-a384-34b51a504344.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3cf2eff-9c0b-47a8-a384-34b51a504344.png)'
- en: In turn, players choose an empty cell and draw their own symbol. The player
    who manages to place three of his symbols in a horizontal, vertical, or diagonal
    line wins. If the game grid is filled without any of the players having succeeded
    in completing a straight line of three symbols, the game ends in a draw. So, if
    played correctly, tic-tac-toe will end in a draw, making the game useless.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家轮流选择一个空格，并绘制自己的符号。成功将三个符号放置在水平、垂直或对角线上的玩家获胜。如果游戏网格填满且没有任何玩家成功完成三符号的直线，游戏以平局结束。因此，如果正确玩法，井字游戏最终会以平局告终，使得游戏毫无意义。
- en: In the following section, we will introduce the `tictactoe` package to play
    the game using the Q-learning algorithm.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍使用Q-learning算法玩游戏的`tictactoe`包。
- en: The tictactoe package
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tictactoe包
- en: To address the tic-tac-toe game, we will use the `tictactoe` package available
    on the CRAN website. This package implements the tic-tac-toe game to play on a
    console, either with human or AI players. Various levels of AI players are trained
    through the Q-learning algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理井字游戏，我们将使用CRAN网站上可用的`tictactoe`包。该包实现了一个控制台上的井字游戏，可以与人类或AI玩家对战。各种等级的AI玩家通过Q-learning算法进行训练。
- en: 'The following table gives some information about this package:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了有关该包的一些信息：
- en: '| Package | `tictactoe` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 包 | `tictactoe` |'
- en: '| Date | 2017-05-26 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2017-05-26 |'
- en: '| Version | 0.2.2 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 0.2.2 |'
- en: '| Title | Tic-Tac-Toe Game |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 井字游戏 |'
- en: '| Author | Kota Mori |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 森本光太 |'
- en: With the help of this package, we will play a first part with the computer to
    highlight the features of the game, and then we will train an artificial agent
    to play a game by following the best policy to get the maximum number of wins.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在该包的帮助下，我们将与计算机进行第一次对局，以突出游戏的特性，然后我们将训练一个人工代理，按照最佳策略进行游戏，以获得最多的胜利。
- en: Playing a tic-tac-toe game
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩井字游戏
- en: 'To begin, we will see how to set the game environment, and we will start a
    first game:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看到如何设置游戏环境，并开始第一次游戏：
- en: 'First we have to import the library:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入库：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we can start the tic-tac-toe game on the R console using the `ttt()` function
    as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以在R控制台上使用`ttt()`函数启动井字游戏，如下所示：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意以下事项：
- en: '`ttt_human()` creates a human tic-tac-toe player; if we wish we can also set
    a name using the name attribute (for example `name = "GIUSEPPE"`).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ttt_human()`创建一个人类井字游戏玩家；如果我们愿意，还可以通过name属性设置名字（例如`name = "GIUSEPPE"`）。'
- en: '`ttt_random()` sets a random player, which merely places the opposite symbol
    (the circle) in one of the places available in a completely random way.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ttt_random()`设置一个随机玩家，该玩家仅随机地将对方的符号（圆圈）放置在一个空位上。'
- en: 'The console returns the following grid:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台返回以下网格：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As anticipated the game is based on a simple 3 x 3 grid; to facilitate the identification
    of a cell, the columns are named with the letters A, B, C, while the rows with
    the numbers 1,2,3\. This means that the first cell on the top left will be identified
    with the symbol A1\. The last line printed on the console invites player 1 (`GIUSEPPE`)
    to make his move.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，游戏基于一个简单的3x3网格；为了方便识别单元格，列用字母A、B、C命名，而行用数字1、2、3表示。这意味着左上角的第一个单元格将被标识为A1。控制台打印的最后一行邀请玩家1（`GIUSEPPE`）进行下一步操作。
- en: 'Let''s start by placing the X in cell A1, the following result is returned:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先在单元格A1中放置X，随后返回以下结果：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see in the preceding code block, the X has been positioned correctly
    in the upper-left cell, then the player 2 (computer) has placed his symbol (O)
    in a free position in a random way. Once again the last row invites player 1 to
    make the next move. We can proceed this way until the game is completed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的代码块中看到的，X已正确放置在左上角的单元格中，然后玩家2（计算机）随机地将符号（O）放置在一个空位上。最后一行再次邀请玩家1进行下一步操作。我们可以这样进行，直到游戏结束。
- en: 'Three results are available—0, 1, and 2, which indicate the draw, the victory
    of player 1, and the victory of player 2, respectively, for example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种结果可供选择——0、1和2，分别表示平局、玩家1获胜和玩家2获胜。例如：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, I won, but only because the computer put its symbols at random
    without following a strategy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我赢了，但仅仅是因为计算机随机地放置了它的符号，并没有遵循任何策略。
- en: Training the agent using Q-learning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Q-learning训练代理
- en: 'We can train the agent to follow a strategy. Let''s see how:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以训练代理遵循一种策略。我们来看看怎么做：
- en: 'To start we can simulate games in order to check the results obtained by two
    artificial agents when they play with each other:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了开始，我们可以模拟游戏，检查两个人工智能代理互相对弈时得到的结果：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the preceding code, the following functions have been used:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，使用了以下函数：
- en: '`ttt_ai()`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ttt_ai()`'
- en: '`ttt_simulate`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ttt_simulate`'
- en: 'The `ttt_ai()` function creates an artificial tic-tac-toe game player. We have
    not used any arguments, in reality it is possible to use the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`ttt_ai()`函数创建了一个人工智能的井字棋玩家。我们没有使用任何参数，实际上可以使用以下参数：'
- en: '`name`: Player name.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：玩家名称。'
- en: '`level`: The AI strength must be an integer from 0 (weakest) to 5 (strongest).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level`：AI强度必须是从0（最弱）到5（最强）之间的整数。'
- en: The `level` argument defines the effectiveness of the agent we are creating;
    from 0 to 5, its skill in game management increase, making the opponent's game
    more difficult. In the previous instructions, we already created an artificial
    agent using the `ttt_random ()` function that represents an alias of the  `ttt_ai
    ()` function in which the level of the agent is set by default equal to 0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`level`参数定义了我们创建的代理的有效性；从0到5，代理的游戏管理技能逐渐提高，使得对手的游戏变得更加困难。在之前的指令中，我们已经使用`ttt_random()`函数创建了一个人工智能代理，它是`ttt_ai()`函数的别名，其中代理的级别默认设置为0。'
- en: The `ttt_ai ()` function returns an object to which the `getmove ()` function
    is associated; this function accepts an object of the `ttt_game` type, and returns
    an optimal move using the political function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`ttt_ai()`函数返回一个对象，该对象与`getmove()`函数关联；此函数接受一个`ttt_game`类型的对象，并使用策略函数返回一个最优的动作。'
- en: A `ttt_ai` object has the value and policy functions. The value function associates
    a game state with the evaluation from the point of view of the first player. The
    political function associates a game state with a set of optimal actions obtained
    from the evaluation of the value function. These functions are trained through
    an algorithm based on Q-learning that we discussed extensively in [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml),
    *Temporal Difference Learning*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`ttt_ai`对象包含值函数和策略函数。值函数将一个游戏状态与从第一个玩家视角的评估相关联。策略函数将一个游戏状态与通过评估值函数获得的一组最优动作相关联。这些函数通过基于Q-learning的算法进行训练，我们在[第7章](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml)中详细讨论了*时序差分学习*。
- en: 'The second function used, I refer to the `ttt_simulate()` function, simulates
    a tic-tac-toe game between two artificial agents. The following arguments are
    passed:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的第二个函数是`ttt_simulate()`，它模拟了两个人工智能代理之间的井字棋游戏。传入的参数如下：
- en: '`player1`, `player2`: Artificial players to simulate'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`player1`, `player2`：用于模拟的人工玩家。'
- en: '`N`: Number of simulation games'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`：模拟游戏的次数。'
- en: 'In addition to these, the following additional arguments are available:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除这些外，还可以使用以下附加参数：
- en: '`verbose`: If true, shows a progress report.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`：如果为真，则显示进度报告。'
- en: '`showboard`: If true, a game transition is displayed.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`showboard`：如果为真，则显示游戏过渡。'
- en: '`pauseif`: Pauses the simulation when specified results occur. This can be
    useful for exploratory purposes.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pauseif`：当出现指定结果时暂停模拟。这对探索性目的很有用。'
- en: The function returns a vector of integers with the results of the simulations
    carried out. In practice, each simulation will return a value between 0,1, and
    2 that, as we have already said, means a draw, a victory for player 1, or a victory
    for player 2, respectively.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回一个整数向量，表示所执行模拟的结果。实际上，每次模拟都会返回一个值，介于 0、1 和 2 之间，分别代表平局、玩家 1 胜利或玩家 2 胜利，正如我们已经提到的那样。
- en: 'We can verify what we have said using the `str()` function, which returns a
    compact view of the internal structure of an object R:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `str()` 函数验证我们所说的内容，该函数返回一个 R 对象内部结构的紧凑视图：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following result is returned:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can do more; for example, we can verify the occurrences of the three game
    results in the whole simulation using the `prop.table()` function as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以做更多的事；例如，我们可以使用 `prop.table()` 函数验证整个模拟中的三种游戏结果的发生情况，方法如下：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This function accepts a table as an argument and calculates the proportions
    of the data it contains. The following results are returned:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此函数接受一个表格作为参数，并计算其包含的数据的比例。返回以下结果：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this way, we can see that in the performed simulation player 1 is the one
    who wins more (51%) than player 2 (37%), and a draw is repeated a decidedly lower
    number of times (12%).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以看到，在进行的模拟中，玩家 1 获胜的次数更多（51%），而玩家 2 获胜的次数较少（37%），平局的次数明显较低（12%）。
- en: 'Now, we repeat the experiment, but this time we will try to improve one of
    the two players through training based on the Q learning algorithm. As already
    done, we first create the two agents:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们重复实验，但这次我们将通过基于 Q 学习算法的训练来提高两个玩家之一的表现。如同之前的做法，我们首先创建两个代理：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After doing this, we focus on player 4 trying to improve his performance through
    a training phase in which he will learn to follow the best strategy:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成此操作后，我们将重点放在玩家 4 上，尝试通过训练阶段提高他的表现，在这个阶段他将学习遵循最佳策略：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `ttt_qlearn()` function trains a tic-tac-toe agent through Q-learning.
    The following arguments are available:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`ttt_qlearn()` 函数通过 Q-learning 训练井字棋代理。以下参数可用：'
- en: '`player`: Artificial player to train.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`player`：待训练的人工玩家。'
- en: '`N`: Number of episodes.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`：回合数。'
- en: '`epsilon`: The fraction of a random exploration move'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`：随机探索动作的比例。'
- en: '`alpha`: Learning rate.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`：学习率。'
- en: '`gamma`: Discount factor.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：折扣因子。'
- en: '`simulate`: If true, conduct simulation during training.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`simulate`：如果为真，则在训练期间进行模拟。'
- en: '`sim_every`: Conduct simulation after this many training games.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sim_every`：在进行此多次训练后进行模拟。'
- en: '`N_sim`: Number of simulation games.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N_sim`：模拟游戏的次数。'
- en: '`verbose`: If true, a progress report is shown.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`：如果为真，则显示进度报告。'
- en: In the Q-learning-based training process, the agent plays against himself to
    update the value function and its policies. The algorithm used is Q-learning with
    epsilon greedy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 Q 学习的训练过程中，代理与自己对弈以更新值函数和策略。所使用的算法是带有 epsilon 贪心策略的 Q 学习。
- en: 'For each state s, the player updates the value function according to the following
    formula:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态 s，玩家根据以下公式更新值函数：
- en: '![](img/99a91547-fe4d-4dd7-a5a8-21469f1dc818.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99a91547-fe4d-4dd7-a5a8-21469f1dc818.png)'
- en: This happens on the first player's turn. When the second player's turn arrives,
    the formula to be used will always be the same, provided you replace the maximum
    with the minimum. In a similar way, we proceed to update the policy; that is,
    we look for the set of actions that allows us to reach the next state in order
    to maximize the value function.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生在第一个玩家的回合。当第二个玩家的回合到来时，使用的公式将始终相同，只要你将最大值替换为最小值。以类似的方式，我们继续更新策略；也就是说，我们寻找一组行动，使我们能够到达下一个状态，从而最大化值函数。
- en: 'The parameters that govern the process are as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 控制过程的参数如下：
- en: The learning rate that determines how often new information is acquired and
    will replace old information. A factor of 0 would prevent the agent from learning;
    however, a factor of 1 would cause the agent to be interested only in recent information.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率决定了新信息被获取的频率，并将替代旧信息。折扣因子为 0 时，会阻止智能体学习；然而，折扣因子为 1 时，智能体只对最近的信息感兴趣。
- en: The discount factor that determines the importance of future rewards. A factor
    of 0 will cause the agent to use only the current rewards, while a factor tending
    to 1 will make the agent also attentive to the rewards he will receive in the
    long-term future.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣因子决定未来奖励的重要性。折扣因子为 0 时，智能体只会使用当前奖励，而折扣因子接近 1 时，智能体也会关注他在长期未来将获得的奖励。
- en: The strategy used in the algorithm causes the player to choose the next action
    with the e-greedy method. This means that the agent will follow his policy with
    the probability 1-e, and will choose random actions with the probability e.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 算法中使用的策略使得玩家采用 e-greedy 方法选择下一个动作。这意味着智能体将在概率 1-e 下遵循其策略，而在概率 e 下选择随机动作。
- en: 'At the end of a game, the player sets the final status as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏结束时，玩家将最终状态设置如下：
- en: 100 if player 1 wins
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果玩家 1 胜利，结果为 100
- en: -100 if player 2 wins
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果玩家 2 获胜，则结果为 -100
- en: 0 if a draw
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是平局，结果为 0
- en: 'The learning process is repeated N times, the value set by the user:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程重复 N 次，N 是用户设定的值：
- en: 'After training player 4, we can simulate the game:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练好玩家 4 后，我们可以模拟游戏：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once again, we verify the number of occurrences of the results:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次验证结果的出现次数：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following results are returned:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this case, it is the trained agent (player 4) who wins the games with the
    most occurrences (48%), then there are the draws (31%), and finally the games
    won by player 3 (21%). Thus, the player's training reversed the results by creating
    an agent who was able to identify a policy that allowed him to get more wins.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练过的智能体（玩家 4）赢得了最多的游戏（48%），其次是平局（31%），最后是玩家 3 赢得的游戏（21%）。因此，玩家的训练通过创建一个能够识别出让他获得更多胜利的策略的智能体，逆转了游戏的结果。
- en: In the following section, the OpenAI Gym library will be introduced; this library
    contains many environments which will allow us to train agents using reinforcement
    learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，将介绍 OpenAI Gym 库；这个库包含许多环境，可以让我们使用强化学习来训练智能体。
- en: Introducing OpenAI Gym library
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 OpenAI Gym 库
- en: OpenAI Gym is a library that helps us to implement algorithms based on reinforcement
    learning. It focuses on the episodic setting of reinforcement learning. In other
    words, the agent's experience is divided into a series of episodes. The initial
    state of the agent is randomly sampled by a distribution, and the interaction
    proceeds until the environment reaches a terminal state. This procedure is repeated
    for each episode, with the aim of maximizing the total reward expectation per
    episode and achieving a high level of performance in the fewest possible episodes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个帮助我们实现基于强化学习算法的库。它专注于强化学习中的阶段性设置。换句话说，智能体的经历被分为一系列的阶段。智能体的初始状态由一个分布随机采样，并且交互直到环境达到终止状态。这一过程对每个阶段重复，目的是最大化每个阶段的总奖励期望，并在尽可能少的阶段内实现高性能。
- en: 'Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    It supports the ability to teach agents everything from walking to playing games
    such as Pong or pinball. The library is available at the following link: [https://gym.openai.com/](https://gym.openai.com).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Gym 是一个用于开发和比较强化学习算法的工具包。它支持训练智能体完成从走路到玩 Pong 或弹球等游戏的各种任务。该库可以通过以下链接访问：[https://gym.openai.com/](https://gym.openai.com)。
- en: OpenAI Gym is part of a much more ambitious project, known as the OpenAI project.
    OpenAI is an **artificial intelligence** (**AI**) research company founded by
    Elon Musk and Sam Altman. It is a non-profit project that aims to promote and
    develop friendly AI in such a way as to benefit humanity as a whole. The organization
    aims to collaborate freely with other institutions and researchers by making their
    patents and research open to the public. The founders decided to undertake this
    project as they were concerned by the existential risk deriving from the indiscriminate
    use of AI.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个更加宏大的项目的一部分，该项目被称为OpenAI项目。OpenAI是一家由埃隆·马斯克和山姆·奥特曼创办的**人工智能**（**AI**）研究公司。它是一个非营利项目，旨在促进和发展友好的AI，以便造福整个人类。该组织的目标是通过将专利和研究公开，来与其他机构和研究人员自由合作。创始人之所以启动这一项目，是因为他们对AI的不加限制使用可能带来的生存风险感到担忧。
- en: OpenAI Gym is a library of programs that allows you to develop AIs, measure
    intellectual abilities, and enhance learning abilities. In short, a *gym* in the
    form of algorithms that trains the present digital brains to the OpenAI Gym and
    projects them into the future. But there is also another goal. OpenAI wants to
    stimulate research in the AI sector by funding projects that make humanity progress
    even in those fields where there is no economic return. With Gym, on the other
    hand, it intends to standardize the measurement of AI so that researchers can
    compete on equal terms and know where their colleagues have come from, but above
    all it focuses on results that are really useful for everyone.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个程序库，允许你开发人工智能、衡量智能能力，并增强学习能力。简而言之，它是一个以算法形式呈现的*健身房*，用来训练当前的数字大脑，并将其投射到未来。除此之外，还有另一个目标。OpenAI希望通过资助那些能够促进人类进步的项目来激励AI领域的研究，即便这些项目在经济上没有直接回报。而通过Gym，它旨在标准化AI的衡量方法，以便研究人员能够在平等的条件下竞争，并了解他们的同事取得了哪些进展，最重要的是，它专注于那些对所有人都真正有益的成果。
- en: The tools available are many. From the ability to play old video games like
    Pong to that of fighting in  GO to control a robot, we just enter our algorithm
    in this digital place to see how it works. The second step is to compare the benchmarks
    obtained with the other ones to see where we stand compared to others, and maybe
    we can collaborate with them to get mutual benefits.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的工具非常多。从玩像Pong这样的老式视频游戏，到在围棋中对战，再到控制机器人，我们只需将算法输入到这个数字化空间，便可看到它如何工作。第二步是将获得的基准数据与其他人进行比较，看看我们在同行中处于什么位置，也许我们还可以与他们合作，共享互利成果。
- en: 'The following list shows some environments available in the library; the environments
    are grouped by category to simplify the search:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了库中可用的一些环境；这些环境按类别分组，便于搜索：
- en: '**Algorithms**: Perform computations such as adding multi-digit numbers and
    reversing sequences. You might object that these tasks are easy for a computer,
    but the challenge is to learn these algorithms purely from examples. These tasks
    have the nice property that it''s easy to vary the difficulty by varying the sequence
    length.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：执行计算任务，如加法多位数和反转序列。你可能会认为这些任务对计算机来说很简单，但挑战在于如何仅通过示例来学习这些算法。这些任务的一个优点是，可以通过变化序列长度轻松调节难度。'
- en: '**Atari**: Plays classic Atari games. We''ve integrated the Arcade learning
    Environment (which has had a big impact on reinforcement learning research) in
    an easy-to-install form.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Atari**：玩经典的Atari游戏。我们已将街机学习环境（对强化学习研究产生了巨大影响）集成到一个易于安装的形式中。'
- en: '**Box2D**: Continuous control tasks in the Box2D simulator.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Box2D**：在Box2D模拟器中进行连续控制任务。'
- en: '**Classic control**: Complete small-scale tasks, mostly from the RL literature.
    They''re here to get you started.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典控制**：完成小规模任务，主要来自强化学习文献。它们的目的是帮助你入门。'
- en: '**MuJoCo**: Continuous control tasks, running in a fast physics simulator.
    This task uses the MuJoCo physics engine, which was designed for fast and accurate
    robot simulation.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MuJoCo**：连续控制任务，在快速物理模拟器中运行。此任务使用MuJoCo物理引擎，该引擎专为快速且精确的机器人模拟设计。'
- en: '**Robotics**: Simulated goal-based tasks for the Fetch and ShadowHand robots.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人技术**：为Fetch和ShadowHand机器人提供基于目标的模拟任务。'
- en: '**Toy text**: Simple text environments to get you started.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Toy text**：简单的文本环境，帮助你入门。'
- en: In particular, the classic control category offers very useful environments
    to reproduce the scenarios of important physical experiments.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，经典控制类别提供了非常有用的环境，能够重现重要物理实验的场景。
- en: OpenAI Gym makes no assumptions about the structure of our agent and is compatible
    with any numerical computation library. The Gym library is a collection of test
    problems—environments—that we can use to work out our reinforcement learning algorithms.
    The environments have a shared interface, allowing you to write general algorithms. To
    begin, let's see how to install the library.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym对我们代理的结构没有假设，并且与任何数值计算库兼容。Gym库是一组测试问题——环境——我们可以用来处理强化学习算法。这些环境具有共享的接口，允许你编写通用的算法。首先，让我们看看如何安装这个库。
- en: OpenAI Gym installation
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym安装
- en: As already mentioned, OpenAI Gym is a library written in a Python environment.
    To be able to integrate it into the R environment, a version of Python must be
    installed on the computer. First we need to install the OpenAI `gym-http-api`
    API; these are APIs that allow access to an ever-increasing variety of environments.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，OpenAI Gym是一个在Python环境中编写的库。为了能够将其集成到R环境中，必须在计算机上安装Python版本。首先，我们需要安装OpenAI的`gym-http-api`
    API；这些是允许访问越来越多环境的API。
- en: The **APIs**, that is an abbreviation for **application programming interfaces**,
    are sets of definitions and protocols with which application software is created
    and integrated. They allow your products or services to communicate with other
    products or services without knowing how they are implemented, thus simplifying
    the development of the app and allowing a net saving of time and money. When creating
    new tools and products or managing existing ones, the APIs offer flexibility,
    guarantee opportunities for innovation, and simplify design, administration, and
    use.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**API**，即**应用程序编程接口**，是创建和集成应用软件的一组定义和协议。它们允许你的产品或服务与其他产品或服务进行通信，而无需知道它们是如何实现的，从而简化了应用开发，并节省了时间和成本。在创建新工具和产品或管理现有工具时，API提供了灵活性，保证了创新机会，并简化了设计、管理和使用。'
- en: 'To download and install the OpenAI `gym-http-api` API, you can run the following
    shell commands. To execute these commands, it is necessary to use a command window:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要下载并安装OpenAI `gym-http-api` API，你可以运行以下shell命令。执行这些命令时，必须使用命令窗口：
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first line of code in the preceding code block uses the Git software to
    download the bees to install from the github repository. The Git software allows
    you to manage a project while maintaining control of the source code and its history,
    and allows more developers to collaborate on it; it is essentially a version control
    system. Git is the de facto standard in the open source community, created in
    2005 by Linus Torvald to work on the Linux kernel, and has been maintained by
    Junio Hamano (a Google developer) two months after its creation; Git is constantly
    evolving and completely free and open source. The second line of code moves the
    command line to the folder where we copied the repository. Finally the third line
    uses the pip software to install the API.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码块中的第一行代码使用Git软件从github仓库下载蜜蜂并进行安装。Git软件允许你在管理项目的同时保持对源代码及其历史的控制，并且允许更多开发者在项目中协作；它本质上是一个版本控制系统。Git是开源社区的事实标准，由Linus
    Torvald在2005年为Linux内核开发而创建，且在创建两个月后由Google开发者Junio Hamano维护；Git不断发展，并且完全免费且开源。第二行代码将命令行移动到我们复制仓库的文件夹中。最后，第三行使用pip软件安装API。
- en: The preceding code is intended to be run locally by a single user. A Python
    client is included, to demonstrate how to interact with the server.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码旨在由单个用户在本地运行。包括一个Python客户端，用于演示如何与服务器进行交互。
- en: 'To start the server from the command line, go to the folder where we installed
    the API, and run the following command:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从命令行启动服务器，进入我们安装API的文件夹，然后运行以下命令：
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following result is returned:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now the server is ready to interact with our scripts. We can install the `gym`
    package:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在服务器已准备好与我们的脚本交互。我们可以安装`gym`包：
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following table gives some information about the `gym` package:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了一些关于`gym`包的信息：
- en: '| Package | `gym` |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 包 | `gym` |'
- en: '| Date | 2016-10-25 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2016-10-25 |'
- en: '| Version | 0.1.0 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 0.1.0 |'
- en: '| Title | Provides Access to the OpenAI Gym API |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 提供对OpenAI Gym API的访问 |'
- en: '| Author | Paul Hendricks |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Paul Hendricks |'
- en: To verify the functioning of the package, we can execute the example script
    supplied with the documentation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证包的功能，我们可以执行文档中提供的示例脚本。
- en: 'Let''s start by importing the library:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始导入这个库：
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To start, we will establish the connection with the client to interact with
    the server:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将与客户端建立连接，以便与服务器进行交互：
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `create_GymClient()` function instantiates a `GymClient` instance to integrate
    with an OpenAI Gym server. The following result is printed:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_GymClient()`函数实例化一个`GymClient`实例，以便与OpenAI Gym服务器集成。以下结果已打印：'
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The connection has been established, and now we can create the environment:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接已建立，现在我们可以创建环境：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following result is printed:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果已打印：
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we can list all the environments created with the bees available to us
    in the current work session:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以列出当前工作会话中创建的所有环境，并列出可用的蜜蜂：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following result is printed:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果已打印：
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this way, we have confirmation that the simulation environment has been
    correctly created, now we can request the information needed to interact with
    it:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以确认模拟环境已正确创建，现在我们可以请求与其交互所需的信息：
- en: '[PRE26]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `env_action_space_info()` function evaluates whether an action is a member
    of an environment action space.  The following results are printed:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`env_action_space_info()`函数评估一个动作是否属于环境的动作空间。以下结果已打印：'
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Only two actions are available in this environment. Let''s create the agent
    now:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此环境中，只有两个动作可用。现在让我们创建代理：
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `random_discrete_agent()` function simply creates a sample random discrete
    agent.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_discrete_agent()`函数仅创建一个示例随机离散代理。'
- en: 'Now we will set the folder in which to save the results, and open a window
    to monitor changes in the environment:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将设置一个文件夹来保存结果，并打开一个窗口以监控环境变化：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s initialize some variables that we will need in the simulation:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们初始化一些在模拟中需要的变量：
- en: '[PRE30]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we will implement two loops to interact with the environment using random
    actions:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将实现两个循环来使用随机动作与环境进行交互：
- en: '[PRE31]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following screenshot is returned:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下截图：
- en: '![](img/016ee645-08c2-4be8-a4a2-d24226453de5.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/016ee645-08c2-4be8-a4a2-d24226453de5.png)'
- en: In the screenshot, you can see the cart-pole that will move according to the
    indications provided by the algorithm.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在截图中，你可以看到小车-杠杆（cart-pole），它将根据算法提供的指示进行移动。
- en: 'Finally, we will close the window, using the following command:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用以下命令关闭窗口：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This example helped us to start interacting with the environments available
    in OpenAI Gym. If you have not understood some passages, don't worry, in the following
    section we can learn more about them.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子帮助我们开始与OpenAI Gym中可用的环境进行交互。如果你没有理解某些部分，不用担心，接下来的部分我们可以深入学习它们。
- en: OpenAI Gym methods
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym方法
- en: 'OpenAI Gym provides the `env` class, which encapsulates the environment and
    its possible internal dynamics. The class has different methods and attributes
    to implement to create a new environment. The most important methods are called
    reset, step, and render, let''s look at them briefly:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym提供了`env`类，该类封装了环境及其可能的内部动态。该类具有不同的方法和属性，用于实现创建新环境。最重要的方法分别是reset、step和render，我们来简要了解一下它们：
- en: The **reset method** has the task of resetting the environment, initializing
    it to the initial state. Within the reset method, the definitions of the elements
    that make up the environment must be contained, in this case the definition of
    the mechanical arm, of the object to be grasped and its support.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**reset方法**的任务是重置环境，将其初始化为初始状态。在reset方法中，必须包含构成环境的元素的定义，在本例中是机械臂、待抓取物体及其支撑物的定义。'
- en: The **step method** has the task of moving the environment forward by one step.
    It requires the action to be performed as input, and returns the new observation
    to the agent. Within the method, the management of the dynamics of the movements,
    the calculation of the status and of the reward, and the controls for completing
    the episode must be defined.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**step方法**的任务是将环境向前推进一步。它需要输入要执行的动作，并返回代理的新观察结果。在该方法中，必须定义运动动态的管理、状态和奖励的计算以及完成回合的控制。'
- en: The third and last method is that we must define which interior to **render**
    to, as the elements at each step must be represented. The method involves different
    types of rendering, such as human, `rgb_array`, or `ansi`. With the human type,
    the rendering is done on the screen or command-line interface, and the method
    does not return anything; with the `rgb_array` type, invoking the method returns
    an n-dimensional array representing the RGB pixels of the screen; choosing the
    third type, the return method returns a string containing a textual representation.
    To render, OpenAI Gym provides the viewer class, through which you can draw the
    elements of the environment as a set of polygons and circles.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三种也是最后一种方法是我们必须定义要**渲染**到哪个内部，因为每个步骤的元素都必须被表示出来。该方法涉及不同类型的渲染，例如人类、`rgb_array`或`ansi`。使用人类类型时，渲染在屏幕或命令行界面上进行，且该方法不返回任何内容；使用`rgb_array`类型时，调用该方法会返回一个表示屏幕RGB像素的n维数组；选择第三种类型时，返回方法会返回一个包含文本表示的字符串。为了渲染，OpenAI
    Gym提供了一个viewer类，通过它可以将环境的元素绘制为一组多边形和圆形。
- en: Regarding the attributes of the environment, the `env` class provides the definition
    of action space, observation space, and reward range. The action space attribute
    represents the action space, which is the set of possible actions that the agent
    can perform within the environment. Using the observation space attribute, the
    number of parameters that makes up the state is defined, and for each of them
    the range of values that can be assumed. The reward range attribute contains the
    minimum and maximum rewards obtainable in the environment, by default set to (-∞,
    + ∞).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 关于环境的属性，`env`类提供了动作空间、观察空间和奖励范围的定义。动作空间属性表示动作空间，即智能体在环境中可以执行的可能动作的集合。通过观察空间属性，定义组成状态的参数的数量，并且为每个参数定义可以假设的值范围。奖励范围属性包含在环境中可以获得的最小和最大奖励，默认设置为（-∞，+∞）。
- en: Using the `env` class proposed by the framework as a basis for new environments,
    the common interface provided by the toolkit is adopted. In this way, the environments
    created can be integrated into the toolkit library, and their dynamics can be
    learned from algorithms already implemented by users of the OpenAI Gym community.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用框架所提出的`env`类作为新环境的基础，采用工具包提供的通用接口。通过这种方式，创建的环境可以被集成到工具包库中，并且它们的动态可以通过OpenAI
    Gym社区用户已实现的算法进行学习。
- en: In the following section, we will implement a robot control system using an
    OpenAI Gym environment.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将使用OpenAI Gym环境实现一个机器人控制系统。
- en: Robot control system using the FrozenLake environment
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FrozenLake环境的机器人控制系统
- en: Technically speaking, a robot can be seen as a particular type of automatic
    control, that is, an automaton physically located in an environment of which it
    can perceive certain characteristics through components called sensors, and on
    which it can perform actions with the aim of making changes to it. These actions
    are performed by so-called actuators.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，机器人可以被看作是一种特殊类型的自动控制，即一种物理上位于环境中的自动化装置，它能够通过传感器感知环境的某些特征，并且可以执行动作以改变环境。这些动作是通过所谓的执行器来完成的。
- en: 'All that is interposed between the measurements made by the sensors and the
    commands given to the actuators can be defined as the control program or the controller
    of the robot. This is the component in which the intelligence of the robot is
    encoded, and in a certain sense it constitutes the brain that must guide its actions
    in order to obtain the desired behavior. A controller can be implemented in various
    ways: usually it is software running on one or more microcontrollers physically
    integrated into the system (onboard), but it can also be obtained through electronic
    circuits (analog or digital) directly wired into the hardware of the robot. Let''s
    start by taking a look at the environment'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器所做的测量与执行器所给出的命令之间的所有内容可以被定义为控制程序或机器人的控制器。这是机器人智能编码的组件，在某种意义上，它构成了必须引导其行动以获得期望行为的大脑。控制器可以通过多种方式实现：通常它是运行在一个或多个微控制器上的软件，这些微控制器物理集成在系统中（车载），但它也可以通过电子电路（模拟或数字）直接连接到机器人的硬件中。让我们从观察环境开始。
- en: The FrozenLake environment
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FrozenLake环境
- en: 'The FrozenLake environment ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)[)](https://gym.openai.com/envs/FrozenLake-v0/)
    is a 4 x 4 grid that contains four possible areas: **Safe** (**S**), **Frozen**
    (**F**), **Hole** (**H**), and **Goal** (**G**). The agent controls the movement
    of a character in a grid world, and moves around the grid until it reaches the
    goal or the hole. Some tiles of the grid are walkable, and others lead to the
    agent falling into the water. If it falls into the hole, it must start from the
    beginning and is rewarded the value 0\. The agent moves following an uncertain
    path that depends only in part on the chosen direction. The agent is rewarded
    when he finds a possible path to the set goal. The agent has four possible moves:
    up, down, left, and right. The process continues until it learns from every mistake
    and reaches the goal eventually.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: FrozenLake 环境（[https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)）是一个
    4 x 4 的网格，包含四个可能的区域：**安全**（**S**）、**冰面**（**F**）、**洞**（**H**）和**目标**（**G**）。智能体在网格世界中控制一个角色的移动，并在网格中移动，直到到达目标或掉入洞中。网格中的一些瓷砖是可行走的，而其他瓷砖则会导致智能体掉进水里。如果掉进洞里，智能体必须从头开始，并且奖励为
    0。智能体沿着一条不确定的路径移动，该路径仅部分依赖于所选择的方向。智能体在找到通向设定目标的可能路径时会获得奖励。智能体有四个可用的移动方向：上、下、左和右。该过程将持续，直到智能体从每次失败中学习并最终到达目标。
- en: 'The surface is described using a grid like the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该表面使用如下网格进行描述：
- en: 'SFFF (**S**: starting point, safe)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SFFF (**S**: 起点，安全)'
- en: 'FHFH (**F**: frozen surface, safe)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FHFH (**F**: 冰面，安全)'
- en: 'FFFH (**H**: hole, fall to your doom)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'FFFH (**H**: 洞，掉进深渊)'
- en: 'HFFG (**G**: goal, where the frisbee is located)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'HFFG (**G**: 目标，飞盘所在位置)'
- en: 'In the following diagram, we can see the FrozenLake grid (4 x 4):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到 FrozenLake 网格（4 x 4）：
- en: '![](img/f63117d0-0eed-4b9c-951a-9918920fdb17.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f63117d0-0eed-4b9c-951a-9918920fdb17.png)'
- en: The episode ends when you reach the goal or fall in a hole. You receive a reward
    of one if you reach the goal, and zero otherwise.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当你到达目标或掉进洞里时，剧集结束。如果你到达目标，你将获得奖励1，否则奖励为0。
- en: The Q-learning solution
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q 学习解决方案
- en: As we said in [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml), *Temporal
    Difference Learning*, Q-learning is one of the most used reinforcement learning
    algorithms. This is due to its ability to compare the expected utility of the
    available actions without requiring an environment model. Thanks to this technique
    it is possible to find an optimal action for every given state in a finished **Markov
    decision process** (**MDP**).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第七章](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml)中所说的，*时序差分学习*，Q 学习是最常用的强化学习算法之一。这是因为它能够在不需要环境模型的情况下比较可用动作的期望效用。得益于这一技术，可以为每个给定状态找到一个最优动作，这适用于完备的
    **马尔可夫决策过程**（**MDP**）。
- en: 'In the following code, we will use a Q-learning approach to find the right
    path from the start cell to goal cell in a 4 x 4 grid environment:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将使用 Q 学习方法，从起点单元格到目标单元格在 4 x 4 网格环境中找到正确的路径：
- en: 'As always, we will analyze the code line by line. Let''s start by importing
    the library:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一如既往，我们将逐行分析代码。让我们从导入库开始：
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To start, we will establish the connection with the client to interact with
    the server:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将建立与客户端的连接，以便与服务器进行交互：
- en: '[PRE34]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `create_GymClient()` function instantiates a `GymClient` instance to integrate
    with an OpenAI Gym server. The following result is printed:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_GymClient()` 函数实例化一个 `GymClient` 实例，以便与 OpenAI Gym 服务器进行集成。以下结果被打印出来：'
- en: '[PRE35]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The connection has been established; now we can create the environment:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接已经建立；现在我们可以创建环境：
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following result is printed:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被打印出来：
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we can list all the environments created with the bees available to us
    in the current work session:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以列出当前工作会话中通过蜜蜂创建的所有环境：
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following result is printed:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被打印出来：
- en: '[PRE39]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this way, we have confirmation that the simulation environment has been correctly
    created.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们确认模拟环境已被正确创建。
- en: 'Now, we will retrieve some information from the environment:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从环境中获取一些信息：
- en: '[PRE40]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `env_action_space_info()` function evaluates whether an action is a member
    of an environment action space.  The following results are printed:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`env_action_space_info()` 函数评估某个动作是否为环境动作空间的成员。以下结果被打印出来：'
- en: '[PRE41]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Four actions are available in this environment, as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，有以下四个可用的动作：
- en: '0: Move left'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0: 向左移动'
- en: '1: Move down'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 向下移动'
- en: '2: Move right'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2: 向右移动'
- en: '3: Move up'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3: 向上移动'
- en: 'Now let''s take a look at the states that an agent that moves in this environment
    can take:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下一个在此环境中移动的智能体可以采取的状态：
- en: '[PRE42]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `env_observation_space_info()` function gets information (name and dimensions/bounds)
    of the environment observation space. The following results are printed:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`env_observation_space_info()` 函数获取环境观察空间的信息（名称和维度/边界）。以下结果被打印出来：'
- en: '[PRE43]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'From the following diagram, we can see that sixteen states are available in
    this environment (0 – 15) covering a 4x4 grid, counting each position from left
    to right, top to bottom as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下图示中，我们可以看到这个环境中有十六个状态（0 – 15），覆盖一个 4x4 的网格，从左到右、从上到下依次编号，如下所示：
- en: '![](img/de441345-9210-4c14-bf3c-b9e38f7935c6.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de441345-9210-4c14-bf3c-b9e38f7935c6.png)'
- en: 'We will then extract these values in order to reuse them:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将提取这些值以便重新使用：
- en: '[PRE44]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s initialize the parameters starting with `QTable`:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，初始化参数，从 `QTable` 开始：
- en: '[PRE45]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`QTable` has a number of rows equal to the size of the observation space (`observation_space_info$n`),
    while the columns are equal to the size of the action space (`action_space_info$n`).
    As we said, the FrozenLake environment provides a state for each cell in the 4
    x 4 grid, and four actions, returning a 16 x 4 table.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`QTable` 的行数等于观察空间的大小 (`observation_space_info$n`)，而列数等于动作空间的大小 (`action_space_info$n`)。正如我们所说，FrozenLake
    环境为 4x4 网格中的每个单元提供一个状态，并且提供四个动作，从而返回一个 16 x 4 的表格。'
- en: 'This table is initialized with all zeros using the `matrix()` function as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格通过 `matrix()` 函数初始化为全零，初始化方式如下：
- en: '[PRE46]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following table is printed:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打印出以下表格：
- en: '[PRE47]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, we define some parameters:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一些参数：
- en: '[PRE48]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Basically, `alpha` is the learning rate, and `gamma` is the discount factor.
    The learning rate handles the updating of the information acquired, in the sense
    that it establishes when it is time to replace the old acquisitions with the new
    ones. Setting the learning rate with a value of 0 means that the agent does not
    learn anything. This will only exploit previous knowledge. With a value of 1 the
    agent considers only the most recent information and ignores previous knowledge.
    This is the exploration-exploitation dilemma. Ideally, the agent must explore
    all possible actions for each state, finding the one that is actually most rewarded
    for exploiting it in achieving its goal. The discount factor determines the importance
    of future rewards. A factor of 0 will consider only current rewards, while a factor
    approaching 1 will make it strive for a long-term high reward.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`alpha` 是学习率，`gamma` 是折扣因子。学习率处理已获取信息的更新，它确定何时该用新的获取信息替代旧的。将学习率设置为 0 表示智能体不会学习任何东西，只会利用以前的知识。将学习率设置为
    1 时，智能体只考虑最新的信息，并忽略以前的知识。这就是探索与利用的困境。理想情况下，智能体必须探索每个状态下的所有可能动作，找到那个实际上最能通过利用它来达成目标的动作。折扣因子决定了未来奖励的重要性。因子为
    0 时，表示只考虑当前奖励，而接近 1 时，智能体将更加努力地追求长期的高奖励。
- en: 'Now, we set the number of episodes:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们设置 episode 的数量：
- en: '[PRE49]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `NumEpisodes` parameter has the following meaning. The agent learns through
    experience, without a tutor to guide him; this way represents a learning without
    supervision. The agent will explore until he reaches the goal, moving from one
    state to the next. Every exploration is called an episode. Each episode consists
    of the movement of the agent that moves from the initial state to the objective
    state. Each time the agent arrives at the target state, we move on to the next
    episode.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`NumEpisodes` 参数的含义如下。智能体通过经验进行学习，没有导师来指导它；这种方式表示没有监督的学习。智能体将持续探索，直到达到目标，从一个状态移动到下一个状态。每次探索称为一次**episode**。每个
    episode 包括智能体从初始状态到目标状态的移动。每当智能体到达目标状态时，我们就开始下一个 episode。'
- en: 'Now, we will create a list to contain total rewards:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个列表来保存总奖励：
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then two more parameters are initialized; we will need them to recover the
    results:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 然后初始化另外两个参数；我们将需要它们来恢复结果：
- en: '[PRE51]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'At this point, after setting the parameters, it is possible to start the Q-learning
    cycle:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，设置好参数后，就可以开始 Q-learning 循环了：
- en: '[PRE52]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'So, we initialize the system using the `env_reset()` method:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，我们通过 `env_reset()` 方法初始化系统：
- en: '[PRE53]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Then, a reward-counter and cycle-counter are initialized:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始化一个奖励计数器和周期计数器：
- en: '[PRE54]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'From this, the Q-learning table algorithm is implemented:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点出发，Q-learning 表格算法被实现：
- en: '[PRE55]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We increase the cycle counter at each new step:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当进入新的一步时，我们就增加周期计数器：
- en: '[PRE56]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, we have to choose an action:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要选择一个动作：
- en: '[PRE57]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: An action is chosen by greedily method-picking from `Qtable`. A noise is added
    because the environment is unknown, so it has to be explored in some way—your
    agent will do so using the power of randomness. Two functions are used, `which.max()`
    and `runif()`. The `which.max()` function returns the indices of the maximum values
    along an axis. The `runif()` function returns a sample (or samples) from the standard
    normal distribution. In the second line of code of the block just analyzed, we
    reduce the action index by one unit. This is necessary because as already mentioned
    the actions available in the environment range from 0 to 3 while in r the indexes
    of the tables start at 1 (unlike Python which instead starts from 0). Then to
    make the results compatible with the environment r it is necessary to make this
    correction.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过贪心方法从`Qtable`中选择动作。由于环境是未知的，因此必须以某种方式探索，代理将利用随机性的力量来进行探索。使用了两个函数，`which.max()`和`runif()`。`which.max()`函数返回沿某一轴的最大值的索引。`runif()`函数返回一个标准正态分布的样本（或多个样本）。在刚分析的代码块的第二行中，我们将动作索引减少了一单位。这是必要的，因为如前所述，环境中的可用动作范围是0到3，而在r中表格的索引从1开始（与Python不同，后者从0开始）。因此，为了使结果与r环境兼容，必须进行这一修正。
- en: 'Now, we will use the `env_step()` method to return the new states in response
    to the actions with which we call it. Obviously, the action we pass to the method
    is the one we have just decided:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`env_step()`方法来返回根据我们传给它的动作作出的新状态。显然，我们传给方法的动作是我们刚刚决定的：
- en: '[PRE58]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The `env_step()` method returns a list consisting of the following:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`env_step()`方法返回一个包含以下内容的列表：'
- en: '`action`: An action to take in the environment'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action`: 在环境中采取的动作'
- en: '`observation`: An agent''s observation of the current environment'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`: 代理对当前环境的观察'
- en: '`reward`: The amount of reward returned after previous action'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`: 上一个动作后返回的奖励数量'
- en: '`done`: Whether the episode has ended'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done`: 回合是否已结束'
- en: '`info`: A list containing auxiliary diagnostic information'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`: 包含辅助诊断信息的列表'
- en: 'For now, we are only interested in the following code:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，我们只对以下代码感兴趣：
- en: '[PRE59]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can then update the `Qtable` field with new knowledge:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以用新知识更新`Qtable`字段：
- en: '[PRE60]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The formula used for the Q-function update was the following:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 用于Q函数更新的公式如下：
- en: '![](img/b2d791ea-9828-439f-a82e-5367a8efd8c6.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2d791ea-9828-439f-a82e-5367a8efd8c6.png)'
- en: Also, in this case, we had to introduce a correction for the state and the action
    (state + 1, action + 1) that the FrozenLake environment expects to assume the
    following values 0-15 for the state and 0-3 for the actions, but as we know the
    row and column index in r starts from 1.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在这种情况下，我们必须为状态和动作引入修正（状态 + 1，动作 + 1），因为FrozenLake环境期望状态值为0-15，动作值为0-3，但我们知道在r中行和列索引是从1开始的。
- en: 'Now, we will update the sum of the rewards with the one just obtained and the
    state of the environment:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将用刚刚获得的奖励和环境状态更新奖励总和：
- en: '[PRE61]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, we insert a check to see if we have reached the end of the episode.
    We remember in this regard that the episode is considered finished when we reach
    the goal or end up in a hole:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们插入一个检查，查看是否已经到达回合结束的条件。关于这一点，我们记得当到达目标或掉入陷阱时，回合被视为结束：
- en: '[PRE62]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The `if` loop used for the end-of-episode check contains a series of instructions
    that guide us in finding the solution. In fact, I inserted a control that discriminates
    between reaching the objective and falling into a hole and then a series of prints
    that allow us to verify the success of the algorithm.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 用于回合结束检查的`if`循环包含一系列指令，帮助我们找到解决方案。实际上，我插入了一个控制，用来区分是否到达目标或掉入陷阱，然后是一系列打印输出，让我们能够验证算法的成功。
- en: 'Before leaving the cycle that runs through the episodes definitively, we will
    try to update the sum of the rewards and the index of the rewards list:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最终离开贯穿各个回合的循环之前，我们将尝试更新奖励总和和奖励列表的索引：
- en: '[PRE63]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, we print the results, first the score:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印结果，首先是分数：
- en: '[PRE64]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Then, we print the `Qtable` values:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们打印`Qtable`的值：
- en: '[PRE65]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The following table is returned:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下表格：
- en: '[PRE66]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The table containing the value function is now ready, and we can use it to extract
    the paths that will take us from the starting cell to the goal cell without falling
    into the holes.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 包含价值函数的表格已经准备好，我们可以使用它来提取从起始单元到目标单元的路径，而不掉入陷阱。
- en: 'To begin, we limit our search to five possible paths:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将搜索限制在五条可能的路径上：
- en: '[PRE67]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'To do this, we used a cycle and we included the print of each episode for a
    check. Now, as we did in the training phase, let''s start: reset the environment
    using the `env_reset()` function:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们使用了一个循环，并包括了每个回合的打印输出以进行检查。现在，正如我们在训练阶段所做的那样，开始吧：使用`env_reset()`函数重置环境：
- en: '[PRE68]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We recall in this regard that the `env_reset()` function returns the environment
    to the initial state, that is, the state 0 which corresponds to the first cell.
    As we have already highlighted previously to make the OpenAI Gym environment designed
    for Python compatible with the environment, it is necessary to make a correction
    by adding a unit. In this way, the state 0 will correspond to the first line of
    the Qtable.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，我们回顾一下，`env_reset()`函数将环境恢复到初始状态，即状态0，对应于Qtable的第一行。正如我们之前所强调的，为了使OpenAI
    Gym环境与该环境兼容，需要通过添加一个单位进行修正。这样，状态0就会对应于Qtable的第一行。
- en: 'The next `for` loop will allow us to move within the grid to reach the goal
    following the directions provided by `Qtable`:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的`for`循环将允许我们在网格中移动，以按照`Qtable`提供的方向到达目标：
- en: '[PRE69]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s now initialize a rewards counter:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们初始化一个奖励计数器：
- en: '[PRE70]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, we will extract the actions that have the maximum expected future reward
    given the actual state, that is, related to the current step. The Qtable returns
    this value as the index of the maximum value in the row corresponding to the current
    state:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将提取在给定实际状态下具有最大预期未来回报的动作，即与当前步骤相关的动作。Qtable返回此值作为当前状态对应行中最大值的索引：
- en: '[PRE71]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Once again, we reduce the action index by one unit.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次将动作索引减去一个单位。
- en: 'Now, we will use the `env_step()` method to return the new states in response
    to the actions extracted:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`env_step()`方法返回响应提取的动作后的新状态：
- en: '[PRE72]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Of the values returned by the `env_step()` function, we are currently only
    interested in the status and reward:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`env_step()`函数返回的值，我们目前只关心状态和奖励：
- en: '[PRE73]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, we will update the reward counter:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将更新奖励计数器：
- en: '[PRE74]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, we will check whether we have reached the end of the episode:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将检查是否已到达回合的结束：
- en: '[PRE75]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: In this way, we have obtained five of the best routes to reach the goal without
    falling into the holes.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们得到了五条最佳路线，可以在不掉入陷阱的情况下到达目标。
- en: Summary
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned the general concepts of game theory. We learned
    about the essential characteristics of the games and how the solutions adopted
    can help us solve real-life problems. We analyzed a series of real applications
    in which the theories are used to obtain solutions. Following that, we analyzed
    in detail the OpenAI Gym library and how to interact with the available environments
    to simulate real problems.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，我们学习了博弈论的基本概念。我们了解了游戏的基本特征，以及所采用的解决方案如何帮助我们解决现实问题。我们分析了一系列实际应用，在这些应用中使用了这些理论来获得解决方案。接着，我们详细分析了OpenAI
    Gym库，以及如何与可用的环境进行交互以模拟现实问题。
- en: 'Then, we tackled the tic-tac-toe game using reinforcement learning. We used
    the `tictactoe` package to set the environment and to train an agent using Q-learning
    to play the game. Finally, we looked at the FrozenLake environment. This is a
    4 × 4 grid that contains four possible areas: Safe (S), Frozen (F), Hole (H),
    and Goal (G). The agent controls the movement of a character in a grid world,
    and moves around the grid until it reaches the goal or the hole. This environment
    is particularly suitable for simulating problems related to the mobility of a
    robot in an environment full of obstacles. After defining the environment, we
    created an agent that is able to move within the environment and find the goal
    using an algorithm based on Q-learning.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们使用强化学习解决了井字游戏问题。我们使用`tictactoe`包来设置环境，并通过Q学习训练代理玩游戏。最后，我们查看了FrozenLake环境。这个环境是一个4
    × 4的网格，包含四个可能的区域：安全区（S）、冰面（F）、陷阱（H）和目标（G）。代理控制一个角色在网格世界中的移动，直到它到达目标或掉进陷阱。这个环境特别适合模拟与机器人在充满障碍物的环境中的移动问题。定义环境之后，我们创建了一个能够在环境中移动并使用基于Q学习的算法找到目标的代理。
- en: In the next chapter, we will learn the fundamental concepts of finance problems;
    how to forecast stock market prices, and how to optimize equity portfolios. Then,
    we will look at how to implement fraud detection techniques.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习金融问题的基本概念；如何预测股市价格，以及如何优化股票投资组合。然后，我们将了解如何实现欺诈检测技术。
