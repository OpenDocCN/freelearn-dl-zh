- en: TRPO and PPO Implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO 和 PPO 实现
- en: In the previous chapter, we looked at policy gradient algorithms. Their uniqueness lies
    in the order in which they solve a **reinforcement learning** (**RL**) problem—policy
    gradient algorithms take a step in the direction of the highest gain of the reward.
    The simpler version of this algorithm (**REINFORCE**) has a straightforward implementation
    that alone achieves good results. Nevertheless, it is slow and has a high variance.
    For this reason, we introduced a value function that has a double goal—to critique
    the actor and to provide a baseline. Despite their great potential, these actor-critic
    algorithms can suffer from unwanted rapid variations in the action distribution
    that may cause a drastic change in the states that are visited, followed by a
    rapid decline in the performance from which they could never recover from.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了策略梯度算法。它们的独特之处在于解决 **强化学习**（**RL**）问题的顺序——策略梯度算法朝着奖励增益最大化的方向迈出一步。该算法的简化版本（**REINFORCE**）具有直接的实现，并且单独使用时能够取得不错的效果。然而，它的速度较慢，且方差较大。因此，我们引入了一个值函数，具有双重目标——批评演员并提供基准。尽管这些演员-评论家算法具有巨大的潜力，但它们可能会受到动作分布中不希望出现的剧烈波动的影响，从而导致访问的状态发生急剧变化，随之而来的是性能的迅速下降，且这种下降可能永远无法恢复。
- en: In this chapter, we will address this problem by showing you how introducing
    a trust-region, or a clipped objective, can mitigate it. We'll show two practical
    algorithms, namely TRPO and PPO. These have shown ability in controlling simulated
    walking, controlling hopping and swimming robots, and playing Atari games. We'll
    cover a new set of environments for continuous control and show how policy gradient
    algorithms can be adapted to work in a continuous action space. By applying TRPO
    and PPO to these new environments, you'll be able to train an agent to run, jump,
    and walk.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过展示如何引入信任区域或剪切目标来解决这一问题，从而减轻该问题的影响。我们将展示两个实际的算法，即 TRPO 和 PPO。这些算法已经证明能在控制模拟行走、控制跳跃和游泳机器人以及玩
    Atari 游戏方面取得良好效果。我们将介绍一组新的连续控制环境，并展示如何将策略梯度算法适配到连续动作空间中。通过将 TRPO 和 PPO 应用于这些新环境，您将能够训练一个智能体进行跑步、跳跃和行走。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Roboschool
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roboschool
- en: Natural policy gradient
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然策略梯度
- en: Trust region policy optimization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任区域策略优化
- en: Proximal policy optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近端策略优化
- en: Roboschool
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Roboschool
- en: Up until this point, we have worked with discrete control tasks such as the
    Atari games in [Chapter 5](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml), *Deep
    Q-Network*, and LunarLander in [Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml),
    *Learning Stochastic and PG Optimization*. To play these games, only a few discrete
    actions have to be controlled, that is, approximately two to five actions. As
    we learned in [Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml), *Learning
    Stochastic and PG Optimization*, policy gradient algorithms can be easily adapted
    to continuous actions. To show these properties, we'll deploy the next few policy
    gradient algorithms in a new set of environments called Roboschool, in which the
    goal is to control a robot in different situations. Roboschool has been developed
    by OpenAI and uses the famous OpenAI Gym interface that we used in the previous
    chapters. These environments are based on the Bullet Physics Engine (a physics
    engine that simulates soft and rigid body dynamics) and are similar to the ones
    of the famous Mujoco physical engine. We opted for Roboschool as it is open source
    (Mujoco requires a license) and because it includes some more challenging environments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经处理了离散控制任务，例如 [第五章](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml)中的 Atari
    游戏，*深度 Q 网络*，以及 [第六章](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml)中的 LunarLander，*学习随机过程和
    PG 优化*。为了玩这些游戏，只需要控制少数几个离散动作，即大约两个到五个动作。如我们在 [第六章](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml)
    *学习随机过程和 PG 优化* 中所学，策略梯度算法可以很容易地适应连续动作。为了展示这些特性，我们将在一组新的环境中部署接下来的几种策略梯度算法，这些环境被称为
    Roboschool，目标是控制机器人在不同情境下进行操作。Roboschool 由 OpenAI 开发，使用了我们在前几章中使用的著名的 OpenAI Gym
    接口。这些环境基于 Bullet Physics 引擎（一个模拟软体和刚体动力学的物理引擎），与著名的 Mujoco 物理引擎的环境类似。我们选择 Roboschool
    是因为它是开源的（而 Mujoco 需要许可证），并且它包含了一些更具挑战性的环境。
- en: 'Specifically, Roboschool incorporates 12 environments, from the simple Hopper
    (RoboschoolHopper), displayed on the left in the following figure and controlled
    by three continuous actions, to a more complex humanoid (RoboschoolHumanoidFlagrun)
    with 17 continuous actions, shown on the right:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Roboschool 包含 12 个环境，从简单的 Hopper（RoboschoolHopper，左图）到更复杂的人形机器人（RoboschoolHumanoidFlagrun，右图），后者有
    17 个连续动作：
- en: '![](img/37f5ad9a-4bb0-4d86-87ec-7c28867cbc76.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37f5ad9a-4bb0-4d86-87ec-7c28867cbc76.png)'
- en: Figure 7.1\. Render of RoboschoolHopper-v1 on the left and RoboschoolHumanoidFlagrun-v1
    on the right
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1\. 左侧为 RoboschoolHopper-v1 渲染图，右侧为 RoboschoolHumanoidFlagrun-v1 渲染图
- en: In some of these environments, the goal is to run, jump, or walk as fast as
    possible to reach the 100 m endpoint while moving in a single direction. In others,
    the goal is to move in a three-dimensional field while being careful of possible
    external factors, such as objects that have been thrown. Also included in the
    set of 12 environments is a multiplayer Pong environment, as well as an interactive
    environment in which a 3D humanoid is free to move in all directions and has to
    move toward a flag in a continuous movement. In addition to this, there is a similar
    environment in which the robot is bombarded with cubes to destabilize the robot,
    who then has to build a more robust control to keep its balance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些环境中的一些，目标是尽可能快速地奔跑、跳跃或行走，以到达 100 米终点，并且在一个方向上移动。其他环境的目标则是移动在三维场地中，同时需要小心可能的外部因素，如被投掷的物体。该环境集合还包括一个多人
    Pong 环境，以及一个互动环境，其中 3D 人形机器人可以自由向各个方向移动，并需要朝着旗帜持续移动。除此之外，还有一个类似的环境，其中机器人被不断投掷立方体以破坏平衡，机器人必须建立更强的控制系统来维持平衡。
- en: The environments are fully observable, meaning that an agent has a complete
    view of its state that is encoded in a `Box` class of variable size, from about
    10 to 40\. As we mentioned previously, the action space is continuous and it is
    represented by a `Box` class of variable size, depending on the environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是完全可观察的，这意味着一个智能体能够完全查看其状态，该状态被编码为一个 `Box` 类，大小可变，约为 10 到 40。正如我们之前提到的，动作空间是连续的，且它由一个
    `Box` 类表示，大小根据环境不同而有所变化。
- en: Control a continuous system
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制连续系统
- en: 'Policy gradient algorithms such as REINFORCE and AC, as well as PPO and TRPO,
    all of which will be implemented in this chapter, can work with a discrete and
    continuous action space. The migration from one type of action to the other is
    pretty simple. Instead of computing a probability for each action in a continuous
    control, the actions can be specified through the parameters of a probability
    distribution. The most common approach is to learn the parameters of a normal
    Gaussian distribution, which is a very important family of distributions that
    is parametrized by a mean, ![](img/9649d114-e700-485e-a586-42154c153513.png), and
    a standard deviation, ![](img/c6cfee24-4967-4627-b62e-8f6c5654706a.png). Examples
    of Gaussian distributions and the change of these parameters are shown in the
    following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将实现的策略梯度算法（如 REINFORCE 和 AC，以及 PPO 和 TRPO）都可以与离散和连续动作空间一起使用。从一种动作类型迁移到另一种非常简单。在连续控制中，不是为每个动作计算一个概率，而是通过概率分布的参数来指定动作。最常见的方法是学习正态高斯分布的参数，这是一个非常重要的分布家族，它由均值！[](img/9649d114-e700-485e-a586-42154c153513.png)
    和标准差！[](img/c6cfee24-4967-4627-b62e-8f6c5654706a.png) 参数化。下图展示了高斯分布及其参数变化的示例：
- en: '![](img/99173b39-afaa-4193-a34f-592648e141eb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99173b39-afaa-4193-a34f-592648e141eb.png)'
- en: Figure 7.2\. A plot of three Gaussian distributions with different means and
    standard deviations
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2\. 三个不同均值和标准差的高斯分布图
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章提到的所有颜色参考，请参阅颜色图像包：[http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)。
- en: For example, a policy that's represented by a parametric function approximation
    (such as deep neural networks) can predict the mean and the standard deviation
    of a normal distribution in the functionality of a state. The mean can be approximated
    as a linear function and, usually, the standard deviation is independent of the
    state. In this case, we'll represent the parameterized mean as a function of a
    state denoted by ![](img/43585b36-5ac1-4e1e-9a69-87d2283053a7.png) and the standard
    deviation as a fixed value denoted by ![](img/4fea26fa-3eb6-460e-96b7-ac230c3f47be.png).
    Moreover, instead of working with standard deviation, it is preferred to use the
    logarithm of the standard deviation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，表示为参数化函数近似（如深度神经网络）的策略可以预测状态功能中正态分布的均值和标准差。均值可以近似为线性函数，通常，标准差是独立于状态的。在这种情况下，我们将表示参数化均值作为状态的函数，记作![](img/43585b36-5ac1-4e1e-9a69-87d2283053a7.png)，标准差作为固定值，记作![](img/4fea26fa-3eb6-460e-96b7-ac230c3f47be.png)。此外，代替直接使用标准差，最好使用标准差的对数值。
- en: 'Wrapping this up, a parametric policy for discrete control can be defined using
    the following line of code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，离散控制的参数化策略可以通过以下代码行定义：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`mlp` is a function that builds a multi-layer perceptron (also called a fully
    connected neural network) with hidden layer sizes specified in `hidden_sizes`,
    an output of the `act_dim` dimension, and the activations specified in the `activation`
    and `last_activation` arguments. These will become part of a parametric policy
    for continuous control and will have the following changes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlp`是一个函数，用于构建一个多层感知器（也称为全连接神经网络），隐藏层的大小由`hidden_sizes`指定，输出为`act_dim`维度，激活函数由`activation`和`last_activation`参数指定。这些将成为连续控制的参数化策略的一部分，并将有以下变化：'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here `p_means` is ![](img/43585b36-5ac1-4e1e-9a69-87d2283053a7.png) and `log_std`
    is ![](img/8ee83f0d-7596-4bf5-89fe-c606afe367b0.png).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`p_means`是![](img/43585b36-5ac1-4e1e-9a69-87d2283053a7.png)，`log_std`是![](img/8ee83f0d-7596-4bf5-89fe-c606afe367b0.png)。
- en: 'Furthermore, if all the actions have a value between 0 and 1, it is better
    to use a `tanh` function as the last activation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果所有的动作值都在0和1之间，最好将最后的激活函数设置为`tanh`：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, to sample from this Gaussian distribution and obtain the actions, the
    standard deviation has to be multiplied by a noisy vector that follows a normal
    distribution with a mean of 0 and a standard deviation of 1 that have been summed
    to the predicted mean:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了从这个高斯分布中采样并获得动作，必须将标准差乘以一个噪声向量，该向量遵循均值为0、标准差为1的正态分布，并加到预测的均值上：
- en: '![](img/bbbae8ae-518b-41cb-82f4-11136f2bbfcc.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbbae8ae-518b-41cb-82f4-11136f2bbfcc.png)'
- en: 'Here, z is the vector of Gaussian noise, ![](img/14af4d96-9a28-42e0-bfed-11a90d8f9cd0.png), with
    the same shape as ![](img/a2223d73-33e9-4c19-a375-0a08feeee23d.png) . This can
    be implemented in just one line of code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，z 是高斯噪声向量，![](img/14af4d96-9a28-42e0-bfed-11a90d8f9cd0.png)，它的形状与![](img/a2223d73-33e9-4c19-a375-0a08feeee23d.png)相同。这个操作可以通过一行代码实现：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since we are introducing noise, we cannot be sure that the values still lie
    in the limit of the actions, so we have to clip `p_noisy` in such a way that the
    action values remain between the minimum and maximum allowed values. The clipping
    is done in the following line of code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们引入了噪声，我们不能确定值仍然位于动作的范围内，因此我们必须以某种方式裁剪`p_noisy`，确保动作值保持在允许的最小值和最大值之间。裁剪操作在以下代码行中完成：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the end, the log probability is computed as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，日志概率通过以下方式计算：
- en: '![](img/cd12c059-7e22-4e81-a02b-32f369aa33f5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd12c059-7e22-4e81-a02b-32f369aa33f5.png)'
- en: 'This formula is computed in the `gaussian_log_likelihood` function, which returns
    the log probability. Thus, we can retrieve the log probability as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式在`gaussian_log_likelihood`函数中计算，该函数返回日志概率。因此，我们可以按以下方式检索日志概率：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, `gaussian_log_likelihood` is defined in the following snippet:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`gaussian_log_likelihood`在以下代码段中定义：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That's it. Now, you can implement it in every PG algorithm and try all sorts
    of environments with continuous action space. As you may recall, in the previous
    chapter, we implemented REINFORCE and AC on LunarLander. The same game is also
    available with continuous control and is called `LunarLanderContinuous-v2`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。现在，你可以在每个PG算法中实现它，并尝试各种具有连续动作空间的环境。正如你可能还记得，在上一章，我们在LunarLander上实现了REINFORCE和AC。相同的游戏也提供了连续控制版本，叫做`LunarLanderContinuous-v2`。
- en: With the necessary knowledge to tackle problems with an inherent continuous
    action space, you are now able to address a broader variety of tasks. However,
    generally speaking, these are also more difficult to solve and the PG algorithms
    we've learned about so far are too weak and not best suited to solving hard problems.
    Thus, in the remaining chapters, we'll look at more advanced PG algorithms, starting
    with the natural policy gradient.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有解决具有连续动作空间固有问题的必要知识后，你现在能够应对更广泛的任务。然而，一般来说，这些任务也更难解决，我们目前所学的PG算法过于弱小，无法很好地解决复杂问题。因此，在接下来的章节中，我们将介绍更先进的PG算法，从自然策略梯度开始。
- en: Natural policy gradient
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然策略梯度
- en: 'REINFORCE and Actor-Critic are very intuitive methods that work well on small
    to medium-sized RL tasks. However, they present some problems that need to be
    addressed so that we can adapt policy gradient algorithms so that they work on
    much larger and complex tasks. The main problems are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE和演员-评论员是非常直观的方法，在中小型RL任务中表现良好。然而，它们存在一些问题需要解决，以便我们能调整策略梯度算法，使其适用于更大、更复杂的任务。主要问题如下：
- en: '**Difficult to choose a correct step size**: This comes from the nature of
    RL being non-stationary, meaning that the distribution of the data changes continuously
    over time and as the agent learns new things, it explores a different state space.
    Finding an overall stable learning rate is very tricky.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**很难选择合适的步长**：这个问题源于强化学习的非平稳性特性，意味着数据的分布随着时间的推移不断变化，且随着智能体学习新知识，它会探索不同的状态空间。找到一个总体稳定的学习率非常棘手。'
- en: '**Instability**: The algorithms aren''t aware of the amount by which the policy
    will change. This is also related to the problem we stated previously. A single,
    not controlled update could induce a substantial shift of the policy that will
    drastically change the action distribution, and that consequently will move the
    agent toward a bad state space. Additionally, if the new state space is very different
    from the previous one, it could take a long time before recovering from it.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不稳定性**：这些算法没有意识到策略会改变的幅度。这也与我们之前提到的问题相关。一次没有控制的更新可能会导致策略发生重大变化，进而剧烈改变动作分布，从而将智能体推向不良的状态空间。此外，如果新状态空间与之前的状态空间差异很大，可能需要很长时间才能恢复。'
- en: '**Bad sample efficiency**: This problem is common to almost all on-policy algorithms.
    The challenge here is to extract more information from the on-policy data before
    discarding it.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本效率低**：这个问题几乎所有的在策略算法都会遇到。这里的挑战是，在丢弃策略数据之前，尽可能从中提取更多信息。'
- en: 'The algorithms that are proposed in this chapter, namely TRPO and PPO, try
    to address these three problems by taking different approaches, though they share
    a common background that will be explained soon. Also, both TRPO and PPO are on-policy
    policy gradient algorithms that belong to the model-free family, as shown in the
    following categorization RL map:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提出的算法，即TRPO和PPO，尝试通过不同的方式来解决这三个问题，尽管它们有一个共同的背景，稍后将进行解释。此外，TRPO和PPO都是在策略的策略梯度算法，属于无模型家族，如下所示的RL分类图：
- en: '![](img/c18ca40e-b73f-42ad-b20c-0e4d3e0101bb.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c18ca40e-b73f-42ad-b20c-0e4d3e0101bb.png)'
- en: Figure 7.3\. The collocation of TRPO and PPO inside the categorization map of
    the RL algorithms
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3. TRPO和PPO在RL算法分类图中的位置
- en: '**Natural Policy Gradient** (**NPG**) is one of the first algorithms that has
    been proposed to tackle the instability problem of the policy gradient methods.
    It does this by presenting a variation in the policy step that takes care of guiding
    the policy in a more controlled way. Unfortunately, it is designed for linear
    function approximations only, and it cannot be applied to deep neural networks.
    However, it''s the base for more powerful algorithms such as TRPO and PPO.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然策略梯度**（**NPG**）是最早提出的解决策略梯度方法不稳定性问题的算法之一。它通过引入策略步长的变化，控制策略的引导方式，从而解决这个问题。不幸的是，它只适用于线性函数逼近，不能应用于深度神经网络。然而，它是更强大算法的基础，如TRPO和PPO。'
- en: Intuition behind NPG
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NPG背后的直觉
- en: 'Before looking at a potential solution to the instability of PG methods, let''s
    understand why it appears. Imagine you are climbing a steep volcano with a crater
    on the top, similar to the function in the following diagram. Let''s also imagine
    that the only sense you have is the inclination of your foot (the gradient) and
    that you cannot see the world around you—you are blind. Let''s also set a fixed
    length of each step you can take (a learning rate), for example, one meter. You
    take the first step, perceive the inclination of your feet, and move 1 m toward
    the steepest ascent direction. After repeating this process many times, you arrive
    at a point near the top where the crater lies, but still, you are not aware of
    it since you are blind. At this point, you observe that the inclination is still
    pointing in the direction of the crater. However, if the volcano only gets higher
    for a length smaller than your step, with the next step, you''ll fall down. At
    this point, the space around you is totally new. In the case outlined in the following
    diagram, you''ll recover pretty soon as it is a simple function, but in general,
    it can be arbitrarily complex. As a remedy, you could use a much smaller step
    size but you''ll climb the mountain much slower and still, there is no guarantee
    of reaching the maximum. This problem is not unique to RL, but here it is more
    serious as the data is not stationary and the damage could be way bigger than
    in other contexts, such as supervised learning. Let''s take a look at the following
    diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻求解决PG方法不稳定性的问题之前，让我们先理解它为什么会出现。想象一下，你正在攀登一座陡峭的火山，火山口位于顶部，类似于下图中的函数。我们还假设你唯一的感官是脚下的倾斜度（梯度），并且你看不见周围的世界——你是盲的。我们还假设每一步的步长是固定的（学习率），例如，步长为一米。你迈出了第一步，感知到脚下的倾斜度，并朝着最陡的上升方向移动1米。在多次重复这一过程后，你到达了接近火山口的一个点，但由于你是盲人，依然没有意识到这一点。此时，你观察到脚下的倾斜度依旧指向火山口的方向。然而，如果火山的高度仅比你的步长小，那么下一步你将跌落下来。此时，周围的空间对你来说是完全陌生的。在下图所示的情况下，你会很快恢复过来，因为这是一个简单的函数，但通常情况下，它可能复杂得无法预料。作为补救方法，你可以使用更小的步长，但这样你爬山的速度会变得非常慢，并且仍然无法保证能够到达最大值。这个问题不仅仅存在于强化学习（RL）中，但在这里它尤为严重，因为数据并非静态，可能造成的损害比其他领域（如监督学习）更大。让我们看看下图：
- en: '![](img/1f666ace-b10f-4191-b8f8-2645f2c7502e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f666ace-b10f-4191-b8f8-2645f2c7502e.png)'
- en: Figure 7.4\. While trying to reach the maximum of this function, you may fall
    inside the crater
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4. 在尝试到达该函数的最大值时，你可能会掉进火山口。
- en: A solution that could come to mind, and one that has been proposed in NPG, is
    to use the curvature of the function in addition to the gradient. The information
    regarding the curvature is carried on by the second derivative. It is very useful
    because a high value indicates a drastic change in the gradient between two points
    and, as prevention, a smaller and more cautious step could be taken, thus avoiding
    possible cliffs. With this new approach, you can use the second derivative to
    gain more information about the action distribution space and make sure that,
    in the case of a drastic shift, the distribution of the action spaces don't vary
    too much. In the following section, we'll see how this is done in NPG.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能想到的解决方案，也是NPG中提出的解决方案，是在梯度的基础上加入函数的曲率。关于曲率的信息由二阶导数携带。这个信息非常有用，因为高值表示两个点之间的梯度发生了剧烈变化，作为预防，可以采取更小、更谨慎的步伐，从而避免可能的悬崖。通过这种新方法，你可以利用二阶导数来获得更多关于动作分布空间的信息，并确保在剧烈变化的情况下，动作空间的分布不会发生太大变化。在接下来的部分中，我们将看到NPG是如何做到这一点的。
- en: A bit of math
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些数学内容
- en: 'The novelty of the NPG algorithm is in how it updates the parameters with a
    step update that combines the first and second derivatives. To understand the
    natural policy gradient step, we have to explain two key concepts: the **Fisher
    Information Matrix** (**FIM**) and the **Kullback-Leibler** (**KL**) divergence.
    But before explaining these two key concepts, let''s look at the formula behind
    the update:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: NPG算法的创新之处在于如何通过结合一阶和二阶导数的步长更新来更新参数。为了理解自然策略梯度步长，我们需要解释两个关键概念：**费舍尔信息矩阵**（**FIM**）和**Kullback-Leibler**（**KL**）散度。但在解释这两个关键概念之前，让我们先看一下更新背后的公式：
- en: '![](img/17666fbf-8eaa-4e6b-a566-46f7179dd190.png) (7.1)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17666fbf-8eaa-4e6b-a566-46f7179dd190.png) (7.1)'
- en: This update differentiates from the vanilla policy gradient, but only by the
    term ![](img/5474c6f5-f532-4ceb-949e-40a546bbb8d2.png), which is used to enhance
    the gradient term.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新与传统的策略梯度有所不同，但仅仅通过项 ![](img/5474c6f5-f532-4ceb-949e-40a546bbb8d2.png)，它用于增强梯度项。
- en: In this formula, ![](img/bcc9d5c5-34e4-44bb-a4bd-9c53476edd92.png) is the FIM
    and ![](img/774e9e83-fce6-4ce7-80b8-5d6755106f99.png) is the objective function.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中， ![](img/bcc9d5c5-34e4-44bb-a4bd-9c53476edd92.png) 是FIM， ![](img/774e9e83-fce6-4ce7-80b8-5d6755106f99.png) 是目标函数。
- en: As we mentioned previously, we are interested in making all the steps of the
    same length in the distribution space, no matter what the gradient is. This is
    accomplished by the inverse of the FIM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们希望在分布空间中使得所有步骤的长度相同，无论梯度是什么。这是通过FIM的逆来实现的。
- en: FIM and KL divergence
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FIM和KL散度
- en: 'The FIM is defined as the covariance of an objective function. Let''s look
    at how it can help us. To be able to limit the distance between the distributions
    of our model, we need to define a metric that provides the distance between the
    new and the old distributions. The most popular choice is to use the KL divergence.
    It measures how far apart two distributions are and is used in many places in
    RL and machine learning. The KL divergence is not a proper metric as it is not
    symmetric, but it is a good approximation of it. The more different two distributions,
    are the higher the KL divergence value. Consider the plot in the following diagram.
    In this example, the KL divergences are computed with respect to the green function.
    Indeed, because the orange function is similar to the green function, the KL divergence
    is 1.11, which is close to 0\. Instead, it''s easy to see that the blue and the
    green lines are quite different. This observation is confirmed by the high KL
    divergence between the two: 45.8\. Note that the KL divergence between the same
    function will be always 0.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: FIM被定义为目标函数的协方差。让我们看看它如何帮助我们。为了限制我们模型的分布之间的距离，我们需要定义一个度量来提供新旧分布之间的距离。最常见的选择是使用KL散度。它衡量两个分布之间的差异，并在强化学习（RL）和机器学习中得到广泛应用。KL散度不是一个真正的度量，因为它不是对称的，但它是一个很好的近似值。两个分布之间的差异越大，KL散度的值就越高。考虑下图中的曲线。在这个例子中，KL散度是相对于绿色函数计算的。事实上，由于橙色函数与绿色函数相似，KL散度为1.11，接近于0。相反，很容易看出蓝色和绿色曲线差异较大。这个观察结果得到了它们之间KL散度45.8的确认。请注意，相同函数之间的KL散度始终为0。
- en: For those of you who are interested, the KL divergence for discrete probability
    distribution is computed as ![](img/05df10d1-9804-496e-8102-c25bb5fe9899.png).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有兴趣的读者，离散概率分布的KL散度计算公式为 ![](img/05df10d1-9804-496e-8102-c25bb5fe9899.png)。
- en: 'Let''s take a look at the following diagram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下以下图示：
- en: '![](img/83415e1d-05c7-4b7e-bbb9-03d15a43096e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83415e1d-05c7-4b7e-bbb9-03d15a43096e.png)'
- en: Figure 7.5\. The KL divergence that's shown in the box is measured between each
    function and the function colored in green. The bigger the value, the farther
    the two functions are apart.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5\. 盒子中显示的KL散度是测量每个函数与绿色着色函数之间的差异。数值越大，两者之间的差距越大。
- en: Thus, using the KL divergence, we are able to compare two distributions and
    get an indication of how they relate to each other. So, how can we use this metric
    in our problem and limit the divergence between two subsequent policies distribution?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，利用KL散度，我们能够比较两个分布，并获得它们相互关系的指示。那么，我们如何在问题中使用这个度量，并限制两个后续策略分布之间的散度呢？
- en: It so happens that the FIM defines the local curvature in the distribution space
    by using the KL divergence as a metric. Thereby, we can obtain the direction and
    the length of the step that keeps the KL divergence distance constant by combining
    the curvature (second-order derivative) of the KL divergence with the gradient
    (first-order derivative) of the objective function (as in formula (7.1)). Thus,
    the update that follows from formula (7.1) will be more cautious by taking small
    steps along the steepest direction when the FIM is high (meaning that there is
    a big distance between the action distributions) and big steps when the FIM is
    low (meaning that there is a plateau and the distributions don't vary too much).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，FIM通过使用KL散度作为度量，在分布空间中定义了局部曲率。因此，通过将KL散度的曲率（二阶导数）与目标函数的梯度（一阶导数）结合（如公式（7.1）中所示），我们可以获得保持KL散度距离恒定的方向和步长。因此，根据公式（7.1）得到的更新将在FIM较高时更为谨慎（意味着动作分布之间存在较大距离时），沿着最陡的方向小步前进，并在FIM较低时采取大步（意味着存在高原且分布变化不大时）。
- en: Natural gradient complications
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然梯度的复杂性
- en: Despite knowing the usefulness of the natural gradient in the RL framework,
    one of the major drawbacks of it is the computational cost that involves the calculation
    of FIM. While the computation of the gradient has a computational cost of ![](img/5d7df3b0-ef0f-43b1-81b6-692b6b72c407.png),
    the natural gradient has a computational cost of ![](img/c9e9563d-9439-47bd-a47e-95bcc8c1846b.png), where ![](img/c1febc28-3777-48a1-941c-268d9ba05b84.png)
    is the number of parameters. In fact, in the NPG paper that dates back to 2003,
    the algorithm has been applied to very small tasks with linear policies. The computation
    of ![](img/3dca5d3e-8058-4d21-80f2-2c6d786918f0.png) is too expensive with modern
    deep neural networks that have hundreds of thousands of parameters. Nonetheless,
    by introducing some approximations and tricks, the natural gradient can be also used
    with deep neural networks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管了解自然梯度在RL框架中的有用性，其主要缺点之一是涉及计算FIM的计算成本。而梯度的计算成本为![](img/5d7df3b0-ef0f-43b1-81b6-692b6b72c407.png)，自然梯度的计算成本为![](img/c9e9563d-9439-47bd-a47e-95bcc8c1846b.png)，其中![](img/c1febc28-3777-48a1-941c-268d9ba05b84.png)是参数数量。事实上，在2003年的NPG论文中，该算法已应用于具有线性策略的非常小的任务。然而，对于具有数十万参数的现代深度神经网络来说，计算![](img/3dca5d3e-8058-4d21-80f2-2c6d786918f0.png)的成本太高。尽管如此，通过引入一些近似和技巧，自然梯度也可以用于深度神经网络。
- en: In supervised learning, the use of the natural gradient is not needed as much
    as in reinforcement learning because the second-order gradient is somehow approximated
    in an empirical way by modern optimizers such as Adam and RMSProp.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，自然梯度的使用并不像在强化学习中那样必要，因为现代优化器（如Adam和RMSProp）可以以经验方式近似处理二阶梯度。
- en: Trust region policy optimization
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任区域策略优化
- en: '**Trust region policy optimization** (**TRPO**) is the first successful algorithm
    that makes use of several approximations to compute the natural gradient with
    the goal of training a deep neural network policy in a more controlled and stable
    way. From NPG, we saw that it isn''t possible to compute the inverse of the FIM
    for nonlinear functions with a lot of parameters. TRPO overcomes these difficulties
    by building on top of NPG. It does this by introducing a surrogate objective function
    and making a series of approximations, which means it succeeds in learning about
    complex policies for walking, hopping, or playing Atari games from raw pixels.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**信任区域策略优化**（**TRPO**）是第一个成功利用多种近似方法计算自然梯度的算法，其目标是以更受控且稳定的方式训练深度神经网络策略。从NPG中我们看到，对于具有大量参数的非线性函数计算FIM的逆是不可能的。TRPO通过在NPG基础上构建，克服了这些困难。它通过引入替代目标函数并进行一系列近似，成功学习复杂策略，例如从原始像素学习步行、跳跃或玩Atari游戏。'
- en: TRPO is one of the most complex model-free algorithms and though we already
    learned the underlying principles of the natural gradient, there are still difficult
    parts behind it. In this chapter, we'll only give an intuitive level of detail
    regarding the algorithm and provide the main equations. If you want to dig into
    the algorithm in more detail, check their paper ([https://arxiv.org/abs/1502.05477](https://arxiv.org/abs/1502.05477))
    for a complete explanation and proof of the theorems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO是最复杂的无模型算法之一，虽然我们已经了解了自然梯度的基本原理，但它背后仍然有许多困难的部分。在这一章中，我们只会给出算法的直观细节，并提供主要方程。如果你想深入了解该算法，查阅他们的论文
    ([https://arxiv.org/abs/1502.05477](https://arxiv.org/abs/1502.05477))，以获得完整的解释和定理证明。
- en: We'll also implement the algorithm and apply it to a Roboschool environment.
    Nonetheless, we won't discuss every component of the implementation here. For
    the complete implementation, check the GitHub repository of this book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将实现该算法，并将其应用于 Roboschool 环境。然而，我们不会在这里讨论实现的每个组件。有关完整的实现，请查看本书的 GitHub 仓库。
- en: The TRPO algorithm
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO 算法
- en: 'From a broad perspective, TRPO can be seen as a continuation of the NPG algorithm
    for nonlinear function approximation. The biggest improvement that was introduced
    in TRPO is the use of a constraint on the KL divergence between the new and the
    old policy that forms a *trust region.* This allows the network to take larger
    steps, always within the trust region. The resulting constraint problem is formulated
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义的角度来看，TRPO 可以视为 NPG 算法在非线性函数逼近中的延续。TRPO 引入的最大改进是对新旧策略之间的KL散度施加约束，形成 *信任区域*。这使得网络可以在信任区域内采取更大的步伐。由此产生的约束问题表述如下：
- en: '![](img/d6ad2df0-d457-498d-9040-fcf62c432f60.png) (7.2)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6ad2df0-d457-498d-9040-fcf62c432f60.png) (7.2)'
- en: Here, ![](img/566596fb-1cc6-4d2e-a26c-c0683f771a5c.png) is the objective surrogate
    function that we'll see soon, ![](img/eb787be5-e07c-4ebc-9303-af6c054265bb.png) is
    the KL divergence between the old policy with the ![](img/7bd85162-5171-4b32-8272-efb7a599dba7.png) parameters,
    and the new policy with
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/566596fb-1cc6-4d2e-a26c-c0683f771a5c.png) 是我们将很快看到的目标代理函数，![](img/eb787be5-e07c-4ebc-9303-af6c054265bb.png)
    是旧策略与 ![](img/7bd85162-5171-4b32-8272-efb7a599dba7.png) 参数之间的KL散度，以及新策略之间的KL散度。
- en: the ![](img/8fcdc5a0-205c-44a7-8338-0f437d7948bb.png) and ![](img/118cb98d-380e-41ec-97c8-e208698c3f45.png) parameters
    is a coefficient of the constraint.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/8fcdc5a0-205c-44a7-8338-0f437d7948bb.png) 和 ![](img/118cb98d-380e-41ec-97c8-e208698c3f45.png)
    参数是约束的系数。'
- en: 'The objective surrogate function is designed in such a way that it is maximized
    with respect to the new policy parameters using the state distribution of the
    old policy. This is done using importance sampling, which estimates the distribution
    of the new policy (the desired one) while only having the distribution of the
    old policy (the known distribution). Importance sampling is required because the
    trajectory was sampled with the old policy, but what we actually care about is
    the distribution of the new one. Using importance sampling, the surrogate objective
    function is defined:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目标代理函数的设计方式是，利用旧策略的状态分布最大化新的策略参数。这个过程通过重要性采样来完成，重要性采样估计新策略（期望策略）的分布，同时只拥有旧策略（已知分布）的分布。重要性采样是必要的，因为轨迹是根据旧策略采样的，但我们实际关心的是新策略的分布。使用重要性采样，代理目标函数定义为：
- en: '![](img/c5d873fc-2f25-4558-8be6-60b7b047b49d.png) (7.3)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5d873fc-2f25-4558-8be6-60b7b047b49d.png) (7.3)'
- en: '![](img/a493723c-4f11-4e85-a046-9e2aaaba2ae8.png) is the advantage function
    of the old policy. Thus, the constraint optimization problem is equivalent to
    the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/a493723c-4f11-4e85-a046-9e2aaaba2ae8.png) 是旧策略的优势函数。因此，约束优化问题等价于以下问题：'
- en: '![](img/511e5dac-0210-48f5-ad72-5583db302cbd.png) (7.4)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511e5dac-0210-48f5-ad72-5583db302cbd.png) (7.4)'
- en: Here, ![](img/a3131585-ed20-4e88-a731-4a6ce71eb67c.png) indicates the actions
    distributions conditioned on the state, ![](img/31a74e77-c25b-4db8-8b02-d3668510fc04.png).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/a3131585-ed20-4e88-a731-4a6ce71eb67c.png) 表示在状态条件下的动作分布，![](img/31a74e77-c25b-4db8-8b02-d3668510fc04.png)。
- en: What we are left to do is replace the expectation with an empirical average
    over a batch of samples and substitute ![](img/cbd8354d-7ff8-4b75-9c16-098f793154e3.png) with
    an empirical estimate.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要做的是，用一批样本的经验平均值来替代期望，并用经验估计替代 ![](img/cbd8354d-7ff8-4b75-9c16-098f793154e3.png)。
- en: 'Constraint problems are difficult to solve and in TRPO, the optimization problem
    in equation (7.4) is approximately solved by using a linear approximation of the
    objective function and a quadratic approximation to the constraint so that the
    solution becomes similar to the NPG update:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 约束问题难以解决，在TRPO中，方程(7.4)中的优化问题通过使用目标函数的线性近似和约束的二次近似来近似求解，使得解变得类似于NPG更新：
- en: '![](img/06ea7307-6019-452a-9c51-3e8bf4003822.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06ea7307-6019-452a-9c51-3e8bf4003822.png)'
- en: Here, ![](img/706a6722-8040-43f4-b1f7-c43338239009.png).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/706a6722-8040-43f4-b1f7-c43338239009.png)。
- en: 'The approximation of the original optimization problem can now be solved using
    the **Conjugate Gradient** (**CG**) method, an iterative method for solving linear
    systems. When we talked about NPG, we emphasize that computing ![](img/29560202-4d86-4173-803f-69befefbc670.png) is
    computationally very expensive with a large number of parameters. However, CG
    can approximately solve a linear problem without forming the full matrix, ![](img/b98d4fbf-7edb-4f5f-a6a7-16f13450a320.png). Thus,
    using CG, we can compute ![](img/d894bb78-8af2-4807-bf95-861935118d06.png) as
    follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以使用**共轭梯度**（**CG**）方法来求解原优化问题的近似解，这是一种用于求解线性系统的迭代方法。当我们谈到NPG时，我们强调计算![](img/29560202-4d86-4173-803f-69befefbc670.png)对于大参数量而言计算非常昂贵。然而，CG可以在不形成完整矩阵![](img/b98d4fbf-7edb-4f5f-a6a7-16f13450a320.png)的情况下近似求解线性问题。因此，使用CG时，我们可以按如下方式计算![](img/d894bb78-8af2-4807-bf95-861935118d06.png)：
- en: '![](img/de87bafe-7bf9-44dc-b851-917f9afe0688.png) (7.5)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de87bafe-7bf9-44dc-b851-917f9afe0688.png) (7.5)'
- en: 'TRPO also gives us a way of estimating the step size:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO还为我们提供了一种估计步长的方法：
- en: '![](img/ad8bc8ea-b7cc-4603-a0ca-5011874e2ffd.png) (7.6)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad8bc8ea-b7cc-4603-a0ca-5011874e2ffd.png) (7.6)'
- en: 'Therefore, the update becomes as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更新变为如下：
- en: '![](img/816d421d-da65-4a88-bb86-428bf93e8de1.png) (7.7)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/816d421d-da65-4a88-bb86-428bf93e8de1.png) (7.7)'
- en: 'So far, we have created a special case of the natural policy gradient step,
    but to complete the TRPO update, we are missing a key ingredient. Remember that
    we approximated the problem with the solution of a linear objective function and
    quadratic constraint. Thus, we are solving only a local approximation to the expected
    return. With the introduction of these approximations, we cannot be certain that
    the KL divergence constraint is still satisfied. To ensure the nonlinear constraint
    while improving the nonlinear objective, TRPO performs a line search to find the
    higher value, ![](img/1ea7c5d6-6679-488f-b2fe-0af78e933cab.png), that satisfies
    the constraint. The TRPO update with the line search becomes the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了自然策略梯度步骤的一个特例，但要完成TRPO更新，我们还缺少一个关键成分。记住，我们通过线性目标函数和二次约束的解来逼近问题。因此，我们只是在求解期望回报的局部近似解。引入这些近似后，我们不能确定KL散度约束是否仍然满足。为了在改进非线性目标的同时确保非线性约束，TRPO执行线搜索以找到满足约束的较高值，![](img/1ea7c5d6-6679-488f-b2fe-0af78e933cab.png)。带有线搜索的TRPO更新变为如下：
- en: '![](img/619b58e7-0948-4811-963b-8a0ee8824efb.png) (7.8)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/619b58e7-0948-4811-963b-8a0ee8824efb.png) (7.8)'
- en: It may seem to you that the line search is a negligible part of the algorithm,
    but as demonstrated in the paper, it has a fundamental role. Without it, the algorithm
    may compute large steps, causing catastrophic degradation in the performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 线搜索可能看起来是算法中微不足道的一部分，但正如论文中所展示的，它起着至关重要的作用。没有它，算法可能会计算出过大的步长，从而导致性能灾难性的下降。
- en: In terms of the TRPO algorithm, it computes a search direction with the conjugate
    gradient algorithm to find a solution for the approximated objective function
    and constraint. Then it uses a line search for the maximal step length, ![](img/42841f37-dbf5-4cee-ba15-1bbdafda1a17.png), so
    that the constraint on the KL divergence is satisfied and the objective is improved.
    To further increase the speed of the algorithm, the conjugate gradient algorithm
    also makes use of an efficient Fisher-Vector product (to learn more about it,
    check out the paper that can be found at [https://arxiv.org/abs/1502.05477paper](https://arxiv.org/abs/1502.05477)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在TRPO算法中，它使用共轭梯度算法计算搜索方向，以寻找逼近目标函数和约束的解。然后，它使用线搜索来找到最大步长，![](img/42841f37-dbf5-4cee-ba15-1bbdafda1a17.png)，从而满足KL散度的约束并改进目标。为了进一步提高算法的速度，共轭梯度算法还利用了高效的Fisher-Vector乘积（想了解更多，可以查看这篇论文：[https://arxiv.org/abs/1502.05477paper](https://arxiv.org/abs/1502.05477)）。
- en: 'TRPO can be integrated into an AC architecture where the critic is included
    in the algorithm to provide additional support to the policy (the actor) in the
    learning of the task. A high-level implementation of such an algorithm (that is,
    TRPO combined with a critic), when written in pseudocode, is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO 可以集成到 AC 架构中，其中评论员被包含在算法中，以为策略（演员）在任务学习中提供额外的支持。这样的算法的高级实现（即 TRPO 与评论员结合）用伪代码表示如下：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After this high-level overview of TRPO, we can finally start implementing it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 TRPO 进行概述之后，我们终于可以开始实现它了。
- en: Implementation of the TRPO algorithm
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO 算法的实现
- en: In this implementation section of the TRPO algorithm, we'll concentrate our
    efforts on the computational graph and the steps that are required to optimize
    the policy. We'll leave out the implementation of other aspects that we looked
    at in the previous chapters (such as the cycle to gather trajectories from the
    environment, the conjugate gradient algorithm, and the line search algorithm).
    However, make sure to check out the full code in this book's GitHub repository.
    The implementation is for continuous control.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TRPO 算法的实现部分，我们将集中精力在计算图和优化策略所需的步骤上。我们将省略在前面章节中讨论的其他方面的实现（例如从环境中收集轨迹的循环、共轭梯度算法和线搜索算法）。但是，请务必查看本书
    GitHub 仓库中的完整代码。该实现用于连续控制。
- en: 'First, let''s create all the placeholders and the two deep neural networks
    for the policy (the actor) and the value function (the critic):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建所有的占位符以及策略（演员）和价值函数（评论员）的两个深度神经网络：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are a few things to note here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点需要注意：
- en: The placeholder with the `old_` prefix refers to the tensors of the old policy.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有`old_`前缀的占位符指的是旧策略的张量。
- en: The actor and the critic are defined in two separate variable scopes because
    we'll need to select the parameters separately later.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员和评论员被定义在两个独立的变量作用域中，因为稍后我们需要分别选择这些参数。
- en: The action space is a Gaussian distribution with a covariance matrix that is
    diagonal and independent of the state. A diagonal matrix can then be resized as
    a vector with one element for each action. We also work with the logarithm of
    this vector.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作空间是一个高斯分布，具有对角矩阵的协方差矩阵，并且与状态独立。然后，可以将对角矩阵调整为每个动作一个元素的向量。我们还会使用这个向量的对数。
- en: 'Now, we can add normal noise to the predicted mean according to the standard
    deviation, clip the actions, and compute the Gaussian log likelihood, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以根据标准差将正常噪声添加到预测的均值中，对动作进行剪切，并计算高斯对数似然，步骤如下：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then have to compute the objective function, ![](img/9c129825-9522-4ccd-a3c7-ce6dbb51176d.png),
    the MSE loss function of the critic, and create the optimizer for the critic,
    as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要计算目标函数！[](img/9c129825-9522-4ccd-a3c7-ce6dbb51176d.png)、评论员的 MSE 损失函数，并为评论员创建优化器，步骤如下：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, the subsequent steps involve the creation of the graph for the points
    (2), (3), and (4), as given in the preceding pseudocode. Actually, (2) and (3)
    are not done in TensorFlow and so they aren''t part of the computational graph.
    Nevertheless, in the computational graph, we have to take care of some related
    things. The steps for this are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤涉及为前面伪代码中给出的（2）、（3）和（4）点创建计算图。实际上，（2）和（3）并不在 TensorFlow 中执行，因此它们不属于计算图的一部分。然而，在计算图中，我们必须处理一些相关的内容。具体步骤如下：
- en: Estimate the gradient of the policy loss function.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计策略损失函数的梯度。
- en: Define a procedure to restore the policy parameters. This is needed because
    in the line search algorithm, we'll optimize the policy and test the constraints,
    and if the new policy doesn't satisfy them, we'll have to restore the policy parameters
    and try with a smaller ![](img/b088313b-2e23-4184-a6a9-8ffc45d388e6.png) coefficient.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个过程来恢复策略参数。这是必要的，因为在进行线搜索算法时，我们将优化策略并测试约束条件，如果新策略不满足这些条件，我们将必须恢复策略参数并尝试使用更小的![](img/b088313b-2e23-4184-a6a9-8ffc45d388e6.png)系数。
- en: Compute the Fisher-vector product. It is an efficient way to compute ![](img/6e8cdfd0-5f49-4f07-a0de-80fd60b13eec.png) without
    forming the full ![](img/d786b2b8-bb88-453a-8ba3-5ad45f4e5a5c.png).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算费舍尔向量积。这是一种有效计算![](img/6e8cdfd0-5f49-4f07-a0de-80fd60b13eec.png)而不形成完整的![](img/d786b2b8-bb88-453a-8ba3-5ad45f4e5a5c.png)的方法。
- en: Compute the TRPO step.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 TRPO 步骤。
- en: Update the policy.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略。
- en: 'Let''s start from step 1, that is, estimating the gradient of the policy loss
    function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 1 步开始，也就是估计策略损失函数的梯度：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since we are working with vector parameters, we have to flatten them using `flatten_list`.
    `variable_in_scope` returns the trainable variables in `scope`. This function
    is used to get the variables of the actor since the gradients have to be computed
    with respect to these variables only.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是向量参数，因此必须使用`flatten_list`将其展平。`variable_in_scope`返回`scope`中的可训练变量。此函数用于获取演员的变量，因为梯度计算仅需针对这些变量。
- en: 'Regarding step 2, the policy parameters are restored in this way:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步骤2，策略参数是通过这种方式恢复的：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It iterates over each layer's variables and assigns the values of the old variables
    to the current one.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 它迭代每一层的变量，并将旧变量的值分配给当前变量。
- en: 'The Fisher-vector product of step 3 is done by calculating the second derivative
    of the KL divergence with respect to the policy variables:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3中的Fisher-向量积通过计算KL散度关于策略变量的二阶导数来完成：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Steps 4 and 5 involve the application of the updates to the policy, where `beta_ph`
    is ![](img/6a731882-7484-4235-87ad-dfa0688ebf4c.png), which is calculated using
    formula (7.6), and `alpha` is the rescaling factor found by line search:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4和5涉及将更新应用到策略中，其中`beta_ph`是![](img/6a731882-7484-4235-87ad-dfa0688ebf4c.png)，该值通过公式(7.6)计算，`alpha`是通过线性搜索找到的缩放因子：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note how, without ![](img/50429154-550d-4b3a-af35-805862f2b90e.png), the update
    can be seen as the NPG update.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在没有![](img/50429154-550d-4b3a-af35-805862f2b90e.png)的情况下，更新可以看作是NPG更新。
- en: 'The update is applied to each variable of the policy. The work is done by `p_v.assign_sub(upd_rsh)`,
    which assigns the `p_v - upd_rsh` values to `p_v`, that i,: ![](img/1e217f24-f0ed-41ec-941a-8dd03a5bdc24.png).
    The subtraction is due to the fact that we converted the objective function into
    a loss function.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 更新应用到策略的每个变量。此工作由`p_v.assign_sub(upd_rsh)`完成，它将`p_v - upd_rsh`的值赋给`p_v`，即：![](img/1e217f24-f0ed-41ec-941a-8dd03a5bdc24.png)。减法是因为我们将目标函数转换为损失函数。
- en: 'Now, let''s briefly see how all the pieces we implemented come together when
    we update the policy at every iteration of the algorithm. The snippets of code
    we''ll present here should be added after the innermost cycle where the trajectories
    are sampled. But before digging into the code, let''s recap what we have to do:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要回顾一下每次迭代更新策略时我们所实现的各个部分是如何协同工作的。我们将在此展示的代码片段应在最内层循环中添加，其中采样了轨迹。但在深入代码之前，让我们回顾一下我们需要做什么：
- en: Get the output, log probability, standard deviation, and parameters of the policy
    that we used to sample the trajectory. This policy is our old policy.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取输出、对数概率、标准差和我们用于采样轨迹的策略参数。这一策略是我们的旧策略。
- en: Get the conjugate gradient.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取共轭梯度。
- en: Compute the step length, ![](img/0137aa34-0139-4e38-9c80-5ed0d857a5a1.png).
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算步长，![](img/0137aa34-0139-4e38-9c80-5ed0d857a5a1.png)。
- en: Execute the backtracking line search to get ![](img/de031bca-d8ab-4690-90c8-9fbc5211c7f2.png).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行回溯线性搜索以获得![](img/de031bca-d8ab-4690-90c8-9fbc5211c7f2.png)。
- en: Run the policy update.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行策略更新。
- en: 'The first point is achieved by running a few operations:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第一点通过运行一些操作来实现：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The conjugate gradient algorithm requires an input function that returns the
    estimated Fisher Information Matrix, the gradient of the objective function, and
    the number of iterations (in TRPO, this is a value between 5 and 15):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 共轭梯度算法需要一个输入函数，该函数返回估算的Fisher信息矩阵、目标函数的梯度和迭代次数（在TRPO中，该值介于5到15之间）：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can then compute the step length, ![](img/14a11483-4f11-47a4-abd3-02cffc682185.png), `beta_np`,
    and the maximum coefficient, ![](img/42343ec7-8b72-4d73-8bdc-b9a70a05f3fe.png),
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以计算步长，![](img/14a11483-4f11-47a4-abd3-02cffc682185.png)，`beta_np`，以及最大系数，![](img/42343ec7-8b72-4d73-8bdc-b9a70a05f3fe.png)。
- en: '`best_alpha`, which satisfies the constraint using the backtracking line search
    algorithm, and run the optimization by feeding all the values to the computational
    graph:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用回溯线性搜索算法满足约束条件的`best_alpha`，并通过将所有值输入到计算图中运行优化：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see, `backtracking_line_search` takes a function called `DKL` that
    returns the KL divergence between the old and the new policy, the ![](img/53ada7b4-4686-498b-ae68-ecd38c630df9.png) coefficient
    (this is the constraint value), and the loss of the old policy. What `backtracking_line_search`
    does is, starting from ![](img/e55a1b33-07c4-459b-aa50-2476397c0b1a.png), incrementally
    decrease the value until it satisfies the following condition: the KL divergence
    is less than ![](img/81708741-4453-4f0f-a4c7-6342d29e7f48.png) and the new loss
    function has decreased.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，`backtracking_line_search`使用一个名为`DKL`的函数，该函数返回旧策略和新策略之间的KL散度，![](img/53ada7b4-4686-498b-ae68-ecd38c630df9.png)系数（这是约束值），以及旧策略的损失值。`backtracking_line_search`的操作是从![](img/e55a1b33-07c4-459b-aa50-2476397c0b1a.png)开始逐步减小该值，直到满足以下条件：KL散度小于![](img/81708741-4453-4f0f-a4c7-6342d29e7f48.png)，并且新损失函数已减小。
- en: 'To this end, the hyperparameters that are unique to TRPO are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TRPO特有的超参数如下：
- en: '`delta`, (![](img/d3132ac7-9721-486b-9de5-2eb7cb568311.png)), the maximum KL
    divergence between the old and new policy.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta`，(![](img/d3132ac7-9721-486b-9de5-2eb7cb568311.png))，旧策略和新策略之间的最大KL散度。'
- en: The number of conjugate iterations, `conj_iters`. Usually, it is a number between
    5 and 15.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共轭迭代次数`conj_iters`的数量。通常情况下，它是一个介于5和15之间的数字。
- en: Congratulations for coming this far! That was tough.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到这一步！那真的很难。
- en: Application of TRPO
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO的应用
- en: 'The efficiency and stability of TRPO allowed us to test it on new and more
    complex environments. We applied it on Roboschool. Roboschool and its Mujoco counterpart
    are often used as a testbed for algorithms that are able to control complex agents
    with continuous actions, such as TRPO. Specifically, we tested TRPO on RoboschoolWalker2d,
    where the task of the agent is to learn to walk as fast as possible. This environment
    is shown in the following figure. The environment terminates whenever the agent
    falls or when more than 1,000 timesteps have passed since the start. The state
    is encoded in a `Box` class of size 22 and the agent is controlled with 6 float
    values with a range of ![](img/23091402-2556-43b3-9c13-75e42b683263.png):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO的效率和稳定性使我们能够在新的更复杂的环境中进行测试。我们在Roboschool上应用了TRPO。Roboschool及其Mujoco对应物通常用作能够控制具有连续动作的复杂代理的算法的测试平台，例如TRPO。具体来说，我们在RoboschoolWalker2d上测试了TRPO，代理的任务是尽可能快地学会行走。环境如下图所示。当代理器倒下或者自开始以来已经过去超过1,000个时间步长时，环境就会终止。状态以大小为22的`Box`类编码，代理器用范围为![](img/23091402-2556-43b3-9c13-75e42b683263.png)的6个浮点值进行控制：
- en: '![](img/42124a47-710b-477f-8fdb-41e8fe5d6178.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42124a47-710b-477f-8fdb-41e8fe5d6178.png)'
- en: Figure 7.6\. Render of the RoboschoolWalker2d environment
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6\. RoboschoolWalker2d环境的渲染
- en: In TRPO, the number of steps to collect from an environment on each episode
    is called the *time horizon.* This number will also determine the size of the
    batch. Moreover, it can be beneficial to run multiple agents in parallel so as
    to collect more representative data of the environment. In this case, the batch
    size will be equal to the time horizon, multiplied by the number of agents. Although
    our implementation is not predisposed to running multiple agents in parallel,
    the same objective can be achieved by using a time horizon longer than the maximum
    number of steps allowed on each episode. For example, knowing that, in RoboschoolWalker2d,
    an agent has a maximum of 1,000 time steps to reach the goal, by using a time
    horizon of 6,000, we are sure that at least six full trajectories are run.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在TRPO中，每个episode从环境中收集的步数称为*time horizon*。这个数字还将决定批次的大小。此外，运行多个代理器并行可以收集更具代表性的环境数据。在这种情况下，批次大小将等于时间跨度乘以代理数量。尽管我们的实现不倾向于并行运行多个代理器，但使用比每个episode允许的最大步数更长的时间跨度可以达到相同的目标。例如，知道在RoboschoolWalker2d中，代理器最多可以进行1,000个时间步长以达到目标，通过使用6,000的时间跨度，我们可以确保至少运行六个完整的轨迹。
- en: 'We run TRPO with the hyperparameters that are reported in the following table.
    Its third column also shows the standard ranges for each hyperparameter:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用报告中的以下表格中列出的超参数来运行TRPO。其第三列还显示了每个超参数的标准范围：
- en: '| **Hyperparameter** | **For RoboschoolWalker2**  | **Range** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **用于RoboschoolWalker2** | **范围** |'
- en: '| Conjugate iterations | 10 | [7-10] |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 共轭迭代次数 | 10 | [7-10] |'
- en: '| Delta (δ) | 0.01 | [0.005-0.03] |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Delta (δ) | 0.01 | [0.005-0.03] |'
- en: '| Batch size (Time Horizon * Number of Agents) | 6000 | [500-20000] |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小（时间跨度*代理数量） | 6000 | [500-20000] |'
- en: The progress of TRPO (and PPO, as we'll see in the next section) can be monitored
    by specifically looking at the total reward accumulated in each game and the state
    values that were predicted by the critic.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO（以及在下一节中我们将看到的PPO）的进展可以通过具体观察每个游戏中累计的总奖励以及由评论者预测的状态值来进行监控。
- en: 'We trained for 6 million steps and the result of the performance is shown in
    the following diagram. With 2 million steps, it is able to reach a good score
    of 1,300 and it is able to walk fluently and with a moderate speed. In the first
    phase of training, we can note a transition period where the score decreases a
    little bit, probably due to a local optimum. After that, the agent recovers and
    improves until reaching a score of 1,250:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了600万步，性能结果如图所示。在200万步时，它能够达到一个不错的分数1300，并且能够流畅行走，速度适中。在训练的第一阶段，我们可以注意到一个过渡期，分数略微下降，可能是由于局部最优解。之后，智能体恢复并改进，直到达到1250分：
- en: '![](img/8d7621e3-f311-415a-b26d-8ee00a945409.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d7621e3-f311-415a-b26d-8ee00a945409.png)'
- en: Figure 7.7\. Learning curve of TRPO on RoboschoolWalker2d
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7\. TRPO 在 RoboschoolWalker2d 上的学习曲线
- en: 'Also, the predicted state value offers an important metric with which we can
    study the results. Generally, it is more stable than the total reward and is easier
    to analyze. The shown is provided in the following diagram. Indeed, it confirms
    our hypothesis since it is showing a smoother function in general, despite a few
    spikes around 4 million and 4\. 5 million steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，预测的状态值提供了一个重要的指标，帮助我们研究结果。通常，它比总奖励更稳定，也更容易分析。以下图所示，确实证明了我们的假设，因为它显示了一个总体上更平滑的函数，尽管在400万和450万步之间有几个峰值：
- en: '![](img/aac3f834-27e6-489f-8c25-23006681c25e.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aac3f834-27e6-489f-8c25-23006681c25e.png)'
- en: Figure 7.8\. State values predicted by the critic of TRPO on RoboschoolWalker2d
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8\. TRPO 在 RoboschoolWalker2d 上由评论者预测的状态值
- en: From this plot, it is also easier to see that after the first 3 million steps, the
    agent continues to learn, if even at a very slow rate.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，我们也更容易看到，在前三百万步之后，智能体继续学习，尽管学习速度非常慢。
- en: As you saw, TRPO is a pretty complex algorithm with many moving parts. Nonetheless,
    it constitutes as proof of the effectiveness of limiting the policy inside a trust
    region so as to keep the policy from deviating too much from the current distribution.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，TRPO是一个相当复杂的算法，涉及许多活动部分。尽管如此，它证明了将策略限制在信任区域内，以防止策略过度偏离当前分布的有效性。
- en: But can we design a simpler and more general algorithm that uses the same underlying
    approach?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们能否设计一个更简单、更通用的算法，使用相同的基本方法？
- en: Proximal Policy Optimization
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近端策略优化（Proximal Policy Optimization）
- en: A work by [Schulman and others](https://arxiv.org/pdf/1707.06347.pdf) shows
    that this is possible. Indeed, it uses a similar idea to TRPO while reducing the
    complexity of the method. This method is called **Proximal Policy Optimization**
    (**PPO**) and its strength is in the use of the first-order optimization only,
    without degrading the reliability compared to TRPO. PPO is also more general and
    sample-efficient than TRPO and enables multi updates with mini-batches.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[Schulman 等人](https://arxiv.org/pdf/1707.06347.pdf)的工作表明这是可能的。实际上，它采用了类似于TRPO的思想，同时减少了方法的复杂性。这种方法称为**近端策略优化**（**PPO**），其优势在于仅使用一阶优化，而不会降低与TRPO相比的可靠性。PPO也比TRPO更通用、样本效率更高，并支持使用小批量进行多次更新。'
- en: A quick overview
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速概述
- en: 'The main idea behind PPO is to clip the surrogate objective function when it
    moves away, instead of constraining it as it does in TRPO. This prevents the policy
    from making updates that are too large. The main objective is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的核心思想是在目标函数偏离时进行剪切，而不是像TRPO那样约束它。这防止了策略进行过大的更新。其主要目标如下：
- en: '![](img/204037a2-8946-4117-975c-5f6b49a65cc4.png) (7.9)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/204037a2-8946-4117-975c-5f6b49a65cc4.png) (7.9)'
- en: 'Here, ![](img/f066d0ab-a18a-4e26-939c-981c5c22b461.png) is defined as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/f066d0ab-a18a-4e26-939c-981c5c22b461.png) 的定义如下：
- en: '![](img/280ee6c1-5a25-4705-b19a-58df34726268.png) (7.10)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/280ee6c1-5a25-4705-b19a-58df34726268.png) (7.10)'
- en: What the objective is saying is that if the probability ratio, ![](img/76a3e45e-ea39-42c5-87c0-7cd767d7e763.png),
    between the new and the old policy is higher or lower than a constant, ![](img/ec8b96f4-57bc-4a2d-861c-dbde954b7eee.png),
    then the minimum value should be taken. This prevents ![](img/592512b4-4696-45ad-bd85-22b34fdf31b1.png) from
    moving outside the interval ![](img/4fa9b030-6938-45e7-a0b4-32b3d2844fa9.png). The
    value of ![](img/84c73744-eb16-45c2-836c-72acd5b2b77a.png) is taken as the reference
    point, ![](img/8d97faa8-64b4-4596-919c-cc9879f3c8bf.png).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 目标所表达的是，如果新旧策略之间的概率比，![](img/76a3e45e-ea39-42c5-87c0-7cd767d7e763.png)，高于或低于一个常数，![](img/ec8b96f4-57bc-4a2d-861c-dbde954b7eee.png)，则应取最小值。这可以防止
    ![](img/592512b4-4696-45ad-bd85-22b34fdf31b1.png) 超出区间 ![](img/4fa9b030-6938-45e7-a0b4-32b3d2844fa9.png)。取
    ![](img/84c73744-eb16-45c2-836c-72acd5b2b77a.png) 作为参考点，![](img/8d97faa8-64b4-4596-919c-cc9879f3c8bf.png)。
- en: The PPO algorithm
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO 算法
- en: 'The practical algorithm that is introduced in the PPO paper uses a truncated
    version of **Generalized Advantage Estimation** (**GAE**), an idea that was introduced
    for the first time in the paper [High-Dimensional Continuous Control using Generalized
    Advantage Estimation](https://arxiv.org/pdf/1506.02438.pdf). GAE calculates the
    advantage as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PPO 论文中介绍的实用算法使用了 **广义优势估计**（**GAE**）的截断版本，GAE 是在论文 [High-Dimensional Continuous
    Control using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438.pdf)
    中首次提出的一个概念。GAE 通过以下方式计算优势：
- en: '![](img/a16f4c12-b653-4188-a10c-d0301750c4f1.png) (7.11)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a16f4c12-b653-4188-a10c-d0301750c4f1.png) (7.11)'
- en: 'It does this instead of using the common advantage estimator:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 它这样做是为了替代常见的优势估计器：
- en: '![](img/72e54cc9-9d38-4877-a8a5-bd6dfb8b3088.png) (7.12)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72e54cc9-9d38-4877-a8a5-bd6dfb8b3088.png) (7.12)'
- en: 'Continuing with the PPO algorithm, on each iteration, *N* trajectories from
    multiple parallel actors are collected with time horizon *T*, and the policy is
    updated *K* times with mini-batches. Following this trend, the critic can also
    be updated multiple times using mini-batches. The following table contains standard
    values of every PPO hyperparameter and coefficient. Despite the fact that every
    problem needs ad hoc hyperparameters, it would be useful to get an idea of their
    ranges (reported in the third column of the table):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 继续讨论 PPO 算法，在每次迭代中，*N* 条轨迹来自多个并行演员，并且时间跨度为 *T*，策略更新 *K* 次，使用小批量。按照这个趋势，评论员也可以使用小批量进行多次更新。下表包含每个
    PPO 超参数和系数的标准值。尽管每个问题都需要特定的超参数，但了解它们的范围（见表格的第三列）仍然是有用的：
- en: '| **Hyperparameter** | **Symbol** | **Range** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **符号** | **范围** |'
- en: '| Policy learning rate  | - | [1e^(-5), 1e^(-3)] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 策略学习率 | - | [1e^(-5), 1e^(-3)] |'
- en: '| Number of policy iterations | K | [3, 15] |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 策略迭代次数 | K | [3, 15] |'
- en: '| Number of trajectories (equivalent to the number of parallel actors) | N
    | [1, 20] |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 轨迹数量（等同于并行演员数量）| N | [1, 20] |'
- en: '| Time horizon | T | [64, 5120] |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 时间跨度 | T | [64, 5120] |'
- en: '| Mini-batch size | - | [64, 5120] |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 小批量大小 | - | [64, 5120] |'
- en: '| Clipping coefficient | ∈ | 0.1 or 0.2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 裁剪系数 | ∈ | 0.1 或 0.2 |'
- en: '| Delta (for GAE) | δ | [0.9, 0.97] |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Delta（用于 GAE）| δ | [0.9, 0.97] |'
- en: '| Gamma (for GAE) | γ | [0.8, 0.995] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Gamma（用于 GAE）| γ | [0.8, 0.995] |'
- en: Implementation of PPO
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO 的实现
- en: Now that we have the basic ingredients of PPO, we can implement it using Python
    and TensorFlow.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了 PPO 的基本要素，可以使用 Python 和 TensorFlow 来实现它。
- en: The structure and implementation of PPO is very similar to the actor-critic
    algorithms but with only a few additional parts, all of which we'll explain here.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 的结构和实现与演员-评论员算法非常相似，但只多了一些附加部分，我们将在这里解释所有这些部分。
- en: 'One such addition is the generalized advantage estimation (7.11) that takes
    just a few lines of code using the already implemented `discounted_rewards` function,
    which computes (7.12):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个附加部分是广义优势估计（7.11），它只需要几行代码，利用已实现的 `discounted_rewards` 函数来计算（7.12）：
- en: '[PRE18]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `GAE` function is used in the `store` method of the `Buffer` class when
    a trajectory is stored:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`GAE` 函数在 `Buffer` 类的 `store` 方法中使用，当存储一条轨迹时：'
- en: '[PRE19]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, `...` stands for the lines of code that we didn't report.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `...` 代表我们没有报告的代码行。
- en: 'We can now define the clipped surrogate loss function (7.9):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义裁剪的替代损失函数（7.9）：
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It is quite intuitive and it doesn't need further explanation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这很直观，不需要进一步解释。
- en: 'The computational graph holds nothing new, but let''s go through it quickly:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图没有什么新东西，但我们还是快速过一遍：
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The code for interaction with the environment and the collection of the experience
    is equal to AC and TRPO. However, in the PPO implementation in this book's GitHub
    repository, you can find a simple implementation that uses multiple agents.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与环境交互和收集经验的代码与 AC 和 TRPO 相同。然而，在本书 GitHub 仓库中的 PPO 实现中，你可以找到一个简单的实现，使用了多个智能体。
- en: 'Once ![](img/11fd3429-de91-4e1a-99fc-3aa01f1b89e2.png) transitions (where *N*
    is the number of trajectories to run and *T* is the time horizon of each trajectory)
    are collected, we are ready to update the policy and the critic. In both cases,
    the optimization is run multiple times and done on mini-batches. But before it,
    we have to run `p_log` on the full batch because the clipped objective needs the
    action log probabilities of the old policy:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收集到过渡数据 ![](img/11fd3429-de91-4e1a-99fc-3aa01f1b89e2.png)（其中 *N* 是运行的轨迹数量，*T*
    是每条轨迹的时间跨度），我们就可以更新策略和评价器。在这两种情况下，优化会多次运行，并且在小批量上进行。但在此之前，我们必须在完整的批量上运行`p_log`，因为裁剪目标需要旧策略的动作对数概率：
- en: '[PRE22]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: On each optimization iteration, we shuffle the batch so that every mini-batch
    is different from the others.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次优化迭代时，我们会对批量数据进行打乱，以确保每个小批量与其他批量不同。
- en: That's everything for the PPO implementation, but keep in mind that before and
    after every iteration, we are also running the summaries that we will later use
    with TensorBoard to analyze the results and debug the algorithm. Again, we don't
    show the code here as it is always the same and is quite long, but you can go
    through it in the full form in this book's repository. It is fundamental for you
    to understand what each plot displays if you want to master these RL algorithms.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 PPO 实现的全部内容，但请记住，在每次迭代的前后，我们还会运行总结信息，稍后我们将使用 TensorBoard 来分析结果并调试算法。再次强调，我们这里不展示代码，因为它总是相同的且较长，但你可以在本书的仓库中查看完整代码。如果你想掌握这些强化学习算法，理解每个图表展示的内容是至关重要的。
- en: PPO application
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO 应用
- en: 'PPO and TRPO are very similar algorithms and we choose to compare them by testing
    PPO in the same environment as TRPO, namely RoboschoolWalker2d. We devoted the
    same computational resources for tuning both of the algorithms so that we have
    a fairer comparison. The hyperparameters for TRPO are the same as those we listed
    in the previous section but instead, the hyperparameters of PPO are shown in the
    following table:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 和 TRPO 是非常相似的算法，我们选择通过在与 TRPO 相同的环境中测试 PPO 来进行比较，即 RoboschoolWalker2d。我们为这两个算法调优时投入了相同的计算资源，以确保比较的公平性。TRPO
    的超参数与前一节列出的相同，而 PPO 的超参数则显示在下表中：
- en: '| **Hyperparameter** | **Value** |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **值** |'
- en: '| Neural network | 64, tanh, 64, tanh |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络 | 64, tanh, 64, tanh |'
- en: '| Policy learning rate  | 3e-4 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 策略学习率 | 3e-4 |'
- en: '| Number of actor iterations | 10 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 执行者迭代次数 | 10 |'
- en: '| Number of agents | 1 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 智能体数量 | 1 |'
- en: '| Time horizon | 5,000 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 时间跨度 | 5,000 |'
- en: '| Mini-batch size | 256 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 小批量大小 | 256 |'
- en: '| Clipping coefficient | 0.2 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 裁剪系数 | 0.2 |'
- en: '| Delta (for GAE) | 0.95 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Delta（用于 GAE） | 0.95 |'
- en: '| Gamma (for GAE) | 0.99 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Gamma（用于 GAE） | 0.99 |'
- en: 'A comparison between PPO and TRPO is shown in the following diagram. PPO needs
    more experience to take off, but once it reaches this state, it has a rapid improvement
    that outpaces TRPO. In these specific settings, PPO also outperforms TRPO in terms
    of its final performance. Keep in mind that further tuning of the hyperparameters
    could bring better and slightly different results:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 PPO 和 TRPO 的比较。PPO 需要更多的经验才能起步，但一旦达到这个状态，它会迅速提升，超过 TRPO。在这些特定设置下，PPO
    在最终表现上也超过了 TRPO。请记住，进一步调整超参数可能会带来更好的结果，且略有不同：
- en: '![](img/fb3db0f3-8ba5-4b5b-9589-febc68af8be2.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb3db0f3-8ba5-4b5b-9589-febc68af8be2.png)'
- en: Figure 7.9\. Comparison of performance between PPO and TRPO
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9. PPO 和 TRPO 性能比较
- en: 'A few personal observations: we found PPO more difficult to tune compared to
    TRPO. One reason for that is the higher number of hyperparameters in PPO. Moreover,
    the actor learning rate is one of the most important coefficients to tune, and
    if not properly tuned, it can greatly affect the final results. A great point
    in favor of TRPO is that it doesn''t have a learning rate and that the policy
    is conditioned on a few hyperparameters that are easy to tune. Instead, an advantage
    of PPO is that it''s faster and has been shown to work with a bigger variety of
    environments.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一些个人观察：我们发现与TRPO相比，PPO的调优更加困难。其原因之一是PPO中超参数的数量较多。此外，演员学习率是最重要的调优系数之一，如果没有正确调节，它会极大地影响最终结果。TRPO的一个大优点是它没有学习率，并且策略仅依赖于几个易于调节的超参数。而PPO的优势则在于其速度更快，且已被证明能在更广泛的环境中有效工作。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how policy gradient algorithms can be adapted to
    control agents with continuous actions and then used a new set of environments
    called Roboschool.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何将策略梯度算法应用于控制具有连续动作的智能体，并使用了一组新的环境，称为Roboschool。
- en: 'You also learned aboutand developed two advanced policy gradient algorithms:
    trust region policy optimization and proximal policy optimization. These algorithms
    make better use of the data sampled from the environment and both use techniques
    to limit the difference in the distribution of two subsequent policies. In particular,
    TRPO (as the name suggests) builds a trust region around the objective function
    using a second-order derivative and some constraints based on the KL divergence
    between the old and the new policy. PPO, on the other hand, optimizes an objective
    function similar to TRPO but using only a first-order optimization method. PPO
    prevents the policy from taking steps that are too large by clipping the objective
    function when it becomes too large.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习并开发了两种高级策略梯度算法：信任域策略优化和近端策略优化。这些算法更好地利用了从环境中采样的数据，并使用技术限制两个后续策略分布之间的差异。具体来说，TRPO（顾名思义）使用二阶导数和基于旧策略与新策略之间KL散度的一些约束，围绕目标函数构建了一个信任域。另一方面，PPO优化的目标函数与TRPO相似，但只使用一阶优化方法。PPO通过在目标函数过大时对其进行裁剪，从而防止策略采取过大的步伐。
- en: PPO and TRPO are still on-policy (like the other policy gradient algorithms)
    but they are more sample-efficient than AC and REINFORCE. This is due to the fact
    that TRPO, using a second-order derivative, is actually extracting a higher order
    of information from the data. The sample efficiency of PPO, on the other hand,
    is due to its ability to perform multiple policy updates on the same on-policy
    data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: PPO和TRPO仍然是基于策略的（与其他策略梯度算法一样），但它们比AC和REINFORCE更具样本效率。这是因为TRPO通过使用二阶导数，实际上从数据中提取了更高阶的信息。而PPO的样本效率则来自于其能够在相同的基于策略的数据上执行多次策略更新。
- en: Thanks to their sample efficiency, robustness, and reliability, TRPO and especially
    PPO are used in many very complex environments such as Dota ([https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其样本效率、鲁棒性和可靠性，TRPO，尤其是PPO，被广泛应用于许多复杂的环境中，如Dota（[https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)）。
- en: PPO and TRPO, as well as AC and REINFORCE, are stochastic gradient algorithms.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: PPO和TRPO，以及AC和REINFORCE，都是随机梯度算法。
- en: In the next chapter, we'll look at two policy gradient algorithms that are deterministic.
    Deterministic algorithms are an interesting alternative because they have some
    useful properties that cannot be replicated in the algorithms we have seen so
    far.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨两种确定性策略梯度算法。确定性算法是一个有趣的替代方案，因为它们具有一些在我们目前看到的算法中无法复制的有用特性。
- en: Questions
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How can a policy neural network control a continuous agent?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略神经网络如何控制连续的智能体？
- en: What's the KL divergence?
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是KL散度？
- en: What's the main idea behind TRPO?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TRPO背后的主要思想是什么？
- en: How is the KL divergence used in TRPO?
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL散度在TRPO中的作用是什么？
- en: What's the main benefit of PPO?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PPO的主要优点是什么？
- en: How does PPO achieve good sample efficiency?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PPO如何实现良好的样本效率？
- en: Further reading
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: If you are interested in the original paper of the NPG, read **A Natural Policy
    Gradient**: [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对NPG的原始论文感兴趣，可以阅读**自然策略梯度**：[https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)。
- en: For the paper that introduced the Generalized Advantage Function, please read *High-Dimensional
    Continuous Control Using Generalized Advantage Estimation*: [https://arxiv.org/pdf/1506.02438.pdf](https://arxiv.org/pdf/1506.02438.pdf).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于介绍广义优势函数的论文，请阅读 *高维连续控制与广义优势估计*： [https://arxiv.org/pdf/1506.02438.pdf](https://arxiv.org/pdf/1506.02438.pdf)。
- en: If you are interested in the original Trust Region Policy Optimization paper,
    then please read **Trust Region Policy** **Optimization**: [https://arxiv.org/pdf/1502.05477.pdf](https://arxiv.org/pdf/1502.05477.pdf).
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对原始的信任域政策优化论文感兴趣，请阅读 **信任域政策** **优化**： [https://arxiv.org/pdf/1502.05477.pdf](https://arxiv.org/pdf/1502.05477.pdf)。
- en: If you are interested in the original paper that introduced the Proximal Policy
    Optimization algorithm, then please read *Proximal Policy Optimization Algorithms*: [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对介绍**邻近政策优化**算法的原始论文感兴趣，请阅读 *邻近政策优化算法*： [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)。
- en: For a further explanation of Proximal Policy Optimization, read the following
    blog post: [https://openai.com/blog/openai-baselines-ppo/](https://openai.com/blog/openai-baselines-ppo/).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要更深入的邻近政策优化解释，请阅读以下博客文章：[https://openai.com/blog/openai-baselines-ppo/](https://openai.com/blog/openai-baselines-ppo/)。
- en: 'If you are interested in knowing how PPO has been applied on Dota 2, check
    the following blog post regarding OpenAI: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对PPO在Dota 2中的应用感兴趣，请查看以下关于OpenAI的博客文章：[https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)。
