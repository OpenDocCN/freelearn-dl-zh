- en: Playing Atari Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩 Atari 游戏
- en: an a machine learn how to play video games by itself and beat human players?
    Solving this problem is the first step toward general **artificial intelligence**
    (**AI**) in the field of gaming. The key technique to creating an AI player is
    **deep reinforcement learning**. In 2015, Google's DeepMind, one of the foremost
    AI/machine learning research teams (who are famous for building AlphaGo, the machine
    that beat Go champion Lee Sedol) proposed the deep Q-learning algorithm to build
    an AI player that can learn to play Atari 2600 games, and surpass a human expert
    on several games. This work made a great impact on AI research, showing the possibility
    of building general AI systems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器如何自己学习玩视频游戏并击败人类玩家？解决这个问题是通向游戏领域**人工智能**（**AI**）的第一步。创建 AI 玩家所需的关键技术是**深度强化学习**。2015
    年，谷歌的 DeepMind（该团队因开发击败围棋冠军李世石的机器 AlphaGo 而闻名）提出了深度 Q 学习算法，用于构建一个能够学习玩 Atari 2600
    游戏，并在多个游戏中超越人类专家的 AI 玩家。这项工作对 AI 研究产生了重大影响，展示了构建通用 AI 系统的可能性。
- en: In this chapter, we will introduce how to use gym to play Atari 2600 games,
    and then explain why the deep Q-learning algorithm works and how to implement
    it using TensorFlow. The goal is to be able to understand deep reinforcement learning
    algorithms and how to apply them to solve real tasks. This chapter will be a solid
    foundation to understanding later chapters, where we will be introducing more
    complex methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用 gym 来玩 Atari 2600 游戏，然后解释为什么深度 Q 学习算法有效，并且如何使用 TensorFlow 实现它。目标是能够理解深度强化学习算法以及如何应用它们来解决实际任务。本章将为理解后续章节奠定坚实的基础，后续章节将介绍更复杂的方法。
- en: 'The topics that we will cover in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖的主题如下：
- en: Introduction to Atari games
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atari 游戏介绍
- en: Deep Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 Q 学习
- en: Implementation of DQN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN 实现
- en: Introduction to Atari games
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari 游戏介绍
- en: 'Atari, Inc. was an American video game developer and home computer company
    founded in 1972 by Nolan Bushnell and Ted Dabney. In 1976, Bushnell developed
    the Atari video computer system, or Atari VCS (later renamed Atari 2600). Atari
    VCS was a flexible console that was capable of playing the existing Atari games,
    which included a console, two joysticks, a pair of paddles, and the combat game
    cartridge. The following screenshot depicts an Atari console:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Atari, Inc. 是一家美国的视频游戏开发公司和家用计算机公司，由 Nolan Bushnell 和 Ted Dabney 于 1972 年创立。1976
    年，Bushnell 开发了 Atari 视频计算机系统（或 Atari VCS，后来更名为 Atari 2600）。Atari VCS 是一款灵活的游戏主机，能够播放现有的
    Atari 游戏，包括主机、两个摇杆、一对控制器和一张战斗游戏卡带。以下截图展示了 Atari 主机：
- en: '![](img/c9a3ad57-527f-4974-8906-2430734aae5a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9a3ad57-527f-4974-8906-2430734aae5a.png)'
- en: Atari 2600 has more than 500 games that were published by Atari, Sears, and
    some third parties. Some famous games are Breakout, Pac-Man, Pitfall!, Atlantis,
    Seaquest, and Space Invaders.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 2600 拥有超过 500 款游戏，由 Atari、Sears 和一些第三方公司发布。一些著名的游戏包括《打砖块》（Breakout）、《吃豆人》（Pac-Man）、《陷阱》（Pitfall!）、《亚特兰蒂斯》（Atlantis）、《海底探险》（Seaquest）和《太空侵略者》（Space
    Invaders）。
- en: As a direct result of the North American video game crash of 1983, Atari, Inc.
    was closed and its properties were split in 1984\. The home computing and game
    console divisions of Atari were sold to Jack Tramiel under the name Atari corporation
    in July 1984.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 1983 年北美视频游戏崩溃的直接结果，Atari, Inc. 于 1984 年关闭，并将其资产拆分。Atari 的家用计算机和游戏主机部门于 1984
    年 7 月被 Jack Tramiel 以 Atari Corporation 名义收购。
- en: 'For readers who are interested in playing Atari games, here are several online
    Atari 2600 emulator websites where you can find many popular Atari 2600 games:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些有兴趣玩 Atari 游戏的读者，这里有几个在线 Atari 2600 模拟器网站，您可以在这些网站上找到许多流行的 Atari 2600 游戏：
- en: '[http://www.2600online.com/](http://www.2600online.com/)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.2600online.com/](http://www.2600online.com/)'
- en: '[http://www.free80sarcade.com/all2600games.php](http://www.free80sarcade.com/all2600games.php)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.free80sarcade.com/all2600games.php](http://www.free80sarcade.com/all2600games.php)'
- en: '[http://www.retrogames.cz/index.php](http://www.retrogames.cz/index.php)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.retrogames.cz/index.php](http://www.retrogames.cz/index.php)'
- en: 'Because our goal is to develop an AI player for these games, it is better to
    play with them first and understand their difficulties. The most important thing
    is to: relax and have fun!'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的目标是为这些游戏开发一个 AI 玩家，所以最好先玩这些游戏并了解它们的难点。最重要的是：放松并享受乐趣！
- en: Building an Atari emulator
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 Atari 模拟器
- en: 'OpenAI gym provides an Atari 2600 game environment with a Python interface.
    The games are simulated by the arcade learning environment, which uses the Stella
    Atari emulator. For more details, read the following papers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI gym 提供了一个具有 Python 接口的 Atari 2600 游戏环境。这些游戏由街机学习环境模拟，街机学习环境使用 Stella
    Atari 模拟器。有关更多细节，请阅读以下论文：
- en: 'MG Bellemare, Y Naddaf, J Veness, and M Bowling, *The arcade learning environment:
    An evaluation platform for general agents*, journal of Artificial Intelligence
    Research (2012)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MG Bellemare, Y Naddaf, J Veness 和 M Bowling，*街机学习环境：通用代理的评估平台*，《人工智能研究杂志》（2012）
- en: 'Stella: A Multi-Platform Atari 2600 VCS emulator, [http://stella.sourceforge.net/](http://stella.sourceforge.net/)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stella：一个多平台的 Atari 2600 VCS 模拟器，[http://stella.sourceforge.net/](http://stella.sourceforge.net/)
- en: Getting started
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速入门
- en: 'If you don''t have a full install of OpenAI `gym`, you can install the Atari
    environment dependencies via the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有完整安装 OpenAI `gym`，可以通过以下方式安装 Atari 环境的依赖项：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This requires the `cmake` tools. This command will automatically compile the
    arcade learning environment and its Python interface, `atari-py`. The compilation
    will take a few minutes on a common laptop, so go have a cup of coffee.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要 `cmake` 工具。此命令将自动编译街机学习环境及其 Python 接口 `atari-py`。编译将在普通笔记本上花费几分钟时间，因此可以去喝杯咖啡。
- en: 'After the Atari environment is installed, try the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 Atari 环境后，可以尝试以下操作：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If it runs successfully, a small window will pop up, showing the screen of
    the game `Breakout`, as shown in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果运行成功，将会弹出一个小窗口，显示 `Breakout` 游戏的屏幕，如下所示的截图所示：
- en: '![](img/cb33c342-7720-4f1e-b366-2e5335aad1bb.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb33c342-7720-4f1e-b366-2e5335aad1bb.png)'
- en: The meaning of the v0 suffix in the `Breakout` rom name will be explained later.
    We will use `Breakout` to test our algorithm for training an AI game player. In
    `Breakout`, several layers of bricks lie on the top of the screen. A ball travels
    across the screen, bouncing off the top and side walls of the screen. When a brick
    is hitted, the ball bounces away and the brick is destroyed, giving the player
    several points according to the color of the brick. The player loses a turn when
    the ball touches the bottom of the screen. In order to prevent this from happening,
    the player has to move the paddle to bounce the ball back.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`Breakout` 的 rom 名称中的 v0 后缀的含义将在稍后解释。我们将使用 `Breakout` 来测试我们的 AI 游戏玩家训练算法。在
    `Breakout` 中，几层砖块位于屏幕的顶部。一颗球在屏幕上移动，撞击屏幕的顶部和侧壁。当球击中砖块时，砖块会被销毁，球会反弹，并根据砖块的颜色为玩家提供一定的分数。当球触及屏幕底部时，玩家失去一次回合。为了避免这种情况，玩家需要移动挡板将球弹回。'
- en: 'Atari VCS uses a joystick as the input device for controlling Atari 2600 games.
    The total number of inputs that a joystick and a paddle can make is 18\. In the
    `gym` Atari environment, these actions are labeled as the integers ranged from
    0 to 17\. The meaning of each action is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Atari VCS 使用摇杆作为控制 Atari 2600 游戏的输入设备。摇杆和挡板能够提供的总输入数为 18。 在 `gym` Atari 环境中，这些动作被标记为从
    0 到 17 的整数。每个动作的含义如下：
- en: '| 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| NO OPERATION | FIRE | UP | RIGHT | LEFT | DOWN |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 无操作 | 开火 | 上 | 右 | 左 | 下 |'
- en: '| 6 | 7 | 8 | 9 | 10 | 11 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 7 | 8 | 9 | 10 | 11 |'
- en: '| UP+RIGHT | UP+LEFT | DOWN+RIGHT | DOWN+LEFT | UP+FIRE | RIGHT+FIRE |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 上+右 | 上+左 | 下+右 | 下+左 | 上+开火 | 右+开火 |'
- en: '| 12 | 13 | 14 | 15 | 16 | 17 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 13 | 14 | 15 | 16 | 17 |'
- en: '| LEFT+FIRE | DOWN+FIRE | UP+RIGHT+FIRE | UP+LEFT+FIRE | DOWN+RIGHT+FIRE |
    DOWN+LEFT+FIRE |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 左+开火 | 下+开火 | 上+右+开火 | 上+左+开火 | 下+右+开火 | 下+左+开火 |'
- en: 'One can use the following code to get the meanings of the valid actions for
    a game:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码获取游戏中有效动作的含义：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For `Breakout`, the actions include the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `Breakout`，动作包括以下内容：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To get the number of the actions, one can also use the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取动作的数量，也可以使用以下代码：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, the member variable, `action_space`, in `atari.env` stores all the information
    about the valid actions for a game. Typically, we only need to know the total
    number of valid actions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`atari.env`中的成员变量`action_space`存储了有关游戏有效动作的所有信息。通常，我们只需要知道有效动作的总数。
- en: 'We now know how to access the action information in the Atari environment.
    But, how do you control the game given these actions? To take an action, one can
    call the `step` function:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道如何访问 Atari 环境中的动作信息。但是，给定这些动作，如何控制游戏呢？要执行一个动作，可以调用 `step` 函数：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The input argument, `a`, is the action you want to execute, which is the index
    in the valid action list. For example, if one wants to take the `LEFT` action,
    the input should be `3` not `4`, or if one takes no action, the input should be
    `0`. The `step` function returns one of the following four values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输入参数`a`是你想要执行的动作，它是有效动作列表中的索引。例如，如果想执行`LEFT`动作，输入应该是`3`而不是`4`，或者如果不执行任何动作，输入应该是`0`。`step`函数返回以下四个值之一：
- en: '`Observation`: An environment-specific object representing your observation
    of the environment. For Atari, it is the screen image of the frame after the action
    is executed.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Observation`：一个环境特定的对象，表示你对环境的观察。对于Atari来说，它是执行动作后屏幕帧的图像。'
- en: '`Reward`: The amount of reward achieved by the action.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reward`：由动作获得的奖励数量。'
- en: '`Done`: Whether it''s time to reset the environment again. In Atari, if you
    lost your last life, `done` will be true, otherwise it is false.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Done`：是否到了重新初始化环境的时间。在Atari游戏中，如果你失去了最后一条生命，`done`会为真，否则为假。'
- en: '`Info`: Diagnostic information useful for debugging. It is not allowed to use
    this information in the learning algorithm, so usually we can ignore it.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Info`：有助于调试的诊断信息。不能在学习算法中使用这些信息，所以通常我们可以忽略它。'
- en: Implementation of the Atari emulator
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari模拟器的实现
- en: 'We are now ready to build a simple Atari emulator using gym. As with other
    computer games, the keyboard input used to control Atari games is as shown here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备使用gym构建一个简单的Atari模拟器。和其他电脑游戏一样，用于控制Atari游戏的键盘输入如下所示：
- en: '| *w* | *a* | *s* | *d* | *space* |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| *w* | *a* | *s* | *d* | *space* |'
- en: '| UP | LEFT | DOWN | RIGHT | FIRE |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 上 | 左 | 下 | 右 | 发射 |'
- en: 'To detect the keyboard inputs, we use the `pynput.keyboard` package, which
    allows us to control and monitor the keyboard ([http://pythonhosted.org/pynput/](http://pythonhosted.org/pynput/)).
    If the `pynput` package is not installed, run the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测键盘输入，我们使用`pynput.keyboard`包，它允许我们控制和监控键盘（[http://pythonhosted.org/pynput/](http://pythonhosted.org/pynput/)）。如果没有安装`pynput`包，请运行以下命令：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`pynput.keyboard` provides a keyboard listener used to capture keyboard events.
    Before creating a keyboard listener, the `Listener` class should be imported:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`pynput.keyboard`提供了一个键盘监听器，用于捕获键盘事件。在创建键盘监听器之前，应该导入`Listener`类：'
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Besides the `Listener` class, the other packages, such as `gym` and `threading`,
    are also necessary in this program.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`Listener`类，程序中还需要其他包，如`gym`和`threading`。
- en: 'The following code shows how to use `Listener` to capture keyboard inputs,
    that is, where one of the *w*, *a*, *s*, *d*, and *space* keys is pressed:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用`Listener`来捕获键盘输入，即按下了*W*、*A*、*S*、*D*或*space*键时的情况：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Actually, a keyboard listener is a Python `threading.Thread` object, and all
    callbacks will be invoked from the thread. In the `keyboard` function, the listener
    registers two callbacks: `on_press` , which is invoked when a key is pressed and
    `on_release` invoked when a key is released. This function uses a synchronized
    queue to share data between different threads. When *w*, *a*, *s*, *d*, or *space*
    is pressed, its ASCII value is sent to the queue, which can be accessed from another
    thread. If *esc* is pressed, a termination signal, `*-*`, is sent to the queue.
    Then, the listener thread stops when *esc* is released.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，键盘监听器是一个Python的`threading.Thread`对象，所有的回调都会从该线程中调用。在`keyboard`函数中，监听器注册了两个回调：`on_press`，当按下一个键时被调用，以及`on_release`，当一个键被释放时调用。该函数使用一个同步队列在不同线程之间共享数据。当*W*、*A*、*S*、*D*或*space*被按下时，其ASCII值会被发送到队列中，可以从另一个线程中访问。如果按下了*esc*键，一个终止信号`*-*`会被发送到队列中。然后，监听器线程会在*esc*键被释放时停止。
- en: 'Starting a keyboard listener has some restrictions on macOS X; that is, one
    of the following should be true:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 启动键盘监听器在macOS X上有一些限制；即以下条件之一应当为真：
- en: The process must run as root
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程必须以root权限运行
- en: The application must be white-listed under enable access for assistive devices
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序必须在辅助设备访问权限中列入白名单
- en: For more information, visit [https://pythonhosted.org/pynput/keyboard.html](https://pythonhosted.org/pynput/keyboard.html).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请访问[https://pythonhosted.org/pynput/keyboard.html](https://pythonhosted.org/pynput/keyboard.html)。
- en: Atari simulator using gym
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gym的Atari模拟器
- en: 'The other part of the emulator is the `gym` Atari simulator:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟器的另一部分是`gym` Atari模拟器：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first step is to create an `Atari` environment using `gym.make`. If you
    are interested in playing other games such as Seaquest or Pitfall, just change
    Breakout-v0 to Seaquest-v0 or `Pitfall-v0`. Then, `get_keys_to_action` is called
    to get the `key to action` mapping, which maps the ASCII values of *w*, *a*, *s*,
    *d,* and *space* to internal actions. Before the Atari simulator starts, the `reset`
    function must be called to reset the game parameters and memory, returning the
    first game screen image. In the main loop, `render` is called to render the Atari
    game at each step. The input action is pulled from the queue without blocking.
    If the action is the termination signal, -1, the game quits. Otherwise, this action
    is taken at the current step by running `atari.step`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用 `gym.make` 创建一个 `Atari` 环境。如果你有兴趣玩其他游戏，比如 Seaquest 或 Pitfall，只需将 Breakout-v0
    改为 Seaquest-v0 或 `Pitfall-v0`。然后，调用 `get_keys_to_action` 获取 `key to action` 映射，该映射将
    *w*、*a*、*s*、*d* 和 *space* 的 ASCII 值映射到内部动作。在 Atari 模拟器启动之前，必须调用 `reset` 函数来重置游戏参数和内存，并返回第一帧游戏画面。在主循环中，`render`
    会在每一步渲染 Atari 游戏。输入动作从队列中拉取，且不会阻塞。如果动作是终止信号 -1，游戏将退出。否则，执行当前步骤的动作，通过运行 `atari.step`。
- en: 'To start the emulator, run the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动模拟器，请运行以下代码：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Press the fire button to start the game and enjoy it! This emulator provides
    a basic framework for testing AI algorithms on the `gym` Atari environment. Later,
    we will replace the `keyboard` function with our AI player.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 按下射击按钮开始游戏并享受它！这个模拟器提供了一个用于在 `gym` Atari 环境中测试 AI 算法的基本框架。稍后，我们将用我们的 AI 玩家替换
    `keyboard` 功能。
- en: Data preparation
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Careful readers may notice that a suffix, v0, follows each game name, and come
    up with the following questions: *What is the meaning of v0?* *Is it allowable
    to replace it with v1 or v2?* Actually, this suffix has a relationship with the
    data preprocessing step for the screen images (observations) extracted from the
    Atari environment.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的读者可能会注意到每个游戏名称后都有一个后缀 v0，并产生以下问题：*v0 的意思是什么？* *是否可以将其替换为 v1 或 v2？* 实际上，这个后缀与从
    Atari 环境提取的屏幕图像（观察）进行数据预处理的步骤有关。
- en: There are three modes for each game, for example, Breakout, BreakoutDeterministic,
    and BreakoutNoFrameskip, and each mode has two versions, for example, Breakout-v0
    and Breakout-v4\. The main difference between the three modes is the value of
    the frameskip parameter in the Atari environment. This parameter indicates the
    number of frames (steps) the one action is repeated on. This is called the **frame-skipping**
    technique, which allows us to play more games without significantly increasing
    the runtime.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每个游戏有三种模式，例如 Breakout、BreakoutDeterministic 和 BreakoutNoFrameskip，每种模式有两个版本，例如
    Breakout-v0 和 Breakout-v4。三种模式的主要区别在于 Atari 环境中 frameskip 参数的值。这个参数表示一个动作重复的帧数（步骤数）。这就是所谓的
    **帧跳过** 技术，它让我们能够在不显著增加运行时间的情况下玩更多游戏。
- en: 'For Breakout, frameskip is randomly sampled from 2 to 5\. The following screenshots
    show the frame images returned by the `step` function when the action `LEFT` is
    submitted:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Breakout，frameskip 是从 2 到 5 随机抽样的。以下截图显示了当提交 `LEFT` 动作时，`step` 函数返回的帧画面：
- en: '![](img/9434b162-7c3b-4322-8015-f484166ff2cc.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9434b162-7c3b-4322-8015-f484166ff2cc.png)'
- en: 'For BreakoutDeterministic, frameskip is set to 3 for the game Space Invaders,
    and 4 for the other games. With the same `LEFT` action, the `step` function returns
    the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BreakoutDeterministic，Space Invaders 游戏的 frameskip 被设置为 3，其他游戏的 frameskip
    为 4。在相同的 `LEFT` 动作下，`step` 函数返回如下：
- en: '![](img/a7d881b9-f685-45ea-ac9f-2dbd28a0cdb9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7d881b9-f685-45ea-ac9f-2dbd28a0cdb9.png)'
- en: 'For BreakoutNoFrameskip, frameskip is always 1 for all of the games, meaning
    no frame-skipping. Similarly, the `LEFT` action is taken at each step:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BreakoutNoFrameskip，所有游戏的 frameskip 始终为 1，意味着没有帧跳过。类似地，`LEFT` 动作在每一步都会执行：
- en: '![](img/162abb44-4fb4-471b-a75e-a70638960a01.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/162abb44-4fb4-471b-a75e-a70638960a01.png)'
- en: These screenshots demonstrate that although the step function is called four
    times with the same action, `LEFT`, the final positions of the paddle are quite
    different. Because frameskip is 4 for BreakoutDeterministic, its paddle is the
    closest one to the left wall. For BreakoutNoFrameskip, frameskip is set to 1 so
    that its paddle is farthest from the left wall. For Breakout, its paddle lies
    in the middle because of frameskip being sampled from [2, 5] at each step.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些截图展示了尽管步进函数在相同的动作`LEFT`下被调用了四次，最终的球板位置却大不相同。由于BreakoutDeterministic的帧跳跃为4，所以它的球板离左墙最近。而BreakoutNoFrameskip的帧跳跃为1，因此它的球板离左墙最远。对于Breakout，球板处于中间位置，因为在每一步中，帧跳跃是从[2,
    5]中采样的。
- en: From this simple experiment, we can see the effect of the frameskip parameter.
    Its value is usually set to 4 for fast learning. Recall that there are two versions,
    v0 and v4, for each mode. Their main difference is the `repeat_action_probability`
    parameter. This parameter indicates the probability that a **no operation** (**NOOP**)
    action is taken, although another action is submitted. It is set to 0.25 for v0,
    and 0.0 for v4\. Because we want a deterministic Atari environment, the v4 version
    is selected in this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个简单的实验中，我们可以看到帧跳跃参数的效果。它的值通常设置为4，以便进行快速学习。回想一下，每个模式都有两个版本，v0和v4。它们的主要区别在于`repeat_action_probability`参数。这个参数表示尽管提交了另一个动作，仍然有概率采取**无操作**（**NOOP**）动作。对于v0，它的值设置为0.25，v4的值为0.0。由于我们希望得到一个确定性的Atari环境，本章选择了v4版本。
- en: If you have played some Atari games, you have probably noticed that the top
    region of the screen in a game usually contains the scoreboard, showing the current
    score you got and the number of lives you have. This information is not related
    to game playing, so that the top region can be cropped. Besides, the frame images
    returned by the step function are RGB images. Actually, in the Atari environment,
    colorful images do not provide more information than grayscale images; namely,
    one can play Atari games as usual with a gray screen. Therefore, it is necessary
    to keep only useful information by cropping frame images and converting them to
    grayscale.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你玩过一些Atari游戏，你可能注意到游戏画面的顶部区域通常包含记分板，显示你当前的得分和剩余生命数。这些信息与游戏玩法无关，因此顶部区域可以被裁剪掉。另外，通过步进函数返回的帧图像是RGB图像。实际上，在Atari环境中，彩色图像并不提供比灰度图像更多的信息；换句话说，使用灰度屏幕也可以照常玩Atari游戏。因此，有必要通过裁剪帧图像并将其转换为灰度图像来保留有用的信息。
- en: 'Converting an RGB image into a grayscale image is quite easy. The value of
    each pixel in a grayscale image represents the light intensity, which can be calculated
    by this formula:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将RGB图像转换为灰度图像非常简单。灰度图像中每个像素的值表示光强度，可以通过以下公式计算：
- en: '![](img/55c3aeec-c33c-49d0-ba30-6e35ed922f3f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55c3aeec-c33c-49d0-ba30-6e35ed922f3f.png)'
- en: 'Here, R, G, and B are the red, green, and blue channels of the RGB image, respectively.
    Given a RGB image with shape (height, width, channel), the following Python code
    can be used to convert it into grayscale:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，R、G和B分别是RGB图像的红色、绿色和蓝色通道。给定一个形状为(height, width, channel)的RGB图像，可以使用以下Python代码将其转换为灰度图像：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following image gives an example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片给出了一个示例：
- en: '![](img/3ef54390-7f38-4876-b5b9-ac784d86c1ca.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ef54390-7f38-4876-b5b9-ac784d86c1ca.png)'
- en: For cropping frame images, we use the `opencv-python` package or `cv2`, a Python
    wrapper around the original C++ OpenCV implementation. For more information, please
    visit the `opencv-python` website at [http://opencv-python-tutroals.readthedocs.io/en/latest/index.html](http://opencv-python-tutroals.readthedocs.io/en/latest/index.html).
    The `opencv-python` package provides basic image transformation operations such
    as image scaling, translation, and rotation. In this chapter, we only need the
    image scaling function resize, which takes the input image, image size, and interpolation
    method as the input arguments, and returns the resized image.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于裁剪帧图像，我们使用`opencv-python`包，或者称为`cv2`，它是一个Python包装器，封装了原始的C++ OpenCV实现。欲了解更多信息，请访问`opencv-python`的官方网站：[http://opencv-python-tutroals.readthedocs.io/en/latest/index.html](http://opencv-python-tutroals.readthedocs.io/en/latest/index.html)。`opencv-python`包提供了基本的图像转换操作，如图像缩放、平移和旋转。在本章中，我们只需要使用图像缩放函数resize，该函数接受输入图像、图像大小和插值方法作为输入参数，并返回缩放后的图像。
- en: 'The following code shows the image cropping operation, which involves two steps:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了图像裁剪操作，涉及两个步骤：
- en: Reshaping the input image such that the width of the resulting image equals
    the resized width, `84`, indicated by the `resized_shape` parameter.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑输入图像，使得最终图像的宽度等于通过`resized_shape`参数指定的调整后的宽度`84`。
- en: 'Cropping the top region of the reshaped image using `numpy` slicing:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`numpy`切片裁剪重塑图像的顶部区域：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For example, given a grayscale input image, the `cv2_resize_image` function
    returns a cropped image with size ![](img/e6e3287e-f259-4902-a935-7c2ca2ce9e38.png),
    as shown in the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一张灰度输入图像，`cv2_resize_image`函数会返回一张裁剪后的图像，大小为 ![](img/e6e3287e-f259-4902-a935-7c2ca2ce9e38.png)，如下图所示：
- en: '![](img/9ac9969b-d898-4ba6-8020-60c06f247212.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ac9969b-d898-4ba6-8020-60c06f247212.png)'
- en: So far, we have finished the data preparation. The data is now ready to be used
    to train our AI player.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完成了数据准备。数据现在已经可以用来训练我们的AI玩家了。
- en: Deep Q-learning
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: Here comes the fun part—the brain design of our AI Atari player. The core algorithm
    is based on deep reinforcement learning or deep RL. In order to understand it
    better, some basic mathematical formulations are required. Deep RL is a perfect
    combination of deep learning and traditional reinforcement learning. Without understanding
    the basic concepts about reinforcement learning, it is difficult to apply deep
    RL correctly in real applications, for example, it is possible that someone may
    try to use deep RL without defining state space, reward, and transition properly.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分——我们AI Atari玩家的大脑设计。核心算法基于深度强化学习（Deep RL）。为了更好地理解它，需要一些基本的数学公式。深度强化学习是深度学习和传统强化学习的完美结合。如果不理解强化学习的基本概念，很难在实际应用中正确使用深度强化学习。例如，可能会有人在没有正确定义状态空间、奖励和转移的情况下尝试使用深度强化学习。
- en: Well, don't be afraid of the difficulty of the formulations. We only need high
    school-level mathematics, and will not go deep into the mathematical proofs of
    why traditional reinforcement learning algorithms work. The goal of this chapter
    is to learn the basic Q-learning algorithm, to know how to extend it into the
    **deep Q-learning algorithm** (**DQN**), and to understand the intuition behind
    these algorithms. Besides, you will also learn what the advantages and disadvantages
    are of DQN, what exploration and exploitation are, why a replay memory is necessary,
    why a target network is needed, and how to design a convolutional neural network
    for state feature representation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，别害怕这些公式的难度。我们只需要高中水平的数学知识，不会深入探讨为什么传统强化学习算法有效的数学证明。本章的目标是学习基本的Q学习算法，了解如何将其扩展为**深度Q学习算法**（**DQN**），并理解这些算法背后的直觉。此外，你还将学习DQN的优缺点，什么是探索与开发，为什么需要重放记忆，为什么需要目标网络，以及如何设计一个卷积神经网络来表示状态特征。
- en: It looks quite interesting, doesn't it? We hope this chapter not only helps
    you to understand how to apply deep reinforcement learning to solve practical
    problems, but also opens a door for deep reinforcement learning research. For
    the readers who are familiar with convolutional neural networks, the Markov decision
    process, and Q-learning, skip the first section and go directly to the implementation
    of DQN.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很有趣，对吧？我们希望本章不仅帮助你理解如何应用深度强化学习来解决实际问题，也为深度强化学习研究打开了一扇门。对于已经熟悉卷积神经网络、马尔可夫决策过程和Q学习的读者，可以跳过第一部分，直接进入DQN的实现。
- en: Basic elements of reinforcement learning
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的基本元素
- en: 'First, let''s us recall some basic elements of reinforcement learning that
    we discussed in the first chapter:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下在第一章中讨论的一些强化学习的基本元素：
- en: '**State**: The state space defines all the possible states of the environment.
    In Atari games, a state is a screen image or a set of several consecutive screen
    images observed by the player at a given time, indicating the game status of that
    moment.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：状态空间定义了环境的所有可能状态。在Atari游戏中，状态是玩家在某一时刻观察到的屏幕图像或几张连续的屏幕图像，表示当时的游戏状态。'
- en: '**Reward function**: A reward function defines the goal of a reinforcement
    learning problem. It maps a state or a state-action pair of the environment to
    a real number, indicating the desirability of that state. The reward is the score
    received after taking a certain action in Atari games.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数**：奖励函数定义了强化学习问题的目标。它将环境的状态或状态-动作对映射到一个实数，表示该状态的可取性。在Atari游戏中，奖励是玩家在采取某个动作后获得的分数。'
- en: '**Policy function**: A policy function defines the behavior of the player at
    a given time, which maps the states of the environment to actions to be taken
    in those states.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略函数**：策略函数定义了玩家在特定时间的行为，它将环境的状态映射到在这些状态下应该采取的动作。'
- en: '**Value function**: A value function indicates which state or state-action
    pair is good in the long run. The value of a state is the total (or discounted)
    amount of reward a player can expect to accumulate over the future, starting from
    that state.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数**：价值函数表示在长期内哪个状态或状态-动作对是好的。一个状态的价值是玩家从该状态开始，未来可以预期积累的奖励的总和（或折扣后的总和）。'
- en: Demonstrating basic Q-learning algorithm
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示基本的Q学习算法
- en: 'To demonstrate the basic Q-learning algorithm, let''s consider a simple problem.
    Imagine that our agent (player) lives in a grid world. One day, she was trapped
    in a weird maze, as shown in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示基本的Q学习算法，我们来看一个简单的问题。假设我们的智能体（玩家）生活在一个网格世界中。一天，她被困在一个奇怪的迷宫中，如下图所示：
- en: '![](img/6b46b3ad-be5c-4d13-abb5-f669a88e09b1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b46b3ad-be5c-4d13-abb5-f669a88e09b1.png)'
- en: The maze contains six rooms. Our agent appears in Room 1, while she has no knowledge
    about the maze, that is, she doesn't know Room 6 has the sweetheart that is able
    to send her back home, or that Room 4 has a lightning bolt that strikes her. Therefore,
    she has to explore the maze carefully to escape as soon as possible. So, how do
    we  make our lovely agent learn from experience?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 迷宫包含六个房间。我们的智能体出现在房间1，但她对迷宫一无所知，也就是说，她不知道房间6有能够将她送回家的“心爱之物”，或者房间4有一个会击打她的闪电。因此，她必须小心地探索迷宫，尽快逃脱。那么，我们如何让我们可爱的智能体通过经验学习呢？
- en: 'Fortunately, her good friend Q-learning can help her survive. This problem
    can be represented as a state diagram, where each room is taken as a state and
    her movement from one room to another is considered as an action. The state diagram
    is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，她的好朋友Q学习可以帮助她生存下来。这个问题可以表示为一个状态图，其中每个房间作为一个状态，智能体从一个房间到另一个房间的移动视为一个动作。状态图如下所示：
- en: '![](img/509db329-0ece-43df-8691-d44997a5aa75.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/509db329-0ece-43df-8691-d44997a5aa75.png)'
- en: 'Here, an action is represented by an arrow and the number associated with an
    arrow is the reward of that state-action pair. For example, when our agent moves
    from Room 5 to Room 6, she gets 100 points because of achieving the goal. When
    she moves from Room 3 to Room 4, she get a negative reward because the lightning
    strike hurts her. This state diagram can also be represented by a matrix:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，动作用箭头表示，箭头上标记的数字是该状态-动作对的奖励。例如，当我们的智能体从房间5移动到房间6时，由于达到了目标，她会获得100分。当她从房间3移动到房间4时，她会得到一个负奖励，因为闪电击中了她。这个状态图也可以用矩阵表示：
- en: '| **state\action** | **1** | **2** | **3** | **4** | **5** | **6** |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **状态\动作** | **1** | **2** | **3** | **4** | **5** | **6** |'
- en: '| 1 | - | 0 | - | - | - | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 1 | - | 0 | - | - | - | - |'
- en: '| 2 | 0 | - | 0 | - | 0 | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | - | 0 | - | 0 | - |'
- en: '| 3 | - | 0 | - | -50 | - | - |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 3 | - | 0 | - | -50 | - | - |'
- en: '| 4 | - | - | 0 | - | - | - |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 4 | - | - | 0 | - | - | - |'
- en: '| 5 | - | 0 | - | - | - | 100 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | 0 | - | - | - | 100 |'
- en: '| 6 | - | - | - | - | - | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 6 | - | - | - | - | - | - |'
- en: The dash line in the matrix indicates that the action is not available in that
    state. For example, our agent cannot move from Room 1 to Room 6 directly because
    there is no door connecting them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的虚线表示在该状态下该动作不可用。例如，我们的智能体不能直接从房间1移动到房间6，因为两者之间没有连接的门。
- en: 'Let''s ![](img/c8412c07-9c66-46b2-a692-158c6b140f63.png) be a state, ![](img/61f535eb-890a-4d2c-af5f-905d0ad05cfc.png)
    be an action, ![](img/91112b13-bd5a-48aa-bcff-dd917bda7680.png) be the reward
    function, and ![](img/1baa24f0-4939-44da-afbe-c8572b2f76b7.png) be the value function.
    Recall that ![](img/57c05ff8-0396-4939-96b9-98ad751f6b59.png) is the desirability
    of the state-action pair ![](img/550d9049-162a-41a8-a884-8310644d63db.png) in
    the long run, meaning that our agent is able to make decisions about which room
    she enters based on ![](img/39cbb45d-6168-4c09-b7de-c13c0ec7ddcc.png). The Q-learning
    algorithm is very simple, which estimates ![](img/1f073dd4-441f-4c3a-b5a3-79025c0a1641.png)
    for each state-action pair via the following update rule:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![](img/c8412c07-9c66-46b2-a692-158c6b140f63.png) 是一个状态，![](img/61f535eb-890a-4d2c-af5f-905d0ad05cfc.png)
    是一个动作，![](img/91112b13-bd5a-48aa-bcff-dd917bda7680.png) 是奖励函数，![](img/1baa24f0-4939-44da-afbe-c8572b2f76b7.png)
    是价值函数。回忆一下，![](img/57c05ff8-0396-4939-96b9-98ad751f6b59.png) 是状态-动作对 ![](img/550d9049-162a-41a8-a884-8310644d63db.png)
    在长期内的期望回报，这意味着我们的智能体能够根据 ![](img/39cbb45d-6168-4c09-b7de-c13c0ec7ddcc.png) 来决定进入哪个房间。Q
    学习算法非常简单，它通过以下更新规则来估计每个状态-动作对的 ![](img/1f073dd4-441f-4c3a-b5a3-79025c0a1641.png)：
- en: '![](img/d6a07f4a-aab3-47ac-a2cd-37a192af0a27.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6a07f4a-aab3-47ac-a2cd-37a192af0a27.png)'
- en: Here, ![](img/46756389-7363-4b76-a9f9-dc9cd63afc31.png) is the current state, ![](img/7e3b8629-ecde-424d-bcab-aceae5e87687.png)
    is the next state after taking action ![](img/21ea2c3c-7fd9-4214-92e9-02031ef4d5b9.png)
    at ![](img/1f98e7be-c917-41e5-b561-0859edb01f46.png), ![](img/4fd06b2f-e807-4fc6-99dd-ddd4eb34b03c.png)
    is the set of the available actions at ![](img/8cdc93c0-28ac-41b7-878e-2dae3b464e3e.png),
    is the discount factor, and ![](img/0b053543-d0c4-485f-98eb-db30fd849896.png) is
    the learning rate. The discount factor ![](img/2f3f8ed1-0f61-4f7d-8112-f7763323f680.png)
    lies in [0,1]. A discount factor smaller than 1 means that our agent prefers the
    current reward more than past rewards.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/46756389-7363-4b76-a9f9-dc9cd63afc31.png) 是当前状态，![](img/7e3b8629-ecde-424d-bcab-aceae5e87687.png)
    是采取行动后进入的下一个状态，![](img/21ea2c3c-7fd9-4214-92e9-02031ef4d5b9.png) 是在 ![](img/1f98e7be-c917-41e5-b561-0859edb01f46.png)
    时的动作，![](img/4fd06b2f-e807-4fc6-99dd-ddd4eb34b03c.png) 是在 ![](img/8cdc93c0-28ac-41b7-878e-2dae3b464e3e.png)
    时可用的动作集， 是折扣因子，![](img/0b053543-d0c4-485f-98eb-db30fd849896.png) 是学习率。折扣因子 ![](img/2f3f8ed1-0f61-4f7d-8112-f7763323f680.png)
    的值位于 [0,1] 范围内。折扣因子小于 1 意味着我们的智能体更偏好当前的奖励，而非过去的奖励。
- en: 'In the beginning, our agent knows nothing about the value function, so ![](img/25114801-7fc6-40bd-b295-c57bc1ea4aa5.png)
    is initialized to 0 for all state-action pairs. She will explore from state to
    state until she reaches the goal. We call each exploration an episode, which consists
    of moving from the initial state (for example, Room 1) to the final state (for
    example, Room 6). The Q-learning algorithm is shown as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我们的智能体对价值函数一无所知，因此 ![](img/25114801-7fc6-40bd-b295-c57bc1ea4aa5.png) 被初始化为所有状态-动作对的
    0。她将从一个状态探索到另一个状态，直到达到目标。我们将每一次探索称为一个回合，它由从初始状态（例如，房间 1）到最终状态（例如，房间 6）组成。Q 学习算法如下所示：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A careful reader may ask a question about how to select action ![](img/4cb8f468-d816-409c-a988-c026494017a1.png)
    in state ![](img/6472d012-4927-484b-abb1-fbae05294eea.png), for example, is action ![](img/69c74860-fd8d-4d7e-a71b-13eb0e694079.png)
    randomly selected among all the possible actions or chosen using the policy derived
    from the current estimated value function, ![](img/62e69dcf-841b-492e-a49d-2c200bda5be8.png)?
    What is ![](img/996156af-478d-4a3e-b683-28051be43f3c.png)greedy? These questions
    are related to two important concepts, namely, exploration and exploitation. Exploration
    means trying something new to gather more information about the environment, while
    exploitation means making the best decision based on all the information you have.
    For example, trying a new restaurant is exploration and going to your favorite
    restaurant is exploitation. In our maze problem, the exploration is that our agent
    tries to enter a new room she hasn't visited before, while the exploitation is
    that she chooses her favorite room based on the information she gathered from
    the environment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 小心的读者可能会问一个问题：如何选择在状态![](img/6472d012-4927-484b-abb1-fbae05294eea.png)下的行动 ![](img/4cb8f468-d816-409c-a988-c026494017a1.png)，例如，行动 ![](img/69c74860-fd8d-4d7e-a71b-13eb0e694079.png)是从所有可能的行动中随机选择，还是根据当前估算的价值函数 ![](img/62e69dcf-841b-492e-a49d-2c200bda5be8.png)派生的策略来选择？什么是![](img/996156af-478d-4a3e-b683-28051be43f3c.png)贪心策略？这些问题涉及到两个重要的概念，即探索和利用。探索意味着尝试新事物以收集更多的环境信息，而利用则是根据你已有的信息做出最佳决策。例如，尝试一家新餐厅是探索，而去你最喜欢的餐厅则是利用。在我们的迷宫问题中，探索是我们的代理尝试进入一个她之前没有去过的新房间，而利用则是她根据从环境中收集到的信息选择她最喜欢的房间。
- en: 'Both exploration and exploitation are necessary in reinforcement learning.
    Without exploration, our agent is not able to get new knowledge about the environment,
    so she will make bad decisions again and again. Without exploitation, the information
    she got from exploration becomes meaningless since she doesn''t learn from it
    to better make a decision. Therefore, a balance or a trade-off between exploration
    and exploitation is indispensable. ![](img/a19c9030-f51d-47c0-b48f-d23865dad042.png)greedy
    is the simplest way to make such a trade-off:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和利用在强化学习中都是必要的。如果没有探索，我们的代理就无法获得关于环境的新知识，因此她将一遍又一遍地做出错误决策。如果没有利用，她从探索中获得的信息就会变得毫无意义，因为她无法从中学习以做出更好的决策。因此，探索和利用之间的平衡或权衡是必不可少的。![](img/a19c9030-f51d-47c0-b48f-d23865dad042.png)贪心策略是实现这种权衡的最简单方式：
- en: '| With probability | Randomly select an action among all the possible actions
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 以概率 | 从所有可能的行动中随机选择一个行动 |'
- en: '| With probability ![](img/dbc78da5-ebe1-41d6-85ab-4b87ab72140c.png) | Select
    the best action based on ![](img/172b2c80-720c-47f7-8788-ae4c1a1f2bc3.png),that
    is, pick so that ![](img/d7ff4c51-1c70-4dee-9b4a-4db1f51f9b18.png) is the largest
    among all the possible actions in state ![](img/1c87ec34-7ab7-4140-825a-f568094ad8e2.png)
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 以概率![](img/dbc78da5-ebe1-41d6-85ab-4b87ab72140c.png) | 基于![](img/172b2c80-720c-47f7-8788-ae4c1a1f2bc3.png)选择最佳行动，即选择使得 ![](img/d7ff4c51-1c70-4dee-9b4a-4db1f51f9b18.png)在状态 ![](img/1c87ec34-7ab7-4140-825a-f568094ad8e2.png)下所有可能的行动中最大
    |'
- en: 'To further understand how Q-learning works, let''s go through several steps
    by hand. For clarity, let''s set the learning rate ![](img/93d46ebc-a126-41e8-9278-35e55cc2b8d3.png)
    and discount factor ![](img/9837b06c-85ec-4df6-b321-95c61facad22.png). The following
    code shows the implementation of Q-learning in Python:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解Q学习的工作原理，我们通过几个步骤来手动演示。为清晰起见，我们设置学习率 ![](img/93d46ebc-a126-41e8-9278-35e55cc2b8d3.png)和折扣因子 ![](img/9837b06c-85ec-4df6-b321-95c61facad22.png)。以下代码展示了Q学习在Python中的实现：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After running for 100 episodes, the value function, ![](img/61445d4b-2ad1-4162-ad47-dc6144988430.png),
    converges to the following(for the readers who are curious about why this algorithm
    converges, refer to *Reinforcement Learning: An Introduction* by Andrew Barto
    and Richard S. Sutton):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 经过100轮训练后，价值函数 ![](img/61445d4b-2ad1-4162-ad47-dc6144988430.png)收敛到以下结果（对于那些对为什么该算法会收敛的读者，请参考*《强化学习：导论》*，作者为Andrew
    Barto和Richard S. Sutton）：
- en: '| **state\action** | **1** | **2** | **3** | **4** | **5** | **6** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **状态\行动** | **1** | **2** | **3** | **4** | **5** | **6** |'
- en: '| 1 | - | 64 | - | - | - | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 1 | - | 64 | - | - | - | - |'
- en: '| 2 | 51.2 | - | 51.2 | - | 80 | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 51.2 | - | 51.2 | - | 80 | - |'
- en: '| 3 | - | 64 | - | -9.04 | - | - |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 3 | - | 64 | - | -9.04 | - | - |'
- en: '| 4 | - | - | 51.2 | - | - | - |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 4 | - | - | 51.2 | - | - | - |'
- en: '| 5 | - | 64 | - | - | - | 100 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | 64 | - | - | - | 100 |'
- en: '| 6 | - | - | - | - | - | - |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 6 | - | - | - | - | - | - |'
- en: 'Therefore, the resulting state diagram becomes this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，得到的状态图变为：
- en: '![](img/b7faa768-8c30-4890-9abf-eeca5a176ca0.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7faa768-8c30-4890-9abf-eeca5a176ca0.png)'
- en: 'This indicates the following optimal paths to the goal state for all the other
    states:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明从其他所有状态到目标状态的最佳路径如下：
- en: '![](img/8beda865-6339-4c70-b8a2-7fd7c8104adf.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8beda865-6339-4c70-b8a2-7fd7c8104adf.png)'
- en: Based on this knowledge, our agent is able to go back home no matter which room
    she is in. More importantly, she becomes smarter and happier, achieving our goal
    to train a smart AI agent or player.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些知识，我们的智能体能够返回家中，无论她处于哪个房间。更重要的是，她变得更加聪明和快乐，实现了我们训练智能AI代理或玩家的目标。
- en: This simplest Q-learning algorithm can only handle discrete states and actions.
    For continuous states, it fails because the convergence is not guaranteed due
    to the existence of infinite states. How can we apply Q-learning in an infinite
    state space such as Atari games? The answer is replacing the tableau with neural
    networks to approximate the action-value function ![](img/95e1fcb0-c839-457f-b9cd-f05c1bd9b255.png).
    This is the intuition behind the *Playing Atari with deep reinforcement learning,*
    by Google DeepMind paper.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最简单的Q学习算法只能处理离散的状态和动作。对于连续状态，它无法处理，因为由于存在无限的状态，收敛性不能得到保证。我们如何在像Atari游戏这样的无限状态空间中应用Q学习？答案是用神经网络代替表格来近似动作-价值函数！[](img/95e1fcb0-c839-457f-b9cd-f05c1bd9b255.png)。这就是谷歌DeepMind论文《*Playing
    Atari with deep reinforcement learning*》背后的直觉。
- en: 'To extend the basic Q-learning algorithm into the deep Q-learning algorithm,
    there are two key questions that need to be answered:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将基本的Q学习算法扩展到深度Q学习算法，需要回答两个关键问题：
- en: What kind of neural networks can be used to extract high-level features from
    observed data such as screen images in the Atari environment?
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用什么样的神经网络来从Atari环境中的观察数据（如屏幕图像）中提取高级特征？
- en: How can we update the action-value function, ![](img/38244a64-bc38-4734-8b1f-48b3af6f641f.png),
    at each training step?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在每个训练步骤中更新动作-价值函数，![](img/38244a64-bc38-4734-8b1f-48b3af6f641f.png)？
- en: 'For the first question, there are several possible ways of approximating the
    action-value function, ![](img/ba2ff8d1-d8b1-42d2-a019-2eef187c1257.png). One
    approach is that both the state and the action are used as the inputs to the neural
    network, which outputs the scalar estimates of their Q-value, as shown in the
    following diagram:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题，有几种方法可以近似动作-价值函数，![](img/ba2ff8d1-d8b1-42d2-a019-2eef187c1257.png)。一种方法是将状态和动作都作为神经网络的输入，网络输出它们的Q值标量估计，如下图所示：
- en: '![](img/7d9f4c83-e398-4c2a-982a-d26f7b745e53.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d9f4c83-e398-4c2a-982a-d26f7b745e53.png)'
- en: 'The main disadvantage of this approach is that an additional forward pass is
    required to compute ![](img/078a4917-00a8-4cbe-b266-891afc82f98a.png) as the action
    is taken as one of the inputs to the network, resulting in a cost that scales
    linearly with the number of all the possible actions. Another approach is that
    the state is the only input to the neural network, while there is a separate output
    for each possible action:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要缺点是需要额外的前向传播来计算！[](img/078a4917-00a8-4cbe-b266-891afc82f98a.png)，因为动作被作为输入之一传递到网络中，这会导致计算成本与所有可能动作的数量成线性关系。另一种方法是只将状态作为神经网络的输入，而每个可能的动作都有一个独立的输出：
- en: '![](img/aa2fd706-8f54-4775-b55f-acd13be9d66b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa2fd706-8f54-4775-b55f-acd13be9d66b.png)'
- en: The main advantage of this approach is the ability to compute Q-values for all
    possible actions in a given state with only a single forward pass through the
    network, and the simplicity to access the Q-value for an action by picking the
    corresponding output head.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优点是能够通过网络的单次前向传播计算出给定状态下所有可能动作的Q值，而且通过选择相应的输出头可以轻松获取某个动作的Q值。
- en: 'In the deep Q-network, the second architecture is applied. Recall that the
    output in the data preprocessing step is an ![](img/99b1f14f-8ad5-49e5-aca9-58720f322382.png)
    grayscale frame image. However, the current screen is not enough for playing Atari
    games because it doesn''t contain the dynamic information about game status. Take
    Breakout as an example; if we only see one frame, we can only know the locations
    of the ball and the paddle, but we cannot know the direction or the velocity of
    the ball. Actually, the direction and the velocity are quite important for making
    decisions about how to move the paddle. Without them, the game is unplayable.
    Therefore, instead of taking only one frame image as the input, the last four
    frame images of a history are stacked together to produce the input to the neural
    network. These four frames form an ![](img/47201bd5-f4f3-43b5-bded-eef0f697882e.png)
    image. Besides the input layer, the Q-network contains three convolutional layers
    and one fully connected layer, which is  shown as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度 Q 网络中，应用了第二种架构。回顾一下，数据预处理步骤中的输出是一个 ![](img/99b1f14f-8ad5-49e5-aca9-58720f322382.png)
    灰度帧图像。然而，当前的屏幕图像不足以进行 Atari 游戏，因为它不包含游戏状态的动态信息。以 Breakout 为例；如果我们只看到一帧，我们只能知道球和球拍的位置，但无法得知球的方向或速度。实际上，方向和速度对于决定如何移动球拍至关重要。如果没有它们，游戏就无法进行。因此，输入网络的不仅是单独的一帧图像，而是历史中的最后四帧图像被堆叠在一起，形成网络的输入。这四帧组成一个
    ![](img/47201bd5-f4f3-43b5-bded-eef0f697882e.png) 图像。除了输入层，Q 网络包含三层卷积层和一层全连接层，如下所示：
- en: '![](img/b942fdc1-a948-431a-9cae-d435343282b0.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b942fdc1-a948-431a-9cae-d435343282b0.png)'
- en: The first convolutional layer has 64 ![](img/ed5987c9-3441-4a2d-b1b4-0057f5970083.png)
    filters with stride 4, followed by a **rectifier nonlinearity** (**RELU**). The
    second convolutional layer has 64 ![](img/8594e75f-261f-4df0-9826-e7cf9ca7bdcb.png)
    filters with stride 2, followed by RELU. The third convolutional layer has 64 ![](img/6d194e05-96a0-4f1f-a044-322cf4760b64.png)
    filters with stride 2, followed by RELU. The fully connected hidden layer has
    512 hidden units, again followed by RELU. The output layer is also a fully connected
    layer with a single output for each action.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层卷积层有64个 ![](img/ed5987c9-3441-4a2d-b1b4-0057f5970083.png) 卷积核，步长为4，之后接一个**整流线性单元**（**RELU**）。第二层卷积层有64个
    ![](img/8594e75f-261f-4df0-9826-e7cf9ca7bdcb.png) 卷积核，步长为2，之后接RELU。第三层卷积层有64个
    ![](img/6d194e05-96a0-4f1f-a044-322cf4760b64.png) 卷积核，步长为2，之后接RELU。全连接的隐藏层有512个隐藏单元，再次接RELU。输出层也是一个全连接层，每个动作对应一个输出。
- en: Readers who are familiar with convolutional neural networks may ask why the
    first convolutional layer uses a ![](img/bdc4b421-2e99-4d82-a67c-543baa012ebf.png)
    filter, instead of a ![](img/f7b07854-81ff-47a0-8555-a3c3dc499e66.png) filter
    or a ![](img/030bd2b9-7e5f-455c-9a76-72f3cb6c1c26.png) filter that is widely applied
    in computer vision applications. The main reason of using a big filter is that
    Atari games usually contain very small objects such as a ball, a bullet, or a
    pellet. A convolutional layer with larger filters is able to zoom in on these
    small objects, providing benefits for learning feature representations of states.
    For the second and third convolutional layers, a relatively small filter is enough
    to capture useful features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉卷积神经网络的读者可能会问，为什么第一层卷积层使用了一个 ![](img/bdc4b421-2e99-4d82-a67c-543baa012ebf.png)
    卷积核，而不是广泛应用于计算机视觉中的 ![](img/f7b07854-81ff-47a0-8555-a3c3dc499e66.png) 卷积核或 ![](img/030bd2b9-7e5f-455c-9a76-72f3cb6c1c26.png)
    卷积核。使用大卷积核的主要原因是，Atari 游戏通常包含非常小的物体，如球、子弹或弹丸。使用较大的卷积核的卷积层能够放大这些小物体，有助于学习状态的特征表示。对于第二层和第三层卷积层，较小的卷积核足以捕捉到有用的特征。
- en: 'So far, we have discussed the architecture of the Q-network. But, how do we
    train this Q-network in the Atari environment with an infinite state space? Is
    it possible to develop an algorithm based on the basic Q-learning to train it?
    Fortunately, the answer is YES. Recall that the update rule for ![](img/f85e009b-d049-4650-ba9e-1263cc12b425.png)
    in  basic Q-learning is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了 Q 网络的架构。那么，我们如何在具有无限状态空间的 Atari 环境中训练这个 Q 网络呢？是否可以基于基本的 Q 学习算法来训练它？幸运的是，答案是肯定的。回顾一下，基本
    Q 学习中 ![](img/f85e009b-d049-4650-ba9e-1263cc12b425.png) 的更新规则如下：
- en: '![](img/c1e957d0-cd19-47d2-b5f5-3191f8d8a01c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1e957d0-cd19-47d2-b5f5-3191f8d8a01c.png)'
- en: 'When the learning rate ![](img/898498da-07ed-4c98-bfd1-6a7e3b3cbad2.png), this
    update rule becomes as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习率为 ![](img/898498da-07ed-4c98-bfd1-6a7e3b3cbad2.png) 时，该更新规则变为如下形式：
- en: '![](img/68f68524-574b-45f6-a65f-6e6192463730.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68f68524-574b-45f6-a65f-6e6192463730.png)'
- en: 'This is called the **Bellman equation**. Actually, the Bellman equation is
    the backbone of many reinforcement learning algorithms. The algorithms using the
    Bellman equation as an iterative update are called value iteration algorithms.
    In this book, we will not go into  detail about value iteration or policy iteration.
    If you are interested in them, refer to *Reinforcement Learning: An Introduction,*
    by Andrew Barto and Richard S. Sutton.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的**贝尔曼方程**。实际上，贝尔曼方程是许多强化学习算法的核心。使用贝尔曼方程作为迭代更新的算法称为值迭代算法。在本书中，我们不会详细讨论值迭代或策略迭代。如果你对它们感兴趣，可以参考Andrew
    Barto和Richard S. Sutton的《强化学习：导论》。
- en: 'The equation just shown is only suitable for a deterministic environment where
    the next state ![](img/ebe2a0c8-a332-4a3f-9fe2-8e72203d3dd6.png) is fixed given
    the current state ![](img/8fb40cc4-2a7a-4e08-8db1-8ee4b723133a.png) and the action
    ![](img/f4ad75de-a4fe-42a7-95b4-78c0a40e42a6.png). In a nondeterministic environment,
    the Bellman equation should be as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才显示的方程仅适用于确定性环境，其中给定当前状态 ![](img/8fb40cc4-2a7a-4e08-8db1-8ee4b723133a.png)和动作 ![](img/f4ad75de-a4fe-42a7-95b4-78c0a40e42a6.png)，下一个状态 ![](img/ebe2a0c8-a332-4a3f-9fe2-8e72203d3dd6.png)是固定的。在非确定性环境中，贝尔曼方程应该如下：
- en: '![](img/c35d79b3-5e13-4c9f-940f-797b4f83d451.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c35d79b3-5e13-4c9f-940f-797b4f83d451.png)'
- en: 'Here, the right-hand side takes the expectation of ![](img/af3e312f-7d10-4d3f-a22a-3fe178d92f04.png)
    with respect to the next state ![](img/3e11b272-3878-432a-ba38-a196a97e28e3.png)
    (for example, the distribution of ![](img/4cea44a6-e9d9-460d-b93b-b7d30189f66b.png)
    is determined by the Atari emulator). For an infinite state space, it is common
    to use a function approximator such as the Q-network to estimate the action-value
    function ![](img/42b48d0e-14b8-46e4-b9c4-be0abead3f0d.png). Then, instead of iteratively
    updating ![](img/06dc14b7-65d8-45cb-a20f-dbbe702cb163.png), the Q-network can
    be trained by minimizing the following loss function at the *i*^(th) iteration:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，右侧是关于下一个状态 ![](img/3e11b272-3878-432a-ba38-a196a97e28e3.png)的期望值（例如，![](img/4cea44a6-e9d9-460d-b93b-b7d30189f66b.png)的分布由Atari模拟器确定）。对于无限状态空间，通常使用函数逼近器（如Q网络）来估计动作价值函数 ![](img/42b48d0e-14b8-46e4-b9c4-be0abead3f0d.png)。然后，Q网络可以通过最小化以下损失函数，在第*i*次迭代中进行训练，而不是迭代更新 ![](img/06dc14b7-65d8-45cb-a20f-dbbe702cb163.png)：
- en: '![](img/61ea8633-6814-45b2-aa03-2dc4d7421f53.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61ea8633-6814-45b2-aa03-2dc4d7421f53.png)'
- en: Here, *Q(s,a;)* represents the Q-network parameterized by, ![](img/6f25c491-93b4-47e4-b64e-ac207c62e209.png) is
    the target for the i^(th) iteration, and ![](img/acba632a-dfff-4842-a8aa-32899fa0be04.png)
    is a probability distribution over sequences and actions. The parameters from
    the previous iteration *i-1* are fixed when optimizing the loss function ![](img/2fd194bc-effd-41fc-87aa-0aef0063524b.png)
    over ![](img/c40e80b6-15d7-4440-85eb-c61a1d535023.png). In practice, it is impossible
    to exactly calculate the expectations in ![](img/899b8f6b-bdc7-4c4a-bdbe-e6aa278babbd.png).
    Instead of optimizing ![](img/1ae281d7-5523-445f-afaf-69e80f91703b.png) directly,
    we minimize the empirical loss of ![](img/fdb4b635-9045-4ce5-a410-ceb03c5644da.png),
    which replaces the expectations by samples ![](img/2cd96ade-1021-4337-af9b-8ab6e8f99a8d.png) from
    the probability distribution ![](img/e4001b8d-2541-49a0-a2b9-1a4003a0d067.png)
    and the Atari emulator. As with other deep learning algorithms, the empirical
    loss function can be optimized by stochastic gradient descent.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Q(s,a;)*表示由 ![](img/6f25c491-93b4-47e4-b64e-ac207c62e209.png)参数化的Q网络，![](img/acba632a-dfff-4842-a8aa-32899fa0be04.png)是第*i*次迭代的目标，![](img/e4001b8d-2541-49a0-a2b9-1a4003a0d067.png)是序列和动作的概率分布。在优化损失函数 ![](img/2fd194bc-effd-41fc-87aa-0aef0063524b.png)时，来自前一次迭代*i-1*的参数是固定的，损失函数是关于 ![](img/c40e80b6-15d7-4440-85eb-c61a1d535023.png)的。在实际应用中，无法精确计算 ![](img/899b8f6b-bdc7-4c4a-bdbe-e6aa278babbd.png)中的期望值。因此，我们不会直接优化 ![](img/1ae281d7-5523-445f-afaf-69e80f91703b.png)，而是最小化 ![](img/fdb4b635-9045-4ce5-a410-ceb03c5644da.png)的经验损失，它通过从概率分布 ![](img/e4001b8d-2541-49a0-a2b9-1a4003a0d067.png)和Atari模拟器中获得的样本 ![](img/2cd96ade-1021-4337-af9b-8ab6e8f99a8d.png)来替代期望值。与其他深度学习算法一样，经验损失函数可以通过随机梯度下降法进行优化。
- en: This algorithm doesn't need to construct an estimate of the emulator, for example,
    it doesn't need to know the internal game mechanism about the Atari emulator,
    because it only uses samples from the emulator to solve the reinforcement learning
    problem. This property is called **model-free**, namely, it can treat the underlying
    model as a black box. Another property of this algorithm is off-policy. It learns
    about the greedy policy ![](img/9a9692db-dc07-4a3d-be2a-87472934c71d.png) while
    following the probability distribution ![](img/127e4a41-2c25-4614-91bd-00e1c3d60cf4.png)
    that balances exploration and exploitation of the state space. As discussed previously,
    ![](img/84a647b9-5a68-495a-bf24-7993adaac0ad.png) can be selected as an ![](img/0a4f00c0-58d2-4cee-a24e-6fc913e2b9c7.png)greedy
    strategy.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法不需要构建仿真器的估计，例如，它不需要知道Atari仿真器的内部游戏机制，因为它仅使用来自仿真器的样本来解决强化学习问题。这个特性称为**无模型**，即它可以将底层模型视为黑盒。这个算法的另一个特性是离策略。它学习贪婪策略![](img/9a9692db-dc07-4a3d-be2a-87472934c71d.png)，同时遵循平衡探索与开发的状态空间概率分布![](img/127e4a41-2c25-4614-91bd-00e1c3d60cf4.png)。如前所述，![](img/84a647b9-5a68-495a-bf24-7993adaac0ad.png)可以作为一种![](img/0a4f00c0-58d2-4cee-a24e-6fc913e2b9c7.png)贪婪策略进行选择。
- en: 'The derivation of the deep Q-learning algorithm may be a little bit difficult
    for readers who are not familiar with reinforcement learning or the Markov decision
    process. In order to make it more understandable, let''s see the following diagram:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习算法的推导对于不熟悉强化学习或马尔科夫决策过程的读者来说可能有点困难。为了使其更容易理解，让我们来看一下下面的图示：
- en: '![](img/3b7b51a9-94e8-402b-8f3c-381677f65707.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b7b51a9-94e8-402b-8f3c-381677f65707.png)'
- en: The brain of our AI player is the Q-network controller. At each time step t,
    she observes the screen image ![](img/d3b118a9-7148-4a00-bf27-891876015028.png)
    (recall that st is an ![](img/c1550c24-4a86-4411-bca9-7e8c4ed1ef98.png) image
    that stacks the last four frames). Then, her brain analyzes this observation and
    comes up with an action, ![](img/669306ba-08b7-442e-903a-164890128a34.png). The
    Atari emulator receives this action and returns the next screen image, ![](img/9a8cf78c-56e3-457a-b1af-3c0f1920b3b2.png),
    and the reward, ![](img/6395f46a-cf4f-488b-9ae2-4b9c536374e1.png). The quadruplet ![](img/e3b66ad7-170d-4ee5-8713-db4be648c25a.png)
    is stored in the memory and is taken as a sample for training the Q-network by
    minimizing the empirical loss function via stochastic gradient descent.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们AI玩家的大脑是Q网络控制器。在每个时间步t，她观察屏幕图像![](img/d3b118a9-7148-4a00-bf27-891876015028.png)（回想一下，st是一个![](img/c1550c24-4a86-4411-bca9-7e8c4ed1ef98.png)堆叠了最后四帧的图像）。然后，她的大脑分析这个观察结果，并做出一个动作，![](img/669306ba-08b7-442e-903a-164890128a34.png)。Atari仿真器接收到这个动作，并返回下一个屏幕图像![](img/9a8cf78c-56e3-457a-b1af-3c0f1920b3b2.png)，以及奖励![](img/6395f46a-cf4f-488b-9ae2-4b9c536374e1.png)。四元组![](img/e3b66ad7-170d-4ee5-8713-db4be648c25a.png)被存储在内存中，并作为样本用于通过随机梯度下降最小化经验损失函数来训练Q网络。
- en: 'How do we draw samples from the quadruplets stored in the memory? One approach
    is that these samples, ![](img/eb191497-6ef0-432c-89c5-e568b5e7056d.png), are
    drawn from our AI player''s interactions with the environment, for example, samples ![](img/a58a78de-c408-499e-b397-c6031a560ffb.png)
    are used to train the Q-network. The main drawback of this approach is that the
    samples in one batch are strongly correlated. The strong correlation breaks the
    assumption that the samples for constructing the empirical loss function are independent,
    making the training procedure unstable and leading to bad performance:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从存储在内存中的四元组中抽取样本？一种方法是，这些样本，![](img/eb191497-6ef0-432c-89c5-e568b5e7056d.png)，是通过我们的AI玩家与环境的互动得出的。例如，样本![](img/a58a78de-c408-499e-b397-c6031a560ffb.png)用于训练Q网络。这个方法的主要缺点是，一批中的样本具有强烈的相关性。强相关性破坏了构建经验损失函数时样本独立性的假设，导致训练过程不稳定，表现不佳：
- en: '![](img/720330a7-48d0-4f8c-933f-abb8ad89a26f.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/720330a7-48d0-4f8c-933f-abb8ad89a26f.png)'
- en: 'The deep Q-learning algorithm applies another approach, utilizing a technique
    known as experience replay. The AI player''s experiences at each time step ![](img/4a7b4f5c-f375-4546-a0c1-dbed21263257.png)
    are stored into the replay memory from which a batch of samples are randomly drawn
    in order to train the Q-network. Mathematically, we cannot guarantee the independence
    between the samples we drew. But practically, this approach can stabilize the
    training procedure and generate reasonable results:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习算法应用了另一种方法，利用了一种叫做经验回放的技术。AI 玩家在每个时间步骤 ![](img/4a7b4f5c-f375-4546-a0c1-dbed21263257.png)
    的经验被存储在回放记忆中，从中随机抽取一批样本以训练 Q 网络。从数学上讲，我们无法保证抽取样本之间的独立性。但在实际操作中，这种方法能够稳定训练过程并产生合理的结果：
- en: '![](img/616efae0-11e3-47a2-81e5-2497eb2c9e5d.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/616efae0-11e3-47a2-81e5-2497eb2c9e5d.png)'
- en: 'So far, we have discussed all the components in the deep Q-learning algorithm.
    The full algorithm is shown as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了深度 Q 学习算法中的所有组件。完整的算法如下所示：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This algorithm works well for some Atari games, for example, Breakout, Seaquest,
    Pong, and Qbert, but it still cannot reach human-level control. One drawback is
    that computing the targets ![](img/5759a82c-7bf5-41ea-b065-ec74d08e7bb2.png) uses
    the current estimate of the action-value function ![](img/fcf4a15e-0ffc-40f4-955a-0b733c40555e.png),
    which makes the training step unstable, that is, an update that increases ![](img/011ebf97-d241-4fd7-a1da-ba8490f2e1a9.png)
    usually also increases ![](img/98fc7dbf-c711-494e-8187-55bd6369945c.png) for all
    and hence also increases the target ![](img/60bb25a5-2d55-444e-bfaf-8678c9ffeb6b.png),
    possibly leading to oscillations or divergence of the policy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在一些 Atari 游戏中表现良好，例如《打砖块》、《海底探险》、《乒乓》和《Qbert》，但仍然无法达到人类水平的控制。一个缺点是计算目标 ![](img/5759a82c-7bf5-41ea-b065-ec74d08e7bb2.png)
    时使用了当前的动作值函数估计 ![](img/fcf4a15e-0ffc-40f4-955a-0b733c40555e.png)，这使得训练步骤变得不稳定，即一个增加 ![](img/011ebf97-d241-4fd7-a1da-ba8490f2e1a9.png)
    的更新通常也会增加所有的 ![](img/98fc7dbf-c711-494e-8187-55bd6369945c.png)，因此也增加了目标 ![](img/60bb25a5-2d55-444e-bfaf-8678c9ffeb6b.png)，这可能导致策略的振荡或发散。
- en: 'To address this problem, Google DeepMind introduced the target network in their
    paper, *Human-level control through deep reinforcement learning*, which was published
    in Nature. The idea behind the target network is quite simple: a separate network
    is used for generating the targets ![](img/5759a82c-7bf5-41ea-b065-ec74d08e7bb2.png)
    in the Q-learning update. More precisely, for every ![](img/6fd13b9a-c296-40cb-b309-cf0f3b6c762a.png)
    Q-learning updates, the network Q is cloned to obtain a target network Q, which
    is used for generating the targets ![](img/b49f9246-7588-483e-a7c6-6b5b6318f825.png)
    in the following ![](img/1b664cc6-cc42-45fd-a8a3-4c671c2bb82b.png) updates to
    Q. Therefore, the deep Q-learning algorithm becomes as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，谷歌 DeepMind 在他们的论文《*通过深度强化学习实现人类水平的控制*》中引入了目标网络，该论文发表于《自然》杂志。目标网络背后的理念相当简单：使用一个独立的网络来生成
    Q 学习更新中的目标 ![](img/5759a82c-7bf5-41ea-b065-ec74d08e7bb2.png)。更准确地说，对于每个 ![](img/6fd13b9a-c296-40cb-b309-cf0f3b6c762a.png)
    Q 学习更新，网络 Q 被克隆以获得目标网络 Q，并用于生成接下来的 Q 更新中的目标 ![](img/b49f9246-7588-483e-a7c6-6b5b6318f825.png)。因此，深度
    Q 学习算法变为如下：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With the target network, the AI player trained by the deep Q-learning algorithm
    is able to surpass the performance of most previous reinforcement learning algorithms
    and achieves a human-level performance across a set of 49 Atari 2600 games, for
    example, Star Gunner, Atlantis, Assault, and Space Invaders.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标网络，通过深度 Q 学习算法训练的 AI 玩家能够超越大多数先前强化学习算法的表现，并在 49 款 Atari 2600 游戏中实现了人类水平的表现，例如《星际枪手》、《亚特兰蒂斯》、《攻击》和《太空侵略者》。
- en: 'The deep Q-learning algorithm has made an important step toward general artificial
    intelligence. Although it performs well in the Atari 2600 games, it still has
    a lot of unsolved issues:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习算法在通用人工智能方面迈出了重要一步。尽管它在 Atari 2600 游戏中表现良好，但仍然存在许多未解决的问题：
- en: '**Slow convergence**: It requires a long time (7 days on one GPU) to reach
    human-level performance'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收敛速度慢**：它需要很长时间（在一块 GPU 上需要 7 天）才能达到人类水平的表现。'
- en: '**Failing with sparse rewards**: It doesn''t work for the game Montezuma''s
    Revenge, which requires long-term planning'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏奖励失败**：它在《蒙特祖玛的复仇》游戏中无法发挥作用，因为该游戏需要长期规划。'
- en: '**Need for a large amount of data**: This is a common issue among most reinforcement
    learning algorithms'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要大量数据**：这是大多数强化学习算法常见的问题。'
- en: In order to solve these issues, different variants of the deep Q-learning algorithm
    have been proposed recently, for example, double Q-learning, prioritized experience
    replay, bootstrapped DQN, and dueling network architectures. We will not discuss
    these algorithms in this book. For readers who want to learn more about DQN, please
    refer to the related papers.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，最近提出了深度Q学习算法的不同变种，例如双重Q学习、优先经验回放、引导式DQN和对抗网络架构。我们在本书中不讨论这些算法。对于想要深入了解DQN的读者，请参考相关论文。
- en: Implementation of DQN
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN的实现
- en: This chapter will show you how to implement all the components, for example,
    Q-network, replay memory, trainer, and Q-learning optimizer, of the deep Q-learning
    algorithm with Python and TensorFlow.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示如何使用Python和TensorFlow实现深度Q学习算法的所有组件，例如Q网络、回放记忆、训练器和Q学习优化器。
- en: 'We will  implement the `QNetwork` class for the Q-network that we discussed
    in the previous chapter, which is defined as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现`QNetwork`类，这是我们在上一章中讨论的Q网络，其定义如下：
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The constructor requires four arguments, `input_shape`, `n_outputs`, `network_type`
    and `scope`. `input_shape` is the size of the input image. After data preprocessing,
    the input is an ![](img/11c9512b-c3c6-4a42-830f-8d17d1475355.png) image, so that
    the default parameter is ![](img/90bd34c3-690d-4b8b-b708-caecfce0be56.png). `n_outputs`
    is the number of all the possible actions, for example, `n_outputs` is four in
    Breakout. `network_type` ,indicates the type of the Q-network we want to use.
    Our implementation contains three different networks. Two of them are the convolutional
    neural networks proposed by Google DeepMind. The other one is a feed-forward neural
    network used for testing. `scope` is the name of the Q-network object, which can
    be set to `q_network` or `target_network`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数需要四个参数，`input_shape`、`n_outputs`、`network_type`和`scope`。`input_shape`是输入图像的大小。经过数据预处理后，输入是一个![](img/11c9512b-c3c6-4a42-830f-8d17d1475355.png)图像，因此默认参数是![](img/90bd34c3-690d-4b8b-b708-caecfce0be56.png)。`n_outputs`是所有可能动作的数量，例如在Breakout游戏中，`n_outputs`为四。`network_type`表示我们要使用的Q网络类型。我们的实现包含三种不同的网络，其中两种是由Google
    DeepMind提出的卷积神经网络，另一个是用于测试的前馈神经网络。`scope`是Q网络对象的名称，可以设置为`q_network`或`target_network`。
- en: In the constructor, three input tensors are created. The `x` variable represents
    the input state (a batch of ![](img/033fb901-89a8-47df-abd0-13017738e6ce.png)
    images). The `y` and `a` variables are the estimates of the action-value function
    and the selected actions corresponding to the input state, which are used for
    training the Q-network. After creating the input tensors, two functions, `build`
    and `build_loss`, are called to build the Q-network.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，创建了三个输入张量。`x`变量表示输入状态（一批![](img/033fb901-89a8-47df-abd0-13017738e6ce.png)图像）。`y`和`a`变量分别表示动作价值函数的估计值和与输入状态对应的选定动作，用于训练Q网络。创建输入张量后，调用`build`和`build_loss`两个函数来构建Q网络。
- en: 'Constructing the Q-network using TensorFlow is quite easy, as shown here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建Q网络非常简单，如下所示：
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As discussed in the previous chapter, the Q-network for the Atari environment
    contains three convolutional layers and one hidden layer, which can be constructed
    when the `network_type` is `cnn`. The `cnn_nips` type is a simplified Q-network
    for Atari games, which only contains two convolutional layers and one hidden layer
    with less filters and hidden units. The `mlp` type is a feed-forward neural network
    with two hidden layers, which is used for debugging. The `vars` variable is the
    list of all the trainable variables in the Q-network.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所讨论的，Atari环境的Q网络包含三个卷积层和一个隐藏层，当`network_type`为`cnn`时可以构建该网络。`cnn_nips`类型是为Atari游戏简化的Q网络，只包含两个卷积层和一个隐藏层，且具有较少的滤波器和隐藏单元。`mlp`类型是一个具有两个隐藏层的前馈神经网络，用于调试。`vars`变量是Q网络中所有可训练变量的列表。
- en: 'Recall that the loss function is ![](img/311d8928-3b5a-4ad1-b94c-f07867e654c2.png),
    which can be implemented as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，损失函数为![](img/311d8928-3b5a-4ad1-b94c-f07867e654c2.png)，可以按如下方式实现：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `tf.gather_nd` function is used to get the action-value ![](img/a242dccc-2a1a-46cd-abbe-ab115852f00b.png)
    given a batch of action,s ai. The variable loss represents the loss function,
    and gradient is the gradient of the loss function with respect to the trainable
    variables. `summary_op` is for TensorBoard visualization.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.gather_nd`函数用于获取给定动作批次的动作值![](img/a242dccc-2a1a-46cd-abbe-ab115852f00b.png)。变量loss表示损失函数，gradient是损失函数相对于可训练变量的梯度。`summary_op`用于TensorBoard可视化。'
- en: 'The implementation of the replay memory doesn''t involve TensorFlow:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 回放记忆的实现不涉及TensorFlow：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `ReplayMemory` class takes four input parameters, that is, `history_len`,
    `capacity`, `batch_size`, and `input_scale`. `history_len` is the number of frames
    stacked together. Typically, `history_len` is set to 4 for Atari games, forming
    an ![](img/1c32790d-8486-44c4-9e05-18ec2582983e.png) input image. `capacity` is
    the capacity of the replay memory, namely, the maximum number of frames that can
    be stored in it. `batch_size` is the size of one batch for training. `input_scale`
    is the normalization factor for input images, for example, it is set to 255 for
    RGB images. The variable frames record all the frame images and the variable others
    record the corresponding actions, rewards, and termination signals.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReplayMemory`类接受四个输入参数，即`history_len`、`capacity`、`batch_size`和`input_scale`。`history_len`是堆叠在一起的帧数。通常，`history_len`在Atari游戏中设置为4，形成一个![](img/1c32790d-8486-44c4-9e05-18ec2582983e.png)输入图像。`capacity`是回放记忆的容量，即可以存储的最大帧数。`batch_size`是训练时的一批样本大小。`input_scale`是输入图像的归一化因子，例如，对于RGB图像，它设置为255。变量frames记录所有帧图像，变量others记录对应的动作、奖励和终止信号。'
- en: '`ReplayMemory` provides a function for adding a record (frame image, action,
    reward, termination signal) into the memory:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReplayMemory`提供了一个将记录（帧图像、动作、奖励、终止信号）添加到内存中的功能：'
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It also provides a function for constructing an ![](img/33f6663f-2d51-4cbb-87b4-fe3c25f809d6.png)
    input image by concatenating the last four frame images of a history:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 它还提供了一个功能，通过连接历史中的最后四帧图像来构建一个![](img/33f6663f-2d51-4cbb-87b4-fe3c25f809d6.png)输入图像：
- en: '[PRE24]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following function randomly draws a transition (state, action, reward,
    next state, termination signal) from the replay memory:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数从回放记忆中随机抽取一个过渡（状态、动作、奖励、下一个状态、终止信号）：
- en: '[PRE25]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Note that only the termination signal corresponding to the last frame in a
    state is allowed to be True. The `_phi(index)` function stacks the four frames
    together:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有对应状态中最后一帧的终止信号可以为True。`_phi(index)`函数将四帧图像堆叠在一起：
- en: '[PRE26]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `Optimizer` class is used for training the Q-network:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`Optimizer`类用于训练Q网络：'
- en: '[PRE27]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It takes the Q-network, the target network, the replay memory, and the size
    of input images as the input arguments. In the constructor, it creates an optimizer
    (one of the popular optimizers such as ADAM, RMSPROP, or MOMENTUM) and then builds
    an operator for training.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受Q网络、目标网络、回放记忆和输入图像的大小作为输入参数。在构造函数中，它创建一个优化器（如ADAM、RMSPROP或MOMENTUM等流行优化器），然后构建一个用于训练的操作符。
- en: 'To train the Q-network, it needs to construct a mini-batch of samples (states,
    actions, targets) corresponding to ![](img/96f94be4-2663-4447-bdcb-0ad52ff91e8c.png), ![](img/5a68c450-24b6-453a-b227-b89c5279dd64.png),
    and ![](img/e23a5362-570b-4f5b-9f34-451d4bde8460.png) in the loss function ![](img/84bead4d-efad-4314-b78c-7cee54ccdfd2.png):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练Q网络，需要构建一个与![](img/96f94be4-2663-4447-bdcb-0ad52ff91e8c.png)、![](img/5a68c450-24b6-453a-b227-b89c5279dd64.png)和![](img/e23a5362-570b-4f5b-9f34-451d4bde8460.png)对应的迷你批样本（状态、动作、目标）来计算损失函数![](img/84bead4d-efad-4314-b78c-7cee54ccdfd2.png)：
- en: '[PRE28]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that the targets ![](img/e23a5362-570b-4f5b-9f34-451d4bde8460.png) are
    computed by the target network instead of the Q-network. Given a mini-batch of
    states, actions, or targets, the Q-network can be easily trained by use of the
    following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目标![](img/e23a5362-570b-4f5b-9f34-451d4bde8460.png)是由目标网络计算的，而不是Q网络。给定一批状态、动作或目标，Q网络可以通过以下方式轻松训练：
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Besides the training procedure, for each `1000` steps, the summary is written
    to the log file. This summary is for monitoring the training process, helping
    to tune the parameters, and debugging.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练过程外，每1000步会将摘要写入日志文件。这个摘要用于监控训练过程，帮助调整参数和调试。
- en: 'Combining these modules together, we can implement the class DQN for the main
    deep Q-learning algorithm:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些模块结合在一起，我们可以实现用于主要深度Q学习算法的类DQN：
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, `config` includes all the parameters of DQN, for example, batch size
    and learning rate for training. `game` is an instance of the Atari environment.
    In the constructor, the replay memory, Q-network, target network, and optimizer
    are initialized. To begin the training process, the following function can be
    called:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`config` 包含 DQN 的所有参数，例如训练的批量大小和学习率。`game` 是 Atari 环境的一个实例。在构造函数中，回放记忆、Q
    网络、目标网络和优化器被初始化。要开始训练过程，可以调用以下函数：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This function is easy to understand. In each episode, it calls `replay_memory.phi`
    to get the current state and calls the `choose_action` function to select an action
    via the  ![](img/ced55201-ffc9-4a33-a0a0-71ec35883db7.png)greedy policy. This
    action is submitted into the Atari emulator by calling the `play` function, which
    returns the corresponding reward, next frame image, and termination signal. Then,
    the transition (current frame image, action, reward, termination) is stored in
    the replay memory. For every `update_interval` step (`update_interval = 1` by
    default), the Q-network is trained with a batch of transitions randomly sampled
    from the replay memory. For every `time_between_two_copies` step, the target network
    copies the Q-network, and the weights of the Q-network are saved to the hard disk.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数很容易理解。在每一轮中，它调用 `replay_memory.phi` 来获取当前状态，并通过 `choose_action` 函数选择一个动作，该函数使用 ![](img/ced55201-ffc9-4a33-a0a0-71ec35883db7.png)贪婪策略。这个动作通过调用
    `play` 函数提交到 Atari 模拟器，后者返回相应的奖励、下一帧图像和终止信号。然后，过渡（当前帧图像、动作、奖励、终止）被存储到回放记忆中。对于每一个
    `update_interval` 步骤（默认 `update_interval = 1`），Q 网络会用从回放记忆中随机采样的一批过渡数据进行训练。对于每
    `time_between_two_copies` 步骤，目标网络会复制 Q 网络，并将 Q 网络的权重保存到硬盘。
- en: 'After the training step, the following function can be called for evaluating
    the AI player''s performance:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练步骤之后，可以调用以下函数来评估 AI 玩家表现：
- en: '[PRE32]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we are ready to train our first AI player for Atari games. The implementation
    is not hard if you understand the intuition behind the algorithm, is it? Now is
    the time to run the program and witness the magic!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好训练我们的第一个 Atari 游戏 AI 玩家了。如果你理解算法背后的直觉，实现并不难，不是吗？现在是时候运行程序，见证魔力了！
- en: Experiments
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: 'The full implementation of the deep Q-learning algorithm can be downloaded
    from GitHub (link xxx). To train our AI player for Breakout, run the following
    command under the `src` folder:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习算法的完整实现可以从 GitHub 下载（链接 xxx）。要训练我们的 Breakout AI 玩家，请在 `src` 文件夹下运行以下命令：
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: There are two arguments in `train.py`. One is `-g` or `--game`, indicating the
    name of the game one wants to test. The other one is `-d` or `--device`, which
    specifies the device (CPU or GPU) one wants to use to train the Q-network.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.py` 中有两个参数。一个是 `-g` 或 `--game`，表示要测试的游戏名称。另一个是 `-d` 或 `--device`，指定要使用的设备（CPU
    或 GPU）来训练 Q 网络。'
- en: 'For Atari games, even with a high-end GPU, it will take 4-7 days to make our
    AI player achieve human-level performance. In order to test the algorithm quickly,
    a special game called **demo** is implemented as a lightweight benchmark. Run
    the demo via the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Atari 游戏，即使使用高端 GPU，也需要 4-7 天才能让我们的 AI 玩家达到人类水平的表现。为了快速测试算法，实现了一个名为 **demo**
    的特殊游戏作为轻量级基准。可以通过以下方式运行 demo：
- en: '[PRE34]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The demo game is based on the GridWorld game on the website at [https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html](https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 演示游戏基于网站上的 GridWorld 游戏，网址为 [https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html](https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html)：
- en: '![](img/3bd4e47a-d8d9-4fe3-98f9-b5d936dba390.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bd4e47a-d8d9-4fe3-98f9-b5d936dba390.png)'
- en: 'In this game, a robot in a 2D grid world has nine eyes pointing in different
    angles, and each eye senses three values along its direction: distance to a wall,
    distance to a green bean, or distance to a red bean. It navigates by using one
    of five actions that turn it different angles. It gets a positive reward (+1)
    for eating green beans while a negative reward (-1) for eating red beans. The
    goal is to eat green beans as much as possible in one episode.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个游戏中，2D 网格世界中的一个机器人有九只眼睛，分别指向不同的角度，每只眼睛沿着其方向感知三个值：距离墙壁的距离、距离绿豆的距离或距离红豆的距离。它通过使用五个不同的动作之一来导航，每个动作使它转向不同的角度。它吃到绿豆时会获得正奖励（+1），而吃到红豆时会获得负奖励（-1）。目标是在一次游戏中尽可能多地吃绿豆。
- en: 'The training will take several minutes. During the training, you can open a
    new terminal and type the following command to visualize the architecture of the
    Q-network and the training procedure:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 训练将持续几分钟。在训练过程中，你可以打开一个新的终端，输入以下命令来可视化 Q 网络的架构和训练过程：
- en: '[PRE35]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here, `logdir` points to the folder where the log file of demo is stored. Once
    TensorBoard is running, navigate your web browser to `localhost:6006` to view
    the TensorBoard:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`logdir` 指向存储示例日志文件的文件夹。一旦 TensorBoard 启动，你可以在浏览器中输入 `localhost:6006` 来查看
    TensorBoard：
- en: '![](img/aedc4c3e-6dd9-40e4-b072-f1a3b1a409fa.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aedc4c3e-6dd9-40e4-b072-f1a3b1a409fa.png)'
- en: The two graphs plot the loss and the score against the training step, respectively.
    Clearly, after 100k training steps, the performance of the robot becomes stable,
    for example, the score is around 40.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这两张图分别绘制了损失和得分与训练步数的关系。显然，经过 10 万步训练后，机器人表现变得稳定，例如得分约为 40。
- en: You can also visualize the weights of the Q-network through TensorBoard. For
    more details, visit the TensorBoard guide at [https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard).
    This tool is quite useful for debugging and optimizing the code, especially for
    complicated algorithms such as DQN.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过 TensorBoard 可视化 Q 网络的权重。更多详情，请访问 TensorBoard 指南 [https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard)。这个工具对于调试和优化代码非常有用，特别是像
    DQN 这样复杂的算法。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Congratulations! You just learned four important things. The first one is how
    to implement an Atari game emulator using gym, and how to play Atari games for
    relaxation and having fun. The second one is that you learned how to preprocess
    data in reinforcement learning tasks such as Atari games. For practical machine
    learning applications, you will spend a great deal of time on understanding and
    refining data, which affects the performance of an AI system a lot. The third
    one is the deep Q-learning algorithm. You learned the intuition behind it, for
    example, why the replay memory is necessary, why the target network is needed,
    where the update rule comes from, and so on. The final one is that you learned
    how to implement DQN using TensorFlow, and how to visualize the training process.
    Now, you are ready for the more advanced topics that we will discuss in the following
    chapters.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你刚刚学习了四个重要的内容。第一个是如何使用 gym 实现一个 Atari 游戏模拟器，并如何玩 Atari 游戏来放松和娱乐。第二个是你学会了如何在强化学习任务中处理数据，比如
    Atari 游戏。对于实际的机器学习应用，你将花费大量时间去理解和优化数据，而数据对 AI 系统的表现有很大的影响。第三个是深度 Q 学习算法。你了解了它的直觉，比如为什么需要重放记忆，为什么需要目标网络，更新规则的来源等等。最后一个是你学会了如何使用
    TensorFlow 实现 DQN，并且如何可视化训练过程。现在，你已经准备好深入探讨我们在接下来章节中要讨论的更高级的主题。
- en: In the next chapter, you will learn how to simulate classic control tasks, and
    how to implement the state-of-the-art actor-critic algorithms for control.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习如何模拟经典的控制任务，以及如何实现最先进的演员-评论家算法来进行控制。
