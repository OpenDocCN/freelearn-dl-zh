- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Exploring Advanced Use Cases of Jina
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Jina 的高级使用案例
- en: 'In this chapter, we discuss more advanced applications of the Jina neural search
    framework. Building on the concepts we have learned in the previous chapters,
    we will now look at what else we can achieve with Jina. We will examine multi-level
    granularity matches, querying while indexing, and a cross-modal example. These
    are challenging concepts in neural search and are required to achieve complex
    real-life applications. In particular, we will be covering these topics in this
    chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论 Jina 神经搜索框架的更高级应用。在前几章中学到的概念基础上，我们将探讨通过 Jina 可以实现的其他功能。我们将考察多级粒度匹配、边索引边查询以及跨模态示例。这些是神经搜索中的挑战性概念，掌握它们对于实现复杂的现实应用至关重要。本章将涵盖以下内容：
- en: Introducing multi-level granularity
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入多级粒度
- en: Cross-modal search with images with text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图像和文本进行跨模态搜索
- en: Concurrent querying and indexing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并发查询和索引数据
- en: These cover a wide variety of real-life requirements of neural search applications.
    Using these examples, together with the basic examples in [*Chapter 6*](B17488_06.xhtml#_idTextAnchor085),
    *Basic Practical Examples with Jina*, you can expand and improve your Jina applications
    to cover even more advanced usage patterns.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些涵盖了神经搜索应用的广泛现实需求。通过这些示例，以及[*第6章*](B17488_06.xhtml#_idTextAnchor085)中的基础示例，*Jina
    的基本实用示例*，你可以扩展和改进你的 Jina 应用，以覆盖更高级的使用模式。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will build and execute the advanced examples provided in
    the GitHub repository. The code is available at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07).
    Make sure to download this and navigate to each of the examples’ respective folders
    when following the instructions for how to reproduce the use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将构建并执行 GitHub 仓库中提供的高级示例。代码可以在 [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07)
    获取。请确保下载该代码并在执行重现使用案例的说明时，导航到每个示例的相应文件夹。
- en: 'To run this code, you will need the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，你需要以下环境：
- en: macOS, Linux, or Windows with WSL2 installed. Jina does not run on native Windows.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了 WSL2 的 macOS、Linux 或 Windows。Jina 无法在原生 Windows 上运行。
- en: Python 3.7 or 3.8
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7 或 3.8
- en: Optionally, a clean new virtual environment for each of the examples
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选：为每个示例提供一个全新的虚拟环境
- en: Docker
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: Introducing multi-level granularity
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入多级粒度
- en: In this section, we will discuss how Jina can capture and leverage the hierarchical
    structure of real-life data. In order to follow along with the existing code,
    check the chapter’s code for a folder named `multires-lyrics-search`. This is
    the example we will be referring to in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论 Jina 如何捕捉并利用现实生活中数据的层次结构。为了跟随现有代码，请查找该章节代码中的一个名为 `multires-lyrics-search`
    的文件夹。这就是我们将在本节中提到的示例。
- en: This example relies on the `Document` type’s capacity to hold chunks (child
    documents) and refer to a specific parent. Using this structure, you can compose
    advanced arbitrary level hierarchies of documents within documents. This mimics
    various real-life data-related problems. Examples could be patches of images,
    sentences of a paragraph, video clips of a longer movie, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例依赖于`Document`类型的能力来容纳块（子文档）并引用特定的父文档。通过这种结构，你可以在文档中组合出任意层次的复杂文档层次结构。这模拟了许多现实生活中与数据相关的问题。例如，图像的补丁、段落的句子、长电影的片段等。
- en: 'See the following code for how to perform this with Jina’s `Document` API:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下代码，了解如何通过 Jina 的 `Document` API 实现：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This can then be chained, with multiple levels of granularity, with each chunk
    having its own chunks. This becomes helpful when dealing with hierarchical data
    structures. For more information on the `Document` data type, you can check the
    *Understanding Jina components* section in [*Chapter 4*](B17488_04.xhtml#_idTextAnchor054),
    *Learning Jina’s Basics*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以通过多个粒度层次进行链式操作，每个块都有自己的子块。当处理层次数据结构时，这非常有用。关于 `Document` 数据类型的更多信息，你可以参考[*第4章*](B17488_04.xhtml#_idTextAnchor054)中的
    *理解 Jina 组件* 部分，*学习 Jina 的基础知识*。
- en: In this example, the dataset will be composed of lyrics from various popular
    songs. In this case, the granularity is based on linguistic concepts. The top
    level will be the entire contents of the body of a song’s lyrics. The level under
    that will be individual sentences extracted from the top-level body. This splitting
    is done using the `Sentencizer` Executor, which splits the long piece of text
    by looking for specific separator text tokens, such as `.` or `,`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，数据集将由来自不同流行歌曲的歌词组成。在这种情况下，粒度基于语言学概念。最高层次是整首歌曲歌词的内容。下一级则是从顶层正文中提取的单独句子。这个拆分是通过`Sentencizer`执行器来完成的，它通过寻找特定的分隔符文本标记（如`.`或`,`）来拆分长文本。
- en: This application helps showcase the concept of **chunking** and its importance
    in search systems. This is important because, in order to get the best results
    in a neural search system, it is best to search with text inputs of the same length.
    Otherwise, the context-to-content ratio will be different between the data you
    are searching with and the data you have trained your model on. Once we have built
    the example, we can visualize how the system is matching input to output via a
    custom frontend.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序有助于展示**分块**概念及其在搜索系统中的重要性。这一点非常关键，因为为了在神经搜索系统中获得最佳结果，最好使用相同长度的文本输入进行搜索。否则，搜索数据与训练模型数据之间的上下文与内容比率将会不同。一旦我们构建了示例，就可以通过自定义前端可视化系统是如何将输入与输出匹配的。
- en: Navigating through the code
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浏览代码
- en: Let’s now go through the logic of the app and the functions of each component.
    You can follow along with the code in the repository, at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search).
    I will explain the purpose and design of the main files in the folder.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们一起看看应用程序的逻辑和每个组件的功能。你可以在[https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search)中跟随仓库中的代码。我将解释文件夹中主要文件的目的和设计。
- en: app.py
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: app.py
- en: 'This is the main entry point of the example. The user can use this script to
    either index (add) new data or search with their desired queries. For indexing
    data, this is done from the command line as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是示例的主要入口点。用户可以使用该脚本来索引（添加）新数据或使用他们期望的查询进行搜索。对于数据索引，可以通过以下命令行操作来完成：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Instead of providing the `index` argument, you can also provide `query` or `query_text`
    as arguments. `query` starts the Flow to be used by an external REST API. You
    can then use the custom frontend provided in the repository to connect to this.
    `query_text` allows the user to search directly from the command line.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供`index`参数外，你还可以提供`query`或`query_text`作为参数。`query`启动外部REST API使用的Flow。然后，你可以使用仓库中提供的自定义前端来连接它。`query_text`允许用户直接从命令行进行搜索。
- en: When indexing, the data is sequentially read from a `CSV` file. We also attach
    relevant tag information, such as author, song name, album name, and language,
    for displaying metadata in the interface. Tags can also be used by the user in
    whatever way they need. They were discussed in the *Accessing nested attributes
    from tags* subsection in the *Understanding Jina components* section in [*Chapter
    4*](B17488_04.xhtml#_idTextAnchor054), *Learning Jina’s Basics*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引时，数据会从`CSV`文件中按顺序读取。我们还会附加相关的标签信息，如作者、歌曲名、专辑名和语言，以便在界面中显示元数据。用户还可以根据需要使用标签。在[*理解Jina组件*](B17488_04.xhtml#_idTextAnchor054)章节中的*访问标签中的嵌套属性*小节中进行了讨论。
- en: index.yml
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: index.yml
- en: 'This file defines the structure of the Flow used when indexing data (adding
    data). Following are the different configuration options provided in the file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件定义了在索引数据（添加数据）时使用的Flow结构。以下是文件中提供的不同配置选项：
- en: '`jtype` informs the YAML parser about the class type of this object. In this
    case, it’s the `Flow` class. The YAML parser will then instantiate the class with
    the respective configuration parameters.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jtype`通知YAML解析器该对象的类类型。在本例中，它是`Flow`类。然后，YAML解析器将使用相应的配置参数实例化该类。'
- en: '`workspace` defines the default location where each Executor might want to
    store its data. Not all Executors require a workspace. This can be overridden
    by each Executor’s `workspace` parameter.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workspace` 定义了每个 Executor 可能想要存储数据的默认位置。并非所有 Executor 都需要一个工作区。这个设置可以被每个 Executor
    的 `workspace` 参数覆盖。'
- en: '`executors` is a list that defines the processing steps in this Flow. These
    steps are defined by specific classes, all of which are subclasses of the `Executor`
    class.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`executors` 是一个定义此流程中处理步骤的列表。这些步骤由特定的类定义，所有这些类都是 `Executor` 类的子类。'
- en: 'The indexing Flow is represented by the following diagram:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 索引流程由下图表示：
- en: '![ Figure 7.1 – Index Flow showing document chunking ](img/Figure_7.1_B17488.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![ 图 7.1 – 索引流程显示文档分块 ](img/Figure_7.1_B17488.jpg)'
- en: Figure 7.1 – Index Flow showing document chunking
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 索引流程显示文档分块
- en: Notice how the data Flow is split at the gateway. The original document is stored
    as is in `root_indexer`, for future retrieval. On the other path, the document
    gets processed in order to extract its chunks, encode them, and then store them
    in the indexer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意数据流程如何在网关处被分割。原始文档存储在 `root_indexer` 中，供将来检索。在另一条路径上，文档被处理以提取其块，编码它们，并将它们存储到索引器中。
- en: 'Following are the different Executors used in this example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例中使用的不同 Executors：
- en: The first one is `segmenter`, which uses the `Sentencizer` class, from Jina
    Hub. We use the default configuration. This splits the body of the lyrics into
    sentences using a set of punctuation markers that usually delimit sentences, such
    as `.`, `,`, `;`, `!`. This is where the chunks are being created and assigned
    to their parent document, based on where these tokens are found in the text.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个是 `segmenter`，它使用来自 Jina Hub 的 `Sentencizer` 类。我们使用默认配置。此配置使用一组通常用来分隔句子的标点符号（如
    `.`、`,`、`;`、`!`）将歌词正文分割成句子。这里是创建并分配文档父级的块的地方，基于这些标记在文本中的位置。
- en: 'The next is `encoder`. This is the component in the Flow that transforms the
    sentence from text into a numeric format. The component uses the `TransformerTorchEncoder`
    class. It downloads the `distilbert-base-cased` model from the `Huggingface` API
    and uses it to encode the text itself into vectors, which can then be used for
    vector similarity computation. We will also define some configuration options
    here:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个是 `encoder`。这是流程中的组件，将句子从文本转换为数字格式。该组件使用 `TransformerTorchEncoder` 类。它从 `Huggingface`
    API 下载 `distilbert-base-cased` 模型，并使用该模型将文本编码成向量，然后可以用来进行向量相似度计算。我们还将在此定义一些配置选项：
- en: '`pooling_strategy: ''cls''`: This is the pooling strategy that is used by the
    encoder.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooling_strategy: ''cls''`：这是编码器使用的池化策略。'
- en: '`pretrained_model_name_or_path: distilbert-base-cased`: This is the deep learning
    model that is used. It is pre-trained and downloaded at the start time by the
    Executor.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path: distilbert-base-cased`：这是使用的深度学习模型。它是预训练的，并在
    Executor 启动时下载。'
- en: '`max_length: 96`: This indicates the maximum number of characters to encode
    from the sentence. Sentences longer than this limit get trimmed (the extra characters
    are removed).'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length: 96`：此项表示从句子中编码的最大字符数。超过此限制的句子会被裁剪（多余的字符会被删除）。'
- en: '`device: ''cpu''`: This configuration instructs the Executor to run on the
    CPU. The Executor can also be run on the GPU (with `''gpu''`).'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device: ''cpu''`：此配置指示 Executor 在 CPU 上运行。Executor 也可以在 GPU 上运行（使用 `''gpu''`）。'
- en: '`default_traversal_paths: [''c'']`: This computes the embeddings on the chunk
    level. This represents the hierarchy level of the sentences extracted by `segmenter`.
    We only encode these, as we will perform the search matching at this level only.
    Matching the entire body of a song’s lyrics will not perform well, due to the
    amount of data a model needs to encode.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_traversal_paths: [''c'']`：此项计算在块级别的嵌入。这表示 `segmenter` 提取的句子的层次级别。我们仅编码这些，因为我们只会在此级别进行搜索匹配。匹配整个歌词的正文效果不佳，因为模型需要编码的数据量太大。'
- en: 'We will now deep-dive into the actual storage engine, `indexer`. For this,
    we use the Executor called `SimpleIndexer`, again from Jina Hub. This uses the
    `DocumentArrayMemmap` class from Jina, to store the data on disk, but at the same
    time, load it into memory for reading and writing as needed, without consuming
    too much memory. We define the following configuration options for it:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将深入了解实际的存储引擎，`indexer`。为此，我们使用名为 `SimpleIndexer` 的 Executor，同样来自 Jina Hub。它使用
    Jina 的 `DocumentArrayMemmap` 类，将数据存储到磁盘，但同时将其加载到内存中，按需读取和写入，而不会消耗过多内存。我们为它定义了以下配置选项：
- en: '`default_traversal_paths: [''c'']`: These options configure the component to
    store the chunks of the documents. This has the same purpose as the previous usage
    of `default_traversal_paths`.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_traversal_paths: [''c'']`：这些选项配置组件以存储文档的块。这与前面使用`default_traversal_paths`的目的相同。'
- en: 'Next is another indexer, `root_indexer`. This is part of the specific requirements
    of this example. Before, at `indexer`, we stored only the chunks of the document.
    But, at search time, we need to also retrieve the parent document itself, in order
    to obtain the tags associated with it (artist name, song name, and much more).
    As such, we need to store these documents somewhere. That is why we need this
    additional Executor. Usually, this will not be required, depending on your use
    case in your application. We define the following configuration options:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是另一个索引器，`root_indexer`。这是本示例的特定要求。之前，在`indexer`中，我们只存储了文档的块。但是，在搜索时，我们还需要检索父文档本身，以便获取与其相关的标签（艺术家名称、歌曲名称等）。因此，我们需要将这些文档存储在某个地方。这就是我们需要这个额外执行器的原因。通常，根据应用程序中的用例，这不会是必需的。我们定义了以下配置选项：
- en: '`default_traversal_paths: [''r'']`: We define that we will index the root level
    of the document (i.e., not chunk-level)'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_traversal_paths: [''r'']`：我们定义了将索引文档的根级别（即，不是块级别）。'
- en: '`needs: [gateway]`: This tells the Flow to send requests in parallel, to two
    separate paths: one is sent to the `segmenter` and `encoder` path, and the other
    is sent directly to `root_indexer`, since this one does not depend on any Executor
    in the other path'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needs: [gateway]`：这告诉Flow并行发送请求到两个不同的路径：一个发送到`segmenter`和`encoder`路径，另一个直接发送到`root_indexer`，因为这个路径不依赖于另一个路径中的任何执行器。'
- en: You will have noticed an additional argument that is repeated across some of
    the Executors, `volumes`. This conforms to the Docker syntax for mounting a local
    directory in the Docker container, in order to mount the workspace in the running
    Docker container.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到在一些执行器中重复出现了一个额外的参数`volumes`。这符合Docker的语法，用于在Docker容器中挂载本地目录，以便在正在运行的Docker容器中挂载工作空间。
- en: query.yml
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: query.yml
- en: 'This file defines the structure of the Flow used when querying data (searching
    data). This is different from the Flow configuration used at index time because
    the order of operations is different. Looking at the following diagram, we notice
    the main change is that the operations at query time are strictly sequential:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本文件定义了查询数据时使用的Flow结构（搜索数据时）。这与索引时使用的Flow配置不同，因为操作的顺序不同。通过查看下图，我们可以看到主要的变化是查询时的操作是严格顺序的：
- en: '![Figure 7.2 – Query Flow showing document chunking ](img/Figure_7.2_B17488.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 查询流程展示文档分块](img/Figure_7.2_B17488.jpg)'
- en: Figure 7.2 – Query Flow showing document chunking
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 查询流程展示文档分块
- en: The matches are retrieved from `indexer`, which operates at the chunk level,
    as we previously defined. `ranker` then creates one single match for each parent
    ID present in the chunks. Finally, the original metadata of this parent match
    document is retrieved from `root_indexer` based on its ID. This is required in
    order to get the full context of the chunk (the parent’s full text contents and
    the name of the artist and song).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配项是从`indexer`中检索的，它在之前定义的块级别操作。然后，`ranker`为每个存在于块中的父ID创建一个单一的匹配项。最后，基于其ID，从`root_indexer`检索此父匹配文档的原始元数据。这是必要的，以便获取块的完整上下文（父文档的完整文本内容以及艺术家和歌曲的名称）。
- en: 'Just like the `index.yml` file, the `query.yml` file also defines a Flow with
    Executors. We will discuss their configuration choices, but we will only cover
    the differences from their equivalent in the `index.yml` file. If a parameter
    is not covered in this section, check the previous section. The following are
    the Executors defined in the query Flow:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与`index.yml`文件类似，`query.yml`文件也定义了一个包含执行器的Flow。我们将讨论它们的配置选择，但我们只会涉及与`index.yml`文件中的对应项不同的部分。如果某个参数在本节中未涉及，请查看前一节。以下是查询Flow中定义的执行器：
- en: '`segmenter` is the same.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmenter`相同。'
- en: '`encoder` is also the same.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder`也相同。'
- en: '`indexer` is also the same.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indexer`也相同。'
- en: 'The first new Executor is `ranker`. This performs a custom ranking and sorting
    of the results from the search. We use `SimpleRanker`, from Jina Hub. The only
    parameter here is `metric: ''cosine''`. This configures the class to use the `cosine`
    metric to base its ranking on. It works by aggregating the scores of a parent
    document’s chunks (children documents) into an overall score for the parent document.
    This is required to ensure that the matches are sorted in a meaningful way for
    the client (the frontend, REST API client, or command-line interface).'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '第一个新的Executor是`ranker`。它执行自定义的排名和搜索结果排序。我们使用的是来自Jina Hub的`SimpleRanker`。这里唯一的参数是`metric:
    ''cosine''`。这将配置该类使用`cosine`度量来进行排名。其工作原理是将父文档的各个块（子文档）的得分聚合为父文档的整体得分。这是确保匹配结果按对客户端（前端、REST
    API客户端或命令行界面）有意义的方式进行排序所必需的。'
- en: The last hop is `root_indexer`. Here, we change `default_traversal_paths` to
    `['m']`. This means that we want to retrieve the metadata of the matches of the
    document, not of the request document itself. This takes the document’s ID and
    performs a lookup for the metadata. As mentioned previously, `indexer` only stores
    the chunks of the document. We need to retrieve the full metadata of the chunks’
    parent Document.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的步骤是`root_indexer`。在这里，我们将`default_traversal_paths`更改为`['m']`。这意味着我们希望检索文档匹配项的元数据，而不是请求文档本身的元数据。它会获取文档的ID并查找其元数据。如前所述，`indexer`只存储文档的块。我们需要检索块的父文档的完整元数据。
- en: Installing and running the example
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装并运行示例
- en: 'I will now guide you through installing and running this example application:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在将指导你安装并运行这个示例应用程序：
- en: Make sure the requirements defined at the beginning of this chapter are fulfilled.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保满足本章开始时定义的要求。
- en: Clone the Git repository from [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)
    and open a terminal in the example’s folder, at `src/Chapter07/multires-lyrics-search`.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)克隆Git仓库，并在示例的文件夹中打开终端，路径为`src/Chapter07/multires-lyrics-search`。
- en: 'Install the requirements:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的依赖项：
- en: '[PRE2]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Download the full dataset. This step is optional; you can skip this step and
    use the sample data provided:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载完整的数据集。此步骤是可选的；你可以跳过此步骤并使用提供的示例数据：
- en: 'Begin by installing the Kaggle library if you haven’t already done so. You
    will also need to set up your API keys as explained here: [https://github.com/Kaggle/kaggle-api#api-credentials:](https://github.com/Kaggle/kaggle-api#api-credentials%0D)'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有安装Kaggle库，请首先安装。你还需要设置API密钥，具体操作请参考这里：[https://github.com/Kaggle/kaggle-api#api-credentials:](https://github.com/Kaggle/kaggle-api#api-credentials%0D)
- en: '[PRE3]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to index the data. This step processes your data and stores
    it in the workspace of the Flow’s Executors:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步是对数据进行索引。此步骤会处理你的数据并将其存储在Flow的Executors的工作区中：
- en: '[PRE5]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Search your data. Here you have two options:'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索你的数据。这里你有两个选项：
- en: '`python app.py -t query_text`: This option starts a command-line application.
    At some point, it will ask for a phrase as input. The phrase will be processed
    and then used as a search query. The results will be displayed in the terminal.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python app.py -t query_text`：此选项启动命令行应用程序。到某个时刻，它会要求你输入一个短语。该短语会被处理并作为搜索查询使用。结果将在终端显示。'
- en: '`python app.py -t query`: This starts the application in server mode. It listens
    for incoming requests on the REST API and responds to the client with the best
    matches.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python app.py -t query`：这将以服务器模式启动应用程序。它监听REST API上的传入请求，并向客户端响应最佳匹配项。'
- en: 'In the second mode, you can use the custom frontend we have built to explore
    the results. You can start the frontend by running the following commands in a
    terminal:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种模式下，你可以使用我们构建的自定义前端来探索结果。你可以通过在终端中运行以下命令来启动前端：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now you can open [http://127.0.0.1:8000/](http://127.0.0.1:8000/) in your browser
    and you should see a web interface. In this interface, you can type your text
    in the left-side box. You will then get results on the right side. The matching
    chunks will be highlighted in the body of the lyrics.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在浏览器中打开[http://127.0.0.1:8000/](http://127.0.0.1:8000/)，应该可以看到一个网页界面。在此界面中，你可以在左侧框中输入文本。然后，你将在右侧看到结果。匹配的文本块将在歌词正文中高亮显示。
- en: 'Following is a screenshot of the interface:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是界面的截图：
- en: '![Figure 7.3 – Lyrics search engine example showing matching songs ](img/Figure_7.3_B17488.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 歌词搜索引擎示例，展示匹配的歌曲](img/Figure_7.3_B17488.jpg)'
- en: Figure 7.3 – Lyrics search engine example showing matching songs
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 歌词搜索引擎示例，展示匹配的歌曲
- en: For example, if you add the sentence `I am very happy today`, you should see
    a similar result. Each of these boxes you see on the right-hand side is a song
    in your dataset. Each highlighted sentence is a *match*. A match is a similar
    sentence, determined by how close two vectors are in embedding space.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你添加了句子 `I am very happy today`，你应该会看到类似的结果。你在右侧看到的每个框都是你数据集中的一首歌曲。每个高亮显示的句子是一个
    *匹配项*。匹配项是相似的句子，依据的是两个向量在嵌入空间中的接近程度。
- en: Similarity can be adjusted using the breakdown slider on the left-hand side.
    As you move the slider to the right, you will see more matches appear. This is
    because we are increasing our radius in the vector space to find similar matches.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度可以通过左侧的滑块进行调整。当你将滑块向右移动时，你会看到更多的匹配项。这是因为我们在向量空间中增大了半径，以便找到更多相似的匹配项。
- en: The relevance score you see at the bottom of the song box summarizes all the
    matches in a song. Each match has a numeric value between 0 and 1, determining
    how close it is to the original input in the vector space. The average of these
    match values is the relevance score. This means that a song with only good matches
    will be ranked as highly relevant.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你在歌曲框底部看到的相关性评分总结了歌曲中的所有匹配项。每个匹配项都有一个介于 0 到 1 之间的数值，表示它与原始输入在向量空间中的相似度。这些匹配值的平均值即为相关性评分。这意味着只有良好的匹配项的歌曲将被排名为高度相关。
- en: The example also allows for more complex, multi-sentence queries. If you input
    two or three sentences when querying, the query Flow will break down the total
    input into individual “chunks.” These chunks in this example are sentences, but
    you can determine what a chunk is for your own data when building Jina.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例还支持更复杂的多句查询。如果你在查询时输入两句或三句，查询流程将把整个输入分解为单独的“块”。在这个示例中，块是句子，但在构建 Jina 时，你可以根据自己的数据确定块的定义。
- en: In this section, we have covered how you can model the hierarchical structure
    of real-life data in the Jina framework. We use the `Document` class and its ability
    to hold chunks as our representation of this data. We have then built an example
    application that we can use to search through song lyrics, on the sentence level.
    This approach can be generalized to any text (or other modality) data application.
    In the next section, we will see how we can leverage a document’s modality in
    order to search for images with text.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何在 Jina 框架中建模现实世界数据的层次结构。我们使用 `Document` 类及其持有块的能力来表示这些数据。然后，我们构建了一个示例应用程序，可以用于在句子级别上搜索歌曲歌词。这个方法可以推广到任何文本（或其他模态）数据应用程序。在接下来的章节中，我们将看到如何利用文档的模态来搜索带有文本的图像。
- en: Cross-modal search with images with text
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有文本的跨模态搜索与图像
- en: 'In this section, we will cover an advanced example showcasing **cross-modal
    search**. Cross-modal search is a subtype of neural search, where the data we
    index and the data we search with belong to different modalities. This is something
    that is unique to neural search, as none of the traditional search technologies
    could easily achieve this. This is possible due to the central neural search technology:
    all deep learning models fundamentally transform all data types to the same shared
    numeric representation of a vector (the embedding extracted from a specific layer
    of the network).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示一个高级示例，展示 **跨模态搜索**。跨模态搜索是神经搜索的一种子类型，其中我们索引的数据和我们用来搜索的数据属于不同的模态。这是神经搜索所独有的，因为传统的搜索技术无法轻易实现这一点。这之所以可行，是由于神经搜索技术的核心：所有深度学习模型将所有数据类型转化为相同的共享数字表示——向量（从网络的特定层提取的嵌入）。
- en: 'These modalities can be represented by different data types: audio, text, video,
    and images. At the same time, they can also be of the same type, but of different
    distributions. An example of this could be searching with a paper summary and
    wanting to get the paper title. They are both texts, but the underlying data distribution
    is different. The distribution is thus a modality as well in this case.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模态可以通过不同的数据类型来表示：音频、文本、视频和图像。同时，它们也可以是相同类型的，但分布不同。举个例子，比如用论文摘要来搜索，目的是获得论文标题。它们都是文本，但底层的数据分布不同。因此，在这种情况下，分布也可以视为一种模态。
- en: The purpose of the example in this section is to show how the Jina framework
    helps us to easily perform this sort of search. We highlight how the Flow can
    be used to split the data processing, depending on modalities, into two pipelines
    of Executors. This is done with the `needs` field, which defines the previously
    required step of an Executor. Chaining these `needs`, we can obtain separate paths.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例目的是展示 Jina 框架如何帮助我们轻松执行此类搜索。我们重点介绍了如何使用 Flow 根据数据的模态将数据处理拆分为两个执行器流水线。这是通过
    `needs` 字段完成的，它定义了执行器之前所需的步骤。通过将这些 `needs` 链接在一起，我们可以获得独立的路径。
- en: Let’s now go through the logic of the app and what each file’s purpose is. The
    code can be found at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)
    in the folder `src/Chapter07/cross-modal-search`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来讲解应用程序的逻辑以及每个文件的作用。代码可以在 [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)
    的 `src/Chapter07/cross-modal-search` 文件夹中找到。
- en: app.py
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: app.py
- en: This is the main entry point of the example. The user can call it to either
    **index** or **search**. It then creates the Flows and either indexes data or
    searches with the query from the user.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是示例的主要入口点。用户可以调用它来进行 **索引** 或 **搜索**。然后，它会创建流程并根据用户的查询进行数据索引或搜索。
- en: flow-index.yml
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: flow-index.yml
- en: This file defines the structure of the Flow used when indexing data (adding
    data). I will explain the different steps.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件定义了在索引数据（添加数据）时使用的流程结构。我将解释不同的步骤。
- en: 'The Flow itself has the following arguments:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 流程本身有以下参数：
- en: '`prefetch` defines the number of documents to prefetch from the client’s request.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefetch` 定义了从客户端请求中预取的文档数量。'
- en: '`workspace` defines the default location where data will be stored. This can
    be overridden by each Executor’s `workspace` parameter.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workspace` 定义了数据存储的默认位置。每个执行器的 `workspace` 参数可以覆盖此设置。'
- en: Then, the `executors` list defines the Executors used in this Flow. Each item
    in this list is an Executor and its configuration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`executors` 列表定义了此流程中使用的执行器。该列表中的每个项目都是一个执行器及其配置。
- en: 'Following is a diagram representing the indexing Flow. Notice how the path
    bifurcates from the gateway, depending on whether the data is image or text:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是表示索引流程的图示。注意，路径如何从网关分岔，这取决于数据是图像还是文本：
- en: '![Figure 7.4 – Index Flow showing cross-modal features ](img/Figure_7.4_B17488.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 显示跨模态特征的索引流程](img/Figure_7.4_B17488.jpg)'
- en: Figure 7.4 – Index Flow showing cross-modal features
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 显示跨模态特征的索引流程
- en: 'We will describe the purpose of each of the Executors, grouped by paths. The
    first path is the path for image data:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按路径对每个执行器的目的进行描述。第一个路径是用于图像数据的路径：
- en: The first Executor is `image_loader`. This uses the `ImageReader` class, defined
    locally in the `flows/executors.py` file. This will load the image files from
    a specific folder and pass them down further into the Flow for processing. When
    a document is created, we can assign it a `mime` type. This can then be used in
    specific Executors to perform custom logic. Here, we are using it to restrict
    which documents go to which Executors.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个执行器是 `image_loader`。它使用 `ImageReader` 类，该类在 `flows/executors.py` 文件中本地定义。它将从特定文件夹加载图像文件，并将其传递到流程中进一步处理。当创建文档时，我们可以为其分配一个
    `mime` 类型。然后可以在特定执行器中使用它来执行自定义逻辑。在这里，我们使用它来限制哪些文档进入哪个执行器。
- en: 'The parameters are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '`py_modules`: This tells the Python process where to find extra classes that
    can then be used in the `uses` parameter.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`py_modules`：这告诉 Python 进程在哪里找到额外的类，然后可以在 `uses` 参数中使用它们。'
- en: '`needs`: This creates a direct connection from the gateway (which is always
    the first and last hop of the Flow) to this Executor. It makes this component
    wait for requests from the gateway. This is required here because we want two
    separate paths for text and images.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needs`：这会从网关（始终是流程的第一个和最后一个跳跃）创建到此执行器的直接连接。它使得该组件等待来自网关的请求。这里需要这样做，因为我们希望文本和图像有两个独立的路径。'
- en: 'The next one is `image_encoder`. This is where the brunt of the work is done.
    Encoders are the Executors that transform data into a numeric representation.
    It uses `CLIPImageEncoder`, version 0.1\. The parameters are as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个是 `image_encoder`。这是执行大部分工作的地方。编码器是将数据转换为数值表示的执行器。它使用 `CLIPImageEncoder`，版本
    0.1。参数如下：
- en: '`needs`: This defines the path of the data on the image path'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needs`：这定义了图像路径上数据的流向。'
- en: '`image_indexer` is the storage for the embeddings and metadata of the documents
    that contain images. It uses `SimpleIndexer`. The parameters are as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`image_indexer`是存储包含图像的文档的嵌入向量和元数据的地方。它使用`SimpleIndexer`。使用的参数如下：'
- en: '`index_file_name`: This defines the folder where the data is stored'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_file_name`：定义存储数据的文件夹。'
- en: '`needs`: This makes the Executor part of the image processing path, by explicitly
    making it depend on `image_encoder`'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needs`：通过明确指定它依赖于`image_encoder`，将该Executor纳入图像处理路径。'
- en: 'The next elements will be part of the text path. `text_filter` is similar to
    `image_filter`. It reads data, but only text-based documents. The parameters used
    here are as follows:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的元素将是文本路径的一部分。`text_filter`类似于`image_filter`。它读取数据，但仅限于文本型文档。这里使用的参数如下：
- en: '`py_modules`: This parameter again defines the files where the `TextFilterExecutor`
    is defined.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`py_modules`：此参数再次定义`TextFilterExecutor`所在的文件。'
- en: '`needs: gateway` defines the path of dependencies between the Executors. In
    this case, this Executor is at the beginning of the path and thus depends on `gateway`.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needs: gateway`定义了Executor之间的依赖路径。在这种情况下，该Executor位于路径的起始位置，因此依赖于`gateway`。'
- en: 'Next, similar to the image path, we have the encoder `text_encoder`. This processes
    the text and encodes it using `CLIPTextEncoder`. The parameters used here are
    as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，类似于图像路径，我们有编码器`text_encoder`。它处理文本并使用`CLIPTextEncoder`对其进行编码。这里使用的参数如下：
- en: '`needs: text_filter`: This parameter specifies that this Executor is part of
    the text pat.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`needs: text_filter`：此参数指定该Executor是文本路径的一部分。'
- en: '`text_indexer` stores the embeddings of the Executor.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`text_indexer`存储Executor的嵌入向量。'
- en: Finally, we join the two paths. `join_all` joins the results from the two paths
    into one. The `needs` parameter here is given a list of Executor names.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将两条路径连接起来。`join_all`将两条路径的结果合并为一个。此处的`needs`参数指定了一个Executor名称的列表。
- en: 'You will have noticed an argument that is repeated across some of the Executors:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到某些Executor中有一个重复的参数：
- en: '`volumes`: This is the Docker syntax for mounting a local directory into the
    Docker container.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`volumes`：这是Docker语法，用于将本地目录挂载到Docker容器中。'
- en: query.yml
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: query.yml
- en: In this section, we will cover the query (search) Flow. This designates the
    process for searching the data you have indexed (stored) with the aforementioned
    index Flow. The configuration of the Executors is the same, at an individual level.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍查询（搜索）Flow。它指定了使用前述索引Flow搜索你已索引（存储）的数据的过程。Executor的配置是相同的，按个别Executor配置。
- en: 'As can be seen from the following diagram, the Flow path is also similar. It
    also bifurcates at the start, depending on the data type:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从下图可以看出，Flow路径也类似。它在开始时也会分叉，取决于数据类型：
- en: '![Figure 7.5 – Query Flow showing cross-modal features ](img/Figure_7.5_B17488.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 显示跨模态特征的查询流程](img/Figure_7.5_B17488.jpg)'
- en: Figure 7.5 – Query Flow showing cross-modal features
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 显示跨模态特征的查询流程
- en: The difference is that now we are searching with documents across the two modalities.
    Thus, `text_loader` sends the documents with text to be encoded by `text_encoder`,
    but the actual similarity matching is done with image documents that have been
    stored in `image_indexer`, from the index Flow. This is the central aspect that
    allows us to achieve cross-modality searching in this example.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于，现在我们在两种模态之间进行文档搜索。因此，`text_loader`将带有文本的文档发送给`text_encoder`进行编码，但实际的相似度匹配是通过存储在`image_indexer`中的图像文档进行的，这些图像文档来自索引Flow。这是使我们能够在本示例中实现跨模态搜索的核心要素。
- en: Installing and running the example
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装并运行示例
- en: 'To run the example, do the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行示例，请执行以下操作：
- en: Make sure the requirements defined at the beginning of this chapter are fulfilled.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保本章开始时定义的要求已满足。
- en: Clone the code from the repository at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)
    and open a terminal in the `src/Chapter07/cross-modal-search` folder.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)仓库克隆代码，并在`src/Chapter07/cross-modal-search`文件夹中打开终端。
- en: 'Note that this example only includes two images as a sample dataset. In order
    to download the entire dataset and explore the results, you will need to download
    it from Kaggle. You can do so by registering for a free Kaggle account. Then,
    set up your API token. Finally, to download the `flickr 8k` dataset, run the following
    command in a terminal:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，本示例仅包含两个图像作为示例数据集。为了下载整个数据集并探索结果，你需要从Kaggle下载数据集。你可以通过注册一个免费的Kaggle账户来实现。然后，设置你的API令牌。最后，要下载`flickr
    8k`数据集，请在终端中运行以下命令：
- en: '[PRE7]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To index the full dataset, run the following:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要索引完整数据集，请运行以下命令：
- en: '[PRE8]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Starting the index Flow and indexing the sample data is done from the command
    line, like so:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动索引流程并索引示例数据是通过命令行完成的，如下所示：
- en: '[PRE9]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This creates the index Flow, processes the data in the specific folder, and
    stores it in a local folder, `workspace`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建索引流程，处理指定文件夹中的数据，并将其存储在本地文件夹`workspace`中。
- en: 'Then, in order to start the searching Flow and allow the user to perform a
    search query, you can run this command:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，为了启动搜索流程并允许用户执行搜索查询，你可以运行以下命令：
- en: '[PRE10]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s begin by running a small test query. This test query actually contains
    both an image and a text document. The text is the sentence `a black dog and a
    spotted dog are fighting.` The image is `toy-data/images/1000268201_693b08cb0e.jpg`.
    The system then searches with both the image and the text, in a cross-modal manner.
    This means the image is used to search across the text data and the text is used
    to search across the image data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先运行一个小的测试查询。这个测试查询实际上包含了图像和文本文档。文本是句子`a black dog and a spotted dog are fighting.`，图像是`toy-data/images/1000268201_693b08cb0e.jpg`。系统然后通过图像和文本以跨模态的方式进行搜索。这意味着图像用来在文本数据中进行搜索，而文本用来在图像数据中进行搜索。
- en: 'The text results from searching with the image will be printed in your terminal
    as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图像进行搜索的文本结果将在你的终端中打印，结果如下：
- en: '![Figure 7.6 – Cross-modal search terminal output ](img/Figure_7.6_B17488.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 跨模态搜索终端输出](img/Figure_7.6_B17488.jpg)'
- en: Figure 7.6 – Cross-modal search terminal output
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 跨模态搜索终端输出
- en: 'The image results will be shown in a `matplotlib` figure as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图像结果将在`matplotlib`图形中显示，结果如下：
- en: '![Figure 7.7 – Cross-modal search plot output ](img/Figure_7.7_B17488.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 跨模态搜索绘图输出](img/Figure_7.7_B17488.jpg)'
- en: Figure 7.7 – Cross-modal search plot output
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 跨模态搜索绘图输出
- en: In this case, a lower score is better, as it measures the distance between the
    vectors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，较低的得分表示较好，因为它衡量的是向量之间的距离。
- en: 'You can pass your own image queries with the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式传递自己的图像查询：
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `path_to_your_image` variable can be provided as either an absolute or relative
    path, from the terminal’s current working directory path.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`path_to_your_image`变量可以提供为绝对路径或相对路径，基于终端当前的工作目录路径。'
- en: 'Or, for text, you can do it like so:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，对于文本，你可以这样做：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this section, we have covered how the Jina framework allows us to easily
    build a cross-modal search application. This is possible due to Jina’s universal
    and generalizable data types, mainly the document, and flexible pipeline construction
    process. We see that the `needs` parameter allows us to split the processing pipeline
    into two paths, depending on the *mime* type. In the following section, we will
    see how we can serve data while modifying it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了Jina框架如何使我们轻松构建跨模态搜索应用程序。这得益于Jina的通用和可扩展的数据类型，主要是文档，以及灵活的管道构建过程。我们看到，`needs`参数允许我们根据*mime*类型将处理管道拆分为两条路径。在接下来的部分中，我们将看到如何在修改数据的同时提供服务。
- en: Concurrent querying and indexing data
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发查询和数据索引
- en: In this section, we will present the methodology for how to continuously serve
    your client’s requests while still being able to update, delete, or add new data
    to your database. This is a common requirement in the industry, but it is not
    trivial to achieve. The challenges here are around maintaining the vector index
    actualized with the most recent data, while also being able to update that data
    in an atomic manner, but also doing all these operations in a scalable, containerized
    environment. With the Jina framework, all of these challenges can be easily met
    and overcome.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何持续响应客户请求，同时能够更新、删除或向数据库中添加新数据的方法。这在行业中是一个常见需求，但实现起来并不简单。这里的挑战在于如何保持向量索引的最新状态，同时能够以原子方式更新数据，并且在可扩展、容器化的环境中执行所有这些操作。使用Jina框架，所有这些挑战都能轻松解决。
- en: By default, in a Jina Flow, you cannot both index data and search at the same
    time. This is due to the nature of the network protocol. In essence, each Executor
    is a single-threaded application. You can use sharding to extend the number of
    copies of an Executor that form an Executor group. However, this is only safe
    for purely parallel operations, such as encoding data. These sorts of operations
    do not affect the state of the Executor. On the other hand, **CRUD** (**Create/Read/Update/Delete**)
    are operations that affect the state. Generally, these are harder to parallelize
    in scalable systems. Thus, if you send a lot of data to index (to add) to your
    application, this will block all searching requests from your clients. This is,
    of course, highly limiting. In this solution, I will show how this can be tackled
    within Jina.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在Jina Flow中，您不能同时进行数据索引和搜索。这是由于网络协议的特性。实际上，每个执行器都是单线程的应用程序。您可以使用分片技术扩展执行器的副本数量，从而形成执行器组。然而，这仅对纯并行操作（如编码数据）安全。这些操作不会影响执行器的状态。另一方面，**CRUD**（**创建/读取/更新/删除**）操作会影响执行器的状态。通常，这些操作在可扩展系统中更难并行化。因此，如果您将大量数据发送到应用程序进行索引（添加），这将会阻塞所有来自客户端的搜索请求。这显然是一个很大的限制。在这个解决方案中，我将展示如何在Jina中解决这个问题。
- en: The key component of the solution is the **HNSWPostgresIndexer** Executor. This
    is an Executor for the Jina framework. It combines an in-memory HNSW vector database
    with a connection to a PostgreSQL database. The metadata of your documents is
    stored in the SQL database, while the embeddings are stored in RAM. Unlike the
    applications in the previous examples, it does not require two distinct Flows.
    All the CRUD operations are performed within one Flow life cycle. This is possible
    due to the Executor’s capacity to synchronize the states between the SQL database
    and its in-memory vector database. This can be configured to be done automatically
    or can be triggered manually at the desired time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的关键组件是**HNSWPostgresIndexer**执行器。这是Jina框架的执行器，它将内存中的HNSW向量数据库与PostgreSQL数据库连接起来。文档的元数据存储在SQL数据库中，而嵌入向量则存储在内存中。与之前示例中的应用程序不同，它不需要两个独立的Flow。所有的CRUD操作都在一个Flow生命周期内完成。这是由于执行器能够在SQL数据库和内存中的向量数据库之间同步状态。这个同步过程可以配置为自动完成，也可以在需要时手动触发。
- en: Let’s now delve into what each component of this example is doing. The code
    can be found at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)
    in the folder `/src/Chapter07/wikipedia-sentences-query-while-indexing`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解这个示例中的每个组件的作用。代码可以在[https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina)的`/src/Chapter07/wikipedia-sentences-query-while-indexing`文件夹中找到。
- en: app.py
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: app.py
- en: 'This is the main entry point of the example. The user can call it to start
    the index and search Flows or to search documents. In order to start the Flows,
    you run `app.py` as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是示例的主要入口点。用户可以调用它来启动索引并搜索流或文档。为了启动流，您可以按照以下方式运行`app.py`：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will initialize the Flow of the Jina application, with its Executors.
    It will then add new data to the **HNSWPostgreSQL** Executor, in batches of five
    documents at a time. This data is at first only inserted into the SQL database.
    This is because the SQL database is considered the primary source of data. The
    **HNSW** vector index will be gradually updated based on the data in the SQL database.
    Once there is data present, the Executor will automatically synchronize it into
    the HNSW vector index. This process continues until the data is fully inserted.
    Once one round has been completed, there will be data available for searching
    for the user. The user can then query the data with the following command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将初始化Jina应用程序的Flow，并启动其执行器。接着，它会按批次一次添加五个文档到**HNSWPostgreSQL**执行器中。最初，这些数据仅被插入到SQL数据库中。因为SQL数据库被视为数据的主要来源。**HNSW**向量索引将根据SQL数据库中的数据逐步更新。一旦有数据存在，执行器将自动将其同步到HNSW向量索引中。这个过程将持续进行，直到数据完全插入。一旦完成一轮，用户就可以查询数据了。用户可以通过以下命令查询数据：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Then the user will be prompted for text input for a query. This text will then
    be encoded and compared with the existing dataset to get the best matches. These
    will be printed back to the terminal.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，系统会提示用户输入查询文本。该文本将被编码并与现有数据集进行比较，以获得最佳匹配结果。这些匹配结果将打印回终端。
- en: flow.yml
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: flow.yml
- en: This file defines the structure of the Flow used both when indexing data (adding
    data) and searching. I will explain the different options.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件定义了 Flow 的结构，用于数据索引（添加数据）和搜索。我将解释不同的选项。
- en: 'Following is the diagram of the index Flow. Notice that it is quite simple:
    we are just encoding and storing the encoded data. The complexity of this example
    application arises from the internal behavior of the **HNSWPostgreSQL** Executor.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是索引 Flow 的图示。注意它非常简单：我们只是编码并存储编码后的数据。这个示例应用的复杂性来自于 **HNSWPostgreSQL** Executor
    的内部行为。
- en: '![Figure 7.8 – Query Flow showing concurrency ](img/Figure_7.8_B17488.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 显示并发的查询流程](img/Figure_7.8_B17488.jpg)'
- en: Figure 7.8 – Query Flow showing concurrency
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 显示并发的查询流程
- en: 'The Flow itself has the following arguments:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Flow 本身有以下参数：
- en: '`protocol`: Defines that the Flow should open its HTTP protocol to the exterior'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`protocol`：定义 Flow 应该开放其 HTTP 协议给外部'
- en: '`port_expose`: Defines the port for listening on'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port_expose`：定义监听的端口'
- en: 'Then, the Executors define the steps in the Flow:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，Executors 定义了 Flow 中的各个步骤：
- en: The first one is `storage_encoder`. This uses `FlairTextEncoder` from Jina Hub.
    This encodes the text into a vector, for the linear algebra operations required
    in machine learning.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个是 `storage_encoder`。这个参数使用 Jina Hub 上的 `FlairTextEncoder`，它将文本编码成向量，用于机器学习中所需的线性代数运算。
- en: 'The second one is `indexer`. This uses `HNSWPostgresIndexer`, also from Jina
    Hub. The parameters used here are the following:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数是 `indexer`。它使用 Jina Hub 上的 `HNSWPostgresIndexer`。这里使用的参数如下：
- en: '`install_requirements`: Setting this to `True` will install the libraries required
    for this Executor'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`install_requirements`：设置为 `True` 时，将安装该 Executor 所需的库'
- en: '`sync_interval`: How many seconds to wait between automatically synchronizing
    the data from the SQL database into the vector database'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sync_interval`：自动将数据从 SQL 数据库同步到向量数据库之间的等待时间，单位为秒'
- en: '`dim`: The dimensionality of the embeddings'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`：嵌入的维度'
- en: 'You will have noticed an additional argument that is repeated across some of
    the Executors:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到某些 Executors 中有一个额外的参数被重复使用：
- en: '`timeout_ready`: This defines the number of seconds to wait for an Executor
    to become available before it’s canceled. We set it to `–1` so we wait as long
    as it’s required. Depending on your scenario, this should be adjusted. For example,
    if you want to safely terminate a long-running downloading request, you can set
    it to whatever amount of seconds you want to wait for the Executor to start.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout_ready`：定义等待 Executor 可用的秒数，超时后取消执行。我们将其设置为 `-1`，因此将根据需要等待。根据您的场景，可以调整此参数。例如，如果你希望安全终止一个长时间运行的下载请求，可以设置等待
    Executor 启动的秒数。'
- en: Installing and running the example
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装并运行示例
- en: 'Before running this example, make sure you understand the basic text search
    from the previous chapter, the chatbot example. Also, you will need to install
    Docker on your computer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此示例之前，确保你理解上一章中的基本文本搜索，特别是聊天机器人示例。另外，你需要在计算机上安装 Docker：
- en: Clone the Git repository from [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing)
    and open a terminal in the example’s folder.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing)
    克隆 Git 仓库，并在示例文件夹中打开终端。
- en: Create a new Python 3.7 environment. Although it is not required, it is strongly
    recommended.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 Python 3.7 环境。虽然不是强制要求，但强烈推荐这样做。
- en: 'Install the requirements:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装依赖：
- en: '[PRE15]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The repository includes a small subset of the Wikipedia dataset, for quick testing.
    You can just use that. If you want to use the entire dataset, run `bash` `get_data.sh`
    and then modify the `DATA_FILE` constant (in `app.py`) to point to that file.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库包含一个 Wikipedia 数据集的小子集，供快速测试使用。你可以直接使用它。如果你想使用整个数据集，运行 `bash get_data.sh`，然后修改
    `app.py` 中的 `DATA_FILE` 常量，将其指向该文件。
- en: 'Then start the Flow with the following command:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用以下命令启动 Flow：
- en: '[PRE16]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This creates the Flow and establishes the data synchronizing loop, as described
    in `app.py` previously.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建 Flow 并建立数据同步循环，如前文 `app.py` 中所描述。
- en: 'In order to query the data, run the following:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查询数据，运行以下命令：
- en: '[PRE17]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You will then be prompted for some text input. Enter whatever query you wish.
    You will then get back the best matches for your query.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，系统会提示你输入一些文本。你可以输入任何你想查询的内容。然后你将得到与查询最匹配的结果。
- en: Since the Flows expose an HTTP protocol, you can query the REST API with the
    Jina Client, cURL, Postman, or the custom Swagger UI built within Jina. The Swagger
    UI can be reached at the URL informed by the Flow, in the terminal. Usually, it’s
    `http://localhost:45678/docs`, but it depends on your configured system.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Flows 公开了 HTTP 协议，你可以通过 Jina 客户端、cURL、Postman 或 Jina 内置的自定义 Swagger UI 来查询
    REST API。你可以通过终端中 Flow 提供的 URL 访问 Swagger UI。通常是 `http://localhost:45678/docs`，但这取决于你配置的系统。
- en: In this section, we have learned how we can use the `HNSWPostgreSQLIndexer`
    Executor to concurrently index and search data in our live system. In the previous
    examples, the Flow needed to be redefined and restarted in order to switch between
    the two modes. Since this Executor combines both the metadata store (via a connection
    to a SQL database) and the embeddings index (via an in-memory HNSW index), it
    is possible to perform all CRUD operations within one Flow life cycle. Using these
    techniques, we can have a real client-facing application that is not blocked by
    the need to update the underlying database in the index.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用 `HNSWPostgreSQLIndexer` 执行器，在我们的实时系统中并行索引和搜索数据。在之前的示例中，Flow 需要重新定义并重启，以在两种模式之间切换。由于该执行器结合了元数据存储（通过与
    SQL 数据库的连接）和嵌入索引（通过内存中的 HNSW 索引），因此可以在一个 Flow 生命周期内执行所有 CRUD 操作。利用这些技术，我们可以拥有一个不受底层数据库索引更新限制的真实客户端应用。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have analyzed and practiced how you can use Jina’s advanced
    features, such as chunking, modality, and the advanced `HNSWPostgreSQL` Executor,
    in order to tackle the most difficult goals of neural search. We implemented solutions
    for arbitrary hierarchical depth data representation, cross-modality searching,
    and non-blocking data updates. Chunking allowed us to reflect on some data’s properties
    of having multiple levels of semantic meaning, such as sentences in a paragraph
    or video clips in longer films. Cross-modal searching opens up one of the main
    advantages of neural search – its data universality. This means that you can search
    with any data for any type of data, as long as you use the correct model for the
    data type. Finally, the `HNSWPostgreSQL` Executor allows us to build a live system
    where users can both search and index at the same time, with the data being kept
    in sync.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们分析并实践了如何使用 Jina 的高级功能，如分块、模态和高级的`HNSWPostgreSQL` 执行器，以应对神经搜索中最具挑战性的目标。我们实现了针对任意层次深度数据表示、跨模态搜索和非阻塞数据更新的解决方案。分块使我们能够反映一些数据具有多层次语义意义的特性，例如段落中的句子或长篇电影中的视频片段。跨模态搜索开启了神经搜索的一个主要优势——数据的通用性。这意味着，只要使用正确的模型，你就可以用任何类型的数据进行任何类型的数据搜索。最后，`HNSWPostgreSQL`
    执行器使我们能够构建一个实时系统，用户既可以搜索也可以索引，同时数据保持同步。
