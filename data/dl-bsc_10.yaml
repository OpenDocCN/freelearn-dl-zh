- en: Appendix A
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 A
- en: About
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于
- en: This section is included to assist the students to perform the activities present
    in the book. It includes detailed steps that are to be performed by the students
    to complete and achieve the objectives of the activity.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容是为了帮助学生完成书中的活动。它包括学生为完成并实现活动目标所需要执行的详细步骤。
- en: Computational Graph of the Softmax-with-Loss Layer
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax-with-Loss 层的计算图
- en: 'The following figure is the computational graph of the Softmax-with-Loss layer
    and obtains backward propagation. We will call the softmax function the Softmax
    layer, the cross-entropy error the **Cross-Entropy Error** layer, and the layer
    where these two are combined the Softmax-with-Loss layer. You can represent the
    Softmax-with-Loss layer with the computational graph provided in *Figure A.1*:
    Entropy:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是 Softmax-with-Loss 层的计算图，并获得反向传播。我们将 softmax 函数称为 Softmax 层，交叉熵误差称为 **交叉熵误差**
    层，而这两者组合的层称为 Softmax-with-Loss 层。你可以通过 *图 A.1* 提供的计算图表示 Softmax-with-Loss 层：熵：
- en: '![Figure A.1: Computational graph of the Softmax-with-Loss layer'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.1：Softmax-with-Loss 层的计算图'
- en: '](img/Figure_A.1.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.1.jpg)'
- en: 'Figure A.1: Computational graph of the Softmax-with-Loss layer'
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A.1：Softmax-with-Loss 层的计算图
- en: The computational graph shown in *Figure A.1* assumes that there is a neural
    network that classifies three classes. The input from the previous layer is (a1,
    a2, a3), and the Softmax layer outputs (y1, y2, y3). The label is (t1, t2, t3)
    and the Cross-Entropy Error layer outputs the loss, L.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 A.1* 中显示的计算图假设有一个神经网络用于分类三个类别。来自上一层的输入是 (a1, a2, a3)，Softmax 层输出 (y1, y2,
    y3)。标签是 (t1, t2, t3)，交叉熵误差层输出损失 L。'
- en: This appendix shows that the result of backward propagation of the Softmax-with-Loss
    layer will be (y1 − t1, y2 − t2, y3 − t3), as shown in *Figure A.1*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录展示了 Softmax-with-Loss 层反向传播的结果为 (y1 − t1, y2 − t2, y3 − t3)，如 *图 A.1* 所示。
- en: Forward Propagation
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播
- en: The computational graph shown in *Figure A.1* does not show the details of the
    Softmax layer and the Cross-Entropy Error layer. Here, we will start by describing
    the details of the two layers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 A.1* 中显示的计算图并没有展示 Softmax 层和交叉熵误差层的详细信息。在这里，我们将首先描述这两个层的细节。'
- en: 'First, let''s look at the Softmax layer. We can represent the softmax function
    with the following equation:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下 Softmax 层。我们可以用以下方程表示 softmax 函数：
- en: '| ![95](img/Figure_A.1a.png) | (A.1) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| ![95](img/Figure_A.1a.png) | (A.1) |'
- en: Therefore, we can show the Softmax layer with the computational graph provided
    in *Figure A.2*. Here, S stands for the sum of exponentials, which is the denominator
    in equation (A.1). The final output is (y1, y2, y3).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过 *图 A.2* 提供的计算图展示 Softmax 层。在这里，S 代表指数和，是方程 (A.1) 中的分母。最终输出为 (y1, y2,
    y3)。
- en: '![Figure A.2: Computational graph of the Softmax layer (forward propagation
    only)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.2：Softmax 层的计算图（仅前向传播）'
- en: '](img/Figure_A.2.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.2.jpg)'
- en: 'Figure A.2: Computational graph of the Softmax layer (forward propagation only)'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A.2：Softmax 层的计算图（仅前向传播）
- en: 'Next, let''s look at the Cross-Entropy Error layer. The following equation
    shows the cross-entropy error:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看一下交叉熵误差层。以下方程展示了交叉熵误差：
- en: '| ![97](img/Figure_A.2a.png) | (A.2) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ![97](img/Figure_A.2a.png) | (A.2) |'
- en: Based on equation (A.2), we can draw the computational graph of the Cross-Entropy
    Error layer as shown in *Figure A.3*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方程 (A.2)，我们可以画出交叉熵误差层的计算图，如 *图 A.3* 所示。
- en: The computational graph shown in *Figure A.3* just shows equation (A. 2) as
    a computational graph. Therefore, I think that there is nothing particularly difficult
    about this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 A.3* 中显示的计算图只是将方程 (A.2) 展示为计算图。因此，我认为这里没有什么特别困难的。'
- en: '![Figure A.3: Computational graph of the Cross-Entropy Error layer (forward
    propagation only)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.3：交叉熵误差层的计算图（仅前向传播）'
- en: '](img/Figure_A.3.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.3.jpg)'
- en: 'Figure A.3: Computational graph of the Cross-Entropy Error layer (forward propagation
    only)'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A.3：交叉熵误差层的计算图（仅前向传播）
- en: 'Now, let''s look at backward propagation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下反向传播：
- en: Backward Propagation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'First, let''s look at backward propagation of the Cross-Entropy Error layer.
    We can draw backward propagation of the Cross-Entropy Error layer as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下交叉熵误差层的反向传播。我们可以如下绘制交叉熵误差层的反向传播：
- en: '![Figure A.4: Backward propagation of the Cross-Entropy Error layer'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.4：交叉熵误差层的反向传播'
- en: '](img/Figure_A.4.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.4.jpg)'
- en: 'Figure A.4: Backward propagation of the Cross-Entropy Error layer'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.4: 交叉熵误差层的反向传播'
- en: 'Please note the following to obtain backward propagation of this computational
    graph:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意以下事项，以便获得此计算图的反向传播：
- en: The initial value of backward propagation (the rightmost value of backward propagation
    in *Figure A.4*) is 1 (because ![98](img/Figure_A.4a.png)).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播的初始值（*图 A.4*中的最右侧反向传播值）为 1（因为![98](img/Figure_A.4a.png)）。
- en: For backward propagation of the "x" node, the "reversed value" of the input
    signal for forward propagation multiplied by the derivative from the upper stream
    is passed downstream.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“x”节点的反向传播，正向传播输入信号的“反转值”乘以上游的导数后传递到下游。
- en: For the "+" node, the derivative from the upper stream is passed without changing it.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“+”节点，来自上游的导数被传递而不进行任何更改。
- en: The backward propagation of the "log" node observes the following equations:![99](img/Figure_A.4b.jpg)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “log”节点的反向传播遵循以下方程：![99](img/Figure_A.4b.jpg)
- en: Based on this, we can obtain backward propagation of the Cross-Entropy Error
    layer easily. As a result, the value ![100](img/Figure_A.4d.png) will be the input
    to the backward propagation of the Softmax layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们可以轻松地获得交叉熵误差层的反向传播。因此，值![100](img/Figure_A.4d.png)将作为反向传播到 Softmax 层的输入。
- en: 'Next, let''s look at backward propagation of the Softmax layer. Because the
    Softmax layer is a little complicated, I want to check its backward propagation
    step by step:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看 Softmax 层的反向传播。由于 Softmax 层稍微复杂，我想一步步检查它的反向传播：
- en: '**Step 1:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步：**'
- en: '![Figure A.5: Step 1'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.5: 第 1 步'
- en: '](img/Figure_A.5.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.5.jpg)'
- en: 'Figure A.5: Step 1'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.5: 第 1 步'
- en: The values of the backward propagation arrive from the previous layer (Cross-Entropy
    Error layer).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的值来自前一层（交叉熵误差层）。
- en: '**Step 2:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步：**'
- en: '![Figure A.6: Step 2'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.6: 第 2 步'
- en: '](img/Figure_A.6.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.6.jpg)'
- en: 'Figure A.6: Step 2'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.6: 第 2 步'
- en: 'The "x" node "reverses" the values of forward propagation for multiplication.
    Here, the following calculation is performed:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “x”节点“反转”正向传播的值以进行乘法计算。这里，执行以下计算：
- en: '| ![101](img/Figure_A.6a.png) | (A.3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![101](img/Figure_A.6a.png) | (A.3) |'
- en: '**Step 3:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步：**'
- en: '![Figure A.7: Step 3'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.7: 第 3 步'
- en: '](img/Figure_A.7.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.7.jpg)'
- en: 'Figure A.7: Step 3'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.7: 第 3 步'
- en: If the flow branches into multiple values in forward propagation, the separated
    values are added in backward propagation. Therefore, three separate values of
    backward propagation, ![102](img/Figure_A.7a.png), are added here. The backward
    propagation of */* is conducted for the added values, resulting in ![103](img/Figure_A.7b.png).
    Here, (*t*1, *t*2, *t*3) is the label and a "one-hot vector." A one-hot vector
    means that one of (*t*1, *t*2, *t*3) is 1 and the others are all 0s. Therefore,
    the sum of (*t*1, *t*2, *t*3) is 1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果正向传播中的流分支成多个值，则在反向传播中这些分离的值会被相加。因此，在这里三个分开的反向传播值![102](img/Figure_A.7a.png)被加在一起。对于添加的值，进行
    */* 的反向传播，结果是![103](img/Figure_A.7b.png)。这里，(*t*1, *t*2, *t*3) 是标签和“独热向量”。独热向量意味着
    (*t*1, *t*2, *t*3) 中的一个值为 1，其他值都为 0。因此，(*t*1, *t*2, *t*3) 的和为 1。
- en: '**Step 4:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 4 步：**'
- en: '![Figure A.8: Step 4'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.8: 第 4 步'
- en: '](img/Figure_A.8.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.8.jpg)'
- en: 'Figure A.8: Step 4'
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.8: 第 4 步'
- en: The "+" node only passes the value without changing it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: “+”节点只传递值而不改变它。
- en: '**Step 5:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 步：**'
- en: '![Figure A.9: Step 5'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.9: 第 5 步'
- en: '](img/Figure_A.9.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.9.jpg)'
- en: 'Figure A.9: Step 5'
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.9: 第 5 步'
- en: The "x" node "reverses" the values for multiplication. Here, ![104](img/Figure_A.9a.png)
    is used to transform the equation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: “x”节点“反转”值以进行乘法计算。这里，![104](img/Figure_A.9a.png)被用来转化方程式。
- en: '**Step 6:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 6 步：**'
- en: '![Figure A.10: Step 6'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.10: 第 6 步'
- en: '](img/Figure_A.10.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.10.jpg)'
- en: 'Figure A.10: Step 6'
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 A.10: 第 6 步'
- en: 'In the "exp" node, the following equations hold true:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在“exp”节点中，以下方程成立：
- en: '| ![105](img/Figure_A.10a.png) | (A.4) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![105](img/Figure_A.10a.png) | (A.4) |'
- en: Thus, the sum of the two separate inputs, which are multiplied by exp(a1), is
    the backward propagation to obtain. We can write this as ![106](img/Figure_A.10c.png)
    and obtain ![107](img/Figure_A.10d.png) after transformation. Thus, in the node
    where the input of forward propagation is ![108](img/Figure_A.10e.png), backward
    propagation is ![109](img/Figure_A.10f.png). For ![110](img/Figure_A.10g.png)
    and ![111](img/Figure_A.10h.png), we can use the same procedure (the results are
    ![112](img/Figure_A.10i.png) and ![113](img/Figure_A.10j.png), respectively).
    With this, it is easy to show that we can achieve the same result even if we want
    to classify n classes instead of three classes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将两个单独输入的和相加，并乘以exp(a1)，就是反向传播所得到的结果。我们可以将其写为 ![106](img/Figure_A.10c.png)，并通过变换得到
    ![107](img/Figure_A.10d.png)。因此，在前向传播输入为 ![108](img/Figure_A.10e.png) 的节点，反向传播为
    ![109](img/Figure_A.10f.png)。对于 ![110](img/Figure_A.10g.png) 和 ![111](img/Figure_A.10h.png)，我们可以使用相同的过程（结果分别为
    ![112](img/Figure_A.10i.png) 和 ![113](img/Figure_A.10j.png)）。通过这种方式，我们可以轻松地证明，即使我们想要分类n类而不是三类，我们也能得到相同的结果。
- en: Summary
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Here, the computational graph of the Softmax-with-Loss layer was shown in detail,
    and its backward propagation was obtained. *Figure A.11* shows the complete computational
    graph of the Softmax-with-Loss layer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里详细展示了带损失的Softmax层的计算图，并得到了其反向传播结果。*图 A.11* 显示了带损失的Softmax层的完整计算图：
- en: '![Figure A.11: Computational graph of the Softmax-with-Loss layer'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 A.11：带损失的Softmax层的计算图'
- en: '](img/Figure_A.11.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_A.11.jpg)'
- en: 'Figure A.11: Computational graph of the Softmax-with-Loss layer'
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A.11：带损失的Softmax层的计算图
- en: The computational graph shown in *Figure A.11* looks complicated. However, if
    you advance step by step using computational graphs, obtaining derivatives (the
    procedure of backward propagation) will be much less troublesome. When you encounter
    a layer that looks complicated (such as the Batch Normalization layer), other
    than the Softmax-with-Loss layer described here, you can use this procedure. This
    will be easier to understand in practice rather than only looking at equations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 A.11*所示，计算图看起来复杂。然而，如果按步骤推进计算图，获得导数（即反向传播过程）将会变得不那么麻烦。当你遇到看起来复杂的层（例如批归一化层）时，除了这里描述的带损失的Softmax层，你可以使用此过程。这比仅仅看公式更容易理解。
