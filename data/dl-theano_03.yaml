- en: Chapter 3. Encoding Word into Vector
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。将单词编码为向量
- en: In the previous chapter, inputs to neural nets were images, that is, vectors
    of continuous numeric values, the **natural language** for neural nets. But for
    many other machine learning fields, inputs may be categorical and discrete.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，神经网络的输入是图像，也就是连续数值的向量，**自然语言**是神经网络的语言。但是对于许多其他机器学习领域，输入可能是类别型的和离散的。
- en: In this chapter, we'll present a technique known as embedding, which learns
    to transform discrete input signals into vectors. Such a representation of inputs
    is an important first step for compatibility with the rest of neural net processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种叫做嵌入的技术，它学会将离散的输入信号转换为向量。这种输入表示是与神经网络其他处理兼容的重要第一步。
- en: Such embedding techniques will be illustrated with an example of natural language
    texts, which are composed of words belonging to a finite vocabulary.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入技术将通过一个自然语言文本的示例来说明，这些文本由属于有限词汇表的单词组成。
- en: 'We will present the different aspects of embedding:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍嵌入的不同方面：
- en: The principles of embedding
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入的原理
- en: The different types of word embedding
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的单词嵌入
- en: One hot encoding versus index encoding
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独热编码与索引编码
- en: Building a network to translate text into vectors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个网络将文本转换为向量
- en: Training and discovering the properties of embedding spaces
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练并发现嵌入空间的特性
- en: Saving and loading the parameters of a model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载模型的参数
- en: Dimensionality reduction for visualization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化的降维
- en: Evaluating the quality of embeddings
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估嵌入的质量
- en: Applications of embedding spaces
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入空间的应用
- en: Weight tying
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重绑定
- en: Encoding and embedding
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码与嵌入
- en: 'Each word can be represented by an index in a vocabulary:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词可以通过词汇表中的索引来表示：
- en: '![Encoding and embedding](img/00043.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Encoding and embedding](img/00043.jpeg)'
- en: 'Encoding words is the process of representing each word as a vector. The simplest
    method of encoding words is called one-hot or 1-of-K vector representation. In
    this method, each word is represented as an ![Encoding and embedding](img/00044.jpeg)
    vector with all 0s and one 1 at the index of that word in the sorted vocabulary.
    In this notation, |V| is the size of the vocabulary. Word vectors in this type
    of encoding for vocabulary {**King**, **Queen**, **Man**, **Woman**, **Child**}
    appear as in the following example of encoding for the word **Queen**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 编码单词是将每个单词表示为一个向量的过程。编码单词的最简单方法称为独热编码或1-of-K向量表示法。在这种方法中，每个单词都表示为一个![Encoding
    and embedding](img/00044.jpeg)向量，该向量的所有元素都是0，只有在该单词在排序词汇表中的索引位置上为1。在这种表示法中，|V|表示词汇表的大小。对于词汇表{**国王**,
    **女王**, **男人**, **女人**, **孩子**}，在这种类型的编码下，单词**女王**的编码示例如下：
- en: '![Encoding and embedding](img/00045.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Encoding and embedding](img/00045.jpeg)'
- en: In the one-hot vector representation method, every word is equidistant from
    the other. However, it fails to preserve any relationship between them and leads
    to data sparsity. Using word embedding does overcome some of these drawbacks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在独热向量表示方法中，每个单词与其他单词的距离相等。然而，它无法保留它们之间的任何关系，并且会导致数据稀疏性。使用单词嵌入可以克服这些缺点。
- en: Word embedding is an approach to distributional semantics that represents words
    as vectors of real numbers. Such representation has useful clustering properties,
    since it groups together words that are semantically and syntactically similar.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入是一种分布式语义方法，它将单词表示为实数向量。这样的表示具有有用的聚类属性，因为它将语义和句法相似的单词聚集在一起。
- en: 'For example, the words **seaworld** and **dolphin** will be very close in the
    created space. The main aim of this step is to map each word into a continuous,
    low-dimensional, and real-valued vector and use it as input to a model such as
    a **Recurrent Neural Network** (**RNN**), a **Convolutional Neural Network** (**CNN**),
    and so on:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，单词**海洋世界**和**海豚**将在创建的空间中非常接近。这一步的主要目的是将每个单词映射到一个连续的、低维的实值向量，并将其用作模型的输入，例如**循环神经网络**（**RNN**）、**卷积神经网络**（**CNN**）等：
- en: '![Encoding and embedding](img/00046.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Encoding and embedding](img/00046.jpeg)'
- en: Such a representation is **dense**. We would expect synonyms and interchangeable
    words to be close in that space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示是**稠密的**。我们期望同义词和可互换的单词在该空间中接近。
- en: 'In this chapter, we will present the very popular model of word embedding,
    Word2Vec, which was initially developed by Mikolov et al. in 2013\. Word2Vec has
    two different models: **Continuous Bag of Words** (**CBOW**) and **Skip-gram**.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将介绍一种非常流行的词嵌入模型——Word2Vec，它最初由Mikolov等人于2013年开发。Word2Vec有两种不同的模型：**连续词袋模型**（**CBOW**）和**跳字模型**（**Skip-gram**）。
- en: 'In the CBOW method, the goal is to predict a word given a surrounding context.
    A Skip-gram predicts a surrounding context of words given a single word (see the
    following figure):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在CBOW方法中，目标是给定上下文来预测一个单词。而跳字模型则是根据单个单词预测周围的上下文（见下图）：
- en: '![Encoding and embedding](img/00047.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![编码与嵌入](img/00047.jpeg)'
- en: For this chapter, we will focus on the CBOW model. We will start by presenting
    the dataset, then we will explain the idea behind the method. Afterwards, we will
    show a simple implementation of it using Theano. Finally, we will end with referring
    to some applications of word embedding.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将重点介绍CBOW模型。我们将从展示数据集开始，然后解释该方法背后的思想。之后，我们将使用Theano展示它的简单实现。最后，我们将提到词嵌入的一些应用。
- en: Dataset
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Before we explain the model part, let us start by processing the text corpus
    by creating the vocabulary and integrating the text with it so that each word
    is represented as an integer. As a dataset, any text corpus can be used, such
    as Wikipedia or web articles, or posts from social networks such as Twitter. Frequently
    used datasets include PTB, text8, BBC, IMDB, and WMT datasets.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释模型部分之前，让我们先通过创建词汇表来处理文本语料库，并将文本与词汇表整合，以便每个单词都可以表示为一个整数。作为数据集，可以使用任何文本语料库，如维基百科或网页文章，或来自社交网络（如Twitter）的帖子。常用的数据集包括PTB、text8、BBC、IMDB和WMT数据集。
- en: 'In this chapter, we use the `text8` corpus. It consists of a pre-processed
    version of the first 100 million characters from a Wikipedia dump. Let us first
    download the corpus:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们使用`text8`语料库。它由维基百科转储中前1亿个字符的预处理版本构成。我们先来下载该语料库：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we construct the vocabulary and replace the rare words with tokens for
    **UNKNOWN**. Let us start by reading the data into a list of strings:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们构建词汇表，并用**UNKNOWN**替换稀有词汇。让我们先将数据读取为一个字符串列表：
- en: 'Read the data into a list of strings:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据读取为字符串列表：
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From the list of strings, we can now build the dictionary. We start by counting
    the frequency of the words in the `word_freq` dictionary. Afterwards, we replace
    the words that are infrequent, that have a number of ocurrences in the corpus
    less than `max_df`, with tokens.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从字符串列表中，我们现在可以构建字典。我们首先在`word_freq`字典中统计单词的频率。接着，我们用符号替换那些在语料库中出现次数少于`max_df`的稀有单词。
- en: 'Build the dictionary and replace rare words with the `UNK` token:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建字典并用`UNK`符号替换稀有单词：
- en: '[PRE2]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let us define the functions of creating the dataset (that is, the contexts
    and targets):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义创建数据集的函数（即上下文和目标）：
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Continuous Bag of Words model
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续词袋模型
- en: 'The design of the neural network to predict a word given its surrounding context
    is shown in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测给定上下文中单词的神经网络设计如下图所示：
- en: '![Continuous Bag of Words model](img/00048.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![连续词袋模型](img/00048.jpeg)'
- en: 'The input layer receives the context while the output layer predicts the target
    word. The model we''ll use for the CBOW model has three layers: input layer, hidden
    layer (also called the projection layer or embedding layer), and output layer.
    In our setting, the vocabulary size is V and the hidden layer size is N. Adjacent
    units are fully connected.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层接收上下文，而输出层预测目标单词。我们将用于CBOW模型的模型有三层：输入层、隐藏层（也称为投影层或嵌入层）和输出层。在我们的设置中，词汇表大小是V，隐藏层大小是N。相邻的单元是完全连接的。
- en: 'The input and the output can be represented either by an index (an integer,
    0-dimensional) or a one-hot-encoding vector (1-dimensional). Multiplying with
    the one-hot-encoding vector `v` consists simply of taking the j-th row of the
    embedding matrix:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出可以通过索引（一个整数，0维）或一-hot-编码向量（1维）表示。与一-hot-编码向量`v`相乘仅仅是取嵌入矩阵的第j行：
- en: '![Continuous Bag of Words model](img/00049.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![连续词袋模型](img/00049.jpeg)'
- en: Since the index representation is more efficient than the one-hot encoding representation
    in terms of memory usage, and Theano supports indexing symbolic variables, it
    is preferable to adopt the index representation as much as possible.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于索引表示在内存使用上比one-hot编码表示更高效，而且Theano支持索引符号变量，因此尽可能采用索引表示是更可取的。
- en: 'Therefore, input (context) will be 2-dimensional, represented by a matrix,
    with two dimensions: the batch size and the context length. The output (target)
    is 1-dimensional, represented by a vector with one dimension: the batch size.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输入（上下文）将是二维的，由一个矩阵表示，具有两个维度：批量大小和上下文长度。输出（目标）是一维的，由一个向量表示，具有一个维度：批量大小。
- en: 'Let''s define the CBOW model:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义CBOW模型：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The context and target variables are the known parameters of this model. The
    unknown parameters of the CBOW model are the connection matrix ![Continuous Bag
    of Words model](img/00050.jpeg), between the input layer and the hidden layer,
    and the connection matrix ![Continuous Bag of Words model](img/00051.jpeg), between
    the hidden layer and the output layer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文和目标变量是该模型的已知参数。CBOW模型的未知参数是输入层和隐藏层之间的连接矩阵！[连续词袋模型](img/00050.jpeg)，以及隐藏层和输出层之间的连接矩阵！[连续词袋模型](img/00051.jpeg)：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each row of ![Continuous Bag of Words model](img/00050.jpeg) is the N-dimension
    vector representation ![Continuous Bag of Words model](img/00052.jpeg) of the
    associated word, `i`, of the input layer, where `N` is the hidden layer size.
    Given a context, when computing the hidden layer output, the CBOW model takes
    the average of the vectors of the input context words, and uses the product of
    the `input -> hidden` weight matrix and the average vector as the output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![连续词袋模型](img/00050.jpeg)的每一行是关联单词`i`在输入层的N维向量表示，`N`是隐藏层的大小。给定一个上下文，在计算隐藏层输出时，CBOW模型会对输入上下文词的向量进行平均，然后使用`input
    -> hidden`权重矩阵与平均向量的乘积作为输出：'
- en: '![Continuous Bag of Words model](img/00053.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![连续词袋模型](img/00053.jpeg)'
- en: 'Here, `C` is the number of words in the context, `w1, w2, w3,..., wc` are the
    words in the context, and ![Continuous Bag of Words model](img/00054.jpeg) is
    the input vector of a word ![Continuous Bag of Words model](img/00055.jpeg). The
    activation function of the output layer is the softmax layer. Equations 2 and
    3 show how we compute the output layer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`C`是上下文中的单词数，`w1, w2, w3,..., wc`是上下文中的单词，![连续词袋模型](img/00054.jpeg)是单词![连续词袋模型](img/00055.jpeg)的输入向量。输出层的激活函数是softmax层。方程2和3展示了我们如何计算输出层：
- en: '![Continuous Bag of Words model](img/00056.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![连续词袋模型](img/00056.jpeg)'
- en: 'Here, ![Continuous Bag of Words model](img/00052.jpeg) is the j-th column of
    the matrix ![Continuous Bag of Words model](img/00051.jpeg) and `V` is the vocabulary
    size. In our settings, the vocabulary size is `vocab_size` and the hidden layer
    size is `emb_size`. The loss function is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![连续词袋模型](img/00052.jpeg)是矩阵![连续词袋模型](img/00051.jpeg)的第j列，`V`是词汇表大小。在我们的设置中，词汇表大小是`vocab_size`，隐藏层大小是`emb_size`。损失函数如下：
- en: '![Continuous Bag of Words model](img/00057.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![连续词袋模型](img/00057.jpeg)'
- en: (4)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (4)
- en: Now, let us translate equations 1, 2, 3, and 4 in Theano.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Theano中翻译方程1、2、3和4。
- en: 'To compute the hidden (projection) layer output: `input -> hidden (eq. 1)`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算隐藏层（投影层）输出：`input -> hidden (eq. 1)`
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The softmax activation (eq. 3) :'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: softmax激活（eq. 3）：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The loss function (eq. 4):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（eq. 4）：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Update the parameters of the model using SGD:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SGD更新模型的参数：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, we need to define the training and evaluation functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要定义训练和评估函数。
- en: 'Let''s make the dataset shared to pass it to the GPU. For simplicity, we assume
    that we have a function called `get_data_set` that returns the set of targets
    and its surrounding context:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据集设为共享，以便将其传递到GPU。为简便起见，我们假设有一个名为`get_data_set`的函数，它返回目标集及其周围上下文：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The input variable of `train_model` is the index of the batch, since the whole
    dataset has been transferred in one pass to the GPU thanks to shared variables.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_model`的输入变量是批次的索引，因为整个数据集已经通过共享变量一次性传输到GPU。'
- en: For validation during training, we evaluate the model using the cosine similarity
    between a mini batch of examples and all embeddings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中进行验证时，我们通过计算小批量示例与所有嵌入的余弦相似度来评估模型。
- en: 'Let''s use a `theano` variable to place the input to the validation model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个`theano`变量来放置验证模型的输入：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Normalized word embedding of the validation input:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 验证输入的标准化词嵌入：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similarity is given by the cosine similarity function:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度由余弦相似度函数给出：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training the model
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now we can start training the model. In this example, we chose to train the
    model using SGD with a batch size of 64 and 100 epochs. To validate the model,
    we randomly selected 16 words and used the similarity measure as an evaluation
    metric:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练模型了。在这个例子中，我们选择使用SGD进行训练，批大小为64，训练100个周期。为了验证模型，我们随机选择了16个词，并使用相似度度量作为评估指标：
- en: 'Let''s begin training:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始训练：
- en: '[PRE14]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Lastly, let us create two generic functions that will help us save any model
    parameters in a reusable `utils.py` utility file:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建两个通用函数，帮助我们将任何模型参数保存在可重用的 `utils.py` 工具文件中：
- en: '[PRE15]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Running on a GPU, the preceding code prints the following results:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GPU上运行时，前面的代码会打印以下结果：
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let us note:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们注意到：
- en: Rare words are updated only a small number of times, while frequent words appear
    more often in inputs and context windows. Subsampling of frequent words can remedy
    to this.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀有词只更新少数几次，而频繁出现的词在输入和上下文窗口中更常出现。对频繁词进行下采样可以缓解这个问题。
- en: All weights are updated in the output embedding, and only a few of them, those
    corresponding to the words in the context window, are updated positively. Negative
    sampling can help rebalance the positives and negatives in the update.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有权重都会在输出嵌入中更新，只有其中一部分，即对应于上下文窗口中的词汇，才会被正向更新。负采样有助于在更新中重新平衡正负样本。
- en: Visualizing the learned embeddings
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化学习到的嵌入
- en: Let us visualize the embedding in a 2D figure in order to get an understanding
    of how well they capture similarity and semantics. For that purpose, we need to
    reduce the number of dimension of the embedding, which is highly dimensional,
    to two dimensions without altering the structure of the embeddings.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将嵌入可视化为二维图形，以便了解它们如何捕捉相似性和语义。为此，我们需要将高维的嵌入降到二维，而不改变嵌入的结构。
- en: Reducing the number of dimension is called manifold learning, and many different
    techniques exist, some of them linear, such as **Principal Component Analysis**
    (**PCA**), **Independent Component Analysis** (**ICA**), **Linear Discriminant
    Analysis** (**LDA**), and **Latent Sementic Analysis** / **Indexing** (**LSA**
    / **LSI**), and some are non-linear, such as **Isomap**, **Locally Linear Embedding**
    (**LLE**), **Hessian** **Eigenmapping**, **Spectral embedding**, **Local tangent
    space embedding**, **Multi Dimensional Scaling** (**MDS**), and **t-distributed
    Stochastic Neighbor Embedding** (**t-SNE**).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 降维称为流形学习，存在许多不同的技术，其中一些是线性的，如**主成分分析**（**PCA**）、**独立成分分析**（**ICA**）、**线性判别分析**（**LDA**）和**潜在语义分析**
    / **索引**（**LSA** / **LSI**），一些是非线性的，如**Isomap**、**局部线性嵌入**（**LLE**）、**海森矩阵特征映射**、**谱嵌入**、**局部切空间嵌入**、**多维尺度法**（**MDS**）和**t-分布随机邻域嵌入**（**t-SNE**）。
- en: 'To display the word embedding, let us use t-SNE, a great technique adapted
    to high dimensional data to reveal local structures and clusters, without crowding
    points together:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显示词嵌入，我们使用t-SNE，这是一种适应高维数据的优秀技术，用于揭示局部结构和簇，而不会将点挤在一起：
- en: 'Visualize the embeddings:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化嵌入：
- en: '[PRE17]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The plotted map displays the words with similar embeddings close to each other:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制的地图显示了具有相似嵌入的词语彼此靠近：
- en: '![Visualizing the learned embeddings](img/00058.jpeg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![可视化学习到的嵌入](img/00058.jpeg)'
- en: Evaluating embeddings – analogical reasoning
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估嵌入 – 类比推理
- en: 'Analogical reasoning is a simple and efficient way to evaluate embeddings by
    predicting syntactic and semantic relationships of the form *a is to b as c is
    to _?*, denoted as *a : b → c : ?*. The task is to identify the held-out fourth
    word, with only exact word matches deemed correct.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '类比推理是一种简单有效的评估嵌入的方法，通过预测语法和语义关系，形式为*a 对 b 就像 c 对 _?*，记作 *a : b → c : ?*。任务是识别被省略的第四个单词，只有完全匹配的单词才被认为是正确的。'
- en: 'For example, the word *woman* is the best answer to the question *king is to
    queen as man is to?*. Assume that ![Evaluating embeddings – analogical reasoning](img/00059.jpeg)
    is the representation vector for the word ![Evaluating embeddings – analogical
    reasoning](img/00060.jpeg) normalized to unit norm. Then, we can answer the question
    *a : b → c : ?* , by finding the word ![Evaluating embeddings – analogical reasoning](img/00061.jpeg)
    with the representation closest to:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，单词*女人*是问题*国王对女王，如同男人对？*的最佳答案。假设![评估嵌入 - 类比推理](img/00059.jpeg)是单词![评估嵌入 -
    类比推理](img/00060.jpeg)的表示向量，并标准化为单位范数。那么，我们可以通过找到与表示向量最接近的单词![评估嵌入 - 类比推理](img/00061.jpeg)来回答问题*a
    : b → c : ?*。'
- en: '![Evaluating embeddings – analogical reasoning](img/00062.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![评估嵌入 - 类比推理](img/00062.jpeg)'
- en: 'According to cosine similarity:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据余弦相似度：
- en: '![Evaluating embeddings – analogical reasoning](img/00063.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![评估嵌入 - 类比推理](img/00063.jpeg)'
- en: 'Now let us implement the analogy prediction function using Theano. First, we
    need to define the input of the function. The analogy function receives three
    inputs, which are the word indices of `a`, `b`, and `c`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用Theano实现类比预测函数。首先，我们需要定义函数的输入。类比函数接收三个输入，即`a`、`b`和`c`的单词索引：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we need to map each input to the word embedding vector. Each row of `a_emb`,
    `b_emb`, `c_emb` is a word''s embedding vector:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将每个输入映射到单词嵌入向量。`a_emb`、`b_emb`、`c_emb`的每一行都是一个单词的嵌入向量：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we can compute the cosine distance between each target and vocab pair.
    We expect that d''s embedding vectors on the unit hyper-sphere is near: `c_emb
    + (b_emb - a_emb)`, which has the shape `[bsz, emb_size]`. `dist` has shape [`bsz,
    vocab_size`].'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算每个目标和词汇对之间的余弦距离。我们预期`d`在单位超球上的嵌入向量接近：`c_emb + (b_emb - a_emb)`，其形状为`[bsz,
    emb_size]`。`dist`的形状为`[bsz, vocab_size]`。
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this example, we consider that the prediction function takes the top four
    words. Thus, we can define the function in Theano as the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们认为预测函数取前四个单词。因此，我们可以在Theano中定义函数如下：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To run the preceding function, we need to load the evaluation data, which is
    in this example the set of analogy questions defined by Google. Each question
    contains four words separated by spaces. The first question can be interpreted
    as *Athens is to Greece as Baghdad is to _?* and the correct answer should be
    *Iraq*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行上述函数，我们需要加载评估数据，在本例中是由Google定义的类比问题集。每个问题包含四个用空格分隔的单词。第一个问题可以解释为*雅典对希腊的关系，如同巴格达对_？*，正确答案应为*伊拉克*：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let us load the analogy questions using the `read_analogies` function that
    is defined in the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码中定义的`read_analogies`函数加载类比问题：
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can run the evaluation model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行评估模型：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This results in:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Evaluating embeddings – quantitative analysis
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估嵌入 - 定量分析
- en: A few words might be enough to indicate that the quantitative analysis of embeddings
    is also possible.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 仅几个单词可能足以表明嵌入的定量分析也是可能的。
- en: 'Some word similarity benchmarks propose human-based distances between concepts:
    Simlex999 (Hill et al., 2016), Verb-143 (Baker et al., 2014), MEN (Bruni et al.,
    2014), RareWord (Luong et al., 2013), and MTurk- 771 (Halawi et al., 2012).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一些单词相似度基准提出了概念之间基于人类的距离：Simlex999（Hill等，2016）、Verb-143（Baker等，2014）、MEN（Bruni等，2014）、RareWord（Luong等，2013）和MTurk-771（Halawi等，2012）。
- en: Our similarity distance between embeddings can be compared to these human distances,
    using Spearman's rank correlation coefficient to quantitatively evaluate the quality
    of the learned embeddings.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的嵌入之间的相似度距离可以与这些人类距离进行比较，使用Spearman秩相关系数来定量评估所学习嵌入的质量。
- en: Application of word embeddings
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词嵌入的应用
- en: Word embeddings capture the meaning of the words. They translate a discrete
    input into an input that can be processed by neural nets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入捕捉单词的含义。它们将离散输入转换为神经网络可处理的输入。
- en: 'Embeddings are the start of many applications linked to language:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是与语言相关的许多应用的起点：
- en: Generating texts, as we'll see in the next chapter
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成文本，正如我们将在下一章看到的那样
- en: Translation systems, where input and target sentences are sequences of words
    and whose embeddings can be processed by end-to-end neural nets ([Chapter 8](part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 8. Translating and Explaining with Encoding – decoding Networks"), *Translating
    and Explaining with Encoding – decoding Networks*)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译系统，其中输入和目标句子是单词序列，且其嵌入可以通过端到端的神经网络处理 ([第8章](part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b
    "第8章. 使用编码解码网络进行翻译和解释"), *使用编码解码网络进行翻译和解释*)
- en: Sentiment analysis ([Chapter 5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 5. Analyzing Sentiment with a Bidirectional LSTM"), *Analyzing Sentiment
    with a Bidirectional LSTM*)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析 ([第5章](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "第5章. 使用双向LSTM分析情感"), *使用双向LSTM分析情感*)
- en: Zero-shot learning in computer vision; the structure in the word language enables
    us to find classes for which no training images exist
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉中的零-shot学习；语言中的结构使我们能够找到没有训练图像的类别
- en: Image annotation/captioning
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像标注/说明
- en: Neuro-psychiatry, for which neural nets can predict with 100% accuracy some
    psychiatric disorders in human beings
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经精神病学，其中神经网络可以100%准确预测某些人类精神障碍
- en: Chatbots, or answering questions from a user ([Chapter 9](part0091_split_000.html#2MP362-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 9. Selecting Relevant Inputs or Memories with the Mechanism of Attention"),
    *Selecting Relevant Inputs or Memories with the Mechanism of Attention*)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人，或回答用户问题 ([第9章](part0091_split_000.html#2MP362-ccdadb29edc54339afcb9bdf9350ba6b
    "第9章. 使用注意力机制选择相关输入或记忆"), *使用注意力机制选择相关输入或记忆*)
- en: As with words, the principle of semantic embedding can be used on any problem
    with categorical variables (classes of images, sounds, films, and so on), where
    the learned embedding for the activation of categorical variables can be used
    as input to neural nets for further classification challenges.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与单词一样，语义嵌入的原理可以用于任何具有类别变量（图像、声音、电影等类别）的任务，其中通过类别变量激活学习到的嵌入可以作为输入传递到神经网络，用于进一步的分类挑战。
- en: As language structures our mind, word embeddings help structure or improve the
    performance of neural net based systems.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言塑造了我们的思维，词嵌入有助于构建或提高基于神经网络的系统性能。
- en: Weight tying
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重绑定
- en: Two weight matrices, ![Weight tying](img/00050.jpeg) and ![Weight tying](img/00051.jpeg)
    have been used for input or output respectively. While all weights of ![Weight
    tying](img/00051.jpeg) are updated at every iteration during back propagation,
    ![Weight tying](img/00050.jpeg)is only updated on the column corresponding to
    the current training input word.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了两个权重矩阵，![权重绑定](img/00050.jpeg)和![权重绑定](img/00051.jpeg)，分别用于输入或输出。虽然![权重绑定](img/00051.jpeg)的所有权重在每次反向传播迭代时都会更新，但![权重绑定](img/00050.jpeg)仅在与当前训练输入词对应的列上更新。
- en: '**Weight tying** (**WT**) consists of using only one matrix, W, for input and
    output embedding. Theano then computes the new derivatives with respect to these
    new weights and all weights in W are updated at every iteration. Fewer parameters
    leads to less overfitting.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重绑定** (**WT**) 由仅使用一个矩阵W来进行输入和输出嵌入组成。Theano 然后计算相对于这些新权重的新导数，并且W中的所有权重在每次迭代时都会更新。参数减少有助于减少过拟合。'
- en: 'In the case of Word2Vec, such a technique does not give better results for
    a simple reason: in the Word2Vec model, the probability of finding the input word
    in the context is given as:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Word2Vec模型，这种技术并没有给出更好的结果，原因很简单：在Word2Vec模型中，找到输入单词在上下文中出现的概率是：
- en: '![Weight tying](img/00064.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![权重绑定](img/00064.jpeg)'
- en: It should be as close to zero but cannot be zero except if W = 0.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该尽量接近零，但不能为零，除非W = 0。
- en: 'But in other applications, such as in **Neural Network Language Models** (**NNLM**)
    in [Chapter 4](part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 4. Generating Text with a Recurrent Neural Net"), *Generating Text with
    a Recurrent Neural Net* and **Neural Machine Translation** (**NMT**) in [Chapter
    8](part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 8. Translating
    and Explaining with Encoding – decoding Networks"), *Translating and Explaining
    with Encoding-decoding Networks*), it can be shown [*Using the output embedding
    to improve the language models*] that:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但在其他应用中，例如在**神经网络语言模型**（**NNLM**）中的[第4章](part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b
    "第4章。用递归神经网络生成文本")，“用递归神经网络生成文本”，以及**神经机器翻译**（**NMT**）中的[第8章](part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b
    "第8章。用编码解码网络进行翻译和解释")，“用编码解码网络进行翻译和解释”，它可以显示[*使用输出嵌入来改进语言模型*]：
- en: Input embeddings are usually worse than output embeddings
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入嵌入通常比输出嵌入差
- en: WT solves this problem
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WT解决了这个问题
- en: The common embedding learned with WT is close in quality to the output embedding
    without WT
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过WT学习的常见嵌入在质量上接近于没有WT的输出嵌入
- en: Inserting a regularized projection matrix P before the output embedding helps
    the networks use the same embedding and leads to even better results under WT
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输出嵌入之前插入一个正则化的投影矩阵P，帮助网络使用相同的嵌入，并且在WT下导致更好的结果
- en: Further reading
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following articles:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下文章：
- en: Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov,
    Kai Chen, Greg Corrado, Jeffrey Dean, Jan 2013
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向量空间中高效估计单词表示，Tomas Mikolov，Kai Chen，Greg Corrado，Jeffrey Dean，2013
- en: Factor-based Compositional Embedding Models, Mo Yu, 2014
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于因子的组合嵌入模型，Mo Yu，2014
- en: Character-level Convolutional Networks for Text Classification, Xiang Zhang,
    Junbo Zhao, Yann LeCun, 2015
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符级卷积网络用于文本分类，Xiang Zhang，Junbo Zhao，Yann LeCun，2015
- en: Distributed Representations of Words and Phrases and their Compositionality,
    Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, 2013
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词和短语的分布式表示及其组合性，Tomas Mikolov，Ilya Sutskever，Kai Chen，Greg Corrado，Jeffrey
    Dean，2013
- en: Using the Output Embedding to Improve Language Models, Ofir Press, Lior Wolf,
    Aug 2016
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用输出嵌入来改进语言模型，Ofir Press，Lior Wolf，2016年8月
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented a very common way to transform discrete inputs in particular
    texts into numerical embeddings, in the case of natural language processing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一种非常常见的方法，特别是将离散的文本输入转换为数值嵌入，用于自然语言处理。
- en: The technique to train these word representations with neural networks does
    not require us to label the data and infers its embedding directly from natural
    texts. Such training is named *unsupervised learning*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络训练这些单词表示的技术不需要我们对数据进行标记，并直接从自然文本推断其嵌入。这样的训练称为*无监督学习*。
- en: One of the main challenges with deep learning is to convert input and output
    signals into representations that can be processed by nets, in particular vectors
    of floats. Then, neural nets give all the tools to process these vectors, to learn,
    decide, classify, reason, or generate.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的主要挑战之一是将输入和输出信号转换为可以由网络处理的表示，特别是浮点向量。然后，神经网络提供所有工具来处理这些向量，学习、决策、分类、推理或生成。
- en: In the next chapters, we'll use these embeddings to work with texts and more
    advanced neural networks. The first application presented in the next chapter
    is about automatic text generation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用这些嵌入来处理文本和更高级的神经网络。下一章中介绍的第一个应用是自动文本生成。
