- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Fairness Notions and Fair Data Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性概念与公平数据生成
- en: In this chapter, we will first set an outline of how fairness has become important
    in the world of predictive modeling by providing examples of different challenges
    faced in society. We will then go deep into the taxonomies and types of fairness
    to present a detailed description of the terms involved. Here, we will understand
    the importance of the defined metrics by citing and substantiating open source
    tools that help evaluate the metrics. Then, we will further emphasize the importance
    of the quality of data as biased datasets can introduce hidden bias in ML models.
    In this context, this chapter discusses different synthetic data generation techniques
    that are available and how they can be effective in removing bias from ML models.
    In addition, the chapter also emphasizes some of the best practices that can not
    only generate synthetic private data but can also scale and fit different types
    of problems well.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先通过提供社会中面临的不同挑战的示例，概述公平性在预测建模领域的重要性。然后，我们将深入研究公平性的分类法和类型，以详细描述相关术语。在此过程中，我们将通过引用并证明开源工具来帮助评估这些定义的指标，从而理解这些指标的重要性。接下来，我们将进一步强调数据质量的重要性，因为有偏的数据集可能会在机器学习模型中引入隐性偏见。在这个背景下，本章讨论了不同的合成数据生成技术，以及它们如何有效地消除机器学习模型中的偏见。此外，本章还强调了一些最佳实践，这些实践不仅能生成合成的私有数据，而且可以扩展并适应不同类型的问题。
- en: 'In this chapter, these topics will be covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以下部分将覆盖这些主题：
- en: Understanding the impact of data on fairness
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据对公平性的影响
- en: Fairness definitions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平性定义
- en: The role of data audits and quality checks in fairness
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据审计和质量检查在公平性中的作用
- en: Fair synthetic datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平的合成数据集
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8 along with some necessary Python
    packages:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要你安装 Python 3.8 以及一些必要的 Python 包：
- en: '`git clone https://github.com/yoshavit/fairml-farm.git` (works with TensorFlow-1.14.0
    or TensorFlow-1.15.0'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/yoshavit/fairml-farm.git`（适用于 TensorFlow-1.14.0
    或 TensorFlow-1.15.0）'
- en: '`python` `setup.py install`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python` `setup.py install`'
- en: '`%tensorboard --``logdir logs/gradient_tape`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%tensorboard --``logdir logs/gradient_tape`'
- en: '`pip install fat-forensics[all]` ([https://github.com/fat-forensics/fat-forensics](https://github.com/fat-forensics/fat-forensics))'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip install fat-forensics[all]` ([https://github.com/fat-forensics/fat-forensics](https://github.com/fat-forensics/fat-forensics))'
- en: '`pip` `install fairlens`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` `install fairlens`'
- en: '`git` `clone` [https://github.com/amazon-research/minimax-fair.git](https://github.com/amazon-research/minimax-fair.git)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`git` `clone` [https://github.com/amazon-research/minimax-fair.git](https://github.com/amazon-research/minimax-fair.git)'
- en: '`python3 main_driver.py`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python3 main_driver.py`'
- en: Understanding the impact of data on fairness
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据对公平性的影响
- en: 'In this first section, let''s understand what fairness is and how data plays
    a part in making a dataset fair. *By fairness, we mean the absence of any prejudice
    or favoritism toward an individual or group based on their inherent or acquired
    characteristics* (A Survey on Bias and Fairness in Machine Learning, Ninareh Mehrabi,
    Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan: [https://arxiv.org/pdf/1908.09635.pdf](https://arxiv.org/pdf/1908.09635.pdf)).
    The stated definition emphasizes the presence of certain biases, allowing preferential,
    unfair treatment toward one individual/sub-group of a population section due to
    certain attributes, such as gender, age, sex, race, or ethnicity. The goal of
    the following sections is to avoid creating unfair algorithms that are biased
    toward a section or group of people.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们先来了解什么是公平性，以及数据在使数据集公平中所扮演的角色。*公平性是指不存在任何基于个体或群体固有或获得特征的偏见或偏袒*（《机器学习中的偏见与公平性调查》，Ninareh
    Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, Aram Galstyan：[https://arxiv.org/pdf/1908.09635.pdf](https://arxiv.org/pdf/1908.09635.pdf)）。这个定义强调了某些偏见的存在，使得由于特定属性（如性别、年龄、性别、种族或民族等），对个体/群体的某一部分给予优待或不公平待遇。以下各节的目标是避免创建对某一部分人群有偏见的、不公平的算法。
- en: Real-world bias examples
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界中的偏见示例
- en: 'To study the impact of datasets, let''s see some real-world examples of where
    and how bias exists and the role of data in creating such biases. One of the most
    prominent examples where bias is visible is the **Correctional Offender Management
    Profiling for Alternative Sanctions** (**COMPAS**) software. This tool has been
    used in many jurisdictions around the US to predict whether a convicted criminal
    is likely to re-offend. The software revealed bias against African-Americans,
    demonstrating a higher false positive rate for African-American offenders than
    Caucasian offenders: the former group (African-Americans) exhibits a higher risk
    of *falsely* being identified as offenders or repeat criminals than the latter.
    One obvious reason is the absence of adequate data representations for minority
    groups. The evidence of racial discrimination has been summarized by ProPublica
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究数据集的影响，我们来看一些现实世界的例子，看看偏见存在的地方以及数据在产生这种偏见中的作用。一个最显著的偏见例子是**矫正犯人管理与替代处罚档案分析**（**COMPAS**）软件。该工具在美国多个司法辖区被用于预测定罪罪犯是否可能再犯。该软件揭示了对非裔美国人的偏见，显示非裔美国犯人的假阳性率高于白人犯人：前者（非裔美国人）比后者（白人）更可能*错误地*被认定为罪犯或重复犯罪者。一个明显的原因是缺乏对少数群体的充分数据表示。ProPublica总结的种族歧视证据如下：
- en: Black defendants were often predicted to be at a higher risk of recidivism than
    they actually were. Our analysis found that black defendants who did not recidivate
    over a two-year period were nearly twice as likely to be misclassified as higher
    risk compared to their white counterparts (45 percent vs. 23 percent).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 黑人被告常常被预测为比实际情况更有可能再次犯罪。我们的分析发现，在两年内没有再犯的黑人被告，比没有再犯的白人被告几乎有两倍的可能性被误判为高风险（45%
    对比 23%）。
- en: White defendants were often predicted to be less risky than they were. Our analysis
    found that white defendants who re-offended within the next two years were mistakenly
    labeled low risk almost twice as often as black re-offenders (48 percent vs. 28
    percent).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 白人被告常常被预测为比实际情况更不具风险。我们的分析发现，白人再犯者在接下来的两年内几乎两倍于黑人再犯者被错误地标记为低风险（48% 对比 28%）。
- en: The analysis also showed that even when controlling for prior crimes, future
    recidivism, age, and gender, black defendants were 45 percent more likely to be
    assigned higher risk scores than white defendants.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分析还表明，即使在控制了先前犯罪记录、未来再犯、年龄和性别的情况下，黑人被告被判定为高风险的可能性比白人被告高45%。
- en: Black defendants were also twice as likely as white defendants to be misclassified
    as being a higher risk of violent recidivism. And white violent recidivists were
    63 percent more likely to have been misclassified as a low risk of violent recidivism,
    compared with black violent recidivists.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 黑人被告在再犯暴力犯罪的风险上被误判为更高的可能性是白人被告的两倍。而白人暴力再犯者比黑人暴力再犯者更有63%的可能性被误判为低风险。
- en: The violent recidivism analysis also showed that even when controlling for prior
    crimes, future recidivism, age, and gender, black defendants were 77 percent more
    likely to be assigned higher risk scores than white defendants.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 暴力再犯分析还表明，即使在控制了先前犯罪记录、未来再犯、年龄和性别的情况下，黑人被告被判定为高风险的可能性比白人被告高77%。
- en: (Quoted from *How We Analyzed the COMPAS Recidivism Algorithm* by Jeff Larson,
    Surya Mattu, Lauren Kirchner, and Julia Angwin)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: （引自*我们如何分析COMPAS再犯算法*，Jeff Larson、Surya Mattu、Lauren Kirchner 和 Julia Angwin著）
- en: The reports published by ProPublica give us a clear idea of the impact of racial
    discrimination. As the tool is a clear demonstration of injustice against minorities,
    the dataset present in COMPAS ([https://www.kaggle.com/code/danofer/compass-fairml-getting-started/data](https://www.kaggle.com/code/danofer/compass-fairml-getting-started/data))
    is more often used for evaluating fairness in ML algorithms, to check the occurrence
    of bias, if any.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ProPublica发布的报告清晰地展示了种族歧视的影响。由于该工具明确展示了对少数群体的不公，COMPAS数据集（[https://www.kaggle.com/code/danofer/compass-fairml-getting-started/data](https://www.kaggle.com/code/danofer/compass-fairml-getting-started/data)）通常用于评估机器学习算法中的公平性，检查是否存在偏见。
- en: This kind of bias is more evident in chatbots, employment matching, flight routing,
    automated legal aid for immigration algorithms, and search and advertising placement
    algorithms. We can also see such bias is present in AI and robotic systems, face
    recognition applications, voice recognition, and search engines.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差在聊天机器人、就业匹配、航班路线规划、自动移民法律援助算法以及搜索和广告投放算法中更为显著。我们还可以看到，这种偏差在人工智能和机器人系统、面部识别应用、语音识别和搜索引擎中也普遍存在。
- en: Bias has a detrimental impact on society when the outcomes of biased ML models
    extend or withhold opportunities, resources, or information (such as hiring, school
    admissions, and lending). Such issues are most visible in face recognition, document
    search, and product recommendation, where system accuracy is impacted. End user
    experiences are impacted when biased algorithmic outcomes generate a feedback
    loop (due to repeated user interactions with the top items on the list) between
    data, algorithms, and users, thereby increasing the number of sources yielding
    further bias.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当有偏的机器学习模型的结果影响或剥夺机会、资源或信息（如招聘、学校录取和贷款）时，偏差对社会产生不利影响。这些问题在面部识别、文档搜索和产品推荐中尤为明显，系统的准确性会受到影响。当有偏的算法结果生成数据、算法和用户之间的反馈循环（由于用户与列表顶部项目的重复互动）时，最终用户的体验会受到影响，从而增加进一步偏差的来源。
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: What is most important for us to know is that when algorithms are trained on
    biased data, the algorithm itself learns the bias during the training process
    and reflects that in its predictions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最需要了解的是，当算法在有偏数据上进行训练时，算法本身会在训练过程中学到这种偏差，并在其预测中反映出来。
- en: 'Now let''s look at *Table 7.1* and understand the different sources of bias.
    Bias resulting from data may come in different forms: **data-to-algorithm** (**DA**)
    bias, **algorithm-to-user** (**AU**) bias, and **user-to-data** (**UD**) bias.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看*表7.1*并理解不同的偏差来源。由数据产生的偏差可能有不同的形式：**数据到算法**（**DA**）偏差、**算法到用户**（**AU**）偏差和**用户到数据**（**UD**）偏差。
- en: '| **Bias name** | **Source of bias** | **Type** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **偏差名称** | **偏差来源** | **类型** |'
- en: '| Measurement bias | Data selection, utilization, transformation, and measurement
    of features. | DA |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 测量偏差 | 数据选择、利用、转换和特征测量的偏差。 | DA |'
- en: '| Omitted variable bias | Important variables excluded from the model. | DA
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 遗漏变量偏差 | 模型中排除的重要变量。 | DA |'
- en: '| Representation bias | Sampling bias from a population during data collection.
    | DA |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 表示偏差 | 在数据收集过程中，由于从人群中采样而产生的偏差。 | DA |'
- en: '| Aggregation bias | False inferences drawn about individuals from the entire
    population. Examples include Simpson’s paradox – an association in aggregated
    source data disappears or changes when data gets disassembled into subgroups.
    | DA |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 聚合偏差 | 从整个群体中得出关于个体的错误推论。例如，辛普森悖论——聚合数据源中的关联在数据被拆解成子群体后消失或发生变化。 | DA |'
- en: '| Sampling bias | Sampling bias resembles representation bias, arising due
    to the non-random sampling of subgroups. | DA |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 采样偏差 | 采样偏差类似于表示偏差，源于对子群体进行非随机采样。 | DA |'
- en: '| Longitudinal data fallacy | Results from the aggregation of diverse cohorts
    at a single point. Prominent due to temporal cross-sectional data analysis and
    modeling. | DA |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 纵向数据谬误 | 由于在单一时点聚合多种队列而产生。由于时间横断面数据分析和建模而突出。 | DA |'
- en: '| Linking bias | Misinterpretation of true user behavior due to user connections,
    activities, or interactions. | DA |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 连接偏差 | 由于用户连接、活动或互动而误解真实用户行为。 | DA |'
- en: '| Algorithmic bias | The input data has no bias but it is added by the algorithm.
    | AU |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 算法偏差 | 输入数据本身没有偏差，但算法会引入偏差。 | AU |'
- en: '| User interaction bias | Triggered from two sources: the user interface and
    when the user imposes their self-selected biased behavior and interaction. | AU
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 用户交互偏差 | 由两个来源触发：用户界面和用户施加的自选偏向行为与交互。 | AU |'
- en: '| Popularity bias | More popular items are exposed more, which often get manipulated
    by fake reviews or social bots. | AU |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 流行偏差 | 更受欢迎的项目被暴露得更多，这些项目常常被虚假评论或社交机器人操控。 | AU |'
- en: '| Emergent bias | Results from interaction with real users due to changes in
    population, cultural values, or societal knowledge. | AU |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 新兴偏差 | 由于人口、文化价值观或社会知识的变化与真实用户的互动而产生。 | AU |'
- en: '| Evaluation bias | Occurs during model evaluation, due to inappropriate evaluation
    techniques. | AU |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 评估偏差 | 在模型评估过程中发生，源于不适当的评估技术。 | AU |'
- en: '| Historic bias | Socio-technical issues in the world can largely impact the
    data generation process, even after the application of perfect sampling and feature
    selection techniques. | UD |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 历史偏差 | 世界上的社会技术问题在很大程度上会影响数据生成过程，即使在应用完美的抽样和特征选择技术之后。 | UD |'
- en: '| Population bias | When statistics, demographics, representatives, and user
    characteristics of the user population of the platform differ from the original
    target population. | UD |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 人口偏差 | 当平台用户人群的统计数据、人口特征、代表性和用户特征与原始目标人群不同。 | UD |'
- en: '| Self-selection bias | A subtype of selection bias where subjects of research
    select themselves. | UD |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 自我选择偏差 | 一种选择偏差的子类型，研究对象由自己选择。 | UD |'
- en: '| Social bias | Social bias happens when others’ actions affect our judgment.
    | UD |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 社会偏差 | 社会偏差发生在他人的行为影响我们的判断时。 | UD |'
- en: '| Behavioral bias | Behavioral bias arises from different user behavior across
    platforms, contexts, or different datasets. | UD |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 行为偏差 | 行为偏差源于不同平台、上下文或不同数据集中的用户行为差异。 | UD |'
- en: '| Temporal bias | Results from differences in populations and behaviors over
    time. | UD |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 时间偏差 | 由不同时间段内的人群和行为差异引起。 | UD |'
- en: '| Content production bias | Results from structural, lexical, semantic, and
    syntactic differences in the content generated by users. | UD |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 内容生产偏差 | 由用户生成内容中的结构、词汇、语义和句法差异引起。 | UD |'
- en: Table 7.1 – Different types of bias and its sources
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1 – 不同类型的偏差及其来源
- en: Now, with our understanding of different types of bias, let's see in *Figure
    7**.1* how they are dependent on each other and circulate in a loop. For example,
    the involvement of user interaction, resulting in behavioral bias, sees its bias
    amplified when fed with data. The input data adds to aggregation or longitudinal
    bias. This in turn is processed by algorithms that add bias, termed ranking or
    emergent bias.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们对不同类型偏差的理解，让我们看看在*图 7.1*中它们是如何相互依赖并形成循环的。例如，用户交互的参与导致行为偏差，而当数据被输入时，这种偏差会被放大。输入数据会加剧聚合偏差或纵向偏差。接着，这些数据会被算法处理，形成偏差，这种偏差被称为排名偏差或突现偏差。
- en: '![Figure 7.1 – Flow of different types of bias based on sources](img/B18681_07_001.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 基于来源的不同类型偏差流动](img/B18681_07_001.jpg)'
- en: Figure 7.1 – Flow of different types of bias based on sources
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 基于来源的不同类型偏差流动
- en: Causes of bias
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差的原因
- en: 'In the following list, we will look at exactly what causes bias:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们将详细了解偏差的具体原因：
- en: '**Skewed dataset**: The dataset may display skewness toward the least-occurring
    class. This kind of bias tends to increase at a compound rate with time. Crime
    datasets exhibit this kind of skewness, as we see a very limited number of criminals
    versus innocent people in any region. Once skewness is observed, detectives and
    police departments also tend to be biased and dispatch more police professionals
    to high-crime areas. This may tend to reveal high crime rates, having more police
    professionals deployed compared to other regions.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集偏斜**：数据集可能会显示出向出现频率较低的类别偏斜。这种偏差通常随着时间的推移以复合速率增加。犯罪数据集表现出这种偏斜性，我们在任何地区都看到有限数量的罪犯与无辜人群的对比。一旦偏斜被观察到，侦探和警察部门也往往会有所偏见，向高犯罪率地区派遣更多的警力。这可能导致高犯罪率的揭示，与其他地区相比，更多的警力部署到这些地区。'
- en: '**Insufficient training data**: We observe that with only limited data for
    certain demographic groups or other groups, an ML model tends to produce biased
    outcomes. Such models fail to notice extraordinary characteristics that are available
    with a very limited population. We saw such examples in [*Chapter 3*](B18681_03.xhtml#_idTextAnchor066),
    with facial recognition technology having greater accuracy with images of white
    males compared to images of black females.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据不足**：我们发现，当某些特定人群或其他群体的数据有限时，机器学习模型往往会产生偏差结果。此类模型未能察觉在极少数人群中存在的特殊特征。我们在[*第3章*](B18681_03.xhtml#_idTextAnchor066)中看到过这样的例子，面部识别技术对白人男性的图像比对黑人女性的图像更为准确。'
- en: '**Human bias**: Datasets often get corrupted due to human bias. One such example
    can be seen when we collect real-world data on US employment. Women hardly account
    for the CEOs in the data on the top 500 companies. This is due to the fact that
    there are fewer female CEOs and we have failed to collect data where women are
    CEOs. Models trained on such data would naturally predict that being female correlates
    poorly with being a CEO.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类偏见**：数据集经常因为人类偏见而受到污染。一个例子可以从我们收集的美国就业数据中看出。在关于美国500强公司的数据中，女性几乎没有担任CEO。这是因为女性CEO的数量较少，而且我们未能收集到女性担任CEO的相关数据。基于这些数据训练的模型自然会预测，女性与担任CEO的相关性较差。'
- en: '**Data de-biasing**: To remove bias from historical data, we have often seen
    approaches such as removing sensitive attributes, but that does not completely
    eliminate bias. Experimental studies reveal correlated attributes are often used
    as proxies, even after the removal of sensitive attributes that pave the way for
    the systematic discrimination of minorities. One example of this kind is when
    a particular locality is predominantly resided in by black people and the removal
    of the race column does not remove the bias due to the presence of the ZIP code
    of that location. Possible recommendations to avoid this are to keep the sensitive
    columns and directly monitor and remediate violations caused by the presence of
    proxy features during model training.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据去偏见**：为了去除历史数据中的偏见，我们常见的方法是删除敏感属性，但这并不能完全消除偏见。实验研究表明，相关的属性常常作为代理，即使在删除了敏感属性后，仍然为系统性歧视少数群体提供了条件。一个例子是，当某个地区主要由黑人居住，删除种族列并不能去除偏见，因为该地区的邮政编码仍然存在。为了避免这种情况，建议保留敏感列，并在模型训练过程中直接监控并修正由于代理特征存在而产生的违规行为。'
- en: '**Side-effects from data debiasing**: Techniques often applied to remove data
    skewness and model bias sometimes result in undesired side-effects in the model
    downstream. This has been observed when a speech recognition algorithm was fine-tuned
    for males and females, with the results significantly worse for female speakers
    compared to male ones. Researchers further observed that testing models for bias
    with holdout samples did not solve this, as the test dataset was also biased.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据去偏见的副作用**：为了去除数据偏斜和模型偏见的技术有时会在下游模型中产生不良的副作用。曾观察到，当一个语音识别算法在男性和女性之间进行微调时，女性的结果明显差于男性。研究人员进一步发现，使用留存样本对模型进行偏见测试并没有解决问题，因为测试数据集本身也存在偏见。'
- en: '**Availability of limited features**: When features are less informative or
    reliable for minority groups than the counterparts of a majority group, models
    demonstrate much lower accuracy for the minority section compared to the majority
    section of the population.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限特征的可用性**：当某些特征对少数群体比对多数群体的对照组信息更少或可靠性较差时，模型在少数群体的准确性通常远低于多数群体。'
- en: '**Diversity among data and AI professionals**: It has been found that the absence
    of diversity has been one of the main causes of bias. Diversity in teams can help
    to mitigate bias. One study put forward by Joy Buolamwini, founder of the Algorithmic
    Justice League and past member of MIT Media Lab, demonstrates that certain discoveries
    were made by her team when a Ghanaian-American computer scientist joined her research
    group. The team together found out that facial recognition tools exhibited bias
    and showcased poor performance on her darker skin tone – and only worked if she
    wore a white mask.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据和AI专业人士的多样性**：研究发现，多样性的缺失是偏见的主要原因之一。团队中的多样性可以帮助减少偏见。由算法公正联盟创始人、MIT媒体实验室前成员Joy
    Buolamwini提出的一项研究表明，当一位加纳裔美国计算机科学家加入她的研究团队时，团队发现了某些问题。她的团队发现，面部识别工具存在偏见，且在她的较深肤色上表现较差——只有她戴上白色面具时才会起作用。'
- en: '**Costs incurred in driving fairness algorithms**: Organizations that have
    not invested in fairness have more biased ML models. Organizations need to invest
    in human resource experts and in educating people to opt for fair ML model designs.
    This may come at the cost of achieving the right trade-off between model accuracy
    and fairness metrics, which can impact profit margins, revenue, or the number
    of customers. Hence, it is necessary to balance the two objectives before enforcing
    regulations.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驾驶公平算法的成本**：没有投资于公平性的组织，其机器学习模型通常更具偏见。组织需要投入人力资源专家，并教育人员选择公平的机器学习模型设计。这可能会以牺牲模型准确性和公平性指标之间的合理权衡为代价，从而影响利润率、收入或客户数量。因此，在执行相关规定之前，必须平衡这两个目标。'
- en: '**External audits**: Bias can also arise from the absence of proper external
    audits. External audits, when put in place, can detect biased datasets or algorithmic
    bias that’s in place. However, organizing such audits may violate GDPR, CCPA,
    and other privacy regulations that strictly enforce the privacy of customers’
    sensitive data. A workaround is to leverage the use of synthetic data tools that
    allow you to generate fully anonymous, completely realistic, and representative
    datasets. An organization can rely on synthetic datasets to train ML models without
    violating privacy laws, as sharing the data does not disrespect individual privacy.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部审计**：偏见也可能源于缺乏适当的外部审计。外部审计到位时，可以检测到偏见数据集或存在的算法偏见。然而，组织这些审计可能会违反GDPR、CCPA及其他严格执行客户敏感数据隐私的法规。一种解决办法是利用合成数据工具，这些工具可以生成完全匿名、完全现实且具有代表性的数据集。一个组织可以依赖合成数据集来训练机器学习模型，而不违反隐私法，因为共享数据不会侵犯个人隐私。'
- en: '**Fair models become biased**: It often happens in the absence of the constant
    monitoring of incoming data and model metrics that fair models become biased.
    One such example is the AI chatbot **Tay**, developed by Microsoft. Microsoft
    had to withdraw Tay from the web as it become misogynistic and racist while learning
    from the conversations of Twitter users. Continuous monitoring and evaluation
    of model metrics can help to prevent bias from arising over time.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平模型变得有偏**：如果没有持续监控输入数据和模型指标，公平模型往往会变得有偏。一个这样的例子是微软开发的AI聊天机器人**Tay**。微软不得不将Tay从网络上撤下，因为它在学习Twitter用户的对话时变得性别歧视和种族主义。持续监控和评估模型指标可以帮助防止偏见随着时间的推移而产生。'
- en: '**Biased AI and its vicious cycle**: If we fail to measure and assess bias
    from AI algorithms, it will percolate deep within our society and will potentially
    be more harmful. One such example was seen with Google’s search algorithm, which
    displayed racist images on applying searches for terms such as **black hands**.
    The searches, instead of perpetuating bias and displaying derogatory depictions,
    could have showcased more neutral images if the initial search results and the
    clicks on the search results were not pointed at biased images.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有偏的AI及其恶性循环**：如果我们没有衡量和评估AI算法中的偏见，它将深入社会，并可能变得更加有害。一个这样的例子是谷歌的搜索算法，在搜索**黑色手**等术语时，显示了种族主义的图像。通过搜索，结果本可以显示出更中立的图像，而不是
    perpetuating 偏见并显示贬低的描述，如果初始搜索结果和点击结果没有指向那些有偏的图像。'
- en: Now, we understand how we can introduce bias while training ML models. We should
    also be aware of discrimination processes that also yield biased models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们理解了在训练机器学习模型时如何引入偏见。我们还应该意识到歧视过程也会产生有偏的模型。
- en: '**Discrimination** is another source of unfairness originating due to human
    prejudice and stereotyping based on sensitive attributes present in the dataset.
    Discrimination primarily originates from systems or statistics. **Systemic discrimination**
    refers to existing policies, customs, or behaviors within an organization that
    enhance discrimination against certain subgroups of the population. **Statistical
    discrimination**, however, occurs when decision-makers use average group statistics
    to judge an individual belonging to that group.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**歧视**是由于人类偏见和基于数据集中敏感属性的刻板印象而引发的不公平的另一种来源。歧视主要来源于系统或统计数据。**系统性歧视**指的是组织内现有的政策、习惯或行为，这些政策、习惯或行为加剧了对某些群体的歧视。**统计歧视**则发生在决策者使用群体的平均统计数据来判断该群体的个体时。'
- en: Hence, we can say in generating biased ML models that the dataset plays an important
    role, as it provides the statistics, features, and patterns of data that are learned
    during the model training phase.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说，在生成有偏的机器学习模型时，数据集扮演着重要角色，因为它提供了在模型训练阶段所学到的统计数据、特征和数据模式。
- en: In this section, we studied different types of bias that exist and creep into
    systems. To avoid bias, we need to incorporate fair treatment for everyone, and
    in order to do that we need to define what it means to offer fair treatment. Let's
    look at that in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们研究了存在于系统中的不同类型的偏见。为了避免偏见，我们需要确保每个人都能获得公平的对待，为此我们需要定义什么是公平的对待。我们将在下一部分中讨论这一点。
- en: Defining fairness
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义公平性
- en: In this section, let's try to understand the different types of fairness that
    researchers have described to avoid **discrimination** or the unfavorable treatment
    of people. ML algorithms and practitioners are increasingly coming under scrutiny
    on this subject to mitigate the risk of unfair treatment in areas such as credit,
    employment, education, and criminal justice. The goal is to design ML algorithms
    and pipelines that are not impacted by protected attributes (such as gender, race,
    and ethnicity) but are still able to offer fair predictions. We’ll look at some
    different fairness definitions with examples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，让我们尝试理解研究人员描述的不同类型的公平性，以避免**歧视**或对人们的不公正对待。机器学习算法和从业人员在这个问题上正受到越来越多的关注，以减少在信用、就业、教育和刑事司法等领域的不公平待遇风险。目标是设计不受受保护属性（如性别、种族和民族）影响的机器学习算法和流程，同时仍能提供公平的预测。我们将通过一些不同的公平性定义和示例来探讨这一问题。
- en: Numerical and binary attributes are most often used to state and test fairness
    criteria. Categorical features can be converted to a set of binary features. Most
    often, we use the terms protected and unprotected groups, advantaged and disadvantaged
    groups, and majority and minority groups interchangeably to differentiate between
    the demographic sections of the population and evaluate fairness for different
    sections of the population. In the definitions discussed in this section, we will
    primarily use married/divorced female entrants and married/divorced male entrants
    to demonstrate how preferential treatment or mistreatment can be avoided, to ensure
    fair and equitable model predictions. Here, *entrants* mean initial-stage job
    applicants.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数值型和二进制属性最常用于陈述和测试公平性标准。分类特征可以转换为一组二进制特征。通常，我们交替使用“受保护群体”和“未受保护群体”、“优势群体”和“劣势群体”、“多数群体”和“少数群体”这几个术语，以区分人口的不同群体，并评估不同群体的公平性。在本部分讨论的定义中，我们将主要使用已婚/离婚女性求职者和已婚/离婚男性求职者来演示如何避免偏袒或不公正对待，以确保模型预测的公平和公正。在这里，*求职者*指的是初始阶段的求职申请者。
- en: Types of fairness based on statistical metrics
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于统计指标的公平性类型
- en: 'The statistical measures of fairness are dependent on metrics, which can be
    best explained by means of a confusion matrix – a table that is generated by running
    predicted outcomes against ground truth data with an actual representation of
    the different accuracy metrics of a classification model. The rows and columns
    represent the predicted and the actual classes respectively. For a binary classifier,
    both predicted and actual classes have two values: positive and negative, as shown
    in the following figure. The following definitions further illustrate the metrics
    that are situated in the four different quadrants of the matrix shown in *Figure
    7**.2*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性的统计度量依赖于指标，这些指标可以通过混淆矩阵来最佳解释——混淆矩阵是一种通过将预测结果与实际数据进行比较生成的表格，实际表示分类模型的不同准确性指标。矩阵的行和列分别表示预测类和实际类。对于二分类器，预测类和实际类都包含两个值：正类和负类，如下图所示。以下定义进一步说明了位于*图
    7.2*中矩阵四个象限的指标：
- en: '![Figure 7.2 – Model classification metrics](img/B18681_07_002.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 模型分类指标](img/B18681_07_002.jpg)'
- en: Figure 7.2 – Model classification metrics
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 模型分类指标
- en: True positive (TP)
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真阳性（TP）
- en: The model’s predicted outcome and ground truth data are both in the positive
    class (true, in binary classification).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的预测结果和实际数据都属于正类（在二分类中为真）。
- en: False positive (FP)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性（FP）
- en: The model’s predicted outcome is true, while the ground truth data is false
    and belongs to the negative class.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的预测结果为真，而实际数据为假，属于负类。
- en: False negative (FN)
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假阴性（FN）
- en: The model’s predicted outcome is false and lies in the negative class, while
    the actual ground truth data is true, belonging to the positive class.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测的结果是错误的，并且属于负类，而实际的真实数据是正确的，属于正类。
- en: True negative (TN)
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真阴性（TN）
- en: The model’s predicted and ground truth data are both false and they lie in the
    negative class.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测和真实数据都是错误的，且它们都属于负类。
- en: Positive predictive value (PPV)/precision
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正预测值（PPV）/精确度
- en: This is the rate of positive cases accurately predicted to be true or lying
    in the positive class out of all predicted positive cases. It represents the probability
    of a subject with true as the predicted outcome truly belonging to the same class
    (having true as the ground truth data), *P*(*Y =* 1*|d =* 1), where an entrant
    with a good predicted qualifying score actually has a ground truth where the qualifying
    score is also good.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有预测为正类的样本中，准确预测为正类或属于正类的正样本的比率。它表示一个预测结果为正的对象，实际上也真正属于正类的概率，*P*(*Y =* 1*|d
    =* 1)，例如，预测为高资格分数的候选人实际上其资格分数也很高。
- en: False discovery rate (FDR)
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假发现率（FDR）
- en: This is the rate of negative cases that were inaccurately predicted to be true.
    The FDR represents the probability of false acceptance, *P*(*Y =* 0*|d =* 1),
    where we observe that a job candidate with a good predicted qualifying score has
    a ground truth where the person’s qualifying score is actually reported as low.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有预测为正类的负样本中，错误预测为正的比率。FDR表示假接受的概率，*P*(*Y =* 0*|d =* 1)，例如，预测为具有良好资格分数的求职者，实际上其真实资格分数却被报告为低。
- en: False omission rate (FOR)
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假遗漏率（FOR）
- en: This is the rate of positive cases that were wrongly classified and predicted
    to belong to the negative class. The FOR represents the probability of samples
    having a true value being inaccurately rejected, *P*(*Y =* 1*|d =* 0). We observe
    this most often when a job entrant has been evaluated to have a low predicted
    qualifying score, but the same candidate in reality has a good score.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将正类样本错误分类并预测为负类的比率。FOR表示真实值样本被不准确拒绝的概率，*P*(*Y =* 1*|d =* 0)。我们最常见的情况是，某个求职者被评估为具有低的预测资格分数，但该候选人实际上有一个较好的分数。
- en: Negative predictive value (NPV)
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负预测值（NPV）
- en: This is the rate of negative samples that are accurately predicted to be in
    the negative class out of all predicted negative cases. The NPV represents the
    probability of a subject (or an entrant) with a negative prediction truly belonging
    to the negative class, *P*(*Y =* 0*|d =* 0).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有预测为负类的样本中，准确预测为负类的负样本的比率。NPV表示一个被预测为负类的对象（或候选人）真正属于负类的概率，*P*(*Y =* 0*|d
    =* 0)。
- en: True positive rate (TPR)
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真阳性率（TPR）
- en: This is the rate of positive samples that are accurately predicted to be in
    the positive class out of all actual positive cases. The TPR is often referred
    to as sensitivity or recall; it represents the probability of a truly positive
    subject being identified in the class it belongs to, *P*(*d =* 1*|Y =* 1). In
    our example, it is the probability that a job entrant with ground truth data representing
    a good qualifying score is accurately predicted by the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是所有实际正类的样本中，准确预测为正类的正样本的比率。TPR通常被称为敏感度或召回率，它表示一个真正属于正类的对象被正确识别的概率，*P*(*d =*
    1*|Y =* 1)。在我们的例子中，它是一个具有良好资格分数的求职者的真实数据被模型准确预测的概率。
- en: False positive rate (FPR)
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假阳性率（FPR）
- en: This is the rate of negative samples inaccurately predicted by the model to
    be true, out of all actual negative cases. The FPR represents the probability
    of false alerts, *P*(*d =* 1*|Y =* 0). An example is when an entrant exhibiting
    a low qualifying score in reality has been misclassified by the model and inaccurately
    given a good qualifying score.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型将负样本错误预测为正的比率，所有实际负类的样本中。FPR表示假警报的概率，*P*(*d =* 1*|Y =* 0)。例如，当一个实际具有低资格分数的候选人被模型错误分类，并被不准确地给予一个高的资格分数时，就会发生这种情况。
- en: False negative rate (FNR)
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假阴性率（FNR）
- en: This is the rate of samples with true values that are mistakenly predicted to
    be false, out of all actual positive cases. The FNR represents the probability
    of a negative result given an actual outcome, *P*(*d =* 0*|Y =* 1). An example
    is when the probability of an entrant having a good qualifying score happens to
    be wrongly classified.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在所有实际为正的案例中，错误地预测为负的样本的比例。FNR代表给定实际结果下的负面结果概率，*P*(*d =* 0*|Y =* 1)。举个例子，当一个入选者的资格分数有可能被错误分类时。
- en: True negative rate (TNR)
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真负率（TNR）
- en: This is the rate of samples with a value of false that are accurately predicted
    to be in the negative class, out of all actual negative cases. The TNR represents
    the probability of an outcome being given a false value and actually belonging
    to the negative class, *P*(*d =* 0*|Y =* 0). We observe this when the probability
    of an entrant with a low qualifying score happens to be accurately classified.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在所有实际为负的案例中，准确预测为负类的样本的比例。TNR代表给定一个假值并且实际上属于负类的结果的概率，*P*(*d =* 0*|Y =* 0)。当一个资格分数较低的入选者被正确分类时，我们会观察到这一点。
- en: 'Let’s calculate these scores on the COMPAS dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在COMPAS数据集上计算这些得分：
- en: 'Let''s first import the necessary Python libraries:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的Python库：
- en: '[PRE0]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we load the data:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载数据：
- en: '[PRE1]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We translate the dataset into a binary classification problem to evaluate whether
    an individual has a high/medium/low risk of recidivism:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集转换为二元分类问题，以评估一个人是否有较高/中等/低风险的再犯：
- en: '[PRE2]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we plot the model’s performance after normalizing by each row – we want
    to see the PPV, FDR, FOR, and NPV:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们绘制在按每行标准化后的模型性能——我们想查看PPV、FDR、FOR和NPV：
- en: '[PRE3]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This yields the following output:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 7.3 – A confusion matrix using the COMPAS dataset](img/B18681_07_003.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 使用COMPAS数据集的混淆矩阵](img/B18681_07_003.jpg)'
- en: Figure 7.3 – A confusion matrix using the COMPAS dataset
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 使用COMPAS数据集的混淆矩阵
- en: 'We can also print the values using an `sklearn` confusion matrix:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以使用`sklearn`混淆矩阵打印这些值：
- en: '[PRE4]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we concentrate on African-American or Caucasian defendants, since they
    are the subject of the ProPublica claim:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们集中在非裔美国人或白人被告身上，因为他们是ProPublica声明的主题：
- en: '[PRE5]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This yields the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 7.4 – Fairness accuracy metrics for African-American and Caucasian](img/B18681_07_004.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 非裔美国人和白人的公平性准确度指标](img/B18681_07_004.jpg)'
- en: Figure 7.4 – Fairness accuracy metrics for African-American and Caucasian
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 非裔美国人和白人的公平性准确度指标
- en: Here, we see that the fairness metric is not the same across both groups.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到公平性度量在两个群体之间并不相同。
- en: Types of fairness based on the metrics of predicted outcomes
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于预测结果度量的公平性类型
- en: The types of fairness listed in this section primarily focus on a predicted
    outcome for various demographic segments of subjects involved in the problem under
    consideration.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本节列出的公平性类型主要关注在考虑的问题中，涉及的不同人口群体的预测结果。
- en: Group fairness/demographic parity
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 群体公平性/人口统计学平衡
- en: 'This is also commonly known as statistical parity or the equal acceptance rate.
    This concerns whether subjects in both protected (race, gender, ethnicity, and
    so on) and unprotected groups exhibit an equal probability of being represented
    in the predicted class classified as true. For example, this condition will be
    satisfied when there is an equal probability of male and female applicants achieving
    an equally good predicted qualifying score: *P* (*d =*1*|G = m*) *= P* (*d =*
    1*|G =* *f*).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这也常被称为统计平衡或等接受率。这关系到是否在保护组（如种族、性别、民族等）和非保护组之间，受试者在预测为正类的分类中表现出相等的概率。例如，当男女申请者有相等的机会达到同样优秀的预测资格分数时，该条件将得到满足：*P*
    (*d =* 1*|G = m*) *= P* (*d =* 1*|G =* *f*)。
- en: To illustrate this further, let's assume an ML model predicts that married/divorced
    male and female entrants have scores of 0.81 and 0.75, respectively. Here, the
    classifier can be said to have failed as it fails to satisfy the objective of
    equal scores for both males and females.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明这一点，假设一个机器学习模型预测已婚/离婚的男性和女性入选者的分数分别为0.81和0.75。在这种情况下，可以说分类器失败了，因为它未能满足男女两者之间分数相等的目标。
- en: Conditional statistical parity
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件统计平衡
- en: 'This definition goes beyond the previous definition by allowing a set of attributes
    that can influence the model’s predictions. To explain this further, this metric
    can be satisfied only when both protected and unprotected groups have an equal
    probability of being designated with the true class. This can be controlled by
    a set of allowable factors, *L* (including the entrant’s credit history, employment,
    and age). Hence, to explain mathematically that both female and male entrants
    demonstrate an equal probability of achieving a good qualifying score, we state
    it as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义超出了之前的定义，通过允许一组可以影响模型预测的属性。进一步解释，这个度量只有在受保护组和非受保护组具有相同概率被指定为真实类时才算满足。这可以通过一组允许的因素*L*（包括参赛者的信用历史、就业状况和年龄）来控制。因此，为了从数学上说明男女参赛者在获得良好资格得分的概率相等，我们可以表示为：
- en: '*P* (*d =* 1*|L = l, G = m*) *= P* (*d =* 1*|L = l, G =* *f*)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d =* 1*|L = l, G = m*) *= P* (*d =* 1*|L = l, G =* f*)'
- en: However, scores for married/divorced male and female entrants can be found to
    be very close; for example, they are 0.46 and 0.49, respectively. When we see
    such a minor difference existing between two groups, we can allow a threshold
    factor to permit this allowable difference.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，已婚/离婚男性和女性参赛者的得分可以非常接近；例如，它们分别是0.46和0.49。当我们看到两组之间存在如此微小的差异时，我们可以允许一个阈值因子来允许这种可接受的差异。
- en: Types of fairness based on the metrics of predicted and actual outcomes
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于预测和实际结果度量的公平性类型
- en: The fairness concepts listed here go beyond consideration of what the model
    predicts as its outcomes, *d*, for different demographic sections of subjects
    involved in the process of classification. They go on to compute evaluation metrics
    that are compared to the actual outcome, *Y* (as evident in the ground truth data),
    and recorded in the dataset.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出的公平性概念超越了模型对不同群体的预测结果*d*的考虑，它们还计算与实际结果*Y*（如实际数据中的真相所示）进行比较的评估度量，并记录在数据集中。
- en: Predictive parity
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测平等性
- en: 'This concept is very important, and it ensures that a classifier can satisfy
    that both protected and unprotected groups demonstrate a measure of equal PPV
    – ensuring that the probability of a subject exhibiting a true or positive predictive
    value is actually included in the positive class. As an example, we would need
    to have both male and female entrants demonstrate the same probability score.
    This can be formulated as:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念非常重要，它确保分类器能够保证受保护组和非受保护组在预测正类值（PPV）上表现出相同的度量——确保一个主体展示真实或正预测值的概率实际被包括在正类中。举个例子，我们需要确保男性和女性参赛者展示出相同的概率得分。这可以表述为：
- en: '*P (Y =* 1*|d =* 1*, G = m*) *= P* (*Y =* 1*|d =* 1*, G =* *f*)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*P (Y =* 1*|d =* 1*, G = m*) *= P* (*Y =* 1*|d =* 1*, G =* f*)'
- en: 'Furthermore, a classifier with an equal PPV will also have an equal FDR, which
    means:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个具有相等PPV的分类器也会有相等的FDR，这意味着：
- en: '*P (Y =* 0*|d =* 1*, G = m) = P (Y =* 0*|d =* 1*, G =* *f)*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*P (Y =* 0*|d =* 1*, G = m) = P (Y =* 0*|d =* 1*, G =* f)*'
- en: The measures may not be fully equal and a threshold margin between the groups
    is permitted. For example, a classifier may record the PPV for married/divorced
    male and female entrants as 0.73 and 0.74, respectively, and the FDR for male
    and female entrants as 0.27 and 0.26, respectively.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些度量可能不完全相等，并且允许组之间存在一定的阈值差异。例如，一个分类器可能记录已婚/离婚男性和女性参赛者的PPV分别为0.73和0.74，男性和女性参赛者的FDR分别为0.27和0.26。
- en: In real-world ML algorithms, most often, we find that any trained classifier
    understands an advantaged group better and predicts them in the positive prediction
    class. The disadvantaged or minority group’s predicted outcome sees a greater
    number of challenges in correct evaluation, mostly due to there being limited
    data for that group.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的机器学习算法中，我们通常发现任何训练好的分类器对有优势的群体更为了解，并将其预测为正类。而劣势或少数群体的预测结果则面临更多正确评估的挑战，主要是因为该群体的数据有限。
- en: False positive error rate balance/predictive equality
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假阳性错误率平衡/预测平等性
- en: 'This metric ensures that the classifier satisfies that both protected and unprotected
    groups demonstrate similar behavior by exhibiting measures that represent an equal
    FPR (a metric used for the misclassification rate) – where a subject with a false
    value and that is included in the negative class possesses a true positive predictive
    value. For example, this concept implies that both male and female entrants show
    the same probability measure, which causes entrants with a low qualifying score
    to be designated a good predicted qualifying score. Mathematically, it can be
    formulated as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标确保分类器满足受保护组和未受保护组在行为上表现出相似性，通过展示平等的假阳性率（FPR）来体现——这是一个用于衡量误分类率的指标——即包含假值且被归入负类的对象实际具有真正的正预测值。例如，这个概念意味着男性和女性的参赛者展示相同的概率度量，导致那些成绩较低的参赛者被预测为具有较高的预期成绩。从数学上讲，可以表示为：
- en: '*P* (*d =* 1*|Y =* 0*, G = m) = P (d =* 1*|Y =* 0*, G =* *f*)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d =* 1*|Y =* 0*, G = m) = P (d =* 1*|Y =* 0*, G =* *f*)'
- en: 'Here, a classifier with an equal FPR will also exhibit an equal TNR. Hence:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，具有相等假阳性率的分类器也将表现出相等的真阴性率（TNR）。因此：
- en: '*P* (*d =* 0*|Y =* 0*, G = m) = P (d =* 0*|Y =* 0*, G =* *f*)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d =* 0*|Y =* 0*, G = m) = P (d =* 0*|Y =* 0*, G =* *f*)'
- en: In terms of actual values, the FPR for married/divorced male and female entrants
    is 0.70 and 0.55, respectively, while the TNR is 0.30 and 0.45, respectively.
    Any tendency of the classifier to assign good qualifying scores to males who previously
    had low credit would cause the classifier to break the definitions stated and
    would result in its failure.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际值来看，已婚/离婚男性和女性参赛者的假阳性率（FPR）分别为0.70和0.55，而真阴性率（TNR）分别为0.30和0.45。如果分类器倾向于将良好的预测分数分配给曾经有低信用的男性，它将违反上述定义，并导致分类器的失败。
- en: False negative error rate balance/equal opportunity
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假阴性错误率平衡/平等机会
- en: 'This concept ensures that a classifier satisfies the condition that both protected
    and unprotected groups have an equal FNR – where a subject containing a true value
    and that is included in the positive class actually possesses a negative predictive
    value. This occurs on account of misclassification. Mathematically, it can be
    formulated as:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念确保分类器满足以下条件：无论是受保护组还是未受保护组，其假阴性率（FNR）相等——即包含真实值且被归入正类的对象实际上具有负预测值。这种情况由于误分类而发生。从数学角度来看，可以表示为：
- en: '*P* (*d =* 0*|Y =* 1*, G = m*) *= P* (*d =* 0*|Y =* 1*, G =* *f*)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d =* 0*|Y =* 1*, G = m*) *= P* (*d =* 0*|Y =* 1*, G =* *f*)'
- en: 'This implies the condition that in both the male and female groups, any entrant
    possessing a good predicted score has been misclassified and predicted as possessing
    a low score. A classifier with an equal FNR will also have an equal TPR. Now in
    terms of equation, the following condition holds, where we see TPR should be same
    for both males and females:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在男性和女性组中，任何具有良好预测分数的参赛者都被误分类，并且预测为具有较低的分数。具有相等假阴性率的分类器也将具有相等的真正阳性率（TPR）。从方程的角度来看，以下条件成立，其中TPR应该对于男性和女性相同：
- en: '*P* (*d =* 1*|Y =* 1*, G = m*) *= P* (*d =* 1*|Y =* 1*, G =* *f*)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d =* 1*|Y =* 1*, G = m*) *= P* (*d =* 1*|Y =* 1*, G =* *f*)'
- en: For example, the FPR and TPR for married/divorced male and female entrants are
    0.13 and 0.87, respectively. As the classifier exhibits equal measures of good
    qualifying scores among males and females, this would lead to equal treatment
    of the two groups. If the classifier can also satisfy the low scores among the
    two groups, it will be said to satisfy the condition of group fairness.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，已婚/离婚男性和女性参赛者的假阳性率（FPR）和真正阳性率（TPR）分别为0.13和0.87。由于分类器在男性和女性中展现出相等的良好预测分数，这将导致这两个组别的平等待遇。如果分类器还能够满足两组的低分数要求，那么它将被视为满足群体公平性的条件。
- en: Equalized odds/disparate mistreatment
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平等化机会/不平等待遇
- en: 'This type of fairness, also commonly known as **conditional procedure accuracy
    equality**, enforces a condition of fairness by combining the previous two definitions.
    It standardizes the error rates for both male and female populations, where a
    classifier is satisfactory if protected and unprotected groups have an equal TPR
    and an equal FPR. Hence, this acts as a combination function that ensures equality
    among both male and female groups, when an entrant with a good qualifying score
    is also correctly designated a good predicted qualifying score by the model. The
    probability of an entrant having a low qualifying score in reality has been seen
    to be inaccurately classified by the model and designated a good predicted qualifying
    score. Mathematically, it can be formulated as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公平性类型，也被称为**条件程序准确性平等**，通过结合前两个定义来强化公平性条件。它标准化了男性和女性人群的错误率，其中分类器在受保护和非受保护群体具有相等的TPR和相等的FPR时才能被认为是满意的。因此，当具有良好资格分数的参与者也被模型正确地指定为良好预测资格分数时，这充当了一个组合函数，确保男性和女性群体之间的平等。在现实中，参与者具有低资格分数的概率被模型错误分类，并指定为良好预测的资格分数。在数学上，它可以表达为：
- en: '*P* (*d =* 1*|Y = i, G = m*) *= P* (*d =* 1*|Y = i, G = f*)*, i* *∈*0, 1'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d =* 1*|Y = i, G = m*) *= P* (*d =* 1*|Y = i, G = f*)*, i* *∈*0, 1'
- en: For example, a classifier exhibiting an FPR for married/divorced male and female
    entrants as 0.70 can only satisfy disparate mistreatment when it also records
    a TPR of 0.86 for both males and females. But on the other hand, it shows preferential
    treatment toward the male group by recording an FPR of 0.80, in contrast to an
    FPR of 0.70 for the female group. This preferential or biased treatment will cause
    the classifier to fail to satisfy the condition of equalized odds. Hence, classifiers
    are often found to satisfy predictive parity but not to satisfy the condition
    of equalized odds.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个分类器展示出已婚/离婚男性和女性参与者的FPR为0.70时，仅当它同时记录了男性和女性的TPR为0.86时，才能满足不同的错误对待条件。但另一方面，它展示了对男性群体的优先对待，记录了0.80的FPR，而对女性群体的FPR则为0.70。这种优先或有偏差的处理将导致分类器未能满足平等的几率条件。因此，分类器通常被发现满足预测的平衡，但未能满足几率的平等条件。
- en: '![Figure 7.5 – Two fairness trees that represent different types of fairness
    on group/individual/overall levels](img/B18681_07_005.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 代表群体/个体/总体公平不同类型的两棵公平树](img/B18681_07_005.jpg)'
- en: Figure 7.5 – Two fairness trees that represent different types of fairness on
    group/individual/overall levels
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 代表群体/个体/总体公平不同类型的两棵公平树
- en: '*Figure 7**.5* gives a condensed representation of these fairness definitions.
    The preceding figure depicts two fairness trees – the first one is for when we
    deal with group fairness and the second one is for when we deal with fairness
    for either individuals or everyone.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7**.5* 提供了这些公平定义的简要表示。前述图像展示了两棵公平树 – 第一棵是处理群体公平性时使用的，第二棵是处理个体或所有人公平性时使用的。'
- en: The overall fairness metric called the **Theil Index** is an economic inequality
    metric used to quantify the divergence of the current distribution of resources
    (for example, income) within and among diverse demographic groups. Thus, it helps
    to measure the inequality of income spread (by giving a weighted average of inequality
    within subgroups) across individuals and groups/sub-groups in a population.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的**Theil指数**是一种经济不平等度量指标，用于量化当前资源（例如收入）在不同人群之间和内部分布的差异。因此，它有助于衡量在人群和亚群体内部的不平等程度（通过给出亚群体内的不平等的加权平均值），从而帮助人们测量人群的不平等性。
- en: To compute the individual fairness metric, we can either use **Manhattan distance**
    (a metric that computes the average Manhattan distance between the samples from
    two datasets) or **Euclidean distance** (a metric that computes the average Euclidean
    distance between the samples from two datasets). For individual fairness, we follow
    a similar method for similar individuals irrespective of their relationship with
    any group.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算个体公平度量，我们可以使用**曼哈顿距离**（计算两个数据集中样本之间的平均曼哈顿距离）或**欧几里得距离**（计算两个数据集中样本之间的平均欧几里得距离）。对于个体公平性，我们遵循一种类似的方法，适用于类似的个体，而不考虑他们与任何群体的关系。
- en: A group fairness tree is generated based on disparate representations or disparate
    errors. It is based on the two following factors.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不同的代表性或不同的错误生成了一个群体公平性树。这基于以下两个因素。
- en: '**Fairness based on disparate representations – equal parity/proportional parity**'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**基于差异化表示的公平性 – 平等对比/比例对比**'
- en: A fairness tree splits to either equal parity, when we are interested in selecting
    an equal number of people from each group, or proportional parity, when we are
    interested in selecting a number of people that is proportional to their percentage
    in the entire population.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性树分裂为平等对比，当我们关注从每个组中选取相同人数时，或者比例对比，当我们关注按其在总体中的比例选取人数时。
- en: '**Fairness based on disparate errors in the system – punitive/assistive error
    metrics**'
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**基于系统中的差异化错误的公平性 – 惩罚性/辅助性错误指标**'
- en: A fairness tree splits to either punitive or assistive error metrics, depending
    on whether we are interested in making interventions that could either hurt or
    help individuals.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性树根据我们是否关注进行可能伤害或帮助个体的干预，分裂成惩罚性或辅助性错误指标。
- en: We further illustrate in *Figure 7**.6* the computation of group fairness metrics
    using the Adult dataset ([https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步在*图 7.6*中说明了使用成人数据集计算群体公平性指标的过程（[https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)）。
- en: '![Figure 7.6 – An evaluation of group fairness metrics on the Adult dataset](img/B18681_07_006.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 基于成人数据集的群体公平性指标评估](img/B18681_07_006.jpg)'
- en: Figure 7.6 – An evaluation of group fairness metrics on the Adult dataset
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 基于成人数据集的群体公平性指标评估
- en: Conditional-use accuracy equality
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件性使用准确度平等
- en: 'This type of fairness is built on the founding principle of joining two equal
    t” conditions, PPV and NPV. Here, the probability of subjects under consideration
    having true predictive values in reality is included in the positive class (PPV).
    We will also observe that the probability of subjects having a false predictive
    value in reality is included in the negative class (NPV). Mathematically, this
    can be formulated as:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的公平性建立在将两个相等条件，PPV和NPV，结合起来的基本原则上。在这里，所考虑的对象在现实中具有真实预测值的概率被归类为正类（PPV）。我们还会观察到，实际中具有错误预测值的对象的概率被归类为负类（NPV）。在数学上，这可以表示为：
- en: (*P* (*Y =* 1*|d =* 1*, G = m*) *= P* (*Y =* 1*|d =* 1*, G = f* ))*∧*(*P* (*Y
    =* 0*|d =* 0*, G = m*) *= P*(*Y =* 0*|d =* 0*,G =* *f* ))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (*P* (*Y =* 1*|d =* 1*, G = m*) *= P* (*Y =* 1*|d =* 1*, G = f* ))*∧*(*P* (*Y
    =* 0*|d =* 0*, G = m*) *= P*(*Y =* 0*|d =* 0*,G =* *f* ))
- en: Equivalent accuracy is obtained for both male and female entrants. This means
    that male and female entrants exhibit an equal probability of demonstrating equivalent
    accuracy values. To explain further, a good predicted qualifying score signals
    a good qualifying score for an entrant, while a low predicted qualifying score
    for an entrant signifies a low qualifying score. As with the previous metrics,
    this one is also not satisfied when the likelihood of a male entrant with a low
    predicted score diminishes (due to gender bias), causing the male entrant to be
    given a good qualifying score when he actually should not. A demonstration of
    this equivalent accuracy metric is that male and female entrants have a PPV of
    0.73 and 0.74, respectively, and an NPV of 0.49 and 0.63, respectively.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 男性和女性参与者的准确度相当。这意味着男性和女性参与者表现出相等的概率，展示相等的准确度值。进一步解释，一个良好的预测资格分数表示参与者应有的资格分数，而低预测资格分数则表示低资格分数。如同前述的指标，当性别偏见影响时，男性参与者的低预测分数的可能性降低，导致男性参与者获得不该有的较好资格分数。通过这个等效准确度指标来演示，男性和女性参与者分别具有0.73和0.74的PPV，以及0.49和0.63的NPV。
- en: Overall accuracy equality
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体准确度平等
- en: 'This forces a classifier to satisfy the condition that both protected and unprotected
    groups demonstrate equal prediction accuracy. Here, in the ground truth data,
    the probability of a subject with values of true or false is classed as either
    positive or negative. The same class labels are also used in the model’s predictions.
    This definition implies that true negatives are as desirable as true positives.
    Mathematically, it can be formulated as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这迫使分类器满足保护组和非保护组表现出相等预测准确度的条件。在实际的基准数据中，真实或错误的预测值的概率被归类为正类或负类。模型的预测也使用相同的类别标签。这一定义意味着真实负样本和真实正样本同样重要。从数学角度，这可以表示为：
- en: '*P* (*d = Y, G = m*) *= P* (*d = Y, G =* *f*)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* (*d = Y, G = m*) *= P* (*d = Y, G =* *f*)'
- en: This metric allows minor differences between males and females, where the two
    groups exhibit an overall accuracy rate of 0.68 and 0.71, respectively. However,
    in this example, we consider overall accuracy and not the individual accuracy
    of predicted classes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标允许男性和女性之间存在微小差异，其中两组的总体准确率分别为0.68和0.71。然而，在此示例中，我们考虑的是总体准确率，而不是预测类别的个别准确率。
- en: Treatment equality
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对待平等
- en: This metric determines the ratio of errors of the classifier instead of considering
    its accuracy. As such, the classifier ensures that both the protected and unprotected
    groups demonstrate an equal ratio of false negatives and false positives (*FN/FP*),
    such as 0.56 and 0.62 for male and female entrants, respectively. This idea has
    been formulated to ensure an equal ratio of **false negatives** (**FN**) to **false
    positives** (**FP**) of the different classes of the population for which the
    ML classifier is being evailuated for fairness.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标确定了分类器的错误比率，而不是考虑其准确度。因此，分类器确保受保护和未受保护群体展示相同的假阴性和假阳性的比率（*FN/FP*），例如男性和女性参赛者的假阴性和假阳性比率分别为0.56和0.62。这个概念的提出旨在确保不同群体在进行机器学习分类器公平性评估时，**假阴性**（**FN**）与**假阳性**（**FP**）的比率相等。
- en: Types of fairness based on similarity-based measures
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于相似性度量的公平性类型
- en: Types of fairness built using statistical metrics often suffer from the limitation
    of ignoring all attributes other than sensitive attributes. This leads to the
    unfair treatment of one group, even when the same ratio of male and female entrants
    exhibits the same skillsets or criteria. Such situations arise when selection
    happens randomly for one group, whereas selection for another group (such as females)
    is based on certain other attributes (for example, having higher savings). This
    results in a discrepancy even though statistical parity will mark the classifier
    as fair. The following types of fairness are put forward to address such issues
    that may arise due to selection bias by removing the marginalization of insensitive
    attributes, *X*, of the classified groups under study.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 基于统计指标构建的公平性类型通常会忽视除敏感属性以外的所有属性，这导致即使同样的男性和女性参赛者展现相同的技能或标准，也会对某一组群体产生不公平待遇。当选择是随机进行的，而另一个群体（如女性）的选择则基于其他某些属性（例如拥有更多储蓄）时，就会出现这种情况。这会导致差异，尽管统计平等会将分类器标记为公平。以下提出了几种公平性类型，用以解决由于选择偏差引起的问题，通过消除被分类群体中无关属性*X*的边缘化。
- en: Causal discrimination
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果歧视
- en: 'A classifier is said to fulfill the condition of causal discrimination when
    it produces the same classification for any two subjects with exactly the same
    features (or attributes), *X*. To satisfy this criterion, both males and females
    with identical attributes should either be designated a good qualifying score
    or a low qualifying score, and this should be the same for both groups. Mathematically,
    this can be expressed as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类器对任何两个具有完全相同特征（或属性）*X*的对象做出相同分类时，就可以说它满足因果歧视的条件。为了满足这一标准，具有相同属性的男性和女性应分别被指定为良好的合格分数或低的合格分数，而且这应该对两组是相同的。数学上可以表示为：
- en: (Xf = Xm ∧ Gf `!` = Gm) → df = dm
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (Xf = Xm ∧ Gf `!` = Gm) → df = dm
- en: To test this fairness measure, for each entrant in the test set, we need to
    generate identical entrants of the opposite gender and compare the predicted classification
    probabilities for those entrants. The classifier will fail if we fail to achieve
    the same probabilities for the two groups.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试此公平性度量，对于测试集中的每个参赛者，我们需要生成对立性别的相同参赛者，并比较这些参赛者的预测分类概率。如果我们未能为这两组实现相同的概率，分类器将失败。
- en: Fairness through unawareness
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过无意识实现公平
- en: 'A classifier is said to fulfill this condition of fairness when no sensitive
    attributes are explicitly used to predict the final model outcome. Training such
    models requires that no gender-, race-, or ethnicity-related features are used
    while training the model. Mathematically, the classification outcome of two similar
    entrants *i* and *j* (of the opposite gender) with identical attributes can be
    expressed as (X : Xi = Xj → di = dj). To test this condition, for each entrant
    in the test set, we need to generate identical entrants of the opposite gender
    and compare the predicted classification probabilities for those two entrants.
    The classifier is then trained with any classification algorithm (such as logistic
    regression or a random forest classifier) without using any sensitive attributes
    and is validated as succeeding if and only if it generates the same probabilities
    for both groups. However, we also need to ensure that no proxy features of the
    direct sensitive attributes are used to train the model, which may again result
    in creating unfair results in the model outcome.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '当分类器没有显式使用任何敏感属性来预测最终模型结果时，可以说它满足公平性条件。训练这样的模型要求在训练过程中不能使用与性别、种族或民族相关的特征。数学上，两个相似的参与者
    *i* 和 *j*（性别不同）具有相同属性时，分类结果可以表示为（X : Xi = Xj → di = dj）。为了验证这一条件，对于测试集中的每个参与者，我们需要生成性别相反的相同参与者，并比较这两个参与者的预测分类概率。然后，使用任何分类算法（如逻辑回归或随机森林分类器）训练该分类器，且不使用任何敏感属性，只有当它为两个组生成相同的概率时，才验证其成功。然而，我们还需要确保不使用任何直接敏感属性的代理特征来训练模型，这可能会导致模型结果产生不公平的结果。'
- en: Fairness through awareness
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过意识实现公平
- en: 'This measure of fairness combines the previous two types to illustrate the
    fact that similar individuals should exhibit similar classifications. We can evaluate
    the similarity of individuals based on a distance metric, where the distributions
    of predicted outcomes for individuals should lie within the computed distance
    between the individuals. The fairness criterion is said to be satisfied when *D*(*M*(*x*)*,
    M*(*y*)) ≤ *k* (*x, y*), where *V* represents the set of entrants, *k* serves
    as the distance metric between the two entrants, *D* (distance) represents the
    metric between the distribution of predicted outputs, and *V × V* *→* *R* creates
    an alignment from a set of entrants to probability distributions over outcomes
    *M: V* *→* *δ**A*.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '这种公平性度量结合了前两种类型，说明了相似的个体应展示相似的分类。我们可以基于距离度量来评估个体的相似性，其中个体的预测结果分布应该位于计算出的个体之间的距离之内。当
    *D*(*M*(*x*), M(*y*)) ≤ *k*(*x, y*) 时，可以说满足公平性标准，其中 *V* 代表参与者集合，*k* 是两个参与者之间的距离度量，*D*（距离）表示预测输出分布之间的度量，*V
    × V* → *R* 创建从一组参与者到结果概率分布的对齐 *M: V* → *δ**A*。'
- en: 'Now let''s illustrate this further with an example. As *k* represents the distance
    between two entrants *i* and *j,* it can hold the value *0* if the features in
    *X* (all features/attributes other than gender) are identical, or *1* if some
    features in *X* vary. *D* could be defined as *0* if the classifier resulted in
    a prediction in the same class, or *1* otherwise. The metric of consideration
    here is the distance, which has been computed by normalizing the difference between
    attributes such as age, income, and others. The reduction to a simpler version
    ensures the easy representation of distance, which is the statistical difference
    between the model’s predicted probabilities for two entrants: *D*(*i, j*) *= S*(*i*)
    *−* *S*(*j*).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个例子进一步说明。由于 *k* 表示两个参与者 *i* 和 *j* 之间的距离，如果 *X* 中的特征（除性别外的所有特征/属性）相同，则
    *k* 的值可以为 *0*；如果 *X* 中的一些特征不同，则 *k* 的值为 *1*。如果分类器的预测结果属于同一类别，则 *D* 可以定义为 *0*，否则定义为
    *1*。这里考虑的度量是距离，它是通过规范化年龄、收入等属性之间的差异计算得到的。简化版本的减少确保了距离的易表示，这表示模型对两个参与者预测概率的统计差异：*D*(*i,
    j*) = *S*(*i*) − *S*(*j*)。
- en: Types of fairness based on causal reasoning
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于因果推理的公平性类型
- en: The types of fairness that we’ve covered have been designed based on a directed
    acyclic graph, where nodes represent the attributes of an entrant and edges represent
    associations between the attributes. Such graphs aid in building fair classifiers
    and other ML algorithms, as they are driven by the relationships between the attributes
    and their impact on the model outcomes. The relationships can be further expressed
    by a different set of equations, to ascertain the impact of sensitive attributes
    and allow only a tolerance threshold to permit discrimination among different
    groups present in the population.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所讨论的公平类型是基于有向无环图（DAG）设计的，其中节点表示一个人的属性，边表示属性之间的关联。这样的图有助于构建公正的分类器和其他机器学习算法，因为它们是由属性之间的关系及其对模型结果的影响驱动的。这些关系可以通过一组不同的方程式进一步表达，以确定敏感属性的影响，并允许设定容忍阈值，从而允许不同群体之间存在一定的歧视。
- en: '![Figure 7.7 – A causal graph with a proxy attribute and a resolving attribute](img/B18681_07_007.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 包含代理属性和解决属性的因果图](img/B18681_07_007.jpg)'
- en: Figure 7.7 – A causal graph with a proxy attribute and a resolving attribute
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 包含代理属性和解决属性的因果图
- en: '*Figure 7**.7* shows a causal graph comprising the attributes credit amount,
    employment length, credit history, **protected attribute** **G** (or gender),
    and predicted outcome **d**. *Figure 7**.7* depicts how a **proxy attribute**
    (like **G**) can be derived from another attribute, which in our case is employment
    length. As the causal graph demonstrates, we can easily derive the entrant’s gender
    from their employment duration. Similarly, we use the term **resolving attribute**
    in a causal graph to describe an attribute that is determined by a protected attribute,
    in an unbiased fashion without any discrimination. Credit amount serves as a resolving
    attribute for **G**, where the variations observed in credit amount for different
    values of **G** are not considered biased and do not hold any discrimination.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7**.7* 显示了一个因果图，其中包含信用额度、就业年限、信用历史、**保护属性** **G**（或性别）和预测结果 **d**。*图 7**.7*
    描述了如何从另一个属性中推导出 **代理属性**（如 **G**），在我们这个例子中，代理属性是就业年限。正如因果图所示，我们可以轻松地从一个人的就业年限推导出其性别。类似地，在因果图中，我们使用
    **解决属性** 这一术语来描述由保护属性决定的属性，这种决定方式是公正的，不带有任何歧视。信用额度作为 **G** 的解决属性，其中不同 **G** 值下的信用额度变化不被视为偏见，也不带有任何歧视。'
- en: 'Now, let''s look at different types of fairness based on causal reasoning:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们基于因果推理来看看不同类型的公平性：
- en: '**Counterfactual fairness**: This type of fairness is applicable to a causal
    graph when the predicted outcome **d** in the graph has no dependence on a descendant
    of the protected attribute **G**. We see in the example shown in *Figure 7**.7*
    that **d** is dependent on credit history, credit amount, and employment length.
    Even the presence of a single direct descendant of **G**, employment length in
    this case, can cause the model to break the condition of being counterfactually
    fair.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反事实公平**：当因果图中的预测结果 **d** 不依赖于保护属性 **G** 的任何后代时，这种公平性就适用。我们在 *图 7**.7* 中看到，**d**
    依赖于信用历史、信用额度和就业年限。即使只有一个直接的后代 **G**，在这种情况下是就业年限，也会导致模型不满足反事实公平的条件。'
- en: '**No unresolved discrimination**: This property exists in a causal graph when
    there is an absence of any path from the protected attribute **G** to the predicted
    outcome **d**. The presence of a resolving variable can be treated as an exception
    and is not a violation of anything. In *Figure 7**.7*, there exists a path from
    **G** to **d** via credit amount, which is non-discriminatory. The presence of
    credit amount enables the establishment of a resolving attribute. It further aids
    in creating a discriminatory graph by generating a path via employment length.
    Hence, this graph demonstrates an example of unresolved discrimination and cannot
    satisfy this type of fairness.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无未解决歧视**：当保护属性 **G** 到预测结果 **d** 之间没有任何路径时，因果图中就存在这种属性。解决变量的存在可以视为例外，不构成任何违规。在
    *图 7**.7* 中，存在一条从 **G** 到 **d** 的路径，经过信用额度，这条路径是不带有歧视的。信用额度的存在使得解决属性得以建立，并进一步通过就业年限生成一条歧视性路径。因此，这个图展示了一个未解决歧视的例子，无法满足这一类型的公平性。'
- en: '**No proxy discrimination**: This property of a causal graph implies that it
    is devoid of any proxy discrimination. In other words, it means there exist zero
    paths from the protected attribute **G** to the predicted outcome**d**. In the
    absence of any blockage caused by a proxy variable, the causal graph can be said
    to have no proxy discrimination, thereby confirming an unbiased representation
    of the data. However, in our example, there is an indirect path from **G** to
    **d**via the employment length proxy attribute, which means the graph has proxy
    discrimination.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无代理歧视**：因果图的这一属性意味着它没有任何代理歧视。换句话说，这意味着从受保护属性**G**到预测结果**d**之间不存在路径。在没有代理变量造成阻塞的情况下，可以说因果图没有代理歧视，从而确认数据的无偏表示。然而，在我们的例子中，从**G**到**d**通过就业时长代理属性存在间接路径，这意味着该图存在代理歧视。'
- en: '**Fair inference**: This type of fairness in a causal graph helps the path
    classification process by labeling the paths in a causal graph as legitimate or
    illegitimate. A causal graph guarantees the satisfaction of the fair inference
    condition, where illegitimate paths from **G** to **d**are absent. However, as
    *Figure 7**.7* shows, the existence of another illegitimate path, via credit amount,
    means it cannot satisfy the condition of fair inference. Employment length is
    an important factor for consideration in credit-related decisions, so even though
    it behaves as a proxy candidate for **G**, the path can be referred to as a legitimate
    path.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平推理**：这种因果图中的公平性通过将路径标记为合法或不合法，帮助路径分类过程。因果图保证满足公平推理条件，即不存在从**G**到**d**的不合法路径。然而，正如*图7.7*所示，通过信用额度存在另一条不合法路径，意味着它无法满足公平推理的条件。就业时长是信用相关决策中的一个重要因素，因此尽管它作为**G**的代理候选人，但这条路径可以被视为合法路径。'
- en: We have studied different statistical measures of fairness, but they alone are
    insufficient to conclude that the predictions are fair, and they assume the availability
    of actual, verified outcomes. Despite using statistical metrics, we cannot be
    certain that outcomes present in training data will always be present in the classified
    model and that predictions will also conform to the same distribution. More advanced
    definitions of fairness using distance-based similarity metrics and causal reasoning
    have also been proposed to support fairness in model predictions, but they require
    expert intervention to confirm results.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了不同的公平性统计衡量标准，但单凭这些是无法得出预测是否公平的结论的，它们假设有实际的、经过验证的结果。即使使用了统计指标，我们也不能确定训练数据中的结果总会出现在分类模型中，并且预测结果也会遵循相同的分布。为了支持模型预测中的公平性，也提出了使用基于距离的相似性度量和因果推理的更先进的公平性定义，但这些方法需要专家干预来确认结果。
- en: A problem in seeking expert judgments is that they can involve bias. Modern
    research techniques explore ways to reduce the search space without compromising
    accuracy. Furthermore, when we try to meet all fairness conditions in a solution,
    the complexity of the solution increases, as doing so requires the exploration
    of a larger search space. Fair prediction also requires the consideration of social
    issues such as unequal access to resources and social conditioning. Teams involved
    in data processing and ML model development should try to analyze social issues’
    impact and incorporate it when designing fair solutions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求专家判断的一个问题是，它们可能会涉及偏见。现代研究技术探讨了如何在不妥协准确性的情况下减少搜索空间。此外，当我们尝试在解决方案中满足所有公平性条件时，解决方案的复杂性会增加，因为这样做需要探索更大的搜索空间。公平预测还需要考虑社会问题，如资源获取不平等和社会条件化。参与数据处理和机器学习模型开发的团队应尽量分析社会问题的影响，并在设计公平解决方案时加以考虑。
- en: With these types of fairness in mind, let's now try to master some of the open
    source tools and techniques available to run data audits and quality checks.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些公平性类型后，我们现在尝试掌握一些开源工具和技术，用于进行数据审计和质量检查。
- en: The role of data audits and quality checks in fairness
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据审计和质量检查在公平性中的作用
- en: Even before digging deep into predictive algorithms and evaluating fairness
    metrics, we must try to see whether the training data used in the process is skewed
    and biased toward a majority of the population. This is mainly because most bias
    results from not having enough data for a disadvantaged or minority sector of
    a population. Additionally, bias also emerges when we do not apply any of the
    techniques to deal with data imbalance. In such scenarios, it is essential for
    us to integrate explainability tools to justify the variability and skewness of
    the data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究预测算法和评估公平性指标之前，我们必须先检查训练数据是否存在偏向于大多数人口群体的倾斜或偏见。这主要是因为大多数偏见源于对于人口中弱势群体或少数群体的数据不足。此外，当我们没有应用任何处理数据不平衡的技术时，偏见也会出现。在这种情况下，整合可解释性工具来证明数据的变化性和偏斜性是至关重要的。
- en: Let's now investigate how to measure data imbalance and explain variability
    with the use of certain tools. One of the tools we are going to use first is **Fairlens**,
    which aids in fairness assessment and improvement (such as evaluating fairness
    metrics, mitigation algorithms, plotting, and so on). Some examples are given
    here with code snippets (on the COMPAS dataset), which will help us to understand
    the data distributions and evaluate the fairness criteria.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探讨如何使用某些工具来衡量数据不平衡并解释变化性。我们首先要使用的工具之一是**Fairlens**，它有助于公平性评估和改进（如评估公平性指标、缓解算法、绘图等）。这里提供了一些示例代码片段（使用COMPAS数据集），这将帮助我们理解数据分布并评估公平性标准。
- en: 'The necessary imports for running all the tests are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 运行所有测试所需的导入如下：
- en: '[PRE6]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Assessing fairness
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性评估
- en: 'Let’s try out Fairlens:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用Fairlens：
- en: 'The `fairlens.FairnessScorer` class can be used to automatically generate a
    fairness report on a dataset, if you provide it with a target column. Here, the
    target column stays independent of the sensitive attributes. We can analyze the
    inherent bias in a dataset used for supervised learning by passing in the name
    of a desired output column. Now, let''s generate the demographic report of the
    dataset:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fairlens.FairnessScorer` 类可以用来自动生成数据集的公平性报告，只需提供目标列。在这里，目标列与敏感属性是独立的。我们可以通过传入所需输出列的名称，分析用于监督学习的数据集中的固有偏见。现在，让我们生成数据集的人口统计报告：'
- en: '[PRE7]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will generate the following output, which shows that there is a complete
    representation of distributional scores of all the major demographic sections
    of the population.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下输出，显示所有主要人口群体的分布评分完整表现。
- en: '![Figure 7.8 – Distribution statistics of the different demographic groups
    of the population](img/B18681_07_008.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 各个人口群体的分布统计](img/B18681_07_008.jpg)'
- en: Figure 7.8 – Distribution statistics of the different demographic groups of
    the population
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 各人口群体的分布统计
- en: 'We also plot the distributions of decile scores in subgroups made of African-Americans
    and Caucasians as shown here:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还绘制了非裔美国人和白人群体的十等分评分分布，如下所示：
- en: '[PRE8]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gives us the following output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![Figure 7.9 – Proportion of two groups – majority and minority](img/B18681_07_009.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 两个群体的比例 – 多数与少数](img/B18681_07_009.jpg)'
- en: Figure 7.9 – Proportion of two groups – majority and minority
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 两个群体的比例 – 多数与少数
- en: Statistical distance
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统计距离
- en: This is another important distance parameter that we want to consider if we
    want to evaluate how the distributions between two sensitive demographic groups/sub-groups
    vary. The metric is used for evaluating the statistical distance between two probability
    distributions, `group1` and `group2`, with respect to the target attribute.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个我们希望考虑的重要距离参数，如果我们想评估两个敏感人口群体/子群体之间分布的差异性。该指标用于评估两个概率分布（`group1`和`group2`）相对于目标属性的统计距离。
- en: 'This can be done using the following code to yield (`0.26075238442125354, 0.9817864673203285`).
    It returns the distance and the p-value, as `p_value` is set to `True`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下代码来完成，返回（`0.26075238442125354, 0.9817864673203285`）。它返回距离和p值，因为`p_value`设置为`True`：
- en: '[PRE9]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Proxy detection
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理检测
- en: 'This metric helps us to evaluate proxy features in some datasets. Some insensitive
    attributes may become highly/partially correlated with sensitive columns, which
    can effectively become proxies for them. This in turn makes the model biased when
    the same dataset is used for training. Here, for the four different data points,
    we can try to evaluate the hidden insensitive proxy features:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标帮助我们评估某些数据集中的代理特征。一些不敏感的属性可能会与敏感列高度/部分相关，从而有效地成为它们的代理。这反过来会使得在使用相同数据集进行训练时，模型变得有偏。这里，我们可以尝试评估四个不同数据点中隐藏的不敏感代理特征：
- en: '[PRE10]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Linear regression
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear regression can help us to identify proxy features, by evaluating the
    correlation between the dependent and independent variables.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可以通过评估因变量和自变量之间的相关性，帮助我们识别代理特征。
- en: Cosine similarity/distance method
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦相似度/距离方法
- en: Cosine similarity is one of the mechanisms used to detect proxy features, where
    the similarity factor evaluates similar items in multidimensional space. Any two
    features in a dataset (say, for example, for a loan application dataset) would
    become proxy features when the cosine similarity between any two vectors falls
    in the same direction. Such cases seem obvious when we see monthly income and
    expenditure behaving as proxy features, particularly in a loan application with
    the applicant’s gender and number of dependents taken together.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度是用于检测代理特征的机制之一，其中相似度因子在多维空间中评估相似项。当数据集中的任意两个特征（例如，在贷款申请数据集中）之间的余弦相似度方向相同，它们就会成为代理特征。这样的情况在看到月收入和支出作为代理特征时尤其明显，尤其是在贷款申请中，考虑到申请人的性别和抚养人数。
- en: The linear association method using variance
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用方差的线性关联方法
- en: 'This metric was discussed in the paper *Hunting for Discriminatory Proxies
    in Linear Regression Models* by Yeom, Datta, and Fredrikson ([https://arxiv.org/pdf/1810.07155.pdf](https://arxiv.org/pdf/1810.07155.pdf))
    and aims to measure the association between two attributes. To compute this metric,
    we need to compute *cov* (*X*1, *X*2)2 / *Var* (*X*1) *Var* (*X*2), which can
    be done as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标在Yeom、Datta和Fredrikson的论文《*在线性回归模型中寻找歧视性代理*》中进行了讨论（[https://arxiv.org/pdf/1810.07155.pdf](https://arxiv.org/pdf/1810.07155.pdf)），旨在衡量两个属性之间的关联。要计算此指标，我们需要计算
    *cov* (*X*1, *X*2)2 / *Var* (*X*1) *Var* (*X*2)，其计算方法如下：
- en: 'First, we need to compute the covariance factor between the two feature attributes:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要计算两个特征属性之间的协方差因子：
- en: '[PRE11]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next step is to compute the individual variances of the feature attributes:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是计算特征属性的各自方差：
- en: '[PRE12]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we try to evaluate the degree of linear association between the feature
    attributes:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们尝试评估特征属性之间的线性关联度：
- en: '[PRE13]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the following output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE14]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, let's look at how we determine the correlations between protected features
    and other features.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何确定保护特征与其他特征之间的相关性。
- en: The variance inflation factor
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方差膨胀因子
- en: The **variance inflation factor** (**VIF**) metric aims to measure multicollinearity
    by evaluating the coefficient of determination (R2) for each variable. As this
    method determines proxy features, it can aid in removing collinear or multicollinear
    features, which act as proxies for sensitive/protected attributes in the dataset.
    Further feature removal can be done by leveraging multiple regression trees.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差膨胀因子**（**VIF**）指标旨在通过评估每个变量的决定系数（R2）来衡量多重共线性。由于该方法确定了代理特征，它可以帮助去除作为敏感/受保护属性代理的共线性或多重共线性特征。通过利用多个回归树，还可以进一步移除特征。'
- en: 'A high value for VIF in a regression model between protected features and other
    features would be interpreted as a sign of collinearity between multiple colinear
    features. Furthermore, it would also indicate the strong presence of another feature
    that is collinear and thus is a proxy for the feature under study:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归模型中，保护特征与其他特征之间的VIF值较高，表示多个共线性特征之间的共线性信号。此外，这也表明另一个特征存在强烈的共线性，因此成为研究特征的代理：
- en: 'The first step is to have the necessary library imports for the VIF and load
    the `german_credit` dataset:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是导入VIF所需的库并加载`german_credit`数据集：
- en: '[PRE15]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step is to create a mapping for the sensitive feature attributes and
    segregate the independent features for which we want to compute the VIF:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是为敏感特征属性创建映射，并分隔我们希望计算VIF的自变量：
- en: '[PRE16]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The final step is to run the VIF on the data, evaluate the VIF for each feature
    attribute, and discover the potential proxy features:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是对数据运行 VIF，评估每个特征属性的 VIF，并发现潜在的代理特征：
- en: '[PRE17]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We get the following output, which clearly shows that `Job` and `Duration`
    have high VIF values. Using them together leads to a model with high multicollinearity.
    They serve as potential candidates for proxy features:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到以下输出，清晰地显示 `Job` 和 `Duration` 具有较高的 VIF 值。将它们一起使用会导致模型出现较高的多重共线性。它们作为潜在的代理特征候选：
- en: '[PRE18]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Mutual information
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互信息
- en: 'This metric signifies the amount of information available for a random feature
    attribute given another feature attribute is existing. It works in non-linear
    tree-based algorithms by computing *I(X1, X2)*, which is a weighted sum of joint
    probabilities, in contrast to *COV(X1, X2)*, which is a weighted sum of the product
    of the two features:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量表示在已知某个特征属性的情况下，另一个特征属性可用的信息量。它在非线性树形算法中通过计算 *I(X1, X2)* 来工作，*I(X1, X2)*
    是联合概率的加权和，与 *COV(X1, X2)* 相对，后者是两个特征乘积的加权和：
- en: 'We can compute the mutual information score for the `german_credit` dataset
    as follows:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以按如下方式计算 `german_credit` 数据集的互信息分数：
- en: '[PRE19]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We get the following output, which again confirms the fact that the relationship
    between credit amount and sex is high, followed by the relationship between the
    sex and credit duration attributes:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到以下输出，这再次确认了信用金额与性别之间的关系较强，其次是性别与信用期限属性之间的关系：
- en: '[PRE20]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Significance tests
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显著性检验
- en: 'When we assess fairness scores and evaluate statistical distances, we may also
    want to test the null hypothesis and see whether the null or alternate hypothesis
    is true. This can be done using bootstrapping or permutation tests to resample
    the data multiple times. These iterations compute the statistic multiple times
    and provide an estimate of its distribution, by computing the p-value or the confidence
    interval for the metric:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估公平性得分并评估统计距离时，我们也可能想要检验原假设，并查看原假设或备择假设是否成立。这可以通过自助法或排列检验来完成，通过多次重新采样数据。这些迭代会多次计算统计量，并通过计算度量的
    p 值或置信区间来提供其分布的估计：
- en: 'Let''s see with the following example how we can compute the confidence interval
    as well as the p-value between male and female distributions:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过以下示例来看一下如何计算男性和女性分布之间的置信区间以及 p 值：
- en: '[PRE21]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The test statistic can be configured with the following code. `t_distribution`can
    be set up using either the permutation or bootstap method between the two groups
    previously created:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过以下代码配置测试统计量。`t_distribution` 可以通过排列或自助法在之前创建的两个组之间进行设置：
- en: '[PRE22]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Or, you can use this code:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用以下代码：
- en: '[PRE23]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s compute the confidence interval and p-value as shown here:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们计算如图所示的置信区间和 p 值：
- en: '[PRE24]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we get the output that follows. For the first case, we get the confidence
    interval as a tuple, while for the second case, we receive the p-value:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们得到以下输出。对于第一个案例，我们得到置信区间作为元组，而对于第二个案例，我们得到 p 值：
- en: '[PRE25]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Evaluating group fairness
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估组公平性
- en: 'We have seen how group fairness is important for fulfilling fairness generally.
    Natalia Martinez, Martin Bertran, and Guillermo Sapiro are the inventors of the
    concept of the minimax fairness criteria, which aims to improve group fairness
    metrics. Let''s now study the important concept of minimax fairness ([https://github.com/amazon-research/minimax-fair](https://github.com/amazon-research/minimax-fair)),
    which strives to achieve fairness when protected group labels are not available.
    Equality of error rates is one of the most intuitive and well-studied forms of
    fairness. But using them poses a major challenge when we try to equalize error
    rates by raising the threshold of error rates, which is undesirable for social
    welfare causes. Hence, increasing the error margins to establish equal error rates
    across equally accurate racial groups, income levels, and geographic locations
    is not a good choice. To further increase group fairness, we can use the minimax
    group error, proposed by Martinez in 2020\. The concept of group fairness does
    not seek to equalize error rates (as stated earlier in the fairness definitions).
    Instead, this metric tries to minimize the largest group error rate, to ensure
    that the worst group demonstrates the same values in terms of fairness metrics.
    This relaxed concept of fairness tries to achieve the right trade-off between
    minimax fairness and overall accuracy. The metric has a two-fold objective:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到群体公平性对实现普遍公平性的重要性。Natalia Martinez、Martin Bertran 和 Guillermo Sapiro 是最小最大公平性标准概念的发明者，该概念旨在改进群体公平性指标。现在让我们研究一下最小最大公平性的重要概念
    ([https://github.com/amazon-research/minimax-fair](https://github.com/amazon-research/minimax-fair))，它在没有保护性群体标签的情况下力图实现公平性。错误率平等是最直观且研究最深入的公平性形式之一。但在尝试通过提高错误率阈值来平衡错误率时，这会带来一个主要挑战，而这对于社会福利而言是不可取的。因此，提高错误容忍度以在种族群体、收入水平和地理位置上实现错误率平等并不是一个好的选择。为了进一步增加群体公平性，我们可以使用
    Martinez 在 2020 年提出的最小最大群体误差。群体公平性的概念并不寻求平衡错误率（如前所述的公平性定义）。相反，这一度量指标试图最小化最大的群体误差率，以确保最差的群体在公平性度量方面表现出相同的值。这个放宽的公平性概念力图在最小最大公平性和整体准确性之间实现正确的平衡。该指标具有双重目标：
- en: First, it tries to find a minimax group fair model from a given statistical
    class
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它会尝试从给定的统计类别中找到一个最小最大群体公平模型
- en: Then it evaluates a model that minimizes the overall error subject, where the
    constraint has been set to bind all group errors below a specified (pre-determined)
    threshold
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它评估一个最小化整体错误的模型，其中约束已被设置为将所有群体的错误控制在指定的（预先确定的）阈值以下
- en: 'The objective of the preceding two steps is to reduce the process to unconstrained
    (non-fair) learning over the same class, where they converge. The minimax fairness
    metric can be further extended to handle different types of error rates, such
    as FP and FN rates, as well as overlapping groups having intersectional characteristics.
    Such groups with intersectional attributes are not just limited to race or gender
    alone, but can also comprise combinations of race and gender:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 前面两个步骤的目标是将过程简化为在同一类别上进行无约束（非公平）学习，在此过程中它们会收敛。最小最大公平性指标可以进一步扩展，以处理不同类型的错误率，如假阳性率（FP）和假阴性率（FN），以及具有交集特征的重叠群体。这些具有交集属性的群体不仅限于种族或性别，还可以是种族和性别的组合：
- en: 'Using a dataset, we generate `X`, `y`, `grouplabels`, and `groupnames`:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据集，我们生成 `X`、`y`、`grouplabels` 和 `groupnames`：
- en: '[PRE26]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here`X` *and* `y` are the features and labels for each type of group. Furthermore,
    `X` is divided into a number of different groups, where each of which has a shared
    linear function. This function is used to sample the labels with noise. In the
    absence of a dataset, to build and evaluate a fair minimax of a model, we can
    employ a synthetic data generation mechanism, as in the following code snippet:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里`X` *和* `y` 是每种群体类型的特征和标签。此外，`X` 被划分为多个不同的群体，每个群体都有一个共享的线性函数。该函数用于带噪声的标签采样。在没有数据集的情况下，为了构建和评估一个公平的最小最大模型，我们可以使用合成数据生成机制，如以下代码片段所示：
- en: '[PRE27]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now the learning process can be kicked off as in the following code snippet,
    with the parameters necessary for learning given in the table that follows:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在可以启动学习过程，如以下代码片段所示，学习所需的参数在接下来的表格中给出：
- en: '[PRE28]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s now see the different parameters and their functionalities involved
    in fair synthetic data generation:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看在公平合成数据生成中涉及的不同参数及其功能：
- en: '| **Parameter name** | **Purpose** |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| **参数名称** | **目的** |'
- en: '| `X` | A NumPy matrix of features with dimensions equal to `numsamples`. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| `X` | 一个特征的NumPy矩阵，其维度等于`numsamples`。 |'
- en: '| `y` | A NumPy array of labels with length `numsamples`. Should be numeric
    (0/1 label binary classification). |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| `y` | 长度为`numsamples`的标签的NumPy数组。应为数值（0/1标签的二进制分类）。 |'
- en: '| `a, b` | Parameters for *eta = a * t ^ (-b*). |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| `a, b` | *eta = a * t ^ (-b*) 的参数。 |'
- en: '| `scale_eta_by_label_range` | Whether or not the input value should be scaled
    by the max absolute label value squared. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| `scale_eta_by_label_range` | 是否应该按最大绝对标签值平方缩放输入值。 |'
- en: '| `rescale_features` | Whether or not the feature values should be rescaled
    for numerical stability. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| `rescale_features` | 是否应该重新调整特征值以确保数值稳定性。 |'
- en: '| `gamma` | The maximum allowed max groups errors by convergence. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| `gamma` | 收敛时允许的最大允许最大分组错误。 |'
- en: '| `relaxed` | Denotes whether we are solving the relaxed version of the problem.
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| `relaxed` | 表示我们是否正在解决问题的放松版本。 |'
- en: '| `model_type` | The `sklearn` model type, such as `LinearRegression`, `LogisticRegression`,
    and so on. |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| `model_type` | `sklearn`模型类型，如`LinearRegression`、`LogisticRegression`等。 |'
- en: '| `error_type` | For classification only: total, FP, FN, and so on. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `error_type` | 仅用于分类：总计、FP、FN等。 |'
- en: '| `extra_error_types` | The set of error types that we want to plot. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `extra_error_types` | 我们想要绘制的错误类型集合。 |'
- en: '| `pop_error_type` | The error type to use on the population: example error
    metric is sum of FP/FN over the entire population. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| `pop_error_type` | 在人群中使用的错误类型：例如，错误度量是整个人群的FP/FN总和。 |'
- en: '| `convergence_threshold` | Converges (early) when the max change in sample
    weights < `convergence_threshold`. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| `convergence_threshold` | 当样本权重的最大变化 < `convergence_threshold`时，提前收敛。 |'
- en: '| `Penalty` | The regularization penalty for logistic regression. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| `Penalty` | logistic回归的正则化惩罚。 |'
- en: '| `C` | The inverse of the regularization strength. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| `C` | 正则化强度的倒数。 |'
- en: '| `logistic_solver` | Which underlying solver to use for logistic regression.
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| `logistic_solver` | 用于logistic回归的底层求解器。 |'
- en: '| `fit_intercept` | Whether or not we should fit an additional intercept. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| `fit_intercept` | 是否应该拟合额外的截距。 |'
- en: '| `max_logi_iters` | The max number of logistic regression iterations. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| `max_logi_iters` | logistic回归迭代的最大次数。 |'
- en: Table 7.2 – Different parameters for training the minimax model
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 – 训练最小最大模型的不同参数
- en: '*Figure 7**.10* shows the variation of individual group errors, group weights,
    and the average population error with synthetic data using minimax group fairness
    with logistic regression. We can see that both the individual sub-groups (log-loss
    errors) and average population error (log loss) roughly follow the same trend
    and remain confined between `0.63` and `0.65`.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7**.10*显示了使用logistic回归和最小最大组公平性的合成数据中个体组错误、组权重和平均人群错误的变化。我们可以看到，个体子组（log损失错误）和平均人群错误（log损失）大致遵循相同的趋势，并保持在`0.63`到`0.65`之间。'
- en: '![Figure 7.10 – A diagram showing different parameters for training the minimax
    model](img/B18681_07_010.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 显示训练最小最大模型的不同参数的图表](img/B18681_07_010.jpg)'
- en: Figure 7.10 – A diagram showing different parameters for training the minimax
    model
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 显示训练最小最大模型的不同参数的图表
- en: Evaluating counterfactual fairness
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估反事实公平性
- en: 'Let''s use the open source **FAT Forensics** (**fatf**) library, which is a
    Python toolbox that can be used for evaluating the fairness, accountability, and
    transparency of predictive systems:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用开源的**FAT Forensics** (**fatf**)库，这是一个用于评估预测系统公平性、问责和透明性的Python工具箱：
- en: 'Let’s first set up the necessary imports:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们设置必要的导入：
- en: '[PRE29]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s load a synthetic healthcare dataset:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载一个合成的医疗保健数据集：
- en: '[PRE30]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With the dataset loaded, let’s map the target indices to target names, dropping
    unnecessary columns, before starting the training process:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集加载后，让我们将目标索引映射到目标名称，删除不必要的列，然后开始训练过程：
- en: '[PRE31]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we start the model training and select instances for counterfactual fairness:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开始模型训练并选择用于反事实公平性的实例：
- en: '[PRE32]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After training the model, let’s select the data instances (its protected features)
    for which we would like to test the counterfactual fairness:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型后，让我们选择数据实例（其受保护特征），以便测试反事实公平性：
- en: '[PRE33]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Print out the protected features and instances:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印受保护特征和实例：
- en: '[PRE34]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This results in the following output:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE35]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The next step is to compute the counterfactually unfair samples:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的步骤是计算反事实不公平的样本：
- en: '[PRE36]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The final step is to print the counterfactually unfair data points:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是打印反事实不公平的数据点：
- en: '[PRE37]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This generates the following output:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE38]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We have already studied what **disparate** **impact** is when looking at fairness.
    Now let's see with code examples how we can measure the three most common disparate
    impact measures.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了在研究公平性时，**不平等影响**的含义。现在，让我们通过代码示例来看如何衡量最常见的三种不平等影响度量。
- en: 'To evaluate the **equal accuracy** metric, we can use the following code snippet:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要评估**等精度**指标，我们可以使用以下代码片段：
- en: '[PRE39]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To evaluate the **equal opportunity** metric, we run the following:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要评估**等机会**指标，我们运行以下命令：
- en: '[PRE40]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To evaluate the **demographic parity** metric, we run this:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要评估**人口统计平等**指标，我们运行以下命令：
- en: '[PRE41]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here is the output we receive on running an evaluation of equal accuracy, equal
    opportunity, and demographic parity:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们在评估等精度、等机会和人口统计平等时得到的输出：
- en: '[PRE42]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As we have learned about the basics of fairness with examples, we should also
    find out how to apply these concepts to follow best practices.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了公平性的基本概念和示例之后，我们还应该了解如何应用这些概念来遵循最佳实践。
- en: Best practices
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践
- en: We have explored with different examples and code snippets the process by which
    we can evaluate different fairness metrics. However, before evaluation, we need
    to ensure that the data used for training our models proportionally represents
    all different demographic groups of the population. Furthermore, to address the
    issue of fairness, another important criterion is to ensure that your predictions
    are calibrated for each group ([https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3](https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3)).
    When model scores are not calibrated for each group, we may end up overestimating
    or underestimating the probability of outcomes for different groups. We may need
    to redesign thresholds and create separate models and decision boundaries for
    each group. This process will alleviate bias and enable fairer predictions for
    each of the groups than a single threshold.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过不同的示例和代码片段探索了如何评估不同的公平性指标。然而，在评估之前，我们需要确保用于训练模型的数据能够成比例地代表人口的所有不同群体。此外，为了解决公平性问题，另一个重要标准是确保你的预测对于每个群体都是经过校准的（[https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3](https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3)）。当模型得分未对每个群体进行校准时，我们可能会高估或低估不同群体的结果概率。我们可能需要重新设计阈值，为每个群体创建单独的模型和决策边界。这个过程将减少偏差，并使每个群体的预测比单一阈值更公平。
- en: Most of the time, we are not able to get data where all groups of the population
    are equally represented. In such situations, the best practice is to generate
    synthetic datasets by applying artificial methods, so that we not only train the
    model with equal representations of the different groups but are also able to
    validate the predicted outcomes against different thresholds customized for each
    group. This will eliminate representation bias and account for geographic diversity
    while creating such datasets.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，我们无法获得所有群体在数据中均等代表的情况。在这种情况下，最佳实践是通过应用人工方法生成合成数据集，以便我们不仅能用不同群体的均等代表来训练模型，还能根据为每个群体定制的不同阈值验证预测结果。这将消除表示偏差，并在创建数据集时考虑地理多样性。
- en: We also have certain tools, mentioned next, for evaluating fairness.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一些工具，接下来会提到，用于评估公平性。
- en: Bias mitigation toolkits
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差缓解工具包
- en: 'Here is a list of some tools:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些工具：
- en: '**Aequitas**: An open source bias and fairness audit toolkit that evaluates
    models for different types of bias and fairness metrics on multiple population
    sub-groups.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Aequitas**：一个开源的偏差和公平性审计工具包，评估模型在多个子群体中的不同类型的偏差和公平性指标。'
- en: '**Microsoft Fairlearn**: This toolkit takes the reduction approach to fair
    classification (e.g. binary classification) by leveraging constraints. The constraint-based
    approach reduces fair classification problems to a sequence of cost-sensitive
    classification problems. Under applied constraints, this yields a randomized classifier,
    with the lowest (empirical) error.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Fairlearn**：该工具包通过利用约束，采用公平分类（例如二分类）的减法方法。基于约束的方法将公平分类问题简化为一系列成本敏感的分类问题。在应用约束的情况下，生成一个随机分类器，并获得最低的（经验）误差。'
- en: '**The What-If Tool**: This open source TensorBoard web application is a toolkit
    provided by Google that allows you to view the counterfactuals to analyze an ML
    model. Users are then able to compare a given data point to the nearest data point
    that resembles it, yet for which the model predicts a different result. This tool
    also comes with the flexibility of adjusting the effects of different classification
    thresholds, along with the ability to tweak different numerical fairness criteria.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设分析工具**：这是一个开源的TensorBoard Web应用程序，由Google提供的工具包，可以让你查看反事实数据，从而分析机器学习模型。用户可以将给定数据点与最相似的另一个数据点进行对比，而该数据点的模型预测结果不同。此工具还具有灵活性，可以调整不同分类阈值的效果，并能够调整不同的数值公平性标准。'
- en: '**AI Fairness 360**: This toolkit, developed by IBM, contains an exhaustive
    set of fairness metrics for datasets and ML models. It comes with explainability
    for these metrics, and different algorithms to mitigate bias in datasets at the
    preprocessing and model training stages.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI公平性360**：这是由IBM开发的工具包，包含了一套全面的公平性度量标准，适用于数据集和机器学习模型。它提供了对这些度量标准的可解释性，并提供了多种算法，以在数据预处理和模型训练阶段减轻数据偏差。'
- en: We discuss some of these toolkits in later chapters.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续章节中讨论一些这些工具包。
- en: Now that we know about the different bias mitigation toolkits that are available,
    and how important auditing data and running frequent quality checks is, let's
    study synthetic datasets and how they can help in modeling fair ML problems.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们了解了可用的不同偏差缓解工具包，以及审计数据和频繁进行质量检查的重要性，让我们研究合成数据集以及它们如何帮助建模公平的机器学习问题。
- en: Fair synthetic datasets
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平的合成数据集
- en: By **synthetic data**, we mean data generated artificially from scratch, with
    statistical properties matching the original or base dataset that is ingested
    into the system. A synthetically generated dataset bears no relationship to the
    real subjects present in the original dataset.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓**合成数据**，是指从头开始人工生成的数据，其统计特性与输入到系统中的原始或基础数据集相匹配。合成生成的数据集与原始数据集中存在的真实对象没有任何关联。
- en: Modern research in **artificial intelligence** (**AI**) has led to innovations
    and the publication of advanced tools that can generate synthetic data in data-intensive
    environments. With the availability of better tools and techniques, fair, privacy-preserving
    synthetic data generation (with tabular data) has been widely adopted by organizations.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现代**人工智能**（**AI**）研究推动了创新，并发布了先进的工具，这些工具能够在数据密集型环境中生成合成数据。随着更好工具和技术的出现，公平的、保护隐私的合成数据生成（尤其是表格数据）已被组织广泛采纳。
- en: Synthetic data-generating algorithms are capable of ingesting real-time data
    and learning its features, different feature correlations, and patterns. They
    can then generate large quantities of artificial data that closely resembles the
    original data in terms of statistical properties and distributions. By and large,
    these newly generated datasets are also scalable, privacy-compliant, and can display
    all valuable insights, without violating data confidentiality rules. AI-generated
    synthetic data is widely used by financial and healthcare providers for scaling
    their AI solutions. Synthetic data has also been successful in generating robust
    replacements for missing and hard-to-acquire data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据生成算法能够实时获取数据并学习其特征、不同特征间的关联性以及模式。然后，它们可以生成大量人工数据，这些数据在统计特性和分布上与原始数据非常相似。总体而言，这些新生成的数据集也具备可扩展性、隐私合规性，并且能够展示所有有价值的见解，而不违反数据保密规则。人工智能生成的合成数据在金融和医疗行业被广泛应用，以扩展其人工智能解决方案。合成数据还成功地为缺失和难以获取的数据生成了强健的替代品。
- en: Sensitive data has often been kept confined to teams and departments. But with
    the implementation of synthetic-data-generation tools, it has become easier to
    establish collaboration with teams and third parties in a privacy-compliant manner
    using private synthetic copies of data. Synthetic data is great for improving
    ML algorithms that are impacted by bias or imbalances due to new, rare incoming
    data, which has a greater influence than historic data. This type of synthetic
    data finds application in determining fraudulent transactions, whose volume is
    < 5% of the overall transactions. Up-sampled synthetic datasets are not only capable
    of detecting bias in transactional data but can also comply with ethics and help
    ML models to yield better, fairer results for minority or disadvantaged groups.
    Hence, synthetic data serves as a very important component in the design of ethical
    ML pipelines. Fair synthetic data can remove societal and historical bias from
    the predictions of ML models.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感数据通常被限制在团队和部门内部。但随着合成数据生成工具的实施，使用私密的合成数据副本以隐私合规的方式与团队和第三方进行合作变得更加容易。合成数据非常适合改善受偏见或不平衡影响的机器学习算法，尤其是在新的、稀有的来访数据具有比历史数据更大影响的情况下。这类合成数据在确定欺诈交易中发挥作用，欺诈交易的比例通常小于整体交易的
    5%。上采样的合成数据集不仅能够检测交易数据中的偏见，还能遵循伦理原则，帮助机器学习模型为少数群体或弱势群体提供更好的、更公平的结果。因此，合成数据在设计伦理机器学习管道中起着至关重要的作用。公平的合成数据能够从机器学习模型的预测中消除社会和历史偏见。
- en: Now, let's study, with an example, a framework that generates private synthetic
    data by using realistic training samples, without disclosing the PII of individuals.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个例子来研究一个框架，该框架通过使用真实的训练样本生成私密合成数据，而不披露个人的 PII（个人身份信息）。
- en: MOSTLY AI’s self-supervised fair synthetic data generator
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MOSTLY AI 的自监督公平合成数据生成器
- en: This framework takes into consideration fairness constraints, in a self-supervised
    learning process, to simulate and generate large quantities of fair synthetic
    data. The framework is made following a generative model architecture that feeds
    in a dataset (in our example, the UCI Adult Census dataset) to get a better dataset
    that has a higher degree of fairness than the original dataset.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架在自监督学习过程中考虑了公平性约束，用以模拟和生成大量的公平合成数据。该框架采用生成模型架构，输入数据集（在我们的例子中是 UCI 成人人口普查数据集），以获得一个比原始数据集具有更高公平度的数据集。
- en: Furthermore, the resultant dataset preserves all the original relationships
    between the attributes but only controls the gender and racial biases present
    in the original data. The propensity scores of the predicted model outcomes demonstrate
    the fact of how fair synthetic data can alleviate bias when compared with the
    original dataset.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，结果数据集保留了属性之间的所有原始关系，但仅控制原始数据中的性别和种族偏见。预测模型结果的倾向评分显示了公平的合成数据与原始数据集相比，如何缓解偏见的事实。
- en: In ML, we often use generative deep neural networks that use new, synthetic
    samples (that are representations of the actual data) to optimize the accuracy
    loss of an ML model being trained. The loss can represent the similarity of the
    fitted function to the observed distributions of the real data. The process of
    generating fair, representative data involves an additional loss to the original
    loss function that can penalize any violation of the statistical parity.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常使用生成深度神经网络，这些网络使用新的合成样本（它们是实际数据的表示）来优化正在训练的机器学习模型的准确性损失。损失可以表示拟合函数与实际数据的观测分布的相似性。生成公平、代表性数据的过程需要对原始损失函数进行额外的损失，这可以惩罚任何违反统计平等的行为。
- en: '![  Figure 7.11 – Different parameters for training the minimax model](img/B18681_07_011.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![ 图 7.11 – 用于训练最小最大模型的不同参数](img/B18681_07_011.jpg)'
- en: Figure 7.11 – Different parameters for training the minimax model
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 用于训练最小最大模型的不同参数
- en: 'Hence, the algorithm for fair synthetic data generation involves optimizing
    a combined loss, which contains the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，公平合成数据生成算法涉及优化一个综合损失，其中包含以下内容：
- en: A weighted sum of the accuracy loss
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性损失的加权和
- en: A fairness loss that is proportional to the deviation from the empirically estimated
    statistical parity
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种与经验估计的统计平等偏差成正比的公平性损失
- en: The right trade-off between accuracy and fairness, where weights are shifted
    from one loss component to the other
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性与公平性之间的正确权衡，其中权重从一个损失组件转移到另一个
- en: '*Figure 7**.11* illustrates the step-by-step sequences of generating synthetic
    data and comparing model performance when models are trained on original datasets
    versus synthetic datasets.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.11*展示了生成合成数据的逐步过程，并比较了在原始数据集和合成数据集上训练模型时的模型表现。'
- en: We have seen how ML-based synthetic data generation is possible. Now, let's
    see how protected attributes such as gender and income in fair synthetic data
    help to satisfy some of the fairness definitions we studied before.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到基于机器学习的合成数据生成是如何实现的。接下来，让我们看看在公平合成数据中，像性别和收入等保护属性如何帮助满足我们之前研究的一些公平性定义。
- en: Influence on fairness statistics
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对公平性统计的影响
- en: We will leverage a generative deep neural network as a data synthesizer for
    tabular data to generate synthetic data samples from the `adult_income` dataset.
    The generative model synthesizer runs end to end 50 times with a parity fairness
    constraint and takes gender as a protected attribute. *Figure 7**.12* shows that
    the gender imbalance present in the original data within the high-income class
    is successfully mitigated by the synthetic data. The disparate impact (derived
    by comparing the difference and the high-income-male-to-high-income-female ratio)
    is observed to be roughly 10/30 = 0.33 in the original dataset, while it is recorded
    as 22/25 =0.88 in the bias-corrected synthetic dataset. We see that this metric
    with synthetic data is considerably higher than 0.80, which is the industry benchmark.
    We can safely conclude that data synthesis helps in mitigating bias.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用生成式深度神经网络作为表格数据的合成器，从`adult_income`数据集中生成合成数据样本。生成式模型合成器会在具有奇偶性公平约束的情况下运行50次，并将性别作为保护属性。*图
    7.12*显示，在原始数据的高收入类别中存在的性别不平衡被合成数据成功缓解。通过比较高收入男性与高收入女性比例得出的差异影响在原始数据集中大约为10/30
    = 0.33，而在经过偏差校正的合成数据集中为22/25 = 0.88。我们看到，合成数据的这一指标显著高于0.80，这是行业基准。因此我们可以放心地得出结论，数据合成有助于缓解偏差。
- en: We further observe that the additional parity constraint during model training
    did not reduce the data quality, as shown in *Figure 7**.12*.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步观察到，在模型训练过程中增加的额外公平约束并未降低数据质量，如*图 7.12*所示。
- en: '![Figure 7.12 – Synthetic data mitigating the income gap](img/B18681_07_012.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 合成数据缓解收入差距](img/B18681_07_012.jpg)'
- en: Figure 7.12 – Synthetic data mitigating the income gap
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 合成数据缓解收入差距
- en: Both the univariate and bivariate distributions demonstrate that the dataset
    is able to preserve both the population-wide male-to-female and high-earner-to-low-earner
    ratios. Only the statistical parity constraint signifying the dependence of income
    and sex has been allowed to be violated to make them un-correlated. With representative
    and synthetic data, this correlation is reduced to the noise level evident in
    the following figure (see the red circle on the light green figure).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量和双变量分布都表明，数据集能够保持人口层面的男女比例和高收入者与低收入者比例。只有统计公平约束，表示收入和性别的依赖关系，被允许违背，使得它们变得不相关。使用代表性和合成数据后，这种相关性被减少到噪音水平，如下图所示（见图中浅绿色区域的红圈）。
- en: '![Figure 7.13 – Univariate versus bivariate statistics for original and synthetic
    data](img/B18681_07_013.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 原始数据与合成数据的单变量与双变量统计](img/B18681_07_013.jpg)'
- en: Figure 7.13 – Univariate versus bivariate statistics for original and synthetic
    data
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 原始数据与合成数据的单变量与双变量统计
- en: 'Studying proxy attributes further shows that they do not introduce unfairness
    through a backdoor. Even the addition of a strongly correlated artificial feature
    (correlated with gender) named “proxy” is found to exhibit a constant correlation
    with “sex” in the original dataset, which is considerably reduced with the introduction
    of parity constraints due to induced fairness. The female section shows that “proxy”
    equals 1 in 90% of cases and equals 0 for the remaining 10%. *Figure 7**.14* further
    illustrates this:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 研究代理属性进一步表明，它们并未通过“后门”引入不公平。即便是加入一个与性别高度相关的人工特征（名为“代理”），也发现它在原始数据集中与“性别”保持恒定的相关性，而通过引入奇偶性约束，由于促成了公平性，这一相关性得到了显著降低。女性部分显示，“代理”在90%的情况下等于1，剩余10%的情况下等于0。*图
    7.14*进一步说明了这一点：
- en: '![Figure 7.14 – The parity fairness constraint holds good for proxy attributes
    such as gender](img/B18681_07_014.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 奇偶性公平约束在性别等代理属性上保持有效](img/B18681_07_014.jpg)'
- en: Figure 7.14 – The parity fairness constraint holds good for proxy attributes
    such as gender
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 奇偶性公平性约束适用于性别等代理属性
- en: For the Adult dataset, the generative model is trained by applying fairness
    constraints on “gender” and “race.” Furthermore, it is evident from *Figure 7**.15*
    that the synthetic data exhibits highly balanced high-income ratios across all
    four groups by race and gender. Even though the solution is not able to guarantee
    complete parity, it can be obtained (when the difference further diminishes) by
    assigning a higher weight to the fairness loss in comparison to the accuracy loss.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 对于成人数据集，通过在“性别”和“种族”上应用公平性约束来训练生成模型。此外，从*图 7.15*可以明显看出，合成数据在所有四个按种族和性别划分的组别中，高收入比例高度平衡。尽管该解决方案无法保证完全的奇偶性，但通过将公平性损失的权重相对于准确性损失的权重设置更高，可以在差异进一步缩小时获得完整的公平性。
- en: '![Figure 7.15 – Constraint-based bias mitigation by accounting for gender and
    race, respectively](img/B18681_07_015.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 通过分别考虑性别和种族的约束进行偏差缓解](img/B18681_07_015.jpg)'
- en: Figure 7.15 – Constraint-based bias mitigation by accounting for gender and
    race, respectively
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 通过分别考虑性别和种族的约束进行偏差缓解
- en: In *Figure 7**.16*, we see the propensity scores of the corresponding predictive
    models, on both the original dataset and the synthetic dataset.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.16*中，我们看到相应预测模型的倾向得分，分别在原始数据集和合成数据集上展示。
- en: '![Figure 7.16 – Prediction of high income for both the original and synthetic
    datasets](img/B18681_07_016.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 对原始数据集和合成数据集的高收入预测](img/B18681_07_016.jpg)'
- en: Figure 7.16 – Prediction of high income for both the original and synthetic
    datasets
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 对原始数据集和合成数据集的高收入预测
- en: Using the same ML model, we find that the model fitted on the original dataset
    exhibits a much lower probability score for women being in the high-income class
    when compared with the opposite gender – men. Hence, the discrepancy with synthetic
    data is reduced and both distributions driven by the gender attribute are found
    to align. As the training process of the predictive model does not include any
    model optimization through fairness, it is obvious that the predicted outcome
    is largely due to using bias-corrected synthetic data. A fairness-constrained
    synthetic data solution ensures group fairness. The dataset maintains the relationship
    between other attributes and at the same time removes dependencies between sensitive
    and target attributes. Furthermore, the parity constraint also ensures fairness
    for hidden proxy attributes.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的机器学习模型，我们发现，原始数据集上拟合的模型显示女性进入高收入类别的概率得分明显低于对立性别——男性。因此，合成数据的差异被缩小，且由性别属性驱动的两个分布被发现一致。由于预测模型的训练过程没有通过公平性优化任何模型，因此显而易见，预测结果主要是由于使用了经过偏差修正的合成数据。一个基于公平性约束的合成数据解决方案确保了群体公平性。数据集保持了其他属性之间的关系，同时消除了敏感属性和目标属性之间的依赖关系。此外，奇偶性约束还确保了对隐藏代理属性的公平性。
- en: 'The following steps illustrate step by step how to generate fair synthetic
    data to equally represent males and females:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤逐步演示如何生成公平的合成数据，以平等地表示男性和女性：
- en: 'We will set up a specified set of hyperparameters (including input training
    data size, layer sizes, and classifier types) that we will train using synthetic
    data:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将设置一组特定的超参数（包括输入训练数据大小、层大小和分类器类型），并使用合成数据进行训练：
- en: '[PRE43]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Our next task is to set up a classifier that will train the network using the
    hyperparameters used in the previous state. While training the dataset, we use
    the validation dataset to validate the training process:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一个任务是设置一个分类器，使用前述状态中使用的超参数训练网络。在训练数据集的过程中，我们使用验证数据集来验证训练过程：
- en: '[PRE44]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `construct_classifier` function is set inside the `fariml-farm` library
    to construct the classifier, as given in the following code snippet:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`construct_classifier` 函数被设置在 `fariml-farm` 库中，用于构建分类器，如以下代码片段所示：'
- en: '[PRE45]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Our next job is to enable an equivalent representation of male (`1`) and female
    (`0`) points through the synthetic data generation process:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一个任务是通过合成数据生成过程，使男性（`1`）和女性（`0`）点的表示等效：
- en: '[PRE46]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The classifier is optimized using a loss function, given by the following:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器使用以下给定的损失函数进行优化：
- en: '[PRE47]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The overall loss is the sum of losses, computed by `dpe`), `fnpe`), `fppe`),
    and `cpe`). The loss function evaluated for `dpe` represents the loss due to `demographic_parity_discrimination`,
    while `fnpe` and `fppe` represent the loss due to `equalized_odds_discrimination`.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 总损失是通过`dpe`、`fnpe`、`fppe`和`cpe`计算的各项损失之和。为`dpe`评估的损失函数表示由`demographic_parity_discrimination`（人口平衡歧视）引起的损失，而`fnpe`和`fppe`则表示由`equalized_odds_discrimination`（均等机会歧视）引起的损失。
- en: 'Now if we try to plot the embeddings, we can use the following code snippet,
    which generates an almost equivalent representation of embeddings between male
    and female with an annual income >= 50K or < 50K:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们尝试绘制嵌入图，我们可以使用以下代码片段，这将生成男性和女性群体年收入>=50K或<50K之间几乎等效的嵌入表示：
- en: '[PRE48]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This produces the following output:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 7.17 – An equivalent representation of embeddings between the male
    and female populations with an annual income >= 50K or < 50K](img/B18681_07_017.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – 男性和女性群体年收入>=50K或<50K之间嵌入的等效表示](img/B18681_07_017.jpg)'
- en: Figure 7.17 – An equivalent representation of embeddings between the male and
    female populations with an annual income >= 50K or < 50K
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 男性和女性群体年收入>=50K或<50K之间嵌入的等效表示
- en: We learned how MOSTLY AI’s fairness GANs are capable of generating synthetic
    datasets that have a positive impact on fairness statistics. Let's explore a GAN-based
    approach that uses a **directed acyclic graph** (**DAG**) to generate fair synthetic
    data.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了MOSTLY AI的公平性GAN如何生成对公平统计产生积极影响的合成数据集。接下来，让我们探讨一种基于GAN的方法，该方法利用**有向无环图**（**DAG**）来生成公平的合成数据。
- en: A GAN-based fair synthetic data generator
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于GAN的公平合成数据生成器
- en: Now let's see with an example how we can use **causally aware generative networks**
    to generate fair, unbiased synthetic data. The objective is to generate an unbiased
    dataset with no loss of the representation of the data distribution. The data-generation
    process can be further modeled by a DAG. Here, the ML model (a single model or
    a cascade of models) being trained on synthetic data not only gives unbiased predictions
    (satisfying the fairness criteria) on synthetic data but is also capable of yielding
    unbiased predictions on real-life available datasets.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个例子来看看如何使用**因果感知生成网络**生成公平、无偏的合成数据。目标是生成一个无偏的数据集，并且不丢失数据分布的表示。数据生成过程可以通过有向无环图（DAG）进一步建模。在这里，训练合成数据的机器学习模型（无论是单个模型还是一系列模型）不仅能对合成数据做出无偏的预测（满足公平性标准），还能够对现实世界的数据集做出无偏的预测。
- en: The **Debiasing Causal Fairness** (**DECAF**) framework, based on GAN, explores
    the principle of causal structure for data synthesis by employing **d** generators
    (each generator being assigned to each variable) to learn about the causal conditionals
    present in the data. The **data-generation process** (**DGP**) embedded within
    DECAF allows variables to be re-engineered and regenerated based on their origin,
    which is the causal parents, through the input layers of the generator. A framework
    like this can remove bias from biased real-world datasets (datasets that under-represent
    minority groups) with inherent bias and real-world datasets where bias has been
    synthetically introduced. Furthermore, DECAF promises to offer protection for
    confidential information through private synthetic data generation processes by
    swapping out the standard discriminator for a differentially private discriminator.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**去偏因果公平性**（**DECAF**）框架基于GAN，利用**d**个生成器（每个生成器分配给每个变量）通过因果结构的原理进行数据合成，以学习数据中存在的因果条件。DECAF中嵌入的**数据生成过程**（**DGP**）允许根据变量的来源（即因果父节点）通过生成器的输入层重新构建和再生变量。像这样的框架可以从有偏的现实世界数据集中去除偏差（例如，低估少数群体的群体），以及那些已被人为引入偏差的现实世界数据集。此外，DECAF还承诺通过差分隐私鉴别器替换标准鉴别器，为机密信息提供保护，从而实现私有合成数据生成过程。'
- en: 'The discriminator and generator run the optimization process in successive
    iterations by adding a regularization loss to both networks. The optimization
    process uses gradient descent and guarantees the same convergence criteria as
    standard GANs. The framework involves the following:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器和生成器通过在连续的迭代中向两个网络添加正则化损失，进行优化过程。优化过程使用梯度下降，并保证与标准GAN相同的收敛标准。该框架包括以下内容：
- en: A data-generating distribution function that satisfies Markov compatibility.
    For a known DAG (*G*), if we find that each node represents a variable in a probability
    distribution (*P*), it is said to be Markov-compatible if each variable *X* present
    in the DAG is independent of all its non-descendants.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 满足马尔可夫兼容性的生成数据分布函数。对于已知的有向无环图（*G*），如果我们发现每个节点代表一个概率分布中的变量（*P*），则当图中的每个变量 *X*
    与其非后代节点独立时，称其为马尔可夫兼容。
- en: Enough capacity for both the generator *G* and discriminator *D*.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器 *G* 和鉴别器 *D* 都需要足够的能力。
- en: Every training iteration successfully optimizes a given DAG *G* and correspondingly
    updates it.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次训练迭代都成功优化给定的有向无环图（*G*）并相应地更新它。
- en: Maximized discriminator loss.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化鉴别器损失。
- en: A distribution function generated by the generator that gets optimized to converge
    it on the true data distribution.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器生成的分布函数，通过优化使其收敛到真实数据分布。
- en: During the training stage, DECAF learns about the causal conditionals that are
    present in the data with a GAN. The GAN is equipped to learn about causal conditions
    in the DAG between the source and destination nodes. At the generation (inference)
    stage, the framework functions by applying three fundamental principles, **CF
    debiasing**, **FTU debiasing**, and **DP debiasing**, which help the generator
    to create fair data. However, it is also assumed that the DGP’s graph *G* is known
    from before.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，DECAF通过GAN学习数据中存在的因果条件。GAN能够学习源节点与目标节点之间因果图中的因果条件。在生成（推理）阶段，框架通过应用三个基本原理来运行，**CF去偏**、**FTU去偏**和**DP去偏**，这些有助于生成器创造公平的数据。然而，还假设在此之前已知数据生成过程（DGP）的图
    *G*。
- en: During inference, the variable synthesis process originates from the root nodes,
    then propagates down to their children (from the generated causal parents in the
    causal graph). The topological process of data synthesis is dependent on the sequential
    data generation technique, which terminates at the leaf nodes. This enables the
    DECAF algorithm to remove bias strategically at inference time by enforcing targeted/biased
    edge removal and further increases the probability of meeting the user-defined
    fairness requirements. DECAF is also found to satisfy the fairness/discrimination
    definitions we discussed earlier and at the same time maintains the high utility
    of generated data so that it can be effectively used by ML models without creating
    algorithmic bias.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，变量合成过程源自根节点，然后传播到它们的子节点（来自因果图中生成的因果父节点）。数据合成的拓扑过程依赖于顺序数据生成技术，并在叶节点处终止。这使得DECAF算法能够在推理时通过执行针对性的/有偏边缘移除策略，战略性地去除偏差，并进一步增加满足用户定义公平性要求的概率。研究还发现，DECAF能够满足我们之前讨论的公平性/歧视定义，同时保持生成数据的高效用性，使其能够被机器学习模型有效使用而不产生算法偏差。
- en: '**Fairness through unawareness** (**FTU**) is measured by keeping all other
    features constant and computing the distinctions between the predictions of a
    downstream classifier, when the classifier’s predictions are either 1 and 0, respectively,
    such that *| PA =* 0 (*Y*ˆ*|X*)–– *PA =* 1(*Y*ˆ*|X*) |. This metric helps to evaluate
    the direct impact of *A* on the predicted outcomes. This metric aims to eliminate
    disparate treatment, which is legally related to direct discrimination, and strives
    to provide the same opportunity to any two equally qualified people, independent
    of their race, gender, or other protected attributes.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过无知实现公平**（**FTU**）是通过保持所有其他特征不变，并计算下游分类器预测之间的差异来衡量的，当分类器的预测分别为1和0时，公式为 *|
    PA =* 0 (*Y*ˆ*|X*)–– *PA =* 1(*Y*ˆ*|X*) |。该度量帮助评估 *A* 对预测结果的直接影响。该度量旨在消除不同的对待，这在法律上与直接歧视相关，并力求为任何两位同等资质的人提供相同的机会，无论他们的种族、性别或其他受保护属性如何。'
- en: '**Demographic parity** (**DP**) is measured in terms of the total variation,
    which signifies and explains the differences between the predictions of a downstream
    classifier. This helps to compute the positive-to-negative ratio between varying
    categories (African, American, Asian, and Hispanic, for instance) of a protected
    variable *A*:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '**人口平衡**（**DP**）是通过总变差来衡量的，这表示并解释下游分类器预测之间的差异。这有助于计算受保护变量 *A* 不同类别（例如非洲裔、美国裔、亚洲裔和西班牙裔）之间的正负比率：'
- en: '*| P* (*Y*ˆ *|A = 0*)*–− P* (*Yˆ |A =* *1*) *|*'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '*| P* (*Y*ˆ *|A = 0*)*–− P* (*Yˆ |A =* *1*) *|*'
- en: This metric does not allow any indirect discrimination unless otherwise provided
    by explainability factors.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标不允许任何间接歧视，除非可解释性因素另有说明。
- en: '**Conditional fairness** (**CF**) aims to generalize both FTU and DP.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件公平性**（**CF**）旨在将 FTU 和 DP 进行泛化。'
- en: 'All DECAF methods yield good data quality metrics: precision, recall, and AUROC.
    One good fair data generation mechanism that deserves special mention is DECAF-DP,
    which performs best across all five of the evaluation metrics (precision, recall,
    AUROC, DP, and FTU) and has better DP performance results, even when the dataset
    exhibits high bias.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 DECAF 方法都能生成良好的数据质量指标：精准率、召回率和 AUROC。一个值得特别提及的优秀公平数据生成机制是 DECAF-DP，它在所有五个评估指标（精准率、召回率、AUROC、DP
    和 FTU）中表现最佳，并且在数据集表现出高偏差时，仍能提供更好的 DP 性能结果。
- en: '![Figure 7.18 – Training phase in DECAF](img/B18681_07_018.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.18 – DECAF 中的训练阶段](img/B18681_07_018.jpg)'
- en: Figure 7.18 – Training phase in DECAF
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – DECAF 中的训练阶段
- en: The training phase from the preceding figure directly follows the inference
    phase in the following diagram.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 前一图中的训练阶段直接跟随以下图中的推理阶段。
- en: '![Figure 7.19 – Fairness inference phase enabling fair data synthesis in DECAF](img/B18681_07_019.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.19 – 公平推理阶段，启用 DECAF 中的公平数据合成](img/B18681_07_019.jpg)'
- en: Figure 7.19 – Fairness inference phase enabling fair data synthesis in DECAF
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 公平推理阶段，启用 DECAF 中的公平数据合成
- en: 'Now let''s see how we can generate synthetic data with DECAF using the following
    code snippets:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用以下代码片段通过 DECAF 生成合成数据：
- en: 'The following code is for all necessary imports:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码包含所有必要的导入：
- en: '[PRE49]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, after setting up the necessary imports, let''s set up the DAG structure
    using `dag_seed`. The causal structure of the graph is stored in `dag_seed`:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在设置必要的导入后，让我们使用`dag_seed`设置 DAG 结构。图的因果结构存储在`dag_seed`中：
- en: '[PRE50]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The next step is to have `DiGraph` invoked with `dag_seed`, using the following
    code snippet. The `dag` structure is stored in the `dag_seed` variable. The edge
    removal is stored in the `bias_dict` variable. Here, we have removed the edge
    between 3 and 6\. Furthermore, `gen_data_nonlinear` is used to apply a perturbation
    at each node:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用以下代码片段调用`DiGraph`，并使用`dag_seed`。`dag`结构存储在`dag_seed`变量中。边缘移除存储在`bias_dict`变量中。在这里，我们移除了
    3 和 6 之间的边缘。此外，`gen_data_nonlinear`用于在每个节点应用扰动：
- en: '[PRE51]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the next stage, we set up the different hyperparameters required to facilitate
    the training process:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一阶段，我们设置不同的超参数，以促进训练过程：
- en: '[PRE52]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The weight decay is initialized, which is used by AdamW, an optimizer that
    is an improved version of **Adam (Adaptive Moment Estimation**) and is capable
    of yielding better models with a faster training speed using stochastic gradient
    descent. AdamW has a weight decay functionality that can be used to regularize
    all network weights:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重衰减已初始化，AdamW 是一个优化器，它是**Adam（自适应矩估计）**的改进版，能够通过随机梯度下降以更快的训练速度生成更好的模型。AdamW
    具有权重衰减功能，可以用来对所有网络权重进行正则化：
- en: '[PRE53]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The next step is to set up the proportion of points to generate, which is negative
    for sequential sampling:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是设置生成点的比例，对于顺序采样来说是负值：
- en: '[PRE54]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: WGAN-GP
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN-GP
- en: 'You can read more about the Wasserstein GAN with gradient penalty (**WGAN-GP**)
    here: [https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490](https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490).'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于 Wasserstein GAN 与梯度惩罚（**WGAN-GP**）的内容：[https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490](https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)。
- en: 'Now let''s initialize and train the DECAF model by setting the different hyperparameters,
    the generator, and the regularization parameters:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们通过设置不同的超参数、生成器和正则化参数来初始化并训练 DECAF 模型：
- en: '[PRE55]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We have now generated synthetic unbiased data that can be fed to any ML model.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经生成了可以输入任何机器学习模型的合成无偏数据。
- en: Summary
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned about the importance of fairness algorithms
    in resolving societal bias, as different types of bias hinder the ability of an
    ML model to yield a fair outcome. We had a deep dive to learn about all the different
    types of fairness and how they can be practically applied to perform data quality
    checks, discover conditional dependencies and attribute relationships, and generate
    audit reports. In the process, we looked at how bias may appear in the data itself
    and/or in the outcome of predictive models. Furthermore, we took the first step
    in learning about fair synthetic data generation processes that can help in removing
    bias.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了公平性算法在解决社会偏见中的重要性，因为不同类型的偏见会妨碍机器学习模型得出公平结果的能力。我们深入探讨了各种公平性类型，并学习了如何将它们应用于数据质量检查、发现条件依赖关系和属性关系以及生成审计报告的实践过程中。在此过程中，我们探讨了偏见如何出现在数据本身和/或预测模型的结果中。此外，我们还迈出了学习公平合成数据生成过程的第一步，旨在帮助消除偏见。
- en: In later chapters, we will learn more about the mechanisms of applying fairness-aware
    models on diverse datasets, or diverse groups within a population. In addition,
    we will also look at the proper selection of protected attributes (such as gender,
    race, and others) based on the domain space of the problem and the dataset under
    study. In our next chapter, [*Chapter 8*](B18681_08.xhtml#_idTextAnchor176), *Fairness
    in Model Training and Optimization*, we will learn more about creating constrained
    optimization functions that can help in building fair ML models.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们将进一步学习如何在多样化数据集或群体中应用公平意识模型的机制。此外，我们还将探讨如何根据问题领域和所研究数据集的特点，选择适当的受保护属性（如性别、种族等）。在下一章，[*第8章*](B18681_08.xhtml#_idTextAnchor176)，*模型训练与优化中的公平性*，我们将深入了解如何创建受约束的优化函数，以帮助构建公平的机器学习模型。
- en: Further reading
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '*Fairness Definitions Explained* in *Proceedings of the International Workshop
    on Software Fairness (FairWare ‘18), Association for Computing Machinery*, Verma
    Sahil and Julia Rubin (2018), [https://fairware.cs.umass.edu/papers/Verma.pdf](
    https://fairware.cs.umass.edu/papers/Verma.pdf )'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《公平性定义解析》*，发表于*《软件公平性国际研讨会论文集》（FairWare ‘18），计算机协会*，Verma Sahil和Julia Rubin（2018年），
    [https://fairware.cs.umass.edu/papers/Verma.pdf](https://fairware.cs.umass.edu/papers/Verma.pdf)'
- en: '*A survey on datasets for fairnessaware-machine learning. Wiley Interdisciplinary
    Reviews: Data Mining and Knowledge Discovery.* *10.1002/widm.1452,* [https://www.researchgate.net/publication/355061634_A_survey_on_datasets_for_fairness-aware_machine_learning](https://www.researchgate.net/publication/355061634_A_survey_on_datasets_for_fairness-aware_machine_learning
    )'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《公平性意识机器学习数据集调查》，《Wiley跨学科评论：数据挖掘与知识发现》*，*10.1002/widm.1452*，[https://www.researchgate.net/publication/355061634_A_survey_on_datasets_for_fairness-aware_machine_learning](https://www.researchgate.net/publication/355061634_A_survey_on_datasets_for_fairness-aware_machine_learning)'
- en: '*Representative & Fair Synthetic Data*, Tiwald, Paul & Ebert, Alexandra & Soukup,
    Daniel. (2021), [https://arxiv.org/pdf/2104.03007.pdf](https://arxiv.org/pdf/2104.03007.pdf
    )'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《代表性与公平合成数据》*，Tiwald, Paul & Ebert, Alexandra & Soukup, Daniel.（2021年）， [https://arxiv.org/pdf/2104.03007.pdf](https://arxiv.org/pdf/2104.03007.pdf)'
- en: '*Minimax Group Fairness: Algorithms and Experiments. Proceedings of the 2021
    AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery,
    New York*. Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and
    Aaron Roth. 2021\. [https://assets.amazon.science/9d/a9/e085008e45b2b32b213786ac0149/minimax-group-fairness-algorithms-and-experiments.pdf](https://assets.amazon.science/9d/a9/e085008e45b2b32b213786ac0149/minimax-group-fairness-algorithms-and-experiments.pdf
    )'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《最小最大群体公平性：算法与实验》*，《2021年AAAI/ACM人工智能、伦理与社会会议论文集》，计算机协会，纽约。Emily Diana、Wesley
    Gill、Michael Kearns、Krishnaram Kenthapadi和Aaron Roth（2021年）。[https://assets.amazon.science/9d/a9/e085008e45b2b32b213786ac0149/minimax-group-fairness-algorithms-and-experiments.pdf](https://assets.amazon.science/9d/a9/e085008e45b2b32b213786ac0149/minimax-group-fairness-algorithms-and-experiments.pdf)'
- en: '*A Survey on Bias and Fairness in Machine Learning. ACM Comput. Surv. 54, 6,
    Article 115 (July 2022)*, Mehrabi Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina
    Lerman, and Aram Galstyan. (2021). [https://arxiv.org/pdf/1908.09635.pdf](https://arxiv.org/pdf/1908.09635.pdf
    )'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《机器学习中的偏见与公平性调查》，ACM计算机评论，54卷，第6期，第115号文章（2022年7月）*，Mehrabi Ninareh、Fred Morstatter、Nripsuta
    Saxena、Kristina Lerman和Aram Galstyan.（2021年）。 [https://arxiv.org/pdf/1908.09635.pdf](https://arxiv.org/pdf/1908.09635.pdf)'
- en: '*FAT Forensics: A Python Toolbox for Algorithmic Fairness, Accountability and
    Transparency*, Sokol, K., Santos-Rodríguez, R., & Flach, P.A. (2019). [https://arxiv.org/pdf/1909.05167.pdf](https://arxiv.org/pdf/1909.05167.pdf
    )'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FAT Forensics: 一个用于算法公平性、问责制和透明度的 Python 工具箱*，Sokol, K., Santos-Rodríguez,
    R., & Flach, P.A.（2019）。[https://arxiv.org/pdf/1909.05167.pdf](https://arxiv.org/pdf/1909.05167.pdf)'
- en: '*Minimax Pareto Fairness: A Multi Objective Perspective*, Martinez Natalia,
    Martin Bertran, Guillermo Sapiro, [http://proceedings.mlr.press/v119/martinez20a/martinez20a.pdf](http://proceedings.mlr.press/v119/martinez20a/martinez20a.pdf)'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Minimax 帕累托公平性：一种多目标视角*，Martinez Natalia, Martin Bertran, Guillermo Sapiro，[http://proceedings.mlr.press/v119/martinez20a/martinez20a.pdf](http://proceedings.mlr.press/v119/martinez20a/martinez20a.pdf)'
