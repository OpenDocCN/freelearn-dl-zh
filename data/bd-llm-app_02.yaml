- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: LLMs for AI-Powered Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs用于AI驱动应用程序
- en: 'In *Chapter 1*, *Introduction to Large Language Models*, we introduced **large
    language models** (**LLM**s) as powerful foundation models with generative capabilities
    as well as powerful common-sense reasoning. Now, the next question is: what should
    I do with those models?'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*大型语言模型简介*中，我们将**大型语言模型**（**LLM**s）介绍为具有生成能力和强大常识推理能力的强大基础模型。现在，下一个问题是：我应该用这些模型做什么？
- en: In this chapter, we are going to see how LLMs are revolutionizing the world
    of software development, leading to a new era of AI-powered applications. By the
    end of this chapter, you will have a clearer picture of how LLMs can be embedded
    in different application scenarios, thanks to the new AI orchestrator frameworks
    that are populating the market of AI development.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到LLMs如何彻底改变软件开发的世界，引领AI驱动应用程序的新时代。到本章结束时，你将更清楚地了解如何利用新的AI编排器框架将LLMs嵌入到不同的应用程序场景中，这些框架正在充斥着AI开发市场。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: How LLMs are changing software development
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs如何改变软件开发
- en: The copilot system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码伴侣系统
- en: Introducing AI orchestrators to embed LLMs into applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍AI编排器以将LLMs嵌入到应用程序中
- en: How LLMs are changing software development
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs如何改变软件开发
- en: 'LLMs have proven to have extraordinary capabilities: from natural language
    understanding tasks (summarization, named entity recognition, and classification)
    to text generation, from common-sense reasoning to brainstorming skills. However,
    they are not just incredible by themselves. As discussed in *Chapter 1*, LLMs
    and, generally speaking, **large foundation models** (**LFMs**), are revolutionizing
    software development by serving as platforms for building powerful applications.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs已被证明具有非凡的能力：从自然语言理解任务（摘要、命名实体识别和分类）到文本生成，从常识推理到头脑风暴技能。然而，它们并不仅仅本身令人难以置信。正如*第一章*中讨论的，LLMs以及一般而言的**大型基础模型**（**LFMs**），通过作为构建强大应用程序的平台，正在彻底改变软件开发。
- en: In fact, instead of starting from scratch, today developers can make API calls
    to a hosted version of an LLM, with the option of customizing it for their specific
    needs, as we saw in the previous chapter. This shift allows teams to incorporate
    the power of AI more easily and efficiently into their applications, similar to
    the transition from single-purpose computing to time-sharing in the past.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们不必从头开始，今天的开发者可以通过调用LLM的托管版本来调用API，并可以选择根据上一章中看到的方式进行定制。这种转变使得团队能够更轻松、更有效地将AI的力量整合到他们的应用程序中，类似于从专用计算到过去的时间共享的转变。
- en: 'But what does it mean, concretely, to incorporate LLMs within applications?
    There are two main aspects to consider when incorporating LLMs within applications:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但在应用程序中具体如何整合LLMs呢？在整合LLMs时，有两个主要方面需要考虑：
- en: '**The technical aspect**, which covers the *how*. Integrating LLMs into applications
    involves embedding them through REST API calls and managing them with AI orchestrators.
    This means setting up architectural components that allow seamless communication
    with the LLMs via API calls. Additionally, using AI orchestrators helps to efficiently
    manage and coordinate the LLMs’ functionality within the application, as we will
    discuss later in this chapter.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术方面**，涵盖了*如何*。将LLMs集成到应用程序中涉及通过REST API调用嵌入它们，并通过AI编排器管理它们。这意味着设置允许通过API调用无缝与LLMs通信的架构组件。此外，使用AI编排器有助于在应用程序中高效地管理和协调LLMs的功能，正如我们将在本章后面讨论的那样。'
- en: '**The conceptual aspect**, which covers the *what*. LLMs bring a plethora of
    new capabilities that can be harnessed within applications. These capabilities
    will be explored in detail later in this book. One way to view LLMs’ impact is
    by considering them as a new category of software, often referred to as *copilot*.
    This categorization highlights the significant assistance and collaboration provided
    by LLMs in enhancing application functionalities.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念方面**，涵盖了*什么*。LLMs带来了大量可以在应用程序中利用的新能力。这些能力将在本书的后续章节中详细探讨。一种看待LLMs影响的方式是将它们视为一种新的软件类别，通常被称为*代码伴侣*。这种分类突出了LLMs在增强应用程序功能方面提供的重大协助和协作。'
- en: We will delve into the technical aspect later on in this chapter, while the
    next section will cover a brand-new category of software – the copilot system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面深入探讨技术方面，而下一节将涵盖一个全新的软件类别——代码伴侣系统。
- en: The copilot system
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副驾驶系统
- en: The copilot system is a new category of software that serves as an expert helper
    to users trying to accomplish complex tasks. This concept was coined by Microsoft
    and has already been introduced into its applications, such as M365 Copilot and
    the new Bing, now powered by GPT-4\. With the same framework that is used by these
    products, developers can now build their own copilots to embed within their applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 副驾驶系统是一种新的软件类别，它作为用户完成复杂任务的专家助手。这一概念由微软提出，并已应用于其应用程序中，例如M365副驾驶和新的Bing，现在由GPT-4提供支持。使用这些产品相同的框架，开发者现在可以构建自己的副驾驶并将其嵌入到他们的应用程序中。
- en: But what exactly is a copilot?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但副驾驶究竟是什么？
- en: As the name suggests, copilots are meant to be AI assistants that work side
    by side with users and support them in various activities, from information retrieval
    to blog writing and posting, from brainstorming ideas to code review and generation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，副驾驶旨在成为与用户并肩工作的AI助手，支持他们在各种活动中，从信息检索到博客写作和发布，从头脑风暴到代码审查和生成。
- en: 'The following are some unique features of copilots:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是副驾驶的一些独特特性：
- en: '**A copilot is powered by LLMs**, or, more generally, LFMs, meaning that these
    are the reasoning engines that make the copilot “intelligent.” This reasoning
    engine is one of its components, but not the only one. A copilot also relies on
    other technologies, such as apps, data sources, and user interfaces, to provide
    a useful and engaging experience for users. The following illustration shows how
    this works:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副驾驶由LLM驱动**，或者更普遍地说，由LFM驱动，这意味着这些是使副驾驶“智能”的推理引擎。这个推理引擎是其组件之一，但不是唯一的。副驾驶还依赖于其他技术，如应用程序、数据源和用户界面，以向用户提供有用且引人入胜的体验。以下插图显示了这是如何工作的：'
- en: '![A cartoon of a person  Description automatically generated](img/B21714_02_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![一个人物的卡通  描述自动生成](img/B21714_02_01.png)'
- en: 'Figure 2.1: A copilot is powered by an LLM'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：副驾驶由LLM驱动
- en: '**A copilot is designed to have a conversational user interface,** allowing
    users to interact with it using natural language. This reduces or even eliminates
    the knowledge gap between complex systems that need domain-specific taxonomy (for
    example, querying tabular data needs the knowledge of programming languages such
    as T-SQL) and users. Let’s look at an example of such a conversation:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副驾驶被设计成具有会话式用户界面**，允许用户使用自然语言与之交互。这减少了甚至消除了需要特定领域分类法（例如，查询表格数据需要了解编程语言如T-SQL）的复杂系统与用户之间的知识差距。让我们看看这样一个对话的例子：'
- en: '![](img/B21714_02_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_02.png)'
- en: 'Figure 2.2: An example of a conversational UI to reduce the gap between the
    user and the database'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：一个用于减少用户与数据库之间差距的对话式UI示例
- en: '**A copilot has a scope.** This means that it is **grounded** to domain-specific
    data so that it is entitled to answer only within the perimeter of the application
    or domain.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副驾驶有一个作用域**。这意味着它被扎根于特定领域的数据，因此它只能在该应用程序或领域的范围内回答。'
- en: '**Definition**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Grounding is the process of using LLMs with information that is use case specific,
    relevant, and not available as part of the LLM’s trained knowledge. It is crucial
    for ensuring the quality, accuracy, and relevance of the output. For example,
    let’s say you want an LLM-powered application that assists you during your research
    on up-to-date papers (not included in the training dataset of your LLM). You also
    want your app to only respond if the answer is included in those papers. To do
    so, you will need to ground your LLM to the set of papers, so that your application
    will only respond within this perimeter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 扎根是通过使用LLM与特定用例的信息，这些信息是使用案例特定的、相关的，并且不是LLM训练知识的一部分。这对于确保输出质量、准确性和相关性至关重要。例如，假设你想要一个由LLM驱动的应用程序，在研究最新的论文（不包括在你的LLM训练数据集中）时帮助你。你还希望你的应用程序只有在答案包含在这些论文中时才做出回应。为此，你需要将你的LLM扎根于这些论文的集合，这样你的应用程序就只能在这个范围内做出回应。
- en: Grounding is achieved through an architectural framework called retrieval-augmented
    generation (RAG), a technique that enhances the output of LLMs by incorporating
    information from an external, authoritative knowledge base before generating a
    response. This process helps to ensure that the generated content is relevant,
    accurate, and up to date.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个名为检索增强生成（RAG）的架构框架实现扎根，这是一种通过在生成响应之前结合外部权威知识库的信息来增强LLM输出的技术。这个过程有助于确保生成的内容是相关、准确且最新的。
- en: 'What is the difference between a copilot and a RAG? RAG can be seen as one
    of the architectural patterns that feature a copilot. Whenever we want our copilot
    to be grounded to domain-specific data, we use a RAG framework. Note that RAG
    is not the only architectural pattern that can feature a copilot: there are further
    frameworks such as function calling or multi-agents that we will explore throughout
    the book.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 副驾驶和RAG之间的区别是什么？RAG可以被看作是一种具有副驾驶特征的架构模式。每当我们要将副驾驶引导到特定领域的数据时，我们就使用RAG框架。请注意，RAG不是唯一可以具有副驾驶特征的架构模式：还有其他框架，如函数调用或多智能体，我们将在本书中探讨。
- en: For example, let’s say we developed a copilot within our company that allows
    employees to chat with their enterprise knowledge base. As fun as it can be, we
    cannot provide users with a copilot they can use to plan their summer trip (it
    would be like providing users with a ChatGPT-like tool at our own hosting cost!);
    on the contrary, we want the copilot to be grounded only to our enterprise knowledge
    base so that it can respond only if the answer is pertinent to the domain-specific
    context.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们在公司内部开发了一个副驾驶，允许员工与企业知识库进行聊天。虽然这很有趣，但我们不能提供用户可以用来规划夏季旅行的副驾驶（那就像是在我们自己的托管成本下提供类似ChatGPT的工具！）；相反，我们希望副驾驶仅限于我们的企业知识库，以便它只能对特定领域上下文相关的答案做出回应。
- en: 'The following figure shows an example of grounding a copilot system:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了引导副驾驶系统的示例：
- en: '![](img/B21714_02_03.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_03.png)'
- en: 'Figure 2.3: Example of grounding a copilot'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：引导副驾驶的示例
- en: '**The copilot’s capabilities can be extended by skills**, which can be code
    or calls to other models. In fact, the LLM (our reasoning engine) might have two
    kinds of limitations:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副驾驶的能力可以通过技能进行扩展**，这些技能可以是代码或对其他模型的调用。实际上，LLM（我们的推理引擎）可能存在两种类型的限制：'
- en: '**Limited parametric knowledge.** This is due to the knowledge base cutoff
    date, which is a physiological feature of LLMs. In fact, their training dataset
    will always be “outdated,” not in line with the current trends. This can be overcome
    by adding non-parametric knowledge with grounding, as previously seen.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数知识有限**。这是由于知识库截止日期，这是LLM的一个生理特征。实际上，它们的训练数据集总是会“过时”，不符合当前趋势。这可以通过添加非参数知识并通过引导来克服，如之前所见。'
- en: '**Lack of executive power.** This means that LLMs by themselves are not empowered
    to carry out actions. Let’s consider, for example, the well-known ChatGPT: if
    we ask it to generate a LinkedIn post about productivity tips, we will then need
    to copy and paste it onto our LinkedIn profile as ChatGPT is not able to do so
    by itself. That is the reason why we need plug-ins. Plug-ins are LLMs’ connectors
    toward the external world that serve not only as input sources to extend LLMs’
    non-parametric knowledge (for example, to allow a web search) but also as output
    sources so that the copilot can actually execute actions. For example, with a
    LinkedIn plug-in, our copilot powered by an LLM will be able not only to generate
    the post but also to post it online.![A cartoon of a person pointing at a plug-in  Description
    automatically generated](img/B21714_02_04.png)'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏执行权力**。这意味着LLM本身没有执行动作的权限。以众所周知的ChatGPT为例：如果我们要求它生成一篇关于生产力技巧的领英帖子，我们随后需要将其复制粘贴到我们的领英个人资料上，因为ChatGPT本身无法完成这一操作。这就是我们需要插件的原因。插件是LLM连接外部世界的桥梁，不仅作为输入源来扩展LLM的非参数知识（例如，允许网络搜索），而且还作为输出源，以便副驾驶能够实际执行动作。例如，通过领英插件，我们由LLM驱动的副驾驶不仅能够生成帖子，还能够将其发布到网上。![一个人指向插件的卡通  自动生成的描述](img/B21714_02_04.png)'
- en: 'Figure 2.4: Example of Wikipedia and LinkedIn plug-ins'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.4：维基百科和领英插件的示例
- en: Note that the user’s prompt in natural language is not the only input the model
    processes. In fact, it is a crucial component of the backend logic of our LLM-powered
    applications and the set of instructions we provide to the model. This *metaprompt*
    or system message is the object of a new discipline called **prompt engineering**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用户在自然语言中的提示并非模型处理的唯一输入。实际上，它是我们基于LLM的应用程序的后端逻辑以及提供给模型的指令集的一个关键组成部分。这种*元提示*或系统消息是新学科**提示工程**的研究对象。
- en: '**Definition**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Prompt engineering is the process of designing and optimizing prompts to LLMs
    for a wide variety of applications and research topics. Prompts are short pieces
    of text that are used to guide the LLM’s output. Prompt engineering skills help
    to better understand the capabilities and limitations of LLMs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是为各种应用程序和研究主题设计并优化LLM提示的过程。提示是用于引导LLM输出的简短文本。提示工程技能有助于更好地理解LLM的能力和局限性。
- en: Prompt engineering involves selecting the right words, phrases, symbols, and
    formats that elicit the desired response from the LLM. Prompt engineering also
    involves using other controls, such as parameters, examples, or data sources,
    to influence the LLM’s behavior. For example, if we want our LLM-powered application
    to generate responses for a 5-year-old child, we can specify this in a system
    message similar to “Act as a teacher who explains complex concepts to 5-year-old
    children.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程涉及选择合适的单词、短语、符号和格式，以从LLM中获得期望的响应。提示工程还涉及使用其他控制手段，如参数、示例或数据源，来影响LLM的行为。例如，如果我们想让我们的LLM驱动应用程序为5岁的孩子生成响应，我们可以在类似“扮演一个向5岁孩子解释复杂概念的教师”的系统消息中指定这一点。
- en: In fact, Andrej Karpathy, the previous Director of AI at Tesla, who returned
    to OpenAI in February 2023, tweeted that “English is the hottest new programming
    language.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，安德烈·卡帕西（Andrej Karpathy），特斯拉前AI总监，于2023年2月重返OpenAI，他在推特上表示：“英语是最新最热门的编程语言。”
- en: We will dive deeper into the concept of prompt engineering in *Chapter 4*, *Prompt
    Engineering*. In the next section, we are going to focus on the emerging AI orchestrators.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第4章“提示工程”中更深入地探讨提示工程的概念。在下一节中，我们将关注新兴的AI编排器。
- en: Introducing AI orchestrators to embed LLMs into applications
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将AI编排器引入以将LLM嵌入到应用程序中
- en: 'Earlier in this chapter, we saw that there are two main aspects to consider
    when incorporating LLMs within applications: a technical aspect and a conceptual
    aspect. While we can explain the conceptual aspect with the brand-new category
    of software called Copilot, in this section, we are going to further explore how
    to technically embed and orchestrate LLMs within our applications.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们看到了在将LLM整合到应用程序中时需要考虑的两个主要方面：技术方面和概念方面。虽然我们可以用名为Copilot的全新软件类别来解释概念方面，但在本节中，我们将进一步探讨如何在我们的应用程序中技术性地嵌入和编排LLM。
- en: The main components of AI orchestrators
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI编排器的主要组件
- en: 'From one side, the paradigm shift of foundation models implies a great simplification
    in the domain of AI-powered applications: after producing models, now the trend
    is consuming models. On the other side, many roadblocks might arise in developing
    this new kind of AI, since there are LLM-related components that are brand new
    and have never been managed before within an application life cycle. For example,
    there might be malicious actors that could try to change the LLM instructions
    (the system message mentioned earlier) so that the application does not follow
    the correct instructions. This is an example of a new set of security threats
    that are typical to LLM-powered applications and need to be addressed with powerful
    counterattacks or preventive techniques.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从一方面来看，基础模型范式转变意味着在人工智能应用领域的大大简化：在生成模型之后，现在的趋势是消费模型。另一方面，在开发这种新型人工智能时可能会遇到许多障碍，因为其中有一些与大型语言模型（LLM）相关的组件是全新的，并且在应用生命周期中之前从未被管理过。例如，可能会有恶意行为者试图更改LLM指令（前面提到的系统消息），使得应用程序不遵循正确的指令。这是一个新的安全威胁集合的例子，这些威胁是LLM驱动应用程序的典型特征，需要用强大的反击或预防技术来解决。
- en: 'The following is an illustration of the main components of such applications:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图展示了此类应用程序的主要组件：
- en: '![A diagram of a computer program  Description automatically generated](img/B21714_02_05.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序图  自动生成描述](img/B21714_02_05.png)'
- en: 'Figure 2.5: High-level architecture of LLM-powered applications'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：LLM驱动应用程序的高级架构
- en: 'Let’s inspect each of these components in detail:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细检查这些组件：
- en: '**Models**: The model is simply the type of LLM we decide to embed in our application.
    There are two main categories of models:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：模型就是我们决定嵌入到我们的应用程序中的LLM类型。模型主要有两大类：'
- en: '**Proprietary LLMs:** Models that are owned by specific companies or organizations.
    Examples include GPT-3 and GPT-4, developed by OpenAI, or Bard, developed by Google.
    As their source code and architecture are not available, those models cannot be
    re-trained from scratch on custom data, yet they can be fine-tuned if needed.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专有LLMs**：由特定公司或组织拥有的模型。例如，由OpenAI开发的GPT-3和GPT-4，或由Google开发的Bard。由于它们的源代码和架构不可用，这些模型不能从头开始在自定义数据上重新训练，但在需要时可以进行微调。'
- en: '**Open-source:** Models with code and architecture freely available and distributed,
    hence they can also be trained from scratch on custom data. Examples include Falcon
    LLM, developed by Abu Dhabi’s **Technology Innovation Institute** (**TII**), or
    LLaMA, developed by Meta.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：代码和架构可以自由获取和分发的模型，因此它们也可以在自定义数据上从头开始训练。例如，由阿布扎比的**技术创新研究所**（**TII**）开发的Falcon
    LLM，或由Meta开发的LLaMA。'
- en: We will dive deeper into the main set of LLMs available today in *Chapter 3*,
    *Choosing an LLM for Your Application*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在**第3章**，**为您的应用程序选择一个LLM**中深入了解今天可用的主要LLM集。
- en: '**Memory**: LLM applications commonly use a conversational interface, which
    requires the ability to refer back to earlier information within the conversation.
    This is achieved through a “memory” system that allows the application to store
    and retrieve past interactions. Note that past interactions could also constitute
    additional non-parametric knowledge to be added to the model. To achieve that,
    it is important to store all the past conversations – properly embedded – into
    VectorDB, which is at the core of the application’s data.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：LLM应用通常使用对话界面，这需要能够在对话中回溯到早期信息。这是通过一个“记忆”系统实现的，该系统允许应用程序存储和检索过去的交互。请注意，过去的交互也可能构成要添加到模型中的额外非参数知识。为了实现这一点，重要的是将所有过去的对话——适当嵌入——存储到VectorDB中，这是应用程序数据的核心。'
- en: '**Definition**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: VectorDB is a type of database that stores and retrieves information based on
    vectorized embeddings, the numerical representations that capture the meaning
    and context of text. By using VectorDB, you can perform semantic search and retrieval
    based on the similarity of meanings rather than keywords. VectorDB can also help
    LLMs generate more relevant and coherent text by providing contextual understanding
    and enriching generation results. Some examples of VectorDBs are Chroma, Elasticsearch,
    Milvus, Pinecone, Qdrant, Weaviate, and **Facebook AI Similarity Search** (**FAISS**).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: VectorDB是一种基于向量嵌入存储和检索信息的数据库类型，这些嵌入是捕获文本意义和上下文的数值表示。通过使用VectorDB，你可以根据意义的相似性而不是关键词进行语义搜索和检索。VectorDB还可以通过提供上下文理解和丰富生成结果来帮助LLMs生成更相关和连贯的文本。VectorDB的例子包括Chroma、Elasticsearch、Milvus、Pinecone、Qdrant、Weaviate以及**Facebook
    AI Similarity Search**（**FAISS**）。
- en: FAISS, developed by Facebook (now Meta) in 2017, was one of the pioneering vector
    databases. It was designed for efficient similarity search and clustering of dense
    vectors and is particularly useful for multimedia documents and dense embeddings.
    It was initially an internal research project at Facebook. Its primary goal was
    to better utilize GPUs for identifying similarities related to user preferences.
    Over time, it evolved into the fastest available library for similarity search
    and can handle billion-scale datasets. FAISS has opened up possibilities for recommendation
    engines and AI-based assistant systems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: FAISS是由Facebook（现在Meta）于2017年开发的，是早期向量数据库之一。它旨在进行密集向量的高效相似性搜索和聚类，特别适用于多媒体文档和密集嵌入。最初，它是在Facebook的一个内部研究项目中。其主要目标是更好地利用GPU来识别与用户偏好相关的相似性。随着时间的推移，它发展成为可用的最快相似性搜索库，可以处理数十亿规模的数据集。FAISS为推荐引擎和基于AI的助手系统开辟了可能性。
- en: '**Plug-ins:** They can be seen as additional modules or components that can
    be integrated into the LLM to extend its functionality or adapt it to specific
    tasks and applications. These plug-ins act as add-ons, enhancing the capabilities
    of the LLM beyond its core language generation or comprehension abilities.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：它们可以被视为可以集成到LLM中的附加模块或组件，以扩展其功能或使其适应特定任务和应用。这些插件作为附加组件，增强了LLM在核心语言生成或理解能力之外的特性。'
- en: The idea behind plug-ins is to make LLMs more versatile and adaptable, allowing
    developers and users to customize the behavior of the language model for their
    specific needs. Plug-ins can be created to perform various tasks, and they can
    be seamlessly incorporated into the LLM’s architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 插件背后的理念是使LLMs更加灵活和适应性强，允许开发者和用户根据他们的特定需求自定义语言模型的行为。可以创建执行各种任务的插件，并且它们可以无缝地集成到LLM的架构中。
- en: '**Prompts**: This is probably the most interesting and pivotal component of
    an LLM-powered application. We’ve already quoted, in the previous section, Andrej
    Karpathy’s affirmation that “English is the hottest new programming language,”
    and you will understand why in the upcoming chapters. Prompts can defined at two
    different levels:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：这可能是LLM驱动的应用程序中最有趣和关键的部分。我们在上一节中已经引用了Andrej Karpathy的断言，“英语是最新最热门的编程语言”，你将在接下来的章节中理解原因。提示可以在两个不同的级别上定义：'
- en: '**“Frontend,” or what the user sees**: A “prompt” refers to the input to the
    model. It is the way the user interacts with the application, asking things in
    natural language.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“前端”，或用户所见**：一个“提示”指的是模型输入。这是用户与应用程序交互的方式，以自然语言提问。'
- en: '**“Backend,” or what the user does not see**: Natural language is not only
    the way to interact, as a user, with the frontend; it is also the way we “program”
    the backend. In fact, on top of the user’s prompt, there are many natural language
    instructions, or meta-promts, that we give to the model so that it can properly
    address the user’s query. Meta-prompts are meant to instruct the model to act
    as it is meant to. For example, if we want to limit our application to answer
    only questions related to the documentation we provided in VectorDB, we will specify
    the following in our meta-prompts to the model: “*Answer only if the question
    is related to the provided documentation.*”'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“后端”，或用户所见不到的**：自然语言不仅是用户与前端交互的方式；它也是我们“编程”后端的方式。实际上，在用户的提示之上，有许多自然语言指令，或元提示，我们提供给模型，以便它能够正确地回答用户的查询。元提示的目的是指导模型按照预期的方式行动。例如，如果我们想限制我们的应用程序只回答与我们在VectorDB中提供的文档相关的问题，我们将在我们的元提示中指定以下内容：“*只有当问题与提供的文档相关时才回答。*”'
- en: Finally, we get to the core of the high-level architecture shown in *Figure
    2.5*, that is, the **AI orchestrator**. With the AI orchestrator, we refer to
    lightweight libraries that make it easier to embed and orchestrate LLMs within
    applications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了*图2.5*所示的高级架构的核心，即**AI编排器**。有了AI编排器，我们指的是那些使嵌入和编排LLMs（大型语言模型）在应用程序中变得更加容易的轻量级库。
- en: 'As LLMs went viral by the end of 2022, many libraries started arising in the
    market. In the next sections, we are going to focus on three of them: LangChain,
    Semantic Kernel, and Haystack.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2022年底，随着LLMs的流行，市场上开始涌现出许多库。在接下来的几节中，我们将重点关注其中的三个：LangChain、Semantic Kernel和Haystack。
- en: LangChain
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangChain
- en: LangChain was launched as an open-source project by Harrison Chase in October
    2022\. It can be used both in Python and JS/TS. It is a framework for developing
    applications powered by language models, making them data-aware (with grounding)
    and agentic – which means they are able to interact with external environments.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain是由Harrison Chase在2022年10月作为开源项目推出的。它可以在Python和JS/TS中使用。它是一个用于开发由语言模型驱动的应用程序的框架，使它们具有数据感知性（具有扎根）和代理性——这意味着它们能够与外部环境交互。
- en: 'Let’s take a look at the key components of LangChain:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看LangChain的关键组件：
- en: '![](img/B21714_02_06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_02_06.png)'
- en: 'Figure 2.6: LangChain’s components'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：LangChain的组件
- en: 'Overall, LangChain has the following core modules:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LangChain具有以下核心模块：
- en: '**Models**: These are the LLMs or LFMs that will be the engine of the application.
    LangChain supports proprietary models, such as those available in OpenAI and Azure
    OpenAI, and open-source models consumable from the **Hugging Face Hub**.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：这些是将成为应用程序引擎的LLMs或LFMs。LangChain支持来自OpenAI和Azure OpenAI的专有模型，以及可以从**Hugging
    Face Hub**获取的开源模型。'
- en: '**Definition**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Hugging Face is a company and a community that builds and shares state-of-the-art
    models and tools for natural language processing and other machine learning domains.
    It developed the Hugging Face Hub, a platform where people can create, discover,
    and collaborate on machine learning models and LLMs, datasets, and demos. The
    Hugging Face Hub hosts over 120k models, 20k datasets, and 50k demos in various
    domains and tasks, such as audio, vision, and language.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 是一家公司和一个社区，致力于构建和分享自然语言处理和其他机器学习领域的最先进模型和工具。它开发了 Hugging Face Hub，这是一个平台，人们可以在这里创建、发现和协作机器学习模型、LLMs、数据集和演示。Hugging
    Face Hub 在各个领域和任务中托管了超过 120k 个模型、20k 个数据集和 50k 个演示，例如音频、视觉和语言。
- en: Alongside models, LangChain also offers many prompt-related components that
    make it easier to manage the prompt flow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型之外，LangChain 还提供了许多与提示相关的组件，这使得管理提示流程变得更加容易。
- en: '**Data connectors**: These refer to the building blocks needed to retrieve
    the additional external knowledge (for example, in RAG-based scenarios) we want
    to provide the model with. Examples of data connectors are document loaders or
    text embedding models.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据连接器**：这些是指构建所需的基本模块，用于检索我们希望提供给模型的额外外部知识（例如，在基于 RAG 的场景中）。数据连接器的例子包括文档加载器或文本嵌入模型。'
- en: '**Memory**: This allows the application to keep references to the user’s interactions,
    in both the short and long term. It is typically based on vectorized embeddings
    stored in VectorDB.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：这允许应用程序在短期和长期内保持对用户交互的引用。它通常基于存储在 VectorDB 中的矢量化嵌入。'
- en: '**Chains**: These are predetermined sequences of actions and calls to LLMs
    that make it easier to build complex applications that require chaining LLMs with
    each other or with other components. An example of a chain might be: take the
    user query, chunk it into smaller pieces, embed those chunks, search for similar
    embeddings in VectorDB, use the top three most similar chunks in VectorDB as context
    to provide the answer, and generate the answer.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链**：这些是预定的一系列动作和对 LLM 的调用，这使得构建需要将 LLMs 链接在一起或与其他组件链接的复杂应用程序变得更加容易。链的一个例子可能是：获取用户查询，将其分成更小的部分，嵌入这些部分，在
    VectorDB 中搜索相似的嵌入，使用 VectorDB 中最相似的三个部分作为上下文来提供答案，并生成答案。'
- en: '**Agents**: Agents are entities that drive decision-making within LLM-powered
    applications. They have access to a suite of tools and can decide which tool to
    call based on the user input and the context. Agents are dynamic and adaptive,
    meaning that they can change or adjust their actions based on the situation or
    the goal.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：代理是驱动 LLM 驱动的应用程序中决策的实体。它们可以访问一系列工具，并根据用户输入和上下文决定调用哪个工具。代理是动态和自适应的，这意味着它们可以根据情况或目标改变或调整其行为。'
- en: 'LangChain offers the following benefits:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供以下好处：
- en: LangChain provides modular abstractions for the components we previously mentioned
    that are necessary to work with language models, such as prompts, memory, and
    plug-ins.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain 为我们之前提到的、与语言模型一起工作的必要组件提供了模块化抽象，例如提示、记忆和插件。
- en: Alongside those components, LangChain also offers pre-built **chains**, which
    are structured concatenations of components. Those chains can be pre-built for
    specific use cases or be customized.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了这些组件之外，LangChain 还提供了预构建的 **链**，这是组件的结构化连接。这些链可以是针对特定用例预构建的，也可以是定制的。
- en: In *Part 2* of this book, we will go through a series of hands-on applications,
    all LangChain based. So, starting from *Chapter 5*, *Embedding LLMs within Your
    Applications*, we will focus much deeper on LangChain components and overall frameworks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的 *第 2 部分* 中，我们将通过一系列基于 LangChain 的动手应用，从 *第 5 章* 开始，专注于将 LLMs 内置于您的应用程序中，我们将更深入地关注
    LangChain 组件和整体框架。
- en: Haystack
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Haystack
- en: Haystack is a Python-based framework developed by Deepset, a startup founded
    in 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. Deepset provides
    developers with the tools to build **natural language processing** (**NLP**)-based
    applications, and with the introduction of Haystack, they are taking them to the
    next level.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack 是由 Deepset 开发的基于 Python 的框架，Deepset 是一家成立于 2018 年的柏林初创公司，由 Milos Rusic、Malte
    Pietsch 和 Timo Möller 创立。Deepset 为开发者提供构建基于自然语言处理（**NLP**）的应用程序的工具，随着 Haystack
    的引入，他们将这些工具提升到了新的水平。
- en: 'The following illustration shows the core components of Haystack:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图显示了 Haystack 的核心组件：
- en: '![](img/B21714_02_07.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_07.png)'
- en: 'Figure 2.7: Haystack’s components'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：Haystack 的组件
- en: 'Let’s look at these components in detail:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些组件：
- en: '**Nodes**: These are components that perform a specific task or function, such
    as a retriever, a reader, a generator, a summarizer, etc. Nodes can be LLMs or
    other utilities that interact with LLMs or other resources. Among LLMs, Haystack
    supports proprietary models, such as those available in OpenAI and Azure OpenAI,
    and open-source models consumable from the Hugging Face Hub.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：这些是执行特定任务或功能的组件，例如检索器、阅读器、生成器、摘要器等。节点可以是 LLM 或其他与 LLM 或其他资源交互的实用工具。在
    LLM 中，Haystack 支持专有模型，例如 OpenAI 和 Azure OpenAI 中可用的模型，以及来自 Hugging Face Hub 的开源模型。'
- en: '**Pipelines**: These are sequences of calls to nodes that perform natural language
    tasks or interact with other resources. Pipelines can be querying pipelines or
    indexing pipelines, depending on whether they perform searches on a set of documents
    or prepare documents for search. Pipelines are predetermined and hardcoded, meaning
    that they do not change or adapt based on the user input or the context.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：这些是调用执行自然语言任务或与其他资源交互的节点的序列。管道可以是查询管道或索引管道，这取决于它们是否在文档集上执行搜索或为搜索准备文档。管道是预先确定和硬编码的，这意味着它们不会根据用户输入或上下文进行更改或适应。'
- en: '**Agent**: This is an entity that uses LLMs to generate accurate responses
    to complex queries. An agent has access to a set of tools, which can be pipelines
    or nodes, and it can decide which tool to call based on the user input and the
    context. An agent is dynamic and adaptive, meaning that it can change or adjust
    its actions based on the situation or the goal.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：这是一个使用 LLM 生成对复杂查询的准确响应的实体。代理可以访问一组工具，这些工具可以是管道或节点，并且可以根据用户输入和上下文决定调用哪个工具。代理是动态和自适应的，这意味着它可以根据情况或目标改变或调整其行为。'
- en: '**Tools**: There are functions that an agent can call to perform natural language
    tasks or interact with other resources. Tools can be pipelines or nodes that are
    available to the agent and they can be grouped into toolkits, which are sets of
    tools that can accomplish specific objectives.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：代理可以调用的函数来执行自然语言任务或与其他资源交互。工具可以是代理可用的管道或节点，它们可以被分组成工具包，这些工具包是一系列可以完成特定目标的工具。'
- en: '**DocumentStores**: These are backends that store and retrieve documents for
    searches. DocumentStores can be based on different technologies, also including
    VectorDB (such as FAISS, Milvus, or Elasticsearch).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DocumentStores**：这些是存储和检索用于搜索的文档的后端。DocumentStores 可以基于不同的技术，包括 VectorDB（例如
    FAISS、Milvus 或 Elasticsearch）。'
- en: 'Some of the benefits offered by Haystack are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack 提供的一些好处包括：
- en: '**Ease of use**: Haystack is user-friendly and straightforward. It’s often
    chosen for lighter tasks and rapid prototypes.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：Haystack 易于使用且直观。它通常用于轻量级任务和快速原型。'
- en: '**Documentation quality**: Haystack’s documentation is considered high-quality,
    aiding developers in building search systems, question-answering, summarization,
    and conversational AI.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档质量**：Haystack 的文档被认为是高质量的，有助于开发者构建搜索系统、问答、摘要和对话式 AI。'
- en: '**End-to-end framework**: Haystack covers the entire LLM project life cycle,
    from data preprocessing to deployment. It’s ideal for large-scale search systems
    and information retrieval.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端框架**：Haystack 覆盖了整个 LLM 项目生命周期，从数据预处理到部署。它非常适合大规模搜索系统和信息检索。'
- en: Another nice thing about Haystack is that you can deploy it as a REST API and
    it can be consumed directly.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 Haystack 的另一个优点是，你可以将其作为 REST API 部署，并且可以直接消费它。
- en: Semantic Kernel
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义内核
- en: Semantic Kernel is the third open-source SDK we are going to explore in this
    chapter. It was developed by Microsoft, originally in C# and now also available
    in Python.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核是我们将在本章中探索的第三个开源 SDK。它最初由微软开发，最初是用 C# 编写的，现在也支持 Python。
- en: This framework takes its name from the concept of a “kernel,” which, generally
    speaking, refers to the core or essence of a system. In the context of this framework,
    a kernel is meant to act as the engine that addresses a user’s input by chaining
    and concatenating a series of components into pipelines, encouraging **function
    composition.**
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架的名字来源于“内核”的概念，一般而言，内核指的是系统的核心或本质。在这个框架的上下文中，内核是指通过将一系列组件链接和连接成管道来处理用户输入的引擎，鼓励**函数组合**。
- en: '**Definition**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In mathematics, function composition is a way to combine two functions to create
    a new function. The idea is to use the output of one function as the input to
    another function, forming a chain of functions. The composition of two functions
    *f* and *g* is denoted as (*f ![](img/circle.png) g*), where the function *g*
    is applied first, followed by the function *f* ![](img/arrow.png)(*f ![](img/circle.png)
    g*)(*x*) = *f*(*g*(*x*)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，函数组合是将两个函数组合起来创建一个新函数的方法。想法是使用一个函数的输出作为另一个函数的输入，形成一个函数链。两个函数f和g的组合表示为(*f
    ![img/circle.png](img/circle.png) g*)，其中函数g首先应用，然后是函数f ![img/arrow.png](img/arrow.png)(*f
    ![img/circle.png](img/circle.png) g*)(*x*) = *f*(*g*(*x*))。
- en: Function composition in computer science is a powerful concept that allows for
    the creation of more sophisticated and reusable code by combining smaller functions
    into larger ones. It enhances modularity and code organization, making programs
    easier to read and maintain.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学中的函数组合是一个强大的概念，它通过将较小的函数组合成较大的函数来创建更复杂和可重用的代码。它增强了模块化和代码组织，使程序更容易阅读和维护。
- en: 'The following is an illustration of the anatomy of Semantic Kernel:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对语义内核解剖结构的说明：
- en: '![](img/B21714_02_08.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_02_08.png)'
- en: 'Figure 2.8: Anatomy of Semantic Kernel'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：语义内核的解剖结构
- en: 'Semantic Kernel has the following main components:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核有以下主要组件：
- en: '**Models**: These are the LLMs or LFMs that will be the engine of the application.
    Semantic Kernel supports proprietary models, such as those available in OpenAI
    and Azure OpenAI, and open-source models consumable from the Hugging Face Hub.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：这些是将成为应用程序引擎的LLM或LFM。语义内核支持专有模型，例如OpenAI和Azure OpenAI中可用的模型，以及可以从Hugging
    Face Hub获取的开源模型。'
- en: '**Memory**: It allows the application to keep references to the user’s interactions,
    both in the short and long term. Within the framework of Semantic Kernel, memories
    can be accessed in three ways:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**：它允许应用程序在短期和长期内保持对用户交互的引用。在语义内核的框架内，可以通过三种方式访问记忆：'
- en: '**Key**-**value pairs**: This consists of saving environment variables that
    store simple information, such as names or dates.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键值对**：这包括保存存储简单信息的环境变量，例如名称或日期。'
- en: '**Local storage**: This consists of saving information to a file that can be
    retrieved by its filename, such as a CSV or JSON file.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地存储**：这包括将信息保存到可以通过其文件名检索的文件中，例如CSV或JSON文件。'
- en: '**Semantic memory search**: This is similar to LangChain’s and Haystack’s memory,
    as it uses embeddings to represent and search for text information based on its
    meaning.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义记忆搜索**：这与LangChain和Haystack的记忆类似，因为它使用嵌入来表示和基于其意义搜索文本信息。'
- en: '**Functions**: Functions can be seen as skills that mix LLM prompts and code,
    with the goal of making users’ asks interpretable and actionable. There are two
    types of functions:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数**：函数可以被视为混合LLM提示和代码的技能，目的是使用户的请求可解释和可操作。有两种类型的函数：'
- en: '**Semantic functions**: These are a type of templated prompt, which is a natural
    language query that specifies the input and output format for the LLM, also incorporating
    prompt configuration, which sets the parameters for the LLM.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义函数**：这是一种模板化提示，是一种自然语言查询，它指定了LLM的输入和输出格式，同时也结合了提示配置，该配置设置了LLM的参数。'
- en: '**Native functions**: These refer to the native computer code that can route
    the intent captured by the semantic function and perform the related task.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地函数**：这些指的是可以路由语义函数捕获的意图并执行相关任务的本地计算机代码。'
- en: To make an example, a semantic function could ask the LLM to write a short paragraph
    about AI, while a native function could actually post it on social media like
    LinkedIn.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明，一个语义函数可以要求LLM写一段关于AI的短段落，而一个本地函数实际上可以在LinkedIn等社交媒体上发布。
- en: '**Plug-ins:** These are connectors toward external sources or systems that
    are meant to provide additional information or the ability to perform autonomous
    actions. Semantic Kernel offers out-of-the-box plug-ins, such as the Microsoft
    Graph connector kit, but you can build a custom plug-in by leveraging functions
    (both native and semantic, or a mix of the two).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：这些是连接到外部来源或系统的连接器，旨在提供额外信息或执行自主操作的能力。语义内核提供了一些现成的插件，例如Microsoft Graph连接器套件，但你也可以通过利用函数（本地和语义，或两者的组合）来构建自定义插件。'
- en: '**Planner**: As LLMs can be seen as reasoning engines, they can also be leveraged
    to auto-create chains or pipelines to address new users’ needs. This goal is achieved
    with a planner, which is a function that takes as input a user’s task and produces
    the set of actions, plug-ins, and functions needed to achieve the goal.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划器**: 由于LLM可以被视为推理引擎，它们也可以被用来自动创建链或管道以满足新用户的需求。这个目标是通过规划器实现的，它是一个接受用户任务作为输入并生成实现目标所需的一组动作、插件和函数的功能。'
- en: 'Some benefits of Semantic Kernel are:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 语义内核的一些好处包括：
- en: '**Lightweight and C# support**: Semantic Kernel is more lightweight and includes
    C# support. It’s a great choice for C# developers or those using the .NET framework.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻量级和C#支持**: 语义内核更轻量级，并包含C#支持。对于C#开发者或使用.NET框架的开发者来说，它是一个极佳的选择。'
- en: '**Wide range of use cases**: Semantic Kernel is versatile, supporting various
    LLM-related tasks.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的应用场景**: 语义内核功能多样，支持各种与LLM相关的任务。'
- en: '**Industry-led**: Semantic Kernel was developed by Microsoft, and it is the
    framework the company used to build its own copilots. Hence, it is mainly driven
    by industry needs and asks, making it a solid tool for enterprise-scale applications.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业引领**: 语义内核是由微软开发的，它是公司构建自身协同飞行员框架所使用的框架。因此，它主要受行业需求和询问的驱动，使其成为企业级应用的稳固工具。'
- en: How to choose a framework
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何选择框架
- en: 'Overall, the three frameworks offer, more or less, similar core components,
    sometimes called by a different taxonomy, yet covering all the blocks illustrated
    within the concept of the copilot system. So, a natural question might be: “Which
    one should I use to build my LLM-powered application?” Well, there is no right
    or wrong answer! All three are extremely valid. However, there are some features
    that might be more relevant for specific use cases or developers’ preferences.
    The following are some criteria you might want to consider:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这三个框架提供，或多或少，类似的核心组件，有时被称为不同的分类法，但涵盖了协同飞行员系统概念中所示的所有模块。因此，一个自然的问题可能是：“我应该使用哪一个来构建我的LLM驱动应用程序？”
    好吧，没有正确或错误答案！三者都非常有效。然而，有些特性可能对特定用例或开发者的偏好更为相关。以下是一些您可能想要考虑的标准：
- en: '**The programming language you are comfortable with or prefer to use:** Different
    frameworks may support different programming languages or have different levels
    of compatibility or integration with them. For example, Semantic Kernel supports
    C#, Python, and Java, while LangChain and Haystack are mainly based on Python
    (even though LangChain also introduced JS/TS support). You may want to choose
    a framework that matches your existing skills or preferences, or that allows you
    to use the language that is most suitable for your application domain or environment.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您熟悉或偏好的编程语言**: 不同的框架可能支持不同的编程语言，或者与它们的兼容性或集成程度不同。例如，语义内核支持C#、Python和Java，而LangChain和Haystack主要基于Python（尽管LangChain也引入了对JS/TS的支持）。您可能希望选择一个与您现有技能或偏好相匹配的框架，或者允许您使用最适合您的应用程序领域或环境的语言。'
- en: '**The type and complexity of the natural language tasks you want to perform
    or support:** Different frameworks may have different capabilities or features
    for handling various natural language tasks, such as summarization, generation,
    translation, reasoning, etc. For example, LangChain and Haystack provide utilities
    and components for orchestrating and executing natural language tasks, while Semantic
    Kernel allows you to use natural language semantic functions to invoke LLMs and
    services. You may want to choose a framework that offers the functionality and
    flexibility you need or want for your application goals or scenarios.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您想要执行或支持的自然语言任务类型和复杂性**: 不同的框架可能具有不同的能力或特性来处理各种自然语言任务，例如摘要、生成、翻译、推理等。例如，LangChain和Haystack提供了编排和执行自然语言任务的实用工具和组件，而语义内核允许您使用自然语言语义函数调用LLM和服务。您可能希望选择一个提供您为应用程序目标或场景所需的功能和灵活性的框架。'
- en: '**The level of customization and control you need or want over the LLMs and
    their parameters or options**: Different frameworks may have different ways of
    accessing, configuring, and fine-tuning the LLMs and their parameters or options,
    such as model selection, prompt design, inference speed, output format, etc. For
    example, Semantic Kernel provides connectors that make it easy to add memories
    and models to your AI app, while LangChain and Haystack allow you to plug in different
    components for the document store, retriever, reader, generator, summarizer, and
    evaluator. You may want to choose a framework that gives you the level of customization
    and control you need or want over the LLMs and their parameters or options.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您需要或希望对LLM及其参数或选项进行定制和控制的程度**：不同的框架可能有不同的方式来访问、配置和微调LLM及其参数或选项，例如模型选择、提示设计、推理速度、输出格式等。例如，Semantic
    Kernel提供了连接器，使得向您的AI应用中添加记忆和模型变得容易，而LangChain和Haystack允许您为文档存储、检索器、阅读器、生成器、摘要器和评估器插入不同的组件。您可能希望选择一个框架，它能够提供您对LLM及其参数或选项所需或希望达到的定制和控制的程度。'
- en: '**The availability and quality of the documentation, tutorials, examples, and
    community support for the framework:** Different frameworks may have different
    levels of documentation, tutorials, examples, and community support that can help
    you learn, use, and troubleshoot the framework. For example, Semantic Kernel has
    a website with documentation, tutorials, examples, and a Discord community; LangChain
    has a GitHub repository with documentation, examples, and issues; Haystack has
    a website with documentation, tutorials, demos, blog posts, and a Slack community.
    You may want to choose a framework that has the availability and quality of documentation,
    tutorials, examples, and community support that can help you get started and solve
    problems with the framework.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架的文档、教程、示例和社区支持的可获得性和质量**：不同的框架可能有不同水平的文档、教程、示例和社区支持，这些可以帮助您学习、使用和调试框架。例如，Semantic
    Kernel有一个包含文档、教程、示例和Discord社区的网站；LangChain有一个包含文档、示例和问题的GitHub仓库；Haystack有一个包含文档、教程、演示、博客文章和Slack社区的网站。您可能希望选择一个具有文档、教程、示例和社区支持的可获得性和质量，这可以帮助您开始使用框架并解决与框架相关的问题。'
- en: 'Let’s briefly summarize the differences between these orchestrators:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要总结一下这些编排器之间的差异：
- en: '| **Feature** | **LangChain** | **Haystack** | **Semantic Kernel** |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | **LangChain** | **Haystack** | **Semantic Kernel** |'
- en: '| **LLM support** | Proprietary and open-source | Proprietary and open source
    | Proprietary and open source |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **LLM支持** | 专有和开源 | 专有和开源 | 专有和开源 |'
- en: '| **Supported languages** | Python and JS/TS | Python | C#, Java, and Python
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **支持的语言** | Python和JS/TS | Python | C#、Java和Python |'
- en: '| **Process orchestration** | Chains | Pipelines of nodes | Pipelines of functions
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **流程编排** | 链 | 节点管道 | 函数管道 |'
- en: '| **Deployment** | No REST API | REST API | No REST API |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **部署** | 无REST API | REST API | 无REST API |'
- en: '| **Feature** | **LangChain** | **Haystack** | **Semantic Kernel** |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | **LangChain** | **Haystack** | **Semantic Kernel** |'
- en: 'Table 2.1: Comparisons among the three AI orchestrators'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：三种AI编排器的比较
- en: Overall, all three frameworks offer a wide range of tools and integrations to
    build your LLM-powered applications, and a wise approach could be to use the one
    that is most in line with your current skills or the company’s overall approach.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这三个框架都提供了一系列的工具和集成，以构建您的LLM驱动应用程序，并且一个明智的方法可能是使用与您当前技能或公司整体方法最一致的框架。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into the new way of developing applications that
    LLMs have been paving, as we introduced the concept of the copilot and discussed
    the emergence of new AI orchestrators. Among those, we focused on three projects
    – LangChain, Haystack, and Semantic Kernel – and we examined their features, main
    components, and some criteria to decide which one to pick.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了LLM（大型语言模型）正在铺就的新应用开发方式，正如我们介绍了协同驾驶员的概念并讨论了新AI编排器的出现。在这些中，我们重点关注了三个项目——LangChain、Haystack和Semantic
    Kernel——并检查了它们的功能、主要组件以及一些选择哪一个的标准。
- en: Once we have decided on the AI orchestrator, another pivotal step is to decide
    which LLM(s) we want to embed into our applications. In *Chapter 3*, *Choosing
    an LLM for Your Application*, we are going to see the most prominent LLMs on the
    market today – both proprietary and open-source – and understand some decision
    criteria to pick the proper models with respect to the application use cases.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了人工智能编排器，另一个关键步骤就是决定我们想要嵌入到我们的应用程序中的LLM（大型语言模型）。在*第3章*，*为您的应用程序选择一个LLM*，我们将了解市场上最突出的LLM——既有专有也有开源的——并了解一些决策标准，以便根据应用程序用例选择合适的模型。
- en: References
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'LangChain repository: [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain仓库：[https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)
- en: 'Semantic Kernel documentation: [https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义内核文档：[https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages)
- en: 'Copilot stack: [https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b](https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Copilot堆栈：[https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b](https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b)
- en: 'The Copilot system: [https://www.youtube.com/watch?v=E5g20qmeKpg](https://www.youtube.com/watch?v=E5g20qmeKpg)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Copilot系统：[https://www.youtube.com/watch?v=E5g20qmeKpg](https://www.youtube.com/watch?v=E5g20qmeKpg)
- en: Join our community on Discord
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm )'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code214329708533108046.png)'
