- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Fine-Tuning with Preference Alignment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏好对齐的微调
- en: '**Supervised Fine-Tuning** (**SFT**) has been crucial in adapting LLMs to perform
    specific tasks. However, SFT struggles to capture the nuances of human preferences
    and the long tail of potential interactions that a model might encounter. This
    limitation has led to the development of more advanced techniques for aligning
    AI systems with human preferences, grouped under the umbrella term *preference
    alignment*.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督微调**（**SFT**）在使大型语言模型（LLMs）执行特定任务方面至关重要。然而，SFT在捕捉人类偏好的细微差别以及模型可能遇到的潜在交互的长尾方面存在困难。这种限制导致了更高级技术的发展，用于将人工智能系统与人类偏好对齐，这些技术被统称为*偏好对齐*。'
- en: Preference alignment addresses the shortcomings of SFT by incorporating direct
    human or AI feedback into the training process. This method allows a more nuanced
    understanding of human preferences, especially in complex scenarios where simple
    supervised learning falls short. While numerous techniques exist for preference
    alignment, this chapter will primarily focus on **Direct Preference Optimization**
    (**DPO**) for simplicity and efficiency.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好对齐通过将直接的人类或AI反馈纳入训练过程来解决SFT的不足。这种方法允许更细微地理解人类偏好，特别是在简单监督学习不足的复杂场景中。虽然存在许多偏好对齐的技术，但本章将主要关注**直接偏好优化**（**DPO**）以实现简单和高效。
- en: In this chapter, we will talk about the type of data that is required by preference
    alignment algorithms like DPO. We will build our own dataset to modify the writing
    style of our model, making it less artificial and more authentic. We will introduce
    the DPO algorithm and implement it to align the model trained in *Chapter 5*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将讨论偏好对齐算法如DPO所需的数据类型。我们将构建自己的数据集来修改我们模型的写作风格，使其更少人工化，更真实。我们将介绍DPO算法并实现它以对齐第5章中训练的模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding preference datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解偏好数据集
- en: How to create our own preference dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建我们自己的偏好数据集
- en: '**Direct preference optimization** (**DPO**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接偏好优化**（**DPO**）'
- en: Implementing DPO in practice to align our model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中实施DPO以对齐我们的模型
- en: By the end of this chapter, you will be able to create your own preference datasets
    and align models with diverse techniques.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够创建自己的偏好数据集，并使用多种技术将模型与模型对齐。
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到：[https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering)。
- en: Understanding preference datasets
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解偏好数据集
- en: 'The principles for creating high-quality preference datasets are the same as
    those discussed in *Chapter 5* for instruction datasets. We want to maximize the
    accuracy, diversity, and complexity of our samples. To achieve this, we follow
    the same stages, as outlined in *Figure 6.1*: data curation, deduplication, decontamination,
    quality evaluation, exploration, generation, and augmentation.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 创建高质量偏好数据集的原则与第5章中讨论的指令数据集的原则相同。我们希望最大化样本的准确性、多样性和复杂性。为了实现这一点，我们遵循与图6.1中概述的相同阶段：数据整理、去重、净化、质量评估、探索、生成和增强。
- en: '![A diagram of a data flow  Description automatically generated](img/B31105_05_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![数据流图  描述自动生成](img/B31105_05_01.png)'
- en: Figure 6.1 – Overview of the post-training data pipeline covered in this chapter
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 本章涵盖的培训后数据管道概述
- en: 'To avoid repetition, this section will focus on the main differences between
    instruction and preference datasets. We will introduce the structure of preference
    samples and the ideal size for preference datasets. Then, we will focus on the
    two stages that differ most from creating instruction datasets: data generation
    and evaluation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重复，本节将专注于指令数据集和偏好数据集之间的主要区别。我们将介绍偏好样本的结构和偏好数据集的理想大小。然后，我们将关注与创建指令数据集最不同的两个阶段：数据生成和评估。
- en: Preference data
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏好数据
- en: Preference datasets lack the standardization of instruction datasets due to
    varying data requirements across different training algorithms. Preference data
    comprises a collection of responses to a given instruction, ranked by humans or
    language models. This chapter focuses on DPO, so we will examine the specific
    data format required by this algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同训练算法对数据要求的不同，偏好数据集缺乏指令数据集的标准性。偏好数据包括对给定指令的一系列响应，这些响应由人类或语言模型按顺序排列。本章重点关注DPO，因此我们将检查该算法所需的具体数据格式。
- en: 'As illustrated in *Table 6.1*, the structure of DPO datasets is straightforward:
    each instruction is paired with one preferred answer and one rejected answer.
    The objective is to train the model to generate the preferred response rather
    than the rejected one.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如*表6.1*所示，DPO数据集的结构简单明了：每条指令都配对了一个首选答案和一个拒绝答案。目标是训练模型生成首选响应而不是拒绝的响应。
- en: '| **Instruction**Tell me a joke about octopuses. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **指令**告诉我一个关于章鱼的故事。 |'
- en: '| **Chosen answer**Why don’t octopuses play cards in casinos? Because they
    can’t count past eight. | **Rejected answer**How many tickles does it take to
    make an octopus laugh? Ten tickles. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **选择答案**为什么章鱼不在赌场里玩牌？因为它们数不过八。 | **拒绝答案**要多少次轻触才能让章鱼笑？十次轻触。 |'
- en: Table 6.1 – Example of sample from the mlabonne/orpo-dpo-mix-40k dataset
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – mlabonne/orpo-dpo-mix-40k数据集的样本示例
- en: 'In preference datasets, the rejected response is as important as the chosen
    one. Without the rejected response, the dataset would be a simple instruction
    set. Rejected responses represent the behavior we aim to eliminate from the model.
    This provides a lot of flexibility and allows us to use preference datasets in
    many contexts. Here is a list of examples where preference datasets are more beneficial
    to use compared to using SFT alone:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏好数据集中，拒绝的响应与选择的响应一样重要。没有拒绝的响应，数据集将只是一个简单的指令集。拒绝的响应代表我们希望从模型中消除的行为。这提供了很大的灵活性，并允许我们在许多情境中使用偏好数据集。以下是一些例子，说明在单独使用SFT相比，使用偏好数据集更有益的情况：
- en: '**Chatbots**: In conversational AI, the quality of responses often depends
    on subjective factors like naturalness, engagement, and contextual appropriateness.
    A preference dataset allows the model to learn these nuanced aspects by comparing
    better and worse responses. Simple SFT might not capture the subtleties of what
    makes one response preferable over another in a given context.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天机器人**：在对话式人工智能中，响应的质量往往取决于主观因素，如自然度、参与度和上下文适宜性。偏好数据集允许模型通过比较更好的和较差的响应来学习这些细微之处。简单的SFT可能无法捕捉到在特定情境下使一个响应比另一个响应更可取的微妙之处。'
- en: '**Content moderation**: Determining whether content is appropriate or violates
    guidelines often involves nuanced judgments. Preference datasets can help the
    model learn to distinguish between borderline cases by comparing examples of content
    that is and isn’t acceptable. This is more effective than binary classification
    through SFT, as it helps the model understand the reasoning behind moderation
    decisions.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容审核**：确定内容是否适当或违反指南通常涉及细微的判断。偏好数据集可以帮助模型通过比较可接受和不可接受的内容的例子来学习区分边缘案例。这比通过SFT进行二进制分类更有效，因为它有助于模型理解审核决策背后的推理。'
- en: '**Summarization**: The quality of a summary often depends on factors like conciseness,
    relevance, and coherence. By using preference datasets, models can learn to generate
    summaries that humans find more useful and informative. Simple SFT might result
    in summaries that are technically correct but less preferable to human readers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要**：摘要的质量往往取决于诸如简洁性、相关性和连贯性等因素。通过使用偏好数据集，模型可以学习生成人类认为更有用和更有信息量的摘要。简单的SFT可能导致技术上正确但不如人类读者偏好的摘要。'
- en: '**Code generation**: In coding tasks, there are often multiple correct solutions,
    but some are more efficient or readable, or follow better practices than others.
    Preference datasets can help the model learn these qualitative aspects of code
    quality, which might not be captured by simple correctness-based SFT.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成**：在编码任务中，通常有多种正确解决方案，但其中一些比其他方案更高效、更易读，或遵循更好的实践。偏好数据集可以帮助模型学习代码质量的这些定性方面，这些方面可能无法通过基于简单正确性的SFT捕捉到。'
- en: '**Creative writing**: For tasks like story generation or poetry writing, the
    quality of the output is highly subjective and multifaceted. Preference datasets
    can capture human judgments about style, creativity, and emotional impact better
    than instruction datasets, which might focus more on technical correctness or
    adherence to prompts.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创意写作**：对于故事生成或诗歌写作等任务，输出的质量高度主观且多方面。偏好数据集可以比指令数据集更好地捕捉人类对风格、创造力和情感影响的判断，后者可能更多地关注技术正确性或遵循提示。'
- en: '**Translation**: While traditional metrics like BLEU scores can measure translation
    accuracy, they don’t always capture the fluency or naturalness of the translation.
    Preference datasets can help models learn to produce translations that native
    speakers prefer, even when multiple translations are technically correct.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译**：虽然像BLEU分数这样的传统指标可以衡量翻译的准确性，但它们并不总是能捕捉到翻译的流畅性或自然性。偏好数据集可以帮助模型学习产生母语人士偏好的翻译，即使有多种翻译在技术上都是正确的。'
- en: In all these scenarios, preference datasets enable a more refined training approach.
    They capture subjective quality assessments and human preferences that extend
    beyond simple correctness or adherence to instructions. This method can produce
    models that generate output that is not only technically accurate but also better
    aligned with human judgment and preferences in complex, open-ended tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些场景中，偏好数据集使训练方法更加精细。它们捕捉主观质量评估和人类偏好，这些偏好超越了简单的正确性或遵循指令。这种方法可以产生不仅技术上准确，而且与人类在复杂、开放性任务中的判断和偏好更好的对齐的模型。
- en: Unlike instruction datasets, there are no standardized storage formats like
    Alpaca or ShareGPT. Most preference datasets follow a structure similar to that
    shown in *Table 6.1*, with columns for an instruction, a preferred answer, and
    a rejected answer. Multi-turn conversations are uncommon in preference alignment.
    At the time of writing, major fine-tuning libraries do not support multi-turn
    conversations and typically extract only the first or last message in a conversation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与指令数据集不同，没有像Alpaca或ShareGPT这样的标准化存储格式。大多数偏好数据集的结构类似于*表6.1*中所示，包含指令、首选答案和拒绝答案的列。在偏好对齐中，多轮对话不常见。在撰写本文时，主要的微调库不支持多轮对话，通常只提取对话中的第一条或最后一条消息。
- en: Data quantity
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据量
- en: DPO datasets typically require fewer samples than instruction datasets to significantly
    impact model behavior. As with instruction datasets, the required sample count
    depends on model size and task complexity. Larger models are more sample-efficient
    and thus require less data, while complex tasks demand more examples to capture
    the desired behavior. Once again, data quality is crucial, and a large number
    of preference pairs is generally beneficial.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DPO数据集通常比指令数据集需要更少的样本量，才能对模型行为产生显著影响。与指令数据集一样，所需的样本数量取决于模型大小和任务复杂性。较大的模型更有效率地使用样本，因此需要的数据更少，而复杂任务则需要更多的示例来捕捉所需的行为。再次强调，数据质量至关重要，大量偏好对通常是有益的。
- en: General-purpose alignment is used by LLM providers to improve the overall performance
    of the fine-tuned models. This requires preference datasets with millions of samples.
    Major players in the AI industry, including Nvidia and Meta, are converging on
    similar post-training pipelines, involving multiple rounds of preference alignment,
    and extensive use of synthetic data. This consensus suggests that these methods
    are proving to be the most effective for pushing the boundaries of language model
    capabilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通用对齐被LLM提供商用于提高微调模型的总体性能。这需要数百万样本的偏好数据集。AI行业的领军企业，包括Nvidia和Meta，正在趋同于类似的训练后管道，涉及多轮偏好对齐和广泛使用合成数据。这一共识表明，这些方法正在证明是推动语言模型能力边界的最有效方法。
- en: On a smaller scale, the open-source community uses datasets ranging from 10,000
    to 100,000 samples to enhance model performance. This approach has proven effective
    not only in improving benchmark scores but also in healing networks after merging,
    pruning, and other modifications. Generally, DPO is less destructive than SFT
    and has a milder impact on the final model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在较小规模上，开源社区使用从10,000到100,000个样本的数据集来提高模型性能。这种方法不仅证明了在提高基准分数方面的有效性，而且在合并、修剪和其他修改后修复网络方面也有效。一般来说，DPO比SFT破坏性小，对最终模型的影响也较轻。
- en: On the other hand, tasks like the ones previously described require fewer preference
    pairs. Task-specific alignment focuses on improving model performance for a particular
    function, such as modifying the writing style, refusing certain instructions,
    and so on. These alignments can often be achieved with smaller datasets, ranging
    from 100 to 10,000 preference pairs, depending on the task’s complexity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如前所述的任务需要较少的偏好对。特定任务的校准关注于提高模型在特定功能上的性能，例如修改写作风格、拒绝某些指令等。这些校准通常可以通过较小的数据集实现，从
    100 到 10,000 个偏好对不等，具体取决于任务的复杂度。
- en: An example of an application that requires few samples is instructing the model
    to state that it wasn’t trained by OpenAI, Meta, or another LLM provider. This
    can be achieved using a preference dataset, where the rejected answers are those
    claiming alternative origins, and the chosen answers are responses where the model
    correctly states that it was trained by you. A relatively small dataset of 200
    to 500 pairs can be enough for this task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 需要少量样本的应用示例之一是指导模型声明它并非由 OpenAI、Meta 或其他大型语言模型（LLM）提供商训练。这可以通过使用偏好数据集来实现，其中被拒绝的答案是那些声称有其他来源的回答，而选中的答案是模型正确声明由你训练的回答。一个包含
    200 到 500 对的相对较小的数据集可能就足够完成这项任务。
- en: Data generation and evaluation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成和评估
- en: When creating preference datasets, data generation and evaluation are closely
    linked. We first create answers and then rate them to make the final dataset.
    In the following, we introduce both steps as one process instead of two separate
    ones.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建偏好数据集时，数据生成和评估是紧密相连的。我们首先创建答案，然后对其进行评分以形成最终的数据集。在下面的介绍中，我们将这两个步骤作为一个过程而不是两个单独的过程来介绍。
- en: Generating preferences
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成偏好
- en: Before making new preference data, it’s good to look at relevant open-source
    datasets. There are fewer of these compared to instruction datasets, but you can
    find high-quality preference datasets on the Hugging Face Hub. These can be used
    for specific tasks or to add to your own dataset. Well-known preference datasets
    include the Anthropic HH-RLHF dataset, which has human preferences for helpful
    and harmless AI responses, and the OpenAI Summarize from Human Feedback dataset,
    which focuses on article summaries.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建新的偏好数据之前，查看相关的开源数据集是个好主意。与指令数据集相比，这些数据集较少，但在 Hugging Face Hub 上可以找到高质量的偏好数据集。这些数据集可以用于特定任务或添加到你的数据集中。知名偏好数据集包括
    Anthropic HH-RLHF 数据集，该数据集包含人类对有益且无害的 AI 响应的偏好，以及 OpenAI 的基于人类反馈的摘要数据集，该数据集专注于文章摘要。
- en: 'DPO datasets can be created using various methods, each with its own trade-offs
    between quality, cost, and scalability. These methods can be tailored to specific
    applications and require varying degrees of human feedback. We divide them into
    four main categories:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: DPO 数据集可以通过各种方法创建，每种方法都有其质量、成本和可扩展性之间的权衡。这些方法可以根据特定应用进行定制，并需要不同程度的人类反馈。我们将它们分为四个主要类别：
- en: '**Human-generated, human-evaluated datasets**: This method involves hiring
    people to both create responses to prompts and evaluate the quality of these responses.
    While this approach can capture nuanced human preferences and is ideal for complex
    tasks, it’s extremely resource-intensive and difficult to scale. As a result,
    it’s primarily used by large AI companies with substantial resources.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由人类生成、由人类评估的数据集**：这种方法涉及雇佣人员来创建对提示的响应并评估这些响应的质量。虽然这种方法可以捕捉到细微的人类偏好，并且对于复杂任务来说是最理想的，但它极其资源密集且难以扩展。因此，它主要被拥有大量资源的大型
    AI 公司所采用。'
- en: '**Human-generated, LLM-evaluated datasets**: This method can be useful if you
    have a lot of existing human-generated content. However, it’s rarely used in practice
    due to inefficiency, as it still requires significant human input for response
    generation while potentially missing nuanced preferences during the LLM evaluation
    stage.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由人类生成、由 LLM 评估的数据集**：如果你有大量现有的人类生成内容，这种方法可能很有用。然而，由于效率低下，这种方法在实践中很少使用，因为它在生成响应时仍然需要大量的人类输入，同时在
    LLM 评估阶段可能会错过细微的偏好。'
- en: '**LLM-generated, human-evaluated datasets**: This method offers a good balance
    between quality and efficiency. LLMs generate multiple responses to prompts, and
    humans rank these responses. This approach is often preferred because humans are
    generally better at judging answers than writing them from scratch. It allows
    the rapid generation of diverse responses while still capturing human preferences
    effectively. However, it may not provide creative or unexpected responses that
    humans might generate.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由LLM生成、人工评估的数据集**：这种方法在质量和效率之间提供了良好的平衡。LLM对提示生成多个响应，人类对这些响应进行排名。这种方法通常更受欢迎，因为人类通常在判断答案方面比从头开始编写答案更擅长。它允许快速生成多样化的响应，同时仍然有效地捕捉人类偏好。然而，它可能不会提供人类可能生成的创造性或意外响应。'
- en: '**LLM-generated, LLM-evaluated datasets**: Fully synthetic datasets, where
    both generation and evaluation are done by LLMs, are becoming increasingly common
    due to their scalability and cost-effectiveness. This method can produce massive
    datasets quickly and improves as LLM capabilities advance. However, it requires
    careful prompt engineering to ensure quality and diversity, and may perpetuate
    biases or limitations of the generating LLM.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由LLM生成、LLM评估的数据集**：完全合成的数据集，其中生成和评估都由LLM完成，由于其可扩展性和成本效益，正变得越来越普遍。这种方法可以快速生成大量数据集，并随着LLM能力的提升而改进。然而，它需要仔细的提示工程以确保质量和多样性，并可能延续生成LLM的偏见或局限性。'
- en: In practice, human-generated datasets are expensive, difficult to scale, and
    not necessarily of the highest quality. On the other hand, human evaluation is
    quite valuable but can be difficult to scale, which is why large datasets benefit
    from LLM evaluation. In addition to these high-level considerations, the way you
    obtain your data and how you plan to use it also need to be considered. For example,
    applications with many users can embed a feedback mechanism to provide preferences.
    This can be as simple as a `like` and `dislike` score, or something more in-depth
    with text.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，人工生成的数据集成本高昂，难以扩展，且不一定质量最高。另一方面，人工评估非常有价值，但可能难以扩展，这就是为什么大型数据集受益于LLM评估。除了这些高级考虑因素之外，你获取数据的方式以及你打算如何使用它也需要被考虑。例如，拥有许多用户的程序可以嵌入反馈机制以提供偏好。这可以简单到“喜欢”和“不喜欢”评分，或者更深入一些，包括文本。
- en: Note that evaluation is not always required and preferences can emerge naturally
    from the generation process. For instance, it is possible to use a high-quality
    model to generate preferred outputs and a lower-quality or intentionally flawed
    model to produce less preferred alternatives. This creates a clear distinction
    in the preference dataset, allowing more effective training of AI systems to recognize
    and emulate high-quality outputs. The `Intel/orca_dpo_pairs` dataset available
    on the Hugging Face Hub was created with this process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，评估并不总是必需的，偏好可以从生成过程中自然出现。例如，可以使用高质量的模型生成首选输出，而使用低质量或故意有缺陷的模型生成不那么首选的替代方案。这会在偏好数据集中产生清晰的区分，从而更有效地训练AI系统识别和模仿高质量输出。Hugging
    Face Hub上可用的`Intel/orca_dpo_pairs`数据集就是通过这个过程创建的。
- en: Another approach is to compare model-generated outputs with human-written responses,
    which can provide insights into how well the model aligns with actual human preferences
    and highlight areas where the model may be lacking. This can be used to copy a
    particular style and give a more authentic tone to the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是比较模型生成的输出与人工编写的响应，这可以提供关于模型与实际人类偏好的匹配程度以及模型可能缺乏的领域的见解。这可以用来复制特定的风格，并为模型提供更真实的语气。
- en: Tips for data generation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据生成技巧
- en: The data generation is consistent between instruction and preference datasets.
    Prompts should be designed to encourage diversity and complexity in the model’s
    responses. By crafting prompts that explicitly request different approaches or
    styles, we can ensure a wide range of outputs that capture the varied nature of
    human preferences.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成在指令和偏好数据集之间是一致的。提示应该设计成鼓励模型响应的多样性和复杂性。通过制作明确要求不同方法或风格的提示，我们可以确保广泛范围的输出，从而捕捉人类偏好的多样性。
- en: For instance, when generating summaries, one might request variations such as
    concise summaries, detailed summaries, and summaries focusing on key points. This
    approach not only produces a diverse dataset but also helps in understanding how
    different styles and approaches align with human preferences.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在生成摘要时，可能会要求不同的变体，如简洁摘要、详细摘要和关注关键点的摘要。这种方法不仅产生多样化的数据集，还有助于了解不同的风格和方法如何与人类偏好相一致。
- en: Introducing variability in the outputs is another crucial aspect of generating
    synthetic preference datasets. This can be achieved by manipulating the temperature
    settings or employing other sampling methods in the LLM. Higher temperature settings
    tend to produce more creative and diverse responses, while lower settings result
    in more focused and deterministic outputs. This creates a trade-off between diversity
    and coherence, which depends on the kind of data we want to generate. For example,
    generating code requires low creativity, thus low temperature, while writing articles
    can be high temperature.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成合成偏好数据集时，引入输出中的可变性是另一个关键方面。这可以通过调整温度设置或在LLM中采用其他采样方法来实现。较高的温度设置往往会产生更具创造性和多样化的响应，而较低的设置则会导致更专注和确定性的输出。这会在多样性和连贯性之间产生权衡，这取决于我们想要生成的数据类型。例如，生成代码需要较低的创造力，因此需要较低的温度，而撰写文章则可以采用较高的温度。
- en: Using multiple LLMs to generate samples can be better than using just one model.
    Some LLMs are better at specific tasks, and this approach also adds more variety.
    This approach is used by popular open-source datasets like `argilla/Capybara-Preferences`,
    combining GPT-4 with open-weight models. The evaluation process then selects the
    chosen and the rejected answers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个LLM生成样本可能比仅使用一个模型更好。一些LLM在特定任务上表现更好，这种方法也增加了更多多样性。这种方法被流行的开源数据集如`argilla/Capybara-Preferences`所采用，结合了GPT-4和开放权重模型。然后，评估过程会选择选定的和被拒绝的答案。
- en: Evaluating preferences
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估偏好
- en: Data evaluation can be performed by human raters or automated with LLMs. **LLM
    evaluation** involves developing detailed criteria, creating a prompt that clearly
    communicates these guidelines to the LLM, and using the model to select preferred
    and rejected responses. While more scalable than human rating and allowing the
    consistent application of criteria, this quality of LLM evaluation depends directly
    on the model’s performance and the provided guidelines. It may miss subtle human
    preferences or cultural nuances. However, as LLMs continue to improve, their ability
    to make nuanced judgments improves as well, potentially leading to higher-quality
    datasets over time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据评估可以通过人工评分或使用LLM自动化完成。**LLM评估**涉及制定详细的准则，创建一个明确传达这些指南给LLM的提示，并使用该模型选择首选和被拒绝的响应。虽然比人工评分更具可扩展性，并允许一致地应用标准，但这种LLM评估的质量直接取决于模型的表现和提供的指南。它可能错过细微的人类偏好或文化细微差别。然而，随着LLM的持续改进，它们进行细微判断的能力也在提高，这可能导致随着时间的推移生成更高质量的数据集。
- en: Implementing LLM evaluation for preference datasets can be done through absolute
    scoring or pairwise ranking. In absolute scoring, the LLM assigns a numerical
    score or categorical rating to each response based on predefined criteria. This
    method is straightforward but may suffer from inconsistency across different prompts
    or evaluation sessions. Pairwise ranking, on the other hand, involves presenting
    the LLM with two responses and asking it to choose the better one or rank them.
    This approach more closely mimics the format of human evaluation and can lead
    to more consistent results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绝对评分或成对排名来实现LLM对偏好数据集的评估。在绝对评分中，LLM根据预定义的标准为每个响应分配一个数值分数或分类评级。这种方法简单直接，但可能在不同提示或评估会话中存在不一致性。另一方面，成对排名涉及向LLM展示两个响应，并要求它选择更好的一个或对它们进行排名。这种方法更接近于人类评估的格式，并可能导致更一致的结果。
- en: 'For absolute scoring, you would create a prompt that outlines the evaluation
    criteria and asks the LLM to rate the response on a specific scale (e.g., 1-5
    or poor/fair/good/excellent). The prompt might look like this: “Rate the following
    response on a scale of 1-5 based on relevance, coherence, and helpfulness: [`INSERT
    RESPONSE`].” For pairwise ranking, the prompt could be: “Compare the following
    two responses. Which one is better in terms of relevance, coherence, and helpfulness?
    Response A: [`INSERT RESPONSE A`] Response B: [`INSERT RESPONSE B`].”'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于绝对评分，您将创建一个概述评估标准的提示，并要求 LLM 在特定尺度上对回答进行评分（例如，1-5 或差/一般/好/优秀）。提示可能看起来像这样：“根据相关性、连贯性和有帮助性，对以下回答进行
    1-5 级别的评分：[`INSERT RESPONSE`]。”对于成对排名，提示可能是：“比较以下两个回答。在相关性、连贯性和有帮助性方面，哪个更好？回答
    A：[`INSERT RESPONSE A`] 回答 B：[`INSERT RESPONSE B`]。”
- en: The comparative nature of preference datasets makes pairwise ranking an ideal
    approach for evaluation. This method is generally more accurate and more closely
    correlated to human judgment than absolute scoring. Pairwise ranking mimics the
    natural way humans compare options, making it easier for both human raters and
    LLMs to provide consistent and meaningful evaluations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好数据集的比较性质使得成对排名成为评估的理想方法。这种方法通常比绝对评分更准确，并且与人类判断的相关性更紧密。成对排名模仿了人类比较选项的自然方式，使得人类评分者和
    LLM 都能提供一致且有意义的评估。
- en: We can further improve the accuracy of pairwise ranking by providing a ground-truth
    answer and using chain-of-thought reasoning. This approach encourages the evaluating
    LLM to consider multiple aspects of the responses and articulate its decision-making
    process, leading to more thorough and justified evaluations. When no ground-truth
    answer is available, we can prompt the LLM to create a grading note, which is
    a description of the expected answer. This technique works particularly well in
    scenarios where the LLM doesn’t have extensive knowledge about a given topic,
    as it forces the model to establish clear criteria for evaluation before assessing
    the responses.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供真实答案和使用思维链推理，我们可以进一步提高成对排名的准确性。这种方法鼓励评估 LLM 考虑回答的多个方面，并阐明其决策过程，从而得出更全面和合理的评估。当没有真实答案可用时，我们可以提示
    LLM 创建评分笔记，即对预期答案的描述。这种技术在 LLM 对特定主题没有广泛知识的情况下尤其有效，因为它迫使模型在评估回答之前建立明确的评估标准。
- en: 'Here’s a concrete implementation of an LLM-as-a-judge prompt to perform pairwise
    ranking:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个将 LLM 作为裁判的提示进行成对排名的具体实现：
- en: '| **Instruction**You are an answer judge. Your goal is to compare answer A
    and answer B. I want to know which answer does a better job of answering the instruction
    in terms of relevance, accuracy, completeness, clarity, structure, and conciseness.Instruction:
    {instruction}Answer A: {answer_a}Answer B: {answer_b}Explain your reasoning step
    by step and output the letter of the best answer using the following structure:Reasoning:
    (compare the two answers)Best answer: (A or B) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **指令**您是一位答案裁判。您的目标是比较答案 A 和答案 B。我想知道哪个答案在相关性、准确性、完整性、清晰度、结构和简洁性方面更好地回答了指令。指令：{instruction}答案
    A：{answer_a}答案 B：{answer_b}逐步解释您的推理，并使用以下结构输出最佳答案的字母：(比较两个答案)最佳答案：(A 或 B) |'
- en: '*Table 6.2* – Example of LLM-as-a-judge prompt for pairwise ranking with one
    instruction and two answers'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 6.2* – 使用一个指令和两个答案的 LLM 作为裁判的成对排名提示示例'
- en: 'However, it’s important to note that LLM-based evaluation can be subject to
    several types of bias:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，基于 LLM 的评估可能受到几种类型的偏差的影响：
- en: '**Position bias**: In relative scoring, LLM judges tend to favor the first
    answer presented. This bias can skew results and lead to inaccurate preferences.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置偏差**：在相对评分中，LLM 裁判倾向于偏好第一个呈现的答案。这种偏差可能会扭曲结果并导致不准确的选择。'
- en: '**Length bias**: Similar to humans, LLM judges often show a preference for
    longer answers, potentially overlooking the quality of shorter, more concise responses.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长度偏差**：与人类类似，LLM 裁判通常偏好较长的答案，可能会忽略较短、更简洁的回答的质量。'
- en: '**Family bias**: LLM judges may favor responses that are generated by themselves
    or models from the same family, potentially due to similarities in language patterns
    or knowledge bases.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**家族偏差**：LLM 裁判可能会偏好由自己或同一家族的模型生成的回答，这可能是由于语言模式或知识库的相似性。'
- en: To mitigate these biases and enhance the quality of preference datasets, several
    solutions can be implemented. One key approach is to randomize the order of answer
    A and answer B in each comparison, which can counteract position bias by ensuring
    that the order of presentation doesn’t consistently influence the evaluation.
    Another valuable strategy involves providing few-shot examples that demonstrate
    a balanced distribution of scores. These examples serve to calibrate the judge
    LLM’s internal scoring mechanism and can effectively address both length and family
    bias by illustrating that shorter answers or those from different model families
    can also be of high quality. Additionally, employing multiple models as a jury,
    rather than relying on a single LLM judge, can significantly improve the robustness
    of the evaluation process. This multi-model approach helps to balance out individual
    biases that may be present in any single model, leading to a more comprehensive
    and accurate assessment of the responses.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些偏差并提高偏好数据集的质量，可以实施几种解决方案。一种关键的方法是在每次比较中随机化答案A和答案B的顺序，这可以通过确保展示顺序不会持续影响评估来抵消位置偏差。另一种有价值的策略是提供少量示例，以展示分数的平衡分布。这些示例用于校准评判LLM的内部评分机制，并可以通过展示较短答案或来自不同模型家族的答案也可以是高质量的来有效解决长度和家族偏差。此外，采用多个模型作为陪审团，而不是依赖单个LLM评判员，可以显著提高评估过程的鲁棒性。这种多模型方法有助于平衡单个模型中可能存在的任何个人偏差，从而对响应进行更全面和准确的评估。
- en: In the next section, we will create our own preference dataset. We will rely
    on the data generation process to naturally create chosen (human-generated) and
    rejected (LLM-generated) answers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建自己的偏好数据集。我们将依靠数据生成过程自然地创建选定的（人工生成）和拒绝的（LLM生成）答案。
- en: Creating our own preference dataset
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们自己的偏好数据集
- en: Our model can currently write paragraphs about topics related to machine learning,
    but it doesn’t have the same writing style as the original authors. This is a
    typical use case for preference alignment, where we want to change the “voice”
    of the model to closely imitate the source data. It’s important to note that,
    experimentally, DPO tends to make models more verbose and pushes them to use very
    formal language. Therefore, the training will need to use DPO surgically to avoid
    this pitfall and instead adopt the less formal style of these blog articles.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式目前可以就与机器学习相关的话题撰写段落，但它没有与原始作者相同的写作风格。这是一个典型的偏好对齐用例，我们希望改变模型的“声音”以更接近源数据。需要注意的是，实验上，DPO往往使模型更加冗长，并推动它们使用非常正式的语言。因此，训练将需要使用DPO进行精细操作，以避免这种陷阱，并采用这些博客文章的较少正式的风格。
- en: In this section, we will create a preference dataset where the chosen answers
    are extracts from the text, while rejected answers are generated by the model.
    To implement it, we will modify the code created in *Chapter 5*, which was designed
    to generate instruction datasets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个偏好数据集，其中选定的答案是从文本中提取的，而拒绝的答案是模型生成的。为了实现这一点，我们将修改在*第五章*中创建的代码，该代码旨在生成指令数据集。
- en: 'As seen in the previous section, preference and instruction datasets rely on
    the same principles. Instead of pairs of instructions and answers, we need triples
    (instruction, answer 1, answer 2). What’s interesting in this setting is that
    we have ground-truth answers in the text chunks, which means we don’t need complex
    evaluation processes like LLM judges. To make sure that these extracts are high-quality,
    we will implement two additional quality filters, based on length and punctuation.
    *Figure 6.2* summarizes the end-to-end process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所示，偏好和指令数据集依赖于相同的原则。而不是指令和答案的配对，我们需要三元组（指令，答案1，答案2）。在这个设置中有趣的是，我们在文本块中有真实答案，这意味着我们不需要像LLM评判员那样的复杂评估过程。为了确保这些提取是高质量的，我们将实施两个额外的质量过滤器，基于长度和标点。*图6.2*总结了端到端的过程：
- en: '![A black background with white lines  Description automatically generated](img/B31105_06_02.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![黑色背景，白色线条，自动生成描述](img/B31105_06_02.png)'
- en: Figure 6.2 – Synthetic data generation pipeline from raw text to preference
    dataset
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 从原始文本到偏好数据集的合成数据生成流程
- en: 'We are now ready to implement the preference data generation pipeline:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好实现偏好数据生成流程：
- en: We start by importing the necessary libraries.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库。
- en: '[PRE0]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Instead of the `InstructionAnswerSet` class, we now have a `PreferenceSet` class.
    This class is designed to handle triples of instructions, generated answers (rejected),
    and extracted answers (chosen).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在不再有 `InstructionAnswerSet` 类，而是有一个 `PreferenceSet` 类。该类旨在处理指令的三元组、生成的答案（被拒绝）和提取的答案（被选中）。
- en: '[PRE1]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `load_articles_from_json`, `clean_text`, and `extract_substrings` functions
    remain unchanged from the original code. Let’s start with `load_articles_from_json`,
    which takes our JSON file (`cleaned_documents.json`) containing the articles as
    input and returns a Hugging Face dataset with the text and metadata (ID, platform,
    author ID, author full name, link).
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_articles_from_json`、`clean_text` 和 `extract_substrings` 函数与原始代码保持不变。让我们从
    `load_articles_from_json` 开始，它接受包含文章的 JSON 文件（`cleaned_documents.json`）作为输入，并返回一个包含文本和元数据（ID、平台、作者
    ID、作者全名、链接）的 Hugging Face 数据集。'
- en: '[PRE2]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `clean_text` function removes non-alphanumeric characters except for apostrophes,
    periods, commas, exclamation marks, and question marks. It also replaces multiple
    whitespaces with a single space to ensure proper formatting.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`clean_text` 函数移除非字母数字字符（除撇号、句号、逗号、感叹号和问号外），并将多个空格替换为单个空格，以确保正确的格式。'
- en: '[PRE3]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `extract_substrings` function splits articles into chunks with a length
    between 1,000 and 2,000 characters. To make sure that the splitting doesn’t break
    sentences, which could modify their meanings, we use a regex to only split after
    the end of a sentence.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`extract_substrings` 函数将文章分割成长度在 1,000 到 2,000 个字符之间的块。为了确保分割不会破坏句子，从而改变其含义，我们使用正则表达式仅在句子末尾进行分割。'
- en: '[PRE4]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `generate_preference_triples` function replaces the original `generate_instruction_answer_pairs`
    function. The prompt is adapted from the instruction version and is designed to
    generate triples instead of pairs. It also provides general guidance about the
    type of instructions we’re interested in, how to extract answers from articles,
    and how to style them:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`generate_preference_triples` 函数取代了原始的 `generate_instruction_answer_pairs`
    函数。提示信息从指令版本中调整，旨在生成三元组而不是对。它还提供了关于我们感兴趣的指令类型、如何从文章中提取答案以及如何格式化它们的通用指导：'
- en: '[PRE5]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the same function, we use GPT-4o-mini to generate our answers using JSON
    mode. We specify in the system prompt that we want triples instead of pairs. The
    JSON answers are directly parsed by our `PreferenceSet` class to return the expected
    list of tuples.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一函数中，我们使用 GPT-4o-mini 以 JSON 模式生成我们的答案。我们在系统提示中指定我们想要三元组而不是对。JSON 答案直接由我们的
    `PreferenceSet` 类解析，以返回预期的元组列表。
- en: '[PRE6]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Two new filtering functions are introduced for the preference data pipeline:
    `filter_short_answers` and `filter_answer_format`. These functions filter out
    short answers and ensure that answers start with an uppercase letter and end with
    proper punctuation. We use them as heuristics to filter out samples with poor
    quality.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为偏好数据处理管道引入了两个新的过滤函数：`filter_short_answers` 和 `filter_answer_format`。这些函数过滤掉短答案，并确保答案以大写字母开头并正确结束。我们将它们用作启发式方法来过滤掉质量较差的样本。
- en: '[PRE7]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `create_preference_dataset` function replaces the original `create_instruction_dataset`
    function. This function now works with triples instead of pairs and uses different
    column names in the resulting dataset.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`create_preference_dataset` 函数取代了原始的 `create_instruction_dataset` 函数。现在，该函数使用三元组而不是对，并在生成的数据集中使用不同的列名。'
- en: '[PRE8]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The main function is updated to include the new filtering steps and to use
    the preference dataset creation function:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主要功能已更新，包括新的过滤步骤，并使用偏好数据集创建功能：
- en: '[PRE9]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `create_preference_dataset()` function generated 2,970 samples. This dataset
    is then heavily filtered to only retain 1,467 samples by removing answers that
    are too short or not properly formatted (for example, answers that start with
    an uppercase letter or end with a period, exclamation mark, or question mark).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_preference_dataset()` 函数生成了 2,970 个样本。然后，通过删除过短或格式不正确的答案（例如，以大写字母开头或以句号、感叹号或问号结尾的答案），该数据集被大量过滤，仅保留
    1,467 个样本。'
- en: 'The final dataset is available on the Hugging Face Hub at the following address:
    [https://huggingface.co/datasets/mlabonne/llmtwin-dpo](https://huggingface.co/datasets/mlabonne/llmtwin-dpo).
    You can see in *Figure 6.3* an example that captures a subtle nuance in terms
    of writing style. Both answers are correct, but the **chosen** (extracted) answer
    sounds slightly more casual.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最终数据集可在以下 Hugging Face Hub 地址获取：[https://huggingface.co/datasets/mlabonne/llmtwin-dpo](https://huggingface.co/datasets/mlabonne/llmtwin-dpo)。您可以在
    *图 6.3* 中看到一个示例，它捕捉到了写作风格方面的微妙差异。两个答案都是正确的，但**选择的**（提取的）答案听起来稍微随意一些。
- en: '![A screenshot of a computer  Description automatically generated](img/B31105_06_03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由系统自动生成](img/B31105_06_03.png)'
- en: Figure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the
    Hugging Face Hub
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – Hugging Face Hub 上 mlabonne/llmtwin-dpo 偏好数据集的屏幕截图
- en: To produce this dataset, we iterated many times over the prompt to generate
    the data. This required some manual evaluation and experiments until we reached
    satisfying results. The quality of the prompt is fundamental in this process,
    which is why it is recommended to follow a similar process to generate your own
    preference datasets.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成这个数据集，我们多次迭代提示以生成数据。这需要一些手动评估和实验，直到我们达到令人满意的结果。提示的质量在这个过程中至关重要，这就是为什么建议遵循类似的过程来生成您自己的偏好数据集。
- en: In the next section, we will introduce concepts related to **Reinforcement Learning
    from Human Feedback** (**RLHF**) and DPO. This will cover new parameters and ideas
    that are implemented in the final section of this chapter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍与 **来自人类反馈的强化学习**（RLHF）和 DPO 相关的概念。这包括在本章最后部分实现的新参数和想法。
- en: Preference alignment
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏好对齐
- en: 'Preference alignment regroups techniques to fine-tune models on preference
    data. In this section, we provide an overview of this field and then focus on
    the technique we will implement: **Direct Preference Optimization** (**DPO**).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好对齐重新组合了用于在偏好数据上微调模型的技巧。在本节中，我们概述了这个领域，然后重点介绍我们将要实施的技巧：**直接偏好优化**（DPO）。
- en: Reinforcement Learning from Human Feedback
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习
- en: '**Reinforcement Learning from Human Feedback** (**RLHF**) combines **reinforcement
    learning** (**RL**) with human input to align models with human preferences and
    values. RLHF emerged as a response to challenges in traditional RL methods, particularly
    the difficulty of specifying reward functions for complex tasks and the potential
    for misalignment between engineered rewards and intended objectives.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自人类反馈的强化学习**（RLHF）将 **强化学习**（RL）与人类输入相结合，以使模型与人类偏好和价值观保持一致。RLHF 作为对传统 RL
    方法挑战的回应而出现，特别是指定复杂任务的奖励函数的困难以及工程奖励与预期目标之间可能出现的偏差。'
- en: The origins of RLHF can be traced back to the field of **preference-based reinforcement
    learning** (**PbRL**), which was independently introduced by Akrour et al. and
    Cheng et al. in 2011\. PbRL aimed to infer objectives from qualitative feedback,
    such as pairwise preferences between behaviors, rather than relying on quantitative
    reward signals. This approach addressed some of the limitations of conventional
    RL, where defining appropriate reward functions can be challenging and prone to
    reward hacking or unintended behaviors.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 的起源可以追溯到基于偏好的强化学习（PbRL）领域，该领域由 Akrour 等人和 Cheng 等人于 2011 年独立引入。PbRL 旨在从定性反馈中推断目标，例如行为之间的成对偏好，而不是依赖于定量奖励信号。这种方法解决了传统强化学习的一些局限性，在传统强化学习中，定义适当的奖励函数可能具有挑战性，并且容易受到奖励黑客攻击或产生意外行为。
- en: The term RLHF was coined later, around 2021-2022, as the approach gained prominence
    in the context of training LLMs. However, the core ideas had been developing for
    years prior. A seminal paper by Christiano et al. in 2017 demonstrated the effectiveness
    of learning reward models from human preferences and using them to train RL agents.
    This work showed that RLHF could match or exceed the performance of agents trained
    on hand-engineered rewards, but with significantly less human effort.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 这个术语是在 2021-2022 年左右提出的，因为这种方法在训练大型语言模型（LLMs）的背景下获得了显著的关注。然而，其核心思想已经发展了数年。Christianos
    等人在 2017 年发表的一篇开创性论文展示了从人类偏好中学习奖励模型并使用它们来训练强化学习代理的有效性。这项工作表明，RLHF 可以匹配或超过基于手工设计的奖励训练的代理的性能，但所需的人类努力显著减少。
- en: 'At its core, RLHF works by iteratively improving both a reward model and a
    policy:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，RLHF 通过迭代改进奖励模型和政策来工作：
- en: '**Reward model learning**: Instead of using a pre-defined reward function,
    RLHF learns a reward model from human feedback. This is typically done by presenting
    humans with different answers and asking them to indicate which one they prefer.
    These preferences are used to train a reward model, often using a Bradley-Terry
    model or similar approaches that map preferences to underlying utility functions.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励模型学习**：RLHF不是使用预定义的奖励函数，而是从人类反馈中学习奖励模型。这通常是通过向人类展示不同的答案并询问他们更喜欢哪一个来完成的。这些偏好被用来训练奖励模型，通常使用布拉德利-特里模型或类似的方法，将偏好映射到潜在效用函数。'
- en: '**Policy optimization**: With the learned reward model, standard RL algorithms
    can be used to optimize a policy. This policy generates new behaviors that aim
    to maximize the predicted rewards from the learned model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略优化**：使用学习到的奖励模型，可以采用标准的强化学习算法来优化策略。这种策略生成新的行为，旨在最大化从学习模型预测的奖励。'
- en: '**Iterative improvement**: As the policy improves, it generates new behaviors
    that can be evaluated by humans, leading to refinements in the reward model. This
    cycle continues, ideally resulting in a policy that aligns well with human preferences.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代改进**：随着策略的改进，它会产生新的行为，这些行为可以通过人类进行评估，从而对奖励模型进行细化。这种循环持续进行，理想情况下会导致与人类偏好高度一致的政策。'
- en: A key innovation in RLHF is its approach to handling the high cost of human
    feedback. Rather than requiring constant human oversight, RLHF allows for asynchronous
    and sparse feedback.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与人类反馈的关键创新在于其处理人类反馈高成本的方法。而不是要求持续的人类监督，强化学习与人类反馈允许异步和稀疏的反馈。
- en: The learned reward model serves as a proxy for human preferences, enabling the
    RL algorithm to train continuously without direct human input for every action.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的奖励模型作为人类偏好的代理，使得强化学习算法能够在没有直接人类输入的情况下，为每个动作持续训练。
- en: As an example, *Figure 6.4* shows a high-level view of the **Proximal Policy
    Optimization** (**PPO**) algorithm, which is one of the most popular RLHF algorithms.
    Here, the reward model is used to score the text that is generated by the trained
    model. This reward is regularized by an additional **Kullback–Leibler** (**KL**)
    divergence factor, ensuring that the distribution of tokens stays similar to the
    model before training (frozen model).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图6.4*展示了**近端策略优化**（**PPO**）算法的高级视图，这是最受欢迎的强化学习与人类反馈融合（RLHF）算法之一。在此，奖励模型用于评估训练模型生成的文本。这种奖励通过额外的**库尔巴克-莱布勒**（**KL**）散度因子进行正则化，确保训练前（冻结模型）的标记分布保持相似。
- en: '![A diagram of a data flow  Description automatically generated](img/B31105_06_04.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![数据流图  描述自动生成](img/B31105_06_04.png)'
- en: Figure 6.4 – High-level view of the PPO algorithm for preference alignment
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 偏好对齐的PPO算法的高级视图
- en: While RLHF has proven effective for aligning AI systems with human preferences,
    it faces challenges due to its iterative nature and reliance on a separate reward
    model, which can be computationally expensive and potentially unstable. Despite
    theoretical superiority, RLHF algorithms have also experimentally underperformed
    compared to simpler approaches. One such approach that has gained significant
    attention is DPO.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RLHF在将人工智能系统与人类偏好对齐方面已被证明是有效的，但由于其迭代性质和对单独奖励模型的依赖，它面临着挑战。这种依赖可能导致计算成本高昂且可能不稳定。尽管在理论上具有优越性，但RLHF算法在实验中与简单方法相比也表现不佳。其中一种受到广泛关注的方法是DPO。
- en: Direct Preference Optimization
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接偏好优化
- en: 'Introduced by Rafailov et al. in their 2023 paper *Direct Preference Optimization:
    Your Language Model is Secretly a Reward Model*, DPO offers a streamlined alternative
    to traditional RLHF methods.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年的论文《直接偏好优化：你的语言模型实际上是奖励模型》中，Rafailov等人引入了DPO，它为传统的RLHF方法提供了一种简化的替代方案。
- en: DPO’s core innovation lies in its reformulation of the preference learning problem.
    Unlike RLHF, which typically involves training a separate reward model and then
    using reinforcement learning algorithms like PPO to fine-tune the language model,
    DPO takes a more direct approach.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: DPO的核心创新在于其对偏好学习问题的重新表述。与RLHF不同，RLHF通常涉及训练一个单独的奖励模型，然后使用如PPO之类的强化学习算法来微调语言模型，DPO采取了一种更直接的方法。
- en: It derives a closed-form expression for the optimal policy under the standard
    RLHF objective of maximizing expected reward subject to a KL-divergence constraint
    with a reference policy. This mathematical insight allows DPO to express the preference
    learning problem directly in terms of the policy, eliminating the need for a separate
    reward model or complex reinforcement learning algorithms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 它在标准RLHF目标下，即在具有参考策略的KL散度约束下最大化预期奖励的情况下，推导出最优策略的闭式表达式。这一数学洞察力使得DPO可以直接用策略来表述偏好学习问题，从而消除了需要单独的奖励模型或复杂的强化学习算法的需求。
- en: In practical terms, DPO can be implemented as a simple binary cross-entropy
    loss function that operates directly on the language model’s output probabilities.
    This loss function encourages the model to assign higher probability to preferred
    responses and lower probability to non-preferred responses, while maintaining
    closeness to a reference (frozen) model. The importance of the reference model
    is directly controlled via a beta parameter between 0 and 1\. The reference model
    is ignored when beta is equal to 0, which means that the trained model can be
    very different from the SFT one. In practice, a value of 0.1 is the most popular
    one, but this can be tweaked, as we’ll see in the next section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，DPO可以作为一个简单的二元交叉熵损失函数实现，该函数直接作用于语言模型的输出概率。这个损失函数鼓励模型将更高的概率分配给首选响应，将更低的概率分配给非首选响应，同时保持与参考（冻结）模型的接近。通过0到1之间的beta参数直接控制参考模型的重要性。当beta等于0时，参考模型被忽略，这意味着训练的模型可以与SFT模型非常不同。在实践中，0.1是最受欢迎的值，但这个值可以根据下一节的内容进行调整。
- en: The simplicity of this approach allows optimization using standard gradient
    descent techniques, without the need for sampling from the model during training
    or implementing complex RL algorithms. *Figure 6.5* shows a high-level view of
    the DPO algorithm, greatly simplifying the training process compared to *Figure
    6.4*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的简单性允许使用标准的梯度下降技术进行优化，无需在训练期间从模型中采样或实现复杂的RL算法。*图6.5*展示了DPO算法的高级视图，与*图6.4*相比，极大地简化了训练过程。
- en: '![A diagram of a data flow  Description automatically generated](img/B31105_06_05.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![数据流图  描述自动生成](img/B31105_06_05.png)'
- en: Figure 6.5 – High-level view of the DPO algorithm for preference alignment
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – DPO算法的偏好对齐高级视图
- en: DPO has several advantages over traditional RLHF methods. As previously mentioned,
    it significantly simplifies the preference learning pipeline, reducing the engineering
    complexity associated with RLHF methods. By eliminating the need for a separate
    reward model and RL algorithms, DPO is more computationally efficient than traditional
    RLHF approaches. Particularly when trained with adapters (LoRA, QLoRA), the frozen
    and trained models don’t have to be separated. Indeed, since we’re only training
    adapters, the trained model is not modified. This allows us to only load one model
    instead of two, which saves additional VRAM.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DPO相对于传统的RLHF方法具有几个优势。如前所述，它显著简化了偏好学习流程，减少了与RLHF方法相关的工程复杂性。通过消除对单独的奖励模型和RL算法的需求，DPO比传统的RLHF方法在计算效率上更高。特别是当与适配器（LoRA，QLoRA）一起训练时，冻结和训练的模型不需要分离。实际上，因为我们只训练适配器，所以训练的模型不会被修改。这使得我们只需要加载一个模型而不是两个，从而节省了额外的VRAM。
- en: Despite its simplicity, DPO often matches the performance of more complex RLHF
    methods. It also tends to be more stable during training and less sensitive to
    hyperparameters. The simplified approach makes DPO easier to implement and scale,
    particularly for small teams without extensive RL knowledge.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法很简单，DPO通常与更复杂的RLHF方法的性能相匹配。它还倾向于在训练期间更加稳定，对超参数的敏感性更低。简化的方法使得DPO更容易实现和扩展，尤其是对于没有广泛RL知识的中小团队来说。
- en: While RLHF allows iterative improvement through multiple training rounds and
    can dynamically adapt to new preferences, DPO offers a more straightforward path
    to achieving similar results. The choice between DPO and PPO-based RLHF often
    comes down to a trade-off between ease of implementation and potential peak performance.
    For large-scale training runs with millions of preference samples, PPO-inspired
    methods still have a higher performance ceiling. However, for most applications,
    DPO provides the majority of the performance benefits at a lower computational
    and engineering cost.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RLHF（Reinforcement Learning from Human Feedback）允许通过多个训练轮次进行迭代改进，并且可以动态适应新的偏好，但DPO提供了实现类似结果的更直接途径。DPO与基于PPO的RLHF之间的选择通常取决于实现简便性和潜在峰值性能之间的权衡。对于包含数百万个偏好样本的大规模训练运行，受PPO启发的方
    法仍然具有更高的性能上限。然而，对于大多数应用来说，DPO在较低的计算和工程成本下提供了大部分的性能优势。
- en: Both RLHF and DPO benefit significantly from the integration of synthetic data.
    As LLMs become more capable, they can generate data that surpasses human-created
    content in quality and diversity. This enables a virtuous cycle where better models
    produce better training data, which in turn leads to further model improvements.
    The iterative nature of both approaches allows multiple rounds of model refinement,
    each focusing on different aspects of model performance and gradually enhancing
    capabilities across various domains.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF和DPO都显著受益于合成数据的集成。随着LLM（Large Language Model）能力的增强，它们可以生成在质量和多样性上超越人类创造内容的数据。这创造了一个良性循环，即更好的模型产生更好的训练数据，反过来又导致模型进一步改进。这两种方法的迭代性质允许进行多轮模型精炼，每轮都关注模型性能的不同方面，并逐渐增强各个领域的功能。
- en: Despite its advantages, DPO is not without drawbacks. Like RLHF, DPO still requires
    paired preference data, which can be expensive and time-consuming to collect.
    DPO lacks some of the theoretical guarantees associated with reinforcement learning
    approaches. There may be scenarios where the added flexibility of RLHF is beneficial,
    particularly for complex tasks or environments.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DPO具有优势，但它并非没有缺点。像RLHF一样，DPO仍然需要成对的偏好数据，这可能很昂贵且耗时收集。DPO缺乏与强化学习方法相关的某些理论保证。可能存在某些场景，其中RLHF增加的灵活性是有益的，特别是对于复杂任务或环境。
- en: Nonetheless, DPO is ideal in most cases, including our twin LLM example. In
    the next section, we will implement it using Unsloth.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，DPO在大多数情况下都是理想的，包括我们的双LLM示例。在下一节中，我们将使用Unsloth实现它。
- en: Implementing DPO
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DPO
- en: In this section, we will DPO fine-tune the **TwinLlama-3.1-8B** model we created
    in *Chapter 5*. For ease of use and to maximize performance, we will again use
    the Unsloth library for our DPO implementation. Depending on the available VRAM,
    you can choose between LoRA (higher quality, speed, and VRAM usage) and QLoRA
    (lower quality, speed, and VRAM usage). This technique, along with other preference
    alignment algorithms, is also available in TRL and Axolotl.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对在第5章中创建的**TwinLlama-3.1-8B**模型进行DPO微调。为了便于使用并最大化性能，我们再次使用Unsloth库来实现我们的DPO。根据可用的VRAM，您可以选择LoRA（更高质量、速度和VRAM使用）和QLoRA（较低质量、速度和VRAM使用）。这种技术与其他偏好对齐算法一起，也存在于TRL和Axolotl中。
- en: This example can be seen as an advanced application of DPO. Indeed, our objective
    of imitating a writing style conflicts with the natural tendency of DPO to encourage
    formal language. This is partly due to the fact that chosen answers are often
    more formal than rejected ones. In practice, this will force us to do light fine-tuning,
    with a low learning rate and number of epochs. To find the best hyperparameters,
    we trained over 20 models and compared their outputs on a set of questions, including
    “Write a paragraph to introduce supervised fine-tuning.” This allowed us to select
    the model and parameters that worked best for this task.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子可以被视为DPO（Document Positioning Operator）的高级应用。事实上，我们模仿写作风格的目标与DPO鼓励正式语言的自然倾向相冲突。这部分原因是因为所选答案通常比被拒绝的答案更正式。在实践中，这会迫使我们进行轻微的微调，使用低学习率和较少的epoch数。为了找到最佳的超参数，我们在一组问题上进行训练，包括“写一段介绍监督微调的段落。”这使我们能够选择最适合此任务的模型和参数。
- en: 'The dependencies are the same as those in *Chapter 5* with SFT and can be found
    in the book’s GitHub repository ([https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering))
    or in Unsloth’s repo ([https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项与第5章中的SFT相同，可以在本书的GitHub仓库（[https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering)）或Unsloth的仓库（[https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)）中找到：
- en: 'First, we want to access a gated model and (optionally) upload our fine-tuned
    model to Hugging Face ([https://huggingface.co/](https://huggingface.co/)). This
    requires us to log in to an account. If you don’t have an account, you can create
    one and store your API key (**Settings | Access Tokens | Create new token**) in
    the `.env` file:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们想要访问一个门控模型，并且（可选地）将我们的微调模型上传到Hugging Face（[https://huggingface.co/](https://huggingface.co/)）。这需要我们登录一个账户。如果您没有账户，您可以创建一个，并将API密钥（**设置
    | 访问令牌 | 创建新令牌**）存储在`.env`文件中：
- en: '[PRE10]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Make sure that your Comet ML API key is also in the `.env` file. Otherwise,
    the code will crash and raise an error when training starts.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的Comet ML API密钥也包含在`.env`文件中。否则，当训练开始时，代码会崩溃并引发错误。
- en: '[PRE11]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Before we import all the necessary packages, we want to apply a patch for the
    `DPOTrainer` class from TRL. This fixes the DPO logs in notebook environments.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在导入所有必要的包之前，我们想要对TRL中的`DPOTrainer`类应用一个补丁。这修复了笔记本环境中的DPO日志。
- en: '[PRE12]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can now import the other libraries. The main difference between DPO and SFT
    is the import of `DPOConfig` and `DPOTrainer` from TRL, which are specific to
    DPO training.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以导入其他库。DPO与SFT之间的主要区别是导入TRL中的`DPOConfig`和`DPOTrainer`，它们是DPO训练特有的。
- en: '[PRE13]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This step loads our fine-tuned model from *Chapter 5*. We use the same configuration
    with a `max_seq_length` of 2048\. You can activate QLoRA by setting `load_in_4bit`
    to `True`. In the following, we will perform LoRA DPO fine-tuning for increased
    speed and quality.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此步骤加载第5章中的微调模型。我们使用相同的配置，`max_seq_length`为2048。您可以通过将`load_in_4bit`设置为`True`来激活QLoRA。在以下步骤中，我们将执行LoRA
    DPO微调以提高速度和质量。
- en: '[PRE14]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s now prepare the model for PEFT with the LoRA configuration. We increase
    the rank (`r`) and `lora_alpha` from `32` (as it was in *Chapter 5*) to `64`.
    This will allow more expressive fine-tuning. We keep a dropout of `0` for speed
    and we target every linear module as per usual.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用LoRA配置准备模型进行PEFT。我们将排名（`r`）和`lora_alpha`从第5章中的`32`增加到`64`。这将允许更丰富的微调。我们保持`0`的dropout以加快速度，并针对每个线性模块进行操作。
- en: '[PRE15]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We load the `llmtwin-dpo` dataset (training split), which contains our prompts,
    chosen, and rejected answers.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载`llmtwin-dpo`数据集（训练分割），其中包含我们的提示、选择和拒绝的答案。
- en: '[PRE16]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The data preparation is significantly different from the SFT example in *Chapter
    5*. Here, we have triples with a prompt, a chosen answer, and a rejected answer.
    In the `format_samples` function, we apply the Alpaca chat template to each individual
    message. Note that the instruction is the only one that requires the chat format:
    chosen and rejected answers only need to be concatenated with the **end of sentence**
    (**EOS**) token. Finally, we create a train/test split with a 95%/5% ratio.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备与第5章中的SFT示例有显著不同。在这里，我们有包含提示、选择答案和拒绝答案的三元组。在`format_samples`函数中，我们将Alpaca聊天模板应用于每个单独的消息。请注意，指令是唯一需要聊天格式的：选择和拒绝答案只需要与**句子结束标记**（**EOS**）连接。最后，我们以95%/5%的比例创建一个训练/测试分割。
- en: '[PRE17]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The model and data are now ready, so we can start fine-tuning. Compared to SFT,
    there are a few new parameters, like `ref_model` and `beta`. Since we’re using
    LoRA (or QLoRA), we don’t directly train the model but instead the adapters. This
    means we can use the original model (without adapters) as a reference, saving
    a lot of VRAM. The `beta` parameter controls the importance of the reference model.
    A standard value of 0.1 works well in most scenarios, but we decided to increase
    it to 0.5 based on our experiments. This is due to the fact that the trained model
    used formal language with lower values. Having it closer to the reference model
    helps to fix this issue.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型和数据现在已准备就绪，我们可以开始微调。与SFT相比，有一些新的参数，如`ref_model`和`beta`。由于我们使用LoRA（或QLoRA），我们不是直接训练模型，而是训练适配器。这意味着我们可以使用原始模型（不带适配器）作为参考，节省大量VRAM。`beta`参数控制参考模型的重要性。在大多数情况下，标准值0.1效果良好，但根据我们的实验，我们决定将其增加到0.5。这是因为训练模型使用了较低值的正式语言。使其更接近参考模型有助于解决这个问题。
- en: 'The learning rate is also lower (from 3e-4 for SFT to 2e-6 here). We train
    for 1 epoch instead of 3, and the `max_seq_length` parameter is now broken down
    into two new parameters: `max_prompt_length` (prompt only) and `max_length` (prompt
    and answer). Note that we also replaced the `TrainingArguments` class with `DPOConfig`.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率也降低了（从SFT的3e-4到这里的2e-6）。我们训练1个epoch而不是3个，并且`max_seq_length`参数现在被分解为两个新的参数：`max_prompt_length`（仅提示）和`max_length`（提示和答案）。请注意，我们还用`DPOConfig`类替换了`TrainingArguments`类。
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once the model is trained, we can run it for a quick sanity check. This step
    is similar to the SFT example. It prepares the model for inference and generates
    a response to a prompt.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以运行它进行快速合理性检查。这一步骤与SFT示例类似。它准备模型进行推理，并对提示生成响应。
- en: '[PRE19]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The trained DPO model returns the following response:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练好的DPO模型返回以下响应：
- en: '[PRE20]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can compare it with the answer provided by the SFT model:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其与SFT模型提供的答案进行比较：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The DPO model provides an answer that is both more accurate and closer to the
    desired writing style. It correctly identifies pre-training language models as
    source models for SFT. It also mentions domain or task-specific finetunes instead
    of alignment with “human expectations,” which is closer to the preference alignment
    stage. The answer is also less formal and something we would use in a blog post.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: DPO模型提供的答案既更准确，又更接近期望的写作风格。它正确地将预训练语言模型识别为SFT的源模型。它还提到了特定领域或任务的微调，而不是与“人类期望”对齐，这更接近偏好对齐阶段。答案也更不正式，是我们会在博客文章中使用的。
- en: Finally, the last step consists of saving the trained model locally and pushing
    it to the Hugging Face Hub.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，最后一步包括将训练好的模型保存在本地并将其推送到Hugging Face Hub。
- en: '[PRE22]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Congratulations! We have trained and exported our DPO model. It is now available
    on the Hugging Face Hub at [https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO](https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO).
    Compared to SFT, DPO has a few additional metrics that need to be tracked during
    training. *Figure 6.6* shows the Comet ML dashboard with the main metrics. You
    can publicly access it using the following URL: [https://www.comet.com/mlabonne/llm-twin-training/](https://www.comet.com/mlabonne/llm-twin-training/)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们已经训练并导出了我们的DPO模型。它现在可在Hugging Face Hub上找到，网址为[https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO](https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO)。与SFT相比，DPO在训练期间需要跟踪一些额外的指标。*图6.6*显示了Comet
    ML仪表板上的主要指标。您可以使用以下URL公开访问它：[https://www.comet.com/mlabonne/llm-twin-training/](https://www.comet.com/mlabonne/llm-twin-training/)
- en: '![](img/B31105_06_06.png)![](img/B31105_06_07.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_06_06.png)![](img/B31105_06_07.png)'
- en: Figure 6.6 – Experiment tracking in Comet ML with DPO metrics
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 使用DPO指标在Comet ML中进行实验跟踪
- en: 'Let’s review these metrics:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些指标：
- en: '**Training loss**: We still want the loss to continuously decrease on average.
    Note that it can rapidly fall to zero, meaning that the model is no longer learning
    anything. This behavior doesn’t necessarily lead to overfitting or bad models
    but needs to be monitored closely.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练损失**：我们仍然希望平均损失持续下降。请注意，它可能会迅速下降到零，这意味着模型不再学习任何东西。这种行为不一定导致过拟合或不良模型，但需要密切监控。'
- en: '**Validation loss**: The same thing can be said about the validation loss.
    We expect a small gap compared to the training loss.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证损失**：关于验证损失，也可以说同样的话。我们预计与训练损失相比会有一个小差距。'
- en: '**Gradient norm**: We expect small gradient norms with few spikes.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度范数**：我们期望梯度范数很小，几乎没有峰值。'
- en: '**Rewards**: We have two different rewards: chosen and rejected. They correspond
    to the mean difference between the log probabilities output by the trained and
    reference models. Over time, we expect the model to choose the chosen answers
    and reject the rejected answers, which means that the gap between them should
    increase. This difference is directly tracked by the `margins` metric, defined
    as the difference between chosen and rejected rewards. A well-trained model’s
    margin will quickly increase and then plateau.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：我们有两种不同的奖励：选择和拒绝。它们对应于训练和参考模型输出的对数概率之间的平均值差异。随着时间的推移，我们预计模型会选择选择的答案并拒绝拒绝的答案，这意味着它们之间的差距应该增加。这种差异直接通过`margins`指标跟踪，定义为选择和拒绝奖励之间的差异。一个训练良好的模型的margin会迅速增加然后趋于平稳。'
- en: '**Accuracies**: This metric represents the percentage of times the model correctly
    identifies the chosen answers. We want this accuracy to gradually increase during
    training, but it doesn’t need to reach 100%. An accuracy of 100%, especially if
    it’s achieved quickly, indicates that the preference dataset might be too easy
    for the model. While the LLM can still learn from such a dataset, it might be
    beneficial to add more challenging examples.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：此指标表示模型正确识别所选答案的百分比。我们希望这个准确率在训练过程中逐渐提高，但不需要达到100%。100%的准确率，尤其是如果它快速实现，可能表明偏好数据集对模型来说可能太简单了。虽然LLM仍然可以从这样的数据集中学习，但添加更多具有挑战性的例子可能是有益的。'
- en: In general, DPO is slightly harder to monitor and debug than SFT because it’s
    a more complex process, involving a reference model. However, it’s also significantly
    easier to use than PPO and other RLHF algorithms. As long as you have a high-quality
    preference dataset and a strong fine-tuned model, you can experiment with different
    ranks, beta parameters, learning rates, and number of epochs to see which experiment
    best captures your preferences.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，DPO比SFT更难监控和调试，因为它是一个更复杂的过程，涉及一个参考模型。然而，它比PPO和其他RLHF算法更容易使用。只要你有高质量的偏好数据集和强大的微调模型，你就可以尝试不同的排名、beta参数、学习率和epoch数量，以查看哪个实验最能捕捉你的偏好。
- en: While this is not the purpose of this chapter, it is possible to automate the
    evaluation of models designed to imitate a writing style. A possible solution
    consists of comparing the distribution of words in the text generated by different
    models (SFT and DPO) with our ground-truth dataset. In this example, we expect
    the SFT model to output a lot of words that are overrepresented in GPT-4o-mini
    (like “delve into”). The distribution output by our DPO model should be a lot
    closer to the chosen answers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是本章的目的，但自动评估旨在模仿写作风格的模型是可能的。一个可能的解决方案是，将不同模型（SFT和DPO）生成的文本中单词的分布与我们的基准数据集进行比较。在这个例子中，我们预计SFT模型会输出很多在GPT-4o-mini中过度表示的单词（如“深入研究”）。我们的DPO模型输出的分布应该与所选答案非常接近。
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored preference alignment techniques for improving LLMs. It
    introduced the concept of preference datasets, explaining their structure and
    importance in capturing nuanced human preferences. We implemented our own custom
    preference data generation pipeline by comparing original and AI-generated text
    from real articles. This pipeline can be reused and customized based on your use
    case.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了用于改进LLM的偏好对齐技术。它介绍了偏好数据集的概念，解释了其结构和在捕捉细微的人类偏好中的重要性。我们通过比较真实文章的原始文本和AI生成的文本，实现了我们自己的定制偏好数据生成管道。此管道可以根据您的用例重用和定制。
- en: We also provided an overview of the evolution of RLHF, leading to the introduction
    of DPO as a simpler and more efficient alternative. Finally, we implemented DPO
    using the Unsloth library to fine-tune our TwinLlama-3.1-8B model from *Chapter
    5*. Our step-by-step tutorial gave practical instructions for training the model,
    as well as highlighting key differences from SFT. The final model is available
    on the Hugging Face Hub.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还概述了RLHF的演变过程，导致了DPO（直接偏好优化）作为一种更简单、更有效的替代方案的出现。最后，我们使用Unsloth库实现了DPO，以微调我们的TwinLlama-3.1-8B模型，该模型来自*第五章*。我们的逐步教程提供了训练模型的实际指导，并突出了与SFT的关键差异。最终模型可在Hugging
    Face Hub上找到。
- en: In the next chapter, we will explore the crucial topic of LLM evaluation, addressing
    the challenges and current approaches in assessing LLM performance. We’ll cover
    the creation of domain-specific evaluation sets, examine why evaluation remains
    a persistent problem in the field, and introduce the concept of using larger models
    to evaluate smaller ones (LLM-as-a-judge). The chapter will conclude with a comprehensive
    evaluation pipeline, providing a structured framework for consistent and effective
    LLM evaluation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨至关重要的LLM评估主题，解决评估LLM性能的挑战和当前方法。我们将涵盖创建特定领域评估集，探讨为什么评估在领域内仍然是一个持续存在的问题，并介绍使用更大模型评估较小模型的概念（LLM作为法官）。本章将以一个全面的评估流程结束，提供一个结构化的框架，以实现一致和有效的LLM评估。
- en: References
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rafael Rafailov et al.. “*Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model*.” arXiv preprint arXiv:2305.18290, May 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafael Rafailov等人。“*直接偏好优化：你的语言模型实际上是一个奖励模型*。”arXiv预印本arXiv:2305.18290，2023年5月。
- en: Timo Kaufmann et al.. “*A Survey of Reinforcement Learning from Human Feedback*.”
    arXiv preprint arXiv:2312.14925, December 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Timo Kaufmann等人. “*人类反馈强化学习的强化学习综述*。” arXiv预印本 arXiv:2312.14925，2023年12月。
- en: 'Anthropic. “*GitHub - anthropics/hh-rlhf: Human preference data for “Training
    a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback*”.”
    github.com, 2022, [https://github.com/anthropics/hh-rlhf](https://github.com/anthropics/hh-rlhf).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anthropic. “*GitHub - anthropics/hh-rlhf: 用于“通过人类反馈的强化学习训练一个有用且无害的助手”的人类偏好数据*。”
    github.com, 2022，[https://github.com/anthropics/hh-rlhf](https://github.com/anthropics/hh-rlhf).'
- en: Nisan Stiennon et al.. “*Learning to summarize from human feedback*.” arXiv
    preprint arXiv:2009.01325, September 2020.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nisan Stiennon等人. “*从人类反馈中学习总结*。” arXiv预印本 arXiv:2009.01325，2020年9月。
- en: Intel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization
    on Intel Gaudi2.” medium.com, March 26, 2024, [https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Intel(R) Neural Compressor. “在Intel Gaudi2上进行的监督微调和直接偏好优化。” medium.com, 2024年3月26日，[https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3).
- en: Argilla. “*GitHub - argilla-io/distilabel*.” [github.com](https://github.com),
    August 23, 2024, [https://github.com/argilla-io/distilabel](https://github.com/argilla-io/distilabel).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argilla. “*GitHub - argilla-io/distilabel*。” [github.com](https://github.com),
    2024年8月23日，[https://github.com/argilla-io/distilabel](https://github.com/argilla-io/distilabel).
- en: Databricks. “*Enhancing LLM-as-a-Judge with Grading Notes*.” databricks.com,
    July 22, 2024, [https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes](https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks. “*使用评分笔记增强LLM-as-a-Judge*。” databricks.com, 2024年7月22日，[https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes](https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes).
- en: Akrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference-Based Policy
    Learning. 12-27\. 10.1007/978-3-642-23780-5_11.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference-Based Policy
    Learning. 12-27\. 10.1007/978-3-642-23780-5_11.
- en: 'Cheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang-Hyeun.
    (2011). *Preference-Based Policy Iteration: Leveraging Preference Learning for
    Reinforcement Learning*. 312-327\. 10.1007/978-3-642-23780-5_30.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang-Hyeun.
    (2011). *基于偏好的策略迭代：利用偏好学习进行强化学习*。 312-327\. 10.1007/978-3-642-23780-5_30.
- en: Paul Christiano et al.. “*Deep reinforcement learning from human preferences*.”
    arXiv preprint arXiv:1706.03741, June 2017.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paul Christiano等人. “*从人类偏好中进行深度强化学习*。” arXiv预印本 arXiv:1706.03741，2017年6月。
- en: Long Ouyang et al.. “*Training language models to follow instructions with human
    feedback*.” arXiv preprint arXiv:2203.02155, March 2022.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long Ouyang等人. “*通过人类反馈训练语言模型以遵循指令*。” arXiv预印本 arXiv:2203.02155，2022年3月。
- en: John Schulman et al.. “*Proximal Policy Optimization Algorithms*.” arXiv preprint
    arXiv:1707.06347, July 2017.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: John Schulman等人. “*近端策略优化算法*。” arXiv预印本 arXiv:1707.06347，2017年7月。
- en: 'unslothai. “*GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral*, Phi
    & Gemma LLMs 2-5x faster with 80% less memory.” github.com, August 21, 2024, [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'unslothai. “*GitHub - unslothai/unsloth: 使用80%更少的内存，将Llama 3.1、Mistral、Phi和Gemma
    LLMs的微调速度提高2-5倍。” github.com, 2024年8月21日，[https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth).'
- en: Join our book’s Discord space
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
