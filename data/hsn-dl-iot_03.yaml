- en: Deep Learning Architectures for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的深度学习架构
- en: In the era of the **Internet of Things** (**IoT**), an enormous amount of sensory
    data for a wide range of fields and applications is being generated and collected
    from numerous sensing devices. Applying analytics over such data streams to discover
    new information, predict future insights, and make controlled decisions, is a
    challenging task, which makes IoT a worthy paradigm for business intelligence
    and quality-of-life improving technology. However, analytics on IoT—enabled devices
    requires a platform consisting of **machine learning** (**ML**) and **deep learning**
    (**DL**) frameworks, a software stack, and hardware (for example, a **Graphical
    Processing Unit** (**GPU**) and **Tensor Processing Unit** (**TPU**)).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在**物联网**（**IoT**）时代，来自众多传感设备的大量感知数据正被生成和收集，这些数据涵盖了广泛的领域和应用。对这些数据流进行分析，发现新信息、预测未来洞察并做出控制决策，是一项具有挑战性的任务，这使得物联网成为商业智能和提高生活质量技术的一个有价值的范式。然而，在物联网启用的设备上进行分析需要一个平台，该平台包括**机器学习**（**ML**）和**深度学习**（**DL**）框架、软件堆栈和硬件（例如，**图形处理单元**（**GPU**）和**张量处理单元**（**TPU**））。
- en: 'In this chapter, we will discuss some basic concepts of DL architectures and
    platforms, which will be used in all subsequent chapters. We will start with a
    brief introduction to ML. Then, we will move onto DL, which is a branch of ML
    based on a set of algorithms that attempts to model high-level abstractions in
    data. We will briefly discuss some of the most well-known and widely used neural
    network architectures. Then, we will look at various features of DL frameworks
    and libraries that can be used for developing DL applications on IoT-enabled devices.
    Briefly, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论一些深度学习（DL）架构和平台的基本概念，这些概念将在后续章节中使用。我们将从简要介绍机器学习（ML）开始。然后，我们将转向深度学习，深度学习是机器学习的一个分支，它基于一组算法，旨在对数据中的高级抽象进行建模。我们将简要讨论一些最著名且广泛使用的神经网络架构。接着，我们将探讨用于在物联网（IoT）设备上开发深度学习应用的各种深度学习框架和库的特性。简而言之，以下主题将被涵盖：
- en: A soft introduction to ML
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的软介绍
- en: Artificial neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Deep neural network architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络架构
- en: DL frameworks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习框架
- en: A soft introduction to ML
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的软介绍
- en: ML approaches are based on a set of statistical and mathematical algorithms
    carrying out tasks such as classification, regression analysis, concept learning,
    predictive modeling, clustering, and mining of useful patterns. Using ML, we aim
    to improve the whole learning process automatically so that we may not need complete
    human interactions, or so that we can at least reduce the level of such interactions
    as much as possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法基于一组统计和数学算法，执行分类、回归分析、概念学习、预测建模、聚类和有用模式挖掘等任务。通过使用机器学习，我们旨在自动改进整个学习过程，从而不再需要完全依赖人工干预，或者至少能够尽可能减少这种干预。
- en: Working principle of a learning algorithm
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习算法的工作原理
- en: 'Tom M. Mitchell explained what learning really means from a computer science
    perspective:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Tom M. Mitchell 从计算机科学的角度解释了学习的真正含义：
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “一个计算机程序如果在某类任务T和性能衡量P下，随着经验E的积累，其在任务T上的表现（由P衡量）得到改进，则该程序被认为从经验E中学习。”
- en: 'Based on this definition, we can conclude that a computer program or machine
    can do the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一定义，我们可以得出结论，一个计算机程序或机器可以做到以下几点：
- en: Learn from data and histories
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据和历史中学习
- en: Improve with experience
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随经验改进
- en: Iteratively enhance a model that can be used to predict outcomes of questions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代地增强一个可用于预测问题结果的模型
- en: 'Since the preceding points are at the core of predictive analytics, almost
    every ML algorithm we use can be treated as an optimization problem. This is about
    finding parameters that minimize an objective function; for example, a weighted
    sum of two terms such as a cost function and regularization. Typically, an objective
    function has two components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前述要点是预测分析的核心，几乎我们使用的每个机器学习算法都可以视为一个优化问题。这涉及到寻找最小化目标函数的参数；例如，两个项的加权和，如代价函数和正则化。通常，目标函数有两个组成部分：
- en: A regularizer that controls the complexity of the model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制模型复杂度的正则化器
- en: The loss that measures the error of the model on the training data
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量模型在训练数据上误差的损失函数
- en: On the other hand, the regularization parameter defines the trade-off between
    minimizing the training error and the model's complexity in an effort to avoid
    overfitting problems. Now, if both of these components are convex, then their
    sum is also convex. So, when using an ML algorithm, the goal is to obtain the
    best hyperparameters of a function that return the minimum error when making predictions.
    Therefore, by using a convex optimization technique, we can minimize the function
    until it converges toward the minimum error.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，正则化参数定义了在最小化训练误差和模型复杂度之间的权衡，以避免过拟合问题。如果这两个组件都是凸的，那么它们的和也是凸的。因此，在使用机器学习算法时，目标是获得最佳超参数，使得在进行预测时返回最小误差。因此，通过使用凸优化技术，我们可以最小化该函数，直到它收敛到最小误差。
- en: Given that a problem is convex, it is usually easier to analyze the asymptotic
    behavior of the algorithm, which shows how fast it converges as the model observes
    more and more training data. The task of ML is to train a model so that it can
    recognize complex patterns from the given input data and can make decisions in
    an automated way. Thus, making predictions is all about testing the model against
    new (that is, unobserved) data and evaluating the performance of the model itself.
    However, in the process as a whole, and for making the predictive model a successful
    one, data acts as the first-class citizen in all ML tasks. In reality, the data
    that we feed to our ML systems must be made up of mathematical objects, such as
    vectors, so that they can consume such data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个问题是凸的，通常更容易分析算法的渐进行为，这表明随着模型观察到越来越多的训练数据，它的收敛速度如何。机器学习的任务是训练一个模型，使其能够从给定的输入数据中识别复杂的模式，并以自动化的方式做出决策。因此，进行预测的关键是将模型应用于新的（即未观察到的）数据，并评估模型本身的性能。然而，在整个过程中，为了使预测模型成为成功的，数据在所有机器学习任务中都扮演着第一类公民的角色。实际上，我们馈送给机器学习系统的数据必须由数学对象组成，如向量，以便它们可以处理这些数据。
- en: Depending on the available data and feature types, the performance of your predictive
    model can vacillate dramatically. Therefore, selecting the right features is one
    of the most important steps before the model evaluation takes place. This is called
    **feature engineering**, where the domain knowledge pertaining to the data is
    used to create only selective or useful features that help prepare the feature
    vectors to be used so that an ML algorithm works.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 根据可用数据和特征类型，预测模型的性能可能会发生剧烈波动。因此，选择正确的特征是模型评估前最重要的步骤之一。这被称为**特征工程**，其中使用与数据相关的领域知识来创建仅对模型有用的选择性特征，以帮助准备特征向量供机器学习算法使用。
- en: For example, comparing hotels is quite difficult unless we already have a personal
    experience of staying in multiple hotels. However, with the help of an ML model,
    which is already trained with quality features out of thousands of reviews and
    features (for example, how many stars does a hotel have, the size of the room,
    the location, and room service, and so on), it is pretty feasible now. We'll see
    several examples throughout the chapters. However, before developing such an ML
    model, knowing a number of ML concepts is also important.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，比较酒店是相当困难的，除非我们已经有多次住酒店的个人经验。然而，在一个已经通过成千上万的评论和特征（例如酒店的星级、房间大小、位置、客房服务等）训练好的机器学习模型的帮助下，这变得相当可行。我们将在各章中看到几个例子。然而，在开发这样一个机器学习模型之前，了解一些机器学习概念也是很重要的。
- en: General ML rule of thumb
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用机器学习经验法则
- en: 'The general ML rule of thumb is that the more data there is, the better the
    predictive model. However, having more features often creates a mess, to the extent
    that the performance degrades drastically, especially if the dataset is multidimensional.
    The entire learning process requires input datasets that can be split into three
    types (or are already provided as such):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通用的机器学习经验法则是，数据越多，预测模型越好。然而，拥有更多特征往往会制造混乱，甚至会严重降低性能，特别是在数据集是多维的情况下。整个学习过程需要将输入数据集拆分为三种类型（或者已经提供为这种类型）：
- en: A **training set** is the knowledge base coming from historical or live data
    that is used to fit the parameters of the ML algorithm. During the training phase,
    the ML model utilizes the training set to find optimal weights of the network
    and reach the objective function by minimizing the training error. Here, the backpropagation
    rule, or an optimization algorithm, is used to train the model, but all the hyperparameters
    are needed to be set before the learning process starts.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**是来自历史或实时数据的知识库，用于拟合机器学习算法的参数。在训练阶段，机器学习模型利用训练集找到网络的最优权重，并通过最小化训练误差来达到目标函数。在这里，反向传播规则或优化算法被用来训练模型，但所有超参数必须在学习过程开始之前设置好。'
- en: A **validation set** is a set of examples used to tune the parameters of an
    ML model. It ensures that the model is trained well and generalizes toward avoiding
    overfitting. Some ML practitioners refer to it as a development set, or dev set
    as well.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**是一组用于调整机器学习模型参数的示例。它确保模型训练得当，并且能够避免过拟合。一些机器学习实践者也将其称为开发集（dev集）。'
- en: A **test set** is used for evaluating the performance of the trained model on
    unseen data. This step is also referred to as **model inferencing**. After assessing
    the final model on the test set (that is, when we're fully satisfied with the
    model's performance), we do not have to tune the model any further, but the trained
    model can be deployed in a production-ready environment.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**用于评估训练好的模型在未见数据上的表现。这个步骤也被称为**模型推理**。在测试集上评估最终模型之后（即，当我们完全满意模型的表现时），我们不再需要调整模型，训练好的模型可以部署到生产环境中。'
- en: A common practice is splitting the input data (after the necessary preprocessing
    and feature engineering) into 60% for training, 10% for validation, and 20% for
    testing, but it really depends on use cases. Sometimes, we also need to perform
    upsampling or downsampling on the data, based on the availability and quality
    of the datasets. This rule of thumb of learning on different types of training
    sets can differ across ML tasks, as we will cover in the next section. However,
    before that, let's take a quick look at a few common phenomena in ML.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的做法是将输入数据（经过必要的预处理和特征工程后）分为60%用于训练，10%用于验证，20%用于测试，但这实际上取决于具体的使用场景。有时，我们还需要根据数据集的可用性和质量对数据进行上采样或下采样。这种关于不同类型训练集的经验法则在不同的机器学习任务中可能会有所不同，我们将在下一节讨论这一点。然而，在此之前，让我们快速回顾一下机器学习中的一些常见现象。
- en: General issues in ML models
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型中的常见问题
- en: 'When we use this input data for training, validation, and testing, usually,
    the learning algorithms cannot learn 100% accurately, which involves training,
    validation, and test error (or loss). There are two types of errors that you may
    encounter in an ML model:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用这些输入数据进行训练、验证和测试时，通常学习算法无法做到100%准确，这涉及训练、验证和测试误差（或损失）。在机器学习模型中，你可能会遇到两种类型的误差：
- en: Irreducible error
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可约误差
- en: Reducible error
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可约误差
- en: The irreducible error cannot be reduced even with the most robust and sophisticated
    model. However, the reducible error, which has two components, called bias and
    variance, can be reduced. Therefore, to understand the model (that is, prediction
    errors), we need to focus on bias and variance only. Bias means how far the predicted
    values are from the actual values. Usually, if the average predicted values are
    very different from the actual values (labels), then the bias is higher.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不可约误差即使使用最强大、最复杂的模型也无法减少。然而，可约误差有两个组成部分，分别称为偏差和方差，它们是可以减少的。因此，为了理解模型（即预测误差），我们需要专注于偏差和方差。偏差是指预测值与实际值之间的距离。通常，如果平均预测值与实际值（标签）之间的差距很大，则偏差较高。
- en: 'An ML model will have a high bias because it can''t model the relationship
    between input and output variables (can''t capture the complexity of data well)
    and becomes very simple. Thus, an overly simple model with high variance causes
    underfitting of the data. The following diagram gives some high-level insights,
    and also shows what a just-right fit model should look like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习模型可能会有较高的偏差，因为它无法建模输入和输出变量之间的关系（无法很好地捕捉数据的复杂性），因此变得非常简单。因此，一个过于简单且具有高方差的模型会导致数据的欠拟合。下图提供了一些高级的见解，并展示了一个恰到好处的拟合模型应该是什么样子：
- en: '![](img/4e475e9e-0922-45e1-8a36-afdca4dc93cd.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e475e9e-0922-45e1-8a36-afdca4dc93cd.png)'
- en: 'Variance signifies the variability between the predicted values and the actual
    values (how scattered they are). If the model has a high training error as well
    as the validation error or test error being the same as the training error, the
    model has a high bias. On the other hand, if the model has a low training error
    but has a high validation or high test error, the model has a high variance. An
    ML model usually performs very well on the training set, but doesn''t work well
    on the test set (because of high error rates). Ultimately, it results in an underfit
    model. We can recap the overfitting and underfitting once more:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 方差表示预测值与实际值之间的变动性（它们的分散程度）。如果模型的训练误差很高，而且验证误差或测试误差与训练误差相同，则模型有很高的偏差。另一方面，如果模型的训练误差较低，但验证误差或测试误差较高，则模型具有较高的方差。一个机器学习模型通常在训练集上表现得非常好，但在测试集上表现不佳（因为错误率很高）。最终，这会导致欠拟合的模型。我们可以再次回顾过拟合和欠拟合：
- en: '**Underfitting**: If your training and validation errors are both relatively
    equal and very high, then your model is most likely underfitting your training
    data.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：如果你的训练误差和验证误差都相对较高且非常相似，那么你的模型很可能是在欠拟合你的训练数据。'
- en: '**Overfitting**: If your training error is low and your validation error is
    high, then your model is most likely overfitting your training data. The just-right
    fit model learns very well and performs better on unseen data too.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：如果你的训练误差很低，而验证误差很高，那么你的模型很可能是在过拟合你的训练数据。恰到好处的拟合模型不仅能很好地学习，而且在未见过的数据上也表现更好。'
- en: 'Bias-variance trade-off: the high bias and high variance issue is often called
    bias-variance trade-off, because a model cannot be too complex or too simple at
    the same time. Ideally, we strive for the best model that has both low bias and
    low variance.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡：高偏差和高方差问题通常被称为偏差-方差权衡，因为一个模型不能同时过于复杂或过于简单。理想情况下，我们努力寻找一个具有低偏差和低方差的最佳模型。
- en: Now we know the basic working principle of an ML algorithm. However, based on
    problem type and the method used to solve a problem, ML tasks can be different;
    for example, supervised learning, unsupervised learning, and reinforcement learning.
    We'll discuss these learning tasks in more detail in the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了机器学习（ML）算法的基本工作原理。然而，根据问题的类型和解决问题的方法，机器学习任务可能有所不同；例如，有监督学习、无监督学习和强化学习。我们将在下一节中更详细地讨论这些学习任务。
- en: ML tasks
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习任务
- en: 'Although every ML problem is more or less an optimization problem, the way
    in which they are solved can vary. In fact, learning tasks can be categorized
    into three types: supervised learning, unsupervised learning, and reinforcement
    learning.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个机器学习问题或多或少都是一个优化问题，但解决这些问题的方法可能有所不同。事实上，学习任务可以分为三种类型：有监督学习、无监督学习和强化学习。
- en: Supervised learning
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: 'Supervised learning is the simplest and most well-known automatic learning
    task. It is based on a number of predefined examples, in which the category to
    which each of the inputs should belong is already known, as shown in the following
    diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习是最简单且最知名的自动学习任务。它基于一组预定义的示例，其中每个输入应属于的类别已经知道，如下图所示：
- en: '![](img/7da0267f-7bf5-4811-bf54-5a19375ab274.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7da0267f-7bf5-4811-bf54-5a19375ab274.png)'
- en: The preceding diagram shows a typical workflow of supervised learning. An actor
    (for example, a data scientist or data engineer) performs the **extraction**,
    **transformation**, **and load** (**ETL**) and the necessary feature engineering
    (including feature extraction, selection, and so on) to get the appropriate data
    with features and labels so that they can be fed into the model. Then, they split
    the data into training, development, and test sets. The training set is used to
    train an ML model, the validation set is used to validate the training against
    the overfitting problem and regularization, and then the actor would evaluate
    the model's performance on the test set (that is, unseen data).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示展示了有监督学习的典型工作流程。一个执行者（例如，数据科学家或数据工程师）执行**提取**、**转换**、**加载**（**ETL**）以及必要的特征工程（包括特征提取、选择等），以获取包含特征和标签的适当数据，这样数据就可以被输入到模型中。然后，他们将数据分为训练集、开发集和测试集。训练集用于训练机器学习模型，验证集用于验证训练过程中的过拟合问题和正则化，然后执行者会评估模型在测试集（即未见过的数据）上的表现。
- en: 'However, if the performance is not satisfactory, the actor can perform additional
    tuning to get the best model based on hyperparameter optimization. Finally, they
    will deploy the best model in a production-ready environment. In the overall life
    cycle, there might be many actors involved (for example, a data engineer, data
    scientist, or an ML engineer), performing each step independently or collaboratively.
    The supervised learning context includes classification and regression tasks;
    classification is used to predict which class a data point is a part of (discrete
    value). It is also used for predicting the label of the class attribute. The following
    diagram summarizes these steps in a nutshell:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果性能不尽如人意，执行者可以通过超参数优化进行额外的调整，以获得最佳模型。最终，他们会将最佳模型部署到生产环境中。在整个生命周期中，可能会涉及许多执行者（例如，数据工程师、数据科学家或机器学习工程师），每个人独立或协作地执行每一步。监督学习的背景包括分类和回归任务；分类用于预测一个数据点属于哪个类别（离散值），也用于预测类别属性的标签。下图简要总结了这些步骤：
- en: '![](img/3ec8d6c2-8f14-485a-a124-40e74f65de64.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ec8d6c2-8f14-485a-a124-40e74f65de64.png)'
- en: On the other hand, regression is used to predict continuous values and make
    a numeric prediction of the class attribute. In the context of supervised learning,
    the learning process required for the input dataset is split randomly into three
    sets; for example, 60% for the training set, 10% for the validation set, and the
    remaining 30% for the testing set.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，回归用于预测连续值，并对类别属性进行数值预测。在监督学习的背景下，输入数据集的学习过程通常会随机划分成三个子集；例如，60% 用于训练集，10%
    用于验证集，剩下的30% 用于测试集。
- en: Unsupervised learning
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'How would you summarize and group a dataset if the labels were not given? You''ll
    probably try to answer this question by finding the underlying structure of a
    dataset and measuring the statistical properties, such as the frequency distribution,
    mean, and standard deviation. If the question is how would you effectively represent
    data in a compressed format, you''ll probably reply saying that you''ll use some
    software for doing the compression, although you might have no idea how that software
    would do it. The following diagram shows the typical workflow of an unsupervised
    learning task:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有给定标签，你将如何总结和归类一个数据集？你可能会通过找到数据集的潜在结构，并衡量其统计特性，如频率分布、均值和标准差来回答这个问题。如果问题是如何有效地以压缩格式表示数据，你可能会回答说你会使用某些软件进行压缩，尽管你可能不知道这些软件是如何实现压缩的。下图展示了一个典型的无监督学习任务的工作流程：
- en: '![](img/cf93f6bf-f0b4-486d-aae2-4e13e70a2012.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf93f6bf-f0b4-486d-aae2-4e13e70a2012.png)'
- en: 'These are precisely two of the main goals of unsupervised learning, which is
    largely a data-driven process. We call this type of learning unsupervised because
    you will have to deal with unlabeled data. The following quote comes from Yann
    LeCun, director of AI research (source: *Predictive Learning*, NIPS 2016, Yann
    LeCun, Facebook Research):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是无监督学习的两个主要目标，它在很大程度上是一个数据驱动的过程。我们称这种学习方式为无监督，因为你将不得不处理无标签数据。以下引述来自 AI 研究负责人
    Yann LeCun（来源：*预测学习*，NIPS 2016，Yann LeCun，Facebook 研究）：
- en: '"Most human and animal learning is unsupervised learning. If intelligence was
    a cake, unsupervised learning would be the cake, supervised learning would be
    the icing on the cake, and reinforcement learning would be the cherry on the cake.
    We know how to make the icing and the cherry, but we don''t know how to make the
    cake. We need to solve the unsupervised learning problem before we can even think
    of getting to true AI."'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '"大多数人类和动物的学习都是无监督学习。如果智力是一块蛋糕，那么无监督学习就是蛋糕本身，监督学习是蛋糕上的糖霜，强化学习则是蛋糕上的樱桃。我们知道如何制作糖霜和樱桃，但我们不知道如何做蛋糕。在我们甚至开始考虑实现真正的人工智能之前，我们需要解决无监督学习的问题。"'
- en: 'A few most widely used unsupervised learning tasks include the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最广泛使用的无监督学习任务包括以下内容：
- en: '**Clustering**: Grouping data points based on similarity (or statistical properties),
    for example, a company such as Airbnb often groups its apartments and houses into
    neighborhoods so that customers can navigate the listed ones more easily'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：根据相似性（或统计特性）将数据点分组，例如，像 Airbnb 这样的公司通常会将其公寓和房屋按邻里分组，以便客户更容易地浏览列出的房源'
- en: '**Dimensionality reduction**: Compressing the data with the structure and statistical
    properties preserved as much as possible, for example, often, the number of dimensions
    of the dataset needs to be reduced for the modelling and visualization'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：尽可能保留数据的结构和统计特性来压缩数据，例如，通常需要减少数据集的维度以便进行建模和可视化'
- en: '**Anomaly detection**: Useful in several applications, such as identification
    of credit card fraud detection, identifying faulty pieces of hardware in an industrial
    engineering process, and identifying outliers in large-scale datasets'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：在多个应用中非常有用，例如识别信用卡欺诈、在工业工程过程中识别故障硬件以及识别大规模数据集中的异常值'
- en: '**Association rule mining**: Often used in market basket analysis, for example,
    asking which items are bought together frequently'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联规则挖掘**：通常用于市场篮子分析，例如，询问哪些商品经常一起购买'
- en: Reinforcement learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'Reinforcement learning is an artificial intelligence approach that focuses
    on the learning of the system through its interactions with the environment. In
    reinforcement learning, the system''s parameters are adapted based on the feedback
    obtained from the environment, which, in turn, provides feedback on the decisions
    made by the system. The following diagram shows a person making decisions in order
    to arrive at their destination. Let''s take an example of the route you take from
    home to work:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种人工智能方法，专注于通过与环境的互动来学习。在强化学习中，系统的参数会根据从环境中获得的反馈进行调整，而环境会根据系统做出的决策提供反馈。下面的图示展示了一个人为了到达目的地而做出决策的过程。让我们举一个你从家到公司通勤路线的例子：
- en: '![](img/78d85aba-6e21-4d7d-9256-878b8542d641.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78d85aba-6e21-4d7d-9256-878b8542d641.png)'
- en: We can take a look at one more example in terms of a system modeling a chess
    player. In order to improve its performance, the system utilizes the result of
    its previous moves; such a system is said to be a system learning with reinforcement.
    In this case, you take the same route to work every day. However, out of the blue
    one day, you get curious and decide to try a different route with a view to finding
    the shortest path. Similarly, based on your experience and the time taken with
    the different route, you'll decide whether you should take that specific route
    more often. We can take a look at one more example in terms of a system modeling
    a chess player.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个建模象棋选手的系统来再举一个例子。为了提高其表现，系统会利用之前行动的结果；这样的系统被称为通过强化学习的系统。在这种情况下，你每天都走相同的路线去上班。然而，有一天，你突然产生了好奇心，决定尝试一条不同的路线，目的是找到最短的路径。类似地，基于你在不同路线上的经验和所花的时间，你会决定是否应该更频繁地走这条特定路线。我们可以再举一个关于建模象棋选手的系统的例子。
- en: So far, we have learned the basic working principles of ML and different learning
    tasks. Let's have a look at each learning task with some example use cases in
    the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了机器学习的基本工作原理以及不同的学习任务。接下来，让我们通过一些示例用例来看看每个学习任务。
- en: Learning types with applications
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带应用的学习类型
- en: 'We have seen the basic working principles of ML algorithms, and we have seen
    what the basic ML tasks are, and how they formulate domain-specific problems.
    However, each of these learning tasks can be solved using different algorithms.
    The following diagram provides a glimpse into this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了机器学习算法的基本工作原理，知道了基本的机器学习任务是什么，并且它们是如何构建领域特定问题的。然而，每个学习任务都可以通过不同的算法来解决。下面的图示给我们提供了一个简要的了解：
- en: '![](img/4352c6ad-bcbf-4f21-9866-b0a6537c871e.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4352c6ad-bcbf-4f21-9866-b0a6537c871e.png)'
- en: However, the preceding diagram lists only a few use cases and applications using
    different ML tasks. In practice, ML is used in numerous use cases and applications.
    We will try to cover a few of them throughout this book.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述图示仅列出了使用不同机器学习任务的一些用例和应用。实际上，机器学习在众多用例和应用中都有广泛的使用。我们将在本书中尽力覆盖其中的一些。
- en: Delving into DL
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解深度学习
- en: Simple ML methods that were used in the normal-size data analysis are no longer
    effective and should be replaced by more robust ML methods. Although classical
    ML techniques allow researchers to identify groups or clusters of related variables,
    the accuracy and effectiveness of these methods diminish with large and multidimensional
    data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规数据分析中使用的简单机器学习方法已经不再有效，应当用更强大的机器学习方法来替代。尽管经典的机器学习技术能够帮助研究者识别相关变量的组或簇，但随着数据量增大和维度增多，这些方法的准确性和有效性会降低。
- en: How did DL take ML to the next level?
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习是如何将机器学习提升到一个新水平的？
- en: 'Simple ML methods used in small-scale data analysis are not effective when
    dealing with large and high-dimensional datasets. However, deep learning (DL),
    which is a branch of ML based on a set of algorithms that attempt to model high-level
    abstractions in data, can handle this issue. Ian Goodfellow defined DL in his
    book "*Deep Learning*, MIT Press, 2016" as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在小规模数据分析中使用的简单机器学习方法，在处理大规模和高维数据集时并不有效。然而，深度学习（DL）作为机器学习的一个分支，基于一组算法，试图对数据进行高层次抽象建模，能够解决这一问题。Ian
    Goodfellow在他的书《*深度学习*，MIT出版社，2016年》中是这样定义深度学习的：
- en: '"Deep learning is a particular kind of machine learning that achieves great
    power and flexibility by learning to represent the world as a nested hierarchy
    of concepts, with each concept defined in relation to simpler concepts, and more
    abstract representations computed in terms of less abstract ones."'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习是一种特殊的机器学习方式，通过学习将世界表示为概念的嵌套层次结构，每个概念是相对于更简单的概念来定义的，更抽象的表示通过更不抽象的表示来计算，从而实现了强大的能力和灵活性。”
- en: Similar to the ML model, a DL model also takes in an input, `X`, and learns
    high-level abstractions or patterns from it to predict an output of `Y`. For example,
    based on the stock prices of the past week, a DL model can predict the stock price
    for the next day. When performing training on such historical stock data, a DL
    model tries to minimize the difference between the prediction and the actual values.
    This way, a DL model tries to generalize to inputs that it hasn't seen before
    and makes predictions on test data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习模型类似，深度学习模型也接受一个输入`X`，并从中学习高层次的抽象或模式，从而预测输出`Y`。例如，基于过去一周的股价，深度学习模型可以预测下一天的股价。当在这样的历史股数据上进行训练时，深度学习模型会尽量减少预测值和实际值之间的差异。通过这种方式，深度学习模型尝试对之前未见过的输入进行泛化，并对测试数据做出预测。
- en: Now, you might be wondering, if an ML model can do the same tasks, why do we
    need DL for this? Well, DL models tend to perform well with large amounts of data,
    whereas old ML models stop improving after a certain point. The core concept of
    DL, inspired by the structure and function of the brain, is called **artificial
    neural networks** (**ANNs**).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会想，如果机器学习模型可以完成相同的任务，为什么我们还需要深度学习呢？其实，深度学习模型在处理大量数据时表现得很好，而传统的机器学习模型在达到某个点后就不再改进。深度学习的核心概念，受到大脑结构和功能的启发，被称为**人工神经网络**（**ANNs**）。
- en: 'Being at the core of DL, ANNs help you to learn the associations between sets
    of inputs and outputs in order to make more robust and accurate predictions. However,
    DL is not only limited to ANNs; there have been many theoretical advances, software
    stacks, and hardware improvements that bring DL to the masses. Let''s look at
    an example in which we want to develop a predictive analytics model, such as an
    animal recognizer, where our system has to resolve two problems:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的核心，人工神经网络（ANNs）帮助你学习输入和输出之间的关联，从而做出更强大、更准确的预测。然而，深度学习不仅限于人工神经网络；随着理论的进步、软件堆栈的完善和硬件的改进，深度学习已经普及到大众中。让我们来看一个例子，假设我们要开发一个预测分析模型，比如一个动物识别器，我们的系统必须解决两个问题：
- en: To classify whether an image represents a cat or a dog
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判断一张图片是猫还是狗
- en: To cluster images of dogs and cats.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对猫狗图片进行聚类。
- en: 'If we solve the first problem using a typical ML method, we must define the
    facial features (ears, eyes, whiskers, and so on) and write a method to identify
    which features (typically non-linear) are more important when classifying a particular
    animal. However, at the same time, we cannot address the second problem because
    classical ML algorithms for clustering images (such as k-means) cannot handle
    nonlinear features. Take a look at the following diagram, which shows a workflow
    that we would follow to classify if the given image is of a cat:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用典型的机器学习方法来解决第一个问题，我们必须定义面部特征（耳朵、眼睛、胡须等），并编写方法来识别哪些特征（通常是非线性的）在分类特定动物时更为重要。然而，同时我们无法解决第二个问题，因为经典的机器学习聚类算法（如k-means）无法处理非线性特征。看看下面的图示，它展示了我们在分类给定图像是否为猫时需要遵循的工作流程：
- en: '![](img/6774fd8b-9b85-47e3-af7e-f9b28f882e9f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6774fd8b-9b85-47e3-af7e-f9b28f882e9f.png)'
- en: DL algorithms take these two problems one step further, and the most important
    features will be extracted automatically after determining which features are
    the most important for classification or clustering. In contrast, when using a
    classical ML algorithm, we would have to provide the features manually. A DL algorithm
    takes more sophisticated steps instead. For example, first, it identifies the
    edges that are the most relevant when clustering cats or dogs. It then tries to
    find various combinations of shapes and edges hierarchically, which is called
    ETL.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DL算法将这两个问题进一步推进，最重要的特征将在确定哪些特征对分类或聚类最为重要后自动提取。相比之下，使用经典的机器学习算法时，我们必须手动提供特征。而DL算法则采取更复杂的步骤。例如，首先，它识别出在聚类猫或狗时最相关的边缘。然后，它尝试以层次结构的方式找到各种形状和边缘的组合，这被称为ETL。
- en: Then, after several iterations, it carries out the hierarchical identification
    of complex concepts and features. Following that, based on the features identified,
    the DL algorithm will decide which of the features are most significant for classifying
    the animal. This step is known as feature extraction. Finally, it takes out the
    label column and performs unsupervised training using **autoencoders** (**AEs**)
    to extract the latent features to be redistributed to k-means for clustering.
    Then, the **clustering assignment hardening loss** (**CAH loss**) and reconstruction
    loss are jointly optimized toward an optimal clustering assignment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在经过几次迭代后，它进行复杂概念和特征的层次化识别。接着，基于已识别的特征，DL算法将决定哪些特征对分类动物最为重要。这个步骤被称为特征提取。最后，它提取出标签列，并使用**自编码器**（**AEs**）进行无监督训练，提取潜在特征，然后将其重新分配给k-means进行聚类。接着，**聚类分配硬化损失**（**CAH损失**）和重建损失被共同优化，以达到最佳的聚类分配。
- en: 'However, in practice, a DL algorithm is fed with a raw image representation,
    which doesn''t see an image as we see it because it only knows the position of
    each pixel and its color. The image is divided into various layers of analysis.
    For example, at a lower level, there is the software analysis—a grid of a few
    pixels with the task of detecting a type of color or various nuances. If it finds
    something, it informs the next level, which, at this point, checks whether or
    not that given color belongs to a larger form, such as a line. The process continues
    to the upper levels until the algorithm understands what is shown in the following
    diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际应用中，DL算法接收到的是原始图像表示，这并不像我们看到的图像那样理解图像，因为它只知道每个像素的位置和颜色。图像被分解成多个分析层次。例如，在较低层次，软件会分析一小块像素网格，任务是检测某种颜色或各种细微的色调变化。如果检测到某些信息，它会通知下一层，此时这一层检查该颜色是否属于更大的形态，如一条线。这个过程一直持续到更高层次，直到算法理解以下图所示的内容：
- en: '![](img/6be27318-8090-42f0-8d8c-005086271f80.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6be27318-8090-42f0-8d8c-005086271f80.png)'
- en: Although a dog versus a cat is an example of a very simple classifier, software
    that's capable of doing these types of things is now widespread and is found in
    systems for recognizing faces, or in those for searching an image on Google, for
    example. This kind of software is based on DL algorithms. By contrast, if we are
    using a linear ML algorithm we cannot build such applications, since these algorithms
    are incapable of handling non-linear image features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然区分狗与猫是一个非常简单的分类器示例，但能够做这些事情的软件现在已经广泛应用，并且出现在面部识别系统中，或例如在Google上搜索图像的系统中。这类软件是基于DL算法的。相反，如果我们使用线性机器学习算法，就无法构建这样的应用，因为这些算法无法处理非线性的图像特征。
- en: Also, using ML approaches, we typically only handle a few hyperparameters. However,
    when neural networks are brought into the mix, things become too complex. In each
    layer, there are millions or even billions of hyperparameters to tune—so many
    that the cost function becomes non-convex. Another reason for this is that the
    activation functions that are used in hidden layers are non-linear, so the cost
    is non-convex. We will discuss this phenomenon in more detail in later chapters,
    but let's take a quick look at ANNs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用机器学习方法时，我们通常只需处理少量的超参数。然而，当神经网络介入时，情况变得非常复杂。每一层中都有成千上万甚至数十亿个超参数需要调整——如此之多，以至于成本函数变得非凸。另一个原因是，隐藏层中使用的激活函数是非线性的，因此成本也是非凸的。我们将在后面的章节中更详细地讨论这一现象，但现在先简要看看人工神经网络（ANNs）。
- en: Artificial neural networks
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: ANNs, which are inspired by how a human brain works, form the core of DL and
    its true realization. Today's revolution around DL would not have been possible
    without ANNs. Thus, to understand DL, we need to understand how neural networks
    work.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）受到人类大脑工作方式的启发，构成了深度学习（DL）的核心及其真正实现。今天围绕深度学习的革命，如果没有ANNs，是不可能发生的。因此，要理解深度学习，我们需要理解神经网络的工作原理。
- en: ANN and the human brain
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络与人类大脑
- en: ANNs represent one aspect of the human nervous system, and how the nervous system
    consists of a number of neurons that communicate with each other using axons.
    The receptors receive the stimuli either internally or from the external world.
    Then, they pass this information to the biological neurons for further processing.
    There are a number of dendrites, in addition to another long extension called
    the axon. Toward the axon's extremities, there are minuscule structures called
    synaptic terminals, which are used to connect one neuron to the dendrites of other
    neurons. Biological neurons receive short electrical impulses called signals from
    other neurons, and, in response, they trigger their own signals.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络代表了人类神经系统的一个方面，神经系统由大量相互沟通的神经元组成，神经元通过轴突进行信息传递。感受器接收来自内外部世界的刺激，然后将这些信息传递给生物神经元进行进一步处理。除了轴突，还有许多树突存在。在轴突的末端，有一些微小结构叫做突触末端，用来将一个神经元与其他神经元的树突连接起来。生物神经元接收到来自其他神经元的短电流脉冲，称为信号，作为回应，它们会触发自己的信号。
- en: We can, therefore, summarize that the neuron comprises a cell body (also known
    as the **soma**), one or more dendrites for receiving signals from other neurons,
    and an axon for carrying out the signals that are generated by the neurons. A
    neuron is in an active state when it is sending signals to other neurons. However,
    when it is receiving signals from other neurons, it is in an inactive state. In
    an idle state, a neuron accumulates all the signals that are received before reaching
    a certain activation threshold. This whole process motivated researchers to test
    out ANNs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以总结出，神经元由一个细胞体（也称为**体 soma**）、一个或多个树突用于接收来自其他神经元的信号，以及一个轴突用于传递神经元生成的信号。当神经元向其他神经元发送信号时，它处于活动状态。然而，当它接收来自其他神经元的信号时，它处于非活动状态。在空闲状态下，神经元会积累接收到的所有信号，直到达到某个激活阈值。这个过程促使研究人员测试人工神经网络（ANNs）。
- en: A brief history of ANNs
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络的简史
- en: Inspired by the working principles of biological neurons, Warren McCulloch and
    Walter Pitts proposed the first artificial neuron model, in 1943, in terms of
    a computational model of nervous activity. This simple model of a biological neuron,
    also known as an **artificial neuron** (**AN**), has one or more binary (on/off)
    inputs and one output only. An AN simply activates its output when more than a
    certain number of its inputs are active.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 受生物神经元工作原理的启发，沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）于1943年提出了第一个人工神经元模型，这一模型是神经活动的计算模型。这个简单的生物神经元模型，也被称为**人工神经元**（**AN**），具有一个或多个二进制（开/关）输入和仅一个输出。当超过某个数量的输入处于活动状态时，人工神经元便激活其输出。
- en: The example sounds too trivial, but even with such a simplified model, it is
    possible to build a network of ANs. Nevertheless, these networks can be combined
    to compute complex logical expressions too. This simplified model inspired John
    von Neumann, Marvin Minsky, Frank Rosenblatt, and many others to come up with
    another model called a **perceptron,** back in 1957\. The perceptron is one of
    the simplest ANN architectures we have seen in the last 60 years. It is based
    on a slightly different AN called a **Linear Threshold Unit** (**LTU**). The only
    difference is that the inputs and outputs are now numbers instead of binary on/off
    values. Each input connection is associated with a weight. The LTU computes a
    weighted sum of its inputs, then applies a step function (which resembles the
    action of an activation function) to that sum, and outputs the result.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子听起来太简单，但即便是这样一个简化模型，也能构建起人工神经元网络。然而，这些网络也可以组合起来计算复杂的逻辑表达式。这个简化模型激发了约翰·冯·诺依曼（John
    von Neumann）、马文·明斯基（Marvin Minsky）、弗兰克·罗斯恩布拉特（Frank Rosenblatt）等人，于1957年提出了另一个模型——**感知器**（**perceptron**）。感知器是过去60年我们看到的最简单的人工神经网络架构之一。它基于一个略有不同的人工神经元，叫做**线性阈值单元**（**LTU**）。唯一的区别是，输入和输出现在是数字，而非二进制的开/关值。每个输入连接都关联一个权重。LTU计算其输入的加权和，然后将一个阶跃函数（类似于激活函数的作用）应用于该和，并输出结果。
- en: One of the downsides of a perceptron is that its decision boundary is linear.
    Therefore, they are incapable of learning complex patterns. They are also incapable
    of solving some simple problems, such as **Exclusive OR** (**XOR**). However,
    later on, the limitations of perceptrons were somewhat eliminated by stacking
    multiple perceptrons, called **MLP**. So, the most significant progress in ANNs
    and DL can be described in the following timeline. We have already discussed how
    the artificial neurons and perceptrons provided the base in 1943 and 1958, respectively.
    In 1969, Marvin *Minsky* and Seymour *Papert* formulated the XOR as a linearly
    non-separable problem, and later, in 1974, Paul *Werbos* demonstrated the backpropagation
    algorithm for training the perceptron.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的一个缺点是它的决策边界是线性的。因此，它们无法学习复杂的模式，也无法解决一些简单的问题，如**异或**（**XOR**）问题。然而，后来通过堆叠多个感知机，称为**MLP**，在一定程度上克服了感知机的局限性。因此，ANNs和深度学习（DL）领域的最重要进展可以通过以下时间线来描述。我们已经讨论过，人工神经元和感知机分别为1943年和1958年提供了基础。1969年，Marvin
    *Minsky* 和 Seymour *Papert* 将XOR问题定义为一个线性不可分的问题，随后在1974年，Paul *Werbos* 展示了用于训练感知机的反向传播算法。
- en: 'However, the most significant advancement happened in 1982, when John Hopfield
    proposed the Hopfield Network. Then, one of the godfathers of the neural network
    and DL—Hinton and his team—proposed the Boltzmann Machine in 1985\. However, in
    1986 Geoffrey Hinton successfully trained the MLP and Jordan M.I. proposed RNNs.
    In the same year, Paul Smolensky also proposed the improved version of the Boltzmann
    Machine, called the **Restricted Boltzmann Machine** (**RBM**). Then, in 1990,
    Lecun et al. proposed LeNet, which is a deep neural network architecture. For
    a brief glimpse, refer to the following diagram:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最重要的进展发生在1982年，当时John Hopfield提出了Hopfield网络。随后，神经网络和深度学习的奠基人之一——Hinton及其团队——在1985年提出了Boltzmann机。1986年，Geoffrey
    Hinton成功地训练了MLP，而Jordan M.I.提出了RNN。同年，Paul Smolensky还提出了Boltzmann机的改进版——**限制玻尔兹曼机**（**RBM**）。接着，1990年，Lecun等人提出了LeNet，这是一种深度神经网络架构。以下是简要概览，请参阅下图：
- en: '![](img/cb122a6b-b05d-4691-8b20-edad562ebfa2.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb122a6b-b05d-4691-8b20-edad562ebfa2.png)'
- en: The most significant year of the 90's era was 1997, when Jordan et al. proposed
    a **recurrent neural network** (**RNN**). In the same year, Schuster et al. proposed
    the improved version of **long-short term memory** (**LSTM**) and the improved
    version of the original RNN called bidirectional RNN.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 90年代最重要的一年是1997年，当时Jordan等人提出了**递归神经网络**（**RNN**）。同年，Schuster等人提出了**长短期记忆**（**LSTM**）的改进版，并提出了原始RNN的改进版本——双向RNN。
- en: Despite significant advances in computing, from 1997 to 2005, we did not experience
    much advancement. Then, in 2006, Hinton struck again when, he and his team proposed
    a **deep belief network** (**DBN**) by stacking multiple RBMs. Then in 2012, Hinton
    invented the dropout that significantly improved the regularization and overfitting
    in the deep neural network. After that, Ian Goodfellow et al. introduced the GANs—a
    significant milestone in image recognition. In 2017, Hinton proposed CapsNet to
    overcome the limitation of regular CNNs, and this is so far one of the most remarkable
    milestones. We will discuss these architectures later in this chapter.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算机技术取得了显著进展，但从1997年到2005年，我们并未看到太多进展。直到2006年，**Hinton**再次带来了突破，他和他的团队通过堆叠多个RBM提出了**深度置信网络**（**DBN**）。随后，在2012年，Hinton发明了dropout技术，显著改善了深度神经网络中的正则化和过拟合问题。之后，Ian
    Goodfellow等人介绍了GANs——这是图像识别领域的一个重大里程碑。2017年，Hinton提出了CapsNet，旨在克服传统CNN的局限性，这至今仍是一个非常重要的里程碑。我们将在本章后续部分讨论这些架构。
- en: How does an ANN learn?
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络是如何学习的？
- en: 'Based on the concept of biological neurons, the term and idea of ANNs arose.
    Similar to biological neurons, the artificial neuron consists of the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生物神经元的概念，人工神经网络（ANN）的术语和理念应运而生。与生物神经元类似，人工神经元由以下几个部分组成：
- en: One or more incoming connections that aggregate signals from neurons
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个输入连接，用于聚合来自其他神经元的信号
- en: One or more output connections for carrying the signal to the other neurons
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个输出连接，用于将信号传递到其他神经元
- en: An activation function, which determines the numerical value of the output signal
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数，决定输出信号的数值
- en: Besides the state of a neuron, synaptic weight is considered, which influences
    the connection within the network. Each weight has a numerical value indicated
    by *W[ij]*, which is the synaptic weight connecting neuron *i* to neuron *j*.
    Now, for each neuron *i*, an input
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了神经元的状态，还考虑了突触权重，它影响网络中的连接。每个权重都有一个数值，表示为*W[ij]*，它是连接神经元*i*和神经元*j*的突触权重。现在，对于每个神经元*i*，输入
- en: 'vector can be defined by *x[i] = (x[1],x[2],...x[n])*, and a weight vector
    can be defined by *w[i] = (w[i1],x[i2],...x[in])*. Now, depending on the position
    of a neuron, the weights and the output function determine the behavior of an
    individual neuron. Then, during forward propagation, each unit in the hidden layer
    gets the following signal:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 向量可以定义为*x[i] = (x[1],x[2],...x[n])*，权重向量可以定义为*w[i] = (w[i1],x[i2],...x[in])*。现在，根据神经元的位置，权重和输出函数决定了单个神经元的行为。然后，在前向传播过程中，隐含层中的每个单元接收到以下信号：
- en: '![](img/239e1275-0719-40b1-9689-eb3ea9bcf6a1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/239e1275-0719-40b1-9689-eb3ea9bcf6a1.png)'
- en: 'Nevertheless, among the weights, there is also a special type of weight called
    a bias unit, *b*. Technically, bias units aren''t connected to any previous layer,
    so they don''t have true activity. But still, the bias *b* value allows the neural
    network to shift the activation function to the left or right. By taking the bias
    unit into consideration, the modified network output is formulated as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在权重中，还有一种特殊类型的权重，称为偏置单元，*b*。从技术上讲，偏置单元没有连接到任何先前的层，因此它们没有真正的活动。但仍然，偏置*b*值使神经网络能够将激活函数向左或向右移动。考虑到偏置单元后，修改后的网络输出公式如下：
- en: '![](img/dc603183-bf05-4ad0-b110-7847a8084d82.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc603183-bf05-4ad0-b110-7847a8084d82.png)'
- en: 'The preceding equation signifies that each hidden unit gets the sum of inputs,
    multiplied by the corresponding weight—this is known as the **Summing junction**.
    Then, the resultant output in the **Summing junction** is passed through the activation
    function, which squashes the output, as depicted in the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程表示，每个隐含单元接收输入的总和，并乘以相应的权重——这被称为**求和结点**。然后，**求和结点**中的输出通过激活函数，该函数压缩输出，如下图所示：
- en: '![](img/2401dc9f-11f6-42e3-90e0-f31f4f14bfb7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2401dc9f-11f6-42e3-90e0-f31f4f14bfb7.png)'
- en: 'A practical neural network architecture, however, is composed of input, hidden,
    and output layers that are composed of nodes that make up a network structure.
    It still follows the working principle of an artificial neuron model, as shown
    in the preceding diagram. The input layer only accepts numeric data, such as features
    in real numbers, and images with pixel values. The following diagram shows a neural
    network architecture for solving a multiclass classification (that is, 10 classes)
    problem based on a data having 784 features:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际的神经网络架构由输入层、隐含层和输出层组成，这些层由节点组成，形成一个网络结构。它仍然遵循人工神经元模型的工作原理，如上图所示。输入层只接收数值数据，例如实数特征和具有像素值的图像。下图展示了一个用于解决多类分类问题（即10个类别）的神经网络架构，数据具有784个特征：
- en: '![](img/e3fd76b9-b477-4275-8b19-8217f4a4483a.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3fd76b9-b477-4275-8b19-8217f4a4483a.png)'
- en: A neural network with one input layer, three hidden layers, and an output layer
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有一个输入层、三个隐含层和一个输出层的神经网络
- en: 'Here, the hidden layers perform most of the computation to learn the patterns,
    and the network evaluates how accurate its prediction is compared to the actual
    output using a special mathematical function called the loss function. It could
    be a complex one or a very simple mean squared error, which can be defined as
    follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，隐含层执行大部分计算以学习模式，网络评估其预测与实际输出相比的准确性，使用一种称为损失函数的特殊数学函数。它可以是一个复杂的函数，也可以是一个非常简单的均方误差，定义如下：
- en: '![](img/bea114d3-df9b-4ac8-98b1-41acf87a2dc2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bea114d3-df9b-4ac8-98b1-41acf87a2dc2.png)'
- en: In the preceding equation, ![](img/4058e15e-01a0-4db0-9f89-4de3196c76f3.png)
    is the prediction made by the network, while *Y* represents the actual or expected
    output. Finally, when the error is no longer being reduced, the neural network
    converges and makes a prediction through the output layer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，![](img/4058e15e-01a0-4db0-9f89-4de3196c76f3.png)是网络做出的预测，而*Y*表示实际或预期的输出。最后，当误差不再减少时，神经网络收敛，并通过输出层进行预测。
- en: Training a neural network
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: The learning process for a neural network is configured as an iterative process
    of the optimization of the weights. The weights are updated in each epoch. Once
    the training starts, the aim is to generate predictions by minimizing the loss
    function. The performance of the network is then evaluated on the test set. We
    already know about the simple concept of an artificial neuron. However, generating
    only some artificial signals is not enough to learn a complex task. As such, a
    commonly used supervised learning algorithm is the backpropagation algorithm,
    which is very often used to train a complex ANN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习过程被配置为一个权重优化的迭代过程。每个epoch都会更新权重。一旦训练开始，目标就是通过最小化损失函数来生成预测。然后，网络的性能会在测试集上进行评估。我们已经了解了人工神经元的简单概念。然而，仅生成一些人工信号不足以学习复杂任务。因此，常用的监督学习算法是反向传播算法，它通常用于训练复杂的人工神经网络（ANN）。
- en: Ultimately, training such a neural network is an optimization problem, too,
    in which we try to minimize the error by adjusting network weights and biases
    iteratively, by using backpropagation through **gradient descent** (**GD**). This
    approach forces the network to backtrack through all its layers to update the
    weights and biases across nodes in the opposite direction of the loss function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，训练这样的神经网络也是一个优化问题，我们通过反向传播和**梯度下降**（**GD**）来迭代地调整网络的权重和偏置，以最小化误差。该方法迫使网络通过其所有层回溯，以更新跨节点的权重和偏置，方向与损失函数相反。
- en: 'However, this process using GD does not guarantee that the global minimum is
    reached. The presence of hidden units and the non-linearity of the output function
    means that the behavior of the error is very complex and has many local minima.
    This backpropagation step is typically performed thousands or millions of times,
    using many training batches, until the model parameters converge to values that
    minimize the cost function. The training process ends when the error on the validation
    set begins to increase, because this could mark the beginning of a phase of overfitting:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用梯度下降（GD）进行的这个过程并不能保证达到全局最小值。隐藏单元的存在和输出函数的非线性意味着误差的行为非常复杂，并且有许多局部最小值。这个反向传播步骤通常会执行成千上万次甚至数百万次，使用许多训练批次，直到模型参数收敛到最小化成本函数的值。当验证集上的误差开始增加时，训练过程结束，因为这可能标志着过拟合阶段的开始：
- en: '![](img/964eadcf-5c58-41d9-92a5-2122d399085d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/964eadcf-5c58-41d9-92a5-2122d399085d.png)'
- en: Searching for the minimum for the error function E, we move in the direction
    in which the gradient G of E is minimal
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找误差函数E的最小值时，我们沿着误差函数E的梯度G最小的方向前进
- en: The downside of using GD is that it takes too long to converge, which makes
    it impossible to meet the demand of handling large-scale training data. Therefore,
    a faster GD, called **stochastic gradient descent** (**SGD**) was proposed, which
    is also a widely used optimizer in DNN training. In SGD, we use only one training
    sample per iteration from the training set to update the network parameters, which
    is a stochastic approximation of the true cost gradient.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降（GD）的缺点是它收敛的时间太长，这使得无法满足处理大规模训练数据的需求。因此，提出了一种更快的梯度下降算法，称为**随机梯度下降**（**SGD**），它也是深度神经网络训练中广泛使用的优化器。在SGD中，我们每次迭代只使用训练集中的一个训练样本来更新网络参数，这是对真实成本梯度的随机近似。
- en: There are other advanced optimizers nowadays such as Adam, RMSProp, ADAGrad,
    and Momentum. Each of them is either a direct or indirect optimized version of
    SGD.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有其他先进的优化器，如Adam、RMSProp、ADAGrad和Momentum。它们中的每一个都是SGD的直接或间接优化版本。
- en: Weight and bias initialization
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重和偏置初始化
- en: 'Now, here''s a tricky question: how do we initialize the weights? Well, if
    we initialize all the weights to the same value (for example, 0 or 1), each hidden
    neuron will get the same signal. Let''s try to break it down:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个棘手的问题：我们如何初始化权重呢？好吧，如果我们将所有权重初始化为相同的值（例如0或1），那么每个隐藏神经元将获得相同的信号。让我们尝试分解这个问题：
- en: If all weights are initialized to 1, then each unit gets a signal equal to the
    sum of the inputs.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有权重初始化为1，那么每个单元将获得等于输入总和的信号。
- en: If all weights are 0, which is even worse, then every neuron in a hidden layer
    will get zero signal.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有的权重都为0，这甚至更糟，那么每个隐藏层中的神经元将会得到零信号。
- en: For network weight initialization, Xavier initialization is used widely. It
    is similar to random initialization, but often turns out to work much better,
    since it can identify the rate of initialization depending on the total number
    of input and output neurons by default. You may be wondering whether you can get
    rid of random initialization while training a regular DNN.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络权重初始化，Xavier 初始化方法被广泛使用。它与随机初始化类似，但通常效果更好，因为它可以根据默认的输入和输出神经元的总数来确定初始化速率。你可能会想，在训练常规
    DNN 时，是否能够摆脱随机初始化。
- en: Well, recently, some researchers have been talking about random orthogonal matrix
    initializations that perform better than just any random initialization for training
    DNNs. When it comes to initializing the biases, we can initialize them to zero.
    But setting the biases to a small constant value, such as 0.01 for all biases,
    ensures that all **rectified linear units** (**ReLU**) can propagate a gradient.
    However, it neither performs well nor shows consistent improvement. Therefore,
    sticking with zero is recommended.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 近期，一些研究者讨论了随机正交矩阵初始化方法，它比普通的随机初始化方法在训练 DNN 时表现更好。至于偏置的初始化，我们可以将其初始化为零。但是，将偏置设置为一个小常数值，例如将所有偏置设为
    0.01，能确保所有**修正线性单元**（**ReLU**）能够传播梯度。然而，这种方法既表现不佳，也没有持续的改善。因此，推荐坚持使用零值。
- en: Activation functions
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'To allow a neural network to learn complex decision boundaries, we apply a
    non-linear activation function to some of its layers. Commonly used functions
    include Tanh, ReLU, softmax, and variants of these. More technically, each neuron
    receives a signal of the weighted sum of the synaptic weights and the activation
    values of the neurons that are connected as input. One of the most widely used
    functions for this purpose is the so-called sigmoid logistic function, which is
    defined as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让神经网络能够学习复杂的决策边界，我们对其部分层应用非线性激活函数。常用的激活函数包括 Tanh、ReLU、softmax 及其变种。从技术上讲，每个神经元接收来自与之相连的神经元的突触权重和激活值的加权总和作为输入。为此目的，最常用的函数之一是所谓的
    Sigmoid 逻辑函数，其定义如下：
- en: '![](img/d18ee45f-3d01-4f2c-9ce3-8ffc88c9d079.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d18ee45f-3d01-4f2c-9ce3-8ffc88c9d079.png)'
- en: The domain of this function includes all real numbers, and the co-domain is
    (0, 1). This means that any value obtained as an output from a neuron (as per
    the calculation of its activation state) will always be between zero and one.
    The Sigmoid function, as
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的定义域包括所有实数，而共域为 (0, 1)。这意味着从神经元输出的任何值（根据其激活状态的计算）都将始终介于零和一之间。Sigmoid 函数如
- en: 'represented in the following diagram, provides an interpretation of the saturation
    rate of a neuron, from not being active (equal to 0) to complete saturation, which
    occurs at a predetermined maximum value (equal to 1):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下图所示，提供了神经元饱和度的解释，从不激活（等于 0）到完全饱和，这发生在预定的最大值（等于 1）时：
- en: '![](img/09c9bd66-be56-4e39-8322-365a9339cfcc.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09c9bd66-be56-4e39-8322-365a9339cfcc.png)'
- en: Sigmoid versus Tanh activation function
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 与 Tanh 激活函数
- en: 'On the other hand, a hyperbolic tangent, or **Tanh**, is another form of activation
    function. **Tanh** flattens a real-valued number between **-1** and **1**. The
    preceding graph shows the difference between the **Tanh** and **Sigmoid** activation
    functions. In particular, mathematically,  speaking the *tanh* activation function
    can be expressed as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，双曲正切（**Tanh**）是另一种形式的激活函数。**Tanh** 将实数值压缩到 **-1** 和 **1** 之间。前图展示了 **Tanh**
    和 **Sigmoid** 激活函数的区别。特别地，从数学角度讲，*tanh* 激活函数可以表示如下：
- en: '![](img/a31a5802-f491-4761-a390-73b2cbc778c3.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a31a5802-f491-4761-a390-73b2cbc778c3.png)'
- en: In general, in the last level of a **feedforward neural network** (**FFNN**),
    the softmax function is applied as the decision boundary. This is a common case,
    especially when solving a classification problem. The softmax function is used
    for the probability distribution over the possible classes in a multiclass classification
    problem. To conclude, choosing proper activation functions and network weight
    initializations are two problems that make a network perform at its best and help
    to obtain good training. Now that we know the brief history of neural networks,
    let's deepdive into different architectures in the next section, which will give
    us an idea of their usage.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在**前馈神经网络**（**FFNN**）的最后一层，应用 softmax 函数作为决策边界。这是一个常见的情况，特别是在解决分类问题时。softmax
    函数用于多类分类问题中可能类别的概率分布。总之，选择合适的激活函数和网络权重初始化是使网络表现最佳并帮助获得良好训练的两个关键问题。现在我们已经了解了神经网络的简要历史，接下来我们将深入探讨不同的架构，这将帮助我们理解它们的使用场景。
- en: Neural network architectures
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: 'Up to now, numerous neural network architectures have been proposed and are
    in use. However, more or less all of them are based on a few core neural network
    architectures. We can categorize DL architectures into four groups:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已经提出并应用了许多神经网络架构。然而，或多或少，所有这些架构都基于少数几个核心神经网络架构。我们可以将深度学习（DL）架构分为四类：
- en: Deep neural networks
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络（Deep neural networks）
- en: Convolutional neural networks
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（Convolutional neural networks）
- en: Recurrent neural networks
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（Recurrent neural networks）
- en: Emergent architectures
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新兴架构（Emergent architectures）
- en: However, DNNs, CNNs, and RNNs have many improved variants. Although most of
    the variants are proposed or developed for solving domain-specific research problems,
    the basic working principles still follow the original DNN, CNN, and RNN architectures.
    The following subsections will give you a brief introduction to these architectures.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DNN、CNN 和 RNN 都有许多改进的变种。虽然大多数变种是为了应对特定领域的研究问题而提出或开发的，但其基本工作原理仍然遵循原始的 DNN、CNN
    和 RNN 架构。接下来的小节将简要介绍这些架构。
- en: Deep neural networks
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络（Deep neural networks）
- en: DNNs are neural networks that have a complex and deeper architecture with a
    large number of neurons in each layer, and many connections between them. Although
    DNN refers to a very deep network, for simplicity, we consider MLP, **stacked
    autoencoder **(**SAE**), and **deep belief networks** (**DBNs**) as DNN architectures.
    These architectures mostly work as an FFNN, meaning information propagates from
    input to output layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 是具有复杂和深层架构的神经网络，每一层有大量神经元，并且层与层之间有许多连接。虽然 DNN 通常指的是非常深的网络，但为了简单起见，我们将 MLP、**堆叠自编码器**（**SAE**）和**深度置信网络**（**DBNs**）视为
    DNN 架构。这些架构大多作为 FFNN 工作，意味着信息从输入层传播到输出层。
- en: 'Multiple perceptrons are stacked together as MLPs, where layers are connected
    as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since
    it has three layers: an input layer, a hidden layer, and an output layer. This
    way, the signal propagates one way, from the input layer to the hidden layers
    to the output layer, as shown in the following diagram:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 多个感知器被堆叠在一起形成多层感知机（MLPs），其中层作为有向图连接。从根本上讲，MLP 是最简单的前馈神经网络（FFNN）之一，因为它有三层：输入层、隐藏层和输出层。这样，信号以单向传播的方式，从输入层经过隐藏层到输出层，如下图所示：
- en: '![](img/2b984dc6-a80d-4037-adc9-dc61d4be1b5a.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b984dc6-a80d-4037-adc9-dc61d4be1b5a.png)'
- en: 'Autoencoders and RBMs are the basic building blocks for SAEs and DBNs, respectively.
    Unlike MLP, which is an FFNN that''s trained in a supervised way, both SAEs and
    DBNs are trained in two phases: unsupervised pretraining and supervised fine-tuning.
    In unsupervised pretraining, layers are stacked in order and trained in a layer-wise
    manner with used unlabeled data.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（Autoencoders）和限制玻尔兹曼机（RBMs）分别是堆叠自编码器（SAEs）和深度置信网络（DBNs）的基础构建模块。与多层感知机（MLP）不同，MLP
    是一种以监督方式训练的前馈神经网络（FFNN），而 SAEs 和 DBNs 都是在两个阶段进行训练的：无监督预训练和监督微调。在无监督预训练阶段，层按顺序堆叠并以逐层的方式进行训练，使用的是未标记数据。
- en: 'In supervised fine-tuning, an output classifier layer is stacked and the complete
    neural network is optimized by retraining with labeled data. One problem with
    MLP is that it often overfits the data, so it doesn''t generalize well. To overcome
    this issue, DBN was proposed by Hinton et al. It uses a greedy, layer-by-layer,
    pretraining algorithm. DBNs are composed of a visible layer and multiple hidden
    unit layers. The building blocks of a DBN are RBMs, as shown in the following
    diagram, where several RBMs are stacked one after another:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督式微调中，会叠加一个输出分类器层，并通过用标签数据重新训练来优化整个神经网络。MLP的一个问题是它经常会发生过拟合，因此泛化能力较差。为了解决这个问题，Hinton等人提出了深度信念网络（DBN）。DBN使用了一种贪婪的逐层预训练算法。DBN由一个可见层和多个隐藏单元层组成。DBN的构建模块是限制玻尔兹曼机（RBM），如下图所示，其中几个RBM层被一个接一个地堆叠起来：
- en: '![](img/6a7b0c86-7732-461b-93b4-3512d208d1b8.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a7b0c86-7732-461b-93b4-3512d208d1b8.png)'
- en: The top two layers have undirected, symmetric connections in-between, but the
    lower layers have directed connections from the preceding layer. Despite numerous
    successes, DBNs are now being replaced with AEs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前两层具有无向对称连接，而下层则有来自前一层的有向连接。尽管自深度信念网络（DBNs）取得了许多成功，但现在它们正被自编码器所替代。
- en: Autoencoders
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'AEs are also special types of neural networks that learn automatically from
    the input data. AEs consist of two components: the encoder and the decoder. The
    encoder compresses the input into a latent-space representation. Then, the decoder
    part tries to reconstruct the original input data from this representation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AEs）也是一种特殊类型的神经网络，可以从输入数据中自动学习。自编码器由两个组件组成：编码器和解码器。编码器将输入压缩成一个潜在空间表示。然后，解码器部分尝试从这个表示中重建原始输入数据：
- en: '**Encoder**: Encodes or compresses the input into a latent-space representation
    using a function known as *h = f(x)*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：使用一个称为*h = f(x)*的函数将输入编码或压缩成一个潜在空间表示'
- en: '**Decoder**: Decodes or reconstructs the input from the latent space representation
    using a function known as *r = g(h)*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：使用一个称为*r = g(h)*的函数从潜在空间表示中解码或重建输入'
- en: 'So, an AE can be described by a function of* g(f(x)) = 0*, where we want 0
    as close to the original input of *x*. The following diagram shows how an AE typically
    works:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，AE可以用* g(f(x)) = 0*的函数来描述，其中我们希望0尽可能接近原始输入*x*。以下图示展示了自编码器的典型工作方式：
- en: '![](img/bcbf8bbf-53d7-4d16-9fdc-0bf5612e3e25.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcbf8bbf-53d7-4d16-9fdc-0bf5612e3e25.png)'
- en: AEs are very useful for data denoising and dimensionality reduction for data
    visualization because they can learn data projections called representations more
    effectively than PCA.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器在数据去噪和数据可视化的降维方面非常有用，因为它们比主成分分析（PCA）更有效地学习被称为表示的数据投影。
- en: Convolutional neural networks
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'CNNs have achieved much and have been widely adopted in computer vision (for
    example, image recognition). In CNN networks, the connection schemes are significantly
    different compared to an MLP or DBN. A few of the convolutional layers are connected
    in a cascade style. Each layer is backed up by an ReLU layer, a pooling layer,
    additional convolutional layers (+ReLU), and another pooling layer, which is followed
    by a fully connected layer and a softmax layer. The following diagram is a schematic
    of the architecture of a CNN that''s used for facial recognition, which takes
    facial images as input and predicts emotions such as anger, disgust, fear, happy,
    and sad:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）取得了巨大成功，并已广泛应用于计算机视觉领域（例如，图像识别）。在卷积神经网络中，连接模式与多层感知机（MLP）或深度信念网络（DBN）有显著不同。几个卷积层以级联样式连接。每一层后面都有一个ReLU层、一个池化层、附加的卷积层（+ReLU），以及另一个池化层，接着是一个全连接层和一个softmax层。以下图示为面部识别用CNN架构的示意图，它接受面部图像作为输入，并预测愤怒、厌恶、恐惧、快乐和悲伤等情绪：
- en: '![](img/2a4a34db-3397-44cb-8825-78eb3053eae0.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a4a34db-3397-44cb-8825-78eb3053eae0.png)'
- en: A schematic architecture of a CNN used for facial recognition
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 用于面部识别的CNN示意架构
- en: Importantly, DNNs have no prior knowledge of how the pixels are organized because
    they do not know that nearby pixels are close. CNNs embed this prior knowledge
    using lower layers by using feature maps in small areas of the image, while the
    higher layers combine lower-level features into larger features.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，深度神经网络（DNNs）并没有预先了解像素如何组织，因为它们不知道相邻像素是相近的。卷积神经网络通过在图像的小区域内使用特征图，利用低层嵌入了这种先验知识，而高层则将低级特征组合成更大的特征。
- en: This setting works well with most of the natural images, giving CNN a decisive
    head start over DNNs. The output from each convolutional layer is a set of objects,
    called feature maps, that are generated by a single kernel filter. Then, the feature
    maps can be used to define a new input to the next layer. Each neuron in a CNN
    network produces an output, followed by an activation threshold, which is proportional
    to the input and not bound.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置在大多数自然图像中效果良好，给CNN带来了比DNN更具决定性的优势。每一层卷积层的输出是一组对象，称为特征图，它们是由单个卷积核滤波器生成的。然后，特征图可以用于定义下一个层的输入。CNN网络中的每个神经元都会产生一个输出，接着是一个激活阈值，这个值与输入成比例并且不受限制。
- en: Recurrent neural networks
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'In RNNs, connections between units form a directed cycle. The RNN architecture
    was originally conceived by Hochreiter and Schmidhuber in 1997\. RNN architectures
    have standard MLPs, plus added loops so that they can exploit the powerful nonlinear
    mapping capabilities of the MLP. They also have some form of memory. The following
    diagram shows a very basic RNN that has an input layer, two recurrent layers,
    and an output layer:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中，单元之间的连接形成了一个有向循环。RNN架构最初是由Hochreiter和Schmidhuber在1997年提出的。RNN架构包含标准的MLP，并增加了循环，以便能够利用MLP强大的非线性映射能力。它们还具备某种形式的记忆。以下图示展示了一个非常基础的RNN，它包含一个输入层、两个递归层和一个输出层：
- en: '![](img/77b89365-780e-4461-a58b-6e3c4e492336.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77b89365-780e-4461-a58b-6e3c4e492336.png)'
- en: 'However, this basic RNN suffers from gradient vanishing and the exploding problem,
    and cannot model long-term dependencies. These architectures include LSTM, **gated
    recurrent units** (**GRUs**), bidirectional-LSTM, and other variants. Consequently,
    LSTM and GRU can overcome the drawbacks of regular RNNs: the gradient vanishing/exploding
    problem and long-short term dependency.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种基本的RNN存在梯度消失和梯度爆炸问题，无法建模长期依赖关系。这些架构包括LSTM、**门控递归单元**（**GRUs**）、双向LSTM和其他变种。因此，LSTM和GRU能够克服常规RNN的缺点：梯度消失/爆炸问题以及长期和短期依赖问题。
- en: Emergent architectures
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新兴架构
- en: Many other emergent DL architectures have been suggested, such as **Deep SpatioTemporal
    Neural Networks** (**DST-NNs**), **Multi-Dimensional Recurrent Neural Networks**
    (**MD-RNNs**), and **Convolutional AutoEncoders** (**CAEs**). Nevertheless, there
    are a few more emerging networks, such as **CapsNets** (which is an improved version
    of a CNN, designed to remove the drawbacks of regular CNNs), RNN for image recognition,
    and **Generative Adversarial Networks** (**GANs**) for simple image generation.
    Apart from these, factorization machines for personalization and deep reinforcement
    learning are also being used widely.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他新兴的深度学习架构也已经被提出，例如**深度时空神经网络**（**DST-NNs**）、**多维递归神经网络**（**MD-RNNs**）和**卷积自编码器**（**CAEs**）。然而，仍有一些新兴网络，如**CapsNets**（它是CNN的改进版本，旨在消除常规CNN的缺点）、用于图像识别的RNN和用于简单图像生成的**生成对抗网络**（**GANs**）。除此之外，个性化的因子分解机和深度强化学习也在广泛应用。
- en: Residual neural networks
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差神经网络
- en: Since there are sometimes millions and millions of hyperparameters and other
    practical aspects, it's really difficult to train deeper neural networks. To overcome
    this limitation, Kaiming H. et al. ( [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1))
    proposed a residual learning framework to ease the training of networks that are
    substantially deeper than those used previously.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有时会有数百万个超参数和其他实际问题，训练更深的神经网络真的很困难。为了克服这一限制，Kaiming H.等人（[https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1)）提出了一种残差学习框架，以简化训练远深于以前的网络。
- en: They also explicitly reformulated the layers as learning residual functions
    with reference to the layer inputs, instead of learning non-referenced functions.
    This way, these residual networks are easier to optimize and can gain accuracy
    from considerably increased depth. The downside is that building a network by
    simply stacking residual blocks inevitably limits the optimization ability. To
    overcome this limitation, Ke Zhang et al. also proposed using a multilevel residual
    network ([https://arxiv.org/abs/1608.02908](https://arxiv.org/abs/1608.02908)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还明确地将层重新表述为参考层输入的学习残差函数，而不是学习无参考的函数。这样，这些残差网络更容易优化，并且可以通过大幅增加网络深度来提高准确性。缺点是，仅仅通过堆叠残差块构建网络必然会限制优化能力。为克服这一局限性，Ke
    Zhang等人还提出了使用多级残差网络（[https://arxiv.org/abs/1608.02908](https://arxiv.org/abs/1608.02908)）。
- en: Generative adversarial networks
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: 'GANs are deep neural net architectures that consist of two networks pitted
    against each other (hence the name *adversarial*). Ian Goodfellow et al. introduced
    GANs in a paper (see more at [https://arxiv.org/abs/1406.2661v1](https://arxiv.org/abs/1406.2661v1)).
    In GANs, the two main components are the **generator and discriminator**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是深度神经网络架构，由两个网络相互对抗（因此得名*对抗性*）。Ian Goodfellow等人通过一篇论文引入了GAN（更多信息见[https://arxiv.org/abs/1406.2661v1](https://arxiv.org/abs/1406.2661v1)）。在GAN中，两个主要组件是**生成器和判别器**：
- en: '![](img/4920409b-74a3-482e-a9f8-638542cdeab9.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4920409b-74a3-482e-a9f8-638542cdeab9.png)'
- en: Working principle of generative adversarial networks
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络的工作原理
- en: 'In a GAN architecture, a generator and a discriminator are pitted against each
    other—hence the name, adversarial:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN架构中，生成器和判别器是相互对抗的—因此得名，对抗性：
- en: The generator tries to generate data samples out of a specific probability distribution
    and is very similar to the actual object.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器尝试从特定的概率分布中生成数据样本，并且这些样本与实际对象非常相似。
- en: The discriminator will judge whether its input is coming from the original training
    set or from the generator part.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器将判断其输入是来自原始训练集还是来自生成器部分。
- en: Many DL practitioners think that GANs were one of the most important advancements
    because GANs can be used to mimic any distribution of data, and, based on the
    data distribution, they can be taught to create robot artist images, super-resolution
    images, text-to-image synthesis, music, speech, and more. For example, because
    of the concept of adversarial training, Facebook's AI research director, Yann
    LeCun, suggested that GANs are the most interesting idea in the last 10 years
    of ML.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习从业者认为GAN是最重要的进展之一，因为GAN可以用于模拟任何数据分布，并且基于数据分布，它们可以被训练来生成机器人艺术家图像、超分辨率图像、文本到图像合成、音乐、语音等。例如，由于对抗训练的概念，Facebook的AI研究总监Yann
    LeCun建议，GAN是过去10年机器学习领域最有趣的想法。
- en: Capsule networks
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络
- en: 'In CNNs, each layer understands an image at a much more granular level through
    a slow receptive field or max pooling operations. If the images have rotation,
    tilt, or very different shapes or orientation, CNNs fail to extract such spatial
    information and show very poor performance at image processing tasks. Even the
    pooling operations in CNNs cannot be much help against such positional invariance.
    This issue in CNNs has led us to the recent advancement of CapsNet through the
    paper entitled *Dynamic Routing Between Capsules* (see more at [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829))
    by Geoffrey Hinton et al:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，每一层通过缓慢的感受野或最大池化操作在更精细的层次上理解图像。如果图像存在旋转、倾斜或非常不同的形状或方向，CNN无法提取这些空间信息，在图像处理任务中表现非常差。即使是CNN中的池化操作，在面对这种位置不变性时也没有太大帮助。CNN中的这个问题促使我们在Geoffrey
    Hinton等人发表的论文《*胶囊间动态路由*》中，推动了CapsNet的最新进展（更多信息见[https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)）：
- en: '"A capsule is a group of neurons whose activity vector represents the instantiation
    parameters of a specific type of entity, such as an object or an object part."'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: “胶囊是神经元的组，其活动向量表示特定类型实体的实例化参数，例如一个物体或物体的部分。”
- en: 'Unlike a regular DNN, where we keep on adding layers, in CapsNet, the idea
    is to add more layers inside a single layer. This way, a CapsNet is a nested set
    of neural layers. In CapsNet, the vector inputs and outputs of a capsule are computed
    using the routing algorithm used in physics, which iteratively transfers information
    and processes the **self-consistent field** (**SCF**) procedure:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规DNN不同，在CapsNet中，思想是将更多的层添加到单一层内部。这样，CapsNet就成为了一组嵌套的神经层。在CapsNet中，胶囊的向量输入和输出是通过在物理学中使用的路由算法进行计算的，这一算法迭代地传递信息并处理**自洽场**（**SCF**）过程：
- en: '![](img/14df1eb2-37d6-4291-b0c9-453194b85145.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14df1eb2-37d6-4291-b0c9-453194b85145.png)'
- en: The preceding diagram shows a schematic diagram of a simple three-layer CapsNet.
    The length of the activity vector of each capsule in the DigiCaps layer indicates
    the presence of an instance of each class, which is used to calculate the loss.
    Now that we have learned about the working principles of neural networks and the
    different neural network architectures, implementing something hands-on would
    be great. However, before that, let's take a look at some popular DL libraries
    and frameworks, which come with the implementation of these network architectures.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个简单的三层CapsNet的示意图。每个Capsule中DigiCaps层的活动向量长度表示每个类别实例的存在，用于计算损失。现在我们已经了解了神经网络的工作原理和不同的神经网络架构，动手实现一些东西会很有趣。不过，在此之前，让我们先看一下几个流行的DL库和框架，它们已经实现了这些网络架构。
- en: Neural networks for clustering analysis
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析的神经网络
- en: Several variants of k-means have been proposed to address issues with higher-dimensional
    input spaces. However, they are fundamentally limited to linear embedding. Hence,
    we cannot model non-linear relationships. Nevertheless, fine-tuning in these approaches
    is based on only cluster assignment hardening loss (see later in this section).
    Therefore, a fine-grained clustering accuracy cannot be achieved. Since the quality
    of the clustering results is dependent on the data distribution, deep architecture
    can help the model learn mapping from the data space to a lower-dimensional feature
    space in which it iteratively optimizes a clustering objective. Several approaches
    have been proposed over the last few years, trying to use the representational
    power of deep neural networks for preprocessing clustering inputs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决高维输入空间的问题，提出了几种k-means的变种。然而，它们本质上仅限于线性嵌入。因此，我们无法建模非线性关系。尽管如此，这些方法中的微调仅基于聚类分配硬化损失（见本节后文）。因此，无法实现精细的聚类准确性。由于聚类结果的质量依赖于数据分布，深度架构可以帮助模型学习从数据空间到较低维特征空间的映射，在该空间中模型可以迭代优化聚类目标。过去几年中，提出了几种方法，尝试利用深度神经网络的表征能力来预处理聚类输入。
- en: A few notable approaches include deep embedded clustering, deep clustering networks,
    discriminatively boosted clustering, clustering CNNs, deep embedding networks,
    convolutional deep embedded clustering, and joint unsupervised learning of deep
    representation for images. Other approaches include DL with non-parametric clustering,
    CNN-based joint clustering and representation learning with feature drift compensation,
    learning latent representations in neural networks for clustering, clustering
    using convolutional neural networks, and deep clustering with convolutional autoencoder
    embedding.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一些值得注意的方法包括深度嵌入聚类、深度聚类网络、判别性增强聚类、聚类CNN、深度嵌入网络、卷积深度嵌入聚类以及图像深度表示的联合无监督学习。其他方法包括基于DL的非参数聚类、基于CNN的联合聚类与特征漂移补偿学习、神经网络中的潜在表示学习用于聚类、使用卷积神经网络进行聚类，以及通过卷积自编码器嵌入进行深度聚类。
- en: 'Most of these approaches follow more or less the same principle: that is, representation
    learning using a deep architecture to transform the inputs into a latent representation
    and using these representations as input for a specific clustering method. Such
    deep architectures include MLP, CNN, DBN, GAN, and variational autoencoders. The
    following diagram shows an example of how to improve the clustering performance
    of a DEC network using convolutional autoencoders and optimizing both reconstruction
    and CAH losses jointly. The latent space out of the encoder layer is fed to K-means
    for soft clustering assignment. Blurred genetic variants signify the existence
    of reconstruction errors:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法大多遵循相似的原理：即使用深度架构进行表示学习，将输入转换为潜在表示，并使用这些表示作为特定聚类方法的输入。这样的深度架构包括MLP、CNN、DBN、GAN和变分自编码器。下图展示了如何通过卷积自编码器优化重构和CAH损失联合来提高DEC网络的聚类性能。来自编码器层的潜在空间被输入到K-means进行软聚类分配。模糊的基因变异表明存在重构误差：
- en: '![](img/2d1aa2d3-1e6d-407c-9349-98e368b48591.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d1aa2d3-1e6d-407c-9349-98e368b48591.png)'
- en: 'DL based clustering (source: Karim et al., Recurrent Deep Embedding Networks
    for Genotype Clustering and Ethnicity Prediction, arXiv:1805.12218)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DL的聚类（来源：Karim等人，《用于基因型聚类和族群预测的递归深度嵌入网络》，arXiv:1805.12218）
- en: In summary, in these approaches, there are three important steps involved—extracting
    cluster-friendly deep features using deep architectures, combining clustering
    and non-clustering losses, and, finally, network updates to optimize clustering
    and non-clustering losses jointly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在这些方法中，涉及三个重要步骤——使用深度架构提取聚类友好的深度特征、结合聚类和非聚类损失，最后，通过网络更新联合优化聚类和非聚类损失。
- en: DL frameworks and cloud platforms for IoT
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于物联网的深度学习框架和云平台
- en: There are several popular DL frameworks. Each of them comes with some pros and
    cons. Some of them are desktop-based, and some of them are cloud-based platforms,
    where you can deploy/run your DL applications. However, most of the libraries
    that are released under an open license help when people are using graphics processors,
    which can ultimately help in speeding up the learning process. Such frameworks
    and libraries include TensorFlow, PyTorch, Keras, Deeplearning4j, H2O, and the
    **Microsoft Cognitive Toolkit** (**CNTK**). Even a few years back, other implementations,
    including Theano, Caffee, and Neon, were used widely. However, these are now obsolete.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个流行的深度学习框架，每个框架都有其优缺点。其中一些是桌面应用程序，另一些则是云平台，可以用来部署或运行你的深度学习应用程序。然而，大多数开源许可下发布的库对于使用图形处理器的用户有所帮助，这最终可以加速学习过程。这样的框架和库包括
    TensorFlow、PyTorch、Keras、Deeplearning4j、H2O 和 **微软认知工具包** (**CNTK**)。即便是在几年前，其他实现方式，如
    Theano、Caffee 和 Neon，也曾被广泛使用，但现在它们已经过时。
- en: '**Deeplearning4j** (**DL4J**) is one of the first commercial-grade, open source,
    distributed DL libraries that was built for Java and Scala. This also provides
    integrated support for Hadoop and Spark. DL4J is built for use in business environments
    on distributed GPUs and CPUs. DL4J aims to be cutting-edge and *Plug and Play*,
    with more convention than configuration, which allows for fast prototyping for
    non-researchers. Its numerous libraries can be integrated with DL4J and will make
    your JVM experience easier, regardless of whether you are developing your ML application
    in Java or Scala. Similar to NumPy for JVM, ND4J comes up with basic operations
    of linear algebra (matrix creation, addition, and multiplication). However, ND4S
    is a scientific computing library for'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**Deeplearning4j** (**DL4J**) 是首批商业级开源分布式深度学习库之一，专为 Java 和 Scala 构建。它还提供对 Hadoop
    和 Spark 的集成支持。DL4J 是为在分布式 GPU 和 CPU 上的商业环境中使用而构建的，旨在成为前沿的且*即插即用*，比配置更注重约定，这使得非研究人员能够快速原型开发。其众多的库可以与
    DL4J 集成，使得无论你是在 Java 还是 Scala 中开发 ML 应用，都能简化 JVM 的使用体验。类似于 JVM 上的 NumPy，ND4J 提供了线性代数的基本操作（矩阵创建、加法和乘法）。然而，ND4S
    是一个面向 JVM 的科学计算库。'
- en: 'linear algebra and matrix manipulation. It also provides n-dimensional arrays
    for JVM-based languages. The following diagram shows last year''s Google Trends,
    illustrating how popular TensorFlow is:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数和矩阵操作。它还为基于 JVM 的语言提供了 n 维数组。以下图表显示了去年 Google Trends，展示了 TensorFlow 的受欢迎程度：
- en: '![](img/be613e47-6b26-477a-a6ca-d92333a53d42.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be613e47-6b26-477a-a6ca-d92333a53d42.png)'
- en: As well as these frameworks, Chainer is a powerful, flexible, and intuitive
    DL framework, which supports CUDA computation. It only requires a few lines of
    code to leverage a GPU. It also runs on multiple GPUs with little effort. Most
    importantly, Chainer supports various network architectures, including feed-forward
    nets, convnets, recurrent nets, and recursive nets. It also supports per-batch
    architectures. One more interesting feature in Chainer is that it supports forward
    computation, by which any control flow statements of Python can be included without
    lacking the ability of backpropagation. It makes code intuitive and easy to debug.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些框架，Chainer 也是一个强大、灵活且直观的深度学习框架，支持 CUDA 计算。它只需要几行代码就能利用 GPU，还能在多个 GPU 上轻松运行。最重要的是，Chainer
    支持多种网络架构，包括前馈网络、卷积网络、循环网络和递归网络。它还支持每批次架构。Chainer 另一个有趣的特点是，它支持前向计算，因此 Python 中的任何控制流语句都可以包含其中，并且不会失去反向传播的能力。这使得代码更加直观，且易于调试。
- en: The DL framework power scores 2018 also shows that TensorFlow, Keras, and PyTorch
    are far ahead of other frameworks (see [https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a](https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a)).
    Scores were calculated based on usage, popularity, and interest in DL frameworks
    through the following sources. Apart from the preceding libraries, there are some
    recent initiatives for DL in the cloud. The idea is to bring DL capability to
    big data with billions of data points and high-dimensional data. For example,
    **Amazon Web Services** (**AWS**), Microsoft Azure, Google Cloud Platform, and
    **NVIDIA GPU Cloud** (**NGC**) all offer machine and DL services that are native
    to their public clouds.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年深度学习框架的实力评分也显示，TensorFlow、Keras和PyTorch远远领先于其他框架（参见[https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a](https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a)）。评分是根据以下来源的深度学习框架的使用情况、流行程度和兴趣计算的。除了前述的库，还有一些针对云中的深度学习的最新倡议。其理念是将深度学习能力引入具有数十亿数据点和高维数据的大数据。例如，**Amazon
    Web Services**（**AWS**）、Microsoft Azure、Google Cloud Platform和**NVIDIA GPU Cloud**（**NGC**）都提供原生于其公共云的机器学习和深度学习服务。
- en: 'In October 2017, AWS released **Deep Learning AMIs** (**DLAMIs**) for **Amazon
    Elastic Compute Cloud** (**Amazon EC2**) P3 instances. These AMIs come preinstalled
    with DL frameworks, such as TensorFlow, Gluon, and Apache MXNet, which are optimized
    for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances. The DL service
    currently offers three types of AMIs: Conda AMI, Base AMI, and AMI with source
    code.The CNTK is Azure''s open source DL service. Similar to the AWS offering,
    it focuses on tools that can help developers build and deploy DL applications.
    Azure also provides a model gallery that includes resources, such as code samples,
    to help enterprises get started with the service.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年10月，AWS发布了**深度学习AMI**（**DLAMIs**）用于**Amazon弹性计算云**（**Amazon EC2**）P3实例。这些AMI预装了深度学习框架，如TensorFlow、Gluon和Apache
    MXNet，这些框架经过优化，适用于Amazon EC2 P3实例中的NVIDIA Volta V100 GPU。目前，DL服务提供三种类型的AMI：Conda
    AMI、基础AMI和带源代码的AMI。CNTK是Azure的开源深度学习服务。与AWS的产品类似，它侧重于可以帮助开发者构建和部署深度学习应用的工具。Azure还提供了一个模型库，其中包含帮助企业入门的资源，如代码示例等。
- en: On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated
    containers (see [https://www. nvidia. com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)).
    The NGC features containerized DL frameworks, such as TensorFlow, PyTorch, MXNet,
    and more that are tuned, tested, and certified by NVIDIA to run on the latest
    NVIDIA GPUs on participating cloud-service providers. Nevertheless, there are
    also third-party services available through their respective marketplaces.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，NGC通过GPU加速容器赋能AI科学家和研究人员（参见[https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)）。NGC包含了诸如TensorFlow、PyTorch、MXNet等容器化的深度学习框架，这些框架经过NVIDIA的调优、测试和认证，可以在参与的云服务提供商的最新NVIDIA
    GPU上运行。然而，第三方服务也可以通过各自的市场提供。
- en: '**When it comes to cloud-based IoT system-development markets, currently it
    forks into three obvious routes:** off-the-shelf platforms (for example, AWS IoT
    Core, Azure IoT Suite, and Google Cloud IoT Core), which trade off vendor lock-in
    and higher-end volume pricing against cost-effective scalability and shorter lead
    times; reasonably well-established MQTT configurations over the Linux stack (example:
    Eclipse Mosquitto); and the more exotic emerging protocols and products (for example,
    Nabto''s P2P protocol) that are developing enough uptake, interest, and community
    investment to stake a claim for strong market presence in the future.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**在基于云的物联网系统开发市场方面，目前它分为三条明显的路线：** 现成平台（例如，AWS IoT Core、Azure IoT Suite和Google
    Cloud IoT Core），这些平台权衡了供应商锁定和更高的批量定价与具成本效益的可扩展性和较短的交付时间；基于Linux堆栈的相对成熟的MQTT配置（示例：Eclipse
    Mosquitto）；以及那些较为新颖的、正在开发并获得足够关注、兴趣和社区投资的协议和产品（例如，Nabto的P2P协议），这些协议和产品将来有望在市场中占据强有力的地位。'
- en: As a DL framework, Chainer Neural Network is a great choice for all devices
    powered by Intel Atom, NVIDIA Jetson TX2, and Raspberry Pi. Therefore, using Chainer,
    we don't need to build and configure the ML framework for our devices from scratch.
    It provides prebuilt packages for three popular ML frameworks, including TensorFlow,
    Apache MXNet, and Chainer. Chainer works in a similar fashion, which depends on
    a library on the Greengrass and a set of model files generated using Amazon SageMaker
    and/or stored directly in an Amazon S3 bucket. From Amazon SageMaker or Amazon
    S3, the ML models can be deployed to AWS Greengrass to be used as a local resource
    for ML inference. Conceptually, AWS IoT Core functions as the managing plane for
    deploying ML inference to the edge.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个深度学习框架，Chainer 神经网络是适用于所有由 Intel Atom、NVIDIA Jetson TX2 和 Raspberry Pi 驱动的设备的绝佳选择。因此，使用
    Chainer 时，我们无需从零开始为设备构建和配置机器学习框架。它提供了三个流行机器学习框架的预构建包，包括 TensorFlow、Apache MXNet
    和 Chainer。Chainer 的工作原理类似，它依赖于 Greengrass 库以及通过 Amazon SageMaker 生成并/或直接存储在 Amazon
    S3 桶中的一组模型文件。从 Amazon SageMaker 或 Amazon S3 中，机器学习模型可以部署到 AWS Greengrass，作为机器学习推理的本地资源使用。从概念上讲，AWS
    IoT Core 充当管理平面，将机器学习推理部署到边缘。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a number of fundamental DL themes. We started
    our journey with a basic, but comprehensive, introduction to ML. Then, we gradually
    moved on to DL and different neural architectures. We then had a brief overview
    of the most important DL frameworks that can be utilized to develop DL-based applications
    for IoT-enabled devices.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些基础的深度学习主题。我们从基本的但全面的机器学习介绍开始了我们的旅程。然后，我们逐步进入了深度学习和不同的神经网络架构。接着，我们简要概述了可以用于开发基于深度学习的物联网设备应用程序的最重要的深度学习框架。
- en: IoT applications, such as smart home, smart city, and smart healthcare, heavily
    rely on video or image data processing for decision making. In the next chapter,
    we will cover DL-based image processing for IoT applications, including image
    recognition, classification, and object detection. Additionally, we will cover
    hands-on video data processing in IoT applications.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网应用程序，如智能家居、智能城市和智能医疗，严重依赖于视频或图像数据处理来进行决策。在下一章中，我们将介绍基于深度学习的物联网应用图像处理，包括图像识别、分类和目标检测。此外，我们还将介绍物联网应用中的视频数据处理实践。
