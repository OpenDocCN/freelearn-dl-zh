- en: Deep Networks for Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度网络用于文本分类
- en: Text data belongs to the unstructured category of data. When developing deep
    network models, we need to complete additional preprocessing steps due to the
    unique nature of such data. In this chapter, you will learn about the steps you'll
    need to follow to develop text classification models using deep neural networks.
    This process will be illustrated with easy– to– follow examples. Text data, such
    as customer comments, product reviews, and movie reviews, plays an important role
    in businesses, and text classification is an important deep learning problem.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据属于非结构化数据类别。在开发深度网络模型时，由于这类数据的独特性，我们需要完成额外的预处理步骤。在本章中，你将了解开发文本分类模型所需遵循的步骤，并通过易于理解的示例进行说明。文本数据，如客户评论、产品评价和电影评论，在商业中扮演着重要角色，而文本分类是一个重要的深度学习问题。
- en: 'In this chapter, we will discuss two text datasets, learn how to prepare text
    data when developing deep network classification models, look at IMDb movie review
    data, develop a deep network architecture, fit and evaluate the model, and discuss
    some tips and best practices. More specifically, in this chapter, we will cover
    the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论两个文本数据集，学习在开发深度网络分类模型时如何准备文本数据，查看IMDb电影评论数据，开发深度网络架构，拟合并评估模型，并讨论一些技巧和最佳实践。具体来说，本章将涵盖以下主题：
- en: Text datasets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据集
- en: Preparing the data for model building
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: Developing deep neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发深度神经网络
- en: Model evaluation and prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Performance optimization tips and best practices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: Text datasets
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据集
- en: 'Text data can be used when we want to practice developing deep network models.
    Such data can be obtained from several publicly available sources. We will go
    over two such resources in this section:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据可以用于我们在实践中开发深度网络模型时。此类数据可以从几个公开可用的来源获取。在本节中，我们将介绍两个这样的资源：
- en: The UCI machine learning repository
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCI机器学习数据集
- en: Text data within Keras
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras中的文本数据
- en: The UCI machine learning repository
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UCI机器学习数据集
- en: The following link provides a variety of datasets that contain text sentences
    that have been extracted from reviews of products (from [amazon.com](https://www.amazon.com/)),
    reviews of movies (from [IMDB.com](https://www.imdb.com/)), and reviews of restaurants
    (from [yelp.com](https://www.yelp.com/)): [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了多种数据集，这些数据集包含从产品评论（来自[amazon.com](https://www.amazon.com/)）、电影评论（来自[IMDB.com](https://www.imdb.com/)）和餐厅评论（来自[yelp.com](https://www.yelp.com/)）中提取的文本句子：[https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)。
- en: Each sentence is labeled in terms of the sentiment that was expressed in the
    reviews. This sentiment is either positive or negative. For each website, there
    are 500 positive and 500 negative sentences, which means there are 3,000 labeled
    sentences in total. This data can be used to develop a sentiment classification
    deep networking model that can help us automatically classify a customer review
    as either positive or negative.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个句子根据评论中表达的情感进行标注。情感可以是正面的或负面的。每个网站上都有500个正面句子和500个负面句子，总共有3000个标注句子。这些数据可以用来开发一个情感分类深度网络模型，帮助我们自动将客户评论分类为正面或负面。
- en: 'The following are some examples of negative reviews from IMDb that have been
    labeled as 0:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些来自IMDb的负面评论示例，这些评论被标注为0：
- en: A very, very, very slow-moving, aimless movie about a distressed, drifting young
    man
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一部非常非常非常慢节奏、没有方向的电影，讲述了一个迷茫的年轻人
- en: Not sure who was more lost—the flat characters or the audience, nearly half
    of whom walked out
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定是谁更迷茫——是平淡无奇的角色还是观众，几乎一半的观众走了。
- en: Attempting artiness with black and white and clever camera angles, the movie
    disappointed—became even more ridiculous—as the acting was poor and the plot and
    lines almost non-existent
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试用黑白色调和巧妙的镜头角度表现艺术性，但电影令人失望——随着演技糟糕、情节和台词几乎没有，变得更加荒谬。
- en: Very little music or anything to speak of
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎没有音乐或任何值得一提的内容
- en: 'The following are some examples of positive reviews from IMDb that have been
    labeled as 1:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些来自IMDb的正面评论示例，这些评论被标注为1：
- en: The best scene in the movie was when Gerardo was trying to find a song that
    kept running through his head
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影中最精彩的场景是Gerardo试图找到一首一直在他脑海中回荡的歌曲。
- en: Saw the movie today and thought it was a good effort, good messages for kids
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天看了这部电影，觉得是个不错的努力，给孩子们传递了很好的信息
- en: Loved the casting of Jimmy Buffet as the science teacher
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 喜欢吉米·巴菲特饰演科学老师的选角
- en: And those baby owls were adorable
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那些小猫头鹰真是太可爱了
- en: The movie showed a lot of Florida at its best, made it look very appealing
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这部电影展示了佛罗里达州最美的一面，让它看起来非常吸引人
- en: 'The following are some examples of negative reviews from Amazon that are labeled
    as 0:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些亚马逊负面评论的例子，标签为0：
- en: So there is no way for me to plug it in here in the US unless I go by a converter
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以，除非我购买转换器，否则我在美国根本无法插上它
- en: Tied to charger for conversations lasting more than 45 minutes. MAJOR PROBLEMS!!
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 45分钟以上的对话必须插着充电器。重大问题！！
- en: I have to jiggle the plug to get it to line up right to get decent volume
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我必须晃动插头才能正确对接，以获得合适的音量
- en: If you have several dozen or several hundred contacts, then imagine the fun
    of sending each of them one by one
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有几十个或几百个联系人，想象一下逐个发送消息的乐趣
- en: I advise EVERYONE DO NOT BE FOOLED!
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我建议大家千万不要上当！
- en: 'The following are some examples of positive reviews from Amazon that are labeled
    as 1:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些亚马逊正面评论的例子，标签为1：
- en: Good case, Excellent value
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的案例，极好的价值
- en: Great for the jawbone
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常适合下巴骨
- en: The mic is great
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 麦克风很棒
- en: If you are Razr owner...you must have this!
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是Razr手机的用户...你一定得拥有这个！
- en: And the sound quality is great
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而且音质非常好
- en: Text data within Keras
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的文本数据
- en: 'There are two text datasets available within Keras, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中有两个文本数据集，如下所示：
- en: The **Internet Movie Database** (**IMDb**), which contains movie review sentiment
    classification
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互联网电影数据库** (**IMDb**)，包含电影评论情感分类数据'
- en: Reuters Newswire's topics classification data
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路透新闻社的主题分类数据
- en: The IMDb review data contains 25,000 reviews that have been classified as containing
    positive or negative sentiments. This data has already been preprocessed, with
    each review encoded as a sequence of integers. Reuters Newswire's topics classification
    data contains 11,228 newswires, and these have also been preprocessed, with each
    encoded as a sequence of integers. The newswires have been classified into 46
    groups or topics, such as livestock, gold, and housing, jobs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb的评论数据包含了25,000条已经被分类为正面或负面情感的评论。该数据已经预处理，每条评论都被编码为整数序列。路透新闻社的主题分类数据包含了11,228条新闻，这些新闻也经过了预处理，每条新闻都被编码为整数序列。这些新闻被分类为46个类别或主题，例如牲畜、黄金、住房、就业等。
- en: 'The following is an example of a positive movie review from the IMDb data from
    Keras:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自IMDb数据中Keras的一条正面电影评论示例：
- en: '*"lavish production values and solid performances in this straightforward adaption
    of jane ? satirical classic about the marriage game within and between the classes
    in ? 18th century england northam and paltrow are a ? mixture as friends who must
    pass through ? and lies to discover that they love each other good humor is a
    ? virtue which goes a long way towards explaining the ? of the aged source material
    which has been toned down a bit in its harsh ? i liked the look of the film and
    how shots were set up and i thought it didn''t rely too much on ? of head shots
    like most other films of the 80s and 90s do very good results."*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*“在这部简洁的改编作品中，奢华的制作价值和扎实的表演令人印象深刻，这部作品改编自简·奥斯汀的讽刺经典，讲述了18世纪英格兰阶级之间和内部的婚姻游戏。诺森和帕特罗饰演的朋友角色需要经历种种谎言，最终发现他们彼此相爱。幽默感是一种?美德，能够很好地解释那部经典老旧材料的魅力，虽然原作中的一些严苛部分被稍微调整过了。我喜欢电影的视觉效果和镜头设置，并且觉得它不像80年代和90年代的其他电影那样过于依赖大量的特写镜头，效果相当好。”*'
- en: 'The following is an example of a negative movie review from the IMDb data from
    Keras:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自IMDb数据中Keras的一条负面电影评论示例：
- en: '*"worst mistake of my life br br i picked this movie up at target for 5 because
    i figured hey it''s sandler i can get some cheap laughs i was wrong completely
    wrong mid way through the film all three of my friends were asleep and i was still
    suffering worst plot worst script worst movie i have ever seen i wanted to hit
    my head up against a wall for an hour then i''d stop and you know why because
    it felt damn good upon bashing my head in i stuck that damn movie in the ? and
    watched it burn and that felt better than anything else i''ve ever done it took
    american psycho army of darkness and kill bill just to get over that crap i hate
    you sandler for actually going through with this and ruining a whole day of my
    life."*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*"我一生中最糟糕的错误 br br 我在Target买了这部电影，价格是5美元，因为我想着‘嘿，这是Sandler的片子，我可以获得一些廉价的笑料’，结果我错了，完全错了，电影放到一半，我的三个朋友都睡着了，而我还在痛苦中，最糟糕的情节，最糟糕的剧本，最糟糕的电影，我看过的最烂的电影，我差点想把头撞到墙上一个小时，然后停下来，你知道为什么吗？因为撞头的感觉太爽了，我把那部该死的电影丢进垃圾桶，看它烧掉，那感觉比我做过的任何事情都好，直到看完《美国精神病人》、《黑暗军团》和《杀死比尔》才算过得了这段噩梦，我恨你，Sandler，居然还做出这种决定，毁掉了我一天的生活。"*'
- en: Preparing the data for model building
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: 'The steps we need to follow in order to prepare the data for model building
    are as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备数据进行模型构建，我们需要遵循以下步骤：
- en: Tokenization
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记化
- en: Converting text into integers
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本转换为整数
- en: Padding and truncation
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充和截断
- en: 'To illustrate the steps involved in data preparation, we will make use of a
    very small text dataset involving five tweets related to when the Apple iPhone
    X released in September 2017\. We will use this small dataset to understand the
    steps that are involved in data preparation and then we will switch to a larger
    IMDb dataset in order to build a deep network classification model. The following
    are the five tweets that we are going to store in `t1` to `t5`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明数据准备过程中涉及的步骤，我们将使用一个非常小的文本数据集，该数据集包含五条与2017年9月发布的苹果iPhone X相关的推文。我们将使用这个小数据集来了解数据准备过程中涉及的步骤，然后切换到更大的IMDb数据集，以构建深度网络分类模型。以下是我们将存储在`t1`到`t5`中的五条推文：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding tweets include text that's in both lowercase and uppercase, punctuation,
    numbers, and special characters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的推文包含了大小写字母、标点符号、数字和特殊字符。
- en: Tokenization
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: 'Each word or number in the tweet is a token, and the process of splitting tweets
    into tokens is called **tokenization**. The code that''s used to carry out tokenization
    is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 推文中的每个单词或数字都是一个标记，拆分推文为标记的过程叫做**标记化**。用于执行标记化的代码如下：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From the preceding code, we can see the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到以下内容：
- en: We started by saving five tweets in `tweets`.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先将五条推文保存到`tweets`中。
- en: For the tokenization process, we specified `num_words` as `10` to indicate we
    want to use 10 of the most frequent words and ignore any others.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标记化过程，我们指定了`num_words`为`10`，表示我们希望使用10个最频繁的词并忽略其他词。
- en: Although we specified that we will have `10` frequent words, the maximum value
    of integers that will be used is actually going to be 10 - 1 = 9.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们指定了要使用`10`个最频繁的词，但实际上将使用的整数的最大值是10 - 1 = 9。
- en: We used `fit_text_tokenizer`, which automatically converts text into lowercase
    and removes any punctuation from the tweets.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`fit_text_tokenizer`，它自动将文本转换为小写并去除推文中的标点符号。
- en: We observed that the top three most frequent words in these five tweets are
    `the`, `aapl`, and `in`.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们观察到，这五条推文中最频繁出现的前三个词是`the`、`aapl`和`in`。
- en: Note that words that have a high frequency may or may not be important for text
    classification.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，频率较高的词语可能对文本分类有用，也可能没有用。
- en: Converting text into sequences of integers
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转换为整数序列
- en: 'The following code is used to convert text into sequences of integers. The
    output is also provided:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于将文本转换为整数序列。也提供了输出结果：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the preceding code, we can see the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到以下内容：
- en: We have used `texts_to_sequences` to convert tweets into sequences of integers.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`texts_to_sequences`将推文转换为整数序列。
- en: Since we've chosen the most frequent words for tokens to be `10`, the integers
    within each sequence of integers have a maximum value of 9.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们选择了将最频繁的词作为标记数量为`10`，因此每个整数序列中的整数的最大值为9。
- en: For each tweet, the number of integers in the sequence is less than how many
    words there are due to only the most frequent words being used.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每条推文，序列中的整数数量少于词语的数量，因为只使用了最频繁的词。
- en: The sequences of integers have different lengths, ranging from 2 to 9.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整数序列的长度不同，范围从2到9不等。
- en: For the purpose of developing a classification model, all of the sequences need
    to be the same length. This is achieved by performing padding or truncation.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了开发分类模型，所有序列需要具有相同的长度。这是通过执行填充或截断来实现的。
- en: Padding and truncation
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充和截断
- en: 'The code for making all the sequences of integers equal is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使所有整数序列相等的代码如下：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the preceding code, we can see the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到以下几点：
- en: We have used `pad_sequences` so that all of the sequences of integers are equal
    in length.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`pad_sequences`，使所有整数序列的长度一致。
- en: When we specify the maximum length of all the sequences (using `maxlen`) to
    be 5, this will truncate sequences that are longer than 5 and add zeros to sequences
    that are shorter than 5.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将所有序列的最大长度（使用`maxlen`）设置为5时，长度超过5的序列将被截断，长度不足5的序列将会填充零。
- en: Note that the default setting for padding here is "pre". This means that when
    a sequence is longer than 5, truncation will effect integers at the beginning
    of the sequence. We can observe this for the first sequence in the preceding output,
    where 4, 5, 6, and 2 have been removed.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，这里的填充默认设置为“pre”。这意味着当序列长度超过5时，截断将作用于序列的开头部分。我们可以从前面的输出中看到这一点，序列的前4、5、6和2被去除。
- en: Similarly, for the third sequence, which has a length of two, three zeros have
    been added to the beginning of the sequence.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，对于第三个序列，它的长度为2，已在序列的开头添加了三个零。
- en: 'There may be situations where you may prefer to truncate or add zeroes to the
    end of the sequences of integers. The code to achieve this is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有些情况，你希望在整数序列的末尾截断或添加零。实现这一点的代码如下：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code, we have specified the padding as `post`. The impact of
    this type of padding can be seen in the output, where zeros have been added to
    the end of sequence 3, which adds up to less than 5.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将填充方式指定为`post`。这种填充方式的影响可以在输出中看到，序列3的末尾添加了零，长度不足5。
- en: Developing a tweet sentiment classification model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发推文情感分类模型
- en: 'To develop a tweet sentiment classification model, we need labels for each
    tweet. However, getting labels that accurately reflect tweet sentiment is challenging.
    Let''s take a look at some existing lexicons for sentiment classification and
    see why it isn''t easy to get appropriate labels. With just five tweets, it isn''t
    possible to develop a sentiment classification model. However, the idea here is
    to look at the process of arriving at an appropriate label for each tweet. This
    will help us appreciate the challenges involved in obtaining accurate labels.
    To automatically extract sentiment scores for each tweet, we will make use of
    the `syuzhet` package. We will also make use of commonly used lexicons for this
    purpose. The **National Research Council** (**NRC**) lexicon helps capture various
    emotions based on certain words. We will use the following code to obtain a sentiment
    score for the five tweets:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个推文情感分类模型，我们需要为每条推文提供标签。然而，获取准确反映推文情感的标签是具有挑战性的。让我们看一下现有的情感分类词典，并探讨为什么很难获得合适的标签。仅凭五条推文，是无法开发出一个情感分类模型的。然而，这里的重点是观察为每条推文得出合适标签的过程。这将帮助我们理解获取准确标签所面临的挑战。为了自动提取每条推文的情感分数，我们将使用`syuzhet`包。同时，我们还将使用一些常用的情感词典。**国家研究委员会**（**NRC**）词典帮助根据特定单词捕捉不同的情感。我们将使用以下代码来获取这五条推文的情感分数：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first tweet results in a score of 1 for both anger and fear. Although it
    contains the word `'bearish'`, if we were to read this tweet, we would determine
    that it's actually positive.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条推文对于愤怒和恐惧的评分都是1。尽管其中包含单词`'bearish'`，但如果我们阅读这条推文，会发现它实际上是积极的。
- en: 'Let''s look at the following code, which contains sentiment scores for the
    words `''bearish''`, `''death''`, and `''animated''`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下代码，其中包含单词`'bearish'`、`'death'`和`'animated'`的情感分数：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From the preceding code, we can determine the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以得出以下结论：
- en: The overall score for the first tweet is based on the word italics, and nothing
    else.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一条推文的整体分数基于单词的斜体形式，除此之外没有其他因素。
- en: The third tweet has a score of 1 for each category except trust.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三条推文在每个类别的评分中，除了信任外，其他的评分都是1。
- en: From reading the tweet, it is obvious to us that the person writing this tweet
    actually feels that animated emojis will be positive for Apple and will be negative
    for Snapchat.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从阅读这条推文，我们很明显可以看出，写这条推文的人实际上认为动画表情符号对苹果是正面的，对Snapchat则是负面的。
- en: 'The sentiment scores are based on two words in this tweet: death and animated.
    They fail to capture the real sentiment that''s expressed in the third tweet,
    which is very positive for Apple.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分数基于这条推文中的两个词：death和animated。它们未能捕捉到第三条推文中表达的真正情感，这条推文对苹果非常积极。
- en: 'When we manually label each of the five tweets with a negative sentiment, which
    is represented by 0, and a positive sentiment, which is represented by 1, we are
    likely to arrive at 1, 0, 1, 1, and 1 for our scores. Let''s use the following code
    to arrive at these sentiment scores by using the `syuzhet`, `bing`, and `afinn`
    lexicons:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们手动为每条推文标注负面情感（由0表示）和正面情感（由1表示）时，我们很可能得到1、0、1、1和1作为我们的分数。让我们使用以下代码，通过使用`syuzhet`、`bing`和`afinn`词典来获得这些情感分数：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Looking at results from the `syuzhet`, `bing`, and `afinn` lexicons, we can
    observe the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从`syuzhet`、`bing`和`afinn`词典的结果来看，我们可以观察到以下几点：
- en: The results vary significantly from the actual sentiments contained in the tweets.
    Thus, trying to automatically label a tweet with an appropriate sentiment score
    is difficult.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果与推文中包含的实际情感有显著差异。因此，尝试自动标注推文并分配合适的情感分数是困难的。
- en: We saw that automatically labeling text sequences is a challenging problem.
    However, one solution is to label a very large number of text sequences, such
    as tweets, manually and then use that to develop a sentiment classification model.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们看到自动标注文本序列是一个具有挑战性的问题。然而，一个解决方案是手动标注大量文本序列，比如推文，然后使用这些标注数据来开发情感分类模型。
- en: In addition, it is important to note that such a sentiment classification model
    will only be helpful for the specific types of text data that were used to develop
    the model.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，值得注意的是，情感分类模型只对用于开发该模型的特定类型的文本数据有帮助。
- en: It isn't possible to use the same model for different text sentiment classification
    applications.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可能为不同的文本情感分类应用使用相同的模型。
- en: Developing deep neural networks
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发深度神经网络
- en: 'Although we won''t be developing a classification model based on just five
    tweets, let''s look at the code for our model''s architecture:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会仅基于五条推文开发分类模型，但我们来看一下模型架构的代码：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码，我们可以观察到以下几点：
- en: We initialized the model using `keras_model_sequential()`.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`keras_model_sequential()`初始化了模型。
- en: We specified the input dimension as 10, which is the number of most frequent
    words.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将输入维度指定为10，这是最常见的词汇数。
- en: The output dimension of 8 leads to the number of parameters being 10 x 8 = 80.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出维度为8，这导致参数数量为10 x 8 = 80。
- en: The input length is the length of the sequence of integers.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入长度是整数序列的长度。
- en: We can get the weights for these 80 parameters using `model$get_weights()`.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`model$get_weights()`获取这80个参数的权重。
- en: Note that these weights will change every time the model is initialized.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些权重将在每次初始化模型时发生变化。
- en: Obtaining IMDb movie review data
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取IMDb电影评论数据
- en: 'Now, we will make use of IMDb movie review data, where the sentiment for each
    review has already been labeled as positive or negative. The code for accessing
    the IMDb movie review data from Keras is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用IMDb电影评论数据，其中每条评论的情感已经被标注为正面或负面。以下是从Keras访问IMDb电影评论数据的代码：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码，我们可以观察到以下几点：
- en: We have used `train_x` and `train_y` to store the data in sequences of integers
    and labels representing positive or negative sentiment, respectively.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`train_x`和`train_y`来存储按整数序列和标签表示的正面或负面情感的数据。
- en: We used a similar convention for the test data, too.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对测试数据也使用了类似的约定。
- en: Both the training and test data consist of 25,000 reviews each.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和测试数据各自包含25,000条评论。
- en: The summary of the sequence length shows that the minimum length for the movie
    reviews based on the most frequent words is 11 and that the maximum sequence length
    is `2494`.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度的总结显示，基于最常见词汇的电影评论最小长度为11，最大序列长度为`2494`。
- en: The median sequence length is `178`.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数序列长度为`178`。
- en: The median value is less than the mean, which suggests that this data will be
    skewed to the right and will have a longer tail on the right-hand side.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数值小于均值，这表明该数据将呈现右偏，并且右侧尾部较长。
- en: 'The histogram for the sequence length of the training data can be plotted as
    follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的序列长度的直方图可以如下绘制：
- en: '![](img/cc0f5389-0333-4e2d-9f2f-845016ae562f.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc0f5389-0333-4e2d-9f2f-845016ae562f.png)'
- en: The preceding histogram for the length of the sequence of integers shows a right-skewed
    pattern. Most of the sequences have less than 500 integers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的整数序列长度的直方图显示了右偏模式。大多数序列的整数数量少于500。
- en: 'Next, we will make the length of the sequence of integers equal using the following
    code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码使整数序列的长度相等：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到以下内容：
- en: We have used `maxlen` of 100 to standardize the length of each sequence to 100
    integers.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`maxlen`为100来标准化每个序列的长度为100个整数。
- en: Sequences longer than 100 will have any additional integers truncated or removed,
    and sequences shorter than 100 will have zeros added to artificially increase
    the length of the sequence so that it reaches 100\. We do this for both the train
    and test sequences.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长度超过100的序列将会截断或移除任何多余的整数，长度小于100的序列将添加零以人为地增加序列的长度，直到达到100。我们对训练集和测试集的序列都做这样处理。
- en: Now, we are ready to build a classification model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好构建分类模型了。
- en: Building a classification model
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建分类模型
- en: 'For the model architecture and model summary, we will use the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型架构和模型摘要，我们将使用以下代码：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到以下内容：
- en: Here, we've added `layer_flatten()` after `layer_embedding()`.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们在`layer_embedding()`之后添加了`layer_flatten()`。
- en: This is followed by a dense layer with 16 nodes and a `relu` activation function.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是一个包含16个节点的全连接层，并使用`relu`激活函数。
- en: The summary of the model shows that there are `33,633` parameters in total.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的摘要显示，总共有`33,633`个参数。
- en: Now, we can compile the model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编译模型。
- en: Compiling the model
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'We need to use the following code to compile the model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用以下代码来编译模型：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到以下内容：
- en: We have used the `rmsprop` optimizer to compile the model.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经使用了`rmsprop`优化器来编译模型。
- en: For loss, we have used `binary_crossentropy` since the response has two values,
    that is, positive or negative. Metrics will make use of accuracy.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于损失，我们使用了`binary_crossentropy`，因为响应有两个值，即正面或负面。评估指标将使用准确度。
- en: Now, let's start fitting the model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始拟合模型。
- en: Fitting the model
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'We need to use the following code to fit the model:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用以下代码来拟合模型：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As shown in the preceding code, we''re using `train_x` and `train_y` to fit
    the model, as well as `10` epochs and a batch size of `128`. We are using 20%
    of the training data to assess the model''s performance in terms of loss and accuracy
    values. After fitting the model, we obtain a plot for loss and accuracy, as shown
    in the following plot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们使用`train_x`和`train_y`来拟合模型，并使用`10`个epoch和批量大小为`128`。我们使用20%的训练数据来评估模型的性能，包括损失值和准确度。拟合模型后，我们得到如下所示的损失和准确度图：
- en: '![](img/59f959bb-5acb-4af9-ba42-2e9562be05bf.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59f959bb-5acb-4af9-ba42-2e9562be05bf.png)'
- en: 'From the preceding plot, we can observe the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以观察到以下内容：
- en: The plot for loss and accuracy shows divergence between the training and validation
    data after about four epochs.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失和准确度图显示，在大约四个epoch之后，训练数据和验证数据之间出现了发散。
- en: Divergence between the training and validation data is observed for both the
    loss and accuracy values.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们观察到训练数据和验证数据在损失和准确度值上出现了发散。
- en: We won't be using this model since there is clear evidence that there's an overfitting
    problem.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不会使用这个模型，因为有明确的证据表明存在过拟合问题。
- en: 'To overcome this overfitting problem, we need to modify the preceding code
    so that it appears as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服过拟合问题，我们需要修改前面的代码，使其如下所示：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Looking at the preceding code, we can observe the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 看看前面的代码，我们可以观察到以下内容：
- en: We're re-running the model and making only one change; that is, we're increasing
    the batch size to 512
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在重新运行模型并只做了一个改变：即我们将批量大小增加到512。
- en: We keep everything else the same and then fit the model using the training data
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保持其他设置不变，然后使用训练数据拟合模型。
- en: 'After fitting the model, the loss and accuracy values that are stored in `model_2`
    are plotted, as shown in the following plot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型后，存储在`model_2`中的损失和准确度值被绘制出来，如下图所示：
- en: '![](img/f61f34c6-0df2-48cd-9f6d-c345ffca0069.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f61f34c6-0df2-48cd-9f6d-c345ffca0069.png)'
- en: 'From the preceding plot, we can observe the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以观察到以下几点：
- en: The loss and accuracy values show better results this time.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一次，损失和准确度值显示出更好的结果。
- en: The curves for training and validation are closer to each other for both loss
    and accuracy.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证的曲线在损失和准确度上更接近彼此。
- en: In addition, the loss and accuracy values that are based on validation data
    don't show the severe deterioration that we had observed for the previous model,
    where the values for the last three epochs are flat here.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，基于验证数据的损失和准确度值并未出现我们在先前模型中观察到的严重恶化，后者的最后三个周期的值是平坦的。
- en: We were able to overcome the problem of overfitting by making minor changes
    to our code.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过对代码进行一些小的修改，克服了过拟合的问题。
- en: We will use this model for evaluation and prediction.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个模型进行评估和预测。
- en: Model evaluation and prediction
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Now, we will evaluate the model using training and test data to obtain the loss,
    accuracy, and confusion matrices. Our objective is to obtain a model that can
    classify sentiment contained in movie reviews as either positive or negative.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用训练数据和测试数据来评估模型，获取损失、准确度和混淆矩阵。我们的目标是获得一个能够将电影评论中的情感分类为正面或负面的模型。
- en: Evaluation using training data
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练数据进行评估
- en: 'The code to obtain the loss and accuracy values from the training data is as
    follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据的损失和准确度值的代码如下：
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we can see, for training data, the loss and accuracy are `0.375` and `0.834`,
    respectively. To look deeper into the model''s sentiment classification performance,
    we need to develop a confusion matrix. To do so, use the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对于训练数据，损失和准确度分别为`0.375`和`0.834`。为了更深入地了解模型的情感分类性能，我们需要构建一个混淆矩阵。为此，请使用以下代码：
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code, we are predicting that the classes for the training
    data are using the model and comparing the results with the actual sentiment classes
    of the movie reviews. This is summarized in a confusion matrix. We can make the
    following observations about the confusion matrix:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在预测训练数据的类别，并将结果与电影评论的实际情感类别进行比较。这通过混淆矩阵进行了总结。我们可以从混淆矩阵中做出以下观察：
- en: The model correctly predicts the negative sentiments contained in 11,128 movie
    reviews.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型正确预测了11,128条电影评论中的负面情感。
- en: The model correctly predicts the positive sentiments contained in 9,729 movie
    reviews.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型正确预测了9,729条电影评论中的正面情感。
- en: Misclassifying a positive review as a negative review is higher (2,771) than
    misclassifying movie reviews that have a negative sentiment and have been incorrectly
    classified as positive (1,372).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将正面评论误分类为负面评论的数量较高（2,771），而将具有负面情感的电影评论误分类为正面评论的数量较少（1,372）。
- en: Next, we'll repeat this process with the test data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对测试数据重复这一过程。
- en: Evaluation using test data
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用测试数据进行评估
- en: 'The code to obtain the loss and accuracy values from the test data is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 获取测试数据的损失和准确度值的代码如下：
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As we can see, in terms of the test data, the loss and accuracy are `0.443`
    and `0.794`, respectively. These results are slightly inferior to the ones that
    were obtained for the training data. We can predict classes for the `test` data
    using the model and compare them with the actual classes of the movie reviews.
    This can be summarized in a confusion matrix, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在测试数据方面，损失和准确度分别为`0.443`和`0.794`。这些结果略低于训练数据获得的结果。我们可以使用模型对`test`数据进行类别预测，并将其与实际的电影评论类别进行比较。这可以总结为一个混淆矩阵，如下所示：
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From the preceding confusion matrix, we can observe the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以观察到以下几点：
- en: Overall, this model seems to be more accurate in correctly predicting negative
    movie reviews (10,586) compared to positive movie reviews (9,253).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，该模型在正确预测负面电影评论（10,586条）方面似乎比预测正面电影评论（9,253条）更为准确。
- en: This pattern is consistent with the results that were obtained with the training
    data.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模式与使用训练数据得到的结果一致。
- en: In addition, although 79% accuracy for test data is decent, there is still scope
    for improving the model's sentiment classification performance.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，尽管测试数据的 79% 准确率已经不错，但仍有改进模型情感分类性能的空间。
- en: In the next section, we will explore performance optimization tips and best
    practices.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨性能优化的建议和最佳实践。
- en: Performance optimization tips and best practices
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化的建议和最佳实践
- en: Now that we've obtained the test data's movie review classification accuracy,
    that is, 79%, we can work on improving this accuracy even further. Arriving at
    such an improvement may involve experimenting with the parameters in the model's
    architecture, the parameters that were used when we compiled the model, and/or
    the settings that were used while we were fitting a model. In this section, we
    will carry out an experiment by changing the maximum length of the sequence of
    words and, at the same time, use a different optimizer compared to what we used
    in the previous model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了测试数据的电影评论分类准确率，即 79%，我们可以继续努力提高这一准确率。达到这样的改进可能涉及实验模型架构中的参数、编译模型时使用的参数和/或拟合模型时使用的设置。在本节中，我们将通过改变单词序列的最大长度并同时使用与前一模型不同的优化器来进行实验。
- en: Experimenting with the maximum sequence length and the optimizer
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验最大序列长度和优化器
- en: 'Let''s start by creating `train` and `test` data for the sequence of integers
    representing movie reviews and their labels using the following code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建代表电影评论和它们标签的整数序列的`train`和`test`数据开始，使用以下代码：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code, we''re storing the length of the sequences based on
    the training data in `z`. By doing this, we get a summary of `z`. From here, we
    can obtain numeric summary values such as the minimum, first quartile, median,
    mean, third quartile, and maximum. The median value for the sequence of words
    is 178\. In the previous sections, we used a maximum length of 100 at the time
    of padding the sequences so that they were of equal length. We will increase this
    to 200 in this experiment so that we have a number closer to the median value,
    as shown in the following code:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们正在存储基于训练数据的序列长度在`z`中。通过这样做，我们可以获得`z`的汇总信息。从这里，我们可以获得数值汇总值，如最小值、第一四分位数、中位数、平均数、第三四分位数和最大值。单词序列的中位数值为
    178。在前几节中，我们在填充序列时使用了最大长度为 100，以便使它们的长度相等。在本实验中，我们将其增加到 200，以便得到一个接近中位数值的数字，如以下代码所示：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Another change we''ll make is using the `adamax` optimizer when compiling the
    model. Note that this a variant of the popular `adam` optimizer. We keep everything
    else the same. After training the model, we plot the resulting loss and accuracy,
    as shown in the following plot:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行的另一个更改是在编译模型时使用`adamax`优化器。请注意，这是流行的`adam`优化器的变体。我们保持其他所有内容不变。训练模型后，我们绘制了结果损失和准确率的图表，如下图所示：
- en: '![](img/6cae3c9d-98a4-41e9-8448-6936c2ce052c.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cae3c9d-98a4-41e9-8448-6936c2ce052c.png)'
- en: 'From the preceding plot for loss and accuracy, we can observe the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述损失和准确率的图表中，我们可以观察到以下情况：
- en: The loss and accuracy values for the training and validation data show rapid
    improvements for about four epochs.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证数据的损失和准确率值表明大约在四个时期内有快速改进。
- en: After four epochs, these improvements slow down for the training data.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过四个时期，这些改进在训练数据上放缓了。
- en: For the validation data, the loss and accuracy values become flat for the last
    few epochs.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于验证数据，损失和准确率值在最后几个时期保持不变。
- en: The plot doesn't show any cause for concern regarding overfitting.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该图表未显示任何有关过度拟合的问题。
- en: 'Next, we need to calculate the loss and accuracy based on the test data using
    the following code:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用以下代码基于测试数据计算损失和准确率：
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Looking at the preceding code, we can observe the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 查看上述代码，我们可以观察到以下情况：
- en: The model's loss and accuracy, based on the test data, are `0.391` and `0.825`,
    respectively.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于测试数据，模型的损失和准确率分别为`0.391`和`0.825`。
- en: Both numbers indicate improvements compared to the performance we retrieved
    in the previous section.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个数字都显示了与我们在前一节中获得的性能相比的改进。
- en: 'To look into the model''s sentiment classification performance even further,
    we can use the following code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步研究模型的情感分类性能，我们可以使用以下代码：
- en: '[PRE22]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'From the preceding confusion matrix, which is based on movie reviews of test
    data, we can observe the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 根据基于测试数据电影评论的混淆矩阵，我们可以观察到以下情况：
- en: The correct classifications of negative (9,970) and positive movie reviews (10,647)
    are much closer now.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，负面（9,970条）和正面电影评论（10,647条）的正确分类结果更为接近。
- en: The correct classification of positive movie reviews is slightly better compared
    to the correct classification of negative reviews.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类正面电影评论的效果略好于负面评论。
- en: This model misclassifies a negative movie review as positive at a slightly higher
    rate (2,530) compared to a positive review being misclassified as a negative review
    (1,853).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型将负面电影评论误分类为正面评论的比例略高（2,530条），相比之下，正面评论被误分类为负面评论的比例为1,853条。
- en: Here, experimenting with the maximum sequence length and the type of optimizer
    that's used to compile the model resulted in improved sentiment classification
    performance. You are encouraged to continue experimenting and improve the model's
    sentiment classification performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通过实验最大序列长度和用于编译模型的优化器类型，提升了情感分类的性能。鼓励你继续进行实验，进一步提高模型的情感分类性能。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by developing deep neural networks for text classification.
    Due to the unique characteristics of text data, several extra preprocessing steps
    are required before a deep neural network sentiment classification model can be
    developed. We used a small sample of five tweets to go over the preprocessing
    steps, including tokenization, converting text data into a sequence of integers,
    and padding/truncation to arrive at the same sequence length. We also highlighted
    that automatically labeling text sequences with the appropriate sentiment is a
    challenging problem and general lexicons may be unable to provide useful results.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们从开发用于文本分类的深度神经网络开始。由于文本数据的独特性，开发深度神经网络情感分类模型之前需要进行一些额外的预处理步骤。我们使用了五条推文的小样本来演示这些预处理步骤，包括分词、将文本数据转换为整数序列，以及填充/截断以确保序列长度一致。我们还强调了自动为文本序列标注正确情感是一个具有挑战性的问题，并且通用的词典可能无法提供有效的结果。
- en: To develop a deep network sentiment classification model, we switched to a larger
    and ready-to-use IMDb movie review dataset that's available as part of Keras.
    To optimize the model's performance, we also experimented with parameters such
    as the maximum sequence length at the time of data preparation, as well as the
    type of optimizer that's used for compiling the model. These experiments yielded
    decent results; however, we will continue to explore this data so that we can
    improve the model's sentiment classification performance on the deep network model
    even further.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发深度网络情感分类模型，我们切换到了一个较大且现成的IMDb电影评论数据集，该数据集作为Keras的一部分提供。为了优化模型的性能，我们还尝试了数据准备时的最大序列长度等参数，以及用于编译模型的优化器类型。这些实验取得了不错的结果；然而，我们将继续探索这组数据，以便进一步提高深度网络模型在情感分类任务中的表现。
- en: In the next chapter, we will make use of the recurrent neural network classification
    model, which is better suited to working with data involving sequences.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将使用递归神经网络分类模型，这种模型更适合处理涉及序列的数据。
