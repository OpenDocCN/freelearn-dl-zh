- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: A Guide to the Gym Toolkit
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gym 工具包指南
- en: OpenAI is an **artificial intelligence** (**AI**) research organization that
    aims to build **artificial general intelligence** (**AGI**). OpenAI provides a
    famous toolkit called Gym for training a reinforcement learning agent.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 是一个 **人工智能** (**AI**) 研究组织，旨在构建 **人工通用智能** (**AGI**)。OpenAI 提供了一个著名的工具包，叫做
    Gym，用于训练强化学习代理。
- en: Let's suppose we need to train our agent to drive a car. We need an environment
    to train the agent. Can we train our agent in the real-world environment to drive
    a car? No, because we have learned that reinforcement learning (RL) is a trial-and-error
    learning process, so while we train our agent, it will make a lot of mistakes
    during learning. For example, let's suppose our agent hits another vehicle, and
    it receives a negative reward. It will then learn that hitting other vehicles
    is not a good action and will try not to perform this action again. But we cannot
    train the RL agent in the real-world environment by hitting other vehicles, right?
    That is why we use simulators and train the RL agent in the simulated environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要训练我们的代理驾驶一辆汽车。我们需要一个环境来训练代理。我们能在现实世界的环境中训练代理驾驶汽车吗？不能，因为我们已经知道强化学习（RL）是一个试错学习过程，所以在训练代理时，它会在学习过程中犯很多错误。例如，假设我们的代理撞到另一辆车，并获得了负奖励。它将学到撞击其他车辆不是一个好的动作，并会尝试避免再次执行这个动作。但我们不能通过让
    RL 代理撞车来训练它在现实环境中驾驶对吧？这就是为什么我们使用模拟器，并在模拟环境中训练 RL 代理的原因。
- en: There are many toolkits that provide a simulated environment for training an
    RL agent. One such popular toolkit is Gym. Gym provides a variety of environments
    for training an RL agent ranging from classic control tasks to Atari game environments.
    We can train our RL agent to learn in these simulated environments using various
    RL algorithms. In this chapter, first, we will install Gym and then we will explore
    various Gym environments. We will also get hands-on with the concepts we have
    learned in the previous chapter by experimenting with the Gym environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具包提供了一个模拟环境，用于训练强化学习（RL）代理。一个流行的工具包是 Gym。Gym 提供了多种环境，用于训练 RL 代理，从经典控制任务到
    Atari 游戏环境应有尽有。我们可以通过各种 RL 算法训练我们的 RL 代理，使其在这些模拟环境中学习。在本章中，首先，我们将安装 Gym，然后我们将探索各种
    Gym 环境。我们还将通过在 Gym 环境中进行实验，实践我们在上一章学到的概念。
- en: Throughout the book, we will use the Gym toolkit for building and evaluating
    reinforcement learning algorithms, so in this chapter, we will make ourselves
    familiar with the Gym toolkit.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用 Gym 工具包来构建和评估强化学习算法，因此在本章中，我们将熟悉 Gym 工具包。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Setting up our machine
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置我们的机器
- en: Installing Anaconda and Gym
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Anaconda 和 Gym
- en: Understanding the Gym environment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Gym 环境
- en: Generating an episode in the Gym environment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Gym 环境中生成一个回合
- en: Exploring more Gym environments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索更多 Gym 环境
- en: Cart-Pole balancing with the random agent
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机代理的倒立摆平衡
- en: An agent playing the Tennis game
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一名代理玩网球游戏
- en: Setting up our machine
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置我们的机器
- en: In this section, we will learn how to install several dependencies that are
    required for running the code used throughout the book. First, we will learn how
    to install Anaconda and then we will explore how to install Gym.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何安装一些运行全书代码所需的依赖项。首先，我们将学习如何安装 Anaconda，然后我们将探索如何安装 Gym。
- en: Installing Anaconda
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Anaconda
- en: Anaconda is an open-source distribution of Python. It is widely used for scientific
    computing and processing large volumes of data. It provides an excellent package
    management environment, and it supports Windows, Mac, and Linux operating systems.
    Anaconda comes with Python installed, along with popular packages used for scientific
    computing such as NumPy, SciPy, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 是一个开源的 Python 发行版，广泛用于科学计算和处理大量数据。它提供了一个优秀的包管理环境，支持 Windows、Mac 和 Linux
    操作系统。Anaconda 自带 Python，并且包括许多用于科学计算的流行包，如 NumPy、SciPy 等。
- en: To download Anaconda, visit [https://www.anaconda.com/download/](https://www.anaconda.com/download/),
    where you will see an option for downloading Anaconda for different platforms.
    If you are using Windows or macOS, you can directly download the graphical installer
    according to your machine architecture and install Anaconda using the graphical
    installer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载Anaconda，请访问[https://www.anaconda.com/download/](https://www.anaconda.com/download/)，你将在该页面上看到适用于不同平台的Anaconda下载选项。如果你使用的是Windows或macOS，你可以根据你的机器架构直接下载图形安装程序，并使用图形安装程序安装Anaconda。
- en: 'If you are using Linux, follow these steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Linux，请按照以下步骤操作：
- en: 'Open the Terminal and type the following command to download Anaconda:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并输入以下命令以下载Anaconda：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After downloading, we can install Anaconda using the following command:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载完成后，我们可以使用以下命令安装Anaconda：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After the successful installation of Anaconda, we need to create a virtual environment.
    What is the need for a virtual environment? Say we are working on project A, which
    uses NumPy version 1.14, and project B, which uses NumPy version 1.13\. So, to
    work on project B we either downgrade NumPy or reinstall NumPy. In each project,
    we use different libraries with different versions that are not applicable to
    the other projects. Instead of downgrading or upgrading versions or reinstalling
    libraries every time for a new project, we use a virtual environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功安装Anaconda之后，我们需要创建一个虚拟环境。为什么需要虚拟环境呢？假设我们正在进行项目A，该项目使用NumPy版本1.14，而项目B使用NumPy版本1.13。那么，要在项目B中工作，我们要么降级NumPy，要么重新安装NumPy。在每个项目中，我们使用的是不同版本的库，这些库在其他项目中不可用。为了避免每次新项目都需要降级、升级版本或重新安装库，我们使用虚拟环境。
- en: 'The virtual environment is just an isolated environment for a particular project
    so that each project can have its own dependencies and will not affect other projects.
    We will create a virtual environment using the following command and name our
    environment `universe`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟环境是为特定项目创建的一个隔离环境，使得每个项目可以拥有自己独立的依赖项，并且不会影响其他项目。我们将使用以下命令创建一个名为`universe`的虚拟环境：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we use Python version 3.6\. Once the virtual environment is created,
    we can activate it using the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是Python版本3.6。虚拟环境创建完成后，我们可以使用以下命令激活它：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's it! Now that we have learned how to install Anaconda and create a virtual
    environment, in the next section, we will learn how to install Gym.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在我们已经学会了如何安装Anaconda并创建虚拟环境，在接下来的章节中，我们将学习如何安装Gym。
- en: Installing the Gym toolkit
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Gym工具包
- en: 'In this section, we will learn how to install the Gym toolkit. Before going
    ahead, first, let''s activate our virtual environment, `universe`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何安装Gym工具包。在继续之前，首先让我们激活虚拟环境`universe`：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, install the following dependencies:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，安装以下依赖项：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can install Gym directly using `pip`. Note that throughout the book, we
    will use Gym version 0.15.4\. We can install Gym using the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`pip`直接安装Gym。请注意，本书中将使用Gym版本0.15.4。我们可以使用以下命令安装Gym：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also install Gym by cloning the Gym repository as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过克隆Gym仓库来安装Gym，如下所示：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Common error fixes
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见错误修复
- en: 'Just in case, if you get any of the following errors while installing Gym,
    the following commands will help:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在安装Gym时遇到以下任何错误，以下命令将有所帮助：
- en: '**Failed building wheel for pachi-py** or **failed building wheel for pachi-py
    atari-py**:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建pachi-py轮文件失败**或**构建pachi-py atari-py轮文件失败**：'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Failed building wheel for mujoco-py**:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建mujoco-py轮文件失败**：'
- en: '[PRE9]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**error: command ''gcc'' failed with exit status 1**:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误：命令''gcc''以退出状态1失败**：'
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have successfully installed Gym, in the next section, let's kickstart
    our hands-on reinforcement learning journey.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功安装了Gym，在接下来的章节中，让我们开始我们的强化学习实践之旅。
- en: Creating our first Gym environment
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的第一个Gym环境
- en: We have learned that Gym provides a variety of environments for training a reinforcement
    learning agent. To clearly understand how the Gym environment is designed, we
    will start with the basic Gym environment. After that, we will understand other
    complex Gym environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，Gym提供了多种环境用于训练强化学习代理。为了清楚地理解Gym环境的设计，我们将从基本的Gym环境开始。之后，我们将理解其他复杂的Gym环境。
- en: 'Let''s introduce one of the simplest environments called the Frozen Lake environment.
    *Figure 2.1* shows the Frozen Lake environment. As we can observe, in the Frozen
    Lake environment, the goal of the agent is to start from the initial state **S**
    and reach the goal state **G**:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一个最简单的环境之一，叫做Frozen Lake环境。*图2.1*显示了Frozen Lake环境。正如我们所观察到的，在Frozen Lake环境中，智能体的目标是从初始状态**S**开始，到达目标状态**G**：
- en: '![](img/B15558_02_01.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_01.png)'
- en: 'Figure 2.1: The Frozen Lake environment'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：Frozen Lake环境
- en: 'In the preceding environment, the following apply:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述环境中，以下内容适用：
- en: '**S** denotes the starting state'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S**表示起始状态'
- en: '**F** denotes the frozen state'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**表示冰冻状态'
- en: '**H** denotes the hole state'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H**表示洞穴状态'
- en: '**G** denotes the goal state'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G**表示目标状态'
- en: 'So, the agent has to start from state **S** and reach the goal state **G**.
    But one issue is that if the agent visits state **H**, which is the hole state,
    then the agent will fall into the hole and die as shown in *Figure 2.2*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，智能体必须从状态**S**开始，达到目标状态**G**。但是有一个问题，如果智能体访问了状态**H**（即洞穴状态），那么智能体将掉进洞里并死亡，如*图2.2*所示：
- en: '![](img/B15558_02_02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_02.png)'
- en: 'Figure 2.2: The agent falls down a hole'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：智能体掉进洞里
- en: 'So, we need to make sure that the agent starts from **S** and reaches **G**
    without falling into the hole state **H** as shown in *Figure 2.3*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要确保智能体从**S**开始并达到**G**，而不掉进洞穴状态**H**，如*图2.3*所示：
- en: '![](img/B15558_02_03.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_03.png)'
- en: 'Figure 2.3: The agent reaches the goal state'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：智能体达到目标状态
- en: Each grid box in the preceding environment is called a state, thus we have 16
    states (**S** to **G**) and we have 4 possible actions, which are *up*, *down*,
    *left*, and *right*. We learned that our goal is to reach the state **G** from
    **S** without visiting **H**. So, we assign +1 reward for the goal state **G**
    and 0 for all other states.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上述环境中的每个格子被称为一个状态，因此我们有16个状态（**S**到**G**），并且我们有4个可能的动作，分别是*上*、*下*、*左*和*右*。我们了解到，我们的目标是从**S**到达**G**，而不经过**H**。因此，我们为目标状态**G**分配+1奖励，为所有其他状态分配0奖励。
- en: Thus, we have learned how the Frozen Lake environment works. Now, to train our
    agent in the Frozen Lake environment, first, we need to create the environment
    by coding it from scratch in Python. But luckily we don't have to do that! Since
    Gym provides various environments, we can directly import the Gym toolkit and
    create a Frozen Lake environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经了解了Frozen Lake环境是如何工作的。现在，为了在Frozen Lake环境中训练我们的智能体，首先，我们需要通过Python从头开始编写代码来创建环境。但幸运的是，我们不必这么做！因为Gym提供了各种环境，我们可以直接导入Gym工具包并创建一个Frozen
    Lake环境。
- en: 'Now, we will learn how to create our Frozen Lake environment using Gym. Before
    running any code, make sure that you have activated our virtual environment `universe`.
    First, let''s import the Gym library:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习如何使用Gym创建我们的Frozen Lake环境。在运行任何代码之前，请确保你已经激活了我们的虚拟环境`universe`。首先，让我们导入Gym库：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can create a Gym environment using the `make` function. The `make`
    function requires the environment id as a parameter. In Gym, the id of the Frozen
    Lake environment is `FrozenLake-v0`. So, we can create our Frozen Lake environment
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`make`函数创建一个Gym环境。`make`函数需要环境ID作为参数。在Gym中，Frozen Lake环境的ID是`FrozenLake-v0`。因此，我们可以按如下方式创建我们的Frozen
    Lake环境：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After creating the environment, we can see how our environment looks like using
    the `render` function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建环境后，我们可以使用`render`函数查看我们的环境长什么样：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code renders the following environment:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码呈现了以下环境：
- en: '![](img/B15558_02_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_04.png)'
- en: 'Figure 2.4: Gym''s Frozen Lake environment'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：Gym的Frozen Lake环境
- en: As we can observe, the Frozen Lake environment consists of 16 states (**S**
    to **G**) as we learned. The state **S** is highlighted indicating that it is
    our current state, that is, the agent is in the state **S**. So whenever we create
    an environment, an agent will always begin from the initial state, which in our
    case is state **S**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，Frozen Lake环境由16个状态（**S**到**G**）组成，正如我们所学到的那样。状态**S**被高亮显示，表示这是我们当前的状态，也就是说，智能体处于状态**S**。因此，每当我们创建一个环境时，智能体总是从初始状态开始，在我们的案例中，初始状态是状态**S**。
- en: That's it! Creating the environment using Gym is that simple. In the next section,
    we will understand more about the Gym environment by relating all the concepts
    we have learned in the previous chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！使用Gym创建环境就是这么简单。在下一部分，我们将通过结合上一章中学到的所有概念，进一步了解Gym环境。
- en: Exploring the environment
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索环境
- en: 'In the previous chapter, we learned that the reinforcement learning environment
    can be modeled as a **Markov decision process** (**MDP**) and an MDP consists
    of the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到强化学习环境可以被建模为 **马尔可夫决策过程**（**MDP**），MDP 包括以下内容：
- en: '**States**: A set of states present in the environment.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：环境中存在的一组状态。'
- en: '**Actions**: A set of actions that the agent can perform in each state.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：代理在每个状态下可以执行的动作集合。'
- en: '**Transition probability**: The transition probability is denoted by ![](img/B15558_02_001.png).
    It implies the probability of moving from a state *s* to the state ![](img/B15558_02_002.png)
    while performing an action *a*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转移概率**：转移概率用 ![](img/B15558_02_001.png) 表示。它表示执行某个动作 *a* 时，从状态 *s* 转移到状态
    ![](img/B15558_02_002.png) 的概率。'
- en: '**Reward function**: The reward function is denoted by ![](img/B15558_02_003.png).
    It implies the reward the agent obtains moving from a state *s* to the state ![](img/B15558_02_004.png)
    while performing an action *a*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数**：奖励函数用 ![](img/B15558_02_003.png) 表示。它表示代理从状态 *s* 转移到状态 ![](img/B15558_02_004.png)
    时，执行动作 *a* 所获得的奖励。'
- en: Let's now understand how to obtain all the above information from the Frozen
    Lake environment we just created using Gym.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解如何从我们刚刚使用 Gym 创建的 Frozen Lake 环境中获取上述所有信息。
- en: States
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态
- en: 'A state space consists of all of our states. We can obtain the number of states
    in our environment by just typing `env.observation_space` as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间由我们所有的状态组成。我们可以通过输入 `env.observation_space` 来获取环境中状态的数量，如下所示：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code will print:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将打印：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It implies that we have 16 discrete states in our state space starting from
    state **S** to **G**. Note that, in Gym, the states will be encoded as a number,
    so the state **S** will be encoded as 0, state **F** will be encoded as 1, and
    so on as *Figure 2.5* shows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的状态空间中有 16 个离散状态，从状态 **S** 到 **G**。注意，在 Gym 中，状态会被编码为数字，因此状态 **S** 会被编码为
    0，状态 **F** 会被编码为 1，以此类推，如 *图 2.5* 所示：
- en: '![](img/B15558_02_05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_05.png)'
- en: 'Figure 2.5: Sixteen discrete states'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：十六个离散状态
- en: Actions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作
- en: 'We learned that the action space consists of all the possible actions in the
    environment. We can obtain the action space by using `env.action_space`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，动作空间由环境中所有可能的动作组成。我们可以通过使用 `env.action_space` 来获取动作空间：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code will print:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将打印：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It shows that we have `4` discrete actions in our action space, which are *left*,
    *down*, *right*, and *up*. Note that, similar to states, actions also will be
    encoded into numbers as shown in *Table 2.1*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示我们在动作空间中有 `4` 个离散动作，分别是 *左*、*下*、*右* 和 *上*。注意，和状态类似，动作也会被编码成数字，如 *表 2.1* 所示：
- en: '![](img/B15558_02_06.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_06.png)'
- en: 'Table 2.1: Four discrete actions'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1：四个离散动作
- en: Transition probability and reward function
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转移概率和奖励函数
- en: Now, let's look at how to obtain the transition probability and the reward function.
    We learned that in the stochastic environment, we cannot say that by performing
    some action *a*, the agent will always reach the next state ![](img/B15558_02_004.png)
    exactly because there will be some randomness associated with the stochastic environment,
    and by performing an action *a* in the state *s*, the agent reaches the next state
    ![](img/B15558_02_004.png) with some probability.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何获取转移概率和奖励函数。我们了解到，在随机环境中，我们不能说通过执行某个动作 *a*，代理就能总是精确到达下一个状态 ![](img/B15558_02_004.png)，因为会有一些与随机环境相关的随机性，执行动作
    *a* 时，代理从状态 *s* 到达下一个状态 ![](img/B15558_02_004.png)的概率是存在的。
- en: 'Let''s suppose we are in state 2 (**F**). Now, if we perform action 1 (*down*)
    in state 2, we can reach state 6 as shown in *Figure 2.6*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于状态 2（**F**）。现在，如果我们在状态 2 执行动作 1（*下移*），我们可以到达状态 6，如 *图 2.6* 所示：
- en: '![](img/Image62187.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image62187.png)'
- en: 'Figure 2.6: The agent performing a down action from state 2'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：代理从状态 2 执行下移动作
- en: 'Our Frozen Lake environment is a stochastic environment. When our environment
    is stochastic, we won''t always reach state 6 by performing action 1 (*down*)
    in state 2; we also reach other states with some probability. So when we perform
    an action 1 (*down*) in state 2, we reach state 1 with probability 0.33333, we
    reach state 6 with probability 0.33333, and we reach state 3 with probability
    0.33333 as shown in *Figure 2.7*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Frozen Lake 环境是一个随机环境。当我们的环境是随机时，执行动作 1（*下移*）在状态 2 时，我们不一定总是能到达状态 6；我们还会以一定的概率到达其他状态。所以，当我们在状态
    2 执行动作 1（*下移*）时，我们以 0.33333 的概率到达状态 1，以 0.33333 的概率到达状态 6，以 0.33333 的概率到达状态 3，如
    *图 2.7* 所示：
- en: '![](img/B15558_02_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_08.png)'
- en: 'Figure 2.7: Transition probability of the agent in state 2'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：状态 2 中智能体的转移概率
- en: As we can see, in a stochastic environment we reach the next states with some
    probability. Now, let's learn how to obtain this transition probability using
    the Gym environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在随机环境中，我们以一定的概率到达下一个状态。现在，让我们学习如何使用 Gym 环境获取这个转移概率。
- en: 'We can obtain the transition probability and the reward function by just typing
    `env.P[state][action]`. So, to obtain the transition probability of moving from
    state **S** to the other states by performing the action *right*, we can type
    `env.P[S][right]`. But we cannot just type state **S** and action *right* directly
    since they are encoded as numbers. We learned that state **S** is encoded as 0
    and the action *right* is encoded as 2, so, to obtain the transition probability
    of state **S** by performing the action *right*, we type `env.P[0][2]` as the
    following shows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过输入 `env.P[state][action]` 来获取转移概率和奖励函数。因此，要获取通过执行动作 *right* 从状态 **S**
    转移到其他状态的转移概率，我们可以输入 `env.P[S][right]`。但是我们不能直接输入状态 **S** 和动作 *right*，因为它们是以数字编码的。我们知道状态
    **S** 编码为 0，动作 *right* 编码为 2，因此，为了获取执行动作 *right* 后状态 **S** 的转移概率，我们输入 `env.P[0][2]`，如下面所示：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The above code will print:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'What does this imply? Our output is in the form of `[(transition probability,
    next state, reward, Is terminal state?)]`. It implies that if we perform an action
    2 (*right*) in state 0 (**S**) then:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么？我们的输出形式为 `[(转移概率, 下一个状态, 奖励, 是否终止状态?)]`。这意味着，如果我们在状态 0 (**S**) 执行动作 2
    (*right*)，则：
- en: We reach state 4 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达状态 4 (**F**)，并获得 0 奖励。
- en: We reach state 1 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达状态 1 (**F**)，并获得 0 奖励。
- en: We reach the same state 0 (**S**) with probability 0.33333 and receive 0 reward.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达相同的状态 0 (**S**)，并获得 0 奖励。
- en: '*Figure 2.8* shows the transition probability:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.8* 显示了转移概率：'
- en: '![](img/B15558_02_09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_09.png)'
- en: 'Figure 2.8: Transition probability of the agent in state 0'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：状态 0 中智能体的转移概率
- en: Thus, when we type `env.P[state][action]`, we get the result in the form of
    `[(transition probability, next state, reward, Is terminal state?)]`. The last
    value is Boolean and tells us whether the next state is a terminal state. Since
    4, 1, and 0 are not terminal states, it is given as false.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们输入 `env.P[state][action]` 时，我们将得到以下形式的结果：`[(转移概率, 下一个状态, 奖励, 是否终止状态?)]`。最后一个值是布尔值，告诉我们下一个状态是否是终止状态。由于
    4、1 和 0 不是终止状态，因此给出的值为 false。
- en: 'The output of `env.P[0][2]` is shown in *Table 2.2* for more clarity:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.P[0][2]` 的输出如 *表 2.2* 所示，以便更清楚地理解：'
- en: '![](img/B15558_02_10.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_10.png)'
- en: 'Table 2.2: Output of env.P[0][2]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2：`env.P[0][2]` 的输出
- en: 'Let''s understand this with one more example. Let''s suppose we are in state
    3 (**F**) as *Figure 2.9* shows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过另一个例子来理解这个问题。假设我们处于状态 3 (**F**)，如 *图 2.9* 所示：
- en: '![](img/B15558_02_11.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_11.png)'
- en: 'Figure 2.9: The agent in state 3'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：状态 3 中的智能体
- en: 'Say we perform action 1 (*down*) in state 3 (**F**). Then the transition probability
    of state 3 (**F**) by performing action 1 (*down*) can be obtained as the following
    shows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在状态 3 (**F**) 执行动作 1 (*down*)。那么，通过执行动作 1 (*down*)，状态 3 (**F**) 的转移概率如下所示：
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code will print:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we learned, our output is in the form of `[(transition probability, next
    state, reward, Is terminal state?)]`. It implies that if we perform action 1 (*down*)
    in state 3 (**F**) then:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所学，输出的形式为 `[(转移概率, 下一个状态, 奖励, 是否终止状态?)]`。这意味着如果我们在状态 3 (**F**) 执行动作 1 (*down*)，那么：
- en: We reach state 2 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达状态 2 (**F**)，并获得 0 奖励。
- en: We reach state 7 (**H**) with probability 0.33333 and receive 0 reward.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达状态 7 (**H**)，并获得 0 奖励。
- en: We reach the same state 3 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达相同的状态 3 (**F**)，并获得 0 奖励。
- en: '*Figure 2.10* shows the transition probability:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.10* 显示了转移概率：'
- en: '![](img/B15558_02_12.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_12.png)'
- en: 'Figure 2.10: Transition probabilities of the agent in state 3'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：状态 3 中智能体的转移概率
- en: 'The output of `env.P[3][1]` is shown in *Table 2.3* for more clarity:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.P[3][1]` 的输出如 *表 2.3* 所示，以便更清楚地理解：'
- en: '![](img/B15558_02_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_13.png)'
- en: 'Table 2.3: Output of env.P[3][1]'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.3：`env.P[3][1]` 的输出
- en: As we can observe, in the second row of our output, we have `(0.33333, 7, 0.0,
    True)`, and the last value here is marked as `True`. It implies that state 7 is
    a terminal state. That is, if we perform action 1 (*down*) in state 3 (**F**)
    then we reach state 7 (**H**) with 0.33333 probability, and since 7 (**H**) is
    a hole, the agent dies if it reaches state 7 (**H**). Thus 7(**H**) is a terminal
    state and so it is marked as `True`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，在输出的第二行中，我们有 `(0.33333, 7, 0.0, True)`，这里的最后一个值被标记为`True`。这意味着状态 7
    是一个终止状态。也就是说，如果我们在状态 3 (**F**) 执行动作 1 (*下*)，则我们会以 0.33333 的概率到达状态 7 (**H**)，并且由于
    7 (**H**) 是一个洞，智能体如果到达状态 7 (**H**) 就会死亡。因此，7 (**H**) 是一个终止状态，所以它被标记为`True`。
- en: Thus, we have learned how to obtain the state space, action space, transition
    probability, and the reward function using the Gym environment. In the next section,
    we will learn how to generate an episode.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学会了如何在 Gym 环境中获取状态空间、动作空间、转移概率和奖励函数。在接下来的部分，我们将学习如何生成一个回合。
- en: Generating an episode in the Gym environment
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Gym 环境中生成一个回合
- en: We learned that the agent-environment interaction starting from an initial state
    until the terminal state is called an episode. In this section, we will learn
    how to generate an episode in the Gym environment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，从初始状态到终止状态的智能体-环境交互过程被称为一个回合。在这一部分，我们将学习如何在 Gym 环境中生成一个回合。
- en: 'Before we begin, we initialize the state by resetting our environment; resetting
    puts our agent back to the initial state. We can reset our environment using the
    `reset()` function as shown as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们通过重置环境来初始化状态；重置操作将智能体带回初始状态。我们可以使用 `reset()` 函数来重置环境，具体如下所示：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Action selection
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作选择
- en: 'In order for the agent to interact with the environment, it has to perform
    some action in the environment. So, first, let''s learn how to perform an action
    in the Gym environment. Let''s suppose we are in state 3 (**F**) as *Figure 2.11*
    shows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让智能体与环境进行互动，它必须在环境中执行一些动作。所以，首先，让我们学习如何在 Gym 环境中执行一个动作。假设我们处于状态 3 (**F**)，如*图
    2.11*所示：
- en: '![](img/B15558_02_14.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_14.png)'
- en: 'Figure 2.11: The agent is in state 3 in the Frozen Lake environment'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11：智能体处于冻结湖环境中的状态 3
- en: 'Say we need to perform action 1 (*down*) and move to the new state 7 (**H**).
    How can we do that? We can perform an action using the `step` function. We just
    need to input our action as a parameter to the `step` function. So, we can perform
    action 1 (*down*) in state 3 (**F**) using the `step` function as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要执行动作 1 (*下*) 并移动到新的状态 7 (**H**)。我们该如何做呢？我们可以使用 `step` 函数来执行一个动作。只需将我们的动作作为参数输入
    `step` 函数。于是，我们可以在状态 3 (**F**) 中使用 `step` 函数执行动作 1 (*下*)，如以下所示：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s render our environment using the `render` function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `render` 函数渲染我们的环境：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As shown in *Figure 2.12*, the agent performs action 1 (*down*) in state 3
    (**F**) and reaches the next state 7 (**H**):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 2.12*所示，智能体在状态 3 (**F**) 中执行动作 1 (*下*) 并到达下一个状态 7 (**H**)：
- en: '![](img/B15558_02_15.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_15.png)'
- en: 'Figure 2.12: The agent in state 7 in the Frozen Lake environment'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：智能体处于冻结湖环境中的状态 7
- en: 'Note that whenever we make an action using `env.step()`, it outputs a tuple
    containing 4 values. So, when we take action 1 (*down*) in state 3 (**F**) using
    `env.step(1)`, it gives the output as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每当我们使用 `env.step()` 执行动作时，它会输出一个包含 4 个值的元组。所以，当我们在状态 3 (**F**) 中使用 `env.step(1)`
    执行动作 1 (*下*) 时，它会给出如下输出：
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As you might have guessed, it implies that when we perform action 1 (*down*)
    in state 3 (**F**):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，这意味着当我们在状态 3 (**F**) 执行动作 1 (*下*) 时：
- en: We reach the next state 7 (**H**).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们到达了下一个状态 7 (**H**)。
- en: The agent receives the reward `0.0.`
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体获得了奖励 `0.0`。
- en: Since the next state 7 (**H**) is a terminal state, it is marked as `True`.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于下一个状态 7 (**H**) 是终止状态，因此它被标记为 `True`。
- en: We reach the next state 7 (**H**) with a probability of 0.33333.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以 0.33333 的概率到达下一个状态 7 (**H**)。
- en: 'So, we can just store this information as:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以将这些信息存储为：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Thus:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '`next_state` represents the next state.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_state` 表示下一个状态。'
- en: '`reward` represents the obtained reward.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward` 表示获得的奖励。'
- en: '`done` implies whether our episode has ended. That is, if the next state is
    a terminal state, then our episode will end, so `done` will be marked as `True`
    else it will be marked as `False`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done`表示我们的回合是否已经结束。也就是说，如果下一个状态是终止状态，那么我们的回合将结束，`done`将标记为`True`，否则它将标记为`False`。'
- en: '`info`—Apart from the transition probability, in some cases, we also obtain
    other information saved as info, which is used for debugging purposes.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`—除了转移概率外，在某些情况下，我们还会获得其他信息，并将其保存为info，这些信息用于调试目的。'
- en: 'We can also sample action from our action space and perform a random action
    to explore our environment. We can sample an action using the `sample` function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从我们的动作空间中采样动作，并执行一个随机动作以探索我们的环境。我们可以使用`sample`函数采样一个动作：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After we have sampled an action from our action space, then we perform our
    sampled action using our step function:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在从我们的动作空间中采样了一个动作后，我们使用我们的步进函数执行采样的动作：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that we have learned how to select actions in the environment, let's see
    how to generate an episode.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何在环境中选择动作，让我们看看如何生成一个回合。
- en: Generating an episode
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成一个回合
- en: Now let's learn how to generate an episode. The episode is the agent environment
    interaction starting from the initial state to the terminal state. The agent interacts
    with the environment by performing some action in each state. An episode ends
    if the agent reaches the terminal state. So, in the Frozen Lake environment, the
    episode will end if the agent reaches the terminal state, which is either the
    hole state (**H**) or goal state (**G**).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习如何生成一个回合。回合是智能体与环境的交互，从初始状态开始，到终止状态结束。智能体通过在每个状态下执行一些动作与环境进行交互。如果智能体到达终止状态，回合结束。因此，在Frozen
    Lake环境中，如果智能体到达终止状态，即坑洞状态（**H**）或目标状态（**G**），回合就会结束。
- en: Let's understand how to generate an episode with the random policy. We learned
    that the random policy selects a random action in each state. So, we will generate
    an episode by taking random actions in each state. So for each time step in the
    episode, we take a random action in each state and our episode will end if the
    agent reaches the terminal state.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解如何使用随机策略生成一个回合。我们了解到，随机策略在每个状态下选择一个随机动作。因此，我们将通过在每个状态下采取随机动作来生成一个回合。所以在回合的每个时间步长中，我们在每个状态下采取一个随机动作，如果智能体到达终止状态，回合就会结束。
- en: 'First, let''s set the number of time steps:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置时间步长的数量：
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For each time step:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个时间步长：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Randomly select an action by sampling from the action space:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从动作空间中采样，随机选择一个动作：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Perform the selected action:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选择的操作：
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If the next state is the terminal state, then break. This implies that our
    episode ends:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态是终止状态，则退出。这意味着我们的回合结束：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding code will print something similar to *Figure 2.13*. Note that
    you might get a different result each time you run the preceding code since the
    agent is taking a random action in each time step.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出类似于*图2.13*的内容。请注意，每次运行上述代码时，你可能会得到不同的结果，因为智能体在每个时间步长中都在执行一个随机动作。
- en: 'As we can observe from the following output, on each time step, the agent takes
    a random action in each state and our episode ends once the agent reaches the
    terminal state. As *Figure 2.13* shows, in time step 4, the agent reaches the
    terminal state **H,** and so the episode ends:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如下输出所示，在每个时间步长上，智能体在每个状态下执行一个随机动作，并且一旦智能体到达终止状态，我们的回合就结束。如*图2.13*所示，在第4个时间步长，智能体到达了终止状态**H**，因此回合结束：
- en: '![](img/B15558_02_16.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_16.png)'
- en: 'Figure 2.13: Actions taken by the agent in each time step'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：智能体在每个时间步长所采取的动作
- en: 'Instead of generating one episode, we can also generate a series of episodes
    by taking some random action in each state:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以生成一个回合，还可以通过在每个状态下执行一些随机动作来生成一系列回合：
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Thus, we can generate an episode by selecting a random action in each state
    by sampling from the action space. But wait! What is the use of this? Why do we
    even need to generate an episode?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过从动作空间中采样，在每个状态下选择一个随机动作来生成一个回合。但等等！这样做有什么用？我们为什么需要生成一个回合？
- en: In the previous chapter, we learned that an agent can find the optimal policy
    (that is, the correct action in each state) by generating several episodes. But
    in the preceding example, we just took random actions in each state over all the
    episodes. How can the agent find the optimal policy? So, in the case of the Frozen
    Lake environment, how can the agent find the optimal policy that tells the agent
    to reach state **G** from state **S** without visiting the hole states **H**?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到，智能体可以通过生成多个回合来找到最优策略（即在每个状态下选择正确的动作）。但是在前面的例子中，我们仅仅是在所有回合中在每个状态下采取了随机动作。那么智能体怎么能找到最优策略呢？那么，在Frozen
    Lake环境中，智能体如何找到最优策略，告诉智能体如何从状态**S**到达状态**G**，而不经过坑洞状态**H**呢？
- en: This is where we need a reinforcement learning algorithm. Reinforcement learning
    is all about finding the optimal policy, that is, the policy that tells us what
    action to perform in each state. We will learn how to find the optimal policy
    by generating a series of episodes using various reinforcement learning algorithms
    in the upcoming chapters. In this chapter, we will focus on getting acquainted
    with the Gym environment and various Gym functionalities as we will be using the
    Gym environment throughout the course of the book.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要强化学习算法的地方。强化学习的核心是寻找最优策略，也就是在每个状态下告诉我们该执行什么动作的策略。我们将在接下来的章节中，通过生成一系列的训练回合，学习如何通过各种强化学习算法来找到最优策略。在本章中，我们将重点了解
    Gym 环境以及 Gym 的各种功能，因为我们将在本书的整个过程中使用 Gym 环境。
- en: So far we have understood how the Gym environment works using the basic Frozen
    Lake environment, but Gym has so many other functionalities and also several interesting
    environments. In the next section, we will learn about the other Gym environments
    along with exploring the functionalities of Gym.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经理解了如何使用基本的 Frozen Lake 环境来工作，但 Gym 还有许多其他功能，以及一些非常有趣的环境。在下一部分，我们将学习其他
    Gym 环境，并探索 Gym 的功能。
- en: More Gym environments
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多 Gym 环境
- en: In this section, we will explore several interesting Gym environments, along
    with exploring different functionalities of Gym.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索几个有趣的 Gym 环境，并探索 Gym 的不同功能。
- en: Classic control environments
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经典控制环境
- en: 'Gym provides environments for several classic control tasks such as Cart-Pole
    balancing, swinging up an inverted pendulum, mountain car climbing, and so on.
    Let''s understand how to create a Gym environment for a Cart-Pole balancing task.
    The Cart-Pole environment is shown below:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Gym 提供了多个经典控制任务的环境，如小车-摆杆平衡、摆动倒立摆、山地车爬坡等。让我们了解如何为小车-摆杆平衡任务创建一个 Gym 环境。小车-摆杆环境如下所示：
- en: '![](img/B15558_02_17.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_17.png)'
- en: 'Figure 2.14: Cart-Pole example'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：小车-摆杆示例
- en: 'Cart-Pole balancing is one of the classical control problems. As shown in *Figure
    2.14*, the pole is attached to the cart and the goal of our agent is to balance
    the pole on the cart, that is, the goal of our agent is to keep the pole standing
    straight up on the cart as shown in *Figure 2.15*:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 小车-摆杆平衡是经典控制问题之一。如*图 2.14*所示，摆杆连接在小车上，我们的智能体目标是保持摆杆平衡在小车上，也就是说，智能体的目标是保持摆杆竖直地立在小车上，如*图
    2.15*所示：
- en: '![](img/B15558_02_18.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_18.png)'
- en: 'Figure 2.15: The goal is to keep the pole straight up'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：目标是保持摆杆竖直
- en: So the agent tries to push the cart left and right to keep the pole standing
    straight on the cart. Thus our agent performs two actions, which are pushing the
    cart to the left and pushing the cart to the right, to keep the pole standing
    straight on the cart. You can also check out this very interesting video, [https://youtu.be/qMlcsc43-lg](https://youtu.be/qMlcsc43-lg),
    which shows how the RL agent balances the pole on the cart by moving the cart
    left and right.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，智能体试图左右推动小车，以保持摆杆竖直立在小车上。我们的智能体执行两种动作，即将小车推向左侧和将小车推向右侧，以保持摆杆竖直立在小车上。你也可以查看这个非常有趣的视频，[https://youtu.be/qMlcsc43-lg](https://youtu.be/qMlcsc43-lg)，展示了强化学习智能体如何通过左右移动小车来平衡小车上的摆杆。
- en: 'Now, let''s learn how to create the Cart-Pole environment using Gym. The environment
    id of the Cart-Pole environment in Gym is `CartPole-v0`, so we can just use our
    `make` function to create the Cart-Pole environment as shown below:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何使用 Gym 创建小车-摆杆环境。Gym 中小车-摆杆环境的环境 ID 是 `CartPole-v0`，所以我们可以使用 `make`
    函数来创建小车-摆杆环境，如下所示：
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After creating the environment, we can view our environment using the `render`
    function:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 创建环境后，我们可以使用 `render` 函数查看环境：
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can also close the rendered environment using the `close` function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `close` 函数关闭渲染的环境：
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: State space
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态空间
- en: Now, let's look at the state space of our Cart-Pole environment. Wait! What
    are the states here? In the Frozen Lake environment, we had 16 discrete states
    from **S** to **G**. But how can we describe the states here? Can we describe
    the state by cart position? Yes! Note that the cart position is a continuous value.
    So, in this case, our state space will be continuous values, unlike the Frozen
    Lake environment where our state space had discrete values (**S** to **G**).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下我们小车-杆环境的状态空间。等等！这里的状态是什么？在Frozen Lake环境中，我们有16个离散状态，从**S**到**G**。但是在这里我们如何描述状态呢？我们能通过小车位置来描述状态吗？可以！请注意，小车位置是一个连续值。所以，在这种情况下，我们的状态空间将是连续值，不像Frozen
    Lake环境中我们的状态空间是离散值（**S**到**G**）。
- en: 'But with just the cart position alone we cannot describe the state of the environment
    completely. So we include cart velocity, pole angle, and pole velocity at the
    tip. So we can describe our state space by an array of values as shown as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，仅凭小车位置我们无法完全描述环境的状态。因此，我们还包括了小车速度、杆角度和杆端的速度。因此，我们可以通过一个值数组来描述我们的状态空间，如下所示：
- en: '[PRE39]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that all of these values are continuous, that is:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有这些值都是连续的，也就是说：
- en: The value of the cart position ranges from `-4.8` to `4.8`.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小车位置的值范围从`-4.8`到`4.8`。
- en: The value of the cart velocity ranges from `-Inf` to `Inf` ( ![](img/B15558_02_007.png)
    to ![](img/B15558_02_008.png) ).
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小车速度的值范围从`-Inf`到`Inf`（![](img/B15558_02_007.png)到![](img/B15558_02_008.png)）。
- en: The value of the pole angle ranges from `-0.418` radians to `0.418` radians.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杆角度的值范围从`-0.418`弧度到`0.418`弧度。
- en: The value of the pole velocity at the tip ranges from `-Inf` to `Inf`.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杆端的杆速度的值范围从`-Inf`到`Inf`。
- en: 'Thus, our state space contains an array of continuous values. Let''s learn
    how we can obtain this from Gym. In order to get the state space, we can just
    type `env.observation_space` as shown as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的状态空间包含一个由连续值组成的数组。让我们学习如何从Gym中获取这一点。为了获取状态空间，我们可以直接输入`env.observation_space`，如下所示：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding code will print:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将输出：
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Box` implies that our state space consists of continuous values and not discrete
    values. That is, in the Frozen Lake environment, we obtained the state space as
    `Discrete(16)`, which shows that we have 16 discrete states (**S** to **G**).
    But now we have our state space denoted as `Box(4,)`, which implies that our state
    space is continuous and consists of an array of 4 values.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`Box`意味着我们的状态空间由连续值组成，而不是离散值。也就是说，在Frozen Lake环境中，我们的状态空间是`Discrete(16)`，表示我们有16个离散状态（**S**到**G**）。但现在我们的状态空间表示为`Box(4,)`，这意味着我们的状态空间是连续的，并由4个值组成的数组表示。'
- en: 'For example, let''s reset our environment and see how our initial state space
    will look like. We can reset the environment using the `reset` function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们重置我们的环境，看看我们的初始状态空间会是什么样子。我们可以使用`reset`函数来重置环境：
- en: '[PRE42]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding code will print:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将输出：
- en: '[PRE43]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that here the state space is randomly initialized and so we will get different
    values every time we run the preceding code.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里状态空间是随机初始化的，因此每次运行前面的代码时，我们将获得不同的值。
- en: 'The result of the preceding code implies that our initial state space consists
    of an array of 4 values that denote the cart position, cart velocity, pole angle,
    and pole velocity at the tip, respectively. That is:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的结果意味着我们的初始状态空间由4个值的数组组成，分别表示小车位置、小车速度、杆角度和杆端的杆速度。也就是说：
- en: '![](img/B15558_02_19.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_19.png)'
- en: 'Figure 2.16: Initial state space'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16：初始状态空间
- en: Okay, how can we obtain the maximum and minimum values of our state space? We
    can obtain the maximum values of our state space using `env.observation_space.high`
    and the minimum values of our state space using `env.observation_space.low`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何获取我们状态空间的最大值和最小值？我们可以使用`env.observation_space.high`获取状态空间的最大值，使用`env.observation_space.low`获取状态空间的最小值。
- en: 'For example, let''s look at the maximum value of our state space:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们来看一下我们状态空间的最大值：
- en: '[PRE44]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code will print:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将输出：
- en: '[PRE45]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'It implies that:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着：
- en: The maximum value of the cart position is `4.8`.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小车位置的最大值是`4.8`。
- en: We learned that the maximum value of the cart velocity is `+Inf`, and we know
    that infinity is not really a number, so it is represented using the largest positive
    real value `3.4028235e+38`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们了解到，小车速度的最大值是`+Inf`，我们知道无穷大并不是真正的数字，因此它用最大的正实数`3.4028235e+38`来表示。
- en: The maximum value of the pole angle is `0.418` radians.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杆角度的最大值是`0.418`弧度。
- en: The maximum value of the pole velocity at the tip is `+Inf`, so it is represented
    using the largest positive real value `3.4028235e+38`.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杆尖的最大速度值为`+Inf`，因此使用最大正实数值`3.4028235e+38`来表示。
- en: 'Similarly, we can obtain the minimum value of our state space as:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以得到我们的状态空间的最小值为：
- en: '[PRE46]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code will print:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出：
- en: '[PRE47]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'It states that:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 它说明：
- en: The minimum value of the cart position is `-4.8`.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推车位置的最小值是`-4.8`。
- en: We learned that the minimum value of the cart velocity is `-Inf`, and we know
    that infinity is not really a number, so it is represented using the largest negative
    real value `-3.4028235e+38`.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们了解到推车速度的最小值是`-Inf`，而且我们知道无穷大实际上不是一个数值，因此使用最大负实数值`-3.4028235e+38`来表示。
- en: The minimum value of the pole angle is `-0.418` radians.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杆角的最小值是`-0.418`弧度。
- en: The minimum value of the pole velocity at the tip is `-Inf`, so it is represented
    using the largest negative real value `-3.4028235e+38`.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杆尖的最小速度值为`-Inf`，因此使用最大负实数值`-3.4028235e+38`来表示。
- en: Action space
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作空间
- en: Now, let's look at the action space. We already learned that in the Cart-Pole
    environment we perform two actions, which are pushing the cart to the left and
    pushing the cart to the right, and thus the action space is discrete since we
    have only two discrete actions.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看动作空间。我们已经学到，在Cart-Pole环境中，我们执行两个动作，即将推车向左推和将推车向右推，因此动作空间是离散的，因为我们只有两个离散的动作。
- en: 'In order to get the action space, we can just type `env.action_space` as the
    following shows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得动作空间，我们可以直接输入`env.action_space`，如下所示：
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code will print:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出：
- en: '[PRE49]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we can observe, `Discrete(2)` implies that our action space is discrete,
    and we have two actions in our action space. Note that the actions will be encoded
    into numbers as shown in *Table 2.4*:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，`Discrete(2)`表示我们的动作空间是离散的，并且我们在动作空间中有两个动作。请注意，动作会编码为数字，如*表2.4*所示：
- en: '![](img/B15558_02_20.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_20.png)'
- en: 'Table 2.4: Two possible actions'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.4：两个可能的动作
- en: Cart-Pole balancing with random policy
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用随机策略进行Cart-Pole平衡
- en: Let's create an agent with the random policy, that is, we create the agent that
    selects a random action in the environment and tries to balance the pole. The
    agent receives a +1 reward every time the pole stands straight up on the cart.
    We will generate over 100 episodes, and we will see the return (sum of rewards)
    obtained over each episode. Let's learn this step by step.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个具有随机策略的智能体，也就是说，我们创建一个在环境中选择随机动作并尝试平衡杆子的智能体。每当杆子在推车上竖直时，智能体会获得+1奖励。我们将生成超过100个回合，看看每个回合中获得的返回值（奖励总和）。让我们一步步学习。
- en: 'First, let''s create our Cart-Pole environment:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建我们的Cart-Pole环境：
- en: '[PRE50]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Set the number of episodes and number of time steps in the episode:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数和每回合的时间步数：
- en: '[PRE51]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For each episode:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: '[PRE52]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Set the return to `0`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将返回值设置为`0`：
- en: '[PRE53]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE54]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'For each step in the episode:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一步：
- en: '[PRE55]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Render the environment:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE56]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Randomly select an action by sampling from the environment:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从环境中采样随机选择一个动作：
- en: '[PRE57]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Perform the randomly selected action:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 执行随机选择的动作：
- en: '[PRE58]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Update the return:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 更新返回值：
- en: '[PRE59]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'If the next state is a terminal state then end the episode:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态是终止状态，则结束回合：
- en: '[PRE60]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For every 10 episodes, print the return (sum of rewards):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 每经过10个回合，打印返回值（奖励总和）：
- en: '[PRE61]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Close the environment:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭环境：
- en: '[PRE62]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The preceding code will output the sum of rewards obtained over every 10 episodes:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出每10个回合获得的奖励总和：
- en: '[PRE63]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Thus, we have learned about one of the interesting and classic control problems
    called Cart-Pole balancing and how to create the Cart-Pole balancing environment
    using Gym. Gym provides several other classic control environments as shown in
    *Figure 2.17*:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们了解了一个有趣且经典的控制问题——Cart-Pole平衡问题，以及如何使用Gym创建Cart-Pole平衡环境。Gym提供了其他几个经典控制环境，如*图2.17*所示：
- en: '![](img/B15558_02_21.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_21.png)'
- en: 'Figure 2.17: Classic control environments'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17：经典控制环境
- en: 'You can also do some experimentation by creating any of the above environments
    using Gym. We can check all the classic control environments offered by Gym here:
    [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过使用Gym创建上述任何环境进行实验。我们可以在这里查看Gym提供的所有经典控制环境：[https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control)。
- en: Atari game environments
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Atari游戏环境
- en: Are you a fan of Atari games? If yes, then this section will interest you. Atari
    2600 is a video game console from a game company called Atari. The Atari game
    console provides several popular games, which include Pong, Space Invaders, Ms.
    Pac-Man, Break Out, Centipede, and many more. Training our reinforcement learning
    agent to play Atari games is an interesting as well as challenging task. Often,
    most of the RL algorithms will be tested out on Atari game environments to evaluate
    the accuracy of the algorithm.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 你是 Atari 游戏的粉丝吗？如果是的话，那你一定会对这一部分感兴趣。Atari 2600 是一款由 Atari 公司推出的视频游戏主机。Atari
    游戏主机提供了几款非常受欢迎的游戏，包括 Pong、Space Invaders、Ms. Pac-Man、Break Out、Centipede 等等。训练我们的强化学习代理玩
    Atari 游戏是一个既有趣又具有挑战性的任务。通常，大多数强化学习算法都会在 Atari 游戏环境中进行测试，以评估算法的准确性。
- en: 'In this section, we will learn how to create the Atari game environment using
    Gym. Gym provides about 59 Atari game environments including Pong, Space Invaders,
    Air Raid, Asteroids, Centipede, Ms. Pac-Man, and so on. Some of the Atari game
    environments provided by Gym are shown in *Figure 2.18* to keep you excited:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何使用 Gym 创建 Atari 游戏环境。Gym 提供了大约 59 个 Atari 游戏环境，包括 Pong、Space Invaders、Air
    Raid、Asteroids、Centipede、Ms. Pac-Man 等等。Gym 提供的某些 Atari 游戏环境展示在 *图 2.18* 中，激起你的兴趣：
- en: '![](img/B15558_02_22.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_22.png)'
- en: 'Figure 2.18: Atari game environments'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18：Atari 游戏环境
- en: In Gym, every Atari game environment has 12 different variants. Let's understand
    this with the Pong game environment. The Pong game environment will have 12 different
    variants as explained in the following sections.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gym 中，每个 Atari 游戏环境有 12 个不同的变体。我们以 Pong 游戏环境为例来理解这一点。Pong 游戏环境将有 12 个不同的变体，具体说明如下所述。
- en: General environment
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般环境
- en: '**Pong-v0 and Pong-v4**: We can create a Pong environment with the environment
    id as Pong-v0 or Pong-v4\. Okay, what about the state of our environment? Since
    we are dealing with the game environment, we can just take the image of our game
    screen as our state. But we can''t deal with the raw image directly so we will
    take the pixel values of our game screen as the state. We will learn more about
    this in the upcoming section.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pong-v0 和 Pong-v4**：我们可以通过环境 ID 创建一个 Pong 环境，ID 可以是 Pong-v0 或 Pong-v4。那么，环境的状态是什么呢？由于我们处理的是游戏环境，我们可以直接将游戏屏幕的图像作为状态。但我们不能直接处理原始图像，所以我们将使用游戏屏幕的像素值作为状态。我们将在接下来的章节中详细了解这一点。'
- en: '**Pong-ram-v0 and Pong-ram-v4**: This is similar to Pong-v0 and Pong-v4, respectively.
    However, here, the state of the environment is the RAM of the Atari machine, which
    is just the 128 bytes instead of the game screen''s pixel values.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pong-ram-v0 和 Pong-ram-v4**：这与 Pong-v0 和 Pong-v4 类似。但在这里，环境的状态是 Atari 机器的
    RAM，即128字节，而不是游戏屏幕的像素值。'
- en: Deterministic environment
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定性环境
- en: '**PongDeterministic-v0 and PongDeterministic-v4**: In this type, as the name
    suggests, the initial position of the game will be the same every time we initialize
    the environment, and the state of the environment is the pixel values of the game
    screen.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PongDeterministic-v0 和 PongDeterministic-v4**：在这种类型中，顾名思义，每次初始化环境时，游戏的初始位置都会相同，环境的状态是游戏屏幕的像素值。'
- en: '**Pong-ramDeterministic-v0 and Pong-ramDeterministic-v4**: This is similar
    to PongDeterministic-v0 and PongDeterministic-v4, respectively, but here the state
    is the RAM of the Atari machine.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pong-ramDeterministic-v0 和 Pong-ramDeterministic-v4**：这与 PongDeterministic-v0
    和 PongDeterministic-v4 类似，但这里的状态是 Atari 机器的 RAM。'
- en: No frame skipping
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不跳帧
- en: '**PongNoFrameskip-v0 and PongNoFrameskip-v4**: In this type, no game frame
    is skipped; all game screens are visible to the agent and the state is the pixel
    value of the game screen.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PongNoFrameskip-v0 和 PongNoFrameskip-v4**：在这种类型中，游戏帧不会被跳过；所有游戏画面对代理都是可见的，状态是游戏屏幕的像素值。'
- en: '**Pong-ramNoFrameskip-v0 and Pong-ramNoFrameskip-v4**: This is similar to PongNoFrameskip-v0
    and PongNoFrameskip-v4, but here the state is the RAM of the Atari machine.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pong-ramNoFrameskip-v0 和 Pong-ramNoFrameskip-v4**：这与 PongNoFrameskip-v0 和
    PongNoFrameskip-v4 类似，但这里的状态是 Atari 机器的 RAM。'
- en: Thus in the Atari environment, the state of our environment will be either the
    game screen or the RAM of the Atari machine. Note that similar to the Pong game,
    all other Atari games have the id in the same fashion in the Gym environment.
    For example, suppose we want to create a deterministic Space Invaders environment;
    then we can just create it with the id `SpaceInvadersDeterministic-v0`. Say we
    want to create a Space Invaders environment with no frame skipping; then we can
    create it with the id `SpaceInvadersNoFrameskip-v0`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在Atari环境中，我们的环境状态将是游戏屏幕或Atari机器的RAM。请注意，与Pong游戏类似，所有其他Atari游戏在Gym环境中的ID都以相同的方式命名。例如，假设我们想创建一个确定性的Space
    Invaders环境，那么我们只需使用ID `SpaceInvadersDeterministic-v0`创建它。如果我们想创建一个没有帧跳过的Space
    Invaders环境，那么我们可以使用ID `SpaceInvadersNoFrameskip-v0`来创建它。
- en: 'We can check out all the Atari game environments offered by Gym here: [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里查看Gym提供的所有Atari游戏环境：[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)。
- en: State and action space
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态和动作空间
- en: Now, let's explore the state space and action space of the Atari game environments
    in detail.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细探讨Atari游戏环境的状态空间和动作空间。
- en: State space
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 状态空间
- en: In this section, let's understand the state space of the Atari games in the
    Gym environment. Let's learn this with the Pong game. We learned that in the Atari
    environment, the state of the environment will be either the game screen's pixel
    values or the RAM of the Atari machine. First, let's understand the state space
    where the state of the environment is the game screen's pixel values.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将理解Gym环境中Atari游戏的状态空间。我们通过Pong游戏来学习这一点。我们已经了解，在Atari环境中，环境的状态将是游戏屏幕的像素值或Atari机器的RAM。首先，让我们理解状态空间，其中环境的状态是游戏屏幕的像素值。
- en: 'Let''s create a Pong environment with the `make` function:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`make`函数创建一个Pong环境：
- en: '[PRE64]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Here, the game screen is the state of our environment. So, we will just take
    the image of the game screen as the state. However, we can't deal with the raw
    images directly, so we will take the pixel values of the image (game screen) as
    our state. The dimension of the image pixel will be `3` containing the image height,
    image width, and the number of the channel.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，游戏屏幕是我们环境的状态。因此，我们将把游戏屏幕的图像作为状态。然而，我们不能直接处理原始图像，因此我们将把图像（游戏屏幕）的像素值作为我们的状态。图像像素的维度将是`3`，包含图像的高度、宽度和通道数。
- en: 'Thus, the state of our environment will be an array containing the pixel values
    of the game screen:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的环境状态将是一个包含游戏屏幕像素值的数组：
- en: '[PRE65]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Note that the pixel values range from 0 to 255\. In order to get the state
    space, we can just type `env.observation_space` as the following shows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，像素值的范围是从0到255。为了获取状态空间，我们只需输入`env.observation_space`，如下所示：
- en: '[PRE66]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The preceding code will print:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将输出：
- en: '[PRE67]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This indicates that our state space is a 3D array with a shape of [`210`,`160`,`3`].
    As we've learned, `210` denotes the height of the image, `160` denotes the width
    of the image, and `3` represents the number of channels.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示我们的状态空间是一个形状为[`210`,`160`,`3`]的三维数组。正如我们所学，`210`表示图像的高度，`160`表示图像的宽度，而`3`表示通道数。
- en: 'For example, we can reset our environment and see how the initial state space
    looks like. We can reset the environment using the reset function:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以重置环境并查看初始状态空间的样子。我们可以使用重置函数来重置环境：
- en: '[PRE68]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The preceding code will print an array representing the initial game screen's
    pixel value.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将输出一个表示初始游戏屏幕像素值的数组。
- en: 'Now, let''s create a Pong environment where the state of our environment is
    the RAM of the Atari machine instead of the game screen''s pixel value:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个Pong环境，其中环境的状态是Atari机器的RAM，而不是游戏屏幕的像素值：
- en: '[PRE69]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s look at the state space:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看状态空间：
- en: '[PRE70]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The preceding code will print:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将输出：
- en: '[PRE71]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This implies that our state space is a 1D array containing 128 values. We can
    reset our environment and see how the initial state space looks like:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的状态空间是一个包含128个值的一维数组。我们可以重置环境并查看初始状态空间的样子：
- en: '[PRE72]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Note that this applies to all Atari games in the Gym environment, for example,
    if we create a space invaders environment with the state of our environment as
    the game screen's pixel value, then our state space will be a 3D array with a
    shape of `Box(210, 160, 3)`. However, if we create the Space Invaders environment
    with the state of our environment as the RAM of Atari machine, then our state
    space will be an array with a shape of `Box(128,)`.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这适用于Gym环境中的所有Atari游戏。例如，如果我们创建一个Space Invaders环境，将环境的状态定义为游戏画面的像素值，那么我们的状态空间将是一个形状为`Box(210,
    160, 3)`的三维数组。然而，如果我们将环境的状态定义为Atari机器的RAM，那么我们的状态空间将是一个形状为`Box(128,)`的数组。
- en: Action space
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作空间
- en: 'Let''s now explore the action space. In general, the Atari game environment
    has 18 actions in the action space, and the actions are encoded from 0 to 17 as
    shown in *Table 2.5*:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索动作空间。一般来说，Atari游戏环境的动作空间有18个动作，这些动作被编码为从0到17，如*表2.5*所示：
- en: '![](img/B15558_02_23.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_23.png)'
- en: 'Table 2.5: Atari game environment actions'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.5：Atari游戏环境动作
- en: 'Note that all the preceding 18 actions are not applicable to all the Atari
    game environments and the action space varies from game to game. For instance,
    some games use only the first six of the preceding actions as their action space,
    and some games use only the first nine of the preceding actions as their action
    space, while others use all of the preceding 18 actions. Let''s understand this
    with an example using the Pong game:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有前述的18个动作并不适用于所有的Atari游戏环境，动作空间因游戏而异。例如，有些游戏只使用前六个动作作为其动作空间，有些游戏只使用前九个动作，而其他游戏则使用全部18个动作。让我们通过一个例子来理解这一点，使用Pong游戏：
- en: '[PRE73]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The preceding code will print:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印：
- en: '[PRE74]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The code shows that we have `6` actions in the Pong action space, and the actions
    are encoded from `0` to `5`. So the possible actions in the Pong game are noop
    (no action), fire, up, right, left, and down.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 代码显示我们在Pong的动作空间中有`6`个动作，动作从`0`到`5`进行编码。因此，Pong游戏中的可能动作是noop（无动作）、fire、up、right、left和down。
- en: 'Let''s now look at the action space of the Road Runner game. Just in case you
    have not come across this game before, the game screen looks like this:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看Road Runner游戏的动作空间。以防你以前没有遇到过这个游戏，游戏画面如下所示：
- en: '![](img/B15558_02_24.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_24.png)'
- en: 'Figure 2.19: The Road Runner environment'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19：Road Runner环境
- en: 'Let''s see the action space of the Road Runner game:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看Road Runner游戏的动作空间：
- en: '[PRE75]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The preceding code will print:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印：
- en: '[PRE76]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: This shows us that the action space in the Road Runner game includes all 18
    actions.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了Road Runner游戏中的动作空间包含所有18个动作。
- en: An agent playing the Tennis game
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩网球游戏的智能体
- en: In this section, let's explore how to create an agent to play the Tennis game.
    Let's create an agent with a random policy, meaning that the agent will select
    an action randomly from the action space and perform the randomly selected action.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们探索如何创建一个智能体来玩网球游戏。我们将创建一个随机策略的智能体，这意味着智能体将从动作空间中随机选择一个动作并执行。
- en: 'First, let''s create our Tennis environment:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建我们的网球环境：
- en: '[PRE77]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s view the Tennis environment:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看网球环境：
- en: '[PRE78]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The preceding code will display the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将显示以下内容：
- en: '![](img/B15558_02_25.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_25.png)'
- en: 'Figure 2.20: The Tennis game environment'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20：网球游戏环境
- en: 'Set the number of episodes and the number of time steps in the episode:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数和每个回合的时间步数：
- en: '[PRE79]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'For each episode:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 每个回合：
- en: '[PRE80]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Set the return to `0`:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 将回报设置为`0`：
- en: '[PRE81]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境初始化状态：
- en: '[PRE82]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'For each step in the episode:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 每个回合的每一步：
- en: '[PRE83]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Render the environment:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE84]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Randomly select an action by sampling from the environment:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从环境中采样随机选择一个动作：
- en: '[PRE85]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Perform the randomly selected action:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 执行随机选择的动作：
- en: '[PRE86]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Update the return:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 更新回报：
- en: '[PRE87]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'If the next state is a terminal state, then end the episode:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态是终止状态，则结束该回合：
- en: '[PRE88]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'For every 10 episodes, print the return (sum of rewards):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 每10个回合打印回报（奖励的总和）：
- en: '[PRE89]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Close the environment:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭环境：
- en: '[PRE90]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The preceding code will output the return (sum of rewards) obtained over every
    10 episodes:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出每10个回合获得的回报（奖励总和）：
- en: '[PRE91]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Recording the game
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记录游戏
- en: We have just learned how to create an agent that randomly selects an action
    from the action space and plays the Tennis game. Can we also record the game played
    by the agent and save it as a video? Yes! Gym provides a wrapper class, which
    we can use to save the agent's gameplay as video.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了如何创建一个代理，随机选择一个动作并玩网球游戏。那么，我们能否同时记录代理玩游戏的过程并将其保存为视频呢？当然可以！Gym提供了一个包装类，我们可以用它将代理的游戏过程保存为视频。
- en: To record the game, our system should support FFmpeg. FFmpeg is a framework
    used for processing media files. So before moving ahead, make sure that your system
    provides FFmpeg support.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 为了录制游戏，我们的系统应该支持FFmpeg。FFmpeg是一个用于处理媒体文件的框架。因此，在继续之前，请确保你的系统支持FFmpeg。
- en: 'We can record our game using the `Monitor` wrapper as the following code shows.
    It takes three parameters: the environment; the directory where we want to save
    our recordings; and the force option. If we set `force = False`, it implies that
    we need to create a new directory every time we want to save new recordings, and
    when we set `force = True`, old recordings in the directory will be cleared out
    and replaced by new recordings:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Monitor`包装器来录制我们的游戏，如下代码所示。它需要三个参数：环境；我们想保存录制文件的目录；以及force选项。如果我们设置`force
    = False`，则意味着每次保存新录制时都需要创建一个新目录；如果我们设置`force = True`，目录中的旧录制文件将被清除并替换为新录制：
- en: '[PRE92]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We just need to add the preceding line of code after creating our environment.
    Let''s take a simple example and see how the recordings work. Let''s make our
    agent randomly play the Tennis game for a single episode and record the agent''s
    gameplay as a video:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要在创建环境后添加前述的代码行。让我们通过一个简单的例子来看看录制是如何工作的。我们让代理随机玩一次网球游戏并记录代理的游戏过程作为视频：
- en: '[PRE93]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Once the episode ends, we will see a new directory called **recording** and
    we can find the video file in MP4 format in this directory, which has our agent''s
    gameplay as shown in *Figure 2.21*:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这一局结束，我们将看到一个名为**recording**的新目录，在这个目录中，我们可以找到MP4格式的视频文件，其中记录了我们代理的游戏过程，如*图
    2.21*所示：
- en: '![](img/B15558_02_26.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_26.png)'
- en: 'Figure 2.21: The Tennis gameplay'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：网球游戏玩法
- en: Other environments
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他环境
- en: Apart from the classic control and the Atari game environments we've discussed,
    Gym also provides several different categories of the environment. Let's find
    out more about them.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们讨论过的经典控制和Atari游戏环境外，Gym还提供了几种不同类别的环境。让我们了解更多关于它们的信息。
- en: Box2D
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Box2D
- en: 'Box2D is the 2D simulator that is majorly used for training our agent to perform
    continuous control tasks, such as walking. For example, Gym provides a Box2D environment
    called `BipedalWalker-v2`, which we can use to train our agent to walk. The `BipedalWalker-v2`
    environment is shown in *Figure 2.22*:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: Box2D是一个二维模拟器，主要用于训练我们的代理执行连续控制任务，如行走。例如，Gym提供了一个名为`BipedalWalker-v2`的Box2D环境，我们可以用它来训练代理行走。`BipedalWalker-v2`环境如*图
    2.22*所示：
- en: '![](img/B15558_02_27.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_27.png)'
- en: 'Figure 2.22: The Bipedal Walker environment'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22：双足行走者环境
- en: 'We can check out several other Box2D environments offered by Gym here: [https://gym.openai.com/envs/#box2d](https://gym.openai.com/envs/#box2d).'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里查看Gym提供的其他Box2D环境：[https://gym.openai.com/envs/#box2d](https://gym.openai.com/envs/#box2d)。
- en: MuJoCo
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MuJoCo
- en: '**Mujoco** stands for **Multi-Joint dynamics with Contact** and is one of the
    most popular simulators used for training our agent to perform continuous control
    tasks. For example, MuJoCo provides an interesting environment called `HumanoidStandup-v2`,
    which we can use to train our agent to stand up. The `HumanoidStandup-v2` environment
    is shown in *Figure 2.23*:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**MuJoCo**代表**带接触的多关节动力学**，是用于训练代理执行连续控制任务的最流行模拟器之一。例如，MuJoCo提供了一个有趣的环境，名为`HumanoidStandup-v2`，我们可以用它来训练代理站立。`HumanoidStandup-v2`环境如*图
    2.23*所示：'
- en: '![](img/B15558_02_28.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_28.png)'
- en: 'Figure 2.23: The Humanoid Stand Up environment'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.23：类人站立环境
- en: 'We can check out several other Mujoco environments offered by Gym here: [https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco).'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里查看Gym提供的其他MuJoCo环境：[https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco)。
- en: Robotics
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器人学
- en: 'Gym provides several environments for performing goal-based tasks for the fetch
    and shadow hand robots. For example, Gym provides an environment called `HandManipulateBlock-v0`,
    which we can use to train our agent to orient a box using a robotic hand. The
    `HandManipulateBlock-v0` environment is shown in *Figure 2.24*:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Gym 提供了几个用于执行目标任务的环境，适用于 fetch 和 shadow hand 机器人。例如，Gym 提供了一个名为 `HandManipulateBlock-v0`
    的环境，我们可以用它来训练我们的智能体，通过机器人手操作一个盒子。`HandManipulateBlock-v0` 环境如 *图 2.24* 所示：
- en: '![](img/B15558_02_29.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_29.png)'
- en: 'Figure 2.24: The Hand Manipulate Block environment'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.24：手部操作块环境
- en: 'We can check out the several robotics environments offered by Gym here: [https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics).'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里查看 Gym 提供的多个机器人环境：[https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics)。
- en: Toy text
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩具文本
- en: 'Toy text is the simplest text-based environment. We already learned about one
    such environment at the beginning of this chapter, which is the Frozen Lake environment.
    We can check out other interesting toy text environments offered by Gym here:
    [https://gym.openai.com/envs/#toy_text](https://gym.openai.com/envs/#toy_text).'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 玩具文本是最简单的基于文本的环境。我们已经在本章开头了解了其中一个环境，即 Frozen Lake 环境。我们可以在这里查看 Gym 提供的其他有趣的玩具文本环境：[https://gym.openai.com/envs/#toy_text](https://gym.openai.com/envs/#toy_text)。
- en: Algorithms
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: Instead of using our RL agent to play games, can we make use of our agent to
    solve some interesting problems? Yes! The algorithmic environment provides several
    interesting problems like copying a given sequence, performing addition, and so
    on. We can make use of the RL agent to solve these problems by learning how to
    perform computation. For instance, Gym provides an environment called `ReversedAddition-v0`,
    which we can use to train our agent to add multiple digit numbers.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否让我们的强化学习（RL）智能体解决一些有趣的问题，而不仅仅是玩游戏？当然可以！算法环境提供了多个有趣的问题，比如复制给定的序列、执行加法等。我们可以利用
    RL 智能体来解决这些问题，学习如何进行计算。例如，Gym 提供了一个名为 `ReversedAddition-v0` 的环境，我们可以用它来训练我们的智能体加多位数。
- en: 'We can check the algorithmic environments offered by Gym here: [https://gym.openai.com/envs/#algorithmic](https://gym.openai.com/envs/#algorithmic).'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里查看 Gym 提供的算法环境：[https://gym.openai.com/envs/#algorithmic](https://gym.openai.com/envs/#algorithmic)。
- en: Environment synopsis
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境简介
- en: 'We have learned about several types of Gym environment. Wouldn''t it be nice
    if we could have information about all the environments in a single place? Yes!
    The Gym wiki provides a description of all the environments with their environment
    id, state space, action space, and reward range in a table: [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了几种类型的 Gym 环境。如果我们能在一个地方查看所有环境的信息，那该多好！没问题！Gym 的 Wiki 提供了所有环境的描述，包括环境
    ID、状态空间、动作空间和奖励范围，以表格的形式呈现：[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)。
- en: 'We can also check all the available environments in Gym using the `registry.all()`
    method:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `registry.all()` 方法查看 Gym 中所有可用的环境：
- en: '[PRE94]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The preceding code will print all the available environments in Gym.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出 Gym 中所有可用的环境。
- en: Thus, in this chapter, we have learned about the Gym toolkit and also several
    interesting environments offered by Gym. In the upcoming chapters, we will learn
    how to train our RL agent in a Gym environment to find the optimal policy.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们了解了 Gym 工具包以及 Gym 提供的几个有趣的环境。在接下来的章节中，我们将学习如何在 Gym 环境中训练我们的 RL 智能体，以找到最优策略。
- en: Summary
  id: totrans-434
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding how to set up our machine by installing
    Anaconda and the Gym toolkit. We learned how to create a Gym environment using
    the `gym.make()` function. Later, we also explored how to obtain the state space
    of the environment using `env.observation_space` and the action space of the environment
    using `env.action_space`. We then learned how to obtain the transition probability
    and reward function of the environment using `env.P`. Following this, we also
    learned how to generate an episode using the Gym environment. We understood that
    in each step of the episode we select an action using the `env.step()` function.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们通过安装 Anaconda 和 Gym 工具包来理解如何设置我们的机器。我们学习了如何使用 `gym.make()` 函数创建一个 Gym
    环境。随后，我们还探索了如何使用 `env.observation_space` 获取环境的状态空间，以及使用 `env.action_space` 获取环境的动作空间。接着，我们学习了如何使用
    `env.P` 获取环境的转移概率和奖励函数。之后，我们还学习了如何使用 Gym 环境生成一个回合。我们明白了在每个回合的步骤中，我们通过 `env.step()`
    函数选择一个动作。
- en: We understood the classic control methods in the Gym environment. We learned
    about the continuous state space of the classic control environments and how they
    are stored in an array. We also learned how to balance a pole using a random agent.
    Later, we learned about interesting Atari game environments, and how Atari game
    environments are named in Gym, and then we explored their state space and action
    space. We also learned how to record the agent's gameplay using the wrapper class,
    and at the end of the chapter, we discovered other environments offered by Gym.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了 Gym 环境中的经典控制方法。我们学习了经典控制环境的连续状态空间，以及它们是如何存储在数组中的。我们还学习了如何使用一个随机智能体来平衡一个杆子。随后，我们了解了有趣的
    Atari 游戏环境，学习了 Atari 游戏环境在 Gym 中的命名方式，然后我们探索了它们的状态空间和动作空间。我们还学习了如何使用包装类记录智能体的游戏过程，并在本章结束时，发现了
    Gym 提供的其他环境。
- en: In the next chapter, we will learn how to find the optimal policy using two
    interesting algorithms called value iteration and policy iteration.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用两种有趣的算法，称为值迭代和策略迭代，来找到最优策略。
- en: Questions
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our newly gained knowledge by answering the following questions:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们新获得的知识：
- en: What is the use of a Gym toolkit?
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gym 工具包的用途是什么？
- en: How do we create an environment in Gym?
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何在 Gym 中创建一个环境？
- en: How do we obtain the action space of the Gym environment?
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何获取 Gym 环境的动作空间？
- en: How do we visualize the Gym environment?
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何可视化 Gym 环境？
- en: Name some classic control environments offered by Gym.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举一些 Gym 提供的经典控制环境。
- en: How do we generate an episode using the Gym environment?
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何使用 Gym 环境生成一个回合？
- en: What is the state space of Atari Gym environments?
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Atari Gym 环境的状态空间是什么？
- en: How do we record the agent's gameplay?
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何记录智能体的游戏过程？
- en: Further reading
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Check out the following resources for more information:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下资源以获取更多信息：
- en: To learn more about Gym, go to [http://gym.openai.com/docs/](http://gym.openai.com/docs/).
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于 Gym 的信息，请访问 [http://gym.openai.com/docs/](http://gym.openai.com/docs/)。
- en: 'We can also check out the Gym repository to understand how Gym environments
    are coded: [https://github.com/openai/gym](https://github.com/openai/gym).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以查看 Gym 仓库，了解 Gym 环境是如何编码的：[https://github.com/openai/gym](https://github.com/openai/gym)。
