- en: Building a Deep Convolutional Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度卷积神经网络
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下内容：
- en: Inaccuracy of traditional neural network when images are translated
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统神经网络在图像平移时的不准确性
- en: Building a CNN from scratch using Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 从零开始构建 CNN
- en: CNNs to improve accuracy in case of image translation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 改善图像平移时的准确性
- en: Gender classification using CNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行性别分类
- en: Data augmentation to improve network accuracy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强以提高网络准确性
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we looked at a traditional deep feedforward neural
    network. One of the limitations of a traditional deep feedforward neural network is
    that it is not translation-invariant, that is, a cat image in the upper-right
    corner of an image would be considered different from an image that has a cat
    in the center of the image. Additionally, traditional neural networks are affected
    by the scale of an object. If the object is big in the majority of the images
    and a new image has the same object in it but with a smaller scale (occupies a
    smaller portion of the image), traditional neural networks are likely to fail
    in classifying the image.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节中，我们介绍了传统的深度前馈神经网络。传统深度前馈神经网络的一个局限性是它不具有平移不变性，即图像右上角的猫图像会被认为与图像中央的猫图像不同。此外，传统神经网络受物体尺度的影响。如果物体在大多数图像中占据较大的位置，而新图像中的物体较小（占据图像的较小部分），传统神经网络很可能在分类图像时失败。
- en: '**Convolutional Neural Networks** (**CNNs**) are used to deal with such issues.
    Given that a CNN is able to deal with translation in images and also the scale
    of images, it is considered a lot more useful in object classification/ detection.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）用于解决这些问题。由于 CNN 能够处理图像中的平移以及图像的尺度问题，因此在物体分类/检测中被认为更为有效。'
- en: 'In this chapter, you will learn about the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Inaccuracy of traditional neural network when images are translated
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统神经网络在图像平移时的不准确性
- en: Building a CNN from scratch using Python
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 从零开始构建 CNN
- en: Using CNNs to improve image classification on a MNIST dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 改善 MNIST 数据集上的图像分类
- en: Implementing data augmentation to improve network accuracy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据增强以提高网络准确性
- en: Gender classification using CNNs
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行性别分类
- en: Inaccuracy of traditional neural networks when images are translated
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统神经网络在图像平移时的不准确性
- en: To understand the need of CNNs further, we will first understand why a feed
    forward **Neural Network** (**NN**) does not work when an image is translated
    and then see how the CNN improves upon traditional feed forward NN.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解 CNN 的必要性，我们将首先了解为什么当图像被平移时，前馈**神经网络**（**NN**）不起作用，然后看看 CNN 是如何改进传统前馈神经网络的。
- en: 'Let''s go through the following scenario:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看以下场景：
- en: We will build a NN model to predict labels from the MNIST dataset
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将构建一个神经网络模型来预测 MNIST 数据集中的标签
- en: We will consider all images that have a label of 1 and take an average of all
    of them (generating an average of 1 image)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将考虑所有标签为1的图像，并对它们求平均（生成一张平均的1标签图像）
- en: We will predict the label of the average 1 image that we have generated in the
    previous step using traditional NN
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用传统的神经网络预测我们在上一步生成的平均1标签图像的标签
- en: We will translate the average 1 image by 1 pixel to the left or right
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将把平均1标签图像平移1个像素到左边或右边
- en: We will make a prediction of the translated image using our traditional NN model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用传统神经网络模型对平移后的图像进行预测
- en: How to do it...
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到......
- en: The strategy defined above is coded as follows (please refer to `Issue_with_image
    translation.ipynb` file in GitHub while implementing the code)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义的策略代码如下（请参考 GitHub 中的`Issue_with_image translation.ipynb`文件以实现代码）
- en: 'Download the dataset and extract the train and test MNIST datasets:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集并提取训练集和测试集的 MNIST 数据集：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Fetch the training set corresponding to label `1` only:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取对应标签`1`的训练集：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Reshape and normalize the original training dataset:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整和标准化原始训练数据集：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'One-hot-encode the output labels:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输出标签进行独热编码：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Build a model and fit it:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型并进行拟合：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s plot the average 1 image that we obtained in step 2:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制在第二步中获得的平均1标签图像：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we initialized an empty picture that is 28 x 28 in dimension
    and took an average pixel value at the various pixel locations of images that
    have a label of 1 (the `X_train1` object) by looping through all the values in
    the `X_train1` object.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们初始化了一个28x28大小的空图像，并通过遍历`X_train1`对象中的所有值，在标记为1的图像的不同像素位置取了平均像素值。
- en: 'The plot of the average 1 image appears as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 平均1图像的绘图如下所示：
- en: '![](img/11fffca9-ea5a-4ea8-8fc4-59bfc3402c22.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11fffca9-ea5a-4ea8-8fc4-59bfc3402c22.png)'
- en: It is to be noted that the more yellow (thick) the pixel is, the more often
    people have written on top of the pixel, and the less yellow (more blue/less thick)
    the pixel, the less often people have written on top of the pixel. Also, it is
    to be noted that the pixel in the middle is the yellowest/thickest (this is because
    most people would be writing over the middle pixels, irrespective of whether the
    whole digit is written in a vertical line or is slanted toward the left or right).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，像素越黄色（越厚），人们在该像素上书写的次数越多，而像素越不黄色（更蓝/更薄），人们在该像素上书写的次数就越少。还需要注意的是，图像中央的像素是最黄/最厚的（这是因为大多数人都会在中间的像素上书写，无论整个数字是垂直书写还是向左或向右倾斜）。
- en: Problems with traditional NN
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统神经网络的问题
- en: '**Scenario 1**: Let''s create a new image where the original image is translated
    by 1 pixel toward the left. In the following code, we are looping through the
    columns of the image and copying the pixel values of the next column to the current
    column:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**情境1**：让我们创建一个新图像，其中原始图像向左平移了1个像素。在下面的代码中，我们遍历图像的列，并将下一列的像素值复制到当前列：'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The left translated average 1 image looks as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧翻译后的平均1图像如下所示：
- en: '![](img/d18d3b21-d722-4c43-9860-79b25e008b2e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d18d3b21-d722-4c43-9860-79b25e008b2e.png)'
- en: 'Let’s go ahead and predict the label of the image using the built model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用构建好的模型预测图像的标签：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The model''s prediction on the translated image is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对翻译后的图像的预测结果如下所示：
- en: '![](img/a8682fa3-d0d9-4dec-9ba8-bf00aea80760.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8682fa3-d0d9-4dec-9ba8-bf00aea80760.png)'
- en: We can see a prediction of 1, though with a lower probability than when pixels
    were not translated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到预测为1，尽管它的概率低于像素未翻译时的预测。
- en: '**Scenario 2**: A new image is created in which the pixels of the original
    average 1 image are shifted by 2 pixels to the right:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**情境2**：创建一个新图像，其中原始平均1图像的像素向右平移了2个像素：'
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The right translated average 1 image looks as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧翻译后的平均1图像如下所示：
- en: '![](img/e2fed1a8-3e6d-4a03-ad3e-3ff32ce0d59c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2fed1a8-3e6d-4a03-ad3e-3ff32ce0d59c.png)'
- en: 'The prediction of this image is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像的预测结果如下所示：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model''s prediction on the translated image is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对翻译后的图像的预测结果如下所示：
- en: '![](img/a2d7d8ca-e1ab-44d3-b039-0199a5ef6b43.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2d7d8ca-e1ab-44d3-b039-0199a5ef6b43.png)'
- en: We can see that the prediction is incorrect with an output of 3\. This is the
    problem that we will be addressing by using a CNN.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到预测结果不正确，输出为3。这正是我们通过使用CNN来解决的问题。
- en: Building a CNN from scratch using Python
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python从零开始构建CNN
- en: In this section, we will learn about how a CNN works by building a feedforward
    network from scratch using NumPy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用NumPy从零开始构建一个前馈网络，学习CNN是如何工作的。
- en: Getting ready
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: A typical CNN has multiple components. In this section, we will go through the
    various components of a CNN before we understand how the CNN improves prediction
    accuracy when an image is translated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的CNN有多个组成部分。在本节中，我们将在理解CNN如何改善图像翻译预测准确性之前，了解CNN的各个组件。
- en: Understanding convolution
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积
- en: We are already aware of how a typical NN works. In this section, let's understand
    the working details of the convolution process in CNN.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了典型神经网络是如何工作的。在本节中，让我们理解CNN中卷积过程的工作原理。
- en: Filter
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滤波器
- en: 'A convolution is a multiplication between two matrices—one matrix being big
    and the other being small. To understand convolution, consider the following example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是两个矩阵之间的乘法——一个矩阵较大，另一个较小。为了理解卷积，考虑以下例子：
- en: 'Matrix *A* is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *A* 如下所示：
- en: '![](img/2dd6021a-9a7e-45c5-9e81-94a543cb720d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dd6021a-9a7e-45c5-9e81-94a543cb720d.png)'
- en: 'Matrix *B* is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *B* 如下所示：
- en: '![](img/e6ad679b-8864-42af-948f-745bea146353.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6ad679b-8864-42af-948f-745bea146353.png)'
- en: When performing convolutions, think of it as we are sliding the smaller matrix
    over the larger matrix, that is, we can potentially come up with nine such multiplications
    as the smaller matrix is slid over the entire area of the bigger matrix. Note
    that it is not matrix multiplication.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行卷积操作时，可以将其视为将较小的矩阵滑动到较大的矩阵上，即在较大的矩阵区域内滑动时，可能会出现九种这样的乘法。请注意，这不是矩阵乘法。
- en: 'The various multiplications that happen between the bigger and smaller matrix
    are as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 较大矩阵与较小矩阵之间的各种乘法如下：
- en: '*{1, 2, 5, 6}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{1, 2, 5, 6}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
- en: '*{2, 3, 6, 7}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{2, 3, 6, 7}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*2*1 + 3*2 + 6*3 + 7*4 = 54*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*2*1 + 3*2 + 6*3 + 7*4 = 54*'
- en: '*{3, 4, 7, 8}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{3, 4, 7, 8}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
- en: '*{5, 6, 9, 10}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{5, 6, 9, 10}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*5*1 + 6*2 + 9*3 + 10*4 = 84*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*5*1 + 6*2 + 9*3 + 10*4 = 84*'
- en: '*{6, 7, 10, 11}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of
    the smaller matrix:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{6, 7, 10, 11}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*6*1 + 7*2 + 10*3 + 11*4 = 94*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*6*1 + 7*2 + 10*3 + 11*4 = 94*'
- en: '*{7, 8, 11, 12}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of
    the smaller matrix:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{7, 8, 11, 12}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*7*1 + 8*2 + 11*3 + 12*4 = 104*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*7*1 + 8*2 + 11*3 + 12*4 = 104*'
- en: '*{9,10,13,14}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{9, 10, 13, 14}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
- en: '*{10, 11, 14, 15}* of the bigger matrix is multiplied with *{1, 2, 3 ,4}* of
    the smaller matrix:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{10, 11, 14, 15}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*10*1 + 11*2 + 14*3 + 15*4 = 134*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*10*1 + 11*2 + 14*3 + 15*4 = 134*'
- en: '*{11, 12, 15, 16}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of
    the smaller matrix:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*{11, 12, 15, 16}* 的较大矩阵与 *{1, 2, 3, 4}* 的较小矩阵相乘：'
- en: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
- en: 'The result of the preceding steps would be the following matrix:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前述步骤的结果将是以下矩阵：
- en: '![](img/d341e2cc-5f69-4bb6-9d34-f6cb88e9ded8.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d341e2cc-5f69-4bb6-9d34-f6cb88e9ded8.png)'
- en: Conventionally, the smaller matrix is called a filter or kernel and the smaller
    matrix values are arrived at statistically through gradient descent. The values
    within the filter are the constituent weights.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，较小的矩阵称为滤波器或卷积核，滤波器的数值通过梯度下降统计得到。滤波器中的数值是其组成权重。
- en: Practically, when the image input shape is 224 x 224 x 3, where there are 3
    channels, a filter that has a shape of 3 x 3 would also have 3 channels so that
    performing the matrix multiplication (sum product) is enabled.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当图像输入形状为 224 x 224 x 3 时，其中有 3 个通道，一个 3 x 3 的滤波器也会有 3 个通道，这样就能进行矩阵乘法（求和积）。
- en: A filter will have as many channels as the number of channels in the matrix
    it multiplies with.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个滤波器的通道数与其乘以的矩阵的通道数相同。
- en: Strides
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步幅
- en: In the preceding steps, given that the filter moved one step at a time both
    horizontally and vertically, the strides for the filter are (1, 1). The higher
    the number of strides, the higher the number of values that are skipped from the
    matrix multiplication.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述步骤中，由于滤波器每次水平和垂直移动一步，因此滤波器的步幅为 (1, 1)。步幅数值越大，跳过的矩阵乘法值就越多。
- en: Padding
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充
- en: In the preceding steps, we missed out on multiplying the leftmost values of
    the filter with the rightmost values of the original matrix. If we were to perform
    such an operation, we would have ensure that there is zero padding around the
    edges (the edges of the image padded with zeros) of the original matrix. This
    form of padding is called **valid** padding. The matrix multiplication we performed
    in the *Filter* section of the *Understanding convolution* recipe was a result
    of the **same** padding.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述步骤中，我们遗漏了将滤波器的最左边值与原矩阵的最右边值相乘。如果我们执行这样的操作，我们需要确保在原矩阵的边缘周围进行零填充（即图像边缘填充零）。这种填充方式称为
    **有效** 填充。我们在 *理解卷积* 配方的 *滤波器* 部分进行的矩阵乘法是 **相同** 填充的结果。
- en: From convolution to activation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从卷积到激活
- en: In a traditional NN, a hidden layer not only multiplies the input values by
    the weights, but also applies a non-linearity to the data, that is, it passes
    the values through an activation function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的神经网络中，隐藏层不仅通过权重乘以输入值，还对数据应用非线性处理，即将值通过激活函数传递。
- en: 'A similar activity happens in a typical CNN too, where the convolution is passed
    through an activation function. CNN supports the traditional activations functions
    we have seen so far: sigmoid, ReLU, tanh, and leaky ReLU.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的卷积神经网络中也会发生类似的活动，其中卷积通过激活函数处理。CNN 支持我们目前见过的传统激活函数：sigmoid、ReLU、tanh 和 leaky
    ReLU。
- en: For the preceding output, we can see that the output remains the same when passed
    through a ReLU activation function, as all the numbers are positive.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的输出，我们可以看到当通过 ReLU 激活函数时，输出保持不变，因为所有数字都是正数。
- en: From convolution activation to pooling
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从卷积激活到池化
- en: 'In the previous section, we looked at how convolutions work. In this section,
    we will understand the typical next step after a convolution: pooling.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分，我们研究了卷积是如何工作的。在这一部分，我们将了解卷积之后的典型下一步：池化。
- en: 'Let''s say the output of the convolution step is as follows (we are not considering
    the preceding example, and this is a new example to only illustrate how pooling
    works):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设卷积步骤的输出如下（我们不考虑前面的例子，这是一个新的例子，仅用于说明池化是如何工作的）：
- en: '![](img/1262742f-f600-4d1e-96f8-8abe2641e16f.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1262742f-f600-4d1e-96f8-8abe2641e16f.png)'
- en: 'In the preceding case, the output of a convolution step is a 2 x 2 matrix.
    Max pooling considers the 2 x 2 block and gives the maximum value as output. Similarly,
    imagine that the output of the convolution step is a bigger matrix, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的情况下，卷积步骤的输出是一个 2 x 2 矩阵。最大池化会考虑这个 2 x 2 块，并将最大值作为输出。同样，假设卷积步骤的输出是一个更大的矩阵，如下所示：
- en: '![](img/9e1ef209-932a-43ac-beec-86f01d8aa9e5.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e1ef209-932a-43ac-beec-86f01d8aa9e5.png)'
- en: 'Max pooling divides the big matrix into non-overlapping blocks of 2 x 2 (when
    the stride value is 2), as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化将大矩阵分成不重叠的 2 x 2 块（当步幅值为 2 时），如下所示：
- en: '![](img/7e811be5-036f-47ac-a50c-2a6c992987dd.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e811be5-036f-47ac-a50c-2a6c992987dd.png)'
- en: 'From each block, only the element that has the highest value is chosen. So,
    the output of the max pooling operation on the preceding matrix would be the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个块中，只有具有最大值的元素被选中。所以，前面矩阵的最大池化操作输出将是以下内容：
- en: '![](img/b947e987-a657-4837-bde2-4ea0c4cdae85.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b947e987-a657-4837-bde2-4ea0c4cdae85.png)'
- en: In practice, it is not necessary to have a 2 x 2 window in all cases, but it
    is used more often than not.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，并不总是需要一个 2 x 2 的窗口，但它比其他类型的窗口更常用。
- en: The other types of pooling involved are sum and average—again, in practice,
    we see a lot of max pooling when compared to other types of pooling.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的池化包括求和和平均池化——在实践中，与其他类型的池化相比，我们看到最大池化的应用更多。
- en: How do convolution and pooling help?
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积和池化是如何帮助的？
- en: One of the drawbacks of traditional NN in the MNIST example is that each pixel
    is associated with a distinct weight.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST示例中，传统神经网络的一个缺点是每个像素都与一个独特的权重相关联。
- en: Thus, if an adjacent pixel, other than the original pixel, were to be highlighted,
    instead of the original pixel, the output would not be very accurate (the example
    of *scenario 1*, where the average one was slightly to the left of the middle).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果一个相邻的像素（而不是原始像素）被突出显示，而不是原始像素，那么输出就不会非常准确（比如*场景1*中的例子，平均值稍微偏左于中心）。
- en: This scenario is now addressed, as the pixels share weights that are constituted
    within each filter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个问题得到了处理，因为像素共享在每个过滤器中构成的权重。
- en: All the pixels get multiplied by all the weights that constitute a filter. In
    the pooling layer, only the values post convolution that have a high value are
    chosen.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所有像素都与构成滤波器的所有权重相乘。在池化层中，仅选择卷积后的值较大的值。
- en: This way, irrespective of whether the highlighted pixel is at the center or
    is slightly away from the center, the output would more often than not be the
    expected value.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，无论突出显示的像素是否位于中心，或者稍微偏离中心，输出通常都会是预期的值。
- en: However, the issue still remains the same when the highlighted pixels are very
    far away from the center.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当突出显示的像素远离中心时，问题依然存在。
- en: How to do it...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: To gain a solid understanding, we'll build a CNN-based architecture using Keras
    and validate our understanding of how CNN works by matching the output obtained
    by building the feedforward propagation part of CNN from scratch with the output
    obtained from Keras.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们将使用 Keras 构建基于 CNN 的架构，并通过从头构建 CNN 的前馈传播部分，与使用 Keras 得到的输出进行对比，来验证我们对
    CNN 工作原理的理解。
- en: 'Let''s implement CNN with a toy example where the input and expected output
    data is defined (the code file is available as `CNN_working_details.ipynb` in
    GitHub):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个玩具示例来实现 CNN，其中输入和期望的输出数据已定义（代码文件在 GitHub 上可用，名为 `CNN_working_details.ipynb`）：
- en: 'Create the input and output dataset:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, we created data where positive input gives an output
    of `0` and negative input gives an output of `1`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了数据，其中正输入输出 `0`，负输入输出 `1`。
- en: 'Scale the input dataset:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放输入数据集：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Reshape the input dataset so that each input image is represented in the format
    of width `x` height `x` number of channels:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑输入数据集，使得每个输入图像以宽度 `x` 高度 `x` 通道数的格式表示：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Build the model architecture:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型架构：
- en: 'Instantiate the model after importing the relevant methods:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 导入相关方法后实例化模型：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the next step, we are performing the convolution operation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们执行卷积操作：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding step, we are performing a 2D convolution (the matrix multiplication
    that we saw in the section on *Understanding convolution*) on input data where
    we have 1 filter of size 3 x 3.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们对输入数据执行了二维卷积（在 *理解卷积* 章节中看到的矩阵乘法），其中使用了 1 个 3 × 3 大小的滤波器。
- en: Additionally, given that this is the first layer since instantiating a model,
    we specify the input shape, which is (4 , 4, 1)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，鉴于这是模型实例化后的第一层，我们指定了输入形状，即 (4, 4, 1)。
- en: Finally, we perform ReLu activation on top of the output of the convolution.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对卷积的输出执行 ReLU 激活。
- en: The output of the convolution operation in this scenario is 2 x 2 x 1 in shape,
    as the matrix multiplication of weights with input would yield a 2 x 2 matrix
    (given that the default strides is 1 x 1).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，卷积操作的输出形状为 2 × 2 × 1，因为权重与输入的矩阵乘法会得到一个 2 × 2 的矩阵（假设默认步长为 1 × 1）。
- en: Additionally, the size of output would shrink, as we have not padded the input
    (put zeros around the input image).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输出的大小会缩小，因为我们没有对输入进行填充（即在输入图像周围添加零）。
- en: 'In the following step, we are adding a layer that performs a max pooling operation,
    as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们添加一个执行最大池化操作的层，具体如下：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We are performing max pooling on top of the output obtained from the previous
    layer, where the pool size is 2 x 2\. This means that the maximum value in a subset
    of the 2 x 2 portion of the image is calculated.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对来自上一层的输出执行最大池化操作，池化大小为 2 × 2。这意味着计算图像中 2 × 2 部分的最大值。
- en: Note that a stride of 2 × 2 in the pooling layer would not affect the output
    in this case as the output of the previous step was 2 × 2\. However, in general,
    a stride that is of a greater size than 1 × 1 would affect the output shape.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在池化层中使用 2 × 2 的步长，在这种情况下不会影响输出，因为前一步的输出是 2 × 2。然而，一般来说，步长大于 1 × 1 的情况会影响输出形状。
- en: 'Let''s flatten the output from the pooling layer:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展平池化层的输出：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once we perform flattening, the process becomes very similar to what we performed
    in standard feedforward neural networks where the input is connected to the hidden
    layer and then to the output layer (we can connect the input to more hidden layers,
    too!).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们执行展平操作，过程就变得非常类似于我们在标准前馈神经网络中所执行的操作，在这种网络中，输入与隐藏层连接，再到输出层（我们也可以将输入连接到更多的隐藏层！）。
- en: 'We are directly connecting the output of the flatten layer to the output layer
    using the sigmoid activation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展平层的输出直接连接到输出层，并使用 sigmoid 激活：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A summary of the model can be obtained and looks as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 可以获得模型的总结，结果如下：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A summary of the output is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的总结如下：
- en: '![](img/6e708f38-84ee-4c40-9b05-299cf9c52816.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e708f38-84ee-4c40-9b05-299cf9c52816.png)'
- en: Note that there are 10 parameters in the convolution layer as the one 3 x 3
    filter would have 9 weights and 1 bias term. The pooling layer and flatten layer
    do not have any parameters as they are either extracting maximum values in certain
    regioned (max pooling) or are flattening the output from the previous layer (flatten)
    and thus no operation where weights need to be modified in either of these layers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，卷积层中有10个参数，因为一个3 x 3的滤波器会有9个权重和1个偏置项。池化层和展平层没有任何参数，因为它们要么在某个区域提取最大值（最大池化），要么展平上一层的输出（展平层），因此在这些层中没有需要修改权重的操作。
- en: The output layer has two parameters since the flatten layer has one output,
    which is connected to the output layer that has one value—hence we will have one
    weight and one bias term connecting the flatten layer and output layer.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层有两个参数，因为展平层有一个输出，该输出连接到输出层，输出层有一个值——因此我们将有一个权重和一个偏置项连接展平层和输出层。
- en: 'Compile and fit the model:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并训练模型：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code, we are specifying the loss as binary cross-entropy because
    the outcome is either a `1` or a `0`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将损失函数指定为二元交叉熵，因为输出结果要么是`1`，要么是`0`。
- en: 'Fit the model:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We are fitting the model to have optimal weights that connect the input layer
    with the output layer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在训练模型，以获得将输入层与输出层连接的最优权重。
- en: Validating the CNN output
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证CNN输出
- en: 'Now that we have fit the model, let''s validate the output we obtain from the
    model by implementing the feedforward portion of the CNN:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，让我们通过实现CNN的前向传播部分来验证我们从模型中获得的输出：
- en: 'Let''s extract the order in which weights and biases are presented:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提取权重和偏置呈现的顺序：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/bc1369d0-c303-4cbe-8080-4e302592a96a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc1369d0-c303-4cbe-8080-4e302592a96a.png)'
- en: You can see that the weights of the convolution layer are presented first, then
    the bias, and finally the weight and bias in the output layer.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到卷积层的权重首先被展示，然后是偏置，最后是输出层中的权重和偏置。
- en: 'Also note that the shape of weights in the convolution layer is (3, 3, 1, 1)
    as the filter is 3 x 3 x 1 in shape (because the image is three-dimensional: 28
    x 28 x 1 in shape) and the final 1 (the fourth value in shape) is for the number
    of filters that are specified in the convolution layer.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，卷积层中的权重形状是(3, 3, 1, 1)，因为滤波器的形状是3 x 3 x 1（因为图像是三维的：28 x 28 x 1），最后的1（形状中的第四个值）表示在卷积层中指定的滤波器数量。
- en: If we had specified 64 as the number of filters in the convolution, the shape
    of weights would have been 3 x 3 x 1 x 64.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在卷积中指定了64个滤波器，则权重的形状将是3 x 3 x 1 x 64。
- en: Similarly, had the convolution operation been performed on an image with 3 channels,
    each filter's shape would have been 3 x 3 x 3.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果卷积操作是在具有3个通道的图像上执行的，则每个滤波器的形状将是3 x 3 x 3。
- en: 'Extract the weight values at various layers:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取各层的权重值：
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s extract the output of the first input so that we can validate it with
    feedforward propagation:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提取第一个输入的输出，以便我们能够通过前向传播验证它：
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/cd6f5149-946f-4c97-8499-a4c6c7d6fe6c.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd6f5149-946f-4c97-8499-a4c6c7d6fe6c.png)'
- en: The output from the iteration we ran is 0.0428 (this could be different when
    you run the model, as the random initialization of weights could be different),
    which we will validate by performing matrix multiplication.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行的迭代输出为0.0428（当你运行模型时，这个值可能会不同，因为权重的随机初始化可能不同），我们将通过执行矩阵乘法来验证它。
- en: We are reshaping the input while passing it to the predict method as it expects
    the input to have a shape of (None, 4, 4, 1), where None specifies that the batch
    size could be any number.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在将输入传递给预测方法时正在重新调整输入的形状，因为该方法期望输入的形状为(None, 4, 4, 1)，其中None表示批次大小可以是任意数字。
- en: 'Perform the convolution of the filter with the input image. Note that the input
    image is 4 x 4 in shape while the filter is 3 x 3 in shape. We will be performing
    the matrix multiplication (convolution) along the rows as well as columns in the
    code here:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行滤波器与输入图像的卷积操作。请注意，输入图像的形状是4 x 4，而滤波器的形状是3 x 3。在这里，我们将在代码中沿着行和列执行矩阵乘法（卷积）：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, we are initializing an empty list named `sumprod` that
    stores the output of each matrix multiplication of the filter with the image's
    subset (the subset of the image is of the size of filter).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们初始化了一个名为`sumprod`的空列表，用来存储每次滤波器与图像子集（图像子集的大小与滤波器一致）进行矩阵乘法的输出。
- en: 'Reshape the output of `sumprod` so that it can then be passed to the pooling
    layer:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整 `sumprod` 的输出形状，以便将其传递给池化层：
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Perform activation on top of the convolution''s output before it is passed
    to the pooling layer:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将卷积输出传递到池化层之前，先对其进行激活操作：
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Pass the convolution output to the pooling layer. However, in the current case,
    given that the output of the convolution is 2 x 2, we will keep it simple and
    just take the maximum value in the output we obtained in *step 6*:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将卷积输出传递到池化层。然而，在当前的情况下，由于卷积输出是 2 x 2，我们将简单地取出在 *第 6 步* 中获得的输出的最大值：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Connect the output of the pooling layer to the output layer:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将池化层的输出连接到输出层：
- en: '[PRE28]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We multiplied by the pooling layer's output with the weight in the output layer
    and added the bias in the output layer.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将池化层的输出与输出层的权重相乘，并加上输出层的偏置。
- en: 'Calculate the sigmoid output:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 sigmoid 输出：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output of preceding operation is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 前一步操作的输出如下：
- en: '![](img/9f3d6105-094e-441a-9139-3c885798733d.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f3d6105-094e-441a-9139-3c885798733d.png)'
- en: The output that you see here will be the same as the one that we obtained using
    the `model.predict` method, thus validating our understanding of how a CNN works.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到的输出将与我们使用 `model.predict` 方法获得的输出相同，从而验证我们对 CNN 工作原理的理解。
- en: CNNs to improve accuracy in the case of image translation
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 用于提高图像平移情况下的准确性
- en: In the previous sections, we learned about the issue of translation in images
    and how a CNN works. In this section, we will leverage that knowledge to learn
    how a CNN works toward improving prediction accuracy when an image is translated.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了图像平移问题以及 CNN 是如何工作的。在这一节中，我们将利用这些知识，学习 CNN 如何通过改进预测精度来处理图像平移。
- en: Getting ready
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: 'The strategy that we will be adopting to build a CNN model is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的构建 CNN 模型的策略如下：
- en: 'Given that the input shape is 28 x 28 x 1, the filters shall be 3 x 3 x 1 in
    size:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于输入形状为 28 x 28 x 1，滤波器的大小应为 3 x 3 x 1：
- en: Note that the size of filter can change, however the number of channels cannot
    change
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，滤波器的大小可以变化，但通道的数量不能变化
- en: Let's initialize 10 filters
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们初始化 10 个滤波器
- en: 'We will perform pooling on top of the output obtained in the previous step
    of convolving 10 filters over the input image:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在前一步中对输入图像进行 10 个滤波器卷积得到的输出上执行池化操作：
- en: This would result in halving the image's dimension
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将导致图像尺寸的减半
- en: We will flatten the output obtained while pooling
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将展平池化操作后的输出
- en: The flattened layer will be connected to another hidden layer that has 1,000
    units
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平层将连接到另一个具有 1,000 个单元的隐藏层
- en: Finally, we connect the hidden layer to the output layer where there are 10
    possible classes (as there are 10 digits, from 0 to 9)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将隐藏层连接到输出层，其中有 10 个可能的类别（因为有 10 个数字，从 0 到 9）
- en: Once we build the model, we will translate the average 1 image by 1 pixel and
    then test the CNN model's prediction on the translated image. Note that the feedforward
    NN architecture was not able to predict the right class in this scenario.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建好模型，我们将对平均 1 图像 1 像素进行平移，然后测试 CNN 模型在平移图像上的预测结果。请注意，在这种情况下，前馈神经网络架构无法预测正确的类别。
- en: How to do it...
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let''s understand using a CNN on MNIST data in code (The code file is available
    as `CNN_image_translation.ipynb` in GitHub):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过代码理解如何在 MNIST 数据上使用 CNN（代码文件可在 GitHub 上找到，文件名为 `CNN_image_translation.ipynb`）：
- en: 'Load and preprocess the data:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并预处理数据：
- en: '[PRE30]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that all the steps that we performed in this step are the same as what
    we performed in [Chapter 2](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml), *Building
    a Deep Feedforward Neural Network*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在此步骤中执行的所有步骤与我们在 [第 2 章](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml) *构建深度前馈神经网络*
    中所执行的相同。
- en: 'Build and compile the model:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE31]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'A summary of the model that we initialized in the preceding code can be obtained
    and is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的代码中初始化的模型的摘要如下：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The summary of the model is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/3e4273c7-43a7-4785-9fcb-a5a0e92fc1bb.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e4273c7-43a7-4785-9fcb-a5a0e92fc1bb.png)'
- en: We have a total of 100 parameters in the convolution layer as there are 10 of
    the 3 x 3 x 1 filters, resulting in a total of 90 weight parameters. Additionally,
    10 bias terms (1 for each filter) add up to form a total of 100 parameters in
    the convolution layer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层总共有 100 个参数，因为有 10 个 3 x 3 x 1 的滤波器，总共有 90 个权重参数。另外，10 个偏置项（每个滤波器一个）加起来形成卷积层的
    100 个参数。
- en: Note that max pooling does not have any parameters, as it is about extracting
    the maximum value within a patch that is 2 x 2 in size.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最大池化没有任何参数，因为它是从 2 × 2 大小的区域内提取最大值。
- en: 'Fit the model:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding model gives an accuracy of 98% in 5 epochs:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 前述模型在 5 个训练周期中达到了 98% 的准确率：
- en: '![](img/2356a9a5-1d51-429c-b4ea-5ca798823335.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2356a9a5-1d51-429c-b4ea-5ca798823335.png)'
- en: 'Let''s identify the average 1 image and then translate it by `1` unit:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们识别出平均的 1 张图像，然后将其平移 `1` 个单位：
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the preceding code, we filtered all the image inputs that have a label of
    `1`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们筛选出了所有标签为 `1` 的图像输入：
- en: '[PRE35]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the preceding code, we took the average 1 image:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们取了平均的 1 张图像：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding code, we translated each pixel in the average 1 image by 1
    unit to the left.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们将平均的 1 张图像中的每个像素向左平移 1 个单位。
- en: 'Predict on the translated 1 image:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对翻译后的 1 张图像进行预测：
- en: '[PRE37]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output of preceding step is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 前一步的输出结果如下：
- en: '![](img/5d530d8c-c505-4b13-aafa-5d0c85335cdd.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d530d8c-c505-4b13-aafa-5d0c85335cdd.png)'
- en: Note that the prediction now (when we use a CNN) has more probability (0.9541)
    for 1 when compared to the scenario where the deep feed forward NN model, which
    is predicted as (0.6335) in the label of the translated image in the *Inaccuracy
    of traditional neural network when images are translated* section.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当前使用 CNN 进行预测时，相较于深度前馈神经网络模型（在 *传统神经网络在图像翻译时的不准确性* 部分预测为 0.6335），其预测值（0.9541）对标签
    `1` 的概率更高。
- en: Gender classification using CNNs
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNN 进行性别分类
- en: In the previous sections, we learned about how a CNN works and how CNNs solve
    the image-translation problem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了 CNN 是如何工作的，以及 CNN 是如何解决图像翻译问题的。
- en: In this section, we will further our understanding of how a CNN works by building
    a model that works toward detecting the gender of person present in image.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过构建一个模型，进一步了解 CNN 是如何工作的，目的是检测图像中人物的性别。
- en: Getting ready
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In this section, let''s formulate our strategy of how we will solve this problem:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将制定如何解决该问题的策略：
- en: We will collect a dataset of images and label each image based on the gender
    of person present in image
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将收集图像数据集，并根据图像中人物的性别对每张图像进行标签
- en: We'll work on only 2,000 images, as the data fetching process takes a considerably
    long time for our dataset (as we are manually downloading images from a website
    in this case study)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只会处理 2,000 张图像，因为数据获取过程对我们的数据集来说耗时较长（因为在这个案例中我们是手动从网站下载图像）。
- en: Additionally, we'll ensure that there is equal representation of male and female
    images in the dataset
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们还将确保数据集中男性和女性图像的比例相等。
- en: Once the dataset is in place, we will reshape the images into the same size
    so that they can be fed into a CNN model
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦数据集准备好，我们将把图像调整为相同的大小，以便它们可以输入到 CNN 模型中。
- en: We will build the CNN model where the output layer has as many classes as the
    number of labels two
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将构建 CNN 模型，输出层的类别数为两个标签的数量
- en: Given that this is a case of predicting one out of the two possible labels in
    the dataset, we will minimize the binary cross-entropy loss
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴于这是一个从数据集中预测两个标签之一的案例，我们将最小化二元交叉熵损失。
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this section, we will code the strategy that we defined prior (the code
    file is available as `Gender classification.ipynb` in GitHub):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编码之前定义的策略（代码文件已上传至 GitHub，文件名为 `Gender classification.ipynb`）：
- en: 'Download the dataset:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集：
- en: '[PRE38]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the dataset and inspect its content:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并检查其内容：
- en: '[PRE39]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A sample of some of the key fields in the dataset is as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的一些关键字段示例如下：
- en: '![](img/6bbc8d52-1050-4284-9dc9-ac82eb5a039c.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bbc8d52-1050-4284-9dc9-ac82eb5a039c.png)'
- en: 'Fetch 1,000 male images and 1,000 female images from the URL links provided
    in the dataset:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中提供的 URL 链接获取 1,000 张男性图像和 1,000 张女性图像：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the preceding code, `final_data` contains URL links for 1,000 male images
    and 1,000 female images. Read the URL links and fetch the images corresponding
    to the URL links. Ensure that all images are 300 × 300 × 3 in shape (as the majority
    of the images in his dataset have that shape) and also that we take care of any
    forbidden to access issues:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，`final_data` 包含了 1,000 张男性图像和 1,000 张女性图像的 URL 链接。读取这些 URL 链接并获取对应的图像。确保所有图像的尺寸为
    300 × 300 × 3（因为该数据集中大多数图像都是这个尺寸），并且处理任何禁止访问的问题：
- en: '[PRE41]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'A sample of the input and their corresponding emotion labels looks as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 输入样本及其对应的情感标签如下所示：
- en: '![](img/d08b8aaf-175f-470f-88f4-49b7f728ff26.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d08b8aaf-175f-470f-88f4-49b7f728ff26.png)'
- en: 'Create the input and output arrays:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数组：
- en: '[PRE42]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the preceding step, we have converted the color image into a grayscale image
    as the color of the image is likely to add additional information (we'll validate
    this hypothesis in [Chapter 5](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml), *Transfer
    Learning*).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们已经将彩色图像转换为灰度图像，因为图像的颜色可能会增加额外的信息（我们将在[第5章](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml)，*迁移学习*中验证这个假设）。
- en: 'Additionally, we have resized our images to a lower size (50 x 50 x 1) in shape.
    The result of this is as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将图像调整为较小的尺寸（50 x 50 x 1）。结果如下所示：
- en: '![](img/014738bb-62bc-4d19-86cf-1a5e419d57b6.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/014738bb-62bc-4d19-86cf-1a5e419d57b6.png)'
- en: Finally, we converted the output into a one-hot-encoded version.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将输出转换为一热编码版本。
- en: 'Create train and test datasets. First, we convert the input and output lists
    into arrays and then shape the input so that it is in a shape that can be be provided
    as input to the CNN:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练集和测试集。首先，我们将输入和输出列表转换为数组，然后调整输入的形状，使其能够作为CNN的输入：
- en: '[PRE43]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output of the first value of `x2` is as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`x2`的第一个值的输出如下：'
- en: '![](img/f75c95fd-9415-4496-9936-f1045c6aa1c3.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f75c95fd-9415-4496-9936-f1045c6aa1c3.png)'
- en: 'Note that the input has values between `0` to `255` and thus we have to scale
    it:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入的值在`0`到`255`之间，因此我们必须对其进行缩放：
- en: '[PRE44]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, we split the input and output arrays into train and test datasets:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将输入和输出数组分割成训练集和测试集：
- en: '[PRE45]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The shapes of the train and test input, output arrays are as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试输入、输出数组的形状如下：
- en: '![](img/dabfed13-ae88-4956-aa54-d93b53a6477a.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dabfed13-ae88-4956-aa54-d93b53a6477a.png)'
- en: 'Build and compile the model:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE46]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'A summary of the model is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/e5f69cca-98f7-42f6-a04d-0f86e829cbe0.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5f69cca-98f7-42f6-a04d-0f86e829cbe0.png)'
- en: Note that the number of channels in the output of the convolution layer would
    be equal to the number of filters specified in that layer. Additionally, we have
    performed a slightly more aggressive pooling on the first convolution layer's
    output.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，卷积层输出的通道数将等于该层中指定的过滤器数量。此外，我们对第一个卷积层的输出进行了稍微更激进的池化。
- en: 'Now, we''ll compile the model to minimize binary cross entropy loss (as the
    output has only two classes) as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将编译模型，以最小化二元交叉熵损失（因为输出只有两个类别），如下所示：
- en: '[PRE47]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Fit the model:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/8efa1177-0525-46b7-a427-c60dcefea2f0.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8efa1177-0525-46b7-a427-c60dcefea2f0.png)'
- en: Once we fit the model, we can see that the preceding code results in an accuracy
    of ~80% in predicting the right gender in an image.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合模型，就可以看到之前的代码在预测图像中的性别时，准确率约为80%。
- en: There's more...
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'The accuracy of classification can be further improved by:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方法进一步提高分类的准确性：
- en: Working on more images
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理更多图像
- en: Working on bigger images (rather than 50 x 50 images) that are used to train
    a larger network
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理更大的图像（而不是50 x 50的图像），这些图像将用于训练更大的网络
- en: Leveraging transfer learning (which will be discussed in [Chapter 5](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml),
    *Transfer Learning*)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用迁移学习（将在[第5章](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml)中讨论，*迁移学习*）
- en: Avoiding overfitting using regularization and dropout
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过正则化和丢弃法避免过拟合
- en: Data augmentation to improve network accuracy
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强以提高网络准确率
- en: It is difficult to classify images accurately if they are translated from their
    original location. However, given an image, the label of the image will remain
    the same, even if we translate, rotate, or scale the image. Data augmentation
    is a way to create more images from the given set of images, that is, by rotating,
    translating, or scaling them and mapping them to the label of the original image.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像从原始位置移动，则很难准确分类图像。然而，给定一张图像，无论我们如何平移、旋转或缩放图像，图像的标签保持不变。数据增强是一种从给定图像集创建更多图像的方法，即通过旋转、平移或缩放它们，并将它们映射到原始图像的标签。
- en: An intuition for this is as follows: an image of a person will still be corresponding
    to the person, even if the image is rotated slightly or the person in the image
    is moved from the middle of the image to far right of the image.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个直觉如下：即使图像稍微旋转，或者图像中的人从图像中间移到图像的最右边，图像仍然会对应于该人。
- en: Hence, we should be in a position to create more training data by rotating and
    translating the original images, where we already know the labels that correspond
    to each image.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该能够通过旋转和平移原始图像来创建更多的训练数据，而我们已经知道每个图像对应的标签。
- en: Getting ready
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will be working on the CIFAR-10 dataset, which contains images
    of objects of 10 different classes.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用CIFAR-10数据集，该数据集包含10个不同类别的物体图像。
- en: 'The strategy that we''ll use is as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的策略如下：
- en: Download the CIFAR-10 dataset
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载CIFAR-10数据集
- en: Preprocess the dataset
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: Scale the input values
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入值进行缩放
- en: One-hot-encode the output classes
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输出类别进行独热编码
- en: Build a deep CNN with multiple convolution and pooling layers
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个包含多个卷积和池化层的深度CNN
- en: Compile and fit the model to test its accuracy on the test dataset
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译并拟合模型，测试其在测试数据集上的准确性
- en: Generate random translations of the original set of images in the training dataset
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成训练数据集中原始图像的随机平移
- en: Fit the same model architecture that was built in the previous step on the total
    images (generated images, plus the original images)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对在上一步中构建的相同模型架构进行拟合，使用全部图像（生成的图像加上原始图像）
- en: Check the accuracy of the model on the test dataset
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查模型在测试数据集上的准确性
- en: We will be implementing data augmentation using the `ImageDataGenerator` method
    in the `keras.preprocessing.image` package.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`ImageDataGenerator`方法在`keras.preprocessing.image`包中实现数据增强。
- en: How to do it...
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: To understand the benefits of data augmentation, let's go through an example
    of calculating the accuracy on the CIFAR-10 dataset with data augmentation and
    without data augmentation (the code file is available as `Data_augmentation_to_improve_network_accuracy.ipynb`
    in GitHub).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解数据增强的好处，让我们通过一个例子来计算CIFAR-10数据集在使用和不使用数据增强情况下的准确性（代码文件在GitHub中以`Data_augmentation_to_improve_network_accuracy.ipynb`提供）。
- en: Model accuracy without data augmentation
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无数据增强的模型准确性
- en: 'Let''s calculate the accuracy without data augmentation in the following steps:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下步骤中计算无数据增强的准确性：
- en: 'Import the packages and data:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包和数据：
- en: '[PRE49]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Preprocess the data:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据：
- en: '[PRE50]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A sample of images, along with their corresponding labels, is as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是图像样本及其对应标签：
- en: '![](img/dff754a8-553a-49ad-bd16-b9b339e04ec1.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dff754a8-553a-49ad-bd16-b9b339e04ec1.png)'
- en: 'Build and compile the model:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE51]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We have a higher learning rate only so that the model converges faster in fewer
    epochs. This enables a faster comparison of the non-data augmentation scenario
    with the data augmentation scenario. Ideally, we would let the model run for a
    greater number of epochs with a lesser learning rate.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用较高的学习率仅仅是为了让模型在更少的轮次内更快地收敛。这使得我们能够更快速地比较数据增强场景与非数据增强场景。理想情况下，我们会使用较小的学习率让模型运行更多的轮次。
- en: 'Fit the model:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE52]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The accuracy of this network is ~66%:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络的准确率约为66%：
- en: '![](img/92ccfaf3-5c22-4811-bab8-237bf334af4d.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92ccfaf3-5c22-4811-bab8-237bf334af4d.png)'
- en: Model accuracy with data augmentation
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据增强的模型准确性
- en: 'In the following code, we will implement data augmentation:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将实现数据增强：
- en: 'Use the `ImageDataGenerator` method in the `keras.preprocessing.image` package:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ImageDataGenerator`方法在`keras.preprocessing.image`包中：
- en: '[PRE53]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In the preceding code, we are generating new images where the images are randomly
    rotated between 0 to 20 degrees. A sample of images after being passed through
    the data generator is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在生成新图像，这些图像会在0到20度之间随机旋转。经过数据生成器处理后的图像样本如下：
- en: '![](img/a17f5da5-2d6d-4b24-b090-3399a1b49dbd.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a17f5da5-2d6d-4b24-b090-3399a1b49dbd.png)'
- en: Note that the images are tilted slightly when compared to the previous set of
    images.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与之前的图像集相比，这些图像略微倾斜。
- en: 'Now, we will pass our total data through the data generator, as follows:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过数据生成器将所有数据传递出去，如下所示：
- en: '[PRE54]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Note that we are rebuilding the model so that the weights are initialized one
    more time as we are comparing between a data augmentation and non-data augmentation
    scenario:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们正在重建模型，以便在比较数据增强和非数据增强场景时再次初始化权重：
- en: '[PRE55]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note that the `fit_generator` method fits the model while generating new images.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`fit_generator`方法会在生成新图像的同时拟合模型。
- en: 'Additionally, `datagen.flow` specifies that new training data points need to
    be generated per the datagen strategy we initialized in step *1*. Along with this,
    we also specify the number of steps per epoch as the ratio of the total number
    of data points over the batch size:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，`datagen.flow`指定了根据我们在步骤*1*中初始化的数据生成策略需要生成新的训练数据点。与此同时，我们还指定了每个epoch的步数，作为总数据点数与批次大小的比例：
- en: '![](img/8ce964fc-7aec-48ba-a6a6-485f4850a457.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ce964fc-7aec-48ba-a6a6-485f4850a457.png)'
- en: The accuracy of this code is ~80%, which is better than the accuracy of 66%
    using just the given dataset (without data augmentation).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码的准确率约为80%，比仅使用给定数据集（不进行数据增强）时的66%准确率更高。
