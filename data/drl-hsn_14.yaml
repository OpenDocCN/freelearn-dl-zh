- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Web Navigation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络导航
- en: 'We will now take a look at some other practical applications of reinforcement
    learning (RL): web navigation and browser automation. This is a really useful
    example of how RL methods could be applied to a practical problem, including the
    complications you might face and how they could be addressed.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将现在看看强化学习（RL）的其他一些实际应用：网页导航和浏览器自动化。这是一个非常实用的例子，展示了RL方法如何应用于实际问题，包括你可能遇到的复杂问题以及如何应对它们。
- en: 'In this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将：
- en: Discuss web navigation in general and the practical application of browser automation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般讨论网页导航和浏览器自动化的实际应用
- en: Explore how web navigation can be solved with an RL approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索如何通过强化学习方法解决网页导航问题
- en: Take a deep look at one very interesting, but commonly overlooked and a bit
    abandoned, RL benchmark that was implemented by OpenAI, called Mini World of Bits
    (MiniWoB).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们深入了解一个非常有趣，但常常被忽视且有些被遗弃的强化学习（RL）基准测试，它是由OpenAI实现的，叫做Mini World of Bits（MiniWoB）。
- en: The evolution of web navigation
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网页导航的演变
- en: When the web was invented, it started as several text-only web pages interconnected
    by hyperlinks. If you’re curious, here is the home of the first web page, [http://info.cern.ch](http://info.cern.ch),
    with text and links. The only thing you can do is read the text and click on links
    to navigate between pages.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络被发明时，它起初是一些仅包含文本的网页，通过超链接互相连接。如果你感兴趣，这里是第一篇网页的主页，[http://info.cern.ch](http://info.cern.ch)，其中包含文本和链接。你能做的唯一事情就是阅读文本并点击链接在页面之间导航。
- en: Several years later, in 1995, the Internet Engineering Task Force (IETF) published
    the HTML 2.0 specification, which had a lot of extensions to the original version
    invented by Tim Berners-Lee. Among these extensions were forms and form elements
    that allowed web page authors to add activity to their websites. Users could enter
    and change text, toggle checkboxes, select drop-down lists, and push buttons.
    The set of controls was similar to a minimalistic set of graphical user interface
    (GUI) application controls. The difference was that this happened inside the browser’s
    window, and both the data and user interface (UI) controls that users interacted
    with were defined by the server’s page, but not by the local installed application.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几年后，1995年，互联网工程任务组（IETF）发布了HTML 2.0规范，这个版本对Tim Berners-Lee发明的原始版本进行了大量扩展。其中一些扩展包括表单和表单元素，允许网页作者为他们的网站添加互动功能。用户可以输入和更改文本、切换复选框、选择下拉列表以及点击按钮。这些控件的集合类似于简约的图形用户界面（GUI）应用程序控件。不同之处在于，这一切发生在浏览器窗口内部，用户交互的数据和UI控件都是由服务器页面定义的，而不是由本地安装的应用程序定义的。
- en: Fast forward 29 years, and now we have JavaScript, HTML5 canvas, and Microsoft
    Office applications working inside our browsers. The boundary between the desktop
    and the web is so thin and blurry that you may not even know whether the app you’re
    using is an HTML page or a native app. However, it is still the browser that understands
    HTML and communicates with the outside world using HTTP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 快进29年，现在我们有了JavaScript、HTML5画布以及在浏览器中运行的Microsoft Office应用程序。桌面和网页之间的边界已经变得如此微薄和模糊，以至于你可能无法分辨你正在使用的应用程序是HTML页面还是本地应用程序。然而，仍然是浏览器理解HTML，并使用HTTP与外界通信。
- en: 'At its core, web navigation is defined as the process of a user interacting
    with a website or websites. The user can click on links, type text, or carry out
    any other actions to reach their goal, such as sending an email, finding out the
    exact dates of the French Revolution, or checking recent Facebook notifications.
    All this will be done using web navigation, so that leaves a question: can our
    program learn how to do the same?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，网页导航被定义为用户与一个或多个网站进行互动的过程。用户可以点击链接、输入文本，或者进行其他任何操作来实现他们的目标，例如发送电子邮件、查找法国大革命的确切日期，或查看最近的Facebook通知。所有这些都将通过网页导航来完成，那么问题就来了：我们的程序能学会如何做同样的事情吗？
- en: Browser automation and RL
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浏览器自动化与强化学习（RL）
- en: For a long time, automating website interaction focused on the very practical
    tasks of website testing and web scraping. Website testing is especially critical
    when you have a complicated website that you (or other people) have developed
    and you want to ensure that it does what it is supposed to do. For example, if
    you have a login page that has been redesigned and is ready to be deployed on
    a live website, then you will want to be sure that this new design does sane things
    in case a wrong password is entered, the user clicks on I forgot my password,
    and so on. A complex website could potentially include hundreds or thousands of
    use cases that should be tested on every release, so all such functions should
    be automated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，自动化网站交互主要集中在网站测试和网页抓取等非常实际的任务上。网站测试在你（或其他人）开发并希望确保其按预期运行的复杂网站上尤为重要。例如，如果你有一个已经重新设计并准备在实际网站上部署的登录页面，那么你希望确保当输入错误密码时，或者用户点击“我忘记密码”等情况时，新设计能够正确处理。一个复杂的网站可能包含数百或数千个需要在每次发布时测试的用例，因此所有这些功能应该实现自动化。
- en: Web scraping solves the problem of extracting data from websites at scale. For
    example, if you want to build a system that aggregates all prices for all the
    pizza places in your town, you will potentially need to deal with hundreds of
    different websites, which could be problematic to build and maintain. Web scraping
    tools try to solve the problem of interacting with websites, providing various
    functionality from simple HTTP requests and subsequent HTML parsing to full emulation
    of the user moving the mouse, clicking buttons, user’s reaction delays, and so
    on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取解决了大规模从网站提取数据的问题。例如，如果你想建立一个系统，聚合你所在城市所有比萨店的价格，你可能需要处理数百个不同的网站，这样构建和维护起来可能会很有问题。网页抓取工具尝试解决与网站交互的问题，提供从简单的
    HTTP 请求和随后的 HTML 解析到完全模拟用户移动鼠标、点击按钮、用户反应延迟等的各种功能。
- en: 'The standard approach to browser automation normally allows you to control
    the real browser, such as Chrome or Firefox, with your program, which can observe
    the web page data, like the Document Object Model (DOM) tree and an object’s location
    on the screen, and issue the actions, like moving the mouse, pressing some keys,
    pushing the Back button, or just executing some JavaScript code. The connection
    to the RL problem setup is obvious: our agent interacts with the web page and
    browser by issuing actions and observing the state. The reward is not that clear
    and should be task-specific, like successfully filling a form in or reaching the
    page with the desired information. Practical applications of a system that could
    learn browser tasks are related to the previous use cases, and include the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器自动化的标准方法通常允许你通过程序控制真实的浏览器，如 Chrome 或 Firefox，它能够观察网页数据，如文档对象模型（DOM）树和对象在屏幕上的位置，并执行相应的操作，如移动鼠标、按下某些键、点击返回按钮，或执行一些
    JavaScript 代码。与强化学习（RL）问题设置的联系显而易见：我们的代理通过发出操作和观察状态与网页和浏览器互动。奖励并不是那么明显，应该是任务特定的，比如成功填写表单或到达包含所需信息的页面。能够学习浏览器任务的系统的实际应用与之前的用例相关，包括以下内容：
- en: In web testing for very large websites, it’s extremely tedious to define the
    testing process using low-level browser actions like “move the mouse five pixels
    to the left, then press the left button.” What you want to do is give the system
    some demonstrations and let it generalize and repeat the shown actions in all
    similar situations, or at least make it robust enough for UI redesign, button
    text change, and so on.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非常大的网站进行网页测试时，使用低级浏览器操作来定义测试过程非常繁琐，比如“将鼠标移动到左边五个像素，再按左键”。你想做的是给系统提供一些示范，让它能够概括并在所有类似的情况下重复所展示的操作，或者至少让它在用户界面重新设计、按钮文本更改等情况下足够健壮。
- en: There are many cases when you don’t know the problem in advance, for example,
    when you want the system to explore the weak points of the website, like security
    vulnerabilities. In that case, the RL agent could try a lot of weird actions very
    quickly, much faster than humans could. Of course, the action space for security
    testing is enormous, so random clicking won’t be as effective as experienced human
    testers. In that case, the RL-based system could, potentially, combine the prior
    knowledge and experience of humans but still keep the ability to explore and learn
    from this exploration.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多情况下，你无法提前知道问题是什么，例如，当你希望系统探索网站的薄弱环节，如安全漏洞时。在这种情况下，RL（强化学习）代理可以非常快速地尝试大量奇怪的操作，比人类能做到的要快得多。当然，安全测试的操作空间非常庞大，所以随机点击不会像经验丰富的人类测试者那样有效。在这种情况下，基于RL的系统可以潜在地结合人类的先验知识和经验，同时保持探索的能力，并从这种探索中学习。
- en: Another potential domain that could benefit from RL browser automation is scraping
    and web data extraction in general. For example, you might want to extract some
    data from hundreds of thousands of different websites, like hotel websites, car
    rental agents, or other businesses around the world. Very often, before you get
    to the desired data, a form with parameters needs to be filled out, which becomes
    a very nontrivial task given the different websites’ design, layout, and natural
    language flexibility. With such a task, an RL agent can save tons of time and
    effort by extracting the data reliably and at scale.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个可能受益于RL浏览器自动化的领域是网页抓取和数据提取。举个例子，你可能需要从成千上万的不同网站上提取数据，比如酒店网站、租车代理商或全球各地的其他企业。通常，在你获取到所需数据之前，需要填写带有参数的表单，考虑到不同网站的设计、布局和自然语言的灵活性，这成为了一个非常复杂的任务。在这样的任务中，RL代理可以通过可靠地大规模提取数据，节省大量的时间和精力。
- en: Challenges in browser automation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浏览器自动化中的挑战
- en: 'Potential practical applications of browser automation with RL are attractive
    but have one very serious drawback: they’re too large to be used for research
    and the comparison of methods. In fact, the implementation of a full-sized web
    scraping system could take months of effort from a team, and most of the issues
    would not be directly related to RL, like data gathering, browser engine communication,
    input and output representation, and lots of other questions that real production
    system development consists of.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RL进行浏览器自动化的潜在实际应用非常吸引人，但有一个非常严重的缺点：它们的规模太大，无法用于研究和方法比较。事实上，实施一个完整的网页抓取系统可能需要团队几个月的努力，而且大多数问题与RL本身并不直接相关，比如数据收集、浏览器引擎通信、输入和输出表示，以及许多其他涉及实际生产系统开发的问题。
- en: By solving all these issues, we can easily miss the forest by looking at the
    trees. That’s why researchers love benchmark datasets, like MNIST, ImageNet, and
    the Atari suite. However, not every problem makes a good benchmark. On the one
    hand, it should be simple enough to allow quick experimentation and comparison
    between methods. On the other hand, the benchmark has to be challenging and leave
    room for improvement. For example, Atari benchmarks consist of a wide variety
    of games, from very simple ones that can be solved in half an hour (like Pong),
    to quite complex games that were properly solved only recently (like Montezuma’s
    Revenge, which requires the complex planning of actions). To the best of my knowledge,
    there is only one such benchmark for the browser automation domain, which makes
    it even worse that this benchmark was undeservedly forgotten by the RL community.
    As an attempt to fix this issue, we will take a look at the benchmark in this
    chapter. Let’s talk about its history first.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决所有这些问题，我们可能会陷入“只见树木，不见森林”的困境。这也是为什么研究人员喜欢使用基准数据集，如MNIST、ImageNet和Atari套件的原因。然而，并不是每个问题都适合作为基准。一方面，基准应该足够简单，以便快速实验并进行方法比较；另一方面，基准必须具有挑战性，并留有改进的空间。例如，Atari基准包括各种各样的游戏，从可以在半小时内解决的非常简单的游戏（比如Pong），到直到最近才得以完全解决的相对复杂的游戏（比如《蒙特祖玛的复仇》，需要复杂的行动规划）。根据我所知，浏览器自动化领域只有一个这样的基准，这使得这个基准被RL社区遗忘的情况变得更加糟糕。为了尝试解决这个问题，我们将在本章中看看这个基准。首先让我们了解它的历史。
- en: The MiniWoB benchmark
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MiniWoB基准
- en: In December 2016, OpenAI published a dataset called MiniWoB that contains 80
    browser-based tasks. These tasks are observed at the pixel level (strictly speaking,
    besides pixels, a text description of tasks is given to the agent) and are supposed
    to be communicated with the mouse and keyboard actions using the Virtual Network
    Computing (VNC) client. VNC is a standard remote desktop protocol by which a VNC
    server allows clients to connect to and work with a server’s GUI applications
    using the mouse and keyboard via the network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2016 年 12 月，OpenAI 发布了一个名为 MiniWoB 的数据集，其中包含 80 个基于浏览器的任务。这些任务在像素级别上进行观察（严格来说，除了像素，任务的文本描述也会提供给代理），并且应该通过鼠标和键盘操作与虚拟网络计算（VNC）客户端进行交互。VNC
    是一种标准的远程桌面协议，VNC 服务器允许客户端通过网络使用鼠标和键盘与服务器的图形用户界面（GUI）应用程序进行交互。
- en: 'The 80 tasks vary a lot in terms of complexity and the actions required from
    the agent. Some tasks are very simple, even for RL, like “click on the dialog’s
    close button,” or “push the single button,” but some require multiple steps, for
    example, “open collapsed groups and click on the link with some text,” or “select
    a specific date using the date picker tool” (and this date is randomly generated
    every episode). Some of the tasks are simple for humans but require character
    recognition, for example, “mark checkboxes with this text” (and the text is generated
    randomly). Some screenshots of MiniWoB problems are shown in the following figure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 80 个任务在复杂性和所需的代理动作方面差异很大。有些任务即使对于强化学习（RL）来说也非常简单，例如“点击对话框的关闭按钮”或“按下单个按钮”，但有些任务需要多个步骤，例如“展开折叠的组并点击带有某些文本的链接”或“使用日期选择工具选择特定日期”（且该日期在每次执行时随机生成）。有些任务对于人类来说很简单，但需要字符识别，例如“标记带有此文本的复选框”（文本是随机生成的）。以下是一些
    MiniWoB 问题的截图：
- en: '![PIC](img/B22150_14_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_14_01.png)'
- en: 'Figure 14.1: MiniWoB environments'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1：MiniWoB 环境
- en: Unfortunately, despite the brilliant idea and the challenging nature of MiniWoB,
    it was almost abandoned by OpenAI right after the initial release. Several years
    later, a group of Stanford researchers released an updated version called MiniWoB++,
    which had more games and a reworked architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管 MiniWoB 拥有出色的创意和挑战性，但在最初发布后，几乎被 OpenAI 放弃了。几年后，一组斯坦福大学的研究人员发布了一个更新版，名为
    MiniWoB++，它增加了更多的游戏并重构了架构。
- en: MiniWoB++
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MiniWoB++
- en: Instead of using VNC protocol and a real web browser, MiniWoB++ uses the Selenium
    ([https://www.selenium.dev](https://www.selenium.dev)) library for web browser
    automation, which has significantly increased the performance and stability of
    the environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MiniWoB++ 不再使用 VNC 协议和真实的网页浏览器，而是使用 Selenium（[https://www.selenium.dev](https://www.selenium.dev)）库来实现网页浏览器自动化，这大大提高了环境的性能和稳定性。
- en: Currently, MiniWob++ is being maintained by the Farama Foundation ( [https://miniwob.farama.org/](https://miniwob.farama.org/)),
    which is really great news for the RL community. In this chapter, we’ll use their
    latest version, but before jumping into the RL part of the agent, we need to understand
    how MiniWoB++ works.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，MiniWoB++ 正在由 Farama Foundation 进行维护（[https://miniwob.farama.org/](https://miniwob.farama.org/)），这对
    RL 社区来说是一个好消息。在本章中，我们将使用他们的最新版本，但在进入代理的 RL 部分之前，我们需要了解 MiniWoB++ 的工作原理。
- en: Installation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'The original MiniWoB used VNC and OpenAI Universe, which created lots of complications
    during installation and usage. The previous edition of this book provided a custom
    Docker image with detailed installation instructions. Now, it is much simpler:
    you don’t need to deal with Docker and VNC anymore. The Selenium library (which
    is a de facto standard in browser automation) hides all the complications of communicating
    with the browser, which is started in the background in headless mode. Selenium
    supports various browsers, but the MiniWoB++ developers recommend using Chrome
    or Chromium, as other browsers might render environments differently.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 MiniWoB 使用了 VNC 和 OpenAI Universe，这在安装和使用过程中带来了许多复杂性。本书前一版提供了一个定制的 Docker
    镜像，并附带详细的安装说明。现在，安装过程简单多了：你不再需要处理 Docker 和 VNC。Selenium 库（它是浏览器自动化的事实标准）隐藏了与浏览器通信的所有复杂性，浏览器在后台以无头模式启动。Selenium
    支持多种浏览器，但 MiniWoB++ 开发者推荐使用 Chrome 或 Chromium，因为其他浏览器可能会以不同的方式渲染环境。
- en: 'Besides the MiniWoB++ package (which can be installed with pip install miniwob==1.0),
    you will need chromedriver to be set up on your machine. ChromeDriver is a small
    binary that communicates with the browser and runs it in the “testing mode.” The
    version of ChromeDriver has to match the installed version of Chrome (to check,
    go to Chrome → About Google Chrome), so please download the chromedriver archive
    for your platform and Chrome version from this website: [https://googlechromelabs.github.io/chrome-for-testing/](https://googlechromelabs.github.io/chrome-for-testing/).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MiniWoB++包（可以通过`pip install miniwob==1.0`进行安装）外，您还需要在机器上安装chromedriver。ChromeDriver是一个小型二进制文件，它与浏览器进行通信，并以“测试模式”运行浏览器。ChromeDriver的版本必须与已安装的Chrome版本匹配（可以通过访问Chrome
    → 关于Google Chrome来检查）。因此，请从以下网站下载适用于您平台和Chrome版本的chromedriver压缩包：[https://googlechromelabs.github.io/chrome-for-testing/](https://googlechromelabs.github.io/chrome-for-testing/)。
- en: 'Be careful: besides the ChromeDriver archive, they also provide archives for
    the full version of Chrome, most likely you don’t need it. For example, chromedriver
    for Chrome v123 on Mac M2 hardware will have this URL: [https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.122/mac-arm64/chromedriver-mac-arm64.zip](https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.122/mac-arm64/chromedriver-mac-arm64.zip).
    In the archive, a single chromedriver binary is present, which should be put somewhere
    in the PATH of your shell (on Mac and Linux machines, you can use the which chromedriver
    console command, which has to write the full path to the binary. If nothing is
    shown, you need to modify the PATH). To test your installation, you can use a
    simple program, Chapter14/adhoc/01_wob_create.py. If everything is working, a
    browser window with a task will appear for 2 seconds.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：除了ChromeDriver压缩包外，它们还提供了完整版本的Chrome压缩包，您很可能不需要它。例如，针对Mac M2硬件的Chrome v123的chromedriver可以通过此URL下载：[https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.122/mac-arm64/chromedriver-mac-arm64.zip](https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.122/mac-arm64/chromedriver-mac-arm64.zip)。该压缩包中包含一个单独的chromedriver二进制文件，您需要将其放到shell的某个路径中（在Mac和Linux机器上，您可以使用`which
    chromedriver`命令，它会返回二进制文件的完整路径。如果没有返回，您需要修改PATH）。要测试您的安装，可以使用一个简单的程序，Chapter14/adhoc/01_wob_create.py。如果一切正常，浏览器窗口会出现任务并显示2秒钟。
- en: Actions and observations
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作与观察
- en: 'In contrast with Atari games and the other Gym environments that we have worked
    with so far, MiniWoB exposes a much more generic action space. Atari games used
    six or seven discrete actions corresponding to the controller’s buttons and joystick
    directions. CartPole’s action space is even smaller, with just two actions. However,
    the browser gives our agent much more flexibility in terms of what it can do.
    First, the full keyboard, with control keys and the up/down state of every key,
    is exposed. So, your agent can decide to press 10 buttons simultaneously and it
    will be totally fine from a MiniWoB point of view. The second part of the action
    space is the mouse: you can move the mouse to any coordinates and control the
    state of its buttons. This significantly increases the dimensionality of the action
    space that the agent needs to learn how to handle. In addition, the mouse allows
    double-clicking and mouse-wheel up/down scrolling events.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止处理过的Atari游戏和其他Gym环境相比，MiniWoB展示了一个更加通用的动作空间。Atari游戏使用六到七个离散动作，对应于控制器的按钮和摇杆方向。CartPole的动作空间甚至更小，仅有两个动作。然而，浏览器给我们的智能体提供了更多的灵活性，允许它做更多的事情。首先，完整的键盘，包括控制键和每个键的上下状态，都被暴露。因此，从MiniWoB的角度来看，您的智能体可以选择同时按下10个按钮，这完全没问题。动作空间的第二部分是鼠标：您可以将鼠标移动到任意坐标，并控制其按钮的状态。这显著增加了智能体需要学习如何处理的动作空间维度。此外，鼠标还允许双击和鼠标滚轮上下滚动事件。
- en: 'In terms of observation space, MiniWoB is also much richer than the environments
    we’ve dealt with so far. The full observation is represented as a dict with the
    following data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察空间方面，MiniWoB也比我们迄今为止处理过的环境更为丰富。完整的观察数据以字典形式表示，包含以下数据：
- en: Text with a description of the task, like Click button ONE or You are playing
    as X in TicTacToe, win the game
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含任务描述的文本，例如点击按钮ONE，或者你在井字棋中扮演X，赢得比赛
- en: Screen’s pixel as RGB values
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 屏幕的像素值，以RGB格式表示
- en: List of all DOM elements from the underlying web page with attributes (dimensions,
    colors, font, etc.)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含所有DOM元素的列表，来自底层网页，并带有属性（尺寸、颜色、字体等）
- en: Besides that, you can access the underlying browser to get even more information
    (to get some information that is not directly provided, like CSS attributes or
    raw HTML data).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，你还可以访问底层浏览器，获取更多的信息（比如获取一些未直接提供的信息，如CSS属性或原始HTML数据）。
- en: 'As you can see, this set of tasks has lots of flexibility for experimentation:
    you can focus on the visual side of the task, working at the pixel level; you
    can use DOM information (the environment allows you to click on specific elements);
    or use NLP components — to understand the task description and plan the actions.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这组任务为实验提供了很大的灵活性：你可以专注于任务的视觉部分，工作在像素级别；你可以使用DOM信息（环境允许你点击特定元素）；或者使用NLP组件——理解任务描述并规划行动。
- en: Simple example
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例
- en: To gain some practical experience with MiniWoB, let’s take a look at the program
    you used to validate your installation, which you will find at Chapter14/adhoc/01_wob_create.py.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一些关于MiniWoB的实际经验，让我们来看一下你用来验证安装的程序，你可以在Chapter14/adhoc/01_wob_create.py中找到它。
- en: 'First, we need to register the MiniWoB environment in Gymnasium, which is done
    with the register_envs() function:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在Gymnasium中注册MiniWoB环境，这是通过register_envs()函数完成的：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In fact, this register_envs() function does nothing, as all the environments
    are registered when the module is imported. But modern IDEs are smart enough to
    start complaining about unused modules, so this method creates the impression
    for the IDE that the module is being used in the code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，register_envs()函数什么也不做，因为所有环境都在模块导入时注册。但是现代的IDE足够智能，会开始抱怨未使用的模块，因此这个方法给IDE留下了模块在代码中被使用的印象。
- en: 'Then we create an environment using the standard gym.make() method:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用标准的gym.make()方法创建一个环境：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In our example, we’re using the click-test-2 problem, which asks you to click
    on one of two buttons randomly placed on the webpage. The Farama website contains
    a very convenient list of environments that you can play with yourself. The click-test-2
    problem is available here: [https://miniwob.farama.org/environments/click-test-2/](https://miniwob.farama.org/environments/click-test-2/).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用的是click-test-2问题，它要求你随机点击网页上放置的两个按钮中的一个。Farama网站提供了一个非常方便的环境列表，你可以自己动手实验。click-test-2问题可以在这里找到：[https://miniwob.farama.org/environments/click-test-2/](https://miniwob.farama.org/environments/click-test-2/)。
- en: 'On environment creation, we passed the render_mode argument. If it equals ’human’,
    then the browser window will be shown in the background. In Figure [14.2](#x1-255021r2),
    you can see the window:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建环境时，我们传入了render_mode参数。如果它等于’human’，则浏览器窗口将在后台显示。在图[14.2](#x1-255021r2)中，你可以看到窗口：
- en: '![PIC](img/file178.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file178.png)'
- en: 'Figure 14.2: click-test-2 environment'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：click-test-2环境
- en: 'If we run the program, it will show us the environment object and information
    about the observation (which is quite a large dict, so, I output just a list of
    its keys). The following is the part that is shown by the code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行程序，它会显示环境对象以及关于观察的信息（这是一个相当大的字典，所以我只输出它的键的列表）。下面是代码显示的部分：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, we have utterance (which is a task to be performed), DOM elements,
    screenshot with exactly the same dimensions as the Atari platform (I don’t think
    this is just a coincidence!), and a list of fields, which are task-specific important
    elements in the DOM tree.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有话语（这是需要执行的任务）、DOM元素、与Atari平台完全相同尺寸的截图（我不认为这只是巧合！）以及字段列表，字段是DOM树中任务特定的重要元素。
- en: 'Now, let’s go back to our code. The following snippet finds the element in
    the dom_elements list that we have to click on to perform the task:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的代码。以下代码片段在dom_elements列表中查找我们需要点击的元素以执行任务：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The code is iterating over the dom_elements observation’s field, filtering
    elements that have the text ONE. The element that is found has quite a rich set
    of attributes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 代码正在遍历dom_elements观察对象的字段，筛选出包含文本ONE的元素。找到的元素具有相当丰富的属性集：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s look at the final piece of the code, where we take the reference
    of the element (which is an integer identifier) and create the CLICK_ELEMENT action:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看代码的最后一部分，在这里我们获取元素的引用（它是一个整数标识符），并创建CLICK_ELEMENT动作：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As we have already mentioned, MiniWoB provides a rich set of actions to be executed.
    This particular one emulates a mouse click on a specific DOM element.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，MiniWoB提供了一组丰富的操作可以执行。这个特别的操作模拟了在特定DOM元素上进行鼠标点击。
- en: 'As a result of this action, we should get a reward, which in fact does happen:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作为此操作的结果，我们应该获得一个奖励，实际上确实会发生：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you disable rendering with RENDER_ENV = False, everything that happens in
    the console and the browser won’t be shown. This mode will also lead to a higher
    reward, as the reward decreases with time. Full headless mode on my machine obtains
    a reward of 0.9918 in 0.09 seconds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过设置 RENDER_ENV = False 禁用渲染，控制台和浏览器中发生的所有事情将不会显示。此模式还会导致更高的奖励，因为奖励会随时间减少。我机器上的完全无头模式在
    0.09 秒内获得了 0.9918 的奖励。
- en: The simple clicking approach
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的点击方法
- en: To get started with web navigation, let’s implement a simple A3C agent that
    decides where it should click given the image observation. This approach can solve
    only a small subset of the full MiniWoB suite, and we will discuss the restrictions
    of this approach later. For now, it will allow us to get a better understanding
    of the problem.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始网页导航，让我们实现一个简单的 A3C 代理，根据图像观察决定应该点击哪里。这种方法只能解决 MiniWoB 套件的一个小子集，稍后我们将讨论这种方法的局限性。现在，它将帮助我们更好地理解问题。
- en: As with the previous chapter, I won’t discuss the complete source code here.
    Instead, we will focus on the most important functions and I will provide a brief
    overview of the rest. The complete source code is available in the GitHub repository.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章一样，我在这里不会讨论完整的源代码。相反，我们将专注于最重要的功能，并简要概述其余部分。完整的源代码可以在 GitHub 仓库中找到。
- en: Grid actions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格操作
- en: 'When we talked about MiniWoB architecture and organization, we mentioned that
    the richness and flexibility of the action space creates a lot of challenges for
    the RL agent. The active area inside the browser is just 210 × 160 pixels, but
    even with such a small area, our agent could be asked to move the mouse, perform
    clicks, drag objects, and so on. Just the mouse alone could be problematic to
    master, as, in the extreme case, there could be an almost infinite number of different
    actions that the agent could perform, like pressing the mouse button at some point
    and dragging the mouse to a different location. In our example, we will simplify
    our problem a lot by just considering clicks at some fixed grid points inside
    the active webpage area. The sketch of our action space is shown in the following
    figure:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论 MiniWoB 的架构和组织时，我们提到过，动作空间的丰富性和灵活性为强化学习代理带来了许多挑战。浏览器中的活动区域只有 210 × 160
    像素，但即使是这么小的区域，我们的代理也可能需要执行移动鼠标、点击、拖动对象等操作。仅仅是鼠标就可能成为一个难题，因为在极端情况下，代理可能需要执行几乎无限多种不同的动作，例如在某个位置按下鼠标按钮并拖动鼠标到另一个位置。在我们的示例中，我们通过只考虑在活动网页区域内某些固定网格点的点击来大大简化问题。我们动作空间的示意图如下所示：
- en: '![PIC](img/file179.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file179.png)'
- en: 'Figure 14.3: A grid action space'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3：网格动作空间
- en: 'In the original version of MiniWob, the wrapper for such actions was already
    present in OpenAI Universe. But since it is not available for MiniWoB++, I implemented
    it myself in the lib/wob.py module. Let’s quickly check the code, starting with
    the constructor:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MiniWob 的原始版本中，这种操作的封装器已经存在于 OpenAI Universe 中。但由于 MiniWoB++ 中无法使用它，我在 lib/wob.py
    模块中自行实现了它。我们来快速查看代码，从构造函数开始：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the constructor, we create the observation space (which is a tensor of 3
    × 210 × 160) and the action space, which will be 256 discrete actions for a bin
    size of 10\. As an option, we can ask the wrapper to preserve the text of the
    task to be performed. This functionality will be used in subsequent examples in
    the chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们创建了观察空间（它是一个 3 × 210 × 160 的张量）和动作空间，动作空间将是 256 个离散动作，针对 10\ 的 bin
    大小。作为一个选项，我们可以要求封装器保存要执行的任务的文本。此功能将在本章的后续示例中使用。
- en: 'Then we provide a class method to create the environment with a specific configuration:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们提供一个类方法，用于创建具有特定配置的环境：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Besides just creating the environment and wrapping it, we’re asking for a custom
    ActionSpaceConfig, which will take into account our grid’s dimensions. With this
    customization, we will need to pass the (x,y) coordinates of the grid cell to
    perform the click action. Then, we define a helper method, which converts the
    full observation dict into the format we need. The reset() method is just calling
    this method:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建环境并进行封装外，我们还请求了一个自定义的 ActionSpaceConfig，它会考虑到我们网格的尺寸。通过这种定制，我们需要传递网格单元的
    (x,y) 坐标来执行点击操作。接着，我们定义了一个辅助方法，它将完整的观察字典转换为我们所需的格式。reset() 方法只是调用了这个方法：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, the final piece of the wrapper, the step() method:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，封装器的最后一部分，step() 方法：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To perform the action, we need to convert the index of the grid cell index (in
    the 0…255 range) into the (x,y) coordinates of the cell. Then, as an action for
    the underlying MiniWoB environment, we pass a dict with action_type=0 (which is
    an index in ActionSpaceConfig we used in the environment creation) and a NumPy
    array with those cell coordinates.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行动作，我们需要将网格单元的索引（在 0...255 范围内）转换为该单元格的 (x, y) 坐标。然后，作为底层 MiniWoB 环境的动作，我们传递一个包含
    action_type=0（这是我们在环境创建中使用的 ActionSpaceConfig 中的索引）和包含这些单元格坐标的 NumPy 数组的字典。
- en: To illustrate the wrapper, there is a small program in the GitHub repository
    in the adhoc/03_clicker.py file, which uses a brute force approach on the click-dialog-v1
    task. The goal is to close the randomly placed dialog using the corner button
    with the cross. In this example (we’re not showing the code here), we sequentially
    click through all the 256 grid cells to illustrate the wrapper.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明包装器，GitHub 仓库中的 adhoc/03_clicker.py 文件有一个小程序，它使用暴力破解方法在 click-dialog-v1
    任务中。目标是使用带有叉号的角落按钮关闭随机放置的对话框。在这个示例中（我们不在这里展示代码），我们依次点击所有 256 个网格单元，以说明包装器。
- en: The RL part of our implementation
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们实现的 RL 部分
- en: With the transformation of observation and actions, the RL part is quite straightforward.
    We will use the A3C method to train the agent, which should decide from the 160
    × 210 observation which grid cell to click on. Besides the policy, which is a
    probability distribution over 256 grid cells, our agent estimates the value of
    the state, which will be used as a baseline in policy gradient estimation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随着观察和动作的转换，RL 部分相当简单。我们将使用 A3C 方法来训练代理，代理需要从 160 × 210 的观察中决定点击哪个网格单元。除了策略，它是
    256 个网格单元的概率分布，我们的代理还会估算状态的价值，这将在策略梯度估算中作为基准。
- en: 'There are several modules in this example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中有几个模块：
- en: 'lib/common.py: Methods shared among examples in this chapter, including the
    already familiar RewardTracker and unpack_batch functions'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/common.py：本章中各个示例共享的方法，包括已熟悉的 RewardTracker 和 unpack_batch 函数
- en: 'lib/model.py: Includes a definition of the model, which we’ll take a look at
    in the next section'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/model.py：包含模型的定义，我们将在下一节中查看
- en: 'lib/wob.py: Includes MiniWoB-specific code, like environment wrappers and other
    utility functions'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/wob.py：包含 MiniWoB 特定的代码，如环境包装器和其他实用函数
- en: 'wob_click_train.py: The script used to train the clicker model'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: wob_click_train.py：用于训练点击器模型的脚本
- en: 'wob_click_play.py: The script that loads the model weights and uses them against
    the single environment, recording observations and counting statistics about the
    reward'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: wob_click_play.py：加载模型权重并在单一环境中使用它们，记录观察并统计奖励的脚本。
- en: There is nothing new in the code in these modules, so it is not shown here.
    You can find it in the GitHub repository.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块中的代码没有新内容，因此这里没有展示。你可以在 GitHub 仓库中找到它。
- en: The model and training code
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型和训练代码
- en: 'The model is very straightforward and uses the same patterns that you have
    seen in other A3C examples. I haven’t spent much time optimizing and fine-tuning
    the architecture and hyperparameters, so it’s likely that the final result could
    be improved significantly (you can try doing this yourself based on what you’ve
    learned in this book so far). The following is the model definition with two convolution
    layers, a single-layered policy, and value heads:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型非常简单，使用了你在其他 A3C 示例中看到的相同模式。我没有花太多时间优化和微调架构和超参数，所以最终结果可能会有显著改进（你可以根据你在本书中学到的内容自己尝试进行改进）。以下是具有两个卷积层、单层策略和价值头的模型定义：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You’ll find the training script in wob_click_train.py, and it is exactly the
    same as in Chapter [12](ch016.xhtml#x1-20300012). We’re using AsyncVectorEnv with
    8 parallel environments, which starts 8 Chrome instances in the background. If
    your machine’s memory allows, you can increase this count and check the effect
    on the training.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在 wob_click_train.py 中找到训练脚本，它与第 [12](ch016.xhtml#x1-20300012) 章中的完全相同。我们使用
    AsyncVectorEnv 和 8 个并行环境，它会在后台启动 8 个 Chrome 实例。如果你的机器内存允许，你可以增加这个数量，并检查它对训练的影响。
- en: Training results
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练结果
- en: 'By default, the training uses the click-dialog-v1 problem, and it took about
    8 minutes of training to reach an average reward of 0.9\. Figure [14.4](#x1-260002r4)
    shows the plots with average reward and number of steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练使用 click-dialog-v1 问题，训练大约花费了 8 分钟，达到了平均奖励 0.9。图 [14.4](#x1-260002r4)
    显示了平均奖励和步骤数的图表：
- en: '![PIC](img/B22150_14_04.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_14_04.png)'
- en: 'Figure 14.4: Training reward (left) and count of steps in episodes (right)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：训练奖励（左）和每集步数计数（右）
- en: 'The episode_steps chart on the right shows the mean count of actions that the
    agent should carry out before the end of the episode. Ideally, for this problem,
    the count should be 1, as the only action that the agent needs to take is to click
    on the dialog’s close button. However, in fact, the agent sees seven to nine frames
    before the episode ends. This happens for two reasons: the cross on the dialog
    close button may appear after some delay, and the browser inside the container
    adds a time gap before the agent clicks and the reward is obtained.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的 episode_steps 图表显示了智能体在集结束前应该执行的平均动作次数。理想情况下，对于这个问题，次数应该是 1，因为智能体唯一需要执行的动作是点击对话框的关闭按钮。然而，实际上，智能体在集结束之前会看到七到九帧图像。这发生的原因有两个：对话框关闭按钮上的叉号可能会有延迟出现，并且容器内的浏览器会在智能体点击和奖励获得之间添加一个时间间隙。
- en: 'To check the learned policy, you can use the wob_click_play.py tool, which
    loads the model and uses it in one environment. It can play several episodes to
    test the average model performance:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查已学习的策略，你可以使用 wob_click_play.py 工具，它加载模型并在一个环境中使用它。它可以播放多个回合以测试模型的平均性能：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If this begins with the --render command-line option, the browser window will
    be shown during the agent’s actions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以--render命令行选项开始，浏览器窗口将在智能体的操作过程中显示。
- en: Simple clicking limitations
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单点击限制
- en: Unfortunately, the demonstrated approach can only be used to solve relatively
    simple problems, like click-dialog. If you try to use it for more complicated
    tasks, convergence is unlikely. There are several reasons for this.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，所展示的方法只能用来解决相对简单的问题，比如点击对话框。如果你尝试将它用于更复杂的任务，收敛性是很难实现的。原因有很多。
- en: First, our agent is stateless, which means that it makes the decisions about
    actions only from observations, without taking into account its previous actions.
    You may remember that in Chapter [1](ch005.xhtml#x1-190001), we discussed the
    Markov property of the Markov decision process (MDP) and that this Markov property
    allowed us to discard all previous history, keeping only the current observation.
    Even in relatively simple problems from MiniWoB, this Markov property could be
    violated. For example, there is a problem called click-button-sequence (the screenshot
    is shown in Figure [14.5](#x1-261002r5), and documentation for this environment
    is available at [https://miniwob.farama.org/environments/click-button-sequence/](https://miniwob.farama.org/environments/click-button-sequence/)),
    which requires our agent to first click on button ONE and then on button TWO.
    Even if our agent is lucky enough to randomly click on the buttons in the required
    order, it won’t be able to distinguish from the single image which button needs
    to be clicked on next.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的智能体是无状态的，这意味着它仅根据观察结果来决定行动，而不考虑之前的行为。你可能还记得在第[1](ch005.xhtml#x1-190001)章中，我们讨论了马尔可夫决策过程（MDP）的马尔可夫性质，这个性质使我们能够丢弃所有之前的历史，只保留当前的观察结果。即使在
    MiniWoB 中相对简单的问题中，这一马尔可夫性质也可能被违反。例如，有一个问题叫做点击按钮序列（截图见图[14.5](#x1-261002r5)，该环境的文档可以在[https://miniwob.farama.org/environments/click-button-sequence/](https://miniwob.farama.org/environments/click-button-sequence/)找到），要求我们的智能体先点击按钮
    ONE，再点击按钮 TWO。即使我们的智能体幸运地按要求的顺序随机点击按钮，它也无法从单一图像中分辨出下一个需要点击的按钮。
- en: '![PIC](img/file182.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file182.png)'
- en: 'Figure 14.5: An example of an environment that the stateless agent could struggle
    to solve'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：一个无状态智能体可能难以解决的环境示例
- en: Despite the simplicity of this problem, we cannot use our RL methods to solve
    it, because MDP formalism is not applicable anymore. Such problems are called
    partially observable MDPs, or POMDPs (we briefly discussed these in Chapter [6](#)),
    and the usual approach for them is to allow the agent to keep some kind of state.
    The challenge here is to find the balance between keeping only minimal relevant
    information and overwhelming the agent with non-relevant information by adding
    everything into the observation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个问题很简单，我们仍然不能使用我们的强化学习方法来解决它，因为 MDP 的形式化不再适用。这样的問題被称为部分可观测的马尔可夫决策过程，或 POMDP（我们在第[6](#)章中简要讨论了这些），通常的处理方法是允许智能体保持某种状态。这里的挑战是找到平衡点，即在保持最小相关信息的同时，避免通过将所有内容都加入观察结果而让智能体被不相关信息淹没。
- en: 'Another issue that we can face with our example is that the data required to
    solve the problem might not be available in the image or could be in an inconvenient
    form. For example, two problems, click-tab ( [https://miniwob.farama.org/environments/click-tab/](https://miniwob.farama.org/environments/click-tab/))
    and click-checkboxes ([https://miniwob.farama.org/environments/click-checkboxes/](https://miniwob.farama.org/environments/click-checkboxes/)),
    are shown in Figure [14.6](#x1-261005r6):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个例子中可能面临的另一个问题是，解决问题所需的数据可能不在图像中，或者可能以不方便的形式出现。例如，两个问题，点击选项卡 ([https://miniwob.farama.org/environments/click-tab/](https://miniwob.farama.org/environments/click-tab/))
    和点击复选框 ([https://miniwob.farama.org/environments/click-checkboxes/](https://miniwob.farama.org/environments/click-checkboxes/))，如图
    [14.6](#x1-261005r6) 所示：
- en: '![PIC](img/file183.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file183.png)'
- en: 'Figure 14.6: An example of environments where the text description is important'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6：文本描述在某些环境中的重要性示例
- en: In the first one, you need to click on one of three tabs, but every time, the
    tab that needs to be clicked is randomly chosen. Which tab needs to be clicked
    is shown in a description (provided with an in-text field of observation and shown
    at the top of the environment’s page), but our agent sees only pixels, which makes
    it complicated to connect the tiny number at the top with the outcome of the random
    click result. The situation is even worse with the click-checkboxes problem, when
    several checkboxes with randomly generated text need to be clicked. One of the
    possible ways to prevent overfitting to the problem is to use some kind of optical
    character recognition (OCR) network to convert the image in the observation into
    text form. Another approach (which will be shown in the next section) is to mix
    the text description into the agent’s observations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，你需要点击三个选项卡中的一个，但每次需要点击的选项卡是随机选择的。需要点击哪个选项卡会在描述中显示（通过一个文本字段提供观察，并显示在环境页面的顶部），但我们的代理只能看到像素，这使得将顶部的数字与随机点击结果的输出联系起来变得复杂。对于点击复选框的问题，情况更为糟糕，因为需要点击几个带有随机生成文本的复选框。防止过拟合问题的一个可能方法是使用某种光学字符识别（OCR）网络，将观察中的图像转换为文本形式。另一种方法（将在下一节中展示）是将文本描述融入到代理的观察中。
- en: 'Yet another issue could be related to the dimensionality of the action space
    that the agent needs to explore. Even for single-click problems, the number of
    actions could be very large, so it can take a long time for the agent to discover
    how to behave. One of the possible solutions here is incorporating demonstrations
    into the training. For example, in Figure [14.7](#x1-261007r7), there is a problem
    called count-sides ([https://miniwob.farama.org/environments/count-sides/](https://miniwob.farama.org/environments/count-sides/)).
    The goal there is to click on the button that corresponds to the number of sides
    of the shape shown:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题可能与代理需要探索的动作空间的维度有关。即使是单次点击问题，动作的数量也可能非常庞大，因此代理可能需要很长时间才能发现如何执行操作。这里的一个可能解决方案是将示范引入训练中。例如，在图
    [14.7](#x1-261007r7) 中，有一个名为 count-sides ([https://miniwob.farama.org/environments/count-sides/](https://miniwob.farama.org/environments/count-sides/))
    的问题。目标是点击对应于形状边数的按钮：
- en: '![PIC](img/file184.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file184.png)'
- en: 'Figure 14.7: Examples of the count-sides environment'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：count-sides 环境示例
- en: This issue was addressed by adding human demonstrations into the training. In
    my experiments, training from scratch gave zero progress after a day of training.
    However, after adding a couple of dozen examples of correct clicks, it successfully
    solved the problem in 15 minutes of training. Of course, we could spend time fine-tuning
    the hyperparameters further, but still, the effect of the demonstrations is quite
    impressive. Later in this chapter, we will take a look at how we can record and
    inject human demonstrations to improve convergence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将人工示范加入训练，解决了这个问题。在我的实验中，从零开始训练经过一天的训练后没有任何进展。然而，在加入几十个正确点击的示范之后，代理在 15 分钟的训练时间内成功解决了这个问题。当然，我们可以花时间进一步微调超参数，但示范的效果确实令人印象深刻。本章后面我们将研究如何记录并注入人工示范以改善收敛性。
- en: Adding text description
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加文本描述
- en: As a first step to improve our clicker agent, we’ll add the text description
    of the problem into the model. I have already mentioned that some problems contain
    vital information that is provided in a text description, like the index of tabs
    that need to be clicked or the list of entries that the agent needs to check.
    The same information is shown at the top of the image observation, but pixels
    are not always the best representation of simple text.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进我们的点击器代理，第一步是将问题的文本描述添加到模型中。我已经提到过，一些问题包含在文本描述中提供的关键信息，比如需要点击的标签索引或代理需要检查的条目列表。这些信息也显示在图像观察的顶部，但像素并不总是简单文本的最佳表示。
- en: To take this text into account, we need to extend our model’s input from an
    image only to an image and text data. We worked with text in the previous chapter,
    so a recurrent neural network (RNN) is quite an obvious choice (maybe not the
    best for such a toy problem, but it is flexible and scalable).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑到这一点，我们需要将模型的输入从仅图片扩展到图片和文本数据。在上一章中我们已经处理了文本，因此递归神经网络（RNN）是一个显而易见的选择（也许对于这样的玩具问题来说不是最佳选择，但它具有灵活性和可扩展性）。
- en: Implementation
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: In this section, we will just focus on the most important points of the implementation.
    You will find the whole code in the Chapter16/wob_click_mm_train.py module. In
    comparison to our clicker model, a text extension doesn’t add too much.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将只关注实现的最重要部分。你可以在Chapter16/wob_click_mm_train.py模块中找到完整的代码。与我们的点击器模型相比，文本扩展并没有增加太多内容。
- en: 'First, we should ask MiniWoBClickWrapper to keep the text obtained from the
    observation. The complete source code of this class was shown earlier in this
    chapter, in the Grid actions section. To keep the text, we should pass keep_text=True
    to the wrapper constructor, which makes this class return a tuple with a NumPy
    array and text string, instead of just a NumPy array with the image. Then, we
    need to prepare our model to be able to process such tuples instead of a batch
    of NumPy arrays. This needs to be done in two places: in our agent (when we use
    the model to choose the action) and in the training code. To adapt the observation
    in a model-friendly way, we can use a special functionality of the PTAN library,
    called preprocessor. The core idea is very simple: preprocessor is a callable
    function that needs to convert the list of observations to a form that is ready
    to be passed to the model. By default, preprocessor converts the list of NumPy
    arrays into a PyTorch tensor and, optionally, copies it into GPU memory. However,
    sometimes, more sophisticated transformations are required, like in our case,
    when we need to pack the images into the tensor, but text strings require special
    handling. In that case, you can redefine the default preprocessor and pass it
    into the ptan.Agent class.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该要求MiniWoBClickWrapper保留从观察中获得的文本。本章前面已经展示了这个类的完整源代码，在Grid actions部分。为了保留文本，我们应该将keep_text=True传递给包装器构造函数，这样该类将返回一个包含NumPy数组和文本字符串的元组，而不仅仅是包含图像的NumPy数组。然后，我们需要准备我们的模型，以便能够处理这样的元组，而不是一批NumPy数组。这需要在两个地方完成：在我们的代理中（当我们使用模型来选择动作时）以及在训练代码中。为了以适合模型的方式适配观察，我们可以使用PTAN库的特殊功能，称为预处理器。其核心思想非常简单：预处理器是一个可调用的函数，需要将观察列表转换为可以传递给模型的形式。默认情况下，预处理器将NumPy数组列表转换为PyTorch张量，并可以选择将其复制到GPU内存中。然而，有时需要更复杂的转换，例如在我们的例子中，当我们需要将图像打包到张量中时，但文本字符串需要特殊处理。在这种情况下，你可以重新定义默认的预处理器并将其传递给ptan.Agent类。
- en: 'In theory, the preprocessor functionality could be moved into the model itself,
    thanks to PyTorch’s flexibility, but the default preprocessor simplifies our lives
    in cases when observations are just NumPy arrays. The following is the preprocessor
    class source code taken from the lib/model.py module:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，预处理器功能可以移到模型本身中，得益于PyTorch的灵活性，但默认的预处理器简化了我们的工作，特别是在观察仅仅是NumPy数组的情况下。以下是来自lib/model.py模块的预处理器类的源代码：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the constructor in the preceding code, we create a mapping from the token
    to the identifier (which will be dynamically extended) and create the tokenizer
    from the nltk package.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码的构造函数中，我们创建了一个从令牌到标识符的映射（该映射将动态扩展），并从nltk包中创建了分词器。
- en: 'Next, we have the __call__() method, which transforms the batch:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有了`__call__()`方法，它将转换批次：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The goal of our preprocessor is to convert a batch of (image, text) tuples
    into two objects: the first has to be a tensor with the image data of shape (batch_size,
    3, 210, 160), and the second has to contain the batch of tokens from text descriptions
    in the form of a packed sequence. The packed sequence is a PyTorch data structure
    suitable for efficient processing with an RNN. We discussed this in Chapter [13](ch017.xhtml#x1-21900013).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预处理器的目标是将一批（图像，文本）元组转换为两个对象：第一个是形状为（batch_size, 3, 210, 160）的图像数据张量，第二个包含来自文本描述的令牌批次，形式为打包序列。打包序列是
    PyTorch 数据结构，适用于高效的 RNN 处理。我们在第 [13](ch017.xhtml#x1-21900013) 章中讨论过这个问题。
- en: 'In fact, the batch could have two different forms: it could be a tuple with
    an image batch and a text batch, or it could be a list of tuples with individual
    (image, text tokens) samples. This happens because of the difference in VectorEnv
    handling of gym.Tuple observation space. But those details are not very relevant
    here; we just handle the difference by checking the type of the batch variable
    and performing the necessary processing.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，批次可以有两种不同的形式：它可以是一个包含图像批次和文本批次的元组，或者它可以是一个包含单个（图像，文本令牌）样本的元组列表。这是因为 `VectorEnv`
    对 gym.Tuple 观测空间的处理方式不同。但这些细节在这里并不太重要；我们只是通过检查批次变量的类型并进行必要的处理来处理这种差异。
- en: As the first step of our transformation, we tokenize text strings and convert
    every token into the list of integer IDs. Then, we sort our batch by decreasing
    the token length, which is a requirement of the underlying cuDNN library for efficient
    RNN processing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 作为转换的第一步，我们对文本字符串进行标记化，并将每个令牌转换为整数 ID 列表。然后，我们按令牌长度降序对批次进行排序，这是底层 cuDNN 库对高效
    RNN 处理的要求。
- en: 'Then, we convert the images into a tensor and the sequences into a padded sequence,
    which is a matrix of batch size × the length of the longest sequence. We saw this
    in the previous chapter:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将图像转换为张量，将序列转换为填充序列，这是一个批次大小 × 最长序列长度的矩阵。我们在前一章中见过这个：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following tokens_to_idx() function converts the list of tokens into a list
    of IDs:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的 `tokens_to_idx()` 函数将令牌列表转换为 ID 列表：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The tricky thing is that we don’t know in advance the size of the dictionary
    from the text descriptions. One approach would be to work on the character level
    and feed individual characters into the RNN, but it would result in sequences
    that are too long to process. The alternative solution is to hard-code a reasonable
    dictionary size, say 100 tokens, and dynamically assign token IDs to tokens that
    we have never seen before. In this implementation, the latter approach is used,
    but it might not be applicable to MiniWoB problems that contain randomly generated
    strings in the text description. As potential solutions for this issue, we can
    either use character-level tokenization or use a pre-defined dictionary.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，我们无法预先知道从文本描述中得到的词典大小。一种方法是按字符级别进行处理，将每个字符输入到 RNN 中，但这将导致序列过长，难以处理。另一种解决方案是硬编码一个合理的词典大小，比如
    100 个令牌，并为我们从未见过的令牌动态分配令牌 ID。在这个实现中，采用了后者的方法，但它可能不适用于包含随机生成字符串的 MiniWoB 问题文本描述。对此问题的潜在解决方案是，使用字符级别的标记化或使用预定义的词典。
- en: 'Now, let’s take a look at our model class:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下我们的模型类：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The difference is in a new embedding layer, which converts integer token IDs
    into dense token vectors and a long short-term memory (LSTM) RNN. The outputs
    from the convolution and RNN layers are concatenated and fed into the policy and
    value heads, so the dimensionality of their input is the image and text features
    combined.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于一个新的嵌入层，它将整数令牌 ID 转换为密集的令牌向量，并且使用长短期记忆（LSTM）RNN。卷积层和 RNN 层的输出被拼接并输入到策略和值头部，因此它们输入的维度是图像和文本特征的组合。
- en: 'This function performs the concatenation of the image and RNN features into
    a single tensor:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将图像和 RNN 特征拼接成一个单一的张量：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, in the forward() function, we expect two objects prepared by the preprocessor:
    a tensor with input images and packed sequences of the batch:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 `forward()` 函数中，我们期望预处理器准备的两个对象：一个包含输入图像的张量和批次的打包序列：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Images are processed with convolutions and text data is fed through the RNN;
    then, the results are concatenated, and the policy and value results are calculated.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通过卷积处理，文本数据则输入到 RNN 中；然后，将结果拼接，并计算策略和值结果。
- en: That’s most of the new code. The training Python script, wob_click_mm_train.py,
    is mostly a copy of wob_click_train.py, with just the small modifications in the
    wrapper creation, a different model, and preprocessor.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大部分新代码。训练的Python脚本wob_click_mm_train.py，基本上是wob_click_train.py的复制版，仅在包装器创建、模型和预处理器上做了些许修改。
- en: Results
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'I ran several experiments in the click-button environment ( [https://miniwob.farama.org/environments/click-button/](https://miniwob.farama.org/environments/click-button/))
    that have the goal of making a selection between several random buttons. In Figure [14.8](#x1-264002r8),
    several situations in this environment are shown:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我在点击按钮环境中进行了多个实验（[https://miniwob.farama.org/environments/click-button/](https://miniwob.farama.org/environments/click-button/)），目标是从多个随机按钮中做出选择。在图 [14.8](#x1-264002r8)中，展示了该环境中的几种情况：
- en: '![PIC](img/file185.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file185.png)'
- en: 'Figure 14.8: Tasks in the click-button environment'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：点击按钮环境中的任务
- en: As shown in Figure [14.9](#x1-264003r9), after 3 hours of training, the model
    was able to learn how to click (the average count of steps in episodes was reduced
    to 5-7) and get to an average reward of 0.2\. But subsequent training had no visible
    effect. It might be an indication that the hyperparameters have to be tuned, or
    of the ambiguity of the environment. In this case, I noticed that this environment
    sometimes shows several buttons with the same title, but only one of them gives
    a positive reward. An example of this is shown in the first section of Figure [14.8](#x1-264002r8),
    where two identical Submit buttons are present.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [14.9](#x1-264003r9)所示，经过3小时的训练，模型已经学会了如何点击（回合中的平均步骤计数减少到5-7），并获得了平均奖励0.2\。但随后训练没有产生明显效果。这可能表明超参数需要调整，或者是环境的模糊性。在这种情况下，我注意到该环境有时会显示多个相同标题的按钮，但只有一个按钮给出正奖励。图 [14.8](#x1-264002r8)的第一部分中就有这样的例子，那里有两个相同的“提交”按钮。
- en: '![PIC](img/B22150_14_09.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_14_09.png)'
- en: 'Figure 14.9: Training reward (left) and count of steps in episodes (right)
    on click-button'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：点击按钮中的训练奖励（左）和回合中的步骤计数（右）
- en: Another environment in which the text description is important is click-tab,
    which demands the agent to click on a specific tab, chosen randomly. Screenshots
    are shown in Figure [14.10](#x1-264005r10).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个文本描述重要的环境是点击标签（click-tab），要求智能体随机点击一个特定的标签。截图见图 [14.10](#x1-264005r10)。
- en: '![PIC](img/file188.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file188.png)'
- en: 'Figure 14.10: Tasks in the click-tab environment'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：点击标签环境中的任务
- en: In this environment, the training was not successful, which is a bit strange,
    as the task looks easier than click-button (the position of the place to click
    is fixed). Most likely, hyperparameter tuning is required. This is another interesting
    challenge that you can try to address through experimentation, using the knowledge
    you’ve gained so far.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，训练并不成功，这有点奇怪，因为这个任务看起来比点击按钮更简单（点击的位置是固定的）。很可能需要进行超参数调优。这是另一个有趣的挑战，你可以通过实验来解决，运用你到目前为止获得的知识。
- en: Human demonstrations
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类示范
- en: 'In order to improve the training process, let’s try to incorporate human demonstrations.
    The idea behind demonstrations is simple: to help our agent to discover the best
    way to solve the task, we show it some examples of actions that we think are required
    for the problem. Those examples might not be the best solution or not 100% accurate,
    but they should be good enough to show the agent promising directions to explore.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善训练过程，我们尝试加入人类示范。示范的思想很简单：为了帮助我们的智能体发现解决任务的最佳方法，我们展示一些我们认为是解决问题所需的行为示例。这些示例可能不是最佳的解决方案，也不完全准确，但它们应该足够好，能为智能体展示出有前景的探索方向。
- en: In fact, this is a very natural thing to do, as all human learning is based
    on some prior examples given by a teacher in class, parents, or other people.
    Those examples could be in a written form (for example, recipe books) or given
    as demonstrations that you need to repeat several times to get right (for example,
    dance classes). Such forms of training are much more effective than random searches.
    Just imagine how complicated and lengthy it would be to learn how to clean your
    teeth by trial and error alone. Of course, there is a danger from learning how
    to follow demonstrations, which could be wrong or not the most efficient way to
    solve the problem; but overall, it’s much more effective than a random search.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这是一个非常自然的做法，因为所有人类学习都是基于一些由老师、父母或其他人提供的先前示例。这些示例可以是书面形式的（例如，食谱书），或者是以示范的形式给出，你需要多次重复才能掌握（例如，舞蹈课程）。这种形式的训练比随机搜索更有效。试想一下，如果只通过试错来学习如何刷牙，那会有多么复杂和漫长。当然，依赖示范学习也有风险，示范可能是错误的，或者不是解决问题的最有效方式；但总体来说，它比随机搜索要有效得多。
- en: 'All our previous examples followed this workflow:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的所有示例都遵循了这个工作流程：
- en: They used zero prior knowledge and started with random weight initializations,
    which caused random actions to be performed at the beginning of the training.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们没有使用任何先验知识，而是从随机权重初始化开始，这导致在训练初期执行的是随机动作。
- en: After some iterations, the agent discovered that some actions in some states
    give more promising results (via the Q-value or policy with the higher advantage)
    and started to prefer those actions over the others.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过几次迭代后，智能体发现某些状态下的某些动作能带来更有前景的结果（通过更高优势的Q值或策略），并开始偏好这些动作而不是其他的动作。
- en: Finally, this process led to a more or less optimal policy, which gave the agent
    a high reward at the end.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，这个过程导致了一个或多或少的最优策略，使得智能体在结束时获得了高奖励。
- en: This worked well when our action space dimensionality was low and the environment’s
    behavior wasn’t very complex, but just doubling the action count caused at least
    twice the observations needed. In the case of our clicker agent, we have 256 different
    actions corresponding to 10 × 10 grids in the active area, which is 128 times
    more actions than we had in the CartPole environment. It is not surprising that
    the training process is lengthy and may fail to converge at all.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的动作空间维度较低且环境行为不太复杂时，这种方法效果很好，但仅仅是将动作数量翻倍，就需要至少两倍的观察量。以我们的点击器智能体为例，我们在活动区域有256种不同的动作，对应10
    × 10的网格，这比CartPole环境中的动作多了128倍。由此可见，训练过程时间很长，并且可能根本无法收敛。
- en: This issue of dimensionality can be addressed in various ways, like smarter
    exploration methods, training with better sampling efficiency (one-shot training),
    incorporating prior knowledge (transfer learning), and other means. There is a
    lot of research activity focused on making RL better and faster, and we can be
    sure that many breakthroughs are ahead. In this section, we will try the more
    traditional approach of incorporating the demonstration recorded by humans into
    the training process.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 维度问题可以通过多种方式解决，比如更智能的探索方法、训练时更高效的采样（一次性训练）、引入先验知识（迁移学习）等手段。当前有大量研究致力于使强化学习更好、更快，我们可以确信，许多突破正在前方等待着我们。在本节中，我们将尝试通过将人类记录的示范融入训练过程来采用更传统的方法。
- en: 'You might remember our discussion about on-policy and off-policy methods (which
    were discussed in Chapter [4](ch008.xhtml#x1-740004) and Chapter [8](ch012.xhtml#x1-1240008)).
    This is very relevant to our human demonstrations because, strictly speaking,
    we can’t use off-policy data (human observation-action pairs) with an on-policy
    method (A3C in our case). That is due to the nature of on-policy methods: they
    estimate the policy gradients using the samples gathered from the current policy.
    If we just push human-recorded samples into the training process, the estimated
    gradient will be relevant for a human policy, but not our current policy given
    by the neural network (NN). To solve this issue, we need to cheat a bit and look
    at our problem from the supervised learning angle. To be concrete, we will use
    the log-likelihood objective to push our NN toward taking actions based on demonstrations.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们关于策略内和策略外方法的讨论（这些内容在第 [4](ch008.xhtml#x1-740004) 章和第 [8](ch012.xhtml#x1-1240008)
    章中有提到）。这与我们的人类演示非常相关，因为严格来说，我们不能将策略外数据（人类观察-行动对）与策略内方法（在我们这里是 A3C）一起使用。这是因为策略内方法的性质：它们使用当前策略收集的样本来估计策略梯度。如果我们仅仅将人类记录的样本输入到训练过程中，估计出的梯度将适用于人类策略，而不适用于我们的神经网络（NN）所给出的当前策略。为了解决这个问题，我们需要稍微“作弊”一下，从监督学习的角度来看待我们的任务。具体来说，我们将使用对数似然目标来推动我们的神经网络根据演示来采取行动。
- en: With this, we’re not replacing RL with supervised learning. Rather, we’re reusing
    supervised learning techniques to help our RL methods. Fundamentally, this isn’t
    the first time we’ve done something similar; for instance, the training of the
    value function in Q-learning is purely supervised learning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们并不是用监督学习取代强化学习。相反，我们是利用监督学习技术来帮助我们的强化学习方法。从根本上说，这并不是我们第一次做类似的事情；例如，Q-learning
    中的价值函数训练就是纯粹的监督学习。
- en: 'Before we can go into the implementation details, we need to address a very
    important question: how do we obtain the demonstrations in the most convenient
    form?'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实现细节之前，我们需要解决一个非常重要的问题：我们如何以最方便的形式获取演示？
- en: Recording the demonstrations
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录演示
- en: Before MiniWoB++ and the transition to Selenium, recording a demonstration was
    technically challenging. In particular, the VNC protocol has to be captured and
    decoded to be able to extract screenshots of the browser and the actions executed
    by the user. In the previous edition of the book, I provided my own version of
    the VNC protocol parser to record the demonstrations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MiniWoB++ 和过渡到 Selenium 之前，记录演示在技术上是具有挑战性的。特别是，必须捕获并解码 VNC 协议，才能提取浏览器的屏幕截图以及用户执行的操作。在本书的前一版中，我提供了自己的
    VNC 协议解析器版本来记录演示。
- en: Luckily, those challenges are mostly gone now. There is no VNC anymore and the
    browser has been started in the local process (before, it was inside the Docker
    container), so we can communicate with it almost directly.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这些挑战现在大多已经消失。现在没有 VNC 了，浏览器已经在本地进程中启动（之前是在 Docker 容器内），所以我们几乎可以直接与之通信。
- en: Farama MiniWoB++ is shipped with a Python script that can capture the demonstrations
    in a JSON file. This script can be started with the python -m miniwob.scripts.record
    command and is documented at [https://miniwob.farama.org/content/demonstrations/](https://miniwob.farama.org/content/demonstrations/).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Farama MiniWoB++ 配有一个 Python 脚本，可以将演示记录到 JSON 文件中。该脚本可以通过 `python -m miniwob.scripts.record`
    命令启动，详细文档请见 [https://miniwob.farama.org/content/demonstrations/](https://miniwob.farama.org/content/demonstrations/)。
- en: 'Unfortunately, it has a limitation: in observations, it captures only the DOM
    structure of the webpage and has no pixel-level information. As examples in this
    chapter make heavy use of pixels, demonstrations recorded by this script are useless.
    To overcome this, I implemented my own version of a tool to record demonstrations
    that include pixels from the browser. It is called Chapter14/record_demo.py and
    can be started as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，它有一个局限性：在观察数据中，它仅捕捉网页的 DOM 结构，并没有像素级的信息。由于本章的示例大量使用像素，使用此脚本记录的演示是无效的。为了解决这个问题，我实现了自己版本的工具来记录包括浏览器像素在内的演示。它被命名为
    Chapter14/record_demo.py，可以通过以下方式启动：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This command starts the environment with render_mode=’human’, which shows the
    browser window and allows you to communicate with the page. In the background,
    it records the observations (with screenshots) and, when the episode is done,
    it joins screenshots to your actions and stores everything in a JSON file in the
    directory given by the -o command-line option. Using the -g command-line option
    allows you to change the environment, and the -d parameter sets the delay in seconds
    between the episodes. If the -d option is not given, you need to press Enter in
    the console to start a new episode. The following screenshot shows the process
    of recording a demonstration:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令以render_mode='human'启动环境，显示浏览器窗口并允许与页面进行交互。在后台，它记录观测数据（带有截图），当回合结束时，它将截图与您的动作结合，并将所有内容存储在由-o命令行选项指定的目录中的JSON文件中。使用-g命令行选项可以更改环境，-d参数设置回合之间的延迟时间（秒）。如果没有给定-d选项，您需要在控制台按Enter键来开始一个新回合。以下截图显示了记录示范的过程：
- en: '![PIC](img/B22150_14_11.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_14_11.png)'
- en: 'Figure 14.11: Recording a human demonstration for tic-tac-toe'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：记录井字棋的人类示范
- en: In the Chapter14/demos directory, I stored the demonstrations used for experiments,
    but you, of course, can record your own demonstrations using the provided script.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在Chapter14/demos目录中，我存储了用于实验的示范，但当然，您可以使用提供的脚本记录自己的示范。
- en: Training with demonstrations
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用示范进行训练
- en: 'Now that we know how to record the demonstration data, we have only one question
    unanswered: how does our training process need to be modified to incorporate human
    demonstrations? The simplest solution, which nevertheless works surprisingly well,
    is to use the log-likelihood objective that we used in training the cross-entropy
    method in Chapter [4](ch008.xhtml#x1-740004). To do this, we need to look at our
    A3C model as a classification problem producing the classification of input observations
    in its policy head. In its simplest form, the value head will be left untouched,
    but, in fact, it won’t be hard to train it: we know the rewards obtained during
    the demonstrations, so what is needed is to calculate the discounted reward from
    every observation until the end of the episode.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何记录示范数据，但还有一个问题没有解答：我们的训练过程需要如何修改才能融入人类示范？最简单的解决方案，尽管如此非常有效，是使用我们在第[4](ch008.xhtml#x1-740004)章中训练交叉熵方法时使用的对数似然目标。为了实现这一点，我们需要将我们的A3C模型视为一个分类问题，在其策略头部产生输入观测的分类。在其最简单的形式中，值头部将保持不变，但实际上，训练它并不困难：我们知道在示范过程中获得的奖励，因此需要做的就是计算每个观测直到回合结束的折扣奖励。
- en: 'To check how it was implemented, let’s look at the relevant code pieces in
    Chapter16/wob_click_train.py. First, we can pass the directory with the demonstration
    data by passing the demo <DIR> option in the command line. This will enable the
    branch shown in the following code block, where we load the demonstration samples
    from the specified directory. The demos.load_demo_dir() function automatically
    loads demonstrations from JSON files in the given directory and converts them
    into ExperienceFirstLast instances:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查它是如何实现的，让我们看一下Chapter16/wob_click_train.py中的相关代码片段。首先，我们可以通过在命令行中传递demo <DIR>选项来传递包含示范数据的目录。这将启用以下代码块中显示的分支，其中我们从指定目录加载示范样本。demos.load_demo_dir()函数会自动从给定目录中的JSON文件加载示范，并将它们转换为ExperienceFirstLast实例：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The second piece of code relevant to demonstration training is inside the training
    loop and is executed before any normal batch. The training from demonstrations
    is performed with some probability (by default, it is 0.5) and specified by the
    DEMO_PROB hyperparameter:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与示范训练相关的第二段代码在训练循环内部，并在任何正常批次之前执行。示范训练是以一定的概率进行的（默认情况下为0.5），由DEMO_PROB超参数指定：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The logic is simple: with probability DEMO_PROB, we sample BATCH_SIZE samples
    from our demonstration data and perform a round of training of our network on
    the data in the batch.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑很简单：以概率DEMO_PROB，我们从示范数据中采样BATCH_SIZE个样本，并对该批数据进行一轮训练。
- en: 'The actual training, which is very simple and straightforward, is performed
    by the model.train_demo() function:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的训练非常简单明了，通过model.train_demo()函数执行：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We split our batch on the observation and the actions list, preprocess the observations
    to convert them into a PyTorch tensor, and place them on the GPU. We then ask
    our A3C network to return the policy and calculate the cross-entropy loss between
    the result and the desired actions. From an optimization point of view, we’re
    pushing our network toward the actions taken in the demonstrations.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将观察和动作列表拆分开来，预处理观察以将其转换为 PyTorch 张量，并将它们放置在 GPU 上。然后，我们请求 A3C 网络返回策略，并计算结果与目标动作之间的交叉熵损失。从优化的角度来看，我们正在推动网络朝着演示中采取的动作前进。
- en: Results
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'To check the effect of demonstrations, I performed two sets of training on
    the count-sides problem with the same hyperparameters: one was done without demonstrations,
    and another used 25 demonstration episodes, which are available in the demos/count-sides
    directory.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查演示的效果，我在计数侧面问题上进行了两组训练，使用相同的超参数：一组没有演示，另一组使用了 25 个演示回合，这些回合可以在 demos/count-sides
    目录中找到。
- en: The difference was dramatic. Training performed from scratch reached the best
    mean reward of -0.4 after 12 hours of training and 4 million frames without any
    significant improvement in the training dynamics. On the other hand, training
    with demonstrations was able to get to the average reward of 0.5 just after 30,000
    training frames, which took 8 minutes. Figure [14.12](#x1-268002r12) shows the
    reward and the count of steps.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 差异非常显著。从零开始的训练在 12 小时的训练和 400 万帧后，达到了最佳平均奖励 -0.4，但在训练动态上没有任何显著的改善。另一方面，使用演示的训练在仅仅
    30,000 帧的训练后，达到了平均奖励 0.5，这仅用了 8 分钟。图 [14.12](#x1-268002r12) 显示了奖励和步骤数量。
- en: '![PIC](img/B22150_14_12.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_14_12.png)'
- en: 'Figure 14.12: Training reward (left) and count of steps in episodes (right)
    on count-sides with demonstrations'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：训练奖励（左）和在带有演示的计数侧面的步骤数量（右）
- en: 'A more challenging problem I experimented with is the Tic Tac Toe game, available
    as the tic-tac-toe environment. Figure [14.13](#x1-268003r13) shows the process
    of one of the demo games I recorded (available in the demos/tic-tac-toe directory).
    The dot shows where the click was performed:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我实验过的一个更具挑战性的问题是井字游戏，可以在 tic-tac-toe 环境中找到。图 [14.13](#x1-268003r13) 展示了我记录的一个演示游戏过程（可在
    demos/tic-tac-toe 目录中找到）。圆点表示点击发生的位置：
- en: '![PIC](img/file192.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file192.png)'
- en: 'Figure 14.13: Demonstration TicTacToe game'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：演示井字游戏
- en: After two hours of training, the best average reward reached was 0.05, which
    means that the agent can win some games, but some are lost or end up in a draw.
    In Figure [14.14](#x1-268005r14), plots with reward dynamics and the count of
    episode steps are shown.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 经过两小时的训练，最佳平均奖励达到了 0.05，这意味着代理可以赢得一些游戏，但有些游戏会失败或以平局结束。在图 [14.14](#x1-268005r14)
    中，展示了奖励动态和回合步骤数量的图表。
- en: '![PIC](img/B22150_14_14.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_14_14.png)'
- en: 'Figure 14.14: Training reward (left) and count of steps in episodes (right)
    on tic-tac-toe with demonstrations'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.14：带有演示的井字游戏训练奖励（左）和回合步骤数量（右）
- en: Things to try
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试的事项
- en: 'In this chapter, we only started playing with MiniWoB++ by looking at some
    of the easiest environments from the full set of over 100 problems, so there is
    plenty of uncharted territory ahead. If you want to practice, there are several
    items you can experiment with:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们只是开始通过查看 MiniWoB++ 中最简单的环境来进行实验，整个集合包含了 100 多个问题，因此前方还有许多未知的领域。如果你想练习，以下是你可以尝试的几个项目：
- en: Testing the robustness of demonstrations to noisy clicks.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试演示在噪声点击下的鲁棒性。
- en: The action space for the clicking approach could be improved by predicting the
    x and y coordinates of the place to click.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击方法的动作空间可以通过预测点击位置的 x 和 y 坐标来改进。
- en: DOM data could be used instead of (or in addition to) screen pixels. Then, the
    prediction will be the element of the tree to be clicked.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 DOM 数据来替代（或补充）屏幕像素。然后，预测将是要点击的树的元素。
- en: Try other problems. There is a wide variety of them, requiring keyboard events
    to be generated, the sequence of actions planned, etc.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试其他问题。这些问题种类繁多，要求生成键盘事件、规划动作序列等。
- en: Very recently, the LaVague project was published ([https://github.com/lavague-ai/LaVague](https://github.com/lavague-ai/LaVague)),
    which uses LLMs for web automation. Their approach is to ask an LLM to generate
    Selenium Python code to perform specific tasks. It will be very interesting to
    check it against MiniWoB++ problems.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近，LaVague 项目发布了（[https://github.com/lavague-ai/LaVague](https://github.com/lavague-ai/LaVague)），该项目使用大型语言模型（LLMs）进行网页自动化。他们的方法是要求
    LLM 生成 Selenium Python 代码以执行特定任务。将其与 MiniWoB++ 问题进行对比将非常有趣。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you saw the practical application of RL methods for browser
    automation and used the MiniWoB++ benchmark. I believe that browser automation
    (and communicating with software humans are using in general) is an important
    milestone in future AI development.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了强化学习方法在浏览器自动化中的实际应用，并使用了 MiniWoB++ 基准。我相信，浏览器自动化（以及与人类使用的软件进行交流）是未来人工智能发展的一个重要里程碑。
- en: This chapter concludes Part 3 of the book. The next part will be devoted to
    more complicated and recent methods related to continuous action spaces, non-gradient
    methods, and other more advanced methods of RL.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了本书的第三部分。下一部分将致力于更复杂和更新的与连续动作空间、非梯度方法以及其他更先进的强化学习方法相关的内容。
- en: In the next chapter, we will take a look at continuous control problems, which
    are an important subfield of RL, both theoretically and practically.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论连续控制问题，这是强化学习领域中的一个重要子领域，既具有理论意义也具有实践意义。
- en: Leave a Review!
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoy it!
    Your feedback is invaluable and helps us improve and grow. Once you’ve completed
    reading it, please take a moment to leave an Amazon review; it will only take
    a minute, but it makes a big difference for readers like you. Scan the QR code
    below to receive a free ebook of your choice. [https://packt.link/NzOWQ](https://packt.link/NzOWQ)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您购买本书，感谢 Packt 出版社的支持——我们希望您喜欢这本书！您的反馈对我们来说非常宝贵，能够帮助我们改进和成长。阅读完后，请花点时间在亚马逊上留下评论；这只需一分钟，但对像您这样的读者来说却意义重大。扫描下面的二维码，免费领取您选择的电子书。[https://packt.link/NzOWQ](https://packt.link/NzOWQ)
- en: '![PIC](img/file3.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file3.png)'
- en: Part 4
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分
- en: Advanced RL
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级强化学习（RL）
