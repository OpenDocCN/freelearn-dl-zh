- en: '*Chapter 7*: Captioning Images with CNNs and RNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：使用CNN和RNN给图像加上字幕'
- en: 'Equipping neural networks with the ability to describe visual scenes in a human-readable
    fashion has to be one of the most interesting yet challenging applications of
    deep learning. The main difficulty arises from the fact that this problem combines
    two major subfields of artificial intelligence: **Computer Vision** (**CV**) and
    **Natural Language Processing** (**NLP**).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 赋予神经网络描述视觉场景的能力以人类可读的方式，必定是深度学习中最有趣但也最具挑战性的应用之一。主要困难在于，这个问题结合了人工智能的两个主要子领域：**计算机视觉**（**CV**）和**自然语言处理**（**NLP**）。
- en: The architectures of most image captioning networks use a **Convolutional Neural
    Network** (**CNN**) to encode images in a numeric format so that they're suitable
    for the consumption of the decoder, which is typically a **Recurrent Neural Network**
    (**RNN**). This is a kind of network specialized in learning from sequential data,
    such as time series, video, and text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数图像字幕网络的架构使用**卷积神经网络**（**CNN**）来将图像编码为数字格式，以便解码器消费，解码器通常是**递归神经网络**（**RNN**）。这是一种专门用于学习序列数据（如时间序列、视频和文本）的网络。
- en: As we'll see in this chapter, the challenges of building a system with these
    capabilities start with preparing the data, which we'll cover in the first recipe.
    Then, we'll implement an image captioning solution from scratch. In the third
    recipe, we'll use this model to generate captions for our own pictures. Finally,
    in the fourth recipe, we'll learn how to include an attention mechanism in our
    architecture so that we can understand what parts of the image the network is
    looking at when generating each word in the output caption.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这一章中将看到的，构建具有这些能力的系统的挑战从准备数据开始，我们将在第一个实例中讨论这一点。然后，我们将从头开始实现一个图像字幕解决方案。在第三个实例中，我们将使用这个模型为我们自己的图片生成字幕。最后，在第四个实例中，我们将学习如何在我们的架构中包含注意力机制，以便我们可以理解网络在生成输出字幕中每个单词时看到图像的哪些部分。
- en: Pretty interesting, don't you agree?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 相当有趣，你同意吗？
- en: 'Specifically, we''ll cover the following recipes in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中我们将涵盖以下实例：
- en: Implementing a reusable image caption feature extractor
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现可重复使用的图像字幕特征提取器
- en: Implementing an image captioning network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现图像字幕网络
- en: Generating captions for your own photos
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您自己的照片生成字幕
- en: Implementing an image captioning network on COCO with attention
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在COCO上实现带注意力的图像字幕网络
- en: Let's get started!
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Image captioning is a problem that requires vast amounts of resources in terms
    of memory, storage, and computing power. My recommendation is that you use a cloud-based
    solution such as AWS or FloydHub to run the recipes in this chapter unless you
    have sufficiently capable hardware. As expected, a GPU is of paramount importance
    to complete the recipes in this chapter. In the *Getting ready* section of each
    recipe, you''ll find what you''ll need to prepare. The code of this chapter is
    available here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字幕是一个需要大量内存、存储和计算资源的问题。我建议您使用像AWS或FloydHub这样的云解决方案来运行本章中的实例，除非您有足够强大的硬件。如预期的那样，GPU对于完成本章中的实例至关重要。在每个实例的“准备就绪”部分，您将找到所需准备的内容。本章的代码在此处可用：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 点击以下链接查看“代码实战”视频：
- en: '[https://bit.ly/3qmpVme](https://bit.ly/3qmpVme).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3qmpVme](https://bit.ly/3qmpVme)。'
- en: Implementing a reusable image caption feature extractor
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现可重复使用的图像字幕特征提取器
- en: The first step of creating an image captioning, deep learning-based solution
    is to transform the data into a format that can be used by certain networks. This
    means we must encode images as vectors, or tensors, and the text as embeddings,
    which are vectorial representations of sentences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 创建基于深度学习的图像字幕解决方案的第一步是将数据转换为可以被某些网络使用的格式。这意味着我们必须将图像编码为向量或张量，将文本编码为嵌入，即句子的向量表示。
- en: In this recipe, we will implement a customizable and reusable component that
    will allow us to preprocess the data we'll need to implement an image captioner
    beforehand, thus saving us tons of time later on in the process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将实现一个可自定义和可重用的组件，允许我们提前预处理实现图像标题生成器所需的数据，从而节省后续过程中大量时间。
- en: Let's begin!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The dependencies we need are `tqdm` (to display a nice progress bar) and `Pillow`
    (to load and manipulate images using TensorFlow''s built-in functions):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的依赖是`tqdm`（用于显示漂亮的进度条）和`Pillow`（用于使用TensorFlow的内置函数加载和处理图像）：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use the `Flickr8k` dataset, which is available on `~/.keras/datasets/flickr8k`
    folder.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Flickr8k`数据集，该数据集位于`~/.keras/datasets/flickr8k`文件夹中。
- en: 'Here are some sample images:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例图像：
- en: '![Figure 7.1 – Sample images from Flickr8k'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – 来自Flickr8k的示例图像'
- en: '](img/B14768_07_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_001.jpg)'
- en: Figure 7.1 – Sample images from Flickr8k
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 来自Flickr8k的示例图像
- en: With that, we are good to go!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就可以开始了！
- en: How to do it…
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Follow these steps to create a reusable feature extractor for image captioning
    problems:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建一个可重用的特征提取器，用于图像标题问题：
- en: 'Import all the necessary dependencies:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的依赖项：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the `ImageCaptionFeatureExtractor` class and its constructor:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`ImageCaptionFeatureExtractor`类及其构造函数：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we must receive the path where the outputs will be stored, along with
    the tokens that we''ll use to delimit the start and end of a text sequence. We
    must also take the input shape of the feature extractor as an argument. Next,
    let''s store these values as members:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须接收输出存储路径，以及我们将用于分隔文本序列起始和结束的标记。我们还必须将特征提取器的输入形状作为参数。接下来，让我们将这些值存储为成员：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we don''t receive any `feature_extractor`, we''ll use `VGG16` by default.
    Next, define a public method that will extract the features from an image, given
    its path:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有接收到任何`feature_extractor`，我们将默认使用`VGG16`。接下来，定义一个公共方法，该方法根据图像路径提取图像的特征：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In order to clean the captions, we must get rid of all the punctuation characters
    and single-letter words (such as *a*). The `_clean_captions()` method performs
    this task, and also adds special tokens; that is, `self.start_token` and `self.end_token`:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了清理标题，我们必须去除所有标点符号和单个字母的单词（如*a*）。`_clean_captions()`方法执行了这个任务，并且还添加了特殊标记，也就是`self.start_token`和`self.end_token`：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We also need to compute the length of the longest caption, which we can do
    with the `_get_max_seq_length()` method. This is defined as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要计算最长标题的长度，可以通过`_get_max_seq_length()`方法来实现。方法定义如下：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define a public method, `extract_features()`, which receives a list of image
    paths and captions and uses them to extract features from both the images and
    text sequences:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个公共方法`extract_features()`，它接收一个包含图像路径和标题的列表，并利用这些数据从图像和文本序列中提取特征：
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that both lists must be of the same size. The next step is to clean the
    captions, compute the maximum sequence length, and fit a tokenizer to all the
    captions:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，两个列表必须具有相同的大小。接下来的步骤是清理标题，计算最大序列长度，并为所有标题适配一个分词器：
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''ll iterate over each image path and caption pair, extracting the features
    from the image. Then, we''ll save an entry in our `data_mapping` `dict`, associating
    the image ID (present in `image_path`) with the corresponding visual features
    and clean caption:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将遍历每一对图像路径和标题，从图像中提取特征。然后，我们将在`data_mapping`的`dict`中保存一个条目，将图像ID（存在于`image_path`中）与相应的视觉特征和清理后的标题相关联：
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''ll save this `data_mapping` to disk, in pickle format:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把这个`data_mapping`保存到磁盘，以pickle格式存储：
- en: '[PRE10]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We''ll complete this method by creating and storing the sequences that''ll
    be inputted to an image captioning network in the future:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过创建和存储将在未来输入图像标题网络的序列来完成此方法：
- en: '[PRE11]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following method creates the input and output sequences that will be used
    to train an image captioning model (see the *How it works…* section for a deeper
    explanation). We will start by determining the number of output classes, which
    is the vocabulary size plus one (to account for out-of-vocabulary tokens). We
    must also define the lists where we''ll store the sequences:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下方法创建了用于训练图像标题模型的输入和输出序列（详细说明请见*如何实现……*部分）。我们将从确定输出类别数开始，这个数值是词汇大小加一（以便考虑超出词汇表的标记）。我们还必须定义存储序列的列表：
- en: '[PRE12]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we''ll iterate over each features-caption pair. We will transform the
    caption from a string into a sequence of numbers that represents the words in
    the sentence:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将迭代每个特征-标题对。我们将把标题从字符串转换为表示句子中单词的数字序列：
- en: '[PRE13]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we''ll generate as many input sequences as there are words in a caption.
    Each input sequence will be used to generate the next word in the sequence. Therefore,
    for a given index, `i`, the input sequence will be all the elements up to `i-1`,
    while the corresponding output sequence, or label, will be the one-hot encoded
    element at `i` (the next word). To ensure all the input sequences are the same
    length, we must pad them:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将生成与标题中单词数量相同的输入序列。每个输入序列将用于生成序列中的下一个单词。因此，对于给定的索引`i`，输入序列将是到`i-1`的所有元素，而相应的输出序列或标签将是在`i`处的独热编码元素（即下一个单词）。为了确保所有输入序列的长度相同，我们必须对它们进行填充：
- en: '[PRE14]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then add the visual feature vector, the input sequence, and the output sequence
    to the corresponding lists:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将视觉特征向量、输入序列和输出序列添加到相应的列表中：
- en: '[PRE15]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we must write the sequences to disk, in pickle format:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须将序列以pickle格式写入磁盘：
- en: '[PRE16]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s define the paths to the `Flickr8k` images and captions:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义`Flickr8k`图像和标题的路径：
- en: '[PRE17]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create an instance of the feature extractor class we just implemented:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建我们刚刚实现的特征提取器类的实例：
- en: '[PRE18]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'List all the image files in the `Flickr8k` dataset:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出`Flickr8k`数据集中的所有图像文件：
- en: '[PRE19]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Read the contents of the captions file:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取标题文件的内容：
- en: '[PRE20]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we must create a map that will associate each image with multiple captions.
    The key is the image ID, while the value is a list of all captions associated
    with such an image:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须创建一个映射，将每个图像与多个标题关联起来。键是图像ID，而值是与该图像相关的所有标题的列表：
- en: '[PRE21]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will only keep one caption per image:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将仅保留每个图像的一个标题：
- en: '[PRE22]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we must use our extractor to produce the data mapping and corresponding
    input sequences:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须使用我们的提取器生成数据映射和相应的输入序列：
- en: '[PRE23]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This process may take a while. After several minutes, we should see the following
    files in the output path:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个过程可能需要一些时间。几分钟后，我们应该在输出路径中看到以下文件：
- en: '[PRE24]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We'll see how this all works in the next section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将详细介绍这一切是如何工作的。
- en: How it works…
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: In this recipe, we learned that one of the keys to creating a good image captioning
    system is to put the data in a suitable format. This allows the network to learn
    how to describe, with text, what's happening in a visual scenario.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们学到了创建良好的图像字幕系统的关键之一是将数据放入适当的格式中。这使得网络能够学习如何用文本描述视觉场景中发生的事情。
- en: There are many ways to frame an image captioning problem, but the most popular
    and effective way is to use each word to generate the next word in the caption.
    This way, we'll construct the sentence, word by word, passing each intermediate
    output as the input to the next cycle. (This is how **RNNs** work. To read more
    about them, refer to the *See also* section.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以构建图像字幕问题，但最流行和有效的方法是使用每个单词来生成标题中的下一个单词。这样，我们将逐词构造句子，通过每个中间输出作为下一个周期的输入传递。
    （这就是**RNNs**的工作原理。要了解更多信息，请参阅*参考*部分。）
- en: You might be wondering how we pass the visual information to the network. This
    is where the feature extraction step is crucial, because we convert each image
    in our dataset into a numeric vector that summarizes the spatial information in
    each picture. Then, we pass the same feature vector along each input sequence
    when training the network. This way, the network will learn to associate all the
    words in a caption with the same image.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道如何将视觉信息传递给网络。这就是特征提取步骤至关重要的地方，因为我们将数据集中的每个图像转换为一个数值向量，该向量总结了每张图片中的空间信息。然后，在训练网络时，我们通过每个输入序列传递相同的特征向量。这样，网络将学会将标题中的所有单词与同一图像关联起来。
- en: If we're not careful, we could get trapped in an endless loop of word generation.
    How can we prevent this? By using a special token to signal the end of a sequence
    (this means the network should stop producing words when it encounters such a
    token). In our case, this token is, by default, `endsequence`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不小心，可能会陷入无限循环的单词生成中。我们如何防止这种情况发生？通过使用一个特殊的标记来信号化序列的结束（这意味着网络在遇到这样的标记时应停止生成单词）。在我们的情况下，默认的标记是`endsequence`。
- en: A similar problem is how to start a sequence. Which word should we use? In this
    case, we must also resort to a special token (our default is `beginsequence`).
    This acts as a seed that the network will use to start producing captions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类似的问题是如何启动一个序列。我们应该使用哪个词？在这种情况下，我们也必须使用一个特殊的标记（我们的默认值是`beginsequence`）。这个标记充当一个种子，网络将基于它开始生成字幕。
- en: All of this might sound confusing now, and that's because we've only focused
    on the data preprocessing stage. In the remaining recipes of this chapter, we'll
    leverage the work we've done here to train many different image captioners, and
    all the pieces will fall into place!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切现在听起来可能有点令人困惑，这是因为我们只专注于数据预处理阶段。在本章的剩余食谱中，我们将利用在这里所做的工作来训练许多不同的图像字幕生成器，一切都会变得明了！
- en: See also
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Here''s a great explanation of how **RNNs** work: [https://www.youtube.com/watch?v=UNmqTiOnRfg](https://www.youtube.com/watch?v=UNmqTiOnRfg).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的关于**RNNs**如何工作的解释：[https://www.youtube.com/watch?v=UNmqTiOnRfg](https://www.youtube.com/watch?v=UNmqTiOnRfg)。
- en: Implementing an image captioning network
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现图像字幕生成网络
- en: An image captioning architecture is comprised of an encoder and a decoder. The
    encoder is a **CNN** (typically a pre-trained one), which converts input images
    into numeric vectors. These vectors are then passed, along with text sequences,
    to the decoder, which is an **RNN**, that will learn, based on these values, how
    to iteratively generate each word in the corresponding caption.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图像字幕生成架构由编码器和解码器组成。编码器是一个**CNN**（通常是一个预训练的模型），它将输入图像转换为数值向量。然后，这些向量与文本序列一起传递给解码器，解码器是一个**RNN**，它将基于这些值学习如何逐步生成对应字幕中的每个单词。
- en: In this recipe, we'll implement an image captioner that's been trained on the
    `Flickr8k` dataset. We'll leverage the feature extractor we implemented in the
    *Implementing a reusable image caption feature extractor* recipe.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将实现一个已在`Flickr8k`数据集上训练的图像字幕生成器。我们将利用在*实现可重用的图像字幕特征提取器*食谱中实现的特征提取器。
- en: Let's begin, shall we?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始吧，好吗？
- en: Getting ready
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The external dependencies we''ll be using in this recipe are `Pillow`, `nltk`,
    and `tqdm`. You can install them all at once with the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用的外部依赖是`Pillow`、`nltk`和`tqdm`。你可以通过以下命令一次性安装它们：
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will use the `Flickr8k` dataset, which you can get from `~/.keras/datasets/flickr8k`
    directory.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Flickr8k`数据集，您可以从`~/.keras/datasets/flickr8k`目录中获取它。
- en: 'The following are some sample images from the `Flickr8k` dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些来自`Flickr8k`数据集的示例图像：
- en: '![Figure 7.2 – Sample images from Flickr8k'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 来自Flickr8k的示例图像'
- en: '](img/B14768_07_002.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_002.jpg)'
- en: Figure 7.2 – Sample images from Flickr8k
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 来自Flickr8k的示例图像
- en: Let's head over to the next section to start this recipe's implementation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入下一部分，开始本食谱的实现。
- en: How to do it…
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Follow these steps to implement a deep learning-based image captioning system:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现基于深度学习的图像字幕生成系统：
- en: 'First, we must import all of the required packages:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须导入所有必需的包：
- en: '[PRE26]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the paths to the images and captions, as well as the output path, which
    is where we''ll store the artifacts that will be created in this recipe:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义图像和字幕的路径，以及输出路径，这将是我们存储在本食谱中创建的工件的位置：
- en: '[PRE27]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define a function that will load a list of image paths and their corresponding
    captions. This implementation is similar to *Steps* *20* through *22* of the *Implementing
    a reusable image caption feature extractor* recipe:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将加载图像路径及其对应的字幕列表。此实现类似于*步骤* *20*到*22*，来自*实现可重用的图像字幕特征提取器*食谱：
- en: '[PRE28]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Compile all the captions:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译所有字幕：
- en: '[PRE29]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define a function that will build the architecture of the network, which receives
    the vocabulary size, the maximum sequence length, and the encoder''s input shape:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将构建网络的架构，接收词汇表大小、最大序列长度以及编码器的输入形状：
- en: '[PRE30]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The first part of the network receives the feature vectors and passes them
    through a fully connected `ReLU` activated layer:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的第一部分接收特征向量并将其通过一个全连接的`ReLU`激活层：
- en: '[PRE31]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The second part of the layer receives the text sequences, transformed into
    numeric vectors, and trains an embedding of 256 elements. Then, it passes that
    embedding to an `LSTM` layer:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层的第二部分接收文本序列，这些文本序列被转换为数值向量，并训练一个包含256个元素的嵌入层。然后，它将该嵌入传递给`LSTM`层：
- en: '[PRE32]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We concatenate the outputs of these two parts and pass the concatenation through
    a fully connected network, with an output layer with as many units as there are
    words in our vocabulary. By `Softmax` activating this output, we get a one-hot
    encoded vector that corresponds to a word in the vocabulary:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这两部分的输出连接起来，并通过一个全连接网络传递，输出层的单元数量与词汇表中的单词数相同。通过对该输出进行`Softmax`激活，我们得到一个对应词汇表中某个单词的
    one-hot 编码向量：
- en: '[PRE33]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we build the model, passing the image features and text sequences
    as inputs, and outputting the one-hot encoded vectors:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们构建模型，传入图像特征和文本序列作为输入，并输出 one-hot 编码向量：
- en: '[PRE34]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define a function that will convert an integer index into a word by using the
    tokenizer''s internal mapping:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，通过使用分词器的内部映射将整数索引转换为单词：
- en: '[PRE35]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define a function that will produce a caption. It will start by feeding the
    `beginsequence` token to the network, which will iteratively construct the sentence
    until the maximum sequence length is reached, or the `endsequence` token is encountered:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来生成标题。它将从将`beginsequence`标记输入到网络开始，网络会迭代构建句子，直到达到最大序列长度或遇到`endsequence`标记：
- en: '[PRE36]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define a function that will evaluate the model''s performance. First, we''ll
    produce a caption for each feature corresponding to an image in the test dataset:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来评估模型的表现。首先，我们将为测试数据集中每个图像的特征生成一个标题：
- en: '[PRE37]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we''ll compute the **BLEU** score using different weights. Although the
    **BLEU** score is outside the scope of this recipe, you can find an excellent
    article that explains it in depth in the *See also* section. All you need to know
    is that it''s used to measure how well a generated caption compares to a set of
    reference captions:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用不同的权重计算**BLEU**分数。虽然**BLEU**分数超出了本教程的范围，但你可以在*另见*部分找到一篇详细解释的优秀文章。你需要知道的是，它用于衡量生成的标题与一组参考标题的相似度：
- en: '[PRE38]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the image paths and captions:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像路径和标题：
- en: '[PRE39]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create the image extractor model:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建图像提取模型：
- en: '[PRE40]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create the image caption feature extractor (passing the regular image extractor
    we created in *Step 15*) and use it to extract the sequences from the data:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建图像标题特征提取器（传入我们在*步骤15*中创建的常规图像提取器），并用它从数据中提取序列：
- en: '[PRE41]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Load the pickled input and output sequences we created in *Step 16*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们在*步骤16*中创建的已序列化输入和输出序列：
- en: '[PRE42]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Use 80% of the data for training and 20% for testing:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 80% 的数据进行训练，20% 用于测试：
- en: '[PRE43]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Instantiate and compile the model. Because, in the end, this is a multi-class
    classification problem, we''ll use `categorical_crossentropy` as our loss function:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化并编译模型。因为最终这是一个多类分类问题，我们将使用`categorical_crossentropy`作为损失函数：
- en: '[PRE44]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Because the training process is so resource-intensive and the network tends
    to give the best results early on, let''s create a `ModelCheckpoint` callback
    that will store the model with the lowest validation loss:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于训练过程非常消耗资源，并且网络通常在早期就能给出最佳结果，因此我们创建了一个`ModelCheckpoint`回调，它将存储具有最低验证损失的模型：
- en: '[PRE45]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Fit the model over 30 epochs. Notice that we must pass two set of inputs or
    features, but only a set of labels:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 30 个训练周期内拟合模型。请注意，我们必须传入两组输入或特征，但只有一组标签：
- en: '[PRE46]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Load the best model. This may vary from run to run, but in this recipe, it''s
    stored in the `model-ep003-loss3.847-val_loss4.328.h5` file:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载最佳模型。这个模型可能会因运行而异，但在本教程中，它存储在`model-ep003-loss3.847-val_loss4.328.h5`文件中：
- en: '[PRE47]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Load the data mapping, which contains all the features paired with the ground
    truth captions. Extract the features and mappings into separate collections:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据映射，其中包含所有特征与真实标题的配对。将特征和映射提取到不同的集合中：
- en: '[PRE48]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Evaluate the model:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型：
- en: '[PRE49]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This step might take a while. In the end, you''ll see an output similar to
    this:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个步骤可能需要一些时间。最终，你会看到类似这样的输出：
- en: '[PRE50]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Training an image captioner is not an easy task. However, by executing the proper
    steps, in the correct order, we were able to create a fairly capable one that
    performed well on the test set, based on the **BLEU** score shown in the preceding
    code block. Head over to the next section to see how it all works!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 训练图像标题生成器并不是一项简单的任务。然而，通过按正确的顺序执行合适的步骤，我们成功创建了一个表现不错的模型，并且在测试集上表现良好，基于前面代码块中显示的**BLEU**分数。继续阅读下一部分，了解它是如何工作的！
- en: How it works…
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we implemented an image captioning network from scratch. Although
    this might seem complicated at first, we must remember it is a variation of an
    encoder-decoder architecture, similar to the ones we studied in [*Chapter 5*](B14768_05_Final_JM_ePub.xhtml#_idTextAnchor177),
    *Reducing Noise with Autoencoders*, and [*Chapter 6*](B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214),
    *Generative Models and Adversarial Attacks*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们从零开始实现了一个图像描述生成网络。尽管一开始看起来可能有些复杂，但我们必须记住，这只是一个编码器-解码器架构的变种，类似于我们在[*第5章*](B14768_05_Final_JM_ePub.xhtml#_idTextAnchor177)《使用自编码器减少噪声》和[*第6章*](B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214)《生成模型与对抗攻击》中研究过的架构。
- en: In this case, the encoder is just a fully connected and shallow network that
    maps the features we extracted from the pre-trained model on ImageNet, to a vector
    of 256 elements.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，编码器只是一个完全连接的浅层网络，将我们从ImageNet的预训练模型中提取的特征映射到一个包含256个元素的向量。
- en: On the other hand, the decoder, instead of using transposed convolutions, uses
    an **RNN** that receives both text sequences (mapped to numeric vectors) and image
    features, concatenated into a long sequence of 512 elements.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，解码器并不是使用转置卷积，而是使用一个**RNN**，它接收文本序列（映射为数字向量）和图像特征，将它们连接成一个由512个元素组成的长序列。
- en: The network is trained so that it learns to predict the next word in a sentence,
    given all the words it generated in previous time steps. Note that in each cycle,
    we pass the same feature vector that corresponds to the image, so the network
    learns to map certain words, in a particular order, to describe the visual data
    encoded in such a vector.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的训练目标是根据前面时间步生成的所有词，预测句子中的下一个词。注意，在每次迭代中，我们传递的是与图像对应的相同特征向量，因此网络会学习按特定顺序映射某些词，以描述编码在该向量中的视觉数据。
- en: The output of the network is one-hot encoded, which means that only the position
    its corresponding to the words the network believes should come next in the sentence
    contains a 1, while the remaining positions contain a 0.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出是独热编码，这意味着只有与网络认为应该出现在句子中的下一个词对应的位置包含1，其余位置包含0。
- en: "To generate captions, we follow a similar process. Of course, we somehow need\
    \ to tell the model to start producing words. With this in mind, we pass the `beginsequence`\
    \ token to the network and iterate until we reach the maximum sequence length,\
    \ or the model outputs an `endsequence` token. Remember, we take the output of\
    \ each iteration and \Luse it as input for the next cycle."
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成描述，我们遵循类似的过程。当然，我们需要某种方式告诉模型开始生成词汇。为此，我们将`beginsequence`标记传递给网络，并不断迭代，直到达到最大序列长度，或模型输出`endsequence`标记。记住，我们将每次迭代的输出作为下一次迭代的输入。
- en: This might seem confusing and cumbersome at first, but you now have the building
    blocks you need to tackle any image captioning problem!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始这可能看起来有些困惑和繁琐，但现在你已经掌握了解决任何图像描述问题所需的构建块！
- en: See also
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Here''s an excellent read if you wish to fully understand the **BLEU** score:
    [https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望全面理解**BLEU**分数，可以参考这篇精彩的文章：[https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)。
- en: Generating captions for your own photos
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为你的照片生成描述
- en: Training a good image captioning system is only one part of the equation. To
    actually use it, we must perform a series of steps, akin to the ones we executed
    during the training phase.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个优秀的图像描述生成系统只是整个过程的一部分。为了实际使用它，我们必须执行一系列的步骤，类似于我们在训练阶段执行的操作。
- en: In this recipe, we'll use a trained image captioning network to produce textual
    descriptions of new images.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将使用一个训练好的图像描述生成网络来生成新图像的文字描述。
- en: Let's get started!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Although we don't need external dependencies for this particular recipe, we
    need access to a trained image captioning network, along with the cleaned captions
    that will be used to fit it. I highly recommend that you complete the *Implementing
    a reusable image caption feature extractor* and *Implementing an image captioning
    network* recipes before tackling this one.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这个特定的教程中我们不需要外部依赖，但我们需要访问一个训练好的图像描述生成网络，并且需要清理过的描述文本来对其进行训练。强烈建议你在开始这个教程之前，先完成*实现可复用的图像描述特征提取器*和*实现图像描述生成网络*的教程。
- en: Are you ready? Let's start captioning!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？让我们开始描述吧！
- en: How to do it…
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Follow this series of steps to produce captions for your own images:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤生成自己图像的标题：
- en: 'As usual, let''s begin by importing the necessary dependencies:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，让我们首先导入必要的依赖项：
- en: '[PRE51]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Define a function that will translate an integer index into the corresponding
    word using the tokenizer''s mapping:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将整数索引转换为分词器映射中的对应单词：
- en: '[PRE52]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Define the `produce_caption()` function, which takes the captioning model,
    the tokenizer, an image to describe, and the maximum sequence length to generate
    a textual description of the input visual scene:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`produce_caption()`函数，该函数接受标题生成模型、分词器、要描述的图像以及生成文本描述所需的最大序列长度：
- en: '[PRE53]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note that we must keep generating words until we either encounter the `endsequence`
    token or we reach the maximum sequence length.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们必须持续生成单词，直到遇到`endsequence`标记或达到最大序列长度。
- en: 'Define a pre-trained **VGG16** network, which we''ll use as our image feature
    extractor:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个预训练的**VGG16**网络，我们将其用作图像特征提取器：
- en: '[PRE54]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Pass the image extractor to an instance of `ImageCaptionFeatureExtractor()`:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像提取器传递给`ImageCaptionFeatureExtractor()`的一个实例：
- en: '[PRE55]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Load the cleaned captions we used to train the model. We need them to fit the
    tokenizer in *Step 7*:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们用于训练模型的清理过的标题。我们需要它们来拟合*步骤7*中的分词器：
- en: '[PRE56]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Instantiate a `Tokenizer()` and fit it to all the captions. Also, compute the
    maximum sequence length:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`Tokenizer()`并将其拟合到所有标题。还需计算最大序列长度：
- en: '[PRE57]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Load the trained network (in this case, the name of the network is `model-ep003-loss3.847-val_loss4.328.h5`):'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练好的网络（在本例中，网络名称为`model-ep003-loss3.847-val_loss4.328.h5`）：
- en: '[PRE58]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Iterate over all the test images in the current location, extracting the corresponding
    numeric features:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历当前目录中的所有测试图像，提取相应的数字特征：
- en: '[PRE59]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Produce the caption and remove the `beginsequence` and `endsequence` special
    tokens:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成标题并移除`beginsequence`和`endsequence`特殊标记：
- en: '[PRE60]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Open the image, add the generated caption as its title, and save it:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开图像，将生成的标题作为其标题并保存：
- en: '[PRE61]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Here''s an image where the network does a very good job of generating a proper
    caption:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个图像，网络在生成适当的标题方面表现得非常好：
- en: '![Figure 7.3 – We can see that the caption is very close to what''s actually
    happening'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3 – 我们可以看到，标题非常接近实际发生的情况'
- en: '](img/B14768_07_003.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_003.jpg)'
- en: Figure 7.3 – We can see that the caption is very close to what's actually happening
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 我们可以看到，标题非常接近实际发生的情况：
- en: 'Here''s another example where the network is technically correct, although
    it could be more precise:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个例子，尽管网络在技术上是正确的，但它的准确性可以更高：
- en: '![Figure 7.4 – A football player in a red uniform is, indeed, in the air, but
    there is more going on'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – 一名穿红色制服的足球运动员确实在空中，但还发生了更多的事情'
- en: '](img/B14768_07_004.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_004.jpg)'
- en: Figure 7.4 – A football player in a red uniform is, indeed, in the air, but
    there is more going on
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 一名穿红色制服的足球运动员确实在空中，但还发生了更多的事情
- en: 'Finally, here''s an instance where the network is clueless:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一个网络完全无能为力的例子：
- en: '![Figure 7.5 – The network couldn''t describe this scene'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.5 – 网络无法描述这一场景'
- en: '](img/B14768_07_005.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_005.jpg)'
- en: Figure 7.5 – The network couldn't describe this scene
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 网络无法描述这一场景
- en: With that, we've seen that our model does well on some images, but still has
    room for improvement. We'll dive a bit deeper in the next section.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经看到模型在一些图像上的表现不错，但仍有提升空间。我们将在下一节深入探讨。
- en: How it works…
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we learned that image captioning is a difficult problem that
    heavily depends on many factors. Some of these factors are as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次配方中，我们了解到图像标题生成是一个困难的问题，且严重依赖于许多因素。以下是一些因素：
- en: A well-trained **CNN** to extract high-quality visual features
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个训练良好的**CNN**用于提取高质量的视觉特征：
- en: A rich set of descriptive captions for each image
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个图像提供一组丰富的描述性标题：
- en: Embeddings with enough capacity to encode the expressiveness of the vocabulary
    with minimal loss
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有足够容量的嵌入，能够以最小的损失编码词汇的表现力：
- en: A powerful **RNN** to learn how to put all of this together
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个强大的**RNN**来学习如何将这一切组合在一起：
- en: 'Despite these clear challenges, in this recipe, we used a trained network on
    the `Flickr8k` dataset to generate captions for new images. The process we followed
    is similar to the one we implemented to train the system in that, first, we must
    go from an image to a feature vector. Then, we must fit a tokenizer to our vocabulary
    to get a proper mechanism so that we can go from sequences to human-readable words.
    Finally, we assemble the captions one word at a time, passing the image features
    along with the sequence we''ve built so far. How do we know when to stop, though?
    We have two stopping criteria:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些明显的挑战，在这个教程中，我们使用了一个在 `Flickr8k` 数据集上训练的网络来生成新图像的标注。我们遵循的过程与我们训练系统时实施的过程类似：首先，我们必须将图像转换为特征向量。然后，我们需要对词汇表进行分词器拟合，获取适当的机制，以便能够将序列转换为人类可读的单词。最后，我们逐字拼接标注，同时传递图像特征和我们已经构建的序列。那么，我们如何知道何时停止呢？我们有两个停止标准：
- en: The caption reached the maximum sequence length.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注达到了最大序列长度。
- en: The network encountered the `endsequence` token.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络遇到了 `endsequence` 标记。
- en: Lastly, we tested our solution on several images, with varied results. In some
    instances, the network is capable of producing very precise descriptions, while
    on other occasions, it generates somewhat vague captions. It also missed the mark
    completely in the last example, which is a clear indication of how much room for
    improvement there is.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在多张图片上测试了我们的解决方案，结果不一。在某些情况下，网络能够生成非常精确的描述，而在其他情况下，生成的标注则稍显模糊。在最后一个示例中，网络完全没有达成预期，这清楚地表明了仍有很大的改进空间。
- en: 'If you want to take a look at other captioned images, consult the official
    repository: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看其他带标注的图像，请查阅官方仓库：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3)。
- en: Implementing an image captioning network on COCO with attention
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 COCO 上实现带注意力机制的图像标注网络
- en: A great way to understand how an image captioning network generates its descriptions
    is by adding an attention component to the architecture. This lets us appreciate
    what parts of the photo a network was looking at when it generated each word.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 理解图像标注网络如何生成描述的一个好方法是向架构中添加一个注意力组件。这使我们能够看到在生成每个单词时，网络注视图像的哪些部分。
- en: In this recipe, we'll train an end-to-end image captioning system on the more
    challenging **Common Objects in Context** (**COCO**) dataset. We'll also equip
    our network with an attention mechanism to improve its performance and to help
    us understand its inner reasoning.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将在更具挑战性的 **常见物体上下文** (**COCO**) 数据集上训练一个端到端的图像标注系统。我们还将为网络配备注意力机制，以提高其性能，并帮助我们理解其内部推理过程。
- en: This is a long and advanced recipe, but don't panic! We'll go step by step.
    If you want to dive deeper into the theory that supports this implementation,
    take a look at the *See also* section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个较长且复杂的教程，但不用担心！我们将逐步进行。如果你想深入了解支撑该实现的理论，请查看 *另请参阅* 部分。
- en: Getting ready
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Although we''ll be using the `COCO` dataset, you don''t need to do anything
    beforehand, because we''ll download it as part of the recipe (however, you can
    read more about this seminal dataset here: https://cocodataset.org/#home).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将使用 `COCO` 数据集，但你无需提前做任何准备，因为我们将在教程中下载它（不过，你可以在这里了解更多关于这个开创性数据集的信息：https://cocodataset.org/#home）。
- en: 'The following is a sample from the `COCO` dataset:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `COCO` 数据集中的一个示例：
- en: '![Figure 7.6 – Sample images from COCO'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 来自 COCO 的示例图片](img/B14768_07_006.jpg)'
- en: '](img/B14768_07_006.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_006.jpg)'
- en: Figure 7.6 – Sample images from COCO
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 来自 COCO 的示例图片
- en: Let's get to work!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始工作吧！
- en: How to do it…
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to complete this recipe:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成这个教程：
- en: 'Import all the necessary dependencies:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的依赖项：
- en: '[PRE62]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 `tf.data.experimental.AUTOTUNE` 定义一个别名：
- en: '[PRE63]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Define a function that will load an image. It must return both the image and
    its path:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来加载图像。它必须返回图像及其路径：
- en: '[PRE64]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Define a function that will get the maximum sequence length. This will be useful
    later on:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来获取最大序列长度。这将在稍后使用：
- en: '[PRE65]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define for the image captioning network a function that will load an image
    from disk (stored in `NumPy` format):'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为图像标注网络定义一个函数，从磁盘加载图像（存储为 `NumPy` 格式）：
- en: '[PRE66]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Implement **Bahdanau''s Attention** using model subclassing:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型子类化实现**巴赫达努的注意力机制**：
- en: '[PRE67]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The previous block defined the network layers. Now, let''s define the forward
    pass inside the `call()` method:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码块定义了网络层。现在，我们在`call()`方法中定义前向传播：
- en: '[PRE68]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Define the image encoder. This is just a `ReLU`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义图像编码器。这只是一个`ReLU`：
- en: '[PRE69]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Define the decoder. This is an `GRU` and attention to learn how to produce
    captions from the visual feature vectors and the text input sequences:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义解码器。它是一个`GRU`和注意力机制，学习如何从视觉特征向量和文本输入序列中生成标题：
- en: '[PRE70]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now that we''ve defined the layers in the **RNN** architecture, let''s implement
    the forward pass. First, we must pass the inputs through the attention sub-network:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了**RNN**架构中的各个层，接下来实现前向传播。首先，我们必须通过注意力子网络传递输入：
- en: '[PRE71]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Then, we must pass the input sequence (`x`) through the embedding layer and
    concatenate it with the context vector we received from the attention mechanism:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须将输入序列（`x`）通过嵌入层，并将其与从注意力机制中获得的上下文向量进行连接：
- en: '[PRE72]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Next, we must pass the merged tensor to the `GRU` layer, and then through the
    dense layers. This returns the output sequence, the state, and the attention weights:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须将合并后的张量传递给`GRU`层，然后通过全连接层。这样返回的是输出序列、状态和注意力权重：
- en: '[PRE73]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Finally, we must define a method that will reset the hidden state:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须定义一个方法来重置隐藏状态：
- en: '[PRE74]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Define `ImageCaptionerClass`. The constructor instantiates the basic components,
    which are the encoder, the decoder, the tokenizer, and the optimizer and loss
    functions needed to train the whole system:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`ImageCaptionerClass`。构造函数实例化基本组件，包括编码器、解码器、分词器、以及训练整个系统所需的优化器和损失函数：
- en: '[PRE75]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Create a method that will compute the loss function:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个方法来计算损失函数：
- en: '[PRE76]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Next, define a function that will perform a single training step. We will start
    by creating the hidden state and the input, which is just a batch of singleton
    sequences containing the index of the `<start>` token, a special element used
    to signal the beginning of a sentence:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个函数来执行单个训练步骤。我们将从创建隐藏状态和输入开始，输入仅是包含`<start>`标记索引的单一序列批次，`<start>`是一个特殊元素，用于指示句子的开始：
- en: '[PRE77]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, we must encode the image tensor. Then, we''ll iteratively pass the resulting
    features to the decoder, along with the outputted sequence so far, and the hidden
    state. For a deeper explanation on how **RNNs** work, head to the *See also* section:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须编码图像张量。然后，我们将反复将结果特征传递给解码器，连同到目前为止的输出序列和隐藏状态。关于**RNNs**如何工作的更深层次解释，请参考*另见*部分：
- en: '[PRE78]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Notice in the previous block that we computed the loss at each time step. To
    get the total loss, we must calculate the average. For the network to actually
    learn, we must backpropagate the total loss by computing the gradients and applying
    them via the optimizer:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码块中我们在每个时间步计算了损失。为了获得总损失，我们必须计算平均值。为了让网络真正学习，我们必须通过反向传播计算梯度，并通过优化器应用这些梯度：
- en: '[PRE79]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The last method in this class is in charge of training the system:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本类中的最后一个方法负责训练系统：
- en: '[PRE80]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Every 100 epochs, we''ll print the loss. At the end of each epoch, we will
    also print the epoch loss and elapsed time:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每经过100个epoch，我们将打印损失。在每个epoch结束时，我们还将打印该epoch的损失和已用时间：
- en: '[PRE81]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Download and unzip the `COCO` dataset''s annotation files. If they''re already
    in the system, just store the file path:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并解压`COCO`数据集的注释文件。如果它们已经在系统中，只需存储文件路径：
- en: '[PRE82]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Download and unzip the `COCO` dataset''s image files. If they''re already in
    the system, just store the file path:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并解压`COCO`数据集的图像文件。如果它们已经在系统中，只需存储文件路径：
- en: '[PRE83]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Load the image paths and the captions. We must add the special `<start>` and
    `<end>` tokens to each caption so that they''re in our vocabulary. These special
    tokens let us specify where a sequence begins and ends, respectively:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像路径和标题。我们必须将特殊的`<start>`和`<end>`标记添加到每个标题中，以便它们包含在我们的词汇表中。这些特殊标记使我们能够分别指定序列的开始和结束位置：
- en: '[PRE84]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: "Because `COCO` is massive, and it would take ages to train a model on it, we'll\
    \ select \La random sample of 30,000 images, along with their captions:"
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`COCO`数据集庞大，训练一个模型需要很长时间，我们将选择30,000张图像及其对应的标题作为随机样本：
- en: '[PRE85]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let''s use a pre-trained instance of `InceptionV3` as our image feature extractor:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`InceptionV3`的预训练实例作为我们的图像特征提取器：
- en: '[PRE86]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Create a `tf.data.Dataset` that maps image paths to tensors. Use it to go over
    all the images in our sample, convert them into feature vectors, and save them
    as `NumPy` arrays. This will allow us to save memory in the future:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `tf.data.Dataset`，将图像路径映射到张量。使用它遍历我们样本中的所有图像，将它们转换为特征向量，并将其保存为 `NumPy`
    数组。这将帮助我们在将来节省内存：
- en: '[PRE87]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Train a tokenizer on the top 5,000 words in our captions. Then, convert each
    text into a numeric sequence and pad them so that they are all the same size.
    Also, compute the maximum sequence length:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们标题中的前 5,000 个单词上训练一个分词器。然后，将每个文本转换为数字序列，并进行填充，使它们的大小一致。同时，计算最大序列长度：
- en: '[PRE88]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We''ll use 20% of the data to test our model and the remaining 80% to train
    it:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 20% 的数据来测试模型，其余 80% 用于训练：
- en: '[PRE89]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'We''ll load batches of 64 images (along with their captions) at a time. Notice
    that we''re using the `load_image_and_caption()` function, defined in *Step 5*,
    which reads the feature vector corresponding to the images, stored in `NumPy`
    format. Moreover, because this function works at the `NumPy` level, we must wrap
    it with `tf.numpy_function` so that it can be used as a valid TensorFlow function
    within the `map()` method:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将一次加载 64 张图像的批次（以及它们的标题）。请注意，我们使用的是 *第 5 步* 中定义的 `load_image_and_caption()`
    函数，它读取与图像对应的特征向量，这些向量以 `NumPy` 格式存储。此外，由于该函数在 `NumPy` 层面工作，我们必须通过 `tf.numpy_function`
    将其包装，以便它能作为有效的 TensorFlow 函数在 `map()` 方法中使用：
- en: '[PRE90]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Let''s instantiate an `ImageCaptioner`. The embeddings will have 256 elements,
    and the number of units for the decoder and the attention model will be 512\.
    The vocabulary size is 5,001\. Finally, we must pass the fitted tokenizer from
    *Step 27*:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实例化一个 `ImageCaptioner`。嵌入层将包含 256 个元素，解码器和注意力模型的单元数将是 512。词汇表大小为 5,001。最后，我们必须传入
    *第 27 步* 中拟合的分词器：
- en: '[PRE91]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Define a function that will evaluate the image captioner on an image. It must
    receive the encoder, the decoder, the tokenizer, the image to caption, the maximum
    sequence length, and the shape of the attention vector. We will start by creating
    a placeholder array, which is where we''ll store the subplots that comprise the
    attention plot:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于在图像上评估图像标题生成器。它必须接收编码器、解码器、分词器、待描述的图像、最大序列长度以及注意力向量的形状。我们将从创建一个占位符数组开始，这里将存储构成注意力图的子图：
- en: '[PRE92]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Next, we must initialize the hidden state, extract the features from the input
    image, and pass them to the encoder. We must also initialize the decoder input
    by creating a singleton sequence with the `<start>` token index:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须初始化隐藏状态，提取输入图像的特征，并将其传递给编码器。我们还必须通过创建一个包含 `<start>` 标记索引的单一序列来初始化解码器输入：
- en: '[PRE93]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Now, let''s build the caption until we reach the maximum sequence length or
    encounter the `<end>` token:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们构建标题，直到达到最大序列长度或遇到 `<end>` 标记：
- en: '[PRE94]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Notice that for each word, we update `attention_plot` with the weights returned
    by the decoder.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，对于每个单词，我们都会更新 `attention_plot`，并返回解码器的权重。
- en: 'Let''s define a function that will plot the attention the network pays to each
    word in the caption. It receives the image, a list of the individual words that
    comprise the caption (`result`), `attention_plot` returned by `evaluate()`, and
    the output path where we''ll store the graph:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，用于绘制网络对每个单词的注意力。它接收图像、构成标题的单个单词列表（`result`）、由 `evaluate()` 返回的 `attention_plot`，以及我们将存储图形的输出路径：
- en: '[PRE95]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'We''ll iterate over each word to create a subplot of the corresponding attention
    graph, titled with the specific word it''s linked to:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将遍历每个单词，创建相应注意力图的子图，并以其链接的特定单词为标题：
- en: '[PRE96]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Finally, we can save the full plot:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以保存完整的图：
- en: '[PRE97]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Evaluate the network on a random image from the validation set:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证集上评估网络的随机图像：
- en: '[PRE98]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Build and clean the actual (ground truth) caption:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并清理实际（真实标签）标题：
- en: '[PRE99]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Generate the caption for the validation image:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为验证图像生成标题：
- en: '[PRE100]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Build and clean the predicted caption:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并清理预测的标题：
- en: '[PRE101]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Print the ground truth and generated captions, and then save the attention
    plot to disk:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印真实标签和生成的标题，然后将注意力图保存到磁盘：
- en: '[PRE102]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'In the following code block, we can appreciate the similarity between the real
    caption and the one outputted by our model:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们可以欣赏到真实标题与模型输出标题之间的相似性：
- en: '[PRE103]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Now, let''s take a look at the attention plot:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下注意力图：
- en: '![Figure 7.7 – Attention plot'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – 注意力图'
- en: '](img/B14768_07_007.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_07_007.jpg)'
- en: Figure 7.7 – Attention plot
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 注意力图
- en: Take note of the areas the network looked at when generating each word in the
    caption. Lighter squares mean that more attention was paid to those pixels. For
    instance, to produce the word *giraffe*, the network looked at the surroundings
    of the giraffe in the photo. Also, we can see that when the network generated
    the word *grass*, it looked at the giraffe legs, which have a grass portion behind
    them. Isn't that amazing?
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在生成每个单词时，网络关注的区域。较浅的方块表示更多的关注被放在这些像素上。例如，要生成单词*giraffe*，网络关注了照片中长颈鹿的周围环境。此外，我们可以看到，当网络生成单词*grass*时，它关注了长颈鹿腿部的草地部分。难道这不令人惊讶吗？
- en: We'll look at this in more detail in the *How it works…* section.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*它是如何工作的...*部分中详细讨论这个问题。
- en: How it works…
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we implemented a more complete image captioning system, this
    time on the considerably more challenging `COCO` dataset, which is not only several
    orders of magnitude bigger than `Flickr8k`, but much more varied and, therefore,
    harder for the network to understand.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们实现了一个更完整的图像描述系统，这一次使用了挑战更大的`COCO`数据集，该数据集不仅比`Flickr8k`大几个数量级，而且更加多样化，因此网络理解起来更为困难。
- en: Nevertheless, we gave our network an advantage by providing it with an attention
    mechanism, inspired by the impressive breakthrough proposed by Dzmitry Bahdanau
    (take a look at the *See also* section for more details). This capability gives
    the model the power to perform a soft search for parts of the source caption that
    are relevant to predicting a target word or simply put, producing the best next
    word in the output sentence. Such an attention mechanism works as an advantage
    over the traditional approach, which consists of using a fixed-length vector (as
    we did in the *Implementing an image captioning network* recipe) from which the
    decoder generates the output sentence. The problem with such a representation
    is that it tends to act as a bottleneck when it comes to improving performance.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通过为网络提供一个注意力机制，使其拥有优势，这一机制灵感来自Dzmitry Bahdanau提出的令人印象深刻的突破（更多细节请参见*另见*部分）。这个功能赋予模型进行软搜索的能力，查找与预测目标词相关的源描述部分，简而言之，就是在输出句子中生成最佳的下一个词。这种注意力机制相对于传统方法具有优势，传统方法是使用固定长度的向量（如我们在*实现图像描述网络*食谱中所做的那样），解码器从中生成输出句子。这样表示的问题在于，当提高性能时，它往往会成为瓶颈。
- en: Also, the attention mechanism allows us to understand how the network thinks
    to produce captions in a more intuitive way.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意力机制使我们能够以更直观的方式理解网络生成描述时的思考过程。
- en: Because neural networks are complex pieces of software (often akin to a black
    box), using visual techniques to inspect their inner workings is a great tool
    at our disposal that can aid us in the training, fine-tuning, and optimization
    process.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因为神经网络是复杂的软件（通常像一个黑箱），使用视觉技术来检查其内部工作原理是我们可以利用的一种很好的工具，有助于我们在训练、微调和优化过程中。
- en: See also
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 另见
- en: 'In this recipe, we implemented our architecture using the Model Subclassing
    pattern, which you can read more about here: [https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用模型子类化模式实现了我们的架构，你可以在这里阅读更多内容：[https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models)。
- en: 'Take a look at the following link for a great refresher on **RNNs**: [https://www.youtube.com/watch?v=UNmqTiOnRfg](https://www.youtube.com/watch?v=UNmqTiOnRfg).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下链接，复习一下**RNN**的内容：[https://www.youtube.com/watch?v=UNmqTiOnRfg](https://www.youtube.com/watch?v=UNmqTiOnRfg)。
- en: 'Finally, I highly encourage you to read Dzmitry Bahdanau''s paper about the
    attention mechanism we just implemented and used: [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我强烈鼓励你阅读Dzmitry Bahdanau关于我们刚刚实现和使用的注意力机制的论文：[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)。
