- en: Building Virtual Worlds in Minecraft
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Minecraft中构建虚拟世界
- en: In the two previous  chapters, we discussed the **deep Q-learning** (**DQN**) algorithm for
    playing Atari games and the **Trust Region Policy Optimization** (**TRPO**) algorithm for
    continuous control tasks. We saw the big success of these algorithms in solving
    complex problems when compared to traditional reinforcement learning algorithms
    without the use of deep neural networks to approximate the value function or the
    policy function. Their main disadvantage, especially for DQN, is that the training
    step converges too slowly, for example, training an agent to play Atari games
    takes about one week. For more complex games, even one week's training is insufficient.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们讨论了**深度Q学习**（**DQN**）算法，用于玩Atari游戏，以及**信任域策略优化**（**TRPO**）算法，用于连续控制任务。我们看到这些算法在解决复杂问题时取得了巨大成功，尤其是与传统强化学习算法相比，后者并未使用深度神经网络来逼近价值函数或策略函数。它们的主要缺点，尤其是对于DQN来说，是训练步骤收敛得太慢，例如，训练一个代理玩Atari游戏需要大约一周时间。对于更复杂的游戏，即使一周的训练时间也不够。
- en: This chapter will introduce a more complicated example, Minecraft, which is
    a popular online video game created by Swedish game developer Markus Persson and
    later developed by Mojang. You will learn how to launch a Minecraft environment
    using OpenAI Gym and play different missions. In order to build an AI player to
    accomplish these missions, you will learn the **asynchronous advantage actor-critic**
    (**A3C**) algorithm, which is a lightweight framework for deep reinforcement learning
    that uses asynchronous gradient descent for optimization of deep neural network
    controllers. A3C is a widely applied deep reinforcement learning algorithm for
    different kinds of tasks, training for half the time on a single multi-core CPU
    instead of a GPU. For Atari games such as Breakout, A3C achieves human-level performance
    after 3 hours' training, which is much faster than DQN, which requires 3 days'
    training. You will learn how to implement A3C using Python and TensorFlow. This
    chapter does not require as much of a mathematical background as the previous
    chapter—just have fun!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一个更复杂的例子——Minecraft，这是由瑞典游戏开发者Markus Persson创作并由Mojang开发的热门在线视频游戏。你将学习如何使用OpenAI
    Gym启动Minecraft环境，并完成不同的任务。为了构建一个AI玩家来完成这些任务，你将学习**异步优势行为者-批评者**（**A3C**）算法，这是一个轻量级的深度强化学习框架，使用异步梯度下降优化深度神经网络控制器。A3C是广泛应用的深度强化学习算法，可以在单个多核CPU上训练一半的时间，而不是GPU。对于像Breakout这样的Atari游戏，A3C在训练3小时后即可达到人类水平的表现，远比DQN需要的3天训练时间要快。你将学习如何使用Python和TensorFlow实现A3C。本章对数学背景的要求不如上一章那么高——尽情享受吧！
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to the Minecraft environment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minecraft环境简介
- en: Data preparation for training an AI bot in the Minecraft environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Minecraft环境中为训练AI机器人准备数据
- en: The asynchronous advantage actor-critic framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步优势行为者-批评者框架
- en: Implementation of the A3C framework
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A3C框架的实现
- en: Introduction to the Minecraft environment
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Minecraft环境简介
- en: The original OpenAI Gym does not contain the Minecraft environment. We need
    to install a Minecraft environment bundle, available at [https://github.com/tambetm/gym-minecraft](https://github.com/tambetm/gym-minecraft).
    This bundle is built based on Microsoft's Malmö, which is a platform for AI experimentation
    and research built on top of Minecraft.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的OpenAI Gym不包含Minecraft环境。我们需要安装一个Minecraft环境包，地址为[https://github.com/tambetm/gym-minecraft](https://github.com/tambetm/gym-minecraft)。这个包是基于微软的Malmö构建的，Malmö是一个建立在Minecraft之上的AI实验和研究平台。
- en: 'Before installing the `gym-minecraft` package, Malmö should first be downloaded
    from [https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo).
    We can download the latest pre-built version from [https://github.com/Microsoft/malmo/releases](https://github.com/Microsoft/malmo/releases).
    After unzipping the package, go to the `Minecraft` folder and run `launchClient.bat`
    on Windows, or `launchClient.sh` on Linux/MacOS, to launch a Minecraft environment.
    If it is successfully launched, we can now install `gym-minecraft` via the following
    scripts:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装`gym-minecraft`包之前，首先需要从[https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo)下载Malmö。我们可以从[https://github.com/Microsoft/malmo/releases](https://github.com/Microsoft/malmo/releases)下载最新的预构建版本。解压包后，进入`Minecraft`文件夹，在Windows上运行`launchClient.bat`，或者在Linux/MacOS上运行`launchClient.sh`，以启动Minecraft环境。如果成功启动，我们现在可以通过以下脚本安装`gym-minecraft`：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we can run the following code to test whether `gym-minecraft` has been
    successfully installed or not:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以运行以下代码来测试`gym-minecraft`是否已经成功安装：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `gym-minecraft` package provides 15 different missions, including `MinecraftDefaultWorld1-v0` and `MinecraftBasic-v0`.
    For example, in `MinecraftBasic-v0`, the agent can move around in a small chamber
    with a box placed in the corner, and the goal is to reach the position of this
    box. The following screenshots show several missions available in `gym-minecraft`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym-minecraft`包提供了15个不同的任务，包括`MinecraftDefaultWorld1-v0`和`MinecraftBasic-v0`。例如，在`MinecraftBasic-v0`中，代理可以在一个小房间内移动，房间角落放置着一个箱子，目标是到达这个箱子的位置。以下截图展示了`gym-minecraft`中可用的几个任务：'
- en: '![](img/9da0172a-3067-4be7-8bbf-d74640b8cf64.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9da0172a-3067-4be7-8bbf-d74640b8cf64.png)'
- en: 'The `gym-minecraft` package has the same interface as other Gym environments,
    such as Atari and classic control tasks. You can run the following code to test
    different Minecraft missions and try to get a high-level understanding of their
    properties, for example, goal, reward, and observation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym-minecraft`包与其他Gym环境（例如Atari和经典控制任务）具有相同的接口。你可以运行以下代码来测试不同的Minecraft任务，并尝试对它们的属性（例如目标、奖励和观察）进行高层次的理解：'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At each step, an action is randomly drawn from the action space by calling `env.action_space.sample()`,
    and then this action is submitted to the system by calling the `env.step(action)` function,
    which returns the observation and the reward corresponding to this action. You
    can also try other missions by replacing `MinecraftBasic-v0` with other names,
    for example, `MinecraftMaze1-v0` and `MinecraftObstacles-v0`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，通过调用`env.action_space.sample()`从动作空间中随机抽取一个动作，然后通过调用`env.step(action)`函数将该动作提交给系统，该函数会返回与该动作对应的观察结果和奖励。你也可以通过将`MinecraftBasic-v0`替换为其他名称来尝试其他任务，例如，`MinecraftMaze1-v0`和`MinecraftObstacles-v0`。
- en: Data preparation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: In the Atari environment, recall that there are three modes for each Atari game,
    for example, Breakout, BreakoutDeterministic, and BreakoutNoFrameskip, and each
    mode has two versions, for example, Breakout-v0 and Breakout-v4\. The main difference
    between the three modes is the frameskip parameter that indicates the number of
    frames (steps) the one action is repeated on. This is called the **frame-skipping**
    technique, which allows us to play more games without significantly increasing
    the runtime.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atari环境中，请记住每个Atari游戏都有三种模式，例如，Breakout、BreakoutDeterministic和BreakoutNoFrameskip，每种模式又有两个版本，例如，Breakout-v0和Breakout-v4。三种模式之间的主要区别是frameskip参数，它表示一个动作在多少帧（步长）上重复。这就是**跳帧**技术，它使我们能够在不显著增加运行时间的情况下玩更多游戏。
- en: 'However, in the Minecraft environment, there is only one mode where the frameskip
    parameter is equal to one. Therefore, in order to apply the frame-skipping technique,
    we need to explicitly repeat a certain action frameskip multiple times during
    one timestep. Besides this, the frame images returned by the `step` function are
    RGB images. Similar to the Atari environment, the observed frame images are converted
    to grayscale and then resized to 84x84\. The following code provides the wrapper
    for `gym-minecraft`, which contains all the data preprocessing steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在Minecraft环境中，只有一种模式下frameskip参数等于1。因此，为了应用跳帧技术，我们需要在每个时间步中显式地重复某个动作多个frameskip次数。除此之外，`step`函数返回的帧图像是RGB图像。类似于Atari环境，观察到的帧图像会被转换为灰度图像，并且被调整为84x84的大小。以下代码提供了`gym-minecraft`的包装器，其中包含了所有的数据预处理步骤：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the constructor, the available actions for Minecraft are restricted to `move`
    and `turn` (not considering other actions, such as the camera controls). Converting
    an RGB image into a grayscale image is quite easy. Given an RGB image with shape
    (height, width, channel), the `rgb_to_gray` function is used to convert an image
    to grayscale. For cropping and reshaping frame images, we use the `opencv-python`
    or `cv2` packages, which contain a Python wrapper around the original C++ OpenCV
    implementation, that is, the `crop` function reshapes an image into an 84x84 matrix.
    Unlike the Atari environment, where `crop_offset` is set to `8` to remove the
    scoreboard from the screen, here, we set `crop_offset` to `0` and just reshape
    the frame images.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，Minecraft的可用动作被限制为`move`和`turn`（不考虑其他动作，如相机控制）。将RGB图像转换为灰度图像非常简单。给定一个形状为（高度，宽度，通道）的RGB图像，`rgb_to_gray` 函数用于将图像转换为灰度图像。对于裁剪和重塑帧图像，我们使用`opencv-python`或`cv2`包，它们包含原始C++
    OpenCV实现的Python封装，即`crop` 函数将图像重塑为84x84的矩阵。与Atari环境不同，在Atari环境中，`crop_offset`设置为`8`，以去除屏幕上的得分板，而在这里，我们将`crop_offset`设置为`0`，并只是重塑帧图像。
- en: The `play_action` function submits the input action to the Minecraft environment
    and returns the corresponding reward, observation, and termination signal. The
    default frameskip parameter is set to `4`, meaning that one action is repeated
    four times for each `play_action` call. The `get_current_feedback` function returns
    the observation that stacks the last four frame images together, since only considering
    the current frame image is not enough for playing Minecraft because it doesn't
    contain dynamic information about the game status.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`play_action` 函数将输入的动作提交给Minecraft环境，并返回相应的奖励、观察值和终止信号。默认的帧跳参数设置为`4`，意味着每次调用`play_action`时，动作会重复四次。`get_current_feedback` 函数返回将最后四帧图像堆叠在一起的观察值，因为仅考虑当前帧图像不足以玩Minecraft，因为它不包含关于游戏状态的动态信息。'
- en: This wrapper has the same interface as the wrappers for the Atari environment
    and classic control tasks. Therefore, you can try to run DQN or TRPO with the
    Minecraft environment without changing anything. If you have one idle GPU, it
    is better to run DQN first before trying the A3C algorithm that we will discuss
    next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个封装器与Atari环境和经典控制任务的封装器具有相同的接口。因此，你可以尝试在Minecraft环境中运行DQN或TRPO，而无需做任何更改。如果你有一块空闲的GPU，最好先运行DQN，然后再尝试我们接下来讨论的A3C算法。
- en: Asynchronous advantage actor-critic algorithm
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步优势演员-评论员算法
- en: 'In the previous chapters, we discussed the DQN for playing Atari games and
    the use of the DPG and TRPO algorithms for continuous control tasks. Recall that
    DQN has the following architecture:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了用于玩Atari游戏的DQN，以及用于连续控制任务的DPG和TRPO算法。回顾一下，DQN的架构如下：
- en: '![](img/29dfe3cd-6d21-40c9-b4c1-9279fcee5630.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29dfe3cd-6d21-40c9-b4c1-9279fcee5630.png)'
- en: At each timestep ![](img/41ed7ed5-deb3-4fd3-a97a-40bb6de0ee10.png), the agent
    observes the frame image ![](img/28fd3757-7d72-4c43-bae2-e929925fd291.png) and
    selects an action ![](img/67c43cdd-89c8-4e29-a2ef-0df2ebe4ae8d.png) based on the
    current learned policy. The emulator (the Minecraft environment) executes this
    action and returns the next frame image ![](img/7a0548c3-d809-4999-8498-143237b64fd8.png)
    and the corresponding reward ![](img/a3a2e63a-aff0-4b10-9e59-181e45fe8f43.png).
    The quadruplet ![](img/38c725ab-c097-44a5-989c-4387ba094df0.png) is then stored
    in the experience memory and is taken as a sample for training the Q-network by
    minimizing the empirical loss function via stochastic gradient descent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长![](img/41ed7ed5-deb3-4fd3-a97a-40bb6de0ee10.png)，智能体观察到帧图像![](img/28fd3757-7d72-4c43-bae2-e929925fd291.png)，并根据当前学习到的策略选择一个动作![](img/67c43cdd-89c8-4e29-a2ef-0df2ebe4ae8d.png)。模拟器（Minecraft环境）执行该动作并返回下一帧图像![](img/7a0548c3-d809-4999-8498-143237b64fd8.png)以及相应的奖励![](img/a3a2e63a-aff0-4b10-9e59-181e45fe8f43.png)。然后，将四元组![](img/38c725ab-c097-44a5-989c-4387ba094df0.png)存储在经验记忆中，并作为训练Q网络的样本，通过最小化经验损失函数进行随机梯度下降。
- en: 'Deep reinforcement learning algorithms based on experience replay have achieved
    unprecedented success in playing Atari games. However, experience replay has several
    disadvantages:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于经验回放的深度强化学习算法在玩Atari游戏方面取得了前所未有的成功。然而，经验回放有几个缺点：
- en: It uses more memory and computation per real interaction
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在每次真实交互中需要更多的内存和计算
- en: It requires off-policy learning algorithms that can update from data generated
    by an older policy
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要能够从由旧策略生成的数据中进行更新的离策略学习算法
- en: In order to reduce memory consumption and accelerate the training of an AI agent,
    Mnih et al. proposed an A3C framework for deep reinforcement learning that dramatically
    reduces the training time without performance loss. This work, *Asynchronous Methods
    for Deep Reinforcement Learning*, was published in ICML, 2016.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少内存消耗并加速AI智能体的训练，Mnih 等人提出了一种A3C框架，用于深度强化学习，能够显著减少训练时间而不会损失性能。该工作《*深度强化学习的异步方法*》发表于2016年ICML。
- en: Instead of experience replay, A3C asynchronously executes multiple agents in
    parallel on multiple instances of the environment, such as the Atari or Minecraft
    environments. Since the parallel agents experience a variety of different states,
    this parallelism breaks the correlation between the training samples and stabilizes
    the training procedure, which means that the experience memory can be removed.
    This simple idea enables a much larger spectrum of fundamental on-policy reinforcement
    learning algorithms, such as Sarsa and actor-critic methods, as well as off-policy
    reinforcement learning algorithms, such as Q-learning, to be applied robustly
    and effectively using deep neural networks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: A3C不是使用经验回放，而是异步地在多个环境实例上并行执行多个智能体，如Atari或Minecraft环境。由于并行智能体经历了多种不同的状态，这种并行性打破了训练样本之间的相关性，从而稳定了训练过程，这意味着可以去除经验记忆。这个简单的想法使得许多基础的强化学习算法（如Sarsa和actor-critic方法）以及离策略强化学习算法（如Q-learning）能够通过深度神经网络得到强健且有效的应用。
- en: Another advantage is that A3C is able to run on a standard multi-core CPU without
    relying on GPUs or massively distributed architectures, and requires far less
    training time than GPU-based algorithms, such as DQN, when applied to Atari games.
    A3C is good for a beginner in deep reinforcement learning since you can apply
    it to Atari games on a standard PC with multiple cores. For example, for Breakout,
    it takes only two-three hours to achieve a score of 300 when executing eight agents
    in parallel.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优势是，A3C能够在标准的多核CPU上运行，而不依赖于GPU或大规模分布式架构，并且在应用于Atari游戏时，所需的训练时间比基于GPU的算法（如DQN）要少得多。A3C适合深度强化学习的初学者，因为你可以在具有多个核心的标准PC上应用它，进行Atari游戏的训练。例如，在Breakout中，当执行八个智能体并行时，只需两到三小时就能达到300分。
- en: 'In this chapter, we will use the same notations as before. At each timestep
    ![](img/45da604e-0854-485a-a60c-23defff392c8.png), the agent observes state ![](img/5be1b484-ebbe-4e3f-ac32-57aaee313588.png),
    takes action ![](img/34265388-a7e5-448c-a166-b266407f287f.png), and then receives
    the corresponding reward ![](img/a7721b22-193a-45c9-8a3c-c67ad83015cb.png) generated
    from a function ![](img/d527382a-d269-415b-879f-c3bc82294fc8.png). We use ![](img/5c97dded-1a77-48a2-814e-1983905130ed.png)
    to denote the policy of the agent, which maps states to a probability distribution
    over the actions. The Bellman equation is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用与之前相同的符号。在每个时间步 ![](img/45da604e-0854-485a-a60c-23defff392c8.png)，智能体观察到状态 ![](img/5be1b484-ebbe-4e3f-ac32-57aaee313588.png)，采取行动 ![](img/34265388-a7e5-448c-a166-b266407f287f.png)，然后从函数 ![](img/d527382a-d269-415b-879f-c3bc82294fc8.png)
    生成的相应奖励 ![](img/a7721b22-193a-45c9-8a3c-c67ad83015cb.png) 中获得反馈。我们使用 ![](img/5c97dded-1a77-48a2-814e-1983905130ed.png)
    来表示智能体的策略，该策略将状态映射到动作的概率分布。贝尔曼方程如下：
- en: '![](img/d3db8e06-3cf6-4ae8-929e-aef016a7b54b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3db8e06-3cf6-4ae8-929e-aef016a7b54b.png)'
- en: 'The state-action value function ![](img/d12ee59d-1fcb-4cd7-bc9c-bd9b56094d6d.png)
    can be approximated by a neural network parameterized by ![](img/bed31c8b-585c-4c44-b793-05c98ef1a50c.png),
    and the policy ![](img/ebacb345-5c0b-40ee-98cd-4a9e7d7685cc.png) can also be represented
    by another neural network parameterized by ![](img/fa647406-dbbe-4c4e-a3d7-f929057aee9a.png).
    Then, ![](img/c65564c6-d8ca-4147-ba07-5c303b533323.png) can be be trained by minimizing
    the following loss function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 状态-行动值函数 ![](img/d12ee59d-1fcb-4cd7-bc9c-bd9b56094d6d.png) 可以通过由 ![](img/bed31c8b-585c-4c44-b793-05c98ef1a50c.png)
    参数化的神经网络来逼近，策略 ![](img/ebacb345-5c0b-40ee-98cd-4a9e7d7685cc.png) 也可以通过另一个由 ![](img/fa647406-dbbe-4c4e-a3d7-f929057aee9a.png)
    参数化的神经网络来表示。然后， ![](img/c65564c6-d8ca-4147-ba07-5c303b533323.png) 可以通过最小化以下损失函数来训练：
- en: '![](img/a34c51ab-d44d-4ee3-9814-987ae393b567.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a34c51ab-d44d-4ee3-9814-987ae393b567.png)'
- en: '![](img/e2dc75c1-f65d-4b16-91b5-74e3bc874fc0.png) is the approximated state-action
    value function at step ![](img/732fb339-3138-4c91-909c-4bb069c0c7ff.png). In one-step
    Q-learning such as DQN, ![](img/9ffb3f36-dc7c-46f9-8ebd-2dd889a37a22.png) equals
    ![](img/ea64e07b-04f6-48ca-b25f-b57528eed50f.png), so that the following is true:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/e2dc75c1-f65d-4b16-91b5-74e3bc874fc0.png) 是在第 ![](img/732fb339-3138-4c91-909c-4bb069c0c7ff.png)
    步的近似状态-动作值函数。在单步 Q 学习（如 DQN）中，![](img/9ffb3f36-dc7c-46f9-8ebd-2dd889a37a22.png)
    等于 ![](img/ea64e07b-04f6-48ca-b25f-b57528eed50f.png)，因此以下公式成立：'
- en: '![](img/2936efe7-364f-497b-9fba-c6a84fd299ff.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2936efe7-364f-497b-9fba-c6a84fd299ff.png)'
- en: 'One drawback of using one-step Q-learning is that obtaining a reward ![](img/e09ee797-bb93-440b-bff7-7951332b22a0.png)
    only directly affects the value of the state action pair ![](img/6b4d4964-7ff3-4316-b58e-00b0846092cc.png)
    that led to the reward. This can make the learning process slow since many updates
    are required to propagate a reward to the relevant preceding states and actions.
    One way of propagating rewards faster is by using n-step returns. In n-step Q-learning, ![](img/caca62a4-f2dc-4df3-9cf8-d6dc2c809c77.png)
    can be set to this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单步 Q 学习的一个缺点是，获得的奖励 ![](img/e09ee797-bb93-440b-bff7-7951332b22a0.png) 只直接影响导致该奖励的状态动作对
    ![](img/6b4d4964-7ff3-4316-b58e-00b0846092cc.png) 的值。这可能导致学习过程变慢，因为需要进行大量更新才能将奖励传播到相关的前置状态和动作。加快奖励传播的一种方法是使用
    n 步回报。在 n 步 Q 学习中，![](img/caca62a4-f2dc-4df3-9cf8-d6dc2c809c77.png) 可以设置为：
- en: '![](img/ef0542d8-0f10-454d-8143-a06eb7c55615.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef0542d8-0f10-454d-8143-a06eb7c55615.png)'
- en: As opposed to value-based methods, a policy-based method, such as TRPO, directly
    optimizes the policy network ![](img/572b6b4e-0f30-4124-9b22-4898ae972d0f.png).
    Besides TRPO, a much simpler method is REINFORCE, which updates the policy parameter ![](img/603c793f-3279-4b6f-9432-da8a0c1565d7.png)
    in the direction ![](img/3bb579e0-2d10-4d82-b2b1-31fec55d2f2f.png), where ![](img/d4fccc88-5310-4eb7-a341-e5edb263382d.png)
    is the the advantage of action ![](img/648a4f3f-f5df-4f96-a31b-6c5a0216537c.png)
    in state ![](img/88de09b7-bcde-459f-ba1c-d29b81f0a05d.png). This method is an
    actor-critic approach due to the fact that it is required to estimate the value
    function ![](img/e39c8f90-7f7f-468a-8623-b840ca133089.png) and the policy ![](img/afbdb39f-4862-40dc-96d8-b9fc6a6cdf4d.png).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于价值的方法不同，基于策略的方法，例如 TRPO，直接优化策略网络 ![](img/572b6b4e-0f30-4124-9b22-4898ae972d0f.png)。除了
    TRPO，更简单的方法是 REINFORCE，它通过更新策略参数 ![](img/603c793f-3279-4b6f-9432-da8a0c1565d7.png)
    在方向 ![](img/3bb579e0-2d10-4d82-b2b1-31fec55d2f2f.png) 上进行更新，其中 ![](img/d4fccc88-5310-4eb7-a341-e5edb263382d.png)
    是在状态 ![](img/648a4f3f-f5df-4f96-a31b-6c5a0216537c.png) 下采取动作 ![](img/88de09b7-bcde-459f-ba1c-d29b81f0a05d.png)
    的优势。该方法属于演员-评论员方法，因为它需要估计价值函数 ![](img/e39c8f90-7f7f-468a-8623-b840ca133089.png)
    和策略 ![](img/afbdb39f-4862-40dc-96d8-b9fc6a6cdf4d.png)。
- en: 'The asynchronous reinforcement learning framework can be applied in the approaches
    already discussed here. The main idea is that we run multiple agents in parallel
    with their own instances of the environment, for example, multiple players play
    the same game using their own games consoles. These agents are likely to be exploring
    different parts of the environment. The parameters ![](img/40e8244b-9873-4b75-9fc6-2072ce8647e4.png)
    and ![](img/b71f66bd-8b71-4552-a87b-de87ced4eedf.png) are shared among all agents.
    Each agent updates the policy and the value function asynchronously without considering
    read–write conflicts. Although it seems weird that there is no synchronization
    in updating the policy, this asynchronous method not only removes the communication
    costs of sending gradients and parameters, but also guarantees the convergence.
    For more details, please refer to the following paper: *A lock-free approach to
    parallelizing stochastic gradient descent*, Recht et al. This chapter focuses
    on  A3C, namely, we apply the asynchronous reinforcement learning framework in
    REINFORCE. The following diagram shows the A3C architecture:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 异步强化学习框架可以应用于之前讨论过的方法中。其主要思想是我们并行运行多个代理，每个代理拥有自己独立的环境实例，例如，多个玩家使用自己的游戏机玩同一款游戏。这些代理可能在探索环境的不同部分。参数
    ![](img/40e8244b-9873-4b75-9fc6-2072ce8647e4.png) 和 ![](img/b71f66bd-8b71-4552-a87b-de87ced4eedf.png)
    在所有代理之间共享。每个代理异步地更新策略和价值函数，而不考虑读写冲突。虽然没有同步更新策略似乎很奇怪，但这种异步方法不仅消除了发送梯度和参数的通信成本，而且还保证了收敛性。更多细节请参阅以下论文：《*一种无锁方法并行化随机梯度下降*》，Recht
    等人。本章聚焦于 A3C，即我们在 REINFORCE 中应用异步强化学习框架。下图展示了 A3C 架构：
- en: '![](img/dbf41bca-f28f-4f1d-9dec-4743f801d062.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dbf41bca-f28f-4f1d-9dec-4743f801d062.png)'
- en: 'For A3C, the policy ![](img/63257f79-470b-4c26-ba1c-e167c49ff93c.png) and the
    value function ![](img/c3d8f91a-a788-4fb6-8200-57c127489e82.png) are approximated
    by two neural networks. A3C updates the policy parameter ![](img/5a301c9e-5246-4be5-b347-6f53e3b147b0.png)
    in the direction ![](img/25d10240-2cd5-4a76-99fd-3ea072246796.png), where ![](img/fdf6594a-5e6c-403e-b0bf-01ad6fcaed65.png)
    is fixed, which is estimated by the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于A3C，策略 ![](img/63257f79-470b-4c26-ba1c-e167c49ff93c.png) 和价值函数 ![](img/c3d8f91a-a788-4fb6-8200-57c127489e82.png)
    是通过两个神经网络来近似的。A3C 更新策略参数 ![](img/5a301c9e-5246-4be5-b347-6f53e3b147b0.png) 的方向是 ![](img/25d10240-2cd5-4a76-99fd-3ea072246796.png)，其中 ![](img/fdf6594a-5e6c-403e-b0bf-01ad6fcaed65.png)
    是固定的，估算方法如下：
- en: '![](img/183390c0-555b-4a40-8872-bde1af962b26.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/183390c0-555b-4a40-8872-bde1af962b26.png)'
- en: 'A3C updates the value function parameter ![](img/491def82-0543-4635-b454-849b899a1f61.png)
    by minimizing the loss:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: A3C通过最小化损失来更新价值函数参数 ![](img/491def82-0543-4635-b454-849b899a1f61.png)：
- en: '![](img/e15cf3b5-e3a0-447d-8358-9259ce6e3bef.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e15cf3b5-e3a0-447d-8358-9259ce6e3bef.png)'
- en: '![](img/7f7fcf68-337b-4497-93af-a621b7f57365.png) is computed via the previous
    estimate. To encourage exploration during training, the entropy of the policy ![](img/c8c82e94-b736-487e-b11b-03699f589607.png)
    is also added to the policy update, acting as a regularization term. Then, the
    gradient for the policy update becomes the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7f7fcf68-337b-4497-93af-a621b7f57365.png) 是通过之前的估算计算得出的。为了在训练过程中鼓励探索，策略的熵 ![](img/c8c82e94-b736-487e-b11b-03699f589607.png)
    也被加入到策略更新中，作为正则化项。然后，策略更新的梯度变成以下形式：'
- en: '![](img/3363f84e-9978-4091-9b6b-eeb53f33e1c2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3363f84e-9978-4091-9b6b-eeb53f33e1c2.png)'
- en: 'The following pseudo code shows the A3C algorithm for each agent (thread):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下伪代码展示了每个代理（线程）的A3C算法：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A3C uses ADAM or RMSProp to perform an asynchronous update of the parameters.
    For different environments, it is hard to tell which method leads to better performance.
    We can use RMSProp for the Atari and Minecraft environments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: A3C使用ADAM或RMSProp来执行参数的异步更新。对于不同的环境，很难判断哪种方法能带来更好的性能。我们可以在Atari和Minecraft环境中使用RMSProp。
- en: Implementation of A3C
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A3C的实现
- en: 'We will now look at how to implement A3C using Python and TensorFlow. Here,
    the policy network and value network share the same feature representation. We
    implement two kinds of policies: one is based on the CNN architecture used in
    DQN, and the other is based on LSTM.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何使用Python和TensorFlow实现A3C。在这里，策略网络和价值网络共享相同的特征表示。我们实现了两种不同的策略：一种基于DQN中使用的CNN架构，另一种基于LSTM。
- en: 'We implement the `FFPolicy` class for the policy based on CNN:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了基于CNN的策略的 `FFPolicy` 类：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The constructor requires three arguments:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数需要三个参数：
- en: '`input_shape`'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`input_shape`'
- en: '`n_outputs`'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_outputs`'
- en: '`network_type`'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`network_type`'
- en: '`input_shape` is the size of the input image. After data preprocessing, the
    input is an 84x84x4 image, so the default parameter is (84, 84, 4). `n_outputs` is
    the number of all the available actions. `network_type` indicates the type of
    the feature representation we want to use. Our implementation contains two different
    networks. One is the CNN architecture used in DQN. The other is a feedforward
    neural network used for testing.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_shape` 是输入图像的大小。经过数据预处理后，输入为84x84x4的图像，因此默认参数为(84, 84, 4)。`n_outputs`
    是所有可用动作的数量。`network_type` 指示我们希望使用的特征表示类型。我们的实现包含两种不同的网络。一种是DQN中使用的CNN架构，另一种是用于测试的前馈神经网络。'
- en: 'In the constructor, the `x` variable represents the input state (a batch of
    84x84x4 images). After creating the input tensors, the `build_model` function
    is called to build the policy and value network. Here is the `build_model`:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构造函数中，`x` 变量表示输入状态（一个84x84x4的图像批次）。在创建输入张量之后，调用 `build_model` 函数来构建策略和价值网络。以下是
    `build_model`：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The CNN architecture contains two convolutional layers and one hidden layer,
    while the feedforward architecture contains two hidden layers. As discussed previously,
    the policy network and the value network share the same feature representation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构包含两个卷积层和一个隐藏层，而前馈架构包含两个隐藏层。如前所述，策略网络和价值网络共享相同的特征表示。
- en: 'The loss function for updating the network parameters can be constructed via
    the following function:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于更新网络参数的损失函数可以通过以下函数构建：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This function creates three input tensors:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此函数创建三个输入张量：
- en: '`action`'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`action`'
- en: '`reward`'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`reward`'
- en: '`advantage`'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`advantage`'
- en: The `action` variable represents the selected actions ![](img/70b47b9a-88ae-42c6-ad2c-83613ebe8d67.png).
    The `reward` variable is the discounted cumulative reward ![](img/a7d4f306-f0a7-4553-8f42-733a355c378a.png)in
    the preceding A3C algorithm. The `advantage` variable is the advantage function
    computed by ![](img/a2dc3329-3c39-43bd-b176-903ab799c8b7.png). In this implementation,
    the losses of the policy and the value function are combined together, since the
    feature representation layers are shared.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`action`变量表示选择的动作！[](img/70b47b9a-88ae-42c6-ad2c-83613ebe8d67.png)。`reward`变量是前述A3C算法中的折扣累计奖励！[](img/a7d4f306-f0a7-4553-8f42-733a355c378a.png)。`advantage`变量是通过！[](img/a2dc3329-3c39-43bd-b176-903ab799c8b7.png)计算的优势函数。在这个实现中，策略损失和价值函数损失被合并，因为特征表示层是共享的。'
- en: Therefore, instead of updating the `policy` parameter and the `value` parameter
    separately, our implementation updates these parameters simultaneously. This function
    also creates `summary_op` for TensorBoard visualization.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们的实现不是分别更新`policy`参数和`value`参数，而是同时更新这两个参数。这个函数还为TensorBoard可视化创建了`summary_op`。
- en: 'The implementation of the LSTM policy is quite similar to the feedforward policy.
    The main difference is the `build_model` function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM策略的实现与前馈策略相似，主要区别在于`build_model`函数：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this function, a LSTM layer follows the feature representation layers. In
    TensorFlow, you can easily create a LSTM layer by constructing `BasicLSTMCell`
    and then calling `tf.nn.dynamic_rnn` to get the layer outputs. `tf.nn.dynamic_rnn` returns
    the output for each time step and the final cell state.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，LSTM层紧随特征表示层。在TensorFlow中，可以通过构造`BasicLSTMCell`并调用`tf.nn.dynamic_rnn`来轻松创建LSTM层，进而得到层的输出。`tf.nn.dynamic_rnn`返回每个时间步的输出和最终的单元状态。
- en: 'We now implement the main A3C algorithm—the `A3C` class:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实现A3C算法的主要部分——`A3C`类：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `system` parameter is the emulator, either the Atari environment or Minecraft
    environment. `directory` indicates the folder for the saved model and logs. `param`
    includes all the training parameters of A3C, for example, the batch size and learning
    rate. `agent_index` is the label for one agent. The constructor calls `init_network`
    to initialize the policy network and the value network. Here is the implementation
    of `init_network`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`system`参数是模拟器，可以是Atari环境或Minecraft环境。`directory`表示保存模型和日志的文件夹。`param`包含A3C的所有训练参数，例如批量大小和学习率。`agent_index`是智能体的标签。构造函数调用`init_network`来初始化策略网络和值网络。以下是`init_network`的实现：'
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The tricky part in this function is how to implement the global shared parameters.
    In TensorFlow, we can do this with the `tf.train.replica_device_setter` function. We
    first create a `global` device shared among all the agents. Within this device,
    the global shared network is created. Then, we create a local device and a local
    network for each agent. To synchronize the global and local parameters, `update_local_ops` is
    created by calling the `update_target_graph` function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数中比较棘手的部分是如何实现全局共享参数。在TensorFlow中，我们可以通过`tf.train.replica_device_setter`函数来实现这一点。我们首先创建一个在所有智能体之间共享的`global`设备。在这个设备内，创建了全局共享网络。然后，为每个智能体创建一个本地设备和本地网络。为了同步全局和本地参数，通过调用`update_target_graph`函数来创建`update_local_ops`：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, the `gradients` op is constructed by calling `build_gradient_op`, which
    is used to compute the gradient update for each agent. With `gradients`, an optimizer
    is built via the `create_optimizer` function that is used for updating the global
    shared parameters. The `create_optimizer` function is used as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过调用`build_gradient_op`构建`gradients`操作，该操作用于计算每个智能体的梯度更新。通过`gradients`，使用`create_optimizer`函数构建优化器，用于更新全局共享参数。`create_optimizer`函数的使用方式如下：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The main function in A3C is `run`, which starts and trains the agent:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: A3C中的主要功能是`run`，用于启动并训练智能体：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At each timestep, it calls `choose_action` to select an action according to
    the current policy, and executes this action by calling `play`. Then, the received
    reward, the new state, and the termination signal, as well as the current state
    and the selected action, are stored in the `replay_memory`, which records the
    trajectory that the agent visited. Given this trajectory, it then calls `n_step_q_learning` to
    estimate the cumulative reward and the `advantage` function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，它调用`choose_action`根据当前策略选择一个动作，并通过调用`play`执行这个动作。然后，将接收到的奖励、新的状态、终止信号，以及当前状态和选择的动作存储在`replay_memory`中，这个内存记录了智能体访问的轨迹。给定这条轨迹，它接着调用`n_step_q_learning`来估计累计奖励和`advantage`函数：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It then updates the global shared parameters by calling `train`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它通过调用`train`来更新全局共享参数：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that the model will be saved on the disk after 40,000 updates, and an evaluation
    procedure starts after `self.eval_frequency` updates.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型将在 40,000 次更新后保存到磁盘，并且在`self.eval_frequency`次更新后开始评估过程。
- en: 'To launch one agent, we can run the following codes written in the `worker.py` file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个智能体，我们可以运行以下写在`worker.py`文件中的代码：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The main function will create a new agent and begin the training procedure if
    the `job_name` parameter is `worker`. Otherwise, it will start the TensorFlow
    parameter server for the global shared parameters. Notice that before launching
    multiple agents, we need to start the parameter server first. In the `train` function,
    an environment is created by calling `new_environment` and then an agent is built
    for this environment. After the agent is successfully created, the global shared
    parameters are initialized and the train procedure starts by calling `a3c.run(sess,
    saver)`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 主函数将在`job_name`参数为`worker`时创建一个新的智能体并开始训练过程。否则，它将启动 TensorFlow 参数服务器，用于全局共享参数。注意，在启动多个智能体之前，我们需要先启动参数服务器。在`train`函数中，通过调用`new_environment`来创建环境，然后为该环境构建智能体。智能体成功创建后，初始化全局共享参数，并通过调用`a3c.run(sess,
    saver)`开始训练过程。
- en: 'Because manually launching 8 or 16 agents is quite inconvenient, this can be
    done automatically by the following script:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于手动启动 8 或 16 个智能体非常不方便，可以通过以下脚本自动执行此操作：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This script creates the bash commands used to create the parameter server and
    a set of agents. To handle the consoles of all the agents, we use TMUX (more information
    is available at [https://github.com/tmux/tmux/wiki](https://github.com/tmux/tmux/wiki)).
    TMUX is a terminal multiplexer that allows us to switch easily between several
    programs in one terminal, detach them, and reattach them to a different terminal.
    TMUX is quite a convenient tool for checking the training status of A3C. Note
    that since A3C runs on CPUs, we set `CUDA_VISIBLE_DEVICES` to empty.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本创建了用于创建参数服务器和一组智能体的 bash 命令。为了处理所有智能体的控制台，我们使用 TMUX（更多信息请参见[https://github.com/tmux/tmux/wiki](https://github.com/tmux/tmux/wiki)）。TMUX
    是一个终端复用工具，允许我们在一个终端中轻松切换多个程序，分离它们，并将它们重新附加到不同的终端上。TMUX 对于检查 A3C 的训练状态是一个非常方便的工具。请注意，由于
    A3C 在 CPU 上运行，我们将`CUDA_VISIBLE_DEVICES`设置为空。
- en: 'A3C is much more sensitive to the training parameters than DQN. Random seed,
    initial weights, learning rate, batch size, discount factor, and even hyperparameters
    for RMSProp can affect the performance a lot. After testing it on different Atari
    games, we select the following hyperparameters listed in the `Parameter` class:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与 DQN 相比，A3C 对训练参数更加敏感。随机种子、初始权重、学习率、批量大小、折扣因子，甚至 RMSProp 的超参数都会极大地影响性能。在不同的
    Atari 游戏上测试后，我们选择了 `Parameter` 类中列出的以下超参数：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, `gamma` is the discount factor, `num_history_frames` is the parameter
    frameskip, `async_update_interval` is the batch size for the training update,
    and `rho` and `rmsprop_epsilon` are the internal hyperparameters for RMSProp.
    This set of hyperparameters can be used for both Atari and Minecraft.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`gamma`是折扣因子，`num_history_frames`是参数 frameskip，`async_update_interval`是训练更新的批量大小，`rho`和`rmsprop_epsilon`是
    RMSProp 的内部超参数。这组超参数可以用于 Atari 和 Minecraft。
- en: Experiments
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: 'The full implementation of the A3C algorithm can be downloaded from our GitHub
    repository ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)).
    There are three environments in our implementation we can test. The first one
    is the special game, `demo`, introduced in [Chapter 3](0cd8e82e-dbf2-42f6-a525-e8689cace21b.xhtml),
    *Playing Atari Games*. For this game, A3C only needs to launch two agents to achieve
    good performance. Run the following command in the `src` folder:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: A3C 算法的完整实现可以从我们的 GitHub 仓库下载（[https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)）。在我们的实现中，有三个环境可以进行测试。第一个是特别的游戏`demo`，它在[第
    3 章](0cd8e82e-dbf2-42f6-a525-e8689cace21b.xhtml)《玩 Atai 游戏》中介绍。对于这个游戏，A3C 只需要启动两个智能体就能取得良好的表现。在`src`文件夹中运行以下命令：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The first argument, `-w`, or `--num_workers`, indicates the number of launched
    agents. The second argument, `-e`, or `--env`, specifies the environment, for
    example, `demo`. The other two environments are Atari and Minecraft. For Atari
    games, A3C requires at least 8 agents running in parallel. Typically, launching
    16 agents can achieve better performance:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数`-w`或`--num_workers`表示启动的代理数量。第二个参数`-e`或`--env`指定环境，例如`demo`。其他两个环境是Atari和Minecraft。对于Atari游戏，A3C要求至少有8个代理并行运行。通常，启动16个代理可以获得更好的性能：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For Breakout, A3C takes about 2-3 hours to achieve a score of 300\. If you
    have a decent PC with more than 8 cores, it is better to test it with 16 agents.
    To test Minecraft, run the following command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Breakout，A3C大约需要2-3小时才能达到300分。如果你有一台性能不错的PC，且有超过8个核心，最好使用16个代理进行测试。要测试Minecraft，运行以下命令：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The Gym Minecraft environment provides more than 10 missions. To try other missions,
    just replace `MinecraftBasic-v0` with other mission names.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Gym Minecraft环境提供了超过10个任务。要尝试其他任务，只需将`MinecraftBasic-v0`替换为其他任务名称。
- en: 'After running one of the preceding commands, type the following to monitor
    the training procedure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述命令之一后，输入以下命令来监控训练过程：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To switch between console windows, press *Ctrl *+ *b* and then press *0*-*9*.
    Window 0 is the parameter server. Windows 1-8 show the training stats of the 8
    agents (if there are 8 launched agents). The last window runs htop. To detach
    TMUX, press *Ctrl* and then press *b*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台窗口之间切换，按*Ctrl* + *b*，然后按*0* - *9*。窗口0是参数服务器。窗口1-8显示8个代理的训练统计数据（如果启动了8个代理）。最后一个窗口运行htop。要分离TMUX，按*Ctrl*，然后按*b*。
- en: 'The `tensorboard` logs are saved in the `save/<environment_name>/train/log_<agent_index>` folder.
    To visualize the training procedure using TensorBoard, run the following command
    under this folder:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorboard`日志保存在`save/<environment_name>/train/log_<agent_index>`文件夹中。要使用TensorBoard可视化训练过程，请在该文件夹下运行以下命令：'
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced the Gym Minecraft environment, available at [https://github.com/tambetm/gym-minecraft](https://github.com/tambetm/gym-minecraft).
    You have learned how to launch a Minecraft mission and how to implement an emulator
    for it. The most important part of this chapter was the asynchronous reinforcement
    learning framework. You learned what the shortcomings of DQN are, and why DQN
    is difficult to apply in complex tasks. Then, you learned how to apply the asynchronous
    reinforcement learning framework in the actor-critic method REINFORCE, which led
    us to the A3C algorithm. Finally, you learned how to implement A3C using Tensorflow
    and how to handle multiple terminals using TMUX. The tricky part in the implementation
    is that of the global shared parameters. This is related to creating a cluster
    of TensorFlow servers. For the readers who want to learn more about this, visit
    [https://www.tensorflow.org/deploy/distributed](https://www.tensorflow.org/deploy/distributed).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Gym Minecraft环境，网址为[https://github.com/tambetm/gym-minecraft](https://github.com/tambetm/gym-minecraft)。你已经学会了如何启动Minecraft任务以及如何为其实现一个模拟器。本章最重要的部分是异步强化学习框架。你了解了DQN的不足之处，以及为什么DQN难以应用于复杂任务。接着，你学会了如何在行为者-评论员方法REINFORCE中应用异步强化学习框架，这引出了A3C算法。最后，你学会了如何使用TensorFlow实现A3C以及如何使用TMUX处理多个终端。实现中的难点是全局共享参数，这与创建TensorFlow服务器集群有关。对于想深入了解的读者，请访问[https://www.tensorflow.org/deploy/distributed](https://www.tensorflow.org/deploy/distributed)。
- en: In the following chapters, you will learn more about how to apply reinforcement
    learning algorithms in other tasks, for example, the board game Go, and generating
    deep image classifiers. This will help you to get a deep understanding about reinforcement
    learning and help you solve real-world problems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习更多关于如何在其他任务中应用强化学习算法，例如围棋和生成深度图像分类器。这将帮助你深入理解强化学习，并帮助你解决实际问题。
