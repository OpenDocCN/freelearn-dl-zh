- en: Chapter 5.  Restricted Boltzmann Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 限制玻尔兹曼机
- en: '|   | *"What I cannot create, I do not understand."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *"我无法创造的东西，我就无法理解。"* |   |'
- en: '|   | --*Richard Feynman* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*理查德·费曼* |'
- en: So far in this book, we have only discussed the discriminative models. The use
    of these in deep learning is to model the dependencies of an unobserved variable
    y on an observed variable *x*. Mathematically, it is formulated as *P(y|x)*. In
    this chapter, we will discuss deep generative models to be used in deep learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中仅讨论了判别模型。这些模型在深度学习中的应用是建模未观察到的变量 y 与已观察到的变量 *x* 之间的依赖关系。数学上，它被公式化为
    *P(y|x)*。在本章中，我们将讨论用于深度学习的深度生成模型。
- en: Generative models are models, which when given some hidden parameters, can randomly
    generate some observable data values out of them. The model works on a joint probability
    distribution over label sequences and observation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是指在给定一些隐藏参数时，可以随机生成一些可观测数据值的模型。该模型基于标签序列和观测的联合概率分布。
- en: The generative models are used in machine and deep learning either as an intermediate
    step to generate a conditional probability density function or modeling observations
    directly from a probability density function.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在机器学习和深度学习中的应用，可以作为生成条件概率密度函数的中间步骤，或直接从概率密度函数中建模观测值。
- en: '**Restricted Boltzmann machines** (**RBMs**) are a popular generative model
    that will be discussed in this chapter. RBMs are basically probabilistic graphical
    models that can also be interpreted as stochastic neural networks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**限制玻尔兹曼机**（**RBM**）是本章讨论的一个流行生成模型。RBM 基本上是概率图模型，也可以解释为随机神经网络。'
- en: Note
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Stochastic neural networks** can be defined as a type of artificial neural
    network that is generated by providing random variations into the network. The
    random variation can be supplied in various ways, such as providing stochastic
    weights or by giving a network''s neurons stochastic transfer functions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机神经网络**可以定义为一种通过向网络中引入随机变化生成的人工神经网络。随机变化可以通过多种方式提供，例如通过提供随机权重或通过为网络的神经元提供随机传输函数。'
- en: In this chapter, we will discuss, a special type of Boltzmann machine called
    RBM, which is the main topic of this chapter. We will discuss how **Energy-Based
    models** (**EBMs**) are related to RBM, and their functionalities. Later in this
    chapter, we will introduce **Deep Belief network** (**DBN**), which is an extension
    of the RBM. The chapter will then discuss the large-scale implementation of these
    in distributed environments. The chapter will conclude by giving examples of RBM
    and DBN with Deeplearning4j.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一种特殊类型的玻尔兹曼机，称为 RBM，这是本章的主要内容。我们将讨论**基于能量的模型**（**EBM**）如何与 RBM 相关，以及它们的功能。随后，我们将介绍**深度置信网络**（**DBN**），它是
    RBM 的扩展。接下来，本章将讨论这些模型在分布式环境中的大规模实现。最后，本章将通过使用 Deeplearning4j 来给出 RBM 和 DBN 的示例。
- en: 'The organization of this chapter is as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的组织结构如下：
- en: Energy-based models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于能量的模型
- en: Boltzmann machine
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玻尔兹曼机
- en: Restricted Boltzmann machine
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机
- en: Convolutional Restricted Boltzmann machine
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积限制玻尔兹曼机
- en: Deep Belief network
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度置信网络
- en: Distributed Deep Belief network
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式深度置信网络
- en: Implementation of RBM and DBN with Deeplearning4j
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Deeplearning4j 实现 RBM 和 DBN
- en: Energy-based models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于能量的模型
- en: The main goal of deep learning and statistical modeling is to encode the dependencies
    between variables. By getting an idea of those dependencies, from the values of
    the known variables, a model can answer questions about the unknown variables.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和统计建模的主要目标是编码变量之间的依赖关系。通过了解这些依赖关系，利用已知变量的值，模型可以回答关于未知变量的问题。
- en: '**Energy-based models** (**EBMs**) [120] gather and collect the dependencies
    by identifying scaler energy, which generally is a measure of compatibility to
    each configuration of the variable. In EBMs, the predictions are made by setting
    the value of observed variables and finding the value of the unobserved variables,
    which minimize the overall energy. Learning in EBMs consists of formulating an
    energy function, which assigns low energies to the correct values of unobserved
    variables and higher energies to the incorrect ones. Energy-based learning can
    be treated as an alternative to probabilistic estimation for classification, decision-making,
    or prediction tasks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于能量的模型**（**EBMs**）[120]通过识别标量能量来收集变量的依赖关系，能量通常是每个变量配置的兼容性度量。在EBM中，通过设置观察到的变量的值并找到未观察到的变量的值，从而最小化总体能量，进而做出预测。EBM中的学习包括制定一个能量函数，该函数将低能量分配给未观察到的变量的正确值，将较高能量分配给不正确的值。基于能量的学习可以视为分类、决策或预测任务中对概率估计的替代方法。'
- en: To give a clear idea about how EBMs work, let us look at a simple example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚地说明EBM是如何工作的，让我们看一个简单的例子。
- en: 'As shown in F*igure 5.1*, let us consider two sets of variables, observed and
    unobserved, namely, *X* and *Y* respectively. Variable *X*, in the figure, represents
    the collection of pixels from an image. Variable *Y* is discrete, and contains
    the possible categories of the object needed for classification. Variable *Y*
    in this case, consists of six possible values, namely: air-plane, animal, human,
    car, truck, and none of the above. The model is used as an energy function that
    will measure the correctness of the mapping between *X* and *Y*.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.1*所示，我们考虑两个变量集合，观察到的和未观察到的，分别为*X*和*Y*。图中的变量*X*表示图像的像素集合。变量*Y*是离散的，包含了对象分类所需的可能类别。在这种情况下，变量*Y*包含六个可能的值，即：飞机、动物、人类、汽车、卡车和以上都不是。该模型作为一个能量函数，用来衡量*X*和*Y*之间映射的正确性。
- en: 'The model uses a convention that small energy values imply highly related configuration
    of the variables. On the other hand, with the increasing energy values, the incompatibility
    of the variables also increases equally. The function that is related to both
    the *X* and *Y* variable is termed as energy function, denoted as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型采用一种约定，小的能量值表示变量的高度相关配置。另一方面，随着能量值的增加，变量的不兼容性也同样增加。与*X*和*Y*变量都相关的函数称为能量函数，表示如下：
- en: '![Energy-based models](img/B05883_05_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![基于能量的模型](img/B05883_05_01.jpg)'
- en: 'In the case of energy models, the input *X* is collected from the surroundings
    and the model generates an output Y, which is more likely to answer about the
    observed variable *X*. The model is required to produce the value *Y^/*, chosen
    from a set *Y**, which will make the value of the energy function *E(Y, X)* least.
    Mathematically, this is represented as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在能量模型的情况下，输入*X*来自周围环境，模型生成一个输出Y，更可能是关于观察到的变量*X*的答案。该模型需要生成一个值*Y^/*，该值从集合*Y*中选择，目的是使能量函数*E(Y,
    X)*的值最小。数学上，这可以表示为：
- en: '![Energy-based models](img/image_05_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![基于能量的模型](img/image_05_001.jpg)'
- en: 'The following *Figure 5.1* depicts the block diagram of the overall example
    mentioned in the preceding section:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图 5.1*展示了前一节中提到的整体示例的框图：
- en: '![Energy-based models](img/image_05_002.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![基于能量的模型](img/image_05_002.jpg)'
- en: 'Figure 5.1: Figure shows a energy model which computes the compatibility between
    the observed variable X and unobserved variable Y. X in the image is a set of
    pixel and Y is the set of level used for categorization of X. The model finds
    choosing ''Animal'' makes the values of energy function least. Image taken from
    [121]'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：图中展示了一个能量模型，它计算了观察到的变量X和未观察到的变量Y之间的兼容性。图中的X是一组像素，而Y是用于对X进行分类的级别集合。模型发现，选择“动物”使得能量函数的值最小。图片来自[121]。
- en: 'EBMs in deep learning are related to probability. Probability is proportional
    to *e* to the power of negative energy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的EBM与概率相关。概率与*e*的负能量幂成正比：
- en: '![Energy-based models](img/image_05_003.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![基于能量的模型](img/image_05_003.jpg)'
- en: EBMs define probabilities indirectly by formulating the function *E(x)*. The
    exponential function makes sure that the probability will always be greater than
    zero. This also implies that in an energy-based model, one is always free to choose
    the energy function based on the observed and unobserved variables. Although the
    probabilities for a classification in an energy-based model can arbitrarily approach
    zero, it will never reach that.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: EBM 通过构造函数 *E(x)* 间接定义概率。指数函数确保概率始终大于零。这也意味着，在基于能量的模型中，可以自由选择能量函数，依据观察和未观察变量。尽管在能量模型中的分类概率可以任意接近零，但永远不会真正达到零。
- en: Distribution in the form of the preceding equation is a form of Boltzmann distribution.
    The EBMs are hence often termed as **Boltzmann machines**. We will explain about
    Boltzmann machines and their various forms in the subsequent sections of this
    chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程形式的分布是一种 Boltzmann 分布。因此，EBM 通常被称为 **Boltzmann 机**。我们将在本章的后续部分解释 Boltzmann
    机及其各种形式。
- en: Boltzmann machines
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Boltzmann 机
- en: Boltzmann machines [122] are a network of symmetrically connected, neuron-like
    units, which are used for stochastic decisions on the given datasets. Initially,
    they were introduced to learn the probability distributions over binary vectors.
    Boltzmann machines possess a simple learning algorithm, which helps them to infer
    and reach interesting conclusions about input datasets containing binary vectors.
    The learning algorithm becomes very slow in networks with many layers of feature
    detectors; however, with one layer of feature detector at a time, learning can
    be much faster.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机 [122] 是一种对称连接的神经元单元网络，用于对给定数据集进行随机决策。最初，它们被引入用于学习二进制向量的概率分布。Boltzmann
    机具有简单的学习算法，这帮助它们推断并得出关于包含二进制向量的输入数据集的有趣结论。在具有多个特征检测层的网络中，学习算法变得非常缓慢；然而，每次只有一层特征检测器时，学习速度可以更快。
- en: To solve a learning problem, Boltzmann machines consist of a set of binary data
    vectors, and update the weight on the respective connections so that the data
    vectors turn out to be good solutions for the optimization problem laid by the
    weights. The Boltzmann machine, to solve the learning problem, makes lots of small
    updates to these weights.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决学习问题，Boltzmann 机由一组二进制数据向量组成，并更新相应连接上的权重，使得数据向量成为优化问题的良好解。为了求解学习问题，Boltzmann
    机对这些权重进行许多小的更新。
- en: "The Boltzmann machine over a d-dimensional binary vector can be defined as\
    \ *x \x88![Boltzmann machines](img/Belongs-t0.jpg) {0, 1} ^d*. As mentioned in\
    \ the earlier section, the Boltzmann machine is a type of energy-based function\
    \ whose joint probability function can be defined using the energy function given\
    \ as follows:"
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: "Boltzmann 机在一个 d 维二进制向量上可以定义为 *x \x88![Boltzmann 机](img/Belongs-t0.jpg) {0,\
    \ 1} ^d*。如前一节所述，Boltzmann 机是一种能量基础的函数，其联合概率函数可以通过以下给定的能量函数定义："
- en: '![Boltzmann machines](img/image_05_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Boltzmann 机](img/image_05_004.jpg)'
- en: 'Here, *E(x)* is the energy function and *Z* is termed as a partition function
    that confirms *Î£[x ]P(x)= 1*. The energy function of the Boltzmann machine is
    given as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*E(x)* 是能量函数，*Z* 称为分配函数，它确认 *Î£[x ]P(x) = 1*。Boltzmann 机的能量函数如下所示：
- en: '![Boltzmann machines](img/image_05_006.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Boltzmann 机](img/image_05_006.jpg)'
- en: Here, *W* is the weight matrix of the model parameters and *b* is the vector
    of the bias parameter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W* 是模型参数的权重矩阵，*b* 是偏置参数的向量。
- en: Boltzmann machines such as EBMs work on observed and unobserved variables. The
    Boltzmann machine works more efficiently when the observed variables are not in
    higher numbers. In those cases, the unobserved or hidden variables behave like
    hidden units of multilayer perceptron and show higher order interactions among
    the visible units.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 像 EBM 这样的 Boltzmann 机工作在观察变量和未观察变量上。当观察变量的数量不高时，Boltzmann 机工作效率更高。在这些情况下，未观察或隐藏变量像多层感知器的隐藏单元一样，展现出可见单元之间的高阶交互。
- en: 'Boltzmann machines have interlayer connections between the hidden layers as
    well as between visible units. *Figure 5.2* shows a pictorial representation of
    the Boltzmann machine:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机在隐藏层之间以及可见单元之间具有层间连接。*图 5.2* 展示了 Boltzmann 机的示意图：
- en: '![Boltzmann machines](img/image_05_007.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Boltzmann 机](img/image_05_007.jpg)'
- en: 'Figure 5.2: Figure shows a graphical representation of a simple Boltzmann machine.
    The undirected edges in the figure signifies the dependency among nodes and w[i,j]
    represents the weight associated between nodes i and j. Figure shows 3 hidden
    nodes and 4 visible nodes'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：图展示了一个简单 Boltzmann 机器的图形表示。图中的无向边表示节点之间的依赖关系，w[i,j] 表示节点 i 和 j 之间的权重。图中显示了
    3 个隐藏节点和 4 个可见节点。
- en: An interesting property of the Boltzmann machine is that the learning rule does
    not change with the addition of hidden units. This eventually helps to learn the
    binary features to capture the higher-order structure in the input data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机器的一个有趣特性是，随着隐藏单元的增加，学习规则不会发生变化。这最终有助于学习二进制特征，从而捕捉输入数据中的高阶结构。
- en: The Boltzmann machine behaves as a universal approximator of probability mass
    function over discrete variables.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机行为作为离散变量上概率质量函数的通用逼近器。
- en: Note
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In statistical learning, **Maximize Likelihood Estimation** (**MLE**) is a procedure
    of finding the parameters of a statistical model given observations, by finding
    the value of one or more parameters, which maximizes the likelihood of making
    the observations with the parameters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学习中，**最大似然估计**（**MLE**）是一种通过找到一个或多个参数的值来最大化观测数据的似然性，从而确定统计模型参数的过程。
- en: How Boltzmann machines learn
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Boltzmann 机器如何学习
- en: The learning algorithms for Boltzmann machines are generally based on maximum
    likelihood estimation method. When Boltzmann machines are trained with learning
    rules based on maximum likelihood estimation, the update of a particular weight
    connecting two units of the model will depend on only those two units concerned.
    The other units of the network take part in modifying the statistics that get
    generated. Therefore, the weight can be updated without letting the rest of the
    network know. In other words, the rest of the network can only know the final
    statistics, but would not know how the statistics are computed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 机器的学习算法通常基于最大似然估计方法。当 Boltzmann 机器使用基于最大似然估计的学习规则进行训练时，连接模型中两个单元的特定权重的更新将仅取决于这两个单元。网络中的其他单元参与修改生成的统计数据。因此，权重可以在不让网络其余部分知晓的情况下更新。换句话说，网络的其他部分只能知道最终的统计数据，但不知道这些统计数据是如何计算的。
- en: Shortfall
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺陷
- en: In Boltzmann machines, with many hidden layers, the network becomes extremely
    large. This makes the model typically slow. The Boltzmann machine stops learning
    with large scale data, as the machine's size also simultaneously grows exponentially.
    With a large network, the weights are generally very large and also the equilibrium
    distribution becomes very high. This unfortunately creates a significant problem
    for Boltzmann machines, which eventually results in a longer time duration to
    reach to an equilibrium state of distribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Boltzmann 机器中，具有许多隐藏层时，网络变得非常庞大，这使得模型通常变得较慢。Boltzmann 机器在大规模数据的学习中会停止学习，因为机器的大小也会同时呈指数增长。对于一个大网络，权重通常非常大，并且平衡分布也会变得非常高。不幸的是，这为
    Boltzmann 机器带来了一个显著问题，最终导致达到分布平衡状态的时间更长。
- en: This limitation can be overcome by restricting the connectivity between two
    layers, and thus simplifying the learning algorithm by learning one latent layer
    at a time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个限制可以通过限制两层之间的连接来克服，从而通过一次学习一个潜在层来简化学习算法。
- en: Restricted Boltzmann machine
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制性 Boltzmann 机器
- en: The **Restricted Boltzmann machine** (**RBM**) is a classic example of building
    blocks of deep probabilistic models that are used for deep learning. The RBM itself
    is not a deep model but can be used as a building block to form other deep models.
    In fact, RBMs are undirected probabilistic graphical models that consist of a
    layer of observed variables and a single layer of hidden variables, which may
    be used to learn the representation for the input. In this section, we will explain
    how the RBM can be used to build many deeper models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**限制性 Boltzmann 机器**（**RBM**）是深度学习中使用的深度概率模型构建块的经典例子。RBM 本身并不是一个深度模型，但可以作为构建其他深度模型的构建块。实际上，RBM
    是一种无向概率图模型，由一层观察变量和一层隐藏变量组成，可以用于学习输入的表示。在本节中，我们将解释如何使用 RBM 构建许多更深层次的模型。'
- en: Let us consider two examples to see the use case of RBM. RBM primarily operates
    on a binary version of factor analysis. Let us say we have a restaurant, and want
    to ask our customer to rate the food on a scale of 0 to 5\. In the traditional
    approach, we will try to explain each food item and customer in terms of the variable's
    hidden factors. For example, foods such as pasta and lasagne will have a strong
    association with the Italian factors. RBM, on the other hand, works on a different
    approach. Instead of asking each customer to rate the food items on a continuous
    scale, they simply mention whether they like it or not, and then RBM will try
    to infer various latent factors, which can help to explain the activation of food
    choices of each customer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过两个例子来查看RBM的使用案例。RBM主要在因子分析的二进制版本上运行。假设我们有一家餐厅，并且希望请顾客根据0到5的评分尺度来评价食物。在传统方法中，我们将试图根据变量的隐藏因素来解释每个食物项和顾客。例如，像意大利面和千层面这样的食物会与意大利因素有很强的关联。另一方面，RBM采用不同的方法。它不是让每个顾客根据连续的评分尺度来评价食物项，而是简单地询问他们是否喜欢该食物，然后RBM将尝试推断出各种潜在因素，这些因素有助于解释每个顾客的食物选择激活。
- en: Another example could be to guess someone's movie choice based on the genre
    the person likes. Say Mr. X has supplied his five binary preferences on the set
    of movies given. The job of the RBM will be to activate his preferences based
    on the hidden units. So, in this case, the five movies will send messages to all
    the hidden units, asking them to update themselves. The RBM will then activate
    the hidden units with high probability based on some preferences given to the
    person earlier.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是根据某人喜欢的电影类型来猜测他可能选择的电影。假设X先生提供了他对给定的五部电影的二进制偏好。RBM的任务是根据隐藏单元激活他的偏好。因此，在这种情况下，这五部电影会向所有隐藏单元发送消息，要求它们更新自身。然后，RBM会根据之前提供给该人的一些偏好，以较高的概率激活隐藏单元。
- en: The basic architecture
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本架构
- en: The RBM is a shallow, two-layer neural network used as a building block to create
    deep models. The first layer of the RBM is called the observed or visible layer,
    the second layer is called the latent or hidden layer. It is a bipartite graph,
    with no interconnection allowed between any variables in the observed layer, or
    between any units in the latent layer. As shown in *Figure 5.3*, there is no intra-layer
    communication between the layers. Due to this restriction, the model is termed
    as a **Restricted Boltzmann machine**. Each node is used for computation that
    processed the input, and participated in the output by making stochastic (randomly
    determined) decisions about whether to convey that input or not.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是一个浅层的、两层的神经网络，用作构建深度模型的基础模块。RBM的第一层称为观测层或可见层，第二层称为潜在层或隐藏层。它是一个二分图，不允许在观测层的任何变量之间，或潜在层的任何单元之间有连接。如*图5.3*所示，层与层之间没有内部通信。由于这种限制，该模型被称为**限制玻尔兹曼机**。每个节点用于计算，处理输入，并通过做出随机（随机确定）决策来参与输出，决定是否传递该输入。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A bipartite graph is a graph wherein the vertices can be split into two disjoint
    sets so that every edge connects a vertex of one set to the other. However, there
    is no connection between the vertices of the same set. The vertex sets are usually
    termed as a part of the graph.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 二分图是一种图，其中顶点可以分为两个不相交的集合，使得每条边都连接一个集合的顶点到另一个集合的顶点。然而，同一集合的顶点之间没有连接。顶点集合通常被称为图的一个部分。
- en: The primary intuition behind the two layers of an RBM is that there are some
    visible random variables (for example, food reviews from different customers)
    and some latent variables (such as cuisines, nationality of the customers or other
    internal factors), and the task of training the RBM is to find the probability
    of how these two sets of variables are interconnected to each other.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RBM两层结构的主要直觉是，有一些可见的随机变量（例如，来自不同顾客的食物评价）和一些潜在的随机变量（如菜系、顾客的国籍或其他内部因素），训练RBM的任务是找到这两组变量如何相互连接的概率。
- en: To mathematically formulate the energy function of an RBM, let's denote the
    observed layer that consists of a set of *n[v]* binary variables collectively
    with the vector *v*. The hidden or latent layers of *n[h]* binary random variables
    are denoted as *h*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数学地构建RBM的能量函数，我们用*v*表示由一组*n[v]*个二进制变量组成的观测层。隐藏层或潜在层由*n[h]*个二进制随机变量表示，记作*h*。
- en: 'Similar to the Boltzmann machine, the RBM is also an energy-based model, where
    the joint probability distribution is determined by its energy function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于玻尔兹曼机，RBM也是一个基于能量的模型，其中联合概率分布由其能量函数决定：
- en: '![The basic architecture](img/B05883_05_07.jpg)![The basic architecture](img/image_05_009.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![基本架构](img/B05883_05_07.jpg)![基本架构](img/image_05_009.jpg)'
- en: 'Figure 5.3: Figure shows a simple RBM. The model is a symmetric bipartite graph,
    where each hidden node is connected to every visible nodes. Hidden units are represented
    as h[i] and visible units as v[i]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：图示展示了一个简单的RBM。该模型是一个对称的二分图，其中每个隐藏节点与每个可见节点相连接。隐藏单元表示为h[i]，可见单元表示为v[i]
- en: 'The energy function of an RBM with binary visible and latent units is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有二进制可见单元和潜在单元的RBM的能量函数如下：
- en: '![The basic architecture](img/B05883_05_08.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![基本架构](img/B05883_05_08.jpg)'
- en: Here, *a*, *b*, and *W* are unconstrained, learnable, real-valued parameters.
    From the preceding *Figure 5.3*, we can see that the model is split into two groups
    of variables, *v* and *h*. The interaction between the units is described by the
    matrix *W*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*a*、*b* 和 *W* 是不受约束的、可学习的实值参数。从前面的*图5.3*中，我们可以看到模型被分为两组变量，*v* 和 *h*。单元之间的交互由矩阵*W*描述。
- en: How RBMs work
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RBM如何工作
- en: 'So, as we are now aware of the basic architecture of an RBM, in this section,
    we will discuss the basic working procedure for this model. The RBM is fed with
    a dataset from which it should learn. Each visible node of the model receives
    a low-level feature from an item of the dataset. For example, for a gray-scale
    image, the lowest level item would be one pixel value of the image, which the
    visible node would receive. Therefore, if an image dataset has n number of pixels,
    the neural network processing them must also possess n input nodes on the visible
    layer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们现在已经了解了RBM的基本架构，在这一节中，我们将讨论该模型的基本工作过程。RBM从一个数据集中接受输入数据进行学习。模型的每个可见节点从数据集中的一项数据接收一个低级特征。例如，对于灰度图像，最低级的项目将是图像中的一个像素值，模型的可见节点将接收该像素值。因此，如果图像数据集有n个像素，处理这些像素的神经网络也必须具有n个输入节点在可见层上：
- en: '![How RBMs work](img/B05883_05_04-300x232.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![RBM如何工作](img/B05883_05_04-300x232.jpg)'
- en: 'Figure 5.4 : Figure shows the computation of an RBM for a one input path'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：图示展示了一个输入路径的RBM计算
- en: Now, let's propagate one single pixel value, *p* through the two-layer network.
    At the first node of the hidden layer, *p* is multiplied by a weight *w*, and
    added to the bias. The final result is then fed into an activation function that
    generates the output of the node. The operation produces the outcome, which can
    be termed as the strength of the signal passing through that node, given an input
    pixel *p*. *Figure 5.4* shows the visual representation of the computation involved
    for a single input RBM.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将一个单一像素值*p*在这两层网络中进行传播。在隐藏层的第一个节点，*p*与权重*w*相乘，并加上偏置。最终结果通过激活函数生成节点的输出。该操作产生了结果，可以称之为通过该节点传递的信号强度，给定输入像素*p*。*图5.4*展示了单个输入RBM所涉及的计算的可视化表示。
- en: '![How RBMs work](img/B05883_05_12.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![RBM如何工作](img/B05883_05_12.jpg)'
- en: 'Every visible node of the RBM is associated with a separate weight. Inputs
    from various units get combined at one hidden node. Each *p* (pixel) from the
    input is multiplied by a separate weight associated with it. The products are
    summed up and added to a bias. This result is passed through an activation function
    to generate the output of the node. The following *Figure 5.5* shows the visual
    representation of the computation involved for multiple inputs to the visible
    layer of RBMs:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: RBM的每个可见节点都与一个独立的权重相关联。来自不同单元的输入在一个隐藏节点中合并。每个来自输入的*p*（像素）都会与其相关的独立权重相乘。所有的乘积会被加总并加入偏置。然后，该结果通过激活函数生成节点的输出。以下*图5.5*展示了多个输入到可见层RBM时所涉及的计算的可视化表示：
- en: '![How RBMs work](img/B05883_05_05-300x232.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![RBM如何工作](img/B05883_05_05-300x232.jpg)'
- en: 'Figure 5.5: Figure shows the computation of an RBM with multiple inputs and
    one hidden unit'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：图示展示了具有多个输入和一个隐藏单元的RBM计算
- en: The preceding *Figure 5.5* shows how the weight associated with every visible
    node is used to compute the final outcome from a hidden node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上述*图5.5*展示了与每个可见节点相关的权重是如何用于计算来自隐藏节点的最终结果的。
- en: '![How RBMs work](img/B05883_05_06-300x203.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![RBM如何工作](img/B05883_05_06-300x203.jpg)'
- en: 'Figure 5.6: Figure shows the computation involved with multiple visible units
    and hidden units for an RBM'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：该图展示了在RBM中多个可见单元和隐藏单元的计算过程
- en: As mentioned earlier, RBMs are similar to a bipartitone graph. Further, the
    machine's structure is basically similar to a symmetrical bipartite graph because
    input received from all the visible nodes are being passed to all the latent nodes
    of the RBM.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，RBM类似于一个二分图。进一步来说，机器的结构基本上类似于一个对称的二分图，因为所有从可见节点接收到的输入都被传递给RBM的所有潜在节点。
- en: 'For every hidden node, each input p gets multiplied with its respective weight
    *w*. Therefore, for a single input *p* and *m* number of hidden units, the input
    would have *m* weights associated with it. In *Figure 5.6*, the input *p* would
    have three weights, making a total of 12 weights altogether: four input nodes
    from the visible layer and three hidden nodes in the next layer. All the weights
    associated between the two layers form a matrix, where the rows are equal to the
    visible nodes, and the columns are equal to the hidden units. In the preceding
    figure, each hidden node of the second layer accepts the four inputs multiplied
    by their respective weights. The final sum of the products is then again added
    to a bias. This result is then passed through an activation algorithm to produce
    one output for each hidden layer. *Figure 5.6* represents the overall computations
    that occur in such scenarios.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个隐藏节点，每个输入p都会与其相应的权重*w*相乘。因此，对于单个输入*p*和*m*个隐藏单元，该输入将有*m*个与之相关的权重。在*图5.6*中，输入*p*将有三个权重，总共形成12个权重：四个来自可见层的输入节点和三个来自下一层的隐藏节点。两层之间所有相关的权重形成一个矩阵，其中行数等于可见节点的数量，列数等于隐藏单元的数量。在前面的图中，第二层的每个隐藏节点都接受四个输入，并与它们各自的权重相乘。所有乘积的总和再加上一个偏置。这一结果会通过一个激活算法，以生成每个隐藏层的一个输出。*图5.6*表示了在这种情况下发生的整体计算过程。
- en: With a stacked RBM, it will form a deeper layer neural network, where the output
    of the first hidden layer would be passed to the next hidden layer as input. This
    will be propagated through as many hidden layers as one uses to reach the desired
    classifying layer. In the subsequent section, we will explain how to use an RBM
    as deep neural networks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过堆叠RBM，它将形成一个更深的神经网络，其中第一个隐藏层的输出将作为输入传递给下一个隐藏层。这将通过你使用的隐藏层的数量传播，直到到达所需的分类层。在接下来的部分，我们将解释如何将RBM用作深度神经网络。
- en: Convolutional Restricted Boltzmann machines
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积限制玻尔兹曼机
- en: Very high dimensional inputs, such as images or videos, put immense stress on
    the memory, computation, and operational requirements of traditional machine learning
    models. In  [Chapter 3](ch03.html "Chapter 3.  Convolutional Neural Network")
    , *Convolutional Neural Network*, we have shown how replacing the matrix multiplication
    by discrete convolutional operations with small kernel resolves these problems.
    Going forward, Desjardins and Bengio [123] have shown that this approach also
    works fine when applied to RBMs. In this section, we will discuss the functionalities
    of this model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 高维度输入，如图像或视频，会给传统的机器学习模型带来巨大的内存、计算和操作压力。在[第3章](ch03.html "第3章. 卷积神经网络")中，我们展示了如何通过用小核的离散卷积运算替代矩阵乘法来解决这些问题。接下来，Desjardins和Bengio
    [123] 也表明，当这种方法应用于RBM时，它同样能有效工作。在本节中，我们将讨论该模型的功能。
- en: '![Convolutional Restricted Boltzmann machines](img/B05883_05_07-266x300.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![卷积限制玻尔兹曼机](img/B05883_05_07-266x300.jpg)'
- en: 'Figure 5.7 : Figure shows the observed variables or the visible units of an
    RBM can be associated with mini batches of image to a compute the final result.
    The weight connections represents a set of filters'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：该图展示了RBM的观察变量或可见单元如何与图像的小批量相关联，以计算最终结果。权重连接表示一组滤波器
- en: Further, in normal RBMs, the visible units are directly related to all the hidden
    variables through different parameters and weights. To describe an image in terms
    of spatially local features ideally needs fewer parameters, which can be generalized
    better. This helps in detecting and extracting the identical and local features
    from a high dimensional image. Therefore, using an RBM to retrieve all the global
    features from an image for object detection is not so encouraging, especially
    for high dimensional images. One simple approach is to train the RBM on mini batches
    sampled from the input image, placed in blocks on Hadoop's Datanodes to generate
    the local features. The representation of this approach, termed as patch-based
    RBM is shown in *Figure 5.7*. This, however, has some potential limitations. The
    patch-based RBM, used in the distributed environment on Hadoop, does not follow
    the spatial relationship of mini batches, and sees each image's mini batches as
    independent patches from the nearby patches. This makes the feature extracted
    from the neighboring patches independent and somewhat significantly redundant.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步来说，在普通的RBM中，可见单元通过不同的参数和权重与所有隐藏变量直接相关。为了描述图像的空间局部特征，理想情况下需要较少的参数，这有助于更好的泛化。这有助于从高维图像中检测和提取相同的局部特征。因此，使用RBM从图像中提取所有全局特征进行物体检测并不太理想，尤其是对于高维图像。一个简单的方法是使用从输入图像中抽样的小批量，在Hadoop的Datanodes上分块训练RBM，以生成局部特征。这个方法的表示称为基于补丁的RBM，如*图
    5.7*所示。然而，这种方法有一些潜在的局限性。在Hadoop的分布式环境中使用的基于补丁的RBM没有遵循小批量的空间关系，而是将每个图像的小批量视为来自附近补丁的独立补丁。这使得从相邻补丁中提取的特征是独立的，并且可能存在显著的冗余。
- en: To handle such a situation, a **Convolutional Restricted Boltzmann machine**
    (**CRBM**) is used, which is an extension of the traditional RBM model. The CRBM
    is structurally almost similar to RBM, a two-layer model in which the visible
    and hidden random variables are structured as matrices. Hence, in CRBM, the locality
    and neighborhood can be defined for both visible and hidden units. In CRBM, the
    visible matrix represents the image and the small windows of the matrix define
    the mini batches of the image. The hidden units of CRBM are partitioned into different
    feature maps to locate the presence of multiple features at multiple positions
    of the visible units. Units within a feature map represent the same feature at
    different locations of the visible unit. The hidden-visible connections of CRBM
    are completely local, and weights are generally split across the clusters of hidden
    units.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种情况，使用了**卷积限制玻尔兹曼机**（**CRBM**），它是传统RBM模型的扩展。CRBM在结构上与RBM几乎相似，是一个两层模型，其中可见和隐藏随机变量被结构化为矩阵。因此，在CRBM中，可以为可见单元和隐藏单元定义局部性和邻域。在CRBM中，可见矩阵表示图像，矩阵的小窗口定义了图像的小批量。CRBM的隐藏单元被划分为不同的特征图，以定位可见单元多个位置上多个特征的存在。特征图中的单元表示在可见单元的不同位置上相同的特征。CRBM的隐藏-可见连接是完全局部的，权重通常会分配到隐藏单元的簇中。
- en: The CRBM's hidden units are used to extract features from the overlapping mini
    batches of visible units. Moreover, the features of neighboring mini batches complement
    each other and collaborate to model the input image.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: CRBM的隐藏单元用于从重叠的小批量可见单元中提取特征。此外，相邻小批量的特征互为补充，并协同工作以建模输入图像。
- en: '*Figure 5.8* shows a CRBM with a matrix of visible units **V** and a matrix
    of hidden units **H**, which are connected with *K 3*3* filters, namely, **W[1]**,
    **W[2]**, **W[3]**,...**W[K]**. The hidden units in the figure are split into
    **K** submatrices called feature map, **H[1]**, **H[2]**,...**H[K]**. Each hidden
    unit **H[i]** signifies the presence of a particular feature at a *3*3* neighborhood
    of visible units.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.8* 显示了一个CRBM（卷积限制玻尔兹曼机），其中包含一个可见单元矩阵**V**和一个隐藏单元矩阵**H**，它们通过*K 3*3*的滤波器连接，即**W[1]**，**W[2]**，**W[3]**，...**W[K]**。图中的隐藏单元被分成**K**个子矩阵，称为特征图，**H[1]**，**H[2]**，...**H[K]**。每个隐藏单元**H[i]**表示在可见单元的*3*3*邻域中某个特征的存在。'
- en: A CRBM, unlike a patch-based RBM, is trained on the whole input image or a large
    region of the image, to learn the local features and exploit the spatial relationship
    of the overlapping mini batches, processed in a distributed manner on Hadoop.
    The hidden units of overlapping mini batches depend and cooperate with each other
    in a CRBM. Therefore, one hidden unit, once explained, does not need to be explained
    again in the neighborhood overlapping mini batch. This in turn helps in reducing
    the redundancy of the features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于补丁的 RBM 不同，CRBM 是在整个输入图像或图像的大区域上进行训练，以学习局部特征，并利用重叠的迷你批次的空间关系，这些迷你批次通过 Hadoop
    以分布式方式处理。在 CRBM 中，重叠迷你批次的隐藏单元相互依赖并合作。因此，一旦解释了一个隐藏单元，就不需要在邻近的重叠迷你批次中再次解释它。这反过来有助于减少特征的冗余。
- en: '![Convolutional Restricted Boltzmann machines](img/B05883_05_08-267x300.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![卷积限制玻尔兹曼机](img/B05883_05_08-267x300.jpg)'
- en: 'Figure 5.8: The computation involved in a CRBM is shown in the figure'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：图中展示了 CRBM 中的计算过程。
- en: Stacked Convolutional Restricted Boltzmann machines
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠卷积限制玻尔兹曼机
- en: CRBMs can be stacked together to form deep neural networks. **Stacked Convolutional
    Restricted Boltzmann machines **(**Stacked CRBMs**) can be trained layer wise,
    with a bottom-up approach, similar to the layer wise training of fully connected
    neural networks. After each CRBM filtering layer, in a stacked network, a deterministic
    subsampling method is implemented. For subsampling the features, max-pooling is
    performed in non-overlapping image regions. The pooling layer, as explained in
    [Chapter 3](ch03.html "Chapter 3.  Convolutional Neural Network") , *Convolutional
    Neural Network*, helps to minimize the dimensionality of the features. On top
    of that, it makes the feature robust to small shifts and helps to propagate the
    higher-level feature to grow over regions of the input image.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: CRBM 可以堆叠在一起形成深度神经网络。**堆叠卷积限制玻尔兹曼机**（**Stacked CRBM**）可以层层训练，采用自下而上的方法，类似于全连接神经网络的层级训练。在每个
    CRBM 滤波层后，在堆叠网络中实现了一种确定性的子采样方法。对于特征的子采样，采用最大池化在非重叠的图像区域中进行。正如在[第 3 章](ch03.html
    "第 3 章. 卷积神经网络")中解释的，*卷积神经网络*有助于最小化特征的维度。除此之外，它使得特征对小的平移具有鲁棒性，并有助于将更高层次的特征传播到输入图像的区域中。
- en: Deep CRBMs require a pooling operation, so that the spatial size of each successive
    layer decreases. Although, most of the traditional convolutional models work fine
    with inputs of a variety of spatial size, for Boltzmann machines it becomes somewhat
    difficult to change the input sizes, mainly due to a couple of reasons. Firstly,
    the partition function of the energy function changes with the size of input.
    Secondly, convolutional networks attain the size invariance by increasing the
    size of the pooling function proportional to the input size. However, scaling
    the pooling regions for Boltzmann machines is very difficult to achieve.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 CRBM 需要一个池化操作，以便每一层的空间大小逐渐减小。尽管大多数传统的卷积模型可以很好地处理各种空间大小的输入，但对于玻尔兹曼机来说，由于几个原因，改变输入大小变得有些困难。首先，能量函数的分区函数会随输入大小的变化而变化。其次，卷积网络通过增加池化函数的大小，使其与输入大小成比例，从而实现尺寸不变性。然而，对于玻尔兹曼机来说，要实现池化区域的缩放是非常困难的。
- en: For CRBMs, the pixels residing at the boundary of the image also impose difficulty,
    which is worsened by the fact that Boltzmann machines are symmetric in nature.
    This can be nullified by implicitly zero-padding the input. Bear in mind that,
    zero-padding the input is often driven by lesser input pixels, which may not be
    activated when needed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CRBM，位于图像边界的像素也会带来困难，尤其是考虑到玻尔兹曼机本身具有对称性。这个问题可以通过隐式零填充输入来解决。请注意，零填充输入通常是由于输入像素较少，这些像素可能在需要时没有被激活。
- en: Deep Belief networks
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: '**Deep Belief networks** (**DBNs**) were one of the most popular, non-convolutional
    models that could be successfully deployed as deep neural networks in the year
    2006-07 [124] [125]. The renaissance of deep learning probably started from the
    invention of DBNs back in 2006\. Before the introduction of DBNs, it was very
    difficult to optimize the deep models. By outperforming the **Support Vector machines**
    (**SVMs**), DBNs had shown that deep models can be really successful; although,
    compared to the other generative or unsupervised learning algorithms, the popularity
    of DBNs has fallen a bit, and is rarely used these days. However, they still play
    a very important role in the history of deep learning.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度信念网络**（**DBN**）是2006至2007年间最受欢迎的非卷积模型之一，可以成功地作为深度神经网络进行部署[124][125]。深度学习的复兴很可能始于2006年DBN的发明。在DBN出现之前，优化深度模型是非常困难的。通过超越**支持向量机**（**SVM**），DBN证明了深度模型的成功可能性；尽管与其他生成性或无监督学习算法相比，DBN的流行度有所下降，且如今很少使用，但它们仍在深度学习历史中扮演着非常重要的角色。'
- en: Note
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A DBN with only one hidden layer is just an RBM.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个隐藏层的DBN实际上就是一个RBM。
- en: DBNs are generative models composed of more than one layer of hidden variables.
    The hidden variables are generally binary in nature; however the visible units
    might consist of binary or real values. In DBNs, every unit of each layer is connected
    to every other unit of its neighbouring layer, although, there can be a DBN with
    sparsely connected units. There possess no connection between the intermediate
    layers. As shown in *Figure 5.9*, DBNs are basically a multilayer network composed
    of several RBMs. The connections between the top two layers are undirected. However,
    the connections between all other layers are directed, where the arrows point
    toward the layer nearest to the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: DBN是生成性模型，由多个隐藏变量层组成。隐藏变量通常是二进制的；然而，可见单元可能包含二进制或实数值。在DBN中，每一层的每个单元都与其相邻层的每个单元连接，尽管也可以有稀疏连接的DBN。各中间层之间没有连接。如*图5.9*所示，DBN基本上是一个由若干RBM构成的多层网络。顶层两个层之间的连接是无向的。而其他所有层之间的连接都是有向的，箭头指向数据最接近的层。
- en: Except for the first and last layer of the stack, each layer of a DBN serves
    two purposes. First, it acts as a hidden layer for its predecessor layer, and
    as a visible layer or input for its next layer. DBNs are mainly used to cluster,
    recognize and generate video sequences and images.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了堆叠中的第一层和最后一层外，DBN的每一层都有两个作用。首先，它作为前一层的隐藏层，同时作为下一层的可见层或输入层。DBN主要用于聚类、识别和生成视频序列和图像。
- en: '![Deep Belief networks](img/image_05_016.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![深度信念网络](img/image_05_016.jpg)'
- en: 'Figure 5.9: A DBN composed of three RBMs is shown in figure'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：图中展示了由三个RBM组成的DBN
- en: Greedy layer-wise training
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贪婪逐层训练
- en: A greedy layer-wise training algorithm for training DBNs was proposed in 2006
    [126]. This algorithm trains the DBN one layer at a time. In this approach, an
    RBM is first trained, which takes the actual data as input and models it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年提出了一种贪婪逐层训练算法用于训练深度信念网络（DBN）[126]。该算法一次训练一个层。采用这种方法时，首先训练一个受限玻尔兹曼机（RBM），它以实际数据为输入并对其进行建模。
- en: A one-level DBN is an RBM. The core philosophy of greedy layer-wise approach
    is that after training the top-level RBM of an m-level DBN, the interpretation
    of the parameters changes while adding them in a (*m+1*) level DBN. In an RBM,
    between layers (*m-1*) and *m*, the probability distribution of layer *m* is defined
    in terms of the parameters of that RBM. However, in the case of a DBN, the probability
    distribution of layer *m* is defined in terms of the upper layer's parameters.
    This procedure can be repeated indefinitely, to connect with as many layers of
    DBNs as one desires.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单层的DBN就是一个RBM。贪婪逐层方法的核心理念是，在训练一个m层DBN的顶层RBM后，解释参数的方式会随着添加(*m+1*)层DBN而发生变化。在RBM中，层(*m-1*)和*m*之间，层*m*的概率分布是基于该RBM的参数来定义的。然而，在DBN的情况下，层*m*的概率分布是基于上层的参数来定义的。这个过程可以无限重复，直到连接上任意数量的DBN层。
- en: Distributed Deep Belief network
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度信念网络
- en: DBNs have so far achieved a lot in numerous applications such as speech and
    phone recognition [127], information retrieval [128], human motion modelling[129],
    and so on. However, the sequential implementation for both RBM and DBNs come with
    various limitations. With a large-scale dataset, the models show various shortcomings
    in their applications due to the long, time consuming computation involved, memory
    demanding nature of the algorithms, and so on. To work with Big data, RBMs and
    DBNs require distributed computing to provide scalable, coherent and efficient
    learning.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，DBN在许多应用中取得了显著成绩，如语音和电话识别[127]、信息检索[128]、人体运动建模[129]等。然而，RBM和DBN的顺序实现存在各种限制。对于大规模数据集，由于涉及长时间的计算、算法对内存的高需求等，模型在应用中会表现出各种不足。为了处理大数据，RBM和DBN需要分布式计算来提供可扩展、一致和高效的学习。
- en: To make DBNs acquiescent to the large-scale dataset stored on a cluster of computers,
    DBNs should acquire a distributed learning approach with Hadoop and Map-Reduce.
    The paper in [130] has shown a key-value pair approach for each level of an RBM,
    where the pre-training is accomplished with layer-wise, in a distributed environment
    in Map-Reduce framework. The learning is performed on Hadoop by an iterative computing
    method for the training RBM. Therefore, the distributed training of DBNs is achieved
    by stacking multiple RBMs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使DBN能够适应存储在计算机集群上的大规模数据集，DBN应该采用带有Hadoop和Map-Reduce的分布式学习方法。[130]中的论文展示了RBM每一层的键值对方法，其中预训练是在Map-Reduce框架的分布式环境中通过层级进行的。学习通过在Hadoop上执行迭代计算方法进行RBM的训练。因此，DBN的分布式训练通过堆叠多个RBM来实现。  '
- en: Distributed training of Restricted Boltzmann machines
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '限制玻尔兹曼机的分布式训练  '
- en: 'As mentioned in the earlier sections, the energy function for an RBM is given
    by:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，RBM的能量函数如下：  '
- en: '![Distributed training of Restricted Boltzmann machines](img/image_05_017.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机的分布式训练](img/image_05_017.jpg)  '
- en: Let an input dataset *I = {x[i] = i= 1, 2,... N}* be used for the distributed
    learning of a RBM. As discussed in the earlier section, for the learning of a
    DBN, the weights and biases in each level of the RBM are initialized at first
    by using a greedy layer-wise unsupervised training. The purpose of the distributed
    training is to learn the weights and the associated biases *b* and *c*. For a
    distributed RBM using Map-Reduce, the one Map-Reduce job is essential in every
    epoch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '设输入数据集 *I = {x[i] = i= 1, 2,... N}* 用于限制玻尔兹曼机（RBM）的分布式学习。如前所述，针对深度信念网络（DBN）的学习，RBM每一层的权重和偏置首先通过贪心层级无监督训练进行初始化。分布式训练的目的是学习权重和相关的偏置
    *b* 和 *c*。对于使用Map-Reduce的分布式RBM，每个训练周期中都需要进行一次Map-Reduce作业。  '
- en: For matrix-matrix multiplication, Gibbs sampling is used, and for training of
    RBMs it takes most of the computation time. Therefore, to truncate the computation
    time for this, Gibbs sampling can be distributed in the map phase among multiple
    datasets running different Datanodes on the Hadoop framework.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '对于矩阵乘法，使用吉布斯采样，RBM的训练大部分计算时间都消耗在此。因此，为了减少计算时间，吉布斯采样可以在Map阶段分布到多个数据集上，这些数据集在Hadoop框架上运行不同的Datanode。  '
- en: Note
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '注意  '
- en: Gibbs sampling is a **Markov chain Monte Carlo** (**MCMC**) algorithm for determining
    the sequence of observations that are estimated from a specified multivariate
    probability distribution, when traditional direct sampling becomes difficult.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '吉布斯采样是一种**马尔可夫链蒙特卡洛**（**MCMC**）算法，用于确定从指定的多元概率分布中估计的观察序列，当传统的直接采样变得困难时。  '
- en: Initially, different parameters needed for training, such as the number of neurons
    for visible and hidden layers, the input layer bias *a*, the hidden layer bias
    *b*, weight *W*, number of epochs (say *N*), learning rate, and so on, are initialized.
    The number of epochs signify that both the map and reduce phase will iterate for
    *N* number of times. For each epoch, the mapper runs for each block of the Datanodes,
    and performs Gibbs sampling to calculate the approximate gradients of *W*, *a*,
    and *b*. The reducer then updates those parameters with the computed increments
    needed for the next epoch. Hence, from the second epoch, the input value for the
    map phase, the updated values of *W*, *a*, and *b*, are calculated from the output
    of the reducer in the previous epoch.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，训练所需的不同参数，如可见层和隐藏层的神经元数量、输入层偏置 *a*、隐藏层偏置 *b*、权重 *W*、周期数（假设为 *N*）、学习率等，都被初始化。周期数表示映射和归约阶段将迭代
    *N* 次。对于每个周期，映射器会针对每个数据节点块运行，并执行 Gibbs 采样来计算 *W*、*a* 和 *b* 的近似梯度。然后，归约器使用计算出的增量更新这些参数，以便为下一个周期做好准备。因此，从第二个周期开始，映射阶段的输入值，即
    *W*、*a* 和 *b* 的更新值，是从前一个周期的归约器输出中计算得出的。
- en: The input dataset I is split into a number of chunks, and stored in different
    blocks, running on each Datanode. Each mapper running on the blocks will compute
    the approximate gradient of the weights and biases for a particular chunk stored
    on that block. The reducers then calculate the increments of respective parameters
    and update them accordingly. The process treats the resulting parameters and the
    updated values as the final outcome of the Map-Reduce phase of that particular
    epoch. After every epoch, the reducer decides whether to store the learned weight,
    if it is the final epoch or whether to increment the epoch index and propagate
    the key-value pair value to the next mapper.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集 I 被拆分成多个块，并存储在不同的块中，分别在每个数据节点上运行。每个在这些块上运行的映射器将计算该块中存储的特定数据块的权重和偏置的近似梯度。然后，归约器计算各自参数的增量，并相应地更新它们。这个过程将得到的参数和更新后的值视为该周期
    Map-Reduce 阶段的最终结果。每个周期结束后，归约器决定是否存储学习到的权重，如果是最后一个周期，或者是否增加周期索引并将键值对传播给下一个映射器。
- en: Distributed training of Deep Belief networks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度置信网络的分布式训练
- en: For distributed training of DBNs with *L* number of hidden layers, the learning
    is performed with pre-training *L* RBMs. The bottom level RBM is trained as discussed,
    however, for the rest of the (*L-1*) RBMs, the input dataset is changed for each
    level.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 *L* 个隐藏层的 DBN 的分布式训练，学习是通过预训练 *L* 个 RBM 完成的。底层 RBM 的训练如前所述，但对于其余的 (*L-1*)
    个 RBM，每一层的输入数据集都会发生变化。
- en: 'The input data for *m[th]* level (*L ![Distributed training of Deep Belief
    networks](img/greater-than-equal.jpg) ¥ m > 1*) RBM will be the conditional probability
    of hidden nodes of (*m-1*)[th] level RBMs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *m[th]* 层的输入数据（*L ![深度置信网络的分布式训练](img/greater-than-equal.jpg) ¥ m > 1*）RBM，将会是
    (*m-1*)[th] 层 RBM 隐藏节点的条件概率：
- en: '![Distributed training of Deep Belief networks](img/Capture-9.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![深度置信网络的分布式训练](img/Capture-9.jpg)'
- en: Distributed back propagation algorithm
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式反向传播算法
- en: This is the second phase of distributed training of back propagation algorithm
    to tune the global network. In this procedure, while computing the gradient of
    weights, the feed-forward and back-propagation methods take up the majority of
    the computation time. Hence, for each epoch, for faster execution, this procedure
    should be run in parallel on each mini-batch of the input dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分布式反向传播算法的第二阶段，用于调优全局网络。在此过程中，计算权重的梯度时，前向传播和反向传播方法占据了大部分的计算时间。因此，为了加快每个周期的执行速度，该过程应在每个输入数据集的小批量上并行运行。
- en: In the first step of the procedure, the learned weights for a *L* level DBN,
    namely, *W1*, *W2*,*...WL* are loaded into the memory, and other hyper-parameters
    are initialized. In this fine-tuning phase, the primary jobs for map and reduce
    phase are similar to that of the RBMs distributed training. The mapper will determine
    the gradients of the weights and eventually update the weight increment. The reducer
    updates the weight increments from one or more weights and passes the output to
    the mapper to perform the next iteration.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在该过程的第一步中，将 *L* 层 DBN 的学习权重，即 *W1*，*W2*，...*WL* 加载到内存中，并初始化其他超参数。在此微调阶段，映射和归约阶段的主要任务与
    RBM 的分布式训练类似。映射器将确定权重的梯度，并最终更新权重增量。归约器从一个或多个权重中更新权重增量，并将输出传递给映射器以执行下一次迭代。
- en: The main purpose of this procedure is to obtain some discriminative power of
    the model by placing the label layer on top of the global network and tuning the
    weights of the entire layers iteratively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的主要目的是通过将标签层放置在全局网络之上，并迭代调整整个层的权重，从而获得模型的一些判别能力。
- en: Performance evaluation of RBMs and DBNs
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RBM和DBN的性能评估
- en: The paper [130] performed an experiment of distributed RBM and DBN on Hadoop
    cluster to provide a comparative study with the traditional sequential approach.
    The experiments were carried out on MNIST datasets for hand-written digits recognition.
    There were 60,000 images for the training set, and 10,000 images for the testing
    set. The block size of HDFS is set to 64 MB with a replication factor of 4\. All
    the nodes are set to run 26 mappers and 4 reducers maximum. Interested readers
    can modify the block size and replication factor to see the final results of the
    experiments with these parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[130]在Hadoop集群上进行了分布式RBM和DBN的实验，以便与传统的顺序方法进行比较研究。实验在MNIST数据集上进行，用于手写数字识别。训练集包含60,000张图片，测试集包含10,000张图片。HDFS的块大小设置为64MB，副本因子为4。所有节点设置为最多运行26个映射器和4个减少器。有兴趣的读者可以修改块大小和副本因子，查看这些参数下实验的最终结果。
- en: Drastic improvement in training time
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练时间的显著提升
- en: The purpose of this experiment is to the compare the distributed RBMs and DBNs
    with the traditional training policy (sequential) in terms of training time. The
    sequential programs were performed on one CPU, whereas the distributed programs
    were on 16 CPUs of a node. Both the experiments were performed on the MNIST datasets
    mentioned earlier. The results obtained are summarized in *Table 5.1* and *Table
    5.2:*
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验的目的是比较分布式RBM和DBN与传统训练策略（顺序训练）在训练时间上的差异。顺序程序在一颗CPU上执行，而分布式程序则在一个节点的16颗CPU上执行。两次实验均在前面提到的MNIST数据集上进行。所得结果汇总在*表5.1*和*表5.2*中：
- en: '![Drastic improvement in training time](img/Capture-7.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![训练时间的显著提升](img/Capture-7.jpg)'
- en: Table 5.1:The table represents the time needed to complete the training of distributed
    and sequential RBMs
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1：该表格表示完成分布式和顺序RBM训练所需的时间
- en: '![Drastic improvement in training time](img/Capture-8.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![训练时间的显著提升](img/Capture-8.jpg)'
- en: Table 5.2:The table represents the time needed to complete the training of distributed
    and sequential DBNs
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2：该表格表示完成分布式和顺序DBN训练所需的时间
- en: The data shown in the table clearly depicts the advantages of using distributed
    RBMs and DBNs using Hadoop, over the traditional sequential approach. The training
    time for the distributed approach for the models has shown drastic improvement
    over the sequential one. Also, one crucial advantage of using the Hadoop framework
    for distribution is that it scales exceptionally well with the size of the training
    datasets, as well as the number of machines used to distribute it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表中显示的数据清楚地描述了使用Hadoop分布式RBM和DBN相较于传统顺序方法的优势。分布式方法在模型训练时间上显示出了显著的提升。此外，使用Hadoop框架进行分布的一个关键优势是，它能随着训练数据集的大小和用于分布的机器数量的增加而表现出卓越的扩展性。
- en: The next section of the chapter will demonstrate the programming approach for
    both the models using Deeplearning4j.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的下一部分将演示使用Deeplearning4j的两种模型的编程方法。
- en: Implementation using Deeplearning4j
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Deeplearning4j的实现
- en: This section of the chapter will provide a basic idea of how to write the code
    for RBMs and DBNs using Deeplearning4j. Readers will be able to learn the syntax
    for using the various hyperparameters mentioned in this chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的这一部分将提供如何使用Deeplearning4j编写RBM和DBN代码的基本思路。读者将能够学习本章中提到的各种超参数的语法。
- en: 'To implement RBMs and DBNs using Deeplearning4j, the whole idea is very simple.
    The overall implementation can be split into three core phases: loading data or
    preparation of the data, network configuration, and training and evaluation of
    the model.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Deeplearning4j实现RBM和DBN，整个思路非常简单。总体实现可以分为三个核心阶段：加载数据或数据准备、网络配置、以及模型的训练和评估。
- en: In this section, we will first discuss RBMs on IrisDataSet, and then we will
    come to the implementation of DBNs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论在IrisDataSet上使用RBM，然后我们将讨论DBN的实现。
- en: Restricted Boltzmann machines
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机
- en: 'For the building and training of RBMs, first we need to define and initialize
    the hyperparameter needed for the model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RBM的构建和训练，首先我们需要定义并初始化模型所需的超参数：
- en: '[PRE0]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The batchsize here can be initialized as `150`, which means `150` samples of
    the dataset will be submitted to the Hadoop framework at a time. Rest assured
    all other parameters are initialized just as we did it in the earlier chapters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的批次大小可以初始化为 `150`，意味着一次会将 `150` 个样本提交到 Hadoop 框架中。请放心，所有其他参数的初始化与我们在之前的章节中所做的一样。
- en: '[PRE1]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the next phase, the Irisdataset is loaded into the system based on the defined
    `batchsize` and number of samples per batch:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一阶段，Irisdataset 根据定义的 `batchsize` 和每批次的样本数量被加载到系统中：
- en: '[PRE2]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, the RBM is created as a layer using `NeuralNetConfiguration.Builder()`.
    Similarly, the object of Restricted Boltzmann is used to store properties such
    as the transforms applied to the observed and hidden layer - Gaussian and Rectified
    Linear Transforms, respectively:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，RBM 作为一层使用 `NeuralNetConfiguration.Builder()` 创建。同样，限制玻尔兹曼机的对象用于存储属性，例如应用于观察层和隐藏层的变换——分别是高斯变换和修正线性变换：
- en: '[PRE3]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`ReLU` is used for activation function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReLU` 被用作激活函数：'
- en: '[PRE4]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`weightInit()` function is used for initialization of the weight, which represents
    the starting value of the coefficients needed to amplify the input signal coming
    into each node:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`weightInit()` 函数用于初始化权重，它表示用于放大每个节点输入信号的系数的初始值：'
- en: '[PRE5]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Gaussian Transformation is used for visible units and Rectified Linear Transformation
    is used for hidden layers. This is very simple in Deeplearning4j. We need to pass
    the parameters `VisibleUnit`.`GAUSSIAN` and `HiddenUnit.RECTIFIED` inside the
    `.visibleUnit` and `.hiddenUnit` methods:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯变换用于可见单元，修正线性变换用于隐藏层。在 Deeplearning4j 中这非常简单。我们需要在 `.visibleUnit` 和 `.hiddenUnit`
    方法中传递参数 `VisibleUnit.GAUSSIAN` 和 `HiddenUnit.RECTIFIED`：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Backpropagation step size is defined here:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的步长在此处定义：
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To scale to dataset, `scale()` can be called with the object of the Dataset
    class:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展数据集，可以通过调用 Dataset 类的对象来执行 `scale()`：
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After the evaluation was done in the earlier process, the model is now completely
    ready to be trained. It can be trained in a similar manner using the `fit()` method,
    as done for the earlier models, and passing `getFeatureMatrix` as the parameter:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前过程中的评估完成后，模型现在已完全准备好进行训练。它可以以类似的方式使用 `fit()` 方法进行训练，和之前的模型一样，并传递 `getFeatureMatrix`
    作为参数：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Deep Belief networks
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度置信网络
- en: As explained in this chapter, a DBN is a stacked version of the number of RBMs.
    In this part, we will show how to deploy DBNs programmatically using Deeplearning4j.
    The flow of the program will follow the standard procedure as with other models.
    The implementation of simple DBNs is pretty simple using Deeplearning4j. The example
    will show how to train and traverse the input MNIST data with DBNs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章所述，DBN 是多个 RBM 的堆叠版本。在这一部分，我们将展示如何使用 Deeplearning4j 编程方式部署 DBN。程序的流程将遵循与其他模型相同的标准过程。使用
    Deeplearning4j 实现简单的 DBN 非常简单。该示例将展示如何使用 DBN 训练和遍历输入的 MNIST 数据。
- en: 'For MNIST dataset, the following line specifies the batchsize and number of
    examples, which one user will specify to load the data in HDFS at one time:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MNIST 数据集，以下行指定了批次大小和示例数量，用户将指定这些参数以便一次性加载数据到 HDFS 中：
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the next phase, the model will be built by stacking 10 RBMs together. The
    following piece of code will specify the way this should be done using Deeplearning4j:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一阶段，模型将通过堆叠 10 个 RBM 来构建。以下代码段将指定如何使用 Deeplearning4j 来完成此操作：
- en: '[PRE11]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the last part, the code will be trained using the loaded MNIST dataset,
    by calling the `fit()` method:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，代码将使用加载的 MNIST 数据集进行训练，通过调用 `fit()` 方法：
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Upon executing the code, the process will give a following output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，过程将输出以下结果：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The RBM is a generative model, which can randomly produce visible data values
    when some latent or hidden parameters are supplied to it. In this chapter, we
    have discussed the concept and mathematical model of the Boltzmann machine, which
    is an energy-based model. The chapter then discusses and gives a visual representation
    of the RBM. Further, this chapter discusses CRBM, which is a combination of Convolution
    and RBMs to extract the features of high dimensional images. We then moved toward
    popular DBNs that are nothing but a stacked implementation of RBMs. The chapter
    further discusses the approach to distribute the training of RBMs as well as DBNs
    in the Hadoop framework.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是一种生成模型，当输入一些潜在或隐藏参数时，它可以随机生成可见数据值。在本章中，我们讨论了Boltzmann机的概念和数学模型，它是一种基于能量的模型。接下来，本章讨论并给出了RBM的可视化表示。此外，本章还讨论了CRBM，它是卷积和RBM的结合，用于提取高维图像的特征。然后，我们介绍了流行的DBN，它实际上是RBM的堆叠实现。最后，本章讨论了如何在Hadoop框架中分布式训练RBM和DBN的方法。
- en: We conclude the chapter by providing code samples for both the models. The next
    chapter of the book will introduce one more generative model called autoencoder
    and its various forms such as de-noising autoencoder, deep autoencoder, and so
    on.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过提供两种模型的代码示例来结束。本书的下一章将介绍一种名为自动编码器的生成模型及其多种形式，如去噪自动编码器、深度自动编码器等。
