- en: Chapter 4. Generating Text with a Recurrent Neural Net
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 使用递归神经网络生成文本
- en: In the previous chapter, you learned how to represent a discrete input into
    a vector so that neural nets have the power to understand discrete inputs as well
    as continuous ones.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何将离散输入表示为向量，以便神经网络能够理解离散输入以及连续输入。
- en: Many real-world applications involve variable-length inputs, such as connected
    objects and automation (sort of Kalman filters, much more evolved); natural language
    processing (understanding, translation, text generation, and image annotation);
    human behavior reproduction (text handwriting generation and chat bots); and reinforcement
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的应用涉及可变长度的输入，例如物联网和自动化（类似卡尔曼滤波器，已经更为进化）；自然语言处理（理解、翻译、文本生成和图像注释）；人类行为重现（文本手写生成和聊天机器人）；强化学习。
- en: 'Previous networks, named feedforward networks, are able to classify inputs
    of fixed dimensions only. To extend their power to variable-length inputs, a new
    category of networks has been designed: the **recurrent neural networks** (**RNN**)
    that are well suited for machine learning tasks on variable-length inputs or sequences.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的网络，称为前馈网络，只能对固定维度的输入进行分类。为了将它们的能力扩展到可变长度的输入，设计了一个新的网络类别：**递归神经网络**（**RNN**），非常适合用于处理可变长度输入或序列的机器学习任务。
- en: 'Three well-known recurrent neural nets (simple RNN, GRU, and LSTM) are presented
    for the example of text generation. The topics covered in this chapter are as
    follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了三种著名的递归神经网络（简单RNN、GRU和LSTM），并以文本生成作为示例。 本章涵盖的主题如下：
- en: The case of sequences
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列的案例
- en: The mechanism of recurrent networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归网络的机制
- en: How to build a simple recurrent network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个简单的递归网络
- en: Backpropagation through time
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: Different types of RNN, LSTM, and GRU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的RNN、LSTM和GRU
- en: Perplexity and word error rate
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 困惑度和词错误率
- en: Training on text data for text generation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本数据上进行训练以生成文本
- en: Applications of recurrent networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归网络的应用
- en: Need for RNN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络的需求
- en: Deep learning networks for natural language is numerical and deals well with
    multidimensional arrays of floats and integers, as input values. For categorical
    values, such characters or words, the previous chapter demonstrated a technique
    known as embedding for transforming them into numerical values as well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络在自然语言处理中的应用是数值化的，能够很好地处理多维数组的浮点数和整数作为输入值。对于类别值，例如字符或单词，上一章展示了一种称为嵌入（embedding）的技术，将它们转换为数值。
- en: So far, all inputs have been fixed-sized arrays. In many applications, such
    as texts in natural language processing, inputs have one semantic meaning but
    can be represented by sequences of variable length.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有的输入都是固定大小的数组。在许多应用中，如自然语言处理中的文本，输入有一个语义含义，但可以通过可变长度的序列来表示。
- en: 'There is a need to deal with variable-length sequences as shown in the following
    diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 需要处理可变长度的序列，如下图所示：
- en: '![Need for RNN](img/00065.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![RNN的需求](img/00065.jpeg)'
- en: '**Recurrent Neural Networks** (**RNN**) are the answer to variable-length inputs.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）是应对可变长度输入的解决方案。'
- en: Recurrence can be seen as applying a feedforward network more than once at different
    time steps, with different incoming input data, but with a major difference, the
    presence of connections to the past, previous time steps, and in one goal, to
    refine the representation of input through time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 递归可以看作是在不同时间步长上多次应用前馈网络，每次应用时使用不同的输入数据，但有一个重要区别，那就是存在与过去时间步长的连接，目标是通过时间不断优化输入的表示。
- en: At each time step, the hidden layer output values represent an intermediate
    state of the network.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长，隐藏层的输出值代表网络的中间状态。
- en: 'Recurrent connections define the transition for moving from one state to another,
    given an input, in order to refine the representation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 递归连接定义了从一个状态到另一个状态的转换，给定输入的情况下，以便不断优化表示：
- en: '![Need for RNN](img/00066.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![RNN的需求](img/00066.jpeg)'
- en: Recurrent neural networks are suited for challenges involving sequences, such
    as texts, sounds and speech, hand writing, and time series.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络适用于涉及序列的挑战，如文本、声音和语音、手写文字以及时间序列。
- en: A dataset for natural language
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言的数据集
- en: As a dataset, any text corpus can be used, such as Wikipedia, web articles,
    or even with symbols such as code or computer programs, theater plays, and poems;
    the model will catch and reproduce the different patterns in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据集，可以使用任何文本语料库，例如 Wikipedia、网页文章，甚至是包含代码或计算机程序、戏剧或诗歌等符号的文本；模型将捕捉并重现数据中的不同模式。
- en: 'In this case, let''s use tiny Shakespeare texts to predict new Shakespeare
    texts or at least, new texts written in a style inspired by Shakespeare; two levels
    of predictions are possible, but can be handled in the same way:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用微型莎士比亚文本来预测新的莎士比亚文本，或者至少是风格上受到莎士比亚启发的新文本；有两种预测层次可以使用，但可以以相同的方式处理：
- en: '**At the character level**: Characters belong to an alphabet that includes
    punctuation, and given the first few characters, the model predicts the next characters
    from an alphabet, including spaces to build words and sentences. There is no constraint
    for the predicted word to belong to a dictionary and the objective of training
    is to build words and sentences close to real ones.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在字符级别**：字符属于一个包含标点符号的字母表，给定前几个字符，模型从字母表中预测下一个字符，包括空格，以构建单词和句子。预测的单词不需要属于字典，训练的目标是构建接近真实单词和句子的内容。'
- en: '**At the word level**: Words belong to a dictionary that includes punctuation,
    and given the first few words, the model predicts the next word out of a vocabulary.
    In this case, there is a strong constraint on the words since they belong to a
    dictionary, but not on sentences. We expect the model to focus more on capturing
    the syntax and meaning of the sentences than on the character level.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在单词级别**：单词属于一个包含标点符号的字典，给定前几个单词，模型从词汇表中预测下一个单词。在这种情况下，单词有强烈的约束，因为它们属于字典，但句子没有这种约束。我们期望模型更多地关注捕捉句子的语法和意义，而不是字符级别的内容。'
- en: In both modes, token designates character/word; dictionary, alphabet, or vocabulary
    designates (the list of possible values for the token);
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种模式下，token 表示字符/单词；字典、字母表或词汇表表示（token 的可能值的列表）；
- en: 'The popular NLTK library, a Python module, is used to split texts into sentences
    and tokenize into words:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的 NLTK 库，一个 Python 模块，用于将文本分割成句子并将其标记化为单词：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In a Python shell, run the following command to download the English tokenizer
    in the `book` package:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python shell 中，运行以下命令以下载 `book` 包中的英语分词器：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s parse the text to extract words:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解析文本以提取单词：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or the `char` library:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 或者 `char` 库：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The additional start token (the `START` word and the `^` character) avoids having
    a void hidden state when the prediction starts. Another solution is to initialize
    the first hidden state with ![A dataset for natural language](img/00067.jpeg).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的开始标记（`START` 单词和 `^` 字符）避免了预测开始时产生空的隐藏状态。另一种解决方案是用 ![A dataset for natural
    language](img/00067.jpeg) 初始化第一个隐藏状态。
- en: The additional end token (the `END` word and the `$` character) helps the network
    learn to predict a stop when the sequence generation is predicted to be finished.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的结束标记（`END` 单词和 `$` 字符）帮助网络学习在序列生成预测完成时预测停止。
- en: Last, the `out of vocabulary` token (the `UNKNOWN` word) replaces words that
    do not belong to the vocabulary to avoid big dictionaries.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`out of vocabulary` 标记（`UNKNOWN` 单词）替换那些不属于词汇表的单词，从而避免使用庞大的词典。
- en: In this example, we'll omit the validation dataset, but for any real-world application,
    keeping a part of the data for validation is a good practice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将省略验证数据集，但对于任何实际应用程序，将一部分数据用于验证是一个好的做法。
- en: 'Also, note that functions from [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* for layer initialization `shared_zeros`
    and `shared_glorot_uniform` and from [Chapter 3](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 3. Encoding Word into Vector"), *Encoding Word into Vector* for model
    saving and loading `save_params` and `load_params` have been packaged into the
    `utils` package:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意，[第 2 章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "第 2 章. 使用前馈网络分类手写数字") 中的函数，*使用前馈网络分类手写数字* 用于层初始化 `shared_zeros` 和 `shared_glorot_uniform`，以及来自[第
    3 章](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b "第 3 章. 将单词编码为向量")，*将单词编码为向量*
    用于模型保存和加载的 `save_params` 和 `load_params` 已被打包到 `utils` 包中：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Simple recurrent network
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的递归网络
- en: 'An RNN is a network applied at multiple time steps but with a major difference:
    a connection to the previous state of layers at previous time steps named hidden
    states ![Simple recurrent network](img/00068.jpeg):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是在多个时间步上应用的网络，但有一个主要的区别：与前一个时间步的层状态之间的连接，称为隐状态！[简单递归网络](img/00068.jpeg)：
- en: '![Simple recurrent network](img/00069.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![简单递归网络](img/00069.jpeg)'
- en: 'This can be written in the following form:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以写成以下形式：
- en: '![Simple recurrent network](img/00070.jpeg)![Simple recurrent network](img/00071.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![简单递归网络](img/00070.jpeg)![简单递归网络](img/00071.jpeg)'
- en: An RNN can be unrolled as a feedforward network applied on the sequence ![Simple
    recurrent network](img/00072.jpeg) as input and with shared parameters between
    different time steps.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RNN可以展开为一个前馈网络，应用于序列！[简单递归网络](img/00072.jpeg)作为输入，并在不同时间步之间共享参数。
- en: 'Input and output''s first dimension is time, while next dimensions are for
    the data dimension inside each step. As seen in the previous chapter, the value
    at a time step (a word or a character) can be represented either by an index (an
    integer, 0-dimensional) or a one-hot-encoding vector (1-dimensional). The former
    representation is more compact in memory. In this case, input and output sequences
    will be 1-dimensional represented by a vector, with one dimension, the time:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出的第一个维度是时间，而后续维度用于表示每个步骤中的数据维度。正如上一章所见，某一时间步的值（一个单词或字符）可以通过索引（整数，0维）或独热编码向量（1维）表示。前者在内存中更加紧凑。在这种情况下，输入和输出序列将是1维的，通过一个向量表示，且该维度为时间：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The structure of the training program remains the same as in [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network* with feedforward network, except
    the model that we''ll define with a recurrent module shares the same weights at
    different time steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 训练程序的结构与[第2章](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "第2章. 用前馈网络分类手写数字")中的*用前馈网络分类手写数字*相同，只是我们定义的模型与递归模块共享相同的权重，适用于不同的时间步：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s define the hidden and input weights:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义隐状态和输入权重：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And the output weights:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以及输出权重：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Initial state can be set to zero while using the start tokens:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 初始状态可以在使用开始标记时设为零：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It returns two tensors, where the first dimension is time and the second dimension
    is data values (0-dimensional in this case).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回两个张量，其中第一个维度是时间，第二个维度是数据值（在这种情况下为0维）。
- en: Gradient computation through the scan function is automatic in Theano and follows
    both direct and recurrent connections to the previous time step. Therefore, due
    to the recurrent connections, the error at a particular time step is propagated
    to the previous time step, a mechanism named **Backpropagation Through Time**
    (**BPTT**).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扫描函数进行的梯度计算在Theano中是自动的，并遵循直接连接和递归连接到前一个时间步。因此，由于递归连接，某一特定时间步的错误会传播到前一个时间步，这种机制被称为**时间反向传播**（**BPTT**）。
- en: It has been observed that the gradients either explode or vanish after too many
    time steps. This is why the gradients are truncated after 10 steps in this example,
    and errors will not be backpropagated to further past time steps.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 已观察到，在过多时间步后，梯度会爆炸或消失。这就是为什么在这个例子中，梯度在10个步骤后被截断，并且错误不会反向传播到更早的时间步。
- en: 'For the remaining steps, we keep the classification as before:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剩余的步骤，我们保持之前的分类方式：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This returns a vector of values at each time step.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在每个时间步返回一个值的向量。
- en: LSTM network
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM网络
- en: One of the main difficulties with RNN is to capture long-term dependencies due
    to the vanishing/exploding gradient effect and truncated backpropagation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的主要困难之一是捕捉长期依赖关系，这是由于梯度消失/爆炸效应和截断反向传播。
- en: To overcome this issue, researchers have been looking at a long list of potential
    solutions. A new kind of recurrent network was designed in 1997 with a memory
    unit, named a cell state, specialized in keeping and transmitting long-term information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，研究人员已经在寻找一长串潜在的解决方案。1997年设计了一种新型的递归网络，带有一个记忆单元，称为*细胞状态*，专门用于保持和传输长期信息。
- en: 'At each time step, the cell value can be updated partially with a candidate
    cell and partially erased thanks to a gate mechanism. Two gates, the update gate
    and the forget gate, decide how to update the cell, given the previously hidden
    state value and current input value:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，单元值可以部分通过候选单元更新，并通过门控机制部分擦除。两个门，更新门和忘记门，决定如何更新单元，给定先前的隐藏状态值和当前输入值：
- en: '![LSTM network](img/00073.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 网络](img/00073.jpeg)'
- en: 'The candidate cell is computed in the same way:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 候选单元的计算方式相同：
- en: '![LSTM network](img/00074.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 网络](img/00074.jpeg)'
- en: 'The new cell state is computed as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 新的单元状态的计算方式如下：
- en: '![LSTM network](img/00075.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 网络](img/00075.jpeg)'
- en: 'For the new hidden state, an output gate decides what information in the cell
    value to output:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新的隐藏状态，输出门决定要输出单元值中的哪些信息：
- en: '![LSTM network](img/00076.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 网络](img/00076.jpeg)'
- en: 'The remaining stays equal with the simple RNN:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分与简单 RNN 保持相同：
- en: '![LSTM network](img/00077.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 网络](img/00077.jpeg)'
- en: This mechanism allows the network to store some information to use a lot further
    in future than it was possible with a simple RNN.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该机制允许网络存储一些信息，并在未来比简单 RNN 更远的时间点使用这些信息。
- en: Many variants of design for LSTM have been designed and it is up to you to test
    these variants on your problems to see how they behave.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 LSTM 设计的变体已经被设计出来，你可以根据你的问题来测试这些变体，看看它们的表现。
- en: In this example, we'll use a variant, where gates and candidates use both the
    previously hidden state and previous cell state.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用一种变体，其中门和候选值同时使用了先前的隐藏状态和先前的单元状态。
- en: 'In Theano, let''s define the weights for:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Theano 中，让我们为以下内容定义权重：
- en: '- the input gate:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '- 输入门：'
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '- the forget gate:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '- 忘记门：'
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '- the output gate:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '- 输出门：'
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '- the cell:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '- 单元：'
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '- the output layer:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '- 输出层：'
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The array of all trainable parameters:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可训练参数的数组：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The step function to be placed inside the recurrent loop :'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要放置在循环中的步进函数：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s create the recurrent loop with the scan operator :'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用扫描操作符创建循环神经网络：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Gated recurrent network
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环网络
- en: 'The GRU is an alternative to LSTM, simplifying the mechanism without the use
    of an extra cell:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 是 LSTM 的替代方法，它简化了机制，不使用额外的单元：
- en: '![Gated recurrent network](img/00078.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![门控循环网络](img/00078.jpeg)'
- en: 'The code to build a gated recurrent network consists simply of defining the
    weights and the `step` function, as before:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 构建门控循环网络的代码仅需定义权重和 `step` 函数，如前所述：
- en: '- Weights for the Update gate:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更新门的权重：'
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '- Weights for the Reset gate:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '- 重置门的权重：'
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '- Weight for the Hidden layer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '- 隐藏层的权重：'
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '- Weight for the Output layer:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '- 输出层的权重：'
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The trainable parameters:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 可训练参数：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The step function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The recurrent loop:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Having introduced the major nets, we'll see how they perform on the text generation
    task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍了主要网络后，我们将看看它们在文本生成任务中的表现。
- en: Metrics for natural language performance
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言性能指标
- en: The **Word Error Rate** (**WER**) or **Character Error Rate** (**CER**) is equivalent
    to the designation of the accuracy error for the case of natural language.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**词错误率** (**WER**) 或 **字符错误率** (**CER**) 等同于自然语言准确度错误的定义。'
- en: 'Evaluation of language models is usually expressed with perplexity, which is
    simply:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的评估通常通过困惑度来表示，困惑度简单地定义为：
- en: '![Metrics for natural language performance](img/00079.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言性能指标](img/00079.jpeg)'
- en: Training loss comparison
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练损失对比
- en: 'During training, the learning rate might be strong after a certain number of
    epochs for fine-tuning. Decreasing the learning rate when the loss does not decrease
    anymore will help during the last steps of training. To decrease the learning
    rate, we need to define it as an input variable during compilation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，学习率在经过一定数量的 epochs 后可能会变强，用于微调。当损失不再减小时，减少学习率将有助于训练的最后步骤。为了减少学习率，我们需要在编译时将其定义为输入变量：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'During training, we adjust the learning rate, decreasing it if the training
    loss is not better:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们调整学习率，如果训练损失没有改善，则减小学习率：
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As a first experiment, let''s see the impact of the size of the hidden layer
    on the training loss for a simple RNN:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个实验，让我们看看隐藏层大小对简单 RNN 训练损失的影响：
- en: '![Training loss comparison](img/00080.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![训练损失对比](img/00080.jpeg)'
- en: More hidden units improve training speed and might be better in the end. To
    check this, we should run it for more epochs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的隐藏单元可以提高训练速度，最终可能表现更好。为了验证这一点，我们应该运行更多的 epochs。
- en: 'Comparing the training of the different network types, in this case, we do
    not observe any improvement with LSTM and GRU:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同网络类型的训练，在这种情况下，我们没有观察到LSTM和GRU有任何改善：
- en: '![Training loss comparison](img/00081.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![训练损失比较](img/00081.jpeg)'
- en: This might be due to the `truncate_gradient` option or because the problem is
    too simple and not so memory-dependent.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是由于`truncate_gradient`选项，或者因为问题过于简单，不依赖于记忆。
- en: Another parameter to tune is the minimum number of occurrences for a word to
    be a part of the dictionary. A higher number will learn on words that are more
    frequent, which is better.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要调整的参数是词汇出现在词典中的最小次数。更高的次数会学习到更频繁的词，这样更好。
- en: Example of predictions
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测示例
- en: 'Let''s predict a sentence with the generated model:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用生成的模型预测一个句子：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that we take the most probable next word (argmax), while we must, in order
    to get some randomness, draw the next word following the predicted probabilities.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们选择最有可能的下一个词（argmax），同时，为了增加一些随机性，我们必须根据预测的概率抽取下一个词。
- en: 'At 150 epochs, while the model has still not converged entirely with learning
    our Shakespeare writings, we can play with the predictions, initiating it with
    a few words, and see the network generate the end of the sentences:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在150个epoch时，虽然模型仍未完全收敛到我们对莎士比亚文笔的学习上，我们可以通过初始化几个单词来玩转预测，并看到网络生成句子的结尾：
- en: '**First citizen**: A word , i know what a word'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一市民**：一句话，我知道一句话'
- en: '**How** now!'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现在**怎么了！'
- en: '**Do** you not this asleep , i say upon this?'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**你**不觉得这样睡着了，我说的是这个吗？'
- en: '**Sicinius**: What, art thou my master?'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锡尼乌斯**：什么，你是我的主人吗？'
- en: '**Well,** sir, come.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**好吧**，先生，来吧。'
- en: '**I have been** myself'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我自己**已经做过'
- en: '**A most** hose, you in thy hour, sir'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最**重要的是你在你时光中的状态，先生'
- en: '**He shall** not this'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**他将不会**这样做'
- en: '**Pray you**, sir'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**祈求你**，先生'
- en: '**Come**, come, you'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来吧**，来吧，你'
- en: '**The crows**?'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**乌鸦**？'
- en: '**I''ll give** you'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我会给**你'
- en: '**What**, ho!'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**什么**，嘿！'
- en: '**Consider you**, sir'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑你**，先生'
- en: '**No more**!'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不再**！'
- en: '**Let us** be gone, or your UNKNOWN UNKNOWN, i do me to do'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们走吧**，或者你的未知未知，我做我该做的事'
- en: '**We are** not now'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们现在**不是'
- en: From these examples, we notice that the model has learned to position punctuation
    correctly, adding a point, comma, question mark, or an exclamation mark at the
    right place to order direct objects, indirect objects, and adjectives correctly.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些例子中，我们可以看出，模型学会了正确地定位标点符号，在正确的位置添加句点、逗号、问号或感叹号，从而正确地排序直接宾语、间接宾语和形容词。
- en: 'The original texts are composed of short sentences in a Shakespeare style.
    Bigger articles such as Wikipedia pages, as well as pushing the training further
    with a validation split to control overfitting will produce longer texts. [Chapter
    10](part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 10. Predicting
    Times Sequences with Advanced RNN"), *Predicting Times Sequence with Advanced
    RNN*: will teach how to predict time sequences with Advanced RNN and present an
    advanced version of this chapter.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 原文由短句组成，风格类似莎士比亚。更长的文章，如维基百科页面，以及通过进一步训练并使用验证集来控制过拟合，将生成更长的文本。[第10章](part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b
    "第10章. 使用先进的RNN预测时间序列")，*使用先进的RNN预测时间序列*：将教授如何使用先进的RNN预测时间序列，并展示本章的进阶版本。
- en: Applications of RNN
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的应用
- en: 'This chapter introduced the simple RNN, LSTM, and GRU models. Such models have
    a wide range of applications in sequence generation or sequence understanding:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了简单的RNN、LSTM和GRU模型。这些模型在序列生成或序列理解中有广泛的应用：
- en: 'Text generation, such as automatic generation of Obama political speech (obama-rnn),
    for example with a text seed on jobs:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成，例如自动生成奥巴马的政治演讲（obama-rnn），例如使用关于工作的话题作为文本种子：
- en: Good afternoon. God bless you. The United States will step up to the cost of
    a new challenges of the American people that will share the fact that we created
    the problem. They were attacked and so that they have to say that all the task
    of the final days of war that I will not be able to get this done. The promise
    of the men and women who were still going to take out the fact that the American
    people have fought to make sure that they have to be able to protect our part.
    It was a chance to stand together to completely look for the commitment to borrow
    from the American people. And the fact is the men and women in uniform and the
    millions of our country with the law system that we should be a strong stretcks
    of the forces that we can afford to increase our spirit of the American people
    and the leadership of our country who are on the Internet of American lives. Thank
    you very much. God bless you, and God bless the United States of America.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 下午好。愿上帝保佑你。美国将承担起解决美国人民面临的新挑战的责任，并承认我们创造了这一问题。他们受到了攻击，因此必须说出在战争最后日子里的所有任务，我无法完成。这是那些依然在努力的人们的承诺，他们将不遗余力，确保美国人民能够保护我们的部分。这是一次齐心协力的机会，完全寻找向美国人民借鉴承诺的契机。事实上，身着制服的男女和我们国家数百万人的法律系统应该是我们所能承受的力量的强大支撑，我们可以增加美国人民的精神力量，并加强我们国家领导层在美国人民生活中的作用。非常感谢。上帝保佑你们，愿上帝保佑美利坚合众国。
- en: You can check this example out in detail at [https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.4nee5wafe.](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.4nee5wafe.)
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在[https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.4nee5wafe.](https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.4nee5wafe.)查看这个例子。
- en: 'Text annotation, for example, the **Part of Speech** (**POS**) tags: noun,
    verb, particle, adverb, and adjective.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本注释，例如，**词性**（**POS**）标签：名词、动词、助词、副词和形容词。
- en: 'Generating human handwriting: [http://www.cs.toronto.edu/~graves/handwriting.html](http://www.cs.toronto.edu/~graves/handwriting.html)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成手写字：[http://www.cs.toronto.edu/~graves/handwriting.html](http://www.cs.toronto.edu/~graves/handwriting.html)
- en: '![Applications of RNN](img/00082.jpeg)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![RNN的应用](img/00082.jpeg)'
- en: Drawing with Sketch-RNN ([https://github.com/hardmaru/sketch-rnn](https://github.com/hardmaru/sketch-rnn))![Applications
    of RNN](img/00083.jpeg)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sketch-RNN绘图 ([https://github.com/hardmaru/sketch-rnn](https://github.com/hardmaru/sketch-rnn))![RNN的应用](img/00083.jpeg)
- en: '**Speech synthesis**: A recurrent network will generate parameters for generating
    each phoneme in a speech or voice speaking. In the following image, time-frequency
    homogeneous blocs are classified in phonemes (or graphemes or letters):![Applications
    of RNN](img/00084.jpeg)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音合成**：递归网络将生成用于生成每个音素的参数。在下面的图像中，时间-频率同质块被分类为音素（或字形或字母）：![RNN的应用](img/00084.jpeg)'
- en: 'Music generation:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐生成：
- en: Melody generation at [https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn](https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn).
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn](https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn)进行旋律生成。
- en: Mozart style music generation with Mozart-RNN at [http://www.hexahedria.com/2015/08/03/composing-music-withrecurrent-neural-networks/](http://www.hexahedria.com/2015/08/03/composing-music-withrecurrent-neural-networks/).
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Mozart-RNN生成莫扎特风格的音乐，[http://www.hexahedria.com/2015/08/03/composing-music-withrecurrent-neural-networks/](http://www.hexahedria.com/2015/08/03/composing-music-withrecurrent-neural-networks/)。
- en: Any classification of sequences, such as sentiment analysis (positive, negative,
    or neutral sentiments) that we'll address in [Chapter 5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 5. Analyzing Sentiment with a Bidirectional LSTM"), *Analyzing* *Sentiment
    with a Bidirectional LSTM*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何序列的分类，如情感分析（积极、消极或中立情感），我们将在[第5章](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "第5章. 使用双向LSTM分析情感")中讨论，*使用双向LSTM分析情感*。
- en: Sequence encoding or decoding that we'll address in [Chapter 6](part0069_split_000.html#21PMQ2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 6. Locating with Spatial Transformer Networks"), *Locating with Spatial
    Transformer Networks*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列编码或解码，我们将在[第6章](part0069_split_000.html#21PMQ2-ccdadb29edc54339afcb9bdf9350ba6b
    "第6章. 使用空间变换网络进行定位")中讨论，*使用空间变换网络进行定位*。
- en: Related articles
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关文章
- en: 'You can refer to the following links for more insight:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接以获得更多深入的见解：
- en: '*The Unreasonable Effectiveness of Recurrent Neural Networks*, Andrej Karpathy
    May 21, 2015 ([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*递归神经网络的非理性有效性*，Andrej Karpathy，2015年5月21日（[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))'
- en: '*Understanding LSTM Networks* on Christopher Colah''s blog''s, 2015 ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解LSTM网络*，Christopher Colah的博客，2015年（[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)）'
- en: 'Use of LSTM for audio classification: *Connectionist Temporal Classification
    and Deep Speech: Scaling up end-to-end speech recognition* ([https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567))'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM进行音频分类：*连接时序分类与深度语音：端到端语音识别的扩展*（[https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567)）
- en: Handwriting demo at [http://www.cs.toronto.edu/~graves/handwriting.html](http://www.cs.toronto.edu/~graves/handwriting.html)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写演示：[http://www.cs.toronto.edu/~graves/handwriting.html](http://www.cs.toronto.edu/~graves/handwriting.html)
- en: '*General Sequence Learning using Recurrent Neural Networks* tutorial at [https://www.youtube.com/watch?v=VINCQghQRuM](https://www.youtube.com/watch?v=VINCQghQRuM)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用递归神经网络的通用序列学习*教程：[https://www.youtube.com/watch?v=VINCQghQRuM](https://www.youtube.com/watch?v=VINCQghQRuM)'
- en: On the difficulty of training Recurrent Neural Networks Razvan Pascanu, Tomas
    Mikolov, Yoshua Bengio 2012
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于训练递归神经网络的难点，Razvan Pascanu，Tomas Mikolov，Yoshua Bengio，2012年
- en: 'Recurrent Neural Networks Tutorial:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络教程：
- en: Introduction to RNNS
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN简介
- en: Implementing RNN with Python, NumPy, and Theano
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python、NumPy和Theano实现RNN
- en: Backpropagation through time and vanishing gradients
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播与时间和梯度消失问题
- en: Implementing a GRU/LSTM RNN with Python and Theano Denny Britz 2015 at [http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python和Theano实现GRU/LSTM RNN，Denny Britz 2015年，[http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
- en: LONG SHORT-TERM MEMORY, Sepp Hochreiter, Jürgen Schmidhuber, 1997
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短时记忆（LONG SHORT-TERM MEMORY），Sepp Hochreiter，Jürgen Schmidhuber，1997年
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Recurrent Neural Networks provides the ability to process variable-length inputs
    and outputs of discrete or continuous data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络提供了处理离散或连续数据的变长输入和输出的能力。
- en: 'While the previous feedforward networks were able to process only one input
    to one output (one-to-one scheme), recurrent neural nets introduced in this chapter
    offered the possibility to make conversions between variable-length and fixed-length
    representations adding new operating schemes for deep learning input/output: one-to-many,
    many-to-many, or many-to-one.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的前馈网络只能处理单一输入到单一输出（一对一方案）的情况下，本章介绍的递归神经网络提供了在变长和定长表示之间进行转换的可能性，新增了深度学习输入/输出的新操作方案：一对多、多对多，或多对一。
- en: The range of applications of RNN is wide. For this reason, we'll study them
    more in depth in the further chapters, in particular how to enhance the predictive
    power of these three modules or how to combine them to build multi-modal, question-answering,
    or translation applications.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的应用范围广泛。因此，我们将在后续章节中更深入地研究它们，特别是如何增强这三种模块的预测能力，或者如何将它们结合起来构建多模态、问答或翻译应用。
- en: In particular, in the next chapter, we'll see a practical example using text
    embedding and recurrent networks for sentiment analysis. This time, there will
    also be an opportunity to review these recurrence units under another library
    Keras, a deep learning library that simplifies writing models for Theano.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，在下一章中，我们将通过一个实际示例，使用文本嵌入和递归网络进行情感分析。此次还将有机会在另一个库Keras下复习这些递归单元，Keras是一个简化Theano模型编写的深度学习库。
