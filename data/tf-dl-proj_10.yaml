- en: Video Games by Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与视频游戏
- en: 'Contrary to supervised learning, where an algorithm has to associate an input
    with an output, in reinforcement learning you have another kind of maximization
    task. You are given an environment (that is, a situation) and you are required
    to find a solution that will act (something that may require to interact with
    or even change the environment itself) with the clear purpose of maximizing a
    resulting reward. Reinforcement learning algorithms, then, are not given any clear,
    explicit goal but to get the maximum result possible in the end. They are free
    to find the way to achieve the result by trial and error. This resembles the experience
    of a toddler who experiments freely in a new environment and analyzes the feedback
    in order to find out how to get the best from their experience. It also resembles
    the experience we have with a new video game: first, we look for the best winning
    strategy; we try a lot of different things and then we decide how to act in the
    game.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，监督学习要求算法将输入与输出关联起来，而强化学习则是另一种最大化任务。在强化学习中，你会被给定一个环境（即一个情境），并需要找到一个解决方案，进行行动（这可能需要与环境进行交互，甚至改变环境本身），其明确目标是最大化最终的奖励。因此，强化学习算法没有明确的目标，而是致力于获得最终可能的最大结果。它们可以通过反复试验和错误的方式自由地找到实现结果的路径。这类似于幼儿在新环境中自由实验并分析反馈，以便找出如何从经验中获得最佳效果的过程。它也类似于我们玩新视频游戏时的体验：首先，我们寻找最佳的获胜策略；尝试许多不同的方式，然后决定如何在游戏中行动。
- en: At the present time, no reinforcement learning algorithm has the general learning
    capabilities of a human being. A human being learns more quickly from several
    inputs, and a human can learn how to behave in very complex, varied, structured,
    unstructured and multiple environments. However, reinforcement learning algorithms
    have proved able to achieve super-human capabilities (yes, they can be better
    than a human) in very specific tasks. A reinforcement learning algorithm can achieve
    brilliant results if specialized on a specific game and if given enough time to
    learn (an example is AlphaGo [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/)—
    the first computer program to defeat a world champion at Go, a complex game requiring
    long-term strategy and intuition).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有任何强化学习算法具有人类的通用学习能力。人类能从多种输入中更快速地学习，并且能够在非常复杂、多变、有结构、无结构和多重环境中学习如何行为。然而，强化学习算法已经证明能够在非常具体的任务中达到超越人类的能力（是的，它们可以比人类做得更好）。如果强化学习算法专注于某个特定游戏，并且有足够的时间进行学习，它们可以取得出色的成果（例如
    AlphaGo [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/)
    —— 第一个击败围棋世界冠军的计算机程序，围棋是一项复杂的游戏，需要长期的策略和直觉）。
- en: In this chapter, we are going to provide you with the challenging project of
    getting a reinforcement learning algorithm to learn how to successfully manage
    the commands of the Atari game Lunar Lander, backed up by deep learning. Lunar
    Lander is the ideal game for this project because reinforcement learning algorithm
    can work successfully on it, the game has few commands and it can be successfully
    completed just by looking at few values describing the situation in the game (there
    is no need even to look at the screen in order to understand what to do, in fact,
    the first version of the game dates back to the 1960s and it was textual).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将为你提供一个具有挑战性的项目，要求你让强化学习算法学习如何成功地控制 Atari 游戏《月球着陆者》的指令，该算法由深度学习提供支持。《月球着陆者》是这个项目的理想游戏，因为强化学习算法可以成功地在其中工作，游戏命令较少，并且仅通过查看描述游戏情境的几个数值，就能成功完成游戏（实际上，甚至不需要看屏幕来理解该怎么做，事实上，游戏的第一个版本可以追溯到1960年代，它是文字版的）。
- en: 'Neural networks and reinforcement learning are not new to each other;  in the
    early 1990s, at IBM, Gerry Tesauro programmed the famous TD-Gammon, combining
    feedforward neural networks with temporal-difference learning (a combination of
    Monte Carlo and dynamic programming) in order to train TD-Gammon to play world-class
    backgammon, which a board game for two players to be played using a couple of
    dices. If curious about the game,  you can read everything about the rules from
    the US Backgammon Federation: [http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/](http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/).
    At the time, the approach worked well with backgammon, due to the role of dices
    in the game that made it a non-deterministic game. Yet, it failed with every other
    game problem which was more deterministic. The last few years, thanks to the Google
    deep learning team of researchers, proved that neural networks can help solve
    problems other than backgammon, and that problem solving can be achieved on anyone''s
    computer. Now, reinforcement learning is at the top of the list of next big things
    in deep learning and machine learning as you can read from Ian Goodfellow, an
    AI research scientist at Google Brain, who is putting it top of the list: [https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002](https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和强化学习彼此并不陌生；在1990年代初期，IBM的Gerry Tesauro编程了著名的TD-Gammon，将前馈神经网络与时间差学习（蒙特卡洛方法和动态规划的结合）结合，训练TD-Gammon玩世界级的西洋双陆棋（一种使用骰子的两人棋盘游戏）。如果你对这款游戏感兴趣，可以通过美国双陆棋协会阅读规则：[http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/](http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/)。当时，这种方法在西洋双陆棋中效果很好，因为骰子在游戏中起着非确定性作用。然而，它在其他更具确定性的问题游戏中却失败了。近年来，感谢谷歌深度学习团队的研究人员证明，神经网络可以帮助解决除西洋双陆棋外的其他问题，而且问题解决可以在任何人的计算机上实现。现在，强化学习已成为深度学习和机器学习领域的下一个重要趋势，正如你可以从谷歌大脑的AI研究科学家Ian
    Goodfellow的文章中看到，他把它列为首要事项：[https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002](https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002)。
- en: The game legacy
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏的遗产
- en: Lunar Lander is an arcade game developed by Atari that first appeared in video
    game arcades around 1979\. Developed in black and white vector graphics and distributed
    in specially devised cabinets, the game showed, as a lateral view, a lunar landing
    pod approaching the moon, where there were special areas for landing. The landing
    areas varied in width and accessibility because of the terrain around them, which
    gave the user different scores when the lander landed. The player was provided
    with information about altitude, speed, amount of fuel available, score, and time
    taken so far. Given the force of gravity attracting the landing pod to the ground,
    the player could rotate or thrust (there were also inertial forces to be considered)
    the landing pod at the expense of some fuel. The fuel was the key to the game.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 《月球着陆者》是由Atari公司开发的一款街机游戏，首次出现在大约1979年的电子游戏街机中。该游戏采用黑白矢量图形开发，并通过特制的机柜分发，展示了一个从侧面视角看到的月球着陆舱接近月球的场景，月球上有专门的着陆区域。由于周围地形的原因，着陆区域的宽度和可达性有所不同，因此着陆时会给玩家不同的分数。玩家会得到关于高度、速度、剩余燃料、得分和至今用时的信息。由于重力的作用将着陆舱吸引到地面，玩家可以旋转或推动着陆舱（同时需要考虑惯性力），以消耗部分燃料。燃料是游戏的关键。
- en: The game ended when the landing pod touched the moon after running out of fuel.
    Until the fuel ran out, you kept on playing, even if you crashed. The commands
    available to the player were just four buttons, two for rotating left and right;
    one for thrusting from the base of the landing pod, pushing the module in the
    direction it is orientated; and the last button was for aborting the landing by
    rotating the landing pod upright and using a powerful (and fuel consuming) thrust
    in order to prevent the landing pod from crashing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏在着陆舱用尽燃料后触碰到月球时结束。直到燃料耗尽，你一直在玩游戏，即使你发生了碰撞。玩家可以使用的指令只有四个按钮：两个用于向左和向右旋转；一个用于从着陆舱底部推力，推动模块朝着其朝向的方向；最后一个按钮用于中止着陆，通过将着陆舱旋转至竖直状态并使用强力（且耗费燃料的）推力，以防止着陆舱坠毁。
- en: The interesting aspect of such a game is that there are clearly costs and rewards,
    but some are immediately apparent (like the quantity of fuel you are spending
    in your attempt) and others that they are all delayed until the time the landing
    pod touches the soil (you will know if the landing was a successful one only once
    it comes to a full stop). Maneuvering to land costs fuel, and that requires an
    economic approach to the game, trying not to waste too much. Landing provides
    a score. The more difficult and the safer the landing, the higher the score.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏的有趣之处在于，尽管存在明显的成本和奖励，但有些是立刻显现的（比如你在尝试过程中消耗的燃料量），而其他则是直到着陆舱触碰到地面时才会出现（只有当着陆舱完全停止后，你才能知道着陆是否成功）。为了着陆而进行的操控消耗燃料，因此需要一种经济的游戏策略，尽量避免浪费过多燃料。着陆会提供一个分数，着陆越困难且越安全，分数就越高。
- en: The OpenAI version
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI 版本
- en: As stated in the documentation available at its website ([https://gym.openai.com/](https://gym.openai.com/)),
    OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    The toolkit actually consists of a Python package that runs with both Python 2
    and Python 3, and the website API, which is useful for uploading your own algorithm's
    performance results and comparing them with others (an aspect of the toolkit that
    we won't be exploring, actually).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其官网上的文档所述（[https://gym.openai.com/](https://gym.openai.com/)），OpenAI Gym 是一个用于开发和比较强化学习算法的工具包。这个工具包实际上是一个
    Python 包，支持 Python 2 和 Python 3 运行，还有网站 API，可以上传你自己算法的性能结果，并与其他结果进行比较（这个工具包的一个方面我们并不会探讨）。
- en: 'The toolkit embodies the principles of reinforcement learning, where you have
    an environment and an agent: the agent can perform actions or inaction in the
    environment, and the environment will reply with a new state (representing the
    situation in the environment) and a reward, which is a score telling the agent
    if it is doing well or not. The Gym toolkit provides everything with the environment,
    therefore it is you that has to code the agent with an algorithm that helps the
    agent to face the environment. The environment is dealt by `env`, a class with
    methods for reinforcement learning which is instantiated when you issue the command
    to create it for a specific game: `gym.make(''environment'')`. Let''s examine
    an example from the official documentation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具包体现了强化学习的基本原理，其中包括一个环境和一个代理：代理可以在环境中执行动作或不动作，环境会返回一个新的状态（表示环境中的情况）和奖励，奖励是一个分数，用来告诉代理它是否做得好。Gym
    工具包提供了环境的一切，因此你需要编写代理的算法，帮助代理应对环境。环境通过 `env` 类来处理，这个类包含强化学习方法，实例化后可用于特定游戏，通过命令
    `gym.make('environment')` 创建。让我们来看一个来自官方文档的示例：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the run environment is `CartPole-v0`. Mainly a control problem,
    in the `CartPole-v0` game, a pendulum is attached to a cart that moves along a
    friction less track. The purpose of the game is to keep the pendulum upright as
    long as possible by applying forward or backward forces to the cart, and you can
    look at the dynamics of the game by watching this sequence on YouTube, which is
    part of a real-life experiment held at the Dynamics and Control Lab, IIT Madras
    and based on Neuron-like adaptive elements that can solve difficult control problems: [https://www.youtube.com/watch?v=qMlcsc43-lg](https://www.youtube.com/watch?v=qMlcsc43-lg).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，运行环境是 `CartPole-v0`。主要是一个控制问题，在 `CartPole-v0` 游戏中，一个摆锤被固定在一个沿无摩擦轨道移动的小车上。游戏的目的是通过对小车施加前进或后退的力量，使摆锤尽可能保持直立。你可以通过观看这个
    YouTube 视频，了解游戏的动态，该视频是 IIT Madras 动力学与控制实验室的一项实际实验的一部分，并基于“类似神经元的自适应元素能够解决困难的控制问题”：[https://www.youtube.com/watch?v=qMlcsc43-lg](https://www.youtube.com/watch?v=qMlcsc43-lg)。
- en: The Cartpole problem is described in *Neuron like adaptive elements that can
    solve difficult learning control problems* ([http://ieeexplore.ieee.org/document/6313077/](http://ieeexplore.ieee.org/document/6313077/))
    by BARTO, Andrew G.; SUTTON, Richard S.; ANDERSON, Charles W. in IEEE transactions
    on systems, man, and Cybernetics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Cartpole 问题在*类似神经元的自适应元素能够解决困难的学习控制问题*（[http://ieeexplore.ieee.org/document/6313077/](http://ieeexplore.ieee.org/document/6313077/)）一文中由BARTO,
    Andrew G.; SUTTON, Richard S.; ANDERSON, Charles W. 在IEEE Transactions on Systems,
    Man, and Cybernetics 中描述。
- en: 'Here is a brief explanation of the env methods, as applied in the example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是应用于示例中的 env 方法的简要说明：
- en: '`reset()`: This resets the environment''s state to the initial default conditions.
    It actually returns the start observations.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reset()`：这将重置环境的状态为初始默认条件。实际上，它返回起始观察值。'
- en: '`step(action)`: This moves the environment by a single time step. It returns
    a four-valued vector made of variables: `observations`, `reward`, `done`, and
    `info`. Observations are a representation of the state of the environment and
    it is represented in each game by a different vector of values. For instance,
    in a game involving physics such as `CartPole-v0`, the returned vector is composed
    of the cart''s position, the cart''s velocity, the pole''s angle, and the pole''s
    velocity. The reward is simply the score achieved by the previous action (you
    need to total the rewards in order to figure out the total score at each point).
    The variable `done` is a Boolean flag telling you whether you are at a terminal
    state in the game (game over). `info` will provide diagnostic information, something
    that you are expected not to use for your algorithm, but just for debugging.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`step(action)`：这将使环境按单个时间步移动。它返回一个由四个值组成的向量：`observations`、`reward`、`done`和`info`。观察值是环境状态的表示，并且在每个游戏中由不同的数值向量表示。例如，在涉及物理的游戏（如`CartPole-v0`）中，返回的向量包括小车的位置、小车的速度、杆的角度和杆的速度。奖励是上一个动作获得的分数（你需要累计奖励，以便计算每一时刻的总得分）。变量`done`是一个布尔标志，告诉你是否已到达游戏的终止状态（游戏结束）。`info`将提供诊断信息，尽管这些信息在算法中不应使用，但可以用于调试。'
- en: '`render( mode=''human'', close=False)`: This renders one time frame of the
    environment. The default mode will do something human-friendly, such as popping
    up a window. Passing the `close` flag signals the rendering engine to close any
    such windows.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`render( mode=''human'', close=False)`：这将渲染环境的一个时间帧。默认模式将执行一些人性化的操作，例如弹出一个窗口。传递`close`标志会指示渲染引擎关闭任何此类窗口。'
- en: 'The resulting effect of the commands is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的最终效果如下：
- en: Set up the `CartPole-v0` environment
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置`CartPole-v0`环境
- en: Run it for 1,000 steps
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行1,000步
- en: Randomly choose whether to apply a positive or negative force to the cart
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择是否对小车施加正向或负向的力
- en: Visualize the results
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化结果
- en: 'The interesting aspect of this approach is that you can change the game easily,
    just by providing a different string to the `gym.make` method (try for instance
    `MsPacman-v0` or `Breakout-v0` or choose any from the list you can obtain by `gym.print(envs.registry.all())`)
    and test your approach to solving different environments without changing anything
    in your code. OpenAI Gym makes it easy to test the generalization of your algorithm
    to different problems by using a common interface for all its environments. Moreover,
    it provides a framework for your reasoning, understanding and solving of agent-environment
    problems according to the schema. At time *t-1* a state and reward are pushed
    to an agent, and the agent reacts with an action, producing a new state and a
    new reward at time *t*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的有趣之处在于，你可以轻松地更改游戏，只需提供不同的字符串给`gym.make`方法（例如尝试`MsPacman-v0`或`Breakout-v0`，或者从通过`gym.print(envs.registry.all())`获得的列表中选择任何一个）即可，在不改变代码的情况下测试你解决不同环境的方案。OpenAI
    Gym通过为所有环境提供通用接口，使得测试你算法在不同问题上的泛化能力变得容易。此外，它为你提供了一个框架，帮助你根据该模式推理、理解和解决智能体与环境的问题。在时刻*t-1*，状态和奖励被传送给智能体，智能体做出反应并执行一个动作，在时刻*t*产生一个新的状态和奖励：
- en: '![](img/15d89b8b-a5c6-482b-8d00-d4e2f6b8b81e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15d89b8b-a5c6-482b-8d00-d4e2f6b8b81e.png)'
- en: 'Figure 1: How the environment and agent interact by means of state, action,
    and reward'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：环境和智能体通过状态、动作和奖励进行交互的方式
- en: 'In every distinct game in OpenAI Gym, both the action space (the commands the
    agent responds to) and the `observation_space` (the representation state) change.
    You can see how they have changed by using some `print` commands, just after you
    set up an environment:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI Gym中的每个不同游戏中，动作空间（智能体响应的命令）和`observation_space`（状态的表示）都会发生变化。你可以通过使用一些`print`命令，在设置环境之后查看它们的变化：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Installing OpenAI on Linux (Ubuntu 14.04 or 16.04)
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Linux（Ubuntu 14.04或16.04）上安装OpenAI
- en: 'We suggest installing the environment on an Ubuntu system. OpenGym AI has been
    created for Linux systems and there is little support for Windows. Depending on
    the previous settings of your system, you may need to install some additional
    things first:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在Ubuntu系统上安装此环境。OpenGym AI是为Linux系统创建的，对Windows的支持较少。根据你系统之前的设置，可能需要先安装一些额外的组件：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We suggest of working with Anaconda, so install Anaconda 3, too. You can find
    everything about installing this Python distribution at [https://www.anaconda.com/download/](https://www.anaconda.com/download/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用 Anaconda，所以也安装 Anaconda 3。你可以在[https://www.anaconda.com/download/](https://www.anaconda.com/download/)找到关于安装这个
    Python 发行版的所有信息。
- en: 'After setting the system requirements, installing OpenGym AI with all its modules
    is quite straightforward:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 设置系统要求后，安装 OpenGym AI 及其所有模块非常简单：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For this project, we are actually interested in working with the Box2D module,
    which is a 2D physics engine providing a rendering of real-world physics in a
    2D environment, as commonly seen in pseudo-realistic video games. You can test
    that the Box2D module works by running these commands in Python:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们实际上是想使用 Box2D 模块，它是一个 2D 物理引擎，提供在 2D 环境中模拟真实世界物理的渲染效果，通常在伪现实的电子游戏中能看到。你可以通过在
    Python 中运行这些命令来测试 Box2D 模块是否正常工作：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If the provided code runs with no problem, you can proceed with the project.
    In some situations, Box2D may become difficult to run and, for instance, there
    could be problems such as those reported in [https://github.com/cbfinn/gps/issues/34](https://github.com/cbfinn/gps/issues/34),
    though there are many other examples around. We have found that installing the
    Gym in a `conda` environment based on Python 3.4 makes things much easier:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供的代码运行没有问题，你可以继续进行项目。在某些情况下，Box2D 可能变得难以运行，例如可能会遇到[https://github.com/cbfinn/gps/issues/34](https://github.com/cbfinn/gps/issues/34)中报告的问题，虽然周围还有许多其他类似的例子。我们发现，将
    Gym 安装在基于 Python 3.4 的 `conda` 环境中会使事情变得更加简单：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This installation sequence should allow you to create a `conda` environment
    that's appropriate for the project we are going to present in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个安装顺序应该能让你创建一个适合本章我们要介绍的项目的 `conda` 环境。
- en: Lunar Lander in OpenAI Gym
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym 中的 Lunar Lander
- en: LunarLander-v2 is a scenario developed by Oleg Klimov, an engineer at OpenAI,
    inspired by the original Atari Lunar Lander ([https://github.com/olegklimov](https://github.com/olegklimov)).
    In the implementation, you have to take your landing pod to a lunar pad that is
    always located at coordinates *x=0* and *y=0*. In addition, your actual *x* and
    *y* position is known since their values are stored in the first two elements
    of the state vector, the vector that contains all the information for the reinforcement
    learning algorithm to decide the best action to take at a certain moment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LunarLander-v2 是由 OpenAI 工程师 Oleg Klimov 开发的一个场景，灵感来源于原始的 Atari Lunar Lander
    ([https://github.com/olegklimov](https://github.com/olegklimov))。在实现中，你需要将着陆舱带到一个始终位于坐标
    *x=0* 和 *y=0* 的月球平台。此外，你的实际 *x* 和 *y* 位置是已知的，因为它们的值存储在状态向量的前两个元素中，状态向量包含所有信息，供强化学习算法在某一时刻决定采取最佳行动。
- en: This renders the task accessible because you won't have to deal with fuzzy or
    uncertain localization of your position with respect to the objective (a common
    problem in robotics).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得任务更加可接近，因为你不需要处理与目标位置相关的模糊或不确定的位置定位问题（这是机器人技术中的常见问题）。
- en: '![](img/a99d1857-625c-476b-8829-5605e0eded24.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a99d1857-625c-476b-8829-5605e0eded24.png)'
- en: Figure 2: LunarLander-v2 in action
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LunarLander-v2 的运行情况
- en: 'At each moment, the landing pod has four possible actions to choose from:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每一刻，着陆舱都有四个可能的行动可供选择：
- en: Do nothing
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么也不做
- en: Rotate left
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向左旋转
- en: Rotate right
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向右旋转
- en: Thrust
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推力
- en: 'There is then a complex system of reward to make things interesting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一个复杂的奖励系统来使事情变得有趣：
- en: Reward for moving from the top of the screen to the landing pad and reaching
    zero speed ranges from 100 to 140 points (landing outside the landing pad is possible)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从屏幕顶部移动到着陆平台并达到零速度的奖励范围从 100 到 140 分（着陆在着陆平台外是可能的）
- en: If the landing pod moves away from landing pad without coming to a stop, it
    loses some of the previous rewards
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果着陆舱在没有停下来的情况下离开着陆平台，它会失去一些之前的奖励
- en: Each episode (the term used to point out a game session) completes when the
    landing pod crashes or it comes to rest, respectively providing additional -100
    or +100 points
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一回合（指游戏会话的术语）会在着陆舱坠毁或停下来时结束，分别提供额外的 -100 或 +100 分
- en: Each leg in contact with the ground is +10
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与地面接触的每条腿加 10 分
- en: Firing the main engine is -0.3 points per frame (but fuel is infinite)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动主引擎每帧扣除 -0.3 分（但燃料是无限的）
- en: Solving the episode grants 200 points
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这一回合会获得 200 分
- en: 'The game works perfectly with discrete commands (they are practically binary:
    full thrust or no thrust) because, as the author of the simulation says, according
    to Pontryagin''s maximum principle it''s optimal to fire the engine at full throttle
    or completely turn it off.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏非常适合离散命令（它们实际上是二进制的：全推力或无推力），因为正如模拟的作者所说，根据庞特里亚金最大值原理，最优的做法是以全推力开火引擎或完全关闭引擎。
- en: 'The game is also solvable using some simple heuristics based on the distance
    to the target and using a **proportional integral derivative** (**PID**) controller
    to manage the descending speed and angle. A PID is an engineering solution to
    control systems where you have feedback. At the following URL, you can get a more
    detailed explanation of how they work: [https://www.csimn.com/CSI_pages/PIDforDummies.html](https://www.csimn.com/CSI_pages/PIDforDummies.html).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏也可以通过一些简单的启发式方法解决，这些方法基于与目标的距离，并使用**比例积分微分**（**PID**）控制器来管理下降的速度和角度。PID是用于控制系统的工程解决方案，在这些系统中你有反馈。你可以通过以下网址获取更详细的解释：[https://www.csimn.com/CSI_pages/PIDforDummies.html](https://www.csimn.com/CSI_pages/PIDforDummies.html)。
- en: Exploring reinforcement learning through deep learning
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度学习探索强化学习
- en: In this project, we are not interested in developing a heuristic (a still valid
    approach to solving many problems in artificial intelligence) or constructing
    a working PID. We intend instead to use deep learning to provide an agent with
    the necessary intelligence to operate a Lunar Lander video game session successfully.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们并不打算开发启发式方法（虽然它仍然是解决许多人工智能问题的有效方法）或构建一个有效的PID控制器。相反，我们打算使用深度学习为智能体提供必要的智能，以成功操作月球着陆器视频游戏。
- en: 'Reinforcement learning theory offers a few frameworks to solve such problems:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习理论提供了一些框架来解决此类问题：
- en: '**Value-based learning**: This works by figuring out the reward or outcome
    from being in a certain state. By comparing the reward of different possible states,
    the action leading to the best state is chosen. Q-learning is an example of this
    approach.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于值的学习**：通过计算处于某个状态时的奖励或结果来工作。通过比较不同可能状态的奖励，选择导致最佳状态的动作。Q-learning就是这种方法的一个例子。'
- en: '**Policy-based learning**: Different control policies are evaluated based on
    the reward from the environment. It is decided upon the policy achieving the best
    results.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的学习**：不同的控制策略基于来自环境的奖励进行评估，并决定哪种策略能够获得最佳结果。'
- en: '**Model-based learning**: Here, the idea is to replicate a model of the environment
    inside the agent, thus allowing the agent to simulate different actions and their
    consequent reward.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的学习**：这里的理念是复制一个环境模型到智能体中，从而允许智能体模拟不同的动作及其相应的奖励。'
- en: 'In our project, we will use a value-based learning framework; specifically,
    we will use the now classical approach in reinforcement learning based on Q-learning,
    which has been successfully controlled games where an agent has to decide on a
    series of moves that will lead to a delayed reward later in the game. Devised
    by C.J.C.H. Watkins in 1989 in his Ph.D. thesis, the method, also called **Q-learning**,
    is based on the idea that an agent operates in an environment, taking into account
    the present state, in order to define a sequence of actions that will lead to
    an ultimate reward:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们将使用基于值的学习框架；具体而言，我们将采用现在在强化学习中已经成为经典的Q-learning方法，这个方法成功地应用于控制游戏，其中一个智能体需要决定一系列动作，最终导致游戏后期的延迟奖励。这个方法由C.J.C.H.
    Watkins在1989年在他的博士论文中提出，也被称为**Q-learning**，它基于这样的理念：智能体在一个环境中操作，考虑当前状态，以定义一系列动作，最终获得奖励：
- en: '![](img/99587134-1e20-4285-b204-3cff85be23f7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99587134-1e20-4285-b204-3cff85be23f7.png)'
- en: In the above formula, it is described how a state *s*, after an action *a*,
    leads to a reward, *r*, and a new state *s'*. Starting from the initial state
    of a game, the formula applies a series of actions that, one after the other,
    transforms each subsequent state until the end of the game. You can then imagine
    a game as a series of chained states by a sequence of actions. You can then also
    interpret the above formula how an initial state *s* is transformed into a final
    state *s'* and a final reward *r* by a sequence of actions *a*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，描述了一个状态*s*在经过一个动作*a*后，如何导致奖励*r*和一个新状态*s'*。从游戏的初始状态开始，该公式应用一系列动作，依次转化每个后续状态，直到游戏结束。你可以将游戏想象成通过一系列动作连接起来的状态链。你还可以解读上述公式，如何通过一系列动作*a*将初始状态*s*转变为最终状态*s'*和最终奖励*r*。
- en: 'In reinforcement terms, a **policy** is how to best choose our sequence of
    actions, *a*.  A policy can be approximated by a function, which is called *Q*,
    so that given the present state, *s*, and a possible action, *a*, as inputs, it
    will provide an estimate of the maximum reward, *r*, that will derive from that
    action:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，**策略**是如何选择最佳的动作序列，*a*。策略可以通过一个函数来逼近，称为*Q*，它以当前状态*s*和可能的动作*a*为输入，提供最大奖励*r*的估计，该奖励将从这个动作中得到：
- en: '![](img/d7fd4657-9ec3-4acf-94ae-c06421c76173.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7fd4657-9ec3-4acf-94ae-c06421c76173.png)'
- en: 'This approach is clearly greedy, meaning that we just choose the best action
    at a precise state because we expect that always choosing the best action at each
    step will lead us to the best outcome. Thus, in the greedy approach, we do not
    consider the possible chain of actions leading to the reward, but just the next
    action, *a*. However, it can be easily proved that we can confidently adopt a
    greedy approach and obtain the maximum reward using such a policy if such conditions
    are met:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法显然是贪心的，意味着我们仅仅选择在某个精确状态下的最佳动作，因为我们预期始终选择最佳动作将导致最佳结果。因此，在贪心方法中，我们并不考虑可能的动作链条，而只是关注下一个动作*a*。然而，可以很容易证明，只要满足以下条件，我们就可以自信地采用贪心方法，并通过这种策略获得最大奖励：
- en: we find the perfect policy oracle, *Q**
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们找到了完美的策略预言机，*Q*
- en: we operate in an environment where information is perfect (meaning we can know
    everything about the environment)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们操作的环境信息是完备的（即我们可以知道环境的所有信息）
- en: the environment adheres to the **Markov principle (see the tip box)**
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境遵循**马尔科夫原理**（见提示框）
- en: the Markov principle states that the future (states, rewards) only depends on
    the present and not the past, therefore we can simply derive the best to be done
    by looking at the present state and ignoring what has previously happened.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科夫原理指出，未来（状态、奖励）仅依赖于当前状态，而与过去无关，因此我们可以通过仅查看当前状态并忽略过去的发生来推导出应该做什么。
- en: In fact, if we build the *Q* function as a recursive function, we just need
    to explore (using a breadth-first search approach) the ramifications to the present
    state of our action to be tested, and the recursive function will return the maximum
    reward possible.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果我们将*Q*函数构建为递归函数，我们只需要使用广度优先搜索方法，探索当前状态下我们测试的动作的影响，递归函数将返回可能的最大奖励。
- en: 'Such an approach works perfectly in a computer simulation, but it makes little
    sense in the real world:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在计算机模拟中效果很好，但在现实世界中意义不大：
- en: Environments are mostly probabilistic. Even if you perform an action, you don't
    have the certainty of the exact reward.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境大多是概率性的。即使你执行了某个动作，也不能确定精确的奖励。
- en: Environments are tied to the past, the present alone cannot describe what could
    be the future because the past can have hidden or long-term consequences.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境与过去紧密相关，单独的当前状态无法描述未来的可能性，因为过去可能会带来隐性或长期的后果。
- en: Environments are not exactly predictable, so you cannot know in advance the
    rewards from an action, but you can know them afterward (this is called an **a
    posteriori** condition).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境并不完全可预测，因此你无法预先知道某个动作的奖励，但你可以在事后知道它们（这被称为**后验**条件）。
- en: Environments are very complex. You cannot figure out in a reasonable time all
    the possible consequences of an action, hence you cannot figure out with certainty
    the maximum reward deriving from an action.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境非常复杂。你无法在合理的时间内弄清楚一个动作可能带来的所有后果，因此你无法确定某个动作产生的最大奖励。
- en: The solution is then to adopt an approximate *Q* function, one that can take
    into account probabilistic outcomes and that doesn't need to explore all the future
    states by prediction. Clearly, it should be a real approximation function, because
    building a search table of values is unpractical in complex environments (some
    state spaces could take continuous values, making the possible combinations infinite).
    Moreover, the function can be learned offline, which implies leveraging the experience
    of the agent (the ability to memorize becomes then quite important).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是采用近似的*Q*函数，它能够考虑概率性结果，并且不需要通过预测来探索所有未来状态。显然，这应该是一个真正的逼近函数，因为在复杂环境中构建值的查找表是不切实际的（一些状态空间可能是连续值，使得可能的组合数是无限的）。此外，函数可以离线学习，这意味着可以利用智能体的经验（记忆能力变得非常重要）。
- en: There have been previous attempts to approximate a *Q* function by a neural
    network, but the only successful application has been `TD_Gammon`, a backgammon
    program that learned to play by reinforcement learning powered by a multi-layer
    perceptron only. `TD_Gammon` achieved a superhuman level of play, but at the time
    its success couldn't be replicated in different games, such as chess or go.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 之前曾有尝试通过神经网络来逼近*Q*函数，但唯一成功的应用是`TD_Gammon`，一个仅通过多层感知器的强化学习来学习玩跳棋的程序。`TD_Gammon`达到了超人类的水平，但当时它的成功无法在其他游戏中复制，比如国际象棋或围棋。
- en: That led to the belief that neural networks were not really suitable for figuring
    out a *Q* function unless the game was somehow stochastic (you have to throw a
    dice in backgammon). In 2013, a paper on deep reinforcement learning, *Playing
    Atari with deep reinforcement learning(*[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)*)*
    Volodymyr Minh, et al, applied to old Atari games demonstrated the contrary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致人们认为神经网络不适合计算*Q*函数，除非游戏本身是随机的（例如你必须在跳棋中掷骰子）。然而，2013年，Volodymyr Minh等人发表了一篇关于深度强化学习的论文，*Playing
    Atari with deep reinforcement learning*（[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)），证明了相反的观点。
- en: 'Such paper demonstrates how a *Q* function could be learned using neural networks
    to play a range of Atari arcade games (such as Beam Rider, Breakout, Enduro, Pong,
    Q*bert, Seaquest, and Space Invaders) just by processing video inputs (by sampling
    frames from a 210 x 160 RGB video at 60 Hz) and outputting joystick and fire button
    commands. The paper names the method a **Deep Q-Network** (**DQN**), and it also introduces
    the concepts of experience replay and exploration versus exploitation, which we
    are going to discuss in the next section. These concepts help to overcome some
    critical problems when trying to apply deep learning to reinforcement learning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文展示了如何使用神经网络学习一个*Q*函数，以便通过处理视频输入（通过以60Hz的频率从210 x 160 RGB视频中采样帧）并输出摇杆和开火按钮命令，来玩一系列的Atari街机游戏（如Beam
    Rider、Breakout、Enduro、Pong、Q*bert、Seaquest和Space Invaders）。论文将这种方法命名为**深度Q网络**（**DQN**），并且它还介绍了经验重放和探索与利用的概念，我们将在下一节讨论这些概念。这些概念有助于克服在将深度学习应用于强化学习时的一些关键问题：
- en: Lack of plenty of examples to learn from—something necessary in reinforcement
    learning and even more indispensable when using deep learning for it
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏足够的例子供学习——这是强化学习所必须的，特别是在使用深度学习时更加不可或缺。
- en: Extended delay between an action and the effective reward, which requires dealing
    with sequences of further actions of variable length before getting a reward
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作与有效奖励之间的延迟较长，这需要处理进一步行动的序列，这些序列长度不定，直到获得奖励为止。
- en: Series of highly correlated sequences of actions (because an action often influences
    the following ones), which may cause any stochastic gradient descent algorithm
    to overfit to the most recent examples or simply converge non-optimally (stochastic
    gradient descent expects random examples, not correlated ones)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列高度相关的行动序列（因为一个动作通常会影响后续的动作），这可能导致任何随机梯度下降算法过拟合最近的例子，或者根本没有以最优方式收敛（随机梯度下降期望的是随机样本，而不是相关样本）。
- en: The paper, *Human-level control through deep reinforcement learning* ([http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)),
    by Mnih and other researchers just confirms DQN efficacy where more games are
    explored by using it and performances of DQN are compared to human players and
    classical algorithms in reinforcement learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 论文*Human-level control through deep reinforcement learning*（[http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)）由Mnih和其他研究人员撰写，进一步确认了DQN的有效性，使用该方法探索了更多的游戏，并将DQN的表现与人类玩家及经典强化学习算法进行了比较。
- en: In many games, DQN proved better than human skills, though the long-term strategy
    is still a problem for the algorithm. In certain games, such as *Breakout*, the
    agent discovers cunning strategies such as digging a tunnel through the wall in
    order to send the ball through and destroy the wall in an effortless manner. In
    other games, such as *Montezuma's Revenge*, the agent remains clueless.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多游戏中，DQN表现得比人类更优秀，尽管长期策略仍然是该算法的一个问题。在某些游戏中，例如*Breakout*，代理发现了巧妙的策略，如通过墙壁挖隧道，将球送过墙壁并轻松摧毁它。在其他游戏中，如*Montezuma's
    Revenge*，代理依然一无所知。
- en: In the paper, the authors discuss at length how the agent understands the nuts
    and bolts of winning a Breakout game and they provide a chart of the response
    of the DQN function demonstrating how high reward scores are assigned to behaviors that
    first dig a hole in the wall and then let the ball pass through it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者详细讨论了智能体如何理解赢得 Breakout 游戏的关键技巧，并提供了一张 DQN 函数响应的图表，展示了如何将较高的奖励分配给那些首先在墙上挖一个洞，然后让球通过它的行为。
- en: Tricks and tips for deep Q-learning
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 学习的技巧与窍门
- en: 'Q-learning obtained by neural networks was deemed unstable until some tricks
    made it possible and feasible. There are two power-horses in deep Q-learning,
    though other variants of the algorithm have been developed recently in order to
    solve problems with performance and convergence in the original solution. Such
    new variants are not discussed in our project: double Q-learning, delayed Q-learning,
    greedy GQ, and speedy Q-learning. The two main DQN power-horses that we are going
    to explore are **experience replay** and the decreasing trade-off between **exploration
    and exploitation**.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过神经网络获得的 Q 学习被认为是不稳定的，直到一些技巧使其变得可行和切实可行。尽管最近已经开发出算法的其他变体来解决原始解决方案中的性能和收敛问题，但深度
    Q 学习有两个关键动力源。我们项目中没有讨论这些新变体：双 Q 学习、延迟 Q 学习、贪婪 GQ 和快速 Q 学习。我们将要探讨的两个主要的 DQN 动力源是
    **经验回放** 和 **探索与利用之间的逐渐权衡**。
- en: With experience replay, we simply store away the observed states of the game
    in a queue of a prefixed size since we discard older sequences when the queue
    is full. Contained in the stored data, we expect to have a number of tuples consisting
    of the present state, the applied action, the consequently obtained state, and
    the reward gained. If we consider a simpler tuple made of just the present state
    and the action, we have the observation of the agent operating in the environment,
    which we can consider the root cause of the consequent state and of the reward.
    We can consider now the tuple (present state and action) as our predictor ( *x*
    vector) with respect to the reward. Consequently, we can use the reward directly
    connected to the action and the reward that will be achieved at the end of the
    game.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过经验回放，我们只需将游戏中观察到的状态存储在一个固定大小的队列中，因为当队列满时，我们会丢弃较早的序列。存储的数据中，我们预计包含多个元组，每个元组由当前状态、所采取的动作、由此得到的状态以及获得的奖励组成。如果我们考虑一个更简单的元组，只包含当前状态和动作，那么我们就得到了智能体在环境中操作的观察，这可以被视为后续状态和奖励的根本原因。现在，我们可以将这个元组（当前状态和动作）视为我们对于奖励的预测器（*x*
    向量）。因此，我们可以直接使用与动作相关的奖励，以及在游戏结束时将会获得的奖励。
- en: Given such stored data (which we can figure out as the memory of our agent),
    we sample a few of them in order to create a batch and use the obtained batch
    to train our neural network. However, before passing the data to the network,
    we need to define our target variable, our *y* vector. Since the sampled states
    mostly won't be the final ones, we will probably have a zero reward or simply
    a partial reward to match against the known inputs (the present state and the
    chosen action). A partial reward is not very useful because it just tells part
    of the story we need to know. Our objective is, in fact, to know the total reward
    we will get at the end of the game, after having taken the action from the present
    state we are evaluating (our *x* value).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些存储的数据（我们可以把它看作是智能体的记忆），我们从中随机抽取一些，以创建一个批次并用来训练我们的神经网络。然而，在将数据传递给网络之前，我们需要定义我们的目标变量，即我们的
    *y* 向量。由于抽取的状态大多数不会是最终状态，我们可能会得到零奖励或仅是部分奖励，用来与已知的输入（当前状态和所选动作）进行匹配。部分奖励的意义不大，因为它只是告诉我们故事的一部分。我们的目标实际上是知道在游戏结束时我们将获得的总奖励，在评估当前状态下所采取的动作后（即我们的
    *x* 值）。
- en: In this case, since we don't have such information, we simply try to approximate
    the value by using our existing *Q* function in order to estimate the residual
    reward that will be the maximum consequence of the (state, action) tuple we are
    considering. After obtaining it, we discount its value using the Bellman equation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于我们没有这样的信息，我们只需尝试通过使用现有的 *Q* 函数来近似值，以估计将会是（状态，动作）元组最大结果的剩余奖励。获得这个值后，我们使用贝尔曼方程对其进行折扣。
- en: 'You can read an explanation of this now classic approach in reinforcement learning
    in this excellent tutorial by Dr. Sal Candido, a software engineer at Google:
    [http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation](http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation)),
    where the present reward is added to the discounted future reward.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这篇由Google的软件工程师Dr. Sal Candido编写的优秀教程中阅读关于这一经典强化学习方法的解释：[http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation](http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation)，其中当前奖励被加上折扣后的未来奖励。
- en: Using a small value (approaching zero) for discounting makes the *Q* function
    more geared toward short-term rewards, whereas using a high discount value (approaching
    one) renders the *Q* function more oriented to future gains.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个较小的折扣值（接近零）使得*Q*函数更倾向于短期奖励，而使用较高的折扣值（接近一）则使*Q*函数更注重未来收益。
- en: The second very effective trick is using a coefficient for trading between exploration
    and exploitation. In exploration, the agent is expected to try different actions
    in order to find the best course of action given a certain state. In exploitation,
    the agent leverages what it learned in the previous explorations and simply decides
    for what it believes the best action to be taken in that situation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个非常有效的技巧是使用一个系数来在探索和利用之间进行权衡。在探索中，智能体会尝试不同的动作，以便在给定某个状态时找到最佳的行动方案。在利用中，智能体则利用之前探索中学到的内容，直接选择它认为在该情境下最好的行动。
- en: Finding a good balance between exploration and exploitation is strictly connected
    to the usage of the experience replay we discussed earlier. At the start of the
    DQN algorithm optimization, we just have to rely on a random set of network parameters.
    This is just like sampling random actions, as we did in our simple introductory
    example to this chapter. The agent in such a situation will explore different
    states and actions, and help to shape the initial *Q* function. For complex games
    such as *Lunar Lander* using random choices won't take the agent far, and it could
    even turn unproductive in the long run because it will prevent the agent from
    learning the expected reward for tuples of state and action that can only be accessed
    if the agent has done the correct things before. In fact, in such a situation
    the DQN algorithm will have a hard time figuring out how to appropriately assign
    the right reward to an action because it will never have seen a completed game. Since the
    game is complex, it is unlikely that it could be solved by random sequences of
    actions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索与利用之间找到一个良好的平衡，与我们之前讨论的经验重放的使用紧密相关。在DQN算法优化的开始阶段，我们只能依赖一组随机的网络参数。这就像在本章的简单入门示例中我们做的那样，随机选择动作。在这种情况下，智能体会探索不同的状态和动作，帮助塑造初始的*Q*函数。对于像*Lunar
    Lander*这样复杂的游戏，单纯依靠随机选择无法带领智能体走得很远，甚至在长期看来可能变得低效，因为它会阻止智能体学习那些只能在智能体做出正确行为后才能访问的状态-动作组合的预期奖励。实际上，在这种情况下，DQN算法会很难弄清楚如何恰当地为一个动作分配正确的奖励，因为它从未见过一个完整的游戏。由于游戏本身很复杂，通过随机动作序列解决问题是不太可能的。
- en: The correct approach, then, is to balance learning by chance and using what
    has been learned to take the agent further in the game to where problems are yet
    to be solved. This resembles finding a solution by a series of successive approximations,
    by taking the agent each time a bit nearer to the correct sequence of actions
    for a safe and successful landing. Consequently, the agent should first learn
    by chance, find the best things to be done in a certain set of situations, then
    apply what has been learned and get access to new situations that, by random choice,
    will be also solved, learned, and applied successively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的方法是平衡通过随机性学习和利用已学知识推动智能体向前发展，直到进入尚未解决的问题区域。这类似于通过一系列逐步逼近找到解决方案，每次都将智能体带得更接近正确的行动序列，以实现安全且成功的着陆。因此，智能体应该首先通过随机方式学习，在某些情境下找到最佳的行动方案，然后应用学到的内容，进入新的情境，在这些情境中，智能体会通过随机选择继续解决问题、学习并逐步应用。
- en: This is done using a decreasing value as the threshold for the agent to decide
    whether, at a certain point in the game, to take a random choice and see what
    happens or leverage what it has learned so far and use it to make the best possible
    action at that point, given its actual capabilities. Picking a random number from
    a uniform distribution [*0*,*1*], the agent compares it with an epsilon value,
    and if the random number is larger than the epsilon it will use its approximate
    neural *Q* function. Otherwise, it will pick a random action from the options
    available. After that, it will decrease the epsilon number. Initially, epsilon
    is set at the maximum value, *1.0*, but depending on a decaying factor, it will
    decrease with time more or less rapidly, arriving at a minimum value that should
    never be zero (no chance of taking a random move) in order for there to always
    be the possibility of learning something new and unexpected (a minimal openness
    factor) by serendipity.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用一个递减的值作为阈值，帮助智能体决定在游戏的某个时刻是选择随机行动看看会发生什么，还是利用迄今为止学到的知识，结合其实际能力来做出最佳的可能行动。通过从均匀分布中选择一个随机数字[*0*,*1*]，智能体将其与一个epsilon值进行比较，如果随机数字大于epsilon，它将使用近似的神经网络*Q*函数。否则，它将从可选动作中随机选择一个。之后，epsilon值会减小。最初，epsilon设置为最大值*1.0*，但根据衰减因子，它会随着时间推移以不同的速度减小，最终达到一个最小值，且永远不为零（避免完全没有随机动作的机会），以确保总有可能通过意外的方式学习到新的、不期而遇的东西（最小的开放性因素）。
- en: Understanding the limitations of deep Q-learning
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度Q学习的局限性
- en: 'Even with deep Q-learning, there are some limitations, no matter whether you
    approximate your *Q* function by deriving it from visual images or other observations
    about the environment:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是深度Q学习，也存在一些限制，无论你是通过从视觉图像还是其他环境观察来近似你的*Q*函数：
- en: 'The approximation takes a long time to converge, and sometimes it doesn''t
    achieve it smoothly: you may even witness the learning indicators of the neural
    network worsening instead of getting better for many epochs.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近似过程需要很长时间才能收敛，有时甚至不能平稳收敛：你可能会看到神经网络的学习指标在许多回合中变得更差，而不是变得更好。
- en: 'Being based on a greedy approach, the approach offered by Q-learning is not
    dissimilar from a heuristic: it points out the best direction but it cannot provide
    detailed planning. When dealing with long-term goals or goals that have to be
    articulated into sub-goals, Q-learning performs badly.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基于贪婪方法，Q学习提供的方法与启发式方法类似：它指明了最佳方向，但无法提供详细的规划。当面对长期目标或需要分解成子目标的目标时，Q学习表现不佳。
- en: Another consequence of how Q-learning works is that it really doesn't understand
    the game dynamics from a general point of view but from a specific one (it replicates
    what it experienced as effective during training). As a consequence, any novelty
    introduced into the game (and never actually experienced during training) can
    break down the algorithm and render it completely ineffective. The same goes when
    introducing a new game to the algorithm; it simply won't perform.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习的另一个后果是，它并不从整体的角度理解游戏动态，而是从特定的角度理解（它复制了在训练过程中有效的经验）。因此，游戏中任何新引入的内容（在训练过程中从未实际经历过）都可能导致算法崩溃，使其完全无效。同样，当引入一个新游戏到算法时，它根本无法表现出应有的效果。
- en: Starting the project
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始项目
- en: 'After this long detour into reinforcement learning and the DQN approach, we
    are finally ready to start coding, having all the basic understanding of how to
    operate an OpenAI Gym environment and how to set a DQN approximation of a *Q*
    function. We simply start importing all the necessary packages:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了关于强化学习和DQN方法的长时间绕行后，我们终于准备好开始编码，已经具备了如何操作OpenAI Gym环境以及如何设置*Q*函数的DQN近似的基本理解。我们只需导入所有必要的包：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `tempfile` module generates temporary files and directories that can be
    used as a temporary storage area for data files. The `deque `command, from the
    `collections` module, creates a double-ended queue, practically a list where you
    can append items at the start or at the end. Interestingly, it can be set to a
    predefined size. When full, older items are discarded in order to make the place
    for new entries.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`tempfile`模块生成临时文件和目录，可以作为数据文件的临时存储区。`deque`命令来自`collections`模块，用于创建一个双端队列，实际上是一个可以在开始或结束处添加项的列表。有趣的是，它可以设置为预定义的大小。队列满时，较旧的项会被丢弃，以腾出位置给新项。'
- en: We will structure this project using a series of classes representing the agent,
    the agent's brain (our DQN), the agent's memory, and the environment, which is
    provided by OpenAI Gym but it needs to be correctly connected to the agent. It
    is necessary to code a class for this.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一系列类来构建这个项目，表示代理、代理的大脑（我们的DQN）、代理的记忆和环境，环境由OpenAI Gym提供，但需要正确地与代理连接。需要为此编写一个类。
- en: Defining the AI brain
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义AI大脑
- en: The first step in the project is to create a `Brain` class containing all the
    neural network code in order to compute a Q-value approximation. The class will
    contain the necessary initialization, the code for creating a suitable TensorFlow
    graph for the purpose, a simple neural network (not a complex deep learning architecture
    but a simple, working network for our project—you can replace it with more complex
    architectures), and finally, methods for fit and predict operations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的第一步是创建一个`Brain`类，其中包含所有的神经网络代码，以便计算Q值近似值。该类将包含必要的初始化代码，用于创建适当的TensorFlow图，构建一个简单的神经网络（不是复杂的深度学习架构，而是一个适用于我们项目的简单、可运行的网络——你可以将其替换为更复杂的架构），最后，还包括拟合和预测操作的方法。
- en: We start from initialization. As inputs, first, we really need to know the size
    of the state inputs (`nS`) corresponding to the information we receive from the
    game, and the size of the action output (`nA`) corresponding to the buttons we
    can press to perform actions in the game. Optionally, but strongly recommended,
    we also have to set the scope. In order to define the scope a string will help
    us to keep separate networks created for different purposes, and in our project,
    we have two, one for processing the next reward and one for guessing the final
    reward.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从初始化开始。作为输入，首先，我们确实需要知道与游戏中接收到的信息相关的状态输入的大小（`nS`），以及与我们可以按下的按钮对应的动作输出的大小（`nA`）。可以选择性地（但强烈推荐）设置作用域。为了定义作用域，我们需要一个字符串来帮助我们区分为不同目的创建的网络，在我们的项目中，我们有两个，一个用于处理下一个奖励，另一个用于猜测最终奖励。
- en: Then, we have to define the learning rate for the optimizer, which is an Adam.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要为优化器定义学习率，优化器使用的是Adam。
- en: The Adam optimizer is described in the following paper: [https://arxiv.org/abs/1412.6980.](https://arxiv.org/abs/1412.6980)It
    is a very efficient gradient-based optimization method that requires very little
    to be tuned in order to work properly. The Adam optimization is a stochastic gradient
    descent algorithm similar to RMSprop with Momentum. This post, [https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/](https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/),
    from the UC Berkeley Computer Vision Review Letters, provides more information.
    From our experience, it is one of the most effective solutions when training a
    deep learning algorithm in batches, and it requires some tuning for the learning
    rate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化器在以下论文中有描述：[https://arxiv.org/abs/1412.6980.](https://arxiv.org/abs/1412.6980)它是一种非常高效的基于梯度的优化方法，只需要很少的调节即可正常工作。Adam优化是一个随机梯度下降算法，类似于带动量的RMSprop。来自UC
    Berkeley计算机视觉评论信函的这篇文章，[https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/](https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/)，提供了更多信息。根据我们的经验，它是训练深度学习算法时最有效的解决方案之一，并且需要对学习率进行一些调优。
- en: 'Finally, we also provide:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还提供：
- en: A neural architecture (if we prefer to change the basic one provided with the
    class)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络架构（如果我们希望更改类中提供的基础架构）
- en: Input the `global_step`, a global variable that will keep track of the number
    of training batches of examples that have been feed to the DQN network  up to
    that moment
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`global_step`，这是一个全局变量，用于追踪到目前为止已经喂入DQN网络的训练批次数量。
- en: The directory in which to store the logs for TensorBoard, the standard visualization
    tool for TensorFlow
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储TensorBoard日志的目录，这是TensorFlow的标准可视化工具。
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The command  `tf.summary.FileWriter` initializes an event file in a target directory
    (`summary_dir`) where we store the key measures of the learning process. The handle
    is kept in `self.summary_writer`, which we will be using later for storing the
    measures we are interested in representing during and after the training for monitoring
    and debugging what has been learned.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 命令`tf.summary.FileWriter`在目标目录（`summary_dir`）中初始化一个事件文件，用于存储学习过程中的关键度量。该句柄保存在`self.summary_writer`中，我们稍后将使用它来存储我们希望在训练过程中以及训练后用于监控和调试学习情况的度量。
- en: The next method to be defined is the default neural network that we will be
    using for this project. As input, it takes the input layer and the respective
    size of the hidden layers that we will be using. The input layer  is defined by
    the state that we are using, which could be a vector of measurements, as in our
    case, or an image, as in the original DQN paper)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来定义的方法是我们将在此项目中使用的默认神经网络。作为输入，它接受输入层以及我们将使用的隐藏层的相应大小。输入层由我们使用的状态定义，状态可以是测量值的向量（如我们案例中的情况），或者是图像（如原始DQN论文中的情况）。
- en: Such layers are simply defined using the higher level ops offered by the `Layers`
    module of TensorFlow ([https://www.tensorflow.org/api_guides/python/contrib.layers](https://www.tensorflow.org/api_guides/python/contrib.layers)).
    Our choice goes for the vanilla `fully_connected`, using the `ReLU` (rectifier)
    `activation` function for the two hidden layers and the linear activation of the
    output layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的层是通过TensorFlow的`Layers`模块提供的高级操作来定义的（[https://www.tensorflow.org/api_guides/python/contrib.layers](https://www.tensorflow.org/api_guides/python/contrib.layers)）。我们选择了基础的`fully_connected`层，使用`ReLU`（修正线性）激活函数用于两个隐藏层，输出层使用线性激活函数。
- en: 'The predefined size of 32 is perfectly fine for our purposes, but you may increment
    it if you like. Also, there is no dropout in this network. Clearly, the problem
    here is not overfitting, but the quality of what is being learned, which could
    only be improved by providing useful sequences of unrelated states and a good
    estimate of the final reward to be associated. It is in the useful sequences of
    states, especially under the light of the trade-off between exploration and exploitation,
    that the key to not having the network overfit resides. In a reinforcement learning
    problem, you have overfitted if you fall into one of these two situations:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 预定义的32的大小对于我们的目的来说是完全合适的，但如果你愿意，可以增加它。此外，网络中没有使用dropout。显然，这里问题不在于过拟合，而是学习的质量，只有通过提供有用的、不相关的状态序列，并对最终的奖励做出良好的估计，才能改善学习质量。在有用的状态序列中，尤其是在探索与利用的权衡下，避免网络过拟合的关键所在。强化学习问题中，如果你陷入以下两种情况之一，就说明你的网络已经过拟合：
- en: 'Sub-optimality: the algorithm suggests sub-optimal solutions, that it is, our
    lander learned a rough way to land and it sticks to it because at least it lands'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 次优性：算法会建议次优的解决方案，也就是说，我们的着陆器学会了一种粗略的着陆方式，并坚持使用这种方式，因为至少它能成功着陆。
- en: 'Helplessness: the algorithm has fallen into a learned helplessness; that is,
    it has not found a way to land correctly, so it just accepts that it is going
    to crash in the least bad way possible'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无助性：算法已经陷入了学到的无助状态，也就是说，它没有找到正确着陆的方法，因此它只是接受最不坏的方式去“撞击”。
- en: These two situations can prove really difficult to overcome for a reinforcement
    learning algorithm such as DQN unless the algorithm can have the chance to explore
    alternative solutions during the game. Taking random moves from time to time is
    not simply a way to mess up things, as you may think at first sight, but a strategy
    to avoid pitfalls.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况对于强化学习算法（如DQN）来说，可能非常难以克服，除非该算法在游戏过程中能够有机会探索替代解决方案。偶尔采取随机动作，并非单纯的“搞乱事情”策略，正如你最初可能会认为的那样，而是一种避免陷阱的策略。
- en: With larger networks than this one, on the other hand, you may instead have
    a problem with a dying neuron requiring you to use a different activation, `tf.nn.leaky_relu` ([https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu)),
    in order to obtain a working network.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于比这个更大的网络，你可能会遇到“神经元死亡”问题，这时需要使用不同的激活函数，如`tf.nn.leaky_relu`（[https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu)），以便获得一个正常工作的网络。
- en: A dead `ReLU` ends up always outputting the same value, usually a zero value,
    and it becomes resistant to backpropagation updates.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 死亡的`ReLU`最终总是输出相同的值，通常是零，并且它对反向传播更新变得抗拒。
- en: 'The activation `leaky_relu` has been available since TensorFlow 1.4\. If you
    are using any previous version of TensorFlow, you can create an `ad hoc` function
    to be used in your custom network:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数 `leaky_relu` 从 TensorFlow 1.4 开始可用。如果你使用的是 TensorFlow 的较早版本，可以创建一个 `ad
    hoc` 函数，用于自定义网络：
- en: '`def leaky_relu(x, alpha=0.2):       return tf.nn.relu(x) - alpha * tf.nn.relu(-x)`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`def leaky_relu(x, alpha=0.2):       return tf.nn.relu(x) - alpha * tf.nn.relu(-x)`'
- en: 'We now proceed to code our `Brain` class, adding some more functions to it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续编写 `Brain` 类的代码，为它添加一些更多的功能：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The method`create_network` combines input, neural network, loss, and optimization.
    The loss is simply created by taking the difference between the original reward
    and the estimated result, squaring it, and taking the average through all the
    examples present in the batch being learned. The loss is minimized using an Adam
    optimizer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_network` 方法结合了输入、神经网络、损失和优化。损失通过计算原始奖励与估计结果之间的差异，平方并计算批次中所有示例的平均值来创建。损失使用
    Adam 优化器进行最小化。'
- en: 'Also, a few summaries are recorded for TensorBoard:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还记录了一些总结供 TensorBoard 使用：
- en: The average loss of the batch, in order to keep track of the fit during training
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次的平均损失，用于跟踪训练过程中的拟合情况
- en: The maximum predicted reward in the batch, in order to keep track of extreme
    positive predictions, pointing out the best-winning moves
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次中的最大预测奖励，用于跟踪极端的正向预测，指出最好的胜利动作
- en: The average predicted reward in the batch, in order to keep track of the general
    tendency of predicting good moves
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次中的平均预测奖励，用于跟踪预测好动作的整体趋势
- en: 'Here is the code for `create_network`, the TensorFlow engine of our project:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `create_network` 的代码，这是我们项目的 TensorFlow 引擎：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The class is completed by a `predict` and a `fit` method. The `fit` method takes
    as input the state matrix, `s`, as the input batch and the vector of reward `r`
    as the outcome. It also takes into account how many epochs you want to train (in
    the original papers it is suggested using just a single epoch per batch in order
    to avoid overfitting too much to each batch of observations). Then, in the present
    session, the input is fit with respect to the outcome and summaries (previously
    defined as we created the network).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该类通过 `predict` 和 `fit` 方法完成。`fit` 方法将状态矩阵 `s` 作为输入批次，将奖励向量 `r` 作为输出结果。它还考虑了训练的轮次数量（在原始论文中建议每个批次仅使用一个
    epoch，以避免过拟合每个批次的观察数据）。然后，在当前会话中，输入会根据结果和总结（我们创建网络时已定义）进行拟合。
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As a result,  `global step` is returned, which is a counter that helps to keep
    track of the number of examples used in training up so far, and then recorded
    for later use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 结果返回 `global step`，它是一个计数器，帮助跟踪到目前为止在训练中使用的示例数量，并记录以备后续使用。
- en: Creating memory for experience replay
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为经验回放创建内存
- en: After defining the brain (the TensorFlow neural network), our next step is to
    define the memory, that is the storage for data that will power the learning process
    of the DQN network. At each training episode each step, made of a state and an
    action, is recorded together with the consequent state and the final reward of
    the episode (something that will be known only when the episode completes).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义大脑（TensorFlow 神经网络）后，下一步是定义内存，这是存储数据的地方，这些数据将驱动 DQN 网络的学习过程。在每次训练的回合中，每一步（由一个状态和一个动作组成）都会被记录下来，并附上相应的状态和回合的最终奖励（这个奖励只有在回合结束时才能知道）。
- en: Adding a flag telling if the observation is a terminal one or not completes
    the set of recorded information. The idea is to connect certain moves not just
    to the immediate reward (which could be null or modest) but the ending reward,
    thus associating every move in that session to it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个标志，标明该观察是否为终止状态，完成了所记录信息的集合。这个想法是将某些动作与不仅是即时奖励（可能为零或适中），而是结束奖励相关联，从而将每个动作与该回合的最终奖励联系起来。
- en: The class memory is simply a queue of a certain size, which is then filled with information on
    the previous game experiences, and it is easy to sample and extract from it. Given
    its fixed size, it is important that older examples are pushed out of the queue,
    thus allowing the available examples to always be among the last ones.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 类 memory 只是一个固定大小的队列，然后用以前游戏经验的信息填充，且可以轻松从中进行采样和提取。由于其大小是固定的，因此很重要的一点是要将较旧的示例从队列中推除出去，从而确保可用示例始终是最近的那些。
- en: 'The class comprises an initialization, where the data structure takes origin
    and its size is fixed, the `len` method (so we know whether the memory is full
    or not, which is useful, for instance, in order to wait for any training at least
    until we have plenty of them for better randomization and variety for learning),
    `add_memory` for recording in the queue, and `recall_memory` for recovering all
    the data from it in a list format:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该类包含一个初始化过程，其中数据结构起源并且其大小是固定的，`len`方法（以便我们知道内存是否已满，这在某些情况下非常有用，例如等待训练，至少等到我们有足够的数据进行更好的随机化和多样化学习），`add_memory`用于记录到队列中，以及`recall_memory`用于以列表格式从队列中恢复所有数据：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating the agent
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建代理
- en: 'The next class is the agent, which has the role of initializing and maintaining
    the brain (providing the *Q-value* function approximation) and the memory. It
    is the agent, moreover, that acts in the environment. Its initialization sets
    a series of parameters that are mostly fixed given our experience in optimizing
    the learning for the Lunar Lander game. They can be explicitly changed, though,
    when the agent is first initialized:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个类是代理，负责初始化和维护大脑（提供*Q值*函数近似）和记忆。代理还会在环境中执行动作。它的初始化设置了一系列参数，这些参数大部分是固定的，根据我们在优化《Lunar
    Lander》游戏学习中的经验得出的。不过，在代理首次初始化时，这些参数是可以显式更改的：
- en: '`epsilon = 1.0` is the initial value in the exploration-exploitation parameter.
    The `1.0` value forces the agent to completely rely on exploration, that is, random
    moving.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon = 1.0` 是探索-利用参数的初始值。`1.0`的值强制代理完全依赖于探索，即随机移动。'
- en: '`epsilon_min = 0.01` sets the minimum value of the exploration-exploitation
    parameter: a value of `0.01` means that there is a 1% chance that the landing
    pod will move randomly and not based on *Q* function feedback. This always provides
    a minimum chance to find another optimal way of completing the game, without compromising
    it.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon_min = 0.01` 设置探索-利用参数的最小值：`0.01`的值意味着着陆舱有1%的几率会随机移动，而不是基于*Q*函数的反馈。这始终提供了一个最小的机会来找到完成游戏的另一种最优方式，而不会影响游戏的进行。'
- en: '`epsilon_decay = 0.9994` is the decay that regulates the speed the `epsilon`
    diminishes toward the minimum. In this setting, it is tuned to reach a minimum
    value after about 5,000 episodes, which on average should provide the algorithm
    at least 2 million examples to learn from.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon_decay = 0.9994` 是调节`epsilon`向最小值衰减速度的衰减因子。在此设置中，它被调整为大约在5,000个回合后达到最小值，这样平均应能为算法提供至少200万次学习的样本。'
- en: '`gamma = 0.99` is the reward discount factor with which the Q-value estimation
    weights the future reward with respect to the present reward, thus allowing the
    algorithm to be short- or long-sighted, according to what is best in the kind
    of game being played (in Lunar Lander it is better to be long-sighted because
    the actual reward will be experienced only when the landing pod lands on the Moon).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma = 0.99` 是奖励折扣因子，通过它，Q值估计将未来奖励相对于当前奖励的权重进行调整，从而使算法根据所玩的游戏类型可以短视或长视（在《Lunar
    Lander》中，最好是长视，因为实际的奖励只有在着陆舱成功着陆月球时才能体验）。'
- en: '`learning_rate = 0.0001` is the learning rate for the Adam optimizer to learn
    the batch of examples.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate = 0.0001` 是Adam优化器用于学习批量样本的学习率。'
- en: '`epochs = 1` is the training epochs used by the neural network in order to
    fit the batch set of examples.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs = 1` 是神经网络用来拟合批量样本集的训练轮次。'
- en: '`batch_size = 32` is the size of the batch examples.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size = 32` 是批量样本的大小。'
- en: '`memory = Memory(memory_size=250000)` is the size of the memory queue.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory = Memory(memory_size=250000)` 是记忆队列的大小。'
- en: Using the preset parameters, you are assured that the present project will work.
    For different OpenAI environments, you may need to find different optimal parameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预设的参数，您可以确保当前项目能够正常运行。对于不同的OpenAI环境，您可能需要找到不同的最优参数。
- en: The initialization will also provide the commands required to define where the
    TensorBoard logs will be placed (by default, the `experiment` directory), the
    model for learning how to estimate the immediate next reward, and another model
    to store the weights for the final reward. In addition, a saver (`tf.train.Saver`)
    will be initialized, allowing the serialization of the entire session to disk
    in order to restore it later and use it for playing the real game, not just learning
    how to play it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化还将提供定义 TensorBoard 日志存放位置的命令（默认情况下为`experiment`目录），用于学习如何估算下一个即时奖励的模型，以及另一个用于存储最终奖励权重的模型。此外，还将初始化一个
    saver（`tf.train.Saver`），允许将整个会话序列化到磁盘，以便稍后恢复并用于实际游戏的播放，而不仅仅是学习如何玩游戏。
- en: 'The two mentioned models are initialized in the same session, using different
    scope names (one will be `q`, the next reward model monitored by the TensorBoard;
    the other one will be `target_q`). Using two different scope names will allow
    easy handling of the neuron''s coefficients, making it possible to swap them with
    another method present in the class:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个提到的模型在同一会话中初始化，使用不同的作用域名称（一个是`q`，用于监控 TensorBoard 的下一个奖励模型；另一个是`target_q`）。使用两个不同的作用域名称将方便神经元系数的处理，使得可以通过类中其他方法交换它们：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `epsilon` dealing with the share of time devoted exploring new solutions
    compared to exploiting the knowledge of the network is constantly updated with
    the `epsilon_update` method, which simply modifies the actual `epsilon` by multiplying
    it by `epsilon_decay` unless it has already reached its allowed minimum value:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`epsilon`涉及用于探索新解决方案的时间比例与利用网络知识的时间比例，并通过`epsilon_update`方法不断更新，该方法通过将当前`epsilon`乘以`epsilon_decay`来修改`epsilon`，除非它已经达到了允许的最小值：'
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `save_weights` and `load_weights` methods simply allow the session to be saved:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`save_weights`和`load_weights`方法仅允许会话保存：'
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `set_weights` and  `target_model_update` methods work together to update
    the target Q network with the weights of the Q network (`set_weights` is a general-purpose,
    reusable function you can use in your solutions, too). Since we named the two
    scopes differently, it is easy to enumerate the variables of each network from
    the list of trainable variables. Once enumerated, the variables are joined in
    an assignment to be executed by the running session:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_weights`和`target_model_update`方法协同工作，用于通过 Q 网络的权重更新目标 Q 网络（`set_weights`是一个通用的、可重用的函数，您也可以在自己的解决方案中使用）。由于我们给这两个作用域命名不同，所以很容易从可训练变量的列表中列出每个网络的变量。一旦列出，变量会被组合并通过运行中的会话执行赋值操作：'
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `act` method is the core of the policy implementation because it will decide,
    based on `epsilon`, whether to take a random move or go for the best possible
    one. If it is going for the best possible move, it will ask the trained Q network
    to provide a reward estimate for each of the possible next moves (represented
    in a binary way by pushing one of four buttons in the Lunar Lander game) and it
    will return the move characterized by the maximum predicted reward (a greedy approach
    to the solution):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`act`方法是策略实现的核心，因为它会根据`epsilon`的值决定是采取随机动作还是选择最佳可能的动作。如果选择最佳动作，它会请求训练好的 Q 网络为每个可能的下一个动作（在
    Lunar Lander 游戏中通过按下四个按钮之一以二进制方式表示）提供奖励估计，并返回具有最大预测奖励的动作（这是一种贪婪方法）：'
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `replay` method completes the class. It is a crucial method because it makes
    learning for the DQN algorithm possible. We are going, therefore, to discuss how
    it works thoroughly. The first thing that the `replay` method does is to sample
    a batch (we defined the batch size at initialization) from the memories of previous
    game episodes (such memories are just the variables containing values about status,
    action, reward, next status, and a flag variable noticing if the observation is
    a final status or not). The random sampling allows the model to find the best
    coefficients in order to learn the *Q* function by a slow adjustment of the network's
    weights, batch after batch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`replay`方法完成了该类。它是一个关键方法，因为它使得 DQN 算法的学习成为可能。因此，我们将详细讨论它的工作原理。`replay`方法做的第一件事是从之前游戏回合的记忆中采样一个批次（批次大小在初始化时定义）（这些记忆变量包含状态、动作、奖励、下一个状态以及一个标志变量，表示观察是否为最终状态）。随机采样使得模型能够通过批量调整网络权重，一步步地学习
    *Q* 函数。'
- en: Then the method finds out whether the sampling recalled statuses are final or
    not. Non-final rewards need to be updated in order to represent the reward that
    you get at the end of the game. This is done by using the target network, which
    represents a snapshot of the *Q* function network as fixed at the end of the previous
    learning. The target network is fed with the following status, and the resulting
    reward is summed, after being discounted by a gamma factor, with the present reward.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，该方法会检查采样回调的状态是否为终止状态。非终止奖励需要更新，以表示游戏结束时获得的奖励。这是通过使用目标网络来完成的，目标网络表示在上次学习结束时固定的*Q*函数网络的快照。目标网络输入以下状态，结果奖励在经过折扣因子gamma调整后，与当前奖励相加。
- en: Using the present *Q* function may lead to instabilities in the learning process
    and it may not result in a satisfying *Q* function network.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前的*Q*函数可能会导致学习过程中的不稳定，并且可能无法得到令人满意的*Q*函数网络。
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When the rewards of non-terminal states have been updated, the batch data is
    fed into the neural network for training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当非终止状态的奖励更新完成后，批量数据被输入到神经网络中进行训练。
- en: Specifying the environment
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定环境
- en: 'The last class to be implemented is the `Environment` class. Actually, the
    environment is provided by the `gym` command, though you need a good wrapper around
    it in order to have it work with the previous `agent` class. That''s exactly what
    this class does. At initialization, it starts the Lunar Lander game and sets key
    variables such as `nS`, `nA` (dimensions of state and action), `agent`, and the
    cumulative reward (useful for testing the solution by providing an average of
    the last 100 episodes):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个需要实现的类是`Environment`类。实际上，环境是由`gym`命令提供的，尽管你需要为它构建一个良好的包装器，以便它能与之前的`agent`类一起工作。这正是这个类所做的事情。在初始化时，它启动了月球着陆器游戏，并设置了关键变量，如`nS`、`nA`（状态和动作的维度）、`agent`以及累计奖励（用于通过提供过去100集的平均值来测试解决方案）：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, we prepare the code for methods for `test`, `train`, and `incremental`
    (incremental training), which are defined as wrappers of the comprehensive `learning`
    method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为`test`、`train`和`incremental`（增量训练）方法准备代码，这些方法被定义为综合`learning`方法的包装器。
- en: 'Using incremental training is a bit tricky and it requires some attention if
    you do not want to spoil the results you have obtained with your training so far.
    The trouble is that when we restart the brain has pre-trained coefficients but
    memory is actually empty (we can call this as a cold restart). Being the memory
    of the agent empty, it cannot support good learning because of too few and limited
    examples. Consequently, the quality of the examples being fed is really not perfect
    for learning (the examples are mostly correlated with each other and very specific
    to the few newly experienced episodes). The risk of ruining the training can be
    mitigated using a very low `epsilon` (we suggest set at the minimum, `0.01` ):
    in this way, the network  will most of the time simply re-learn its own weights
    because it will suggest for each state the actions it already knows, and its performance
    shouldn''t worsen but oscillate in a stable way until there are enough examples
    in memory and it will start improving again.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用增量训练有点棘手，如果你不想破坏到目前为止通过训练获得的结果，它需要一些关注。问题在于，当我们重新启动时，大脑有预训练的系数，但记忆实际上是空的（我们可以称之为冷启动）。由于代理的记忆为空，它无法支持良好的学习，因为示例太少且有限。因此，传入的示例质量实际上并不完美，学习的效果不好（这些示例大多是相互关联的，并且非常特定于那些新体验过的少数几集）。可以通过使用非常低的`epsilon`来降低破坏训练的风险（我们建议设置为最低值`0.01`）：这样，网络大部分时间将仅仅重新学习自己的权重，因为它会为每个状态建议它已经知道的动作，并且它的表现不会恶化，而是以稳定的方式振荡，直到记忆中有足够的示例，网络才会开始再次改进。
- en: 'Here is the code for issuing the correct methods for training and testing:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是发出正确训练和测试方法的代码：
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The final method is `learn`, arranging all the steps for the agent to interact
    with and learn from the environment. The method takes the `epsilon` value (thus
    overriding any previous `epsilon` value the agent had), the number of episodes
    to run in the environment, whether it is being trained or not (a Boolean flag),
    and whether the training is continuing from the training of a previous model (another
    Boolean flag).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最终方法是`learn`，它安排了代理与环境交互并从中学习的所有步骤。该方法接受`epsilon`值（因此会覆盖代理之前的`epsilon`值）、在环境中运行的集数、是否进行训练（布尔标志）以及训练是否从之前模型的训练继续（另一个布尔标志）。
- en: 'In the first block of code, the method loads the previously trained weights
    of the network for Q value approximation if we want:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一块代码中，方法加载了网络之前训练的权重，以进行Q值近似（如果我们需要的话）。
- en: to test the network and see how it works;
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试网络并查看其如何工作；
- en: to carry on some previous training using further examples.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更多的示例继续进行之前的训练。
- en: Then the method delves into a nested iteration. The outside iteration is running
    through the required number of episodes (each episode a Lunar Lander game has
    taken to its conclusion). Whereas the inner iteration is instead running through
    a maximum of 1,000 steps making up an episode.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，方法深入到一个嵌套迭代中。外部迭代运行所需的回合数（每个回合一个Lunar Lander游戏的结束）。而内部迭代则是运行一个最多1,000步构成的回合。
- en: At each time step in the iteration, the neural network is interrogated on the
    next move. If it is under test, it will always simply provide the answer about
    the next best move. If it is under training, there is some chance, depending on
    the value of `epsilon`, that it won't suggest the best move but it will instead
    propose making a random move.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步的迭代中，神经网络会被询问下一步的动作。如果处于测试模式，它将始终简单地提供关于下一个最佳动作的答案。如果处于训练模式，根据`epsilon`的值，可能有一定的概率它不会推荐最佳动作，而是建议进行随机动作。
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After the move, all the information is gathered (initial state, chosen action,
    obtained reward, and consequent state) and saved into memory. At this time frame,
    if the memory is large enough to create a batch for the neural network approximating
    the *Q* function, then a training session is run. When all the time frames of
    the episode have been consumed, the weights of the DQN get stored into another
    network to be used as a stable reference as the DQN network is learning from a
    new episode.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 移动后，所有信息（初始状态、选择的动作、获得的奖励和后续状态）都会被收集并保存到内存中。在这个时间框架内，如果内存足够大以创建一个用于神经网络近似*Q*函数的批次，那么将执行训练过程。当本次回合的所有时间框架都已经消耗完毕，DQN的权重会被存储到另一个网络中，作为稳定的参考，以便DQN网络在学习新一轮回合时使用。
- en: Running the reinforcement learning process
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行强化学习过程
- en: 'Finally, after all the digression on reinforcement learning and DQN and writing
    down the complete code for the project, you can run it using a script or a Jupyter
    Notebook, leveraging the `Environment` class that puts all the code functionalities
    together:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在完成关于强化学习和DQN的所有讨论，并编写完整的项目代码后，你可以通过脚本或Jupyter Notebook运行它，利用将所有代码功能结合在一起的`Environment`类：
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After instantiating it, you just have to run the `train`, starting from `epsilon=1.0`
    and setting the goal to `5000` episodes (which corresponds to about 2.2 million
    examples of chained variables of state, action and reward). The actual code we
    provided is set to successfully accomplish a fully trained DQN model, though it
    may take some time, given your GPU''s availability and its computing capabilities:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化之后，你只需运行`train`，从`epsilon=1.0`开始，并将目标设定为`5000`回合（相当于约220万个状态、动作和奖励的链变量示例）。我们提供的实际代码已经设定为成功完成一个完全训练的DQN模型，尽管这可能需要一些时间，具体取决于你的GPU的可用性和计算能力：
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the end, the class will complete the required training, leaving a saved
    model on disk (which could be run or even reprised anytime). You can even inspect
    the TensorBoard using a simple command that can be run from a shell:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，类将完成所需的训练，并将保存的模型保存在磁盘上（可以随时运行或重新启动）。你甚至可以使用一个简单的命令来检查TensorBoard，该命令可以从shell运行：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The plots will appear on your browser, and they will be available for inspection
    at the local address `localhost:6006`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图表将出现在浏览器中，可以在本地地址`localhost:6006`进行查看：
- en: '![](img/3007e975-dc2b-4ce9-98b9-b9607a870578.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3007e975-dc2b-4ce9-98b9-b9607a870578.png)'
- en: Figure 4: The loss trend along the training, the peaks represent break-thoughts
    in learning such as at 800k examples
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：训练中的损失趋势，峰值代表学习中的瓶颈，例如在80万示例时。
- en: when it started landing safely on the ground.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当它成功安全着陆时。
- en: 'The loss plot will reveal that, contrary to other projects, the optimization
    is still characterized by a decreasing loss, but with many spikes and problems
    along the way:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 损失图表将显示，尽管与其他项目不同，优化过程仍然表现为损失逐渐减少，但在过程中会出现许多峰值和问题：
- en: The plots represented here are the result of running the project once. Since
    there is a random component in the process, you may obtain slightly different
    plots when running the project on your own computer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里表示的图表是运行项目一次的结果。由于过程中的随机成分，你在自己计算机上运行项目时可能会得到略有不同的图表。
- en: '![](img/29069c1b-c954-4c9b-8c2c-baae261a3d4a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29069c1b-c954-4c9b-8c2c-baae261a3d4a.png)'
- en: 'Figure 5: The trend of maximum q values obtained in a batch session of learning'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在批量学习会话中获得的最大q值趋势
- en: 'The same story is told by the maximum predicted *q* value and the average predicted
    *q* value.  The network improves at the end, though it can slightly retrace its
    steps and linger on plateaus for a long time:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最大预测的*q*值和平均预测的*q*值讲述了同样的故事。网络在最后有所改善，尽管它可能会略微回溯并在平台上停留较长时间：
- en: '![](img/4a361ba7-4c97-4b4e-b03f-c64b3a51345a.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a361ba7-4c97-4b4e-b03f-c64b3a51345a.png)'
- en: 'Figure 6: The trend of average q values obtained in a batch session of learning'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在批量学习会话中获得的平均q值趋势
- en: 'Only if you take the average of the last 100 final rewards do you see an incremental
    path, hinting at a persistent and steady improvement of the DQN network:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在你计算最后100个最终奖励的平均值时，才能看到一个渐进的路径，暗示DQN网络持续稳定的改进：
- en: '![](img/6ce2623c-3338-47c5-8059-45e78cc942fa.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ce2623c-3338-47c5-8059-45e78cc942fa.png)'
- en: 'Figure 7: The trend of actually obtained scores at the end of each learning
    episode, it more clearly depicts the growing capabilities of the DQN'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：每次学习结束时实际获得的分数趋势，更清晰地描绘了DQN能力的增长
- en: Using the same information, from the output, not from the TensorBoard, you'll
    also figure out that the number of actions changes on average depending on the
    `epsilon` value. At the beginning, the number of actions required to finish an
    episode was under 200\. Suddenly, when `epsilon` is `0.5`, the average number
    of actions tends to grow steadily and reach a peak at about 750 (the landing pod
    has learned to counteract gravity by using its rockets).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的信息，从输出中，而不是从TensorBoard中，你也会发现，所需的动作数量平均取决于`epsilon`值。一开始，完成一集所需的动作数不到200。突然，当`epsilon`为`0.5`时，所需的平均动作数稳定增长，并在约750时达到峰值（着陆舱学会了通过使用火箭来对抗重力）。
- en: 'In the end, the network discovers this is a sub-optimal strategy and when `epsilon`
    turns below `0.3`, the average number of actions for completing an episode drops
    as well. The DQN in this phase is discovering how to successfully land the pod
    in a more efficient way:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，网络发现这是一种次优策略，当`epsilon`下降到`0.3`以下时，完成一集所需的平均动作数也有所下降。在这个阶段，DQN正在发现如何以更高效的方式成功地着陆舱：
- en: '![](img/9948d955-1382-4c2c-a97d-472505d1ca43.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9948d955-1382-4c2c-a97d-472505d1ca43.png)'
- en: 'Figure 8: The relationship between the epsilon (the exploration/exploitation
    rate) and the efficiency of the DQN network,'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：epsilon（探索/利用率）与DQN网络效率之间的关系
- en: expressed as a number of moves used to complete an episode
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以完成一集所使用的移动次数表示
- en: 'If for any reason, you believe that the network needs more examples and learning,
    you can reprise the learning using the incremental `method`, keeping in mind that
    `epsilon` should be very low in this case:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某种原因，你认为网络需要更多的示例和学习，你可以通过增量`method`重新开始学习，记住在这种情况下`epsilon`应该非常低：
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After the training, if you need to see the results and know, on average every
    100 episodes, how much the DQN can score (the ideal target is a `score >=200`), 
    you can just run the following command:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结束后，如果你需要查看结果并了解DQN在每100集中的平均得分（理想目标是`score >=200`），你只需运行以下命令：
- en: '[PRE25]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Acknowledgements
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: At the conclusion of this project, we would like to indeed thank Peter Skvarenina,
    whose project "Lunar Lander II" ([https://www.youtube.com/watch?v=yiAmrZuBaYU](https://www.youtube.com/watch?v=yiAmrZuBaYU))
    has been the key inspiration for our own project, and for all his suggestions
    and hints during the making of our own version of the Deep Q-Network.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目的结尾，我们确实要感谢Peter Skvarenina，他的项目“Lunar Lander II”([https://www.youtube.com/watch?v=yiAmrZuBaYU](https://www.youtube.com/watch?v=yiAmrZuBaYU))是我们自己项目的主要灵感来源，并感谢他在制作我们自己版本的深度Q网络时提供的所有建议和提示。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this project, we have explored what a reinforcement algorithm can manage
    to achieve in an OpenAI environment, and we have programmed a TensorFlow graph
    capable of learning how to estimate a final reward in an environment characterized
    by an agent, states, actions, and consequent rewards. This approach, called DQN,
    aims to approximate the result from a Bellman equation using a neural network
    approach. The result is a Lunar Lander game that the software can play successfully
    at the end of training by reading the game status and deciding on the right actions
    to be taken at any time.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们探讨了强化学习算法在 OpenAI 环境中能够实现的目标，并且我们编写了一个 TensorFlow 图，能够学习如何估算一个由代理、状态、动作和相应奖励所构成的环境中的最终奖励。这个方法叫做
    DQN，旨在通过神经网络方法来近似贝尔曼方程的结果。最终的结果是一个“月球着陆者”游戏，软件在训练结束后能够成功地通过读取游戏状态并随时决定采取正确的行动来玩这个游戏。
