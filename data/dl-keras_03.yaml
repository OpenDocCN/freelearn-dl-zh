- en: Deep Learning with ConvNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络的深度学习
- en: 'In previous chapters, we discussed dense nets, in which each layer is fully
    connected to the adjacent layers. We applied those dense networks to classify
    the MNIST handwritten characters dataset. In that context, each pixel in the input
    image is assigned to a neuron for a total of 784 (28 x 28 pixels) input neurons.
    However, this strategy does not leverage the spatial structure and relations of
    each image. In particular, this piece of code transforms the bitmap representing
    each written digit into a flat vector, where the spatial locality is gone:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了密集网络，其中每一层都与相邻的层完全连接。我们将这些密集网络应用于分类MNIST手写字符数据集。在那种情况下，输入图像中的每个像素都被分配给一个神经元，总共有784个（28
    x 28像素）输入神经元。然而，这种策略并没有利用每个图像的空间结构和关系。特别地，这段代码将每个书写数字的位图转换为一个平坦的向量，在这个过程中，空间局部性丧失：
- en: '[PRE0]'
  id: totrans-2
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Convolutional neural networks (also called ConvNet) leverage spatial information
    and are therefore very well suited for classifying images. These nets use an ad
    hoc architecture inspired by biological data taken from physiological experiments
    done on the visual cortex. As discussed, our vision is based on multiple cortex
    levels, each one recognizing more and more structured information. First, we see
    single pixels; then from them, we recognize simple geometric forms. And then...
    more and more sophisticated elements such as objects, faces, human bodies, animals,
    and so on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（也称为ConvNet）利用空间信息，因此非常适合分类图像。这些网络使用一种受生物学数据启发的特殊架构，这些数据来自对视觉皮层进行的生理实验。正如我们所讨论的，我们的视觉是基于多个皮层层级的，每一层识别越来越结构化的信息。首先，我们看到单个像素；然后从这些像素中，我们识别出简单的几何形状。接着...越来越复杂的元素，如物体、面孔、人类身体、动物等等。
- en: Convolutional neural networks are indeed fascinating. Over a short period of
    time, they become a *disruptive* technology, breaking all the state-of-the-art
    results in multiple domains, from text, to video, to speech going well beyond
    the initial image processing domain where they were originally conceived.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络确实令人着迷。在短短的时间内，它们成为了一种*颠覆性*技术，突破了多个领域的所有最先进成果，从文本到视频，再到语音，远远超出了最初的图像处理领域。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Deep convolutional neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积神经网络
- en: Image classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类
- en: Deep convolutional neural network — DCNN
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积神经网络 — DCNN
- en: 'A **deep convolutional neural network** (**DCNN**) consists of many neural
    network layers. Two different types of layers, convolutional and pooling, are
    typically alternated. The depth of each filter increases from left to right in
    the network. The last stage is typically made of one or more fully connected layers:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度卷积神经网络**（**DCNN**）由许多神经网络层组成。通常交替使用两种不同类型的层，卷积层和池化层。每个滤波器的深度从网络的左到右逐渐增加。最后阶段通常由一个或多个全连接层组成：'
- en: '![](img/B06258_04_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_01.png)'
- en: 'There are three key intuitions beyond ConvNets:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络之外有三个关键直觉：
- en: Local receptive fields
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部感受野
- en: Shared weights
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享权重
- en: Pooling
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化
- en: Let's review them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下它们。
- en: Local receptive fields
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部感受野
- en: If we want to preserve spatial information, then it is convenient to represent
    each image with a matrix of pixels. Then, a simple way to encode the local structure
    is to connect a submatrix of adjacent input neurons into one single hidden neuron
    belonging to the next layer. That single hidden neuron represents one local receptive
    field. Note that this operation is named convolution and it gives the name to
    this type of network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望保持空间信息，那么用像素矩阵表示每张图像是很方便的。然后，一种简单的编码局部结构的方法是将相邻输入神经元的子矩阵连接到下一个层中的一个隐藏神经元。这一个隐藏神经元代表一个局部感受野。需要注意的是，这个操作被称为卷积，它也给这种类型的网络命名。
- en: Of course, we can encode more information by having overlapping submatrices.
    For instance, let's suppose that the size of each single submatrix is 5 x 5 and
    that those submatrices are used with MNIST images of 28 x 28 pixels. Then we will
    be able to generate 23 x 23 local receptive field neurons in the next hidden layer.
    In fact it is possible to slide the submatrices by only 23 positions before touching
    the borders of the images. In Keras, the size of each single submatrix is called
    **stride length**, and this is a hyperparameter that can be fine-tuned during
    the construction of our nets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以通过使用重叠的子矩阵来编码更多的信息。例如，假设每个子矩阵的大小为 5 x 5，并且这些子矩阵与 28 x 28 像素的 MNIST 图像一起使用。这样，我们将能够在下一个隐藏层中生成
    23 x 23 的局部感受野神经元。实际上，只有在滑动子矩阵 23 个位置后，才会触及图像的边界。在 Keras 中，每个子矩阵的大小称为**步幅长度**，这是一个可以在构建网络时进行微调的超参数。
- en: Let's define the feature map from one layer to another layer. Of course, we
    can have multiple feature maps that learn independently from each hidden layer.
    For instance, we can start with 28 x 28 input neurons for processing MINST images
    and then recall *k* feature maps of size 23 x 23 neurons each (again with a stride
    of 5 x 5) in the next hidden layer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们定义从一层到另一层的特征图。当然，我们可以有多个特征图，它们各自独立地从每个隐藏层中学习。例如，我们可以从 28 x 28 的输入神经元开始处理
    MNIST 图像，然后在下一个隐藏层中回调 *k* 个特征图，每个特征图的大小为 23 x 23 个神经元（同样步幅为 5 x 5）。
- en: Shared weights and bias
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享权重和偏置
- en: Let's suppose that we want to move away from the pixel representation in a row
    by gaining the ability to detect the same feature independently from the location
    where it is placed in the input image. A simple intuition is to use the same set
    of weights and bias for all the neurons in the hidden layers. In this way, each
    layer will learn a set of position-independent latent features derived from the
    image.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想摆脱行像素表示，转而通过获得在输入图像中放置位置无关的相同特征的能力。一个简单的直觉是对隐藏层中的所有神经元使用相同的一组权重和偏置。这样，每一层都会学习从图像中提取的、位置无关的潜在特征。
- en: Assuming that the input image has shape *(256, 256)* on three channels with
    *tf* (TensorFlow) ordering, this is represented as *(256, 256, 3)*. Note that
    with th (Theano) mode, the channel's dimension (the depth) is at index *1*; in
    *tf* (TensoFlow) mode, it is at index *3*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入图像的形状是 *(256, 256)*，并且具有三个通道，在 *tf*（TensorFlow）顺序下表示为 *(256, 256, 3)*。请注意，在
    th（Theano）模式下，通道维度（深度）位于索引 *1* 位置；而在 *tf*（TensorFlow）模式下，位于索引 *3* 位置。
- en: 'In Keras, if we want to add a convolutional layer with dimensionality of the
    output 32 and extension of each filter 3 x 3, we will write:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，如果我们想添加一个卷积层，输出维度为 32，每个滤波器的扩展为 3 x 3，我们将写道：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Alternatively, we will write:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们将写道：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This means that we are applying a 3 x 3 convolution on a 256 x 256 image with
    three input channels (or input filters), resulting in 32 output channels (or output
    filters).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在一个 256 x 256 的图像上应用 3 x 3 的卷积，图像有三个输入通道（或输入滤波器），并且结果是 32 个输出通道（或输出滤波器）。
- en: 'An example of convolution is provided in the following diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是卷积示例：
- en: '![](img/B06258_04_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_02.png)'
- en: Pooling layers
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: Let's suppose that we want to summarize the output of a feature map. Again,
    we can use the spatial contiguity of the output produced from a single feature
    map and aggregate the values of a submatrix into a single output value that synthetically
    describes the *meaning* associated with that physical region.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想总结一个特征图的输出。同样，我们可以利用来自单个特征图的输出的空间连续性，将子矩阵的值聚合为一个单一的输出值，这个值合成地描述了与该物理区域相关的*意义*。
- en: Max-pooling
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化
- en: 'One easy and common choice is *max-pooling*, which simply outputs the maximum
    activation as observed in the region. In Keras, if we want to define a max-pooling
    layer of size 2 x 2, we will write:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单而常见的选择是*最大池化*，它仅输出在该区域内观察到的最大激活值。在 Keras 中，如果我们想定义一个 2 x 2 大小的最大池化层，我们将写道：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'An example of max-pooling is shown in the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最大池化示例：
- en: '![](img/B06258_04_03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_03.png)'
- en: Average pooling
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均池化
- en: Another choice is average pooling, which simply aggregates a region into the
    average values of the activations observed in that region.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是平均池化，它仅将一个区域的激活值聚合为该区域观察到的激活值的平均值。
- en: 'Note that Keras implements a large number of pooling layers and a complete
    list is available at: [https://keras.io/layers/pooling/](https://keras.io/layers/pooling/).
    In short, all pooling operations are nothing more than a summary operation on
    a given region.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Keras 实现了大量的池化层，完整的池化层列表可以在以下链接找到：[https://keras.io/layers/pooling/](https://keras.io/layers/pooling/)。简而言之，所有的池化操作无非是在给定区域上的汇总操作。
- en: ConvNets summary
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ConvNets 总结
- en: So far, we have described the basic concepts of ConvNets. CNNs apply convolution
    and pooling operations in one dimension for audio and text data along the time
    dimension, in two dimensions for images along the (height x width) dimensions,
    and in three dimensions for videos along the (height x width x time) dimensions.
    For images, sliding the filter over input volume produces a map that gives the
    responses of the filter for each spatial position. In other words, a ConvNet has
    multiple filters stacked together which learn to recognize specific visual features
    independently of the location in the image. Those visual features are simple in
    the initial layers of the network, and then more and more sophisticated deeper
    in the network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了 ConvNets 的基本概念。卷积神经网络（CNNs）在一维（时间维度）上对音频和文本数据应用卷积和池化操作，在二维（高 ×
    宽）上对图像进行卷积和池化操作，在三维（高 × 宽 × 时间）上对视频进行卷积和池化操作。对于图像，将滤波器滑动在输入体积上，产生一个映射，显示每个空间位置的滤波器响应。换句话说，ConvNet
    有多个滤波器堆叠在一起，它们学习识别特定的视觉特征，而不受图像中位置的影响。网络初期的视觉特征较为简单，随着网络层数加深，特征变得越来越复杂。
- en: An example of DCNN — LeNet
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DCNN 示例 — LeNet
- en: 'Yann le Cun proposed (for more information refer to: *Convolutional Networks
    for Images, Speech, and Time-Series*, by Y. LeCun and Y. Bengio, brain theory
    neural networks, vol. 3361, 1995) a family of ConvNets named LeNet trained for
    recognizing MNIST handwritten characters with robustness to simple geometric transformations
    and to distortion. The key intuition here is to have low-layers alternating convolution
    operations with max-pooling operations. The convolution operations are based on
    carefully chosen local receptive fields with shared weights for multiple feature
    maps. Then, higher levels are fully connected layers based on a traditional MLP
    with hidden layers and softmax as the output layer.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Yann le Cun 提出了一个名为 LeNet 的卷积神经网络（更多信息参见：*Convolutional Networks for Images,
    Speech, and Time-Series*，作者 Y. LeCun 和 Y. Bengio，脑理论神经网络，卷3361，1995年），该网络用于识别
    MNIST 手写字符，具有对简单几何变换和失真的鲁棒性。这里的关键直觉是，低层交替使用卷积操作和最大池化操作。卷积操作基于精心选择的局部感受野，并为多个特征图共享权重。然后，更高层是基于传统多层感知器（MLP）结构的全连接层，其中包含隐藏层，输出层使用
    softmax 激活函数。
- en: LeNet code in Keras
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 中的 LeNet 代码
- en: 'To define LeNet code, we use a convolutional 2D module, which is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义 LeNet 代码，我们使用一个卷积 2D 模块，具体如下：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `filters` is the number of convolution kernels to use (for example, the
    dimensionality of the output), `kernel_size` is an integer or tuple/list of two
    integers, specifying the width and height of the 2D convolution window (can be
    a single integer to specify the same value for all spatial dimensions), and `padding='same'`
    means that padding is used. There are two options: `padding='valid'` means that
    the convolution is only computed where the input and the filter fully overlap,
    and therefore the output is smaller than the input, while `padding='same'` means
    that we have an output that is the *same* size as the input, for which the area
    around the input is padded with zeros.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`filters` 表示使用的卷积核数量（例如，输出的维度），`kernel_size` 是一个整数或一个包含两个整数的元组/列表，指定 2D
    卷积窗口的宽度和高度（可以是一个单独的整数，以指定所有空间维度的相同值），而 `padding='same'` 表示使用了填充。这里有两个选项：`padding='valid'`
    表示卷积仅在输入和滤波器完全重叠的地方计算，因此输出小于输入；而 `padding='same'` 表示输出的尺寸与输入*相同*，为此，输入周围的区域会用零进行填充。
- en: 'In addition, we use a `MaxPooling2D` module:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用 `MaxPooling2D` 模块：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `pool_size=(2, 2)` is a tuple of two integers representing the factors
    by which the image is vertically and horizontally downscaled. So *(2, 2)* will
    halve the image in each dimension, and `strides=(2, 2)` is the stride used for
    processing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`pool_size=(2, 2)` 是一个包含两个整数的元组，表示图像在垂直和水平方向上缩小的比例。因此，*(2, 2)* 将在每个维度上将图像减半，而
    `strides=(2, 2)` 是处理时使用的步长。
- en: 'Now, let us review the code. First we import a number of modules:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下代码。首先我们导入一些模块：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then we define the LeNet network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义 LeNet 网络：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We have a first convolutional stage with ReLU activations followed by a max-pooling.
    Our net will learn 20 convolutional filters, each one of which has a size of 5
    x 5\. The output dimension is the same one of the input shape, so it will be 28
    x 28\. Note that since the `Convolution2D` is the first stage of our pipeline,
    we are also required to define its `input_shape`. The max-pooling operation implements
    a sliding window that slides over the layer and takes the maximum of each region
    with a step of two pixels vertically and horizontally:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含ReLU激活的第一卷积阶段，然后是最大池化。我们的网络将学习20个卷积滤波器，每个大小为5 x 5。输出维度与输入形状相同，因此为28 x
    28。请注意，由于`Convolution2D`是我们管道的第一个阶段，我们还需要定义它的`input_shape`。最大池化操作实现一个滑动窗口，在层上滑动，并在每个区域垂直和水平方向上以两个像素的步长取最大值：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then a second convolutional stage with ReLU activations follows, again by a
    max-pooling. In this case, we increase the number of convolutional filters learned
    to 50 from the previous 20\. Increasing the number of filters in deeper layers
    is a common technique used in deep learning:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是第二个带ReLU激活的卷积阶段，再次进行最大池化。在这种情况下，我们将从之前的20增加到50个学习的卷积滤波器。在深层增加滤波器的数量是深度学习中常用的技术：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we have a pretty standard flattening and a dense network of 500 neurons,
    followed by a softmax classifier with 10 classes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有一个相当标准的扁平化和500个神经元的密集网络，后跟一个有10类的softmax分类器：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Congratulations, You have just defined the first deep learning network! Let''s
    see how it looks visually:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚定义了第一个深度学习网络！让我们看看它的视觉效果：
- en: '![](img/B06258_04_04.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_04.png)'
- en: 'Now we need some additional code for training the network, but this is very
    similar to what we have already described in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Network Foundations*. This time, we also show the code for printing the
    loss:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一些额外的代码来训练网络，但这与我们已在[第1章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)中描述的内容非常相似，*神经网络基础*。这次，我们还展示了打印损失的代码：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let''s run the code. As you can see, the time had a significant increase
    and each iteration in our deep net now takes ~134 seconds against ~1-2 seconds
    for the net defined in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Network Foundations*. However, the accuracy has reached a new peak at
    99.06%:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行代码。正如您所看到的，时间显著增加，我们深度网络中的每次迭代现在需要约134秒，而在[第1章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)中定义的网络，*神经网络基础*，每次迭代只需约1-2秒。然而，准确率已经达到了新的峰值99.06%：
- en: '![](img/B06258_04_05.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_05.png)'
- en: 'Let''s plot the model accuracy and the model loss, and we understand that we
    can train in only 4 - 5 iterations to achieve a similar accuracy of 99.2%:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型的准确率和损失，我们可以理解，仅需4 - 5次迭代即可达到类似99.2%的准确率：
- en: '| ![](img/B06258_04_06.png) | ![](img/B06258_04_07.png) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_04_06.png) | ![](img/B06258_04_07.png) |'
- en: 'In the following screenshot, we show the final accuracy achieved by our model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们展示了我们模型达到的最终准确率：
- en: '![](img/B06258_04_08.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_08.png)'
- en: 'Let''s see some of the MNIST images just to understand how good the number
    99.2% is! For instance, there are many ways in which humans write a 9, one of
    them appearing in the following diagram. The same holds for 3, 7, 4, and 5\. The
    number **1** in this diagram is so difficult to recognize that probably even a
    human will have issues with it:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些MNIST图像，以了解99.2%的准确率有多好！例如，人们写数字9的方式有很多种，在以下图表中显示了其中一种。对于数字3、7、4和5也是如此。在这张图中，数字**1**是如此难以识别，甚至可能连人类也会有问题：
- en: '![](img/B06258_04_09.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_09.png)'
- en: 'We can summarize all the progress made so far with our different models in
    the following graph. Our simple net started with an accuracy of 92.22%, which
    means that about 8 handwritten characters out of 100 are not correctly recognized.
    Then, we gained 7% with the deep learning architecture by reaching an accuracy
    of 99.20%, which means that about 1 handwritten character out of 100 is incorrectly
    recognized:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过不同的模型总结了所有的进展，如下图所示。我们的简单网络从92.22%的准确率开始，这意味着大约100个手写字符中有8个识别错误。然后，通过深度学习架构获得了7%，达到了99.20%的准确率，这意味着大约100个手写字符中有1个识别错误：
- en: '![](img/B06258_04_10.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_10.png)'
- en: Understanding the power of deep learning
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习的力量
- en: 'Another test that we can run to better understand the power of deep learning
    and ConvNet is to reduce the size of the training set and observe the consequent
    decay in performance. One way to do this is to split the training set of 50,000
    examples into two different sets:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行另一个测试，以更好地理解深度学习和卷积神经网络（ConvNet）的强大能力，那就是减少训练集的大小，并观察性能随之下降的情况。一种方法是将50,000个样本的训练集拆分成两个不同的子集：
- en: The proper training set used for training our model will progressively reduce
    its size of (5,900, 3,000, 1,800, 600, and 300) examples
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练我们模型的合适训练集将逐步减少其大小（5,900、3,000、1,800、600和300个样本）
- en: The validation set used to estimate how well our model has been trained will
    consist of the remaining examples
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于估算模型训练效果的验证集将由剩余的样本组成
- en: Our test set is always fixed and it consists of 10,000 examples.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试集始终固定，包含10,000个样本。
- en: 'With this setup, we compare the just-defined deep learning ConvNet against
    the first example of neural network defined in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Network Foundations*. As we can see in the following graph, our deep network
    always outperforms the simple network and the gap is more and more evident when
    the number of examples provided for training is progressively reduced. With 5,900
    training examples the deep learning net had an accuracy of 96.68% against an accuracy
    of 85.56% of the simple net. More important, with only 300 training examples our
    deep learning net still has an accuracy of 72.44% while the simple net shows a
    significant decay at 48.26%. All the experiments are run for only four training
    iterations. This confirms the breakthrough progress achieved with deep learning.
    At first glance this could be surprising from a mathematical point of view because
    the deep network has many more unknowns (the weights), so one would think we need
    many more data points. However, preserving the spatial information, adding convolution,
    pooling, and feature maps is innovation with ConvNets, and this was optimized
    on millions of years (since this organization has been inspired by the visual
    cortex):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置下，我们将刚定义的深度学习卷积神经网络与[第1章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)中定义的第一个神经网络示例进行比较，*神经网络基础*。正如我们在下面的图表中看到的那样，我们的深度网络始终优于简单网络，并且当提供的训练样本数量逐步减少时，差距越来越明显。使用5,900个训练样本时，深度学习网络的准确率为96.68%，而简单网络的准确率为85.56%。更重要的是，即使只有300个训练样本，我们的深度学习网络仍然能够达到72.44%的准确率，而简单网络的准确率则显著下降至48.26%。所有实验只进行了四次训练迭代。这证明了深度学习所带来的突破性进展。乍一看，这从数学角度来看可能会令人惊讶，因为深度网络有更多的未知数（权重），所以人们可能会认为需要更多的数据点。然而，保持空间信息，加入卷积、池化和特征图是卷积神经网络的创新，这一结构已经在数百万年的时间里得到了优化（因为它的灵感来源于视觉皮层）：
- en: '![](img/B06258_04_11.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_11.png)'
- en: 'A list of state-of-the-art results for MNIST is available at: [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html).
    As of January, 2017, the best result has an error rate of 0.21%.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MNIST的最新成果列表可访问：[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)。截至2017年1月，最佳结果的错误率为0.21%。
- en: Recognizing CIFAR-10 images with deep learning
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习识别CIFAR-10图像
- en: 'The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels
    divided into 10 classes. Each class contains 6,000 images. The training set contains
    50,000 images, while the test sets provides 10,000 images. This image taken from
    the CIFAR repository ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)) describes
    a few random examples from the 10 classes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据集包含60,000张32 x 32像素、3通道的彩色图像，分为10个类别。每个类别包含6,000张图像。训练集包含50,000张图像，而测试集提供10,000张图像。这张来自CIFAR数据集的图像（[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)）展示了10个类别中的一些随机示例：
- en: '![](img/B06258_04_12.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_12.png)'
- en: The goal is to recognize previously unseen images and assign them to one of
    the 10 classes. Let us define a suitable deep net.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是识别先前未见过的图像，并将它们分配到10个类别中的一个。让我们定义一个合适的深度网络。
- en: 'First of all we import a number of useful modules, define a few constants,
    and load the dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入一些有用的模块，定义几个常量，并加载数据集：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now let''s do a one-hot encoding and normalize the images:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进行独热编码并对图像进行归一化：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our net will learn 32 convolutional filters, each of which with a 3 x 3 size.
    The output dimension is the same one of the input shape, so it will be 32 x 32
    and activation is ReLU, which is a simple way of introducing non-linearity. After
    that we have a max-pooling operation with pool size 2 x 2 and a dropout at 25%:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络将学习32个卷积滤波器，每个滤波器的大小为3 x 3。输出维度与输入形状相同，因此为32 x 32，激活函数为ReLU，这是一种引入非线性的简单方式。之后我们会进行一个2
    x 2大小的最大池化操作，并使用25%的dropout：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next stage in the deep pipeline is a dense network with 512 units and ReLU
    activation followed by a dropout at 50% and by a softmax layer with 10 classes
    as output, one for each category:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 深度管道的下一阶段是一个包含512个单元的密集网络，激活函数为ReLU，随后是50%的dropout，最后是一个包含10个类别输出的softmax层，每个类别对应一个输出：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After defining the network, we can train the model. In this case, we split
    the data and compute a validation set in addition to the training and testing
    sets. The training is used to build our models, the validation is used to select
    the best performing approach, while the test set is to check the performance of
    our best models on fresh unseen data:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了网络后，我们可以训练模型。在这种情况下，我们将数据拆分，并计算出一个验证集，除了训练集和测试集。训练集用于构建我们的模型，验证集用于选择表现最好的方法，而测试集用于检验我们最好的模型在全新数据上的表现：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this case we save the architecture of our deep network:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们保存了我们深度网络的架构：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let us run the code. Our network reaches a test accuracy of 66.4% with 20 iterations.
    We also print the accuracy and loss plot, and dump the network with `model.summary()`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行代码。我们的网络经过20次迭代，测试准确率达到了66.4%。我们还打印了准确率和损失图，并使用`model.summary()`输出了网络结构：
- en: '![](img/B06258_04_13-1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_13-1.png)'
- en: 'In the following graph, we report the accuracy and the lost achieved by our
    net on both train and test datasets:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了网络在训练集和测试集上所达到的准确率和损失：
- en: '| ![](img/B06258_04_14.png) | ![](img/B06258_04_15.png) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_04_14.png) | ![](img/B06258_04_15.png) |'
- en: Improving the CIFAR-10 performance with deeper a network
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过更深的网络提高CIFAR-10的性能
- en: 'One way to improve the performance is to define a deeper network with multiple
    convolutional operations. In this example, we have a sequence of modules:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的一种方法是定义一个更深的网络，包含多个卷积操作。在这个示例中，我们有一个模块序列：
- en: '*conv+conv+maxpool+dropout+conv+conv+maxpool*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*conv+conv+maxpool+dropout+conv+conv+maxpool*'
- en: Followed by a standard *dense+dropout+dense*. All the activation functions are
    ReLU.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接着是一个标准的*dense+dropout+dense*结构。所有的激活函数均为ReLU。
- en: 'Let us see the code for the new network:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看新网络的代码：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Congratulations! You have defined a deeper network. Let us run the code! First
    we dump the network, then we run for 40 iterations reaching an accuracy of 76.9%:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经定义了一个更深的网络。让我们运行代码！首先我们输出网络结构，然后运行40次迭代，准确率达到了76.9%：
- en: '![](img/B06258_04_16.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_16.png)'
- en: 'In the following screenshot, we will see the accuracy reached after 40 iterations:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到经过40次迭代后所达到的准确率：
- en: '![](img/B06258_04_17-2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_17-2.png)'
- en: 'So we have an improvement of 10.5% with respect to the previous simpler deeper
    network. For the sake of completeness, let us also report the accuracy and loss
    during training, shown as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所以相较于之前的简单深层网络，我们提高了10.5%的性能。为了完整起见，我们还报告了训练过程中的准确率和损失，如下所示：
- en: '| ![](img/B06258_04_18.png) | ![](img/B06258_04_19-1.png) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_04_18.png) | ![](img/B06258_04_19-1.png) |'
- en: Improving the CIFAR-10 performance with data augmentation
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过数据增强提高CIFAR-10的性能
- en: 'Another way to improve the performance is to generate more images for our training.
    The key intuition is that we can take the standard CIFAR training set and augment
    this set with multiple types of transformations including rotation, rescaling,
    horizontal/vertical flip, zooming, channel shift, and many more. Let us see the
    code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的另一种方法是为我们的训练生成更多的图像。关键的直觉是，我们可以使用标准的CIFAR训练集，并通过多种类型的变换对其进行增强，包括旋转、重缩放、水平/垂直翻转、缩放、通道偏移等。让我们看看代码：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `rotation_range` is a value in degrees (`0` - `180`) for randomly rotating
    pictures. `width_shift` and `height_shift` are ranges for randomly translating
    pictures vertically or horizontally. `zoom_range` is for randomly zooming pictures. `horizontal_flip`
    is for randomly flipping half of the images horizontally. `fill_mode` is the strategy
    used for filling in new pixels that can appear after a rotation or a shift:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`rotation_range` 是一个表示随机旋转图片的度数范围（`0` - `180`）。`width_shift` 和 `height_shift`
    是表示随机平移图片的垂直或水平方向的范围。`zoom_range` 是用于随机缩放图片的范围。`horizontal_flip` 是用于随机水平翻转一半图片的选项。`fill_mode`
    是旋转或平移后，用于填充可能出现的新像素的策略：'
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After augmentation, we will have generated many more training images starting
    from the standard CIFAR-10 set:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据增强后，我们将从标准的 CIFAR-10 数据集生成更多的训练图片：
- en: '![](img/B06258_04_20.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_20.png)'
- en: 'Now we can apply this intuition directly for training. Using the same ConvNet
    defined previously we simply generate more augmented images and then we train.
    For efficiency, the generator runs in parallel to the model. This allows an image
    augmentation on the CPU and in parallel to training on the GPU. Here is the code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以直接将这一思路应用于训练。使用之前定义的同一个卷积神经网络（ConvNet），我们只需要生成更多的增强图像，然后进行训练。为了提高效率，生成器与模型并行运行。这使得图像增强在
    CPU 上进行，并且与 GPU 上的训练并行执行。以下是代码：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Each iteration is now more expensive because we have more training data. So
    let us run for 50 iterations only and see that we reach an accuracy of 78.3%:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在有更多的训练数据，每次迭代的开销更大。因此，我们只运行50次迭代，看看能否达到78.3%的准确率：
- en: '![](img/B06258_04_21.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_21.png)'
- en: 'The results obtained during our experiments are summarized in the following
    graph:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验中获得的结果总结在下面的图表中：
- en: '![](img/B06258_04_22.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_22.png)'
- en: 'A list of state-of-the-art results for CIFAR-10 is available at: [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html).
    As of January, 2017, the best result has an accuracy of 96.53%.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 CIFAR-10 的最先进结果列表可以在以下网址找到：[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)。截至2017年1月，最佳结果的准确率为96.53%。
- en: Predicting with CIFAR-10
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CIFAR-10 进行预测
- en: 'Now let us suppose that we want to use the deep learning model we just trained
    for CIFAR-10 for a bulk evaluation of images. Since we saved the model and the
    weights, we do not need to train every time:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想使用刚刚为 CIFAR-10 训练的深度学习模型进行大批量图像评估。由于我们已经保存了模型和权重，我们无需每次都重新训练：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now let us get the prediction for a ![](img/B06258_04_23-2.jpg) and for a ![](img/B06258_04_24-1.jpg).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为一张 ![](img/B06258_04_23-2.jpg) 和一张 ![](img/B06258_04_24-1.jpg) 获取预测结果。
- en: 'We get categories `3` (cat) and `5` (dog) as output, as expected:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了类别 `3`（猫）和 `5`（狗）作为输出，正如预期的那样：
- en: '![](img/B06258_04_25-2.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_25-2.png)'
- en: Very deep convolutional networks for large-scale image recognition
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于大规模图像识别的非常深的卷积神经网络
- en: 'In 2014, an interesting contribution for image recognition was presented (for
    more information refer to: *Very Deep Convolutional Networks for Large-Scale Image
    Recognition*, by K. Simonyan and A. Zisserman, 2014). The paper shows that, *a
    significant improvement on the prior-art configurations can be achieved by pushing
    the depth to 16-19 weight layers*. One model in the paper denoted as *D* or VGG-16
    has 16 deep layers. An implementation in Java Caffe ([http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/)) has
    been used for training the model on the ImageNet ILSVRC-2012 ([http://image-net.org/challenges/LSVRC/2012/](http://image-net.org/challenges/LSVRC/2012/)) dataset,
    which includes images of 1,000 classes and is split into three sets: training
    (1.3 million images), validation (50,000 images), and testing (100,000 images).
    Each image is (224 x 224) on three channels. The model achieves 7.5% top 5 error
    on ILSVRC-2012-val and 7.4% top 5 error on ILSVRC-2012-test.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，关于图像识别的一个有趣贡献被提出（更多信息请参考：*用于大规模图像识别的非常深的卷积神经网络*，作者：K. Simonyan 和 A. Zisserman，2014年）。该论文表明，*通过将深度推向16-19层，可以显著改善先前的网络配置*。论文中有一个模型被称为
    *D* 或 VGG-16，它有16层深度。该模型在 Java Caffe 中实现（[http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/)），用于在
    ImageNet ILSVRC-2012 数据集上训练模型，该数据集包含1,000个类别的图像，并分为三个部分：训练集（130万张图像）、验证集（5万张图像）和测试集（10万张图像）。每张图像的尺寸为（224
    x 224），且有三个通道。该模型在 ILSVRC-2012-验证集上达到了7.5%的Top 5 错误率，在 ILSVRC-2012-测试集上达到了7.4%的Top
    5 错误率。
- en: 'According to the ImageNet site:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 ImageNet 网站的描述：
- en: The goal of this competition is to estimate the content of photographs for the
    purpose of retrieval and automatic annotation using a subset of the large hand-labeled
    ImageNet dataset (10 million labeled images depicting 10,000 + object categories)
    as training. Test images will be presented with no initial annotation—no segmentation
    or labels—and algorithms will have to produce labelings specifying what objects
    are present in the images.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本次竞赛的目标是估计照片的内容，用于检索和自动注释，训练数据集使用的是一个大规模手工标注的ImageNet子集（包含1000万个标注的图像，涵盖10,000多个物体类别）。测试图像将以没有初始注释的形式呈现——没有分割或标签——算法需要生成标签，指明图像中有哪些物体。
- en: 'The weights learned by the model implemented in Caffe have been directly converted in
    Keras (for more information refer to: [https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3))
    and can be used for preloading into the Keras model, which is implemented next
    as described in the paper:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Caffe中实现的模型学习到的权重已直接转换为Keras（更多信息请参阅：[https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)），并可用于预加载到Keras模型中，接下来按论文描述实现：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Recognizing cats with a VGG-16 net
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VGG-16网络识别猫
- en: 'Now let us test the image of a ![](img/B06258_04_26.jpg):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们测试一张图片！[](img/B06258_04_26.jpg)：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When the code is executed, the class `285` is returned, which corresponds (for
    more information refer to: [https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a))
    to Egyptian cat:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码执行时，返回类别`285`，对应（更多信息请参阅：[https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)）埃及猫：
- en: '![](img/B06258_04_27-2.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_27-2.png)'
- en: Utilizing Keras built-in VGG-16 net module
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras内置的VGG-16网络模块
- en: 'Keras applications are pre-built and pre-trained deep learning models. Weights
    are downloaded automatically when instantiating a model and stored at `~/.keras/models/`.
    Using built-in code is very easy:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Keras应用程序是预构建和预训练的深度学习模型。权重在实例化模型时自动下载并存储在`~/.keras/models/`中。使用内置代码非常简单：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let us consider a train:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一列火车：
- en: '![](img/B06258_04_28.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_28.jpg)'
- en: 'It''s like the ones my grandfather drove. If we run the code, we get result
    `820`, which is the image net code for *steaming train*. Equally important is
    the fact that all the other classes have very weak support, as shown in the following
    graph:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 它就像我祖父驾驶过的那种。如果我们运行代码，我们得到结果`820`，这是*蒸汽火车*的ImageNet代码。同样重要的是，所有其他类别的支持非常弱，如下图所示：
- en: '![](img/B06258_04_29-1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_29-1.png)'
- en: 'To conclude this section, note that VGG-16 is only one of the modules that
    are pre-built in Keras. A full list of pre-trained Keras models is available at:
    [https://keras.io/applications/](https://keras.io/applications/).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分时，请注意，VGG-16只是Keras中预构建的模块之一。Keras模型的完整预训练模型列表可以在此处找到：[https://keras.io/applications/](https://keras.io/applications/)。
- en: Recycling pre-built deep learning models for extracting features
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环使用预构建的深度学习模型进行特征提取
- en: 'One very simple idea is to use VGG-16 and, more generally, DCNN, for feature
    extraction. This code implements the idea by extracting features from a specific
    layer:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的想法是使用VGG-16，更一般地说，使用DCNN进行特征提取。此代码通过从特定层提取特征来实现该想法：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now you might wonder why we want to extract the features from an intermediate
    layer in a DCNN. The key intuition is that, as the network learns to classify
    images into categories, each layer learns to identify the features that are necessary
    to do the final classification. Lower layers identify lower order features such
    as color and edges, and higher layers compose these lower order feature into higher
    order features such as shapes or objects. Hence the intermediate layer has the
    capability to extract important features from an image, and these features are
    more likely to help in different kinds of classification. This has multiple advantages.
    First, we can rely on publicly available large-scale training and transfer this
    learning to novel domains. Second, we can save time for expensive large training.
    Third, we can provide reasonable solutions even when we don't have a large number
    of training examples for our domain. We also get a good starting network shape
    for the task at hand, instead of guessing it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会想，为什么我们要从DCNN的中间层提取特征。关键的直觉是，当网络学习将图像分类到不同类别时，每一层都会学习识别做最终分类所需的特征。较低层识别较低阶的特征，如颜色和边缘，而较高层则将这些较低阶的特征组合成较高阶的特征，如形状或物体。因此，中间层具有从图像中提取重要特征的能力，这些特征更有可能帮助进行不同种类的分类。这有多个优势。首先，我们可以依赖公开可用的大规模训练，并将这种学习迁移到新的领域。其次，我们可以节省时间，避免进行昂贵的大规模训练。第三，即使我们没有大量的训练样本，也能提供合理的解决方案。我们还可以为当前任务提供一个良好的起始网络形状，而不必盲目猜测。
- en: Very deep inception-v3 net used for transfer learning
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于迁移学习的非常深的Inception-v3网络
- en: Transfer learning is a very powerful deep learning technique which has more
    applications in different domains. The intuition is very simple and can be explained
    with an analogy. Suppose you want to learn a new language, say Spanish; then it
    could be useful to start from what you already know in a different language, say
    English.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种非常强大的深度学习技术，在不同领域有着更多的应用。其直觉非常简单，可以通过一个类比来解释。假设你想学习一门新语言，比如西班牙语，那么从你已经掌握的另一种语言（如英语）开始可能会很有帮助。
- en: Following this line of thinking, computer vision researchers now commonly use
    pre-trained CNNs to generate representations for novel tasks, where the dataset
    may not be large enough to train an entire CNN from scratch. Another common tactic
    is to take the pre-trained ImageNet network and then to fine-tune the entire network
    to the novel task.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个思路，计算机视觉研究人员现在通常使用预训练的CNN来为新任务生成表示，其中数据集可能不足以从头训练整个CNN。另一个常用的策略是采用预训练的ImageNet网络，然后对整个网络进行微调，以适应新的任务。
- en: 'Inception-v3 net is a very deep ConvNet developed by Google. Keras implements
    the full network described in the following diagram and it comes pre-trained on
    ImageNet. The default input size for this model is 299 x 299 on three channels:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-v3网络是Google开发的一个非常深的卷积神经网络。Keras实现了下面图示的完整网络，并且它已经在ImageNet上进行了预训练。该模型的默认输入大小是299
    x 299，且有三个通道：
- en: '![](img/B06258_04_59.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04_59.png)'
- en: 'This skeleton example is inspired by a scheme available at: [https://keras.io/applications/](https://keras.io/applications/).
    We suppose to have a training dataset *D* in a domain, different from ImageNet.
    *D* has 1,024 features in input and 200 categories in output. Let us see a code
    fragment:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架示例的灵感来自于以下方案：[https://keras.io/applications/](https://keras.io/applications/)。我们假设在一个与ImageNet不同的领域中有一个训练数据集*D*。*D*的输入有1,024个特征，输出有200个类别。让我们看看一个代码片段：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We use a trained inception-v3; we do not include the top model because we want
    to fine-tune on *D*. The top level is a dense layer with 1,024 inputs and where
    the last output level is a softmax dense layer with 200 classes of output. `x
    = GlobalAveragePooling2D()(x)` is used to convert the input to the correct shape
    for the dense layer to handle. In fact, `base_model.output` tensor has the shape
    *(samples, channels, rows, cols)* for `dim_ordering="th"` or *(samples, rows,
    cols, channels)* for `dim_ordering="tf"` but dense needs them as *(samples, channels)*
    and `GlobalAveragePooling2D` averages across *(rows, cols)*. So if you look at
    the last four layers (where `include_top=True`), you see these shapes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用经过训练的Inception-v3模型；我们不包括顶层模型，因为我们想在*D*上进行微调。顶层是一个具有1,024个输入的全连接层，最后的输出层是一个softmax全连接层，输出200个类别。`x
    = GlobalAveragePooling2D()(x)`用于将输入转换为适合全连接层处理的正确形状。实际上，`base_model.output`张量的形状为`dim_ordering="th"`时是*(samples,
    channels, rows, cols)*，或者`dim_ordering="tf"`时是*(samples, rows, cols, channels)*，但全连接层需要的是*(samples,
    channels)*，而`GlobalAveragePooling2D`会对*(rows, cols)*进行平均化。因此，如果查看最后四层（`include_top=True`时），你会看到这些形状：
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'When you do `include_top=False,` you are removing the last three layers and
    exposing the `mixed10` layer, so the `GlobalAveragePooling2D` layer converts the
    *(None, 8, 8, 2048)* to *(None, 2048)*, where each element in the *(None, 2048)*
    tensor is the average value for each corresponding *(8, 8)* subtensor in the *(None,
    8, 8, 2048)* tensor:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当你设置`include_top=False`时，你正在移除最后三层并暴露出`mixed10`层，因此`GlobalAveragePooling2D`层将*(None,
    8, 8, 2048)*转换为*(None, 2048)*，其中*(None, 2048)*张量中的每个元素都是*(None, 8, 8, 2048)*张量中对应*(8,
    8)*子张量的平均值：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'All the convolutional levels are pre-trained, so we freeze them during the
    training of the full model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的卷积层都已预训练，因此在训练完整模型时我们会冻结它们：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The model is then compiled and trained for a few epochs so that the top layers
    are trained:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对模型进行编译并训练几个epoch，以便训练顶层：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then we freeze the top layers in inception and fine-tune some inception layer.
    In this example, we decide to freeze the first 172 layers (an hyperparameter to
    tune):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们冻结Inception模型的顶层，并对某些Inception层进行微调。在这个例子中，我们决定冻结前172层（这是一个需要调整的超参数）：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The model is then recompiled for fine-tune optimization. We need to recompile
    the model for these modifications to take effect:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重新编译模型以进行微调优化。我们需要重新编译模型，以使这些修改生效：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now we have a new deep network that reuses the standard Inception-v3 network,
    but it is trained on a new domain *D* via transfer learning. Of course, there
    are many parameters to fine-tune for achieving good accuracy. However, we are
    now reusing a very large pre-trained network as a starting point via transfer
    learning. In doing so, we can save the need to train on our machines by reusing
    what is already available in Keras.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个新的深度网络，它重用了标准的Inception-v3网络，但通过迁移学习在新的领域*D*上进行了训练。当然，为了获得良好的准确性，有很多参数需要微调。然而，我们现在通过迁移学习重用了一个非常大的预训练网络作为起点。通过这样做，我们可以节省不需要在自己的机器上进行训练，而是重用Keras中已经可用的资源。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to use Deep Learning ConvNets for recognizing
    MNIST handwritten characters with high accuracy. Then we used the CIFAR 10 dataset
    to build a deep learning classifier in 10 categories, and the ImageNet datasets
    to build an accurate classifier in 1,000 categories. In addition, we investigated
    how to use large deep learning networks such as VGG16 and very deep networks such
    as InceptionV3\. The chapter concluded with a discussion on transfer learning
    in order to adapt pre-built models trained on large datasets so that they can
    work well on a new domain.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何使用深度学习卷积网络（ConvNets）高精度识别MNIST手写字符。接着，我们使用CIFAR 10数据集构建了一个10个类别的深度学习分类器，并使用ImageNet数据集构建了一个1,000个类别的准确分类器。此外，我们还研究了如何使用像VGG16这样的深度学习大网络以及像InceptionV3这样非常深的网络。最后，本章讨论了迁移学习，旨在适应在大型数据集上训练的预构建模型，使其能够在新领域上有效工作。
- en: In the next chapter, we will introduce generative adversarial networks used
    to reproduce synthetic data that looks like data generated by humans; and we will
    present WaveNet, a deep neural network used for reproducing human voice and musical
    instruments with high quality.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍生成对抗网络，它用于生成看起来像是人类生成的数据的合成数据；我们还将介绍WaveNet，一种用于高质量重现人类声音和乐器声音的深度神经网络。
