- en: Unity ML-Agents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity ML-Agents
- en: Unity has embraced machine learning, and deep reinforcement learning in particular, with
    determination and vigor with the aim of producing a working **seep reinforcement
    learning** (**DRL**) SDK for game and simulation developers. Fortunately, the
    team at Unity, led by Danny Lange, has succeeded in developing a robust cutting-edge
    DRL engine capable of impressive results. This engine is the top of the line and
    outclasses the DQN model we introduced earlier in many ways. Unity uses a **proximal
    policy optimization** (**PPO**) model as the basis for its DRL engine. This model
    is significantly more complex and may differ in some ways, but, fortunately, this
    is at the start of many more chapters, and we will have plenty of time to introduce
    the concepts as we go—this is a hands-on book, after all.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Unity已经坚定并充满活力地拥抱了机器学习，尤其是深度强化学习（**DRL**），目标是为游戏和仿真开发者提供一个有效的**深度强化学习**（**DRL**）SDK。幸运的是，Unity团队在Danny
    Lange的带领下，成功开发了一个强大的前沿DRL引擎，能够实现令人印象深刻的效果。这个引擎在多个方面超越了我们之前介绍的DQN模型，是顶级的。Unity使用**近端策略优化**（**PPO**）模型作为其DRL引擎的基础。这个模型显著更为复杂，且可能在某些方面有所不同，但幸运的是，这只是许多章节的开始，我们会有足够的时间介绍这些概念——毕竟这是一本实践性强的书籍。
- en: 'In this chapter, we introduce the **Unity ML-Agents** tools and SDK for building
    DRL agents to play games and simulations. While this tool is both powerful and
    cutting-edge, it is also easy to use and provides a few tools to help us learn
    concepts as we go. In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了**Unity ML-Agents**工具和SDK，用于构建DRL代理来玩游戏和仿真。虽然这个工具既强大又前沿，但它也很容易使用，并且提供了一些帮助我们边学边用的工具。在本章中，我们将涵盖以下主题：
- en: Installing ML-Agents
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装ML-Agents
- en: Training an agent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个代理
- en: What's in a brain?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大脑里有什么？
- en: Monitoring training with TensorBoard
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorBoard监控训练过程
- en: Running an agent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个代理
- en: 'We would like to thank the team members at Unity for their great work on ML-Agents;
    here are the team members at the time of writing:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想感谢Unity团队成员们在ML-Agents方面的卓越工作；以下是写作时的团队成员名单：
- en: Danny Lange ([https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+D](https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+D))
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danny Lange ([https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+D](https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+D))
- en: Arthur Juliani ([https://arxiv.org/search/cs?searchtype=author&query=Juliani%2C+A](https://arxiv.org/search/cs?searchtype=author&query=Juliani%2C+A))
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arthur Juliani ([https://arxiv.org/search/cs?searchtype=author&query=Juliani%2C+A](https://arxiv.org/search/cs?searchtype=author&query=Juliani%2C+A))
- en: Vincent-Pierre Berges ([https://arxiv.org/search/cs?searchtype=author&query=Berges%2C+V](https://arxiv.org/search/cs?searchtype=author&query=Berges%2C+V))
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent-Pierre Berges ([https://arxiv.org/search/cs?searchtype=author&query=Berges%2C+V](https://arxiv.org/search/cs?searchtype=author&query=Berges%2C+V))
- en: Esh Vckay ([https://arxiv.org/search/cs?searchtype=author&query=Vckay%2C+E](https://arxiv.org/search/cs?searchtype=author&query=Vckay%2C+E))
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esh Vckay ([https://arxiv.org/search/cs?searchtype=author&query=Vckay%2C+E](https://arxiv.org/search/cs?searchtype=author&query=Vckay%2C+E))
- en: Yuan Gao ([https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y))
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan Gao ([https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y))
- en: Hunter Henry ([https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+H](https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+H))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hunter Henry ([https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+H](https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+H))
- en: Marwan Mattar ([https://arxiv.org/search/cs?searchtype=author&query=Mattar%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Mattar%2C+M))
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marwan Mattar ([https://arxiv.org/search/cs?searchtype=author&query=Mattar%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Mattar%2C+M))
- en: Adam Crespi ([https://arxiv.org/search/cs?searchtype=author&query=Crespi%2C+A](https://arxiv.org/search/cs?searchtype=author&query=Crespi%2C+A))
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam Crespi ([https://arxiv.org/search/cs?searchtype=author&query=Crespi%2C+A](https://arxiv.org/search/cs?searchtype=author&query=Crespi%2C+A))
- en: Jonathan Harper ([https://arxiv.org/search/cs?searchtype=author&query=Harper%2C+J](https://arxiv.org/search/cs?searchtype=author&query=Harper%2C+J))
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jonathan Harper ([https://arxiv.org/search/cs?searchtype=author&query=Harper%2C+J](https://arxiv.org/search/cs?searchtype=author&query=Harper%2C+J))
- en: Be sure you have Unity installed as per the section in [Chapter 4](a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml),
    *Building a Deep Learning Gaming Chatbot,* before proceeding with this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本章之前，请确保已按照[第4章](a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml)中的说明安装Unity，*构建深度学习游戏聊天机器人*。
- en: Installing ML-Agents
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装ML-Agents
- en: In this section, we cover a high-level overview of the steps you will need to
    take in order to successfully install the ML-Agents SDK. This material is still
    in beta and has already changed significantly from version to version. As such,
    if you get stuck going through these high-level steps, just go back to the most
    recent Unity docs; they are very well written.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将概述成功安装 ML-Agents SDK 所需的高层步骤。这些材料仍然处于 beta 版本，并且在各版本之间已经发生了显著变化。因此，如果在执行这些高层步骤时遇到困难，只需返回到最新的
    Unity 文档；它们写得非常清晰。
- en: 'Jump on your computer and follow these steps; there may be many sub steps,
    so expect this to take a while:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到你的计算机上操作并跟随这些步骤；可能会有很多子步骤，所以预计这会花费一些时间：
- en: Be sure you have **Git** installed on your computer; it works from the command
    line. Git is a very popular source code management system, and there is a ton
    of resources on how to install and use Git for your platform. After you have installed
    Git, just be sure it works by test cloning a repository, any repository.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你的计算机已安装 **Git**；它可以通过命令行使用。Git 是一个非常流行的源代码管理系统，关于如何安装和使用 Git 的资源非常丰富，适用于你的平台。安装
    Git 后，确保它正常工作，可以通过克隆一个仓库来测试，任意一个仓库都可以。
- en: Open a command window or a regular shell. Windows users can open an Anaconda
    window.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令窗口或常规 shell。Windows 用户可以打开 Anaconda 窗口。
- en: 'Change to a working folder where you want to place the new code, and enter
    the following command (Windows users may want to use `C:\ML-Agents`):'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到一个工作文件夹，放置新代码的文件夹，并输入以下命令（Windows 用户可以选择使用 `C:\ML-Agents`）：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will clone the `ml-agents` repository onto your computer and create a
    new folder with the same name. You may want to take the extra step of also adding
    the version to the folder name. Unity, and pretty much the whole AI space, is
    in continuous transition, at least at the moment. This means new and constant
    changes are always happening. At the time of writing, we will clone to a folder
    named `ml-agents.6`, like so:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将把 `ml-agents` 仓库克隆到你的计算机上，并创建一个同名的文件夹。你可能还想额外为文件夹名称添加版本号。Unity，以及整个 AI 领域，至少在当前阶段，是在不断变化的。这意味着新的变化和持续的更新始终在发生。写作时，我们将仓库克隆到名为
    `ml-agents.6` 的文件夹中，如下所示：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The author of this book previously wrote a book on ML-Agents and had to rewrite
    several chapters over the course of a short time in order to accommodate the major
    changes. In fact, this chapter has had to be also rewritten a few times to account
    for more major changes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的作者曾经写过一本关于 ML-Agents 的书，并且在短时间内重新编写了几章，以适应重要的变更。实际上，本章也经历了几次重写，以应对更多的重大变化。
- en: 'Create a new virtual environment for `ml-agents` and set it to `3.6`, like
    so:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 `ml-agents` 创建一个新的虚拟环境并将其设置为 `3.6`，像这样：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Activate the environment, again, using Anaconda:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活该环境，同样通过 Anaconda：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install TensorFlow. With Anaconda, we can do this by using the following:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 TensorFlow。使用 Anaconda，我们可以通过以下命令进行安装：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Install the Python packages. On Anaconda, enter the following:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Python 包。在 Anaconda 中，输入以下命令：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will install all the required packages for the Agents SDK and may take
    several minutes. Be sure to leave this window open, as we will use it shortly.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将安装 Agents SDK 所需的所有软件包，可能需要几分钟时间。请确保保持此窗口打开，因为我们稍后会使用它。
- en: This is the basic installation of TensorFlow and does not use a GPU. Consult
    the Unity documentation in order to learn how to install the GPU version. This
    may or may not have a dramatic impact on your training performance, depending
    on the power of your GPU.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 TensorFlow 的基本安装，不使用 GPU。请查阅 Unity 文档，了解如何安装 GPU 版本。这可能会对你的训练性能产生显著影响，具体取决于你的
    GPU 性能。
- en: This should complete the setup of the Unity Python SDK for ML-Agents. In the
    next section, we will learn how to set up and train one of the many example environments
    provided by Unity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该完成 Unity Python SDK for ML-Agents 的设置。在下一部分中，我们将学习如何设置并训练 Unity 提供的多个示例环境之一。
- en: Training an agent
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个智能体
- en: 'For much of this book, we have spent our time looking at code and the inner
    depths of **deep learning** (**DL**) and **reinforcement learning** (**RL**).
    With that knowledge established, we can now jump in and look at examples where
    **deep reinforcement learning** (**DRL**) is put to use. Fortunately, the new
    agent''s toolkit provides several examples to demonstrate the power of the engine.
    Open up Unity or the Unity Hub and follow these steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分时间，我们都在研究代码以及 **深度学习** (**DL**) 和 **强化学习** (**RL**) 的内部知识。基于这些知识，我们现在可以进入并查看
    **深度强化学习** (**DRL**) 的实际应用。幸运的是，新的代理工具包提供了多个示例，展示了引擎的强大功能。打开 Unity 或 Unity Hub
    并按照以下步骤操作：
- en: Click the Open project button at the top of the Project dialog.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击项目对话框顶部的“打开项目”按钮。
- en: 'Locate and open the `UnitySDK` project folder as shown in the following screenshot:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位并打开`UnitySDK`项目文件夹，如下图所示：
- en: '![](img/ecbdfb91-2cfb-47ef-b264-b8cfe8e45d3b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecbdfb91-2cfb-47ef-b264-b8cfe8e45d3b.png)'
- en: Opening the UnitySDK project
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 UnitySDK 项目
- en: Wait for the project to load and then open the Project window at the bottom
    of the editor. If you are asked to update the project, just be sure to say yes
    or continue. Thus far, all of the agent code has been designed to be backward
    compatible.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待项目加载完成后，打开编辑器底部的项目窗口。如果系统提示更新项目，请确保选择“是”或“继续”。到目前为止，所有代理代码都已设计为向后兼容。
- en: 'Locate and open the GridWorld scene as shown in this screenshot:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位并打开如截图所示的 GridWorld 场景：
- en: '![](img/3456149c-1ca2-4371-b20e-bbba7590e13f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3456149c-1ca2-4371-b20e-bbba7590e13f.png)'
- en: Opening the GridWorld example scene
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 GridWorld 示例场景
- en: Select the GridAcademy object in the Hierarchy window.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口中选择 GridAcademy 对象。
- en: 'Then direct your attention to the Inspector window, and beside the Brains,
    click the target icon to open the Brain selection dialog:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将注意力集中在 Inspector 窗口，点击 Brains 旁边的目标图标以打开大脑选择对话框：
- en: '![](img/3dfa4068-3490-4a17-b3fb-8f968b14433f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3dfa4068-3490-4a17-b3fb-8f968b14433f.png)'
- en: Inspecting the GridWorld example environment
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 GridWorld 示例环境
- en: Select the GridWorldPlayer brain. This brain is a *player* brain, meaning that
    a player, you, can control the game. We will look at this brain concept more in
    the next section.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 GridWorldPlayer 大脑。这个大脑是 *玩家* 大脑，意味着玩家（即你）可以控制游戏。我们将在下一部分详细探讨这个大脑概念。
- en: Press the Play button at the top of the editor and watch the grid environment
    form. Since the game is currently set to a player, you can use the **WASD** controls
    to move the cube. The goal is much like the FrozenPond environment we built a
    DQN for earlier. That is, you have to move the blue cube to the green + symbol
    and avoid the red X.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下编辑器顶部的播放按钮，观察网格环境的形成。由于游戏当前设置为玩家控制，你可以使用 **WASD** 控制键来移动方块。目标与我们之前为 FrozenPond
    环境构建的 DQN 类似。也就是说，你需要将蓝色方块移动到绿色的 + 符号处，并避免碰到红色的 X。
- en: Feel free to play the game as much as you like. Note how the game only runs
    for a certain amount of time and is not turn-based. In the next section, we will
    learn how to run this example with a DRL agent.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以玩游戏，注意游戏运行的时间是有限的，并且不是回合制的。在下一部分中，我们将学习如何使用 DRL 代理运行这个示例。
- en: What's in a brain?
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大脑中有什么？
- en: One of the brilliant aspects of the ML-Agents platform is the ability to switch
    from player control to AI/agent control very quickly and seamlessly. In order
    to do this, Unity uses the concept of a **brain**. A brain may be either player-controlled,
    a player brain, or agent-controlled, a learning brain. The brilliant part is that
    you can build a game and test it, as a player can then turn the game loose on
    an RL agent. This has the added benefit of making any game written in Unity controllable
    by an AI with very little effort. In fact, this is such a powerful workflow that
    we will spend an entire chapter, [Chapter 12](323523c2-82f9-48c4-b1b5-35d417f90558.xhtml), *Debugging/Testing
    a Game with DRL*, on testing and debugging your games with RL.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 平台的一个亮点是能够非常快速且无缝地从玩家控制切换到 AI/代理控制。为了实现这一点，Unity 引入了 **大脑** 的概念。大脑可以是玩家控制的，也可以是代理控制的，后者被称为学习大脑。亮点在于，你可以构建一个游戏并进行测试，之后玩家可以将游戏交给
    RL 代理来控制。这一流程的额外好处是，让任何用 Unity 编写的游戏都能通过极少的努力由 AI 控制。事实上，这种强大的工作流让我们决定专门花一整章时间来讨论，[第12章](323523c2-82f9-48c4-b1b5-35d417f90558.xhtml)，*使用
    DRL 调试/测试游戏*，来学习如何使用 RL 测试和调试你的游戏。
- en: 'Training an RL agent with Unity is fairly straightforward to set up and run.
    Unity uses Python externally to build the learning brain model. Using Python makes
    far more sense, since as we have already seen, several DL libraries are built
    on top of it. Follow these steps to train an agent for the GridWorld environment:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Unity训练RL代理非常简单，设置和运行都很直接。Unity在外部使用Python来构建学习脑模型。使用Python是更有意义的，因为正如我们之前所见，多个深度学习库都是基于Python构建的。按照以下步骤训练GridWorld环境中的代理：
- en: 'Select the GridAcademy again and switch the Brains from GridWorldPlayer to
    GridWorldLearning as shown:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新选择GridAcademy，并将Brains从GridWorldPlayer切换为GridWorldLearning，如下所示：
- en: '![](img/574c1a5c-98dd-4d89-8d2a-5eb9a81d1eb4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/574c1a5c-98dd-4d89-8d2a-5eb9a81d1eb4.png)'
- en: Switching the brain to use GridWorldLearning
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 切换脑部使用GridWorldLearning
- en: Make sure to click the Control option at the end. This simple setting is what
    tells the brain it may be controlled externally. Be sure to double-check that
    the option is enabled.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保点击最后的Control选项。这个简单的设置告诉脑部它可以被外部控制。一定要再次确认该选项已启用。
- en: 'Select the trueAgent object in the Hierarchy window, and then, in the Inspector
    window, change the Brain property under the Grid Agent component to a GridWorldLearning
    brain:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口中选择`trueAgent`对象，然后在检查器窗口中，将Grid Agent组件下的Brain属性更改为GridWorldLearning脑：
- en: '![](img/438357ea-b6c5-49c3-a2a2-9e6d44c7f0ee.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/438357ea-b6c5-49c3-a2a2-9e6d44c7f0ee.png)'
- en: Setting the brain on the agent to GridWorldLearning
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将代理的脑设置为GridWorldLearning
- en: For this sample, we want to switch our Academy and Agent to use the same brain,
    GridWorldLearning. In more advanced cases we will explore later, this is not always
    the case. You could of course have a player and an agent brain running in tandem,
    or many other configurations.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于此示例，我们希望将Academy和Agent切换为使用相同的brain，即GridWorldLearning。在我们稍后探索的更高级用法中，情况不总是如此。当然，你也可以让一个玩家和一个代理脑同时运行，或者其他多种配置。
- en: Be sure you have an Anaconda or Python window open and set to the `ML-Agents/ml-agents` folder
    or your versioned `ml-agents` folder.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你已打开Anaconda或Python窗口，并设置为`ML-Agents/ml-agents`文件夹或你的版本化`ml-agents`文件夹。
- en: 'Run the following command in the Anaconda or Python window using the `ml-agents`
    virtual environment:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Anaconda或Python窗口中，使用`ml-agents`虚拟环境运行以下命令：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will start the Unity PPO trainer and run the agent example as configured.
    At some point, the command window will prompt you to run the Unity editor with
    the environment loaded.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动Unity PPO训练器，并根据配置运行代理示例。在某些时候，命令窗口将提示你运行已加载环境的Unity编辑器。
- en: 'Press Play in the Unity editor to run the GridWorld environment. Shortly after,
    you should see the agent training with the results being output in the Python
    script window:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unity编辑器中按下播放按钮，运行GridWorld环境。不久之后，你应该能看到代理正在训练，且结果会输出到Python脚本窗口中：
- en: '![](img/28c402ce-1770-4986-ad61-3250d518b949.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28c402ce-1770-4986-ad61-3250d518b949.png)'
- en: Running the GridWorld environment in training mode
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模式下运行GridWorld环境
- en: Note how the `mlagents-learn` script is the Python code that builds the RL model
    to run the agent. As you can see from the output of the script, there are several
    parameters, or what we refer to as **hyper-parameters**, that need to be configured.
    Some of these parameters may sound familiar, and they should, but several may
    be unclear. Fortunately, for the rest of this chapter and this book, we will explore
    how to tune these parameters in some detail.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意`mlagents-learn`脚本是构建RL模型以运行代理的Python代码。从脚本的输出可以看出，有多个参数，或者我们称之为**超参数**，需要进行配置。部分参数可能会让你感到熟悉，没错，但也有一些可能不太明了。幸运的是，在本章和本书的后续内容中，我们将详细探讨如何调整这些参数。
- en: Let the agent train for several thousand iterations and note how quickly it
    learns. The internal model here, called **PPO**, has been shown to be a very effective
    learner at multiple forms of tasks and is very well suited for game development.
    Depending on your hardware, the agent may learn to perfect this task in less than
    an hour.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理训练几千次，并注意它学习的速度。这里的内部模型，称为**PPO**，已被证明在多种任务中非常有效，且非常适合游戏开发。根据你的硬件，代理可能在不到一小时的时间内就能完美完成此任务。
- en: Keep the agent training, and we will look at more ways to inspect the agent's
    training progress in the next section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 继续让代理训练，我们将在下一节中探讨更多检查代理训练进度的方法。
- en: Monitoring training with TensorBoard
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard监控训练过程
- en: 'Training an agent with RL, or any DL model for that matter, while enjoyable,
    is not often a simple task and requires some attention to detail. Fortunately,
    TensorFlow ships with a set of graph tools called **TensorBoard** we can use to
    monitor training progress. Follow these steps to run TensorBoard:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强化学习（RL）或任何深度学习（DL）模型来训练智能体，虽然很有趣，但通常不是一件简单的任务，需要注意细节。幸运的是，TensorFlow自带了一套名为**TensorBoard**的图形工具，我们可以用它来监控训练进度。按照以下步骤运行TensorBoard：
- en: Open an Anaconda or Python window. Activate the `ml-agents` virtual environment.
    Don't shut down the window running the trainer; we need to keep that going.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Anaconda或Python窗口。激活`ml-agents`虚拟环境。不要关闭运行训练器的窗口；我们需要保持它运行。
- en: 'Navigate to the `ML-Agents/ml-agents` folder and run the following command:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`ML-Agents/ml-agents`文件夹，并运行以下命令：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will run TensorBoard with its own built-in web server. You can load the
    page by using the URL that is shown after you run the previous command.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动TensorBoard并运行它内置的Web服务器。你可以通过运行前述命令后显示的URL加载页面。
- en: 'Enter the URL for TensorBoard as shown in the window, or use `localhost:6006`
    or `machinename:6006` in your browser. After an hour or so, you should see something
    similar to the following:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入窗口中显示的TensorBoard URL，或者在浏览器中使用`localhost:6006`或`machinename:6006`。大约一个小时后，你应该会看到类似以下的内容：
- en: '![](img/40e5492f-96ed-4f05-b12a-b74a9ddcd12c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40e5492f-96ed-4f05-b12a-b74a9ddcd12c.png)'
- en: The TensorBoard graph window
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard图形窗口
- en: 'In the preceding screenshot, you can see each of the various graphs denoting
    an aspect of training. Understanding each of these graphs is important to understanding
    how your agent is training, so we will break down the output from each section:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上面的截图中，你可以看到每个不同的图表表示了训练的一个方面。理解这些图表对于了解智能体的训练过程非常重要，因此我们将逐个分析每个部分的输出：
- en: 'Environment: This section shows how the agent is performing overall in the
    environment. A closer look at each of the graphs is shown in the following screenshot
    with their preferred trend:'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境：这一部分展示了智能体在环境中的整体表现。接下来的截图展示了每个图表的更详细查看及其优选趋势：
- en: '![](img/f404a429-9388-4c55-8bef-433bd24e2523.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f404a429-9388-4c55-8bef-433bd24e2523.png)'
- en: Closer look at the Environment section plots
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步查看环境部分的图表
- en: 'Cumulative Reward: This is the total reward the agent is maximizing. You generally
    want to see this going up, but there are reasons why it may fall. It is always
    best to maximize rewards in the range of 1 to -1\. If you see rewards outside
    this range on the graph, you also want to correct this as well.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积奖励：这是智能体正在最大化的总奖励。通常你希望看到它上升，但也有可能出现下降的情况。最好将奖励最大化在1到-1的范围内。如果你在图表中看到超出这个范围的奖励，也需要进行修正。
- en: 'Episode Length: It usually is a better sign if this value decreases. After
    all, shorter episodes mean more training. However, keep in mind that the episode
    length could increase out of need, so this one can go either way.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练回合长度：如果这个值减小，通常是个好兆头。毕竟，较短的回合意味着更多的训练。然而，请记住，回合长度可能因需要而增加，因此这个值可能会有所波动。
- en: 'Lesson: This represents which lesson the agent is on and is intended for Curriculum
    Learning. We will learn more about Curriculum Learning in [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml),
    *Rewards and Reinforcement Learning*.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程：这表示智能体当前所在的课程，适用于课程学习。我们将在[第9章](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml)中学习更多关于课程学习的内容，*奖励和强化学习*。
- en: 'Losses: This section shows graphs that represent the calculated loss or cost
    of the policy and value. Of course, we haven''t spent much time explaining PPO
    and how it uses a policy, so, at this point, just understand the preferred direction
    when training. A screenshot of this section is shown next, again with arrows showing
    the optimum preferences:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失：这一部分显示了表示策略和价值计算损失或成本的图表。当然，我们并没有花太多时间解释PPO及其如何使用策略，所以在此时，只需理解训练时的优选方向。接下来是该部分的截图，箭头显示了最佳的偏好方向：
- en: '![](img/357e6132-9381-4c6b-bde8-3b66bd378e4f.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/357e6132-9381-4c6b-bde8-3b66bd378e4f.png)'
- en: Losses and preferred training direction
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 损失和优选训练方向
- en: 'Policy Loss: This determines how much the policy is changing over time. The
    policy is the piece that decides the actions, and in general this graph should
    be showing a downward trend, indicating that the policy is getting better at making
    decisions.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略损失：这个值决定了策略随时间的变化程度。策略是决定行为的部分，通常这个图表应该显示出下降趋势，表明策略在做决策时越来越好。
- en: 'Value Loss: This is the mean or average loss of the `value` function. It essentially
    models how well the agent is predicting the value of its next state. Initially,
    this value should increase, and then after the reward is stabilized, it should
    decrease.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值损失：这是`值`函数的平均损失。它基本上表示代理对下一个状态的价值预测得有多准确。最初，这个值应增加，奖励稳定后它应减少。
- en: 'Policy: PPO uses the concept of a policy rather than a model to determine the
    quality of actions. Again, we will spend more time on this in [Chapter 8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding
    PPO,* where we will uncover further details about PPO. The next screenshot shows
    the policy graphs and their preferred trend:'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略：PPO 使用策略的概念，而不是模型来确定行动的质量。同样，我们将在[第8章](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml)《*理解
    PPO*》中花更多时间讨论这一点，并揭示 PPO 的更多细节。下一张截图展示了策略图及其优先趋势：
- en: '![](img/cc002fa5-7562-415a-93ea-f4f5ab381170.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc002fa5-7562-415a-93ea-f4f5ab381170.png)'
- en: Policy graphs and preferred trends
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 策略图和优先趋势
- en: 'Entropy: This represents how much the agent is exploring. You want this value
    to decrease as the agent learns more about its surroundings and needs to explore
    less.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熵：表示代理探索的程度。随着代理对环境了解得更多，它需要探索的程度减少，因此该值应减少。
- en: 'Learning Rate: Currently, this value is set to decrease linearly over time.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：当前该值设置为随时间线性下降。
- en: 'Value Estimate: This is the mean or average value visited by all states of
    the agent. This value should increase in order to represent a growth of the agent''s
    knowledge and then stabilize.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值估计：这是所有代理状态访问的平均值或均值。此值应增加，以代表代理知识的增长，然后稳定下来。
- en: These graphs are all designed to work with the implementation of the PPO method
    Unity is based on. Don't worry too much about understanding these new terms just
    yet. We will explore the foundations of PPO in [Chapter 7](9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml),
    *Agent and the Environment*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表都旨在与 Unity 基于 PPO 方法的实现一起使用。暂时不用太担心理解这些新术语，我们将在[第7章](9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml)《*代理与环境*》中深入探讨
    PPO 的基础知识。
- en: Let the agent run to completion and keep TensorBoard running.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理运行到完成，并保持 TensorBoard 运行。
- en: 'Go back to the Anaconda/Python window that was training the brain and run this
    command:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到正在训练大脑的 Anaconda/Python 窗口，并运行以下命令：
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You will again be prompted to press Play in the editor; be sure to do so. Let
    the agent start the training and run for a few sessions. As you do so, monitor
    the TensorBoard window and note how the `secondRun` is shown on the graphs. Feel
    free to let this agent run to completion as well, but you can stop it now, if
    you want to.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将再次被提示在编辑器中按下播放按钮；请确保这样做。让代理开始训练并运行几轮。在此过程中，监控 TensorBoard 窗口，并注意图表上如何显示 `secondRun`。你也可以让这个代理运行到完成，但如果你愿意，现在可以停止它。
- en: In previous versions of ML-Agents, you needed to build a Unity executable first
    as a game-training environment and run that. The external Python brain would still
    run the same. This method made it very difficult to debug any code issues or problems
    with your game. All of these difficulties were corrected with the current method;
    however, we may need to use the old executable method later for some custom training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ML-Agents 的早期版本中，你需要先构建一个 Unity 可执行文件作为游戏训练环境并运行它。外部 Python 大脑依旧按照相同的方式运行。这种方法使得调试代码问题或游戏中的问题变得非常困难。所有这些问题已经通过当前的方法得到解决；然而，对于某些自定义训练，我们可能需要以后再使用旧的可执行文件方法。
- en: Now that we have seen how easy it is to set up and train an agent, we will go
    through the next section to see how that agent can be run without an external
    Python brain and run directly in Unity.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到设置和训练代理是多么简单，接下来我们将通过下一部分来了解如何在没有外部 Python 大脑的情况下直接在 Unity 中运行该代理。
- en: Running an agent
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行代理
- en: Using Python to train works well, but it is not something a real game would
    ever use. Ideally, what we want to be able to do is build a TensorFlow graph and
    use it in Unity. Fortunately, a library was constructed, called TensorFlowSharp,
    that allows .NET to consume TensorFlow graphs. This allows us to build offline
    TFModels and later inject them into our game. Unfortunately, we can only use trained
    models and not train in this manner, at least not yet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 进行训练效果很好，但它不是实际游戏中会使用的方式。理想情况下，我们希望能够构建一个 TensorFlow 图并在 Unity 中使用它。幸运的是，构建了一个名为
    TensorFlowSharp 的库，它允许 .NET 使用 TensorFlow 图。这使我们能够构建离线的 TF 模型，并在之后将其注入到我们的游戏中。不幸的是，我们只能使用已训练的模型，暂时不能以这种方式进行训练。
- en: 'Let''s see how this works by using the graph we just trained for the GridWorld
    environment and use it as an internal brain in Unity. Follow the exercise in the
    next section to set up and use an internal brain:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用刚刚为 GridWorld 环境训练的图表并将其作为 Unity 中的内部大脑来看看它是如何工作的。请按下一个小节中的练习步骤来设置并使用内部大脑：
- en: Download the TFSharp plugin from this link: [https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage](https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage).
    [](https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '从此链接下载 TFSharp 插件: [https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage](https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage)。'
- en: If this link does not work, consult the Unity docs or the Asset Store for a
    new one. The current version is described as experimental and subject to change.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此链接无法使用，请查阅 Unity 文档或 Asset Store 获取新链接。当前版本被描述为实验性，并可能发生变化。
- en: From the editor menu, select Assets | Import Package | Custom Package...
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从编辑器菜单中选择 Assets | Import Package | Custom Package...
- en: Locate the asset package you just downloaded and use the import dialogs to load
    the plugin into the project. If you need help with these basic Unity tasks, there
    is plenty of help online that can guide you further.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到刚刚下载的资产包，并使用导入对话框将插件加载到项目中。如果您需要有关这些基础 Unity 任务的帮助，网上有大量的资料可以进一步指导您。
- en: From the menu, select Edit | Project Settings. This will open the Settings window
    (new in 2018.3)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从菜单中选择 Edit | Project Settings。这将打开设置窗口（2018.3 新增功能）
- en: 'Locate under the Player options the Scripting Define Symbols and set the text
    to `ENABLE_TENSORFLOW` and enable Allow Unsafe Code, as shown in this screenshot:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Player 选项下找到 Scripting Define Symbols，并将文本设置为 `ENABLE_TENSORFLOW`，并启用 Allow
    Unsafe Code，如下图所示：
- en: '![](img/ab714c99-1b2d-4e27-89e4-eff8395c7f43.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab714c99-1b2d-4e27-89e4-eff8395c7f43.png)'
- en: Setting the ENABLE_TENSORFLOW flag
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 `ENABLE_TENSORFLOW` 标志
- en: Locate the GridWorldAcademy object in the Hierarchy window and make sure it
    is using the Brains | GridWorldLearning. Turn the Control option off under the
    Brains section of the Grid Academy script.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Hierarchy 窗口中找到 GridWorldAcademy 对象，并确保它使用的是 Brains | GridWorldLearning。然后关闭
    Grid Academy 脚本中 Brains 部分下的 Control 选项。
- en: 'Locate the GridWorldLearning brain in the `Assets/Examples/GridWorld/Brains`
    folder and make sure the Model parameter is set in the Inspector window, as shown
    in this screenshot:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Assets/Examples/GridWorld/Brains` 文件夹中找到 GridWorldLearning 大脑，并确保在 Inspector
    窗口中设置了 Model 参数，如下图所示：
- en: '![](img/321101c9-21cb-4a31-b298-dc4c3a51cc32.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/321101c9-21cb-4a31-b298-dc4c3a51cc32.png)'
- en: Setting the model for the brain to use
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为大脑设置使用的模型
- en: The Model should already be set to theGridWorldLearningmodel. In this example,
    we are using the TFModel that is shipped with the GridWorld example. You could
    also easily use the model we had trained from the earlier example by just importing
    it into the project and then setting it as the model.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Model` 应该已经设置为 GridWorldLearning 模型。在这个例子中，我们使用的是与 GridWorld 示例一起提供的 TFModel。您也可以通过将我们在前一个示例中训练的模型导入到项目中并将其设置为模型来轻松使用它。'
- en: Press Play to run the editor and watch the agent control the cube.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下 Play 运行编辑器并观察代理控制立方体。
- en: Right now, we are running the environment with the pre-trained Unity brain.
    In the next section, we will look at how to use the brain we trained in the previous
    section.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们正在使用预训练的 Unity 大脑运行环境。在下一个小节中，我们将查看如何使用我们在前一节中训练的大脑。
- en: Loading a trained brain
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载已训练的大脑
- en: 'All of the Unity samples come with pre-trained brains you can use to explore
    the samples. Of course, we want to be able to load our own TF graphs into Unity
    and run them. Follow the next steps in order to load a trained graph:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Unity 示例都带有预训练的大脑，您可以使用它们来探索示例。当然，我们希望能够将我们自己的 TF 图加载到 Unity 中并运行它们。请按照以下步骤加载已训练的图表：
- en: 'Locate the `ML-Agents/ml-agents/models/firstRun-0` folder. Inside this folder,
    you should see a file named `GridWorldLearning.bytes`. Drag this file into the
    Unity editor into the `Project/Assets/ML-Agents/Examples/GridWorld/TFModels` folder,
    as shown:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到 `ML-Agents/ml-agents/models/firstRun-0` 文件夹。在此文件夹中，您应看到一个名为 `GridWorldLearning.bytes`
    的文件。将该文件拖入 Unity 编辑器中的 `Project/Assets/ML-Agents/Examples/GridWorld/TFModels`
    文件夹，如下所示：
- en: '![](img/a41646b7-b3df-4fdf-a9bc-2ca3f9370961.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a41646b7-b3df-4fdf-a9bc-2ca3f9370961.png)'
- en: Dragging the bytes graph into Unity
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将字节图表拖入 Unity
- en: This will import the graph into the Unity project as a resource and rename it
    `GridWorldLearning 1`. It does this because the default model already has the
    same name.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将把图形导入Unity项目作为资源，并将其重命名为`GridWorldLearning 1`。这样做是因为默认模型已经有了相同的名称。
- en: 'Locate the `GridWorldLearning` from the `brains` folder and select it in the Inspector
    windows and drag the new GridWorldLearning 1 model onto the Model slot under the
    Brain Parameters:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`brains`文件夹中找到`GridWorldLearning`，在Inspector窗口中选择它，并将新的`GridWorldLearning 1`模型拖动到大脑参数下的模型插槽中：
- en: '![](img/21bef292-7d4d-4f6f-a20c-9243c3c4bb8e.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21bef292-7d4d-4f6f-a20c-9243c3c4bb8e.png)'
- en: Loading the Graph Model slot in the brain
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在大脑中加载图形模型插槽。
- en: We won't need to change any other parameters at this point, but pay special
    attention to how the brain is configured. The defaults will work for now.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一阶段，我们不需要更改其他参数，但请特别注意大脑的配置。默认设置目前是可行的。
- en: Press Play in the Unity editor and watch the agent run through the game successfully.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unity编辑器中按下播放按钮，观察智能体成功地完成游戏。
- en: How long you trained the agent for will really determine how well it plays the
    game. If you let it complete the training, the agent should be equal to the already
    trained Unity agent.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练智能体的时间将真正决定它在游戏中的表现。如果让它完成训练，智能体的表现应当与已经训练好的Unity智能体相当。
- en: There are plenty of Unity samples that you can now run and explore on your own.
    Feel free to train several of the examples on your own or as listed in the exercises
    in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有很多Unity示例，您可以自行运行和探索。可以随意训练多个示例，或者按照下一节中的练习进行训练。
- en: Exercises
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Use the exercises in this section to enhance and reinforce your learning. Attempt
    at least a few of these exercises on your own, and remember this is really for
    your benefit:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本节中的练习来增强和巩固您的学习。至少尝试其中一些练习，记住这些练习对您有益：
- en: Set up and run the 3DBall example environment to train a working agent. This
    environment uses multiple games/agents to train.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行3DBall示例环境以训练一个有效的智能体。这个环境使用多个游戏/智能体来进行训练。
- en: Set the 3DBall example to let half of the games use an already trained brain
    and the other to use training or external learning.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置3DBall示例，让一半的游戏使用已经训练好的大脑，另一半使用训练或外部学习。
- en: Train the PushBlock environment agents using external learning.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用外部学习训练PushBlock环境中的智能体。
- en: Train the VisualPushBlock environment. Note how this example uses a visual camera
    to capture the environment state.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练VisualPushBlock环境。注意这个示例如何使用视觉摄像头捕捉环境状态。
- en: Run the Hallway scene as a player and then train the scene using an external
    learning brain.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为玩家运行Hallway场景，然后使用外部学习大脑训练该场景。
- en: Run the VisualHallway scene as a player and then train the scene using an external
    learning brain.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为玩家运行VisualHallway场景，然后使用外部学习大脑训练该场景。
- en: Run the WallJump scene and then run it under training conditions. This example
    uses Curriculum Training, which we will look at further in [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml), *Rewards
    and Reinforcement Learning*.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行WallJump场景，然后在训练条件下运行它。这个示例使用了课程训练，稍后我们将在[第9章](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml)中深入探讨，*奖励与强化学习*。
- en: Run the Pyramids scene and then set it up for training.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行金字塔场景，然后为训练进行设置。
- en: Run the VisualPyramids scene and set it up for training.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行VisualPyramids场景并为训练进行设置。
- en: Run the Bouncer scene and set it up for training.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Bouncer场景并为训练进行设置。
- en: While you don't have to run all these exercises/examples, it can be helpful
    to familiarize yourself with them. They can often be the basis for creating new
    environments, as we will see in the next chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您不必运行所有这些练习/示例，但熟悉它们是很有帮助的。它们往往可以作为创建新环境的基础，正如我们在下一章中所看到的那样。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As you have learned, the workflow for training RL and DRL agents in Unity is
    much more integrated and seamless than in OpenAI Gym. We didn't have to write
    a line of code to train an agent in a grid world environment, and the visuals
    are just plain better. For this chapter, we started by installing the ML-Agents
    toolkit. Then we loaded up a GridWorld environment and set it up to train with
    an RL agent. From there, we looked at TensorBoard for monitoring agent training
    and progress. After we were done training, we first loaded up a Unity pre-trained
    brain and ran that in the GridWorld environment. Then we used a brain we just
    trained and imported that into Unity as an asset and then as the GridWorldLearning
    brain's model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所学，Unity中训练RL和DRL代理的工作流比在OpenAI Gym中更加集成和无缝。我们不需要写一行代码就能在网格世界环境中训练代理，而且视觉效果要好得多。对于本章，我们从安装ML-Agents工具包开始。然后我们加载了一个GridWorld环境，并设置它与RL代理进行训练。从那时起，我们查看了TensorBoard来监控代理的训练和进展。在训练完成后，我们首先加载了一个Unity预训练的大脑，并在GridWorld环境中运行它。接着，我们使用了一个刚刚训练好的大脑，并将其作为资产导入到Unity中，作为GridWorldLearning大脑的模型。
- en: In the next chapter, we will explore how to construct a new RL environment or
    game we can use an agent to learn and play. This will allow us to peek under the
    covers further about the various details we skimmed over in this chapter.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何构建一个新的RL环境或游戏，我们可以使用代理来学习和玩耍。这将使我们进一步了解本章中略过的各种细节。
