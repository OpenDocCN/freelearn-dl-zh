- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Exploring Stable Diffusion XL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Stable Diffusion XL
- en: After the not-very-successful Stable Diffusion 2.0 and Stable Diffusion 2.1,
    July 2023 saw the launch of Stability AI’s latest release, **Stable Diffusion
    XL** (**SDXL**) [1]. I eagerly applied the model weights data as soon as registration
    was open. Both my tests and those conducted by the community indicate that SDXL
    has made significant strides forward. It now allows us to generate higher-quality
    images at increased resolutions, vastly outperforming the Stable Diffusion V1.5
    base model. Another notable enhancement is the ability to use more intuitive “natural
    language” prompts to generate images, eliminating the need to cobble together
    a multitude of “words” to form a meaningful prompt. Furthermore, we can now generate
    desired images with more concise prompts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在不太成功的 Stable Diffusion 2.0 和 Stable Diffusion 2.1 之后，2023 年 7 月 Stability AI
    发布了其最新版本，**Stable Diffusion XL**（**SDXL**）[1]。注册一开放，我就迫不及待地应用了模型权重数据。我的测试和社区进行的测试都表明，SDXL
    取得了显著的进步。它现在允许我们以更高的分辨率生成更高质量的图像，远远超过了 Stable Diffusion V1.5 基础模型。另一个显著的改进是能够使用更直观的“自然语言”提示来生成图像，消除了需要拼凑大量“单词”来形成一个有意义的提示的需求。此外，我们现在可以用更简洁的提示生成所需的图像。
- en: 'SDXL has improved in almost every aspect compared to the previous versions,
    and it is worth the time and effort to start using it for better and stable image
    generation. In this chapter, we will discuss in detail what’s new in SDXL and
    explain why the aforementioned changes led to its improvements. For example, we
    will explore what is new in the **Variational Autoencoder** (**VAE**), UNet, and
    TextEncoder design compared to Stable Diffusion V1.5\. In a nutshell, this chapter
    will cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的版本相比，SDXL 在几乎每个方面都有所改进，值得花费时间和精力开始使用它以实现更好的稳定图像生成。在本章中，我们将详细讨论 SDXL 的新特性，并解释上述变化为何导致了其改进。例如，我们将探讨与
    Stable Diffusion V1.5 相比，**变分自编码器**（**VAE**）、UNet 和 TextEncoder 设计中有什么新内容。简而言之，本章将涵盖以下内容：
- en: What’s new in SDXL?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDXL 的新特性是什么？
- en: Using SDXL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SDXL
- en: Then, we will use Python code to demonstrate the latest SDXL base and community
    models in action. We will cover basic usage and also advanced usage, such as loading
    multiple LoRA models and using unlimited weighted prompts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用 Python 代码演示最新的 SDXL 基础模型和社区模型在实际中的应用。我们将涵盖基本用法，以及高级用法，例如加载多个 LoRA 模型和使用无限加权提示。
- en: Let’s begin.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: What’s new in SDXL?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SDXL 的新特性是什么？
- en: 'SDXL is still a latent diffusion model, maintaining the same overall architecture
    used in Stable Diffusion v1.5\. According to the original paper behind SDXL [2],
    SDXL expands every component, making them wider and bigger. The SDXL backbone
    UNet is three times larger, there are two text encoders in the SDXL base model,
    and a separate diffusion-based refinement model is included. The overall architecture
    is shown in *Figure 16**.1*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL 仍然是一个潜在扩散模型，保持了与 Stable Diffusion v1.5 相同的整体架构。根据 SDXL 背后的原始论文 [2]，SDXL
    扩展了每个组件，使它们更宽、更大。SDXL 的骨干 UNet 大了三倍，SDXL 基础模型中有两个文本编码器，还包括了一个基于扩散的细化模型。整体架构如图
    *16.1* 所示：
- en: '![Figure 16.1: SDXL architecture](img/B21263_16_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.1：SDXL 架构](img/B21263_16_01.jpg)'
- en: 'Figure 16.1: SDXL architecture'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1：SDXL 架构
- en: Note that the refiner is optional; we can decide whether to use the refiner
    model or not. Next, let’s drill down to each component one by one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，细化器是可选的；我们可以决定是否使用细化器模型。接下来，让我们逐一深入到每个组件。
- en: The VAE of the SDXL
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SDXL 的 VAE
- en: A **VAE** is a pair of encoder and decoder neural networks. A VAE encoder encodes
    an image into a latent space, and its paired decoder can decode a latent image
    to a pixel image. Many articles on the web tell us that a VAE is a technique used
    to improve the quality of images; however, this is not the whole picture. The
    core responsibility of VAE in Stable Diffusion is converting pixel images to and
    from the latent space. Of course, a good VAE can improve the image quality by
    adding high-frequency details.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**VAE** 是一对编码器和解码器神经网络。VAE 编码器将图像编码到潜在空间中，其配对的解码器可以将潜在图像解码为像素图像。许多网络文章告诉我们
    VAE 是一种用于提高图像质量的技巧；然而，这并不是全部。在 Stable Diffusion 中，VAE 的核心责任是将像素图像转换为潜在空间，并将其从潜在空间转换回来。当然，一个好的
    VAE 可以通过添加高频细节来提高图像质量。'
- en: The VAE used in SDXL is a retrained one, using the same autoencoder architecture
    but with an increased batch size (256 versus 9) and, additionally, tracking the
    weights with an exponential moving average [2]. The new VAE outperforms the original
    model in all evaluated metrics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL中使用的VAE是一个重新训练的版本，它使用相同的自动编码器架构，但批量大小有所增加（256与9相比），并且还使用指数移动平均跟踪权重[2]。新的VAE在所有评估指标上都优于原始模型。
- en: 'Because of these implementation differences, instead of reusing the VAE code
    introduced in [*Chapter 5*](B21263_05.xhtml#_idTextAnchor097), we will need to
    write new code if we decide to use VAE independently. Here, we will provide an
    example of some common usage of the SDXL VAE:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些实现差异，如果我们决定独立使用VAE，而不是重用[*第5章*](B21263_05.xhtml#_idTextAnchor097)中引入的VAE代码，我们需要编写新的代码。在这里，我们将提供一个SDXL
    VAE的一些常见用法的示例：
- en: 'Initialize a VAE model:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个VAE模型：
- en: '[PRE0]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Encode an image using the VAE model. Before executing the following code, replace
    the `cat.png` file with a validated accessible image path:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用VAE模型编码图像。在执行以下代码之前，将`cat.png`文件替换为验证的可访问图像路径：
- en: '[PRE6]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Decode an image from latent space:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从潜在空间解码图像：
- en: '[PRE16]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding code, you first encode an image to latent space. An image in
    the latent space is invisible to our eyes, but it captures the features of an
    image in the latent space (in other words, in a high-dimensional vector space).
    Then, the decode part of the code decodes the image in the latent space to pixel
    space. From the preceding code, we know what the core functionality of a VAE is.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你首先将图像编码到潜在空间。潜在空间中的图像对我们来说是不可见的，但它捕捉了图像在潜在空间中的特征（换句话说，在高维向量空间中）。然后，代码的解码部分将潜在空间中的图像解码到像素空间。从前面的代码中，我们可以知道VAE的核心功能是什么。
- en: You might be curious as to why knowledge about the VAE is necessary. It has
    numerous applications. For instance, it allows you to save the generated latent
    image in a database and decode it only when needed. This method can reduce image
    storage by up to 90% without much loss of information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能好奇为什么了解VAE的知识是必要的。它有众多应用。例如，它允许你将生成的潜在图像保存在数据库中，仅在需要时解码。这种方法可以将图像存储减少多达90%，而信息损失很小。
- en: The UNet of SDXL
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SDXL的UNet
- en: The UNet model is the backbone neural network of SDXL. The UNet in SDXL is almost
    three times larger than the previous Stable Diffusion models. SDXL’s UNet is a
    2.6 GB billion parameter neural network, while the Stable Diffusion V1.5’s UNet
    has 860 million parameters. Although the current open source LLM model is much
    larger in terms of neural network size, SDXL’s UNet is, so far, the largest among
    those open source Diffusion models at the time of writing (October 2023), which
    directly leads to higher VRAM demands. 8 GB of VRAM can meet most of the use cases
    when using SD V1.5\. For SDXL, 15 GB of VRAM is commonly required; otherwise,
    we will need to reduce the image resolution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: UNet模型是SDXL的骨干神经网络。SDXL中的UNet比之前的Stable Diffusion模型大近三倍。SDXL的UNet是一个26GB的亿参数神经网络，而Stable
    Diffusion V1.5的UNet有8.6亿参数。尽管当前的开源LLM模型在神经网络大小方面要大得多，但截至写作时（2023年10月），SDXL的UNet是开源Diffusion模型中最大的，这直接导致了更高的VRAM需求。8GB的VRAM在大多数使用SD
    V1.5的情况下可以满足需求；对于SDXL，通常需要15GB的VRAM；否则，我们需要降低图像分辨率。
- en: Besides the model size expansion, SDXL rearranges its Transformer block’s position,
    which is crucial for better and more precise natural language-to-image guidance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型大小扩展外，SDXL还重新排列了其Transformer块的顺序，这对于更好的、更精确的自然语言到图像指导至关重要。
- en: Two text encoders in SDXL
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SDXL中的两个文本编码器
- en: One of the most significant changes in SDXL is the text encoder. SDXL uses two
    text encoders together, CLIP ViT-L [5] and OpenCLIP ViT-bigG (also named OpenCLIP
    G/14). Furthermore, SDXL uses pooled embeddings from OpenCLIP ViT-bigG.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL中最显著的变化之一是文本编码器。SDXL使用两个文本编码器一起，CLIP ViT-L [5] 和 OpenCLIP ViT-bigG（也称为OpenCLIP
    G/14）。此外，SDXL使用OpenCLIP ViT-bigG的池化嵌入。
- en: CLIP ViT-L is one of the most widely used models from OpenAI, which is also
    the text encoder or embedding model used in Stable Diffusion V1.5\. What is the
    OpenCLIP ViT-bigG model? OpenCLIP is an open source implementation of **CLIP**
    (**Contrastive Language-Image Pre-Training**). OpenCLIP G/14 is the largest and
    best OpenClip model trained on the LAION-2B dataset [9], a 100 TB dataset containing
    2 billion images. While the OpenAI CLIP model generates a 768-dimensional embedding
    vector, OpenClip G/14 outputs a 1,280-dimensional embedding. By concatenating
    the two embeddings (of the same length), a 2,048-dimensional embedding is output.
    This is much larger than the previous 768-dimensional embedding from Stable Diffusion
    v1.5.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP ViT-L是OpenAI最广泛使用的模型之一，也是Stable Diffusion V1.5中使用的文本编码器或嵌入模型。OpenCLIP ViT-bigG模型是什么？OpenCLIP是**CLIP**（**Contrastive
    Language-Image Pre-Training**）的开源实现。OpenCLIP G/14是在LAION-2B数据集[9]上训练的最大和最好的OpenClip模型，该数据集包含2亿张图片，总大小为100
    TB。虽然OpenAI CLIP模型生成一个768维度的嵌入向量，但OpenClip G/14输出一个1,280维度的嵌入。通过连接两个嵌入（长度相同），输出一个2,048维度的嵌入。这比Stable
    Diffusion v1.5之前的768维度嵌入大得多。
- en: 'To illustrate the text encoding process, let’s take the sentence `a running
    dog` as input; the ordinary text tokenizer will first convert the sentence into
    tokens, as shown in the following code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明文本编码过程，让我们以句子`a running dog`作为输入；普通的文本标记器首先将句子转换为标记，如下面的代码所示：
- en: '[PRE23]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code will return the following result:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将返回以下结果：
- en: '[PRE24]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding result, `49406` is the beginning token and `49407` is the end
    token.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的结果中，`49406`是起始标记，而`49407`是结束标记。
- en: 'Next, the following code uses the CLIP text encoder to convert the tokens into
    embedding vectors:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下面的代码使用CLIP文本编码器将标记转换为嵌入向量：
- en: '[PRE25]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The result embedding tensor includes five 768 dimension vectors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果嵌入张量包括五个768维度的向量：
- en: '[PRE26]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The previous code used OpenAI’s CLIP to convert the prompt text to 768-dimension
    embeddings. The following code uses the OpenClip G/14 model to encode the tokens
    into five 1,280-dimension embeddings:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用了OpenAI的CLIP将提示文本转换为768维度的嵌入。下面的代码使用OpenClip G/14模型将标记编码为五个1,280维度的嵌入：
- en: '[PRE27]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result embedding tensor includes five 1,280-dimension vectors:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果嵌入张量包括五个1,280维度的向量：
- en: '[PRE28]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, the next question is, what are **pooled embeddings**? Embedding pooling
    is the process of converting a sequence of tokens into one embedding vector. In
    other words, pooling embedding is a lossy compression of information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，下一个问题是，什么是**池化嵌入**？嵌入池化是将一系列标记转换为单个嵌入向量的过程。换句话说，池化嵌入是信息的有损压缩。
- en: 'Unlike the embedding process we used before, which encodes each token into
    an embedding vector, a pooled embedding is one vector that represents the whole
    input text. We can generate the pooled embedding from OpenClip using the following
    Python code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前使用的嵌入过程不同，该过程将每个标记编码为一个嵌入向量，池化嵌入是一个代表整个输入文本的向量。我们可以使用以下Python代码从OpenClip生成池化嵌入：
- en: '[PRE29]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The preceding code will return a `torch.Size([1, 1280])` pooled embedding vector
    from the text encoder. The maximum token size for a pooled embedding is `77`.
    In SDXL, the pooled embedding is provided to the UNet together with the token-level
    embedding from both CLIP and OpenCLIP, guiding the image generation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将从文本编码器返回一个`torch.Size([1, 1280])`的池化嵌入向量。池化嵌入的最大标记大小为`77`。在SDXL中，池化嵌入与来自CLIP和OpenCLIP的标记级别嵌入一起提供给UNet，以指导图像生成。
- en: Don’t worry – you won’t need to manually provide these embeddings before using
    SDXL. `StableDiffusionXLPipeline` from the `Diffusers` package does everything
    for us. All we need to do is provide the prompt and negative prompt text. We will
    provide the sample code in the *Using* *SDXL* section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——在使用SDXL之前，您不需要手动提供这些嵌入。`Diffusers`包中的`StableDiffusionXLPipeline`会为我们做所有事情。我们只需要提供提示和负面提示文本。我们将在*使用*
    *SDXL*部分提供示例代码。
- en: The two-stage design
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两阶段设计
- en: Another design addition in SDXL is its refiner model. According to the SDXL
    paper [2], the refiner model is used to enhance an image by adding more details
    and making it better, especially during the last 10 steps.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL的另一个设计增加是其精细模型。根据SDXL论文[2]，精细模型用于通过添加更多细节并使其更好来增强图像，尤其是在最后10步。
- en: The refiner model is just another image-to-image model that can help fix broken
    images and add more elements to the images generated by the base model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 精细模型只是另一个图像到图像的模型，可以帮助修复损坏的图像，并为基模型生成的图像添加更多元素。
- en: Based on my observations, for community-shared checkpoint models, the refiner
    model may not be necessary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的观察，对于社区共享的检查点模型，精炼器模型可能不是必需的。
- en: Next, we are going to use SDXL for common use cases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用SDXL进行常见用例。
- en: Using SDXL
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SDXL
- en: We briefly covered loading the SDXL model in [*Chapter 6*](B21263_06.xhtml#_idTextAnchor117)
    and SDXL ControlNet usage in [*Chapter 13*](B21263_13.xhtml#_idTextAnchor257).
    You can find the sample codes there. In this section, we will cover more common
    SDXL usages, including loading community-shared SDXL models and how to use the
    image-to-image pipeline to enhance the model, using SDXL with community-shared
    LoRA models, and the unlimited length prompt pipeline from Diffuser (provided
    by the author of this book).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第6章*](B21263_06.xhtml#_idTextAnchor117)中简要介绍了SDXL模型的加载，在[*第13章*](B21263_13.xhtml#_idTextAnchor257)中介绍了SDXL
    ControlNet的使用。您可以在那里找到示例代码。在本节中，我们将介绍更多常见的SDXL用法，包括加载社区共享的SDXL模型以及如何使用图像到图像的管道来增强模型，使用SDXL与社区共享的LoRA模型，以及来自本书作者的Diffuser的无限制长度提示管道。
- en: Use SDXL community models
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SDXL社区模型
- en: Just months after the release of SDXL, the open source community has released
    countless fine-tuned SDXL models based on the base model from Stability AI. We
    can find these models on Hugging Face and CIVITAI ([https://civitai.com/](https://civitai.com/)),
    and the number keeps growing.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在SDXL发布几个月后，开源社区就基于Stability AI的基础模型发布了无数经过微调的SDXL模型。我们可以在Hugging Face和CIVITAI([https://civitai.com/](https://civitai.com/))上找到这些模型，数量还在不断增加。
- en: 'Here, let’s load one model from HuggingFace, using the SDXL model ID:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从HuggingFace加载一个模型，使用SDXL模型ID：
- en: '[PRE30]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that in the preceding code, `base_pipe.watermark = None` will remove the
    invisible watermark from the generated image.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的代码中，`base_pipe.watermark = None`将移除生成的图像中的不可见水印。
- en: 'Next, move the model to CUDA, generate an image, and then offload the model
    from CUDA:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将模型移动到CUDA，生成图像，然后从CUDA卸载模型：
- en: '[PRE31]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With just one line prompt and not needing to provide any negative prompt, SDXL
    generates an amazing image, as shown in *Figure 16**.2:*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 仅需一行提示，无需提供任何负面提示，SDXL就能生成令人惊叹的图像，如图*图16**.2*所示：
- en: '![Figure 16.2: A cat pilot generated by SDXL](img/B21263_16_02.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2：由SDXL生成的猫飞行员](img/B21263_16_02.jpg)'
- en: 'Figure 16.2: A cat pilot generated by SDXL'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：由SDXL生成的猫飞行员
- en: You may want to use the refiner model to enhance the image, but the refiner
    model doesn’t make a significant difference. Instead, we will use the image-to-image
    pipeline with the same model data to upscale the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想使用精炼器模型来增强图像，但精炼器模型并没有带来显著的变化。相反，我们将使用具有相同模型数据的图像到图像管道来提高图像的分辨率。
- en: Using SDXL image-to-image to enhance an image
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SDXL图像到图像增强图像
- en: 'Let’s first upscale the image to twice:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先提高图像到两倍：
- en: '[PRE32]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, start an image-to-image pipeline by reusing the model data from the previous
    text-to-image pipeline, saving the RAM and VRAM usage:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过重用前一个文本到图像管道中的模型数据来启动图像到图像管道，从而节省RAM和VRAM的使用：
- en: '[PRE33]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, it is time to call the pipeline to further enhance the image:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候调用管道来进一步增强图像：
- en: '[PRE34]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that we set the strength to `0.3` to preserve most of the original input
    image information. We will get a new, better image, as shown in *Figure 16**.3*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将强度设置为`0.3`以保留大部分原始输入图像信息。我们将得到一个新、更好的图像，如图*图16**.3*所示：
- en: '![Figure 16.3: The refined cat pilot image from an image-to-image pipeline](img/B21263_16_03.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图16.3：图像到图像管道中精炼的猫飞行员图像](img/B21263_16_03.jpg)'
- en: 'Figure 16.3: The refined cat pilot image from an image-to-image pipeline'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：图像到图像管道中精炼的猫飞行员图像
- en: While you might not notice many differences in this book at first glance, upon
    closer inspection of the image on a computer monitor, you will discover numerous
    additional details.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您可能一开始在书中看不到太多差异，但仔细检查计算机屏幕上的图像，您会发现许多额外的细节。
- en: Now, let’s explore how to utilize LoRA with Diffusers. If you’re unfamiliar
    with LoRA, I recommend turning back to [*Chapter 8*](B21263_08.xhtml#_idTextAnchor153),
    which delves into the usage of Stable Diffusion LoRA in greater detail, and [*Chapter
    21*](B21263_21.xhtml#_idTextAnchor405), which provides comprehensive coverage
    of LoRA training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨如何利用LoRA与Diffusers结合使用。如果您对LoRA不熟悉，我建议您回顾[*第8章*](B21263_08.xhtml#_idTextAnchor153)，其中更详细地介绍了Stable
    Diffusion LoRA的用法，以及[*第21章*](B21263_21.xhtml#_idTextAnchor405)，其中全面介绍了LoRA的训练。
- en: Using SDXL LoRA models
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SDXL LoRA模型
- en: Not long ago, it was impossible to load LoRA using Diffusers, not to mention
    loading multiple LoRA models into one pipeline. With the massive work that has
    been done by the Diffusers team and community contributors, we can now load multiple
    LoRA models into the SDXL pipeline with the LoRA scale number specified.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，使用Diffusers加载LoRA是不可能的，更不用说将多个LoRA模型加载到一个管道中。随着Diffusers团队和社区贡献者的巨大工作，我们现在可以将多个LoRA模型加载到SDXL管道中，并指定LoRA缩放数值。
- en: 'And its usage is also extremely simple. It takes just two lines of code to
    add one LoRA to the pipeline:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其使用也非常简单。只需两行代码即可将一个LoRA添加到管道中：
- en: '[PRE35]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To add two LoRA models:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加两个LoRA模型：
- en: '[PRE36]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As we discussed in [*Chapter 8*](B21263_08.xhtml#_idTextAnchor153), there are
    two ways to use LoRA – one is merging with the backbone model weights, and the
    other is dynamic monkey patching. Here, for SDXL, the method is model merging,
    which means unloading a LoRA from the pipeline. To unload a LoRA model, we will
    need to load the LoRA again but with a negative `lora_scale`. For example, if
    we want to unload `lora2.safetensors` from the pipeline, we can use the following
    code to achieve it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[*第8章*](B21263_08.xhtml#_idTextAnchor153)中讨论的，使用LoRA有两种方式——一种是将它与骨干模型权重合并，另一种是动态猴子补丁。在这里，对于SDXL，方法是模型合并，这意味着从管道中卸载一个LoRA。为了卸载一个LoRA模型，我们需要再次加载LoRA，但带有负的`lora_scale`。例如，如果我们想从管道中卸载`lora2.safetensors`，我们可以使用以下代码来实现：
- en: '[PRE37]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Besides using `fuse_lora` to load a LoRA model, we can also use PEFT-integrated
    LoRA loading. The code is very similar to the one we just used, but we add one
    more parameter called `adapter_name`, like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`fuse_lora`加载LoRA模型外，我们还可以使用PEFT集成LoRA加载。代码与我们刚刚使用的代码非常相似，但我们添加了一个名为`adapter_name`的额外参数，如下所示：
- en: '[PRE38]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can adjust the LoRA scale dynamically with the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码动态调整LoRA缩放：
- en: '[PRE39]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'And we can also disable LoRA as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以禁用LoRA，如下所示：
- en: '[PRE40]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Alternatively, we can disable one of the two loaded LoRA models, like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以禁用两个加载的LoRA模型中的一个，如下所示：
- en: '[PRE41]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the preceding code, we disabled `lora1` while continuing to use `lora2`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们禁用了`lora1`，同时继续使用`lora2`。
- en: With proper LoRA management code, you can use SDXL with an unlimited number
    of LoRA models. Speaking of “unlimited,” next, we will cover the “unlimited” length
    prompt for SDXL.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当的LoRA管理代码，你可以使用SDXL和无限数量的LoRA模型。说到“无限”，接下来，我们将介绍SDXL的“无限”长度提示词。
- en: Using SDXL with an unlimited prompt
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SDXL和无限制提示词
- en: By default, SDXL, like previous versions, supports only a maximum of 77 tokens
    for one-time image generation. In [*Chapter 10*](B21263_10.xhtml#_idTextAnchor197),
    we delved deep into implementing a text embedding encoder that supports weighted
    prompts without length limitation. For SDXL, the idea is similar but more complex
    and a bit harder to implement; after all, there are now two text encoders.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SDXL，像之前的版本一样，一次图像生成只支持最多77个token。在[*第10章*](B21263_10.xhtml#_idTextAnchor197)中，我们深入探讨了实现支持无长度限制的加权提示词的文本嵌入编码器。对于SDXL，想法类似但更复杂，实现起来也稍微困难一些；毕竟，现在有两个文本编码器。
- en: I built a long-weighted SDXL pipeline, `lpw_stable_diffusion_xl`, which is merged
    with the official `Diffusers` package. In this section, I will introduce the usage
    of this pipeline to enable a long-weighted and unlimited pipeline.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我构建了一个长权重SDXL管道，`lpw_stable_diffusion_xl`，它与官方的`Diffusers`包合并。在本节中，我将介绍如何使用此管道以长权重和无限制的方式启用管道。
- en: 'Make sure you have updated your `Diffusers` package to the latest version with
    the following command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已使用以下命令更新了您的`Diffusers`包到最新版本：
- en: '[PRE42]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, use the following code to use the pipeline:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下代码使用管道：
- en: '[PRE43]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The preceding code uses `DiffusionPipeline` to load a custom pipeline, `lpw_stable_diffusion_xl`,
    contributed by an open source community member (i.e., me).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用`DiffusionPipeline`加载一个由开源社区成员（即我）贡献的自定义管道`lpw_stable_diffusion_xl`。
- en: Note that in the code, the prompt is multiplied by 2, making it definitely longer
    than 77 tokens. At the end of the prompt, `a (cute cat:1.5) aside` is appended.
    If the pipeline supports prompts longer than 77 tokens, there should be a cat
    in the generated result.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在代码中，提示词被乘以2，使其长度肯定超过77个token。在提示词的末尾，附加了`a (cute cat:1.5) aside`。如果管道支持超过77个token的提示词，生成的结果中应该有一只猫。
- en: 'The image generated from the preceding code is shown in *Figure 16**.4*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成的图像显示在*图16**.4*中：
- en: '![Figure 16.4: A man with a cat, generated using an unlimited prompt length
    pipeline – lpw_stable_diffusion_xl](img/B21263_16_04.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图16.4：使用无限提示长度管道生成的一男一猫，lpw_stable_diffusion_xl](img/B21263_16_04.jpg)'
- en: 'Figure 16.4: A man with a cat, generated using an unlimited prompt length pipeline
    – lpw_stable_diffusion_xl'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：使用无限提示长度管道生成的一男一猫，lpw_stable_diffusion_xl
- en: From the image, we can see that all elements in the prompt are reflected, and
    there is now a cute cat sitting alongside the man.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中，我们可以看到提示中的所有元素都得到了反映，现在有一只可爱的小猫坐在男人的旁边。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covers the newest and best Stable Diffusion model – SDXL. We first
    introduced the basics of SDXL and why it is powerful and efficient, and then we
    drilled down into each component of the newly released model, covering VAE, UNet,
    text encoders, and the new two-stage design.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了最新和最好的稳定扩散模型——SDXL。我们首先介绍了SDXL的基本知识以及为什么它强大且高效，然后深入研究了新发布模型的每个组件，包括VAE、UNet、文本编码器和新的两阶段设计。
- en: We provided a sample code for each of the components to help you understand
    SDXL inside out. These code samples can also be used to leverage the power of
    the individual components. For example, we can use VAE to compress images and
    a text encoder to generate text embeddings for images.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个组件提供了示例代码，以帮助您深入了解SDXL。这些代码示例也可以用来利用各个组件的力量。例如，我们可以使用VAE来压缩图像，并使用文本编码器为图像生成文本嵌入。
- en: In the second half of this chapter, we covered some common use cases of SDXL,
    such as loading community-shared checkpoint models, using the image-to-image pipeline
    to enhance and upscale images, and introducing a simple and effective solution
    to load multiple LoRA models into one pipeline. Finally, we provided an end-to-end
    solution to use unlimited length-weighted prompts for SDXL.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们介绍了SDXL的一些常见用例，例如加载社区共享的检查点模型，使用图像到图像的管道增强和放大图像，以及介绍一种简单有效的解决方案来将多个LoRA模型加载到一个管道中。最后，我们提供了一个端到端解决方案，用于使用无限长度的加权提示来使用SDXL。
- en: With the help of SDXL, we can generate amazing images with short prompts and
    achieve much better results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在SDXL的帮助下，我们可以用简短的提示生成令人惊叹的图像，并实现更好的结果。
- en: In the next chapter, we are going to discuss how to write Stable Diffusion prompts
    and leverage LLM to help produce and enhance prompts automatically.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何编写稳定扩散提示，并利用LLM自动生成和增强提示。
- en: References
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'SDXL: [https://stability.ai/stable-diffusion](https://stability.ai/stable-diffusion)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SDXL：[https://stability.ai/stable-diffusion](https://stability.ai/stable-diffusion)
- en: 'SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis:
    [https://arxiv.org/abs/2307.01952](https://arxiv.org/abs/2307.01952)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SDXL：改进潜在扩散模型以实现高分辨率图像合成：[https://arxiv.org/abs/2307.01952](https://arxiv.org/abs/2307.01952)
- en: 'Stable Diffusion XL Diffusers: [https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稳定扩散XL扩散器：[https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl)
- en: 'CLIP from OpenAI: [https://openai.com/research/clip](https://openai.com/research/clip)'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'CLIP from OpenAI: [https://openai.com/research/clip](https://openai.com/research/clip)'
- en: 'CLIP VIT Large model: [https://huggingface.co/openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CLIP VIT Large模型：[https://huggingface.co/openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
- en: 'REACHING 80% ZERO-SHOT ACCURACY WITH OPENCLIP: VIT-G/14 TRAINED ON LAION-2B:
    [https://laion.ai/blog/giant-openclip/](https://laion.ai/blog/giant-openclip/)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用OPENCLIP达到80%的无监督准确率：在LAION-2B上训练的VIT-G/14：[https://laion.ai/blog/giant-openclip/](https://laion.ai/blog/giant-openclip/)
- en: 'CLIP-ViT-bigG-14-laion2B-39B-b160k: [https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CLIP-ViT-bigG-14-laion2B-39B-b160k：[https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
- en: 'OpenCLIP GitHub repository: [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenCLIP GitHub仓库：[https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)
- en: 'LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS: [https://laion.ai/blog/laion-5b/](https://laion.ai/blog/laion-5b/)'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LAION-5B：开放大规模多模态数据集的新时代：[https://laion.ai/blog/laion-5b/](https://laion.ai/blog/laion-5b/)
