- en: Neural Networks and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习
- en: Neural networks are the main machine learning models that we will be looking
    at in this book. Their applications are countless, as are their application fields.
    These range from computer vision applications (where an object should be localized
    in an image), to finance (where neural networks are applied to detect frauds),
    passing trough trading, to reaching even the art field, where neural networks
    are used together with the adversarial training process to create models that
    are able to generate new and unseen kinds of art with astonishing results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是本书中我们将深入探讨的主要机器学习模型。它们的应用领域无数，涵盖了从计算机视觉（例如，图像中的物体定位），金融（例如，神经网络用于检测欺诈行为），交易，到甚至艺术领域（神经网络与对抗训练过程结合，能够生成新颖且未见过的艺术作品，取得惊人的成果）。
- en: This chapter, which is perhaps the richest in terms of theory in this whole
    book, shows you how to define neural networks and how to make them learn. To begin,
    the mathematical formula for artificial neurons will be presented, and we will
    highlight why a neuron must have certain features to be able to learn. After that,
    fully connected and convolutional neuronal topologies will be explained in detail
    since these are the building blocks of almost every neural network architecture.
    At the same time, the concept of deep learning and deep architectures will be
    introduced. Introducing this concept is a must since it is because of deep architectures
    that, nowadays, neural networks are used to solve challenging problems with super-human
    performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章可能是全书中最具理论性的部分，将展示如何定义神经网络及如何使其学习。首先，我们将介绍人工神经元的数学公式，并强调神经元必须具备哪些特征才能学习。接下来，将详细解释全连接和卷积神经网络拓扑结构，因为这些是几乎所有神经网络架构的构建模块。同时，我们将介绍深度学习和深度架构的概念。引入这一概念是必不可少的，因为正是深度架构让神经网络如今能够解决具有超人表现的挑战性问题。
- en: To conclude, the optimization process that's required to train a parametric
    model, together with some regularization techniques that are used to improve the
    model's performance, will be shown. Gradient descent, the chain rule, and the
    graphical representation of the computations all have their own dedicated sections
    since it is extremely important for any machine learning practitioner to know
    what happens when a framework is used to train a model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将展示训练一个参数化模型所需的优化过程，并结合一些正则化技术来提升模型的表现。梯度下降、链式法则和计算图的表示都将单独讲解，因为对任何机器学习实践者来说，了解在框架训练模型时发生的事情是至关重要的。
- en: If you are already familiar with the concepts presented in this chapter, you
    can jump directly to the next chapter, [Chapter 3](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml), *TensorFlow
    Graph Architecture*, which is dedicated to the TensorFlow graph architecture.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉本章所介绍的概念，可以直接跳到下一章，[第3章](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml)，*TensorFlow
    图架构*，本章专门讲解 TensorFlow 的图架构。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化
- en: Convolutional neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Regularization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: Neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'The definition of a neural network, as provided by the inventor of one of the
    first neurocomputers, *Dr. Robert Hecht-Nielson,* in *Neural Network Primer—Part
    I*, is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的定义来自于最早期神经计算机之一的发明者*罗伯特·赫希特-尼尔森博士*，他在《神经网络入门——第一部分》中这样描述：
- en: '"A computing system made up of a number of simple, highly interconnected processing
    elements, which process information by their dynamic state response to external
    inputs."'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “一个由许多简单且高度互联的处理单元组成的计算系统，这些单元通过对外部输入的动态状态响应来处理信息。”
- en: In practice, we can think of artificial neural networks as a computational model
    that is based on how the brain is believed to work. Hence, the mathematical model
    is inspired by biological neurons.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以将人工神经网络视为一种计算模型，它基于大脑的工作原理。因此，数学模型受生物神经元的启发。
- en: Biological neurons
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物神经元
- en: 'The main computational units of the brain are known as neurons; in the human
    nervous system, approximately 86 billion neurons can be found, all of which are
    connected by synapses. The following diagram shows a biological neuron and the
    mathematical model that draws inspiration from it:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑的主要计算单元被称为神经元；在人类神经系统中，大约有860亿个神经元，它们通过突触互相连接。以下图示展示了生物神经元及其数学模型：
- en: '![](img/a05f5586-0ad6-4a20-8935-4c44a2176e6f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a05f5586-0ad6-4a20-8935-4c44a2176e6f.png)'
- en: 'Representation of the biological neuron, (a), on the left and its mathematical
    model, (b), on the right. Source: Stanford cs231n'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的表示，左图为（a），右图为其数学模型（b）。来源：斯坦福 cs231n
- en: 'Biological neurons are made up of the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元由以下部分组成：
- en: '**Dendrites**: Minor fibers that carry information, in the form of an electric
    signal, from the outside to the nucleus.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树突**：细小的纤维，以电信号的形式将信息从外部传递到细胞核。'
- en: '**Synapses**: These are the connection points among neurons. Neurons receive
    input signals on the synapses that are connected to the dendrites.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突触**：这些是神经元之间的连接点。神经元通过连接到树突的突触接收输入信号。'
- en: '**Nucleus**: This receives the signals from the dendrites, elaborates on them,
    and produces a response (output signal) that it sends to the axon.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细胞核**：接收来自树突的信号，对其进行处理，并产生响应（输出信号），然后将其发送到轴突。'
- en: '**Axon**: The output channel of the neuron. It can be connected to other neuron synapses.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突**：神经元的输出通道。它可以连接到其他神经元的突触。'
- en: Each neuron receives input signals from its dendrites and transports them to
    the nucleus where they are processed; dendrites process the signals, thereby integrating
    (adding up or combining) excitation and inhibition from every input synapse. The
    nucleus receives the integrated signals and adds them. If the final sum is above
    a certain threshold, the neuron fires and the resulting information is carried
    down through the axon and thus to any other connected neuron.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元从其树突接收输入信号，并将其传输到细胞核进行处理；树突对信号进行处理，从而整合（相加或合并）来自每个输入突触的兴奋与抑制。细胞核接收这些综合信号并将它们相加。如果最终的总和超过某个阈值，神经元就会发放，并将结果信息通过轴突传递到其他连接的神经元。
- en: The amount of signal that's transmitted among neurons depends on the strength
    of the connections. It is the arrangement of the neurons and the strength of these
    synapses that establish the function of the neural network.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经元之间传递的信号量取决于连接的强度。正是神经元的排列和这些突触的强度决定了神经网络的功能。
- en: The learning phase of biological neurons is based on the modification of the
    output signal generated by the nucleus over time, as a function of certain types
    of input signals. Neurons specialize themselves in recognizing certain stimuli
    during their lifetime.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的学习阶段是基于细胞核生成的输出信号随时间的变化，作为某些类型输入信号的函数。神经元在其生命周期内会专门化，识别特定的刺激。
- en: Artificial neurons
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'Artificial neurons are based on the structure of the biological neuron and
    use mathematical functions with real values to simulate their behavior. Such artificial
    neurons are called **perceptrons**, a concept that was developed in the 50s and
    60s by the scientist Frank Rosenblatt. Taking this mathematical analogy into account,
    we can talk about the biological neurons as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元基于生物神经元的结构，使用带有实数值的数学函数来模拟其行为。这种人工神经元被称为**感知器**，这一概念是由科学家弗兰克·罗森布拉特在20世纪50到60年代提出的。考虑到这一数学类比，我们可以这样描述生物神经元：
- en: '**Dendrites**: The number of inputs the neuron accepts. It can also be seen
    as the number of dimensions, *D*, of the input data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树突**：神经元接收输入的数量。它也可以看作是输入数据的维度数，*D*。'
- en: '**Synapses**:  ![](img/c63ff61c-545a-41e4-b0ec-57332f0d83b3.png)  weights associated
    with the dendrites. These are the values that change during the training phase.
    At the end of the training phase, we say the neuron is specialized (it learned
    to extract particular features from the input).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突触**： ![](img/c63ff61c-545a-41e4-b0ec-57332f0d83b3.png) 与树突相关的权重。这些值在训练阶段会发生变化。在训练阶段结束时，我们说神经元已经专门化（它学会了从输入中提取特定的特征）。'
- en: If ![](img/747cd994-4daa-4673-940e-a76a4d293266.png) is a D-dimensional input
    vector, the operation that's executed by the synapses is ![](img/e7a6566e-2a4d-4bb1-892c-93e979c5ef86.png)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](img/747cd994-4daa-4673-940e-a76a4d293266.png) 是一个D维输入向量，那么突触执行的操作是 ![](img/e7a6566e-2a4d-4bb1-892c-93e979c5ef86.png)
- en: '**Nucleus** (body cell): This is a function that bonds the values coming from
    the synapses, thereby defining the behavior of the neuron. To simulate the action
    of the biological neuron, that is, firing (activating) only when there are certain
    stimuli in the input, the nucleus is modeled with **non-linear** functions.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心体**（细胞体）：这是一个将来自突触的值结合起来的函数，从而定义神经元的行为。为了模拟生物神经元的动作，即只有在输入中有特定刺激时才会激活（发射），核心体被建模为**非线性**函数。'
- en: 'If ![](img/e49a8892-74d1-462d-b219-ead61cd07dd1.png) is a non-linear function,
    the output of the neuron, which takes into account all input stimuli, is given
    by the following equation:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果![](img/e49a8892-74d1-462d-b219-ead61cd07dd1.png)是一个非线性函数，那么神经元的输出，即考虑到所有输入刺激的结果，由以下方程给出：
- en: '![](img/c5c1b918-f08f-4320-9f07-991087923360.png).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/c5c1b918-f08f-4320-9f07-991087923360.png).'
- en: Here, ![](img/5b69a829-f2ab-4c5a-a836-2c82601b0cf4.png) is the **bias term**,
    which is of fundamental importance. It allows you to learn about a decision boundary
    that's not centered on the origin of the D-dimensional space.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，![](img/5b69a829-f2ab-4c5a-a836-2c82601b0cf4.png)是**偏置项**，它具有基础性的重要性。它允许你学习一个不以D维空间原点为中心的决策边界。
- en: 'If we remove the non-linear (also called **activation**) function for a moment,
    we can easily see that the synapses define a hyper-plane with the following equation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们暂时去除非线性（也称为**激活**）函数，我们可以很容易地看到，突触定义了一个具有以下方程的超平面：
- en: '[![](img/7a2788a7-db86-43dc-8480-78a4609c1c88.png)].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/7a2788a7-db86-43dc-8480-78a4609c1c88.png)].'
- en: A single neuron is able to perform *only* binary classification because the
    D-dimensional vector, ![](img/a99782dc-877b-4629-b277-f9ee1ba29535.png), can just
    be over or under the hyperplane it defines.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元只能执行*二元*分类，因为D维向量![](img/a99782dc-877b-4629-b277-f9ee1ba29535.png)只能位于它定义的超平面之上或之下。
- en: A perceptron can correctly classify samples in a D-dimensional space if—and
    only if—those samples are linearly separable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果—且仅当—样本是线性可分的，那么感知机能够在D维空间中正确分类样本。
- en: The nucleus, with its non-linearity, maps the hyperplane defined by the dendrites in
    a more general hypersurface, which is the learned decision boundary. Non-linearity,
    in the best-case scenario, transforms the hyperplane into a hypersurface that's
    able to correctly classify points in a D-dimensional space. However, it only does
    this if those points are separable in two regions by a single hypersurface.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 核心体通过其非线性特性，将由树突定义的超平面映射到更一般的超曲面上，这就是学习到的决策边界。在最理想的情况下，非线性将超平面转化为一个超曲面，该超曲面能够正确分类D维空间中的点。然而，只有在这些点可以通过单一超曲面在两个区域之间分离时，它才会这样做。
- en: 'This is the main reason we need multi-layer neural networks: if the input data
    is not separable by a single hypersurface, adding another layer on top that works
    by transforming the learned hypersurface into a new hypersurface with an additional
    classification region allows it to learn complex classification boundaries that
    are capable of separating the regions correctly.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要多层神经网络的主要原因：如果输入数据无法通过单个超平面分离，添加另一层，通过将学习到的超平面转化为一个带有额外分类区域的新超平面，使其能够学习到复杂的分类边界，从而正确分离区域。
- en: Moreover, it is worth noting that feed-forward neural networks, such as neural
    networks with connections among neurons that do not form a cycle, are universal
    function approximators. This means that, if a way to separate regions exists,
    a well-trained neural network with enough capacity will learn to approximate that
    function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得注意的是，前馈神经网络，例如那些神经元之间存在连接但不形成循环的神经网络，是通用的函数近似器。这意味着，如果存在一种分离区域的方法，经过充分训练、具有足够容量的神经网络将能够学习并近似该函数。
- en: '**Axon**: This is the output value of the neuron. It can be used as input by
    other neurons.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突**：这是神经元的输出值，其他神经元可以将其作为输入。'
- en: 'It''s important to stress that this model of a biological neuron is very coarse:
    for example, there are many different types of neuron, each with different properties.
    The dendrites in biological neurons perform complex nonlinear computations. The
    synapses are not just a single weight; they are a complex non-linear dynamical
    system. There are many other simplifications in the model because the reality
    is way more complicated and tougher to model than this. Hence, this biological
    inspiration is just a nice way to think about neural networks, but don''t be fooled
    by all of these similarities: artificial neural networks are only loosely inspired
    by biological neurons.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，这种生物神经元模型是非常粗略的：例如，神经元有许多不同的类型，每种类型都有不同的特性。生物神经元的树突执行复杂的非线性计算。突触不仅仅是一个单一的权重；它们是一个复杂的非线性动态系统。该模型还有许多其他简化之处，因为现实要比这个模型复杂得多，且更难以建模。因此，这种生物学上的灵感仅仅是一种理解神经网络的方式，但不要被这些相似之处所迷惑：人工神经网络仅仅是受到生物神经元的松散启发。
- en: '*"Why we should use neural networks and not other machine learning models?"*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我们为什么要使用神经网络而不是其他机器学习模型？”*'
- en: Traditional machine learning models are powerful but usually not as flexible as
    neural networks. Neural networks can be arranged in different topologies, and
    the geometry changes what the neural networks see (the input stimuli). Moreover,
    it's straightforward to create layers upon layers of neural networks with different
    topologies, creating deep models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习模型很强大，但通常不如神经网络那样灵活。神经网络可以以不同的拓扑结构排列，几何形状改变了神经网络所看到的内容（输入刺激）。此外，创建不同拓扑结构的神经网络层叠层非常简单，从而创建出**深度模型**。
- en: 'One of the greatest strengths of neural networks is their ability to become
    feature extractors: other machine learning models need the input data to be processed,
    have their meaningful features extracted, and only on those features (manually
    defined!) can the model be applied.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个最大优点是其成为特征提取器的能力：其他机器学习模型需要对输入数据进行处理、提取有意义的特征，并且只能基于这些特征（手动定义的！）应用模型。
- en: Neural networks, on the other hand, can extract meaningful features from any
    input data by themselves (depending on the topology of the layers that are used).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，神经网络可以通过自身从任何输入数据中提取有意义的特征（这取决于所使用层的拓扑结构）。
- en: The single perceptron illustrates how it is possible to weigh and add different
    types of input to make a simple decision; a complex network of perceptrons could
    make a quite subtle decision. A **neural network architecture**, therefore, is
    made up of neurons, all of which are connected through synapses (biologically)
    where the information flows through them. During training, the neurons fire when
    they learn specific patterns from the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 单个感知机展示了如何对不同类型的输入进行加权和求和，从而做出简单的决策；而感知机的复杂网络则可以做出相当微妙的决策。因此，**神经网络架构**由神经元组成，所有神经元通过突触（生物学上）相连，信息通过这些突触流动。在训练过程中，神经元会在学习到数据中的特定模式时触发。
- en: This fire rate is modeled using an activation function. More precisely, the
    neurons are connected in an acyclic graph; c
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这一触发率是通过激活函数来建模的。更准确地说，神经元是以无环图的形式连接的；
- en: ycles are not allowed since that would imply an infinite loop in the forward
    pass of the network (these types of networks are called **feed-forward neural
    networks**). Instead of amorphous blobs of connected neurons, neural network models
    are often organized into distinct layers of neurons. The most common layer type
    is the fully connected layer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 循环是不允许的，因为这会导致网络前向传递中的无限循环（这类网络被称为**前馈神经网络**）。神经网络模型通常将神经元组织成不同的层，而不是将神经元简单地连接成无规则的团块。最常见的层类型是全连接层。
- en: Fully connected layers
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: The fully connected configuration is a particular network topology in which
    neurons between two adjacent layers are fully pairwise-connected, but neurons
    within a single layer share no connections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接配置是一种特定的网络拓扑，其中两个相邻层之间的神经元是完全成对连接的，但单层内的神经元之间没有任何连接。
- en: 'Organizing networks into layers allows us to create stacks of fully connected
    layers, with a different number of neurons per layer. We can think about a multi-layer
    neural network as a model with visible and hidden layers. The visible layers are
    just the input and output layers; the hidden layers are the ones that aren''t
    connected to the outside:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将网络组织成层可以让我们创建由多个完全连接的层组成的堆叠，每层中神经元的数量不同。我们可以将多层神经网络看作一个包含可见层和隐藏层的模型。可见层仅指输入层和输出层；隐藏层是那些没有与外部连接的层：
- en: '![](img/ef4794f3-77ae-4d57-a534-cc05481441aa.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef4794f3-77ae-4d57-a534-cc05481441aa.png)'
- en: A typical representation of a fully connected neural network, with two hidden
    layers. Every layer reduces the dimensionality of its input with the aim of producing
    two different outputs given the ten input features.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个典型的完全连接神经网络的表示，包含两个隐藏层。每一层都减少其输入的维度，目的是根据十个输入特征生成两个不同的输出。
- en: The number of neurons in the hidden layers is entirely arbitrary, and it changes
    the learning capacity of the network. The input and output layers, instead, have
    a fixed dimension due to the task we are going to solve (for example, if we want
    to solve an *n*-classes classification on D-dimensional inputs, then we need an
    input layer with D inputs and an output layer with n outputs).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中神经元的数量是完全任意的，并且它改变网络的学习能力。输入层和输出层则有固定的维度，这是由我们要解决的任务决定的（例如，如果我们要解决一个*n*类分类问题，且输入维度为D，那么我们需要一个具有D个输入的输入层和一个具有n个输出的输出层）。
- en: 'Mathematically, it is possible to define the output of a fully connected layer
    as the result of a matrix product. Let''s say we have the following equation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，可以将完全连接层的输出定义为矩阵乘积的结果。假设我们有以下方程：
- en: '![](img/d2192c7e-2156-49c3-a22d-842fd589f073.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2192c7e-2156-49c3-a22d-842fd589f073.png)'
- en: 'The output, *O*, is given by the following formula:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 *O* 由以下公式给出：
- en: '![](img/0f1acf9a-a09f-43f9-ad9e-293314aab109.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f1acf9a-a09f-43f9-ad9e-293314aab109.png)'
- en: Here, M is the arbitrary number of neurons in the layer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，M是该层中神经元的任意数量。
- en: While the design of the input and output layers of a neural network is straightforward,
    the design of the hidden layers is not so simple. There are no rules; neural networks
    researchers have developed many design heuristics for hidden layers which help
    to get the correct behavior (for example, when there's a trade-off between the
    number of hidden layers and the time to train the network).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然神经网络的输入层和输出层设计较为简单，但隐藏层的设计并不那么简单。没有固定的规则；神经网络研究者为隐藏层开发了许多设计启发式方法，帮助获得正确的行为（例如，当隐藏层数量与训练网络的时间之间存在权衡时）。
- en: In general, increasing the number of neurons per layer and/or the number of
    layers in a neural network means having to increase the network capacity. This
    means that the neural network can express more complicated functions and that
    the space of representable functions grows; however, this is good and bad at the
    same time. It's good because we can learn more complicated functions, but it's
    bad because having more trainable parameters increases the risk of overfitting
    the training data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，增加每层中的神经元数量和/或神经网络中层的数量意味着需要增加网络的容量。这意味着神经网络可以表示更复杂的函数，并且可表示函数的空间增长；然而，这同时有好处也有坏处。好处是我们可以学习更复杂的函数，但坏处是更多的可训练参数会增加过拟合训练数据的风险。
- en: In general, smaller neural networks should be preferred if the data is not complex
    or we are working with small datasets. Fortunately, there are different techniques
    that allow you to prevent overfitting the data when you're using high-capacity
    models. These techniques are called regularization techniques (L2 penalties on
    the parameters, dropout, batch normalization, data augmentation, and so on). We
    will dig into them in upcoming chapters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果数据不复杂或我们使用的是小型数据集，应优先选择较小的神经网络。幸运的是，有不同的技术可以在使用大容量模型时防止过拟合数据。这些技术被称为正则化技术（对参数的L2惩罚、丢弃法、批量归一化、数据增强等）。我们将在接下来的章节中深入探讨这些技术。
- en: 'The activation function is another important part of the design of every neural
    network. It is applied to every single neuron: nobody forces us to use the same
    non-linear function on every neuron, but it is a convention to pick a form of
    nonlinearity and use it for every neuron in the same layer.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是每个神经网络设计中另一个重要的部分。它应用于每个神经元：没有人强迫我们在每个神经元上使用相同的非线性函数，但约定俗成的是选择一种非线性形式，并在同一层的每个神经元上使用它。
- en: If we are building a classifier, we are interested in evaluating the output
    layer of the network and being able to interpret the output values to understand
    what the network predicted. Let's say we have a linear activation function that's
    been applied to every single neuron of the output layer, where every neuron is
    associated with a particular class (looking at the preceding image, we have a
    3-dimensional input and two output neurons, one for each class) – how can we interpret
    those values, since their codomain is the whole set of real numbers? It's hard
    to interpret values that are expressed in this way.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在构建一个分类器，我们关心的是评估网络的输出层，并能够解释输出值来理解网络的预测。假设我们对输出层的每个神经元应用了线性激活函数，每个神经元与一个特定类别相关联（看前面的图像，我们有一个3维输入和两个输出神经元，每个类别一个）——那么我们如何解释这些值呢？因为它们的值域是整个实数集，所以很难解释以这种方式表达的值。
- en: 'The most natural way is to constrain the sum of the output values to the [0,1]
    range so that we can consider the output values as sampled from the probability
    distribution over the predicted classes and we can consider the neuron with the
    highest value as the predicted class. Alternatively, we could choose to apply
    a thresholding operation on the values in order to simulate the biological neurons
    firing: if the output of a neuron is greater then a certain threshold value, we
    can output a value of 1, or 0 otherwise.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最自然的方法是将输出值的总和限制在[0,1]范围内，这样我们就可以将输出值视为从预测类别的概率分布中采样，并可以将具有最高值的神经元视为预测的类别。或者，我们可以选择对这些值应用阈值操作，以模拟生物神经元的放电：如果神经元的输出大于某个阈值，我们可以输出值1，否则输出0。
- en: Another thing we can do is squash every single neuron's output in the [0,1]
    range if, for instance, we are solving a multi-class classification task where
    the classes are not mutually exclusive.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将每个神经元的输出限制在[0,1]的范围内，例如在我们解决一个多类分类任务时，类之间不是互斥的。
- en: It's easy to understand why a certain non-linearity in the output layer is important
    – it can change the behavior of the network since the way we interpret the network's
    output depends on it. However, understanding why non-linearity is important in
    every single layer is mandatory for a complete understanding of neural networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解为什么输出层中的某种非线性非常重要——它可以改变网络的行为，因为我们如何解释网络的输出依赖于此。然而，要完全理解神经网络，理解每一层非线性的重要性是必须的。
- en: Activation functions
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'As we already know, the output value of the *i*-th neuron in a layer is computed
    as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，层中第*i*个神经元的输出值计算方式如下：
- en: 'The activation function, ![](img/cc9f57ec-e305-4b23-a956-360d0bf40fe8.png),
    is important for several reasons:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数，![](img/cc9f57ec-e305-4b23-a956-360d0bf40fe8.png)，因多种原因而重要：
- en: As stated in the previous section, depending on the layer we are applying the
    non-linearity to, it allows us to interpret the result of the neural network.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前一节所述，根据我们应用非线性的层次，它使我们能够解释神经网络的结果。
- en: If the input data is not linearly separable, it's non-linearity allows you to
    approximate a non-linear function that's capable of separating data in a non-linear
    way (just think about the transformation of a hyperplane into a generic hypersurface).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果输入数据是不可线性分割的，其非线性特性使得你能够近似一个非线性函数，从而以非线性的方式分割数据（想一想超平面如何转化为一般的超曲面）。
- en: 'Without non-linearities among adjacent layers, multi-layer neural networks
    are equivalent to a single neural network with a single hidden layer, and so they
    are able to separate only two regions of the input data. In fact, given:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果相邻层之间没有非线性，那么多层神经网络等价于只有一个隐藏层的单层神经网络，因此它们只能分割输入数据的两个区域。事实上，给定：
- en: '[![](img/840f8ae1-cb55-4dee-a6f9-afb63507b410.png)]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/840f8ae1-cb55-4dee-a6f9-afb63507b410.png)]'
- en: 'And two perceptrons stacked:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 两个感知机堆叠：
- en: '[![](img/89dacb54-3f48-42e0-97a3-038597d46210.png)],'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/89dacb54-3f48-42e0-97a3-038597d46210.png)],'
- en: 'We know that the output of the second perceptron is equivalent to the output
    of a single perceptron:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，第二个感知机的输出等同于单个感知机的输出：
- en: '[![](img/ba89ea59-dade-46b2-bd23-18f3ce03eda9.png)],'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/ba89ea59-dade-46b2-bd23-18f3ce03eda9.png)],'
- en: Where [![](img/025f29e7-7378-415e-a1f1-470c0576b486.png)] and [![](img/3dff90d5-1a3e-4cbf-9e5e-b3c69ce83826.png)] are
    the matrix of weights and the bias vector is equivalent to the product of the
    single weight matrices and bias vectors.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中，[![](img/025f29e7-7378-415e-a1f1-470c0576b486.png)] 和 [![](img/3dff90d5-1a3e-4cbf-9e5e-b3c69ce83826.png)]
    是权重矩阵，偏置向量等同于单个权重矩阵和偏置向量的乘积。
- en: This means that, when [![](img/cc9f57ec-e305-4b23-a956-360d0bf40fe8.png)] is linear, a
    multi-layer neural network is always equal to a single layer neural network (hence,
    it has the same learning capacity). If not, the last equation doesn't hold.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，当 [![](img/cc9f57ec-e305-4b23-a956-360d0bf40fe8.png)] 是线性时，多层神经网络始终等同于单层神经网络（因此，它具有相同的学习能力）。如果不是这样，最后的方程就不成立。
- en: 'Non-linearities make the network robust to noisy input. If the input data contains
    noise (the training set contains values that are not perfect – it happens, and
    it happens often), the non-lineary avoids its propagation to the output. This
    can be demonstrated as follows:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性使得网络对噪声输入具有鲁棒性。如果输入数据包含噪声（即训练集中的值不完美——这种情况时常发生），非线性可以防止噪声传播到输出。以下是这一点的证明：
- en: '[![](img/60b167dc-c66d-4b10-9223-20e985ab5cb5.png)].'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/60b167dc-c66d-4b10-9223-20e985ab5cb5.png)]。'
- en: Two of the most frequently used activation functions are the sigmoid (![](img/098ad987-518d-4116-88e6-0046e571c631.png))and
    the hyperbolic tangent (![](img/995ab973-a9e9-4291-827b-987f67f5a715.png)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的两种激活函数是 sigmoid（![](img/098ad987-518d-4116-88e6-0046e571c631.png)）和双曲正切（![](img/995ab973-a9e9-4291-827b-987f67f5a715.png)）。
- en: 'The first is used as the activation function of the output layer in almost
    every classification problem since it squashes the output in the [0,1] range and
    allows you to interpret the prediction as a probability:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个激活函数几乎在所有分类问题的输出层中使用，因为它将输出压缩到[0,1]范围内，使你可以将预测结果解释为一个概率：
- en: '![](img/2c0d2c3a-c044-4063-b46e-3845f2c0e6f9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c0d2c3a-c044-4063-b46e-3845f2c0e6f9.png)'
- en: The hyperbolic tangent, instead, is used as the activation function of the output
    layer of almost every generative model that's trained to generate images. Even
    in this case, the reason we use it is to correctly interpret the output and to
    create a meaningful bond among the input images and the generated images. We are
    used to scaling the input values from [0,255] to [-1,1], which is the range of
    the ![](img/7e4d457c-5fc1-4a06-8529-8210d777dcc1.png) function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数则作为几乎所有生成模型输出层的激活函数，通常用于生成图像的训练。即便如此，我们使用它的原因是为了正确地解释输出，并在输入图像和生成图像之间建立有意义的联系。我们习惯将输入值从[0,255]范围缩放到[-1,1]，这是 ![](img/7e4d457c-5fc1-4a06-8529-8210d777dcc1.png)函数的范围。
- en: 'However, using functions such as ![](img/6be6bd2c-1dad-4e1f-8349-0a473db8694d.png)
    and ![](img/6b3d6095-8a78-4677-803e-481b6e8ae40c.png) as activations in the hidden
    layer isn''t the best choice for reasons related to training via backpropagation
    (as we will see in the following sections, saturating nonlinearities can be a
    problem). Many other activation functions have been developed in order to overcome
    the problems that have been introduced by saturating nonlinearities. A short visual
    overview of the most common nonlinearities that have been developed is shown in
    the following diagram:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用像 ![](img/6be6bd2c-1dad-4e1f-8349-0a473db8694d.png) 和 ![](img/6b3d6095-8a78-4677-803e-481b6e8ae40c.png) 这样的函数作为隐藏层的激活函数并不是最佳选择，这与反向传播训练过程相关（如我们将在后续章节中看到的，饱和非线性可能会成为问题）。为了克服饱和非线性带来的问题，许多其他激活函数被开发了出来。下图展示了最常见的非线性函数的简短视觉概览：
- en: '![](img/fefa202a-4a07-4467-821d-2f0c8716a113.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fefa202a-4a07-4467-821d-2f0c8716a113.png)'
- en: 'A list of the most common activation functions. Source: Stanford cs231n.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见激活函数的列表。来源：Stanford cs231n。
- en: Once the network structure has been defined, as well as the activation functions
    to use in the hidden and the output layers, it's time to define the relation among
    the training data and the network's output in order to be able to train the network and
    make it solve the task at hand.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络结构定义完成，并且确定了要在隐藏层和输出层中使用的激活函数，就可以定义训练数据与网络输出之间的关系，从而能够训练网络并使其解决当前的任务。
- en: In the upcoming sections, we will talk about a discrete classification problem
    that follows on from [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What
    is Machine Learning? *We're talking about the fact that everything that holds
    for a classification problem also holds for continuous variables since we are
    using neural networks as a tool to solve a supervised learning problem. Since
    a neural network is a parametric model, training it means that we need to update
    the parameters,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论一个离散分类问题，该问题继承自[第1章](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml)，*什么是机器学习？*我们在讨论的是，分类问题的所有结论也适用于连续变量，因为我们使用神经网络作为工具来解决监督学习问题。由于神经网络是一个参数模型，训练它意味着我们需要更新这些参数。
- en: '![](img/d054cd5a-5fb8-4652-83d5-1e63d2b345ce.png), to find the configuration
    that solves the problem in the best possible way.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/d054cd5a-5fb8-4652-83d5-1e63d2b345ce.png)，以找到能够以最佳方式解决问题的配置。'
- en: 'Training a neural network is mandatory if we wish to define a relationship
    among the input data and the desired output: an objective function—or loss function,
    since we want to minimize the loss as our objective.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望定义输入数据和期望输出之间的关系，那么训练神经网络是必不可少的：这就是目标函数——或者称为损失函数，因为我们希望将损失最小化作为目标。
- en: Loss function
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: After defining the network architecture, the model must be trained. It's now
    time to define the relationship between the model's output and the real data.
    To do so, a loss function must be defined.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了网络架构后，模型必须进行训练。现在是时候定义模型输出和真实数据之间的关系了。为此，必须定义一个损失函数。
- en: The loss function is used to assesses the goodness-of-fit of a model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数用于评估模型的拟合优度。
- en: There are several loss functions, each one expressing a relationship among the
    network output and the real data, and their form completely influences the quality
    of the model's prediction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种损失函数，每个损失函数都表达了网络输出与真实数据之间的关系，而它们的形式完全影响模型预测的质量。
- en: 'For a discrete classification problem over ![](img/df395d74-ce33-4d18-bf37-ae6953cd08fa.png)
    classes, we can model the defined neural network that accepts a D-dimensional
    input vector, ![](img/4226cdb4-79cd-473b-b532-e842f7ee12c8.png), and produces
    an ![](img/cf076cfb-87a0-49bd-a1ee-b1b1f7ebbacf.png)-dimensional vector of predictions
    as a function of its parameters,![](img/50596d95-1217-443a-884b-c017f8359979.png), like
    so:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在![](img/df395d74-ce33-4d18-bf37-ae6953cd08fa.png)个类别上的离散分类问题，我们可以定义一个神经网络模型，该模型接受一个D维的输入向量，![](img/4226cdb4-79cd-473b-b532-e842f7ee12c8.png)，并生成一个![](img/cf076cfb-87a0-49bd-a1ee-b1b1f7ebbacf.png)维的预测向量，作为其参数的函数，![](img/50596d95-1217-443a-884b-c017f8359979.png)，如下所示：
- en: '![](img/c25c25e7-a0eb-47ac-a593-e2a2de349c83.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c25c25e7-a0eb-47ac-a593-e2a2de349c83.png)'
- en: The model produces an M-dimensional output vector that contains the probabilities
    the model assigns to the input, ![](img/fb3cf79f-e92c-4dd7-8c10-1204ce356b49.png), for
    every possible class (if we applied the sigmoid activation to the output layer,
    we can interpret the output in this way).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成一个M维的输出向量，包含模型分配给输入的概率，![](img/fb3cf79f-e92c-4dd7-8c10-1204ce356b49.png)，对于每一个可能的类别（如果我们对输出层应用了sigmoid激活函数，可以这样解读输出）。
- en: 'It''s easy to extract the position of the neuron in the output layer that produced
    the highest value. The equation for the predicted class is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易提取输出层中产生最大值的神经元的位置。预测类别的公式如下：
- en: '![](img/85b9814f-fc1e-4196-8202-a64e6ab3f9bc.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85b9814f-fc1e-4196-8202-a64e6ab3f9bc.png)'
- en: 'By using this, we can find the index of the neuron that produced the highest
    classification score. Since we know the label associated with the input, ![](img/25a9a721-29f9-4e85-8410-f1fc7554e79e.png),
    we are almost ready to define the relationship between the prediction and the
    label. The last problem we will face is the label format: the label is a scalar
    value, whereas the network output is an M-dimensional vector. Although we can
    find the position of the neuron with the highest probability value, we are interested
    in the whole output layer, since we want to increase the probability of the correct
    class and penalize the incorrect ones.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个方法，我们可以找到产生最高分类分数的神经元的索引。由于我们知道与输入相关的标签，![](img/25a9a721-29f9-4e85-8410-f1fc7554e79e.png)，我们几乎已经准备好定义预测与标签之间的关系。我们面临的最后一个问题是标签的格式：标签是一个标量值，而网络输出是一个M维向量。虽然我们可以找到具有最高概率值的神经元位置，但我们关心的是整个输出层，因为我们希望增加正确类别的概率，并惩罚错误类别。
- en: For this reason, the label must be converted into an M-dimensional representation
    so that we can create a bond between every output neuron and the label.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，标签必须转换为 M 维表示，以便我们可以在每个输出神经元和标签之间建立联系。
- en: 'The most natural conversion from a scalar value to an M-dimensional representation
    is called **one-hot** encoding. This encoding consists of the creation of an M-dimensional
    vector that has a value of 1 in the position of the label and 0 in every other
    position. Therefore, we can consider the one-hot encoded-label as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从标量值到 M 维表示的最自然转换称为**独热**编码。此编码由创建一个 M 维向量组成，该向量在标签的位置上值为 1，在其他位置上值为 0。因此，我们可以将独热编码的标签表示为如下：
- en: '![](img/4b47d8b8-13b3-455e-8991-b107a617fb45.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b47d8b8-13b3-455e-8991-b107a617fb45.png)'
- en: 'It''s now possible to define the general formulation of the loss-function for
    the *i*-th training set instance as a real-valued function that creates a bond
    between the ground truth (the label that''s been correctly encoded) and the predicted
    value:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以定义损失函数的一般公式，该公式适用于 *i* 第个训练集实例，作为一个实值函数，创建一个地面真实值（正确编码的标签）和预测值之间的联系：
- en: '![](img/fd197327-5b89-4c4b-9687-4892f9fd2c9f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd197327-5b89-4c4b-9687-4892f9fd2c9f.png)'
- en: 'The general formulation of a loss function that''s applied to the complete
    training set of cardinality, *k*, can be expressed as the mean of the loss that''s
    computed on the single instances:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于完整训练集的损失函数的一般公式，设训练集大小为 *k*，可以表示为对单个实例计算的损失的均值：
- en: '![](img/06ef012e-6bec-4994-a209-11a815a73b75.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06ef012e-6bec-4994-a209-11a815a73b75.png)'
- en: 'The loss must be chosen (or defined) based on the problem at hand. The simplest
    and most intuitive loss function for a classification problem (of mutually exclusive
    classes) is the L2 distance among the one-hot encoded representation of the label
    and the network output. The aim is to minimize the distance between the network
    output and the one-hot encoded label, thereby making the network predict an M-dimensional
    vector that looks like the correct label:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 损失必须根据具体问题来选择（或定义）。对于分类问题（互斥类别），最简单和最直观的损失函数是标签的独热编码表示和网络输出之间的 L2 距离。目标是最小化网络输出与独热编码标签之间的距离，从而使网络预测一个类似于正确标签的
    M 维向量：
- en: '![](img/6b3e944a-47a2-4d41-ab21-32e54add72da.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b3e944a-47a2-4d41-ab21-32e54add72da.png)'
- en: The minimization of the loss function occurs through small iterative adjustments
    of the model's parameter values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的最小化是通过对模型参数值进行小的迭代调整来实现的。
- en: Parameter initialization
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数初始化
- en: 'The initial model parameter values are the solution to the problem the training
    phase iteratively refines: there''s no unique way of initializing the network
    parameters, and perhaps the only working suggestions regarding the parameter''s
    initialization are as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 初始模型参数值是训练阶段迭代优化问题的解：没有唯一的方式初始化网络参数，关于参数初始化的唯一有效建议如下：
- en: '**Do not initialize the network parameters to zero**: It is impossible to find
    a new solution using gradient descent (as we will see in the next section) since
    the whole gradient is 0 and therefore there''s no indication of the update direction.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不要将网络参数初始化为零**：使用梯度下降法是无法找到新解的（如我们将在下一节中看到的），因为整个梯度为 0，因此没有更新方向的指示。'
- en: '**Break symmetry between different units**: If two hidden units with the same
    activation function are connected to the same input, then these two inputs must
    have a different initial parameter value. This is required because almost every
    solution requires a set of different parameters to be assigned to each neuron
    to find a meaningful solution. If we start with all the parameters with the same
    value instead, every update step will update all the network parameters by the
    same amount since the updated value depends on the error, which is equal for every
    neuron in the network. Due to this, we will be unable to find a meaningful solution.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**打破不同单元之间的对称性**：如果两个使用相同激活函数的隐藏单元连接到相同输入，则这两个输入必须具有不同的初始参数值。这是必要的，因为几乎每个解决方案都要求为每个神经元分配一组不同的参数，以找到有意义的解。如果我们一开始将所有参数都设置为相同的值，那么每次更新步骤都会以相同的量更新所有网络参数，因为更新值取决于误差，而误差对于网络中的每个神经元来说都是相等的。因此，我们将无法找到有意义的解决方案。'
- en: Usually, the initial solution to the problem is sampled by a random normal distribution
    with zero mean and unary variance. This distribution ensures that network parameters
    are small and equally distributed around the zero value while being different
    among them, therefore breaking the symmetry.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，问题的初始解是通过具有零均值和单位方差的随机正态分布来采样的。这种分布确保了网络参数较小并且围绕零值均匀分布，同时它们之间有所不同，从而打破了对称性。
- en: Now that we have defined the network architecture, correctly formatted the input
    labels, and defined the input-output relation with the loss function, how can
    we minimize the loss? How can we iteratively adjust the model parameters to minimize
    the loss and thus solve the problem?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了网络架构，正确格式化了输入标签，并用损失函数定义了输入输出关系，那么我们如何最小化损失呢？我们如何迭代调整模型参数以最小化损失，从而解决问题？
- en: It's all a matter of optimization and optimization algorithms.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是关于优化和优化算法的问题。
- en: Optimization
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: Operation research gives us efficient algorithms that we can use to solve optimization
    problems by finding the global optimum (the global minimum point) if the problems
    are expressed as a function with well-defined characteristics (for instance, convex
    optimization requires the function to be a convex).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 运筹学为我们提供了高效的算法，可以通过寻找全局最优解（全局最小点）来解决优化问题，如果问题可以表达为具有明确定义特性的函数（例如，凸优化要求函数为凸函数）。
- en: Artificial neural networks are universal function approximators; therefore, it
    is not possible to make assumptions about the shape of the function the neural
    network is approximating. Moreover, the most common optimization methods exploit
    geometric considerations, but we know from [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What
    is Machine Learning?**,* that geometry works in an unusual way when dimensionality
    is high due to the curse of dimensionality.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是通用的函数逼近器；因此，我们无法对神经网络所逼近的函数形状做出假设。此外，最常见的优化方法利用几何考虑，但正如我们在[第1章](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml)中了解到的，*什么是机器学习？*，由于维度灾难，当维度很高时，几何学的表现非常不寻常。
- en: For these reasons, it is not possible to use operation research methods that
    are capable of finding the global optimum of an optimization (minimization) problem.
    Instead, we have to use an iterative refinement method that, starting from an
    initial solution tries, to refine it (by updating the model parameters that represent
    the solution) with the aim of finding a good, local optimum.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些原因，无法使用能够找到优化（最小化）问题全局最优解的运筹学方法。相反，我们必须使用一种迭代优化方法，该方法从初始解出发，尝试通过更新表示解的模型参数来细化解，以寻找一个好的局部最优解。
- en: 'We can think about the model parameters, ![](img/58b62beb-2225-4afb-9f44-73127d592942.png), as
    the initial solution to a minimization problem. Therefore, we can start evaluating
    the loss function at the training step, 0 ![](img/1432f8ae-a8cc-4427-ba37-04412b6a62db.png),
    so that we have an idea about the value it assumes with the actual initial configuration
    of parameters, ![](img/c14904a6-26e2-4c70-8cd8-265c03f0efa3.png). Now, we have
    to decide on how to update the model parameters. To do this, we need to perform
    the first update step, which we do by following the information that the loss
    gives us. We can proceed in two ways:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型参数，![](img/58b62beb-2225-4afb-9f44-73127d592942.png)，视为最小化问题的初始解。因此，我们可以从训练步骤0开始评估损失函数，![](img/1432f8ae-a8cc-4427-ba37-04412b6a62db.png)，以便了解它在当前初始参数配置下的值，![](img/c14904a6-26e2-4c70-8cd8-265c03f0efa3.png)。现在，我们必须决定如何更新模型参数。为此，我们需要执行第一次更新步骤，方法是根据损失给出的信息进行操作。我们可以通过两种方式继续：
- en: '**Random perturbations**: We can apply a random perturbation, ![](img/6b4079db-023b-4ab2-a317-528be4f62854.png), to
    the current set of parameters and compute the loss value on the obtained new set
    of parameters, ![](img/d7c875ab-85c7-45d7-bc2a-64f076174514.png).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机扰动**：我们可以对当前的一组参数应用随机扰动，![](img/6b4079db-023b-4ab2-a317-528be4f62854.png)，并计算获得的新参数集上的损失值，![](img/d7c875ab-85c7-45d7-bc2a-64f076174514.png)。'
- en: If the loss value at the training step, ![](img/36c33b0c-8754-4502-95f6-12fd0e510b73.png), is
    less than the value at the previous one, we can accept the found solution and
    move on with a new random perturbation that's applied to the new set of parameters.
    Otherwise, we have to repeat the random perturbation until a better solution is
    found.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练步骤中的损失值，![](img/36c33b0c-8754-4502-95f6-12fd0e510b73.png)，小于前一步的值，我们可以接受找到的解并继续进行新的随机扰动，应用到新一组参数上。否则，我们必须重复随机扰动，直到找到更好的解。
- en: '**Estimation of the update direction**: Instead of generating a new set of
    parameters randomly, is it possible to guide the local optimum research process
    toward the direction of the maximum descent of the function.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新方向的估计**：与其随机生成一组新参数，不如指导局部最优解搜索过程朝着函数的最大下降方向进行。'
- en: The second approach is the de facto standard for training parametric machine
    learning models that are expressed as differentiable functions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是训练作为可微函数表示的参数化机器学习模型的 de facto 标准。
- en: To properly understand this gradient descent method, we have to think about
    the loss function as a way of defining a surface in the parameter space—our objective,
    that is, minimizing the loss, means that we need to find the lowest point on this
    surface.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确理解这个梯度下降方法，我们必须将损失函数看作是在参数空间中定义的一个表面——我们的目标，即最小化损失，意味着我们需要找到这个表面上的最低点。
- en: Gradient descent
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is a method that''s used to calculate the best direction to
    move in when we''re searching for the solution to a minimization/maximization
    problem. This method suggests the direction to follow when we''re updating the
    model parameters: the direction that''s found, depending on the input data that''s
    used, is the direction of the steepest descent of the loss surface. The data that''s
    used is of extreme importance since it follows the evaluation of the loss function
    and therefore the surface that''s used to evaluate the update direction.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种用于计算在寻找最小化/最大化问题解时，应该沿着哪个方向移动的方法。这种方法建议我们在更新模型参数时应遵循的方向：根据所使用的输入数据，找到的方向是损失面最陡峭下降的方向。所使用的数据极为重要，因为它决定了损失函数的评估方式，从而决定了用来评估更新方向的表面。
- en: 'The update direction is given by the gradient of the loss function. It''s known
    from calculus that the derivative operation for a single variable differentiable
    function, ![](img/b2f0595a-05ea-4981-b131-dd0467a0a09f.png), in point ![](img/f07ae0c3-7f9d-4a9d-90e8-77fa60aa7f42.png) is
    given by the following formula:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 更新方向由损失函数的梯度给出。从微积分中知道，对于一个单变量可微函数，![](img/b2f0595a-05ea-4981-b131-dd0467a0a09f.png)，在点 ![](img/f07ae0c3-7f9d-4a9d-90e8-77fa60aa7f42.png)的导数由以下公式给出：
- en: '![](img/4b9aced6-e6f2-4851-9807-f9acae93e082.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b9aced6-e6f2-4851-9807-f9acae93e082.png)'
- en: 'This operation gives us a description of the behavior of the function in ![](img/4ba59bc9-d41e-4b45-8758-2e5f68c35f5b.png):
    it shows us how much the function varies with respect to the ![](img/2fa106b7-259a-4b3c-9f1f-19004dda9d59.png) variable
    in an infinitely small region centered in ![](img/ad1c59c0-9c33-4535-a361-51106af7897f.png).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作为我们提供了在 ![](img/4ba59bc9-d41e-4b45-8758-2e5f68c35f5b.png)中的函数行为描述：它展示了函数在![](img/2fa106b7-259a-4b3c-9f1f-19004dda9d59.png)变量附近一个无穷小区域内的变化。
- en: 'The generalization of the derivative operation for an n-variables function
    is given by the gradient, that is, the vector of the partial derivatives (the
    vector of the derivatives of the function with respect to a single variable considering
    constants any other variable). In the case of our loss function, it is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于n变量函数的导数操作的推广就是梯度，即偏导数的向量（函数对单一变量的导数向量，考虑其他变量为常数）。在我们的损失函数的情况下，它如下所示：
- en: '![](img/8f89ad48-7a5d-4489-b34f-9d6b627052be.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f89ad48-7a5d-4489-b34f-9d6b627052be.png)'
- en: '![](img/d29ed3ba-8018-45d0-b3dc-28b5c014d4ff.png) indicates the direction along
    which the function is growing. Hence, since our objective is to find the minimum,
    we have to move along the direction indicated by the anti-gradient, like so:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/d29ed3ba-8018-45d0-b3dc-28b5c014d4ff.png) 表示函数增长的方向。因此，由于我们的目标是找到最小值，我们必须沿着由反梯度指示的方向移动，如下所示：'
- en: '![](img/d10a5132-7220-453a-b128-126bd8af5117.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d10a5132-7220-453a-b128-126bd8af5117.png)'
- en: 'Here, the anti-gradient represents the direction to follow when performing
    the parameter update. The parameter update step now looks as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，反梯度表示执行参数更新时应该遵循的方向。现在，参数更新步骤如下所示：
- en: '![](img/de3e35e9-2313-406e-8607-058f113a430e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de3e35e9-2313-406e-8607-058f113a430e.png)'
- en: 'The ![](img/b39bcd22-c232-4adb-aff2-c3b55ed3d1d2.png) parameter is the learning
    rate and is a hyperparameter of the training phase with gradient descent. Choosing
    the correct value for the learning rate is more of an art than a science, and
    the only thing we can do is use our intuition to choose a value that works well
    for our model and dataset. We have to keep in mind that the anti-gradient only
    tells us the direction to follow; it doesn''t give us any information about the
    distance from the current solution to the minimum point. The distance, or the
    strength of the update, is regulated by the learning rate:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/b39bcd22-c232-4adb-aff2-c3b55ed3d1d2.png) 参数是学习率，是梯度下降训练阶段的超参数。选择正确的学习率值更像是一门艺术而非科学，我们唯一能做的就是凭借直觉选择一个适合我们的模型和数据集的值。我们必须记住，反梯度只是告诉我们应该遵循的方向，它并没有给出当前解与最小点之间的距离信息。距离或更新强度由学习率来调节：'
- en: A learning rate that's too high could make the training phase unstable due to
    jumps around the local minima. This causes oscillations of the loss function's
    value. To remember this, we can just think about a U shaped surface. If the learning
    rate is too high, we jump from the left to the right of the U, and vice versa
    in the next update step, without ever descending the valley (because the distance
    from the two peaks of the U is greater than ![](img/b39bcd22-c232-4adb-aff2-c3b55ed3d1d2.png)).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率过高可能会导致训练阶段不稳定，因为在局部最小值附近会出现跳跃，这会导致损失函数值的震荡。为了帮助记住这一点，我们可以把它想象成一个U型的曲面。如果学习率过高，我们在U的左右两侧之间跳跃，而在下一个更新步骤中则反向跳跃，永远无法下降到谷底（因为U的两个峰值之间的距离大于 ![](img/b39bcd22-c232-4adb-aff2-c3b55ed3d1d2.png)）。
- en: A learning rate that's too small could make the training phase suboptimal since
    we never jump out of a valley that is not the point of the global minimum. Hence,
    there's a risk of being stuck in a local minimum. Moreover, another risk with
    a learning rate that's too small is never finding a good solution – not because
    we are stuck in a local minimum, but because we are moving too slowly toward the
    direction at hand. Since this is an iterative process, the research could take
    too long.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率过小可能会导致训练阶段的效果不理想，因为我们永远无法跳出一个不是全局最小值的谷底。因此，有被困在局部最小值中的风险。此外，学习率过小的另一个风险是永远找不到一个好的解——并不是因为我们被困在局部最小值中，而是因为我们朝着目标前进的速度太慢。由于这是一个迭代过程，研究可能需要花费太长时间。
- en: In order to face the challenge of choosing the learning rate value, various
    strategies have been developed that change its value during the training phase,
    usually reducing it in order to find a trade-off between the exploration of the
    landscape using a big learning rate and the refinement of the found solution (descending
    the valley) using a smaller learning rate value.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对选择学习率值的挑战，已经开发出多种策略，这些策略在训练阶段调整学习率的值，通常是逐渐减小，以便在使用较大学习率进行探索（寻找更广泛的解空间）和使用较小学习率进行精细调整（下降到更深的谷底）之间找到平衡。
- en: So far, we've looked at updating parameters by considering a loss function that's
    computed using the complete dataset, all at once. This method is called **batch
    gradient descent**. This method, in practice, can never be applied to a real scenario
    since modern applications of neural networks deal with huge amounts of data that
    rarely fit inside the computer's memory.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了通过使用完整数据集计算得到的损失函数来更新参数，这种方法称为**批量梯度下降**。实际上，这种方法无法应用于现实场景，因为现代神经网络应用处理的数据量巨大，通常无法完全存入计算机内存。
- en: Several variants of batch gradient descent have been developed to overcome its
    limitations, together with different strategies for updating the model parameters,
    that will help us solve face some challenges related to the gradient methods themselves.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发出了几种批量梯度下降的变体，以克服其局限性，同时也有不同的策略来更新模型参数，这些策略将帮助我们解决与梯度方法本身相关的挑战。
- en: Stochastic gradient descent
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: 'Stochastic gradient descent updates the model parameter for every element of
    the training dataset—one example, one update step:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降会针对训练数据集中的每个元素进行模型参数更新——每个样本一次更新步骤：
- en: '![](img/cf9fde1e-c705-4e1e-8dc5-b3ecac1f28c1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf9fde1e-c705-4e1e-8dc5-b3ecac1f28c1.png)'
- en: 'If the dataset has high variance, stochastic gradient descent causes huge fluctuations
    of the loss value during the training phase. This can be both an advantage and
    a disadvantage:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集具有较高的方差，随机梯度下降会在训练阶段造成损失值的巨大波动。这既可以是优势也可以是劣势：
- en: It can be an advantage because, due to the fluctuations of the loss, we jump
    into unexplored zones of the solution space that could contain a better minimum.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可能是一个优势，因为由于损失的波动，我们进入了解决方案空间中的未探索区域，这些区域可能包含更好的最小值。
- en: It is a method suited for online training. This means training with new data
    during the whole lifetime of the model (which means we can continue to train the
    model with new data, usually coming from a sensor).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一种适用于在线训练的方法。这意味着在模型的整个生命周期中使用新数据进行训练（这意味着我们可以继续用新的数据训练模型，通常来自传感器）。
- en: The convergence is slower and finding a good minimum is more difficult since
    the updates have high variance.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛较慢，找到一个好的最小值更困难，因为更新具有较高的方差。
- en: The de facto method for training neural networks that try to keep the advantages
    of both batch and stochastic gradient descent is known as mini-batch gradient
    descent.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的事实标准方法，旨在保持批量梯度下降和随机梯度下降的优点，称为小批量梯度下降。
- en: Mini-batch gradient descent
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: 'Mini-batch gradient descent keeps the best parts of the batch and stochastic
    gradient descent methods. It updates the model parameters using a subset of cardinality, ![](img/9e376903-315b-45f3-9d66-ab20fde2b198.png),
    of the training set, which is a mini-batch:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降保留了批量梯度下降和随机梯度下降方法的优点。它使用训练集的一个子集（即小批量）更新模型参数，子集的基数为 ![](img/9e376903-315b-45f3-9d66-ab20fde2b198.png)：
- en: '![](img/3923a990-cd69-4524-910a-1e4a0d1f6093.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3923a990-cd69-4524-910a-1e4a0d1f6093.png)'
- en: 'This is the most widely used approach due to the following reasons:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最广泛使用的方法，原因如下：
- en: Using mini-batches reduces the parameter's update variance, and so it causes
    faster convergence in the training process
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小批量减少了参数更新的方差，因此在训练过程中加速了收敛速度。
- en: Using a mini-batch of cardinality allows you to reuse the same method for online
    training
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有卡尔达利数的小批量使你能够复用相同的方法进行在线训练
- en: 'It''s possible to write down a generic formula for gradient descent at the
    update step, *s*, as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 可以写出梯度下降在更新步骤 *s* 时的通用公式，如下所示：
- en: '![](img/ccb450d2-31b0-4728-a205-a43e450ad77e.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb450d2-31b0-4728-a205-a43e450ad77e.png)'
- en: For ![](img/213bad97-a25e-469a-8e6b-328f9f15ea2a.png), the method is stochastic
    gradient descent
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![](img/213bad97-a25e-469a-8e6b-328f9f15ea2a.png)，该方法是随机梯度下降
- en: For ![](img/4fba78fd-d5ec-4b11-b2f2-8a010fd4ead7.png), the method is batch gradient
    descent
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![](img/4fba78fd-d5ec-4b11-b2f2-8a010fd4ead7.png)，该方法是批量梯度下降
- en: For ![](img/7776fe65-4456-4dcd-a3f2-14f4cdad03bb.png), the method is mini-batch
    gradient descent
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 ![](img/7776fe65-4456-4dcd-a3f2-14f4cdad03bb.png)，该方法是小批量梯度下降
- en: The three methods that have been shown here update the model parameters in a
    so-called **vanilla** way that only considers the current parameter's value and
    the anti-gradient that's computed by applying the definition. They all use a fixed
    value for the learning rate.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的三种方法以所谓的**传统**方式更新模型参数，该方式只考虑当前参数的值和通过应用定义计算出的反向梯度。它们都使用固定的学习率值。
- en: 'Other parameter optimization algorithms exist, and all of them have been developed
    with the aim of finding better solutions, exploring the parameter space in a better
    way, and overcoming all the problems that a vanilla approach can face when searching
    for a good minimum:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参数优化算法也存在，所有这些算法的开发目标是找到更好的解决方案、更好地探索参数空间，并克服传统方法在寻找良好最小值时可能遇到的所有问题：
- en: '**Choose the learning rate**: The learning rate is probably the most important
    hyperparameter of the whole training phase. These reasons were explained at the
    end of the *Gradient descent* section.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择学习率**：学习率可能是整个训练阶段最重要的超参数。这些原因在 *梯度下降* 部分的末尾已有解释。'
- en: '**Constant learning rate**: The vanilla update strategy doesn''t change the
    learning rate value during the training phase. Moreover, it uses the same learning
    rate to update every parameter. Is this always desirable? Probably not, since
    treating parameters associated with input features with a different frequency
    of appearance in the same manner is not reasonable. Intuitively, we want to update
    the parameters associated with low appearance frequency features and the others
    with smaller steps.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常数学习率**：传统更新策略在训练过程中不会改变学习率的值。此外，它使用相同的学习率来更新每一个参数。这总是可取的吗？可能不是，因为将与输入特征的不同出现频率相关的参数同等对待并不合理。从直观来看，我们希望以较小的步长更新与低频特征相关的参数，而以较大的步长更新其他参数。'
- en: '**Saddle points and plateau**: The loss functions that are used to train neural
    networks are a function of a huge number of parameters and thus are non-convex
    functions. During the optimization process, it is possible to run into saddle
    points (points in which the value of the function increases along one dimension,
    but decreases along other dimensions) or plateaus (locally constant regions of
    the loss surface).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鞍点和平台**：用于训练神经网络的损失函数是大量参数的函数，因此是非凸函数。在优化过程中，可能会遇到鞍点（函数在某个维度上值增加，但在其他维度上值减少的点）或平台（损失面上的局部常数区域）。'
- en: In these cases, the gradient is almost zero along every dimension, and so the
    direction that's pointed to by the anti-gradient is nearly 0\. This means we are
    stuck, and the optimization process can't go on. We have been fooled by the constant
    value that was assumed by the loss function during several training steps; we
    think we have found a good minimum, but in reality, we are stuck inside a meaningless
    region of the solution space.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，梯度几乎在每个维度上都接近零，因此由反梯度指引的方向几乎为零。这意味着我们已经被卡住了，优化过程无法继续。我们在多个训练步骤中被损失函数假定的常数值所迷惑；我们以为已经找到了一个好的最小值，但实际上，我们困在了解空间中的一个无意义区域。
- en: Gradient descent optimization algorithms
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降优化算法
- en: 'Several optimization algorithms have been developed to improve the efficiency
    of vanilla optimization. In the upcoming sections, we will recap on vanilla optimization
    and show the two most common optimization algorithms: momentum and ADAM. The former
    will be discussed because it shows how a physical interpretation of the loss surface
    can lead to successful results, while the latter will be discussed because it
    is the most widely adaptive optimization method that''s used.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了几种优化算法来提高传统优化方法的效率。在接下来的章节中，我们将回顾传统优化方法，并展示两种最常见的优化算法：动量法和ADAM。我们讨论动量法是因为它展示了如何通过对损失面进行物理解释，从而取得成功的结果；而讨论ADAM则是因为它是最广泛应用的自适应优化方法。
- en: Vanilla
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统方法
- en: 'As we saw previously, the update formula only requires an estimation of the
    direction, which it gets by using the anti-gradient and the learning rate:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，更新公式只需要估计方向，这个方向是通过使用反梯度和学习率来获得的：
- en: '![](img/d3109de1-4053-4766-8911-7b7452387ed1.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3109de1-4053-4766-8911-7b7452387ed1.png)'
- en: Momentum
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量
- en: The momentum optimization algorithm is based on a physical interpretation of
    the loss surface. Let's think about the loss surface as a messy landscape where
    a particle is moving around, with the aim of finding the global minimum.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 动量优化算法基于对损失面的一种物理解释。我们可以将损失面看作是一片混乱的景观，粒子在其中运动，目标是找到全局最小值。
- en: The vanilla algorithm updates the position of the particle as a function of
    the direction that was found by calculating the anti-gradient, making the particle
    jump from one position to another without any physical meaning. This can be seen
    as an unstable system rich in energy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 传统算法通过计算反梯度找到的方向来更新粒子的位置，使得粒子在没有任何物理意义的情况下从一个位置跳到另一个位置。这可以看作是一个充满能量的不稳定系统。
- en: The basic idea that was introduced in the momentum algorithm is to update the
    model parameters by considering the interaction between the surface and the particle,
    just like you would in a physical system.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 动量算法引入的基本思想是通过考虑表面和粒子之间的相互作用来更新模型参数，就像你在物理系统中一样。
- en: In the real world, a system that teleports a particle from one point to a new
    point in zero time and without loss of energy does not exist. The initial energy
    of the system is lost due to external forces and because the velocity changes
    over time.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，系统无法在零时间内将粒子从一个点传送到另一个点并且不损失能量。系统的初始能量会因为外部力量的作用以及速度随时间的变化而丧失。
- en: 'In particular, we can use the analogy of an object (the particle) that slides
    over a surface (the loss surface) and is subject to a kinetic friction force that
    reduces its energy and speed over time. In machine learning, we call friction
    coefficient momentum, but in practice, we can reason exactly like we do in physics.
    Hence, given a friction coefficient, ![](img/a2851ec3-fff2-4c9b-aaeb-46744da7521c.png) (a
    hyperparameter with values in the [0,1] range but usually in the [0.9, 0.999]
    range), the update rule of the Momentum algorithm is given by the following equation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们可以使用一个物体（粒子）在表面（损失表面）上滑动，并受到一个动摩擦力的类比，这个力随着时间推移减少其能量和速度。在机器学习中，我们将摩擦系数称为动量，但实际上，我们可以像在物理学中那样进行推理。因此，给定一个摩擦系数，![](img/a2851ec3-fff2-4c9b-aaeb-46744da7521c.png)（一个取值范围在[0,1]之间，但通常在[0.9,
    0.999]范围内的超参数），动量算法的更新规则由以下方程给出：
- en: '![](img/a433d5f6-3b77-4117-9181-d718dec9110a.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a433d5f6-3b77-4117-9181-d718dec9110a.png)'
- en: Here, ![](img/0a3a3069-a338-4782-9c96-d2f1aa775173.png) is the vectorial velocity
    of the particle (every component if the vector is the velocity in a particular
    dimension). The analogy of velocity is natural since, in one dimension, the derivative
    of the position with respect to time is the velocity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/0a3a3069-a338-4782-9c96-d2f1aa775173.png) 是粒子的矢量速度（矢量的每个分量是特定维度上的速度）。速度的类比是自然的，因为在一维中，位置对时间的导数就是速度。
- en: This method takes into account the vectorial velocity that's reached by the
    particle at the previous step and reduces it for those components that go in a
    different direction, while increasing it for points that go in the same direction
    for subsequent updates.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法考虑了粒子在上一步所达到的矢量速度，并且对于那些朝不同方向运动的分量进行减速，同时对朝相同方向运动的分量进行加速，以便进行后续更新。
- en: 'In this way, the overall energy of the system is reduced, which in turn reduces
    the oscillations and gets faster convergence, as we can see from the following
    diagram, which shows the difference between the vanilla (on the left) and the
    momentum (on the right) optimization algorithms:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，系统的总体能量得以减少，从而减少了振荡并加速了收敛过程，正如我们从下图中看到的那样，图中显示了香草算法（左侧）和动量算法（右侧）之间的差异：
- en: '![](img/614fe9e7-0b9c-4e74-88a8-dcd0d3d0f74d.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/614fe9e7-0b9c-4e74-88a8-dcd0d3d0f74d.png)'
- en: Visual representation of the vanilla (left) and momentum (right) optimization
    algorithms. Momentum causes fewer loss oscillations and reaches the minimum faster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是香草算法（左）和动量算法（右）的可视化表示。动量算法导致较少的损失振荡，并且更快地达到最小值。
- en: ADAM
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ADAM
- en: 'The vanilla and the momentum optimization algorithms consider the ![](img/c7fc9572-6b26-4936-9b82-1a12bd1643d8.png) parameter
    as being constant: the strength of the update (the step size) is the same for
    every parameter in the network; there''s no distinction among parameters associated
    with high or low occurrence features. To face this problem and increase the efficiency
    of the optimization algorithms, a whole set of new algorithms has been developed,
    known as **adaptive learning rate optimization methods**.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 香草算法和动量算法将![](img/c7fc9572-6b26-4936-9b82-1a12bd1643d8.png)参数视为常量：每个网络参数的更新强度（步长）都是相同的；没有区分与高频或低频特征相关的参数。为了解决这个问题并提高优化算法的效率，开发了一整套新的算法，称为**自适应学习率优化方法**。
- en: 'The idea behind these algorithms is to associate a different learning rate
    to every parameter of the network and thus update them using a learning rate that
    adapts itself to the type of feature the neuron is specialized to extract (or
    in general, to adapt itself to the different features the neuron sees as input):
    small updates associated with a high frequency of occurrence features, bigger
    otherwise. **Adaptive Moment Estimation** (**ADAM**) wasn''t the first adaptive
    method to be developed, but it is the most commonly used because it outperforms
    almost every other adaptive and non-adaptive algorithm on many different tasks:
    it increases the model''s generalization capabilities while speeding up its convergence.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法背后的理念是为网络中的每个参数分配不同的学习率，从而使用适应神经元专门提取的特征类型（或者一般来说，适应神经元看到的不同输入特征）的学习率来更新它们：与高频特征相关联的小更新，反之则较大的更新。**自适应矩估计**（**ADAM**）并不是第一个被开发的自适应方法，但它是最常用的，因为它在许多不同的任务中超过了几乎所有其他自适应和非自适应算法：它提高了模型的泛化能力，同时加快了收敛速度。
- en: 'Being an adaptive method, it creates a learning rate for every parameter in
    the model, like so:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种自适应方法，它为模型中的每个参数创建一个学习率，如下所示：
- en: '![](img/e71fd61d-2aec-4659-8daa-4c1657fb8141.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e71fd61d-2aec-4659-8daa-4c1657fb8141.png)'
- en: 'The algorithm''s authors decided to take into account how the (square of the)
    gradients changes, as well as their variance:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的作者决定考虑梯度（平方）的变化以及它们的方差：
- en: '![](img/dfdb2e60-2894-4f3d-9bf5-5773ed6086ef.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfdb2e60-2894-4f3d-9bf5-5773ed6086ef.png)'
- en: The first term is the exponential moving average of the gradients (estimation
    of the first-order momentum), while the second term is the exponential moving
    average of the square of the gradients (estimation of the second-order momentum).
    Both ![](img/04814af9-ae83-463b-acdf-2a56da1b6dc7.png) and ![](img/3cd62c43-3e77-4bc8-8204-7c0c7cec70e7.png) are
    vectors with ![](img/ee8135dc-044d-436e-a90d-af3333d49492.png) components, and
    both have been initialized to 0.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项是梯度的指数移动平均（即一阶动量的估计），第二个项是梯度平方的指数移动平均（二阶动量的估计）。![](img/04814af9-ae83-463b-acdf-2a56da1b6dc7.png)
    和 ![](img/3cd62c43-3e77-4bc8-8204-7c0c7cec70e7.png) 都是具有![](img/ee8135dc-044d-436e-a90d-af3333d49492.png)分量的向量，且都已初始化为0。
- en: '![](img/e2512472-c9ac-4f41-9914-54c65afce56a.png) and ![](img/3f4b443e-eab4-4386-8620-403ff099e5c0.png) are
    the decaying factors of the exponential moving average and are hyperparameters
    of the algorithm.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/e2512472-c9ac-4f41-9914-54c65afce56a.png) 和 ![](img/3f4b443e-eab4-4386-8620-403ff099e5c0.png)
    是指数移动平均的衰减因子，并且是算法的超参数。'
- en: The zero initializations of the ![](img/056848f6-156e-44ac-b37e-4fb49ae57c72.png) and ![](img/a6e47090-46d4-4fc5-83bf-e028c5e1a316.png) vectors
    make their value close to 0, especially if the decaying factors are close to 1
    (hence a low decay rate).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/056848f6-156e-44ac-b37e-4fb49ae57c72.png) 和 ![](img/a6e47090-46d4-4fc5-83bf-e028c5e1a316.png)
    向量的零初始化使得它们的值接近0，尤其是在衰减因子接近1时（因此衰减率较低）。'
- en: 'This is a problem since we are estimating values close to zero, and without
    any influence from any possible update rule. To solve this, the authors suggested
    to correct the first and second-order momentums by computing them in the following
    way:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题，因为我们正在估计接近零的值，而没有受到任何可能的更新规则的影响。为了解决这个问题，作者建议通过以下方式来修正一阶和二阶动量：
- en: '![](img/9b927b51-ccdf-40de-899c-476d3f6ce1ed.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b927b51-ccdf-40de-899c-476d3f6ce1ed.png)'
- en: 'Finally, they suggested an update rule that was inspired by other adaptive
    algorithms (Adadelta and RMSProp, which are not explained in this book):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，他们建议了一种受其他自适应算法启发的更新规则（如Adadelta和RMSProp，本书中未解释）：
- en: '![](img/b1662318-f34e-4fd1-8095-4fd508488548.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1662318-f34e-4fd1-8095-4fd508488548.png)'
- en: They suggested that we use decaying rates close to 1 and a very small value
    for the epsilon parameter (it's only there to avoid divisions by zero).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 他们建议我们使用接近1的衰减率，并为epsilon参数设置一个非常小的值（它仅用于避免除以零的情况）。
- en: Why should using the first and second-order moment estimation and this update
    rule to update every single parameter of the network improve the model's speed
    convergence and improve the generalization capabilities of the model?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用一阶和二阶矩估计以及这种更新规则来更新网络中的每个单独参数会改善模型的速度收敛性，并提高模型的泛化能力？
- en: The effective learning rate, ![](img/e55675c7-89ab-4d5a-842b-4e7ac2d62027.png),
    adapts itself during training for every single parameter and takes the frequency
    of occurrence of the input features for every neuron into account. The denominator
    will increase if the computed partial derivatives associated with the current
    parameter are different from zero, such as if the input feature associated with
    that neuron occurs frequently. The higher the occurrence frequency, the smaller
    the update steps becomes during training.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有效学习率，![](img/e55675c7-89ab-4d5a-842b-4e7ac2d62027.png)，在训练过程中会根据每个参数进行自适应调整，并考虑每个神经元输入特征的出现频率。如果与当前参数相关的计算偏导数不为零，比如与该神经元相关的输入特征出现频繁时，分母会增加。出现频率越高，训练过程中更新步长就会越小。
- en: If, instead, the partial derivatives are almost zero every time, the update
    steps are almost constant and never change their size during training.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果偏导数几乎每次都接近零，更新步长将几乎保持不变，且在训练过程中从未改变大小。
- en: Every gradient descent optimization algorithm that we've presented thus far
    requires that we compute the gradient of the loss function. Since neural networks
    can approximate any function and their topology can be very complex, how can we
    compute the gradient of a complex function efficiently? Representing the computation
    using data flow graphs and the backpropagation algorithm is the solution.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止我们展示的每一种梯度下降优化算法，都需要我们计算损失函数的梯度。由于神经网络可以逼近任何函数且其拓扑结构可能非常复杂，那么我们如何有效地计算复杂函数的梯度呢？通过使用数据流图表示计算过程，并结合反向传播算法，便可以解决这个问题。
- en: Backpropagation and automatic differentiation
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播与自动微分
- en: Computing partial derivatives is a process that's repeated thousands upon thousands
    of times while training a neural network and for this reason, this process must
    be as efficient as possible.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 计算偏导数是训练神经网络时需要重复上千上万次的过程，因此，这个过程必须尽可能高效。
- en: 'In the previous sections, we showed you how, by using a loss function, is it
    possible to create a bond between the model''s output, the input, and the label.
    If we represent the whole neural network architecture using a graph, it''s easy
    to see how, given an input instance, we are just performing a mathematical operation
    (input multiplied by a parameter, adding those multiplication results, and applying
    the non-linearity function to the sum) in an ordinate manner. At the input of
    this graph, we have the input samples from the dataset. The output nodes of the
    graph are the predictions; the graph can be seen as a set of compound functions
    of the type:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们展示了如何通过使用损失函数，建立模型输出、输入和标签之间的联系。如果我们用图表示整个神经网络架构，就可以很容易地看到，对于一个输入实例，我们只是按顺序执行数学运算（输入乘以参数，将这些乘法结果相加，并将非线性函数应用于总和）。在这个图的输入端，我们有来自数据集的输入样本。图的输出节点是预测值；这个图可以看作是一组复合函数，类型如下：
- en: '![](img/40ed459e-d6d6-4ce6-bba4-33cb8dfdaa1d.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40ed459e-d6d6-4ce6-bba4-33cb8dfdaa1d.png)'
- en: 'The output of a neuron with two inputs, ![](img/d1f9677a-f85f-4ae5-9c01-479169f35aa4.png) and
    ![](img/f2207421-d662-4328-8211-5c93b6445f3c.png), that uses the ReLU activation
    function is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有两个输入的神经元的输出，![](img/d1f9677a-f85f-4ae5-9c01-479169f35aa4.png) 和 ![](img/f2207421-d662-4328-8211-5c93b6445f3c.png)，使用ReLU激活函数的结果如下：
- en: '![](img/7c8e664a-cc1b-4ee8-8406-d27969c1f4c8.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c8e664a-cc1b-4ee8-8406-d27969c1f4c8.png)'
- en: 'The functions that are used in the previous equations are as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中使用的函数如下：
- en: '![](img/545dc695-bfba-4e29-9f85-173a3109fdc3.png) is the product function of
    an input for a parameter'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/545dc695-bfba-4e29-9f85-173a3109fdc3.png) 是输入与参数的乘积函数'
- en: '![](img/6ccab198-d7d3-4f7c-9529-8d994a2993c2.png) is the sum function of two
    values'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/6ccab198-d7d3-4f7c-9529-8d994a2993c2.png) 是两个值的求和函数'
- en: '![](img/3ee445bc-073b-4e55-9224-32842ad10fd8.png) is the rectified linear unit
    activation function'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/3ee445bc-073b-4e55-9224-32842ad10fd8.png) 是修正线性单元（ReLU）激活函数'
- en: 'Hence, we can represent the output neuron as a composition of these functions:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将输出神经元表示为这些函数的组合：
- en: '![](img/173342e1-c695-4e41-8625-3cccac226f70.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/173342e1-c695-4e41-8625-3cccac226f70.png)'
- en: 'Keep in mind that the variables are not the input values of the functions,
    but the model parameters ![](img/69090582-e723-4b1f-9841-d2f4c95c7484.png). We
    are interested in computing the partial derivatives of the loss function in order
    to train the network. We do this using the gradient descent algorithm. As a simple
    example, we can just consider a simple loss function:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，变量不是函数的输入值，而是模型参数 ![](img/69090582-e723-4b1f-9841-d2f4c95c7484.png)。我们感兴趣的是计算损失函数的偏导数，以便训练网络。我们使用梯度下降算法来完成这个过程。作为一个简单的例子，我们可以考虑一个简单的损失函数：
- en: '![](img/974b9d87-bb4b-4144-95f0-27453fa5d2ba.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/974b9d87-bb4b-4144-95f0-27453fa5d2ba.png)'
- en: 'To compute the loss gradient with respect to the variables (![](img/7afc4997-4f59-4c23-92dd-825d86f6975c.png)),
    it is possible to apply the chain rule (the rule of the derivatives of compound
    functions):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算关于变量的损失梯度 (![](img/7afc4997-4f59-4c23-92dd-825d86f6975c.png))，可以应用链式法则（复合函数导数的法则）：
- en: '![](img/1b903714-fdfe-4978-bf9e-ed2e70f79be2.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b903714-fdfe-4978-bf9e-ed2e70f79be2.png)'
- en: 'Using the Leibniz notation, it is easier to see how the chain rule can be applied
    to compute the partial derivatives of any differentiable function, which is represented
    as a graph (and thus represented as a set of compound functions):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用莱布尼茨符号，可以更容易地看出如何应用链式法则来计算任何可微函数的偏导数，该函数表示为一个图（因此也可以表示为一组复合函数）：
- en: '![](img/28203b41-9d23-4285-9d9e-99163c6301ec.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28203b41-9d23-4285-9d9e-99163c6301ec.png)'
- en: 'In the end, it is just a matter of expressing the operations as compound functions,
    and using a graph is a natural way to do this. We can associate a graph node with
    a function: its inputs are the function inputs; the node performs the function
    computation and outputs the result. Moreover, a node can have attributes, such
    as a formula to apply when calculating the partial derivative with respect to
    its inputs.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这只是将操作表示为复合函数的问题，使用图表示是一种自然的方式。我们可以将图的节点与函数关联：它的输入是函数的输入；该节点执行函数计算并输出结果。此外，节点可以具有属性，例如在计算相对于其输入的偏导数时应用的公式。
- en: Moreover, a graph can be traversed in both directions. We can traverse it in
    the forward direction (forward pass of the backpropagation algorithm), and thus
    compute the loss value. We can also traverse it in the backward direction, applying
    the formula of the derivative of the output with respect to the input associated
    with every node and multiplying the value coming from the previous node with the
    current to compute the partial derivative. This is the application of the chain
    rule.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图可以在两个方向上遍历。我们可以沿着正向方向遍历（反向传播算法的正向传播），从而计算损失值。我们也可以沿着反向方向遍历，应用每个节点与输入相关的输出对输入的导数公式，并将来自前一个节点的值与当前节点的值相乘来计算偏导数。这就是链式法则的应用。
- en: Representing computations as graphs allow us to perform automatic differentiation
    by computing the gradient of complex functions. We only consider operations singularly,
    and just look at the node's inputs and outputs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算表示为图形使我们能够通过计算复杂函数的梯度来执行自动微分。我们只考虑单个操作，并仅查看节点的输入和输出。
- en: There are two different ways of applying the chain rule on a graph – forward
    and backward mode. A detailed explanation of the automatic differentiation in
    both forward and backward mode is beyond the scope of this book; however, in upcoming
    chapters, we will see how TensorFlow implements automatic differentiation in backward
    mode and how it applies the chain rule to compute the loss value and then traverse
    the graph in a backward fashion ![](img/da51f7dc-33d2-4d02-9bbf-e4d33d34bfd9.png) times.
    Automatic differentiation in backward mode depends on the input cardinality and
    not on the number of parameters of the network, compared to implementing it in
    forwarding mode (it's now easy to imagine why TensorFlow implements automatic
    differentiation in backward mode; neural networks can have millions of parameters).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在图上应用链式法则有两种不同的方式——正向模式和反向模式。关于正向模式和反向模式下自动微分的详细解释超出了本书的范围；然而，在接下来的章节中，我们将看到TensorFlow如何在反向模式下实现自动微分，以及它是如何应用链式法则来计算损失值并反向遍历图的
    ![](img/da51f7dc-33d2-4d02-9bbf-e4d33d34bfd9.png) 次。与在正向模式下实现它相比，反向模式下的自动微分依赖于输入的基数，而不是网络的参数数量（现在可以很容易理解为什么TensorFlow在反向模式下实现自动微分；神经网络可能有数百万个参数）。
- en: 'So far, we''ve described optimization algorithms and strategies that can be
    applied to compute the loss function so that it fits the training data. We do
    this by using a generic function that''s been approximated by our neural network.
    In practice, we only introduced one neural network architecture: the fully connected
    architecture. However, there are several different neural network architectures
    that can be applied to solve different problems, depending on the dataset type.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们描述了可以应用于计算损失函数以使其适合训练数据的优化算法和策略。我们通过使用一个被我们的神经网络近似的通用函数来实现这一点。实际上，我们只引入了一个神经网络架构：全连接架构。然而，根据数据集类型，还有几种不同的神经网络架构可以用来解决不同的问题。
- en: One of the strengths of neural networks is their ability to be able to perform
    different tasks, depending on the neuron topology that's used.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个优点是其能够执行不同的任务，这取决于所使用的神经元拓扑结构。
- en: 'The fully connected configuration is a global view on the input—every neuron
    sees everything. However, there are certain types of data that do not require
    a complete view to be correctly used by a neural network, or that are computationally
    intractable with a fully connected configuration. Think about a high-resolution
    image with millions of pixels; we have to connect every neuron to every single
    pixel, creating a network with a number of parameters equal to the number of pixels
    times the number of neurons: a network with only two neurons will lead to [![](img/5a2a2927-7a40-46f2-ac81-728913b0e5ba.png)] parameters—that
    is completely intractable!'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接配置是对输入的全局视图——每个神经元都看到了一切。然而，有些类型的数据不需要完整的视图就可以正确地被神经网络使用，或者在全连接配置下是计算不可行的。想象一下具有数百万像素的高分辨率图像；我们必须将每个神经元连接到每个像素，从而创建一个参数数量等于像素数乘以神经元数的网络：一个只有两个神经元的网络将导致 [![](img/5a2a2927-7a40-46f2-ac81-728913b0e5ba.png)] 参数——这是完全无法处理的！
- en: The architecture that's been developed to work with images, and maybe the most
    important neuronal layer that's been developed in the past years, is the convolutional
    neural network.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 用于处理图像的架构，也许是过去几年中发展的最重要的神经层，是卷积神经网络。
- en: Convolutional neural networks
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolutional Neural Networks** (**CNNs**) are the fundamental building blocks
    of modern computer vision, speech recognition, and even natural language processing
    applications. In this section, we are going to describe the convolution operator,
    how it is used in the signal analysis domain, and how convolution is used in machine
    learning.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是现代计算机视觉、语音识别甚至自然语言处理应用的基本构建块。在本节中，我们将描述卷积算子，它在信号分析领域的使用以及在机器学习中的应用。'
- en: The convolution operator
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积算子
- en: 'Signal theory gives us all the tools we need to properly understand the convolution
    operation: why it is so widely used in many different domains and why CNNs are
    so powerful. The convolution operation is used to study the response of certain
    physical systems when a signal is applied to their input. Different input stimuli
    can make a system, *S*, produce a different output, and the behavior of a system
    can be modeled using the convolution operation.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 信号理论为我们提供了理解卷积操作所需的所有工具：为什么它在许多不同领域中被广泛使用以及为什么CNNs如此强大。当信号应用于其输入时，卷积操作用于研究某些物理系统的响应。不同的输入刺激可以使系统*S*产生不同的输出，并且可以使用卷积操作来建模系统的行为。
- en: Let's start from the one-dimensional case by introducing the concept of the **Linear
    Time-Invariant** (**LTI**) system.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一维情况开始，引入**线性时不变**（**LTI**）系统的概念。
- en: 'A system, *S*, that accepts an input signal and produces an output signal, ![](img/fddf2624-e066-4e73-99ea-5cac2ac7559a.png),
    is an LTI system if the following properties hold:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统*S*接受一个输入信号并产生一个输出信号 ![](img/fddf2624-e066-4e73-99ea-5cac2ac7559a)，则称该系统为LTI系统，如果以下属性成立：
- en: '**Linearity**: ![](img/9aad29ea-c241-4282-a0a8-5fa019acfc75.png)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性性**: ![](img/9aad29ea-c241-4282-a0a8-5fa019acfc75.png)'
- en: '**Time invariance**:** ![](img/cb68d1a7-4a82-4132-90f9-88cfd0bd2d59.png)**'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间不变性**:** ![](img/cb68d1a7-4a82-4132-90f9-88cfd0bd2d59.png)**'
- en: 'Is it possible to analyze the behavior of an LTI system by analyzing its response
    to the Dirac Delta function, δ(t). δ(t) is a function with a value of zero in
    every point of its domain, except in ![](img/a7944924-1073-48ec-8c6b-c738690a2b37.png).
    In ![](img/8509f617-d95d-429c-b9fa-00b13a3a2cda.png), it assumes a value that
    makes its definition true:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可以通过分析LTI系统对Dirac Delta函数 δ(t) 的响应来分析其行为？δ(t) 是一个函数，其在其定义域内的每个点的值均为零，除了在 ![](img/a7944924-1073-48ec-8c6b-c738690a2b37.png)
    处。 在 ![](img/8509f617-d95d-429c-b9fa-00b13a3a2cda.png) 处，它假设一个值，使其定义成立：
- en: '![](img/f04b13a9-d157-4643-90d0-a1917bbd6961.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f04b13a9-d157-4643-90d0-a1917bbd6961.png)'
- en: 'Intuitively, applying δ(t) to a function, φ(t), means sample the φ(t) in 0\.
    Hence, if we put δ(t) as the input of a system, *S*, we get its response to a
    unitary impulse centered on zero. The system output when the input is the Dirac
    Delta function is called the system impulse response, and is noted with the following
    equation:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，将 δ(t) 应用到一个函数 φ(t) 上意味着在 0 处对 φ(t) 进行采样。因此，如果我们将 δ(t) 作为系统 *S* 的输入，则我们得到系统对单位冲击的响应，单位冲击以零为中心。当输入是Dirac
    Delta函数时，系统的输出称为系统的冲激响应，并通过以下方程表示：
- en: '![](img/d22c5c09-6adc-4339-8e02-08a6ceac1d0d.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d22c5c09-6adc-4339-8e02-08a6ceac1d0d.png)'
- en: The system impulse response is of fundamental importance since it allows us
    to compute the response of an LTI system to any input.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的冲激响应具有重要的基础性意义，因为它允许我们计算LTI系统对任何输入的响应。
- en: 'A generic signal, *x(t)*, can be seen as the sum of the value it assumes on
    every instant, *t*. This can be modeled as the application of δ(t) that''s translated
    in every point of the *x* domain:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用信号 *x(t)* 可以看作是它在每个瞬间 *t* 所取的值的总和。这可以通过将 δ(t) 应用到 *x* 域的每个点并进行平移来建模：
- en: '![](img/1c557149-3270-4106-ac6e-779476a177a7.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c557149-3270-4106-ac6e-779476a177a7.png)'
- en: This formula is the definition of convolution among two signals.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式是两个信号之间卷积的定义。
- en: So, why is the convolution operation important for the study of LTI systems?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么卷积操作对LTI系统的研究很重要呢？
- en: 'Given *x(t)* as a generic input signal and *h(t)* as the impulse response of
    an LTI system, we get the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *x(t)* 作为通用输入信号，*h(t)* 作为LTI系统的冲激响应，我们得到以下结果：
- en: '![](img/86a0aff0-a8b6-4c20-b30f-d304290e08af.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86a0aff0-a8b6-4c20-b30f-d304290e08af.png)'
- en: The result of the convolution represents the behavior of the LTI system that's
    modeled by its impulse response, *h(t)*, when *x(t)* is its input. This is an
    important result since it shows us how the impulse response completely characterizes
    the system and how the convolution operation can be used to analyze the output
    of an LTI system when given any input signal.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的结果表示LTI系统的行为，该系统通过其冲激响应 *h(t)* 进行建模，当 *x(t)* 是其输入时。这是一个重要的结果，因为它展示了冲激响应如何完全表征系统，并且卷积操作如何用于分析LTI系统在给定任何输入信号时的输出。
- en: The convolution operation is commutative and the result of the operation is
    a function (a signal).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作是交换的，操作的结果是一个函数（一个信号）。
- en: 'So far, we''ve only considered the continuous case, but there''s a natural
    generalization on the discrete domain. If ![](img/b4bdd466-10c1-410d-bed3-9878b3d6eed8.png)
    and ![](img/79ef65c0-fdd6-4c59-915c-aaa424f1c1e3.png) are defined on ![](img/f25a2724-cd2b-46c9-98fe-31b0ba0d6db3.png),
    the convolution is computed as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了连续情况，但在离散域上有一个自然的推广。如果 ![](img/b4bdd466-10c1-410d-bed3-9878b3d6eed8.png)
    和 ![](img/79ef65c0-fdd6-4c59-915c-aaa424f1c1e3.png) 定义在 ![](img/f25a2724-cd2b-46c9-98fe-31b0ba0d6db3.png)
    上，则卷积按如下方式计算：
- en: '![](img/0a67c9c0-107b-4ef7-ba03-8a5846dd779d.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a67c9c0-107b-4ef7-ba03-8a5846dd779d.png)'
- en: 2D convolution
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2D卷积
- en: 'The generalization of the 1D convolution we''ve introduced in terms of the
    2D case is natural. Images, in particular, can be seen as 2D discrete signals.
    In the 2D case, the counterpart of the Dirac Delta function is the Kronecker Delta
    function, and it can be expressed independently from the dimensionality of the
    space it is used in. It''s seen as a tensor, δ, with components:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在二维情况下引入的1D卷积的推广是自然的。特别是，图像可以被看作是二维离散信号。在二维情况下，Dirac Delta函数的对应物是Kronecker
    Delta函数，它可以独立于所使用空间的维度来表示。它被视为一个张量δ，其分量为：
- en: '![](img/8d7511bf-bc0d-4aad-af0e-9f18c591c32c.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d7511bf-bc0d-4aad-af0e-9f18c591c32c.png)'
- en: Images can be thought as 2D versions of LTI systems. In this case, we are talking
    about **Linear Space-Invariant** (**LSI**) systems.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图像可以被看作是LTI系统的二维版本。在这种情况下，我们讨论的是 **线性空间不变** (**LSI**) 系统。
- en: 'In the bi-dimensional discrete case, the convolution operation is defined as
    follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维离散情况下，卷积操作定义如下：
- en: '![](img/b018f87c-2eed-4f71-b9e1-296ddccd3d9e.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b018f87c-2eed-4f71-b9e1-296ddccd3d9e.png)'
- en: 'Images are finite dimension signals with a well-defined spatial extent. This
    means that the previously introduced formula becomes the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是有限维度的信号，具有明确的空间范围。这意味着之前介绍的公式变成了以下形式：
- en: '![](img/61bcc68e-9af0-4468-94a9-e919c41fb19a.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61bcc68e-9af0-4468-94a9-e919c41fb19a.png)'
- en: 'Here, we have the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '![](img/db207dad-9299-4a07-8d20-8fbe4dff1c48.png) is the input image'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/db207dad-9299-4a07-8d20-8fbe4dff1c48.png)是输入图像'
- en: '![](img/3e983129-600b-4904-a5aa-9c3f107ea69e.png) is the convolutional filter
    (also called the kernel) itself and ![](img/60df3516-8d34-4750-9402-3dc8fa95cf63.png) is
    its side'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/3e983129-600b-4904-a5aa-9c3f107ea69e.png)是卷积滤波器（也称为卷积核）本身，而 ![](img/60df3516-8d34-4750-9402-3dc8fa95cf63.png)是其边长'
- en: '![](img/63514cb4-bf69-4077-9c2f-65eefa156bc9.png)is the output pixel, in the ![](img/a4367dd5-d676-43a4-bd56-ac5183ac20b3.png) position'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/63514cb4-bf69-4077-9c2f-65eefa156bc9.png)是输出像素，位于 ![](img/a4367dd5-d676-43a4-bd56-ac5183ac20b3.png)位置'
- en: 'The operation that we''ve described is performed for every *(i,j)* position
    of the input image that has a complete overlap with the convolutional filter,
    as it slides over the input image:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的操作是对输入图像的每一个*(i,j)*位置执行的，只有当卷积滤波器与该位置完全重叠时，它才会滑动在输入图像上：
- en: '![](img/b73289d2-6c28-4abf-a4d9-80f5b6aacce0.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b73289d2-6c28-4abf-a4d9-80f5b6aacce0.png)'
- en: The convolution operation between the input image (on the left) and the convolution
    kernel produces the feature map on the right
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像（左侧）与卷积核之间的卷积操作产生右侧的特征图
- en: As shown in the preceding diagram, different convolutional filters extract different
    features from the input image. In fact, in the preceding diagram, we can see how
    that rectangular filter (Sobel filter) is able to extract the edges of the input
    image. Convolving an image with a different convolutional filter means having
    to extract different input features that the kernel can capture. Before the introduction
    of convolutional neural networks, as we will see in the next section, we would
    had to manually design convolutional kernels that were able to extract the features
    needed that were to solve the task at hand.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，不同的卷积滤波器从输入图像中提取不同的特征。事实上，在上图中，我们可以看到这个矩形滤波器（Sobel滤波器）是如何提取输入图像的边缘的。用不同的卷积滤波器对图像进行卷积意味着必须提取不同的输入特征，卷积核能够捕捉到这些特征。在卷积神经网络出现之前，正如我们将在下一节看到的那样，我们必须手动设计能够提取所需特征的卷积核，以解决当前的任务。
- en: There are two additional parameters that aren't shown in the preceding formula
    that control how the convolution operation is performed. These parameters are
    the horizontal and vertical stride; they tell the operation how many pixels to
    skip when we move the kernel over the input image over the horizontal and vertical
    directions. Usually, the horizontal and vertical strides are equal, and they are
    noted with the letter S.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个额外的参数没有在前面的公式中显示出来，它们控制卷积操作的执行方式。这些参数是水平和垂直步幅；它们告诉操作在水平和垂直方向上每次移动卷积核时需要跳过多少像素。通常，水平和垂直步幅是相等的，并用字母S表示。
- en: 'If the input image has side ![](img/80e97c32-8975-4de7-81ac-fafe3b917468.png),
    then the resolution of the output signal resulting from the convolution with a
    kernel of size *k* can be computed as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入图像的边长为 ![](img/80e97c32-8975-4de7-81ac-fafe3b917468.png)，那么通过与大小为 *k* 的卷积核进行卷积所产生的输出信号的分辨率可以按以下方式计算：
- en: '![](img/8371c927-70bc-4fd4-ab37-073cbad71ede.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8371c927-70bc-4fd4-ab37-073cbad71ede.png)'
- en: 2D convolutions among volumes
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积在体积间的二维运算
- en: So far, we've only considered the case of a grayscale image, that is, an image
    with a single channel. The images we are used to seeing in real life are all RGB
    images, which are images with three color channels. The convolution operation
    also works well when the input image has more than one channel; in fact, its definition
    has been slightly changed in order to make the convolution operation span every
    channel.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了灰度图像的情况，即只有一个通道的图像。我们在现实生活中常见的图像都是RGB图像，也就是有三个颜色通道的图像。当输入图像有多个通道时，卷积操作同样有效；事实上，它的定义已经稍微变化，以便让卷积操作跨越每一个通道。
- en: This extended version requires the convolution filter to have the same number
    of channels as the input image; in short, if the input image has three channels,
    the convolutional kernel must have three channels too. This way, we are treating
    images as stacks of 2D signals; we call these volumes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个扩展版本要求卷积滤波器的通道数与输入图像相同；简而言之，如果输入图像有三个通道，卷积核也必须有三个通道。这样，我们就将图像视为二维信号的堆叠；我们称这些为体积（volumes）。
- en: As a volume, every image (or convolutional kernel) is identified by the triple
    (W, H, D), where W, H, and D are the width, height, and depth, respectively.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个体积，每个图像（或卷积核）由三元组（W，H，D）来标识，其中 W、H 和 D 分别是宽度、高度和深度。
- en: 'By considering images and kernels as volumes, we can treat them as unordered
    sets. In fact, the order (RGB, BGR) of the channels only changes how the software
    interprets the data, while the content remains the same:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将图像和卷积核视为体积，我们可以将它们视为无序集合。事实上，通道的顺序（RGB，BGR）仅仅改变了软件对数据的解释方式，而内容保持不变：
- en: '![](img/e9634856-f0df-4107-a316-d797f326060e.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9634856-f0df-4107-a316-d797f326060e.png)'
- en: 'This reasoning allows us to extend the previous formula, thereby making it
    take the input depth into account:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这种推理使我们能够扩展之前的公式，从而使其考虑输入的深度：
- en: '![](img/8dc9046b-f480-4014-8808-36a8d9ba40e9.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dc9046b-f480-4014-8808-36a8d9ba40e9.png)'
- en: The result of this convolution operation is called a feature map. Even though
    the convolution is performed among volumes, the output is a feature map with unitary
    depth since the convolution operation sums the feature maps that have been produced
    to take into account all the information of the pixels that share the same spatial
    (x,y) location. In fact, summing the resulting D feature maps is a way to treat a
    set of 2D convolutions as a single 2D convolution.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这个卷积操作的结果叫做特征图。即使卷积是在体积之间进行的，输出也是一个具有单一深度的特征图，因为卷积操作将已生成的特征图相加，以考虑所有共享相同空间（x，y）位置的像素的信息。事实上，将得到的
    D 个特征图相加是一种将一组二维卷积处理为单个二维卷积的方法。
- en: This means that every single position of the resulting activation map, *O*,
    contains the information that was captured from the same input location through
    its complete depth. This is the intuitive idea behind the convolution operation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着结果激活图的每个位置，*O*，都包含了从相同输入位置通过其完整深度捕获的信息。这就是卷积操作背后的直观思想。
- en: Alright; we now have a grasp of the convolution operation in 1 and two spatial
    dimensions; we also introduced the concept of convolutional kernel highlighting
    whereby defining the kernel value is a manual operation where different kernels
    can extract different features from the input image/volume.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 好的；现在我们已经掌握了在一维和二维空间维度中的卷积操作；我们还引入了卷积核高亮的概念，即通过定义卷积核值是一种手动操作，其中不同的卷积核可以从输入图像/体积中提取不同的特征。
- en: 'The process of kernel definition is pure engineering, and defining them is
    not easy: different tasks can require different kernels; some of them have never
    been defined, and most of them can be simply impossible to design since certain
    features can only be extracted by processing a processed signal, which means we
    would have to apply the convolution operation on the result of another convolution
    operation (a cascade of convolution operations).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核的定义过程是纯粹的工程工作，定义它们并不容易：不同的任务可能需要不同的卷积核；其中一些卷积核以前从未定义过，而大多数卷积核的设计几乎是不可能的，因为某些特征只能通过处理过的信号来提取，这意味着我们必须在另一个卷积操作的结果上应用卷积操作（卷积操作的级联）。
- en: 'Convolutional neural networks solve this problem: instead of manually defining
    the convolutional kernels, we can just define convolutional kernels made of neurons.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络解决了这个问题：我们不再手动定义卷积核，而是可以定义由神经元构成的卷积核。
- en: We can extract features from the input volume by convolving it with multiple
    volumes of filters and combining them while considering the feature maps that
    extract new input for a new convolutional layer.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过与多个滤波器体积进行卷积并将它们组合，来从输入体积中提取特征，同时考虑提取新输入的特征图，作为新的卷积层的输入。
- en: 'The deeper the network becomes, the more abstract the extracted feature becomes.
    One of the greatest strengths of CNNs is their ability to combine features that
    have been extracted, ranging from raw, basic features that were extracted by the
    first convolutional layers to high-level abstract features that were extracted
    by the last layers and learned as a combination of the low-level features that
    were extracted by the other layers:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 网络越深，提取的特征越抽象。CNNs 的一个最大优点是能够将提取的特征进行组合，从由第一层卷积层提取的原始、基础特征，到最后一层提取的高级抽象特征，这些高级特征是由其他层提取的低级特征的组合学习而成：
- en: '![](img/15c9bc11-2a4e-426e-a274-c813b1901dcd.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15c9bc11-2a4e-426e-a274-c813b1901dcd.png)'
- en: CNNs learn to extract low-level features in the first layers; as the networks
    become deeper, the abstraction level of the extracted features increases. Image
    from Zeiler and Fergus, 2013.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 学习在第一层提取低级特征；随着网络变得更深，提取的特征的抽象层次也会增加。图片来自 Zeiler 和 Fergus，2013 年。
- en: Another advantage of convolutional layers with respect to fully connected layers
    is their local-view nature.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层相对于全连接层的另一个优点是其局部视图性质。
- en: To process an image, a fully connected layer has to linearize the input image
    and create a connection from every pixel value to every neuron of the layer. The
    memory requirements are huge, and making every neuron see the whole input isn't
    the ideal way to extract meaningful features.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理一张图像，全连接层必须将输入图像线性化，并从每个像素值到该层的每个神经元创建连接。这样内存需求非常大，并且让每个神经元看到整个输入并不是提取有意义特征的理想方式。
- en: 'There are certain features that, due to their nature, are not global like the
    ones that are captured by a fully connected layer. Instead, they are local. For
    example, the edges of an object are local features to a certain input region,
    not the whole image. Therefore, CNNs can learn to extract only local features
    and combine them in the following layers. Another advantage of convolutional architectures
    is their low number of parameters: they don''t need to see (and thus create connections)
    the whole input; they only need to have a view of their local receptive field.
    Convolution operations requires fewer parameters to extract meaningful feature
    maps, all of which capture the local features of the input volume.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其特性，某些特征不像全连接层捕获的那样是全局的，而是局部的。例如，物体的边缘是特定输入区域的局部特征，而不是整个图像的特征。因此，CNNs 可以学习仅提取局部特征，并在后续层中将其组合。卷积架构的另一个优点是其较少的参数数量：它们不需要查看（从而创建连接）整个输入；它们只需要查看局部感受野。卷积操作需要更少的参数来提取有意义的特征图，这些特征图都捕捉了输入体积的局部特征。
- en: CNNs are usually used with another layer, known as the pooling layer. Without
    digging too much into the details of this operation (it tends to be avoided in
    today's architectures), we can just think about it as an operation with the same
    structure as the convolution operation (hence a window that moves in the horizontal
    and vertical direction of the input) but without a learnable kernel. In every
    region of the input, a non-learnable function is applied. The aim of this operation
    is to reduce the size of the feature maps that are produced by a convolution operation
    in order to reduce the number of parameters of the network.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 通常与另一层一起使用，称为池化层。在不深入探讨该操作细节的情况下（它在今天的架构中往往被避免），我们可以将其视为与卷积操作结构相同的操作（因此是一个在输入的水平和垂直方向上移动的窗口），但没有可学习的内核。在输入的每个区域中，应用一个不可学习的函数。该操作的目的是减少卷积操作生成的特征图的大小，从而减少网络的参数数量。
- en: 'So that we have an idea of what the common convolutional neural network architecture
    looks like, the following diagram presents the LeNet 5 architecture that uses
    a convolutional layer, max-pooling (a pooling operation where the non-learnable
    function is the max operation over the window), and fully connected layers with
    the aim of classifying images of handwritten digits in 10 classes:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们了解常见的卷积神经网络架构是什么样子，以下图展示了 LeNet 5 架构，它使用了卷积层、最大池化（最大池化是一种池化操作，其中不可学习的函数是窗口内的最大值操作）以及全连接层，目的是对手写数字图像进行
    10 类分类：
- en: '![](img/c66bb426-c824-4e25-bd3a-f95e53bb6147.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c66bb426-c824-4e25-bd3a-f95e53bb6147.png)'
- en: 'LeNet 5 architecture – each plane is a feature map. Source: Gradient-Based
    Learning Applied to Document Recognition, Yann LeCun at al—1998'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet 5 架构——每个平面是一个特征图。来源：Gradient-Based Learning Applied to Document Recognition,
    Yann LeCun 等——1998 年
- en: Defining network architectures such as LeNet 5 is an art – there are no precise
    rules on the number of layers you can use, the number of convolutional filters
    to learn, or the number of neurons in the fully connected layers. Moreover, even
    picking the right activation function for the hidden layer is another hyperparameter
    to search for. Complex models are not only rich in terms of learnable parameters,
    but also rich in terms of hyperparameters to tune, making the definition of deep
    architectures non-trivial and challenging.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 定义像 LeNet 5 这样的网络架构是一门艺术——没有关于可以使用的层数、要学习的卷积滤波器数量或全连接层中神经元数量的精确规则。此外，甚至选择隐藏层的正确激活函数也是另一个要搜索的超参数。复杂模型不仅在可学习参数方面丰富，而且在调整的超参数方面也很丰富，使得定义深度架构非常复杂和具有挑战性。
- en: Convolutions among volumes allow us to do fancy things such as replace every
    fully connected layer with a 1 x 1 x D convolutional layer and use a 1 x 1 x D
    convolution inside the network to reduce the dimensionality of the input.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 体积之间的卷积允许我们做一些花哨的事情，例如用 1 x 1 x D 卷积层替换每个全连接层，并在网络内部使用 1 x 1 x D 卷积来减少输入的维度。
- en: 1 x 1 x D convolutions
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 x 1 x D 卷积
- en: '![](img/a17c5c54-a4e3-4ba9-ba02-982cc9de6279.png) convolutions are important
    building blocks of state-of-the-art models because they can be used for different
    goals.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/a17c5c54-a4e3-4ba9-ba02-982cc9de6279.png) 卷积是最先进模型的重要构建模块，因为它们可以用于不同的目标。'
- en: One goal is the use them as a dimensionality reduction technique. Let's understand
    this by going through an example.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一个目标是将它们用作降维技术。我们通过一个例子来理解这一点。
- en: If the convolution operation is applied to an input volume of ![](img/f4d95ccf-5747-4e0b-8f50-8fc3ca4eb3ef.png)
    and it is convolved with a set of ![](img/a8512514-2b3d-43f7-8829-10aed76735cc.png)
    filters, each one being ![](img/2696cd65-26ca-4860-bc34-741a5e6a5cf9.png) in size,
    the number of features is reduced from 512 to ![](img/741dd8e9-80e5-4610-a8a6-7769d32a5ea7.png).
    The output volume now has a shape of ![](img/62854add-6e95-40c1-a316-a9ba6e08a489.png).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将卷积操作应用于一个输入体积为 ![](img/f4d95ccf-5747-4e0b-8f50-8fc3ca4eb3ef.png)，并且与一组大小为
    ![](img/a8512514-2b3d-43f7-8829-10aed76735cc.png) 的滤波器进行卷积，每个滤波器的尺寸为 ![](img/2696cd65-26ca-4860-bc34-741a5e6a5cf9.png)，则特征数从
    512 降至 ![](img/741dd8e9-80e5-4610-a8a6-7769d32a5ea7.png)。输出体积现在的形状为 ![](img/62854add-6e95-40c1-a316-a9ba6e08a489.png)。
- en: 'A ![](img/a17c5c54-a4e3-4ba9-ba02-982cc9de6279.png) convolution is also equivalent
    to a fully connected layer. The main difference lies in the nature of the convolution
    operator and the architectural structure of the fully connected layer: while the
    latter requires the input to have a fixed size, the former accepts every volume
    with a spatial extent greater than or equal to![](img/fb51544f-def0-408d-90f3-c2f30f12a92c.png) as
    input. A ![](img/583a75a4-89fe-4dee-933c-49a54a4111b7.png) convolution can therefore
    substitute any fully connected layer because of this equivalence.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 ![](img/a17c5c54-a4e3-4ba9-ba02-982cc9de6279.png) 卷积也等效于一个全连接层。主要区别在于卷积操作符的性质和全连接层的架构结构：后者要求输入具有固定大小，而前者接受每个具有大于或等于
    ![](img/fb51544f-def0-408d-90f3-c2f30f12a92c.png) 空间范围的体积作为输入。因此，一个 ![](img/583a75a4-89fe-4dee-933c-49a54a4111b7.png)
    卷积可以替代任何全连接层，因为它们是等效的。
- en: Additionally, the ![](img/09084700-e4d3-441f-ada9-d1696ab3e2e4.png) convolutions
    not only reduce the features in the input to the next layer but also introduce
    new parameters and new non-linearity into the network that could help increase
    the model's accuracy.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此外， ![](img/09084700-e4d3-441f-ada9-d1696ab3e2e4.png) 卷积不仅将输入的特征减少到下一层，还引入了新的参数和新的非线性到网络中，这可能有助于提高模型的准确性。
- en: When a ![](img/6a28df78-051d-47f5-80d7-98940333115d.png) convolution is placed
    at the end of a classification network, it acts exactly like a fully connected
    layer, but instead of thinking about it as a dimensionality reduction technique,
    it's more intuitive to think about it as a layer that will output a tensor with
    a shape of ![](img/139f5c3a-ae49-4e6e-b7a6-826f91453690.png). The spatial extent
    of the output tensor (identified by W and H) is dynamic and is determined by the
    locations of the input image that the network analyzed.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 ![](img/6a28df78-051d-47f5-80d7-98940333115d.png) 卷积放置在分类网络的末尾时，它的作用就像一个全连接层，但是不要把它看作是降维技术，更直观地看待它作为一个输出形状为
    ![](img/139f5c3a-ae49-4e6e-b7a6-826f91453690.png) 的张量的层。输出张量的空间范围（由 W 和 H 标识）是动态的，并由网络分析的输入图像的位置确定。
- en: If the network has been defined with an input of 200 x 200 x 3 and we give it
    an image with this size as input, the output will be a map with ![](img/b5135bd2-7d73-4628-a087-69cc2ce2241b.png)
    and ![](img/b2f3fa0e-99ca-4682-8263-47b7d2e77e70.png). However, if the input image
    has a spatial extent greater than ![](img/68dd81b4-620a-4db0-a698-8cf7be4d6ff0.png),
    then the convolutional network will analyze different locations of the input image
    (just like a standard convolution does, since it's not possible to consider the
    whole convolutional architecture as a convolution operation with its own kernel
    side and stride parameters) and will produce a tensor with ![](img/b431996f-3149-42b6-83d2-e24390645ef7.png)
    and ![](img/50c37809-537b-4428-bbd0-d6739caf3158.png). This is not possible with
    a fully connected layer that constrains the network to accept a fixed-size input
    and produce a fixed-size output.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络定义时输入大小为 200 x 200 x 3，并且我们给定一个相同大小的图像作为输入，输出将是一个包含 ![](img/b5135bd2-7d73-4628-a087-69cc2ce2241b.png)
    和 ![](img/b2f3fa0e-99ca-4682-8263-47b7d2e77e70.png) 的地图。然而，如果输入图像的空间范围大于 ![](img/68dd81b4-620a-4db0-a698-8cf7be4d6ff0.png)，则卷积网络将分析输入图像的不同位置（就像标准卷积一样，因为不可能将整个卷积架构视为具有自己内核大小和步幅参数的卷积操作），并生成一个包含
    ![](img/b431996f-3149-42b6-83d2-e24390645ef7.png) 和 ![](img/50c37809-537b-4428-bbd0-d6739caf3158.png)
    的张量。这是全连接层无法实现的，因为全连接层限制了网络只接受固定大小的输入并生成固定大小的输出。
- en: '![](img/441b5a58-8d8a-4a9e-a518-d4b64f64ccad.png) convolutions are also the
    fundamental building blocks of semantic segmentation networks, as we will see
    in the upcoming chapters.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/441b5a58-8d8a-4a9e-a518-d4b64f64ccad.png) 卷积也是语义分割网络的基本构建模块，正如我们将在接下来的章节中看到的那样。'
- en: Convolutional, pooling, and fully connected layers are the building blocks of
    almost every neural network architecture that's used nowadays to solve computer
    vision tasks such as image classification, object detection, semantic segmentation,
    image generation, and many others!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层、池化层和全连接层是几乎所有神经网络架构的基本构建模块，这些架构如今被广泛用于解决计算机视觉任务，如图像分类、目标检测、语义分割、图像生成等许多问题！
- en: We will implement all of these neural network architectures using TensorFlow
    2.0 in the upcoming chapters.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中使用 TensorFlow 2.0 实现所有这些神经网络架构。
- en: Although CNNs have a reduced number of parameters, even this model can suffer
    from the problem of overfitting when used in a deep configuration (a stack of
    convolutional layers).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卷积神经网络（CNN）有较少的参数，但即使是这种模型，在深度配置（堆叠的卷积层）中使用时，也可能会遇到过拟合问题。
- en: Hence, another fundamental topic any ML practitioner should be aware of is regularization.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何机器学习实践者都应该了解的另一个基本话题是正则化。
- en: Regularization
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: 'Regularization is a way to deal with the problem of overfitting: the goal of
    regularization is to modify the learning algorithm, or the model itself, to make
    the model perform well—not just on the training data, but also on new inputs.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是处理过拟合问题的一种方法：正则化的目标是修改学习算法或模型本身，使模型不仅能在训练数据上表现良好，而且能在新的输入数据上也表现良好。
- en: One of the most widely used solutions to the overfitting problem—and probably
    one of the most simple to understand and analyze—is known as **dropout**.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 解决过拟合问题的最广泛使用的解决方案之一——可能也是最容易理解和分析的——被称为 **dropout**（丢弃法）。
- en: Dropout
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）
- en: The idea of dropout is to train an ensemble of neural networks and average the
    results instead of training only a single standard network. Dropout builds new
    neural networks, starting from a standard neural network, by dropping out neurons
    with ![](img/6e9b401e-2a38-422f-b6b8-4c4d06475673.png) probability.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）的思想是训练一个神经网络集成，并对结果进行平均，而不是仅训练一个标准的网络。Dropout通过以 ![](img/6e9b401e-2a38-422f-b6b8-4c4d06475673.png)
    的概率丢弃神经元，从标准神经网络开始，构建新的神经网络。
- en: 'When a neuron is dropped out, its output is set to zero. This is shown in the
    following diagram:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个神经元被丢弃时，它的输出会被设为零。如下图所示：
- en: '![](img/e5a3e4ba-804f-4bb5-aa91-1769ceb28f2b.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5a3e4ba-804f-4bb5-aa91-1769ceb28f2b.png)'
- en: 'On the left, a standard fully connected architecture. On the right, a possible
    network architecture that''s been obtained by dropping out neurons, which means
    it used dropout during the training phase. Source: Dropout: A simple way to Prevent
    Neural Networks from Overfitting - N. Srivastava—2014'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是标准的全连接架构，右侧是通过丢弃神经元得到的网络架构，这意味着在训练阶段使用了丢弃法。来源：Dropout：一种防止神经网络过拟合的简单方法 -
    N. Srivastava—2014
- en: The dropped neurons do not contribute to the training phase. Since neurons are
    dropped randomly at each new training iteration, using dropout makes the training
    phase different every time. In fact, using dropout means that every training step
    is performed on a new network—and even better, a network with a different topology.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 被丢弃的神经元不参与训练阶段。由于神经元在每次新的训练迭代中都是随机丢弃的，使用丢弃法使得每次训练阶段都不同。事实上，使用丢弃法意味着每次训练步骤都是在一个新的网络上执行——更好的是，这个网络具有不同的拓扑结构。
- en: 'N. Srivastava et al. in *Dropout: A simple way to Prevent Neural Networks from
    Overfitting (the *paper that introduced this regularization technique) explained
    this concept very well:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 'N. Srivastava等人在*Dropout: A simple way to Prevent Neural Networks from Overfitting*（介绍该正则化技术的论文）中很好地解释了这一概念：'
- en: <q>"In a standard neural network, the derivative that's received by each parameter
    tells it how it should change, so the final loss function is reduced, given what
    all the other units are doing. Therefore, units may change in a way that they
    fix the mistakes of the other units.</q> This may lead to complex co-adaptations.
    This, in turn, leads to overfitting because these co-adaptations do not generalize
    to unseen data. We hypothesize that, for each hidden unit, dropout prevents co-adaptation
    by making the presence of other hidden units unreliable.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: <q>在标准神经网络中，每个参数收到的导数会告诉它如何变化，以便减少最终的损失函数，考虑到所有其他单元的行为。因此，单元可能会以一种修正其他单元错误的方式变化。</q>
    这可能会导致复杂的共适应关系。反过来，这会导致过拟合，因为这些共适应关系无法推广到未见过的数据。我们假设，对于每个隐藏单元，丢弃法通过使其他隐藏单元的存在变得不可靠，从而防止共适应。
- en: Therefore, a hidden unit cannot rely on other specific units to correct its
    mistakes."
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，隐藏单元不能依赖其他特定单元来纠正其错误。
- en: Dropout works well in practice because it prevents the co-adaption of neurons
    during the training phase. In the upcoming sections, we will analyze how dropout
    works and how it is implemented.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃法在实际中效果良好，因为它防止了在训练阶段神经元之间的共适应。在接下来的章节中，我们将分析丢弃法如何工作以及它是如何实现的。
- en: How dropout works
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丢弃法的工作原理
- en: 'We can analyze how dropout works by looking at its application on a single
    neuron. Let''s say we have the following:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过观察丢弃法在单个神经元上的应用来分析其工作原理。假设我们有以下内容：
- en: '![](img/2c58e518-fe94-4e6a-84d1-2f6a7c08b864.png) as a linear neuron'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/2c58e518-fe94-4e6a-84d1-2f6a7c08b864.png) 作为线性神经元'
- en: '![](img/fbbd7bb5-e61d-42bb-a8eb-5805dd1050b4.png) as an activation function'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/fbbd7bb5-e61d-42bb-a8eb-5805dd1050b4.png) 作为激活函数'
- en: 'By using these, it is possible to model the application of dropout – in the
    training phase only – as a modification of the activation function:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些方法，我们可以将丢弃法的应用——仅在训练阶段——建模为激活函数的修改：
- en: '![](img/0a39aef6-94c0-4700-a6e8-4c5dbdbe0c41.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a39aef6-94c0-4700-a6e8-4c5dbdbe0c41.png)'
- en: Here,
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，
- en: '![](img/708655cb-6a8c-4fa3-be9b-8dafaa1e374d.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/708655cb-6a8c-4fa3-be9b-8dafaa1e374d.png)'
- en: Is a ![](img/1eb41971-a3f1-4a84-b7a0-9adad3a37fd1.png)-dimensional vector of
    Bernoulli random variables, ![](img/cab4fb0c-9771-43f5-9c00-c146c6d1b89c.png).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个![](img/1eb41971-a3f1-4a84-b7a0-9adad3a37fd1.png)维伯努利随机变量的向量，![](img/cab4fb0c-9771-43f5-9c00-c146c6d1b89c.png)。
- en: 'A Bernoulli random variable has the following probability mass distribution:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 一个伯努利随机变量具有以下概率质量分布：
- en: '![](img/ae9d177b-578f-4440-b6c8-3ad4776e6568.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae9d177b-578f-4440-b6c8-3ad4776e6568.png)'
- en: 'Here, ![](img/5e33fffd-7b0c-461f-8cbf-feb02b635fe5.png) is the possible outcomes.
    The Bernoulli random variable correctly models the dropout application on a neuron
    since the neuron is turned off with the probability of ![](img/b8b68122-db98-4ed8-a819-97393c50ea2a.png) and
    kept on otherwise. It can be useful to see the application of dropout on the generic
    *i*-th neuron of a fully-connected layer (but the same holds for the application
    on a single neuron of a convolutional layer):'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/5e33fffd-7b0c-461f-8cbf-feb02b635fe5.png) 是可能的结果。伯努利随机变量正确地建模了丢弃法在神经元上的应用，因为神经元以![](img/b8b68122-db98-4ed8-a819-97393c50ea2a.png)的概率被关闭，否则保持开启。在完全连接层的通用*i*号神经元上应用丢弃法的情况可能是有用的（但在卷积层的单个神经元上应用时也是一样的）：
- en: '![](img/46bc5ccb-a72c-42ae-b8a2-105927df6069.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46bc5ccb-a72c-42ae-b8a2-105927df6069.png)'
- en: Here, ![](img/f7f094f4-5367-4d05-9ebc-a0db16b34ad1.png).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/f7f094f4-5367-4d05-9ebc-a0db16b34ad1.png)。
- en: During training, a neuron is kept on with probability ![](img/28839481-ee56-4492-9416-b00020fdda79.png).
    Therefore, during the test phase, we have to emulate the behavior of the ensemble
    of networks that were used in the training phase. To do this, we need to scale
    the neuron's output by a factor of ![](img/5cc47d91-61c6-4c8b-b790-b78ccdcb4bdd.png)
    d.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，一个神经元以概率![](img/28839481-ee56-4492-9416-b00020fdda79.png)保持激活。因此，在测试阶段，我们需要模拟训练阶段使用的神经网络集群的行为。为此，我们需要将神经元的输出按![](img/5cc47d91-61c6-4c8b-b790-b78ccdcb4bdd.png)
    d的因子进行缩放。
- en: 'Thus, we have the following:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到以下结果：
- en: '![](img/d3d37f89-abe1-4b0d-9aea-ecb48f55661f.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3d37f89-abe1-4b0d-9aea-ecb48f55661f.png)'
- en: Inverted dropout
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向 dropout
- en: A slightly different approach—and the one that's used in practice in almost
    every deep learning framework – is to use inverted dropout. This approach consists
    of scaling the activations during the training phase, with the obvious advantage
    of not having to change the network architecture during the test phase.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一种稍微不同的方法——几乎所有深度学习框架中使用的做法——是使用反向 dropout。此方法包括在训练阶段缩放激活，显而易见的优势是不需要在测试阶段改变网络架构。
- en: 'The scale factor is the inverse of the keep probability,![](img/0e09929f-15bb-4375-9347-a236fcc47928.png),
    and so we have the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子是保持概率的倒数，![](img/0e09929f-15bb-4375-9347-a236fcc47928.png)，因此我们得到以下结果：
- en: '**![](img/f300107d-65f3-4b9c-8596-814671b1644d.png)**'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/f300107d-65f3-4b9c-8596-814671b1644d.png)**'
- en: Inverted dropout is how dropout is implemented in practice because it helps
    us define the model and just change a parameter (the keep/drop probability) to
    train and test on the same model.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 反向 dropout 是 dropout 在实践中的实现方式，因为它帮助我们定义模型，并仅通过更改一个参数（保持/丢弃概率）就可以在相同的模型上进行训练和测试。
- en: Direct dropout, which is the version that was presented in the previous section,
    forces you to modify the network during the test phase because, if you don't multiply
    by ![](img/7831c757-0f28-4927-8ee2-c8d3114b6cbd.png), the neuron will produce
    values that are higher with respect to the one expected by the successive neurons
    (thus the following neurons can saturate or explode). This is why inverted dropout
    is the more common implementation.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 直接 dropout，即在上一节中介绍的版本，迫使你在测试阶段修改网络，因为如果不乘以![](img/7831c757-0f28-4927-8ee2-c8d3114b6cbd.png)，神经元会产生比后续神经元期望的值更高的值（从而使后续神经元可能饱和或爆炸）。这就是为什么反向
    dropout 更为常见的实现方式。
- en: Dropout and L2 regularization
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout 和 L2 正则化
- en: Dropout is often used with L2 normalization and other parameter constraint techniques,
    but this is not always the case.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 常常与 L2 归一化和其他参数约束技术一起使用，但并非总是如此。
- en: 'Normalization helps keep model parameter values low. In this way, a parameter
    can''t grow too much. In brief, the L2 normalization is an additional term to
    the loss, where ![](img/c7924e24-0afd-429a-be70-96b6044340c5.png) is a hyperparameter
    called regularization strength, ![](img/178b99c3-c3e5-4a59-90b3-ee4614ce70ab.png) is
    the model, and ![](img/67310229-1d29-4969-8e5e-c61e2a05b3b7.png) is the error
    function between the real ![](img/9f579730-4561-4fcb-8f58-63013856e698.png) and
    the predicted ![](img/11375194-8a2b-43b3-85bc-f999060f0900.png) value:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化有助于保持模型参数值较低。这样，参数就不会增长过多。简而言之，L2 归一化是损失函数的附加项，其中![](img/c7924e24-0afd-429a-be70-96b6044340c5.png)是一个名为正则化强度的超参数，![](img/178b99c3-c3e5-4a59-90b3-ee4614ce70ab.png)是模型，![](img/67310229-1d29-4969-8e5e-c61e2a05b3b7.png)是实际值![](img/9f579730-4561-4fcb-8f58-63013856e698.png)与预测值![](img/11375194-8a2b-43b3-85bc-f999060f0900.png)之间的误差函数：
- en: '![](img/f15888b1-63ed-4ab7-9fa1-be92c76fefe6.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f15888b1-63ed-4ab7-9fa1-be92c76fefe6.png)'
- en: 'It''s easy to understand that this additional term, when we''re doing back-propagation
    via gradient descent, reduces the update amount. If ![](img/2a9e1749-082f-4022-9843-3626a2515e02.png)
    is the learning rate, the update amount of the parameter ![](img/a69ccf6f-1f31-4683-ac2e-37595c5e3715.png)
    is as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解，当我们通过梯度下降进行反向传播时，这个附加项减少了更新量。如果![](img/2a9e1749-082f-4022-9843-3626a2515e02.png)是学习率，那么参数![](img/a69ccf6f-1f31-4683-ac2e-37595c5e3715.png)的更新量如下：
- en: '![](img/ee04dc3e-0b77-4110-b2e5-28d38c70d816.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee04dc3e-0b77-4110-b2e5-28d38c70d816.png)'
- en: Dropout alone does not have any way of preventing parameter values from becoming
    too large during this update phase.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用 Dropout 在更新阶段并没有任何防止参数值过大的机制。
- en: There are two other solutions that are extremely easy to implement that do not
    even require the model to be changed or for the loss to have additional terms.
    These are known as data augmentation and early stopping.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两种极其容易实现的解决方案，它们甚至不需要改变模型或在损失函数中加入额外的项。这些被称为数据增强和提前停止。
- en: Data augmentation
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is a simple way to increase the dataset's size. This is done
    by applying a set of transformations on the train data. Its aim is to make the
    model aware that certain input variations are possible and thus make it perform
    better on a variety of input data.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种简单的方法，用于增加数据集的大小。通过对训练数据应用一系列变换来实现。它的目的是让模型意识到某些输入变化是可能的，从而在各种输入数据上表现得更好。
- en: 'The set of transformations highly depends on the dataset itself. Usually, when
    working with an image dataset, the transformations to apply are as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 变换集高度依赖于数据集本身。通常，在处理图像数据集时，需要应用的变换如下：
- en: Random flip left/right
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机左右翻转
- en: Random flip up/down
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机上下翻转
- en: Adding random noise to the input image
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向输入图像添加随机噪声
- en: Random brightness variation
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机亮度变化
- en: Random saturation variation
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机饱和度变化
- en: 'However, before applying any of these transformations to our training set,
    we have to ask: *is this transformation meaningful for this data type, for my
    dataset, and for the task at hand?*'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在对训练集应用任何这些变换之前，我们必须问自己：*这个变换对于这种数据类型、我的数据集和当前任务来说有意义吗？*
- en: 'Just think about the random flip left/right of the input image: if our dataset
    is a dataset of drawn arrows, each labeled with its direction, and we are training
    a model to predict the arrow''s direction, mirroring the image will just break
    our training set.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 试想一下输入图像的随机左右翻转：如果我们的数据集是由标有方向的箭头图像组成，并且我们训练模型来预测箭头的方向，那么翻转图像只会破坏我们的训练集。
- en: Early stopping
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提前停止
- en: As we introduced in [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What
    is Machine Learning?**, *measuring the performance of the model during the training
    phase on both the validation and training sets is a good habit.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第一章](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml)中介绍的，*什么是机器学习？*，*在训练阶段测量模型在验证集和训练集上的表现是一个好习惯。*
- en: This good habit can help us prevent overfitting and save us a lot of training
    time since the measured metrics tell us whether the model is starting to overfit
    the training data and thus if it is time to stop the training process.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这个好习惯可以帮助我们防止过拟合，并节省大量的训练时间，因为测量指标能告诉我们模型是否开始过拟合训练数据，从而判断是否是停止训练过程的时候。
- en: Let's think about a classifier—we measure the validation accuracy, the training
    accuracy, and the loss value.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来思考一个分类器——我们衡量验证准确率、训练准确率和损失值。
- en: Looking at the loss value, we can see that, as the training process goes on,
    the loss decreases. Of course, this is true only for healthy training. Training
    is healthy when the loss trend decreases. It is possible to just observe the fluctuation
    that was introduced by mini-batch gradient descent or the usage of the stochastic
    regularization process (dropout).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 观察损失值时，我们可以看到，随着训练过程的进行，损失在下降。当然，这仅适用于健康的训练。训练是健康的，当损失趋势下降时。你也可能只是看到由于小批量梯度下降或使用随机正则化过程（dropout）引起的波动。
- en: If the training process is healthy and the loss trend decreases, the training
    accuracy will increase. Training accuracy measures how well the model learns the
    training set—it does not capture its generalization capabilities. Validation accuracy,
    on the other hand, is the measure of how good the predictions of your model are
    on unseen data.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练过程是健康的，并且损失趋势在下降，那么训练准确率将会提高。训练准确率衡量模型对训练集的学习效果——它并不能捕捉到模型的泛化能力。而验证准确率则是衡量模型在未见过的数据上的预测能力。
- en: If the model is learning, the validation accuracy increases. If the model is
    overfitting, the validation accuracy stops increasing and can even start to decrease,
    while the accuracy measured on the training set reaches the maximum value.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在学习，验证准确率会增加。如果模型发生过拟合，验证准确率会停止增加，甚至可能开始下降，而在训练集上的准确率则达到最大值。
- en: If you stop training the model as soon as the validation accuracy (or whatever
    the monitored metric is) stops increasing, then you are facing the overfitting
    problem easily and effectively.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在验证准确率（或任何被监控的指标）停止增长时立即停止训练模型，那么你就能轻松有效地解决过拟合问题。
- en: Data augmentation and early stopping are two ways of reducing overfitting without
    changing the model's architecture.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强和早期停止是减少过拟合的两种方法，且不需要改变模型的架构。
- en: However, similar to dropout, there is another common regularization technique,
    known as batch normalization, that requires that we change the model architecture
    that we use. This helps speed up the training process and lets us achieve better
    performance.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，类似于 dropout，还有另一种常见的正则化技术，称为批量归一化，它要求我们改变使用的模型架构。这有助于加速训练过程，并让我们获得更好的性能。
- en: Batch normalization
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Batch normalization is not only a regularization technique—it is also a good
    way to speed up the training process. To increase the stability of the learning
    process, and thus reduce the oscillation of the loss function, batch normalization
    normalizes the output of a layer by subtracting the batch mean and dividing it
    by the batch standard deviation.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化不仅是一种正则化技术——它也是加速训练过程的好方法。为了提高学习过程的稳定性，从而减少损失函数的波动，批量归一化通过减去批次均值并除以批次标准差来规范化某一层的输出。
- en: 'After this normalization, which is not a learned process, batch normalization
    adds two trainable parameters: the standard deviation parameter (gamma) and the
    mean parameter (beta).'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个归一化过程中（它不是一个学习过程），批量归一化添加了两个可训练的参数：标准差参数（gamma）和均值参数（beta）。
- en: Batch normalization not only helps speed up convergence by reducing the training
    oscillations, it also helps in reducing overfitting since it introduces stochasticity
    in the training process in a way that's similar to dropout. The difference is
    that, while dropout adds noise in an explicit manner, batch normalization introduces
    stochasticity by computing the mean and the variance over the batch.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化不仅通过减少训练过程中的波动来加速收敛，而且还帮助减少过拟合，因为它在训练过程中引入了类似于 dropout 的随机性。不同之处在于，dropout
    通过显式的方式添加噪声，而批量归一化通过计算批次的均值和方差来引入随机性。
- en: 'The following image, which was taken from the original paper, *Batch Normalization
    – Accelerating Deep Network Training*, by Reducing Internal Covariate Shift (Ioffe
    et al. 2015), shows the algorithm that''s applied during the training process:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 下图来自原始论文《*批量归一化——通过减少内部协方差偏移加速深度网络训练*》（Ioffe 等，2015），展示了在训练过程中应用的算法：
- en: '![](img/42174206-4adb-457d-9c54-396c7322aeca.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42174206-4adb-457d-9c54-396c7322aeca.png)'
- en: 'The batch normalization algorithm. Source: *Batch Normalization – Accelerating
    Deep Network Training by Reducing Internal Covariate Shift*, Ioffe et al. 2015'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化算法。来源：*批量归一化——通过减少内部协方差偏移加速深度网络训练*，Ioffe 等，2015
- en: At the end of the training process, it is required that you apply the same affine
    transformation that was learned during the training process. However, instead
    of computing the mean and the variance over the input batch, the mean and the
    variance that accumulated during the training process are used. In fact, batch
    normalization, just like dropout, has a different behavior during the training
    and inference phases. During the training phase, it computes the mean and variance
    over the current input batch, while it accumulates the moving mean and variance
    use during the inference phase.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程结束时，要求应用在训练过程中学习到的相同仿射变换。然而，和计算输入批次的均值和方差不同，使用的是训练过程中积累的均值和方差。事实上，批量归一化就像
    dropout 一样，在训练阶段和推理阶段的行为是不同的。在训练阶段，它计算当前输入批次的均值和方差，而在推理阶段，它使用训练过程中积累的均值和方差。
- en: Fortunately, since this is a very common operation, TensorFlow has a BatchNormalization
    layer ready to use, so we don't have to worry about the accumulation of statistics
    during the training and having to change the layer's behavior during the inference
    phase.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于这是一个非常常见的操作，TensorFlow 已经提供了一个可以直接使用的 BatchNormalization 层，因此我们不必担心在训练过程中统计量的积累，以及在推理阶段需要改变层的行为。
- en: Summary
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter is probably the most theory intensive of this whole book; however,
    it is required that you have at least an intuitive idea of the building blocks
    of neural networks and of the various algorithms that are used in machine learning
    so that you can start developing a meaningful understanding of what's going on.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章可能是整本书中最理论密集的一章；然而，你至少需要对神经网络的构建模块以及机器学习中使用的各种算法有一个直观的理解，才能开始理解其中发生的事情。
- en: We have looked at what a neural network is, what it means to train it, and how
    to perform a parameter update with some of the most common update strategies.
    You should now have a basic understanding of how the chain rule can be applied
    in order to compute the gradient of a function efficiently.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了什么是神经网络，训练神经网络意味着什么，以及如何使用一些最常见的更新策略进行参数更新。现在，你应该对如何应用链式法则以高效计算函数的梯度有了基本的理解。
- en: We haven't explicitly talked about deep learning, but in practice, that is what
    we did; keep in mind that stacking layers of neural networks is like stacking
    different classifiers that combine their expressive power. We indicated this with
    the term deep learning. In practice, we can say that deep neural networks (a deep
    learning model) are just neural networks with more than one hidden layer.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有明确讨论深度学习，但在实际应用中，这就是我们所做的；请记住，堆叠神经网络层就像堆叠不同的分类器，它们结合了各自的表达能力。我们用“深度学习”这个术语来表示这一点。实际上，我们可以说，深度神经网络（一个深度学习模型）就是具有多个隐藏层的神经网络。
- en: Later in this chapter, we introduced a lot of important concepts about parametric
    model training, the origin of neural networks, as well as their mathematical formulation.
    It is of extreme importance to have at least an intuitive idea of what happens
    when we define a fully connected (among others) layer when we define the loss
    and use a certain optimization strategy to train a model using a machine learning
    framework such as TensorFlow.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 本章后面，我们介绍了许多关于参数化模型训练、神经网络的起源以及它们的数学公式的重要概念。理解在定义全连接层（以及其他层）、定义损失函数并使用特定的优化策略来训练模型时发生的事情，至少需要有一个直观的概念，这是极其重要的，特别是在使用诸如TensorFlow这样的机器学习框架时。
- en: TensorFlow hides the complexity of everything we've described in this chapter,
    but having an understanding of what happens under the hood will allow you to debug
    a model just by looking at its behavior. You will also have an idea of why certain
    things happen during the training phase and how to solve certain problems. For
    instance, knowledge of optimization strategies will help you understand why your
    loss function value follows a certain trend and assumes certain values during
    the training phase, and will give you an idea of how to choose the right hyperparameters.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏了我们在本章中描述的所有复杂性，但理解底层发生了什么，将使你能够通过观察模型的行为来调试它。你还将了解为什么在训练阶段会发生某些事情，并知道如何解决特定问题。例如，了解优化策略将帮助你理解为什么损失函数的值在训练阶段遵循某种趋势并假定某些值，并能帮助你选择正确的超参数。
- en: In the next chapter, [Chapter 3](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml), *TensorFlow
    Graph Architecture*, we will see how all the theoretical concepts presented in
    this chapter, using the graph representation of the computation, can be effectively
    implemented in TensorFlow.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第3章](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml)，*TensorFlow 图架构*中，我们将看到如何使用计算的图表示将本章中介绍的所有理论概念在TensorFlow中有效实现。
- en: Exercises
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'This chapter was filled with various theoretical concepts to understand so,
    just like the previous chapter, don''t skip the exercises:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 本章充满了各种理论概念需要理解，因此，和上一章一样，不要跳过练习：
- en: What are the similarities between artificial and biological neurons?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工神经元和生物神经元之间有哪些相似之处？
- en: Does the neuron's topology change the neural network's behavior?
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经元的拓扑结构是否会改变神经网络的行为？
- en: Why do neurons require a non-linear activation function?
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么神经元需要非线性激活函数？
- en: If the activation function is linear, a multi-layer neural network is the same
    as a single layer neural network. Why?
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果激活函数是线性的，那么多层神经网络与单层神经网络是一样的，为什么？
- en: How is an error in input data treated by a neural network?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络如何处理输入数据中的错误？
- en: Write the mathematical formulation of a generic neuron.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出一个通用神经元的数学公式。
- en: Write the mathematical formulation of a fully connected layer.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出一个全连接层的数学公式。
- en: Why can a multi-layer configuration solve problems with non-linearly separable
    solutions?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么多层配置能够解决非线性可分的解问题？
- en: Draw the graph of the sigmoid, tanh, and ReLu activation functions.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制sigmoid、tanh和ReLu激活函数的图像。
- en: Is it always required to format training set labels into a one-hot encoded representation?
    What if the task is regression?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否总是需要将训练集标签格式化为独热编码表示？如果任务是回归问题怎么办？
- en: 'The loss function creates a bond between the desired outcome and the model
    output: why is this required for the loss function to be differentiable?'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数在期望输出与模型输出之间建立了联系：为什么损失函数需要是可微的，这一点很重要？
- en: What does the gradient of the loss function indicate? What about the anti-gradient?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数的梯度表示什么？反向梯度呢？
- en: What is a parameter update rule? Explain the vanilla update rule.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是参数更新规则？解释经典的更新规则。
- en: Write the mini-batch gradient descent algorithm and explain the three possible
    scenarios.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出小批量梯度下降算法并解释三种可能的情景。
- en: Is random perturbation a good update strategy? Explain the pros and cons of
    this approach.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机扰动是一个好的更新策略吗？解释这种方法的优缺点。
- en: What's the difference between a non-adaptive and adaptive optimization algorithm?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非自适应优化算法与自适应优化算法有什么区别？
- en: What's the relationship between the concept of velocity and momentum update?
    Describe the momentum update algorithm.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 速度与动量更新的概念有什么关系？描述动量更新算法。
- en: What is an LTI system? How is it related to the convolution operation?
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是LTI系统？它与卷积操作有什么关系？
- en: What is a feature vector?
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是特征向量？
- en: Are CNNs feature extractors? If yes, can a fully connected layer be used to
    classify the output of a convolutional layer?
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是特征提取器吗？如果是，能否使用全连接层对卷积层的输出进行分类？
- en: What are the guidelines for model parameter initialization? Is assigning a constant
    value of 10 to every parameter of the network a good initialization strategy?
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型参数初始化的准则是什么？将每个网络参数都赋值为常数10是一个好的初始化策略吗？
- en: What are the differences between direct and inverted dropout? Why does TensorFlow
    implement the inverted version?
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接dropout与倒置dropout有何区别？为什么TensorFlow实现了倒置版本？
- en: Why is the L2 normalization of network parameters useful when using dropout?
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当使用dropout时，网络参数的L2正则化为什么有用？
- en: 'Write the formula of convolution among volumes: show how it behaves in the
    case of a 1 x 1 x D convolutional kernel. Why is there an equivalence between
    the fully connected layer and a 1 x 1 x D convolution?'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出卷积在体积（volumes）之间的公式：展示当卷积核为1 x 1 x D时它的行为。为什么全连接层与1 x 1 x D卷积是等价的？
- en: If, while training a classifier, the validation accuracy stops increasing, what
    does this mean? Can adding dropout or increasing the drop probability if dropout layers
    are already present make the network improve the validation accuracy again? Why
    or why not?
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在训练分类器时，验证精度停止提高，这意味着什么？如果已经存在dropout层，增加dropout或提高drop概率能让网络再次提高验证精度吗？为什么或为什么不？
