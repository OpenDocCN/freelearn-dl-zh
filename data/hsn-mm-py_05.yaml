- en: Parameter Inference Using the Bayesian Approach
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用贝叶斯方法进行参数推断
- en: 'In the previous chapter, we discussed inferring the parameters using the maximum-likelihood
    approach. In this chapter, we will explore the same issue through a Bayesian approach.
    The main topics are as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了使用最大似然法推断参数。在本章中，我们将通过贝叶斯方法探讨相同的问题。主要内容如下：
- en: Introduction to Bayesian learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯学习简介
- en: Bayesian learning in HMMs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型中的贝叶斯学习
- en: Approximate algorithms for estimating distributions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于估计分布的近似算法
- en: Bayesian learning
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯学习
- en: 'In the maximum-likelihood approach to learning, we try to find the most optimal
    parameters for our model that maximizes our likelihood function. But data in real
    life is usually really noisy, and in most cases, it doesn''t represent the true
    underlying distribution. In such cases, the maximum-likelihood approach fails.
    For example, consider tossing a fair coin a few times. It is possible that all
    of our tosses result in either heads or tails. If we use a maximum-likelihood
    approach on this data, it will assign a probability of 1 to either heads or tails,
    which would suggest that we would never get the other side of the coin. Or, let''s
    take a less extreme case: let''s say we toss a coin 10 times and get three heads
    and seven tails. In this case, a maximum-likelihood approach will assign a probability
    of 0.3 to heads and 0.7 to tails, which is not the true distribution of a fair
    coin. This problem is also commonly known as **overfitting**.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大似然学习方法中，我们试图找到使得似然函数最大化的最优参数。但现实中的数据通常是非常嘈杂的，并且在大多数情况下，它并不能代表真实的底层分布。在这种情况下，最大似然方法会失败。例如，考虑投掷一枚公平的硬币几次。可能我们的所有投掷结果都是正面或反面。如果我们对这些数据使用最大似然方法，它会将正面或反面分别赋予概率1，意味着我们永远不会得到另一面。或者，假设我们投掷硬币10次，得到了三次正面和七次反面。在这种情况下，最大似然方法会将正面的概率定为0.3，反面的概率定为0.7，这并不是公平硬币的真实分布。这个问题也通常被称为**过拟合**。
- en: Bayesian learning takes a slightly different approach to learn these parameters.
    We start by assigning a prior distribution over the parameters of our model. The
    prior makes our assumptions about the model explicit. In the case of tossing the
    coin, we can start by using a prior that assigns equal probabilities to both heads
    and tails. Then we apply the Bayes theorem to compute the posterior distribution
    over our parameters based on the data. This allows us to shift our belief (prior)
    toward where the data points to, and this makes us do a less extreme estimate
    of the parameters. And in this way, Bayesian learning can solve one of the major
    drawbacks of maximum likelihood.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯学习采用略有不同的方法来学习这些参数。我们首先为模型的参数分配一个先验分布。先验使我们对模型的假设变得显式。在投掷硬币的例子中，我们可以从一个将正面和反面赋予相等概率的先验开始。然后，我们应用贝叶斯定理根据数据计算参数的后验分布。这使我们能够根据数据将我们的信念（先验）向数据的指向转移，从而进行更为温和的参数估计。通过这种方式，贝叶斯学习能够解决最大似然方法的一个主要缺点。
- en: 'In more general terms, in the case of Bayesian learning, we try to learn a
    distribution over the parameters of our model instead of learning a single parameter
    that maximizes the likelihood. For learning this distribution over the parameters,
    we use the Bayes theorem, given by the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般来说，在贝叶斯学习的情况下，我们尝试学习模型参数的分布，而不是学习一个能够最大化似然的单一参数。为了学习这种参数分布，我们使用贝叶斯定理，公式如下：
- en: '![](img/0f629c8c-5429-46a3-a693-2f2d0b609354.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f629c8c-5429-46a3-a693-2f2d0b609354.png)'
- en: 'Here, *P(θ)* is our prior over the parameters of the model, *P(D|θ)* is the
    likelihood of the data given the parameters, and *P(D)* is the probability of
    the observed data. *P(D)* can also be written in terms of prior and likelihood
    as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*P(θ)* 是我们对模型参数的先验分布，*P(D|θ)* 是给定参数下数据的似然，*P(D)* 是观察到数据的概率。*P(D)* 也可以通过先验和似然表示如下：
- en: '![](img/dbdfc5df-65e2-41d4-a980-28e5f75354ec.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dbdfc5df-65e2-41d4-a980-28e5f75354ec.png)'
- en: Now let's talk about each of these terms separately and see how can we compute
    them. The prior, *P(θ)*, is a probability distribution over the parameters representing
    our belief about the values of the parameters. For example, in the case of coin
    tossing, we can have our initial belief as *θ* is in between 0 and 1 and is uniformly
    distributed. The likelihood term, *P(D|θ)*, is the same term that we tried to
    maximize in [Chapter 4](8d06a68a-e427-4f7d-9472-9be25b5351c0.xhtml), *Parameter
    Inference using Maximum Likelihood*. It represents how likely our observed data
    is, given the parameters of the model. The next term, *P(D)*, is the probability
    of observing our data and it acts as the normalizing term. It is computationally
    difficult to compute because it requires us to sum over all the possible values
    of *θ* and, for any sufficiently large number of parameters, it quickly becomes
    intractable. In the next sections of this chapter, we will see the different algorithms
    that we can use to approximate these values. The term that we are trying to compute,
    *P(D|θ),* is known as the **posterior**. It represents our final probability distribution
    over the parameters of the model given our observed data. Basically, our prior
    is updated using the likelihood term to give the final distribution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们分别讨论这些术语，看看我们如何计算它们。先验，*P(θ)*，是一个参数的概率分布，代表我们对参数值的信念。例如，在抛硬币的情况下，我们可以将初始信念设为*θ*在0到1之间，并且是均匀分布的。似然项，*P(D|θ)*，是我们在[第4章](8d06a68a-e427-4f7d-9472-9be25b5351c0.xhtml)中尝试最大化的相同项，*使用最大似然进行参数推断*。它表示给定模型参数的情况下，观测数据的可能性。下一个术语，*P(D)*，是观测到数据的概率，它充当归一化项。由于它要求我们对所有可能的*θ*值求和，因此计算上非常困难，对于任何足够多的参数，计算会迅速变得不可处理。在本章的接下来的部分中，我们将看到可以用来近似这些值的不同算法。我们试图计算的术语，*P(D|θ)*，被称为**后验**。它表示给定我们的观测数据后，模型参数的最终概率分布。基本上，我们的先验通过使用似然项来更新，得到最终的分布。
- en: Another problem that Bayesian learning solves is the model selection. Since
    Bayesian learning gives a distribution over the different possible models rather
    than a single model, we have a couple of options of how we want to do predictions
    from these models. The first method is to just select a specific model that has
    the maximum probability, which is also commonly known as the **Maximum Aposteriori**
    (**MAP**) estimate. The other possible way is to compute the expectation of the
    prediction from all the models based on the posterior distribution. This allows
    us to regularize our predictions since we are computing expectation over all possible
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯学习解决的另一个问题是模型选择。由于贝叶斯学习为不同的可能模型提供了一个分布，而不是单一的模型，我们有几种方式可以从这些模型中进行预测。第一种方法是选择具有最大概率的特定模型，这也通常被称为**最大后验（MAP）**估计。另一种可能的方法是基于后验分布计算所有模型的预测期望。这使我们能够对预测进行正则化，因为我们是在对所有可能的模型进行期望计算。
- en: Selecting the priors
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择先验
- en: A common question when doing Bayesian learning is how to select the appropriate
    prior. As David Mackay has said, *there is no inference without assumptions, we
    need to make a guess for the prior*. Our prior should be representative of what
    we think the most likely parameters are for our model. A huge benefit of using
    our own prior is that we make our assumption about the model explicit. Once we
    start applying Bayes, theorem using our prior and the observed data, our posterior
    would be a shift from our prior toward a distribution that represents our data
    better.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行贝叶斯学习时，一个常见的问题是如何选择合适的先验。正如大卫·麦凯所说，*没有假设就没有推理，我们需要对先验做出一个猜测*。我们的先验应该代表我们认为模型中最可能的参数。使用我们自己的先验的一个巨大好处是，我们使得关于模型的假设变得明确。一旦我们开始应用贝叶斯定理，使用我们的先验和观测数据，后验分布就会从我们的先验向一个更能代表数据的分布偏移。
- en: Theoretically, this sounds good as we can probably select very complex priors
    that capture our idea of the model, but for applying the Bayes theorem, we need
    to multiply our prior with the likelihood, and for complex distributions, it very
    quickly becomes computationally intractable. Therefore, in practice, we usually
    select a prior that is a conjugate distribution to our likelihood. A conjugate
    prior allows us to have a closed-form solution to the Bayes theorem. Because of
    this, Gaussian distributions are used for priors and likelihoods as multiplying
    a Gaussian distribution with another Gaussian distribution results in a Gaussian
    distribution. Also computations it's not expensive to compute the product of two
    Gaussians.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，这听起来不错，因为我们可以选择非常复杂的先验分布来捕捉我们对模型的理解，但在应用贝叶斯定理时，我们需要将先验与似然相乘，而对于复杂的分布，这很快会变得计算上不可处理。因此，在实践中，我们通常选择与似然分布共轭的先验分布。共轭先验使我们能够得到贝叶斯定理的闭式解。正因为如此，高斯分布被用作先验和似然，因为将一个高斯分布与另一个高斯分布相乘的结果仍然是一个高斯分布。而且，计算两个高斯分布的乘积并不昂贵。
- en: Intractability
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不可处理性
- en: 'Apart from selecting difficult priors, another source of intractability in
    Bayesian learning is the denominator term of Bayes'' theorem:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择困难的先验分布外，贝叶斯学习中的另一个不可处理性的来源是贝叶斯定理中的分母项：
- en: '![](img/94eab2cf-86ad-4a4d-8333-2fe419d34fd4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94eab2cf-86ad-4a4d-8333-2fe419d34fd4.png)'
- en: As we can see in the preceding equation for computing *P(D)*, we need to compute
    a summation over all the possible values of *θ*, which is the set of all the parameters
    of our model. If we have a lot of parameters in our model, it is computationally
    intractable to compute this term since the size of the term grows exponentially
    with the number of parameters. A lot of work has been done to approximate this
    value, as we will see in the next section of this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的公式中看到的，为了计算*P(D)*，我们需要对所有可能的*θ*值求和，而*θ*是我们模型中所有参数的集合。如果我们模型中的参数很多，那么计算这个项在计算上是不可处理的，因为项的大小会随着参数数量的增加而呈指数增长。很多工作已经致力于近似这个值，正如我们在本章的下一节中将看到的那样。
- en: Bayesian learning in HMM
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HMM中的贝叶斯学习
- en: As we saw in the previous section, in the case of Bayesian learning we assume
    all the variables as a random variable, assign a prior to it, and then try to
    compute the posterior based on that. Therefore, in the case of HMM, we can assign
    a prior on our transition probabilities, emission probabilities, or the number
    of observation states.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中看到的，在贝叶斯学习的情况下，我们假设所有变量都是随机变量，给它们分配一个先验，然后根据这个先验计算后验。因此，在HMM的情况下，我们可以对我们的转移概率、发射概率或观测状态的数量分配先验。
- en: Therefore, the first problem that we need to solve is to select the prior. Theoretically,
    a prior can be any distribution over the parameters of the model, but in practice,
    we usually try to use a conjugate prior to the likelihood, so that we have a closed-form
    solution to the equation. For example, in the case when the output of the HMM
    is discrete, a common choice of prior is the Dirichlet distribution. It is mainly
    for two reasons, the first of which is that the Dirichlet distribution is a conjugate
    distribution to multinomial distribution which allows us to multiply them easily.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要解决的第一个问题是选择先验。从理论上讲，先验可以是模型参数上的任何分布，但在实践中，我们通常尝试使用与似然分布共轭的先验，这样我们就可以得到方程的闭式解。例如，在HMM输出为离散的情况下，常见的先验选择是Dirichlet分布。这主要有两个原因，第一个原因是Dirichlet分布是多项式分布的共轭分布，允许我们轻松地将它们相乘。
- en: '**Conjugate distribution**: A family of priors is said to be conjugate to a
    family of likelihoods if the posterior obtained by multiplying the prior by the
    likelihood is in the same family of distribution as the prior distribution.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**共轭分布**：如果通过将先验与似然相乘得到的后验分布属于与先验分布相同的分布族，则称一族先验分布是与一族似然分布共轭的。'
- en: 'For example, since the likelihood of the initial state given the *π* parameter
    vector is multinomial:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于给定*π*参数向量时，初始状态的似然是多项式分布：
- en: '![](img/d5960650-e1ce-46e9-8598-7c16419a6e9c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5960650-e1ce-46e9-8598-7c16419a6e9c.png)'
- en: 'And if the prior probability of *π* is Dirichlet:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*π*的先验概率是Dirichlet分布：
- en: '![](img/1b0a0141-4f7d-4935-a5f1-661689aafd0f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b0a0141-4f7d-4935-a5f1-661689aafd0f.png)'
- en: 'Where *u = [u[1], u[2], ..., u[K]]* is the hyperparameter vector and *Z* is
    the normalizing constant. We can now compute the posterior from the likelihood
    and the prior, which is given as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *u = [u[1], u[2], ..., u[K]]* 是超参数向量，*Z* 是归一化常数。我们现在可以根据似然函数和先验来计算后验，其公式如下：
- en: '![](img/df4cf2c1-e1bf-4876-91f9-e7338aa89ada.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df4cf2c1-e1bf-4876-91f9-e7338aa89ada.png)'
- en: And we can see that the posterior is also a Dirichlet distribution. Hence we
    can say that the Dirichlet distribution is a conjugate prior to the multinomial
    distribution. And in a similar way, we can set up Dirichlet priors for our transition
    matrix and emission matrix.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到后验分布也是一个狄利克雷分布。因此，我们可以说狄利克雷分布是多项式分布的共轭先验。以类似的方式，我们可以为我们的转移矩阵和发射矩阵设定狄利克雷先验。
- en: The second reason for choosing a Dirichlet prior is that it has the desirable
    property that its hyperparameters can be interpreted as a hypothetical count of
    observations. In the preceding example, if *u[i] = 2* and *u[j] = 1* for *j ≠
    i*, the MAP estimate of *π* would be the same as a maximum-likelihood estimation
    with the assumption that the training data had an extra data point with the initial
    state being in state *i*. This conjugate property allows us to do MAP estimation
    in the case of Dirichlet priors by doing a minor variation in the Baum-Welch algorithm.
    It also gives theoretical justification for the seemingly ad hoc but very common
    regularization method for HMMs, which just adds a small positive number to all
    elements of the parameter vector.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 选择狄利克雷先验的第二个原因是它具有一个理想的性质，即其超参数可以解释为假设的观测次数。在前面的例子中，如果 *u[i] = 2* 且 *u[j] =
    1* （对于 *j ≠ i*），那么 *π* 的MAP估计将与最大似然估计相同，假设训练数据中有一个额外的数据点，其初始状态为 *i*。这种共轭性质使得我们可以通过对Baum-Welch算法做一个小的变动来进行MAP估计。这也为隐马尔可夫模型中看似临时但非常常见的正则化方法提供了理论依据，该方法只是将一个小的正数加到参数向量的所有元素上。
- en: In the last couple of paragraphs, we talked specifically about the case when
    the output is discrete. But the same concepts can be extended to the case of continuous
    output as well. Conjugate distributions exist in the case of continuous distributions
    as well. One of the most commonly used distributions is the Gaussian distribution
    as it stays in the same family after different operations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的几段中，我们专门讨论了输出为离散的情况。但相同的概念也可以扩展到连续输出的情况。在连续分布的情况下也存在共轭分布。最常用的分布之一是高斯分布，因为它在进行不同操作后仍然保持在同一分布族中。
- en: Approximating required integrals
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近似所需的积分
- en: As we discussed before, the Bayesian approach treats all unknown quantities
    as random variables. We assign prior distributions to these variables and then
    estimate the posterior distribution over these after the data is observed. In
    the case of HMMs, the unknown quantities comprise the structure of the HMM, that
    is, the number of states, the parameters of the network, and the hidden states.
    Unlike maximum-likelihood or MAP estimations, in which we find point estimates
    for these parameters, we now have distributions over these parameters. This allows
    us to compare between model structures, but for doing that we need to integrate
    over both the parameters and the hidden states of the model. This is commonly
    known as Bayesian integration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，贝叶斯方法将所有未知量视为随机变量。我们为这些变量分配先验分布，然后在观察到数据后估计这些变量的后验分布。在隐马尔可夫模型（HMM）的情况下，未知量包括HMM的结构，即状态的数量、网络的参数和隐藏状态。与最大似然估计或MAP估计不同，在这些方法中我们为这些参数找到点估计，而现在我们有这些参数的分布。这使得我们可以在模型结构之间进行比较，但为了做到这一点，我们需要对模型的参数和隐藏状态进行积分。这通常被称为贝叶斯积分。
- en: Since these integrations are computationally intractable, we resort to approximate
    methods to compute these values. In the next few subsections, we will give an
    overview of some of these methods. A detailed analysis of these methods is outside
    the scope of this book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些积分在计算上难以处理，我们采用近似方法来计算这些值。在接下来的几个小节中，我们将概述其中的一些方法。这些方法的详细分析超出了本书的范围。
- en: Sampling methods
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采样方法
- en: Sampling methods are one of the most common ways to estimate intractable distributions.
    The general idea is to sample points from the distribution space in a way such
    that we get more samples from high-probability areas. And then based on these
    samples we estimate the distributions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 采样方法是估计难以处理的分布的最常见方式之一。其基本思想是以某种方式从分布空间中采样，使得我们能从高概率区域获得更多的样本。然后基于这些样本，我们估计分布。
- en: Laplace approximations
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拉普拉斯近似
- en: 'Laplace approximations use the central limit theorem, which from well-behaved
    priors and data asserts that the posterior parameter will converge in the limit
    of a large number of training samples to a Gaussian around the MAP estimate of
    the parameters. To estimate the evidence using the Laplace approximation, MAP
    parameters are found in the usual optimization routines and then the Hessian of
    the log-likelihood is computed at the MAP estimate. The evidence is approximated
    by evaluating the *P(θ,D)/P(θ|D)* ratio at the MAP estimate of *θ*, using the
    Gaussian approximation in the denominator. The Laplace approximation suffers from
    several disadvantages:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯近似使用中央极限定理，该定理从表现良好的先验和数据中得出，后验参数将在大量训练样本的极限中收敛到围绕参数 MAP 估计的高斯分布。为了使用拉普拉斯近似估计证据，首先在常规优化过程中找到
    MAP 参数，然后在 MAP 估计处计算对数似然的海森矩阵。证据通过在 MAP 估计的 *θ* 处评估 *P(θ,D)/P(θ|D)* 比率来近似，分母使用高斯近似。拉普拉斯近似有几个缺点：
- en: Computing the Hessian matrix from the parameters is usually very costly
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从参数计算海森矩阵通常非常昂贵。
- en: The Gaussian approximation is not very good for models with parameters that
    are positive and sum to 1, especially when there are many parameters relative
    to the size of the dataset
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯近似对于参数为正且总和为 1 的模型并不适用，特别是当相对于数据集的大小有许多参数时。
- en: For these reasons, the Laplace approximation is usually not used for HMMs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，拉普拉斯近似通常不用于隐马尔可夫模型（HMM）。
- en: Stolke and Omohundro's method
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stolke 和 Omohundro 的方法
- en: In the famous paper *HMM induction by Bayesian model merging* Stolke and Omohundro
    present a new technique for approximating the Bayesian integrals of HMMs. Consider the
    case of having all the states of the HMM to be observed and the priors to be Dirichlet
    distributions. In this case, when learning the parameters using Bayesian learning,
    the posteriors are also going to be Dirichlet distributions, and then the evidence
    integral can be represented as a product of Dirichlet integrals, which can be
    easily computed. Therefore, in a sense, we can say that the reason for the intractability
    of evidence integrals is the fact that the states and parameters are hidden.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在著名的论文 *HMM 通过贝叶斯模型合并进行归纳* 中，Stolke 和 Omohundro 提出了一种新的技术，用于近似 HMM 的贝叶斯积分。考虑一种情况，所有的
    HMM 状态都是观察到的，且先验是狄利克雷分布。在这种情况下，当使用贝叶斯学习法学习参数时，后验也将是狄利克雷分布，然后证据积分可以表示为狄利克雷积分的乘积，这些积分可以轻松计算。因此，从某种意义上讲，我们可以说，证据积分难以处理的原因是状态和参数是隐藏的。
- en: Stolke and Omohundro's method proposed to find the single most likely sequence
    of hidden states using a Viterbi-like algorithm and using this sequence as observed
    states. Using these observed values, we can easily do evidence integrals. The
    method proposes to iterate between these two steps, incrementally searching over
    model structures, merging or splitting states based on comparisons of this approximate
    evidence. In their paper, Stolke and Omohundro show that this method of trading
    off integration over hidden variables by integrating over parameters is able to
    get good results.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Stolke 和 Omohundro 提出了一种方法，旨在使用类似维特比算法的方法找到最可能的隐藏状态序列，并将此序列作为观察状态。使用这些观察值，我们可以轻松进行证据积分。该方法建议在这两个步骤之间迭代，逐步搜索模型结构，根据近似证据的比较合并或拆分状态。在他们的论文中，Stolke
    和 Omohundro 表明，通过将对隐藏变量的积分替代为对参数的积分，这种方法能够获得良好的结果。
- en: Variational methods
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分方法
- en: Variational methods are another very common method used for approximating distributions.
    The general idea is to start by choosing a simpler family of distributions and
    then try to find the hyperparameters of this distribution such that the distribution
    is as close as possible to our original distribution. There are different metrics
    that are used to determine the closeness of two distributions; the most commonly
    used metric is Kullback-Leiber divergence. This method basically converts an inference
    problem into an optimization problem where we try to minimize our divergence metric.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 变分方法是另一种非常常见的近似分布的方法。一般思路是从选择一个更简单的分布族开始，然后尝试找到这个分布的超参数，使得该分布尽可能接近我们的原始分布。用于确定两个分布接近度的度量有多种；最常用的度量是
    Kullback-Leibler 散度。这种方法本质上将推理问题转化为一个优化问题，在这个问题中我们尝试最小化我们的散度度量。
- en: 'In the case of HMMs, we usually make an assumption that the hidden states are
    independent of the parameters of the model. This allows us to approximate distributions
    over both the hidden states and parameters simultaneously. More specifically,
    the evidence can be lower bounded by applying Jenson''s inequality twice:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HMMs 的情况下，我们通常假设隐藏状态与模型的参数是独立的。这使得我们可以同时近似隐藏状态和参数的分布。更具体地说，通过应用 Jensen 不等式两次，可以对证据进行下界估计：
- en: '![](img/f6441a3c-4b9b-4f5f-b46e-47a3ede9e751.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6441a3c-4b9b-4f5f-b46e-47a3ede9e751.png)'
- en: The variational Bayesian approach iteratively maximizes ![](img/a53434d0-4c21-4db9-9f3f-611212895d68.png) as
    a functional of the two free distributions, *Q(S) *and *Q(θ)*. In the preceding
    equations, we can see that this maximization is equivalent to minimizing the KL
    divergence between *Q(S)Q(θ)* and the joint posterior over hidden states and the *P(S,θ|D,M)* parameters.
    David MacKay first presented a variational Bayesian approach to learning in HMMs.
    He assumed the prior to be a Dirichlet distribution, making the assumption that
    the parameters are independent of the hidden states, he showed that the optimal
    *Q(θ)* is a Dirichlet distribution. Furthermore, he showed that the optimal *Q(S)* could
    be obtained by applying the forward-backward algorithm to an HMM with pseudo-parameters
    given by ![](img/881223eb-6a9e-4209-a083-d9fa362eac42.png), which can be evaluated
    for Dirichlet distributions. Thus the whole variational Bayesian method can be
    implemented as a simple modification of the Baum-Welch algorithm. Essentially
    we can state that the variational Bayesian method is a combination of special
    cases of both the MAP approach and Stolke and Omohundro's approach. This is very
    promising, especially given that it has been used successfully for non-trivial
    model-structure learning in other models; its potential has not been fully explored
    for HMMs and their extensions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 变分贝叶斯方法通过迭代地最大化 ![](img/a53434d0-4c21-4db9-9f3f-611212895d68.png) 作为两个自由分布，*Q(S)*
    和 *Q(θ)* 的泛函。在前述方程中，我们可以看到，这种最大化等价于最小化 *Q(S)Q(θ)* 和隐藏状态及 *P(S,θ|D,M)* 参数的联合后验之间的
    KL 散度。David MacKay 首次提出了一种变分贝叶斯方法来进行 HMMs 中的学习。他假设先验是一个狄利克雷分布，并且假设参数与隐藏状态是独立的，他展示了最优的
    *Q(θ)* 是一个狄利克雷分布。此外，他还展示了可以通过将前向-后向算法应用于具有伪参数的 HMM 来获得最优的 *Q(S)*，这些伪参数由 ![](img/881223eb-6a9e-4209-a083-d9fa362eac42.png) 给出，且可以对狄利克雷分布进行评估。因此，整个变分贝叶斯方法可以作为
    Baum-Welch 算法的简单修改来实现。本质上，我们可以说，变分贝叶斯方法是 MAP 方法和 Stolke 与 Omohundro 方法的特殊情况的结合。这是非常有前景的，尤其是考虑到它在其他模型中已成功应用于非平凡的模型结构学习；它在
    HMMs 及其扩展中的潜力尚未得到充分探索。
- en: Code
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: Currently, there are no packages in Python that support learning using Bayesian
    learning and it would be really difficult to write the complete code to fit in
    this book. And even though there are a lot of advantages to using Bayesian learning,
    it is usually computationally infeasible in a lot of cases. For these reasons,
    we are skipping the code for Bayesian learning in HMMs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Python 中没有支持使用贝叶斯学习进行学习的包，而且编写完整的代码以适应本书的内容是非常困难的。尽管使用贝叶斯学习有很多优点，但在很多情况下，它通常在计算上是不可行的。基于这些原因，我们跳过了隐藏马尔可夫模型（HMMs）中的贝叶斯学习代码。
- en: Summary
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about applying Bayesian learning in the case of learning
    parameters in HMMs. Bayesian learning has a few benefits over the maximum-likelihood
    estimator, but it turns out to be computationally quite expensive except when
    we have closed-form solutions. Closed-form solutions are only possible when we
    use conjugate priors. In the following chapters, we will discuss detailed applications
    of HMMs for a wide variety of problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在隐马尔可夫模型（HMMs）中应用贝叶斯学习来学习参数。贝叶斯学习相比最大似然估计器有一些优势，但除非我们有闭式解，否则它在计算上是相当昂贵的。闭式解只有在使用共轭先验时才有可能。接下来的章节中，我们将讨论隐马尔可夫模型在各种问题中的详细应用。
