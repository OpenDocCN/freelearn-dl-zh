- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Unleashing Machine Learning Potentials in Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在自然语言处理中释放机器学习的潜力
- en: In this chapter, we will delve into the fundamentals of **Machine Learning**
    (**ML**) and preprocessing techniques that are essential for **natural language
    processing** (**NLP**) tasks. ML is a powerful tool for building models that can
    learn from data, and NLP is one of the most exciting and challenging applications
    of ML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究机器学习（ML）的基本原理和预处理技术，这些技术对于自然语言处理（NLP）任务是必不可少的。机器学习是构建可以从数据中学习的模型的有力工具，而自然语言处理（NLP）是机器学习中最激动人心和最具挑战性的应用之一。
- en: By the end of this chapter, you will have gained a comprehensive understanding
    of data exploration, preprocessing, and data split, know how to deal with imbalanced
    data techniques, and learned about some of the common ML models required for successful
    ML, particularly in the context of NLP.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将全面了解数据探索、预处理和数据拆分，了解如何处理不平衡数据技术，并了解一些成功机器学习所需的常见机器学习模型，特别是在自然语言处理（NLP）的背景下。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Data exploration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索
- en: Common ML models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见机器学习模型
- en: Model underfitting and overfitting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型欠拟合和过拟合
- en: Splitting data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据拆分
- en: Hyperparameter tuning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Ensemble models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型
- en: Handling imbalanced data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据
- en: Dealing with correlated data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理相关数据
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Prior knowledge of programming languages, particularly Python, is assumed in
    this chapter and subsequent chapters of this book. It is also expected that you
    have already gone through previous chapters to become acquainted with the necessary
    linear algebra and statistics concepts that will be discussed in detail.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以及本书后续章节假定你具备编程语言（尤其是Python）的先验知识。还期望你已经阅读了前面的章节，以便熟悉将在本章详细讨论的必要线性代数和统计学概念。
- en: Data exploration
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: When working in a methodological environment, datasets are often well known
    and preprocessed, such as Kaggle datasets. However, in real-world business environments,
    one important task is to define the dataset from all possible sources of data,
    explore the gathered data to find the best method for preprocessing it, and ultimately
    decide on the ML and natural language models that fit the problem and the underlying
    data best. This process requires careful consideration and analysis of the data,
    as well as a thorough understanding of the business problem at hand.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在方法论环境中工作，数据集通常是众所周知的且已预处理，例如Kaggle数据集。然而，在现实世界的商业环境中，一项重要任务是定义来自所有可能数据来源的数据集，探索收集到的数据以找到最佳预处理方法，并最终决定最适合问题和底层数据的机器学习和自然语言模型。这个过程需要仔细考虑和分析数据，以及对当前业务问题的深入理解。
- en: In NLP, the data can be quite complex, as it often includes text and speech
    data that can be unstructured and difficult to analyze. This complexity makes
    preprocessing an essential step in preparing the data for ML models. The first
    step of any NLP or ML solution starts with exploring the data to learn more about
    it, which helps us decide on our path to tackle the problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，数据可能相当复杂，因为它通常包括文本和语音数据，这些数据可能是非结构化的且难以分析。这种复杂性使得预处理成为准备数据供机器学习模型使用的一个基本步骤。任何自然语言处理（NLP）或机器学习（ML）解决方案的第一步都是探索数据，以了解更多关于它的信息，这有助于我们决定解决问题的路径。
- en: Once the data has been preprocessed, the next step is to explore it to gain
    a better understanding of its characteristics and structure. Data exploration
    is an iterative process that involves visualizing and analyzing the data, looking
    for patterns and relationships, and identifying potential issues or outliers.
    This process can help us to determine which features are most important for our
    ML models and identify any potential biases or data quality issues. To streamline
    data and enhance analysis through ML models, preprocessing methods such as tokenization,
    stemming, and lemmatization can be employed. In this chapter, we will provide
    an overview of general preprocessing techniques for ML problems. In the following
    chapter, we will delve into preprocessing techniques specific to text processing.
    It is important to note that employing effective preprocessing techniques can
    significantly enhance the performance and accuracy of ML models, making them more
    robust and reliable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据预处理完成，下一步就是探索它，以便更好地理解其特性和结构。数据探索是一个迭代过程，涉及可视化和分析数据，寻找模式和关系，以及识别潜在的问题或异常值。这个过程可以帮助我们确定哪些特征对我们的人工智能模型最重要，并识别任何潜在的数据质量或偏差问题。为了通过人工智能模型简化数据并增强分析，可以采用诸如分词、词干提取和词形还原等预处理方法。在本章中，我们将概述人工智能问题的一般预处理技术。在下一章中，我们将深入探讨特定于文本处理的预处理技术。值得注意的是，采用有效的预处理技术可以显著提高人工智能模型的性能和准确性，使它们更加稳健和可靠。
- en: Finally, once the data has been preprocessed and explored, we can start building
    our ML models. There is no single magical solution that works for all ML problems,
    so it’s important to carefully consider which models are best suited for the data
    and the problem at hand. Different types of NLP models exist, encompassing rule-based,
    statistical, and deep learning models. Each model type possesses unique strengths
    and weaknesses, underscoring the importance of selecting the most fitting one
    for the specific problem and dataset at hand.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦数据预处理和探索完成，我们就可以开始构建我们的人工智能模型。没有一种神奇的解决方案适用于所有人工智能问题，因此仔细考虑哪种模型最适合当前的数据和问题是至关重要的。存在多种类型的自然语言处理模型，包括基于规则的、统计的和深度学习模型。每种模型类型都有其独特的优势和劣势，这强调了选择最适合特定问题和数据集的重要性。
- en: Data exploration is an important and initial step in the ML workflow that involves
    analyzing and understanding the data before building a ML model. The goal of data
    exploration is to gain insights about the data, identify patterns, detect anomalies,
    and prepare the data for modeling. Data exploration helps in choosing the right
    ML algorithm and determining the best set of features to use.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索是人工智能工作流程中的重要且初始步骤，涉及在构建人工智能模型之前分析和理解数据。数据探索的目标是深入了解数据，识别模式，检测异常，并为建模准备数据。数据探索有助于选择正确的人工智能算法，并确定最佳的特征集。
- en: 'Here are some common techniques that are used in data exploration:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些在数据探索中常用的技术：
- en: '**Data visualization**: Data visualization involves depicting data through
    graphical or pictorial formats. It enables visual exploration of data, providing
    insights into its distribution, patterns, and relationships. Widely employed techniques
    in data visualization encompass scatter plots, bar charts, heatmaps, box plots,
    and correlation matrices.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化**：数据可视化涉及通过图形或图像格式来表示数据。它使数据可视化探索成为可能，提供了关于其分布、模式和关系的见解。在数据可视化中广泛使用的技术包括散点图、条形图、热图、箱线图和相关性矩阵。'
- en: '**Data cleaning**: Data cleaning is a step of preprocessing where we identify
    the errors, inconsistencies, and missing values and correct them. It affects the
    final results of the model since ML models are sensitive to errors in the data.
    Removing duplicates and filling in missing values are some of the common data
    cleaning techniques.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：数据清洗是预处理的一个步骤，其中我们识别错误、不一致性和缺失值并纠正它们。由于人工智能模型对数据中的错误敏感，这会影响模型的最终结果。删除重复项和填充缺失值是一些常见的数据清洗技术。'
- en: '**Feature engineering**: Feature engineering plays a crucial role in optimizing
    the effectiveness of machine learning models by crafting new features from existing
    data. This process involves not only identifying pertinent features but also transforming
    the existing ones and introducing novel features. Various feature engineering
    techniques, including scaling, normalization, dimensionality reduction, and feature
    selection, contribute to refining the overall performance of the models.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：特征工程在通过从现有数据中构建新特征来优化机器学习模型的有效性方面发挥着至关重要的作用。这个过程不仅涉及识别相关特征，还包括转换现有特征和引入新特征。包括缩放、归一化、降维和特征选择在内的各种特征工程技术有助于提高模型的总体性能。'
- en: '**Statistical analysis**: Statistical analysis utilizes a range of statistical
    techniques to scrutinize data, revealing valuable insights into its inherent properties.
    Essential statistical methods include hypothesis testing, regression analysis,
    and time series analysis, all of which contribute to a comprehensive understanding
    of the data’s characteristics.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计分析**：统计分析运用一系列统计技术来审查数据，揭示其内在属性的有价值见解。基本的统计方法包括假设检验、回归分析和时间序列分析，所有这些都有助于全面理解数据的特征。'
- en: '**Domain knowledge**: Leveraging domain knowledge entails applying a pre-existing
    understanding of the data domain to extract insights and make informed decisions.
    This knowledge proves valuable in recognizing pertinent features, interpreting
    results, and choosing the most suitable ML algorithm for the task at hand.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域知识**：利用领域知识意味着应用对数据领域已有的理解来提取洞察并做出明智的决策。这种知识在识别相关特征、解释结果以及选择最适合当前任务的机器学习算法方面非常有价值。'
- en: We will explore each of these techniques in the following subsections.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的小节中探讨这些技术的每一个。
- en: Data visualization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Data visualization is a crucial component of machine learning as it allows us
    to understand and explore complex datasets more easily. It involves creating visual
    representations of data using charts, graphs, and other types of visual aids.
    By visually presenting data, we can discern patterns, trends, and relationships
    that might not be readily evident when examining the raw data alone.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化是机器学习的一个关键组成部分，因为它使我们能够更轻松地理解和探索复杂的数据集。这涉及到使用图表、图形和其他类型的视觉辅助工具来创建数据的视觉表示。通过视觉呈现数据，我们可以识别出在仅检查原始数据时可能不明显存在的模式、趋势和关系。
- en: For NLP tasks, data visualization can help us gain insights into the linguistic
    patterns and structures in text data. For example, we can create word clouds to
    visualize the frequency of words in a corpus or use heatmaps to display the co-occurrence
    of words or phrases. We can also use scatter plots and line graphs to visualize
    changes in sentiment or topic over time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言处理任务，数据可视化可以帮助我们深入了解文本数据中的语言模式和结构。例如，我们可以创建词云来可视化语料库中单词的频率，或使用热图来显示单词或短语的共现。我们还可以使用散点图和折线图来可视化随时间变化的情感或主题。
- en: One common type of visualization for ML is the scatter plot, which is used to
    display the relationship between two variables. By plotting the values of two
    variables on the X and Y axes, we can identify any patterns or trends that exist
    between them. Scatter plots are particularly useful for identifying clusters or
    groups of data points that share similar characteristics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习来说，散点图是一种常见的可视化类型，用于显示两个变量之间的关系。通过在X轴和Y轴上绘制两个变量的值，我们可以识别它们之间存在的任何模式或趋势。散点图在识别具有相似特征的数据点的簇或组方面特别有用。
- en: Another type of visualization that’s frequently employed in ML is the histogram,
    a tool that illustrates the distribution of a single variable. By grouping data
    into bins and portraying the frequency of data points in each bin, we can pinpoint
    the range of values that predominate in the dataset. Histograms prove useful for
    detecting outliers or anomalies, and they aid in recognizing areas where the data
    may exhibit skewness or bias.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中经常使用的一种可视化类型是直方图，这是一种展示单个变量分布的工具。通过将数据分组到不同的区间并描绘每个区间中数据点的频率，我们可以确定在数据集中占主导地位的价值范围。直方图在检测异常值或异常情况以及识别数据可能表现出偏斜或偏差的区域方面非常有用。
- en: In addition to these basic visualizations, ML practitioners often use more advanced
    techniques, such as dimensionality reduction and network visualizations. Dimensionality
    reduction techniques, such as **principal component analysis** (**PCA**) and **t-distributed
    stochastic neighbor embedding** (**t-SNE**), are commonly used for dimensional
    reduction and to visualize or analyze the data more easily. Network visualizations,
    on the other hand, are used to display complex relationships between entities,
    such as the co-occurrence of words or the connections between social media users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本的可视化之外，机器学习从业者经常使用更高级的技术，例如降维和网络可视化。降维技术，如**主成分分析**（**PCA**）和**t-分布随机邻域嵌入**（**t-SNE**），常用于降维以及更轻松地可视化或分析数据。另一方面，网络可视化用于显示实体之间的复杂关系，例如词语的共现或社交媒体用户之间的联系。
- en: Data cleaning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Data cleaning, alternatively termed data cleansing or data scrubbing, involves
    recognizing and rectifying or eliminating errors, inconsistencies, and inaccuracies
    within a dataset. This crucial phase in data preparation for ML significantly
    influences the accuracy and performance of a model, relying on the quality of
    the data used for training. Numerous prevalent techniques are employed in data
    cleaning. Let’s take a closer look.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗，也称为数据净化或数据清理，涉及识别和纠正或消除数据集中的错误、不一致性和不准确性。在机器学习数据准备的这个关键阶段，它对模型的准确性和性能有重大影响，依赖于用于训练的数据质量。在数据清洗中采用了许多常见的技术。让我们更详细地了解一下。
- en: Handling missing values
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Missing data is a common problem that occurs in many machine learning projects.
    Dealing with missing data is important because ML models cannot handle missing
    data and will either produce errors or provide inaccurate results.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据是许多机器学习项目中常见的问题。处理缺失数据很重要，因为机器学习模型无法处理缺失数据，可能会产生错误或提供不准确的结果。
- en: 'There are several methods for dealing with missing data in ML projects:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中处理缺失数据有几种方法：
- en: '**Dropping rows**: Addressing missing data can involve a straightforward approach
    of discarding rows that contain such values. Nevertheless, exercising caution
    is paramount when employing this method as excessive row removal may result in
    the loss of valuable data, impacting the overall accuracy of the model. We usually
    use this method when we have a few rows in our dataset, and we have a few rows
    with missing values. In this case, removing a few rows can be a good and easy
    approach to training our model while the final performance will not be affected
    significantly.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除行**：处理缺失数据可能涉及一种简单的方法，即丢弃包含这些值的行。然而，在采用这种方法时，谨慎行事至关重要，因为过度删除行可能会导致宝贵数据的丢失，从而影响模型的总体准确性。我们通常在数据集中只有几行，并且有几行缺失值时使用这种方法。在这种情况下，删除几行可能是训练我们的模型的一个好方法，而且最终性能不会受到显著影响。'
- en: '**Dropping columns**: Another approach is to drop the columns that contain
    missing values. This method can be effective if the missing values are concentrated
    in a few columns and if those columns are not important for the analysis. However,
    dropping important columns can lead to a loss of valuable information. It is better
    to perform some sort of correlation analysis to see the correlation of the values
    in these columns with the target class or value before dropping these columns.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除列**：另一种方法是删除包含缺失值的列。如果缺失值集中在少数几列，并且这些列对于分析不是很重要，那么这种方法可能是有效的。然而，删除重要的列可能会导致宝贵信息的丢失。在删除这些列之前，最好进行某种相关性分析，以查看这些列中的值与目标类别或值的关联性。'
- en: '**Mean/median/mode imputation**: Mean, median, and mode imputation entail substituting
    missing values with the mean, median, or mode derived from the non-missing values
    within the corresponding column. This method is easy to implement and can be effective
    when the missing values are few and randomly distributed. However, it can also
    introduce bias and affect the variability of the data.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值/中位数/众数插补**：均值、中位数和众数插补涉及用从相应列中非缺失值得到的均值、中位数或众数来替换缺失值。这种方法易于实现，当缺失值很少且随机分布时可能有效。然而，它也可能引入偏差并影响数据的变异性。'
- en: '**Regression imputation**: Regression imputation involves predicting the missing
    values based on the values of other variables in the dataset. This method can
    be effective when the missing values are related to other variables in the dataset,
    but it requires a regression model to be built for each column with missing values.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归插补**：回归插补涉及根据数据集中其他变量的值预测缺失值。当缺失值与数据集中的其他变量相关时，此方法可能有效，但需要为每个具有缺失值的列构建回归模型。'
- en: '**Multiple imputation**: Multiple imputation encompasses generating multiple
    imputed datasets through statistical models, followed by amalgamating the outcomes
    to produce a conclusive dataset. This approach proves efficacious, particularly
    when dealing with non-randomly distributed missing values and a substantial number
    of gaps in the dataset.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多重插补**：多重插补包括通过统计模型生成多个插补数据集，然后合并结果以生成最终数据集。这种方法在处理非随机分布的缺失值和数据集中大量空缺时特别有效。'
- en: '**K-nearest neighbor imputation**: K-nearest neighbor imputation entails identifying
    the k-nearest data points to the missing value and utilizing their values to impute
    the absent value. This method can be effective when the missing values are clustered
    together in the dataset. In this approach, we can find the most similar records
    to the dataset to the record that has the missing value, and then use the mean
    of the values of those records for that specific record as the missed value.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K最近邻插补**：K最近邻插补包括识别缺失值附近的k个最近数据点，并使用它们的值来插补缺失值。当缺失值在数据集中聚集在一起时，此方法可能有效。在这种情况下，我们可以找到与具有缺失值的数据集记录最相似的记录，然后使用这些记录的值的平均值作为该特定记录的缺失值。'
- en: In essence, selecting a method to handle missing data hinges on factors such
    as the nature and extent of the missing data, analysis objectives, and resource
    availability. It is crucial to thoughtfully assess the pros and cons of each method
    and opt for the most suitable approach tailored to the specific project.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，选择处理缺失数据的方法取决于缺失数据的性质和程度、分析目标和资源可用性。仔细评估每种方法的优缺点，并选择最适合特定项目的最合适方法至关重要。
- en: Removing duplicates
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除重复项
- en: Eliminating duplicates is a prevalent preprocessing measure that’s employed
    to cleanse datasets by detecting and removing identical records. The occurrence
    of duplicate records may be attributed to factors such as data entry errors, system
    glitches, or data merging processes. The presence of duplicates can skew models
    and yield inaccurate insights. Hence, it is imperative to recognize and eliminate
    duplicate records to uphold the accuracy and dependability of the dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 消除重复项是一种常见的预处理措施，通过检测和删除重复记录来净化数据集。重复记录的出现可能归因于数据输入错误、系统故障或数据合并过程。重复项的存在可能会扭曲模型并导致不准确的分析。因此，识别和消除重复记录对于维护数据集的准确性和可靠性至关重要。
- en: There are different methods for removing duplicates in a dataset. The most common
    method is to compare all the rows of the dataset to identify duplicate records.
    If two or more rows have the same values in all the columns, they are considered
    duplicates. In some cases, it may be necessary to compare only a subset of columns
    if certain columns are more prone to duplicates.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中移除重复项有不同的方法。最常见的方法是比较数据集的所有行以识别重复记录。如果有两个或更多行在所有列中都具有相同的值，则它们被认为是重复的。在某些情况下，如果某些列更容易出现重复，可能只需要比较列的子集。
- en: Another method is to use a unique identifier column to identify duplicates.
    A unique identifier column is a column that contains unique values for each record,
    such as an ID number or a combination of unique columns. By comparing the unique
    identifier column, it is possible to identify and remove duplicate records from
    the dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用唯一标识符列来识别重复项。唯一标识符列是包含每个记录唯一值的列，例如ID号码或唯一列的组合。通过比较唯一标识符列，可以识别并从数据集中删除重复记录。
- en: After identifying the duplicate records, the next step is to decide which records
    to keep and which ones to remove. One approach is to keep the first occurrence
    of a duplicate record and remove all subsequent occurrences. Another approach
    is to keep the record with the most complete information, or the record with the
    most recent timestamp.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别出重复记录之后，下一步是决定保留哪些记录以及删除哪些记录。一种方法是保留重复记录的第一个出现，并删除所有后续出现。另一种方法是保留信息最完整的记录，或者是最新的时间戳的记录。
- en: It’s crucial to recognize that the removal of duplicates might lead to a reduction
    in dataset size, potentially affecting the performance of ML models. Consequently,
    assessing the impact of duplicate removal on both the dataset and the ML model
    is essential. In some cases, it may be necessary to keep duplicate records if
    they contain important information that cannot be obtained from other records.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到删除重复记录可能会导致数据集大小的减少，这可能会影响机器学习模型的表现至关重要。因此，评估删除重复记录对数据集和机器学习模型的影响是必不可少的。在某些情况下，如果重复记录包含其他记录无法获得的重要信息，可能需要保留重复记录。
- en: Standardizing and transforming data
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化和转换数据
- en: 'Standardizing and transforming data is a critical step in preparing data for
    ML tasks. This process involves scaling and normalizing the numerical features
    of the dataset to make them easier to interpret and compare. The main objective
    of standardizing and transforming data is to enhance the accuracy and performance
    of a ML model by mitigating the influence of features with diverse scales and
    ranges. A widely used method for standardizing data is referred to as “standardization”
    or “Z-score normalization.” This technique involves transforming each feature
    such that it has a mean of zero and a standard deviation of one. The formula for
    standardization is shown in the following equation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化和转换数据是准备数据用于机器学习任务的关键步骤。这个过程涉及对数据集的数值特征进行缩放和归一化，以便更容易解释和比较。标准化和转换数据的主要目标是通过对不同尺度和范围的特性影响进行缓解，从而提高机器学习模型的准确性和性能。用于标准化数据的一种广泛使用的方法被称为“标准化”或“Z分数归一化”。这种技术涉及将每个特征转换，使其具有零均值和标准差为1。标准化的公式如下所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/128.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/128.png)'
- en: Here, *x* represents the feature, *mean(x)* denotes the mean of the feature,
    *std(x)* indicates the standard deviation of the feature, and *x’* represents
    the new value assigned to the feature. By standardizing the data in this way,
    the range of each feature is adjusted to be centered around zero, which makes
    it easier to compare features and prevents features with large values from dominating
    the analysis.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 代表特征，*mean(x)* 表示特征的均值，*std(x)* 表示特征的标准差，而 *x’* 表示分配给特征的新值。通过这种方式标准化数据，每个特征的取值范围被调整为以零为中心，这使得比较特征更容易，并防止具有大值的特征主导分析。
- en: 'Another technique for transforming data is “min-max scaling.” This method rescales
    the data to a consistent range of values, commonly ranging between 0 and 1\. The
    formula for min-max scaling is shown here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种转换数据的技术是“最小-最大缩放”。这种方法将数据重新缩放到一个一致的范围，通常在0到1之间。最小-最大缩放的公式如下所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/129.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>m</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/129.png)'
- en: In this equation, *x* represents the feature, *min(x)* signifies the minimum
    value of the feature, and *max(x)* denotes the maximum value of the feature. Min-max
    scaling proves beneficial when the precise distribution of the data is not crucial,
    but there is a need to standardize the data for meaningful comparisons across
    different features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*x* 代表特征，*min(x)* 表示特征的最低值，而 *max(x)* 表示特征的最高值。Min-max缩放在数据的精确分布不是关键，但需要标准化数据以进行不同特征之间的有意义的比较时是有益的。
- en: Transforming data can also involve changing the distribution of the data. A
    frequently applied transformation is the log transformation, which is employed
    to alleviate the influence of outliers and skewness within the data. This transformation
    involves taking the logarithm of the feature values, which can help to normalize
    the distribution and reduce the influence of extreme values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换也可能涉及改变数据的分布。常用的转换是对数转换，它用于减轻数据中的异常值和偏斜的影响。这种转换涉及对特征值取对数，这有助于使分布正常化并减少极端值的影响。
- en: Overall, standardizing and transforming data constitute a pivotal stage in the
    data preprocessing workflow for ML endeavors. Through scaling and normalizing
    features, we can enhance the accuracy and performance of the ML model, rendering
    the data more interpretable and conducive to meaningful comparisons.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，标准化和转换数据是机器学习数据预处理工作流程中的关键阶段。通过缩放和归一化特征，我们可以提高机器学习模型的准确性和性能，使数据更具可解释性，并有利于进行有意义的比较。
- en: Handling outliers
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Outliers are data points that markedly deviate from the rest of the observations
    in a dataset. Their occurrence may stem from factors such as measurement errors,
    data corruption, or authentic extreme values. The presence of outliers can wield
    a substantial influence on the outcomes of ML models, introducing distortion to
    the data and disrupting the relationships between variables. Therefore, handling
    outliers is an important step in preprocessing data for ML.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是明显偏离数据集中其他观测值的数据点。它们的产生可能源于测量错误、数据损坏或真实的极端值。异常值的存在可能会对机器学习模型的输出产生重大影响，扭曲数据并破坏变量之间的关系。因此，处理异常值是机器学习数据预处理的重要步骤。
- en: 'There are several methods for handling outliers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值有几种方法：
- en: '**Removing outliers**: One straightforward approach involves eliminating observations
    identified as outliers from the dataset. Nevertheless, exercising caution is paramount
    when adopting this method as excessive removal of observations may result in the
    loss of valuable information and potentially introduce bias to the analysis results.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除异常值**：一种直接的方法是消除数据集中被识别为异常值的观测。然而，采用这种方法时必须谨慎，因为过度移除观测值可能会导致丢失有价值的信息，并可能引入分析结果的偏差。'
- en: '**Transforming data**: Applying mathematical functions such as logarithms or
    square roots to transform the data can mitigate the influence of outliers. For
    instance, taking the logarithm of a variable can alleviate the impact of extreme
    values, given the slower rate of increase in the logarithmic scale compared to
    the original values.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换数据**：应用对数或平方根等数学函数来转换数据可以减轻异常值的影响。例如，对一个变量取对数可以减轻极端值的影响，因为对数尺度上的增长速度比原始值慢。'
- en: '**Winsorizing**: Winsorizing is a technique that entails substituting extreme
    values with the nearest highest or lowest value in the dataset. Employing this
    method aids in maintaining the sample size and overall distribution of the data.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Winsorizing**：Winsorizing是一种技术，涉及用数据集中最近的最高或最低值替换极端值。采用这种方法有助于保持样本大小和数据的整体分布。'
- en: '**Imputing values**: Imputation involves replacing missing or extreme values
    with estimated values derived from the remaining observations in the dataset.
    For instance, substituting extreme values with the median or mean of the remaining
    observations is a common imputation technique.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值填充**：值填充涉及用从数据集中剩余观测值中估计的值替换缺失或极端值。例如，用剩余观测值的均值或中位数替换极端值是一种常见的填充技术。'
- en: '**Using robust statistical methods**: Robust statistical methods exhibit lower
    sensitivity to outliers, leading to more accurate results even in the presence
    of such extreme values. For instance, opting for the median instead of the mean
    can effectively diminish the influence of outliers on the final results.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用稳健的统计方法**：稳健的统计方法对异常值的敏感性较低，即使在存在极端值的情况下也能得出更准确的结果。例如，选择中位数而不是均值可以有效地减少异常值对最终结果的影响。'
- en: It’s crucial to emphasize that selecting an outlier-handling method should be
    tailored to the unique characteristics of the data and the specific problem at
    hand. Generally, employing a combination of methods is advisable to address outliers
    comprehensively, and assessing the impact of each method on the results is essential.
    Moreover, documenting the steps taken to manage outliers is important for reproducibility
    and to provide clarity on the decision-making process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 强调选择异常值处理方法应针对数据的独特特性和具体问题量身定制至关重要。通常，建议采用多种方法的组合来全面处理异常值，并评估每种方法对结果的影响是必要的。此外，记录管理异常值的步骤对于可重复性和阐明决策过程非常重要。
- en: Correcting errors
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纠正错误
- en: Rectifying errors during preprocessing stands as a vital stage in readying data
    for ML. Errors may manifest due to diverse reasons such as data entry blunders,
    measurement discrepancies, sensor inaccuracies, or transmission glitches. Correcting
    errors in data holds paramount significance in guaranteeing that ML models are
    trained on dependable and precise data, consequently enhancing the accuracy and
    reliability of predictions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理阶段纠正错误是准备数据供机器学习使用的关键阶段。错误可能由于数据输入错误、测量差异、传感器不准确或传输故障等多种原因产生。在数据中纠正错误对于确保机器学习模型在可靠和精确的数据上训练至关重要，从而提高预测的准确性和可靠性。
- en: 'Several techniques exist to rectify errors in data. Here are some widely utilized
    methods:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种纠正数据错误的技术。以下是一些广泛使用的方法：
- en: '**Manual inspection**: An approach to rectify errors in data involves a manual
    inspection of the dataset, wherein errors are corrected by hand. This method is
    frequently employed, particularly when dealing with relatively small and manageable
    datasets.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工检查**：纠正数据错误的一种方法是对数据集进行人工检查，其中错误通过手工进行更正。这种方法经常被采用，尤其是在处理相对较小且可管理的数据集时。'
- en: '**Statistical methods**: Statistical methods prove effective in identifying
    and rectifying errors in data. For instance, when the data adheres to a recognized
    distribution, statistical techniques such as the Z-score can be employed to detect
    outliers, which can then be either removed or replaced.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计方法**：统计方法在识别和纠正数据错误方面非常有效。例如，当数据遵循已知的分布时，可以使用如Z分数这样的统计技术来检测异常值，然后可以选择删除或替换这些异常值。'
- en: '**ML methods**: Utilizing ML algorithms facilitates the detection and correction
    of errors in data. For instance, clustering algorithms prove valuable in pinpointing
    data points that markedly deviate from the broader dataset. Subsequently, these
    identified data points can undergo further examination and correction.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习方法**：利用机器学习算法可以促进对数据错误的检测和纠正。例如，聚类算法在确定明显偏离更大数据集的数据点方面非常有价值。随后，这些确定的数据点可以进行进一步的检查和纠正。'
- en: '**Domain knowledge**: Leveraging domain knowledge is instrumental in pinpointing
    errors within data. For instance, when collecting data from sensors, it becomes
    feasible to identify and rectify errors by considering the anticipated range of
    values that the sensor is capable of producing.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域知识**：利用领域知识对于在数据中定位错误至关重要。例如，当从传感器收集数据时，考虑到传感器能够产生的预期值范围，可以识别和纠正错误。'
- en: '**Imputation**: Imputation serves as a method to populate missing values in
    the data. This can be accomplished through various means, including statistical
    methods such as mean or median imputation, as well as ML algorithms such as k-nearest
    neighbor imputation.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插补**：插补是一种在数据中填充缺失值的方法。这可以通过各种方式完成，包括使用均值或中位数插补等统计方法，以及k最近邻插补等机器学习算法。'
- en: Choosing a technique hinges on factors such as the nature of the data, the dataset’s
    size, and the resources at your disposal.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 选择技术取决于数据性质、数据集的大小以及您可用的资源等因素。
- en: Feature selection
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择
- en: Feature selection involves choosing the most pertinent features from a dataset
    for constructing a ML model. The objective is to decrease the number of features
    without substantially compromising the model’s accuracy, resulting in enhanced
    performance, quicker training, and a more straightforward interpretation of the
    model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择涉及从数据集中选择最相关的特征来构建机器学习模型。目标是减少特征数量，同时不显著降低模型的准确性，从而提高性能、加快训练速度，并使模型解释更加直接。
- en: Several approaches to feature selection exist. Let’s take a look.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种特征选择方法。让我们看一下。
- en: Filter methods
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤方法
- en: These techniques employ statistical methods to rank features according to their
    correlation with the target variable. Common methods encompass chi-squared, mutual
    information, and correlation coefficients. Features are subsequently chosen based
    on a predefined threshold.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术使用统计方法根据特征与目标变量的相关性对特征进行排序。常见的方法包括卡方、互信息和相关系数。然后根据预定义的阈值选择特征。
- en: Chi-squared
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卡方
- en: The chi-squared test is a widely employed statistical method in ML for feature
    selection that’s particularly effective for categorical variables. This test gauges
    the dependence between two random variables, providing a P-value that signifies
    the likelihood of obtaining a result as extreme as or more extreme than the actual
    observations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方检验是机器学习中广泛使用的统计方法，特别适用于分类变量。此测试衡量两个随机变量之间的依赖性，提供一个P值，表示获得与实际观察结果一样或更极端结果的可能性。
- en: In hypothesis testing, the chi-squared test assesses whether the collected data
    aligns with the expected data. A small chi-squared test statistic indicates a
    robust match, while a large statistic implies a weak match. A P-value less than
    or equal to 0.05 leads to the rejection of the null hypothesis, considering it
    highly improbable. Conversely, a P-value greater than 0.05 results in accepting
    or “failing to reject” the null hypothesis. When the P-value hovers around 0.05,
    further scrutiny of the hypothesis is warranted.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设检验中，卡方检验用于评估收集到的数据是否与预期数据一致。卡方检验统计量较小表示匹配良好，而较大的统计量则表示匹配较弱。当P值小于或等于0.05时，会导致拒绝零假设，认为其极不可能。相反，当P值大于0.05时，结果为接受或“未能拒绝”零假设。当P值在0.05附近波动时，需要对假设进行进一步审查。
- en: 'In feature selection, the chi-squared test evaluates the relationship between
    each feature and the target variable in the dataset. It determines significance
    based on whether a statistically significant difference exists between the observed
    and expected frequencies of the feature, assuming independence between the feature
    and target. Features with a high chi-squared score exhibit a stronger dependence
    on the target variable, making them more informative for classification or regression
    tasks. The formula for calculating the chi-squared is presented in the following
    equation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择中，卡方检验评估数据集中每个特征与目标变量之间的关系。它根据特征观察频率与预期频率之间是否存在统计上显著差异来确定显著性，假设特征与目标变量之间相互独立。具有高卡方分数的特征表现出对目标变量的更强依赖性，这使得它们在分类或回归任务中更具信息量。计算卡方的公式在以下方程中给出：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>X</mi><mn>2</mn></msup><mo>=</mo><mo>∑</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi><mi>i</mi></msub><mo>−</mo><msub><mi>E</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mi>i</mi></msub></mfrac></mrow></mrow></math>](img/130.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>X</mi><mn>2</mn></msup><mo>=</mo><mo>∑</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi><mi>i</mi></msub><mo>−</mo><msub><mi>E</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mi>i</mi></msub></mfrac></mrow></mrow></math>](img/130.png)'
- en: In this equation, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/131.png)
    represents the observed value and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/132.png)
    represents the expected value. The computation involves finding the difference
    between the observed frequency and the expected frequency, squaring the result,
    and then dividing by the expected frequency. The summation of these values across
    all categories of the feature yields the overall chi-squared statistic for that
    feature.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/131.png)
    表示观察值，而![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/132.png)
    表示期望值。计算涉及找到观察频率与期望频率之间的差异，将结果平方，然后除以期望频率。这些值在所有特征类别上的总和给出了该特征的总体卡方统计量。
- en: The degrees of freedom for the test relies on the number of categories in the
    feature and the number of categories in the target variable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 测试的自由度取决于特征中的类别数和目标变量中的类别数。
- en: An exemplary application of chi-squared feature selection lies in text classification,
    particularly in scenarios where the presence or absence of specific words in a
    document serves as features. The chi-squared test helps identify words strongly
    associated with a particular class or category of documents, subsequently enabling
    their use as features in a ML model. In categorical data, especially where the
    relationship between features and the target variable is non-linear, chi-squared
    proves to be a valuable method for feature selection. However, its suitability
    diminishes for continuous or highly correlated features, where alternative feature
    selection methods may be more fitting.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方特征选择的典型应用在于文本分类，特别是在文档中特定单词的存在或缺失作为特征的场景中。卡方检验有助于识别与特定类别或文档类别强烈相关的单词，从而使其在机器学习模型中作为特征使用。在分类数据中，尤其是在特征与目标变量之间关系非线性的情况下，卡方检验证明是特征选择的有价值方法。然而，对于连续或高度相关的特征，其适用性会降低，此时可能需要其他特征选择方法。
- en: Mutual information
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 互信息
- en: Mutual information acts as a metric to gauge the interdependence of two random
    variables. In the context of feature selection, it quantifies the information
    a feature provides about the target variable. The core methodology entails calculating
    the mutual information between each feature and the target variable, ultimately
    selecting features with the highest mutual information scores.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息作为一种度量，用于衡量两个随机变量之间的相互依赖性。在特征选择的背景下，它量化了特征对目标变量提供的信息。核心方法包括计算每个特征与目标变量之间的互信息，最终选择互信息得分最高的特征。
- en: 'Mathematically, the mutual information between two discrete random variables,
    *X* and *Y*, can be defined as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，两个离散随机变量 *X* 和 *Y* 之间的互信息可以定义为以下：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/133.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/133.png)'
- en: In the given equation, *p(x, y)* represents the joint probability mass function
    of *X* and *Y*, while *p(x)* and *p(y)* denote the marginal probability mass functions
    of *X* and *Y*, respectively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中，*p(x, y)* 代表 *X* 和 *Y* 的联合概率质量函数，而 *p(x)* 和 *p(y)* 分别代表 *X* 和 *Y* 的边缘概率质量函数。
- en: In the context of feature selection, mutual information calculation involves
    treating the feature as *X* and the target variable as *Y*. By computing the mutual
    information score for each feature, we can then select features with the highest
    scores.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择的情况下，互信息计算涉及将特征视为 *X*，将目标变量视为 *Y*。通过计算每个特征的互信息得分，然后我们可以选择得分最高的特征。
- en: To estimate the probability mass functions needed for calculating mutual information,
    histogram-based methods can be employed. This involves dividing the range of each
    variable into a fixed number of bins and estimating the probability mass functions
    based on the frequencies of observations in each bin. Alternatively, kernel density
    estimation can be utilized to estimate the probability density functions, and
    mutual information can then be computed based on the estimated densities.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算互信息所需的概率质量函数，可以使用基于直方图的方法。这涉及到将每个变量的范围划分为固定数量的桶，并根据每个桶中观察到的频率来估计概率质量函数。或者，可以使用核密度估计来估计概率密度函数，然后基于估计的密度计算互信息。
- en: In practical applications, mutual information is often employed alongside other
    feature selection methods, such as chi-squared or correlation-based methods, to
    enhance the overall performance of the feature selection process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，互信息通常与其他特征选择方法（如卡方检验或基于相关性的方法）一起使用，以增强特征选择过程的整体性能。
- en: Correlation coefficients
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相关系数
- en: Correlation coefficients serve as indicators of the strength and direction of
    the linear relationship between two variables. In the realm of feature selection,
    these coefficients prove useful in identifying features highly correlated with
    the target variable, thus serving as potentially valuable predictors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数是两个变量之间线性关系强度和方向的指标。在特征选择领域，这些系数在识别与目标变量高度相关的特征方面非常有用，因此可以作为潜在的宝贵预测因子。
- en: 'The prevalent correlation coefficient employed for feature selection is the
    Pearson correlation coefficient, also referred to as Pearson’s *r*. Pearson’s
    r measures the linear relationship between two continuous variables, ranging from
    -1 (indicating a perfect negative correlation) to 1 (indicating a perfect positive
    correlation), with 0 denoting no correlation. Its calculation involves dividing
    the covariance between the two variables by the product of their standard deviations,
    as depicted in the following equation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择中普遍使用的相关系数是皮尔逊相关系数，也称为皮尔逊的 *r*。皮尔逊的 *r* 衡量两个连续变量之间的线性关系，范围从 -1（表示完美的负相关）到
    1（表示完美的正相关），0 表示无相关。其计算涉及将两个变量的协方差除以它们标准差的乘积，如下方程所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>∙</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>Y</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/134.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>∙</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>Y</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/134.png)'
- en: In the given equation, *X* and *Y* represent the two variables of interest,
    *cov()* denotes the covariance function, and *std()* represents the standard deviation
    function.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中，*X* 和 *Y* 代表感兴趣的两个变量，*cov()* 表示协方差函数，而 *std()* 表示标准差函数。
- en: 'Utilizing Pearson’s *r* for feature selection involves computing the correlation
    between each feature and the target variable. Features with the highest absolute
    correlation coefficients are then selected. A high absolute correlation coefficient
    signifies a strong correlation with the target variable, whether positive or negative.
    The interpretation of Pearson correlation values and their degree of correlation
    is outlined in *Table 3.1*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用皮尔逊相关系数（Pearson’s *r*）进行特征选择涉及计算每个特征与目标变量之间的相关性。然后选择具有最高绝对相关系数的特征。高绝对相关系数表示与目标变量有强烈的关联，无论是正相关性还是负相关性。皮尔逊相关值及其相关程度的解释见
    *表 3.1*：
- en: '| **Pearson** **Correlation Value** | **Degree** **of Correlation** |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **皮尔逊** **相关系数** | **相关** **程度** |'
- en: '| --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ± 1 | Perfect |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ± 1 | 完美相关 |'
- en: '| ± 0.50 - ± 1 | High degree |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ± 0.50 - ± 1 | 高度相关 |'
- en: '| ± 0.30 - ± 0.49 | Moderate degree |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ± 0.30 - ± 0.49 | 中度相关 |'
- en: '| < +0.29 | Low degree |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| < +0.29 | 低度相关 |'
- en: '| 0 | No correlation |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 无相关 |'
- en: Table 3 .1 – Pearson correlation values and their degree of correlation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3 .1 – 皮尔逊相关值及其相关程度
- en: It’s worth noting that Pearson’s *r* is only appropriate for identifying linear
    relationships between variables. If the relationship is nonlinear, or if one or
    both of the variables are categorical, other correlation coefficients such as
    Spearman’s ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ρ</mml:mi></mml:math>](img/135.png)or
    Kendall’s ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>τ</mml:mi></mml:math>](img/136.png)
    may be more appropriate. Additionally, it is important to be cautious when interpreting
    correlation coefficients as a high correlation does not necessarily imply causation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，皮尔逊相关系数（Pearson’s *r*）仅适用于识别变量之间的线性关系。如果关系是非线性的，或者如果其中一个或两个变量是分类的，那么其他相关系数，如斯皮尔曼的
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ρ</mml:mi></mml:math>](img/135.png)
    或肯德尔的相关系数 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>τ</mml:mi></mml:math>](img/136.png)
    可能更合适。此外，在解释相关系数时需要谨慎，因为高相关性并不一定意味着因果关系。
- en: Wrapper methods
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包装方法
- en: These techniques delve into subsets of features through iterative model training
    and testing. Widely known methods encompass forward selection, backward elimination,
    and recursive feature elimination. While computationally demanding, these methods
    have the potential to significantly enhance model accuracy.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术通过迭代模型训练和测试深入到特征子集。众所周知的方法包括前向选择、后向消除和递归特征消除。虽然计算量较大，但这些方法有可能显著提高模型精度。
- en: A concrete illustration of a wrapper method is **recursive feature elimination**
    (**RFE**). Functioning as a backward elimination approach, RFE systematically
    removes the least important feature until a predetermined number of features remains.
    During each iteration, a machine learning model is trained on the existing features,
    and the least important feature is pruned based on its feature importance score.
    This sequential process persists until the specified number of features is attained.
    The feature importance score can be extracted from diverse methods, including
    coefficient values from linear models or feature importance scores derived from
    decision trees. RFE is a computationally expensive method, but it can be useful
    when the number of features is very large and there is a need to reduce the feature
    space. An alternative approach is to have feature selection during the training
    process, something that’s done via embedding methods.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法的具体示例是**递归特征消除**（**RFE**）。作为一种反向消除方法，RFE系统地移除最不重要的特征，直到剩余预定的特征数量。在每次迭代中，机器学习模型在现有特征上训练，并根据其特征重要性分数修剪最不重要的特征。这个过程持续进行，直到达到指定的特征数量。特征重要性分数可以从多种方法中提取，包括线性模型的系数值或从决策树中导出的特征重要性分数。RFE是一种计算成本较高的方法，但在特征数量非常大且需要减少特征空间时可能很有用。另一种方法是训练过程中的特征选择，这通常通过嵌入方法来完成。
- en: Embedded methods
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入方法
- en: These methods select features during the training process of the model. Popular
    methods include LASSO and ridge regression, decision trees, and random forests.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在模型的训练过程中选择特征。常见的方法包括LASSO和岭回归、决策树和随机森林。
- en: LASSO
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LASSO
- en: '**LASSO**, an acronym for **Least Absolute Shrinkage and Selection Operator**,
    serves as a linear regression technique that’s commonly employed for feature selection
    in machine learning. Its mechanism involves introducing a penalty term to the
    standard regression loss function. This penalty encourages the model to reduce
    the coefficients of less important features to zero, effectively eliminating them
    from the model.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**LASSO**，即**最小绝对收缩和选择算子**的缩写，是一种常用于机器学习特征选择的线性回归技术。其机制涉及向标准回归损失函数中引入惩罚项。这种惩罚鼓励模型将不太重要的特征的系数减少到零，从而有效地从模型中消除它们。'
- en: The LASSO method proves especially valuable when grappling with high-dimensional
    data, where the number of features far exceeds the number of samples. In such
    scenarios, discerning the most crucial features for predicting the target variable
    can be challenging. LASSO comes to the fore by automatically identifying the most
    relevant features while simultaneously shrinking the coefficients of others.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理高维数据时，特征的数量远超过样本数量，LASSO方法特别有价值。在这种情况下，区分预测目标变量的最关键特征可能具有挑战性。LASSO通过自动识别最相关的特征，同时缩小其他特征的系数，脱颖而出。
- en: 'The LASSO method works by finding the solution for the following optimization
    problem, which is a minimization problem:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO方法通过寻找以下优化问题的解来工作，这是一个最小化问题：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>min</mi><mi
    mathvariant="bold">w</mi></munder><msubsup><mfenced open="‖" close="‖"><mrow><mi
    mathvariant="bold">y</mi><mo>−</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">w</mi></mrow></mfenced><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi
    mathvariant="bold-italic">λ</mi><msub><mfenced open="‖" close="‖"><mi mathvariant="bold">w</mi></mfenced><mn>1</mn></msub></mrow></mrow></math>](img/137.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>min</mi><mi
    mathvariant="bold">w</mi></munder><msubsup><mfenced open="‖" close="‖"><mrow><mi
    mathvariant="bold">y</mi><mo>−</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">w</mi></mrow></mfenced><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi
    mathvariant="bold-italic">λ</mi><msub><mfenced open="‖" close="‖"><mi mathvariant="bold">w</mi></mfenced><mn>1</mn></msub></mrow></mrow></math>](img/137.png)'
- en: In the given equation, vector *y* represents the target variable, *X* denotes
    the feature matrix, *w* signifies the vector of regression coefficients, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/138.png)
    is a hyperparameter dictating the intensity of the penalty term, and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/139.png)
    stands for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/140.png)norm
    of the coefficients (that is, the sum of their absolute values).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中，向量 *y* 代表目标变量，*X* 表示特征矩阵，*w* 表示回归系数向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/138.png)
    是一个超参数，用于决定惩罚项的强度，而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/139.png)
    表示系数的 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/140.png)
    范数（即它们的绝对值之和）。
- en: The inclusion of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/141.png)
    penalty term in the objective function prompts the model to precisely zero out
    certain coefficients, essentially eliminating the associated features from the
    model. The degree of penalty strength is governed by the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/142.png)
    hyperparameter, which can be fine-tuned through the use of cross-validation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数中包含 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/141.png)
    惩罚项促使模型精确地将某些系数置零，从而从模型中消除相关的特征。惩罚强度的程度由 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/142.png)
    超参数控制，这可以通过交叉验证进行微调。
- en: LASSO has several advantages over other feature selection methods, such as its
    ability to handle correlated features and its ability to perform feature selection
    and regression simultaneously. However, LASSO has some limitations, such as its
    tendency to select only one feature from a group of correlated features, and its
    performance may deteriorate if the number of features is much larger than the
    number of samples.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他特征选择方法相比，LASSO具有多个优势，例如其处理相关特征的能力以及同时执行特征选择和回归的能力。然而，LASSO也有一些局限性，例如它倾向于从一组相关特征中仅选择一个特征，并且如果特征数量远大于样本数量，其性能可能会下降。
- en: Consider the application of LASSO for feature selection in predicting house
    prices. Imagine a dataset encompassing details about houses – such as the number
    of bedrooms, lot size, construction year, and so on – alongside their respective
    sale prices. Employing LASSO, we can pinpoint the most crucial features to predict
    the sale price while concurrently fitting a linear regression model to the dataset.
    The outcome is a model that’s ready to forecast the sale price of a new house
    based on its features.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑LASSO在预测房价中的特征选择应用。想象一个包含关于房屋的详细信息的数据集，例如卧室数量、地块大小、建筑年份等，以及它们各自的售价。使用LASSO，我们可以确定预测售价的最关键特征，同时将线性回归模型拟合到数据集上。结果是这样一个模型，可以根据房屋的特征预测新房屋的售价。
- en: Ridge regression
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: Ridge regression, a linear regression method applicable to feature selection,
    closely resembles ordinary least squares regression but introduces a penalty term
    to the cost function to counter overfitting.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归是一种适用于特征选择的线性回归方法，它与普通最小二乘回归非常相似，但引入了惩罚项以防止过拟合。
- en: In ridge regression, the cost function undergoes modification with the inclusion
    of a penalty term directly proportional to the square of the coefficients’ magnitude.
    This penalty term is regulated by a hyperparameter, often denoted as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/138.png)
    or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/144.png)
    dictating the regularization strength. When ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/144.png)
    is set to zero, ridge regression reverts to ordinary least squares regression.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在岭回归中，成本函数通过包含一个与系数幅度的平方成正比的惩罚项而进行修改。这个惩罚项由一个超参数控制，通常表示为 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/138.png)
    或 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/144.png)，它决定了正则化的强度。当
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/144.png)
    设置为零时，岭回归就退化为普通最小二乘回归。
- en: The penalty term’s impact manifests in shrinking the coefficients’ magnitude
    toward zero. This proves beneficial in mitigating overfitting, discouraging the
    model from excessively relying on any single feature. In effect, the penalty term
    acts as a form of feature selection by reducing the importance of less relevant
    features.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚项的影响体现在将系数的幅度缩小到零。这有助于减轻过拟合，阻止模型过度依赖任何单个特征。实际上，惩罚项通过减少不相关特征的重要性，充当一种特征选择的形式。
- en: 'The equation for the ridge regression loss function is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归损失函数的方程如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>min</mi><mi
    mathvariant="bold">w</mi></munder><msubsup><mfenced open="‖" close="‖"><mrow><mi
    mathvariant="bold">y</mi><mo>−</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">w</mi></mrow></mfenced><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi
    mathvariant="bold-italic">α</mi><msub><mfenced open="‖" close="‖"><mi mathvariant="bold">w</mi></mfenced><mn>2</mn></msub></mrow></mrow></math>](img/146.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>min</mi><mi
    mathvariant="bold">w</mi></munder><msubsup><mfenced open="‖" close="‖"><mrow><mi
    mathvariant="bold">y</mi><mo>−</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">w</mi></mrow></mfenced><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi
    mathvariant="bold-italic">α</mi><msub><mfenced open="‖" close="‖"><mi mathvariant="bold">w</mi></mfenced><mn>2</mn></msub></mrow></mrow></math>](img/146.png)'
- en: 'Here, we have the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '*N* is the number of samples in the training set.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 是训练集中样本的数量。'
- en: '*y* is the column vector of target values of size *N*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是大小为 *N* 的目标值的列向量。'
- en: '*X* is the design matrix of input features.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X* 是输入特征的设计矩阵。'
- en: '*w* is the vector of regression coefficients to be estimated.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w* 是待估计的回归系数向量。'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/147.png)
    is the regularization parameter that controls the strength of the penalty term.
    It is a hyperparameter that needs to be tuned.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/147.png)
    是控制惩罚项强度的正则化参数。它是一个需要调整的超参数。'
- en: The first term in the loss function measures the mean squared error between
    the predicted values and the true values. The second term is the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/148.png)
    penalty term that shrinks the coefficients toward zero. The ridge regression algorithm
    finds the values of the regression coefficients that minimize this loss function.
    By tuning the regularization parameter, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/149.png),
    we can control the bias-variance trade-off of the model, with higher alpha values
    leading to more regularization and lower overfitting.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的第一个项衡量的是预测值与真实值之间的均方误差。第二个项是![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/148.png)惩罚项，它将系数缩小到零。岭回归算法找到使该损失函数最小化的回归系数值。通过调整正则化参数![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/149.png)，我们可以控制模型的偏差-方差权衡，较高的α值会导致更强的正则化和较低的过拟合。
- en: Ridge regression can be used for feature selection by examining the magnitudes
    of the coefficients produced by the model. Features with coefficients that are
    close to zero or smaller are considered less important and can be dropped from
    the model. The value of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/150.png)
    can be tuned using cross-validation to find the optimal balance between model
    complexity and accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归可以通过检查模型产生的系数的幅度来进行特征选择。系数接近零或更小的特征被认为不太重要，可以从模型中删除。可以使用交叉验证调整![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/150.png)的值，以找到模型复杂性和准确性之间的最佳平衡。
- en: One of the main advantages of ridge regression is its ability to handle multicollinearity,
    which occurs when there are strong correlations between the independent variables.
    In such cases, ordinary least squares regression can produce unstable and unreliable
    coefficient estimates, but ridge regression can help stabilize the estimates and
    improve the overall performance of the model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归的一个主要优点是它能够处理多重共线性，当自变量之间存在强相关性时会发生多重共线性。在这种情况下，普通最小二乘回归可能会产生不稳定和不可靠的系数估计，但岭回归可以帮助稳定估计并提高模型的总体性能。
- en: Choosing LASSO or ridge regression
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择LASSO或岭回归
- en: Ridge regression and LASSO are both regularization techniques that are used
    in linear regression to prevent overfitting of the model by penalizing the model’s
    coefficients. While both methods seek to prevent overfitting, they differ in their
    approach to how the coefficients are penalized.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归和LASSO都是线性回归中使用的正则化技术，通过惩罚模型的系数来防止模型过拟合。虽然这两种方法都旨在防止过拟合，但它们在惩罚系数的方法上有所不同。
- en: Ridge regression adds a penalty term to the **sum of squared errors** (**SSE**)
    that is proportional to the square of the magnitude of the coefficients. The penalty
    term is controlled by a regularization parameter (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/151.png)),
    which determines the amount of shrinkage applied to the coefficients. This penalty
    term shrinks the values of the coefficients toward zero but does not set them
    exactly to zero. Therefore, ridge regression can be used to reduce the impact
    of irrelevant features in a model, but it will not eliminate them completely.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归向**平方误差和**（**SSE**）添加一个惩罚项，该惩罚项与系数幅度的平方成正比。惩罚项由正则化参数![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/151.png)控制，它决定了应用于系数的收缩量。这个惩罚项将系数的值缩小到零，但不会将它们精确地设置为零。因此，岭回归可以用来减少模型中无关特征的影响，但不会完全消除它们。
- en: On the other hand, LASSO also adds a penalty term to the SSE, but the penalty
    term is proportional to the absolute value of the coefficients. Like ridge, LASSO
    also has a regularization parameter (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/152.png))
    that determines the amount of shrinkage applied to the coefficients. However,
    LASSO has a unique property of setting some of the coefficients exactly to zero
    when the regularization parameter is sufficiently high. Therefore, LASSO can be
    used for feature selection as it can eliminate irrelevant features and set their
    corresponding coefficients to zero.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LASSO也向SSE添加一个惩罚项，但惩罚项与系数的绝对值成比例。与岭回归一样，LASSO也有一个正则化参数（![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/152.png)），它决定了应用于系数的收缩量。然而，当正则化参数足够高时，LASSO具有将一些系数精确设置为零的独特属性。因此，LASSO可以用作特征选择，因为它可以消除无关特征并将它们的对应系数设置为0。
- en: In general, if the dataset has many features and a small number of them are
    expected to be important, LASSO regression is a better choice as it will set the
    coefficients of irrelevant features to zero, leading to a simpler and more interpretable
    model. On the other hand, if most of the features in the dataset are expected
    to be relevant, ridge regression is a better choice as it will shrink the coefficients
    toward zero but not set them exactly to zero, preserving all the features in the
    model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果数据集具有许多特征，其中只有少数被认为是重要的，LASSO回归是一个更好的选择，因为它会将无关特征的系数设置为0，从而得到一个更简单、更可解释的模型。另一方面，如果预计数据集中的大多数特征都是相关的，则岭回归是一个更好的选择，因为它会将系数缩小到零，但不会将它们精确地设置为零，从而保留模型中的所有特征。
- en: However, it is important to note that the optimal choice between ridge and LASSO
    depends on the specific problem and dataset, and it is often recommended to try
    both and compare their performance using cross-validation techniques.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，岭回归和LASSO之间的最佳选择取决于具体问题和数据集，并且通常建议尝试两者并使用交叉验证技术比较它们的性能。
- en: Dimensionality reduction techniques
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度约简技术
- en: These methods transform the features into a lower-dimensional space while retaining
    as much information as possible. Popular methods include PCA, **linear discriminant
    analysis** (**LDA**), and **t-**SNE.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法将特征转换到低维空间，同时尽可能保留信息。常见的方法包括PCA、**线性判别分析**（**LDA**）和**t-SNE**。
- en: PCA
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA
- en: PCA is a widely used technique in machine learning for reducing the dimensionality
    of large datasets while retaining most of the important information. The basic
    idea of PCA is to transform a set of correlated variables into a set of uncorrelated
    variables known as principal components.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是机器学习中广泛使用的技术，用于降低大型数据集的维度，同时保留大部分重要信息。PCA的基本思想是将一组相关变量转换为一组称为主成分的不相关变量。
- en: The goal of PCA is to identify the directions of maximum variance in the data
    and project the data in these directions, reducing the dimensionality of the data.
    The principal components are sorted in order of the amount of variance they explain,
    with the first principal component explaining the most variance in the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的目标是在数据中识别最大方差的方向，并将数据投影到这些方向上，从而降低数据的维度。主成分按解释的方差量排序，第一个主成分解释了数据中最大的方差。
- en: 'The PCA algorithm involves the following steps:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法包括以下步骤：
- en: '**Standardize the data**: PCA requires the data to be standardized – that is,
    each feature must have zero mean and unit variance.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标准化数据**: PCA要求数据被标准化——也就是说，每个特征必须具有零均值和单位方差。'
- en: '**Compute the covariance matrix**: The covariance matrix is a square matrix
    that measures the linear relationships between pairs of features in the data.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算协方差矩阵**: 协方差矩阵是一个方阵，它衡量数据中成对特征之间的线性关系。'
- en: '**Compute the eigenvectors and eigenvalues of the covariance matrix**: The
    eigenvectors represent the primary directions of the highest variance within the
    dataset, while the eigenvalues quantify the extent of variance elucidated by each
    eigenvector.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算协方差矩阵的特征向量和特征值**: 特征向量代表数据集中最高方差的主要方向，而特征值量化了每个特征向量解释的方差程度。'
- en: '**Select the number of principal components**: The number of principal components
    to retain can be determined by analyzing the eigenvalues and selecting the top
    *k* eigenvectors that explain the most variance.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择主成分的数量**：保留主成分的数量可以通过分析特征值，并选择解释最大方差的前**k**个特征向量来确定。'
- en: '**Project the data onto the selected principal components**: The original data
    is projected onto the selected principal components, resulting in a lower-dimensional
    representation of the data.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据投影到选定的主成分上**：原始数据被投影到选定的主成分上，从而得到数据的低维表示。'
- en: PCA can be used for feature selection by selecting the top *k* principal components
    that explain the most variance in the data. This can be useful for reducing the
    dimensionality of high-dimensional datasets and improving the performance of machine
    learning models. However, it’s important to note that PCA may not always lead
    to improved performance, especially if the data is already low-dimensional or
    if the features are not highly correlated. It’s also important to consider the
    interpretability of the selected principal components as they may not always correspond
    to meaningful features in the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）可以通过选择解释数据中最大方差的前**k**个主成分来进行特征选择。这可以用于降低高维数据集的维度并提高机器学习模型的性能。然而，需要注意的是，PCA并不总是能带来性能的提升，特别是当数据已经是低维的或者特征之间没有高度相关性时。同时，还需要考虑所选主成分的可解释性，因为它们并不总是对应于数据中的有意义特征。
- en: LDA
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LDA
- en: LDA is a dimensionality reduction technique that’s used for feature selection
    in machine learning. It is often used in classification tasks to reduce the number
    of features by transforming them into a lower-dimensional space while retaining
    as much class-discriminatory information as possible.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种降维技术，用于机器学习中的特征选择。它通常用于分类任务，通过将特征转换到低维空间来减少特征数量，同时尽可能保留尽可能多的类区分信息。
- en: In LDA, the goal is to find a linear combination of the original features that
    maximizes the separation between classes. The input to LDA is a dataset of labeled
    examples, where each example is a feature vector with a corresponding class label.
    The output of LDA is a set of linear combinations of the original features, which
    can be used as new features in a machine learning model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性判别分析（LDA）中，目标是找到一个原始特征的线性组合，以最大化类之间的分离度。LDA的输入是一个标记示例的数据集，其中每个示例都是一个具有相应类标签的特征向量。LDA的输出是一组原始特征的线性组合，这些组合可以用作机器学习模型中的新特征。
- en: To perform LDA, the first step is to compute the mean and covariance matrix
    of each class. The overall mean and covariance matrix are then calculated from
    the class means and covariance matrices. The goal is to project the data onto
    a lower-dimensional space while still retaining the class information. This is
    achieved by finding the eigenvectors and eigenvalues of the covariance matrix,
    sorting them in descending order of the eigenvalues, and selecting the top *k*
    eigenvectors that correspond to the *k* largest eigenvalues. The selected eigenvectors
    form the basis for the new feature space.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 执行LDA的第一步是计算每个类的均值和协方差矩阵。然后从类均值和协方差矩阵中计算出整体均值和协方差矩阵。目标是投影数据到低维空间，同时保留类信息。这是通过找到协方差矩阵的特征向量和特征值，按特征值降序排列，并选择与**k**个最大特征值对应的**k**个特征向量来实现的。所选特征向量构成新特征空间的基础。
- en: 'The LDA algorithm can be summarized in the following steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）算法可以总结如下步骤：
- en: Compute the mean vector of each class.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个类的均值向量。
- en: Compute the covariance matrix of each class.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个类的协方差矩阵。
- en: Compute the overall mean vector and overall covariance matrix.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算整体均值向量和整体协方差矩阵。
- en: Compute the between-class scatter matrix.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类间散布矩阵。
- en: Compute the within-class scatter matrix.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类内散布矩阵。
- en: 'Compute the eigenvectors and eigenvalues of the matrix using the following
    equation:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下方程计算矩阵的特征向量和特征值：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math>](img/153.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math>](img/153.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/154.png)
    is the within-class scatter matrix and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math>](img/155.png)
    is the between-class scatter matrix.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/154.png)是类内散布矩阵，而![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math>](img/155.png)是类间散布矩阵。
- en: 7. Select the top *k* eigenvectors with the highest eigenvalues as the new feature
    space.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 选择具有最高特征值的**k**个主特征向量作为新的特征空间。
- en: LDA is particularly useful when the number of features is large and the number
    of examples is small. It can be used in a variety of applications, including image
    recognition, speech recognition, and NLP. However, it assumes that the classes
    are normally distributed and that the class covariance matrices are equal, which
    may not always be the case in practice.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LDA在特征数量大而示例数量少时特别有用。它可以用于各种应用，包括图像识别、语音识别和NLP。然而，它假设类是正态分布的，并且类协方差矩阵是相等的，但在实际应用中这并不总是成立。
- en: t-SNE
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: t-SNE
- en: t-SNE is a dimensionality reduction technique that’s used for visualizing high-dimensional
    data in a low-dimensional space, often used for feature selection. It was developed
    by Laurens van der Maaten and Geoffrey Hinton in 2008.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种降维技术，用于在低维空间中可视化高维数据，常用于特征选择。它由Laurens van der Maaten和Geoffrey Hinton于2008年开发。
- en: The basic idea behind t-SNE is to preserve the pairwise similarities of data
    points in a low-dimensional space, as opposed to preserving the distances between
    them. In other words, it tries to retain the local structure of the data while
    discarding the global structure. This can be useful in situations where the high-dimensional
    data is difficult to visualize, but there may be meaningful patterns and relationships
    among the data points.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE的基本思想是在低维空间中保留数据点的成对相似性，而不是保留它们之间的距离。换句话说，它试图保留数据的局部结构，同时丢弃全局结构。这在高维数据难以可视化的情况下可能很有用，但数据点之间可能存在有意义的模式和关系。
- en: t-SNE starts by calculating the pairwise similarity between each pair of data
    points in the high-dimensional space. The similarity is usually measured using
    a Gaussian kernel, which gives higher weights to nearby points and lower weights
    to distant points. The similarity matrix is then converted into a probability
    distribution using a softmax function. This distribution is used to create a low-dimensional
    space, typically 2D or 3D.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE首先计算高维空间中每对数据点之间的成对相似性。相似性通常使用高斯核来衡量，它给附近的点更高的权重，给远离的点更低的权重。然后，使用softmax函数将相似性矩阵转换为概率分布。这个分布用于创建一个低维空间，通常是2D或3D。
- en: In the low-dimensional space, t-SNE again calculates the pairwise similarities
    between each pair of data points, but this time using a student’s t-distribution
    instead of a Gaussian distribution. The t-distribution has heavier tails than
    the Gaussian distribution, which helps to better preserve the local structure
    of the data. t-SNE then adjusts the position of the points in the low-dimensional
    space to minimize the difference between the pairwise similarities in the high-dimensional
    space and the pairwise similarities in the low-dimensional space.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维空间中，t-SNE再次计算每对数据点之间的成对相似度，但这次使用学生t分布而不是高斯分布。t分布的尾部比高斯分布更重，这有助于更好地保留数据的局部结构。然后t-SNE调整低维空间中点的位置，以最小化高维空间中的成对相似度与低维空间中的成对相似度之间的差异。
- en: t-SNE is a powerful technique for visualizing high-dimensional data by reducing
    it to a low-dimensional space. However, it is not typically used for feature selection
    as its primary purpose is to create visualizations of complex datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种通过将其降低到低维空间来可视化高维数据的有力技术。然而，它通常不用于特征选择，因为其主要目的是创建复杂数据集的视觉表示。
- en: 'Instead, t-SNE can be used to help identify clusters of data points that share
    similar features, which may be useful in identifying groups of features that are
    important for a particular task. For example, suppose you have a dataset of customer
    demographics and purchase history, and you want to identify groups of customers
    that are similar based on their purchasing behavior. You could use t-SNE to reduce
    the high-dimensional feature space to two dimensions, and then plot the resulting
    data points on a scatter plot. By examining the plot, you might be able to identify
    clusters of customers with similar purchasing behavior, which could then inform
    your feature selection process. Here’s a sample t-SNE for the MNIST dataset:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，t-SNE可以用来帮助识别具有相似特征的数据点簇，这可能有助于识别对特定任务重要的特征组。例如，假设你有一个包含客户人口统计信息和购买历史的数据集，你想根据他们的购买行为识别相似的客户组。你可以使用t-SNE将高维特征空间降低到二维，然后在散点图上绘制结果数据点。通过检查图表，你可能能够识别具有相似购买行为的客户簇，这可以随后指导你的特征选择过程。以下是对MNIST数据集的t-SNE示例：
- en: '![Figure 3.1 – t-SNE on the MNIST dataset](img/B18949_03_1.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – MNIST数据集上的t-SNE](img/B18949_03_1.jpg)'
- en: Figure 3.1 – t-SNE on the MNIST dataset
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – MNIST数据集上的t-SNE
- en: It’s worth noting that t-SNE is primarily a visualization tool and should not
    be used as the sole method for feature selection. Instead, it can be used in conjunction
    with other techniques, such as LDA or PCA, to gain a more complete understanding
    of the underlying structure of your data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，t-SNE主要是一个可视化工具，不应作为特征选择的唯一方法。相反，它可以与其他技术结合使用，例如LDA或PCA，以更全面地了解数据的潜在结构。
- en: The choice of feature selection method depends on the nature of the data, the
    size of the dataset, the complexity of the model, and the computational resources
    available. It is important to carefully evaluate the performance of the model
    after feature selection to ensure that important information has not been lost.
    Another important process is feature engineering, which is about transforming
    or selecting features for the machine learning models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择方法的选择取决于数据的性质、数据集的大小、模型的复杂性和可用的计算资源。在特征选择后仔细评估模型的性能，以确保没有丢失重要信息，这是一个重要的过程。另一个重要的过程是特征工程，它涉及为机器学习模型转换或选择特征。
- en: Feature engineering
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering is the process of selecting, transforming, and extracting
    features from raw data to improve the performance of machine learning models.
    Features are the individual measurable properties or characteristics of the data
    that can be used to make predictions or classifications.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是从原始数据中选择、转换和提取特征的过程，以提高机器学习模型的表现。特征是数据中可以用来进行预测或分类的个别可测量属性或特征。
- en: One common technique in feature engineering is feature selection, which involves
    selecting a subset of relevant features from the original dataset to improve the
    model’s accuracy and reduce its complexity. This can be done through statistical
    methods such as correlation analysis or feature importance ranking using decision
    trees or random forests.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程中的一种常见技术是特征选择，它涉及从原始数据集中选择一组相关特征以提高模型的准确性和降低其复杂性。这可以通过统计方法如相关分析或使用决策树或随机森林进行特征重要性排名来实现。
- en: 'Another technique in feature engineering is feature extraction, which involves
    transforming the raw data into a new set of features that may be more useful for
    the model. The primary distinction between feature selection and feature engineering
    lies in their approaches: while feature selection retains a subset of the original
    features without modifying the selected features, feature engineering algorithms
    reconfigure and transform the data into a new feature space. Feature engineering
    can be done through techniques such as dimensionality reduction, PCA, or t-SNE.
    Feature selection and extraction were explained in detail in the previous subsection
    (3-1-3).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程中的另一种技术是特征提取，它涉及将原始数据转换为可能对模型更有用的新特征集。特征选择与特征工程之间的主要区别在于它们的方法：特征选择在不修改所选特征的情况下保留原始特征的一个子集，而特征工程算法重新配置和转换数据到新的特征空间。特征选择和提取在先前的子节（3-1-3）中已详细解释。
- en: Feature scaling is another important technique in feature engineering that involves
    scaling the values of features to the same range, typically between 0 and 1 or
    -1 and 1\. This is done to prevent certain features from dominating others in
    the model and to ensure that the algorithm can converge quickly during training.
    When the features in the dataset have different scales, this can lead to issues
    when using certain machine learning algorithms that are sensitive to the relative
    magnitudes of the features. Feature scaling can help to address this problem by
    ensuring that all features are on a similar scale. Common methods for feature
    scaling include min-max scaling, Z-score scaling, and scaling by the maximum absolute
    value.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放是特征工程中另一个重要的技术，它涉及将特征值缩放到相同的范围，通常是介于 0 和 1 或 -1 和 1 之间。这样做是为了防止某些特征在模型中主导其他特征，并确保算法在训练过程中可以快速收敛。当数据集中的特征具有不同的尺度时，这可能会导致使用某些对特征相对大小敏感的机器学习算法时出现问题时。特征缩放可以通过确保所有特征处于相似尺度来帮助解决这个问题。常见的特征缩放方法包括最小-最大缩放、Z
    分数缩放和基于最大绝对值的缩放。
- en: 'There are several common methods for feature scaling:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放有几种常见的方法：
- en: '**Min-max scaling**: Also known as normalization, this technique scales the
    values of the feature to be between a specified range, typically between 0 and
    1 (for regular machine learning models, and sometimes -1 and 1 for deep learning
    models). The formula for min-max scaling is shown here:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大缩放**：也称为归一化，这种技术将特征值缩放到指定的范围，通常是介于 0 和 1 之间（对于常规机器学习模型，有时对于深度学习模型是 -1
    和 1）。最小-最大缩放的公式如下所示：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/156.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/156.png)'
- en: Here, *x* is the original feature value, *min(x)* is the minimum value of the
    feature, and *max(x)* is the maximum value of the feature.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是原始特征值，*min(x)* 是特征的最小值，而 *max(x)* 是特征的最大值。
- en: '**Standardization**: This technique transforms the feature values to have a
    mean of 0 and a standard deviation of 1\. Standardization is less affected by
    outliers in the data than min-max scaling. The formula for standardization is
    shown here:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：这种技术将特征值转换为具有 0 均值和 1 标准差。与最小-最大缩放相比，标准化受数据中异常值的影响较小。标准化的公式如下所示：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/157.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/157.png)'
- en: Here, *x* is the original feature value, *mean(x)* is the mean of the feature,
    and *std(x)* is the standard deviation of the feature.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是原始特征值，*mean(x)* 是特征的平均值，*std(x)* 是特征的标准差。
- en: '**Robust scaling**: This technique is similar to standardization but uses the
    median and **interquartile range** (**IQR**) instead of the mean and standard
    deviation. Robust scaling is useful when the data contains outliers that would
    significantly affect the mean and standard deviation. The formula for robust scaling
    is shown here:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒缩放**: 这种技术与标准化类似，但使用中位数和**四分位距**（**IQR**）而不是平均值和标准差。当数据包含会显著影响平均值和标准差的外部值时，鲁棒缩放很有用。鲁棒缩放的公式如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>Q</mi><mn>3</mn><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>Q</mi><mn>1</mn><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/158.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>Q</mi><mn>3</mn><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>Q</mi><mn>1</mn><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/158.png)'
- en: Here, *x* is the original feature value, *median(x)* is the median of the feature,
    *Q1(x)* is the first quartile of the feature, and *Q3(x)* is the third quartile
    of the feature.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是原始特征值，*median(x)* 是特征的中位数，*Q1(x)* 是特征的第一四分位数，*Q3(x)* 是特征的第三四分位数。
- en: '**Log transformation**: This technique is used when the data is highly skewed
    or has a long tail. By taking the logarithm of the feature values, the distribution
    can be made more normal or symmetric, which can improve the performance of some
    machine learning algorithms. The formula for log transformation is shown here:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数变换**: 当数据高度偏斜或具有长尾时，使用此技术。通过对特征值取对数，可以使分布更加正常或对称，这可以提高某些机器学习算法的性能。对数变换的公式如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/159.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/159.png)'
- en: Here, *x* is the original feature value.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是原始特征值。
- en: '**Power transformation**: This technique is similar to log transformation but
    allows for a broader range of transformations. The most common power transformation
    is the Box-Cox transformation, which raises the feature values to a power that
    is determined using maximum likelihood estimation. The formula for the Box-Cox
    transformation is shown here:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂变换**: 这种技术与对数变换类似，但允许更广泛的变换范围。最常见的幂变换是Box-Cox变换，它将特征值提升到通过最大似然估计确定的幂。Box-Cox变换的公式如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>x</mi><mi>λ</mi></msup><mo>−</mo><mn>1</mn></mrow><mi>λ</mi></mfrac></mrow></mrow></math>](img/160.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>x</mi><mi>λ</mi></msup><mo>−</mo><mn>1</mn></mrow><mi>λ</mi></mfrac></mrow></mrow></math>](img/160.png)'
- en: Here, *x* is the original feature value, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/161.png)
    is the power parameter that is estimated using maximum likelihood.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是原始特征值，而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/161.png)
    是通过最大似然估计得到的功率参数。
- en: These are some of the most common methods for feature scaling in machine learning.
    The choice of method depends on the distribution of the data, the machine learning
    algorithm being used, and the specific requirements of the problem.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是机器学习中特征缩放的一些最常见方法。方法的选择取决于数据的分布、所使用的机器学习算法以及问题的具体要求。
- en: One final technique in feature engineering is feature construction, which involves
    creating new features by combining or transforming existing ones. This can be
    done through techniques such as polynomial expansion, logarithmic transformation,
    or interaction terms.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程中的最后一项技术是特征构造，它涉及通过组合或转换现有特征来创建新的特征。这可以通过多项式展开、对数变换或交互项等技术来完成。
- en: Polynomial expansion
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多项式展开
- en: Polynomial expansion is a feature construction technique that involves creating
    new features by taking polynomial combinations of existing features. This technique
    is commonly used in machine learning to model nonlinear relationships between
    features and the target variable.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式展开是一种特征构造技术，它通过将现有特征的幂组合来创建新的特征。这种技术在机器学习中常用以建模特征与目标变量之间的非线性关系。
- en: The idea behind polynomial expansion is to create new features by raising the
    existing features to different powers and taking their products. For example,
    suppose we have a single feature, *x*. We can create new features by taking the
    square of *x (**![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow></math>](img/162.png)**)*.
    We can also create higher-order polynomial features by taking *x* to even higher
    powers, such as ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>3</mn></msup></mrow></math>](img/163.png)*,*
    *![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>4</mn></msup></mrow></math>](img/164.png)*,
    and so on. In general, we can create polynomial features of degree *d* by taking
    all possible combinations of products and powers of the original features up to
    degree *d*.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式展开背后的思想是通过将现有特征提升到不同的幂次并取其乘积来创建新的特征。例如，假设我们有一个单个特征，*x*。我们可以通过取 *x* 的平方（**![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow></math>](img/162.png)**）来创建新的特征。我们还可以通过将
    *x* 提升到更高的幂次来创建更高阶的多项式特征，例如 ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>3</mn></msup></mrow></math>](img/163.png)*，*
    *![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>4</mn></msup></mrow></math>](img/164.png)*，等等。一般来说，我们可以通过取原始特征的乘积和幂的所有可能组合来创建度数为
    *d* 的多项式特征。
- en: In addition to creating polynomial features from a single feature, we can also
    create polynomial features from multiple features. For example, suppose we have
    two features, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/165.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/166.png).
    We can create new polynomial features by taking their products (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/167.png))
    and raising them to different powers ( ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/168.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/169.png),
    and so on). Again, we can create polynomial features of any degree by taking all
    possible combinations of products and powers of the original features.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从单个特征创建多项式特征外，我们还可以从多个特征创建多项式特征。例如，假设我们有两个特征，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/165.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/166.png)。我们可以通过取它们的乘积
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/167.png))
    并将它们提升到不同的幂次（![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/168.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/169.png),
    等等）来创建新的多项式特征。同样，我们可以通过取原始特征的所有可能的乘积和幂次的组合来创建任何程度的多项式特征。
- en: One important consideration when using polynomial expansion is that it can quickly
    lead to a large number of features, especially for high degrees of polynomials.
    This can make the resulting model more complex and harder to interpret, and can
    also lead to overfitting if the number of features is not properly controlled.
    To address this issue, it is common to use regularization techniques or feature
    selection methods to select a subset of the most informative polynomial features.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用多项式展开时，一个重要的考虑因素是它可能会迅速导致大量特征的产生，尤其是在高次多项式的情况下。这可能会使得到的模型更加复杂且难以解释，如果特征数量没有得到适当的控制，还可能导致过拟合。为了解决这个问题，通常使用正则化技术或特征选择方法来选择最有信息量的多项式特征子集。
- en: Overall, polynomial expansion is a powerful feature construction technique that
    can help capture complex nonlinear relationships between features and the target
    variable. However, it should be used with caution and with appropriate regularization
    or feature selection to avoid overfitting and maintain model interpretability.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，多项式展开是一种强大的特征构造技术，可以帮助捕捉特征与目标变量之间的复杂非线性关系。然而，在使用时应谨慎，并采用适当的正则化或特征选择来避免过拟合并保持模型的可解释性。
- en: For example, in a regression problem, you might have a dataset with a single
    feature, say *x*, and you want to fit a model that can capture the relationship
    between *x* and the target variable, *y*. However, the relationship between *x*
    and *y* may not be linear, and a simple linear model may not be sufficient. In
    this case, polynomial expansion can be used to create additional features that
    capture the non-linear relationship between *x* and *y*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个回归问题中，你可能有一个只包含一个特征的数据集，比如说*x*，你想要拟合一个模型来捕捉*x*和目标变量*y*之间的关系。然而，*x*和*y*之间的关系可能不是线性的，一个简单的线性模型可能不足以捕捉这种关系。在这种情况下，可以使用多项式展开来创建额外的特征，以捕捉*x*和*y*之间的非线性关系。
- en: To illustrate, let’s say you have a dataset with a single feature, *x*, and
    a target variable, *y*, and you want to fit a polynomial regression model. The
    goal is to find a function, *f(x)*, that minimizes the difference between the
    predicted and actual values of *y*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，假设你有一个包含一个特征*x*和一个目标变量*y*的数据集，并且你想要拟合一个多项式回归模型。目标是找到一个函数*f(x*)，它最小化预测值和实际值*y*之间的差异。
- en: Polynomial expansion can be used to create additional features based on *x*,
    such as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/170.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/171.png),
    and so on. This can be done using libraries such as `scikit-learn`, which has
    a `PolynomialFeatures` function that can automatically generate polynomial features
    of a specified degree.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多项式展开来创建基于*x*的额外特征，例如![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/170.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/171.png)，等等。这可以使用如`scikit-learn`这样的库来完成，它有一个`PolynomialFeatures`函数可以自动生成指定度数的多项式特征。
- en: By adding these polynomial features, the model becomes more expressive and can
    capture the non-linear relationship between *x* and *y*. However, it’s important
    to be careful not to overfit the data as adding too many polynomial features can
    lead to a model that is overly complex and performs poorly on new, unseen data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加这些多项式特征，模型变得更加具有表现力，可以捕捉*x*和*y*之间的非线性关系。然而，重要的是要注意不要过度拟合数据，因为添加过多的多项式特征可能导致模型过于复杂，在新数据上表现不佳。
- en: Logarithmic transformation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数变换
- en: Logarithmic transformation is a common feature engineering technique that’s
    used in data preprocessing. The goal of logarithmic transformation is to make
    data less skewed and more symmetric by applying a logarithmic function to the
    features. This technique can be particularly useful for features that are skewed,
    such as those with a long tail of high values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换是一种常见的数据预处理特征工程技术。对数变换的目标是通过应用对数函数到特征上，使数据更加对称并减少偏斜。这种技术对于具有长尾高值的偏斜特征尤其有用。
- en: 'The logarithmic transformation is defined as an equation taking the natural
    logarithm of the data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换被定义为对数据进行自然对数运算的方程：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/172.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/172.png)'
- en: Here, *y* is the transformed data and *x* is the original data. The logarithmic
    function maps the original data to a new space, where the relationship between
    the values is preserved but the scale is compressed. The logarithmic transformation
    is particularly useful for features with large ranges or that are distributed
    exponentially, such as the prices of products or the incomes of individuals.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y*是变换后的数据，*x*是原始数据。对数函数将原始数据映射到一个新的空间，其中值之间的关系得到保留，但尺度被压缩。对数变换对于具有大范围或呈指数分布的特征特别有用，例如产品的价格或个人的收入。
- en: One of the benefits of the logarithmic transformation is that it can help normalize
    data and make it more suitable for certain machine learning algorithms that assume
    normally distributed data. Additionally, logarithmic transformation can reduce
    the impact of outliers on the data, which can help improve the performance of
    some models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换的一个好处是它可以帮助数据归一化，使其更适合某些假设数据正态分布的机器学习算法。此外，对数变换可以减少异常值对数据的影响，这有助于提高某些模型的表现。
- en: It’s important to note that the logarithmic transformation is not appropriate
    for all types of data. For example, if the data includes zero or negative values,
    the logarithmic transformation cannot be applied directly. In these cases, a modified
    logarithmic transformation, such as adding a constant before taking the logarithm,
    may be used. Overall, logarithmic transformation is a useful technique for feature
    engineering that can help improve the performance of machine learning models,
    especially when dealing with skewed or exponentially distributed data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，对数变换并不适用于所有类型的数据。例如，如果数据包含零或负值，则不能直接应用对数变换。在这些情况下，可以使用修改后的对数变换，例如在对数之前添加一个常数。总的来说，对数变换是特征工程中的一种有用技术，可以帮助提高机器学习模型的表现，尤其是在处理偏斜或指数分布的数据时。
- en: In summary, feature engineering is a critical step in the machine learning pipeline
    as it can significantly impact the performance and interpretability of the resulting
    models. Effective feature engineering requires domain knowledge, creativity, and
    an iterative process of testing and refining different techniques until the optimal
    set of features is identified.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，特征工程是机器学习流程中的关键步骤，因为它可以显著影响最终模型的表现力和可解释性。有效的特征工程需要领域知识、创造力和测试和改进不同技术的迭代过程，直到确定最佳特征集。
- en: Interaction terms
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交互项
- en: In feature construction, interaction terms refer to creating new features by
    combining two or more existing features in a dataset through multiplication, division,
    or other mathematical operations. These new features capture the interaction or
    relationship between the original features, and they can help improve the accuracy
    of machine learning models.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征构建中，交互项指的是通过将数据集中两个或更多现有特征通过乘法、除法或其他数学运算组合来创建新的特征。这些新特征捕捉了原始特征之间的交互或关系，并且可以帮助提高机器学习模型的准确性。
- en: For example, in a dataset of real estate prices, you might have features such
    as the number of bedrooms, the number of bathrooms, and the square footage of
    the property. By themselves, these features provide some information about the
    price of the property, but they do not capture any interaction effects between
    the features. However, by creating an interaction term between the number of bedrooms
    and the square footage, you can capture the idea that larger properties with more
    bedrooms tend to be more expensive than smaller ones with the same number of bedrooms.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个房地产价格数据集中，你可能会有诸如卧室数量、浴室数量和房产面积等特征。单独来看，这些特征提供了一些关于房产价格的信息，但它们并没有捕捉到特征之间的任何交互效应。然而，通过在卧室数量和面积之间创建交互项，你可以捕捉到这样一个观点：具有更多卧室的大面积房产通常比具有相同卧室数量的较小房产更贵。
- en: 'In practice, interaction terms are created by multiplying or dividing two or
    more features together. For example, if we have two features, *x* and *y*, we
    can create an interaction term by multiplying them together: *xy*. We can also
    create interaction terms by dividing one feature by another: *x/y*.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，交互项是通过将两个或更多特征相乘或相除来创建的。例如，如果我们有两个特征，*x* 和 *y*，我们可以通过将它们相乘来创建一个交互项：*xy*。我们也可以通过将一个特征除以另一个特征来创建交互项：*x/y*。
- en: 'When creating interaction terms, it is important to consider which features
    to combine and how to combine them. Here are some common techniques:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建交互项时，重要的是要考虑要组合哪些特征以及如何组合它们。以下是一些常见的技术：
- en: '**Domain knowledge**: Use domain knowledge or expert intuition to identify
    which features are likely to interact and how they might interact.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域知识**：使用领域知识或专家直觉来确定哪些特征可能存在交互以及它们可能如何交互。'
- en: '**Pairwise combinations**: Create interaction terms by pairwise combining all
    pairs of features in the dataset. This can be computationally expensive, but it
    can help identify potential interaction effects.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成对组合**：通过成对组合数据集中所有特征的成对来创建交互项。这可能计算成本较高，但可以帮助识别潜在的交互效应。'
- en: '**PCA**: Use PCA to identify the most important combinations of features, and
    create interaction terms based on these combinations.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCA**：使用PCA来识别最重要的特征组合，并基于这些组合创建交互项。'
- en: Overall, interaction terms are a powerful tool in feature construction that
    can help capture complex relationships between features and improve the accuracy
    of machine learning models. However, it is important to be careful when creating
    interaction terms as too many or poorly chosen terms can lead to overfitting or
    decreased model interpretability.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，交互项是特征构建中的强大工具，可以帮助捕捉特征之间的复杂关系，并提高机器学习模型的准确性。然而，在创建交互项时需要谨慎，因为过多或选择不当的项可能导致过拟合或降低模型的可解释性。
- en: Common machine learning models
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的机器学习模型
- en: Here, we will explain some of the most common machine learning models, as well
    as their advantages and disadvantages. Knowing this information will help you
    pick the best model for the problem and be able to improve the implemented model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将解释一些最常用的机器学习模型，以及它们的优缺点。了解这些信息将帮助您选择最适合问题的模型，并能够改进所实现的模型。
- en: Linear regression
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear regression is a type of supervised learning algorithm that’s used to
    model the relationship between a dependent variable and one or more independent
    variables. It assumes a linear relationship between the input features and the
    output. The goal of linear regression is to find the best-fit line that predicts
    the value of the dependent variable based on the independent variables.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种监督学习算法，用于建模因变量和一或多个自变量之间的关系。它假设输入特征与输出之间存在线性关系。线性回归的目标是找到最佳拟合线，根据自变量预测因变量的值。
- en: 'The equation for a simple linear regression with one independent variable (also
    called a **simple linear equation**) is as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 具有一个自变量的简单线性回归方程（也称为**简单线性方程**）如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/173.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/173.png)'
- en: 'Here, we have the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '*y* is the dependent variable (the variable we want to predict)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是因变量（我们想要预测的变量）'
- en: '*x* is the independent variable (the input variable)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是自变量（输入变量）'
- en: '*m* is the slope of the line (how much *y* changes when *x* changes)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* 是直线的斜率（当*x*变化时*y*的变化量）'
- en: '*b* is the y-intercept (where the line intercepts the *Y*-axis when *x* = 0)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 是y截距（当*x* = 0时，直线与*Y*轴的交点）'
- en: The goal of linear regression is to find the values of *m* and *b* that minimize
    the difference between the predicted values and the actual values of the dependent
    variable. This difference is typically measured using a cost function, such as
    mean squared error or mean absolute error.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是找到*m*和*b*的值，以最小化预测值与因变量的实际值之间的差异。这种差异通常使用成本函数来衡量，例如均方误差或平均绝对误差。
- en: 'Multiple linear regression is an extension of simple linear regression, where
    there are multiple independent variables. The equation for multiple linear regression
    is shown here:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归是简单线性回归的扩展，其中存在多个自变量。多元线性回归的方程如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>b</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/174.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>b</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/174.png)'
- en: 'Here we have the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有以下内容：
- en: '*y* is the dependent variable'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是因变量'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/175.png)
    are the independent variables'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/175.png)是自变量'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/176.png)
    is the y-intercept (when all the independent variables are equal to 0)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/176.png)是y轴截距（当所有自变量都等于0时）'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/177.png)
    are the coefficients (how much *y* changes when each independent variable changes)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/177.png)是系数（当每个自变量变化时，*y*的变化量）'
- en: Similar to simple linear regression, the goal of multiple linear regression
    is to find the values of ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/178.png)![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/179.png)
    that minimize the difference between the predicted values and the actual values
    of the dependent variable.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的线性回归类似，多重线性回归的目标是找到![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/178.png)![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/179.png)的值，以最小化预测值与依赖变量实际值之间的差异。
- en: 'The advantages of linear regression are as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的优点如下：
- en: It’s simple and easy to understand
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它简单易懂
- en: It can be used to model a wide range of relationships between the dependent
    and independent variables
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用来模拟依赖变量和自变量之间广泛的多种关系
- en: It’s computationally efficient, making it fast and suitable for large datasets
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它计算效率高，使其快速且适用于大型数据集
- en: It provides interpretable results, allowing for the analysis of the impact of
    each independent variable on the dependent variable
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供可解释的结果，允许分析每个自变量对依赖变量的影响
- en: 'The disadvantages of linear regression are as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的缺点如下：
- en: It assumes a linear relationship between the input features and the output,
    which may not always be the case in real-world data
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设输入特征与输出之间存在线性关系，这在现实世界的数据中可能并不总是如此
- en: It may not capture complex non-linear relationships between the input features
    and the output
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能无法捕捉输入特征与输出之间的复杂非线性关系
- en: It’s sensitive to outliers and influential observations, which can affect the
    accuracy of the model
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对异常值和有影响力的观测值敏感，这可能会影响模型的准确性
- en: It assumes that the errors are normally distributed with constant variance,
    which may not always hold true in practice
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设误差是正态分布且方差恒定，这在实践中可能并不总是成立
- en: Logistic regression
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is a popular machine learning algorithm that’s used for
    classification problems. Unlike linear regression, which is used for predicting
    continuous values, logistic regression is used for predicting discrete outcomes,
    typically binary outcomes (0 or 1).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种流行的机器学习算法，用于分类问题。与用于预测连续值的线性回归不同，逻辑回归用于预测离散结果，通常是二元结果（0或1）。
- en: The goal of logistic regression is to estimate the probability of a certain
    outcome based on one or more input variables. The output of logistic regression
    is a probability score, which can be converted into a binary class label by applying
    a threshold value. The threshold value can be adjusted to balance between precision
    and recall based on the specific requirements of the problem.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的目标是根据一个或多个输入变量估计某个结果的概率。逻辑回归的输出是一个概率分数，可以通过应用阈值值将其转换为二元类别标签。阈值值可以根据问题的具体要求进行调整，以在精确度和召回率之间取得平衡。
- en: 'The logistic regression model assumes that the relationship between the input
    variables and the output variable is linear in the logit (log odds) space. The
    logit function is defined as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型假设输入变量与输出变量之间的关系在 logit（对数几率）空间中是线性的。对数几率函数定义为以下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>p</mi><mo>/</mo><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/180.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>p</mi><mo>/</mo><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/180.png)'
- en: Here, *p* is the probability of the positive outcome (that is, the probability
    of the event occurring).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p* 是正结果的概率（即事件发生的概率）。
- en: 'The logistic regression model can be represented mathematically as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型可以用以下数学公式表示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>..</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></mrow></math>](img/181.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>..</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></mrow></math>](img/181.png)'
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>,</mo><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>β</mi><mi>n</mi></msub></mrow></mrow></math>](img/182.png)
    are the coefficients of the model, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/183.png)
    are the input variables, and *logit(p)* is the logit function of the probability
    of a positive outcome.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>,</mo><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>β</mi><mi>n</mi></msub></mrow></mrow></math>](img/182.png)
    是模型的系数，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/183.png)
    是输入变量，而 *logit(p)* 是正结果概率的对数几率函数。
- en: The logistic regression model is trained using a dataset of labeled examples,
    where each example consists of a set of input variables and a binary label indicating
    whether the positive outcome occurred or not. The coefficients of the model are
    estimated using maximum likelihood estimation, which seeks to find the values
    of the coefficients that maximize the likelihood of the observed data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型使用带标签的示例数据集进行训练，其中每个示例由一组输入变量和一个二元标签组成，表示正结果是否发生。模型的系数通过最大似然估计来估计，该估计旨在找到使观察数据的似然性最大化的系数值。
- en: 'The advantages of logistic regression are as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的优点如下：
- en: '**Interpretable**: The coefficients of the model can be interpreted as the
    change in the log odds of the positive outcome associated with a unit change in
    the corresponding input variable, making it easy to understand the impact of each
    input variable on the predicted probability of the positive outcome'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：模型的系数可以解释为与对应输入变量单位变化相关的正结果对数概率的变化，这使得理解每个输入变量对预测正结果概率的影响变得容易'
- en: '**Computationally efficient**: Logistic regression is a simple algorithm that
    can be trained quickly on large datasets'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算效率高**：逻辑回归是一个简单的算法，可以在大型数据集上快速训练'
- en: '**Works well with small datasets**: Logistic regression can be effective even
    with a small number of observations, provided that the input variables are relevant
    to the prediction task'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于小型数据集**：逻辑回归即使在观察值数量较少的情况下也可以有效，只要输入变量与预测任务相关'
- en: 'The disadvantages of logistic regression are as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的缺点如下：
- en: '**Assumes linearity**: Logistic regression assumes a linear relationship between
    the input variables and the logit of the probability of the positive outcome,
    which may not always be the case in real-world datasets'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设线性关系**：逻辑回归假设输入变量与正结果概率的对数之间的线性关系，但在现实世界的数据集中这并不总是成立'
- en: '**May suffer from overfitting**: If the number of input variables is large
    compared to the number of observations, the model may suffer from overfitting,
    leading to poor generalization performance on new data'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能存在过拟合问题**：如果输入变量的数量与观察值的数量相比很大，模型可能存在过拟合问题，导致在新数据上的泛化性能较差'
- en: '**Not suitable for non-linear problems**: Logistic regression is a linear algorithm
    and is not suitable for problems where the relationship between the input variables
    and the output variable is non-linear'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不适用于非线性问题**：逻辑回归是一个线性算法，不适用于输入变量与输出变量之间关系为非线性的问题'
- en: Decision trees
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are a type of supervised learning algorithm used for classification
    and regression analysis. A decision tree consists of a series of nodes that represent
    decision points, each of which has one or more branches that lead to other decision
    points or a final prediction.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种用于分类和回归分析的监督学习算法。决策树由一系列节点组成，每个节点代表一个决策点，每个节点有一个或多个分支，这些分支通向其他决策点或最终预测。
- en: In a classification problem, each leaf node of the tree represents a class label,
    while in a regression problem, each leaf node represents a numerical value. The
    process of building a decision tree involves choosing a sequence of attributes
    that best splits the data into subsets that are more homogenous concerning the
    target variable. This process is typically repeated recursively for each subset
    until a stopping criterion is met, such as a minimum number of instances in each
    subset or a maximum depth of the tree.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，树的每个叶节点代表一个类标签，而在回归问题中，每个叶节点代表一个数值。构建决策树的过程涉及选择一个属性序列，以最佳方式将数据分割成更同质化的子集，这些子集与目标变量相关。这个过程通常在每个子集上递归重复，直到满足停止标准，例如每个子集中的最小实例数或树的最大深度。
- en: 'The equations for decision trees involve calculating the information gain (or
    another splitting criterion, such as Gini impurity or entropy) for each potential
    split at each decision point. The attribute with the highest information gain
    is selected as the split criterion for that node. The conceptual formula for information
    gain is shown here:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的方程涉及在每个决策点计算每个潜在分割的信息增益（或另一个分割标准，如基尼不纯度或熵）。具有最高信息增益的属性被选为该节点的分割标准。信息增益的概念公式如下所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>I</mi><mi>n</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo>=</mo><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>(</mo><mi>p</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>)</mo><mo>−</mo><mo>[</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>d</mi><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>i</mi><mi>e</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>n</mi><mrow><mi>t</mi><mo>′</mo></mrow><mi>s</mi><mi>c</mi><mi>h</mi><mi>i</mi><mi>l</mi><mi>d</mi><mi>r</mi><mi>e</mi><mi>n</mi><mo>]</mo></mrow></mrow></mrow></math>](img/184.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>I</mi><mi>n</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo>=</mo><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>(</mo><mi>p</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>)</mo><mo>−</mo><mo>[</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>d</mi><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>i</mi><mi>e</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>n</mi><mrow><mi>t</mi><mo>′</mo></mrow><mi>s</mi><mi>c</mi><mi>h</mi><mi>i</mi><mi>l</mi><mi>d</mi><mi>r</mi><mi>e</mi><mi>n</mi><mo>]</mo></mrow></mrow></mrow></math>](img/184.png)'
- en: Here, *entropy* is a measure of the impurity or randomness of a system. In the
    context of decision trees, entropy is used to measure the impurity of a node in
    the tree.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*熵* 是衡量系统不纯度或随机性的度量。在决策树的情况下，熵用于衡量树中节点的杂乱程度。
- en: 'The *entropy* of a node is calculated as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的 *熵* 计算如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msub><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow><mn>2</mn></msub><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math>](img/185.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msub><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow><mn>2</mn></msub><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math>](img/185.png)'
- en: Here, *c* is the number of classes and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/186.png)
    is the proportion of the samples that belong to class *i* in the node.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*c* 是类的数量，而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/186.png)
    是属于节点类 *i* 的样本比例。
- en: The entropy of a node ranges from 0 to 1, with 0 indicating a pure node (that
    is, all samples belong to the same class) and 1 indicating a node that is evenly
    split between all classes.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的熵值范围从 0 到 1，其中 0 表示纯节点（即所有样本都属于同一类），1 表示节点在所有类别之间均匀分割。
- en: In a decision tree, the entropy of a node is used to determine the splitting
    criterion for the tree. The idea is to split the node into two or more child nodes
    such that the entropy of the child nodes is lower than the entropy of the parent
    node. The split with the lowest entropy is chosen as the best split.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，节点的熵用于确定树的分割标准。其思想是将节点分割成两个或更多子节点，使得子节点的熵低于父节点的熵。具有最低熵的分割被选为最佳分割。
- en: Please note that the choice of the next node in the decision tree differs based
    on the underlying algorithm – for example, CART, ID3, or C4.5\. What we explained
    here was CART, which uses Gini impurity and entropy to split the data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树中下一个节点的选择取决于底层算法——例如，CART、ID3 或 C4.5。这里我们解释的是 CART，它使用基尼不纯度和熵来分割数据。
- en: The advantage of using entropy as a splitting criterion is that it can handle
    both binary and multi-class classification problems. It is also relatively computationally
    efficient compared to other splitting criteria. However, one disadvantage of using
    entropy is that it tends to create biased trees in favor of attributes with many
    categories.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用熵作为分割标准的好处是它可以处理二分类和多分类分类问题。与其他分割标准相比，它也相对计算效率高。然而，使用熵的一个缺点是它倾向于创建偏向于具有许多类别的属性的偏斜树。
- en: 'Here are some of the advantages of decision trees:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是决策树的一些优点：
- en: Easy to understand and interpret, even for non-experts
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易理解和解释，即使是对于非专家来说也是如此
- en: Can handle both categorical and numerical data
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理分类数据和数值数据
- en: Can handle missing data and outliers
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理缺失数据和异常值
- en: Can be used for feature selection
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用于特征选择
- en: Can be combined with other models in ensemble methods, such as random forests
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以与其他模型结合使用，例如随机森林
- en: 'Here are some of the disadvantages of decision trees:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是决策树的一些缺点：
- en: Can be prone to overfitting, especially if the tree is too deep or complex
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能容易过拟合，特别是如果树太深或太复杂
- en: Can be sensitive to small changes in the data or the way the tree is built
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能对数据或树构建方式中的微小变化敏感
- en: Can be biased toward features with many categories or high cardinality
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会偏向于具有许多类别或高基数的特征
- en: Can have problems with rare events or imbalanced datasets
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会与罕见事件或不平衡数据集有问题
- en: Random forest
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Random forest is an ensemble learning method that’s versatile and can perform
    classification and regression tasks. It operates by generating multiple decision
    trees during training, predicting the target class for classification based on
    the majority of the trees, and the predicted value based on the mean prediction
    by trees for regression tasks. The algorithm for constructing a random forest
    can be summarized in the following steps:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种灵活的集成学习方法，可以执行分类和回归任务。它通过在训练过程中生成多个决策树来操作，对于分类任务，基于多数决策树的投票结果预测目标类别，对于回归任务，基于树的平均预测值进行预测。随机森林构建算法可以总结如下步骤：
- en: '**Bootstrap sampling**: Randomly select a subset of the data with replacement
    to create a new dataset that’s the same size as the original dataset.'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自助采样**：随机选择与原数据集相同大小的数据子集，并允许重复选择以创建新的数据集。'
- en: '**Feature selection**: Randomly select a subset of the features (columns) for
    each split when building a decision tree. This helps to create diversity in the
    trees and reduce overfitting.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征选择**：在构建决策树时，随机选择每个分割的特征（列）子集。这有助于在树之间创建多样性并减少过拟合。'
- en: '**Tree building**: Construct a decision tree for each bootstrap sample and
    feature subset. The decision tree is constructed recursively by splitting the
    data based on the selected features until a stopping criterion is met (for example,
    maximum depth or minimum number of samples in a leaf node).'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**树构建**：为每个自助采样和特征子集构建一个决策树。决策树通过根据选定的特征递归地分割数据来构建，直到满足停止标准（例如，最大深度或叶节点中的最小样本数）。'
- en: '**Ensemble learning**: Combine the predictions of all decision trees to make
    a final prediction. For classification, the class that receives the most votes
    from the decision trees is the final prediction. For regression, the average of
    the predictions from all decision trees is the final prediction.'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**集成学习**：将所有决策树的预测结果结合起来进行最终预测。对于分类，决策树投票最多的类别是最终预测。对于回归，所有决策树的预测平均值是最终预测。'
- en: The random forest algorithm can be expressed mathematically as follows.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法可以用以下数学公式表示。
- en: Given a dataset, *D*, with *N* samples and *M* features, we create *T* decision
    trees {Tre e 1, Tre e 2, … , Tre e T} by applying the preceding steps. Each decision
    tree is constructed using a bootstrap sample of the data, *D’*, with size *N’
    (N’ <= N)* and a subset of the features, *F’*, with size *m (m <= M)*. For each
    split in the decision tree, we randomly select *k (k < m)* features from *F’*
    and choose the best feature to split the data based on an impurity measure (for
    example, Gini index or entropy). The decision tree is built until a stopping criterion
    is met (for example, the maximum depth or minimum number of samples in a leaf
    node).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 *N* 个样本和 *M* 个特征的样本集 *D*，我们通过应用前面的步骤创建 *T* 个决策树 {树1, 树2, … , 树T}。每个决策树都是使用数据集
    *D* 的自助样本 *D’*（大小为 *N’*（N’ <= N））和特征子集 *F’*（大小为 *m*（m <= M））构建的。对于决策树中的每个分割，我们从
    *F’* 中随机选择 *k*（k < m）个特征，并根据一个不纯度度量（例如，基尼指数或熵）选择最佳特征来分割数据。决策树会构建到满足停止标准（例如，最大深度或叶节点中的最小样本数）为止。
- en: The final prediction, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/187.png),
    for a new sample, *x*, is obtained by aggregating the predictions from all decision
    trees.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新样本 *x* 的最终预测，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/187.png)，是通过聚合所有决策树的预测得到的。
- en: 'For classification, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/188.png)
    is the class that receives the most votes from all decision trees:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/188.png)
    是从所有决策树中获得最多投票的类别：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><mi
    mathvariant="bold">y</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>j</mi></msub><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/189.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><mi
    mathvariant="bold">y</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>j</mi></msub><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/189.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/190.png)
    is the prediction of the *j-th* decision tree for the *i-th* sample, and *I()*
    is the indicator function that returns 1 if the condition is true and 0 otherwise.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/190.png)
    是第 *j* 个决策树对第 *i* 个样本的预测，而 *I()* 是指示函数，当条件为真时返回 1，否则返回 0。
- en: 'For regression, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/188.png)
    is the average of the predictions from all decision trees:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/188.png)
    是所有决策树预测的平均值：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mover><mi
    mathvariant="bold">y</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mo>(</mo><mn>1</mn><mo>/</mo><mi>T</mi><mo>)</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math>](img/192.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mover><mi
    mathvariant="bold">y</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mo>(</mo><mn>1</mn><mo>/</mo><mi>T</mi><mo>)</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math>](img/192.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/193.png)
    is the prediction of the *i-th* decision tree for the new sample, *x*.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/193.png)
    是对新样本 *x* 的第 *i* 个决策树的预测。
- en: In summary, random forest is a powerful machine learning algorithm that can
    handle high-dimensional and noisy datasets. It works by constructing multiple
    decision trees using bootstrap samples of the data and feature subsets, and then
    aggregating the predictions of all decision trees to make a final prediction.
    The algorithm is scalable, easy to use, and provides a measure of feature importance,
    making it a popular choice for many machine learning applications.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，随机森林是一种强大的机器学习算法，可以处理高维和噪声数据集。它通过使用数据的自助样本和特征子集构建多个决策树，然后汇总所有决策树的预测来做出最终预测。该算法可扩展、易于使用，并提供特征重要性的度量，使其成为许多机器学习应用的流行选择。
- en: 'The advantages of random forests are as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的优点如下：
- en: '**Robustness**: Random forest is a very robust algorithm that can handle a
    variety of input data types, including numerical, categorical, and ordinal data'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：随机森林是一个非常鲁棒的算法，可以处理各种输入数据类型，包括数值、分类和有序数据。'
- en: '**Feature selection**: Random forest can rank the importance of features, allowing
    users to identify the most important features for classification or regression
    tasks'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：随机森林可以对特征的重要性进行排序，使用户能够识别分类或回归任务中最重要的特征。'
- en: '**Overfitting**: Random forest has a built-in mechanism for reducing overfitting,
    called bagging, which helps to generalize well on new data'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：随机森林具有一个内置的减少过拟合的机制，称为袋装（bagging），这有助于在新数据上很好地泛化。'
- en: '**Scalability**: Random forest can handle large datasets with a high number
    of features, making it a good choice for big data applications'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：随机森林可以处理具有大量特征的大型数据集，使其成为大数据应用的不错选择。'
- en: '**Outliers**: Random forest is robust to the presence of outliers as it is
    based on decision trees, which can handle outliers effectively'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值**：随机森林对异常值的存在具有鲁棒性，因为它基于决策树，可以有效地处理异常值。'
- en: 'The disadvantages of random forests are as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的缺点如下：
- en: '**Interpretability**: Random forest models can be difficult to interpret as
    they are based on an ensemble of decision trees'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：由于随机森林模型基于决策树的集成，因此它们可能难以解释。'
- en: '**Training time**: The training time of a random forest can be longer than
    other simpler algorithms, especially when the number of trees in the ensemble
    is large'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：随机森林的训练时间可能比其他简单的算法长，尤其是在集成中树的数量很大时。'
- en: '**Memory usage**: Random forest requires more memory than some other algorithms
    as it has to store the decision trees in memory'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存使用**：由于随机森林需要存储决策树在内存中，因此它比其他一些算法需要更多的内存。'
- en: '**Bias**: Random forest can suffer from bias if the data is imbalanced or if
    the target variable has a high cardinality'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**：如果数据不平衡或目标变量具有高基数，随机森林可能会出现偏差。'
- en: '**Overfitting**: Although random forest is designed to prevent overfitting,
    it is still possible to overfit the model if the hyperparameters are not properly
    tuned'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：尽管随机森林旨在防止过拟合，但如果超参数没有适当调整，仍然可能导致模型过拟合。'
- en: Overall, random forest is a powerful machine learning algorithm that has many
    advantages, but it is important to carefully consider its limitations before applying
    it to a particular problem.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，随机森林是一种强大的机器学习算法，具有许多优点，但在将其应用于特定问题之前，仔细考虑其局限性是很重要的。
- en: Support vector machines (SVMs)
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机（SVMs）
- en: 'SVMs are considered robust supervised learning algorithms that can perform
    both classification and regression tasks. They excel in scenarios with intricate
    decision boundaries, surpassing the limitations of linear models. At their core,
    SVMs aim to identify a hyperplane within a multi-dimensional space that maximally
    segregates the classes. This hyperplane is positioned to maximize the distance
    between itself and the closest points from each class, known as support vectors.
    Here’s how SVMs work for a binary classification problem. Given a set of training
    data, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>)</mo><mo>,</mo><mo>(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>)</mo><mo>,</mo><mo>...</mo><mo>,</mo><mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo>)</mo><mo>}</mo></mrow></mrow></mrow></math>](img/194.png),
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/195.png)
    is a d-dimensional feature vector and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    is the binary class label (+1 or -1), the goal of an SVM is to find a hyperplane
    that separates the two classes with the largest margin. The margin is defined
    as the distance between the hyperplane and the closest data points from each class:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs 被认为是鲁棒的有监督学习算法，可以执行分类和回归任务。它们在具有复杂决策边界的场景中表现出色，超越了线性模型的限制。在核心上，SVMs 的目标是识别一个多维空间中的超平面，该超平面最大限度地分离了类别。这个超平面定位在最大化其与每个类中最近点的距离，这些点称为支持向量。以下是
    SVMs 在二元分类问题中的工作方式。给定一组训练数据，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>)</mo><mo>,</mo><mo>(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>)</mo><mo>,</mo><mo>...</mo><mo>,</mo><mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo>)</mo><mo>}</mo></mrow></mrow></mrow></math>](img/194.png)，其中
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/195.png)
    是一个 d 维特征向量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    是二进制类别标签（+1 或 -1），SVM 的目标是找到一个超平面，该超平面以最大的间隔分离两个类别。间隔定义为超平面与每个类中最近数据点之间的距离：
- en: '![Figure 3.2 – SVM margins](img/B18949_03_2.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – SVM 边界](img/B18949_03_2.jpg)'
- en: Figure 3.2 – SVM margins
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – SVM 边界
- en: 'The hyperplane is defined by a weight vector, *w*, and a bias term, *b*, such
    that for any new data point, *x*, the predicted class label, *y*, is given by
    the following equation:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面由权重向量 *w* 和偏置项 *b* 定义，对于任何新的数据点 *x*，预测的类别标签 *y* 由以下方程给出：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/197.png)+b)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/197.png)+b)'
- en: Here, *sign* is the sign function, which returns +1 if the argument is positive
    and -1 otherwise.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*符号函数*是返回+1如果参数为正，否则返回-1的函数。
- en: 'The objective function of an SVM is to minimize the classification error subject
    to the constraint that the margin is maximized. This can be formulated as an optimization
    problem:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 的目标函数是在最大化间隔的约束下最小化分类误差。这可以表述为一个优化问题：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>z</mi><mi>e</mi><mn>1</mn><mo>/</mo><mn>2</mn><msup><mrow><mo>|</mo><mo>|</mo><mi
    mathvariant="bold">w</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></mrow></math>](img/198.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>z</mi><mi>e</mi><mn>1</mn><mo>/</mo><mn>2</mn><msup><mrow><mo>|</mo><mo>|</mo><mi
    mathvariant="bold">w</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></mrow></math>](img/198.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>s</mi><mi>u</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>t</mi><mi>o</mi><msub><mi>y</mi><mi>i</mi></msub><mo>(</mo><msup><mi
    mathvariant="bold">w</mi><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo>)</mo><mo>≥</mo><mn>1</mn><mi>f</mi><mi>o</mi><mi>r</mi><mi>i</mi><mo>=</mo><mn>1,2</mn><mo>,</mo><mo>...</mo><mo>,</mo><mi>n</mi></mrow></mrow></mrow></math>](img/199.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>s</mi><mi>u</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>t</mi><mi>o</mi><msub><mi>y</mi><mi>i</mi></msub><mo>(</mo><msup><mi
    mathvariant="bold">w</mi><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo>)</mo><mo>≥</mo><mn>1</mn><mi>f</mi><mi>o</mi><mi>r</mi><mi>i</mi><mo>=</mo><mn>1,2</mn><mo>,</mo><mo>...</mo><mo>,</mo><mi>n</mi></mrow></mrow></mrow></math>](img/199.png)'
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mrow><mo>|</mo><mo>|</mo><mi
    mathvariant="bold">w</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></math>](img/200.png)
    is the squared Euclidean norm of the weight vector, *w*. The constraints ensure
    that all data points are correctly classified and that the margin is maximized.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mrow><mo>|</mo><mo>|</mo><mi
    mathvariant="bold">w</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></math>](img/200.png)
    是权重向量 *w* 的欧几里得范数的平方。约束条件确保所有数据点都被正确分类，并且边缘被最大化。
- en: 'Here are some of the advantages of SVMs:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 SVM 的一些优点：
- en: Effective in high-dimensional spaces, which is useful when the number of features
    is large
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维空间中有效，这在特征数量大时很有用
- en: Can be used for both classification and regression tasks
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于分类和回归任务
- en: Works well with both linearly separable and non-linearly separable data
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与线性可分和非线性可分数据都兼容
- en: Can handle outliers well due to the use of the margin concept
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用了边缘概念，可以很好地处理异常值
- en: Has a regularization parameter that allows you to control overfitting
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个正则化参数，允许你控制过拟合
- en: 'Here are some of the disadvantages of SVMs:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 SVM 的一些缺点：
- en: Can be sensitive to the choice of kernel function, which can greatly affect
    the performance of the model
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对核函数的选择可能很敏感，这可能会极大地影响模型的表现
- en: Computationally intensive for large datasets
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集计算密集
- en: It can be difficult to interpret the results of an SVM model
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 SVM 模型的结果可能很困难
- en: Requires careful tuning of parameters to achieve good performance
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要仔细调整参数以实现良好的性能
- en: Neural networks and transformers
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络和转换器
- en: Neural networks and transformers are both powerful machine learning models that
    are used for a variety of tasks, such as image classification, NLP, and speech
    recognition.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和转换器都是强大的机器学习模型，用于各种任务，如图像分类、自然语言处理和语音识别。
- en: Neural networks
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Neural networks draw inspiration from the structure and functioning of the
    human brain. They represent a category of machine learning models that are proficient
    in various tasks such as classification, regression, and more. Comprising multiple
    layers of interconnected nodes known as neurons, these networks adeptly process
    and manipulate data. The output of each layer is fed into the next layer, creating
    a hierarchy of feature representations. The input to the first layer is the raw
    data, and the output of the final layer is the prediction. A simple neural network
    for detecting the gender of a person based on their height and weight is shown
    in *Figure 3**.3*:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络从人脑的结构和功能中汲取灵感。它们代表一类机器学习模型，擅长各种任务，如分类、回归等。这些网络由称为神经元的相互连接的多层节点组成，能够熟练地处理和操作数据。每一层的输出被输入到下一层，创建了一个特征表示的层次结构。第一层的输入是原始数据，最后一层的输出是预测。一个简单的神经网络，根据一个人的身高和体重检测其性别，如图
    *3**.3* 所示：
- en: '![Figure 3.3 – Simple neural network](img/B18949_03_3.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 简单神经网络](img/B18949_03_3.jpg)'
- en: Figure 3.3 – Simple neural network
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 简单神经网络
- en: 'The operation of a single neuron in a neural network can be represented by
    the following equation:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中单个神经元的操作可以用以下方程表示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow></mrow></mfenced></mrow></mrow></math>](img/201.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow></mrow></mfenced></mrow></mrow></math>](img/201.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/202.png)
    is the input values, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/203.png)
    is the weights of the connections between the neurons, *b* is the bias term, and
    *f* is the activation function. The activation function applies a non-linear transformation
    to the weighted sum of the inputs and bias term.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/202.png)
    是输入值，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/203.png)
    是神经元之间的连接权重，*b* 是偏差项，*f* 是激活函数。激活函数将非线性变换应用于加权输入和偏差项的总和。
- en: Training a neural network involves adjusting the weights and biases of the neurons
    to minimize a loss function. This is typically done using an optimization algorithm
    such as stochastic gradient descent.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及调整神经元的权重和偏差以最小化损失函数。这通常是通过使用随机梯度下降等优化算法来完成的。
- en: The advantages of neural networks include their ability to learn complex non-linear
    relationships between input and output data, their ability to automatically extract
    meaningful features from raw data, and their scalability to large datasets.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的优势包括它们能够学习输入和输出数据之间的复杂非线性关系，它们能够从原始数据中自动提取有意义的特征，以及它们能够扩展到大型数据集。
- en: The disadvantages of neural networks include their high computational and memory
    requirements, their sensitivity to hyperparameter tuning, and the difficulty of
    interpreting their internal representations.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的缺点包括它们的高计算和内存需求，对超参数调整的敏感性，以及解释其内部表示的困难。
- en: Transformers
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变压器
- en: Transformers are a type of neural network architecture that is particularly
    well suited to sequential data such as text or speech. They were introduced in
    the context of NLP and have since been applied to a wide range of tasks.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是一种特别适合于序列数据（如文本或语音）的神经网络架构。它们在自然语言处理（NLP）的背景下被引入，并且自那时起已应用于广泛的任务中。
- en: The core component of a transformer is the self-attention mechanism, which allows
    the model to attend to different parts of the input sequence when computing the
    output. The self-attention mechanism is based on a dot product between a query
    vector, a set of key vectors, and a set of value vectors. The resulting attention
    weights are used to weight the values, which are then combined to produce the
    output.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的核心组件是自注意力机制，它允许模型在计算输出时关注输入序列的不同部分。自注意力机制基于查询向量、一组键向量和一组值向量之间的点积。产生的注意力权重用于加权值，然后将这些值组合起来生成输出。
- en: 'The self-attention operation can be represented by the following equations:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力操作可以用以下方程表示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></mrow></mrow></math>](img/204.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></mrow></mrow></math>](img/204.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></mrow></mrow></math>](img/205.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></mrow></mrow></math>](img/205.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></mrow></mrow></math>](img/206.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></mrow></mrow></math>](img/206.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>A</mi><mo>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    open="(" close=")"><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>K</mi></msub></msqrt></mfrac></mfenced><mi>V</mi></mrow></mrow></mrow></math>](img/207.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>A</mi><mo>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    open="(" close=")"><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>K</mi></msub></msqrt></mfrac></mfenced><mi>V</mi></mrow></mrow></mrow></math>](img/207.png)'
- en: Here, *X* is the input sequence, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math>](img/208.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math>](img/209.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math>](img/210.png)
    are learned projection matrices for the query, key, and value vectors, respectively,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math>](img/211.png)
    is the dimensionality of the key vectors, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math>](img/212.png)
    is a learned projection matrix that maps the output of the attention mechanism
    to the final output.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X* 是输入序列，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math>](img/208.png)，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math>](img/209.png)，和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math>](img/210.png)分别是查询、键和值向量的学习投影矩阵，![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math>](img/211.png)是键向量的维度，而![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math>](img/212.png)是一个将注意力机制的输出映射到最终输出的学习投影矩阵。
- en: The advantages of transformers include their ability to handle variable-length
    input sequences, their ability to capture long-range dependencies in the data,
    and their state-of-the-art performance on many NLP tasks.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的优点包括其处理可变长度输入序列的能力、捕捉数据中长距离依赖关系的能力以及在许多NLP任务上的最先进性能。
- en: The disadvantages of transformers include their high computational and memory
    requirements, their sensitivity to hyperparameter tuning, and their difficulty
    in handling tasks that require explicit modeling of sequential dynamics.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的缺点包括其高计算和内存需求、对超参数调整的敏感性以及处理需要显式建模序列动态的任务的困难。
- en: These are just a few of the most popular machine learning models. The choice
    of model depends on the problem at hand, the size and quality of the data, and
    the desired outcome. Now that we have explored the most common machine learning
    models, we will explain model underfitting and overfitting, which happens during
    the training process.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是最受欢迎的机器学习模型中的几个。模型的选择取决于手头的问题、数据的大小和质量以及期望的结果。现在我们已经探讨了最常见的机器学习模型，我们将解释训练过程中发生的模型欠拟合和过拟合。
- en: Model underfitting and overfitting
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型欠拟合和过拟合
- en: In machine learning, the ultimate goal is to build a model that can generalize
    well on unseen data. However, sometimes, a model can fail to achieve this goal
    due to either underfitting or overfitting.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，最终目标是构建一个能够在未见数据上良好泛化的模型。然而，有时模型可能由于欠拟合或过拟合而无法实现这一目标。
- en: 'Underfitting occurs when a model is too simple to capture the underlying patterns
    in the data. In other words, the model can’t learn the relationship between the
    features and the target variable properly. This can result in poor performance
    on both the training and testing data. For example, in *Figure 3**.4*, we can
    see that the model is underfitted, and it cannot present the data very well. This
    is not what we like in machine learning models, and we usually like to see a precise
    model, as shown in *Figure 3**.5*:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型过于简单，无法正确捕捉数据中的潜在模式时，就会发生欠拟合。换句话说，模型无法正确学习特征与目标变量之间的关系。这可能导致训练和测试数据上的性能较差。例如，在
    *图 3.4* 中，我们可以看到模型欠拟合，它无法很好地呈现数据。这不是我们希望在机器学习模型中看到的，我们通常希望看到一个精确的模型，如图 *图 3.5*
    所示：
- en: '![Figure 3.4 – The machine learning model underfitting on the training data](img/B18949_03_4.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 机器学习模型在训练数据上的欠拟合](img/B18949_03_4.jpg)'
- en: Figure 3.4 – The machine learning model underfitting on the training data
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 机器学习模型在训练数据上的欠拟合
- en: 'Underfitting happens when the model is not trained well, or the model complexity
    is not enough to catch the underlying pattern in the data. To solve this problem,
    we can use more complex models, and continue the training process:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型训练不当或模型复杂度不足以捕捉数据中的潜在模式时，就会发生欠拟合。为了解决这个问题，我们可以使用更复杂的模型，并继续训练过程：
- en: '![Figure 3.5 – Optimal fitting of the machine learning model on the training
    data](img/B18949_03_5.jpg)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 机器学习模型在训练数据上的最佳拟合](img/B18949_03_5.jpg)'
- en: Figure 3.5 – Optimal fitting of the machine learning model on the training data
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 机器学习模型在训练数据上的最佳拟合
- en: 'Optimal fitting happens when the model captures the pattern in the data pretty
    well but does not overfit every single sample. This helps the model work better
    on unseen data:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型很好地捕捉到数据中的模式但不过拟合每个样本时，就会发生最佳拟合。这有助于模型在未见数据上更好地工作：
- en: '![Figure 3.6 – Overfitting the model on the training data](img/B18949_03_6.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 在训练数据上过拟合模型](img/B18949_03_6.jpg)'
- en: Figure 3.6 – Overfitting the model on the training data
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 在训练数据上过拟合模型
- en: On the other hand, overfitting occurs when a model is too complex and fits the
    training data too closely, which can lead to poor generalization on new, unseen
    data, as shown in *Figure 3**.6*. This happens when the model learns the noise
    or random fluctuations in the training data, rather than the underlying patterns.
    In other words, the model becomes too specialized for the training data and does
    not perform well on the testing data. As shown in the preceding figure, the model
    is overfitted, and the model tried to predict every single sample very precisely.
    The problem with this model is that it does not learn the general pattern, and
    learns the pattern of each individual sample, which makes it work poorly when
    facing new, unseen records.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当模型过于复杂且与训练数据拟合得太紧密时，就会发生过拟合，这可能导致在新、未见数据上的泛化能力较差，如图 *图 3.6* 所示。这发生在模型学习训练数据中的噪声或随机波动，而不是潜在的模式。换句话说，模型对训练数据过于专业化，在测试数据上表现不佳。如图所示，模型过拟合，试图非常精确地预测每个样本。这个模型的问题在于它没有学习到一般模式，而是学习了每个单个样本的模式，这使得它在面对新的、未见记录时表现不佳。
- en: 'A useful way to understand the trade-off between underfitting and overfitting
    is through the bias-variance trade-off. Bias refers to the difference between
    the predicted values of the model and the actual values in the training data.
    A high bias means that the model is not complex enough to capture the underlying
    patterns in the data and underfits the data (*Figure 3**.7*). An underfit model
    has poor performance on both the training and testing data:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 理解欠拟合和过拟合之间的权衡的一个有用方法是偏差-方差权衡。偏差是指模型预测值与训练数据中实际值之间的差异。高偏差意味着模型复杂度不足以捕捉数据中的潜在模式，并且欠拟合数据（*图
    3.7*）。欠拟合模型在训练和测试数据上的性能都较差：
- en: '![Figure 3.7 – High bias](img/B18949_03_7.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 高偏差](img/B18949_03_7.jpg)'
- en: Figure 3.7 – High bias
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 高偏差
- en: 'Variance, on the other hand, refers to the sensitivity of the model to small
    fluctuations in the training data. A high variance means that the model is overly
    complex and overfits the data, which leads to poor generalization performance
    on new data. An overfit model has good performance on the training data but poor
    performance on the testing data:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，方差是指模型对训练数据中微小波动的敏感度。高方差意味着模型过于复杂，过度拟合了数据，这导致在新数据上的泛化性能较差。一个过拟合的模型在训练数据上表现良好，但在测试数据上表现较差：
- en: '![Figure 3.8 – Just right (not high bias, not high variance)](img/B18949_03_8.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 正好合适（既不高偏差，也不高方差）](img/B18949_03_8.jpg)'
- en: Figure 3.8 – Just right (not high bias, not high variance)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 正好合适（既不高偏差，也不高方差）
- en: 'To strike a balance between bias and variance, we need to choose a model that
    is neither too simple nor too complex. As mentioned previously, this is often
    referred to as the bias-variance trade-off (*Figure 3**.8*). A model with a high
    bias and low variance can be improved by increasing the complexity of the model,
    while a model with a high variance and low bias can be improved by decreasing
    the complexity of the model:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在偏差和方差之间取得平衡，我们需要选择一个既不太简单也不太复杂的模型。如前所述，这通常被称为偏差-方差权衡（*图3**.8*）。具有高偏差和低方差的模型可以通过增加模型的复杂性来改进，而具有高方差和低偏差的模型可以通过减少模型的复杂性来改进：
- en: '![Figure 3.9 – High variance](img/B18949_03_9.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 高方差](img/B18949_03_9.jpg)'
- en: Figure 3.9 – High variance
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 高方差
- en: There are several methods to reduce bias and variance in a model. One common
    approach is regularization, which adds a penalty term to the loss function to
    control the complexity of the model. Another approach is to use ensembles, which
    combine multiple models to improve the overall performance by reducing the variance.
    Cross-validation can also be used to evaluate the model’s performance and tune
    its hyperparameters to find the optimal balance between bias and variance.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型中的偏差和方差有几种方法。一种常见的方法是正则化，它通过向损失函数添加惩罚项来控制模型的复杂性。另一种方法是使用集成，它通过结合多个模型来减少方差，从而提高整体性能。交叉验证也可以用来评估模型的性能，并调整其超参数以找到偏差和方差之间的最佳平衡。
- en: Overall, understanding bias and variance is crucial in machine learning as it
    helps us to choose an appropriate model and identify the sources of error in the
    model.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，在机器学习中理解偏差和方差至关重要，因为它帮助我们选择合适的模型并识别模型中的错误来源。
- en: Bias refers to the error that is introduced by approximating a real-world problem
    with a simplified model. Variance, on the other hand, refers to the error that
    is introduced by the model’s sensitivity to small fluctuations in the training
    data.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是指通过用一个简化的模型近似现实世界问题而引入的错误。另一方面，方差是指模型对训练数据中微小波动敏感度所引入的错误。
- en: When a model has high bias and low variance, it is underfitting. This means
    that the model is not capturing the complexity of the problem and is making overly
    simplistic assumptions. When a model has low bias and high variance, it is overfitting.
    This means that the model is too sensitive to the training data and is fitting
    the noise instead of the underlying patterns.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型具有高偏差和低方差时，它是欠拟合的。这意味着模型没有捕捉到问题的复杂性，并做出了过于简化的假设。当一个模型具有低偏差和高方差时，它是过拟合的。这意味着模型对训练数据过于敏感，并拟合了噪声而不是潜在的模式。
- en: 'To overcome underfitting, we can try increasing the complexity of the model,
    adding more features, or using a more sophisticated algorithm. To prevent overfitting,
    several methods can be used:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服欠拟合，我们可以尝试增加模型的复杂性，添加更多特征，或使用更复杂的算法。为了防止过拟合，可以使用以下几种方法：
- en: '**Cross-validation**: Assessing the performance of machine learning models
    is essential. Cross-validation serves as a method for assessing the effectiveness
    of a machine learning model. It entails training the model on one portion of the
    data and testing it on another. By employing distinct subsets for training and
    evaluation, cross-validation mitigates the risk of overfitting. Further elaboration
    on this technique will be provided in the subsequent section on data splitting.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：评估机器学习模型的性能是至关重要的。交叉验证作为一种评估机器学习模型有效性的方法，它包括在数据的一部分上训练模型，并在另一部分上测试它。通过使用不同的子集进行训练和评估，交叉验证可以降低过拟合的风险。关于这种技术的进一步阐述将在数据拆分的下一节中提供。'
- en: '**Regularization**: Regularization is a technique that’s used to add a penalty
    term to the loss function during training, which helps to reduce the complexity
    of the model and prevent overfitting. There are different types of regularization,
    including L1 regularization (LASSO), L2 regularization (ridge), and elastic net
    regularization.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：正则化是一种在训练过程中向损失函数添加惩罚项的技术，有助于降低模型的复杂性并防止过拟合。正则化有多种类型，包括 L1 正则化（LASSO）、L2
    正则化（岭回归）和弹性网络正则化。'
- en: '**Early stopping**: Early stopping is a technique that’s used to stop the training
    process when the performance of the model on the validation data starts to degrade.
    This helps to prevent overfitting by stopping the model from continuing to learn
    from the training data when it has already reached its maximum performance. This
    technique is usually used in iterative algorithms such as deep learning methods,
    where the model is being trained for multiple iterations (epochs). To use early
    stopping, we usually train the model while evaluating the model performance on
    the training and validation subsets. The model’s performance usually improves
    on the training set with more training, but since the model has not seen the validation
    set, the validation error usually decreases initially and at some point, starts
    increasing again. This point is where the model starts overfitting. By visualizing
    the training and validation error of the model during training, we can identify
    and stop the model at this point (*Figure 3**.10*):'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提前停止**：提前停止是一种在验证数据的模型性能开始下降时停止训练过程的技术。这有助于通过在模型已经达到最大性能时停止其从训练数据中继续学习来防止过拟合。这种技术通常用于迭代算法，如深度学习方法，其中模型被训练多个迭代（周期）。要使用提前停止，我们通常在评估模型在训练和验证子集上的性能的同时训练模型。随着训练的增加，模型的性能通常在训练集上提高，但由于模型尚未看到验证集，验证错误通常最初会下降，并在某个时刻开始再次增加。这一点是模型开始过拟合的地方。通过在训练过程中可视化模型的训练和验证错误，我们可以识别并在此点停止模型（*图
    3**.10*）：'
- en: '![Figure 3.10 – Early stopping](img/B18949_03_10.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 提前停止](img/B18949_03_10.jpg)'
- en: Figure 3.10 – Early stopping
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 提前停止
- en: '**Dropout**: Dropout is a technique in deep learning models that is used to
    randomly drop out some neurons during training, which helps to prevent the model
    from relying too heavily on a small subset of features or neurons and overfitting
    the training data. By dropping the weight of neurons in the model during the process,
    we make the model learn the general pattern and prevent it from memorizing the
    training data (overfitting).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：Dropout 是一种在深度学习模型中使用的技巧，用于在训练过程中随机丢弃一些神经元，这有助于防止模型过度依赖一小部分特征或神经元，并避免对训练数据过拟合。在训练过程中通过降低神经元的权重，我们使模型学习一般模式，并防止它记住训练数据（过拟合）。'
- en: '**Data augmentation**: Data augmentation is a method that we can use to artificially
    expand the training data size by applying transformations, such as rotation, scaling,
    and flipping, to the existing dataset, which helps us to extend our training data.
    This strategy aids in mitigating overfitting by offering the model a more diverse
    set of examples to learn from.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：数据增强是一种方法，我们可以通过应用变换，如旋转、缩放和翻转，来人工扩大现有数据集的大小，从而扩展我们的训练数据。这种策略通过为模型提供更多样化的例子来学习，有助于减轻过拟合。'
- en: '**Ensemble methods**: Ensemble methods are techniques that are used to combine
    multiple models to improve their performance and prevent overfitting. This can
    be done by using techniques such as bagging, boosting, or stacking.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：集成方法是用来结合多个模型以改进其性能和防止过拟合的技术。这可以通过使用诸如袋装、提升或堆叠等技术来实现。'
- en: By using these techniques, it is possible to prevent overfitting and build models
    that generalize well to new, unseen data. In practice, it is important to monitor
    both the training and testing performance of the model and make adjustments accordingly
    to achieve the best possible generalization performance. We will explain how to
    split our data into training and testing in the next section.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些技术，可以防止过拟合并构建对新、未见数据具有良好泛化能力的模型。在实践中，监控模型的训练和测试性能并相应地调整，以实现最佳可能的泛化性能非常重要。我们将在下一节中解释如何将数据分为训练集和测试集。
- en: Splitting data
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分割
- en: When developing a machine learning model, it’s important to split the data into
    training, validation, and test sets; this is called data splitting. This is done
    to evaluate the performance of the model on new, unseen data and to prevent overfitting.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发机器学习模型时，将数据分为训练集、验证集和测试集非常重要；这被称为数据分割。这样做是为了评估模型在新、未见过的数据上的性能，并防止过拟合。
- en: 'The most common method for splitting the data is the train-test split, which
    splits the data into two sets: the training set, which is used to train the model,
    and the test set, which is used to evaluate the performance of the model. The
    data is randomly divided into two sets, with a typical split being 80% of the
    data for training and 20% for testing. Using this approach the model will be trained
    using the majority of the data (training data) and then tested on the remaining
    data (test set). Using this approach, we can ensure that the model’s performance
    is based on new, unseen data.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 分割数据最常见的方法是训练-测试分割，它将数据分为两个集合：训练集，用于训练模型；测试集，用于评估模型的性能。数据被随机分为两个集合，典型的分割是80%的数据用于训练，20%用于测试。使用这种方法，模型将使用大部分数据（训练数据）进行训练，然后在对剩余数据（测试集）进行测试。使用这种方法，我们可以确保模型的性能是基于新的、未见过的数据。
- en: Most of the time in machine learning model development, we have a set of hyperparameters
    for our model that we like to tune (we will explain hyperparameter tuning in the
    next subsection). In this case, we like to make sure that the performance that
    we get on the test set is reliable and not just by chance based on a set of hyperparameters.
    In this case, based on the size of our training data, we can divide the data into
    60%, 20%, and 20% (or 70%, 15%, and 15%) for training, validation, and testing.
    In this case, we train the model on the training data and select the set of hyperparameters
    that give us the best performance on the validation set. We then report the actual
    model performance on the test set, which has not been seen or used before during
    model training or hyperparameter selection.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型开发的大部分时间里，我们有一组我们希望调整的模型超参数（我们将在下一小节解释超参数调整）。在这种情况下，我们希望确保我们在测试集上获得的是可靠的性能，而不是仅仅基于一组超参数的偶然性。在这种情况下，根据我们训练数据的大小，我们可以将数据分为60%、20%和20%（或70%、15%和15%）用于训练、验证和测试。在这种情况下，我们在训练数据上训练模型，并选择在验证集上给出最佳性能的超参数集。然后，我们报告测试集上的实际模型性能，该测试集在模型训练或超参数选择之前从未见过或使用过。
- en: A more advanced method for splitting the data, especially when the size of our
    training data is limited, is k-fold cross-validation. In this method, the data
    is split into *k* equally sized “folds,” and the model is trained and tested *k*
    times, with each fold being used as the test set once and the remaining folds
    used as the training set. The results of each fold are then averaged to get an
    overall measure of the model’s performance. K-fold cross-validation is useful
    for small datasets where the train-test split may result in a large variance in
    performance evaluation. In this case, we report the average, minimum, and maximum
    performance of the model on each of the *k* folds, as shown in *Figure 3**.11*.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的训练数据量有限时，分割数据的一种更高级的方法是k折交叉验证。在这个方法中，数据被分为*k*个大小相等的“折”，模型被训练和测试*k*次，每个折被用作测试集一次，其余的折用作训练集。然后，将每个折的结果平均，以获得模型性能的整体度量。K折交叉验证对于小数据集很有用，因为训练-测试分割可能会导致性能评估的大幅波动。在这种情况下，我们报告模型在每个*k*个折上的平均、最小和最大性能，如图*3.11*所示。
- en: '![Figure 3.11 – K-fold cross-validation](img/B18949_03_11.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – K折交叉验证](img/B18949_03_11.jpg)'
- en: Figure 3.11 – K-fold cross-validation
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – K折交叉验证
- en: Another variant of k-fold cross-validation is stratified k-fold cross-validation,
    which ensures that the distribution of the target variable is consistent across
    all folds. This is useful when dealing with imbalanced datasets, where the number
    of instances of one class is much smaller than the others.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: k折交叉验证的另一种变体是分层k折交叉验证，它确保目标变量的分布在整个折中是一致的。这在处理不平衡数据集时很有用，其中一个类的实例数量远小于其他类。
- en: 'Time series data requires special attention when splitting. In this case, we
    typically use a method called time series cross-validation, which preserves the
    temporal order of the data. In this method, the data is split into multiple segments,
    with each segment representing a fixed time interval. The model is then trained
    on the past data and tested on the future data. This helps to evaluate the performance
    of the model in real-world scenarios. You can see an example of how to split the
    data in a time series problem in *Figure 3**.12*:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据在分割时需要特别注意。在这种情况下，我们通常使用一种称为时间序列交叉验证的方法，该方法保留了数据的时序顺序。在这个方法中，数据被分割成多个段，每个段代表一个固定的时间间隔。然后，模型在历史数据上训练并在未来数据上测试。这有助于评估模型在实际场景中的性能。您可以在*图3.12*中看到一个如何分割时间序列数据示例：
- en: '![Figure 3.12 – Time series data splitting](img/B18949_03_12.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12 – 时间序列数据分割](img/B18949_03_12.jpg)'
- en: Figure 3.12 – Time series data splitting
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 时间序列数据分割
- en: In all cases, it’s important to ensure that the split is done randomly but with
    the same random seed each time to ensure the reproducibility of the results. It’s
    also important to ensure that the split is representative of the underlying data
    – that is, the distribution of the target variable should be consistent across
    all sets. Once we have split the data into different subsets for training and
    testing our model, we can try to find the best set of hyperparameters for our
    model. This process is called hyperparameter tuning and will be explained next.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，确保随机分割但每次使用相同的随机种子，以确保结果的再现性是很重要的。同样重要的是要确保分割代表底层数据 – 即，目标变量的分布应在所有集合中保持一致。一旦我们将数据分割成不同的子集用于训练和测试我们的模型，我们就可以尝试找到我们模型的最佳超参数集。这个过程称为超参数调整，将在下文中解释。
- en: Hyperparameter tuning
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Hyperparameter tuning is an important step in the machine learning process that
    involves selecting the best set of hyperparameters for a given model. Hyperparameters
    are values that are set before the training process begins and can have a significant
    impact on the model’s performance. Examples of hyperparameters include learning
    rate, regularization strength, number of hidden layers in a neural network, and
    many others.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是机器学习过程中的一个重要步骤，涉及为给定模型选择最佳的超参数集。超参数是在训练过程开始之前设置的值，可以显著影响模型的表现。超参数的例子包括学习率、正则化强度、神经网络中的隐藏层数量等。
- en: The process of hyperparameter tuning involves selecting the best combination
    of hyperparameters that results in the optimal performance of the model. This
    is typically done by searching through a predefined set of hyperparameters and
    evaluating their performance on a validation set.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整的过程涉及选择最佳的超参数组合，以实现模型的最佳性能。这通常是通过在一个预定义的超参数集中搜索并评估它们在验证集上的性能来完成的。
- en: There are several methods for hyperparameter tuning, including grid search,
    random search, and Bayesian optimization. Grid search involves creating a grid
    of all possible hyperparameter combinations and evaluating each one on a validation
    set to determine the optimal set of hyperparameters. Random search, on the other
    hand, randomly samples hyperparameters from a predefined distribution and evaluates
    their performance on a validation set.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整有几种方法，包括网格搜索、随机搜索和贝叶斯优化。网格搜索涉及创建一个所有可能的超参数组合的网格，并在验证集上评估每个组合以确定最佳的超参数集。另一方面，随机搜索则是从预定义的分布中随机采样超参数，并在验证集上评估它们的性能。
- en: '**Random search** and **grid search** are methods that are used to search the
    search space, entirely or randomly, without considering previous hyperparameter
    results. Thus, these methods are inefficient. An alternative Bayesian optimization
    method has been proposed that iteratively computes the posterior distribution
    of the function and considers past evaluations to find the best hyperparameters.
    Using this approach, we can find the best set of hyperparameters with less iterations.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机搜索**和**网格搜索**是用于在整个搜索空间或随机搜索的方法，不考虑先前超参数的结果。因此，这些方法效率低下。已经提出了一种替代的贝叶斯优化方法，该方法迭代地计算函数的后验分布，并考虑过去的评估以找到最佳的超参数。使用这种方法，我们可以通过更少的迭代找到最佳的超参数集。'
- en: 'Bayesian optimization utilizes past evaluations to probabilistically map hyperparameters
    to objective function scores, as demonstrated in the following equation:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化利用过去的评估结果，以概率地将超参数映射到目标函数得分，如下面的方程所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>|</mo><mi>h</mi><mi>y</mi><mi>p</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo>)</mo></mrow></mrow></mrow></math>](img/213.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>|</mo><mi>h</mi><mi>y</mi><mi>p</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo>)</mo></mrow></mrow></mrow></math>](img/213.png)'
- en: 'Here are the steps Bayesian optimization undertakes:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是贝叶斯优化采取的步骤：
- en: It develops a surrogate probabilistic model for the objective function.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它为目标函数开发了一个代理概率模型。
- en: It identifies the optimal hyperparameters based on the surrogate.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它根据代理确定最优超参数。
- en: It utilizes these hyperparameters in the actual objective function.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将这些超参数用于实际的目标函数。
- en: It updates the surrogate model to integrate the latest results.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它更新代理模型以整合最新的结果。
- en: It reiterates *steps 2* to *4* until it reaches the maximum iteration count
    or time limit.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它重复*步骤2*到*步骤4*，直到达到最大迭代次数或时间限制。
- en: '**Sequential model-based optimization** (**SMBO**) methods are a formalization
    of Bayesian optimization, with trials run one after another, trying better hyperparameters
    each time and updating a probability model (surrogate). SMBO methods differ in
    *steps 3* and *4* – specifically, how they build a surrogate of the objective
    function and the criteria used to select the next hyperparameters. These variants
    include Gaussian processes, random forest regressions, and tree-structured Parzen
    estimators, among others.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于序列模型的优化**（**SMBO**）方法是贝叶斯优化的形式化，试验一个接一个地运行，每次尝试更好的超参数并更新概率模型（代理）。SMBO方法在*步骤3*和*步骤4*上有所不同——具体来说，它们如何构建目标函数的代理以及选择下一个超参数的准则。这些变体包括高斯过程、随机森林回归和树结构帕尔森估计器等。'
- en: In low-dimensional problems with numerical hyperparameters, Bayesian optimization
    is considered the best available hyperparameter optimization method. However,
    it is restricted to problems of moderate dimension.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有数值超参数的低维问题中，贝叶斯优化被认为是可用的最佳超参数优化方法。然而，它仅限于中等维度的问题。
- en: In addition to these methods, there are also several libraries available that
    automate the process of hyperparameter tuning. Examples of these libraries include
    scikit-learn’s `GridSearchCV` and `RandomizedSearchCV`, `Keras Tuner`, and `Optuna`.
    These libraries allow for efficient hyperparameter tuning and can significantly
    improve the performance of machine learning models.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些方法之外，还有一些库可以自动化超参数调整的过程。这些库的例子包括scikit-learn的`GridSearchCV`和`RandomizedSearchCV`，`Keras
    Tuner`和`Optuna`。这些库允许进行高效的超参数调整，并且可以显著提高机器学习模型的性能。
- en: 'Hyperparameter optimization in machine learning can be a complex and time-consuming
    process. Two primary complexity challenges arise in the search process: the trial
    execution time and the complexity of the search space, including the number of
    evaluated hyperparameter combinations. In deep learning, these challenges are
    especially pertinent due to the extensive search space and the utilization of
    large training sets.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的超参数优化可能是一个复杂且耗时的过程。在搜索过程中出现了两个主要的复杂性挑战：试验执行时间和搜索空间的复杂性，包括评估的超参数组合的数量。在深度学习中，这些挑战尤其相关，因为搜索空间广泛，并且使用了大量的训练集。
- en: To address these issues and reduce the search space, some standard techniques
    may be used. For example, reducing the size of the training dataset based on statistical
    sampling or applying feature selection techniques can help reduce the execution
    time of each trial. Additionally, identifying the most important hyperparameters
    for optimization and using additional objective functions beyond just accuracy,
    such as the number of operations or optimization time, can help reduce the complexity
    of the search space.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题并减少搜索空间，可以使用一些标准技术。例如，根据统计抽样减少训练数据集的大小或应用特征选择技术可以帮助减少每个试验的执行时间。此外，确定优化中最重要的超参数以及使用除准确度之外的其他目标函数，例如操作数量或优化时间，可以帮助减少搜索空间的复杂性。
- en: By combining accuracy with visualization through a deconvolution network, researchers
    have achieved superior results. However, it’s important to note that these techniques
    are not exhaustive, and the best approach may depend on the specific problem at
    hand.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合通过反卷积网络进行的可视化，研究人员已经取得了优异的结果。然而，需要注意的是，这些技术并不全面，最佳方法可能取决于具体的问题。
- en: Another common approach for improving model performance is to use multiple models
    in parallel; these are called ensemble models. They are very useful in dealing
    with machine learning problems.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 提高模型性能的另一种常见方法是使用多个并行模型；这些被称为集成模型。它们在处理机器学习问题时非常有用。
- en: Ensemble models
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模型
- en: Ensemble modeling is a technique in machine learning that combines the predictions
    of multiple models to improve overall performance. The idea behind ensemble models
    is that multiple models can be better than a single model as different models
    may capture different patterns in the data.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 集成建模是机器学习中的一种技术，通过结合多个模型的预测来提高整体性能。集成模型背后的理念是，多个模型可能比单个模型更好，因为不同的模型可能捕捉到数据中的不同模式。
- en: There are several types of ensemble models, all of which we’ll cover in the
    following sections.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种类型的集成模型，所有这些我们将在接下来的章节中介绍。
- en: Bagging
  id: totrans-455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: '**Bootstrap aggregating**, also known as **bagging**, is an ensemble method
    that combines multiple independent models trained on different subsets of the
    training data to reduce variance and improve model generalization.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bootstrap aggregating**，也称为 **Bagging**，是一种集成方法，通过结合在训练数据的不同子集上训练的多个独立模型来减少方差并提高模型泛化能力。'
- en: 'The bagging algorithm can be summarized as follows:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 算法可以概括如下：
- en: Given a training dataset of size *n*, create *m* bootstrap samples of size *n*
    (that is, sample *n* instances with replacement *m* times).
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个大小为 *n* 的训练数据集，创建 *m* 个大小为 *n* 的自助样本（即，用替换方式抽取 *n* 个实例 *m* 次）。
- en: Train a base model (for example, a decision tree) on each bootstrap sample independently.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 独立地对每个自助样本训练一个基础模型（例如，决策树）。
- en: Aggregate the predictions of all base models to obtain the ensemble prediction.
    This can be done by either taking the majority vote (in the case of classification)
    or the average (in the case of regression).
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有基础模型的预测进行聚合以获得集成预测。这可以通过在分类的情况下进行多数投票或在回归的情况下进行平均来实现。
- en: The bagging algorithm is particularly effective when the base models are unstable
    (that is, have high variance), such as decision trees, and when the training dataset
    is small.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 当基础模型不稳定（即，具有高方差）时，Bagging 算法特别有效，例如决策树，以及当训练数据集较小时。
- en: 'The equation for aggregating the predictions of the base models depends on
    the type of problem (classification or regression). For classification, the ensemble
    prediction is obtained by taking the majority vote:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的预测聚合方程取决于问题的类型（分类或回归）。对于分类，通过多数投票获得集成预测：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>Y</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>j</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mi>j</mi><mo>)</mo></mrow></mrow></mrow></math>](img/214.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>Y</mi><mrow><mi>e</mi><mi>n</mi><mi}s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>j</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mi>j</mi><mo>)</mo></mrow></mrow></mrow></math>](img/214.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/215.png)
    is the predicted class of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/216.png)
    base model for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/217.png)
    instance and *I()* is the indicator function (equal to 1 if x is true, and 0 otherwise).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/215.png)
    是![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/216.png)
    基础模型对![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/217.png)
    实例的预测类别，而 *I()* 是指示函数（如果 x 为真，则等于 1，否则为 0）。
- en: 'For regression, the ensemble prediction is obtained by taking the average score:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归，通过取平均分数来获得集成预测：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>Y</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></math>](img/218.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>Y</mi><mrow><mi>e</mi><mi>n</mi><mi}s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></math>](img/218.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/193.png)
    is the predicted value of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    base model.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/193.png)
    是![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    基础模型的预测值。
- en: 'The advantages of bagging are as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 套袋法的优点如下：
- en: Improved model generalization by reducing variance and overfitting
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少方差和过拟合来提高模型泛化能力
- en: Ability to handle high-dimensional datasets with complex relationships
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够处理具有复杂关系的超高维数据集
- en: Can be used with a variety of base models
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以与各种基础模型一起使用
- en: 'The disadvantages of bagging are as follows:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 套袋法的缺点如下：
- en: Increased model complexity and computation time due to the use of multiple base
    models
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用多个基础模型，模型复杂度和计算时间增加
- en: It can sometimes lead to overfitting if the base models are too complex or the
    dataset is too small
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果基础模型过于复杂或数据集太小，有时会导致过拟合
- en: It does not work well when the base models are highly correlated or biased
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当基础模型高度相关或存在偏差时，效果不佳
- en: Boosting
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升法
- en: Boosting is another popular ensemble learning technique that aims to improve
    the performance of weak classifiers by combining them into a stronger classifier.
    Unlike bagging, boosting focuses on iteratively improving the accuracy of the
    classifier by adjusting the weights of the training examples. The basic idea behind
    boosting is to learn from the mistakes of the previous weak classifiers and to
    put more emphasis on the examples that were incorrectly classified in the previous
    iteration.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是另一种流行的集成学习技术，旨在通过将它们组合成一个更强的分类器来提高弱分类器的性能。与bagging不同，boosting侧重于通过调整训练示例的权重来迭代地提高分类器的精度。提升背后的基本思想是从先前弱分类器的错误中学习，并更加关注在先前迭代中被错误分类的示例。
- en: 'There are several boosting algorithms, but one of the most popular ones is
    AdaBoost (short for adaptive boosting). The AdaBoost algorithm works as follows:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种提升算法，但其中最受欢迎的一种是AdaBoost（即自适应提升）。AdaBoost算法的工作原理如下：
- en: First, it initializes the weights of the training examples to be equal.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它将训练示例的权重初始化为相等。
- en: Then, it trains a weak classifier on the training set.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它在训练集上训练一个弱分类器。
- en: Next, it computes the weighted error rate of the weak classifier.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，它计算弱分类器的加权错误率。
- en: After, it computes the importance of the weak classifier based on its weighted
    error rate.
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它根据其加权错误率计算弱分类器的重要性。
- en: Then, it increases the weights of the examples that were misclassified by the
    weak classifier.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它增加被弱分类器错误分类的示例的权重。
- en: Once it’s done this, it normalizes the weights of the examples so that they
    sum up to one.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成这一步后，它将示例的权重归一化，使它们的总和为1。
- en: It repeats *steps 2* to *6* for a predetermined number of iterations or until
    the desired accuracy is achieved.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它重复步骤2到6，直到达到预定的迭代次数或达到所需的精度。
- en: Finally, it combines the weak classifiers into a strong classifier by assigning
    weights to them based on their importance.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它通过根据它们的重要性分配权重，将弱分类器组合成一个强分类器。
- en: 'The final classifier is a weighted combination of the weak classifiers. The
    importance of each weak classifier is determined by its weighted error rate, which
    is computed as an equation:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 最终分类器是弱分类器的加权组合。每个弱分类器的重要性由其加权错误率决定，该错误率按以下方程计算：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>E</mi><mi>R</mi><mi>R</mi><mi>O</mi><mi>R</mi></mrow><mi>m</mi></msub><mo>=</mo><mfrac><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mi>I</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow></math>](img/221.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>E</mi><mi>R</mi><mi>R</mi><mi>O</mi><mi>R</mi></mrow><mi>m</mi></msub><mo>=</mo><mfrac><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mi>I</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow></math>](img/221.png)'
- en: Here, *m* is the index of the weak classifier, *N* is the number of training
    examples, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/222.png)
    is the weight of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    training example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    is the true label of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    training example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>](img/226.png)
    is the prediction of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/227.png)
    weak classifier for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    training example, and ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced><mo>)</mo></mrow></mrow></mrow></math>](img/229.png)
    is an indicator function that returns 1 if the prediction of the weak classifier
    is incorrect and 0 otherwise.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m* 是弱分类器的索引，*N* 是训练样本的数量，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/222.png)
    是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    训练样本的权重，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    训练样本的真实标签，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>](img/226.png)
    是 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/227.png)
    弱分类器对 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    训练样本的预测，并且 ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced><mo>)</mo></mrow></mrow></mrow></math>](img/229.png)
    是一个指示函数，如果弱分类器的预测错误则返回 1，否则返回 0。
- en: 'The importance of the weak classifier is computed by the following equation:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 弱分类器的重要性通过以下公式计算：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="normal">α</mi><mi>m</mi></msub><mo>=</mo><mi>ln</mi><mfrac><mrow><mn>1</mn><mo>−</mo><msub><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><mi>m</mi></msub></mrow><msub><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><mi>m</mi></msub></mfrac></mrow></mrow></math>](img/230.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="normal">α</mi><mi>m</mi></msub><mo>=</mo><mi>ln</mi><mfrac><mrow><mn>1</mn><mo>−</mo><msub><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><mi>r</mi></msub></mrow><msub><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><mi>r</mi></msub></mfrac></mrow></mrow></math>](img/230.png)'
- en: 'The weights of the examples are updated based on their importance:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 样本的权重根据其重要性进行更新：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>i</mi><mrow><msub><mi
    mathvariant="normal">α</mi><mi>m</mi></msub><mi>I</mi><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mfenced></mrow></msubsup></mrow></mrow></math>](img/231.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>i</mi><mrow><msub><mi
    mathvariant="normal">α</mi><mi>m</mi></msub><mi>I</mi><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mfenced></mrow></msubsup></mrow></mrow></math>](img/231.png)'
- en: 'The final classifier is then obtained by combining the weak classifiers:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 最终分类器是通过组合弱分类器得到的：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math>](img/232.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math>](img/232.png)'
- en: Here, *M* is the total number of weak classifiers, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/233.png)
    is the prediction of the *m-th* weak classifier, and `sign()` is a function that
    returns +1 if its argument is positive and -1 otherwise.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*M* 是弱分类器的总数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/233.png)
    是第 *m* 个弱分类器的预测，`sign()` 是一个函数，如果其参数为正则返回 +1，否则返回 -1。
- en: 'Let’s look at some of the advantages of boosting:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看增强学习的优点：
- en: Boosting can improve the accuracy of weak classifiers and can lead to a significant
    improvement in performance
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强学习可以提高弱分类器的准确性，并可能导致性能的显著提升
- en: Boosting is relatively easy to implement and can be applied to a wide range
    of classification problems
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强学习相对容易实现，并且可以应用于广泛的分类问题
- en: Boosting can handle noisy data and reduce the risk of overfitting
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强学习可以处理噪声数据并降低过拟合的风险
- en: 'Here are some of the disadvantages of boosting:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 增强学习的缺点包括：
- en: Boosting can be sensitive to outliers and can overfit to noisy data
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强学习对异常值敏感，并且可能对噪声数据过拟合
- en: Boosting can be computationally expensive, especially when dealing with large
    datasets
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强学习在处理大数据集时可能计算成本较高
- en: Boosting can be difficult to interpret as it involves combining multiple weak
    classifiers
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升方法可能难以解释，因为它涉及到多个弱分类器的组合
- en: Stacking
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stacking
- en: Stacking is another popular ensemble learning technique that combines the predictions
    of multiple base models by training a higher-level model on their predictions.
    The idea behind stacking is to leverage the strengths of different base models
    to achieve better predictive performance.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: Stacking是一种流行的集成学习技术，通过在基础模型的预测上训练一个高级模型来结合多个基础模型的预测。Stacking背后的理念是利用不同基础模型的优势，以实现更好的预测性能。
- en: 'Here’s how stacking works:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何进行Stacking的：
- en: 'Divide the training data into two parts: the first part is used to train the
    base models, while the second part is used to create a new dataset of predictions
    from the base models.'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据分为两部分：第一部分用于训练基础模型，第二部分用于创建一个由基础模型预测组成的新数据集。
- en: Train multiple base models on the first part of the training data.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据的第一个部分上训练多个基础模型。
- en: Use the trained base models to make predictions on the second part of the training
    data to create a new dataset of predictions.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的基础模型对训练数据的第二部分进行预测，以创建一个预测的新数据集。
- en: Train a higher-level model (also known as a metamodel or blender) on the new
    dataset of predictions.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预测的新数据集上训练一个高级模型（也称为元模型或混合器）。
- en: Use the trained higher-level model to make predictions on the test data.
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的高级模型对测试数据进行预测。
- en: The higher-level model is typically a simple model such as a linear regression,
    logistic regression, or a decision tree. The idea is to use the predictions of
    the base models as input features for the higher-level model. This way, the higher-level
    model learns to combine the predictions of the base models to make more accurate
    predictions.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 高级模型通常是一个简单的模型，例如线性回归、逻辑回归或决策树。其理念是使用基础模型的预测作为高级模型的输入特征。这样，高级模型就能学会结合基础模型的预测，以做出更准确的预测。
- en: Random forests
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: One of the most commonly known ensemble models is random forest, where the model
    combines the predictions of multiple decision trees and outputs the predictions.
    This is usually more accurate and prone to overfitting. We elaborated on Random
    Forest earlier in this chapter.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的集成模型之一是随机森林，其中模型结合了多个决策树的预测并输出预测结果。这通常更准确，但容易过拟合。我们在本章前面详细介绍了随机森林。
- en: Gradient boosting
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting is another ensemble model that can be used for classification
    and regression tasks. It works by getting a weak classifier (such as a simple
    tree), and in each step tries to improve this weak classifier to build a better
    model. The main idea here is that the model tries to focus on its mistakes in
    each step and improve itself by fitting the model by correcting the errors made
    in previous trees.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是另一种可以用于分类和回归任务的集成模型。它通过获取一个弱分类器（例如简单的树），并在每一步尝试改进这个弱分类器以构建更好的模型。这里的核心理念是模型试图关注每一步中的错误，并通过纠正前一步树中犯下的错误来调整模型。
- en: During each iteration, the algorithm computes the negative gradient of the loss
    function concerning the predicted values, followed by fitting a decision tree
    to these negative gradient values. The predictions of the new tree are then combined
    with the predictions of the previous trees, using a learning rate parameter that
    controls the contribution of each tree to the final prediction.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，算法计算关于预测值的损失函数的负梯度，然后拟合一个决策树到这些负梯度值。然后，使用一个学习率参数（它控制每个树对最终预测的贡献）将新树的预测与先前树的预测相结合。
- en: The overall prediction of the gradient boosting model is obtained by summing
    up the predictions of all the trees, which are weighted by their respective learning
    rates.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升模型的总体预测是通过将所有树的预测相加得到的，这些预测根据它们各自的学习率进行加权。
- en: Let’s take a look at the equation for the gradient boosting algorithm.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看梯度提升算法的公式。
- en: 'First, we initialize the model with a constant value:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用一个常数值初始化模型：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>F</mi><mn>0</mn></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mi>c</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>c</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/234.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>F</mi><mn>0</mn></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mi>c</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>c</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/234.png)'
- en: Here, *c* is a constant, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    is the true label of the *i-th* sample, *N* is the number of samples, and *L*
    is the loss function, which is used to measure the error between the predicted
    and true labels.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*c* 是一个常数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    是第 *i* 个样本的真实标签，*N* 是样本数量，*L* 是损失函数，它用于衡量预测标签和真实标签之间的误差。
- en: 'At each iteration, *m*, the algorithm fits a decision tree to the negative
    gradient values of the loss function concerning the predicted values, ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msub><mi>r</mi><mi>m</mi></msub><mo>=</mo><mo>−</mo><mo>∇</mo><mi>L</mi><mo>(</mo><mi>y</mi><mo>,</mo><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/236.png).
    The decision tree predicts the negative gradient values, which are then used to
    update the predictions of the model via the following equation:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，*m*，算法将决策树拟合到损失函数关于预测值的负梯度值，即![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msub><mi>r</mi><mi>m</mi></msub><mo>=</mo><mo>−</mo><mo>∇</mo><mi>L</mi><mo>(</mo><mi>y</mi><mo>,</mo><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/236.png)。决策树预测负梯度值，然后使用以下方程更新模型的预测：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>F</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msub><mi>F</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>η</mi><mi
    mathvariant="normal">*</mi><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/237.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>F</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msub><mi>F</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>η</mi><mi
    mathvariant="normal">*</mi><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/237.png)'
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msub><mi>F</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/238.png)
    is the prediction of the model at the previous iteration, *η* is the learning
    rate, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/239.png)
    is the prediction of the decision tree at the current iteration.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msub><mi>F</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/238.png)
    是模型在前一次迭代的预测，*η* 是学习率，而![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/239.png)
    是当前迭代中决策树的预测。
- en: 'The final prediction of the model is obtained by combining the predictions
    of all the trees:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最终预测是通过结合所有树的预测得到的：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>η</mi><mi>m</mi></msub><mi
    mathvariant="normal">*</mi><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/240.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>η</mi><mi>m</mi></msub><mi
    mathvariant="normal">*</mi><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/240.png)'
- en: Here, *M* is the total number of trees in the model and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/241.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/242.png)
    are the learning rate and prediction of the *m-th* tree, respectively.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*M* 是模型中树的总数，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/241.png)
    和 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/242.png)
    分别是 *m* 棵树的学习率和预测值。
- en: 'Let’s look at some of the advantages of gradient boosting:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看梯度提升的一些优点：
- en: High prediction accuracy
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高预测准确率
- en: Handles both regression and classification problems
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理回归和分类问题
- en: Can handle missing data and outliers
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理缺失数据和异常值
- en: Can be used with various loss functions
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以与各种损失函数一起使用
- en: Can handle high-dimensional data
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理高维数据
- en: 'Now, let’s look at some of the disadvantages:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看一些缺点：
- en: Sensitive to overfitting, especially when the number of trees is large
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对过拟合敏感，尤其是在树的数量很大时
- en: Computationally expensive and time-consuming to train, especially for large
    datasets
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练计算量大，耗时，尤其是对于大数据集
- en: Requires careful tuning of hyperparameters, such as the number of trees, the
    learning rate, and the maximum depth of the trees
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要仔细调整超参数，例如树的数量、学习率和树的最大深度
- en: With that, we have reviewed the ensemble models that can help us improve our
    model performance. However, sometimes, our dataset has some features that we need
    to consider before we apply machine learning models. One common case is when we
    have an imbalanced dataset.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经回顾了可以帮助我们提高模型性能的集成模型。然而，有时我们的数据集有一些特征，在应用机器学习模型之前我们需要考虑。一个常见的情况是我们有一个不平衡的数据集。
- en: Handling imbalanced data
  id: totrans-541
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据
- en: 'In most real-world problems, our data is imbalanced, which means that the distribution
    of records from different classes (such as patients with and without cancer) is
    different. Handling imbalanced datasets is an important task in machine learning
    as it is common to have datasets with uneven class distribution. In such cases,
    the minority class is often under-represented, which can cause poor model performance
    and biased predictions. The reason behind this is that machine learning methods
    are trying to optimize their fitness function to minimize the error in the training
    set. Now, let’s say that we have 99% of the data from the positive class and 1%
    from the negative class. In this case, if the model predicts all records as positive,
    the error will be 1%; however, this model is not useful for us. That’s why, if
    we have an imbalanced dataset, we need to use various methods to handle imbalanced
    data. In general, we can have three categories of methods to handle imbalanced
    datasets:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数现实世界的问题中，我们的数据是不平衡的，这意味着不同类别的记录分布不同（例如，有癌症和无癌症的患者）。在机器学习中处理不平衡数据集是一个重要的任务，因为常见的情况是数据集具有不均匀的类别分布。在这种情况下，少数类通常代表性不足，这可能导致模型性能不佳和预测偏差。其背后的原因是机器学习方法试图优化其适应度函数以最小化训练集中的错误。现在，假设我们有99%的数据来自正类，1%来自负类。在这种情况下，如果模型将所有记录预测为正，错误率将是1%；然而，这个模型对我们来说没有用。这就是为什么，如果我们有一个不平衡的数据集，我们需要使用各种方法来处理不平衡数据。一般来说，我们可以有三种处理不平衡数据集的方法：
- en: '**Undersampling**: A very simple method that comes to mind is to use fewer
    training records from the majority class. This method works, but we need to consider
    that by using less training data, we are feeding less information to the model
    causes to have a less robust training and final model.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下采样**：一个简单的方法是使用少数类的较少训练记录。这种方法有效，但我们需要考虑的是，通过使用较少的训练数据，我们向模型提供的信息更少，这会导致训练和最终模型不够鲁棒。'
- en: '**Resampling**: Resampling methods involve modifying the original dataset to
    create a balanced distribution. This can be achieved by either oversampling the
    minority class (creating more samples of the minority class) or undersampling
    the majority class (removing samples from the majority class). Oversampling techniques
    include **random oversampling**, **Synthetic Minority Oversampling Technique**
    (**SMOTE**), and **Adaptive Synthetic Sampling** (**ADASYN**). Undersampling techniques
    include **random undersampling**, **Tomek links**, and **cluster centroids**.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重采样**：重采样方法涉及修改原始数据集以创建平衡分布。这可以通过对少数类进行过采样（创建更多少数类的样本）或对多数类进行下采样（从多数类中删除样本）来实现。过采样技术包括
    **随机过采样**、**合成少数过采样技术**（**SMOTE**）和**自适应合成采样**（**ADASYN**）。下采样技术包括**随机下采样**、**Tomek
    链接**和**聚类中心**。'
- en: '**Handling imbalanced datasets in machine learning models**: Such as modifying
    cost function, or modified batching in deep learning models.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理机器学习模型中的不平衡数据集**：例如修改代价函数，或在深度学习模型中进行修改的批量处理。'
- en: SMOTE
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTE
- en: SMOTE is a widely used algorithm for handling imbalanced datasets in machine
    learning. It is a synthetic data generation technique that creates new, synthetic
    samples in the minority class by interpolating between existing samples. SMOTE
    works by identifying the k-nearest neighbors of a minority class sample and then
    generating new samples along the line segments that connect these neighbors.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE 是处理机器学习中不平衡数据集的常用算法。它是一种合成数据生成技术，通过在现有样本之间插值来创建新的、合成的少数类样本。SMOTE 通过识别少数类样本的
    k 个最近邻，然后在这些邻居之间连接的线段上生成新的样本。
- en: 'Here are the steps of the SMOTE algorithm:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 SMOTE 算法的步骤：
- en: Select a minority class sample, *x*.
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个少数类样本，*x*。
- en: Choose one of its k-nearest neighbors, *x’*.
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择其 k 个最近邻之一，*x’*。
- en: 'Generate a synthetic sample by interpolating between *x* and *x’*. To do this,
    choose a random number, *r*, between 0 and 1, and then calculate the synthetic
    sample, as follows:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在 *x* 和 *x’* 之间插值来生成一个合成样本。为此，选择一个介于 0 和 1 之间的随机数，*r*，然后按照以下方式计算合成样本：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>r</mi><mi
    mathvariant="normal">*</mi><mfenced open="(" close=")"><mrow><mrow><mi>x</mi><mo>′</mo></mrow><mo>−</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/243.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>r</mi><mi
    mathvariant="normal">*</mi><mfenced open="(" close=")"><mrow><mrow><mi>x</mi><mo>′</mo></mrow><mo>−</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/243.png)'
- en: This creates a new sample that is somewhere between *x* and *x’*, but not the
    same as either one.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个新的样本，它在 *x* 和 *x’* 之间，但不是两者中的任何一个。
- en: 4. Repeat *steps 1* to *3* until the desired number of synthetic samples has
    been generated.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 重复步骤 1 到 3，直到生成所需的合成样本数量。
- en: 'Here are the advantages and disadvantages of SMOTE:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 SMOTE 的优缺点：
- en: It helps to address the problem of class imbalance by creating synthetic samples
    in the minority class.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在少数类中创建合成样本，有助于解决类别不平衡的问题。
- en: SMOTE can be combined with other techniques, such as random undersampling or
    Tomek links, to further improve the balance of the dataset.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE 可以与其他技术结合使用，例如随机下采样或 Tomek 链接，以进一步改善数据集的平衡。
- en: SMOTE can be applied to both categorical and numerical data.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE 可以应用于分类数据和数值数据。
- en: SMOTE can sometimes create synthetic samples that are unrealistic or noisy,
    leading to overfitting.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE 有时可能会创建出不真实或噪声的合成样本，导致过拟合。
- en: SMOTE can sometimes cause the decision boundary to be too sensitive to the minority
    class, leading to poor performance of the majority class.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE 有时会导致决策边界对少数类过于敏感，从而影响多数类的性能。
- en: SMOTE can be computationally expensive for large datasets.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集，SMOTE 可能会计算成本较高。
- en: 'Here is an example of SMOTE in action. Suppose we have a dataset with two classes:
    the majority class (class 0) has 900 samples, and the minority class (class 1)
    has 100 samples. We want to use SMOTE to generate synthetic samples for the minority
    class:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 SMOTE 在实际应用中的例子。假设我们有一个包含两个类别的数据集：多数类（类别 0）有 900 个样本，少数类（类别 1）有 100 个样本。我们想使用
    SMOTE 为少数类生成合成样本：
- en: We select a minority class sample, *x*.
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择一个少数类样本，*x*。
- en: We choose one of its k-nearest neighbors, *x’*.
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择其 k 个最近邻之一，*x’*。
- en: 'We generate a synthetic sample by interpolating between *x* and *x’* using
    a random number, *r*:'
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用随机数 *r* 在 *x* 和 *x’* 之间进行插值来生成一个合成样本：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>r</mi><mi
    mathvariant="normal">*</mi><mo>(</mo><mi>x</mi><mo>′</mo><mo>−</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/244.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>r</mi><mi
    mathvariant="normal">*</mi><mo>(</mo><mi>x</mi><mo>′</mo><mo>−</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/244.png)'
- en: 'For example, suppose *x* is (*1, 2*), *x’* is (*3, 4*), and *r* is *0.5*. In
    this case, the new sample is as follows:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 *x* 是 (*1, 2*)，*x’* 是 (*3, 4*)，而 *r* 是 *0.5*。在这种情况下，新的样本如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>n</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mo>(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>)</mo><mo>+</mo><mn>0.5</mn><mi
    mathvariant="normal">*</mi><mo>(</mo><mo>(</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>)</mo><mo>−</mo><mo>(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>)</mo><mo>)</mo><mo>=</mo><mo>(</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>)</mo></mrow></mrow></mrow></math>](img/245.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>n</mi><mi>e</mi><mi}s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mo>(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>)</mo><mo>+</mo><mn>0.5</mn><mi
    mathvariant="normal">*</mi><mo>(</mo><mo>(</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>)</mo><mo>−</mo><mo>(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>)</mo><mo>)</mo><mo>=</mo><mo>(</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>)</mo></mrow></mrow></mrow></math>](img/245.png)'
- en: 4. We repeat *steps 1* to *3* until we have generated the desired number of
    synthetic samples. For example, suppose we want to generate 100 synthetic samples.
    We repeat *steps 1* to *3* for each of the 100 minority class samples and then
    combine the original minority class samples with the synthetic samples to create
    a balanced dataset with 200 samples in each class.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 我们重复 *步骤 1* 到 *3*，直到我们生成了所需的合成样本数量。例如，如果我们想生成 100 个合成样本，我们将对每个 100 个少数类样本重复
    *步骤 1* 到 *3*，然后将原始的少数类样本与合成样本结合起来，创建每个类别有 200 个样本的平衡数据集。
- en: The NearMiss algorithm
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近失算法
- en: The `NearMiss` algorithm is a technique for balancing class distribution by
    undersampling (removing) the records from the major class. When two classes have
    records that are very close to each other, eliminating some of the records from
    the majority class increases the distance between the two classes, which helps
    the classification process. To avoid information loss problems in the majority
    of undersampling methods, near-miss methods are widely used.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '`NearMiss` 算法是一种通过下采样（移除）主要类别的记录来平衡类别分布的技术。当两个类别的记录非常接近时，从多数类中移除一些记录可以增加两个类别之间的距离，这有助于分类过程。为了避免大多数下采样方法中的信息丢失问题，近失法被广泛使用。'
- en: 'The working of nearest-neighbor methods is based on the following steps:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻方法的工作基于以下步骤：
- en: Find the distances between all records from the major class and minor class.
    Our goal is to undersample the records from the major class.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到主要类和少数类中所有记录之间的距离。我们的目标是下采样主要类的记录。
- en: Choose *n* records from the major class that are closest to the minor class.
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从主要类中选择 *n* 个最接近少数类的记录。
- en: If there are *k* records in the minor class, the nearest method will return
    *k*n* records from the major class.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果少数类中有 *k* 个记录，最近的方法将返回主要类中的 *k*n* 个记录。
- en: 'There are three variations of applying the `NearMiss` algorithm that we can
    use to find the *n* closest records in the major class:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用三种 `NearMiss` 算法的变体来找到主要类中的 *n* 个最近记录：
- en: We can select the records of the major class for which the average distances
    to the k-closest records of the minor class are the smallest.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择主要类中平均距离到少数类 k 个最近记录最小的记录。
- en: We can select the records of the major class for which the average distances
    to the k-farthest records of the minor class are the smallest.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择多数类别的记录，使得到少数类别的k个最远记录的平均距离最小。
- en: We can implement two steps. In the first step, for each record from the minor
    class, their *M* nearest neighbors will be stored. Then, the records from the
    major class are selected such that the average distance to the *N* nearest neighbors
    is the largest.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以实施两个步骤。在第一步中，对于少数类别的每个记录，将存储其*M*个最近的邻居。然后，选择多数类别的记录，使得到*N*个最近邻居的平均距离最大。
- en: Cost-sensitive learning
  id: totrans-580
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本敏感学习
- en: Cost-sensitive learning is a method that’s used to train machine learning models
    on imbalanced datasets. In imbalanced datasets, the number of examples in one
    class (usually the minority class) is much lower than in the other class (usually
    the majority class). Cost-sensitive learning involves assigning misclassification
    costs to the model that differ based on the class being predicted, which can help
    the model focus more on correctly classifying the minority class.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 成本敏感学习是一种用于在不平衡数据集上训练机器学习模型的方法。在不平衡数据集中，某一类别的示例数量（通常是少数类别）远低于另一类别（通常是多数类别）。成本敏感学习涉及为模型分配基于预测类别的不同误分类成本，这可以帮助模型更多地关注正确分类少数类别。
- en: Let’s assume we have a binary classification problem with two classes, positive
    and negative. In cost-sensitive learning, we assign different costs to different
    types of errors. For example, we may assign a higher cost to misclassifying a
    positive example as negative because in an imbalanced dataset, the positive class
    is the minority class, and misclassifying positive examples can have a greater
    impact on the performance of the model.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个具有两个类别（正面和负面）的二分类问题。在成本敏感学习中，我们为不同类型的错误分配不同的成本。例如，我们可能将错误地将正面示例分类为负面的成本设置得更高，因为在不平衡数据集中，正面类别是少数类别，错误分类正面示例可能会对模型的性能产生更大的影响。
- en: 'We can assign costs in the form of a confusion matrix:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将成本以混淆矩阵的形式分配：
- en: '|  | **Predicted Positive** | **Predicted Negative** |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测正面** | **预测负面** |'
- en: '| --- | --- | --- |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Actual Positive** | `TP_cost` | `FN_cost` |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| **实际正面** | `TP_cost` | `FN_cost` |'
- en: '| **Actual Negative** | `FP_cost` | `TN_cost` |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| **实际负面** | `FP_cost` | `TN_cost` |'
- en: Table 3.2 – Confusion matrix costs
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 – 混淆矩阵成本
- en: Here, `TP_cost`, `FN_cost`, `FP_cost`, and `TN_cost` are the costs associated
    with true positives, false negatives, false positives, and true negatives, respectively.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`TP_cost`、`FN_cost`、`FP_cost`和`TN_cost`分别是真实正面、假阴性、假阳性和真实负面的相关成本。
- en: 'To incorporate the cost matrix into the training process, we can modify the
    standard loss function that the model optimizes during training. One common cost-sensitive
    loss function is the weighted cross-entropy loss, which is defined as follows:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将成本矩阵纳入训练过程，我们可以修改模型在训练期间优化的标准损失函数。一个常见的成本敏感损失函数是加权交叉熵损失，其定义如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><mo>(</mo><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mi
    mathvariant="normal">*</mi><mi>y</mi><mi mathvariant="normal">*</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mover><mi
    mathvariant="normal">y</mi><mo stretchy="true">ˆ</mo></mover><mo>)</mo><mo>+</mo><msub><mi>w</mi><mrow><mi>n</mi><mi>e</mi><mi>g</mi></mrow></msub><mi
    mathvariant="normal">*</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo><mi
    mathvariant="normal">*</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><mover><mi
    mathvariant="normal">y</mi><mo stretchy="true">ˆ</mo></mover><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/246.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi}s</mi><mi}s</mi><mo>=</mo><mo>−</mo><mo>(</mo><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mi
    mathvariant="normal">*</mi><mi>y</mi><mi mathvariant="normal">*</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mover><mi
    mathvariant="normal">y</mi><mo stretchy="true">ˆ</mo></mover><mo>)</mo><mo>+</mo><msub><mi>w</mi><mrow><mi>n</mi><mi>e</mi><mi>g</mi></mrow></msub><mi
    mathvariant="normal">*</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo><mi
    mathvariant="normal">*</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><mover><mi
    mathvariant="normal">y</mi><mo stretchy="true">ˆ</mo></mover><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/246.png)'
- en: Here, *y* is the true label (either 0 or 1), ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/247.png)
    is the predicted probability of the positive class, and ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/248.png)and
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/249.png)
    are weights that are assigned to the positive and negative classes, respectively.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y*是真实标签（要么是0要么是1），![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math>](img/247.png)是预测的正类概率，而![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/248.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/249.png)是分别分配给正类和负类的权重。
- en: The weights, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/250.png)and
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/251.png),
    can be determined by the costs assigned in the confusion matrix. For example,
    if we assign a higher cost to false negatives (that is, misclassifying a positive
    example as negative), we may set ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/252.png)to
    a higher value than ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/249.png).
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 权重，![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/250.png)和![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/251.png)，可以通过混淆矩阵中分配的成本来确定。例如，如果我们对假阴性（即错误地将正例分类为负例）分配更高的成本，我们可能会将![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/252.png)的值设置得比![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/249.png)高。
- en: 'Cost-sensitive learning can also be used with other types of models, such as
    decision trees and SVMs. The concept of assigning costs to different types of
    errors can be applied in various ways to improve the performance of a model on
    imbalanced datasets. However, it’s important to carefully select the appropriate
    cost matrix and loss function based on the specific characteristics of the dataset
    and the problem being solved:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 成本敏感学习也可以与其他类型的模型一起使用，例如决策树和SVMs。将成本分配给不同类型的错误的概念可以以多种方式应用于不平衡数据集以提高模型性能。然而，根据数据集和解决问题的具体特征，仔细选择合适的成本矩阵和损失函数是非常重要的：
- en: '**Ensemble techniques**: Ensemble techniques combine multiple models to improve
    predictive performance. In imbalanced datasets, an ensemble of models can be trained
    on different subsets of the dataset, ensuring that each model is trained on both
    the minority and majority classes. Examples of ensemble techniques for imbalanced
    datasets include bagging and boosting.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成技术**：集成技术通过结合多个模型来提高预测性能。在不平衡数据集中，可以针对数据集的不同子集训练模型集，确保每个模型都在少数和多数类上进行了训练。不平衡数据集的集成技术示例包括bagging和boosting。'
- en: '**Anomaly detection**: Anomaly detection techniques can be used to identify
    the minority class as an anomaly in the dataset. These techniques aim to identify
    rare events that are significantly different from the majority class. The identified
    samples can then be used to train the model on the minority class.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：异常检测技术可以用来在数据集中识别少数类作为异常。这些技术旨在识别与多数类显著不同的罕见事件。然后，可以识别的样本用于在少数类上训练模型。'
- en: Data augmentation
  id: totrans-597
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: The idea behind data augmentation is to generate new examples by applying transformations
    to the original ones, while still retaining the label. These transformations can
    include rotation, translation, scaling, flipping, and adding noise, among others.
    This can be particularly useful for imbalanced datasets, where the number of examples
    in one class is much smaller than in the other.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强背后的思想是通过应用变换到原始示例来生成新的示例，同时仍然保留标签。这些变换可以包括旋转、平移、缩放、翻转和添加噪声等。这对于不平衡数据集尤其有用，其中一个类别的示例数量远小于另一个类别。
- en: In the context of imbalanced datasets, data augmentation can be used to create
    new examples of the minority class, effectively balancing the dataset. This can
    be done by applying the same set of transformations to the minority class examples,
    creating a new set of examples that are still representative of the minority class
    but are slightly different from the original ones.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡数据集的背景下，数据增强可以用来创建少数类的新示例，从而有效地平衡数据集。这可以通过对少数类示例应用相同的变换集来实现，创建一组新的示例，这些示例仍然代表少数类，但与原始示例略有不同。
- en: 'The equations that are involved in data augmentation are relatively simple
    as they are based on applying transformation functions to the original examples.
    For example, to rotate an image by a certain angle, we can use a rotation matrix:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强中涉及到的方程相对简单，因为它们是基于对原始示例应用变换函数。例如，为了将图像旋转一定角度，我们可以使用旋转矩阵：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mi>x</mi><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>−</mo><mi>y</mi><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/254.png)'
  id: totrans-601
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mi>x</mi><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>−</mo><mi>y</mi><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/254.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>′</mo><mo>=</mo><mi>x</mi><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>+</mo><mi>y</mi><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/255.png)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>′</mo><mo>=</mo><mi>x</mi><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>+</mo><mi>y</mi><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/255.png)'
- en: Here, *x* and *y* are the original coordinates of a pixel in the image, *x’*
    and *y’* are the new coordinates after rotation, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>θ</mml:mi></mml:math>](img/256.png)
    is the angle of rotation.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 和 *y* 是图像中像素的原始坐标，*x’* 和 *y’* 是旋转后的新坐标，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>θ</mml:mi></mml:math>](img/256.png)
    是旋转角度。
- en: 'Similarly, to apply translation, we can simply shift the image by a certain
    number of pixels:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，为了应用平移，我们可以简单地通过一定数量的像素移动图像：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/257.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/257.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>′</mo><mo>=</mo><mi>y</mi><mo>+</mo><mi>d</mi><mi>y</mi></mrow></mrow></math>](img/258.png)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>′</mo><mo>=</mo><mi>y</mi><mo>+</mo><mi>d</mi><mi>y</mi></mrow></mrow></math>](img/258.png)'
- en: Here, *dx* and *dy* are the horizontal and vertical shifts, respectively.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*dx* 和 *dy* 分别是水平和垂直位移。
- en: Data augmentation can be a powerful technique for addressing imbalanced datasets
    as it can create new examples that are representative of the minority class, while
    still preserving the label information. However, it is important to be careful
    when applying data augmentation as it can also introduce noise and artifacts in
    the data, and can lead to overfitting if not done properly.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强可以是一种强大的技术，用于解决不平衡数据集，因为它可以创建代表少数类的示例，同时仍然保留标签信息。然而，在应用数据增强时需要谨慎，因为它也可能在数据中引入噪声和伪影，并且如果不当处理可能会导致过拟合。
- en: In conclusion, handling imbalanced datasets is an important aspect of machine
    learning. There are several techniques available to handle imbalanced datasets,
    each with its advantages and disadvantages. The choice of technique depends on
    the dataset, the problem, and the available resources. Besides having imbalanced
    data, in the case of working on time series data, we might face correlated data.
    We’ll take a closer look at this next.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，处理不平衡数据集是机器学习的一个重要方面。有几种技术可以处理不平衡数据集，每种技术都有其优缺点。技术选择取决于数据集、问题和可用资源。除了处理不平衡数据之外，在处理时间序列数据的情况下，我们可能会遇到相关数据。我们将在下一部分更详细地探讨这一点。
- en: Dealing with correlated data
  id: totrans-610
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理相关数据
- en: 'Dealing with correlated time series data in machine learning models can be
    challenging as traditional techniques such as random sampling can introduce biases
    and overlook dependencies between data points. Here are some approaches that can
    help:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中处理相关时间序列数据可能具有挑战性，因为传统的技术，如随机抽样，可能会引入偏差并忽略数据点之间的依赖关系。以下是一些可以帮助的方法：
- en: '**Time series cross-validation**: Time series data is often dependent on past
    values and it’s important to preserve this relationship during model training
    and evaluation. Time series cross-validation involves splitting the data into
    multiple folds, with each fold consisting of a continuous block of time. This
    approach ensures that the model is trained on past data and evaluated on future
    data, which better simulates how the model will perform in real-world scenarios.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列交叉验证**：时间序列数据通常依赖于过去值，在模型训练和评估期间保持这种关系很重要。时间序列交叉验证涉及将数据分成多个折叠，每个折叠包含一个连续的时间块。这种方法确保模型在历史数据上训练并在未来数据上评估，这更好地模拟了模型在实际场景中的表现。'
- en: '**Feature engineering**: Correlated time series data can be difficult to model
    with traditional machine learning algorithms. Feature engineering can help transform
    the data into a more suitable format. Examples of feature engineering for time
    series data include creating lags or differences in the time series, aggregating
    data into time buckets or windows, and creating rolling statistics such as moving
    averages.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：相关时间序列数据可能难以用传统的机器学习算法进行建模。特征工程可以帮助将数据转换成更合适的格式。时间序列数据特征工程的一些例子包括在时间序列中创建滞后或差异，将数据聚合到时间桶或窗口中，以及创建滚动统计量，如移动平均数。'
- en: '**Time series-specific models**: There are several models specifically designed
    for time series data, such as **AutoRegressive Integrated Moving Average** (**ARIMA**),
    **Seasonal ARIMA** (**SARIMA**), **Prophet**, and **Long Short-Term Memory** (**LSTM**)
    networks. These models are designed to capture the dependencies and patterns in
    time series data and may outperform traditional machine learning models.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列特定模型**：有几个模型专门设计用于时间序列数据，例如**自回归积分移动平均**（**ARIMA**）、**季节性自回归积分移动平均**（**SARIMA**）、**Prophet**和**长短期记忆**（**LSTM**）网络。这些模型旨在捕捉时间序列数据中的依赖性和模式，并且可能优于传统的机器学习模型。'
- en: '**Time series preprocessing techniques**: Time series data can be preprocessed
    to remove correlations and make the data more suitable for machine learning models.
    Techniques such as differencing, detrending, and normalization can help remove
    trends and seasonal components from the data, which can help reduce correlations.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列预处理技术**：时间序列数据可以预处理以去除相关性并使数据更适合机器学习模型。例如，差分、去趋势和归一化等技术可以帮助从数据中去除趋势和季节性成分，这有助于减少相关性。'
- en: '**Dimensionality reduction techniques**: Correlated time series data can have
    a high dimensionality, which can make modeling difficult. Dimensionality reduction
    techniques such as PCA or autoencoders can help reduce the number of variables
    in the data while preserving the most important information.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维技术**：相关的时间序列数据可能具有高维度，这可能会使建模变得困难。PCA或自动编码器等降维技术可以帮助减少数据中的变量数量，同时保留最重要的信息。'
- en: In general, it’s important to approach time series data with techniques that
    preserve the temporal dependencies and patterns in the data. This can require
    specialized modeling techniques and preprocessing steps.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用能够保留数据中时间依赖性和模式的技术来处理时间序列数据是很重要的。这可能需要专门的建模技术和预处理步骤。
- en: Summary
  id: totrans-618
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about various concepts related to machine learning,
    starting with data exploration and preprocessing techniques. We then explored
    various machine learning models, such as logistic regression, decision trees,
    support vector machines, and random forests, along with their strengths and weaknesses.
    We also discussed the importance of splitting data into training and test sets,
    as well as techniques for handling imbalanced datasets.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了与机器学习相关的各种概念，从数据探索和预处理技术开始。然后我们探讨了各种机器学习模型，如逻辑回归、决策树、支持向量机和随机森林，以及它们的优缺点。我们还讨论了将数据分为训练集和测试集的重要性，以及处理不平衡数据集的技术。
- en: The chapter also covered the concepts of model bias, variance, underfitting,
    and overfitting, and how to diagnose and address these issues. We also explored
    ensemble methods such as bagging, boosting, and stacking, which can improve model
    performance by combining the predictions of multiple models.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涵盖了模型偏差、方差、欠拟合和过拟合的概念，以及如何诊断和解决这些问题。我们还探讨了集成方法，如袋装法、提升法和堆叠法，这些方法可以通过结合多个模型的预测来提高模型性能。
- en: Finally, we learned about the limitations and challenges of machine learning,
    including the need for large amounts of high-quality data, the risk of bias and
    unfairness, and the difficulty of interpreting complex models. Despite these challenges,
    machine learning offers powerful tools for solving a wide range of problems and
    has the potential to transform many industries and fields.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了机器学习的局限性和挑战，包括需要大量高质量数据、存在偏差和不公平的风险，以及解释复杂模型的困难。尽管存在这些挑战，机器学习仍然提供了强大的工具来解决广泛的问题，并有可能改变许多行业和领域。
- en: In the next chapter, we will discuss text preprocessing, which is required for
    text to be used by machine learning models.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论文本预处理，这是使文本能够被机器学习模型使用所必需的。
- en: References
  id: totrans-623
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N.: *Taking
    the human out of the loop: A review of Bayesian optimization. Proceedings of the
    IEEE 104(1), 148–175 (2016).* *DOI 10.1109/JPROC.2015.2494218*.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N.: *将人从循环中移除：贝叶斯优化的综述。IEEE汇刊，第104卷第1期，148–175页
    (2016)。* *DOI 10.1109/JPROC.2015.2494218*.'
