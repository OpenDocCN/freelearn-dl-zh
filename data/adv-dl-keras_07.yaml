- en: Chapter 7. Cross-Domain GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 跨领域GAN
- en: In computer vision, computer graphics, and image processing a number of tasks
    involve translating an image from one form to another. As an example, colorization
    of grayscale images, converting satellite images to maps, changing the artwork
    style of one artist to another, making night-time images into daytime, and summer
    photos to winter, are just a few examples. These tasks are referred to as **cross-domain
    transfer and will be the focus of this chapter**. An image in the source domain
    is transferred to a target domain resulting in a new translated image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉、计算机图形学和图像处理领域，许多任务涉及将图像从一种形式转换为另一种形式。例如，灰度图像的上色、将卫星图像转换为地图、将一位艺术家的作品风格转换为另一位艺术家的风格、将夜间图像转换为白天图像、将夏季照片转换为冬季照片，这些都是例子。这些任务被称为**跨领域转换，将是本章的重点**。源领域中的图像被转换到目标领域，从而生成一个新的转换图像。
- en: A cross-domain transfer has a number of practical applications in the real world.
    As an example, in autonomous driving research, collecting road scene driving data
    is both time-consuming and expensive. In order to cover as many scene variations
    as possible in that example, the roads would be traversed during different weather
    conditions, seasons, and times giving us a large and varied amount of data. With
    the use of a cross-domain transfer, it's possible to generate new synthetic scenes
    that look real by translating existing images. For example, we may just need to
    collect road scenes in the summer from one area and gather road scenes in the
    winter from another place. Then, we can transform the summer images to winter
    and the winter images to summer. In this case, it reduces the number of tasks
    having to be done by half.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域转换在现实世界中有许多实际应用。例如，在自动驾驶研究中，收集道路场景驾驶数据既费时又昂贵。为了尽可能覆盖多种场景变化，在这个例子中，车辆将会在不同的天气条件、季节和时间下行驶，获取大量多样的数据。利用跨领域转换，能够通过转换现有图像生成看起来逼真的新合成场景。例如，我们可能只需要从一个地区收集夏季的道路场景，从另一个地方收集冬季的道路场景。然后，我们可以将夏季图像转换为冬季图像，将冬季图像转换为夏季图像。这样，可以将需要完成的任务数量减少一半。
- en: Generation of realistic synthesized images is an area that GANs excel at. Therefore,
    cross-domain translation is one of the applications of GANs. In this chapter,
    we're going to focus on a popular cross-domain GAN algorithm called **CycleGAN**
    [2]. Unlike other cross-domain transfer algorithms, such as a **pix2pix** [3],
    CycleGAN doesn't require aligned training images to work. In aligned images, the
    training data should be a pair of images made up of the source image and its corresponding
    target image. For example, a satellite image and the corresponding map derived
    from this image. CycleGAN only requires the satellite data images and maps. The
    maps may be from another satellite data and are not necessarily previously generated
    from the training data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成逼真合成图像是生成对抗网络（GANs）擅长的领域。因此，跨领域转换是GAN的一种应用。在本章中，我们将重点介绍一种流行的跨领域GAN算法——**CycleGAN**
    [2]。与其他跨领域转换算法（如**pix2pix** [3]）不同，CycleGAN不需要对齐的训练图像就能工作。在对齐图像中，训练数据应由一对图像组成，即源图像及其对应的目标图像。例如，一张卫星图像及其相应的地图。CycleGAN只需要卫星数据图像和地图。地图可能来自其他卫星数据，并不一定是之前从训练数据生成的。
- en: 'In this chapter, we will explore the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下内容：
- en: The principles of CycleGAN, including its implementation in Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN的原理，包括其在Keras中的实现
- en: Example applications of CycleGAN, including the colorization of grayscale images
    using the CIFAR10 dataset and style transfer as applied on MNIST digits and **Street
    View House Numbers** (**SVHN**) [1] datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN的示例应用，包括使用CIFAR10数据集进行灰度图像上色和在MNIST数字及**街景房屋号码**（**SVHN**）[1]数据集上进行风格转换
- en: Principles of CycleGAN
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CycleGAN的原理
- en: '![Principles of CycleGAN](img/B08956_07_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN原理](img/B08956_07_01.jpg)'
- en: 'Figure 7.1.1: Example of aligned image pair: left, original image and right,
    transformed image using a Canny edge detector. Original photos were taken by the
    author.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.1：对齐图像对的示例：左侧为原始图像，右侧为使用Canny边缘检测器转换后的图像。原始照片由作者拍摄。
- en: 'Translating an image from one domain to another is a common task in computer
    vision, computer graphics, and image processing. The preceding figure shows edge
    detection which is a common image translation task. In this example, we can consider
    the real photo (left) as an image in the source domain and the edge detected photo
    (right) as a sample in the target domain. There are many other cross-domain translation
    procedures that have practical applications such as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个领域到另一个领域的图像转换是计算机视觉、计算机图形学和图像处理中的常见任务。前面的图示了边缘检测，这是一个常见的图像转换任务。在这个例子中，我们可以将左侧的真实照片视为源域中的一张图像，而右侧的边缘检测图像视为目标域中的一个样本。还有许多其他跨领域转换过程具有实际应用，例如：
- en: Satellite image to map
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卫星图像转换为地图
- en: Face image to emoji, caricature or anime
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部图像转换为表情符号、漫画或动漫
- en: Body image to the avatar
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身体图像转换为头像
- en: Colorization of grayscale photos
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰度照片的着色
- en: Medical scan to a real photo
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学扫描图像转换为真实照片
- en: Real photo to an artist's painting
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实照片转换为艺术家画作
- en: 'There are many more examples of this in different fields. In computer vision
    and image processing, for example, we can perform the translation by inventing
    an algorithm that extracts features from the source image to translate it into
    the target image. Canny edge operator is an example of such an algorithm. However,
    in many cases, the translation is very complex to hand-engineer that it is almost
    impossible to find a suitable algorithm. Both the source and target domain distributions
    are high-dimensional and complex:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同领域中有许多类似的例子。例如，在计算机视觉和图像处理领域，我们可以通过发明一个提取源图像特征并将其转换为目标图像的算法来执行转换。Canny 边缘检测算子就是这样一个算法的例子。然而，在许多情况下，转换过程非常复杂，手动设计几乎不可能找到合适的算法。源域和目标域的分布都是高维且复杂的：
- en: '![Principles of CycleGAN](img/B08956_07_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN原理](img/B08956_07_02.jpg)'
- en: 'Figure 7.1.2: Example of not aligned image pair: left, a photo of real sunflowers
    along University Avenue, University of the Philippines and right, Sunflowers by
    Vincent Van Gogh at the National Gallery, London, UK. Original photos were taken
    by the author.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.2：未对齐的图像对示例：左侧是菲律宾大学大学大道上的真实向日葵照片，右侧是伦敦国家美术馆的文森特·梵高的《向日葵》。原始照片由作者拍摄。
- en: A workaround on the image translation problem is to use deep learning techniques.
    If we have a sufficiently large dataset from both the source and target domains,
    we can train a neural network to model the translation. Since the images in the
    target domain must be automatically generated given a source image, they must
    look like real samples from the target domain. GANs are a suitable network for
    such cross-domain tasks. The pix2pix [3] algorithm is an example of a cross-domain
    algorithm.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解决图像转换问题的一种方法是使用深度学习技术。如果我们拥有来自源域和目标域的足够大的数据集，我们可以训练神经网络来建模转换。由于目标域中的图像必须根据源图像自动生成，因此它们必须看起来像目标域中的真实样本。GANs是适合此类跨领域任务的网络。pix2pix
    [3]算法就是一个跨领域算法的例子。
- en: The pix2pix bears a resemblance to **Conditional GAN** (**CGAN**) [4] that we
    discussed in [Chapter 4](ch04.html "Chapter 4. Generative Adversarial Networks
    (GANs)"), *Generative Adversarial Networks (GANs)*. We can recall, that in conditional
    GANs, on top of the noise input, *z*, a condition such as in the form of a one-hot
    vector constrains the generator's output. For example, in the MNIST digit, if
    we want the generator to output the digit 8, the condition is the one-hot vector
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]. In pix2pix, the condition is the image to be translated.
    The generator's output is the translated image. The pix2pix is trained by optimizing
    the conditional GAN loss. To minimize blurring in the generated images, the *L1*
    loss is also included.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix 类似于我们在[第4章](ch04.html "第4章. 生成对抗网络 (GANs)")中讨论的**条件GAN**（**CGAN**）[4]，*生成对抗网络
    (GANs)*。我们可以回顾一下，在条件GAN中，除了噪声输入 *z* 外，一个条件（如一-hot向量）会限制生成器的输出。例如，在MNIST数字中，如果我们希望生成器输出数字8，则条件是一-hot向量[0,
    0, 0, 0, 0, 0, 0, 0, 1, 0]。在pix2pix中，条件是待转换的图像。生成器的输出是转换后的图像。pix2pix通过优化条件GAN损失进行训练。为了最小化生成图像中的模糊，还包括了*L1*损失。
- en: The main disadvantage of neural networks similar to pix2pix is the training
    input, and output images must be aligned. *Figure 7.1.1* is an example of an aligned
    image pair. The sample target image is generated from the source. In most occasions,
    aligned image pairs are not available or expensive to generate from the source
    images, or we have no idea on how to generate the target image from the given
    source image. What we have are sample data from the source and target domains.
    *Figure 7.1.2* is an example of data from the source domain (real photo) and the
    target domain (Van Gogh's art style) on the same sunflower subject. The source
    and target images are not necessarily aligned.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 pix2pix 的神经网络的主要缺点是训练输入和输出图像必须对齐。*图 7.1.1* 是一个对齐图像对的示例。样本目标图像是从源图像生成的。在大多数情况下，对齐的图像对无法获得，或者从源图像生成对齐图像的成本较高，或者我们不知道如何从给定的源图像生成目标图像。我们拥有的是来自源领域和目标领域的样本数据。*图
    7.1.2* 是一个示例，展示了同一向日葵主题的源领域（真实照片）和目标领域（梵高艺术风格）的数据。源图像和目标图像不一定对齐。
- en: 'Unlike pix2pix, CycleGAN learns image translation as long as there are a sufficient
    amount and variation of source and target data. No alignment is needed. CycleGAN
    learns the source and target distributions and how to translate from source to
    target distribution from given sample data. No supervision is needed. In the context
    of *Figure 7.1.2*, we just need thousands of photos of real sunflowers and thousands
    of photos of Van Gogh''s paintings of sunflowers. After training the CycleGAN,
    we''re able to translate a photo of sunflowers to a Van Gogh''s painting:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与 pix2pix 不同，CycleGAN 只要有足够的源数据和目标数据的变换和多样性，就能学习图像翻译。无需对齐。CycleGAN 学习源分布和目标分布，并从给定的样本数据中学习如何从源分布翻译到目标分布。不需要监督。在
    *图 7.1.2* 的背景下，我们只需要成千上万的真实向日葵照片和成千上万的梵高向日葵画作照片。训练完 CycleGAN 后，我们就能将一张向日葵照片翻译成一幅梵高的画作：
- en: '![Principles of CycleGAN](img/B08956_07_03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 原理](img/B08956_07_03.jpg)'
- en: 'Figure 7.1.3: The CycleGAN model is made of four networks: Generator G, Generator
    F, Discriminator D[y], and Discriminator D[x]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.3：CycleGAN 模型由四个网络组成：生成器 G、生成器 F、判别器 D[y] 和判别器 D[x]
- en: The CycleGAN Model
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CycleGAN 模型
- en: '*Figure 7.1.3* shows the network model of the CycleGAN. The objective of the CycleGAN
    is to learn the function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.1.3* 显示了 CycleGAN 的网络模型。CycleGAN 的目标是学习以下函数：'
- en: '*y''* = *G*(*x*) (Equation 7.1.1)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*y''* = *G*(*x*) （公式 7.1.1）'
- en: That generates fake images, *y* *'*, in the target domain as a function of the
    real source image, *x*. Learning is unsupervised by capitalizing only on the available
    real images, *x*, in the source domain and real images, *y*, in the target domain.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成目标领域的虚假图像，*y* *'*, 作为真实源图像 *x* 的函数。学习是无监督的，仅利用源领域和目标领域中可用的真实图像 *x* 和 *y*
    进行学习。
- en: 'Unlike regular GANs, CycleGAN imposes the cycle-consistency constraint. The
    forward cycle-consistency network ensures that the real source data can be reconstructed
    from the fake target data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规的 GAN 不同，CycleGAN 强加了循环一致性约束。正向循环一致性网络确保能够从虚假的目标数据中重建真实的源数据：
- en: '*x''* = *F*(*G*(*x*)) (Equation 7.1.2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*x''* = *F*(*G*(*x*)) （公式 7.1.2）'
- en: 'This is done by minimizing the forward cycle-consistency *L1* loss:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化正向循环一致性 *L1* 损失来实现这一目标：
- en: '![The CycleGAN Model](img/B08956_07_001.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 模型](img/B08956_07_001.jpg)'
- en: (Equation 7.1.3)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （公式 7.1.3）
- en: 'The network is symmetric. The backward cycle-consistency network also attempts
    to reconstruct the real target data from the fake source data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是对称的。反向循环一致性网络也试图从虚假的源数据中重建真实的目标数据：
- en: '*y* *''* = *G*(*F*(*y*)) (Equation 7.1.4)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* *''* = *G*(*F*(*y*)) （公式 7.1.4）'
- en: 'This is done by minimizing the backward cycle-consistency *L1* loss:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化反向循环一致性 *L1* 损失来实现这一目标：
- en: '![The CycleGAN Model](img/B08956_07_002.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 模型](img/B08956_07_002.jpg)'
- en: (Equation 7.1.5)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: （公式 7.1.5）
- en: 'The sum of these two losses is known as cycle-consistency loss:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个损失的和被称为循环一致性损失：
- en: '![The CycleGAN Model](img/B08956_07_003.jpg)![The CycleGAN Model](img/B08956_07_004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 模型](img/B08956_07_003.jpg)![CycleGAN 模型](img/B08956_07_004.jpg)'
- en: (Equation 7.1.6)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: （公式 7.1.6）
- en: The cycle-consistency loss uses *L1* or **Mean Absolute Error** (**MAE**) since
    it generally results in less blurry image reconstruction compared to *L2* or **Mean
    Square Error** (**MSE**).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失使用 *L1* 或 **均值绝对误差** (**MAE**)，因为与 *L2* 或 **均方误差** (**MSE**) 相比，它通常能得到更少模糊的图像重建。
- en: 'Similar to other GANs, the ultimate objective of CycleGAN is for the generator
    *G* to learn how to synthesize fake target data, *y* *''*, that can fool the discriminator,
    *D*[y], in the forward cycle. Since the network is symmetric, CycleGAN also wants
    the generator *F* to learn how to synthesize fake source data, *x* *''*, that
    can fool the discriminator, *D*[x], in the backward cycle. Inspired by the better
    perceptual quality of **Least Squares GAN** (**LSGAN**) [5], as described in [Chapter
    5](ch05.html "Chapter 5. Improved GANs"), *Improved GANs*, CycleGAN also uses
    MSE for the discriminator and generator losses. Recall that the difference of
    LSGAN from the original GAN is that the use of the MSE loss instead of a binary
    cross-entropy loss. CycleGAN expresses the generator-discriminator loss functions
    as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他GAN类似，CycleGAN的最终目标是让生成器*G*学习如何合成能够欺骗正向循环判别器*D*[y]的伪造目标数据*y* *'*。由于网络是对称的，CycleGAN还希望生成器*F*学习如何合成能够欺骗反向循环判别器*D*[x]的伪造源数据*x*
    *'*。受**最小二乘GAN**（**LSGAN**）[5]的更好感知质量启发，如[第5章](ch05.html "第5章 改进的GAN")中所述，*改进的GAN*，CycleGAN也使用MSE作为判别器和生成器的损失。回想一下LSGAN与原始GAN的不同之处在于，LSGAN使用MSE损失而不是二元交叉熵损失。CycleGAN将生成器-判别器损失函数表示为：
- en: '![The CycleGAN Model](img/B08956_07_005.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_005.jpg)'
- en: (Equation 7.1.7)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.7)
- en: '![The CycleGAN Model](img/B08956_07_006.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_006.jpg)'
- en: (Equation 7.1.8)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.8)
- en: '![The CycleGAN Model](img/B08956_07_007.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_007.jpg)'
- en: (Equation 7.1.9)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.9)
- en: '![The CycleGAN Model](img/B08956_07_008.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_008.jpg)'
- en: (Equation 7.1.10)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.10)
- en: '![The CycleGAN Model](img/B08956_07_009.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_009.jpg)'
- en: (Equation 7.1.11)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.11)
- en: '![The CycleGAN Model](img/B08956_07_010.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_010.jpg)'
- en: (Equation 7.1.12)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.12)
- en: 'The total loss of CycleGAN is shown as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN的总损失如下所示：
- en: '![The CycleGAN Model](img/B08956_07_011.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_011.jpg)'
- en: (Equation 7.1.13)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (方程式 7.1.13)
- en: 'CycleGAN recommends the following weight values:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN推荐以下权重值：
- en: '![The CycleGAN Model](img/B08956_07_012.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_012.jpg)'
- en: and
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![The CycleGAN Model](img/B08956_07_013.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_013.jpg)'
- en: to give more importance to the cyclic consistency check.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以便更重视循环一致性检查。
- en: The training strategy is similar to the vanilla GAN. *Algorithm* *7.1.1* summarizes
    the CycleGAN training procedure.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练策略类似于原始GAN。*算法* *7.1.1* 总结了CycleGAN的训练过程。
- en: 'Repeat for *n* training steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重复进行*n*次训练步骤：
- en: Minimize![The CycleGAN Model](img/B08956_07_014.jpg)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化![CycleGAN模型](img/B08956_07_014.jpg)
- en: by training the forward-cycle discriminator using real source and target data.
    A minibatch of real target data, *y*, is labeled 1.0\. A minibatch of fake target
    data, *y* *'* = *G*(*x*), is labelled 0.0.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用真实的源数据和目标数据训练正向循环判别器。一个真实目标数据的小批量，*y*，被标记为1.0。一个伪造目标数据的小批量，*y* *'* = *G*(*x*)，被标记为0.0。
- en: Minimize![The CycleGAN Model](img/B08956_07_015.jpg)
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化![CycleGAN模型](img/B08956_07_015.jpg)
- en: by training the backward-cycle discriminator using real source and target data.
    A minibatch of real source data, *x*, is labeled 1.0\. A minibatch of fake source
    data, *x* *'* = *F*(*y*), is labeled 0.0.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用真实的源数据和目标数据训练反向循环判别器。一个真实源数据的小批量，*x*，被标记为1.0。一个伪造源数据的小批量，*x* *'* = *F*(*y*)，被标记为0.0。
- en: Minimize![The CycleGAN Model](img/B08956_07_016.jpg)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化![CycleGAN模型](img/B08956_07_016.jpg)
- en: and
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 和
- en: '![The CycleGAN Model](img/B08956_07_017.jpg)'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_017.jpg)'
- en: by training the forward-cycle and backward-cycle generators in the adversarial
    networks. A minibatch of fake target data, *y* *'* = *G*(*x*), is labeled 1.0\.
    A minibatch of fake source data, *x* *'* = *F*(*y*), is labeled 1.0\. The weights
    of discriminators are frozen.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在对抗网络中训练正向循环和反向循环生成器。一个伪造目标数据的小批量，*y* *'* = *G*(*x*)，被标记为1.0。一个伪造源数据的小批量，*x*
    *'* = *F*(*y*)，被标记为1.0。判别器的权重被冻结。
- en: '![The CycleGAN Model](img/B08956_07_04.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_04.jpg)'
- en: 'Figure 7.1.4: During style transfer, the color composition may not be transferred
    successfully. To address this issue, the identity loss is added to the total loss
    function.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.4：在风格迁移过程中，颜色组成可能无法成功迁移。为了解决这个问题，加入了身份损失到总损失函数中。
- en: '![The CycleGAN Model](img/B08956_07_05.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型](img/B08956_07_05.jpg)'
- en: 'Figure 7.1.5: The CycleGAN model with identity loss as shown on the left side
    of the image'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.5：包含身份损失的CycleGAN模型，如图像左侧所示
- en: 'In neural style transfer problems, the color composition may not be successfully
    transferred from source image to the fake target image. This problem is shown
    in *Figure 7.1.4*. To address this problem, CycleGAN proposes to include the forward and
    backward-cycle identity loss function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经风格迁移问题中，颜色组成可能无法从源图像成功传递到假目标图像中。这个问题如*图 7.1.4*所示。为了解决这个问题，CycleGAN 提出了包含前向和反向循环身份损失函数的方案：
- en: '![The CycleGAN Model](img/B08956_07_018.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 模型](img/B08956_07_018.jpg)'
- en: (Equation 7.1.14)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 7.1.14）
- en: 'The total loss of CycleGAN becomes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的总损失为：
- en: '![The CycleGAN Model](img/B08956_07_019.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 模型](img/B08956_07_019.jpg)'
- en: (Equation 7.1.15)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: （方程 7.1.15）
- en: with
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与
- en: '![The CycleGAN Model](img/B08956_07_020.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 模型](img/B08956_07_020.jpg)'
- en: . The identity loss is also optimized during adversarial training. *Figure 7.1.5*
    shows CycleGAN with identity loss.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 身份损失也会在对抗训练过程中得到优化。*图 7.1.5*展示了带有身份损失的 CycleGAN。
- en: Implementing CycleGAN using Keras
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 实现 CycleGAN
- en: Let us tackle a simple problem that CycleGAN can address. In [Chapter 3](ch03.html
    "Chapter 3. Autoencoders"), *Autoencoders*, we used an autoencoder to colorize
    grayscale images from the CIFAR10 dataset. We can recall that the CIFAR10 dataset
    is made of 50,000 trained data and 10,000 test data samples of 32 × 32 RGB images
    belonging to ten categories. We can convert all color images into grayscale using
    `rgb2gray(RGB)` as discussed in [Chapter 3](ch03.html "Chapter 3. Autoencoders"),
    *Autoencoders*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解决一个 CycleGAN 可以处理的简单问题。在[第 3 章](ch03.html "第 3 章 自动编码器")中，*自动编码器*，我们使用一个自动编码器对
    CIFAR10 数据集中的灰度图像进行上色。我们可以回想起，CIFAR10 数据集由 50,000 个训练数据和 10,000 个测试数据样本组成，所有图像都是
    32 × 32 的 RGB 图像，属于十个类别。我们可以使用 `rgb2gray(RGB)` 将所有彩色图像转换为灰度图像，如[第 3 章](ch03.html
    "第 3 章 自动编码器")中讨论的*自动编码器*。
- en: 'Following on from that, we can use the grayscale train images as source domain
    images and the original color images as the target domain images. It''s worth
    noting that although the dataset is aligned, the input to our CycleGAN is a random
    sample of color images and a random sample of grayscale images. Thus, our CycleGAN
    will not see the train data as aligned. After training, we''ll use the test grayscale
    images to observe the performance of the CycleGAN:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 继承之前，我们可以使用灰度训练图像作为源领域图像，原始彩色图像作为目标领域图像。值得注意的是，尽管数据集是对齐的，但我们输入到 CycleGAN 中的是一组随机的彩色图像样本和一组随机的灰度图像样本。因此，我们的
    CycleGAN 不会将训练数据视为对齐的。训练完成后，我们将使用测试灰度图像来观察 CycleGAN 的性能：
- en: '![Implementing CycleGAN using Keras](img/B08956_07_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 实现 CycleGAN](img/B08956_07_06.jpg)'
- en: 'Figure 7.1.6: The forward cycle generator G, implementation in Keras. The generator
    is a U-Network made of encoder and decoder.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.6：前向循环生成器 G，Keras 中的实现。该生成器是一个由编码器和解码器构成的 U-Net 网络。
- en: As discussed in the previous section, to implement the CycleGAN, we need to
    build two generators and two discriminators. The generator of CycleGAN learns
    the latent representation of the source input distribution and translates this
    representation into target output distribution. This is exactly what autoencoders
    do. However, typical autoencoders similar to the ones discussed in [Chapter 3](ch03.html
    "Chapter 3. Autoencoders"), *Autoencoders*, use an encoder that downsamples the
    input until the bottleneck layer at which point the process is reversed in the
    decoder. This structure is not suitable in some image translation problems since
    many low-level features are shared between the encoder and decoder layers. For
    example, in colorization problems, the form, structure, and edges of the grayscale
    image are the same as in the color image. To circumvent this problem, the CycleGAN
    generators use a **U-Net** [7] structure as shown in *Figure 7.1.6*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一节所讨论的，要实现 CycleGAN，我们需要构建两个生成器和两个判别器。CycleGAN 的生成器学习源输入分布的潜在表示，并将该表示转换为目标输出分布。这正是自动编码器所做的。然而，类似于[第
    3 章](ch03.html "第 3 章 自动编码器")中讨论的典型自动编码器，*自动编码器*，使用一个编码器，该编码器将输入下采样直到瓶颈层，在该层之后的过程在解码器中被反转。这种结构在某些图像翻译问题中并不适用，因为编码器和解码器层之间共享了许多低级特征。例如，在上色问题中，灰度图像的形状、结构和边缘与彩色图像中的相同。为了解决这个问题，CycleGAN
    生成器采用了**U-Net** [7]结构，如*图 7.1.6*所示。
- en: In a U-Net structure, the output of the encoder layer *e* *n-i* is concatenated
    with the output of the decoder layer *d* *i*, where *n* = 4 is the number of encoder/decoder
    layers and *i* = 1, 2 and 3 are layer numbers that share information.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 U-Net 结构中，编码器层 *e* *n-i* 的输出与解码器层 *d* *i* 的输出进行拼接，其中 *n* = 4 是编码器/解码器层的数量，*i*
    = 1, 2 和 3 是共享信息的层编号。
- en: We should note that although the example uses *n* = 4, problems with a higher
    input/output dimensions may require deeper encoder/decoder. The U-Net structure
    enables a free flow of feature-level information between encoder and decoder.
    An encoder layer is made of `Instance Normalization(IN)-LeakyReLU-Conv2D` while
    the decoder layer is made of `IN-ReLU-Conv2D`. The encoder/decoder layer implementation
    is shown in *Listing* *7.1.1* while the generator implementation is shown in *Listing*
    *7.1.2*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，尽管示例中使用了*n* = 4，但具有更高输入/输出维度的问题可能需要更深的编码器/解码器。U-Net 结构允许编码器和解码器之间自由流动特征级别的信息。编码器层由
    `Instance Normalization(IN)-LeakyReLU-Conv2D` 组成，而解码器层由 `IN-ReLU-Conv2D` 组成。编码器/解码器层的实现见
    *Listing* *7.1.1*，生成器的实现见 *Listing* *7.1.2*。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The complete code is available on GitHub:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)'
- en: '**Instance Normalization** (**IN**) is **Batch Normalization** (**BN**) per
    sample of data (that is, IN is BN per image or per feature). In style transfer,
    it''s important to normalize the contrast per sample not per batch. Instance normalization
    is equivalent to contrast normalization. Meanwhile, Batch normalization breaks
    contrast normalization.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**实例归一化**（**IN**）是每个数据样本的 **批归一化**（**BN**）（即 IN 是每个图像或每个特征的 BN）。在风格迁移中，归一化对比度是按每个样本进行的，而不是按批次进行。实例归一化等同于对比度归一化。同时，批归一化会破坏对比度归一化。'
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Remember to install `keras-contrib` before using instance normalization:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用实例归一化之前，请记得安装 `keras-contrib`：
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 7.1.1, `cyclegan-7.1.1.py` shows us the encoder and decoder layers
    implementation in Keras:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 7.1.1*，`cyclegan-7.1.1.py` 展示了 Keras 中编码器和解码器层的实现：'
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 7.1.2, `cyclegan-7.1.1.py`. Generator implementation in Keras:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 7.1.2*，`cyclegan-7.1.1.py`。Keras 中的生成器实现：'
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The discriminator of CycleGAN is similar to vanilla GAN discriminator. The input
    image is downsampled several times (in this example, three times). The final layer
    is a `Dense(1)` layer which predicts the probability that the input is real. Each
    layer is similar to the encoder layer of the generator except that no IN is used.
    However, in large images, computing the image as real or fake with a single number
    turns out to be parameter inefficient and results in poor image quality for the
    generator.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的判别器类似于普通的 GAN 判别器。输入图像被下采样多次（在本示例中，下采样了三次）。最后一层是一个 `Dense(1)` 层，用来预测输入图像是真实的概率。每一层与生成器的编码器层相似，只是没有使用
    IN。然而，在处理大图像时，使用单一的数值来判断图像是“真实”还是“假”在参数上效率较低，且会导致生成器生成的图像质量较差。
- en: The solution is to use PatchGAN [6] which divides the image into a grid of patches
    and use a grid of scalar values to predict the probability that the patches are
    real. The comparison between the vanilla GAN discriminator and a 2 × 2 PatchGAN
    discriminator is shown in *Figure 7.1.7*. In this example, the patches do not
    overlap and meet at their boundaries. However, in general, patches may overlap.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用 PatchGAN [6]，它将图像划分为一个补丁网格，并使用标量值网格来预测这些补丁是否真实。普通 GAN 判别器与 2 × 2 PatchGAN
    判别器的比较见 *图 7.1.7*。在本示例中，补丁之间没有重叠，且在边界处相接。然而，通常情况下，补丁可能会有重叠。
- en: 'We should note that PatchGAN is not introducing a new type of GAN in CycleGAN.
    To improve the generated image quality, instead of having one output to discriminate,
    we have four outputs to discriminate if we used a 2 × 2 PatchGAN. There are no
    changes in the loss functions. Intuitively, this makes sense since the whole image
    will look more real if every patch or section of the image looks real:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，PatchGAN 在 CycleGAN 中并没有引入一种新的 GAN 类型。为了提高生成图像的质量，若使用 2 × 2 PatchGAN，我们不再只有一个输出去进行判别，而是有四个输出进行判别。损失函数没有变化。直观来说，这是有道理的，因为如果图像的每个补丁或部分看起来都是真实的，那么整张图像看起来也会更真实。
- en: '![Implementing CycleGAN using Keras](img/B08956_07_07.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 实现 CycleGAN](img/B08956_07_07.jpg)'
- en: 'Figure 7.1.7: A comparison between GAN and PatchGAN discriminators'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.7：GAN 和 PatchGAN 判别器的比较
- en: 'Following figure shows the discriminator network as implemented in Keras. The
    illustration shows the discriminator determining how likely the input image or
    a patch is a color CIFAR10 image. Since the output image is small at only 32 ×
    32 RGB, a single scalar representing that the image is real is sufficient. However,
    we also evaluate the results when PatchGAN is used. *Listing* *7.1.3* shows the
    function builder for the discriminator:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了在Keras中实现的判别器网络。图示展示了判别器判断输入图像或图像块是否为彩色CIFAR10图像的概率。由于输出图像仅为32×32 RGB的小图像，使用一个标量表示图像是否真实就足够了。然而，我们也评估了使用PatchGAN时的结果。*清单*
    *7.1.3*展示了判别器的函数构建器：
- en: '![Implementing CycleGAN using Keras](img/B08956_07_08.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras实现CycleGAN](img/B08956_07_08.jpg)'
- en: 'Figure 7.1.8: The target discriminator, *D*[y], implementation in Keras. The
    PatchGAN discriminator is shown on the right.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.8：目标判别器*D*[y]在Keras中的实现。PatchGAN判别器显示在右侧。
- en: 'Listing 7.1.3, `cyclegan-7.1.1.py` shows discriminator implementation in Keras:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 清单7.1.3，`cyclegan-7.1.1.py`展示了在Keras中实现的判别器：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using the generator and discriminator builders, we are now able to build the
    CycleGAN. *Listing* *7.1.4* shows the builder function. In line with our discussion
    in the previous section, two generators, `g_source` = *F* and `g_target` = *G*,
    and two discriminators, `d_source` = *D*[x] and `d_target` = *D*[y] are instantiated.
    The forward cycle is *x* *'* = *F*(*G*(*x*)) = `reco_source = g_source(g_target(source_input))`.
    The backward cycle is *y* *'* = *G*(*F*(*y*)) = `reco_target = g_target(g_source(target_input))`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成器和判别器构建器，我们现在可以构建CycleGAN。*清单* *7.1.4*展示了构建器函数。根据前一节中的讨论，实例化了两个生成器，`g_source`
    = *F*和`g_target` = *G*，以及两个判别器，`d_source` = *D*[x]和`d_target` = *D*[y]。正向循环为*x*
    *'* = *F*(*G*(*x*)) = `reco_source = g_source(g_target(source_input))`。反向循环为*y*
    *'* = *G*(*F*(*y*)) = `reco_target = g_target(g_source(target_input))`。
- en: The inputs to the adversarial model are the source and target data while the
    outputs are the outputs of *D*[x] and *D*[y] and the reconstructed inputs, *x'*
    and *y.'* The identity network is not used in this example due to the difference
    between the number of channels of the grayscale image and color image. We use
    the recommended loss weights of
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型的输入是源数据和目标数据，输出是*D*[x]和*D*[y]的输出以及重建的输入，*x'*和*y'*。由于灰度图像和彩色图像在通道数量上的不同，本例中未使用身份网络。我们使用推荐的损失权重：
- en: '![Implementing CycleGAN using Keras](img/B08956_07_021.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras实现CycleGAN](img/B08956_07_021.jpg)'
- en: and
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Implementing CycleGAN using Keras](img/B08956_07_022.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras实现CycleGAN](img/B08956_07_022.jpg)'
- en: for the GAN and cyclic consistency losses respectively. Similar to GANs in the
    previous chapters, we use RMSprop with a learning rate of 2e-4 and decay rate
    of 6e-8 for the optimizer of the discriminators. The learning and decay rate for
    the adversarial is half of the discriminator's.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分别用于GAN和循环一致性损失。与前几章的GAN类似，我们使用学习率为2e-4、衰减率为6e-8的RMSprop优化器来优化判别器。对抗网络的学习率和衰减率是判别器的一半。
- en: 'Listing 7.1.4, `cyclegan-7.1.1.py` shows us the CycleGAN builder in Keras:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 清单7.1.4，`cyclegan-7.1.1.py`展示了我们在Keras中实现的CycleGAN构建器：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We follow the training procedure in *Algorithm* *7.1.1* from the previous section.
    Following listing shows the CycleGAN training. The minor difference between this
    training from the vanilla GAN is there are two discriminators to be optimized.
    However, there is only one adversarial model to optimize. For every 2000 steps,
    the generators save the predicted source and target images. We'll use a batch
    size of 32\. We also tried a batch size of one, but the output quality is almost
    the same and takes a longer amount of time to train (43 ms/image for a batch size
    of one vs. 3.6 ms/image for a batch size of 32 on an NVIDIA GTX 1060).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循前一节中的*算法* *7.1.1*的训练程序。以下清单展示了CycleGAN训练。这与传统GAN训练的细微区别在于，CycleGAN有两个判别器需要优化。然而，只有一个对抗模型需要优化。每2000步，生成器保存预测的源图像和目标图像。我们使用批次大小为32。我们也尝试过批次大小为1，但输出质量几乎相同，只是训练时间更长（批次大小为1时每张图像43毫秒，批次大小为32时每张图像3.6毫秒，使用NVIDIA
    GTX 1060）。
- en: 'Listing 7.1.5, `cyclegan-7.1.1.py` shows us the CycleGAN training routine in
    Keras:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 清单7.1.5，`cyclegan-7.1.1.py`展示了我们在Keras中实现的CycleGAN训练例程：
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, before we can use the CycleGAN to build and train functions, we have
    to perform some data preparation. The modules `cifar10_utils.py` and `other_utils.py`
    load the CIFAR10 train and test data. Please refer to the source code for details
    of these two files. After loading, the train and test images are converted to grayscale
    to generate the source data and test source data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们可以使用CycleGAN来构建和训练功能之前，我们需要进行一些数据准备。`cifar10_utils.py`和`other_utils.py`模块加载CIFAR10的训练数据和测试数据。有关这两个文件的详细信息，请参考源代码。加载数据后，训练和测试图像将被转换为灰度图像，以生成源数据和测试源数据。
- en: Following listing shows how the CycleGAN is used to build and train a generator
    network (`g_target`) for colorization of grayscale images. Since CycleGAN is symmetric,
    we also build and train a second generator network (`g_source`) that converts
    from color to grayscale. Two CycleGAN colorization networks were trained. The
    first use discriminators with a scalar output similar to vanilla GAN. The second
    uses a 2 × 2 PatchGAN.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段展示了如何使用CycleGAN构建和训练一个生成器网络（`g_target`）来对灰度图像进行着色。由于CycleGAN是对称的，我们还构建并训练了第二个生成器网络（`g_source`），将彩色图像转化为灰度图像。两个CycleGAN着色网络已经训练完成。第一个使用类似普通GAN的标量输出判别器；第二个使用2
    × 2的PatchGAN判别器。
- en: 'Listing 7.1.6, `cyclegan-7.1.1.py` shows us the CycleGAN for colorization problem:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1.6中的`cyclegan-7.1.1.py`展示了CycleGAN在着色问题中的应用：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Generator outputs of CycleGAN
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CycleGAN的生成器输出
- en: '*Figure 7.1.9* shows the colorization results of CycleGAN. The source images
    are from the test dataset. For comparison, we show the ground truth and the colorization
    results using a plain autoencoder described in [Chapter 3](ch03.html "Chapter 3. Autoencoders"),
    *Autoencoders*. Generally, all colorized images are perceptually acceptable. Overall,
    it seems that each colorization technique has both its own pros and cons. All
    colorization methods are not consistent with the right color of the sky and vehicle.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1.9*展示了CycleGAN的着色结果。源图像来自测试数据集。为了进行比较，我们展示了地面真实图像以及使用简单自动编码器（第3章，自动编码器）进行着色的结果。总体而言，所有着色图像在视觉上都是可以接受的。总体来看，每种着色技术都有其优缺点。所有着色方法在天空和车辆的真实颜色上都有不一致的地方。'
- en: For example, the sky in the background of the plane (3^(rd) row, 2^(nd) column)
    is white. The autoencoder got it right, but the CycleGAN thinks it is light brown
    or blue. For the 6^(th) row, 6^(th) column, the boat on the dark sea had an overcast
    sky but was colorized with blue sky and blue sea by autoencoder and blue sea and
    white sky by CycleGAN without PatchGAN. Both predictions make sense in the real
    world. Meanwhile, the prediction of CycleGAN with PatchGAN is similar to the ground
    truth. On 2^(nd) to the last row and 2^(nd) column, no method was able to predict
    the red color of the car. On animals, both flavors of CycleGAN have closer colors
    to the ground truth.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，飞机背景中的天空（第3行，第2列）是白色的。自动编码器正确预测了这一点，但CycleGAN认为它是浅棕色或蓝色的。对于第6行，第6列，海上船只的灰暗天空被自动编码器着色为蓝天蓝海，而CycleGAN（没有PatchGAN）则预测为蓝海白天。两种预测在现实世界中都有其合理性。同时，使用PatchGAN的CycleGAN的预测接近真实值。在倒数第二行和第二列，任何方法都未能预测出汽车的红色。在动物图像上，CycleGAN的两种变体都接近真实值的颜色。
- en: 'Since CycleGAN is symmetric, it also predicts the grayscale image given a color
    image. *Figure 7.1.10* shows the color to grayscale conversion performed by the
    two CycleGAN variations. The target images are from the test dataset. Except for
    minor differences in the grayscale shades of some images, the predictions are
    generally accurate:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CycleGAN是对称的，它也能根据彩色图像预测灰度图像。*图7.1.10*展示了两种CycleGAN变体执行的彩色转灰度转换。目标图像来自测试数据集。除了某些图像灰度色调的细微差异外，预测结果通常是准确的：
- en: '![Generator outputs of CycleGAN](img/B08956_07_09.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN生成器输出](img/B08956_07_09.jpg)'
- en: 'Figure 7.1.9: Colorization using different techniques. Shown are the ground
    truth, colorization using autoencoder (Chapter 3, Autoencoders,), colorization
    using CycleGAN with a vanilla GAN discriminator, and colorization using CycleGAN
    with PatchGAN discriminator. Best viewed in color. Original color photo can be
    found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.9：使用不同技术进行的着色。展示了地面真实图像、使用自动编码器（第3章，自动编码器）进行的着色、使用带有普通GAN判别器的CycleGAN进行的着色，以及使用PatchGAN判别器的CycleGAN进行的着色。最佳观看效果为彩色。原始彩色照片可在本书的GitHub库中找到，网址为：https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md。
- en: '![Generator outputs of CycleGAN](img/B08956_07_10.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN的生成器输出](img/B08956_07_10.jpg)'
- en: 'Figure 7.1.10: Color (from Figure 7.1.9) to the grayscale conversion of CycleGAN'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.10：CycleGAN的彩色（图7.1.9中的内容）到灰度转换
- en: 'The reader can run the image translation by using the pretrained models for
    CycleGAN with PatchGAN:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以通过使用预训练的带PatchGAN的CycleGAN模型来运行图像翻译：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: CycleGAN on MNIST and SVHN datasets
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN在MNIST和SVHN数据集上的应用
- en: We're now going to tackle a more challenging problem. Suppose we use MNIST digits
    in grayscale as our source data, and we want to borrow style from SVHN [1] which
    is our target data. The sample data in each domain are shown in *Figure 7.1.11*.
    We can reuse all the build and train functions for CycleGAN that were discussed
    in the previous section to perform style transfer. The only difference is we have
    to add routines for loading MNIST and SVHN data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要解决一个更具挑战性的问题。假设我们使用灰度的MNIST数字作为源数据，并希望借用SVHN [1]（我们的目标数据）中的风格。每个领域的示例数据如*图7.1.11*所示。我们可以重用上一节中讨论的所有构建和训练CycleGAN的函数来执行风格迁移。唯一的区别是我们需要为加载MNIST和SVHN数据添加例程。
- en: 'We introduce module `mnist_svhn_utils.py` to help us with this task. *Listing*
    *7.1.7* shows the initialization and training of the CycleGAN for cross-domain
    transfer. The CycleGAN structure is same as in the previous section except that
    we use a kernel size of 5 since the two domains are drastically different:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了模块`mnist_svhn_utils.py`来帮助我们完成这项任务。*列表* *7.1.7*展示了用于跨领域迁移的CycleGAN的初始化和训练。CycleGAN结构与前一节相同，只是我们使用了5的核大小，因为这两个领域之间有很大的差异：
- en: '![CycleGAN on MNIST and SVHN datasets](img/B08956_07_11.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN在MNIST和SVHN数据集上的表现](img/B08956_07_11.jpg)'
- en: 'Figure 7.1.11: Two different domains with data that are not aligned. Original
    color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1.11：两个不同领域的数据未对齐。原始彩色照片可以在书籍的GitHub库中找到，网址为 https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md。
- en: Note
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Remember to install `keras-contrib` before using instance normalization:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用实例归一化之前，请记得安装`keras-contrib`：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Listing 7.1.7, `cyclegan-7.1.1.py` shows us the CycleGAN for cross-domain style
    transfer between MNIST and SVHN:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1.7中的`cyclegan-7.1.1.py`展示了MNIST和SVHN之间跨领域风格迁移的CycleGAN：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The results for transferring the MNIST from the test dataset to SVHN are shown
    in *Figure 7.1.12*. The generated images have the style of SVHN, but the digits
    are not completely transferred. For example, on the 4^(th) row, digits 3, 1, and
    3 are stylized by CycleGAN. However, on the 3^(rd) row, digits 9, 6, and 6 are
    stylized as 0, 6, 01, 0, 65, and 68 for the CycleGAN without and with PatchGAN
    respectively.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从测试数据集将MNIST迁移到SVHN的结果如*图7.1.12*所示。生成的图像具有SVHN的风格，但数字没有完全迁移。例如，在第4行中，数字3、1和3被CycleGAN进行了风格化。然而，在第3行中，数字9、6和6分别被CycleGAN风格化为0、6、01、0、65和68，分别在没有PatchGAN和使用PatchGAN时的结果不同。
- en: The results of the backward cycle are shown in *Figure 7.1.13*. In this case,
    the target images are from the SVHN test dataset. The generated images have the
    style of MNIST, but the digits are not correctly translated. For example, on the
    1^(st) row, the digits 5, 2, and 210 are stylized as 7, 7, 8, 3, 3, and 1 for
    the CycleGAN without and with PatchGAN respectively.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 向后循环的结果如*图7.1.13*所示。在这种情况下，目标图像来自SVHN测试数据集。生成的图像具有MNIST的风格，但数字没有正确转换。例如，在第1行中，数字5、2和210被CycleGAN分别转换为7、7、8、3、3和1，其中不使用PatchGAN和使用PatchGAN时的结果不同。
- en: In the case of PatchGAN, the output 1 is understandable given the predicted
    MNIST digit is constrained to one digit. There are somehow correct predictions
    like in 2^(nd) row last 3 columns of the SVHN digits, 6, 3, and 4 are converted
    to 6, 3, and 6 by CycleGAN without PatchGAN. However, the outputs on both flavors
    of CycleGAN are consistently single digit and recognizable.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PatchGAN，输出1是可以理解的，因为预测的MNIST数字被限制为一个数字。在SVHN数字的第2行最后3列中，像6、3和4这样的数字被CycleGAN转换为6、3和6，但没有PatchGAN的情况下。然而，CycleGAN的两个版本的输出始终是单一数字并且具有可识别性。
- en: 'The problem exhibited in the conversion from MNIST to SVHN where a digit in
    the source domain is translated to another digit in the target domain is called
    **label flipping** [8]. Although the predictions of CycleGAN are cycle-consistent,
    they are not necessarily semantic consistent. The meaning of digits is lost during
    translation. To address this problem, Hoffman [8] introduced an improved CycleGAN
    called **CyCADA** (**Cycle-Consistent Adversarial Domain Adaptation**). The difference
    is the additional semantic loss term ensures that the prediction is not only cycle-consistent
    but also sematic-consistent:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MNIST 转换到 SVHN 时出现的问题，其中源域中的一个数字被转换为目标域中的另一个数字，称为 **标签翻转** [8]。尽管 CycleGAN
    的预测是循环一致的，但它们不一定是语义一致的。数字的意义在转换过程中丧失。为了解决这个问题，Hoffman [8] 提出了改进版的 CycleGAN，称为
    **CyCADA**（**循环一致对抗领域适配**）。其区别在于额外的语义损失项确保了预测不仅是循环一致的，而且是语义一致的：
- en: '![CycleGAN on MNIST and SVHN datasets](img/B08956_07_12.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 在 MNIST 和 SVHN 数据集上的应用](img/B08956_07_12.jpg)'
- en: 'Figure 7.1.12: Style transfer of test data from the MNIST domain to SVHN. Original
    color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.12：将测试数据从 MNIST 域进行风格迁移到 SVHN。原始彩色照片可以在本书的 GitHub 仓库中找到，网址为 https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md。
- en: '![CycleGAN on MNIST and SVHN datasets](img/B08956_07_13.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 在 MNIST 和 SVHN 数据集上的应用](img/B08956_07_13.jpg)'
- en: 'Figure 7.1.13: Style transfer of test data from SVHN domain to MNIST. Original
    color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.13：将测试数据从 SVHN 域进行风格迁移到 MNIST。原始彩色照片可以在本书的 GitHub 仓库中找到，网址为 https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md。
- en: '![CycleGAN on MNIST and SVHN datasets](img/B08956_07_14.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 在 MNIST 和 SVHN 数据集上的应用](img/B08956_07_14.jpg)'
- en: 'Figure 7.1.14: Forward cycle of CycleGAN with PatchGAN on MNIST (source) to
    SVHN (target). The reconstructed source is similar to the original source. Original
    color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.14：CycleGAN 与 PatchGAN 在 MNIST（源）到 SVHN（目标）的前向循环。重建后的源图像与原始源图像相似。原始彩色照片可以在本书的
    GitHub 仓库中找到，网址为 https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md。
- en: '![CycleGAN on MNIST and SVHN datasets](img/B08956_07_15.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN 在 MNIST 和 SVHN 数据集上的应用](img/B08956_07_15.jpg)'
- en: 'Figure 7.1.15: The backward cycle of CycleGAN with PatchGAN on MNIST (source)
    to SVHN (target). The reconstructed target is not entirely similar to the original
    target. Original color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1.15：CycleGAN 与 PatchGAN 在 MNIST（源）到 SVHN（目标）的反向循环。重建后的目标图像与原始目标图像不完全相似。原始彩色照片可以在本书的
    GitHub 仓库中找到，网址为 https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter7-cross-domain-gan/README.md。
- en: In *Figure 7.1.3*, CycleGAN is described to be cycle consistent. In other words,
    given source *x*, CycleGAN reconstructs the source in the forward cycle as *x*
    *'*. In addition, given target *y*, CycleGAN reconstructs the target in the backward
    cycle as *y* *'*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 7.1.3* 中，CycleGAN 被描述为是循环一致的。换句话说，给定源 *x*，CycleGAN 在前向循环中重建源为 *x* *'*。此外，给定目标
    *y*，CycleGAN 在反向循环中重建目标为 *y* *'*。
- en: '*Figure 7.1.14* shows CycleGAN reconstructing MNIST digits in the forward cycle.
    The reconstructed MNIST digits are almost identical with the source MNIST digits.
    *Figure 7.1.15* shows the CycleGAN reconstructing SVHN digits in the backward
    cycle. Many target images are reconstructed. Some digits are clearly the same
    such as the 2^(nd) row last 2 columns (3 and 4). While some are the same but blurred
    like 1st row first 2 columns (5 and 2). Some digits are transformed to another
    digit although the style remains like 2^(nd) row first two columns (from 33 and
    6 to 1 and an unrecognizable digit).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.1.14* 显示了 CycleGAN 在前向循环中重建 MNIST 数字。重建后的 MNIST 数字几乎与源 MNIST 数字完全相同。*图
    7.1.15* 显示了 CycleGAN 在反向循环中重建 SVHN 数字。许多目标图像被重建。某些数字是完全相同的，例如第二行最后两列（3 和 4）。而有些数字相同但模糊，如第一行前两列（5
    和 2）。有些数字被转换为另一个数字，尽管风格保持不变，例如第二行前两列（从 33 和 6 变为 1 和一个无法识别的数字）。'
- en: 'On a personal note, I encourage you to run the image translation by using the
    pretrained models of CycleGAN with PatchGAN:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在个人层面上，我建议你使用 CycleGAN 的预训练模型与 PatchGAN 进行图像翻译：
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Conclusion
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we've discussed CycleGAN as an algorithm that can be used for
    image translation. In CycleGAN, the source and target data are not necessarily
    aligned. We demonstrated two examples, *grayscale* ↔ *color,* and *MNIST* ↔ *SVHN*.
    Though there are many other possible image translations that CycleGAN can perform.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 CycleGAN 作为一种可以用于图像翻译的算法。在 CycleGAN 中，源数据和目标数据不一定是对齐的。我们展示了两个例子，*灰度*
    ↔ *彩色*，和 *MNIST* ↔ *SVHN*。虽然 CycleGAN 可以执行许多其他可能的图像翻译任务。
- en: In the next chapter, we'll embark on another type of generative model, **Variational
    AutoEncoders** (**VAEs**). VAEs have a similar objective of learning how to generate
    new images (data). They focus on learning the latent vector modeled as a Gaussian
    distribution. We'll demonstrate other similarities in the problem being addressed
    by GANs in the form of conditional VAEs and the disentangling of latent representations
    in VAEs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一类生成模型，**变分自编码器**（**VAE**）。VAE 的目标与生成新图像（数据）相似，重点在于学习作为高斯分布建模的潜在向量。我们还将展示
    GAN 所解决问题的其他相似之处，表现为条件 VAE 和 VAE 中潜在表示的解耦。
- en: References
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Yuval Netzer and others. *Reading Digits in Natural Images with Unsupervised
    Feature Learning*. NIPS workshop on deep learning and unsupervised feature learning.
    Vol. 2011\. No. 2\. 2011([https://www-cs.stanford.edu/~twangcat/papers/nips2011_housenumbers.pdf](https://www-cs.stanford.edu/~twangcat/papers/nips2011_housenumbers.pdf)).
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yuval Netzer 等人. *使用无监督特征学习读取自然图像中的数字*. NIPS 深度学习与无监督特征学习研讨会. Vol. 2011. No.
    2. 2011 ([https://www-cs.stanford.edu/~twangcat/papers/nips2011_housenumbers.pdf](https://www-cs.stanford.edu/~twangcat/papers/nips2011_housenumbers.pdf)).
- en: Zhu, Jun-Yan and others. *Unpaired Image-to-Image Translation Using Cycle-Consistent
    Adversarial Networks*. 2017 IEEE International Conference on Computer Vision (ICCV).
    IEEE, 2017 ([http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)).
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhu, Jun-Yan 等人. *使用循环一致生成对抗网络进行无配对图像到图像翻译*. 2017 IEEE 国际计算机视觉大会 (ICCV). IEEE,
    2017 ([http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)).
- en: Phillip Isola and others. *Image-to-Image Translation with Conditional Adversarial
    Networks*. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
    IEEE, 2017 ([http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf)).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Phillip Isola 等人. *使用条件生成对抗网络的图像到图像翻译*. 2017 IEEE 计算机视觉与模式识别大会 (CVPR). IEEE,
    2017 ([http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf)).
- en: Mehdi Mirza and Simon Osindero. *Conditional Generative Adversarial Nets*. arXiv
    preprint arXiv:1411.1784, 2014([https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)).
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mehdi Mirza 和 Simon Osindero. *条件生成对抗网络*. arXiv 预印本 arXiv:1411.1784, 2014 ([https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)).
- en: Xudong Mao and others. *Least Squares Generative Adversarial Networks*. 2017
    IEEE International Conference on Computer Vision (ICCV). IEEE, 2017([http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)).
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Xudong Mao 等人. *最小二乘生成对抗网络*. 2017 IEEE 国际计算机视觉大会 (ICCV). IEEE, 2017 ([http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)).
- en: Chuan Li and Michael Wand. *Precomputed Real-Time Texture Synthesis with Markovian
    Generative Adversarial Networks*. European Conference on Computer Vision. Springer,
    Cham, 2016([https://arxiv.org/pdf/1604.04382.pdf](https://arxiv.org/pdf/1604.04382.pdf)).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chuan Li 和 Michael Wand. *使用马尔可夫生成对抗网络的预计算实时纹理合成*. 欧洲计算机视觉会议. Springer, Cham,
    2016 ([https://arxiv.org/pdf/1604.04382.pdf](https://arxiv.org/pdf/1604.04382.pdf)).
- en: 'Olaf Ronneberger, Philipp Fischer, and Thomas Brox. *U-Net: Convolutional Networks
    for Biomedical Image Segmentation*. International Conference on Medical image
    computing and computer-assisted intervention. Springer, Cham, 2015([https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf)).'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Olaf Ronneberger, Philipp Fischer 和 Thomas Brox. *U-Net: 用于生物医学图像分割的卷积网络*.
    国际医学图像计算与计算机辅助干预会议。Springer，Cham，2015 ([https://arxiv.org/pdf/1505.04597.pdf](https://arxiv.org/pdf/1505.04597.pdf))。'
- en: 'Judy Hoffman and others. *CyCADA: Cycle-Consistent Adversarial Domain Adaptation*.
    arXiv preprint arXiv:1711.03213, 2017([https://arxiv.org/pdf/1711.03213.pdf](https://arxiv.org/pdf/1711.03213.pdf)).'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Judy Hoffman 等人. *CyCADA: 循环一致性对抗域适应*. arXiv 预印本 arXiv:1711.03213，2017 ([https://arxiv.org/pdf/1711.03213.pdf](https://arxiv.org/pdf/1711.03213.pdf))。'
