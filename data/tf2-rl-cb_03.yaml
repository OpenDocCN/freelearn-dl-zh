- en: '*Chapter 3*: Implementing Advanced RL Algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：实现高级 RL 算法'
- en: This chapter provides short and crisp recipes to implement advanced **Reinforcement
    Learning** (**RL**) algorithms and agents from scratch using **TensorFlow 2.x**.
    It includes recipes to build **Deep-Q-Networks** (**DQN**), **Double and Dueling
    Deep Q-Networks** (**DDQN**, **DDDQN**), **Deep Recurrent Q-Networks** (**DRQN**),
    **Asynchronous Advantage Actor-Critic** (**A3C**), **Proximal Policy Optimization**
    (**PPO**), and **Deep Deterministic Policy Gradients** (**DDPG**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了简短而清晰的食谱，帮助你使用 **TensorFlow 2.x** 从零开始实现先进的 **强化学习**（**RL**）算法和代理。包括构建
    **深度 Q 网络**（**DQN**）、**双重和决斗深度 Q 网络**（**DDQN**，**DDDQN**）、**深度递归 Q 网络**（**DRQN**）、**异步优势演员-评论家**（**A3C**）、**近端策略优化**（**PPO**）和
    **深度确定性策略梯度**（**DDPG**）的食谱。
- en: 'The following recipes are discussed in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下食谱：
- en: Implementing the Deep Q-Learning algorithm, DQN, and Double-DQN agent
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度 Q 学习算法、DQN 和 Double-DQN 代理
- en: Implementing the Dueling DQN agent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现决斗 DQN 代理
- en: Implementing the Dueling Double DQN algorithm and DDDQN agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现决斗双重 DQN 算法和 DDDQN 代理
- en: Implementing the Deep Recurrent Q-Learning algorithm and DRQN agent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度递归 Q 学习算法和 DRQN 代理
- en: Implementing the Asynchronous Advantage Actor-Critic algorithm and A3C agent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现异步优势演员-评论家算法和 A3C 代理
- en: Implementing the Proximal Policy Optimization algorithm and PPO agent
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现近端策略优化算法（Proximal Policy Optimization）和 PPO 代理
- en: Implementing the Deep Deterministic Policy Gradient algorithm and DDPG agent
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度确定性策略梯度算法和 DDPG 代理
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04
    and should work with later versions of Ubuntu if Python 3.6+ is available. With
    Python 3.6+ installed along with the necessary Python packages as listed before
    the start of each of the recipes, the code should run fine on Windows and Mac
    OS X too. It is advised to create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Miniconda or Anaconda installation
    for Python virtual environment management is recommended.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的代码在 Ubuntu 18.04 和 Ubuntu 20.04 上经过广泛测试，并且如果安装了 Python 3.6+，应该可以在之后版本的 Ubuntu
    上运行。安装 Python 3.6+ 以及之前章节所列的必要 Python 包后，代码也应该能够在 Windows 和 Mac OS X 上正常运行。建议创建并使用名为
    `tf2rl-cookbook` 的 Python 虚拟环境来安装包并运行本书中的代码。推荐使用 Miniconda 或 Anaconda 安装 Python
    虚拟环境进行管理。
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个章节中每个食谱的完整代码可以在此处获取：[https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook)。
- en: Implementing the Deep Q-Learning algorithm, DQN, and Double-DQN agent
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度 Q 学习算法、DQN 和 Double-DQN 代理
- en: DQN agent uses a deep neural network to learn the Q-value function. DQN has
    shown itself to be a powerful algorithm for discrete action-space environments
    and problems and is considered to be a notable milestone in the history of deep
    reinforcement learning when DQN mastered Atari Games.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 代理使用深度神经网络学习 Q 值函数。DQN 已经证明自己是离散动作空间环境和问题中一种强大的算法，并且被认为是深度强化学习历史上的一个重要里程碑，当
    DQN 掌握了 Atari 游戏时，成为了一个标志性的成果。
- en: The Double-DQN agent uses two identical deep neural networks that are updated
    differently and so hold different weights. The second neural network is a copy
    of the main neural network from some time in the past (typically from the last
    episode).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Double-DQN 代理使用两个相同的深度神经网络，它们的更新方式不同，因此权重也不同。第二个神经网络是从过去某一时刻（通常是上一轮）复制的主神经网络。
- en: By the end of this recipe, you will have implemented a complete DQN and Double-DQN
    agent from scratch using TensorFlow 2.x that is ready to be trained in any discrete
    action-space RL environment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节结束时，你将从零开始使用 TensorFlow 2.x 实现一个完整的 DQN 和 Double-DQN 代理，能够在任何离散动作空间的强化学习环境中进行训练。
- en: Let's get started.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，你首先需要激活 `tf2rl-cookbook` Conda Python 虚拟环境，并运行 `pip install -r requirements.txt`。如果以下导入语句没有问题，那么你就可以开始了！
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we can begin.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始了。
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'The DQN agent comprises a few components, namely, the `DQN` class, the `Agent`
    class, and the `train` method. Perform the following steps to implement each of
    these components from scratch to build a complete DQN agent using TensorFlow 2.x:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'DQN 智能体包含几个组件，分别是`DQN`类、`Agent`类和`train`方法。执行以下步骤，从零开始实现这些组件，构建一个完整的 DQN 智能体，使用
    TensorFlow 2.x：  '
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个参数解析器来处理脚本的配置输入：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s now create a Tensorboard logger to log useful statistics during the
    agent''s training:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个 Tensorboard 日志记录器，用于在智能体训练过程中记录有用的统计数据：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let''s implement a `ReplayBuffer` class:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个`ReplayBuffer`类：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It''s now time to implement the DQN class that defines the deep neural network
    in TensorFlow 2.x:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '现在是时候实现 DQN 类了，该类定义了 TensorFlow 2.x 中的深度神经网络：  '
- en: '[PRE4]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To get the prediction and action from the DQN, let''s implement the `predict`
    and `get_action` methods:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '为了从 DQN 获取预测和动作，让我们实现`predict`和`get_action`方法：  '
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With the other components implemented, we can begin implementing our `Agent`
    class:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '实现了其他组件后，我们可以开始实现我们的`Agent`类：  '
- en: '[PRE6]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The crux of the Deep Q-learning algorithm is the q-learning update and experience
    replay. Let''s implement that next:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '深度 Q 学习算法的核心是 q 学习更新和经验回放。让我们接下来实现它：  '
- en: '[PRE7]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '下一步至关重要的是实现`train`函数来训练智能体：  '
- en: '[PRE8]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, let''s create the main function to start training the agent:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最后，让我们创建主函数以开始训练智能体：  '
- en: '[PRE9]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To train the DQN agent in the default environment (`CartPole-v0`), execute
    the following command:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '要在默认环境（`CartPole-v0`）中训练 DQN 智能体，请执行以下命令：  '
- en: '[PRE10]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space
    environment using the command-line arguments:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '你还可以使用命令行参数在任何 OpenAI Gym 兼容的离散动作空间环境中训练 DQN 智能体：  '
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, to implement the Double DQN agent, we must modify the `replay_experience`
    method to use Double Q-learning''s update step, as shown here:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '现在，为了实现 Double DQN 智能体，我们必须修改`replay_experience`方法，以使用 Double Q 学习的更新步骤，如下所示：  '
- en: '[PRE12]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, to train the Double DQN agent, save and run the script with the updated
    `replay_experience` method or use the script provided as part of the source code
    for this book:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最后，为了训练 Double DQN 智能体，保存并运行脚本，更新后的`replay_experience`方法，或者使用作为本书源代码一部分提供的脚本：  '
- en: '[PRE13]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let's see how it works.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们看看它是如何工作的。  '
- en: How it works…
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '它是如何工作的...  '
- en: 'Updates to the weights in the DQN are performed as per the following Q learning
    equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'DQN 中的权重更新按以下 Q 学习方程进行：  '
- en: '![](img/Formula_B15074_03_001.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B15074_03_001.jpg)  '
- en: Here, ![](img/Formula_B15074_03_002.png) is the change in the parameters (weights)
    of the DQN, s is the current state, a is the current action, s' is the next state,
    w represents the weights of the DQN, ![](img/Formula_B15074_03_003.png) is the
    discount factor, ![](img/Formula_B15074_03_004.png) is the learning rate, and
    ![](img/Formula_B15074_03_005.png) represents the Q-value for the given state
    (s) and action (a) predicted by the DQN with a weight ![](img/Formula_B15074_03_006.png).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '这里，![](img/Formula_B15074_03_002.png) 是 DQN 参数（权重）的变化，s 是当前状态，a 是当前动作，s'' 是下一个状态，w
    代表 DQN 的权重，![](img/Formula_B15074_03_003.png) 是折扣因子，![](img/Formula_B15074_03_004.png)
    是学习率，![](img/Formula_B15074_03_005.png) 表示由 DQN 预测的给定状态（s）和动作（a）的 Q 值，权重为 ![](img/Formula_B15074_03_006.png)。  '
- en: To understand the difference between the DQN agent and the Double-DQN agent,
    compare the `replay_experience` method in step 8 (DQN) and step 13 (Double DQN).
    You will notice that the key difference lies in calculating the `next_q_values`.
    The DQN agent uses the maximum of the predicted Q-values (which can be an overestimation),
    whereas the Double DQN agent uses the predicted Q-value using two distinct neural
    Q networks. This is done in Double DQN to avoid the problem of overestimating
    the Q-values by the DQN agent.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解 DQN 智能体与 Double-DQN 智能体的区别，请对比第 8 步（DQN）和第 13 步（Double DQN）中的`replay_experience`方法。你会注意到，关键区别在于计算`next_q_values`。DQN
    智能体使用预测的 Q 值的最大值（这可能是高估的），而 Double DQN 智能体使用两个不同神经网络的预测 Q 值。这种方法是为了避免 DQN 智能体高估
    Q 值的问题。  '
- en: Implementing the Dueling DQN agent
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '实现对抗性 DQN 智能体  '
- en: 'A Dueling DQN agent explicitly estimates two quantities through a modified
    network architecture:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗性 DQN 智能体通过修改的网络架构显式地估计两个量：  '
- en: State values, V(*s*)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '状态值，V(*s*)  '
- en: Advantage values, A(*s*, *a*)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '优势值，A(*s*, *a*)  '
- en: The state value estimates the value of being in state s, and the advantage value
    represents the advantage of taking action *a* in state *s*. This key idea of explicitly
    and separately estimating the two quantities enables the Dueling DQN to perform
    better in comparison to DQN. This recipe will walk you through the steps to implement
    a Dueling DQN agent from scratch using TensorFlow 2.x.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值估计了处于状态 s 时的价值，优势值表示在状态 *s* 中采取行动 *a* 的优势。通过显式和独立地估计这两个数量，Dueling DQN 相较于
    DQN 表现得更好。这个配方将带你逐步实现一个从零开始的 Dueling DQN 智能体，使用 TensorFlow 2.x。
- en: Getting ready
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个配方，首先需要激活 `tf2rl-cookbook` Conda Python 虚拟环境，并运行 `pip install -r requirements.txt`。如果以下导入语句没有问题，则说明可以开始了！
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we can begin.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始了。
- en: How to do it…
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'The Dueling DQN agent comprises a few components, namely, the `DuelingDQN`
    class, the `Agent` class, and the `train` method. Perform the following steps
    to implement each of these components from scratch to build a complete Dueling
    DQN agent using TensorFlow 2.x:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Dueling DQN 智能体由几个组件组成，即 `DuelingDQN` 类、`Agent` 类和 `train` 方法。按照以下步骤，从零开始实现这些组件，利用
    TensorFlow 2.x 构建一个完整的 Dueling DQN 智能体：
- en: 'As a first step, let''s create an argument parser to handle command-line configuration
    inputs to the script:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，让我们创建一个参数解析器，用于处理脚本的命令行配置输入：
- en: '[PRE15]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To log useful statistics during the agent''s training, let''s create a TensorBoard
    logger:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在智能体训练过程中记录有用的统计信息，让我们创建一个 TensorBoard 日志记录器：
- en: '[PRE16]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, let''s implement a `ReplayBuffer` class:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个 `ReplayBuffer` 类：
- en: '[PRE17]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s time to implement the DuelingDQN class that defines the deep neural network
    in TensorFlow 2.x:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候实现 DuelingDQN 类，该类在 TensorFlow 2.x 中定义深度神经网络了：
- en: '[PRE18]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To get the prediction and action from the Dueling DQN, let''s implement the
    `predict`, `get_action`, and `train` methods:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从 Dueling DQN 获取预测和动作，让我们实现 `predict`、`get_action` 和 `train` 方法：
- en: '[PRE19]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now begin implementing our `Agent` class:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始实现 `Agent` 类：
- en: '[PRE20]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The crux of the Dueling Deep Q-learning algorithm is the q-learning update
    and experience replay. Let''s implement that next:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dueling Deep Q-learning 算法的关键在于 q-learning 更新和经验回放。接下来，让我们实现这些：
- en: '[PRE21]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个关键步骤是实现 `train` 函数来训练智能体：
- en: '[PRE22]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, let''s create the main function to start training the agent:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建主函数来启动智能体的训练：
- en: '[PRE23]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To train the Dueling DQN agent in the default environment (`CartPole-v0`),
    execute the following command:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在默认环境（`CartPole-v0`）中训练 Dueling DQN 智能体，请执行以下命令：
- en: '[PRE24]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space
    environment using the command-line arguments:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以在任何与 OpenAI Gym 兼容的离散动作空间环境中训练 DQN 智能体，使用命令行参数：
- en: '[PRE25]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let's see how it works.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看它是如何工作的。
- en: How it works…
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The Dueling-DQN agent differs from the DQN agent in terms of the neural network
    architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Dueling-DQN 智能体在神经网络架构上与 DQN 智能体有所不同。
- en: 'The differences are summarized in the following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异在下图中进行了总结：
- en: '![Figure 3.1 – DQN and Dueling-DQN compared ](img/B15074_03_001.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – DQN 和 Dueling-DQN 比较](img/B15074_03_001.jpg)'
- en: Figure 3.1 – DQN and Dueling-DQN compared
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – DQN 和 Dueling-DQN 的比较
- en: The DQN (top half of the diagram) has a linear architecture and predicts a single
    quantity (Q(s, a)), whereas the Dueling-DQN has a bifurcation in the last layer
    and predicts multiple quantities.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: DQN（图的上半部分）具有线性架构，预测一个单一的数量（Q(s, a)），而 Dueling-DQN 在最后一层有一个分叉，预测多个数量。
- en: Implementing the Dueling Double DQN algorithm and DDDQN agent
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Dueling Double DQN 算法和 DDDQN 智能体
- en: '**Dueling Double DQN** (**DDDQN**) combines the benefits of both Double Q-learning
    and Dueling architecture. Double Q-learning corrects DQN from overestimating the
    action values. The Dueling architecture uses a modified architecture to separately
    learn the state value function (V) and the advantage function (A). This explicit
    separation allows the algorithm to learn faster, especially when there are many
    actions to choose from and when the actions are very similar to each other. The
    dueling architecture enables the agent to learn even when only one action in a
    state has been taken, as it can update and estimate the state value function,
    unlike the DQN agent, which cannot learn from actions that were not taken yet.
    By the end of this recipe, you will have a complete implementation of the DDDQN
    agent.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dueling Double DQN**（**DDDQN**）结合了 Double Q-learning 和 Dueling 架构的优势。Double
    Q-learning 修正了 DQN 过高估计动作值的问题。Dueling 架构使用修改后的架构，分别学习状态值函数（V）和优势函数（A）。这种显式分离使算法能够更快学习，特别是在有许多动作可选且动作之间非常相似的情况下。Dueling
    架构使智能体即使在一个状态下只采取了一个动作时也能进行学习，因为它可以更新和估计状态值函数，这与 DQN 智能体不同，后者无法从尚未采取的动作中学习。在完成这个食谱后，你将拥有一个完整的
    DDDQN 智能体实现。'
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好了吗？
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，你首先需要激活 `tf2rl-cookbook` Conda Python 虚拟环境并运行 `pip install -r requirements.txt`。如果以下导入语句没有问题，那么你就可以开始了！
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We are ready to begin!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好了，开始吧！
- en: How to do it…
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'The DDDQN agent combines the ideas in DQN, Double DQN and the Dueling DQN.
    Perform the following steps to implement each of these components from scratch
    to build a complete Dueling Double DQN agent using TensorFlow 2.x:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DDDQN 智能体结合了 DQN、Double DQN 和 Dueling DQN 中的思想。执行以下步骤，从头开始实现这些组件，以便使用 TensorFlow
    2.x 构建一个完整的 Dueling Double DQN 智能体：
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个参数解析器来处理脚本的配置输入：
- en: '[PRE27]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, let''s create a Tensorboard logger to log useful statistics during the
    agent''s training process:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个 Tensorboard 日志记录器，用于记录智能体训练过程中的有用统计数据：
- en: '[PRE28]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s implement a `ReplayBuffer`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个 `ReplayBuffer`：
- en: '[PRE29]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It''s now time to implement the Dueling DQN class that defines the neural network
    as per the dueling architecture to which we will add Double DQN updates in later
    steps:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候实现 Dueling DQN 类了，它将按照 Dueling 架构定义神经网络，后续我们会在此基础上添加 Double DQN 更新：
- en: '[PRE30]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To get the prediction and action from the Dueling DQN, let''s implement the
    `predict` and `get_action` methods:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从 Dueling DQN 获取预测和动作，让我们实现 `predict` 和 `get_action` 方法：
- en: '[PRE31]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With the other components implemented, we can begin implementing our `Agent`
    class:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他组件实现完成后，我们可以开始实现 `Agent` 类：
- en: '[PRE32]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The main elements in the Dueling Double Deep Q-learning algorithm are the Q-learning
    update and experience replay. Let''s implement that next:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dueling Double Deep Q-learning 算法的主要元素是 Q-learning 更新和经验回放。接下来我们将实现这些：
- en: '[PRE33]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个关键步骤是实现 `train` 函数来训练智能体：
- en: '[PRE34]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, let''s create the main function to start training the agent:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建主函数来开始训练智能体：
- en: '[PRE35]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To train the DQN agent in the default environment (`CartPole-v0`), execute
    the following command:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在默认环境（`CartPole-v0`）中训练 DQN 智能体，请执行以下命令：
- en: '[PRE36]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can also train the Dueling Double DQN agent in any OpenAI Gym-compatible
    discrete action-space environment using the command-line arguments:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以在任何兼容 OpenAI Gym 的离散动作空间环境中使用命令行参数训练 Dueling Double DQN 智能体：
- en: '[PRE37]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works…
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The Dueling Double DQN architecture combines the advancements introduced by
    the Double DQN and Dueling architectures together.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Dueling Double DQN 架构结合了 Double DQN 和 Dueling 架构引入的进展。
- en: Implementing the Deep Recurrent Q-Learning algorithm and DRQN agent
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度递归 Q-learning 算法和 DRQN 智能体
- en: DRQN uses a recurrent neural network to learn the Q-value function. DRQN is
    more suited for reinforcement learning in environments with partial observability.
    The recurrent network layers in the DRQN allow the agent to learn by integrating
    information from a temporal sequence of observations. For example, DRQN agents
    can infer the velocity of moving objects in the environment without any changes
    to their inputs (for example, no frame stacking is required). By the end of this
    recipe, you will have a complete DRQN agent ready to be trained in an RL environment
    of your choice.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DRQN使用递归神经网络来学习Q值函数。DRQN更适合在部分可观察环境中进行强化学习。DRQN中的递归网络层允许智能体通过整合时间序列的观察信息来进行学习。例如，DRQN智能体可以推测环境中移动物体的速度，而无需任何输入的变化（例如，不需要帧堆叠）。在完成这个配方后，您将拥有一个完整的DRQN智能体，准备在您选择的强化学习环境中进行训练。
- en: Getting ready
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个配方，您首先需要激活`tf2rl-cookbook` Conda Python 虚拟环境，并运行`pip install -r requirements.txt`。如果以下导入语句没有问题，那么您就准备好开始了！
- en: '[PRE38]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Let's begin!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么做…
- en: 'The Dueling Double DQN agent combines the ideas in DQN, Double DQN, and the
    Dueling DQN. Perform the following steps to implement each of these components
    from scratch to build a complete DRQN agent using TensorFlow 2.x:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗双重DQN智能体结合了DQN、双重DQN和对抗DQN的理念。执行以下步骤，从零开始实现这些组件，以使用TensorFlow 2.x构建完整的DRQN智能体：
- en: 'First, create an argument parser to handle configuration inputs to the script:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个参数解析器来处理脚本的配置输入：
- en: '[PRE39]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s log useful statistics during the agent''s training using Tensorboard:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在智能体的训练过程中使用Tensorboard记录有用的统计信息：
- en: '[PRE40]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, let''s implement a `ReplayBuffer`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个`ReplayBuffer`：
- en: '[PRE41]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It''s now time to implement the DRQN class that defines the deep neural network
    using TensorFlow 2.x:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候实现定义深度神经网络的DRQN类了，使用的是TensorFlow 2.x：
- en: '[PRE42]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To get the prediction and action from the DRQN, let''s implement the `predict`
    and `get_action` methods:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从DRQN获取预测和动作，让我们实现`predict`和`get_action`方法：
- en: '[PRE43]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With the other components implemented, we can begin implementing our `Agent`
    class:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现了其他组件后，我们可以开始实现我们的`Agent`类：
- en: '[PRE44]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In addition to the `train` method in the DRQN class that we implemented in
    step 6, the crux of the deep recurrent Q-learning algorithm is the q-learning
    update and experience replay. Let''s implement that next:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了我们在第6步中实现的DRQN类中的`train`方法外，深度递归Q学习算法的核心是Q学习更新和经验回放。接下来，让我们实现这一部分：
- en: '[PRE45]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Since the DRQN agent uses recurrent states, let''s implement the `update_states`
    method to update the recurrent state of the agent:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于DRQN智能体使用递归状态，让我们实现`update_states`方法来更新智能体的递归状态：
- en: '[PRE46]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个关键步骤是实现`train`函数来训练智能体：
- en: '[PRE47]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, let''s create the main training loop for the agent:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们为智能体创建主要的训练循环：
- en: '[PRE48]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To train the DRQN agent in the default environment (`CartPole-v0`), execute
    the following command:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在默认环境（`CartPole-v0`）中训练DRQN智能体，请执行以下命令：
- en: '[PRE49]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space
    environment using the command-line arguments:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以使用命令行参数在任何OpenAI Gym兼容的离散动作空间环境中训练DQN智能体：
- en: '[PRE50]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How it works…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The DRQN agent uses an LSTM layer, which adds a recurrent learning capability
    to the agent. The LSTM layer is added to the agent's network in step 5 of the
    recipe. The other steps in the recipe have similar components as the DQN agent.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: DRQN智能体使用LSTM层，这为智能体增加了递归学习能力。LSTM层在配方的第5步中添加到智能体的网络中。配方中的其他步骤与DQN智能体类似。
- en: Implementing the Asynchronous Advantage Actor-Critic algorithm and A3C agent
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现异步优势演员评论家算法和A3C智能体
- en: The A3C algorithm builds upon the Actor-Critic class of algorithms by using
    a neural network to approximate the actor (and critic). The actor learns the policy
    function using a deep neural network, while the critic estimates the value function.
    The asynchronous nature of the algorithm allows the agent to learn from different
    parts of the state space, allowing parallel learning and faster convergence. Unlike
    DQN agents, which use an experience replay memory, the A3C agent uses multiple
    workers to gather more samples for learning. By the end of this recipe, you will
    have a complete script to train an A3C agent for any continuous action valued
    environment of your choice!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: A3C 算法在 Actor-Critic 类算法的基础上构建，通过使用神经网络来逼近演员（actor）和评论家（critic）。演员使用深度神经网络学习策略函数，而评论家则估计价值函数。算法的异步性质使得智能体能够从状态空间的不同部分进行学习，从而实现并行学习和更快的收敛。与使用经验回放记忆的
    DQN 智能体不同，A3C 智能体使用多个工作线程来收集更多样本进行学习。在本配方的结尾，你将拥有一个完整的脚本，可以用来训练一个适用于任何连续动作值环境的
    A3C 智能体！
- en: Getting ready
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个配方，你首先需要激活 `tf2rl-cookbook` Conda Python 虚拟环境并运行 `pip install -r requirements.txt`。如果以下的导入语句没有问题，那就说明你已经准备好开始了！
- en: '[PRE51]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Now we can begin.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始了。
- en: How to do it…
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We will implement an **Asynchronous, Advantage Actor-Critic** (**A3C**) algorithm
    by making use of Python''s multiprocessing and multithreading capabilities. The
    following steps will help you to implement a complete A3C agent from scratch using
    TensorFlow 2.x:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过利用 Python 的多进程和多线程功能实现一个 **异步优势演员评论家（A3C）** 算法。以下步骤将帮助你从零开始使用 TensorFlow
    2.x 实现一个完整的 A3C 智能体：
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个参数解析器，用来处理脚本的配置输入：
- en: '[PRE52]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let''s now create a Tensorboard logger to log useful statistics during the
    agent''s training:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建一个 Tensorboard 日志记录器，以便在智能体训练过程中记录有用的统计信息：
- en: '[PRE53]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To have a count of the global episode number, let''s define a global variable:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了统计全局回合数，让我们定义一个全局变量：
- en: '[PRE54]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can now focus on implementing the `Actor` class, which will contain a neural
    network-based policy to act in the environments:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以集中精力实现 `Actor` 类，它将包含一个基于神经网络的策略来在环境中执行动作：
- en: '[PRE55]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To get an action from the actor given a state, let''s define the `get_action`
    method:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在给定状态下从演员获取动作，让我们定义 `get_action` 方法：
- en: '[PRE56]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, to compute the loss, we need to calculate the log of the policy (probability)
    density function:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了计算损失，我们需要计算策略（概率）密度函数的对数：
- en: '[PRE57]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Let''s now use the `log_pdf` method to compute the actor loss:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用 `log_pdf` 方法来计算演员损失：
- en: '[PRE58]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'As the final step in the `Actor` class implementation, let''s define the `train`
    method:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为 `Actor` 类实现的最后一步，让我们定义 `train` 方法：
- en: '[PRE59]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'With the `Actor` class defined, we can move on to define the `Critic` class:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义好 `Actor` 类后，我们可以继续定义 `Critic` 类：
- en: '[PRE60]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, let''s define the `train` method and a `compute_loss` method to train
    the critic:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义 `train` 方法和一个 `compute_loss` 方法来训练评论家：
- en: '[PRE61]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'It is time to implement the `A3CWorker` class based on Python''s Thread interface:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是时候基于 Python 的线程接口实现 `A3CWorker` 类了：
- en: '[PRE62]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We will be using **n-step Temporal Difference (TD)** learning updates. Therefore,
    let''s define a method to calculate the n-step TD target:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 **n 步时间差分 (TD)** 学习更新。因此，让我们定义一个方法来计算 n 步 TD 目标：
- en: '[PRE63]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We will also need to calculate the advantage values. The advantage value in
    its simplest form is easy to implement:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要计算优势值。优势值的最简单形式很容易实现：
- en: '[PRE64]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We will split the implementation of the `train` method into the following two
    steps. First, let''s implement the outer loop:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把 `train` 方法的实现分为以下两个步骤。首先，让我们实现外部循环：
- en: '[PRE65]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this step, we will complete the `train` method implementation:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将完成 `train` 方法的实现：
- en: '[PRE66]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The run method for the `A3CWorker` thread will simply be the following:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`A3CWorker` 线程的 run 方法将是以下内容：'
- en: '[PRE67]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, let''s implement the `Agent` class:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现 `Agent` 类：
- en: '[PRE68]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The A3C agent makes use of several concurrent workers. In order to update each
    of the workers to update the A3C agent, the following code is necessary:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A3C 智能体利用多个并发工作线程。为了更新每个工作线程以更新 A3C 智能体，以下代码是必要的：
- en: '[PRE69]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'With that, our A3C agent implementation is complete, and we are ready to define
    our main function:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们的 A3C 智能体实现就完成了，接下来我们准备定义我们的主函数：
- en: '[PRE70]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works…
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In simple terms, the crux of the A3C algorithm can be summarized in the following
    sequence of steps for each iteration:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，A3C算法的核心可以通过以下步骤总结，每个迭代中都会执行这些步骤：
- en: '![Figure 3.2 – Updating steps in the A3C agent learning iteration ](img/B15074_03_002.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – A3C智能体学习迭代中的更新步骤](img/B15074_03_002.jpg)'
- en: Figure 3.2 – Updating steps in the A3C agent learning iteration
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – A3C智能体学习迭代中的更新步骤
- en: The steps repeat again from top to bottom for the next iteration and so on until
    convergence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤会从上到下重复进行，直到收敛。
- en: Implementing the Proximal Policy Optimization algorithm and PPO agent
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现邻近策略优化算法和PPO智能体
- en: The **Proximal Policy Optimization** (**PPO**) algorithm builds upon the work
    of **Trust Region Policy Optimization** (**TRPO**) to constrain the new policy
    to be within a trust region from the old policy. PPO simplifies the implementation
    of this core idea by using a clipped surrogate objective function that is easier
    to implement, yet quite powerful and efficient. It is one of the most widely used
    RL algorithms, especially for continuous control problems. By the end of this
    recipe, you will have built a PPO agent that you can train in your RL environment
    of choice.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**邻近策略优化**（**PPO**）算法是在**信任域策略优化**（**TRPO**）的基础上发展而来的，通过将新策略限制在旧策略的信任区域内。PPO通过使用一个剪切的替代目标函数简化了这一核心思想的实现，这个目标函数更容易实现，但仍然非常强大和高效。它是最广泛使用的强化学习算法之一，尤其适用于连续控制问题。在完成本教程后，你将构建一个PPO智能体，并能在你选择的强化学习环境中进行训练。'
- en: Getting ready
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本教程，你需要先激活`tf2rl-cookbook` Conda Python虚拟环境并运行`pip install -r requirements.txt`。如果以下导入语句没有问题，你就准备好开始了！
- en: '[PRE71]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We are ready to get started.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好开始了。
- en: How to do it…
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The following steps will help you to implement a complete PPO agent from scratch
    using TensorFlow 2.x:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你从头开始使用TensorFlow 2.x实现一个完整的PPO智能体：
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个参数解析器来处理脚本的配置输入：
- en: '[PRE72]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Next, let''s create a Tensorboard logger to log useful statistics during the
    agent''s training:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个Tensorboard日志记录器，以便在智能体训练过程中记录有用的统计信息：
- en: '[PRE73]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We can now focus on implementing the `Actor` class, which will contain a neural
    network-based policy to act:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以集中精力实现`Actor`类，它将包含一个基于神经网络的策略来执行动作：
- en: '[PRE74]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'To get an action from the actor given a state, let''s define the `get_action`
    method:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从演员那里获得一个动作给定一个状态，先定义`get_action`方法：
- en: '[PRE75]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Next, to compute the loss, we need to calculate the log of the policy (probability)
    density function:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了计算损失，我们需要计算策略（概率）密度函数的对数：
- en: '[PRE76]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let''s now use the `log_pdf` method to compute the actor loss:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用`log_pdf`方法来计算演员损失：
- en: '[PRE77]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'As the final step in the `Actor` class implementation, let''s define the `train`
    method:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为`Actor`类实现的最后一步，让我们定义`train`方法：
- en: '[PRE78]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'With the `Actor` class defined, we can move on to define the `Critic` class:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义好`Actor`类后，我们可以继续定义`Critic`类：
- en: '[PRE79]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Next, let''s define the `train` method and a `compute_loss` method to train
    the critic:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义`train`方法和`compute_loss`方法来训练评论员：
- en: '[PRE80]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'It is now time to implement the PPO `Agent` class:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候实现PPO `Agent`类了：
- en: '[PRE81]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We will be using the **Generalized Advantage Estimates** (**GAE**). Let''s
    implement a method to calculate the GAE target values:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用**广义优势估计**（**GAE**）。让我们实现一个方法来计算GAE目标值：
- en: '[PRE82]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We will now split the implementation of the `train` method. First, let''s implement
    the outer loop:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将拆分`train`方法的实现。首先，让我们实现外部循环：
- en: '[PRE83]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'In this step, we will start the inner loop (per episode) implementation and
    finish it in the next couple of steps:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将开始内循环（每个回合）实现，并在接下来的几个步骤中完成它：
- en: '[PRE84]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'In this step, we will use the value predictions made by the PPO algorithm to
    prepare for the policy update process:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用PPO算法所做的价值预测来为策略更新过程做准备：
- en: '[PRE85]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'In this step, we will implement the PPO algorithm''s policy update steps. These
    happen inside the inner loop whenever enough of an agent''s trajectory information
    is available in the form of sampled experience batches:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将实现PPO算法的策略更新步骤。这些步骤发生在内循环中，每当足够的智能体轨迹信息以采样经验批次的形式可用时：
- en: '[PRE86]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'As the final step of the `train` method, we will reset the intermediate variables
    and print a summary of the episode reward obtained by the agent:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为 `train` 方法的最后一步，我们将重置中间变量，并打印出智能体获得的每个回合奖励的总结：
- en: '[PRE87]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: With that, our PPO agent implementation is complete, and we are ready to define
    our main function to start training!
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些，我们的 PPO 智能体实现就完成了，接下来我们可以定义主函数来开始训练！
- en: '[PRE88]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: How it works…
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The PPO algorithm uses clipping to form a surrogate loss function, and uses
    multiple epochs of **Stochastic Gradient Decent/Ascent** (**SGD**) optimization
    per the policy update. The clipping introduced by PPO reduces the effective change
    that can be applied to the policy, thereby improving the stability of the policy
    while learning.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 算法使用截断来形成一个替代损失函数，并根据策略更新使用多个周期的 **随机梯度下降/上升**（**SGD**）优化。PPO 引入的截断减少了对策略的有效变化，从而提高了策略在学习过程中的稳定性。
- en: The PPO agent uses actor(s) to collect samples from the environment using the
    latest policy parameters. The loop defined in step 15 of the recipe samples a
    mini-batch of experience and trains the network for n epochs (passed as the `--epoch`
    argument to the script) using the clipped surrogate objective function. The process
    is then repeated with new samples of experiences.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 智能体使用演员（Actor）根据最新的策略参数从环境中收集样本。第 15 步中定义的循环会从经验中采样一个小批量，并使用截断的替代目标函数在 n
    个周期（通过 `--epoch` 参数传递给脚本）中训练网络。然后使用新的经验样本重复此过程。
- en: Implementing the Deep Deterministic Policy Gradient algorithm and DDPG agent
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度确定性策略梯度算法和 DDPG 智能体
- en: '**Deterministic Policy Gradient (DPG)** is a type of Actor-Critic RL algorithm
    that uses two neural networks: one for estimating the action value function, and
    the other for estimating the optimal target policy. The **Deep Deterministic Policy
    Gradient** (**DDPG**) agent builds upon the idea of DPG and is quite efficient
    compared to vanilla Actor-Critic agents due to the use of deterministic action
    policies. By completing this recipe, you will have access to a powerful agent
    that can be trained efficiently in a variety of RL environments.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**确定性策略梯度（DPG）** 是一种演员-评论家强化学习算法，使用两个神经网络：一个用于估计动作价值函数，另一个用于估计最优目标策略。**深度确定性策略梯度**（**DDPG**）智能体建立在
    DPG 的基础上，并且由于使用了确定性动作策略，相较于普通的演员-评论家智能体，它在效率上更高。通过完成这个食谱，你将获得一个强大的智能体，可以在多种强化学习环境中高效训练。'
- en: Getting ready
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，首先需要激活 `tf2rl-cookbook` Conda Python 虚拟环境，并运行 `pip install -r requirements.txt`。如果以下导入语句没有问题，那么你已经准备好开始了！
- en: '[PRE89]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Now we can begin.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始了。
- en: How to do it…
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'The following steps will help you to implement a complete DDPG agent from scratch
    using TensorFlow 2.x:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你从零开始使用 TensorFlow 2.x 实现一个完整的 DDPG 智能体：
- en: 'Let''s first create an argument parser to handle command-line configuration
    inputs to the script:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先创建一个参数解析器来处理脚本的命令行配置输入：
- en: '[PRE90]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Let''s create a Tensorboard logger to log useful statistics during the agent''s
    training:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个 Tensorboard 日志记录器，用来记录在智能体训练过程中的有用统计信息：
- en: '[PRE91]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Let''s now implement an experience replay memory:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们实现一个经验回放内存：
- en: '[PRE92]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We can now focus on implementing the `Actor` class, which will contain a neural
    network-based policy to act:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以集中精力实现 `Actor` 类，它将包含一个基于神经网络的策略进行操作：
- en: '[PRE93]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'To get an action from the actor given a state, let''s define the `get_action`
    method:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了根据状态从演员获取动作，让我们定义 `get_action` 方法：
- en: '[PRE94]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, we''ll implement a predict function to return the predictions made by
    the actor network:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个预测函数来返回演员网络的预测结果：
- en: '[PRE95]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'As the final step in the `Actor` class implementation, let''s define the `train`
    method:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为 `Actor` 类实现的最后一步，让我们定义 `train` 方法：
- en: '[PRE96]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'With the `Actor` class defined, we can move on to define the `Critic` class:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了 `Actor` 类后，我们可以继续定义 `Critic` 类：
- en: '[PRE97]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'In this step, we will be implementing a method to calculate the gradients of
    the Q function:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将实现一个方法来计算 Q 函数的梯度：
- en: '[PRE98]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'As a convenience method, let''s also define a `predict` function to return
    the critic network''s prediction:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为一个便捷方法，我们还可以定义一个 `predict` 函数来返回评论家网络的预测结果：
- en: '[PRE99]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Next, let''s define the `train` method and a `compute_loss` method to train
    the critic:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义 `train` 方法和 `compute_loss` 方法来训练评论家：
- en: '[PRE100]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'It is now time to implement the DDPG `Agent` class:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候实现DDPG的`Agent`类了：
- en: '[PRE101]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Let''s now implement the `update_target` method to update the actor and critic
    network''s weights with that of the respective target networks:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们实现`update_target`方法，用目标网络的权重来更新演员和评论员网络的权重：
- en: '[PRE102]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Next, let''s implement a helper method to calculate the TD targets:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个辅助方法来计算TD目标：
- en: '[PRE103]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The purpose of the Deterministic Policy Gradient algorithm is to add noise
    to the actions sampled from the deterministic policy. Let''s use the **Ornstein-Uhlenback**
    (**OU**) process to generate noise:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定性策略梯度算法的目的是向从确定性策略中采样的动作添加噪声。我们将使用**奥恩斯坦-乌伦贝克**（**OU**）过程来生成噪声：
- en: '[PRE104]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'In this step, we will use experience replay to update the actor and critic:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将使用经验回放来更新演员网络和评论员网络：
- en: '[PRE105]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'With all the components we have implemented, we are now ready to put them together
    in the `train` method:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用我们实现的所有组件，我们现在准备将它们组合在一起，放入`train`方法中：
- en: '[PRE106]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: With that, our DDPG agent implementation is complete, and we are ready to define
    our main function to start training!
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 至此，我们的DDPG代理实现已经完成，我们准备定义主函数以开始训练！
- en: '[PRE107]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: How it works…
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The DDPG agent estimates two quantities – the Q-value function and the optimal
    policy. DDPG combines the ideas introduced in DQN and DPG. DDPG uses a policy
    gradient update rule in addition to the ideas introduced in DQN, as can be seen
    in the update steps defined in step 14.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG代理估计两个量——Q值函数和最优策略。DDPG结合了DQN和DPG中介绍的思想。DDPG除了采用DQN中引入的思想外，还使用了策略梯度更新规则，如第14步中定义的更新步骤所示。
