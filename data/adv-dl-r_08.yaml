- en: Image Classification Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像分类
- en: '**Convolutional neural networks** (**CNNs**) are popular deep neural networks
    and are considered to be the gold standard for large-scale image classification
    tasks. Applications involving CNNs include image recognition and classification,
    natural language processing, medical image classification, and many others. In
    this chapter, we will continue with supervised learning situations where a response
    variable exists. This chapter provides steps for applying image classification
    and recognition using convolutional neural networks with an easy-to-follow practical
    example involving fashion-related **Modified National Institute of Standards and
    Technology** (**MNIST**) data. We also make use of images of fashion items downloaded
    from the internet to explore the generalization potential of the classification
    model that we develop.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是流行的深度神经网络，并且被认为是大规模图像分类任务的金标准。涉及CNN的应用包括图像识别与分类、自然语言处理、医学图像分类等。本章我们将继续讨论有响应变量的监督学习情境。本章提供了使用卷积神经网络进行图像分类与识别的步骤，并通过一个易于跟随的实践示例，利用与时尚相关的**修改版国家标准与技术研究所**（**MNIST**）数据进行讲解。我们还利用从互联网上下载的时尚物品图像，探讨我们开发的分类模型的泛化潜力。'
- en: 'More specifically in this chapter, we cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将更具体地涵盖以下主题：
- en: Data preparation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Layers in convolutional neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络中的层
- en: Fitting the model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Model evaluation and prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Performance optimization tips and best practices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化技巧与最佳实践
- en: Summary
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Data preparation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'In this chapter, we will make use of the Keras and EBImage libraries:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Keras和EBImage库：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s get started by looking at some images downloaded from the internet.
    There are 20 images that include fashion articles such as shirts, bags, sandals,
    dresses, and others. These images were obtained using a Google search. We will
    try to develop an image recognition and classification model that recognizes these
    images and classifies them in appropriate categories. And to develop such a model,
    we will make use of the fashion-MNIST database of fashion articles:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始看一些从互联网上下载的图像。这里有20张图像，包括衬衫、包、凉鞋、连衣裙等时尚物品。这些图像是通过Google搜索获得的。我们将尝试开发一个图像识别与分类模型，识别这些图像并将其分类到适当的类别中。为了开发这样的模型，我们将使用fashion-MNIST时尚物品数据库：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The 20 images of fashion items downloaded from the internet are shown as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从互联网上下载的20张时尚物品图像如下所示：
- en: '![](img/909db000-7ce0-40ca-9380-3972d160ed9a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/909db000-7ce0-40ca-9380-3972d160ed9a.png)'
- en: Next, let's look at the fashion-MNIST data that contains a much larger collection
    of images of such fashion items.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下包含大量此类时尚物品图像的fashion-MNIST数据。
- en: Fashion-MNIST data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fashion-MNIST 数据
- en: 'We can access fashion-MNIST data from Keras using the`dataset_fashion_mnist`
    function. Take a look at the following code and its output:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`dataset_fashion_mnist`函数从Keras获取fashion-MNIST数据。看看以下代码及其输出：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Looking at the structure of the preceding data, we see that it contains train
    data with 60,000 images and test data with 10,000 images. All these images are
    28 x 28 grayscale images. We know from the previous chapter that images can be
    represented as numeric data based on color and intensity. The independent variable
    x contains the intensity values, and the dependent variable y contains labels
    from 0 to 9.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的数据结构来看，我们看到它包含60,000张训练图像和10,000张测试图像。所有这些图像都是28 x 28的灰度图像。我们从上一章了解到，图像可以基于颜色和强度表示为数字数据。自变量x包含强度值，因变量y包含从0到9的标签。
- en: 'The 10 different fashion items in the fashion-MNIST dataset are labelled from
    0 to 9, as shown in the following table:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: fashion-MNIST 数据集中的10个不同时尚物品被标记为从0到9，如下表所示：
- en: '| Label | Description |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 描述 |'
- en: '| --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | T-shirt/Top |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 0 | T恤/上衣 |'
- en: '| 1 | Trouser |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 长裤 |'
- en: '| 2 | Pullover |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 套头衫 |'
- en: '| 3 | Dress |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 连衣裙 |'
- en: '| 4 | Coat |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 外套 |'
- en: '| 5 | Sandal |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 凉鞋 |'
- en: '| 6 | Shirt |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 衬衫 |'
- en: '| 7 | Sneaker |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 运动鞋 |'
- en: '| 8 | Bag |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 包 |'
- en: '| 9 | Ankle Boot |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 脚踝靴 |'
- en: Looking at the preceding table, we may observe that developing a classification
    model for these images will be challenging as some categories will be difficult
    to differentiate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的表格中，我们可以观察到，为这些图像开发分类模型将具有挑战性，因为某些类别将很难区分。
- en: Train and test data
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练与测试数据
- en: 'We extract train image data, store it in `trainx`, and store the respective
    labels in `trainy`. In a similar fashion, we create `testx` and `testy` from the
    test data. The table based on `trainy` indicates that there are exactly 6,000
    images each for the 10 different fashion articles in the train data, while in
    the test data, there are exactly 1,000 images of each fashion article:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取训练图像数据，将其存储在`trainx`中，并将相应的标签存储在`trainy`中。同样，我们从测试数据中创建`testx`和`testy`。基于`trainy`的表格显示，训练数据中每种时尚物品恰好有6,000张图片，而测试数据中每种时尚物品恰好有1,000张图片：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We next plot the first 64 images in the train data. Note that these are grayscale
    image data and that each image has a black background. Since our image classification
    model will be based on this data, the color images that we started with will have
    to be converted into grayscale images too. In addition, images of shirts, coats,
    and dresses are somewhat challenging to differentiate and this may impact the
    accuracy of our model. Let''s have a look at the following lines of code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制训练数据中的前64张图像。请注意，这些是灰度图像数据，每张图像都有黑色背景。由于我们的图像分类模型将基于这些数据，因此我们最初的彩色图像也需要转换为灰度图像。此外，衬衫、外套和连衣裙等图像比较难以区分，这可能会影响我们模型的准确性。我们来看看以下代码：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get the output of the first 64 images in the train data as shown:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到训练数据中前64张图像的输出，如下所示：
- en: '![](img/08e58ae2-975a-47d4-afba-8d1169790f2a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08e58ae2-975a-47d4-afba-8d1169790f2a.png)'
- en: 'A histogram based on the first image (an ankle boot) in the train data is shown
    in the following graph:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练数据中第一张图片（踝靴）的直方图如下图所示：
- en: '![](img/d6a41ac4-0875-4150-bd49-e01e44962d9d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6a41ac4-0875-4150-bd49-e01e44962d9d.png)'
- en: The highest bar on the left is from the low intensity data points that capture
    the black background in the image. The higher intensity values representing the
    lighter color of the ankle boot are reflected in the higher bars toward the right.
    These intensity values in the histogram range from 0 to 255.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的最高柱状条来自低强度数据点，这些数据点捕捉到了图像中的黑色背景。代表踝靴较浅颜色的高强度值在右侧的更高柱状条中有所体现。这些强度值的范围从0到255。
- en: Reshaping and resizing
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重塑和调整大小
- en: 'Next, we reshape, train, and test data. We also divide the train and the test
    data by 255 to change the range of values from 0-255 to 0-1\. The codes used are
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重塑、训练并测试数据。我们还将训练数据和测试数据除以255，将数值范围从0-255变为0-1。所使用的代码如下：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The structure of the preceding `trainx `shows that after reshaping the train
    data, we now have data with 60,000 rows and 784 (28 x 28) columns.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`trainx`结构表明，在对训练数据进行重塑后，我们现在得到了包含60,000行和784列（28 x 28）的数据。
- en: 'We get the output of the histogram based on the first image (an ankle boot)
    in the train data after dividing the data by 255, as shown in the following screenshot:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据除以255之后，我们获得了基于训练数据中第一张图像（踝靴）的直方图输出，如下截图所示：
- en: '![](img/8732ae1c-7c1b-460f-af40-9c69b7ec14e2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8732ae1c-7c1b-460f-af40-9c69b7ec14e2.png)'
- en: The preceding histogram shows that the range of data points has now changed
    to values between 0 and 1\. However, the shape observed in the previous histogram
    has not changed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的直方图显示，数据点的范围现在已变为0到1之间。然而，在前一个直方图中观察到的形状没有变化。
- en: One-hot encoding
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'Next, we do one-hot encoding of labels stored in `trainy` and `testy` using
    the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对存储在`trainy`和`testy`中的标签进行独热编码，使用的代码如下：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After one-hot encoding, the first row for the train data indicates a value of
    1 for the tenth category (ankle boot). Similarly, the second row for the train
    data indicates a value of 1 for the first category (t-shirt/top). After completing
    the changes mentioned previously, now the fashion-MNIST data is ready for developing
    an image recognition and classification model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 经独热编码后，训练数据的第一行表示第十类（踝靴）的值为1。类似地，训练数据的第二行表示第一类（T恤/上衣）的值为1。完成前述更改后，时尚-MNIST数据现在已经准备好用于开发图像识别和分类模型。
- en: Layers in the convolutional neural networks
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络中的层
- en: In this section, we will develop model architecture and then compile the model.
    We will also carry out calculations to compare the convolutional network with
    a fully connected network. Let's get started by specifying the model architecture.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将开发模型架构，然后编译模型。我们还将进行计算，以将卷积网络与全连接网络进行比较。让我们从指定模型架构开始。
- en: Model architecture and related calculations
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构及相关计算
- en: 'We start by creating a model using the `keras_model_sequential` function. The
    codes used for the model architecture are given as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过 `keras_model_sequential` 函数创建模型。模型架构所使用的代码如下：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As shown in the preceding code, we add various layers to develop a CNN model.
    The input layer in this network has 28 x 28 x 1 dimensions based on the height
    and width of the images, which are 28 each. And since we are using grayscale images,
    the color channel is one. We use two-dimensional convolutional layers here as
    we are building a deep learning model with gray-scale images.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们添加了不同的层来开发 CNN 模型。该网络中的输入层基于图像的高度和宽度为 28 x 28 x 1，其中每个维度都是 28。由于我们使用的是灰度图像，颜色通道为
    1。由于我们正在构建一个使用灰度图像的深度学习模型，因此这里使用的是二维卷积层。
- en: Note that when developing an image recognition and classification model with
    gray scale image data, we make use of a 2D convolutional layer, and with color
    images, we make use of a 3D convolutional layer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在使用灰度图像数据开发图像识别和分类模型时，我们使用的是 2D 卷积层；而对于彩色图像，我们使用的是 3D 卷积层。
- en: Let's look at some calculations involving the first convolutional layer of the
    network, which will help us to appreciate the use of such layers compared to a
    densely connected layer. In a CNN, neurons in a layer are not connected to all
    the neurons in the next layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下与网络的第一个卷积层相关的一些计算，这将帮助我们理解与全连接层相比，使用卷积层的优势。在 CNN 中，一层的神经元并不是与下一层的所有神经元相连接的。
- en: Here, the input layer has an image with dimensions of 28 x 28 x 1\. To obtain
    the output shape, we subtract three (from `kernel_size`) from 28 (height of the
    input image) and add one. This gives us 26\. The final dimension for the output
    shape becomes 26 x 26 x 32, where 32 is the number of output filters. Thus, the
    output shape has reduced height and width, but it has a greater depth. To arrive
    at the number of parameters, we use 3 x 3 x 1 x 32 + 32 = 320, where 3 x 3 is
    the `kernel_size`, 1 is the number of channels for the image, 32 is the number
    of output filters, and to this we add 32 bias terms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，输入层的图像尺寸为 28 x 28 x 1。为了获得输出形状，我们从 28（输入图像的高度）中减去 3（来自 `kernel_size`），然后再加
    1。这给我们得到 26。最终的输出形状为 26 x 26 x 32，其中 32 是输出滤波器的数量。因此，输出形状的高度和宽度减小，但深度增大。为了计算参数数量，我们使用
    3 x 3 x 1 x 32 + 32 = 320，其中 3 x 3 是 `kernel_size`，1 是图像的通道数，32 是输出滤波器的数量，并且我们还加上了
    32 个偏置项。
- en: If we compare this to a fully connected neural network, we will obtain a much
    larger number of parameters. In a fully connected network, 28 x 28 x 1 = 784 neurons
    will be connected to 26 x 26 x 32 = 21,632 neurons. So, the total number of parameters
    will be 784 x 21,632 + 21,632 = 16,981,120\. This is more than 53,000 times the
    number of parameters for a densely connected layer compared to what we get for
    a convolutional layer. This, in turn, helps to significantly reduce the processing
    time and thereby the processing cost.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将其与全连接神经网络进行比较，我们会得到一个更大的参数数量。在全连接网络中，28 x 28 x 1 = 784 个神经元将与 26 x 26 x 32
    = 21,632 个神经元连接。因此，总参数数量为 784 x 21,632 + 21,632 = 16,981,120。这比卷积层的参数数量大了超过 53,000
    倍。反过来，这有助于显著减少处理时间，从而降低处理成本。
- en: 'The number of parameters for each layer is indicated in the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层的参数数量在以下代码中给出：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output shape for the second convolutional network is 24 x 24 x 64, where
    64 is the number of output filters. Here too, the output shape has a reduced height
    and width, but it has a greater depth. To arrive at a number of parameters, we
    use 3 x 3 x 32 x 64 + 64 = 18,496, where 3 x 3 is the `kernel_size`, 32 is the
    number of filters in the previous layer, and 64 is the number of output filters,
    and to this, we add 64 bias terms.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个卷积网络的输出形状为 24 x 24 x 64，其中 64 是输出滤波器的数量。在这里，输出形状的高度和宽度有所减少，但深度增大。为了计算参数数量，我们使用
    3 x 3 x 32 x 64 + 64 = 18,496，其中 3 x 3 是 `kernel_size`，32 是前一层的滤波器数量，64 是输出滤波器的数量，我们还需要加上
    64 个偏置项。
- en: The next layer is the pooling layer, which is usually placed after the convolutional
    layer and performs a down-sampling operation. This helps to reduce processing
    time and also helps to reduce overfitting. To obtain output shape, we can divide
    24 by 2, where 2 comes from the pool size that we have specified. The output shape
    here is 12 x 12 x 64 and no new parameters are added. The pooling layer is followed
    by a dropout layer with the same output shape and, once again, no new parameters
    are added.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一层是池化层，通常放置在卷积层之后，执行下采样操作。这有助于减少处理时间，也有助于减少过拟合。为了获得输出形状，我们可以将24除以2，其中2是我们指定的池化大小。这里的输出形状是12
    x 12 x 64，并且没有添加新的参数。池化层后是一个dropout层，具有相同的输出形状，再次没有添加新的参数。
- en: In the flattened layer, we go from 3 dimensions (12 x 12 x 64) to one dimension
    by multiplying the three numbers to obtain 9,216\. This is followed by a densely
    connected layer with 64 units. The number of parameters here can be obtained as
    9216 x 64 + 64 = 589,888\. This is followed by another dropout layer to avoid
    the overfitting problem and no parameters are added here. And finally, we have
    the last layer, which is a densely connected layer with 10 units representing
    10 fashion items. The number of parameters here is 64 x 10 + 10 = 650\. The total
    number of parameters is thus 609,354\. In the CNN architecture that we have, we
    are using the relu activation function for the hidden layers and softmax for the
    output layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在展平层中，我们通过将三个数字相乘（12 x 12 x 64）来将三维数据转换为一维，得到9,216。接下来是一个具有64个单元的全连接层。这里的参数数量可以通过计算9216
    x 64 + 64 = 589,888来获得。接下来是另一个dropout层，以避免过拟合问题，这里没有添加任何参数。最后，我们有最后一层，这是一个全连接层，具有10个单元，表示10种时尚单品。这里的参数数量是64
    x 10 + 10 = 650。因此，参数的总数为609,354。在我们使用的CNN架构中，隐藏层使用relu激活函数，输出层使用softmax激活函数。
- en: Compiling the model
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'Next, we compile the model using the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码来编译模型：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding code, loss is specified as `categorical_crossentropy` as there
    are 10 categories of fashion items. For the optimizer, we make use of `optimizer_adadelta`
    with its recommended default settings. Adadelta is an adaptive learning rate method
    for gradient descent. As the name suggests, it dynamically adapts over time and
    doesn't require manual tuning of the learning rate. We have also specified `accuracy`
    for the metrics.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，损失函数被指定为`categorical_crossentropy`，因为有10个时尚单品类别。对于优化器，我们使用`optimizer_adadelta`，并使用其推荐的默认设置。Adadelta是一种自适应学习率的梯度下降方法，正如其名所示，它会随着时间的推移动态调整，并且不需要手动调节学习率。我们还指定了`accuracy`作为度量指标。
- en: In the next section, we will fit the model for image recognition and classification.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将为图像识别和分类拟合模型。
- en: Fitting the model
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'For fit the model, we will continue with the format that we have been using
    in the earlier chapters. The following code is used to fit the model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合模型，我们将继续使用之前章节中的格式。以下代码用于拟合模型：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we are using 20 epochs, a batch size of 128, and 20% of the training data
    is reserved for validation. Since the neural network used here is more complex
    than the previous chapters, each run is likely to take relatively more time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用20个epoch，批量大小为128，且20%的训练数据被保留用于验证。由于这里使用的神经网络比前几章中的更复杂，因此每次运行可能需要相对更多的时间。
- en: Accuracy and loss
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率和损失
- en: 'After fitting the model, accuracy and loss values for the 15 epochs are plotted
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合模型后，15个epoch的准确率和损失值如下图所示：
- en: '![](img/ee1fd997-7042-4d80-bc61-e1b1badabac2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee1fd997-7042-4d80-bc61-e1b1badabac2.png)'
- en: We can observe from the preceding plot that training accuracy continues to increase,
    whereas validation accuracy for the last few epochs is more or less flat. A similar
    pattern in the opposite direction is observed for the loss values. However, we
    do not observe any major overfitting problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从上面的图中看到，训练准确率持续增加，而验证准确率在最后几个epoch基本保持平稳。相反的模式也出现在损失值上。然而，我们没有观察到任何显著的过拟合问题。
- en: Let's now evaluate this model and see how predictions with this model perform.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估这个模型，看看使用该模型的预测效果如何。
- en: Model evaluation and prediction
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: After fitting the model, we will evaluate its performance in terms of loss and
    accuracy. We will also create a confusion matrix to assess classification performance
    across all 10 types of fashion items. We will perform a model evaluation and prediction
    for both train and test data. We will also obtain images of fashion items that
    do not belong to the MNIST fashion data and explore how well the performance of
    the model can be generalized to new images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型后，我们将根据损失和准确率评估其表现。我们还将创建一个混淆矩阵，以评估所有10种时尚物品的分类表现。我们将对训练数据和测试数据进行模型评估和预测。同时，我们还将获取不属于MNIST时尚数据集的时尚物品图像，并探讨该模型在新图像上的泛化能力。
- en: Training data
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据
- en: 'Loss and accuracy based on training data are obtained as 0.115 and 0.960, respectively,
    as shown in the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练数据的损失和准确率分别为0.115和0.960，如下方代码所示：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we create a confusion matrix based on the predicted and actual values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将根据预测值和实际值创建一个混淆矩阵：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'From the preceding confusion matrix, we can make the following observations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以得出以下观察结果：
- en: The correct classifications shown on the diagonal for all 10 categories have
    large values, with the lowest being 5,263 out of 6,000 for item 6 (shirt).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有10个类别在对角线上的正确分类都有较大的值，最低的是第6项（衬衫）的5,263/6,000。
- en: The best classification performance is seen for item 8 (bag), where this model
    correctly classifies 5,974 bag images out of 6,000.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳的分类表现出现在第8项（包），该模型正确分类了6,000张包的图像中的5,974张。
- en: Among off-diagonal numbers representing misclassifications by the model, the
    highest value is 359, where item 6 (shirt) is mistaken for item 0 (t-shirt/top).
    There are 230 occasions when item-0 (t-shirt/top) is misclassified as item 6 (shirt).
    So, this model certainly has some difficulty differentiating between item 0 and
    item 6.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代表模型误分类的非对角线数字中，最高值为359，其中第6项（衬衫）被错误地分类为第0项（T恤/上衣）。还有230次情况，第0项（T恤/上衣）被误分类为第6项（衬衫）。因此，该模型确实在区分第0项和第6项时存在一些困难。
- en: 'Let''s also look deeper by calculating prediction probabilities for the first
    five items, as shown in the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过计算前五个物品的预测概率来深入分析，如下方代码所示：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can observe from the preceding output that all five fashion items are correctly
    classified. The correct classification probabilities range from 0.656 (item 0
    in the fifth row) to 1.000 (item 0 in the second row). These probabilities are
    significantly high to effect correct classification without any confusion.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出可以看出，所有五个时尚物品都已正确分类。正确的分类概率从0.656（第五行的第0项）到1.000（第二行的第0项）不等。这些概率非常高，能够确保没有混淆地进行正确分类。
- en: Now, let's see whether this performance is replicated with test data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这种表现是否在测试数据中得以复制。
- en: Test data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试数据
- en: 'We start by looking at loss and accuracy values based on the test data:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从查看基于测试数据的损失和准确率值开始：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We observe that loss is higher and accuracy is lower compared to the values
    obtained from the train data. This is as expected considering a similar situation
    with validation data that we observed earlier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，与训练数据相比，损失值较高，准确率较低。这与我们之前在验证数据中观察到的类似情况一致。
- en: 'Confusion matrix for test data is provided as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据的混淆矩阵如下所示：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'From the preceding confusion matrix, we can make the following observations:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以得出以下观察结果：
- en: This model is most confused regarding item 6 (shirt), with 91 instances where
    it classifies fashion items as item-0 (t-shirt/top).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型在第6项（衬衫）上最为混淆，存在91个实例，其中将时尚物品错误分类为第0项（T恤/上衣）。
- en: The best image recognition and classification performance is for item-5 (sandal),
    with 988 correct predictions out of 1,000.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳的图像识别和分类表现是第5项（凉鞋），其在1,000个预测中正确分类了988个。
- en: Overall, the confusion matrix exhibits a similar pattern to the one we observed
    with the training data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，混淆矩阵表现出与我们在训练数据中观察到的类似模式。
- en: 'Looking at prediction probabilities for the first five items in the test data,
    we observe that all five predictions are correct. Prediction probability for all
    five items is quite high:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 查看测试数据中前五个物品的预测概率时，我们观察到所有五个预测都是正确的。所有五个物品的预测概率都相当高：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, with sufficiently high classification performances with both the training
    and testing data in terms of accuracy, let's see if we can do the same with the
    20 images of fashion items with which we started this chapter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在训练和测试数据的准确度方面都有足够高的分类性能，让我们看看是否可以对这20张时尚物品图像做到同样的事情，这些图像是我们在本章开始时所用的。
- en: 20 fashion items from the internet
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自互联网的20个时尚物品
- en: 'We read the 20 colored images from the desktop and change them to gray to maintain
    compatibility with the data and model that we have used so far. Take a look at
    the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从桌面读取了20张彩色图像，并将其转换为灰度图，以保持与我们迄今为止使用的数据和模型的兼容性。看看以下代码：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As seen previously, we also resize all 20 images to 28 x 28, and the resulting
    20 images to be classified are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所示，我们还将所有20张图像调整为28 x 28，并且最终要分类的20张图像如下：
- en: '![](img/4b80c10e-8cee-4778-901e-047fd020ab1a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b80c10e-8cee-4778-901e-047fd020ab1a.png)'
- en: 'As we can observe from the preceding plot, there are two fashion items belonging
    to each of the 10 categories of the fashion-MNIST data:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以观察到，每一类时尚-MNIST数据的10个类别中都有两个时尚物品：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We reshape images in the required dimensions and then row-bind them. Looking
    at the structure of `new`, we see a 20 x 784 matrix. However, to get an appropriate,
    structure we will reshape it further to 20 x 28 x 28 x 1, as shown in the following
    code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将图像调整为所需的尺寸，然后按行将它们绑定。观察`new`的结构，我们看到一个20 x 784的矩阵。然而，为了得到适当的结构，我们将其进一步调整为20
    x 28 x 28 x 1，如以下代码所示：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We reshape `new` to get the appropriate format, and save the result in `newx`.
    We use `newy` to store the actual labels for the 20 fashion items.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`new`重新调整为合适的格式，并将结果保存在`newx`中。我们使用`newy`来存储20个时尚物品的实际标签。
- en: 'Now, we are ready to use the prediction model, and create a confusion matrix
    as shown in the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好使用预测模型，并创建一个混淆矩阵，如以下代码所示：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We observe from the numbers on the diagonal that only 10 items are correctly
    classified out of 20\. This translates to a low accuracy of only 50%, compared
    to over 90% accuracy observed for the train and test data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从对角线上的数字可以看出，在20个物品中只有10个被正确分类。这意味着准确率为50%，与训练数据和测试数据中观察到的90%以上的准确率相比，这个准确率较低。
- en: 'Next, we summarize these predictions in the form of a plot that includes the
    prediction probabilities, predicted class, and actual class, using the following
    code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码以图表的形式总结这些预测，图表包括预测概率、预测类别和实际类别：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding plot summarizes the performance of the classification model with
    the help of prediction probabilities, predicted class, and actual class (`model-one`):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表总结了分类模型的性能，结合了预测概率、预测类别和实际类别（`model-one`）：
- en: '![](img/281a96f9-b448-475d-bea5-c3c323e6f39b.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/281a96f9-b448-475d-bea5-c3c323e6f39b.png)'
- en: In the preceding plot, the first number in the top-left position is the prediction
    probability, the second number in the top-middle position is the predicted class,
    and the third number in the top-right position is the actual class. Looking at
    some of these misclassifications, what stands out is the fact that surprisingly,
    all images of the sandal (item 5), sneakers (item 7), and ankle boots (item 9)
    are incorrectly classified. These categories of images were classified with high
    accuracy in the training as well as test data. These six misclassifications have
    contributed to the significantly low accuracy value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，左上角的第一个数字是预测概率，顶部中间的第二个数字是预测类别，顶部右侧的第三个数字是实际类别。观察一些错误分类的情况，值得注意的是，令人惊讶的是，所有凉鞋（物品5）、运动鞋（物品7）和短靴（物品9）的图像都被错误分类了。这些类别的图像在训练数据和测试数据中的分类准确率都很高。这六个错误分类导致了显著较低的准确率。
- en: 'Two key aspects of what we have done so far can now be summarized as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的两大关键方面可以总结如下：
- en: The first one is what we would generally expect—model performance with the test
    data is usually lower compared to what is observed with the training data.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个结果是我们通常会预期的——模型在测试数据上的表现通常比在训练数据上的表现差。
- en: The second one is a bit of an unexpected outcome. The 20 fashion item images
    that were downloaded from the internet had significantly reduced accuracy with
    the same model.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个结果是一个稍微出乎意料的结果。来自互联网的20张时尚物品图像，在使用相同模型时准确率显著降低。
- en: Let's see whether we can devise a strategy or make changes to the model to obtain
    better performance. We plan to have a closer look at the data and find a way to
    translate the performance that we saw with training and test data, if possible,
    to the 20 new images.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否能制定一种策略或对模型进行更改，以获得更好的性能。我们计划更仔细地查看数据，并找到一种方法，如果可能的话，将我们在训练和测试数据中看到的性能转化为这20张新图片的表现。
- en: Performance optimization tips and best practices
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化建议与最佳实践
- en: In any data analysis task, it is important to understand how the data was collected.
    With the model that we developed in the previous section, the accuracy dropped
    from over 90% for the test data to 50% for the 20 fashion item images that were
    downloaded from the internet. If this difference is not addressed, it will be
    difficult for this model to generalize well with any fashion items that are not
    part of the training or test data, and therefore will not be of much practical
    use. In this section, we will explore improvements to the classification performance
    of the model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据分析任务中，了解数据是如何收集的都非常重要。通过我们在前一节中开发的模型，测试数据的准确度从90%以上下降到从互联网上下载的20张时尚商品图片的50%。如果不解决这种差异，该模型将很难对任何未包含在训练或测试数据中的时尚商品进行良好的泛化，因此在实际应用中不会有太大帮助。在这一节中，我们将探索如何改进模型的分类性能。
- en: Image modification
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像修改
- en: 'Looking at the 64 images at the beginning of this chapter reveals some clues
    as to what''s going on. We notice that images of the sandals, sneakers, and ankle
    boots seem to have a specific pattern. In all pictures involving these fashion
    items, the toe has always been pictured pointing in the left direction. On the
    other hand, in the images downloaded from the internet for the three footwear
    fashion items, we notice that the toe has been pictured pointing in the right
    direction. To address this, let''s modify images of the 20 fashion items with
    a `flop` function that will make the toes point in the left direction, and then
    we can again assess the classification performance of the model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下本章开头的64张图片，能为我们揭示一些线索。我们注意到，凉鞋、运动鞋和踝靴的图片似乎有一个特定的模式。在所有涉及这些时尚商品的图片中，鞋头总是指向左侧。另一方面，在从互联网上下载的三种鞋类时尚商品的图片中，我们注意到鞋头是指向右侧的。为了解决这个问题，让我们用`flop`函数修改这20张时尚商品图片，使鞋头指向左侧，然后我们可以再次评估模型的分类性能：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following screenshot shows the prediction probabilities, predicted class,
    and actual class after applying the flop (`model-one`) function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在应用`flop`（`model-one`）函数后，预测概率、预测类别和实际类别：
- en: '![](img/8438c6a4-c1fe-4b8c-9964-92537cc9a22e.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8438c6a4-c1fe-4b8c-9964-92537cc9a22e.png)'
- en: As observed from the preceding plot, after changing the orientation of the images
    of the fashion items, we now get correct classifications by the model for sandals,
    sneakers, and ankle boots. With 16 correct classifications out of 20, the accuracy
    improves to 80%, compared to a figure of 50% that we obtained earlier. Note that
    this improvement in accuracy comes from the same model. The only thing that we
    did here was to observe how the original data was collected and then maintain
    consistency with the new image data being used. Next, let's work on the modification
    of the deep network architecture and see whether we can improve results further.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以观察到，在改变了时尚商品图片的方向后，我们现在可以正确分类凉鞋、运动鞋和踝靴。在20张图片中有16张分类正确，准确度提高到80%，相比之下之前只有50%的准确度。请注意，这一准确度的提升来自于相同的模型。我们所做的唯一事情就是观察原始数据是如何收集的，并且在新图像数据的使用中保持一致性。接下来，让我们着手修改深度网络架构，看看能否进一步改进结果。
- en: Before prediction models are used for generalizing the results to new data,
    it is a good idea to review how data was originally collected and then maintain
    consistency in terms of the format for the new data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在将预测模型用于将结果推广到新数据之前，回顾数据最初是如何收集的，然后在新数据的格式上保持一致性是一个好主意。
- en: We encourage you to experiment a bit further to explore and see what happens
    if certain percentages of images in the fashion-MNIST data are changed to their
    mirror images. Can this help to generalize even better without a need to make
    changes to the new data?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你进一步实验，探索如果将时尚-MNIST数据中的某些图片更改为其镜像图像，会发生什么。这样是否能帮助更好地泛化，而不需要对新数据做出更改？
- en: Changes to the architecture
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对架构的更改
- en: 'We modify the architecture of the CNN by adding more convolutional layers to
    illustrate how such layers can be added. Take a look at the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过增加更多的卷积层来修改CNN的架构，目的是说明如何添加这样的层。请看下面的代码：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, for the first two convolutional layers, we use 32 filters
    each, and for the next set of convolutional layers, we use 64 filters each. After
    each pair of convolutional layers, as done earlier, we add pooling and dropout
    layers. Another change carried out here is the use of 512 units in the dense layer.
    Other settings are similar to the earlier network.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，对于前两层卷积层，我们使用了32个滤波器，每个卷积层；而对于接下来的卷积层，我们使用了64个滤波器。每对卷积层后，如前所示，我们添加了池化层和丢弃层。这里的另一个变化是使用了512个单元的全连接层。其他设置与之前的网络相似。
- en: 'The following screenshot shows accuracy and loss for training and validation
    data (`model_two`):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`model_two`在训练数据和验证数据上的准确率和损失：
- en: '![](img/ea618897-a548-4815-a69e-d52c9bf5036d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea618897-a548-4815-a69e-d52c9bf5036d.png)'
- en: The plot based on `model_two` shows closer performance between training and
    validation data for loss and accuracy compared to `model_one`. In addition, a
    flattening of the lines toward the fifteenth epoch also suggests that increasing
    the number of epochs is not likely to help much in improving the classification
    performance further.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于`model_two`的图表显示，与`model_one`相比，训练数据和验证数据在损失和准确度方面的表现更加接近。此外，随着第十五个周期后曲线的平滑，也表明增加周期数不太可能进一步改善分类性能。
- en: 'Loss and accuracy values for the training data are obtained as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的损失和准确度值如下：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The loss and accuracy values based on this model do not show a major improvement,
    with the loss value being slightly higher and accuracy values being slightly lower.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 基于该模型的损失和准确度值没有显著改善，损失值稍高，而准确度值略低。
- en: 'The following confusion matrix summarizes the predicted and actual classes:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下混淆矩阵总结了预测类别和实际类别：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'From the confusion matrix, we can make the following observations:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中，我们可以得出以下观察结论：
- en: It shows that the model has maximum confusion (456 misclassifications) between
    item 6 (shirt) and item 0 (t-shirt/top). And this confusion is observed in both
    directions, where item 6 is confused for item 0, and item-0 being confused for
    item 6.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它显示模型在项目6（衬衫）和项目0（T恤/上衣）之间存在最大混淆（456次错误分类）。这种混淆是双向的，既有项目6被误分类为项目0，也有项目0被误分类为项目6。
- en: Item 8 (bag) has been classified most accurately, with 5,974 instances out of
    a total of 6,000 (about 99.6% accuracy).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目8（包）被分类为最准确的，6,000个实例中有5,974个被正确分类（约99.6%的准确率）。
- en: Item-6 (shirt) has been classified with the lowest accuracy out of 10 categories,
    with 4,700 instances out of 6,000 (about 78.3% accuracy).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目6（衬衫）在10个类别中被分类为准确度最低的，6,000个实例中有4,700个被误分类（约78.3%的准确率）。
- en: 'For the test data loss, the accuracy and confusion matrices are provided as
    follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试数据的损失，我们提供了以下的准确率和混淆矩阵：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'From the preceding output, we observe that loss is lower than we obtained with
    the earlier model, while accuracy is slightly lower than the earlier performance.
    From the confusion matrix, we can make the following observations:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们观察到损失低于我们之前模型的结果，而准确度略低于之前的表现。从混淆矩阵中，我们可以得出以下观察结论：
- en: It shows that the model has the maximum confusion (104 misclassifications) between
    item 6 (shirt) and item 0 (t-shirt/top).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它显示模型在项目6（衬衫）和项目0（T恤/上衣）之间存在最大混淆（104次错误分类）。
- en: Item 8 (bag) has been classified most accurately, with 989 instances out of
    a total of 1,000 (about 98.9% accuracy).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目8（包）被分类为最准确的，1,000个实例中有989个被正确分类（约98.9%的准确率）。
- en: Item 6 (shirt) has been classified with the lowest accuracy out of 10 categories,
    with 720 instances out of 1,000 (about 72.0% accuracy).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目6（衬衫）在10个类别中被分类为准确度最低的，1,000个实例中有720个被误分类（约72.0%的准确率）。
- en: Thus, overall, we observe a similar performance to the one that we observed
    with the training data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总体上，我们观察到与训练数据时相似的表现。
- en: 'For the 20 images of fashion items downloaded from the internet, the following
    screenshot summarizes the performance of the model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从互联网下载的20张时尚商品图片，以下截图总结了模型的表现：
- en: '![](img/dfb34d18-652e-478c-a156-d52b01e6da0a.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfb34d18-652e-478c-a156-d52b01e6da0a.png)'
- en: As seen from the preceding plot, this time, we have 17 out 20 images correctly
    classified. Although this is a slightly better performance, it is still a little
    lower than the figure in the region of 92% accuracy for the test data. In addition,
    note that due to a much smaller sample, the accuracy values can fluctuate significantly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表可以看出，这次我们成功地正确分类了20张图像中的17张。虽然这表现稍好一些，但仍然略低于测试数据中92%准确率的水平。另外，由于样本量较小，准确率的波动可能会比较大。
- en: In this section, we made modifications to the 20 new images and made some changes
    to the CNN model architecture to obtain a better classification performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对20张新图像进行了修改，并对CNN模型架构进行了一些调整，以获得更好的分类性能。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we showed how to use a **convolutional neural network** (**CNN**)
    deep learning model for image recognition and classification. We made use of the
    popular fashion-MNIST data for training and testing the image classification model.
    We also went over calculations involving a number of parameters, and were able
    to contrast this with the number of parameters that would have been needed by
    a densely connected neural network. CNN models help to significantly reduce the
    number of parameters needed and thus result in significant savings in computing
    time and resources. We also used images of fashion items downloaded from the internet
    to see whether a classification model based on fashion-MNIST data can be generalized
    to similar items. We did notice that it is important to maintain consistency in
    the way images are laid out in the training data. Additionally, we also showed
    how we can add more convolutional layers in the model architecture to develop
    a deeper CNN model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们展示了如何使用**卷积神经网络**（**CNN**）深度学习模型进行图像识别和分类。我们使用了流行的fashion-MNIST数据集进行图像分类模型的训练和测试。我们还讨论了涉及多个参数的计算，并将其与密集连接神经网络所需的参数数量进行了对比。CNN模型大大减少了所需的参数数量，从而显著节省了计算时间和资源。我们还使用了从互联网上下载的时尚商品图像，测试了基于fashion-MNIST数据的分类模型是否能推广到类似的物品上。我们确实注意到，保持训练数据中图像布局的一致性非常重要。此外，我们还展示了如何在模型架构中添加更多的卷积层，以开发更深的CNN模型。
- en: So far, we have gradually progressed from not-so-deep neural network models
    to more complex and deeper neural network models. We also mainly covered such
    applications that are categorized under supervised learning methods. In the next
    chapter, we will go over another interesting class of deep neural network models
    called autoencoders. We will cover applications involving autoencoder networks
    that can be classified under unsupervised learning approaches.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们逐渐从不太深的神经网络模型过渡到更复杂、更深的神经网络模型。我们也主要讨论了属于监督学习方法的应用。在下一章中，我们将介绍另一类有趣的深度神经网络模型——自编码器。我们将介绍涉及自编码器网络的应用，这些应用可以归类为无监督学习方法。
