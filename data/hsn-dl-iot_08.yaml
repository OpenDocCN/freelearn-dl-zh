- en: Physiological and Psychological State Detection in IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网中的生理和心理状态检测
- en: Human physiological and psychological states can provide very useful information
    about a person's activity and emotions. This information can be used in many application
    domains, including smart homes, smart cars, entertainment, education, rehabilitation
    and health support, sports, and industrial manufacturing, to improve existing
    services and/or offer new services. Many IoT applications incorporate sensors
    and processors for human pose estimation or activity and emotion recognition.
    However, the detection of activities or emotions based on the sensor data is a
    challenging task. In recent years, DL-based approaches have become a popular and
    effective way to address this challenge.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人体生理和心理状态能够提供关于个人活动和情感的非常有用的信息。这些信息可以应用于多个领域，包括智能家居、智能汽车、娱乐、教育、康复与健康支持、体育和工业制造等，以改进现有服务和/或提供新服务。许多物联网应用都集成了用于人体姿态估计或活动和情感识别的传感器和处理器。然而，基于传感器数据检测活动或情感是一项具有挑战性的任务。近年来，基于深度学习的方法已成为解决这一挑战的流行且有效的方式。
- en: 'This chapter presents DL-based human physiological and psychological state
    detection techniques for IoT applications in general. The first part of this chapter
    will briefly describe different IoT applications and their physiological and psychological
    state detection-based decision making. In addition, it will briefly discuss two
    IoT applications and their physiological and psychological state detection-based
    implementations in a real-world scenario. In the second part of the chapter, we
    will present the DL-based implementations of the two IoT applications. In this
    chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了基于深度学习的物联网应用中的人体生理和心理状态检测技术。章节的第一部分将简要描述不同的物联网应用及其基于生理和心理状态检测的决策制定。此外，还将简要讨论两个物联网应用及其在现实场景中的基于生理和心理状态检测的实现。在章节的第二部分，我们将展示这两个物联网应用的基于深度学习的实现。本章将涵盖以下主题：
- en: IoT-based human physiological and psychological state detection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于物联网的人体生理和心理状态检测
- en: 'Use case one: IoT-based remote progress monitoring of physiotherapy'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例一：基于物联网的远程物理治疗进度监控
- en: Implementation of IoT-based remote progress monitoring of physiotherapy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于物联网的远程物理治疗进度监控的实现
- en: 'Use case two: the smart classroom'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例二：智能教室
- en: Implementation of the smart classroom
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能教室的实现
- en: Deep learning for human activity and emotion detection in IoT
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于深度学习的人体活动与情感检测在物联网中的应用
- en: LSTM and CNNs and transfer learning for HAR/FER in IoT applications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM和CNN以及转移学习在物联网应用中的人体活动/情感识别（HAR/FER）
- en: Data collection
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data preprocessing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Model training
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Evaluation of the models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估
- en: IoT-based human physiological and psychological state detection
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于物联网的人体生理和心理状态检测
- en: 'In recent years, human physiological and psychological state detection have
    been used in many application domains to improve existing services and/or offer
    new services. IoT, combined with DL techniques, can be used in applications to
    detect human physiological and psychological states. The following diagram highlights
    a few key applications of these detection approaches:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人体生理和心理状态检测已在多个应用领域得到广泛应用，旨在提升现有服务和/或提供新服务。物联网结合深度学习技术可用于检测人体的生理和心理状态。以下图表展示了这些检测方法的一些关键应用：
- en: '![](img/78f2e175-fada-4f76-a8be-67400c6d4590.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78f2e175-fada-4f76-a8be-67400c6d4590.png)'
- en: 'Now, we will learn in detail about the two state detection variants:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将详细了解这两种状态检测变体：
- en: '**Physiological state detection**: Physiological state or activity detection
    are useful tools in many applications, including assisted living for vulnerable
    people, such as the elderly, and in remote physical therapy/rehabilitation systems.
    In assisted living for elderly people, falls among older people are detrimental
    to the health of the victim because of the associated risk of physical injury.
    A fall can also have financial consequences, due to the medical costs and need
    for hospitalization. Moreover, falls can also reduce the person''s life expectancy,
    especially in the case of a **long-lie**. It is also worth noting that the medical
    expenses linked with falls are extremely high. For example, the yearly cost of
    falls in the US alone is expected to reach US $67 billion by 2020\. In this context,
    automated and remote fall detection using DL-supported IoT applications can address
    the challenge, thus improving the quality of life for elderly people and minimizing
    the associated costs. Another key area of human activity detection applications
    is the remote physical therapy monitoring system. This is the first use case for
    this chapter, and we will present an overview of it in the next section.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生理状态检测**：生理状态或活动检测在许多应用中都非常有用，包括对脆弱人群（如老年人）的辅助生活，以及远程物理治疗/康复系统。在老年人的辅助生活中，摔倒对受害者的健康有害，因为摔倒伴随的身体伤害风险。摔倒还可能带来经济后果，因其需要医疗费用和住院治疗。此外，摔倒还可能缩短一个人的预期寿命，特别是**长时间卧床不起**的情况下。值得注意的是，摔倒相关的医疗费用极为昂贵。例如，仅在美国，摔倒的年成本预计到2020年将达到670亿美元。在此背景下，使用深度学习支持的物联网（IoT）应用进行自动化和远程摔倒检测可以解决这一问题，从而提高老年人的生活质量并减少相关成本。人类活动检测应用的另一个关键领域是远程物理治疗监控系统。这是本章的第一个应用案例，我们将在下一节中概述它。'
- en: '**Psychological state detection: **Facial expressions are good reflections
    of human psychological states, and they are important factors in human communication
    that help us to understand the intentions of humans. Generally, we can infer the
    emotional states of other people, such as joy, sadness, and anger, by analyzing
    their facial expressions and vocal tone. Forms of non-verbal communication make
    up two-thirds of all human interactions. Facial expressions, in their imparted
    emotional meaning, are one of the main non-verbal interpersonal communication
    channels. Hence, facial expression-based emotion detection can be useful in understanding
    people''s behavior. It can, therefore, help to improve existing services and/or
    new services, including personalized customer services. IoT applications, such
    as smart healthcare, smart education, and security and safety, can improve their
    services through DL-based emotion detection or sentiment analysis. For example,
    in a smart classroom, a teacher can analyze the students’ sentiment in real time
    or quasi real time to offer personalized and/or group-orientated teaching. This
    will improve their learning experience.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心理状态检测：**面部表情是人类心理状态的良好反映，它们是人类交流中的重要因素，帮助我们理解他人的意图。通常，通过分析他人的面部表情和语音语调，我们可以推测他们的情感状态，如快乐、悲伤和愤怒。非语言交流的形式构成了所有人类互动的三分之二。面部表情作为情感表达的方式，是主要的非语言人际沟通渠道之一。因此，基于面部表情的情感检测可以在理解人们行为方面发挥作用。因此，它可以帮助改进现有服务和/或新服务，包括个性化的客户服务。物联网应用，如智能医疗、智能教育和安全保障，可以通过基于深度学习的情感检测或情绪分析来改善其服务。例如，在智能课堂中，教师可以实时或准实时地分析学生的情感，从而提供个性化或小组导向的教学。这将改善学生的学习体验。'
- en: Use case one – remote progress monitoring of physiotherapy
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例一 - 远程物理治疗进展监控
- en: 'Physical therapy is a big part of healthcare. There is a huge gap between the
    demand for physical therapy and our ability to deliver that therapy. Most countries
    in the world are still greatly dependent on the one-to-one patient-therapist interaction
    (which is the gold standard), but it is not a scalable solution and not cost-effective
    for either patients or healthcare providers. In addition, most existing therapies
    and their updates rely on average data instead of an individual’s unique data,
    and sometimes this data is qualitative (for example, *Yes, I did what you told
    me to do*) rather than quantitative. This is a challenge regarding effective therapy.
    Finally, many people—especially elderly people—are living with **multiple chronic
    conditions **(**MCC**), and these conditions are generally treated separately.
    This can result in suboptimal care or even cases where these conditions can conflict
    with each other. For example, in the case of a patient with diabetes and back
    pain: a diabetic carer may recommend walking, whereas a back pain carer may forbid
    it. In this context, IoT is already changing healthcare. It can address most of
    these challenges with the support of machine learning/deep learning and data analysis
    tools, and offer effective physiotherapy by providing real-time or quasi real-time
    information.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 物理治疗是医疗保健中的重要组成部分。物理治疗的需求与我们提供该治疗的能力之间存在巨大的差距。世界上大多数国家仍然严重依赖一对一的患者-治疗师互动（这是黄金标准），但这种方式不可扩展，也不具备成本效益，无论对患者还是医疗提供者而言。此外，大多数现有的治疗及其更新依赖于平均数据，而非个体的独特数据，有时这些数据是定性的（例如，*是的，我做了你告诉我做的事情*），而非定量的。这是有效治疗面临的一个挑战。最后，许多人，特别是老年人，正面临**多重慢性病**（**MCC**），这些病症通常是分开治疗的，这可能导致治疗效果不佳，甚至出现病症之间的冲突。例如，对于糖尿病和背痛的患者：糖尿病护理人员可能建议步行，而背痛护理人员则可能禁止此举。在这种情况下，物联网已经在改变医疗保健。它可以借助机器学习/深度学习和数据分析工具解决大多数挑战，通过提供实时或准实时的信息，提供有效的物理治疗。
- en: Implementation of use case one
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例一的实现
- en: 'Progress monitoring is a key challenge in traditional therapy. An IoT-based
    therapy can solve the progress monitoring issue. The following diagram briefly
    presents how the IoT-based remote physiotherapy monitoring system will work:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 进展监测是传统治疗中的一个关键挑战。基于物联网的治疗可以解决这一进展监测问题。下图简要展示了基于物联网的远程物理治疗监控系统如何运作：
- en: '![](img/bfb73806-d56e-4a24-9626-2f6359fab343.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfb73806-d56e-4a24-9626-2f6359fab343.png)'
- en: 'One of the key components of this application is the activity monitoring of
    the subject (patient) that will help the therapist remotely observe how the patient is
    complying with the suggested therapy, and whether they are making progress. As
    shown in the preceding diagram, the IoT-based remote physiotherapy monitoring
    system consists of four main elements:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用的一个关键组成部分是对受试者（患者）活动的监控，帮助治疗师远程观察患者是否遵循建议的治疗方案，并了解其是否取得了进展。如前面的图示所示，基于物联网的远程物理治疗监控系统由四个主要元素组成：
- en: '**Sensors and patient-side computing platform**: For this use case, we are
    considering two sensors: an accelerometer and a gyroscope. Both of them measure
    three-dimensional readings linked with the subject''s activities. For these sensors,
    we can use dedicated sensors or a smartphone''s sensors (these sensors are embedded
    within most smartphones). For the client-side computing platform, we can consider
    Raspberry Pi for the dedicated sensors and the smartphone (if we are using the
    smartphone sensors). The sensors need to be properly placed in order to measure
    signals correctly. The sensors can be used for continuous or event-wise (such
    as during exercise) monitoring of the subject’s activities.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器和患者端计算平台**：对于这个用例，我们考虑使用两个传感器：加速度计和陀螺仪。它们都可以测量与受试者活动相关的三维数据。对于这些传感器，我们可以使用专用传感器或智能手机内置的传感器（这些传感器通常嵌入在大多数智能手机中）。对于客户端计算平台，我们可以考虑使用树莓派来配合专用传感器，或者使用智能手机（如果我们使用智能手机传感器）。传感器需要正确放置，才能准确测量信号。传感器可以用于受试者活动的连续或事件式监控（如运动期间）。'
- en: '**Care providers and therapists**: Care providers, such as hospitals with doctors
    and medical/healthcare databases, are connected through a cloud platform/HealthCloud.
    The main care provider for the therapy use case is a therapist, and hospitals/doctors
    will offer support to the therapist when required.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**护理人员和治疗师**：护理人员，如拥有医生和医疗/健康数据库的医院，通过云平台/HealthCloud进行连接。治疗用例的主要护理提供者是治疗师，医院/医生将在需要时为治疗师提供支持。'
- en: '**DL-based human activity detection**: In this phase, the edge-computing device
    will be installed with an app. The installed app on the smartphone or Raspberry
    Pi will be loaded with a pretrained human activity detection and classification
    model. Once the accelerometer and gyroscope detect any signal, they send it to
    the smartphone or Raspberry Pi for processing and detection using the DL model,
    and finally inform the therapist for their feedback or intervention if necessary.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于深度学习的人体活动检测**：在此阶段，边缘计算设备将安装一个应用程序。安装在智能手机或树莓派上的应用程序将加载一个预训练的人体活动检测与分类模型。一旦加速度计和陀螺仪检测到任何信号，它们将其发送到智能手机或树莓派进行处理，并使用深度学习模型进行检测，最后将结果告知治疗师，以便其根据需要提供反馈或干预。'
- en: '**HealthCloud for model learning**: The HealthCloud is a cloud computing platform
    mainly designed for healthcare-related services. This will train the selected
    DL model in human activity detection and classification using reference datasets.
    This learned model will be preinstalled in the smartphone or Raspberry Pi.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HealthCloud进行模型学习**：HealthCloud是一个主要为医疗健康相关服务设计的云计算平台。它将使用参考数据集训练选定的深度学习模型，以进行人体活动的检测和分类。训练后的模型将预装在智能手机或树莓派上。'
- en: Use case two — IoT-based smart classroom
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例二 — 基于物联网的智能教室
- en: 'The higher education dropout rate is increasing worldwide. For example, dropout
    rates among UK university students have increased for the third consecutive year.
    Three of the top eight reasons for these dropouts are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 全球高等教育的辍学率正在增加。例如，英国大学生的辍学率已经连续第三年上升。辍学的前八个原因中有三个是：
- en: Lack of quality time with teachers and counselors
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏与教师和辅导员的优质时间
- en: Demotivating school environment
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沮丧的学校环境
- en: Lack of student support
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学生支持不足
- en: One of the key challenges in addressing these issues is knowing the students
    (such as knowing whether a student is following a topic or not) and delivering
    lectures/tutorials and other support accordingly. One potential approach is to
    know the emotions of the students, which is challenging in a large classroom,
    computer lab, or in e-learning environments. The use of technologies (including
    IoT with the support of DL models) can help to recognize emotion using facial
    expression and/or speech. The second use case of this chapter aims at increasing
    student performance in the classroom by detecting emotions and managing the lecture/lab
    accordingly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的关键挑战之一是了解学生（例如，了解学生是否跟得上某个话题）并据此提供讲座/辅导及其他支持。一种潜在的方法是了解学生的情感，这在大班、计算机实验室或电子学习环境中是非常具有挑战性的。利用技术（包括在深度学习模型支持下的物联网）可以帮助通过面部表情和/或语音识别情感。本章的第二个用例旨在通过检测情感并相应管理讲座/实验室，提升学生在教室中的表现。
- en: Implementation of use case two
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例二的实现
- en: 'The following diagram shows a simplified implementation of an IoT-based smart
    classroom application:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了基于物联网的智能教室应用的简化实现：
- en: '![](img/4e9c55a8-bd4c-4f57-a31c-fe7911ee824b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e9c55a8-bd4c-4f57-a31c-fe7911ee824b.png)'
- en: 'The facial-expression-based emotion analysis implementation consists of three
    main elements:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于面部表情的情感分析实现包含三个主要元素：
- en: '**Sensors and computing platform: **For this use case, we need at least one
    CCTV camera that can cover the classroom and be connected to the computing platform
    wirelessly or via a concealed cable in the walls. The lecturer''s computer in
    the classroom can work as the computing platform. The computer will continuously
    process the video signals and convert them into images for the image-based facial
    expression analysis.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器和计算平台：**对于此用例，我们需要至少一台可以覆盖教室的闭路电视（CCTV）摄像头，并通过无线或墙内隐蔽的电缆连接到计算平台。教室内讲师的电脑可以充当计算平台。该电脑将持续处理视频信号并将其转化为图像，以便进行基于图像的面部表情分析。'
- en: '**Facial expression-based emotion detection**: The lecturer''s computer will
    be installed with an app. The installed app will be loaded with a pretrained facial
    expression-based detection and classification model. Once the DL model receives
    the facial images of the students, it identifies their emotions (such as happy/unhappy/confused)
    regarding a lecture and notifies the lecturer to take the necessary action.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于面部表情的情感检测**：讲师的电脑将安装一个应用程序。该应用程序将加载一个预训练的基于面部表情的检测和分类模型。一旦深度学习模型接收到学生的面部图像，它会识别学生的情感（如快乐/不开心/困惑）并通知讲师采取必要的行动。'
- en: '**Desktop or server for model learning:** The lecturer''s computer will be
    connected to a university server or cloud computing platform, and this will train/retrain
    the model for facial expression-based emotion recognition and classification using
    reference datasets. This learned model will be preinstalled in the lecturer''s
    PC in the classroom.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桌面或服务器用于模型学习：** 讲师的电脑将连接到大学服务器或云计算平台，这将用于基于面部表情的情感识别和分类的模型训练/重训，使用参考数据集。这一学习过的模型将预装在讲师的课堂电脑中。'
- en: All of the following sections will describe the implementation of the DL-based
    human activity and emotion recognition needed for the aforementioned use cases. All
    of the necessary codes are available in the chapter's code folder.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将描述为实现上述使用案例所需的基于深度学习的人类活动和情感识别。所有必要的代码都可以在本章的代码文件夹中找到。
- en: Deep learning for human activity and emotion detection in IoT
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于物联网的人类活动与情感检测深度学习
- en: It is important to understand the working principle of an accelerometer- and
    gyroscope-based human activity detection system, and of a facial expression-based
    emotion detection system, before discussing the useful deep learning models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论有用的深度学习模型之前，了解基于加速度计和陀螺仪的人类活动检测系统，以及基于面部表情的情感检测系统的工作原理是非常重要的。
- en: Automatic human activity recognition system
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动人类活动识别系统
- en: 'Automatic **human activity recognition** (**HAR**) systems detect human activity,
    based on raw accelerometer and gyroscope signals. The following diagram shows
    a schematic of a DL-based HAR that consists of three different phases. They are
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 自动**人类活动识别**（**HAR**）系统基于原始的加速度计和陀螺仪信号来检测人类活动。下图展示了一个基于深度学习的HAR示意图，该系统包括三个不同的阶段。具体如下：
- en: IoT deployment or instrumentation of the subject or person
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物联网部署或对受试者或人的仪器化
- en: Feature extraction and model development
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取和模型开发
- en: Activity classification/identification
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活动分类/识别
- en: '![](img/e3327277-f93f-46f8-9279-7cebe6cdef39.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3327277-f93f-46f8-9279-7cebe6cdef39.png)'
- en: Generally, classical HAR approaches mainly rely on heuristic hand-crafted feature
    extraction methods, which is a complex process and not well suited for resource-constrained
    IoT devices. More recent DL-based HAR approaches perform the feature extraction
    automatically, and they can work well on resource-constrained IoT devices. Most
    HAR approaches consider six different activities, including walking, running,
    sitting, standing, climbing upstairs, and coming downstairs. These activities
    exhibit differences in accelerometer and gyroscope signals, and a classifier exploits
    the differences to identify the current activity—which could form a part of physiotherapy
    (such as running).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，经典的HAR方法主要依赖于启发式的手工特征提取方法，这一过程较为复杂，不太适合资源受限的物联网设备。较新的基于深度学习的HAR方法能够自动进行特征提取，并且能够在资源受限的物联网设备上良好工作。大多数HAR方法考虑了六种不同的活动，包括走路、跑步、坐着、站立、上楼和下楼。这些活动在加速度计和陀螺仪信号中表现出差异，分类器利用这些差异来识别当前活动——这可能是物理治疗的一部分（例如跑步）。
- en: Automated human emotion detection system
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化人类情感识别系统
- en: 'Automated **human emotion recognition** (**HER**) can be done by using either
    one of the following signals/inputs from the subject (human) or a combination
    thereof:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化**人类情感识别**（**HER**）可以通过使用以下信号/输入之一或其组合来完成：
- en: Facial expression
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部表情
- en: Speech/audio
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音/音频
- en: Text
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本
- en: 'This chapter considers the **facial expression recognition** (**FER**)-based
    HER. A DL-based automated FER consists of three main steps: preprocessing, deep
    feature learning, and classification. The following diagram highlights these main
    steps in an FER-based HER.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论基于**面部表情识别**（**FER**）的HER。基于深度学习的自动化面部表情识别包括三个主要步骤：预处理、深度特征学习和分类。下图突出显示了基于FER的HER中的这些主要步骤。
- en: 'Image processing for facial expression analysis requires preprocessing, since
    the different types of emotions (such as anger, disgust, fear, happiness, sadness,
    surprise, and neutral) have subtle differences. Variations in input images that
    are irrelevant to FER, including different backgrounds, illuminations, and head
    poses, can be removed through preprocessing to improve the model prediction/classification
    accuracy. Face alignment, data augmentation, and image normalization are a few
    of the key preprocessing techniques. Most open source datasets for FER are not
    sufficient to generalize the FER approach. Data augmentation is essential in order to
    improve an existing dataset in terms of FER. Face alignment and image normalization
    are useful for improving individual images. The final stage of the FER DL pipeline
    is for the DL algorithm to learn and classify the features, hence emotions. Most
    image recognition DL algorithms, including CNN and RNN, are suitable for the final
    stage:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 面部表情分析的图像处理需要预处理，因为不同类型的情感（如愤怒、厌恶、恐惧、快乐、悲伤、惊讶和中立）之间有微妙的差异。输入图像中与FER无关的变化，包括不同的背景、光照和头部姿势，可以通过预处理去除，从而提高模型预测/分类的准确性。面部对齐、数据增强和图像归一化是一些关键的预处理技术。大多数用于FER的开源数据集不足以泛化FER方法。数据增强对于改进现有数据集中的FER至关重要。面部对齐和图像归一化对于改善单个图像非常有用。FER深度学习流程的最后阶段是让深度学习算法学习并分类特征，从而识别情感。大多数图像识别深度学习算法，包括CNN和RNN，适用于这一最后阶段：
- en: '![](img/2569a659-4081-4324-a81a-9958f6c121e1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2569a659-4081-4324-a81a-9958f6c121e1.png)'
- en: Deep learning models for HAR and emotion detection
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型在HAR和情感检测中的应用
- en: Generally, human activity recognition systems use accelerometer and gyroscope
    signals, which are time series data. Sometimes, the recognition process uses a
    combination of time series and spatial data. In this context, **Recurrent Neural
    Network** (**RNN**) and LSTM are potential candidates for the former type of signals
    because of their capability to incorporate temporal features of input during evolution.
    On the other hand, CNNs are good for spatial aspects of accelerometer and gyroscope
    signals. Hence, a combination or hybrid of CNNs and LSTMs/RNNs is ideal for the
    former type of signals. We will use an LSTM model for the HAR use case as it can address
    the temporal aspects of human activities.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人类活动识别系统使用加速度计和陀螺仪信号，这些信号是时间序列数据。有时，识别过程会结合时间序列和空间数据。在这种情况下，**循环神经网络**（**RNN**）和LSTM是处理前者类型信号的潜在候选模型，因为它们能够在演化过程中结合输入的时间特征。另一方面，CNN对于加速度计和陀螺仪信号的空间特征非常有效。因此，CNN与LSTM/RNN的组合或混合模型对于前者类型的信号是理想选择。我们将使用LSTM模型来处理HAR用例，因为它能够处理人类活动的时间特征。
- en: Unlike HAR systems, FER-based human emotion detection systems generally rely
    on facial expressions images, which rely on the local or spatial correlations
    between the pixel values of the images. Any DL model that works well for image
    recognition is fine for an FER task and, equally, for emotion detection. A number
    of deep learning algorithms or models have been used for image recognition, and
    the **deep belief network** (**DBN**) and CNNs are the top two candidates. In
    this chapter, we are considering CNNs because of their performance in image recognition.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与HAR系统不同，基于FER的人类情感检测系统通常依赖于面部表情图像，这些图像依赖于图像像素值之间的局部或空间相关性。任何能够很好地进行图像识别的深度学习模型都适用于FER任务，同样适用于情感检测。许多深度学习算法或模型已经被用于图像识别，其中**深度信念网络**（**DBN**）和CNN是最重要的两个候选模型。在本章中，我们将考虑CNN，因为它在图像识别中的表现非常出色。
- en: LSTM, CNNs, and transfer learning for HAR/FER in IoT applications
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM、CNN和迁移学习在物联网应用中的HAR/FER中的应用
- en: LSTM is the widely used DL model for HAR—including in IoT-based HAR—because
    its memory capacity can deal better with time series data (such as HAR data) than
    other models, including CNN. The LSTM implementation of HAR can support transfer
    learning and is suitable for resource-constrained IoT devices. Generally, FER
    relies on image processing, and the CNN is the best model for image processing.
    Therefore, we implement use case two (FER) using a CNN model. In [Chapter 3](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml), *Image
    Recognition in IoT*, we presented an overview of two popular implementations of
    the CNN (such as incentive V3 and Mobilenets) and their corresponding transfer
    learning. In the following paragraphs, we briefly present an overview of the baseline
    LSTM.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是广泛使用的深度学习模型，用于HAR，包括物联网（IoT）基础的HAR，因为其记忆能力可以比其他模型（包括CNN）更好地处理时间序列数据（如HAR数据）。LSTM实现的HAR可以支持迁移学习，并且适用于资源受限的物联网设备。通常，FER依赖于图像处理，而CNN是处理图像的最佳模型。因此，我们使用CNN模型实现用例二（FER）。在[第3章](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml)中，*物联网中的图像识别*，我们概述了两种流行的CNN实现（如Incentive
    V3和Mobilenets）及其相应的迁移学习。在接下来的段落中，我们简要介绍基础LSTM的概述。
- en: 'LSTM is an extension of RNNs. Many variants of LSTM are proposed, and they
    follow the baseline LSTM. The following is a schematic diagram of the baseline
    LSTM:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是RNN的扩展。许多LSTM的变种已被提出，它们遵循基础LSTM。以下是基础LSTM的示意图：
- en: '![](img/fee37e7b-6974-4ddf-bc1c-d04d9a769793.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fee37e7b-6974-4ddf-bc1c-d04d9a769793.png)'
- en: As shown in the preceding diagram, LSTM mainly consists of two components. They
    have a memory cell or neuron, and each cell or neuron has a multiplicative forget
    gate, read gate, and write gate. These gates control the access to memory cells/neurons
    and prevent them from being disturbed by irrelevant inputs. These gates are controlled
    through 0/1 or off/on. For example, if the forget gate is on/1, the neuron/cell
    writes its data to itself, and if the gate is off/0, the neuron forgets its last
    content. Other gates are controlled in a similar fashion.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，LSTM主要由两个组件组成。它们具有一个记忆单元或神经元，每个单元或神经元都有一个乘法遗忘门、读取门和写入门。这些门控控制对记忆单元/神经元的访问，并防止它们受到无关输入的干扰。这些门控通过0/1或开/关进行控制。例如，如果遗忘门开启/1，神经元/单元将其数据写入自身；如果门关闭/0，神经元将遗忘其上次的内容。其他门控的控制方式类似。
- en: Unlike RNNs, LSTMs use forget gates to actively control the cell/neuron states
    and ensure they do not degrade. Importantly, LSTM models perform better than RNN
    models in the case of data that has a long dependency in time. Many IoT applications,
    such as human activity recognition and disaster prediction based on environmental
    monitoring, exhibit this long-time dependency.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN不同，LSTM使用遗忘门主动控制单元/神经元状态，并确保它们不会退化。重要的是，LSTM模型在数据具有长期时间依赖性的情况下，比RNN模型表现更好。许多物联网应用，如基于环境监测的人体活动识别和灾难预测，都展示了这种长期时间依赖性。
- en: As the FER considered for use case two is based on image processing, CNNs are
    the best choice. CNN has different implementations, including a simple CNN, two
    versions of Mobilenets, and Incentive3\. Use case two will explore a simple CNN
    and Mobilenet V1 for the FER part of the implementation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于用例二的FER是基于图像处理的，因此CNN是最佳选择。CNN有不同的实现方式，包括一个简单的CNN、两个版本的Mobilenets和Incentive3。用例二将探索简单的CNN和Mobilenet
    V1，用于实现FER部分。
- en: Data collection
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Data collection for HAR and/FER is a challenging task for many reasons, including
    privacy. As a result, open source quality datasets are limited in number. For
    the HAR implementation in use case one, we are using a very popular and open source
    **Wireless Sensor Data Mining** (**WISDM**) lab dataset . The dataset consists
    of 54,901 samples collected from 36 different subjects. For privacy reasons, usernames
    are masked with ID numbers from 1-36\. The data was collected for six different
    activities undertaken by the subjects: standing, sitting, jogging, walking, going
    downstairs, and climbing upstairs. The dataset contains three-axis accelerometer
    data with more than 200 time steps for each sample. The following screenshot is
    a sample of the dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: HAR和/FER的数据收集因多种原因而具有挑战性，其中包括隐私问题。因此，开源高质量数据集的数量有限。对于用例一中的HAR实现，我们使用了一个非常流行且开源的**无线传感器数据挖掘**(**WISDM**)实验室数据集。该数据集包含来自36个不同受试者的54,901个样本。出于隐私原因，用户名已被ID号（1-36）掩盖。数据收集涵盖了受试者进行的六种不同活动：站立、坐着、慢跑、走路、下楼和爬楼。数据集包含三个方向的加速度计数据，每个样本有超过200个时间步长。以下截图是数据集的一个样本：
- en: '![](img/96d959f8-1fb5-4f89-9718-7e82fb7f64bb.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96d959f8-1fb5-4f89-9718-7e82fb7f64bb.png)'
- en: 'For the FER-based emotion detection in use case two, we used two different
    datasets. The first one is the popular and open source FER2013 dataset. This dataset
    contains 48 x 48 pixel grayscale images of human faces. These images are preprocessed
    and ready to be used directly for training and validation. The images can be classified
    into seven categories (*0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise,*
    and *6=Neutral*). The dataset in CSV format contains information about pixel values
    of the face images rather than the images. The following screenshot shows a few
    values of the dataset:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在用例二中基于FER的情感检测，我们使用了两个不同的数据集。第一个是流行的开源FER2013数据集。该数据集包含48 x 48像素的灰度人脸图像。这些图像已经预处理，并且可以直接用于训练和验证。图像可以分为七类（*0=愤怒，1=恶心，2=恐惧，3=快乐，4=悲伤，5=惊讶，*
    和 *6=中立*）。该数据集采用CSV格式，包含面部图像的像素值信息，而不是图像本身。以下截图显示了数据集中的一些值：
- en: '![](img/9bcdb674-6626-49b7-ae3d-0f0898470dd7.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bcdb674-6626-49b7-ae3d-0f0898470dd7.png)'
- en: The split ratio between the training and the validation dataset is *80:20*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集与验证集的拆分比例为*80:20*。
- en: 'We also prepared a dataset through Google search, particularly for Mobilenet
    V1\. The dataset is not a big one, as it consists of five classes of emotions,
    and each of those consists of more than 100 images. These images are not preprocessed.
    The following diagram shows a folder view of the prepared dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过Google搜索准备了一个数据集，特别针对Mobilenet V1。该数据集不大，包含五个情感类别，每个类别都有超过100张图像。这些图像未经预处理。以下图示显示了准备好的数据集文件夹视图：
- en: '![](img/54928366-063a-44fa-ac2b-1d5051415c06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54928366-063a-44fa-ac2b-1d5051415c06.png)'
- en: 'For data collection (each class of the dataset), we can follow a four step
    process:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据收集（每个类别的数据集），我们可以遵循一个四步过程：
- en: '**Search:** Use any browser (we used Chrome), go to Google, and search the
    appropriate word combination for the class/emotion (such as *angry human*) in
    Google images.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**搜索：**使用任意浏览器（我们使用了Chrome），访问Google并搜索与类别/情感相关的合适词组（例如*愤怒的人类*）的Google图片。'
- en: '**Image URL gatherings**: This step utilizes a few lines of JavaScript code
    to gather the image URLs. The gathered URLs can be used in Python to download
    the images. To do that, select the JavaScript console (assuming you will use the
    Chrome web browser, but you can use Firefox as well) by clicking the View | Developer
    | JavaScript console (in macOS) and customize and control**Google Chrome** | **More
    tools** | **Developer tools** (Windows OS). Once you have selected the JavaScript
    console, this will enable you to execute JavaScript in a REPL-like manner. Now,
    do the following in order:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像URL收集：**此步骤利用几行JavaScript代码来收集图像URL。收集到的URL可以在Python中用于下载图像。为此，选择JavaScript控制台（假设你使用的是Chrome浏览器，也可以使用Firefox），点击View
    | Developer | JavaScript console（在macOS上），或自定义并控制**Google Chrome** | **更多工具**
    | **开发者工具**（Windows操作系统）。一旦选择了JavaScript控制台，这将使你能够以REPL的方式执行JavaScript。接下来，按照以下步骤操作：'
- en: Scroll down the page until you have found all images relevant to your query.
    From there, you need to grab the URLs for the images.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动页面，直到找到与你的查询相关的所有图像。然后，你需要抓取这些图像的URL。
- en: 'Switch back to the JavaScript console and then copy and paste the following
    JavaScript snippet into the console:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 切换回JavaScript控制台，然后将以下JavaScript代码片段复制并粘贴到控制台中：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code snippet will pull down the jQuery JavaScript library, and
    now you can use a CSS selector to grab a list of URLs using the following snippet:'
  id: totrans-85
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码片段将下载jQuery JavaScript库，现在你可以使用CSS选择器通过以下代码片段抓取URL列表：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, write the URLs to a file (one per line) using the following snippet:'
  id: totrans-87
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下代码片段将URL写入文件（每行一个）：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once you execute the preceding code snippet, you will have a file named `emotion_images_urls.txt` in
    your default download directory.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码片段后，你将在默认下载目录下得到一个名为`emotion_images_urls.txt`的文件。
- en: '**Downloading the images**: Now, you are ready to download the images running `download_images.py`
    (available in the code folder of the chapter) on the previously downloaded `emotion_images_urls.txt`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载图像：**现在，你已经准备好下载图像，运行`download_images.py`（位于章节代码文件夹中），使用之前下载的`emotion_images_urls.txt`文件：'
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Exploration**:Once we have downloaded the images, we need to explore the
    images in order to delete the irrelevant ones. We can do this through a bit of
    manual inspection. After that, we need to resize and crop match our requirements.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**探索**：一旦我们下载了图像，就需要对图像进行探索，以便删除不相关的图像。我们可以通过手动检查来完成此过程。之后，我们需要调整图像的大小并裁剪以满足要求。'
- en: Data exploration
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'In this section, we will examine in more detail the datasets that we will be
    using:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地检查我们将使用的数据集：
- en: '**HAR dataset**:The dataset is a text file that consists of the different subjects
    accelerations for each of the six activities. We can do a data distribution check
    for the dataset as it is not easy to perceive the data distribution by looking
    at the text file only. The following graph summarizes the breakdown for the training
    set:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAR数据集**：该数据集是一个文本文件，包含了六种活动中不同受试者的加速度数据。由于仅凭文本文件很难直观感知数据分布，我们可以对数据集进行数据分布检查。以下图表总结了训练集的分布情况：'
- en: ^(![](img/6bccc879-56a3-4d13-8030-66e1b6500f92.png))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ^(![](img/6bccc879-56a3-4d13-8030-66e1b6500f92.png))
- en: 'As we can see from the preceding graph, the training dataset consists of more
    walking and jogging data than the other four activities. This is good for the
    DL model, since walking and jogging are moving activities, where the range of acceleration
    data could be wide. To visualize this, we have explored activity-wise acceleration
    measurements/data for 200 time steps for each activity. The following screenshot represents
    200 time step acceleration measurements for sitting:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从前面的图表中看到的，训练数据集包含比其他四种活动更多的步行和慢跑数据。这对深度学习模型是有利的，因为步行和慢跑属于动态活动，涉及的加速度数据范围可能较广。为了可视化这一点，我们探索了每项活动的200个时间步加速度测量数据。以下截图显示了坐着活动的200个时间步加速度测量值：
- en: '![](img/fb95debf-8537-4d23-a147-f39438dd5467.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb95debf-8537-4d23-a147-f39438dd5467.png)'
- en: 'The following screenshot represents 200 time step acceleration measurements
    for standing:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了站立活动的200个时间步加速度测量值：
- en: '![](img/de028247-dee1-43f9-9457-481c235ff5ed.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de028247-dee1-43f9-9457-481c235ff5ed.png)'
- en: 'The following screenshot represents 200 time step acceleration measurements
    for walking:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了步行活动的200个时间步加速度测量值：
- en: '![](img/5c2dedc6-73ea-4372-9891-cf093c734c0e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c2dedc6-73ea-4372-9891-cf093c734c0e.png)'
- en: 'The following screenshot represents 200 time step acceleration measurements
    for jogging:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了慢跑活动的200个时间步加速度测量值：
- en: '![](img/464b6e30-9e87-493c-b45e-e71b4af961b6.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/464b6e30-9e87-493c-b45e-e71b4af961b6.png)'
- en: It is clear from the preceding diagrams that walking and jogging activities
    are busier than the other activities as they reflect the user's movements.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示可以看出，步行和慢跑活动比其他活动更为活跃，因为它们反映了用户的运动状态。
- en: '**FER dataset**:We need to convert the FER2013 dataset pixel values of the
    face images into actual images to explore them. We can use the following code
    to convert the pixel values to images:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FER数据集**：我们需要将FER2013数据集中面部图像的像素值转换为实际图像，以便进行进一步探索。我们可以使用以下代码将像素值转换为图像：'
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can execute the previous code using the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码执行前面的代码：
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once we have the diagram, we can run the following code for image exploration:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了图表，就可以运行以下代码进行图像探索：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will produce a figure similar to the following image:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成类似以下图像的结果：
- en: '![](img/bff0196b-d563-496a-ad52-b2a3ad85bdb3.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bff0196b-d563-496a-ad52-b2a3ad85bdb3.png)'
- en: 'As we can see in the preceding image, the FER dataset is preprocessed well.
    On the other hand, the second dataset (we named it FER2019) is not preprocessed,
    including the image sizes, as can be seen in the following image:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，FER数据集经过良好的预处理。另一方面，第二个数据集（我们将其命名为FER2019）没有进行预处理，包括图像大小，正如以下图像所示：
- en: '![](img/738621cb-f6c4-4c56-941f-f477391992e9.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/738621cb-f6c4-4c56-941f-f477391992e9.png)'
- en: Data preprocessing
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Data preprocessing is an essential step for a deep learning pipeline. The HAR
    and FER2013 datasets are preprocessed well. However, the downloaded image files
    for the second dataset of use case two are not preprocessed. As shown in the preceding
    image, the images are not uniform in size or pixels and the dataset is not large
    in size; hence, they require data augmentation. Popular augmentation techniques
    are flip, rotation, scale, crop, translation, and Gaussian noise. Many tools are
    available for each of these activities. You can use the tools or write their own
    script to do the data augmentation. A useful tool is **Augmentor**, a Python library
    for machine learning. We can install the tool in our Python and use it for augmentation.
    The following code (`data_augmentation.py`) is a simple data augmentation process
    that executes flipping, rotation, cropping, and resizing of the input images:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是深度学习流程中的一个关键步骤。HAR 和 FER2013 数据集已经进行了良好的预处理。然而，用例二的第二个数据集的下载图像文件尚未经过预处理。如前图所示，这些图像在大小和像素上不统一，且数据集的大小也不大，因此需要进行数据增强。常见的增强技术包括翻转、旋转、缩放、裁剪、平移和高斯噪声。每个活动都有许多可用的工具。你可以使用这些工具或编写自己的脚本来进行数据增强。一款有用的工具是**Augmentor**，这是一个用于机器学习的
    Python 库。我们可以在 Python 中安装该工具并使用它进行增强。以下代码（`data_augmentation.py`）是一个简单的数据增强过程，它执行图像的翻转、旋转、裁剪和调整大小：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following image presents two original images and their augmented samples
    (3 out of 25 samples):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了两张原始图像及其增强后的样本（25个样本中的3个）：
- en: '![](img/16b0faa7-a06b-4825-b765-cda89f838ecd.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16b0faa7-a06b-4825-b765-cda89f838ecd.png)'
- en: As shown in the preceding image, the augmented images are uniform in size, flipped,
    and rotated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，增强后的图像大小一致，已进行翻转和旋转处理。
- en: 'The following are two key issues to be noted during the training image set
    preparation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在准备训练图像集时需要注意的两个关键问题：
- en: '**Data size**: We need to collect at least 100 images for each class to train
    a model that works well. The more we can gather, the better the likely accuracy
    of the trained model. However, one-shot learning (an object categorization technique)
    can work using fewer than 100 training samples. We also made sure that the images
    are a good representation of what our application will actually face in real implementation.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据大小**：我们需要收集至少 100 张图像每个类别，以训练一个效果良好的模型。收集的越多，训练后模型的准确度越高。然而，一次性学习（对象分类技术）可以在少于
    100 个训练样本的情况下工作。我们还确保图像能够很好地代表我们在实际应用中可能遇到的情况。'
- en: '**Data heterogeneity***:* Data collected for training should be heterogeneous.
    For example, images for FER should be from a diverse range of skin tones or different
    views of the same expressions.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据异质性**：用于训练的数据应具有异质性。例如，FER 数据集中的图像应来自不同肤色或相同表情的不同视角。'
- en: Model training
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: As we mentioned earlier, we are using LSTM for use case one and two implementations
    of CNN (simple CNN and Mobilenet V1) for use case two. All of these DL implementations
    support transfer learning for both use cases that do not require training from
    scratch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们在用例一中使用 LSTM，在用例二中使用 CNN（简单的 CNN 和 Mobilenet V1）。所有这些深度学习实现都支持迁移学习，适用于两个用例，无需从头开始训练。
- en: Use case one
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例一
- en: 'We consider a stacked LSTM, which is a popular DL model for sequence prediction,
    including time series problems. A stacked LSTM architecture consists of two or
    more LSTM layers. We implemented the HAR for use case one, using a two-layered
    stacked LSTM architecture. The following diagram presents a two-layered LSTM,
    where the first layer provides a sequence of outputs instead of a single value
    output to the second LSTM layer:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑使用堆叠 LSTM，它是一个常用于序列预测（包括时间序列问题）的流行深度学习模型。堆叠 LSTM 架构由两层或更多 LSTM 层组成。我们为用例一实现了
    HAR，采用了一个两层的堆叠 LSTM 架构。以下图示展示了一个两层的 LSTM，其中第一层向第二层 LSTM 提供一系列输出，而非单一的值输出：
- en: '![](img/06b140d6-c70c-442a-93af-5194e6d1018f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06b140d6-c70c-442a-93af-5194e6d1018f.png)'
- en: 'We can train and test the model by running the `LSTM -HAR.py` code, available
    in the `use-case-1` folder (after making the necessary changes to your setup,
    such as the `data` directory):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行 `LSTM -HAR.py` 代码来训练和测试模型，代码位于 `use-case-1` 文件夹中（在对你的环境做必要修改后，例如`data`目录）：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Use case two
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例二
- en: 'We used two different architectures of CNN for the FER-based emotion detection
    in the smart classroom. The first one is a simple CNN architecture. To train the
    model on the FER2013 dataset, we need to run `CNN-FER2013.py`, which is available
    in the chapter''s `use-case-2` code folder, or use the notebook. To run in all
    default settings of `CNN-FER2013.py` *(*after making any necessary changes to
    your setup, such as the `data` directory)*,* we need to run the following in the
    Command Prompt:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两种不同架构的卷积神经网络（CNN）来进行智能教室中的基于FER的情感检测。第一种是简单的CNN架构。为了在FER2013数据集上训练模型，我们需要运行`CNN-FER2013.py`，该文件可以在本章的`use-case-2`代码文件夹中找到，或者使用笔记本。要在`CNN-FER2013.py`的所有默认设置下运行*（在对设置做出必要更改后，比如`data`目录）*，我们需要在命令提示符下运行以下命令：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The training and testing of the model on the FER2103 dataset could take a few
    hours. The following diagram, generated from the TensorBoard log files, presents
    the network used for use case two:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在FER2013数据集上训练和测试模型可能需要几个小时。下图是通过TensorBoard日志文件生成的，展示了用于用例二的网络：
- en: '![](img/2e49dc9c-ed05-4ac9-9e21-002a00a9b6e4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e49dc9c-ed05-4ac9-9e21-002a00a9b6e4.png)'
- en: 'We can retrain Mobilenet V1 on FER2019 by running the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码在FER2019数据集上重新训练Mobilenet V1：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once we run the preceding commands, they will generate the retrain models (`retrained_graph.pb`)
    and label text (`retrained_labels.txt` ) in the given directory. This will also
    store the model''s summary information in a directory. The summary information
    (the `--summaries_dir` argument with `retrain_logs` as the default value) can
    be used by the TensorBoard to visualize different aspects of the models, including
    the networks and their performance graphs. If we type the following command into
    the Terminal or command window, it will run the TensorBoard:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行前面的命令，它们将生成重新训练的模型（`retrained_graph.pb`）和标签文本（`retrained_labels.txt`）到指定目录中。这也会将模型的摘要信息存储在一个目录中。摘要信息（`--summaries_dir`参数默认值为`retrain_logs`）可以被TensorBoard用于可视化模型的不同方面，包括网络结构和性能图。如果我们在终端或命令窗口中输入以下命令，它将启动TensorBoard：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once the TensorBoard is running, navigate your web browser to `localhost:6006` to
    view the TensorBoard and the network of the corresponding model. The following
    diagram presents a network for the Mobilenet V1 architecture used in the implementation:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦TensorBoard启动，可以通过浏览器访问`localhost:6006`来查看TensorBoard和相应模型的网络。下图展示了在实现中使用的Mobilenet
    V1架构的网络：
- en: '![](img/d39947f7-a751-4270-9fbf-2630e6d6610b.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d39947f7-a751-4270-9fbf-2630e6d6610b.png)'
- en: Model evaluation
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'We can evaluate the models in three different aspects:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从三个不同的方面评估模型：
- en: Learning/(re)training time
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习/(重新)训练时间
- en: Storage requirement
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储需求
- en: Performance (accuracy)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能（准确度）
- en: In terms of training time, in a desktop (Intel Xenon CPU E5-1650 v3@3.5 GHz
    and 32 GB RAM) with GPU support, LSTM on the HAR dataset, CNN on FER2013, and
    Mobilenet V1 on the FER2019 dataset, it took less than an hour to train/retrain
    the model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时间方面，在一台桌面电脑（Intel Xenon CPU E5-1650 v3@3.5 GHz，32 GB RAM）并支持GPU的环境下，LSTM在HAR数据集上，CNN在FER2013数据集上，以及Mobilenet
    V1在FER2019数据集上的训练/重新训练时间都少于一个小时。
- en: 'The storage requirement of a model is an essential consideration in resource-constrained
    IoT devices. The following diagram presents the storage requirements for the three
    models we tested for the two use cases. As shown in the diagram, the simple CNN
    takes up only 2.6 MB, smaller than one sixth of the Mobilenet V1 (17.1 MB). Also,
    the LSTM for the HAR took up 1.6 MB (not in the diagram) of storage. In terms
    of storage requirements, all the models are fine to be deployed in many resource-constrained
    IoT devices, including Raspberry Pi or smartphones:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的存储需求是资源受限的物联网设备中的一个重要考虑因素。下图展示了我们在两个用例中测试的三种模型的存储需求。如图所示，简单CNN只占用2.6 MB，远小于Mobilenet
    V1（17.1 MB）的一六分之一。此外，HAR用的LSTM占用了1.6 MB（图中未展示）的存储空间。在存储需求方面，所有模型都适合在许多资源受限的物联网设备中部署，包括树莓派或智能手机：
- en: '![](img/d70ca87d-aa69-4dec-8ae4-9e6c18547674.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d70ca87d-aa69-4dec-8ae4-9e6c18547674.png)'
- en: 'Finally, we have evaluated the performance of the models. Two levels of performance
    evaluation can be executed for the use cases:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了模型的性能。可以对用例执行两级性能评估：
- en: Dataset-wide evaluation or testing has been done during the retraining phase
    in the desktop PC platform/server side.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集级别的评估或测试已经在桌面PC平台/服务器端的重新训练阶段完成。
- en: Individual activity signals for human activity and facial images for emotion
    detection were tested or evaluated in the Raspberry Pi 3 environment.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人体活动的单独信号和面部图像情感检测已在树莓派3环境中进行测试或评估。
- en: Model performance (use case one)
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能（用例一）
- en: 'The following graph presents the progressive training and test accuracy of
    the LSTM model against the HAR dataset. As we can see from the graph, training
    accuracy is close to 1.0, or 100%, and test accuracy is above .90, or 90%. With
    this test accuracy, we believe that the LSTM model can detect human activities
    in most cases, including whether the subject is doing the assigned physiotherapy
    activities:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了LSTM模型在HAR数据集上的逐步训练和测试准确率。从图中可以看到，训练准确率接近1.0或100%，而测试准确率在.90或90%以上。通过这个测试准确率，我们相信LSTM模型能够在大多数情况下检测人体活动，包括判断被试是否在进行指定的物理治疗活动：
- en: '![](img/12ffe603-6af5-4dc5-890b-bfa2f92e8b97.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12ffe603-6af5-4dc5-890b-bfa2f92e8b97.png)'
- en: 'The following diagram is a confusion matrix of the model against the HAR test
    dataset. As seen in the diagram, the model gets confused between **Downstairs** and
    **Upstairs**, and **Sitting** and **Standing** activities, as they have very limited
    or zero mobility, which means there is no significant acceleration to differentiate
    them:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是该模型在HAR测试数据集上的混淆矩阵。从图中可以看到，模型在**下楼**和**上楼**、**坐下**和**站立**的活动之间产生了混淆，因为这些活动的移动性非常有限或没有移动，意味着没有显著的加速度来区分它们：
- en: '![](img/0ad58033-786b-41ef-8e67-55988260b906.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ad58033-786b-41ef-8e67-55988260b906.png)'
- en: Model performance (use case two)
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能（用例二）
- en: 'The following screenshot shows the training and validation performance of the
    simple CNN model on the FER2013 dataset. The accuracy of this dataset is not great
    (training–.83, and validation–.63), but the test or validation accuracy should
    be able to detect the distinctive and necessary emotions (such as happy, sad,
    and confused) for the smart classroom:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了简单CNN模型在FER2013数据集上的训练和验证表现。该数据集的准确率并不高（训练准确率为.83，验证准确率为.63），但测试或验证准确率应该能够检测到智能教室中必要且独特的情感（如开心、悲伤和困惑）：
- en: '![](img/cee139c1-5676-4c9b-9bb3-34509c430bd6.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cee139c1-5676-4c9b-9bb3-34509c430bd6.png)'
- en: 'The following diagram is a confusion matrix of the model against the FER2013
    test dataset. As expected, the model is showing confusion for all the expressions
    (such as 156 angry expressions being detected as sad expressions). This is one
    of the applications of deep learning where further research is needed to improve
    performance:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是该模型在FER2013测试数据集上的混淆矩阵。如预期，模型在所有表情（例如156个愤怒表情被误识为悲伤表情）上都表现出混淆。这是深度学习的一个应用，进一步研究仍然需要提升性能：
- en: '![](img/35f1d314-5857-4ec4-a487-ac1da3c460f6.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35f1d314-5857-4ec4-a487-ac1da3c460f6.png)'
- en: 'For use case two, we have tested Mobilenet V1\. The following diagrams shows
    the overall performance of model Mobilenet V1 on the FER2019 dataset. As we can
    see from the figure, this is showing better training accuracy, but no improvement
    in validation and test accuracy. One potential reason for this could be the size
    and quality of the data, since, after data augmentation, every sample may not
    contain a facial expression image. Further preprocessing that includes manual
    inspection may improve data quality and the model''s performance:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用例二，我们测试了Mobilenet V1。以下图表展示了模型Mobilenet V1在FER2019数据集上的整体表现。从图中可以看到，这显示了更好的训练准确率，但验证和测试准确率没有提升。造成这一结果的一个潜在原因可能是数据的大小和质量，因为经过数据增强后，每个样本可能不包含面部表情图像。进一步的预处理，包括人工检查，可能会提高数据质量和模型性能：
- en: '![](img/0d2bf23a-5c49-470b-9ab9-a9120a05c56c.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d2bf23a-5c49-470b-9ab9-a9120a05c56c.png)'
- en: '![](img/53844e93-cfa2-40ec-bd76-5c109ec4c24d.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53844e93-cfa2-40ec-bd76-5c109ec4c24d.png)'
- en: 'In order to test the model on an individual image, and transfer the learning
    of the model, we need to do the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在单个图像上测试该模型，并转移模型的学习，我们需要执行以下步骤：
- en: Export the trained model (such as `fer2013_trained.hdf5`) and the `label_image.py`
    file (image classifier) into a Raspberry Pi (installed with TensorFlow)/smartphone.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练好的模型（例如`fer2013_trained.hdf5`）和`label_image.py`文件（图像分类器）导出到树莓派（已安装TensorFlow）/智能手机中。
- en: 'Run the image classifier (do not forget to update the `test_image` path) using
    the following command:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令运行图像分类器（不要忘记更新`test_image`路径）：
- en: '[PRE12]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will produce the test result for your test image.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成你测试图像的测试结果。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Automatic human physiological and psychological state detection is becoming
    a popular means by which people can learn a person's physical and mental state
    to interact and react accordingly. There are many applications within smart education,
    healthcare, and entertainment where these state detection techniques can be useful.
    Machine learning and DL algorithms are essential for these detection techniques.
    In the first part of this chapter, we briefly described different IoT applications
    using human physiological and psychological state detection. We also briefly discussed
    two potential use cases of IoT where DL algorithms can be useful in human physiological
    and psychological state detection. The first use case considers an IoT-based remote
    physiotherapy progress monitoring system. The second use case is an IoT-based
    smart classroom application that uses facial expressions of the students to know
    their feedback. In the second part of the chapter, we briefly discussed the data
    collection process for the use cases, and we discussed the rationale behind selecting
    LSTM for the HAR and CNNs for the FER. The remainder of the chapter described
    all of the necessary components of the DL pipeline for these models and their
    results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 自动检测人体生理和心理状态正成为一种流行的方式，人们通过这种方式了解一个人的身心状态，从而进行相应的互动和反应。在智能教育、医疗保健和娱乐等领域，许多应用都可以利用这些状态检测技术。机器学习和深度学习算法对这些检测技术至关重要。在本章的第一部分，我们简要介绍了利用人体生理和心理状态检测的不同物联网应用。我们还简要讨论了两个潜在的物联网应用案例，其中深度学习算法可以在人类生理和心理状态检测中发挥作用。第一个应用案例考虑了基于物联网的远程物理治疗进度监控系统。第二个应用案例是基于物联网的智能课堂应用，利用学生的面部表情来了解他们的反馈。在本章的第二部分，我们简要讨论了这些应用案例的数据收集过程，并讨论了选择LSTM进行人体活动识别（HAR）和选择卷积神经网络（CNN）进行面部表情识别（FER）的理由。本章的其余部分描述了这些模型的深度学习流程的所有必要组件及其结果。
- en: One of the key challenges in IoT applications is security. Many IoT applications,
    such as driverless cars, connected healthcare, and smart grid, are mission-critical
    applications. Security is an essential element for these and many other IoT applications.
    In the next chapter, we will discuss security in IoT applications, and show how
    deep learning can be used for IoT security solutions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网应用中的一个关键挑战是安全性。许多物联网应用，如无人驾驶汽车、智能医疗和智能电网，都是关键任务应用。安全性是这些以及许多其他物联网应用的核心要素。在下一章中，我们将讨论物联网应用中的安全性，并展示深度学习如何用于物联网安全解决方案。
- en: References
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'K. Rapp, C. Becker, I.D. Cameron, H.H. König, and G. Büchele, *Epidemiology
    of falls in residential aged care: analysis of more than 70,000 falls from residents
    of Bavarian nursing homes*, J. Am. Med. Dir. Assoc. 13 (2) (2012) 187.e1–187.e6.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K. Rapp, C. Becker, I.D. Cameron, H.H. König 和 G. Büchele, *老年护理机构跌倒流行病学：分析来自巴伐利亚养老院居民的超过70,000起跌倒事件*,
    J. Am. Med. Dir. Assoc. 13 (2) (2012) 187.e1–187.e6.
- en: Centers for disease control and prevention. *Cost of Falls Among Older Adults*,
    2014\. [http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html](http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html)
    (accessed 14.04.19).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 疾病控制与预防中心. *老年人跌倒的成本*, 2014\. [http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html](http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html)（访问时间：14.04.19）。
- en: M. S. Hossain and G. Muhammad, *Emotion-Aware Connected Healthcare Big Data
    Towards 5G*, in IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2399-2406,
    Aug. 2018.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. S. Hossain 和 G. Muhammad, *面向5G的情感感知连接健康大数据*，发表于IEEE物联网期刊，第5卷，第4期，页码2399-2406，2018年8月。
- en: 'M. A. Razzaque, Muta Tah Hira, and Mukta Dira. 2017\. Q*oS in Body Area Networks:
    A Survey. ACM Trans*. Sen. Netw. 13, 3, Article 25 (August 2017), 46 pages.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. A. Razzaque, Muta Tah Hira 和 Mukta Dira. 2017\. 体域网中的Q*oS：一项调查。ACM Trans*.
    Sen. Netw. 13, 3, 第25篇文章（2017年8月），46页。
- en: Nigel Bosch, Sidney K. D'Mello, Ryan S. Baker, Jaclyn Ocumpaugh, Valerie Shute,
    Matthew Ventura, Lubin Wang, and Weinan Zhao. 2016\. Detecting student emotions
    in computer-enabled classrooms. In *Proceedings of the Twenty-Fifth International
    Joint Conference on Artificial Intelligence*(IJCAI'16), Gerhard Brewka (Ed.).
    AAAI Press 4125-4129.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nigel Bosch, Sidney K. D'Mello, Ryan S. Baker, Jaclyn Ocumpaugh, Valerie Shute,
    Matthew Ventura, Lubin Wang 和 Weinan Zhao. 2016\. 在计算机辅助课堂中检测学生情绪。载于 *第二十五届国际人工智能联合会议论文集*（IJCAI'16），Gerhard
    Brewka（编）。AAAI Press 4125-4129。
- en: 'Isabel Sagenmüller, *Student retention: 8 reasons people drop out of higher
    education*, [https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education](https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isabel Sagenmüller, *学生保持：8个导致人们辍学的原因*, [https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education](https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education)。
- en: Nikki Bardsley, *Drop-out rates among university students increases for third
    consecutive year*, [https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year](https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nikki Bardsley, *大学生辍学率连续第三年上升*, [https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year](https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year)。
- en: S. Hochreiter and J. Schmidhuber, *Long Short-Term Memory*, neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Hochreiter 和 J. Schmidhuber, *长短期记忆*, 神经计算, 第9卷，第8期, 第1735–1780页, 1997年。
- en: '[http://www.cis.fordham.edu/wisdm/dataset.php](http://www.cis.fordham.edu/wisdm/dataset.php).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.cis.fordham.edu/wisdm/dataset.php](http://www.cis.fordham.edu/wisdm/dataset.php)。'
- en: 'I. Goodfellow, D. Erhan, PL Carrier, A. Courville, M. Mirza, B. Hamner, W.
    Cukierski, Y. Tang, DH Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis,
    J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra,
    J. Xie, L. Romaszko, B. Xu, Z. Chuang, and Y. Bengio., *Challenges in Representation
    Learning: A report on three machine learning contests*. arXiv 2013.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I. Goodfellow, D. Erhan, PL Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski,
    Y. Tang, DH Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis,
    J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra,
    J. Xie, L. Romaszko, B. Xu, Z. Chuang, 和 Y. Bengio, *表征学习中的挑战：三项机器学习竞赛报告*. arXiv
    2013年。
