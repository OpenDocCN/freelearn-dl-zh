- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Building Workflows with LangGraph
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LangGraph 构建工作流程
- en: So far, we’ve learned about LLMs, LangChain as a framework, and how to use LLMs
    with LangChain in a vanilla mode (just asking to generate a text output based
    on a prompt). In this chapter, we’ll start with a quick introduction to LangGraph
    as a framework and how to develop more complex workflows with LangChain and LangGraph
    by chaining together multiple steps. As an example, we’ll discuss parsing LLM
    outputs and look into error handling patterns with LangChain and LangGraph. Then,
    we’ll continue with more advanced ways to develop prompts and explore what building
    blocks LangChain offers for few-shot prompting and other techniques.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了 LLMs、LangChain 作为框架，以及如何在纯模式（仅基于提示生成文本输出）下使用 LangChain 与 LLMs
    结合。在本章中，我们将从 LangGraph 作为框架的快速介绍开始，并探讨如何通过连接多个步骤来使用 LangChain 和 LangGraph 开发更复杂的工作流程。作为一个例子，我们将讨论解析
    LLM 输出，并使用 LangChain 和 LangGraph 探讨错误处理模式。然后，我们将继续探讨开发提示的更高级方法，并探索 LangChain 为少样本提示和其他技术提供的构建块。
- en: We’re also going to cover working with multimodal inputs, utilizing the long
    context, and adjusting your workloads to overcome limitations related to the context
    window size. Finally, we’ll look into the basic mechanisms of managing memory
    with LangChain. Understanding these fundamental and key techniques will help us
    read LangGraph code, understand tutorials and code samples, and develop our own
    complex workflows. We’ll, of course, discuss what LangGraph workflows are and
    will continue building on that skill in *Chapters 5* and *6*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍如何处理多模态输入，利用长上下文，以及调整工作负载以克服与上下文窗口大小相关的限制。最后，我们将探讨使用 LangChain 管理内存的基本机制。理解这些基本和关键技术将帮助我们阅读
    LangGraph 代码，理解教程和代码示例，并开发我们自己的复杂工作流程。当然，我们还将讨论 LangGraph 工作流程是什么，并在第 5 章和第 6
    章中继续构建这一技能。
- en: 'In a nutshell, we’ll cover the following main topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在本章中，我们将涵盖以下主要主题：
- en: LangGraph fundamentals
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangGraph 基础知识
- en: Prompt engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Working with short context windows
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与短上下文窗口一起工作
- en: Understanding memory mechanisms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解内存机制
- en: 'As always, you can find all the code samples on our public GitHub repository
    as Jupyter notebooks: [https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3](https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，您可以在我们的公共 GitHub 仓库中找到所有代码示例，作为 Jupyter 笔记本：[https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3](https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3)。
- en: LangGraph fundamentals
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangGraph 基础知识
- en: LangGraph is a framework developed by LangChain (as a company) that helps control
    and orchestrate workflows. Why do we need another orchestration framework? Let’s
    park this question until [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), where
    we’ll touch on agents and agentic workflows, but for now, let us mention the flexibility
    of LangGraph as an orchestration framework and its robustness in handling complex
    scenarios.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph 是由 LangChain（作为一家公司）开发的一个框架，它有助于控制和编排工作流程。为什么我们需要另一个编排框架呢？让我们把这个问题放在一边，直到第
    5 章（[E_Chapter_5.xhtml#_idTextAnchor231](E_Chapter_5.xhtml#_idTextAnchor231)），在那里我们将触及代理和代理工作流程，但现在，让我们提到
    LangGraph 作为编排框架的灵活性及其在处理复杂场景中的稳健性。
- en: Unlike many other frameworks, LangGraph allows cycles (most other orchestration
    frameworks operate only with directly acyclic graphs), supports streaming out
    of the box, and has many pre-built loops and components dedicated to generative
    AI applications (for example, human moderation). LangGraph also has a very rich
    API that allows you to have very granular control of your execution flow if needed.
    This is not fully covered in our book, but just keep in mind that you can always
    use a more low-level API if you need to.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他框架不同，LangGraph 允许循环（大多数其他编排框架仅使用直接无环图），支持开箱即用的流式处理，并且有许多预构建的循环和组件，专门用于生成式
    AI 应用（例如，人工审核）。LangGraph 还有一个非常丰富的 API，允许您在需要时对执行流程进行非常细粒度的控制。这在我们书中并未完全涵盖，但请记住，如果您需要，您始终可以使用更底层的
    API。
- en: A **Directed Acyclic Graph (DAG)** is a special type of graph in graph theory
    and computer science. Its edges (connections between nodes) have a direction,
    which means that the connection from node A to node B is different from the connection
    from node B to node A. It has no cycles. In other words, there is no path that
    starts at a node and returns to the same node by following the directed edges.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**有向无环图（DAG**）是图论和计算机科学中的一种特殊类型的图。它的边（节点之间的连接）有方向，这意味着从节点A到节点B的连接与从节点B到节点A的连接不同。它没有环。换句话说，没有路径可以从一个节点开始，通过跟随有向边返回到同一个节点。'
- en: DAGs are often used as a model of workflows in data engineering, where nodes
    are tasks and edges are dependencies between these tasks. For example, an edge
    from node A to node B means that we need output from node A to execute node B.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据工程中，DAG（有向无环图）通常用作工作流程的模型，其中节点是任务，边是这些任务之间的依赖关系。例如，从节点A到节点B的边意味着我们需要从节点A获取输出以执行节点B。
- en: For now, let’s start with the basics. If you’re new to this framework, we would
    also highly recommend a free online course on LangGraph that is available at [https://academy.langchain.com/](https://academy.langchain.com/)
    to deepen your understanding.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，让我们从基础知识开始。如果你是框架的新手，我们强烈推荐参加一个免费的在线课程，该课程可在[https://academy.langchain.com/](https://academy.langchain.com/)找到，以加深你对LangGraph的理解。
- en: State management
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态管理
- en: State management is crucial in real-world AI applications. For example, in a
    customer service chatbot, the state might track information such as customer ID,
    conversation history, and outstanding issues. LangGraph’s state management lets
    you maintain this context across a complex workflow of multiple AI components.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的AI应用中，状态管理至关重要。例如，在一个客户服务聊天机器人中，状态可能会跟踪客户ID、对话历史和未解决的问题等信息。LangGraph的状态管理让你能够在多个AI组件的复杂工作流程中维护这个上下文。
- en: LangGraph allows you to develop and execute complex workflows called **graphs**.
    We will use the words *graph* and *workflow* interchangeably in this chapter.
    A graph consists of nodes and edges between them. Nodes are components of your
    workflow, and a workflow has a *state*. What is it? Firstly, a state makes your
    nodes aware of the current context by keeping track of the user input and previous
    computations. Secondly, a state allows you to persist your workflow execution
    at any point in time. Thirdly, a state makes your workflow truly interactive since
    a node can change the workflow’s behavior by updating the state. For simplicity,
    think about a state as a Python dictionary. Nodes are Python functions that operate
    on this dictionary. They take a dictionary as input and return another dictionary
    that contains keys and values to be updated in the state of the workflow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph允许你开发和执行复杂的称为**图**的工作流程。在本章中，我们将交替使用*图*和*工作流程*这两个词。一个图由节点及其之间的边组成。节点是工作流程的组成部分，而工作流程有一个*状态*。那是什么意思呢？首先，状态通过跟踪用户输入和之前的计算来使节点意识到当前上下文。其次，状态允许你在任何时间点持久化你的工作流程执行。第三，状态使你的工作流程真正交互式，因为一个节点可以通过更新状态来改变工作流程的行为。为了简单起见，可以把状态想象成一个Python字典。节点是操作这个字典的Python函数。它们接受一个字典作为输入，并返回另一个包含要更新工作流程状态的键和值的字典。
- en: 'Let’s understand that with a simple example. First, we need to define a state’s
    schema:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来理解这一点。首先，我们需要定义一个状态的模式：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A `TypedDict` is a Python type constructor that allows to define dictionaries
    with a predefined set of keys and each key can have its own type (as opposed to
    a `Dict[str, str]` construction).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`TypedDict`是一个Python类型构造函数，允许定义具有预定义键集的字典，每个键都可以有自己的类型（与`Dict[str, str]`构造相反）。'
- en: LangGraph state’s schema shouldn’t necessarily be defined as a `TypedDict`;
    you can use data classes or Pydantic models too.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph状态的模式不一定必须定义为`TypedDict`；你也可以使用数据类或Pydantic模型。
- en: 'After we have defined a schema for a state, we can define our first simple
    workflow:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义了状态的模式之后，我们可以定义我们的第一个简单工作流程：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we defined two Python functions that are components of our workflow. Then,
    we defined our workflow by providing a state’s schema, adding nodes and edges
    between them. `add_node` is a convenient way to add a component to your graph
    (by providing its name and a corresponding Python function), and you can reference
    this name later when you define edges with `add_edge`. `START` and `END` are reserved
    built-in nodes that define the beginning and end of the workflow accordingly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了两个Python函数，它们是我们工作流程的组成部分。然后，我们通过提供状态的模式，在它们之间添加节点和边来定义我们的工作流程。`add_node`是一种方便的方法，可以将组件添加到您的图中（通过提供其名称和相应的Python函数），您可以在定义边时使用`add_edge`引用此名称。`START`和`END`是保留的内置节点，分别定义工作流程的开始和结束。
- en: 'Let’s take a look at our workflow by using a built-in visualization mechanism:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用内置的可视化机制来查看我们的工作流程：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 3.1: LangGraph built-in visualization of our first workflow](img/B32363_03_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1：LangGraph内置可视化我们的第一个工作流程](img/B32363_03_01.png)'
- en: 'Figure 3.1: LangGraph built-in visualization of our first workflow'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：LangGraph内置可视化我们的第一个工作流程
- en: Our function accesses the state by simply reading from the dictionary that LangGraph
    automatically provides as input. LangGraph isolates state updates. When a node
    receives the state, it gets an immutable copy, not a reference to the actual state
    object. The node must return a dictionary containing the specific keys and values
    it wants to update. LangGraph then handles merging these updates into the master
    state. This pattern prevents side effects and ensures that state changes are explicit
    and traceable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能通过简单地从LangGraph自动提供的输入字典中读取来访问状态。LangGraph隔离状态更新。当一个节点接收到状态时，它得到一个不可变的副本，而不是实际状态对象的引用。节点必须返回一个包含它想要更新的特定键和值的字典。LangGraph然后将这些更新合并到主状态中。这种模式防止了副作用，并确保状态更改是明确和可追踪的。
- en: 'The only way for a node to modify a state is to provide an output dictionary
    with key-value pairs to be updated, and LangGraph will handle it. A node should
    modify at least one key in the state. A `graph` instance itself is a `Runnable`
    (to be precise, it inherits from `Runnable`) and we can execute it. We should
    provide a dictionary with the initial state, and we’ll get the final state as
    an output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 节点修改状态的唯一方式是提供一个包含要更新的键值对的输出字典，LangGraph将处理它。节点至少应该修改状态中的一个键。`graph`实例本身就是一个`Runnable`（更准确地说，它继承自`Runnable`），我们可以执行它。我们应该提供一个包含初始状态的字典，我们将得到最终状态作为输出：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We used a very simple graph as an example. With your real workflows, you can
    define parallel steps (for example, you can easily connect one node with multiple
    nodes) and even cycles. LangGraph executes the workflow in so-called *supersteps*
    that can call multiple nodes at the same time (and then merge state updates from
    these nodes). You can control the depth of recursion and amount of overall supersteps
    in the graph, which helps you avoid cycles running forever, especially because
    the LLMs output is non-deterministic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个非常简单的图作为示例。对于您的实际工作流程，您可以定义并行步骤（例如，您可以轻松地将一个节点与多个节点连接起来）甚至循环。LangGraph通过所谓的*超级步骤*执行工作流程，这些步骤可以同时调用多个节点（然后合并这些节点的状态更新）。您可以在图中控制递归深度和总的超级步骤数量，这有助于您避免循环无限运行，尤其是在LLMs的输出非确定性时。
- en: '**A superstep** on LangGraph represents a discrete iteration over one or a
    few nodes, and it’s inspired by Pregel, a system built by Google for processing
    large graphs at scale. It handles parallel execution of nodes and updates sent
    to the central graph’s state.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**LangGraph上的超级步骤**代表对一或几个节点的离散迭代，它受到了Google构建的用于大规模处理大型图的系统Pregel的启发。它处理节点的并行执行和发送到中央图状态的状态更新。'
- en: In our example, we used direct edges from one node to another. It makes our
    graph no different from a sequential chain that we could have defined with LangChain.
    One of the key LangGraph features is the ability to create conditional edges that
    can direct the execution flow to one or another node depending on the current
    state. A conditional edge is a Python function that gets the current state as
    an input and returns a string with the node’s name to be executed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用了从节点到另一个节点的直接边。这使得我们的图与我们可以用LangChain定义的顺序链没有区别。LangGraph的一个关键特性是能够创建条件边，这些边可以根据当前状态将执行流程导向一个或另一个节点。条件边是一个Python函数，它接收当前状态作为输入，并返回一个包含要执行节点的名称的字符串。
- en: 'Let’s look at an example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’ve defined an edge `is_suitable_condition` that takes a state and returns
    either an `END` or `generate_application` string by analyzing the current state.
    We used a `Literal` type hint since it’s used by LangGraph to determine which
    destination nodes to connect the source node with when it’s creating conditional
    edges. If you don’t use a type hint, you can provide a list of destination nodes
    directly to the `add_conditional_edges` function; otherwise, LangGraph will connect
    the source node with all other nodes in the graph (since it doesn’t analyze the
    code of an edge function itself when creating a graph). The following figure shows
    the output generated:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个边缘 `is_suitable_condition`，它通过分析当前状态来接收一个状态并返回一个 `END` 或 `generate_application`
    字符串。我们使用了 `Literal` 类型提示，因为 LangGraph 使用它来确定在创建条件边缘时将源节点连接到哪些目标节点。如果你不使用类型提示，你可以直接向
    `add_conditional_edges` 函数提供一个目标节点列表；否则，LangGraph 将将源节点连接到图中所有其他节点（因为它在创建图时不会分析边缘函数的代码）。以下图显示了生成的输出：
- en: '![Figure 3.2: A workflow with conditional edges (represented as dotted lines)](img/B32363_03_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：具有条件边缘的工作流程（用虚线表示）](img/B32363_03_02.png)'
- en: 'Figure 3.2: A workflow with conditional edges (represented as dotted lines)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：具有条件边缘的工作流程（用虚线表示）
- en: Conditional edges are visualized with dotted lines, and now we can see that,
    depending on the output of the `analyze_job_description` step, our graph can perform
    different actions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 条件边缘用虚线表示，现在我们可以看到，根据 `analyze_job_description` 步骤的输出，我们的图可以执行不同的操作。
- en: Reducers
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还原器
- en: So far, our nodes have changed the state by updating the value for a corresponding
    key. From another point of view, at each superstep, LangGraph can produce a new
    value for a given key. In other words, for every key in the state, there’s a sequence
    of values, and from a functional programming perspective, a `reduce` function
    can be applied to this sequence. The default reducer on LangGraph always replaces
    the final value with the new value. Let’s imagine we want to track custom actions
    (produced by nodes) and compare three options.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的节点通过更新对应键的值来改变状态。从另一个角度来看，在每次超级步骤中，LangGraph 可以为给定的键生成一个新的值。换句话说，对于状态中的每个键，都有一个值的序列，并且从函数式编程的角度来看，可以将
    `reduce` 函数应用于这个序列。LangGraph 上的默认还原器始终用新值替换最终值。让我们想象我们想要跟踪自定义操作（由节点产生）并比较三个选项。
- en: 'With the first option, a node should return a list as a value for the key `actions`.
    We provide short code samples just for illustration purposes, but you can find
    full ones on Github. If such a value already exists in the state, it will be replaced
    with the new one:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选择是，节点应该返回一个列表作为 `actions` 键的值。我们只提供简短的代码示例以供说明，但你可以从 Github 上找到完整的示例。如果这样的值已经存在于状态中，它将被新的一个所取代：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Another option is to use the default `add` method with the `Annotated` type
    hint. By using this type hint, we tell the LangGraph compiler that the type of
    our variable in the state is a list of strings, and it should use the `add` method
    to concatenate two lists (if the value already exists in the state and a node
    produces a new one):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用带有 `Annotated` 类型提示的默认 `add` 方法。通过使用此类型提示，我们告诉 LangGraph 编译器状态中变量的类型是字符串列表，并且它应该使用
    `add` 方法将两个列表连接起来（如果值已经存在于状态中并且节点产生了一个新的值）：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last option is to write your own custom reducer. In this example, we write
    a custom reducer that accepts not only a list from the node (as a new value) but
    also a single string that would be converted to a list:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个选项是编写自己的自定义还原器。在这个例子中，我们编写了一个自定义还原器，它不仅接受来自节点的列表（作为新值），还接受一个将被转换为列表的单个字符串：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'LangGraph has a few built-in reducers, and we’ll also demonstrate how you can
    implement your own. One of the important ones is `add_messages`, which allows
    us to merge messages. Many of your nodes would be LLM agents, and LLMs typically
    work with messages. Therefore, according to the conversational programming paradigm
    we’ll talk about in more detail in *Chapters 5* and *6*, you typically need to
    keep track of these messages:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph 有几个内置的还原器，我们还将演示如何实现自己的还原器。其中之一是 `add_messages`，它允许我们合并消息。许多节点将是 LLM
    代理，而 LLM 通常与消息一起工作。因此，根据我们在第 5 章和第 6 章中将更详细讨论的对话编程范式，你通常需要跟踪这些消息：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since this is such an important reducer, there’s a built-in state that you
    can inherit from:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个如此重要的还原器，因此有一个内置的状态你可以继承：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, as we have discussed reducers, let’s talk about another important concept
    for any developer – how to write reusable and modular workflows by passing configurations
    to them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经讨论了reducers，让我们谈谈对任何开发者都非常重要的另一个概念——如何通过传递配置来编写可重用和模块化的工作流。
- en: Making graphs configurable
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使图形可配置
- en: LangGraph provides a powerful API that allows you to make your graph configurable.
    It allows you to separate parameters from user input – for example, to experiment
    between different LLM providers or pass custom callbacks. A node can also access
    the configuration by accepting it as a second argument. The configuration will
    be passed as an instance of `RunnableConfig.`
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph提供了一个强大的API，允许您使您的图形可配置。它允许您将参数与用户输入分离——例如，在不同的LLM提供商之间进行实验或传递自定义回调。一个节点也可以通过接受它作为第二个参数来访问配置。配置将以`RunnableConfig`实例的形式传递。
- en: '`RunnableConfig` is a typed dictionary that gives you control over execution
    control settings. For example, you can control the maximum number of supersteps
    with the `recursion_limit` parameter. `RunnableConfig` also allows you to pass
    custom parameters as a separate dictionary under a `configurable` key.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`RunnableConfig`是一个类型化字典，它让您可以控制执行控制设置。例如，您可以使用`recursion_limit`参数控制最大超步数。`RunnableConfig`还允许您在`configurable`键下作为单独的字典传递自定义参数。'
- en: 'Let’s allow our node to use different LLMs during application generation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们的节点在应用程序生成期间使用不同的LLM：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s now compile and execute our graph with a custom configuration (if you
    don’t provide any, LangGraph will use the default one):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用自定义配置（如果您不提供任何配置，LangGraph将使用默认配置）编译和执行我们的图形：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we’ve established how to structure complex workflows with LangGraph,
    let’s look at a common challenge these workflows face: ensuring LLM outputs follow
    the exact structure needed by downstream components. Robust output parsing and
    graceful error handling are essential for reliable AI pipelines.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经建立了如何使用LangGraph构建复杂工作流的方法，让我们看看这些工作流面临的一个常见挑战：确保LLM的输出符合下游组件所需的精确结构。强大的输出解析和优雅的错误处理对于可靠的AI管道至关重要。
- en: Controlled output generation
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受控输出生成
- en: When you develop complex workflows, one of the common tasks you need to solve
    is to force an LLM to generate an output that follows a certain structure. This
    is called a controlled generation. This way, it can be consumed programmatically
    by the next steps further down the workflow. For example, we can ask the LLM to
    generate JSON or XML for an API call, extract certain attributes from a text,
    or generate a CSV table. There are multiple ways to achieve this, and we’ll start
    exploring them in this chapter and continue in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231).
    Since an LLM might not always follow the exact output structure, the next step
    might fail, and you’ll need to recover from the error. Hence, we’ll also begin
    discussing error handling in this section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开发复杂的工作流时，您需要解决的一个常见任务是强制LLM生成遵循特定结构的输出。这被称为受控生成。这样，它可以由工作流中更进一步的步骤以编程方式消费。例如，我们可以要求LLM为API调用生成JSON或XML，从文本中提取某些属性，或生成CSV表格。有多种方法可以实现这一点，我们将在本章开始探索它们，并在[第5章](E_Chapter_5.xhtml#_idTextAnchor231)中继续讨论。由于LLM可能并不总是遵循确切的输出结构，下一步可能会失败，您需要从错误中恢复。因此，我们还将开始在本节中讨论错误处理。
- en: Output parsing
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出解析
- en: Output parsing is essential when integrating LLMs into larger workflows, where
    subsequent steps require structured data rather than natural language responses.
    One way to do that is to add corresponding instructions to the prompt and parse
    the output.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当将LLM集成到更大的工作流中时，输出解析至关重要，因为后续步骤需要结构化数据而不是自然语言响应。一种方法是向提示中添加相应的指令并解析输出。
- en: 'Let’s see a simple task. We’d like to classify whether a certain job description
    is suitable for a junior Java programmer as a step of our pipeline and, based
    on the LLM’s decision, we’d like to either continue with an application or ignore
    this specific job description. We can start with a simple prompt:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的任务。我们希望将某个工作描述是否适合初级Java程序员作为我们管道的一个步骤进行分类，并根据LLM的决定，我们希望继续申请或忽略这个具体的工作描述。我们可以从一个简单的提示开始：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the output of the LLM is free text, which might be difficult
    to parse or interpret in subsequent pipeline steps. What if we add a specific
    instruction to a prompt?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，LLM的输出是自由文本，这可能在后续的管道步骤中难以解析或解释。如果我们向提示中添加一个特定的指令会怎样呢？
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, how can we parse this output? Of course, our next step can be to just look
    at the text and have a condition based on a string comparison. But that won’t
    work for more complex use cases – for example, if the next step expects the output
    to be a JSON object. To deal with that, LangChain offers plenty of OutputParsers
    that take the output generated by the LLM and try to parse it into a desired format
    (by checking a schema if needed) – a list, CSV, enum, pandas DatafFrame, Pydantic
    model, JSON, XML, and so on. Each parser implements a `BaseGenerationOutputParser`
    interface, which extends the `Runnable` interface with an additional `parse_result`
    method.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何解析这个输出？当然，我们的下一步可以是简单地查看文本并根据字符串比较进行条件判断。但这对于更复杂的使用案例不起作用——例如，如果下一步期望输出是一个
    JSON 对象。为了处理这种情况，LangChain 提供了大量的 OutputParsers，它们可以接受 LLM 生成的输出并将其尝试解析为所需的格式（如果需要，则检查模式）——列表、CSV、枚举、pandas
    DataFrame、Pydantic 模型、JSON、XML 等等。每个解析器都实现了 `BaseGenerationOutputParser` 接口，该接口扩展了
    `Runnable` 接口并添加了一个额外的 `parse_result` 方法。
- en: 'Let’s build a parser that parses an output into an enum:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个解析器，将输出解析为枚举：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `EnumOutputParser` converts text output into a corresponding `Enum` instance.
    Note that the parser handles any generation-like output (not only strings), and
    it actually also strips the output.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`EnumOutputParser` 将文本输出转换为相应的 `Enum` 实例。请注意，解析器处理任何类似生成的输出（不仅仅是字符串），并且实际上它还会去除输出。'
- en: You can find a full list of parsers in the documentation at [https://python.langchain.com/docs/concepts/output_parsers/](https://python.langchain.com/docs/concepts/output_parsers/),
    and if you need your own parser, you can always build a new one!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在文档中找到完整的解析器列表，链接为 [https://python.langchain.com/docs/concepts/output_parsers/](https://python.langchain.com/docs/concepts/output_parsers/)，如果你需要自己的解析器，你总是可以构建一个新的！
- en: 'As a final step, let’s combine everything into a chain:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，让我们将所有内容组合成一个链：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s make this chain part of our LangGraph workflow:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这个链作为我们 LangGraph 工作流程的一部分：
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We made two important changes. First, our newly built chain is now part of a
    Python function that represents the `analyze_job_description` node, and that’s
    how we implement the logic within the node. Second, our conditional edge function
    doesn’t return a string anymore, but we added a mapping of returned values to
    destination edges to the `add_conditional_edges` function, and that’s an example
    of how you could implement a branching of your workflow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出了两个重要的更改。首先，我们新构建的链现在是表示 `analyze_job_description` 节点的 Python 函数的一部分，这就是我们在节点内实现逻辑的方式。其次，我们的条件边函数不再返回一个字符串，而是我们在
    `add_conditional_edges` 函数中添加了返回值到目标边的映射，这是一个如何实现工作流程分支的例子。
- en: Let’s take some time to discuss how to handle potential errors if our parsing
    fails!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花些时间讨论如果我们的解析失败，如何处理潜在的错误！
- en: Error handling
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误处理
- en: 'Effective error management is essential in any LangChain workflow, including
    when handling tool failures (which we’ll explore in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231)
    when we get to tools). When developing LangChain applications, remember that failures
    can occur at any stage:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何 LangChain 工作流程中，有效的错误管理都是必不可少的，包括处理工具故障（我们将在 [*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)
    中探讨，当我们到达工具时）。在开发 LangChain 应用程序时，请记住，失败可能发生在任何阶段：
- en: API calls to foundation models may fail
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用基础模型的 API 可能会失败
- en: LLMs might generate unexpected outputs
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 可能会生成意外的输出
- en: External services could become unavailable
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部服务可能会不可用
- en: One of the possible approaches would be to use a basic Python mechanism for
    catching exceptions, logging them for further analysis, and continuing your workflow
    either by wrapping an exception as a text or by returning a default value. If
    your LangChain chain calls some custom Python function, think about appropriate
    exception handling. The same goes for your LangGraph nodes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的方法之一是使用基本的 Python 机制来捕获异常，将其记录以供进一步分析，并通过将异常包装为文本或返回默认值来继续你的工作流程。如果你的 LangChain
    链调用某些自定义 Python 函数，请考虑适当的异常处理。同样适用于你的 LangGraph 节点。
- en: Logging is essential, especially as you approach production deployment. Proper
    logging ensures that exceptions don’t go unnoticed, allowing you to monitor their
    occurrence. Modern observability tools provide alerting mechanisms that group
    similar errors and notify you about frequently occurring issues.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志是至关重要的，尤其是在你接近生产部署时。适当的日志记录确保异常不会被忽略，从而允许你监控其发生。现代可观察性工具提供警报机制，可以分组类似错误并通知你关于频繁发生的问题。
- en: 'Converting exceptions to text enables your workflow to continue execution while
    providing downstream LLMs with valuable context about what went wrong and potential
    recovery paths. Here is a simple example of how you can log the exception but
    continue executing your workflow by sticking to the default behavior:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将异常转换为文本使您的流程能够在提供有关出错情况和潜在恢复路径的有价值上下文的同时继续执行。以下是一个简单的示例，说明您如何记录异常但通过坚持默认行为继续执行您的流程：
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To test our error handling, we need to simulate LLM failures. LangChain has
    a few `FakeChatModel` classes that help you to test your chain:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的错误处理，我们需要模拟 LLM 失败。LangChain 有几个 `FakeChatModel` 类可以帮助您测试您的链：
- en: '`GenericFakeChatModel` returns messages based on a provided iterator'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GenericFakeChatModel` 根据提供的迭代器返回消息'
- en: '`FakeChatModel` always returns a `"fake_response"` string'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FakeChatModel` 总是返回一个 `"fake_response"` 字符串'
- en: '`FakeListChatModel` takes a list of messages and returns them one by one on
    each invocation'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FakeListChatModel` 接收一条消息列表，并在每次调用时逐个返回它们'
- en: 'Let’s create a fake LLM that fails every second time:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个每两次失败一次的假 LLM：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we provide this to our graph (the full code sample is available in our
    GitHub repo), we can see that the workflow continues despite encountering an exception:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将此提供给我们的图（完整的代码示例可在我们的 GitHub 仓库中找到）时，我们可以看到即使在遇到异常的情况下，工作流程也会继续：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When an error occurs, sometimes it helps to try again. LLMs have a non-deterministic
    nature, and the next attempt might be successful; also, if you’re using third-party
    APIs, various failures might happen on the provider’s side. Let’s discuss how
    to implement proper retries with LangGraph.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生错误时，有时再次尝试可能会有所帮助。LLM 具有非确定性，下一次尝试可能会成功；此外，如果您正在使用第三方 API，提供商的侧可能会有各种失败。让我们讨论如何使用
    LangGraph 实现适当的重试。
- en: Retries
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重试
- en: 'There are three distinct retry approaches, each suited to different scenarios:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同的重试方法，每种方法都适合不同的场景：
- en: Generic retry with Runnable
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Runnable 的通用重试
- en: Node-specific retry policies
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点特定的重试策略
- en: Semantic output repair
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义输出修复
- en: Let’s look at these in turn, starting with generic retries that are available
    for every `Runnable.`
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看这些内容，从每个 `Runnable` 可用的通用重试开始。
- en: 'You can retry any `Runnable`or LangGraph node using a built-in mechanism:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用内置机制重试任何 `Runnable` 或 LangGraph 节点：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With LangGraph, you can also describe specific retries for every node. For
    example, let’s retry our `analyze_job_description` node two times in case of a
    `ValueError`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LangGraph，您还可以为每个节点描述特定的重试。例如，让我们在发生 `ValueError` 的情况下重试我们的 `analyze_job_description`
    节点两次：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The components you’re using, often known as building blocks, might have their
    own retry mechanism that tries to algorithmically fix the problem by giving an
    LLM additional input on what went wrong. For example, many chat models on LangChain
    have client-side retries on specific server-side errors.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在使用的组件，通常称为构建块，可能有自己的重试机制，该机制通过向 LLM 提供有关出错情况的信息来尝试算法性地修复问题。例如，LangChain 上的许多聊天模型在特定的服务器端错误上具有客户端重试。
- en: ChatAnthropic has a `max_retries` parameter that you can define either per instance
    or per request. Another good example of a more advanced building block is trying
    to recover from a parsing error. Retrying a parsing step won’t help since typically
    parsing errors are related to the incomplete LLM output. What if we retry the
    generation step and hope for the best, or actually give LLM a hint about what
    went wrong? That’s exactly what a `RetryWithErrorOutputParser` is doing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ChatAnthropic 有一个 `max_retries` 参数，您可以在每个实例或每个请求中定义。另一个更高级的构建块示例是尝试从解析错误中恢复。重试解析步骤通常没有帮助，因为通常解析错误与不完整的
    LLM 输出有关。如果我们重试生成步骤并寄希望于最好的结果，或者实际上给 LLM 提供有关出错情况的提示呢？这正是 `RetryWithErrorOutputParser`
    所做的。
- en: '![Figure 3.3: Adding a retry mechanism to a chain that has multiple steps](img/B32363_03_03.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3：向具有多个步骤的链添加重试机制](img/B32363_03_03.png)'
- en: 'Figure 3.3: Adding a retry mechanism to a chain that has multiple steps'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：向具有多个步骤的链添加重试机制
- en: 'In order to use `RetryWithErrorOutputParser`, we need to first initialize it
    with an LLM (used to fix the output) and our parser. Then, if our parsing fails,
    we run it and provide our initial prompt (with all substituted parameters), generated
    response, and parsing error:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 `RetryWithErrorOutputParser`，我们首先需要用 LLM（用于修复输出）和我们的解析器来初始化它。然后，如果我们的解析失败，我们运行它并提供我们的初始提示（包含所有替换参数）、生成的响应和解析错误：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can read the source code on GitHub to better understand what’s going on,
    but in essence, that’s an example of a pseudo-code without too many details. We
    illustrate how we can pass the parsing error and the original output that led
    to this error back to an LLM and ask it to fix the problem:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 GitHub 上阅读源代码以更好地理解发生了什么，但本质上，这是一个没有太多细节的伪代码示例。我们展示了如何将解析错误和导致此错误的原输出传递回
    LLM，并要求它修复问题：
- en: '[PRE27]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We introduced the `StrOutputParser` in [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044)
    to convert the output of the ChatModel from an AIMessage to a string so that we
    can easily pass it to the next step in the chain.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第二章*](E_Chapter_2.xhtml#_idTextAnchor044)中介绍了 `StrOutputParser`，用于将 ChatModel
    的输出从 AIMessage 转换为字符串，这样我们就可以轻松地将它传递到链中的下一步。
- en: Another thing to keep in mind is that LangChain building blocks allow you to
    redefine parameters, including default prompts. You can always check them on Github;
    sometimes it’s a good idea to customize default prompts for your workflows.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，LangChain 的构建块允许你重新定义参数，包括默认提示。你总是可以在 GitHub 上检查它们；有时为你的工作流程自定义默认提示是个好主意。
- en: "You can read about other available output-fixing parsers here: [https://python.langchain.com/docs/how_to/output_parser_retry/](https://python.l\uFEFF\
    angchain.com/docs/how_to/output_parser_retry/)."
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: "您可以在此处了解其他可用的输出修复解析器：[https://python.langchain.com/docs/how_to/output_parser_retry/](https://python.l\uFEFF\
    angchain.com/docs/how_to/output_parser_retry/)。"
- en: Fallbacks
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回退
- en: In software development, a **fallback** is an alternative program that allows
    you to recover if your base one fails. LangChain allows you to define fallbacks
    on a `Runnable` level. If execution fails, an alternative chain is triggered with
    the same input parameters. For example, if the LLM you’re using is not available
    for a short period of time, your chain will automatically switch to a different
    one that uses an alternative provider (and probably different prompts).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发中，**回退**是一个备选程序，允许你在基本程序失败时恢复。LangChain 允许你在 `Runnable` 级别定义回退。如果执行失败，将触发一个具有相同输入参数的替代链。例如，如果你使用的
    LLM 在短时间内不可用，你的链将自动切换到使用替代提供者（可能还有不同的提示）的另一个链。
- en: 'Our fake model fails every second time, so let’s add a fallback to it. It’s
    just a lambda that prints a statement. As we can see, every second time, the fallback
    is executed:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假模型每秒失败一次，所以让我们给它添加一个回退。它只是一个打印语句的 lambda 函数。正如我们所看到的，每秒都会执行回退：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Generating complex outcomes that can follow a certain template and can be parsed
    reliably is called structured generation (or controlled generation). This can
    help to build more complex workflows, where an output of one LLM-driven step can
    be consumed by another programmatic step. We’ll pick this up again in more detail
    in *Chapters 5* and *6*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 生成可以遵循特定模板且可以可靠解析的复杂结果称为结构化生成（或受控生成）。这有助于构建更复杂的流程，其中 LLM 驱动的步骤的输出可以被另一个程序性步骤消费。我们将在*第五章*和*第六章*中更详细地介绍这一点。
- en: Prompts that you send to an LLM are one of the most important building blocks
    of your workflows. Hence, let’s discuss some basics of prompt engineering next
    and see how to organize your prompts with LangChain.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 发送到 LLM 的提示是您工作流程中最重要的构建块之一。因此，让我们接下来讨论一些提示工程的基本知识，并看看如何使用 LangChain 组织您的提示。
- en: Prompt engineering
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程
- en: Let’s continue by looking into prompt engineering and exploring various LangChain
    syntaxes related to it. But first, let’s discuss how prompt engineering is different
    from prompt design. These terms are sometimes used interchangeably, and it creates
    a certain level of confusion. As we discussed in [*Chapter 1*](E_Chapter_1.xhtml#_idTextAnchor001),
    one of the big discoveries about LLMs was that they have the capability of domain
    adaptation by *in-context learning*. It’s often enough to describe the task we’d
    like it to perform in a natural language, and even though the LLM wasn’t trained
    on this specific task, it performs extremely well. But as we can imagine, there
    are multiple ways of describing the same task, and LLMs are sensitive to this.
    Improving our prompt (or prompt template, to be specific) to increase performance
    on a specific task is called prompt engineering. However, developing more universal
    prompts that guide LLMs to generate generally better responses on a broad set
    of tasks is called prompt design.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探讨提示工程，并探索与它相关的各种LangChain语法。但首先，让我们讨论提示工程与提示设计之间的区别。这些术语有时被互换使用，这造成了一定程度的混淆。正如我们在[*第一章*](E_Chapter_1.xhtml#_idTextAnchor001)中讨论的那样，关于LLMs的一个重大发现是它们具有通过*上下文学习*进行领域适应的能力。通常，仅用自然语言描述我们希望它执行的任务就足够了，即使LLM没有在这个特定任务上接受过训练，它也能表现出极高的性能。但正如我们可以想象的那样，描述同一任务的方式有很多种，LLMs对这一点很敏感。为了提高特定任务上的性能而改进我们的提示（或更具体地说，提示模板）被称为提示工程。然而，开发更通用的提示，以引导LLMs在广泛的任务集上生成更好的响应，被称为提示设计。
- en: There exists a large variety of different prompt engineering techniques. We
    won’t discuss many of them in detail in this section, but we’ll touch on just
    a few of them to illustrate key LangChain capabilities that would allow you to
    construct any prompts you want.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着大量不同的提示工程技术。我们在这个部分不会详细讨论许多技术，但我们会简要介绍其中的一些，以展示LangChain的关键功能，这些功能将允许你构建任何想要的提示。
- en: 'You can find a good overview of prompt taxonomy in the paper *The Prompt Report:
    A Systematic Survey of Prompt Engineering Techniques*, published by Sander Schulhoff
    and colleagues: [https://arxiv.org/abs/2406.06608](https://arxiv.org/abs/2406.06608).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Sander Schulhoff及其同事发表的论文《提示报告：提示工程技术的系统调查》中找到一个关于提示分类学的良好概述：[https://arxiv.org/abs/2406.06608](https://arxiv.org/abs/2406.06608)。
- en: Prompt templates
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示模板
- en: What we did in [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044) is called *zero-shot
    prompting*. We created a prompt template that contained a description of each
    task. When we run the workflow, we substitute certain values of this prompt template
    with runtime arguments. LangChain has some very useful abstractions to help with
    that.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](E_Chapter_2.xhtml#_idTextAnchor044)中我们所做的是被称为*零样本提示*的工作。我们创建了一个包含每个任务描述的提示模板。当我们运行工作流程时，我们会用运行时参数替换这个提示模板中的某些值。LangChain有一些非常有用的抽象方法来帮助完成这项工作。
- en: 'In [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044), we introduced `PromptTemplate`,
    which is a `RunnableSerializable`. Remember that it substitutes a string template
    during invocation – for example, you can create a template based on f-string and
    add your chain, and LangChain would pass parameters from the input, substitute
    them in the template, and pass the string to the next step in the chain:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](E_Chapter_2.xhtml#_idTextAnchor044)中，我们介绍了`PromptTemplate`，它是一个`RunnableSerializable`。记住，它在调用时替换一个字符串模板——例如，你可以基于f-string创建一个模板并添加你的链，LangChain会从输入中传递参数，在模板中替换它们，并将字符串传递到链的下一步：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For chat models, an input can not only be a string but also a list of `messages`
    – for example, a system message followed by a history of the conversation. Therefore,
    we can also create a template that prepares a list of messages, and a template
    itself can be created based on a list of messages or message templates, as in
    this example:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天模型，输入不仅可以是一个字符串，还可以是`messages`的列表——例如，一个系统消息后跟对话的历史记录。因此，我们也可以创建一个准备消息列表的模板，模板本身可以基于消息列表或消息模板创建，如下例所示：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can also do the same more conveniently without using chat prompt templates
    but by submitting a tuple (just because it’s faster and more convenient sometimes)
    with a type of message and a templated string instead:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以更方便地完成同样的工作，而不使用聊天提示模板，只需提交一个包含消息类型和模板字符串的元组（因为有时它更快更方便）：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Another important concept is a *placeholder*. This substitutes a variable with
    a list of messages provided in real time. You can add a placeholder to your prompt
    by using a `placeholder` hint, or adding a `MessagesPlaceholder`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的概念是*占位符*。它用实时提供的消息列表替换变量。你可以通过使用`placeholder`提示或添加`MessagesPlaceholder`来将占位符添加到提示中。
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now our input consists of four messages – a system message, two history messages
    that we provided, and one human message from a templated prompt. The best example
    of using a placeholder is to input a history of a chat, but we’ll see more advanced
    ones later in this book when we’ll talk about how an LLM interacts with an external
    world or how different LLMs coordinate together in a multi-agent setup.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的输入由四条消息组成——一条系统消息，两条我们提供的历史消息，以及一条来自模板提示的人类消息。使用占位符的最佳例子是输入聊天历史，但我们在本书后面的章节中将会看到更高级的例子，届时我们将讨论LLM如何与外部世界互动，或者不同的LLM如何在多智能体设置中协同工作。
- en: Zero-shot vs. few-shot prompting
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本提示与少样本提示
- en: As we have discussed, the first thing that we want to experiment with is improving
    the task description itself. A description of a task without examples of solutions
    is called **zero-shot** prompting, and there are multiple tricks that you can
    try.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，我们首先想要实验的是改进任务描述本身。没有解决方案示例的任务描述被称为**零样本提示**，你可以尝试多种技巧。
- en: What typically works well is assigning the LLM a certain role (for example,
    “*You are a useful enterprise assistant working for XXX Fortune-500 company*”)
    and giving some additional instruction (for example, whether the LLM should be
    creative, concise, or factual). Remember that LLMs have seen various data and
    they can do different tasks, from writing a fantasy book to answering complex
    reasoning questions. But your goal is to instruct them, and if you want them to
    stick to the facts, you’d better give very specific instructions as part of their
    role profile. For chat models, such role setting typically happens through a system
    message (but remember that, even for a chat model, everything is combined to a
    single input prompt formatted on the server side).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有效的方法是为LLM分配一个特定的角色（例如，“*你是为XXX财富500强公司工作的有用企业助理*”）并给出一些额外的指令（例如，LLM是否应该具有创造性、简洁或事实性）。记住，LLM已经看到了各种数据，它们可以执行不同的任务，从写奇幻小说到回答复杂的推理问题。但你的目标是指导它们，如果你想让他们坚持事实，你最好在它们的角色配置文件中给出非常具体的指令。对于聊天模型，这种角色设置通常通过系统消息完成（但请记住，即使是聊天模型，所有内容也是组合成单个输入提示，在服务器端格式化）。
- en: 'The Gemini prompting guide recommends that each prompt should have four parts:
    a persona, a task, a relevant context, and a desired format. Keep in mind that
    different model providers might have different recommendations on prompt writing
    or formatting, hence if you have complex prompts, always check the documentation
    of the model provider, evaluate the performance of your workflows before switching
    to a new model provider, and adjust prompts accordingly if needed. If you want
    to use multiple model providers in production, you might end up with multiple
    prompt templates and select them dynamically based on the model provider.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini提示指南建议每个提示应包含四个部分：一个角色、一个任务、一个相关上下文和一个期望的格式。请记住，不同的模型提供商可能有不同的提示编写或格式化建议，因此如果你有复杂的提示，始终检查模型提供商的文档，在切换到新的模型提供商之前评估你的工作流程的性能，并在必要时相应地调整提示。如果你想在生产中使用多个模型提供商，你可能会拥有多个提示模板，并根据模型提供商动态选择它们。
- en: Another big improvement can be to provide an LLM with a few examples of this
    specific task as input-output pairs as part of the prompt. This is called few-shot
    prompting. Typically, few-shot prompting is difficult to use in scenarios that
    require a long input (such as RAG, which we’ll talk about in the next chapter)
    but it’s still very useful for tasks with relatively short prompts, such as classification,
    extraction, etc.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重大的改进是，可以在提示中为LLM提供一些这个特定任务的输入输出对作为示例。这被称为少样本提示。通常，在需要长输入的场景中（例如我们将在下一章中讨论的RAG），少样本提示难以使用，但对于相对较短的提示任务，如分类、提取等，仍然非常有用。
- en: Of course, you can always hard-code examples in the prompt template itself,
    but this makes it difficult to manage them as your system grows. A better way
    might be to store examples in a separate file on disk or in a database and load
    them into your prompt.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你总是可以在提示模板本身中硬编码示例，但这会使随着系统增长而管理它们变得困难。可能更好的方式是将示例存储在磁盘上的单独文件或数据库中，并将它们加载到提示中。
- en: Chaining prompts together
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将提示链在一起
- en: 'As your prompts become more advanced, they tend to grow in size and complexity.
    One common scenario is to partially format your prompts, and you can do this either
    by string or function substitution. The latter is relevant if some parts of your
    prompt depend on dynamically changing variables (for example, current date, user
    name, etc.). Below, you can find an example of a partial substitution in a prompt
    template:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的提示变得更加高级，它们的大小和复杂性也会增加。一个常见的场景是部分格式化你的提示，你可以通过字符串或函数替换来实现。如果提示的某些部分依赖于动态变化的变量（例如，当前日期、用户名等），则后者是相关的。下面，你可以在提示模板中找到一个部分替换的示例：
- en: '[PRE33]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Another way to make your prompts more manageable is to split them into pieces
    and chain them together:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使你的提示更易于管理的方法是将它们分成几部分并链接在一起：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can also build more complex substitutions by using the class `langchain_core.prompts.PipelinePromptTemplate`.
    Additionally, you can pass templates into a `ChatPromptTemplate` and they will
    automatically be composed together:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过使用`langchain_core.prompts.PipelinePromptTemplate`类来构建更复杂的替换。此外，你可以将模板传递给`ChatPromptTemplate`，它们将自动组合在一起：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Dynamic few-shot prompting
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态少量提示
- en: As the number of examples used in your few-shot prompts continues to grow, you
    might limit the number of examples to be passed into a specific prompt’s template
    substitution. We select examples for every input – by searching for examples similar
    to the user’s input (we’ll talk more about semantic similarity and embeddings
    in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152)), limiting them by length,
    taking the freshest ones, etc.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你用于少量提示的示例数量继续增加，你可能需要限制传递到特定提示模板替换中的示例数量。我们为每个输入选择示例——通过搜索与用户输入类似的内容（我们将在[*第4章*](E_Chapter_4.xhtml#_idTextAnchor152)中更多地讨论语义相似性和嵌入），通过长度限制，选择最新的等。
- en: '![Figure 3.4: An example of a workflow with a dynamic retrieval of examples
    to be passed to a few-shot prompt](img/B32363_03_04.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4：一个动态检索示例以传递给少量提示的工作流程示例](img/B32363_03_04.png)'
- en: 'Figure 3.4: An example of a workflow with a dynamic retrieval of examples to
    be passed to a few-shot prompt'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：一个动态检索示例以传递给少量提示的工作流程示例
- en: There are a few already built-in selectors under `langchain_core.example_selectors`.
    You can directly pass an instance of an example selector to the `FewShotPromptTemplate`
    instance during instantiation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在`langchain_core.example_selectors`下已经内置了一些选择器。你可以在实例化时直接将示例选择器的实例传递给`FewShotPromptTemplate`实例。
- en: Chain of Thought
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链
- en: The Google Research team introduced the **Chain-of-Thought** (**CoT**) technique
    early in 2022\. They demonstrated that a relatively simple modification to a prompt
    that encouraged a model to generate intermediate step-by-step reasoning steps
    significantly increased the LLM’s performance on complex symbolic reasoning, common
    sense, and math tasks. Such an increase in performance has been replicated multiple
    times since then.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年初，谷歌研究团队引入了**思维链**（**CoT**）技术。他们展示了通过修改提示，鼓励模型生成逐步推理步骤，可以显著提高大型语言模型（LLM）在复杂符号推理、常识和数学任务上的性能。自那时以来，这种性能提升已被多次复制。
- en: 'You can read the original paper introducing CoT, *Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models*, published by Jason Wei and colleagues:
    [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以阅读由Jason Wei及其同事发表的介绍CoT的原始论文，*Chain-of-Thought Prompting Elicits Reasoning
    in Large Language Models*：[https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)。
- en: There are different modifications of CoT prompting, and because it has long
    outputs, typically, CoT prompts are zero-shot. You add instructions that encourage
    an LLM to think about the problem first instead of immediately generating tokens
    representing the answer. A very simple example of CoT is just to add to your prompt
    template something like “Let’s think step by step.”
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CoT提示有不同的修改版本，因为它有很长的输出，通常CoT提示是零样本的。你添加指令鼓励LLM首先思考问题，而不是立即生成代表答案的标记。CoT的一个非常简单的例子就是在你的提示模板中添加类似“让我们一步步思考”的内容。
- en: 'There are various CoT prompts reported in different papers. You can also explore
    the CoT template available on LangSmith. For our learning purposes, let’s use
    a CoT prompt with few-shot examples:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的论文中报告了各种CoT提示。你还可以探索LangSmith上可用的CoT模板。为了我们的学习目的，让我们使用一个带有少量示例的CoT提示：
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We used a prompt from LangSmith Hub – a collection of private and public artifacts
    that you can use with LangChain. You can explore the prompt itself here: [https://smith.langchain.com/hub.](https://smith.langchain.com/hub.
    )'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自LangSmith Hub的提示——LangChain可以使用的私有和公共工件集合。您可以在以下链接中探索提示本身：[https://smith.langchain.com/hub.](https://smith.langchain.com/hub.
    )
- en: 'In practice, you might want to wrap a CoT invocation with an extraction step
    to provide a concise answer to the user. For example, let us first run a `cot_chain`
    and then pass its output (please note that we pass a dictionary with an initial
    `question` and a `cot_output` to the next step) to an LLM that will use a prompt
    to create a final answer based on CoT reasoning:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您可能希望将CoT调用与提取步骤包装在一起，以便向用户提供简洁的答案。例如，让我们首先运行一个`cot_chain`，然后将输出（请注意，我们将包含初始`question`和`cot_output`的字典传递给下一个步骤）传递给一个LLM，该LLM将使用提示根据CoT推理创建最终答案：
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Although a CoT prompt seems to be relatively simple, it’s extremely powerful
    since, as we’ve mentioned, it has been demonstrated multiple times that it significantly
    increases performance in many cases. We will see its evolution and expansion when
    we discuss agents in *Chapters 5* and *6*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CoT提示似乎相对简单，但它非常强大，因为我们已经提到，它已被多次证明在许多情况下显著提高了性能。当我们讨论第5章和第6章中的代理时，我们将看到其演变和扩展。
- en: These days, we can observe how the CoT pattern gets more and more application
    with so-called reasoning models such as o3-mini or gemini-flash-thinking. To a
    certain extent, these models do exactly the same (but often in a more advanced
    manner) – they think before they answer, and this is achieved not only by changing
    the prompt but also by preparing training data (sometimes synthetic) that follows
    a CoT format.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，我们可以观察到所谓的推理模型（如o3-mini或gemini-flash-thinking）的CoT模式越来越广泛地应用。在某种程度上，这些模型确实做了完全相同的事情（但通常以更高级的方式）——它们在回答之前会思考，这不仅仅是通过改变提示，还包括准备遵循CoT格式的训练数据（有时是合成的）。
- en: 'Please note that alternatively to using reasoning models, we can use CoT modification
    with additional instructions by asking an LLM to first generate output tokens
    that represent a reasoning process:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，作为使用推理模型的替代方案，我们可以通过要求LLM首先生成代表推理过程的输出标记来使用CoT修改和附加指令：
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Self-consistency
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自洽性
- en: 'The idea behind self-consistency is simple: let’s increase an LLM’s temperature,
    sample the answer multiple times, and then take the most frequent answer from
    the distribution. This has been demonstrated to improve the performance of LLM-based
    workflows on certain tasks, and it works especially well on tasks such as classification
    or entity extraction, where the output’s dimensionality is low.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自洽性的理念很简单：让我们提高一个LLM的温度，多次采样答案，然后从分布中选取最频繁的答案。这已被证明可以提高基于LLM的工作流程在特定任务上的性能，尤其是在分类或实体提取等输出维度较低的任务上。
- en: 'Let’s use a chain from a previous example and try a quadratic equation. Even
    with CoT prompting, the first attempt might give us a wrong answer, but if we
    sample from a distribution, we will be more likely to get the right one:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用前一个示例中的链并尝试一个二次方程。即使使用CoT提示，第一次尝试可能给出错误的答案，但如果我们从分布中进行采样，我们更有可能得到正确的答案：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, we first created a list containing multiple outputs generated
    by an LLM for the same input and then created a `Counter` class that allowed us
    to easily find the most common element in this list, and we took it as a final
    answer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们首先创建了一个包含由LLM为相同输入生成的多个输出的列表，然后创建了一个`Counter`类，使我们能够轻松地找到这个列表中最常见的元素，并将其作为最终答案。
- en: '**Switching between model providers**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**在模型提供者之间切换**'
- en: Different providers might have slightly different guidance on how to construct
    the best working prompts. Always check the documentation on the provider’s side
    – for example, Anthropic emphasizes the importance of XML tags to structure your
    prompts. Reasoning models have different prompting guidelines (for example, typically,
    you should not use either CoT or few-shot prompting with such models).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的提供者可能对如何构建最佳工作提示有略微不同的指导。始终检查提供者侧的文档——例如，Anthropic强调XML标签在结构化您的提示中的重要性。推理模型有不同的提示指南（例如，通常，您不应使用CoT或few-shot提示与这些模型）。
- en: Last but not least, if you’re changing the model provider, we highly recommend
    running an evaluation and estimating the quality of your end-to-end application.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，如果您正在更改模型提供者，我们强烈建议运行评估并估计您端到端应用程序的质量。
- en: Now that we have learned how to efficiently organize your prompt and use different
    prompt engineering approaches with LangChain, let’s talk about what can we do
    if prompts become too long and they don’t fit into the model’s context window.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何高效地组织你的提示，并使用LangChain的不同提示工程方法，让我们来谈谈如果提示太长而无法适应模型上下文窗口时我们能做什么。
- en: Working with short context windows
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与短上下文窗口一起工作
- en: 'A context window of 1 or 2 million tokens seems to be enough for almost any
    task we could imagine. With multimodal models, you can just ask the model questions
    about one, two, or many PDFs, images, or even videos. To process multiple documents
    (for summarization or question answering), you can use what’s known as the **stuff**
    approach. This approach is straightforward: use prompt templates to combine all
    inputs into a single prompt. Then, send this consolidated prompt to an LLM. This
    works well when the combined content fits within your model’s context window.
    In the coming chapter, we’ll discuss further ways of using external data to improve
    models’ responses.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 1百万或200万个标记的上下文窗口似乎足够应对我们所能想象的大多数任务。使用多模态模型，你可以向模型提问关于一个、两个或多个PDF、图像甚至视频的问题。为了处理多个文档（摘要或问答），你可以使用所谓的**stuff**方法。这种方法很简单：使用提示模板将所有输入组合成一个单一的提示。然后，将这个综合提示发送给LLM。当组合内容适合你的模型上下文窗口时，这种方法效果很好。在下一章中，我们将讨论进一步使用外部数据来改进模型响应的方法。
- en: Keep in mind that, typically, PDFs are treated as images by a multimodal LLM.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，通常情况下，PDF文件会被多模态LLM当作图像处理。
- en: 'Compared to the context window length of 4096 input tokens that we were working
    with only 2 years ago, the current context window of 1 or 2 million tokens is
    tremendous progress. But it is still relevant to discuss techniques of overcoming
    limitations of context window size for a few reasons:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们两年前使用的4096个输入标记的上下文窗口长度相比，当前的上下文窗口长度为100万或200万个标记，这是一个巨大的进步。但仍有几个原因需要讨论克服上下文窗口大小限制的技术：
- en: Not all models have long context windows, especially open-sourced ones or the
    ones served on edge.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有模型都有长上下文窗口，尤其是开源模型或在边缘上提供的服务模型。
- en: Our knowledge bases and the complexity of tasks we’re handling with LLMs are
    also expanding since we might be facing limitations even with current context
    windows.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的知识库以及我们用LLM处理的任务复杂性也在扩大，因为我们可能面临即使是在当前上下文窗口下的限制。
- en: Shorter inputs also help reduce costs and latency.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较短的输入也有助于降低成本和延迟。
- en: Inputs like audio or video are used more and more, and there are additional
    limitations on the input length (total size of PDF files, length of the video
    or audio, etc.).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像音频或视频这样的输入越来越多，并且对输入长度（PDF文件的总大小、视频或音频的长度等）有额外的限制。
- en: 'Hence, let’s take a close look at what we can do to work with a context that
    is larger than a context window that an LLM can handle – summarization is a good
    example of such a task. Handling a long context is similar to a classical Map-Reduce
    (a technique that was actively developed in the 2000s to handle computations on
    large datasets in a distributed and parallel manner). In general, we have two
    phases:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们仔细看看我们能做什么来处理一个比LLM可以处理的上下文窗口更大的上下文——摘要是一个很好的例子。处理长上下文类似于经典的Map-Reduce（一种在2000年代积极发展的技术，用于以分布式和并行方式处理大型数据集的计算）。一般来说，我们有两个阶段：
- en: '**Map**: We split the incoming context into smaller pieces and apply the same
    task to every one of them in a parallel manner. We can repeat this phase a few
    times if needed.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Map**：我们将传入的上下文分割成更小的部分，并以并行方式对每个部分应用相同的任务。如果需要，我们可以重复这个阶段几次。'
- en: '**Reduce**: We combine outputs of previous tasks together.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reduce**：我们将先前任务的结果合并在一起。'
- en: '![Figure 3.5: A Map-Reduce summarization pipeline](img/B32363_03_05.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5：一个Map-Reduce摘要管道](img/B32363_03_05.png)'
- en: 'Figure 3.5: A Map-Reduce summarization pipeline'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：一个Map-Reduce摘要管道
- en: Summarizing long video
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概括长视频
- en: 'Let’s build a LangGraph workflow that implements the Map-Reduce approach presented
    above. First, let’s define the state of the graph that keeps track of the video
    in question, the intermediate summaries we produce during the phase step, and
    the final summary:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个LangGraph工作流程，实现上面提到的Map-Reduce方法。首先，让我们定义跟踪所讨论视频、在阶段步骤中产生的中间摘要以及最终摘要的图状态：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Our state schema now tracks all input arguments (so that they can be accessed
    by various nodes) and intermediate results so that we can pass them across nodes.
    However, the Map-Reduce pattern presents another challenge: we need to schedule
    many similar tasks that process different parts of the original video in parallel.
    LangGraph provides a special `Send` node that enables dynamic scheduling of execution
    on a node with a specific state. For this approach, we need an additional state
    schema called `_ChunkState` to represent a map step. It’s worth mentioning that
    ordering is guaranteed – results are collected (in other words, applied to the
    main state) in exactly the same order as nodes are scheduled.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的状态模式现在跟踪所有输入参数（以便它们可以被各个节点访问）和中间结果，这样我们就可以在节点之间传递它们。然而，Map-Reduce 模式提出了另一个挑战：我们需要调度许多处理原始视频不同部分的相似任务以并行执行。LangGraph
    提供了一个特殊的 `Send` 节点，它允许在具有特定状态的节点上动态调度执行。对于这种方法，我们需要一个额外的状态模式，称为 `_ChunkState`，来表示映射步骤。值得一提的是，顺序是有保证的——结果以与节点调度完全相同的顺序收集（换句话说，应用于主状态）。
- en: 'Let’s define two nodes:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义两个节点：
- en: '`summarize_video_chunk` for the Map phase'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summarize_video_chunk` 用于映射阶段'
- en: '`_generate_final_summary` for the Reduce phase'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_generate_final_summary` 用于归约阶段'
- en: 'The first node operates on a state different from the main state, but its output
    is added to the main state. We run this node multiple times and outputs are combined
    into a list within the main graph. To schedule these map tasks, we will create
    a conditional edge connecting the `START` and `_summarize_video_chunk` nodes with
    an edge based on a `_map_summaries` function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个节点在主状态之外操作状态，但其输出被添加到主状态中。我们运行此节点多次，并将输出组合到主图中的列表中。为了调度这些映射任务，我们将创建一个基于 `_map_summaries`
    函数的边缘，将 `START` 和 `_summarize_video_chunk` 节点连接起来：
- en: '[PRE42]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let’s put everything together and run our graph. We can pass all arguments
    to the pipeline in a simple manner:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将所有这些放在一起并运行我们的图。我们可以以简单的方式将所有参数传递给管道：
- en: '[PRE43]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now, as we’re prepared to build our first workflows with LangGraph, there’s
    one last important topic to discuss. What if your history of conversations becomes
    too long and won’t fit into the context window or it would start distracting an
    LLM from the last input? Let’s discuss the various memory mechanisms LangChain
    offers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们准备使用 LangGraph 构建我们的第一个工作流程，还有一个最后的重要主题需要讨论。如果您的对话历史变得过长，无法适应上下文窗口，或者它可能会分散
    LLM 对最后输入的注意力怎么办？让我们讨论 LangChain 提供的各种内存机制。
- en: Understanding memory mechanisms
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解内存机制
- en: LangChain chains and any code you wrap them with are stateless. When you deploy
    LangChain applications to production, they should also be kept stateless to allow
    horizontal scaling (more about this in [*Chapter 9*](E_Chapter_9.xhtml#_idTextAnchor448)).
    In this section, we’ll discuss how to organize memory to keep track of interactions
    between your generative AI application and a specific user.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 链和您用它们包装的任何代码都是无状态的。当您将 LangChain 应用程序部署到生产环境中时，它们也应该保持无状态，以允许水平扩展（更多关于这一点在
    [*第 9 章*](E_Chapter_9.xhtml#_idTextAnchor448)）。在本节中，我们将讨论如何组织内存以跟踪您的生成式 AI 应用程序与特定用户之间的交互。
- en: Trimming chat history
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪切聊天历史
- en: Every chat application should preserve a dialogue history. In prototype applications,
    you can store it in a variable, though this won’t work for production applications,
    which we’ll address in the next section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聊天应用程序都应该保留对话历史。在原型应用程序中，您可以在变量中存储它，但这对于生产应用程序是不适用的，我们将在下一节中解决这个问题。
- en: The chat history is essentially a list of messages, but there are situations
    where trimming this history becomes necessary. While this was a very important
    design pattern when LLMs had a limited context window, these days, it’s not that
    relevant since most of the models (even small open-sourced models) now support
    8192 tokens or even more. Nevertheless, understanding trimming techniques remains
    valuable for specific use cases.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天历史本质上是一系列消息，但在某些情况下，剪切这个历史变得有必要。虽然当 LLMs 有一个有限的范围窗口时，这是一个非常重要的设计模式，但如今，它并不那么相关，因为大多数模型（即使是小型开源模型）现在支持
    8192 个标记甚至更多。尽管如此，了解剪切技术对于特定用例仍然很有价值。
- en: 'There are five ways to trim the chat history:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切聊天历史有五种方法：
- en: '**Discard messages based on length** (like tokens or messages count): You keep
    only the most recent messages so their total length is shorter than a threshold.
    The special LangChain function `from langchain_core.messages import trim_messages`
    allows you to trim a sequence of messages. You can provide a function or an LLM
    instance as a `token_counter` argument to this function (and a corresponding LLM
    integration should support a `get_token_ids` method; otherwise, a default tokenizer
    might be used and results might differ from token counts for this specific LLM
    provider). This function also allows you to customize how to trim the messages
    – for example, whether to keep a system message and whether a human message should
    always come first since many model providers require that a chat always starts
    with a human message (or with a system message). In that case, you should trim
    the original sequence of `human, ai, human, ai` to a `human, ai` one and not `ai,
    human, ai` even if all three messages do fit within the context window threshold.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根据长度丢弃消息**（如标记或消息计数）：你只保留最新的消息，以确保它们的总长度短于一个阈值。特殊的LangChain函数`from langchain_core.messages
    import trim_messages`允许你裁剪一系列消息。你可以提供一个函数或LLM实例作为`token_counter`参数传递给此函数（并且相应的LLM集成应支持`get_token_ids`方法；否则，可能会使用默认的分词器，结果可能与特定LLM提供商的标记计数不同）。此函数还允许你自定义如何裁剪消息
    – 例如，是否保留系统消息，以及是否应该始终将人类消息放在第一位，因为许多模型提供商要求聊天始终以人类消息（或系统消息）开始。在这种情况下，你应该将原始的`human,
    ai, human, ai`序列裁剪为`human, ai`，而不是`ai, human, ai`，即使所有三条消息都适合上下文窗口的阈值。'
- en: '**Summarize the previous conversation**: On each turn, you can summarize the
    previous conversation to a single message that you prepend to the next user’s
    input. LangChain offered some building blocks for a running memory implementation
    but, as of March 2025, the recommended way is to build your own summarization
    node with LangGraph.You can find a detailed guide in the LangChain documentation
    section: [https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要之前的对话**：在每一轮中，你可以将之前的对话摘要为一条单一的消息，并将其前置到下一个用户的输入之前。LangChain提供了一些用于运行时内存实现的构建块，但截至2025年3月，推荐的方式是使用LangGraph构建自己的摘要节点。你可以在LangChain文档部分的详细指南中找到：[https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/))。'
- en: When implementing summarization or trimming, think about whether you should
    keep both histories in your database for further debugging, analytics, etc. You
    might want to keep the short-memory history of the latest summary and the message
    after that summary for the application itself, and you probably want to keep track
    of the whole history (all raw messages and all the summaries) for further analysis.
    If yes, design your application carefully. For example, you probably don’t need
    to load all the raw history and summary messages; it’s enough to dump new messages
    into the database keeping track of the raw history.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现摘要或裁剪时，考虑是否应该在数据库中保留两个历史记录以供进一步调试、分析等。你可能希望保留最新的摘要的短期记忆历史以及该摘要之后的消息，以供应用程序本身使用，并且你可能希望保留整个历史记录（所有原始消息和所有摘要）以供进一步分析。如果是这样，请仔细设计你的应用程序。例如，你可能不需要加载所有原始历史和摘要消息；只需将新消息倒入数据库，同时跟踪原始历史即可。
- en: '**Combine both trimming and summarization**: Instead of simply discarding old
    messages that make the context window too long, you could summarize these messages
    and prepend the remaining history.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结合裁剪和摘要**：而不仅仅是简单地丢弃使上下文窗口太长的旧消息，你可以对这些消息进行摘要，并将剩余的历史记录前置。'
- en: '**Summarize long messages into a short one**: You could also summarize long
    messages. This might be especially relevant for RAG use cases, which we’re going
    to discuss in the next chapter, when your input to the model might include a lot
    of additional context added on top of the actual user’s input.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将长消息摘要为短消息**：你也可以对长消息进行摘要。这可能在下一章将要讨论的RAG用例中特别相关，当你的模型输入可能包括很多附加上下文，这些上下文是建立在实际用户输入之上的。'
- en: '**Implement your own trimming logic**: The recommended way is to implement
    your own tokenizer that can be passed to a `trim_messages` function since you
    can reuse a lot of logic that this function already cares for.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现自己的裁剪逻辑**：推荐的方式是实现自己的分词器，并将其传递给`trim_messages`函数，因为你可以重用该函数已经考虑到的很多逻辑。'
- en: Of course, the question remains on how you can persist the chat history. Let’s
    examine that next.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，关于如何持久化聊天历史的问题仍然存在。让我们接下来探讨这个问题。
- en: Saving history to a database
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将历史记录保存到数据库
- en: As mentioned above, an application deployed to production can’t store chat history
    in a local memory. If you have your code running on more than one machine, there’s
    no guarantee that a request from the same user will hit the same server at the
    next turn. Of course, you can store history on the frontend and send it back and
    forth each time, but that also makes sessions not sharable, increases the request
    size, etc.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，部署到生产环境的应用程序不能在本地内存中存储聊天历史。如果你在多台机器上运行代码，无法保证同一用户的请求在下次轮次会击中相同的服务器。当然，你可以在前端存储历史记录并在每次来回发送，但这也会使会话不可共享，增加请求大小等。
- en: Various database providers might offer an implementation that inherits from
    the `langchain_core.chat_history.BaseChatMessageHistory`, which allows you to
    store and retrieve a chat history by `session_id`. If you’re saving a history
    to a local variable while prototyping, we recommend using `InMemoryChatMessageHistory`
    instead of a list to be able to later switch to integration with a database.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的数据库提供商可能提供继承自`langchain_core.chat_history.BaseChatMessageHistory`的实现，这允许你通过`session_id`存储和检索聊天历史。如果你在原型设计时将历史记录保存到本地变量，我们建议使用`InMemoryChatMessageHistory`而不是列表，以便以后能够切换到与数据库的集成。
- en: 'Let’s look at an example. We create a fake chat model with a callback that
    prints out the amount of input messages each time it’s called. Then we initialize
    the dictionary that keeps histories, and we create a separate function that returns
    a history given the `session_id`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们创建了一个带有回调的假聊天模型，每次调用时都会打印出输入消息的数量。然后我们初始化一个保持历史记录的字典，并创建一个单独的函数，根据`session_id`返回一个历史记录：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now we create a trimmer that uses a `len` function and threshold `1` – i.e.,
    it always removes the entire history and keeps a system message only:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建了一个使用`len`函数和阈值`1`的裁剪器——即它总是移除整个历史记录，只保留一个系统消息：
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now let’s run it and make sure that our history keeps all the interactions
    with the user but a trimmed history is passed to the LLM:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们运行它并确保我们的历史记录保留了与用户的全部交互，但裁剪后的历史记录被传递给了LLM：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We used a `RunnableWithMessageHistory` that takes a chain and wraps it (like
    a decorator) with calls to history before executing the chain (to retrieve the
    history and pass it to the chain) and after finishing the chain (to add new messages
    to the history).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个`RunnableWithMessageHistory`，它接受一个链并使用装饰器模式将其包装（在执行链之前调用历史记录以检索并传递给链，以及在完成链之后添加新消息到历史记录）。
- en: Database providers might have their integrations as part of the `langchain_commuity`
    package or outside of it – for example, in libraries such as `langchain_postgres`
    for a standalone PostgreSQL database or `langchain-google-cloud-sql-pg` for a
    managed one.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库提供商可能将他们的集成作为`langchain_commuity`包的一部分或外部提供——例如，在`langchain_postgres`库中为独立的PostgreSQL数据库或`langchain-google-cloud-sql-pg`库中为托管数据库。
- en: 'You can find the full list of integrations to store chat history on the documentation
    page: [python.langchain.com/api_reference/community/chat_message_histories.html](https://python.langchain.com/api_reference/community/chat_message_histories.html).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在文档页面上找到存储聊天历史的完整集成列表：[python.langchain.com/api_reference/community/chat_message_histories.html](https://python.langchain.com/api_reference/community/chat_message_histories.html)。
- en: When designing a real application, you should be cautious about managing access
    to somebody’s sessions. For example, if you use a sequential `session_id`, users
    might easily access sessions that don’t belong to them. Practically, it might
    be enough to use a `uuid` (a uniquely generated long identifier) instead of a
    sequential `session_id`, or, depending on your security requirements, add other
    permissions validations during runtime.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计真实的应用程序时，你应该小心管理对某人会话的访问。例如，如果你使用顺序的`session_id`，用户可能会轻易访问不属于他们的会话。实际上，可能只需要使用一个`uuid`（一个唯一生成的长标识符）而不是顺序的`session_id`，或者根据你的安全要求，在运行时添加其他权限验证。
- en: LangGraph checkpoints
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangGraph检查点
- en: 'A checkpoint is a snapshot of the current state of the graph. It keeps all
    the information to continue running the workflow from the moment when the snapshot
    has been taken – including the full state, metadata, nodes that were planned to
    be executed, and tasks that failed. This is a different mechanism from storing
    the chat history since you can store the workflow at any given point in time and
    later restore from the checkpoint to continue. It is important for multiple reasons:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点是对图当前状态的快照。它保存了所有信息，以便从快照被捕获的那一刻开始继续运行工作流程——包括完整的状态、元数据、计划执行的任务节点以及失败的任务。这与存储聊天历史记录的机制不同，因为你可以在任何给定的时间点存储工作流程，稍后从检查点恢复以继续。这有多个重要原因：
- en: Checkpoints allow deep debugging and “time travel.”
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点允许深入调试和“时间旅行”。
- en: Checkpoints allow you to experiment with different paths in your complex workflow
    without the need to rerun it each time.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点允许你在复杂的流程中尝试不同的路径，而无需每次都重新运行它。
- en: Checkpoints facilitate human-in-the-loop workflows by making it possible to
    implement human intervention at a given point and continue further.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点通过在特定点实现人工干预并继续进一步，促进了人工介入的工作流程。
- en: Checkpoints help to implement production-ready systems since they add a required
    level of persistence and fault tolerance.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点有助于实现生产就绪的系统，因为它们增加了所需的持久性和容错级别。
- en: 'Let’s build a simple example with a single node that prints the amount of messages
    in the state and returns a fake `AIMessage`. We use a built-in `MessageGraph`
    that represents a state with only a list of messages, and we initiate a `MemorySaver`
    that will keep checkpoints in local memory and pass it to the graph during compilation:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的示例，其中包含一个打印状态中消息数量并返回假`AIMessage`的单节点。我们使用一个内置的`MessageGraph`，它表示一个只有消息列表的状态，并初始化一个`MemorySaver`，它将在本地内存中保存检查点，并在编译期间将其传递给图：
- en: '[PRE47]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, each time we invoke the graph, we should provide either a specific checkpoint
    or a thread-id (a unique identifier of each run). We invoke our graph two times
    with different `thread-id` values, make sure they each start with an empty history,
    and then check that the first thread has a history when we invoke it for the second
    time:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每次我们调用图时，我们都应该提供一个特定的检查点或线程-id（每次运行的唯一标识符）。我们用不同的`thread-id`值调用我们的图两次，确保它们每个都以空的历史记录开始，然后检查当我们第二次调用它时，第一个线程有一个历史记录：
- en: '[PRE49]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can inspect checkpoints for a given thread:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查特定线程的检查点：
- en: '[PRE50]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let’s also restore from the initial checkpoint for `thread-a`. We’ll see that
    we start with an empty history:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再从`thread-a`的初始检查点恢复。我们会看到我们从一个空的历史记录开始：
- en: '[PRE51]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can also start from an intermediate checkpoint, as shown here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从一个中间检查点开始，如下所示：
- en: '[PRE53]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'One obvious use case for checkpoints is implementing workflows that require
    additional input from the user. We’ll run into exactly the same problem as above
    – when deploying our production to multiple instances, we can’t guarantee that
    the next request from the user hits the same server as before. Our graph is stateful
    (during the execution), but the application that wraps it as a web service should
    remain stateless. Hence, we can’t store checkpoints in local memory, and we should
    write them to the database instead. LangGraph offers two integrations: `SqliteSaver`
    and `PostgresSaver`. You can always use them as a starting point and build your
    own integration if you’d like to use another database provider since all you need
    to implement is storing and retrieving dictionaries that represent a checkpoint.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点的一个明显用途是实现需要用户额外输入的工作流程。我们将遇到与上面完全相同的问题——当我们将我们的生产部署到多个实例时，我们无法保证用户的下一个请求击中与之前相同的服务器。我们的图是状态性的（在执行期间），但将其作为网络服务封装的应用程序应该保持无状态。因此，我们无法在本地内存中存储检查点，而应该将它们写入数据库。LangGraph提供了两个集成：`SqliteSaver`和`PostgresSaver`。你可以始终将它们作为起点，并在需要使用其他数据库提供者时构建自己的集成，因为你需要实现的是存储和检索表示检查点的字典。
- en: Now, you’ve learned the basics and are fully equipped to develop your own workflows.
    We’ll continue to look at more complex examples and techniques in the next chapter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经学到了基础知识，并且已经完全准备好开发你自己的工作流程。我们将在下一章继续探讨更复杂的一些示例和技术。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we dived into building complex workflows with LangChain and
    LangGraph, going beyond simple text generation. We introduced LangGraph as an
    orchestration framework designed to handle agentic workflows and also created
    a basic workflow with nodes and edges, and conditional edges, that allow workflow
    to branch based on the current state. Next, we shifted to output parsing and error
    handling, where we saw how to use built-in LangChain output parsers and emphasized
    the importance of graceful error handling.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了使用 LangChain 和 LangGraph 构建复杂工作流程，超越了简单的文本生成。我们介绍了 LangGraph 作为一种编排框架，旨在处理代理工作流程，并创建了一个基本的工作流程，包括节点和边，以及条件边，允许工作流程根据当前状态进行分支。接下来，我们转向输出解析和错误处理，展示了如何使用内置的
    LangChain 输出解析器，并强调了优雅错误处理的重要性。
- en: We then looked into prompt engineering and discussed how to use zero-shot and
    dynamic few-shot prompting with LangChain, how to construct advanced prompts such
    as CoT prompting, and how to use substitution mechanisms. Finally, we discussed
    how to work with long and short contexts, exploring techniques for managing large
    contexts by splitting the input into smaller pieces and combining the outputs
    in a Map-Reduce fashion, and worked on an example of processing a large video
    that doesn’t fit into a context.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了提示工程，讨论了如何使用 LangChain 的零样本和动态少样本提示，如何构建高级提示，如 CoT 提示，以及如何使用替换机制。最后，我们讨论了如何处理长和短上下文，探索了通过将输入拆分为更小的部分并按
    Map-Reduce 方式组合输出来管理大上下文的技术，并处理了一个处理大型视频的示例，该视频不适合上下文。
- en: Finally, we covered memory mechanisms in LangChain, emphasized the need for
    statelessness in production deployments, and discussed methods for managing chat
    history, including trimming based on length and summarizing conversations.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们涵盖了 LangChain 中的内存机制，强调了在生产部署中保持无状态的需求，并讨论了管理聊天历史的方法，包括基于长度的修剪和总结对话。
- en: We will use what we learned here to develop a RAG system in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152)
    and more complex agentic workflows in *Chapters 5* and *6*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里学到的知识用于在 [*第 4 章*](E_Chapter_4.xhtml#_idTextAnchor152) 中开发 RAG 系统，以及在
    *第 5 章* 和 *第 6 章* 中开发更复杂的代理工作流程。
- en: Questions
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is LangGraph, and how does LangGraph workflow differ from LangChain’s vanilla
    chains?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangGraph 是什么，LangGraph 工作流程与 LangChain 的标准链有什么不同？
- en: What is a “state” in LangGraph, and what are its main functions?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangGraph 中的“状态”是什么，它的主要功能是什么？
- en: Explain the purpose of *add_node* and *add_edge* in LangGraph.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释 LangGraph 中 *add_node* 和 *add_edge* 的作用。
- en: What are “supersteps” in LangGraph, and how do they relate to parallel execution?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangGraph 中的“supersteps”是什么，它们与并行执行有什么关系？
- en: How do conditional edges enhance LangGraph workflows compared to sequential
    chains?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与顺序链相比，条件边如何增强 LangGraph 工作流程？
- en: What is the purpose of the Literal type hint when defining conditional edges?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义条件边时，Literal 类型提示的作用是什么？
- en: What are reducers in LangGraph, and how do they allow modification of the state?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangGraph 中的“reducers”是什么，它们如何允许修改状态？
- en: Why is error handling crucial in LangChain workflows, and what are some strategies
    for achieving it?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么错误处理在 LangChain 工作流程中至关重要，以及实现它的策略有哪些？
- en: How can memory mechanisms be used to trim the history of a conversational bot?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用内存机制来修剪对话机器人的历史记录？
- en: What is the use case of LangGraph checkpoints?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangGraph 检查点的用例是什么？
- en: Subscribe to our weekly newsletter
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 订阅我们的每周通讯
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](E_Chapter_3.xhtml).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅 AI_Distilled，这是人工智能专业人士、研究人员和创新者的首选通讯，请访问 [https://packt.link/Q5UyU](E_Chapter_3.xhtml)。
- en: '![](img/Newsletter_QRcode1.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![Newsletter_QRcode1.jpg](img/Newsletter_QRcode1.jpg)'
