- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Teaching the Robot to Navigate and Avoid Stairs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教机器人导航和避免楼梯
- en: Let’s have a quick review of our quest to create a robot that picks up toys.
    We’ve created a toy detector and trained the robot arm. What’s next on our to-do
    list? We need to drive the robot to the location of the toy in order to pick it
    up. That sounds important.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下我们创建能够捡起玩具的机器人的目标。我们已经创建了一个玩具检测器并训练了机器人手臂。接下来我们的待办事项是什么？我们需要将机器人驱动到玩具的位置以便捡起它。这听起来很重要。
- en: This chapter covers **navigation** and **path planning** for our toy-grabbing
    robot helper. You have to admit that this is one of the most difficult problems
    in robotics. There are two parts to the task – figuring out where you are (localization),
    and then figuring out where you want to go (path planning). Most robots at this
    point would be using some sort of **simultaneous localization and mapping** (**SLAM**)
    algorithm that would first map the room, and then figure out where the robot is
    within it. But is this really necessary? First of all, SLAM generally requires
    some sort of 3D sensor, which we don’t have, and a lot of processing, which we
    don’t want to do. We can also add that it does not use machine learning, and this
    is a book about **artificial** **intelligence** (**AI**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了我们的玩具抓取机器人助手的任务中的**导航**和**路径规划**。你必须承认这是机器人学中最困难的问题之一。任务分为两部分——确定你的位置（定位），然后确定你想去哪里（路径规划）。到目前为止，大多数机器人都会使用某种形式的**同时定位与地图构建**（**SLAM**）算法，该算法首先绘制房间地图，然后确定机器人在其中的位置。但这是否真的必要呢？首先，SLAM通常需要某种3D传感器，我们没有，而且需要大量的处理，我们不想这样做。我们还可以补充说，它不使用机器学习，而这本书是关于**人工****智能**（**AI**）的。
- en: Is it possible to perform our task without making maps or ranging sensors? Can
    you think of any other robot that cleans rooms but does not do mapping? Of course
    you can. You probably even have a Roomba® (I have three), and most models do not
    do any mapping at all – they navigate by means of a pseudo-random statistical
    cleaning routine.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可以在不制作地图或使用测距传感器的情况下完成任务？你能想到任何其他清洁房间但不进行制图的机器人吗？当然可以。你可能甚至有一个Roomba®（我有三个），而且大多数型号根本不做任何制图——它们通过伪随机的统计清洁程序进行导航。
- en: Our task in this chapter is to create a reliable navigation system for our robot
    that is adaptable to our mission of cleaning a single room or floor of toys, and
    that uses the sensors we already have.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的任务是创建一个可靠的导航系统，用于我们的机器人，该系统能够适应我们的任务，即清洁单个房间或玩具层，并且使用我们已有的传感器。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding the SLAM methodology
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SLAM方法
- en: Exploring alternative navigation techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索替代导航技术
- en: Introducing the Floor Finder algorithm for avoiding obstacles
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍用于避开障碍物的Floor Finder算法
- en: Implementing neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现神经网络
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We require the **Robot Operating System Version 2** (**ROS 2**) for this chapter.
    This book uses the Foxy Fitzroy release: [http://wiki.ros.org/foxy/Installation](http://wiki.ros.org/foxy/Installation).
    This chapter assumes that you have completed [*Chapter 6*](B19846_06.xhtml#_idTextAnchor205),
    where we gave the robot a voice and the ability to receive voice commands. We
    will be using the Mycroft interface and voice text-to-speech system, which is
    called Mimic: [https://github.com/MycroftAI/mimic3](https://github.com/MycroftAI/mimic3).
    You’ll find the code for this chapter in the GitHub repository for this book at
    [https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要**机器人操作系统版本2**（**ROS 2**）来完成本章。本书使用Foxy Fitzroy版本：[http://wiki.ros.org/foxy/Installation](http://wiki.ros.org/foxy/Installation)。本章假设你已经完成了[*第6章*](B19846_06.xhtml#_idTextAnchor205)，其中我们给了机器人声音和接收语音命令的能力。我们将使用Mycroft界面和语音文本到语音系统，称为Mimic：[https://github.com/MycroftAI/mimic3](https://github.com/MycroftAI/mimic3)。你可以在本书的GitHub仓库中找到本章的代码：[https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e)。
- en: 'We will also be using the **Keras** library for Python ([https://keras.io](https://keras.io)),
    which is a powerful library for machine learning applications and lets us build
    custom neural networks. You can install it using the following command:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用Python的**Keras**库([https://keras.io](https://keras.io))，这是一个强大的机器学习应用库，允许我们构建自定义神经网络。你可以使用以下命令安装它：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will also need **PyTorch**, which is installed with this command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要**PyTorch**，可以使用以下命令安装：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Task analysis
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务分析
- en: As we do for each chapter, let’s review what we are aiming to accomplish. We
    will be driving the robot around the house, looking for toys. Once we have a toy,
    we will take that toy to the toy box and put it away by dropping it into the toy
    box. Then, the robot will go look for more toys. Along the way, we need to avoid
    obstacles and hazards, which include a set of stairs going downward that would
    definitely damage the robot.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为每一章做的那样，让我们回顾一下我们想要达成的目标。我们将驾驶机器人绕着房子转，寻找玩具。一旦我们有了玩具，我们将把它带到玩具箱，通过把它扔进玩具箱来把它放好。然后，机器人将去寻找更多的玩具。在这个过程中，我们需要避开障碍物和危险，包括一组会导致机器人损坏的向下楼梯。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: I used a baby gate to cover the stairs for the first part of testing and put
    pillows on the stairs for the second part. There is no need to bounce the robot
    down the stairs while it is still learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我在测试的第一部分用婴儿门封住了楼梯，在第二部分则在楼梯上放上了枕头。在机器人还在学习的时候，没有必要让它从楼梯上弹跳下来。
- en: We are going to start with the assumption that nothing in this task list requires
    the robot to know where it is. Is that true? We need to find the toy box – that
    is important. Can we find the toy box without knowing where it is? The answer
    is, of course, that the robot can just search for the toy box using its camera
    until it locates it. We developed a technique for recognizing the toy box back
    in [*Chapter 4*](B19846_04.xhtml#_idTextAnchor126) with a neural network.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设这个任务列表中的任何一项都不需要机器人知道自己的位置。这是真的吗？我们需要找到玩具箱——这很重要。我们能否在不了解位置的情况下找到玩具箱？答案是，当然，机器人可以使用它的摄像头搜索玩具箱，直到找到它。我们之前在[*第4章*](B19846_04.xhtml#_idTextAnchor126)中开发了一种使用神经网络识别玩具箱的技术。
- en: Now, if the robot was doing a bigger job, such as cleaning a 1,000,000-square-foot
    warehouse, then we would need a map. But our task is to clean a single 16 x 16
    room. The time lost searching for the toy box is not all that significant, considering
    we can’t get too far away, and we must drive to the toy box anyway. We will set
    this as a challenge, then, to accomplish our task without making a map.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果机器人要完成更大的任务，比如清理一个100万平方英尺的仓库，那么我们就需要一张地图。但我们的任务是清理一个16 x 16英尺的房间。考虑到我们不能走得太远，而且我们无论如何都必须开车到玩具箱，寻找玩具箱所花费的时间并不那么重要。因此，我们将设定一个挑战，在不制作地图的情况下完成我们的任务。
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: I once oversaw the evaluation of a robot system created at the Massachusetts
    Institute of Technology. They had a navigation system that did not use a map,
    and I was quite skeptical. In my defense, the robot actually got lost during the
    test. Now, I’m making a mapless navigator, and they are welcome to offer critique.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经监督过麻省理工学院创建的一个机器人系统的评估。他们有一个不使用地图的导航系统，我非常怀疑。为了自卫，机器人实际上在测试中迷路了。现在，我正在制作一个无地图的导航器，他们可以提出批评。
- en: 'We also need to get the robot to do the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要让机器人完成以下任务：
- en: Navigate the room avoiding obstacles (toys and furniture) and hazards (stairs).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在房间里导航，避开障碍物（玩具和家具）和危险（楼梯）。
- en: Find toys in the room (with the toy detector we created earlier).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在房间里找到玩具（使用我们之前创建的玩具探测器）。
- en: Drive to a location where the robot arm can reach the toy.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驾驶到机器人手臂可以触及玩具的位置。
- en: Pick up the toy with the robot arm.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用机器人手臂拿起玩具。
- en: Carry the toy to the toy box.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 把玩具拿到玩具箱。
- en: Put the toy in the toy box.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 把玩具放进玩具箱。
- en: Go and find another toy.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去找另一个玩具。
- en: If there are no more toys, then stop.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有更多的玩具，那么停止。
- en: We’ve covered finding the toy and picking it up in other chapters. In this chapter,
    we will discuss driving up to the toy to pick it up.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在其他章节中已经涵盖了找到玩具和拿起玩具的内容。在本章中，我们将讨论开车到玩具那里去拿它。
- en: 'I’m a big fan of the movie *The Princess Bride*. It has sword fights, cliffs,
    two battles of wits, and **Rodents of Unusual Size** (**ROUS**). It also has a
    lesson in planning that we can emulate. When our heroes, Fezzik the Giant, Inigo
    Montoya, and Westley, plan on storming the castle to rescue the princess, the
    first things Westley asks are “What are our liabilities?” and “What are our assets?”
    Let’s do this for our use case:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常喜欢电影《公主新娘》。它有剑斗、悬崖、两次智慧较量，还有**不同寻常的大老鼠**（**ROUS**）。它还包含了一个我们可以效仿的计划课程。当我们的英雄，巨人菲兹克，伊尼戈·蒙托亚，和韦斯利计划突袭城堡以营救公主时，韦斯利首先问的是“我们的责任是什么？”和“我们的资产是什么？”让我们为我们的用例做同样的事情：
- en: '**Our liabilities**: We have a small robot with a very limited sensor and compute
    capability. We have a room full of misplaced toys and a set of deadly stairs the
    robot can fall down.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们的责任**：我们有一个小型机器人，其传感器和计算能力非常有限。我们有一个房间里满是错位的玩具，还有一组机器人可能会掉下去的致命楼梯。'
- en: '**Our assets**: We have a robot with omni wheels that can drive around, a voice,
    one camera, and a robot arm. The robot has a datalink via Wi-Fi to a control computer.
    We have this book. We have a toy box that is a distinctive color. And lots of
    **Toys of Usual** **Size** (**TOUS**).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们的资产**：我们有一个带有全向轮的机器人，可以四处行驶，一个声音，一个摄像头和一个机器人臂。机器人通过Wi-Fi与控制计算机连接。我们有这本书。我们有一个颜色独特的玩具箱。还有许多**标准尺寸**的**玩具**（**TOUS**）。'
- en: The appropriate next step, whether we are designing robots or invading castles,
    is to do some brainstorming. How would you go about solving this problem?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是设计机器人还是入侵城堡，适当的下一步是做一些头脑风暴。你将如何解决这个问题？
- en: We could use SLAM and make a map, then locate the robot on the map, and use
    that to navigate. Although we ultimately will not be following this method, let’s
    quickly take a look at how it works.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用SLAM制作地图，然后定位机器人在地图上的位置，并使用它来导航。虽然我们最终不会遵循这种方法，但让我们快速看看它是如何工作的。
- en: Understanding the SLAM methodology
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SLAM方法学
- en: 'SLAM is a common methodology for navigating indoor robots. Before we get into
    the specifics, let’s look at two key issues:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM是室内机器人导航的常见方法。在我们深入具体细节之前，让我们看看两个关键问题：
- en: The first problem we have in indoor robot driving is that we don’t have a map
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 室内机器人驾驶中我们遇到的第一问题是我们没有地图。
- en: The second problem we have is that we have no frame of reference to locate ourselves
    – GPS does not work indoors
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们遇到的第二个问题是，我们没有定位自己的参考框架——室内GPS不起作用。
- en: That is two problems – we need a map, and then we need a way to locate ourselves
    on that map. While SLAM starts with the letter *S* for “simultaneous,” in truth,
    most robots make a map, store it away, and then drive on it later. Of course,
    while maps are being made, the robot must make the map and then locate itself
    on the map – usually in the center.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是两个问题——我们需要一个地图，然后我们需要一种方法来定位自己在地图上。虽然SLAM以字母"S"代表“同时”，但事实上，大多数机器人在开始时制作地图，将其存储起来，然后稍后驾驶。当然，在制作地图的同时，机器人必须制作地图并定位自己在地图上——通常在中心位置。
- en: How does SLAM work? The sensor usually associated with SLAM is the spinning
    LIDAR. You can think of LIDAR as laser radar – it uses a laser to measure the
    distance to objects and spins in a circle to collect data all around the robot.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM是如何工作的？通常与SLAM相关的传感器是旋转的LIDAR。你可以把LIDAR想象成激光雷达——它使用激光测量物体距离，并旋转以收集机器人周围的所有数据。
- en: 'We can summarize the SLAM method as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将SLAM方法总结如下：
- en: The robot takes a measurement of the room by sweeping a laser rangefinder in
    a circle.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人通过在圆形内旋转激光测距仪来测量房间的尺寸。
- en: The data returned is a list of distance measurements, where the angular measure
    is a function of the position in the list. If we have a list of 360 measurements
    in a circle, then the first number in our list is 0 degrees, the next is 1 degree,
    and so on.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回的数据是一系列距离测量值，其中角度测量是列表中位置的函数。如果我们有一个圆周上360个测量的列表，那么列表中的第一个数字是0度，下一个是1度，以此类推。
- en: We can extract features in the LIDAR data by looking for corners, edges, jumps,
    and discontinuities.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过寻找角落、边缘、跳跃和不连续性来从LIDAR数据中提取特征。
- en: We look at the angle and distance to each feature from succeeding measurements
    and create a function that gives the best estimate of how much the robot moved.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们查看每个特征从后续测量中的角度和距离，并创建一个函数，给出机器人移动的最佳估计。
- en: We use that information to transform the LIDAR data from the sensor-centric
    coordinate system to some sort of room coordinate system, usually by assuming
    that the starting position of the robot is coordinate 0,0\. Our transform, or
    mathematical transformation, will be a combination of translation (movement) and
    rotation of the robot’s body frame.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这些信息将LIDAR数据从传感器中心坐标系转换到某种房间坐标系，通常是通过假设机器人的起始位置是坐标0,0。我们的变换，或者说数学变换，将是机器人身体框架的平移（移动）和旋转的组合。
- en: One way of estimating this transform is to use **particles**. We create samples
    of the robot’s movement space at every point possible that the robot could have
    moved, and randomly place dots along all points. We compute the transform for
    each of these samples and then test to see which sample best fits the data collected.
    This is called a **particle filter** and is the technique I use for most of my
    SLAM projects.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计这种变换的一种方法就是使用**粒子**。我们在机器人可能移动的每一个可能点上创建机器人的运动空间样本，并在所有点上随机放置点。我们计算每个样本的变换，然后测试哪个样本最适合收集到的数据。这被称为**粒子滤波器**，是我用于大多数我的SLAM项目的技术。
- en: For more details, you can refer to [https://www.cs.cmu.edu/~16831-f14/notes/F12/16831_lecture04_dfouhey.pdf](https://www.cs.cmu.edu/~16831-f14/notes/F12/16831_lecture04_dfouhey.pdf).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节，您可以参考[https://www.cs.cmu.edu/~16831-f14/notes/F12/16831_lecture04_dfouhey.pdf](https://www.cs.cmu.edu/~16831-f14/notes/F12/16831_lecture04_dfouhey.pdf)。
- en: 'It can be difficult or impossible for SLAM to work in long, featureless hallways,
    for instance, as it simply has no information to work with – one lidar sweep looks
    just like the next. To help with this problem, many SLAM systems require the addition
    of other sensors to the robot, which measure wheel odometry or use optical flow
    to measure movement to provide additional data for the position estimate. The
    following is an illustration of a SLAM map of an office building made with ROS
    and displayed in RViz. The robot uses 500 particles for each LIDAR sample to estimate
    which changes in robot position best line up the lidar data with the data in the
    rest of the map. This is one of my earlier robot projects:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在长而特征不明显的走廊中，SLAM可能难以工作或根本无法工作，因为它没有任何信息可以工作——一个LIDAR扫描看起来就像下一个。为了帮助解决这个问题，许多SLAM系统需要向机器人添加其他传感器，这些传感器测量轮距或使用光流来测量运动，以提供额外的位置估计数据。以下是用ROS和RViz显示的由ROS制作的SLAM地图的示意图。机器人使用500个粒子来估计机器人位置的最佳变化，以使LIDAR数据与地图中其余数据对齐。这是我早期的一个机器人项目：
- en: '![Figure 7.1 – A map generated by a SLAM navigation process](img/B19846_07_1.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 由SLAM导航过程生成的地图](img/B19846_07_1.jpg)'
- en: Figure 7.1 – A map generated by a SLAM navigation process
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 由SLAM导航过程生成的地图
- en: 'What we have to do in the SLAM process is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在SLAM过程中，我们必须做的是以下内容：
- en: First, take a sweep that measures the distance from the robot to all of the
    objects in the room.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，进行一次扫描，测量机器人到房间中所有物体的距离。
- en: Then, we move the robot some distance – for example, three inches forward.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将机器人移动一段距离——例如，向前移动三英寸。
- en: Then, we take another sweep and measure the distances again.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们进行另一次扫描并再次测量距离。
- en: We now need to come up with a transformation that converts the data in the second
    sweep to line up with the data in the first sweep. To do this, there must be information
    in the two sweeps that can be correlated – corners, doorways, edges, and furniture.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要想出一个转换，将第二次扫描中的数据转换成与第一次扫描中的数据对齐。为此，两个扫描中必须有可以相关联的信息——角落、门道、边缘和家具。
- en: You can get a very small robot LIDAR (e.g., the RPLidar from SLAMtec) for around
    $100, and use it to make maps. There is an excellent ROS package called *Hector
    Mapping* that makes using this LIDAR straightforward. You will find that SLAM
    is not a reliable process and will require several fits and starts to come up
    with a map that is usable. Once the map is created, you must keep it updated if
    anything in the room changes, such are re-arranging the furniture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以花大约100美元购买一个非常小的机器人LIDAR（例如，SLAMtec的RPLidar），并使用它来制作地图。有一个名为*Hector Mapping*的优秀ROS包，它使得使用这种LIDAR变得简单。你会发现SLAM不是一个可靠的过程，并且需要多次尝试才能生成一个可用的地图。一旦地图创建完成，如果房间中的任何东西发生变化，例如重新布置家具，你必须保持地图更新。
- en: 'The SLAM process is actually very interesting, not for what happens in an individual
    scan, but in how scans are stitched together. There is an excellent video titled
    *Handheld Mapping in the Robocup 2011 Rescue Arena* that the authors of Hector
    SLAM, at the University of Darmstadt, Germany, put together, illustrating map
    making. It is available at the following link: [https://www.youtube.com/watch?v=F8pdObV_df4list=PL0E462904E5D35E29](https://www.youtube.com/watch?v=F8pdObV_df4list=PL0E462904E5D35E29).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM过程实际上非常有趣，不是因为单个扫描中发生了什么，而是因为扫描是如何拼接在一起的。有一个名为*手持式地图在RoboCup 2011救援竞技场*的优秀视频，由德国达姆施塔特大学的Hector
    SLAM作者制作，展示了地图制作过程。视频可在以下链接中找到：[https://www.youtube.com/watch?v=F8pdObV_df4list=PL0E462904E5D35E29](https://www.youtube.com/watch?v=F8pdObV_df4list=PL0E462904E5D35E29)。
- en: 'I wanted to give you a quick heads-up on SLAM so that we could discuss why
    we are not going to use it. SLAM is an important topic and is widely used for
    navigation, but it is not the only way to solve our problem by any means. The
    weaknesses of SLAM for our purposes include the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我想快速提醒你关于SLAM，这样我们就可以讨论为什么我们不会使用它。SLAM是一个重要的话题，并且广泛应用于导航，但它绝不是解决我们问题的唯一方法。SLAM在我们目的中的弱点包括以下内容：
- en: The need for some sort of sweeping sensor, such as LIDAR, ultrasound, or infrared,
    which can be expensive, mechanically complicated, and generate a lot of data.
    We want to keep our robot cheap, reliable, and simple.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要某种类型的扫描传感器，如LIDAR、超声波或红外线，这些可能很昂贵，机械复杂，并产生大量数据。我们希望保持我们的机器人便宜、可靠和简单。
- en: SLAM often works better if the robot has wheel odometers, which don’t work on
    omni-wheeled vehicles such as our Albert. Omni wheels slide or skid over the surface
    in order to turn – we don’t have Ackerman steering, such as a car with wheels
    that point. When the wheel skids, it is moving over the surface without turning,
    which invalidates any sort of wheel odometry, which assumes that the wheels are
    always turning in contact with a surface.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果机器人有轮式里程计，SLAM通常工作得更好，但我们的Albert机器人没有。全向轮在转弯时会滑动或打滑，以改变方向——我们没有阿克曼转向，比如车轮指向的汽车。当车轮打滑时，它是在没有转向的情况下在表面上移动，这使任何类型的轮式里程计无效，因为轮式里程计假设车轮始终在接触表面上转动。
- en: SLAM does not deal with floorplans that are changing. The Albert robot has to
    deal with toys being distributed around the room, which would interfere with LIDAR
    and change the floorplan that SLAM uses to estimate position. The robot is also
    changing the floorplan as it picks up toys and puts them away.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SLAM不处理正在变化的平面图。Albert机器人必须处理房间内分布的玩具，这会干扰LIDAR并改变SLAM用于估计位置的平面图。当机器人捡起玩具并放回原处时，它也在改变平面图。
- en: SLAM is computationally expensive. It requires the use of sensors to develop
    maps and then compares real-time sensor data to the map to localize the robot,
    which is a complex process.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SLAM在计算上很昂贵。它需要使用传感器来开发地图，然后将实时传感器数据与地图进行比较，以定位机器人，这是一个复杂的过程。
- en: SLAM has problems if data is ambiguous, or if there are not enough features
    for the robot to estimate changes on. I’ve had problems with featureless hallways
    as well as rooms that are highly symmetrical.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据模糊不清，或者没有足够的特点供机器人估计变化，SLAM就会遇到问题。我在没有特征的走廊以及高度对称的房间里也遇到过问题。
- en: So, why did I use this amount of space to talk about SLAM when I’m not going
    to teach you how to use it? Because you need to know what it is and how it works,
    because you may have a task that needs to make a map. There are lots of good tutorials
    on SLAM available, but very few on what I’m going to teach you next, which will
    be using AI to navigate safely without a map.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为什么我要用这么多空间来谈论SLAM，尽管我并不打算教你们如何使用它？因为你们需要知道它是什么以及它是如何工作的，因为你们可能有一个需要制作地图的任务。关于SLAM有很多好的教程，但关于我接下来要教你们的，即使用AI在没有地图的情况下安全导航的内容，却非常少。
- en: Exploring alternative navigation techniques
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索替代导航技术
- en: 'In this section, we’ll look at some potential alternative methods for navigation
    that we could use for our robot now that we’ve ruled out the SLAM methodology:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些潜在的替代导航方法，这些方法现在我们可以用于我们的机器人，因为我们已经排除了SLAM方法：
- en: 'We could just drive around randomly, looking for toys. When we find a toy,
    the robot picks it up and then drives around randomly looking for the toy box.
    When it sees the toy box, it drives up to it and deposits the toy. But we still
    need a method to avoid running over obstacles. We could follow a process called
    **structure from motion** (**SfM**) to get depth information out of our single
    camera and use that to make a map. Structure from motion requires a lot of textures
    and edges, which houses may not have. It also leaves lots of voids (holes) that
    must be filled in the map. Structure from motion uses parallax in the video images
    to estimate the distance to the object in the camera’s field of view. There has
    been a lot of interesting work in this area, and I have used it to create some
    promising results. The video image has to have a lot of detail in it so that the
    process can match points from one video image to the next. Here is a survey article
    on various approaches to SfM, if you are interested you can refer: [https://www.ijcit.com/archives/volume6/issue6/IJCIT060609.pdf](https://www.ijcit.com/archives/volume6/issue6/IJCIT060609.pdf).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以随意驾驶，寻找玩具。当我们找到玩具时，机器人会捡起它，然后随意驾驶寻找玩具箱。当它看到玩具箱时，它会开到那里并放下玩具。但我们仍然需要一个避免撞到障碍物的方法。我们可以遵循一个叫做**运动结构**（**SfM**）的过程，从我们的单摄像头中获取深度信息，并使用它来制作地图。运动结构需要大量的纹理和边缘，而房屋可能没有。它还在地图中留下许多必须填充的空洞。运动结构使用视频图像中的视差来估计摄像头视场中物体的距离。这个领域已经有很多有趣的工作，我使用它创造了一些有希望的结果。视频图像必须有很多细节，这样过程才能将一个视频图像中的点与下一个视频图像中的点匹配起来。如果你对此感兴趣，可以参考以下关于SfM各种方法的调查文章：[https://www.ijcit.com/archives/volume6/issue6/IJCIT060609.pdf](https://www.ijcit.com/archives/volume6/issue6/IJCIT060609.pdf)。
- en: You may have heard about a technique called **floor finding**, which is used
    in other robots and self-driving cars. I learned a great deal about floor finding
    from the sophisticated algorithm written by Stephen Gentner in the software package
    *RoboRealm*, which is an excellent tool for prototyping robot vision systems.
    You can find it at [http://www.roborealm.com](http://www.roborealm.com).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能听说过一种叫做**地面寻找**的技术，它在其他机器人和自动驾驶汽车中得到了应用。我从Stephen Gentner在软件包*RoboRealm*中编写的复杂算法中学到了很多关于地面寻找的知识，这是一个用于原型设计机器人视觉系统的优秀工具。你可以在[http://www.roborealm.com](http://www.roborealm.com)找到它。
- en: This floor finding technique is what we’ll be using in this chapter. Let’s discuss
    this in detail in the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层面寻找技术就是我们将在本章中使用的。让我们在下一节详细讨论这个问题。
- en: Introducing the Floor Finder technique
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍地面寻找技巧
- en: What I will be presenting in this chapter is my version of a Floor Finder technique
    that is different from RoboRealm, or other floor-finder algorithms, but that accomplishes
    the same results. Let’s break this simple concept down for ease of understanding.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中将要介绍的是一种不同于RoboRealm或其他地面寻找算法的地面寻找技巧版本，但它能实现相同的结果。让我们为了便于理解，把这个简单概念分解开来。
- en: We know that the floor directly in front of the robot is free from obstacles.
    We use the video image pixels of the area just in front of the robot as an example
    and look for the same texture to be repeated farther away. We are matching the
    texture of the part of the image we know is the floor with pixels farther away.
    If the textures match, we mark that area green to show that it is drivable and
    free of obstacles. We will be using bits of this technique in this chapter. By
    the way, did you notice that I said *texture* and not *color*? We are not matching
    the color of the floor, because the floor is not all one color. I have a brown
    carpet in my upstairs game room, which still has considerable variation in coloring.
    Using color matching, which is simple, just won’t cut it. We have to match the
    texture, which can be described in terms of color, intensity (brightness), hue,
    and roughness (a measure of how smooth the color of the surface is).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道机器人正前方的地面是没有障碍物的。我们以机器人前方区域的视频图像像素为例，寻找更远处的相同纹理。我们正在将图像中已知为地面的部分纹理与更远处的像素进行匹配。如果纹理匹配，我们将该区域标记为绿色，以显示该区域可行驶且无障碍。我们将在本章中使用这个技巧的某些部分。顺便问一下，你注意到我说的是*纹理*而不是*颜色*了吗？我们不是匹配地面的颜色，因为地面不是单一的颜色。我在楼上的游戏室里有一块棕色地毯，它的颜色变化仍然相当大。仅仅使用颜色匹配，这种方法是行不通的。我们必须匹配纹理，这可以用颜色、强度（亮度）、色调和粗糙度（表面颜色平滑度的度量）来描述。
- en: 'Let’s try some quick experiments in this area with our image of the floor in
    my game room. There are several steps involved when doing this for real:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在游戏室的地板图像上尝试一些快速实验。在实际操作中涉及几个步骤：
- en: We start with the image we get from the camera. In order to accelerate processing
    and make the most efficient use of bandwidth, we set the native resolution of
    our camera – which has a full resolution of 1900 x 1200 – down to a mere 640 x
    480\. Since our robot is small, we are using a small computer – the Nvidia Jetson
    Nano.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从相机获取的图像开始。为了加速处理并最大限度地有效利用带宽，我们将相机的原生分辨率——全分辨率为1900 x 1200——降低到仅为640 x 480。由于我们的机器人体积小，我们使用了一台小型计算机——Nvidia
    Jetson Nano。
- en: We move that to our image processing program, using **OpenCV**, an open source
    computer vision library that also has been incorporated into ROS.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这个方法应用到我们的图像处理程序中，使用**OpenCV**，这是一个开源的计算机视觉库，它也被集成到了ROS中。
- en: Our first step is to blur the image using the **Gaussian blur** function. The
    Gaussian blur uses a parabolic function to reduce the amount of high-frequency
    information in the image – it makes the image fuzzier by reducing the differences
    between neighboring pixels. To get enough blurring, I had to apply the blur function
    three times with a 5 x 5 **convolution kernel**. A convolution kernel is a matrix
    function – in this case, a 5 x 5 matrix of numbers. We use this function to modify
    a pixel based on its neighbors (the pixels around it). This smoothing makes the
    colors more uniform, reducing noise, and making the next steps easier. To blur
    the image, we take a bit from the surrounding pixels – two on either side – and
    add that to the center pixel. We discussed convolution kernels in [*Chapter 4*](B19846_04.xhtml#_idTextAnchor126).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的第一步是使用**高斯模糊**函数对图像进行模糊处理。高斯模糊使用抛物线函数来减少图像中的高频信息量——通过减少相邻像素之间的差异来使图像变得模糊。为了获得足够的模糊效果，我不得不将模糊函数应用三次，使用一个5
    x 5的**卷积核**。卷积核是一个矩阵函数——在这种情况下，一个5 x 5的数字矩阵。我们使用这个函数根据其邻居（周围的像素）来修改一个像素。这种平滑使颜色更加均匀，减少噪声，并使下一步更容易。为了模糊图像，我们从周围的像素中取一点——每边两个——并将其加到中心像素上。我们曾在[*第4章*](B19846_04.xhtml#_idTextAnchor126)中讨论过卷积核。
- en: We designate an area in front of the robot to be an area with a clear view of
    the floor. I used a triangular area, but a square area works as well. I picked
    each of the colors found within the triangle and grabbed all of the pixels that
    had a value with 15 units of that color. What does 15 units mean? Each color is
    encoded with an RGB value from 0 to 255\. Our carpet color, brown, is around 162,
    127, and 22 in red, green, and blue units. We select all the colors that are within
    15 units of that color, which, for red, is from 147 to 177\. This selects the
    areas of the image similar in color to our floor. Our wall is a very similar brown
    or beige, but fortunately, there is a white baseboard that we can isolate so that
    the robot does not try to climb the walls.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定机器人前方的一个区域为能够清晰看到地板的区域。我使用了一个三角形区域，但正方形区域也同样适用。我选择了三角形内发现的每种颜色，并抓取了所有具有15个单位该颜色值的像素。15个单位是什么意思？每种颜色都用一个从0到255的RGB值进行编码。我们的地毯颜色，棕色，在红、绿、蓝单位中大约是162、127和22。我们选择所有与该颜色相差15个单位的颜色，对于红色来说，是从147到177。这选择了与我们的地板颜色相似的区域。我们的墙壁是一种非常相似的棕色或米色，但幸运的是，有一个白色的踢脚板，我们可以将其隔离，这样机器人就不会试图爬上墙壁。
- en: 'Color is not the only way to match pixels on our floor. We can also look for
    pixels with a similar hue (shade of color, regardless of how bright or dark it
    is), pixels with the same **saturation** (darkness or lightness of color), and
    colors with the same value or **luminosity** (which is the same result as matching
    colors in a monochrome image or grayscale image). I compiled a chart illustrating
    this principle:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 颜色并不是匹配我们地板上像素的唯一方式。我们还可以寻找具有相似色调（无论颜色是亮是暗，都是颜色的阴影）的像素，具有相同**饱和度**（颜色的暗淡或明亮程度）的像素，以及具有相同值或**亮度**（与在单色图像或灰度图像中匹配颜色相同的结果）的颜色。我编制了一张图表来说明这个原理：
- en: '![Figure 7.2 – Selecting pixels in an image by similarity of various attributes,
    such as color, hue, or saturation](img/B19846_07_2.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 通过各种属性（如颜色、色调或饱和度）的相似性选择图像中的像素](img/B19846_07_2.jpg)'
- en: Figure 7.2 – Selecting pixels in an image by similarity of various attributes,
    such as color, hue, or saturation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 通过各种属性（如颜色、色调或饱和度）的相似性选择图像中的像素
- en: The preceding figure shows the ability of various selection attributes (color,
    hue, saturation, and luminosity) as a tool to perform floor finding for our robot.
    The hue attribute seems to provide the best results in this test. I tested it
    on another image to be sure it was working. It seems not to separate out the baseboards,
    which are not part of the safe area to drive on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示展示了各种选择属性（颜色、色调、饱和度和亮度）作为工具来为我们的机器人进行地面定位的能力。在这个测试中，色调属性似乎提供了最佳结果。我在另一张图片上测试了它，以确保它正在正常工作。它似乎无法将踢脚线区分开来，而踢脚线不属于安全驾驶区域。
- en: We select all of the pixels that match our floor colors and paint them green
    – or, to be more correct, we create a mask region in a copy of the image that
    has all of the pixels we want to be designated somehow. We can use the number
    10, for instance. We make a blank buffer the size of our image and turn all of
    the pixels in that buffer to 10, which would be the floor in the other image.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择所有与我们的地面颜色匹配的像素并将它们涂成绿色——或者更准确地说，我们在图像的副本中创建一个掩码区域，其中包含我们想要以某种方式指定的所有像素。例如，我们可以使用数字10。我们制作一个与我们的图像大小相同的空白缓冲区，并将该缓冲区中的所有像素都设置为10，这在另一张图像中将是地面。
- en: 'Performing an erode function on the masked data can help in this regard. There
    may be small holes or noise where one or two pixels did not match our carpet colors
    exactly – say there is a spot where someone dropped a cookie. The erode function
    reduces the level of detail in the mask by selecting a small region – for example,
    3 x 3, and setting the mask pixel to 10 only if all of the surrounding pixels
    are also 10\. This reduces the border of the mask by one pixel and removes any
    small speckles or dots that may be one or two pixels big. You can see from *Figure
    7**.3* that I was quite successful in isolating the floor area with a very solid
    mask. Given that we now know where the floor is, we paint the other pixels in
    our mask red, or some number signifying that it is unsafe to travel there. Let’s
    use 255:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对掩码数据进行膨胀操作可以帮助在这方面。可能存在一些小孔或噪声，其中一两个像素没有完全匹配我们的地毯颜色——比如说有人掉落了一块饼干。膨胀函数通过选择一个小区域（例如3
    x 3）并将掩码像素设置为10，只有当所有周围的像素也是10时，来降低掩码的细节水平。这减少了掩码的边界像素，并移除了任何可能是一两个像素大的小斑点和点。你可以从*图7.3*中看到，我非常成功地使用一个非常坚固的掩码隔离了地面区域。鉴于我们现在知道地面在哪里，我们将我们的掩码中的其他像素涂成红色，或者某个表示那里不安全行驶的数字。让我们使用255：
- en: '![Figure 7.3 – My version of the ﬂoor ﬁnder algorithm](img/B19846_07_3.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 我版本的地面定位算法](img/B19846_07_3.jpg)'
- en: Figure 7.3 – My version of the ﬂoor ﬁnder algorithm
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 我版本的地面定位算法
- en: Note that it does a very good job in this case of identifying where it is safe
    to drive. The projected paths are required to prevent the robot from trying to
    drive up the wall. You get bonus points if you can identify the robot in the corner.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，它非常成功地识别了安全驾驶的区域。投射的路径是必要的，以防止机器人试图沿着墙壁行驶。如果你能识别出角落里的机器人，你将获得额外的分数。
- en: 'Our next step may take some thought on your part. We need to identify the areas
    that are safe to drive. There are two cases when using this process that may cause
    us problems:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一步可能需要你思考。我们需要识别安全驾驶的区域。在使用此过程时，有两种情况可能会给我们带来问题：
- en: We may have an object in the middle of the floor by itself – such as a toy –
    that has green pixels on either side of it
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地面中间可能有一个单独的物体，比如一个玩具，它的两侧都有绿色像素。
- en: We may also have a concave region that the robot can get into but not out of
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可能有一个凹形区域，机器人可以进入但不能出来。
- en: 'In *Figure 7**.3*, you can see that the algorithm painted the wall pixels green
    since they match the color of the floor. There is a strong red band of no-go pixels
    where the baseboard is. To detect these two cases, we project lines from the robot’s
    position up from the floor and identify the first red pixel we hit. That sets
    the boundary for where the robot can drive. You can get a similar result if you
    trace upward from the bottom of the image straight up until you hit a red pixel,
    and stop at the first one. Let’s try the Floor Finder process again, but add some
    toys to the image so that we can be sure we are getting the result we want:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图7.3*中，你可以看到算法将墙壁像素涂成了绿色，因为它们与地面的颜色相匹配。在踢脚线处有一个强烈的红色禁止区域。为了检测这两种情况，我们从机器人的位置向上投射线条，并识别我们遇到的第一个红色像素。这设定了机器人可以行驶的边界。如果你从图像底部向上追踪直到遇到红色像素，并在第一个红色像素处停止，你也会得到类似的结果。让我们再次尝试地面定位过程，但向图像中添加一些玩具，以确保我们得到想要的结果：
- en: '![Figure 7.4 – Adding toys to the image to determine if we are detecting toys
    as obstacles](img/B19846_07_4.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 向图像中添加玩具以确定我们是否将玩具检测为障碍物](img/B19846_07_4.jpg)'
- en: Figure 7.4 – Adding toys to the image to determine if we are detecting toys
    as obstacles
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 向图像中添加玩具以确定我们是否将玩具检测为障碍物
- en: That seems to be working well. We are able to find a good path to drive on.
    Keep in mind that we are constantly updating the obstacle view with the Floor
    Finder and updating our path as we drive. There is one shortcoming of this process.
    If a toy matches the color and texture of the carpet, then we might have a lot
    of difficulty finding it. You can add strip of masking tape to objects to deal
    with this issue, giving the camera something to see.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎效果很好。我们能够找到一条良好的道路来驾驶。记住，我们在驾驶过程中不断使用地面探测器更新障碍物视图，并更新我们的路径。这个过程有一个缺点。如果一个玩具与地毯的颜色和纹理相匹配，那么我们可能很难找到它。你可以在物体上添加一条遮蔽胶带，以便相机能够看到。
- en: 'Another trick we can use with this process is to use the fixed camera geometry
    to do distance and size estimates. We have a “locked-down” camera – it is fixed
    in position on the robot, a set height from the floor, and, therefore, distance
    along the floor can be measured from the *y* value of the pixels. We would need
    to carefully calibrate the camera by using a tape measure and a box to match pixel
    values to the distance along the same path line we drew from the robot base to
    the obstacle. The distances will be nonlinear and only valid out to the distance
    the pixels continue to change. Since the camera is perpendicular to the floor,
    we get a certain amount of perspective effect that diminishes to 0 about 20 feet
    from the camera. My calibration resulted in the following table:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个过程中，我们可以使用的一个技巧是利用固定的相机几何形状来进行距离和尺寸估计。我们有一个“固定”的相机——它固定在机器人上，距离地面有一定高度，因此可以从像素的
    *y* 值测量沿地面的距离。我们需要通过使用卷尺和一个盒子来仔细校准相机，将像素值与从机器人基座到障碍物所绘的相同路径线的距离相匹配。这些距离将是非线性的，并且仅在像素继续变化的距离内有效。由于相机垂直于地面，我们得到一定量的透视效果，这种效果在距离相机大约20英尺处减小到0。我的校准结果如下表：
- en: '| **Measurement** **in inches** | **Distance** **from top** | **Distance**
    **from bottom** |'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| **英寸测量** | **顶部距离** | **底部距离** |'
- en: '| 0 | 1080 | 0 |'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 1080 | 0 |'
- en: '| 12 | 715 | 365 |'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 12 | 715 | 365 |'
- en: '| 24 | 627 | 453 |'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 24 | 627 | 453 |'
- en: '| 36 | 598.3 | 481.7 |'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 36 | 598.3 | 481.7 |'
- en: '| 48 | 581.5 | 498.5 |'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 48 | 581.5 | 498.5 |'
- en: '| 60 | 571.8 | 508.2 |'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 60 | 571.8 | 508.2 |'
- en: '| 72 | 565 | 515 |'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 72 | 565 | 515 |'
- en: Table 7.5 – Table of measurements comparing pixels to inches for scale
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.5 – 像素与英寸比较的测量表，用于确定比例
- en: 'The following image shows the technique for measuring distance in the robot
    camera field of view. The object is located four feet away from the robot base
    along the tape measure. Albert uses a 180-degree fisheye lens on an HD-capable
    web camera. We need the wide field of view later in [*Chapter 9*](B19846_09.xhtml#_idTextAnchor294)
    when we do navigation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了在机器人相机视场中测量距离的技术。物体位于卷尺上距离机器人基座四英尺的位置。Albert在具有高清功能的网络摄像头中使用180度鱼眼镜头。我们需要在[*第9章*](B19846_09.xhtml#_idTextAnchor294)中做导航时使用宽视场：
- en: '![Figure 7.5 – Determining the scale of the pixels in our navigation camera
    image](img/B19846_07_5.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 确定导航相机图像中像素的比例](img/B19846_07_5.jpg)'
- en: Figure 7.5 – Determining the scale of the pixels in our navigation camera image
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 确定导航相机图像中像素的比例
- en: One thing to watch out for is narrow passages that the robot will not fit into.
    We can estimate widths based on distance and pixels. One common robot technique
    is to put a border around all the obstacles equal to 1/2 the width of the robot.
    If there are obstacles on both sides, then the two borders will meet and the robot
    will know it does not fit.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，机器人无法进入的狭窄通道。我们可以根据距离和像素来估计宽度。一种常见的机器人技术是在所有障碍物周围放置一个等于机器人宽度一半的边界。如果有障碍物在两侧，那么两个边界将相遇，机器人将知道它无法进入。
- en: In the next section, we will create a **convolutional neural network** (**CNN**)
    to take our images and turn them into robot commands – in essence, teaching our
    robot how to drive by seeing landmarks or features in the video image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建一个**卷积神经网络**（**CNN**）来处理我们的图像，并将它们转换为机器人命令——从本质上讲，通过在视频图像中看到地标或特征来教我们的机器人如何驾驶。
- en: Implementing neural networks
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现神经网络
- en: So, what does a neural network do? We use a neural network to predict some association
    of an input with an output. When we use a CNN, we can associate a picture with
    some desired output. What we did in our previous chapter was to associate a class
    name (toys) with certain images. But what if we tried to associate something else
    with images?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，神经网络到底做什么呢？我们使用神经网络来预测输入与输出之间的某些关联。当我们使用CNN时，我们可以将一张图片与某些期望的输出关联起来。我们在上一章中做的是将类别名称（玩具）与某些图像关联起来。但如果我们尝试将其他东西与图像关联起来呢？
- en: How about this? We use a neural network to classify the images from our camera.
    We drive the robot around manually, using a joystick, and take a picture about
    four times a second. We record what the robot is doing in each picture – going
    forward, turning right, turning left, or backing up. We use that information to
    predict the robot’s motion command given the image. We make a CNN, with the camera
    image as the input and four outputs – commands for go forward, go left, or go
    right. This has the advantage of avoiding fixed obstacles and hazards automatically.
    When we get to the stairs (remember that I have stairs going down in my game room
    that would damage the robot), the robot will know to turn around, because that
    is what we did in training – we won’t deliberately drive the robot down the stairs
    during training (right?). We are teaching the robot to navigate the room by example.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 怎么样？我们使用神经网络来对来自相机的图像进行分类。我们手动驾驶机器人，使用摇杆，每秒大约拍照四次。我们记录下机器人在每张图片中的动作——前进、向右转、向左转或后退。我们使用这些信息来预测给定图像的机器人运动命令。我们制作了一个CNN，以相机图像作为输入，四个输出——前进、向左或向右的命令。这有一个优点，可以自动避免固定障碍和危险。当我们到达楼梯时（记得我在游戏室里有通往下方的楼梯，那会损坏机器人），机器人会知道要转身，因为这是我们训练时所做的——我们不会故意在训练中让机器人沿着楼梯下去（对吧？）。我们通过示例教机器人如何导航房间。
- en: You may be yelling at the book at this moment (and you should be) saying, “What
    about the toys?” Unless, of course, you are following my thought process and thinking
    to yourself, “Oh, that is why we just spent all that time talking about Floor
    Finder!” The neural network approach will get us around the room, and avoid the
    hazards and furniture, but will not help the robot to avoid toys, which are not
    in the training set. We can’t put them in this training set because the toys are
    never in the same place twice. We will use the Floor Finder to help avoid the
    toys. How do we combine the two? The neural network provides the longer-range
    goal to the robot, and the Floor Finder modifies that goal to avoid local, short-range
    objects. In our program, we evaluate the neural network first and then use Floor
    Finder to pick a clear route.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能正在对着这本书大喊大叫（你应该这么做）说，“那玩具怎么办？”除非，当然，你正在跟随我的思维过程，并且自己在想，“哦，这就是我们为什么花了那么多时间谈论地面定位器！”神经网络方法将帮助我们绕过房间，避免危险和家具，但不会帮助机器人避免训练集中没有的玩具。我们无法将它们放入这个训练集中，因为玩具永远不会出现在同一个地方两次。我们将使用地面定位器来帮助避免玩具。我们如何结合两者？神经网络为机器人提供更长的目标，地面定位器则修改这个目标以避免局部、短程物体。在我们的程序中，我们首先评估神经网络，然后使用地面定位器选择一条清晰的路线。
- en: 'On that theme, we are also going to pull another trick for training our robot.
    Since our floor surface is subject to change, and may be covered with toys, we
    will leave that part out of the training images. Before sending the image to the
    neural network, we’ll cut the image in half and only use the top half. Since our
    camera is fixed and level with the floor, that gives us only the upper half of
    the room to use for navigation. Our image is a 180-degree wide angle, so we have
    a lot of information to work with. This should give us the resiliency to navigate
    under any conditions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题上，我们还将为训练我们的机器人施展另一个技巧。由于我们的地面可能会发生变化，并且可能覆盖着玩具，我们将这部分从训练图像中排除。在将图像发送到神经网络之前，我们将图像切成两半，只使用上半部分。由于我们的相机是固定并且与地面平齐的，这给了我们只有房间上半部分可用于导航。我们的图像是180度的广角，因此我们有大量的信息可以处理。这应该能让我们在任何条件下都能导航：
- en: '![Figure 7.6 – The training set for driving the robot only includes the top
    of the image](img/B19846_07_6.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 训练机器人驾驶的数据集仅包括图像的顶部](img/B19846_07_6.jpg)'
- en: Figure 7.6 – The training set for driving the robot only includes the top of
    the image
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 训练机器人驾驶的数据集仅包括图像的顶部
- en: Our second problem is locating the toy box. For that, we need to create a new
    training set of images, which will represent an alternative driving pattern. We
    start the robot in various random locations, and then simply drive to the toy
    box. We use exactly the same process we used before for navigation – we are creating
    a training set that tells the robot how to get to the toy box. The trick is to
    get a good sample of every possible starting location. We do have a bit of a break
    – if a point on the map (a position in the room) is already on one path, we don’t
    need to cover it again. In other words, all points that are included in another
    path don’t need to be repeated. We still want to have at least 1,000 images to
    train from both sets of images – the one that explores the room, and the set that
    drives to the toy box.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第二个问题是定位玩具箱。为此，我们需要创建一个新的图像训练集，它将代表一种替代的驾驶模式。我们在各种随机的位置启动机器人，然后简单地驾驶到玩具箱。我们使用与之前导航相同的流程
    – 我们正在创建一个训练集，告诉机器人如何到达玩具箱。关键是获取每个可能起始位置的良好样本。我们确实有一些休息时间 – 如果地图上的一个点（房间中的位置）已经在一条路径上，我们就不需要再次覆盖它。换句话说，所有包含在其他路径中的点都不需要重复。我们仍然希望从这两组图像中至少有1,000张图像进行训练
    – 一组是探索房间，另一组是驾驶到玩具箱。
- en: I created a simple program that just lets the operator drive the robot with
    a joystick. It automatically takes a snapshot once a second. Each frame is labeled
    by simply looking at the value for the `cmd_vel` topic, which is how we control
    the motion of the robot base. If the angular velocity Z term (`angular.z`) is
    positive, we are turning right. If it is negative, we are turning left, and if
    the term is zero (you guessed it), we are driving straight ahead. I created an
    independent program that works with the camera and stores a snapshot whenever
    it receives a `TAKE PIC LEFT`, `RIGHT`, `CENTER`, or `BACK` command on the ROS
    `syscommand` topic. These programs will be in the GitHub repository for the book
    – I’m not going to include them here. We put each category of picture in its own
    subdirectory.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个简单的程序，它只允许操作员用操纵杆驾驶机器人。它每秒自动拍摄一张快照。每一帧都通过查看`cmd_vel`主题的值来标记，这是我们控制机器人底盘运动的方式。如果角速度Z项（`angular.z`）为正，我们正在向右转。如果它是负的，我们正在向左转，如果这个项为零（你猜对了），我们正在直行。我创建了一个独立程序，它与相机一起工作，并在接收到ROS
    `syscommand`主题上的`TAKE PIC LEFT`、`RIGHT`、`CENTER`或`BACK`命令时存储快照。这些程序将在本书的GitHub仓库中提供
    – 我不会在这里包括它们。我们将每个类别的图片放在它自己的子目录中。
- en: 'You can think of the neural network as working like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将神经网络想象成这样工作：
- en: We present an image to the neural network.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图像展示给神经网络。
- en: It selects features from that image and then selects the images in the training
    database that are most like the features in the image provided.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从该图像中提取特征，然后选择训练数据库中最像提供的图像特征的图像。
- en: Each picture in the training database is associated with a driving command (left,
    center, right). So, if the image most closely resembles an image where the robot
    turned left, then the network will return *turn left*.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据库中的每张图片都与一个驾驶命令（左、中、右）相关联。因此，如果图像最接近机器人向左转的图像，那么网络将返回*向左转*。
- en: Now let’s look at these processes in greater detail.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看看这些流程。
- en: Processing the image
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理图像
- en: 'Now, we have several steps to take before we can present our data to the neural
    network for training. Our camera on the robot has way too much resolution for
    what we need for the network, and we want to use the minimum amount of data in
    the neural network we can get away with:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们将数据呈现给神经网络进行训练之前，我们需要进行几个步骤。机器人上的相机分辨率远远超出了网络所需的，我们希望使用神经网络中尽可能少的数据：
- en: '![Figure 7.7 – Image processing for CNN](img/B19846_07_7.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – CNN的图像处理](img/B19846_07_7.jpg)'
- en: Figure 7.7 – Image processing for CNN
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – CNN的图像处理
- en: 'Let’s break down this process to make this clearer:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个流程，使其更清晰：
- en: The first image in the preceding figure represents our original image.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的图中的第一幅图像代表我们的原始图像。
- en: Our first step is to downsample the image to 640 x 480\. We cut the image in
    half and keep only the top half, which eliminates the floor from our consideration.
    We resize the rectangular image to 244 x 244, which is an appropriate size for
    our neural network to process.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的第一步是将图像下采样到640 x 480。我们将图像切成两半，只保留上半部分，这样就消除了地板的考虑。我们将矩形图像调整大小为244 x 244，这是我们的神经网络处理的一个合适大小。
- en: 'We convert the image to greyscale, so that we only have one channel to process,
    using this formula (proposed by the **National Television Standards** **Committee**
    (**NTSC**)):'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图像转换为灰度，这样我们只有一个通道需要处理，使用这个公式（由**国家电视标准委员会**（**NTSC**）提出）：
- en: '*Greyscale = 0.299 * R + 0.587 * G + 0.114 * B*'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*灰度 = 0.299 * R + 0.587 * G + 0.114 * B*'
- en: Our next step is to equalize the image to take the entire range of possible
    values. The raw output of the camera contains neither pure white (`255`) nor pure
    black (`0`). The lowest value may be `53` and the highest, `180`, for a range
    of `127`. We scale the grayscale values by subtracting the low (`53`) and multiplying
    by the scale factor (`127`/`255`). This expands the range of the image to the
    full scale and eliminates a lot of the variation in lighting and illumination
    that may exist. We are trying to present consistent data to the neural network.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下一步是对图像进行直方图均衡化，以获取所有可能值的整个范围。摄像头的原始输出既没有纯白色（`255`），也没有纯黑色（`0`）。最低值可能是`53`，最高值是`180`，范围是`127`。我们通过减去低值（`53`）并乘以缩放因子（`127`/`255`）来缩放灰度值。这扩大了图像的范围到全尺度，并消除了可能存在的许多光照和照明变化。我们试图向神经网络呈现一致的数据。
- en: The next step is to perform a Gaussian blur function on the data. We want to
    reduce some of the high-frequency data in the image, to smooth out some of the
    edges. This is an optional step, and may not be necessary for your environment.
    I have a lot of detail in the robot’s field of view, and I feel that the blur
    will give us better results. It also fills in some of the gaps in the grayscale
    histogram left by the equalization process in our previous step.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是对数据进行高斯模糊处理。我们希望减少图像中的一些高频数据，以平滑一些边缘。这是一个可选步骤，可能对您的环境不是必需的。我在机器人的视野中有很多细节，我觉得模糊会给我们更好的结果。它还填补了上一步中直方图均衡化过程留下的部分灰度值空缺。
- en: We have to normalize the data to reduce the scale from `0-255` to `0-1`. This
    is to satisfy the artificial neural network’s input requirements. To perform this
    operation, we just divide each pixel by `255`. We also must convert the data from
    the OpenCV image format to a NumPy array. All of this is part of normal CNN preprocessing.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须对数据进行归一化，将范围从`0-255`缩小到`0-1`。这是为了满足人工神经网络的输入要求。为了执行此操作，我们只需将每个像素除以`255`。我们还必须将数据从OpenCV图像格式转换为NumPy数组。所有这些都是CNN预处理的一部分。
- en: Our neural network is a nine-layer CNN. I used this common architecture because
    it is a variation of **LeNet**, which is widely used for this sort of task ([http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)).
    However, in our final step, rather than being a binary output determined by a
    binary classifier, we will use a **Softmax classifier** with four outputs – forward,
    left turn, or right turns. We can actually make more categories if we want to
    and have easy right and hard right turns rather than just one level of turns.
    I’m not using the full capability of the new omni wheels on my robot to keep this
    problem simple. Remember that the number of output categories must match our training
    set labels exactly.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的网络是一个九层的CNN。我使用这种常见的架构，因为它是对**LeNet**的变体，LeNet被广泛用于此类任务（[http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)）。然而，在我们的最终步骤中，我们不会使用由二元分类器确定的二元输出，而是使用具有四个输出的**Softmax分类器**
    – 前进、左转或右转。如果我们想的话，我们实际上可以创建更多的类别，并且有容易的右转和困难的右转，而不仅仅是单一级别的转向。我没有使用机器人新全向轮的完整功能，以保持这个问题简单。请记住，输出类别的数量必须与我们的训练集标签完全匹配。
- en: 'In our CNN, the first six layers are pairs of CNNs with max pooling layers
    in between. This lets the network deal with incrementally larger details in the
    image. The final two layers are fully connected with **rectified linear units**
    (**ReLU**) activations. Remember that ReLU only takes the positive values from
    the other layers. Here is our final layer, which is a Softmax classifier with
    four outputs:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的卷积神经网络（CNN）中，前六个层是成对的卷积神经网络，层间有最大池化层。这使得网络能够处理图像中逐渐增大的细节。最后的两个层是完全连接的，并使用**修正线性单元**（**ReLU**）激活。请记住，ReLU只取来自其他层的正值。这是我们的最终层，它是一个具有四个输出的Softmax分类器：
- en: '![Figure 7.8 – Organization of our neural network](img/B19846_07_8.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 我们神经网络的组织结构](img/B19846_07_8.jpg)'
- en: Figure 7.8 – Organization of our neural network
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 我们神经网络的组织结构
- en: Like any other neural network training task, the next set of steps in the process
    involves splitting the input data into training sets and validation sets. Let’s
    learn how to train the neural network next.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他神经网络训练任务一样，过程中的下一步是将输入数据分成训练集和验证集。让我们学习如何训练神经网络。
- en: Training the neural network for navigation
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练用于导航的神经网络
- en: We’ll use 80% of our data on training and 20% on validation. We really can’t
    use a process that sweetens the data by duplicating images with random rotations,
    as we did with the toy recognition program, since we are not just recognizing
    images, but using them for direction. Changing rotations would mess up our directions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用80%的数据进行训练，20%进行验证。我们真的不能使用通过随机旋转重复图像来美化数据的过程，就像我们在玩具识别程序中所做的那样，因为我们不仅是在识别图像，而且是在使用它们进行方向导航。改变旋转会搞乱我们的方向。
- en: 'Now, let’s put our training program together. This program was partially inspired
    by Adrian Rosebrock’s *pyImageSearch* blog and by the paper *Deep Obstacle Avoidance*
    by Sullivan and Lawson at the Naval Research Lab. You can follow these steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把我们的训练程序组合起来。这个程序部分受到了Adrian Rosebrock的*pyImageSearch*博客以及Sullivan和Lawson在海军研究实验室发表的论文*Deep
    Obstacle Avoidance*的启发。你可以按照以下步骤进行：
- en: We need to collect our training data by driving the robot around and recording
    our driving movements. This separated our data into three sets – left turn, right
    turn, and go straight. We have our training images in three subfolders to match
    our labels. We read in our data, associate it with the labels, and preprocess
    the data to present it to the neural network.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要通过驾驶机器人并记录我们的驾驶动作来收集我们的训练数据。这把我们的数据分成了三个集合——左转、右转和直行。我们的训练图像有三个子文件夹来匹配我们的标签。我们读取数据，将其与标签关联，并预处理数据以供神经网络使用。
- en: Note
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I’m doing the training runs on my desktop computer, not on the Jetson Nano.
    We’ll deploy on the Jetson Nano later with our fully trained networks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的台式计算机上运行训练，而不是在Jetson Nano上。稍后我们将使用完全训练好的网络在Jetson Nano上部署。
- en: 'Here are the imports that we need for this program – there are quite a few:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里列出了我们这个程序需要的导入项——相当多：
- en: '[PRE2]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the setup for the CNN:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是CNN的设置：
- en: We have three convolution layers, each followed by a `maxpooling` layer. Remember
    that each `maxpooling` layer will reduce the resolution of the image considered
    by the network by half, which is ¼ of the data, because we halve the width and
    the height. The convolution layers use the ReLU activation function since we don’t
    want any negative pixel values.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有三个卷积层，每个卷积层后面跟着一个`maxpooling`层。记住，每个`maxpooling`层都会将网络考虑的图像分辨率减半，也就是数据量的1/4，因为我们把宽度和高度都减半了。卷积层使用ReLU激活函数，因为我们不希望有任何负像素值。
- en: After the convolution layers, we have two fully connected layers with 500 neurons
    each.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷积层之后，我们有两个各有500个神经元的全连接层。
- en: The final layer is our three neuron output layers, with a Softmax classifier
    that will output the percentage of each classification (left, right, and center).
    The output will look like `(0.8, 0.15, 0.05)`, with three numbers that add up
    to 1.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层是我们的三个神经元输出层，包含一个Softmax分类器，它将输出每个分类（左、右和中心）的百分比。输出将看起来像`(0.8, 0.15, 0.05)`，三个数字加起来等于1。
- en: 'This is a generic convolution network class that can be reused for other things,
    as it is a general multi-class image classification CNN:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个通用的卷积网络类，可以用于其他事物，因为它是一个通用的多类图像分类CNN：
- en: '[PRE3]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we set up our learning regime. We will run 25 training runs, with a learning
    rate of 0.001\. We set a batch size of 32 images per batch, which we can reduce
    if we end up running out of memory:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们设置我们的学习计划。我们将运行25次训练，学习率为0.001。我们设置每个批次的图像数量为32张，如果我们内存不足，我们可以减少这个数量：
- en: '[PRE4]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next section loads all of our images. We set the path here where the images
    reside. We put the three types of training images in folders named `left`, `right`,
    and `center`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个部分加载所有我们的图像。我们在这里设置了图像所在的路径。我们将三种类型的训练图像放在名为`left`、`right`和`center`的文件夹中：
- en: '[PRE5]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, you can refer back to my diagram (*Figure 7**.7*) of the process we will
    go through to preprocess the images. We will cut the image in half and just process
    the upper half of the picture. Then, we reduce the image to 244 x 244 to fit into
    the neural network, which needs square images. We will convert the image to grayscale
    (black and white) since we don’t need to consider color, just shapes. This cuts
    our data down further. We will equalize the image, which rescales the range of
    gray colors to fill the whole area from 0 to 255\. This evens out the illumination
    and sets the contrast:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以参考我关于我们将要经历的图像预处理过程的图（*图7**.7*）。我们将把图像切成两半，只处理图像的上半部分。然后，我们将图像缩小到244 x
    244以适应神经网络，神经网络需要正方形图像。由于我们不需要考虑颜色，只需形状，我们将图像转换为灰度（黑白）。这将进一步减少我们的数据。我们将对图像进行均衡化，这会将灰度颜色的范围重新缩放到0到255，从而平衡光照并设置对比度：
- en: '[PRE6]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we have the Gaussian blur. This is an optional item – you may want to
    remove it if your room does not have a lot of detail. My game room has lots of
    furniture, so I think reducing the noise will improve performance:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有高斯模糊。这是一个可选项——如果你的房间没有很多细节，你可能想移除它。我的游戏室有很多家具，所以我认为减少噪声会提高性能：
- en: '[PRE7]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We convert the image to a NumPy array of floats scaled from 0 to 1, instead
    of a set of integers from 0 to 255\. This neural network toolkit only permits
    NumPy arrays for inputs. We also put the number associated with the labels (left
    = `0`, right=`1`, and center = `2`) into the matching `labels` NumPy array:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图像转换为浮点数的NumPy数组，范围从0到1，而不是从0到255的整数集。这个神经网络工具包只允许NumPy数组作为输入。我们还把与标签相关的数字（左=`0`，右=`1`，中心=`2`）放入匹配的`labels`
    NumPy数组中：
- en: '[PRE8]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We split the data into two parts – a training set that we use to train the
    neural network, and the testing set that we validate the training set with. We’ll
    use 80% of the image samples for training and 20% for testing:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据分成两部分——一个用于训练神经网络的训练集和一个用于验证训练集的测试集。我们将使用80%的图像样本进行训练，20%进行测试：
- en: '[PRE9]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have to convert the labels to a tensor, which is just a particular data
    format:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须将标签转换为张量，这仅仅是一种特定的数据格式：
- en: '[PRE10]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we build our actual neural network by instantiating the `ConvNet` object,
    which actually builds our CNN in Keras. We set up the optimizer, which is **Adaptive
    Moment Estimation** (**ADAM**), a type of adaptive gradient descent. ADAM acts
    against the error gradient like a heavy ball with friction – it has some momentum,
    but does not pick up speed quickly:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们通过实例化`ConvNet`对象来构建我们的实际神经网络，这个对象实际上在Keras中构建我们的CNN。我们设置了优化器，它是**自适应动量估计**（**ADAM**），一种自适应梯度下降法。ADAM像带有摩擦的重球一样作用于误差梯度——它有一定的动量，但不会快速加速：
- en: '[PRE11]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We train the network in this step. This will take quite some time to complete
    – from 15 minutes to an hour or two – depending on how many images you have. We
    want the training to come out somewhere above 80%. If not, add some epochs to
    see where the learning curve levels off. If that still does not do the trick,
    you need more training images. I’m aiming for 1,000 images in each set, which
    is roughly 50 minutes of driving the robot around:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这一步训练网络。这需要相当长的时间才能完成——从15分钟到一两个小时——具体取决于你有多少图像。我们希望训练结果至少达到80%。如果没有，可以增加一些周期来观察学习曲线何时趋于平稳。如果这仍然不起作用，你需要更多的训练图像。我目标是每个集合有1,000张图像，这大约需要50分钟的机器人驾驶时间：
- en: '[PRE12]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We are all done now, so we save the model we created to disk so that we can
    transfer it to the robot’s computer, the Nvidia Jetson Nano.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了，所以我们将创建的模型保存到磁盘上，以便我们可以将其传输到机器人的电脑，即Nvidia Jetson Nano。
- en: 'Now, make your second training set of driving from random locations to the
    toy box. Pick random spots and use the joystick to drive the robot to the toy
    box from each. Keep going until you have 1,000 images or so. Run these through
    the training program and label this model `toybox_model` by changing the last
    line of the program:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，制作你的第二个驾驶训练集，从随机位置到玩具箱。选择随机位置，并使用摇杆将机器人驾驶到玩具箱。继续这样做，直到你有大约1,000张图像。将这些图像通过训练程序运行，并通过更改程序的最后一行将此模型标记为`toybox_model`：
- en: '[PRE13]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is great – we have built and trained our neural network. Now, we need to
    put it to use to drive the robot around, which we’ll do in the next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这太棒了——我们已经构建并训练了我们的神经网络。现在，我们需要将其用于驾驶机器人，我们将在下一节中这样做。
- en: CNN robot control implementation
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN机器人控制实现
- en: 'We need to combine a program that sends out ROS commands with our neural network
    classification process. I added some commands through the ROS `syscommand` topic,
    which I use for non-periodic commands to my robots. `syscommand` just publishes
    a string, so you can use it for just about anything. You can follow these steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要结合一个发送ROS命令的程序和我们的神经网络分类过程。我通过ROS的`syscommand`主题添加了一些命令，我使用这个主题向我的机器人发送非周期性命令。`syscommand`只是发布一个字符串，所以你可以用它来做几乎所有的事情。你可以按照以下步骤进行：
- en: 'We start with our imports from ROS, OpenCV2, and Keras, as we will be combining
    functions from all three libraries:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从ROS、OpenCV2和Keras开始导入，因为我们将会结合这三个库中的函数：
- en: '[PRE14]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This first section is the ROS interface. I like to encapsulate the ROS interface
    this way, with all of the publish and subscribe in one place. We have several
    topics to set up – we need to be able to send and receive commands on the `syscommand`
    topic. We will be publishing commands to the robot’s motors on the `cmd_vel` topic.
    We receive images from the camera on `image_topic`. We use callbacks to handle
    the event when a topic is published elsewhere on the robot. These can be called
    at any time. We have more control when we publish to a topic, which is handled
    using the `pubTwist` and `pubCmd` methods. I added flags to received commands
    and images so that we don’t accidentally process the same image or command twice:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一部分是ROS接口。我喜欢以这种方式封装ROS接口，将所有的发布和订阅放在一个地方。我们需要设置几个主题——我们需要能够在`syscommand`主题上发送和接收命令。我们将向机器人的电机在`cmd_vel`主题上发布命令。我们从`image_topic`主题接收来自摄像头的图像。我们使用回调来处理在机器人上其他地方发布的主题事件。这些可以在任何时间调用。当我们向主题发布时，我们拥有更多的控制权，这通过`pubTwist`和`pubCmd`方法来处理。我添加了标志来接收命令和图像，这样我们就不小心重复处理相同的图像或命令两次：
- en: '[PRE15]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This next function is the means for the rest of the program to get the latest
    image from the camera system, which is published on ROS on `image_topic`. We grab
    the latest image and set the `newImage` variable to `False`, so that we know next
    time whether we are trying to process the same image twice in a row. Each time
    we get a new image, we set `newImage` to `True`, and each time we use an image,
    we set `newImage` to `False`:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个函数是程序其余部分获取摄像头系统最新图像的手段，该图像在ROS上发布在`image_topic`。我们获取最新图像并将`newImage`变量设置为`False`，这样我们就知道下次是否正在尝试连续两次处理相同的图像。每次我们获取到新图像时，我们将`newImage`设置为`True`，每次我们使用图像时，我们将`newImage`设置为`False`：
- en: '[PRE16]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This section sends speed commands to the robot to match what the CNN output
    predicts for us to do. The output of the CNN is one of three values: left, right,
    or straight ahead. These come out of the neural network as one of three enumerated
    values – `0`, `1`, or `2`. We convert them back to left, right, and center values,
    and then use that information to send a motion command to the robot. The robot
    uses the `Twist` message to send motor commands. The `Twist` data message is designed
    to accommodate very complex robots, quadcopters, and omni-wheel drive systems
    that can move in any direction, so it has a lot of extra values. We send a `Twist.linear.x`
    command to set the speed of the robot forward and backward, and a `Twist.angular.z`
    value to set the rotation, or turning, of the base. In our case, a positive `angular.z`
    rotation value goes to the right, and a negative value to the left. Our last statement
    publishes the data values on the `cmd_vel` topic as a `Twist` message:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一部分向机器人发送速度命令，以匹配CNN预测的我们应执行的操作。CNN的输出是三个值之一：左转、右转或直行。这些作为神经网络中的三个枚举值之一——`0`、`1`或`2`输出。我们将它们转换回左转、右转和中心值，然后使用这些信息向机器人发送运动命令。机器人使用`Twist`消息发送电机命令。`Twist`数据消息旨在适应非常复杂的机器人、四旋翼飞行器和全向轮驱动系统，这些系统可以朝任何方向移动，因此它有很多额外的值。我们发送一个`Twist.linear.x`命令来设置机器人的前后速度，以及一个`Twist.angular.z`值来设置底座的旋转或转向。在我们的情况下，正的`angular.z`旋转值向右转，负值向左转。我们的最后一条语句将数据值作为`Twist`消息发布到`cmd_vel`主题：
- en: '[PRE17]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We create a function to do all of our image processing with one command. This
    is the exact replica of how we preprocessed the images for the training program
    – just as you might think. You may think it a bit strange that I scale the image
    up, only to then scale it down again. The reason for this is to have detail for
    the vertical part of the image. If I scaled it down to 240 x 240 and then cut
    it in half, I would be stretching pixels afterward to get it square again. I like
    having extra pixels when scaling down. The big advantage of this technique is
    that it does not matter what resolution the incoming image is at – we will end
    up with the correctly sized and cropped image.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个函数，通过一个命令来完成所有的图像处理。这正是我们为训练程序预处理图像的方式——正如你可能想象的那样。你可能觉得我先将图像放大，然后再缩小，这有点奇怪。这样做的原因是为了在图像的垂直部分有细节。如果我将它缩小到
    240 x 240，然后将其切成两半，我会在之后拉伸像素以再次使其成为正方形。我喜欢在缩小时有额外的像素。这种技术的最大优点是，它不关心输入图像的分辨率——我们最终会得到正确尺寸和裁剪的图像。
- en: 'The other steps involve converting the image to grayscale, performing an equalization
    on the contrast range, which expands our color values to fill the available space,
    and performing a Gaussian blur to reduce noise. We normalize the image for the
    neural network by converting our integer 0-255 grayscale values to floating point
    values from 0 to 1:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他步骤包括将图像转换为灰度，对对比度范围进行均衡，这会扩展我们的颜色值以填充可用空间，并执行高斯模糊以减少噪声。我们通过将我们的整数 0-255 灰度值转换为
    0 到 1 的浮点值来对图像进行归一化，以便神经网络使用：
- en: '[PRE18]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we’re set up, we go into the main program. We have to initialize our
    ROS node so that we can talk to the ROS publish/subscribe system. We create a
    variable, mode, that we use to control what branch of processing to go down. We
    make an interface to allow the operator to turn the navigation function on and
    off, and to select between normal navigation and our toy-box-seeking mode.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了，我们进入主程序。我们必须初始化我们的 ROS 节点，这样我们才能与 ROS 发布/订阅系统通信。我们创建一个变量，mode，我们用它来控制要进入哪个处理分支。我们创建一个接口，允许操作员打开和关闭导航功能，并在正常导航和我们的玩具箱寻找模式之间进行选择。
- en: 'In this first section, we will load both neural network models that we trained
    before:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本节中，我们将加载我们之前训练的两个神经网络模型：
- en: '[PRE19]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This section begins the processing loop that runs while the program is active.
    Running `rospy.spin()` tells the ROS system to process any message that may be
    waiting for us. Our final step is to pause the program for 0.02 seconds to allow
    the Raspberry Pi to process other data and run other programs:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本节开始处理循环，该循环在程序运行时执行。运行 `rospy.spin()` 告诉 ROS 系统处理任何可能等待我们的消息。我们的最后一步是暂停程序 0.02
    秒，以便让 Raspberry Pi 处理其他数据并运行其他程序：
- en: '[PRE20]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So, that concludes our navigation chapter. We’ve covered both obstacle avoidance
    and room navigation using a neural network to teach the robot to drive about using
    landmarks on the ceiling – and without a map.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的导航章节到此结束。我们介绍了使用神经网络教机器人通过天花板上的地标来驾驶，以及如何避开障碍物和进行房间导航——而且无需地图。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced some concepts for robot navigation in an unstructured
    environment, which is to say, in the real world, where the designers of the robot
    don’t have control over the content of the space. We started by introducing SLAM,
    along with some of the strengths and weaknesses of map-based navigation. We talked
    about how Roomba navigates, by random interaction and statistical models. The
    method selected for our toy-gathering robot project, Albert, combined two algorithms
    that both relied mostly on vision sensors.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些在非结构化环境中进行机器人导航的概念，也就是说，在现实世界中，机器人的设计者无法控制空间的内容。我们首先介绍了 SLAM，以及基于地图导航的一些优缺点。我们讨论了
    Roomba 如何通过随机交互和统计模型进行导航。我们为玩具收集机器人项目 Albert 选择的算法结合了两种主要依赖视觉传感器的算法。
- en: The first was the Floor Finder, a technique I learned when it was used by the
    winning entry in the DARPA Grand Challenge. The Floor Finder algorithm uses the
    near vision (next to the robot) to teach the far vision (away from the robot)
    what the texture of the floor is. We can then divide the room into things that
    are safe to drive on, and things that are not safe. This deals with our obstacle
    avoidance. Our navigation technique used a trained neural network to identify
    the path around the room by associating images of the room from the horizon up
    (the top half of the room) with directions to travel. This also served to teach
    the robot to stay away from the stairs. We discarded the bottom half of the room
    from the image for the neural network because that is where the toys are. We used
    the same process to train another neural network to find the toy box.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项是 Floor Finder，这是一种我在 DARPA 大挑战获胜者使用时学到的技术。Floor Finder 算法使用近视（靠近机器人）来教远视（远离机器人）地板的纹理。然后我们可以将房间分成可以驾驶的安全区域和不可驾驶的区域。这解决了我们的避障问题。我们的导航技术使用训练好的神经网络通过将房间从地平线向上的图像（房间的上半部分）与行驶方向关联起来，来识别房间内的路径。这也教会了机器人远离楼梯。我们因为玩具在那里，所以从图像中排除了房间的下半部分，并使用相同的过程训练另一个神经网络来找到玩具箱。
- en: This process was the same as we saw in [*Chapter 4*](B19846_04.xhtml#_idTextAnchor126),
    but the training images were all labeled with the path from that spot to the toy
    box. This combination gave us the ability to teach the robot to find its way around
    by vision, and without a map, just like you do.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程与我们看到的 [*第 4 章*](B19846_04.xhtml#_idTextAnchor126) 中的过程相同，但训练图像都被标记了从该点到玩具箱的路径。这种组合使我们能够通过视觉教会机器人找到路径，而且不需要地图，就像你做的那样。
- en: In the next chapter, we’ll cover classifying objects, and review some other
    path-planning methods.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍对象分类，并回顾一些其他路径规划方法。
- en: Questions
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Regarding SLAM, what sensor is most commonly used to create the data that SLAM
    needs to make a map?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于 SLAM，最常用的传感器是什么，用于创建 SLAM 制作地图所需的数据？
- en: Why does SLAM work better with wheel odometer data available?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么当有轮式里程计数据可用时，SLAM 工作得更好？
- en: In the Floor Finder algorithm, what does the Gaussian blur function do to improve
    the results?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Floor Finder 算法中，高斯模糊函数是如何改善结果的？
- en: The final step in the Floor Finder is to trace upward from the robot position
    to the first red pixel. In what other way can this step be accomplished (referring
    to *Figure 7**.3*)?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Floor Finder 的最后一步是从机器人位置向上追踪到第一个红色像素。这个步骤还可以通过什么其他方式完成（参考 *图 7**.3*）？
- en: Why did we cut the image in half horizontally before doing our neural network
    processing?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们在进行神经网络处理之前要将图像水平切半？
- en: What advantages does using the neural network approach provide that a technique
    such as SLAM does not?
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用神经网络方法相比 SLAM 技术有什么优势？
- en: If we used just a random driving function instead of the neural network, what
    new program or function would we have to add to the robot to achieve the same
    results?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们只用随机驾驶函数而不是神经网络，我们需要为机器人添加什么新的程序或函数才能达到相同的结果？
- en: How did we end up avoiding the stairs in the approach presented in the chapter?
    Do you feel this is adequate? Would you suggest any other means for accomplishing
    this task?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在章节中提出的方法是如何避免楼梯的？你认为这足够吗？你会建议其他完成这项任务的方法吗？
- en: Further reading
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Deep Obstacle Avoidance* by Sullivan and Lawson, published by Naval Research
    Labs, Rosebrock, Adrian.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《深度避障》*，作者 Sullivan 和 Lawson，由海军研究实验室，Rosebrock，Adrian 出版。'
- en: '*Artificial Intelligence with Python Cookbook* by Ben Auffarth, Packt Publishing,
    2020'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《使用 Python 的人工智能食谱》*，作者 Ben Auffarth，Packt Publishing，2020年'
- en: '*Artificial Intelligence with Python – Second Edition*, by Prateek Joshi, Packt
    Publishing, 2020'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《使用 Python 的人工智能 - 第二版》*，作者 Prateek Joshi，Packt Publishing，2020年'
- en: '*Python Image Processing Cookbook* by Sandipan Dey, Packt Publishing, 2020'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Python 图像处理食谱》*，作者 Sandipan Dey，Packt Publishing，2020年'
