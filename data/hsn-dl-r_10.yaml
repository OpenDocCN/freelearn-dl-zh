- en: Long Short-Term Memory Networks for Stock Forecasting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于股市预测的长短期记忆网络
- en: This chapter will show you how to use **long short-term memory** (**LSTM**) models
    to forecast stock prices. This type of model is particularly useful for time series-based
    forecasting tasks. An LSTM model is a special type of **recurrent neural network**
    (**RNN**). These models contain special characteristics that allow you to reuse
    recent output as input. In this way, these types of models are often described
    as having memory. We will begin by creating a simple baseline model for predicting
    stock prices. From there, we will create a minimal LSTM model and we will dive
    deeper into the advantages of this model type over our baseline model, as well
    as explore how this model type is an improvement over a more traditional RNN.
    Lastly, we will look at some ways to tune our model to further improve its performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示如何使用**长短期记忆**（**LSTM**）模型预测股价。这种模型特别适用于基于时间序列的预测任务。LSTM模型是**递归神经网络**（**RNN**）的一种特殊类型。这些模型具有特殊的特点，允许你将最近的输出作为输入重复使用。通过这种方式，这些类型的模型通常被描述为具有记忆。我们将从创建一个简单的基准模型开始，预测股价。从这里出发，我们将创建一个最小化的LSTM模型，并深入探讨这种模型类型相较于我们的基准模型的优势，以及它如何在某种程度上优于更传统的RNN。最后，我们将讨论一些调整模型的方式，以进一步提高其性能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding common methods for stock market prediction
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解股票市场预测的常见方法
- en: Preparing and preprocessing data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备和预处理数据
- en: Configuring a data generator
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置数据生成器
- en: Training and evaluating the model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: Tuning hyperparameters to improve performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数以提高性能
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files used in this chapter at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)找到本章使用的代码文件。
- en: Understanding common methods for stock market prediction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解股票市场预测的常见方法
- en: In this chapter, we will learn about a different type of neural network called
    an RNN. In particular, we will apply a type of RNN known as an LSTM model to predict
    stock prices. Before we begin, let's first look at some common methods of predicting
    stock prices to better understand the problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一种不同类型的神经网络，叫做RNN。特别地，我们将应用一种RNN模型，称为LSTM模型，来预测股价。在我们开始之前，首先让我们了解一些常见的股票价格预测方法，以更好地理解这个问题。
- en: Predicting stock prices is a time-series problem. With most other machine learning
    problems, variables can be split at random and used in training and test datasets,
    but this is not possible when solving a time-series problem. The variables must
    remain in order. The features to solve the problem can be found in the sequence
    of events and, consequently, the chronology of how events occurred must be maintained
    to generate meaningful predictions of what will happen next. While this places
    a constraint on which methods can be used, it also provides an opportunity to
    use some specific models that are well suited for these types of tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 预测股价是一个时间序列问题。与大多数其他机器学习问题不同，变量可以随机分割并用于训练和测试数据集，但在解决时间序列问题时，这种做法不可行。变量必须保持顺序。解决问题的特征可以在事件的序列中找到，因此，事件发生的时间顺序必须保持，以生成有意义的预测，预测接下来会发生什么。虽然这对可用方法施加了限制，但也提供了一个机会，可以使用一些特别适合这些任务的模型。
- en: Let's start with some approaches that are relatively straightforward to create
    a baseline model before creating our deep learning solution, which we can use
    to compare results later. The first model that we will construct is an **Auto-Regressive Integrated
    Moving Average** (**ARIMA**) model. Actually, the concepts from the name of this
    model explain a lot about the particular challenges of modeling on time-series
    data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建深度学习解决方案之前，让我们先从一些相对简单的方法开始，创建一个基准模型，之后可以用来比较结果。我们将构建的第一个模型是**自回归积分滑动平均**（**ARIMA**）模型。实际上，这个模型名称中的概念解释了时间序列数据建模的许多特殊挑战。
- en: The **AR** in ARIMA stands for **auto-regressive** and refers to the fact that
    the model inputs will be for a given observation and set number of lagged observations. The
    **MA** in ARIMA stands for **moving average** and refers to the autoregressive
    nature of the model, which states that the variables will include an observation
    at a given point in time and a set of lagged variables. The moving average component
    takes into account the mean value between a set of variables over time to capture
    a more generalized value explaining a trend over time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**AR** 在ARIMA中代表**自回归**，指的是模型的输入将是给定观测值和一组滞后观测值。**MA** 在ARIMA中代表**移动平均**，指的是模型的自回归特性，表示变量将包含在给定时间点的观测值和一组滞后变量。移动平均部分考虑了随时间变化的一组变量的平均值，从而捕捉到解释趋势的更一般化的值。'
- en: The **I** in ARIMA stands for **integrated**, which in this context means that
    the entire time-series is considered as a whole. More specifically, it refers
    to the idea that the solutions must be generalizable across the entire series
    just as we strive for with other machine learning solutions. To aid with this,
    with time-series problems we transform the data so that it is said to be stationary.
    With ARIMA, we look at the differences between an observation and the observation
    preceding it and then use these relative differences. By using the relative differences,
    we can maintain a more generalized shape over the series, which helps to control
    the mean and variance, which are important elements in forecasting predicted future
    states.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**I** 在ARIMA中代表**积分**，在这个上下文中意味着整个时间序列被视为一个整体。更具体地说，它指的是解决方案必须能够在整个序列上进行推广，这就像我们在其他机器学习解决方案中追求的一样。为此，在时间序列问题中，我们会对数据进行转换，使其成为平稳序列。对于ARIMA，我们会查看一个观测值与前一个观测值之间的差异，并使用这些相对差异。通过使用相对差异，我们可以保持更广义的形状，从而有助于控制均值和方差，这对于预测未来状态至关重要。'
- en: 'With this context, let''s now create an ARIMA model. To get started, we will
    use the `quantmod` package to load stock information. The `getSymbols` function
    from this package is a very convenient way to pull in stock price information
    for any company within a set time frame from a number of sources. We set `auto.assign`
    to `FALSE` since we will assign this ourselves. For our example, we will load
    5 years of Facebook stock:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个背景，接下来我们将创建一个ARIMA模型。首先，我们将使用`quantmod`包加载股票信息。这个包中的`getSymbols`函数是一个非常方便的方式，可以从多个来源在设定的时间框架内提取任何公司股票的价格信息。我们将`auto.assign`设置为`FALSE`，因为我们会自己分配这个对象。对于我们的例子，我们将加载5年的Facebook股票数据：
- en: 'In the following code, we will read in the data as well as load all the libraries
    we will be using:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将读取数据并加载我们将使用的所有库：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After running this code, we will see that we have an `FB` object in our environment
    with an `xts` class type. This object class type is similar to a standard data
    frame, but the row names are dates. Let's inspect a few rows of `xts` data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们会看到在环境中有一个`FB`对象，它的类类型是`xts`。这个对象的类类型类似于标准的数据框，但行名是日期。让我们查看几行`xts`数据。
- en: 'We can view the first five rows from the `FB` object by using the following
    line of code:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码查看`FB`对象的前五行：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After running this code, you will see the following output printed to your
    console:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，你将在控制台中看到以下输出：
- en: '![](img/70d9271c-8576-4b59-b820-d60a5e88b5c6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70d9271c-8576-4b59-b820-d60a5e88b5c6.png)'
- en: We can see from these selected rows that we have a number of stock price points
    taken from the day, including the opening and closing price, as well as the highest
    and lowest price for the stock during the day. The volume of stock traded is also
    included. For our purposes, we will use the price at the close of trading.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些选定的行中我们可以看到，数据中包含了来自当天的多个股票价格点，包括开盘价、收盘价、以及当天的最高价和最低价。同时也包含了股票交易量。对于我们的目的，我们将使用收盘时的价格。
- en: 'We select just the closing prices from the data and store them in a new object
    by running the following code:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从数据中只选择收盘价，并通过运行以下代码将其存储在一个新对象中：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now use the `plot.xts` function to plot the stock data. Usually, plotting
    this data would require the dates to be held in a column, but this plotting function
    is a convenient way of plotting a time series without needing the dates present
    within a column in the data frame. We can plot the full 5 years of Facebook closing
    stock prices by running the following line of code:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用`plot.xts`函数来绘制股价数据。通常，绘制这些数据需要将日期存储在一个列中，但这个绘图函数提供了一种便捷的方式来绘制时间序列数据，而无需在数据框中保留日期列。我们可以通过运行以下代码来绘制Facebook过去五年的收盘股价：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After we run this line of code, we will see the following plot generated in
    our **Plots**tab:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行这一行代码后，我们将在**Plots**选项卡中看到如下生成的图表：
- en: '![](img/bb32a7ae-6c0c-49c6-97c3-2b235c5e8ab9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb32a7ae-6c0c-49c6-97c3-2b235c5e8ab9.png)'
- en: 'Now that we can see how stock prices for Facebook have changed over this time
    frame, let''s build an ARIMA model. Afterward, we will create a forecast using
    the ARIMA model and then plot these forecast values. We create our model, forecast,
    and plot by running the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以看到Facebook股价在这个时间段内的变化，让我们来构建一个ARIMA模型。之后，我们将使用该ARIMA模型创建一个预测，并绘制这些预测值。我们通过运行以下代码来创建模型、进行预测并绘制图形：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running the preceding code, we will see the following plot in the **Plots**
    tab:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们将在**Plots**选项卡中看到如下图表：
- en: '![](img/99086d54-705a-474e-824e-b11bfb521a46.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99086d54-705a-474e-824e-b11bfb521a46.png)'
- en: The ARIMA model did not find a pattern and instead offers an upper and lower
    bound, within which prices are predicted, which is not very helpful. ARIMA is
    a popular baseline model for forecasting time-series data that often performs
    well. However, in this case, we can see that our ARIMA model doesn't appear to
    provide much useful information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ARIMA模型没有找到规律，反而给出了一个上下边界，在这个范围内预测股价，这并不特别有用。ARIMA是一个流行的基准模型，用于预测时间序列数据，通常表现不错。然而，在这个案例中，我们可以看到我们的ARIMA模型似乎没有提供太多有用的信息。
- en: 'Before continuing, we can add in the actual stock price values from this frame
    to see whether the prices did fall within the bounds predicted by the ARIMA model.
    To pull in the data and add it to our plot, we run the following code:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，我们可以将这个数据框中的实际股价值添加进来，看看股价是否落在ARIMA模型预测的范围内。为了将数据拉入并添加到我们的图表中，我们运行以下代码：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After we run the preceding code, we will see the following plot in the **Plots**
    tab:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行前面的代码后，我们将在**Plots**选项卡中看到如下图表：
- en: '![](img/aa240e84-9b4f-46b4-b891-351f51a51a53.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa240e84-9b4f-46b4-b891-351f51a51a53.png)'
- en: As noted, the values for these dates are outside the bounds of the ARIMA model.
    We can see that we chose a difficult time-series dataset to model since the stock
    price is on a downward trend and then begins to rise again. With this being said,
    over the 5-year period, there is a general upward trend. Can we build a model
    that learns this upward trend and reflects it properly in the predicted results?
    Let's take what we have learned about the particular challenges of modeling time-series
    data and see whether we can improve upon our baseline results using a deep learning
    approach.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些日期的值超出了ARIMA模型的预测范围。我们可以看到，我们选择了一个比较复杂的时间序列数据集来进行建模，因为股价呈现下降趋势，然后又开始上升。话虽如此，整个五年期间，股价整体呈现上升趋势。我们能否构建一个能够学习这种上升趋势并在预测结果中正确反映的模型呢？让我们利用我们学到的关于时间序列数据建模的特殊挑战，看看是否能通过深度学习方法来改进我们的基准结果。
- en: Preparing and preprocessing data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备与预处理
- en: When working with time-series data, there are a number of data type formats
    to choose from and use for conversion. We have already used two of these formats,
    of which there are three that are most widely used. Let's briefly review these
    data types before moving on to our deep learning model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理时间序列数据时，有多种数据类型格式可以选择并进行转换。我们已经使用了其中的两种格式，实际上有三种格式是最常用的。在进入深度学习模型之前，让我们简要回顾一下这些数据类型。
- en: When we wanted to add actual data as an overlay to our ARIMA model plot, we
    used the `ts` function to create a time-series data object. For this object, the
    index values must be integers. In the case of using the `autolayer` function with
    the `arima` plot, a time-series data object is required. This is one of the more
    simple time-series data types and it will look like a vector in your **Environment**
    tab. However, this only works with regular time series.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望将实际数据作为叠加层添加到ARIMA模型图中时，我们使用了`ts`函数来创建时间序列数据对象。对于此对象，索引值必须是整数。在使用`autolayer`函数与`arima`图结合时，也需要一个时间序列数据对象。这是更简单的时间序列数据类型，它在**环境**标签中看起来像一个向量。然而，这仅适用于常规时间序列。
- en: Another data type is `zoo`. The `zoo` data type will work with regular and irregular
    time series, and zoo objects can also contain a number of different data types
    as index values. The drawback of the `zoo` objects is that there is not much information
    available in the **Environment** pane. The only detail provided is the date range.
    At times, the `zoo` data works better for plotting, especially when overlaying
    multiple time-series objects, which is what we will use it for later in this chapter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种数据类型是`zoo`。`zoo`数据类型适用于常规和不规则的时间序列，且zoo对象还可以包含不同的数据类型作为索引值。`zoo`对象的缺点是**环境**窗格中提供的信息较少，唯一的细节是日期范围。有时，`zoo`数据在绘图时表现更好，尤其是在叠加多个时间序列对象时，这也是我们在本章后面将使用它的原因。
- en: The last time-series data type is `xts`. This data type is an extension of `zoo`.
    Like `zoo`, the index values are dates. However, in addition, the data is stored
    in a matrix and numerous attributes are present in the **Environment** pane, making
    it easier to inspect the size and contents of the data object. This is generally
    a good choice for working with time-series data, unless there are particular reasons
    to use one of the other types that we have, and will, cover. Another benefit of
    `xts` is that the default plot function uses different formatting to the base
    plot.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种时间序列数据类型是`xts`。这种数据类型是`zoo`的扩展。与`zoo`一样，索引值是日期。然而，此外，数据被存储在一个矩阵中，并且在**环境**窗格中会显示许多属性，使得检查数据对象的大小和内容更加容易。这通常是处理时间序列数据的好选择，除非有特别的理由使用我们已经涵盖的其他类型。`xts`的另一个好处是，默认的绘图函数使用不同于基础绘图的格式。
- en: 'When we are working with time-series data, there is another part of preprocessing
    in addition to data type conversion that is often useful for modeling: converting
    our data to delta values rather than absolute values in order to make the data
    stationary. In this case, we will also log transform the price values to further
    control the outliers. When we are done with this process, we will need to remove
    the missing value that we introduced as well. Let''s convert our combined stock
    price data from the closing price to the daily change in the log value of the
    closing price by running the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理时间序列数据时，除了数据类型转换之外，还有另一个常用于建模的预处理步骤：将数据转换为增量值，而非绝对值，以便使数据平稳。在这种情况下，我们还将对价格值进行对数变换，以进一步控制异常值。当我们完成此过程时，我们还需要去除我们引入的缺失值。让我们通过运行以下代码，将我们的合并股票价格数据从收盘价转换为收盘价对数值的日变化：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running this code, we have data that should be more stationary than the
    data we were working with before. Let''s view what our data looks like now by
    running the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们得到了比之前处理的数据更为平稳的数据。现在，让我们通过运行以下代码查看我们的数据现在是什么样的：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When we run the preceding code, we will see the following plot generated in
    the **Plots** pane:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们将在**图形**窗格中看到以下生成的图表：
- en: '![](img/60ee2ff4-12f0-437c-8270-03c2b92ff69c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60ee2ff4-12f0-437c-8270-03c2b92ff69c.png)'
- en: 'The values we are working with are far more constrained and the patterns from
    one section appear as if they will generalize better to explain movement in a
    different section. Even though we can see that the values are better scaled here,
    we can also run a quick test to prove that the data is now stationary. To check
    for stationarity, we can use the **Augmented Dickey-Fuller test**, which we can
    run on our data using this line of code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理的值约束更多，一个部分的模式看起来能够更好地概括并解释不同部分的运动。尽管我们可以看到这里的值已经更好地进行了缩放，但我们还可以进行一个快速测试，证明数据现在是平稳的。为了检查平稳性，我们可以使用**扩展的迪基-富勒检验**，我们可以通过以下代码行在我们的数据上运行该检验：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After running this code, we will see the following output printed to the console:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们将在控制台中看到以下输出：
- en: '![](img/7b7bd7e7-f212-475c-80d1-11d854923f48.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b7bd7e7-f212-475c-80d1-11d854923f48.png)'
- en: Although the actual p-value isn't reported as output when running this function,
    we can see that the p-value is small enough that we can accept the alternate hypothesis
    that the data is stationary. With this preprocessing complete, we can now move
    on to setting up our `train` and `test` sets for our deep learning model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在运行此函数时不会输出实际的p值，但我们可以看到p值足够小，因此我们可以接受数据是平稳的备择假设。完成此预处理后，我们现在可以继续为深度学习模型设置`train`和`test`数据集。
- en: Configuring a data generator
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置数据生成器
- en: Similar to ARIMA, for our LSTM model, we would like the model to use lagging
    historical data to predict actual data at a given point in time. However, in order
    to feed this data forward to an LSTM model, we must format the data so that a
    given number of columns contain all the lagging values and one column contains
    the target value. In the past, this was a slightly tedious process, but now we
    can use a data generator to make this task much simpler. In our case, we will
    use a time-series generator that produces a tensor that we can use for our LSTM
    model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于ARIMA，对于我们的LSTM模型，我们希望模型使用滞后的历史数据来预测给定时间点的实际数据。然而，为了将这些数据传递给LSTM模型，我们必须将数据格式化，使得一定数量的列包含所有滞后的值，而一列包含目标值。在过去，这个过程略显繁琐，但现在我们可以使用数据生成器使这一任务变得更加简单。在我们的案例中，我们将使用一个时间序列生成器，它生成一个我们可以用于LSTM模型的张量。
- en: The arguments we will include when generating our data are the data objects
    we will use along with the target. In this case, we can use the same data object
    as values for both arguments. The reason this is possible has to do with the next
    argument, called `length`, which configures the time steps to look back in order
    to populate the lagging price values. Afterward, we define the sampling rate and
    the stride, which determine the number of time steps between consecutive values
    for the target values per row and the lagging values per sequence, respectively.
    We also define the starting index value and the ending index value. We also need
    to determine whether data should be shuffled or kept in chronological order and
    whether the data should be in reverse chronological order or maintain its current
    sort order. Finally, we select a batch size, which specifies how many time series
    samples should be in each batch of the model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成数据时，我们将包括的数据参数是我们将使用的数据对象及其目标。在这种情况下，我们可以将相同的数据对象作为这两个参数的值。之所以可以这样做，是因为下一个参数`length`，它配置了回溯的时间步数，用来填充滞后价格值。然后，我们定义采样率和步幅，它们分别决定目标值每行的连续时间步数以及每个序列的滞后值的时间步数。我们还定义了起始索引值和结束索引值。我们还需要确定数据是否应被打乱，或保持按时间顺序排列，或者是否应按逆时间顺序排列，或保持当前排序。最后，我们选择批量大小，这决定了每批模型中应包含多少时间序列样本。
- en: 'For this model, we will create our generated time-series data with a `length`
    value of `3`, meaning that we will look back 3 days to predict a given day. We
    will keep the sampling rate and stride at `1` to include all data. Next, we will
    split the `train` and `test` sets with an index point of `1258`. We will not shuffle
    or reverse the data, but rather maintain its chronological order and set the batch
    size to `1` so that we model one price at a time. We create our `train` and `test`
    sets using these values for our parameters via the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，我们将创建生成的时间序列数据，其`length`值为`3`，意味着我们将回溯3天以预测给定的那一天。我们将保持采样率和步幅为`1`，以包括所有数据。接下来，我们将通过`1258`作为索引点来拆分`train`和`test`数据集。我们不会打乱数据或反转数据，而是保持其时间顺序，并将批量大小设置为`1`，以便一次建模一个价格。我们通过以下代码使用这些参数值来创建`train`和`test`数据集：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After running this code, you will see two tensor objects in your **Environment**
    pane. Now that we have configured our data generator and used it to create two
    sequence tensors, we are ready to model our data using LSTM.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，你将在**环境**面板中看到两个张量对象。现在我们已经配置了数据生成器并使用它创建了两个序列张量，我们可以开始使用LSTM模型对数据进行建模了。
- en: Training and evaluating the model
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: Our data is properly formatted and we can now train our model. For this task,
    we are using LSTM. This is a particular type of RNN. These types of neural networks
    are a good choice for time-series data because they are able to take time into
    account during the modeling process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据已正确格式化，现在可以训练我们的模型。为了这个任务，我们使用LSTM。这是一种特殊类型的RNN。此类神经网络非常适合时间序列数据，因为它们能够在建模过程中考虑时间因素。
- en: Most neural networks are classified as **feedforward** **networks**. In these
    model architectures, the signals start at the input node and are passed forward
    to any number of hidden layers until they reach an output node. There is some
    variation in feedforward networks. A multilayer perceptron model is composed of
    all dense, fully connected layers while a convolutional neural network includes
    layers that operate on particular parts of the input data before arriving at a
    dense layer and subsequent output layer. In these types of models, the backpropagation
    step passes back derivatives from the cost function, but this happens after the
    entire feedforward pass is complete. RNNs differ in a very important way. Rather
    than waiting until the entire forward pass is complete, a certain data point that
    represents a point in time is fed forward and evaluated by the activation function
    in a hidden layer unit. The output from this activation function is then fed back
    into the node and calculated with the next time-based data element. In this way,
    RNNs can use what they just learned to inform how to process the next data point.
    We can see now why these work so well when we are considering data that includes
    a time component.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数神经网络被归类为**前馈** **网络**。在这些模型架构中，信号从输入节点开始，传递到任意数量的隐藏层，直到它们到达输出节点。前馈网络之间有一些变化。多层感知机模型由所有密集的全连接层组成，而卷积神经网络则包含在到达密集层和随后的输出层之前，作用于输入数据特定部分的层。在这些类型的模型中，反向传播步骤将从成本函数中传递回导数，但这发生在整个前馈传递完成之后。循环神经网络（RNN）在一个非常重要的方面有所不同。它们不是等到整个前馈传递完成，而是将某个时间点的数据传递到前面并通过隐藏层单元的激活函数进行评估。该激活函数的输出随后会反馈到节点，并与下一个基于时间的数据元素一起计算。通过这种方式，RNN
    能够利用它们刚学到的内容来指导如何处理下一个数据点。现在我们可以看到，为什么在考虑包含时间因素的数据时，这些方法表现得如此有效。
- en: While RNNs are designed to work well for time-series data, they do have one
    important limitation. The recurrent element of the model only takes into account
    the time period that just preceded it and, during backpropagation, the signal
    being passed back can decay when there are a large number of hidden layers. These
    two model characteristics mean that a given node cannot use the information it
    has learned on a distant time horizon, even though it may be useful. The LSTM
    model solves this problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RNN 被设计成能够很好地处理时间序列数据，但它们确实存在一个重要的限制。模型中的递归元素仅考虑它前面紧接的时间段，并且在反向传播过程中，当隐藏层数量很大时，传递回来的信号可能会衰减。这两种模型特性意味着，给定节点无法使用它在远期时间范围内学到的信息，尽管这些信息可能是有用的。LSTM
    模型解决了这个问题。
- en: 'In the LSTM model, there are two paths for data entering a node. One is the
    same as in an RNN and contains the given time-based data point as well as the
    output from the time point preceding it. However, if the output from the activation
    function of this vector is greater than `0`, then it is passed forward. In the
    next node, it goes on a separate path towards an activation function known as
    the **forget gate**. If the data passes through this function with a positive
    value, then it is combined with the output from the activation function that takes
    the current state and immediate past output as input. In this way, we can see
    how data from many time periods in the past can continue to be included as input
    to nodes much further forward. Using this model design, we can overcome the limitations
    of a traditional RNN. Let''s get started with training our model:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 模型中，数据进入节点有两条路径。一条与 RNN 中相同，包含给定的基于时间的数据点以及之前时间点的输出。然而，如果该向量的激活函数输出大于
    `0`，则它会被传递到前方。在下一个节点，它会走上一条独立的路径，通向一个被称为**遗忘门**的激活函数。如果数据通过这个函数时值为正，那么它将与取当前状态和紧接着的过去输出作为输入的激活函数的输出相结合。通过这种方式，我们可以看到，来自过去多个时间段的数据可以继续作为输入，传递到更远处的节点。通过这种模型设计，我们可以克服传统
    RNN 的局限性。让我们开始训练我们的模型吧：
- en: 'First, we run the following line of code to initiate a Keras sequential model:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们运行以下代码行来初始化一个 Keras 顺序模型：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After running this line of code, we will now add our LSTM layer. In our LSTM
    layer, we will choose the number of units for our hidden layer and also define
    our input shape, which is the length of the look-back defined earlier as one dimension
    and `1` as the other. We will then add a dense layer with one unit, which will
    be assigned our predicted price. To define our LSTM model, we use the following
    code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此行代码后，我们将添加LSTM层。在我们的LSTM层中，我们将选择隐藏层的单元数量，并定义输入形状，这里输入形状的一个维度是之前定义的回溯长度，另一个维度是`1`。然后，我们将添加一个具有一个单元的全连接层，该单元将赋予我们的预测价格。我们通过以下代码定义我们的LSTM模型：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After defining our model, we next include the `compile` step. In this case,
    we will use mean squared error (`mse`) as our loss function, since this is a regression
    task, and we will use `adam` as our optimizer. We define the `compile` step and
    view our model using the following code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义模型后，我们接下来进行`compile`步骤。在这种情况下，我们将使用均方误差（`mse`）作为损失函数，因为这是一个回归任务，我们将使用`adam`作为优化器。我们通过以下代码定义`compile`步骤并查看我们的模型：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After running this block of code, you will see the following printed to your
    console:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码块后，您将在控制台中看到以下输出：
- en: '![](img/28154366-27a3-495a-951d-5ac1a97d1e3f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28154366-27a3-495a-951d-5ac1a97d1e3f.png)'
- en: 'We can now train our model. To train our model, we use the generated `train`
    dataset. We will run our model for `100` rounds initially and just take one time
    step every round. We will set the verbose argument to print the results of every
    round. We train our LSTM model using the following code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练模型。为了训练模型，我们使用生成的`train`数据集。我们将初步运行100轮，每轮只取一个时间步。我们将设置verbose参数，打印每一轮的结果。我们使用以下代码训练我们的LSTM模型：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that the model has been trained, we can make our predictions. In the `predict`
    step, we choose a given number of time steps for both the `train` and `test` data.
    Afterward, we can compare these predictions with the actual values. We use our
    LSTM model to predict stock prices by running the following code:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然模型已经训练完成，我们可以进行预测。在`predict`步骤中，我们为`train`和`test`数据选择一个给定的时间步数。之后，我们可以将这些预测结果与实际值进行比较。我们通过运行以下代码，使用LSTM模型预测股票价格：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After running the preceding code, we now have our predictions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们现在得到了预测结果。
- en: 'Our next step will be to plot these together with the actual values to see
    how well our model worked. This step will require converting our data between
    a number of formats. Our first step will be to convert our vector of prediction
    to an `xts` object. In order to do this, we need to define the index values. We
    will use the `4` through `1203` index values for our `trainpredict` data. The
    reason that we start at four is because we have three lagging values being used
    to predict the value at the fourth index point. We will do the same for the test,
    but we start at the `1263` index point. We create our `xts` data objects from
    our predictions by running the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一步是将这些与实际值一起绘制，看看模型的效果如何。这一步需要在多个格式之间转换数据。我们的第一步是将预测向量转换为`xts`对象。为此，我们需要定义索引值。我们将使用`trainpredict`数据的`4`到`1203`索引值。之所以从4开始，是因为我们有三个滞后值用于预测第四个索引点的值。我们将对测试数据做同样的处理，但从`1263`索引点开始。我们通过运行以下代码从预测中创建`xts`数据对象：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we add the values from these `xts` objects to our `closing_deltas` object.
    Next, we plot our actual values, along with overlaying our predicted values. In
    order to do this, we first add columns for all the NAs and then populate just
    the rows that match the index points reflected in the prediction objects. We add
    additional columns to our `closing_delta` and `xts` objects, reflecting the predictions
    on the `train` and `test` sets by running the following code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将这些`xts`对象中的值添加到`closing_deltas`对象中。接下来，我们绘制实际值，并将预测值叠加上去。为了做到这一点，我们首先为所有的NA添加列，然后仅填充那些与预测对象中的索引点匹配的行。我们通过运行以下代码，向`closing_delta`和`xts`对象中添加反映`train`和`test`集预测值的额外列：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have the predictions merged together with the actual data, we can
    plot the results. We plot predictions as solid dark lines over the actual values
    represented by light gray, dotted lines using the following code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将预测结果与实际数据合并在一起，可以绘制结果。我们将预测值绘制为实心的深色线条，实际值则用浅灰色的虚线表示，使用以下代码：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After running this code, you will see the following plot in your **Plots**
    tab:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，您将在**Plots**标签页中看到以下图表：
- en: '![](img/555809c0-ce1e-4060-8386-589e250971cf.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/555809c0-ce1e-4060-8386-589e250971cf.png)'
- en: While the predicted results are a little conservative, note how the model is
    detecting the nuance at various time points. This model is finding more patterns
    and resulting in output with more movement than our ARIMA model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预测结果略显保守，但请注意，模型在不同时间点捕捉到了细微差别。这个模型发现了更多的模式，输出的波动比我们的ARIMA模型更大。
- en: 'In addition to plotting our data, we can also print the results of calling
    the `evaluate_generator` function to calculate the error rate. To print the error
    rate for our model, we run the following code:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了绘制数据图表外，我们还可以打印调用`evaluate_generator`函数计算误差率的结果。为了打印模型的误差率，我们运行以下代码：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running the preceding code, we see the following error rate values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们会看到以下误差率值：
- en: '![](img/1fd80fda-9471-4c1b-bf3a-aeeaf1349cae.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fd80fda-9471-4c1b-bf3a-aeeaf1349cae.png)'
- en: The warnings printed in the console can be ignored. At the time of writing,
    this is a known issue with TensorFlow through Keras. Our LSTM model so far is
    fairly minimal. Let's take a look at tuning some hyperparameters next. We will
    see what we can tune to try to achieve better performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台中打印的警告可以忽略。到目前为止，这个问题是TensorFlow通过Keras的已知问题。我们的LSTM模型到目前为止相对简单。接下来，让我们看看如何调整一些超参数。我们将尝试调整哪些参数，以期实现更好的性能。
- en: Tuning hyperparameters to improve performance
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数以提高性能
- en: To improve our model, we will now tune our hyperparameters. There are a number
    of options for tuning our LSTM model. We will focus on adjusting the length value
    when creating the time-series data with our data generator. In addition, we will
    add additional layers, adjust the number of units in the layer, and modify our
    optimizer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进我们的模型，我们现在将调整超参数。调整LSTM模型有很多选项。我们将专注于在创建时间序列数据时调整`length`值。在此基础上，我们还会添加额外的层，调整层中的单元数，并修改优化器。
- en: 'We will do so using the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤来实现：
- en: 'To get started, let''s switch the value that we pass to the `length` argument
    in the `timeseries_generator` function from `3` to `10` so that our model has
    a longer window of prices to use for forecasting calculations. To make this change,
    we run the following code:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了开始，我们将把传递给`timeseries_generator`函数中`length`参数的值从`3`改为`10`，这样我们的模型就有了更长的价格窗口来进行预测计算。为了做这个更改，我们运行以下代码：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have kept this code the same as we did earlier with just the one change to
    `length`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保持此代码与之前相同，只做了`length`的一个更改。
- en: 'Next, we will make our LSTM model deeper by adding an additional LSTM layer,
    dropout layer, and one additional dense layer. We will also change the input shape
    to reflect the next `length` parameter in the generator. Lastly, we set `return_sequences`
    to `True` in the first layer so that the signal can flow through to the additional
    layers. Without setting this to `True`, you will get an error related to the expected
    and actual dimensions of the data entering the second LSTM layer. We add additional
    layers to our LSTM model by running the following code:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过添加一个额外的LSTM层、丢弃层和一个额外的全连接层来加深我们的LSTM模型。我们还将更改输入形状，以反映生成器中的下一个`length`参数。最后，我们将第一层的`return_sequences`设置为`True`，这样信号就可以流向额外的层。如果没有将其设置为`True`，您将遇到与数据进入第二个LSTM层时期望和实际维度相关的错误。我们通过运行以下代码为LSTM模型添加额外的层：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our last modification will be to make a change to the optimizer. In this case,
    we will lower the learning rate for our optimizer. We do this to avoid any major
    spikes in our predicted values. We can adjust our optimizer by running the following
    code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的最后一个修改将是对优化器进行调整。在这种情况下，我们将降低优化器的学习率。这样做是为了避免预测值出现大的波动。我们可以通过运行以下代码来调整优化器：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After running the preceding code, the following will be printed to your console:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，以下内容将打印到控制台：
- en: '![](img/f8a77053-d0f5-46d4-82b4-42f6894d5866.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8a77053-d0f5-46d4-82b4-42f6894d5866.png)'
- en: 'Next, we can train our model as before using the following code:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以像以前一样使用以下代码来训练我们的模型：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After training our model, we can evaluate how well it performed and compare
    this with our first model. We evaluate our model and print the results to our
    console using the following code:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完我们的模型后，我们可以评估它的表现，并与我们的第一个模型进行对比。我们通过以下代码来评估模型并将结果打印到控制台：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When we run this code, we see the following results:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此代码时，我们将看到以下结果：
- en: '![](img/d8a8d662-0e64-4421-b651-58c7fec386ff.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8a8d662-0e64-4421-b651-58c7fec386ff.png)'
- en: Our modifications have produced mixed results. While the loss value for the
    `train` data is slightly worse, the error rate for the `test` data is improved.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的修改产生了混合的结果。虽然`train`数据的损失值稍微变差，但`test`数据的误差率有所改善。
- en: At this point, we have walked through all the steps necessary to create an LSTM
    model and have taken one pass at adjusting parameters to improve performance.
    Creating deep learning models is often as much an art as it is a science, so we
    encourage you to continue making adjustments and seeing whether you can further
    improve on the model. You may want to try different optimizers other than `adam` or
    experiment with including additional hidden layers. With these foundations in
    place, you are ready to make additional changes or apply this approach to a different
    dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了创建 LSTM 模型所需的所有步骤，并且初步调整了参数以提高性能。创建深度学习模型往往既是一门艺术，也是一门科学，因此我们鼓励你继续进行调整，看看是否能进一步提升模型的表现。你可能想尝试除了`adam`之外的其他优化器，或者尝试加入额外的隐藏层。有了这些基础，你已经准备好进行更多的修改，或者将这种方法应用到不同的数据集上。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by creating a baseline model to predict stock prices.
    To do this, we used an ARIMA model. Based on this model, we explored some important
    components of machine learning with time-series data, including using lagging
    variable values to predict a current variable value and the importance of stationarity.
    From there, we built a deep learning solution using Keras to assemble LSTM and
    then tuned this model further. In the process, we observed that this deep learning
    approach has some marked advantages compared to other traditional models, such
    as ARIMA. In the next chapter, we will use a generative adversarial network to
    create a synthetic face image.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先创建了一个基准模型来预测股价。为此，我们使用了 ARIMA 模型。基于这个模型，我们探索了使用时间序列数据进行机器学习的一些重要组成部分，包括使用滞后变量值来预测当前变量值以及平稳性的重要性。从这里开始，我们使用
    Keras 构建了一个深度学习解决方案，组合了 LSTM，然后进一步调优了这个模型。在这个过程中，我们观察到，与其他传统模型如 ARIMA 相比，这种深度学习方法具有一些显著的优势。在下一章中，我们将使用生成对抗网络来创建一个合成的人脸图像。
