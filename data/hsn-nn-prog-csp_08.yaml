- en: Decision Trees and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和随机森林
- en: Decision trees and random forests are powerful techniques that you can use to
    add power to your applications. Let's walk through some concepts and some code
    and hopefully have you up and running in no time.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和随机森林是强大的技术，您可以使用它们来增强您的应用程序。让我们浏览一些概念和一些代码，并希望您能迅速上手。
- en: 'In this chapter, we are going to learn about decision trees and random forests.
    We will:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习决策树和随机森林。我们将：
- en: Work through a lot of code samples to show you how you can add this powerful
    functionality to your applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过大量的代码示例向您展示如何将此强大的功能添加到您的应用程序中
- en: Discuss decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论决策树
- en: Discuss random forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论随机森林
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have Microsoft Visual Studio installed in your system.
    You may also need to refer to open source SharpLearning framework's GitHub repository
    at [https://github.com/mdabros/SharpLearning](https://github.com/mdabros/SharpLearning).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您的系统需要安装 Microsoft Visual Studio。您可能还需要参考开源 SharpLearning 框架的 GitHub 仓库，网址为
    [https://github.com/mdabros/SharpLearning](https://github.com/mdabros/SharpLearning)。
- en: Check out the following video to see Code in Action: [http://bit.ly/2O1Lbhr](http://bit.ly/2O1Lbhr).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际应用：[http://bit.ly/2O1Lbhr](http://bit.ly/2O1Lbhr)。
- en: Decision trees
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees can be used for both classification and regression. Decision
    trees answer sequential questions with a yes/no, true/false response. Based upon
    those responses, the tree follows predetermined paths to reach its goal. Trees
    are more formally a version of what is known as a directed acyclic graph. Finally,
    a decision tree is built using the entire dataset and all features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以用于分类和回归。决策树通过是/否、真/假的响应来回答连续问题。基于这些响应，树按照预定的路径达到其目标。从更正式的角度来看，树是已知的有向无环图的一种版本。最后，决策树是使用整个数据集和所有特征构建的。
- en: Here is an example of a decision tree. You may not know it as a decision tree,
    but for sure you know the process. Anyone for a doughnut?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个决策树的例子。您可能不知道它是一个决策树，但肯定知道这个过程。有人想要甜甜圈吗？
- en: '![](img/a372cb1a-d005-4c00-b232-ada277d070fa.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a372cb1a-d005-4c00-b232-ada277d070fa.png)'
- en: As you can see, the flow of a decision tree starts at the top and works its
    way downward until a specific result is achieved. The root of the tree is the
    first decision that splits the dataset. The tree recursively splits the dataset
    according to what is known as the **splitting metric** at each node. Two of the
    most popular metrics are **Gini Impurity** and **Information Gain**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，决策树流程从顶部开始，向下工作，直到达到特定的结果。树的根是第一个分割数据集的决定。树根据每个节点上所谓的**分割度量**递归地分割数据集。最流行的两个度量是**基尼不纯度**和**信息增益**。
- en: Here is another depiction of a decision tree, albeit without the great donuts!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个决策树的表示，尽管没有那些美味的甜甜圈！
- en: '![](img/ee594bda-9688-4c4f-976b-acbd2cce21cc.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee594bda-9688-4c4f-976b-acbd2cce21cc.png)'
- en: The *depth* of a decision tree represents how many questions have been asked
    so far. This is the deepest that the tree can go (the total number of questions
    that may be asked), even if some results can be achieved using fewer questions.
    For instance, using the preceding diagram, some results can be obtained after
    1 question, some after 2\. Therefore, the *depth* of that decision tree would
    be 2.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的**深度**表示到目前为止已经问过多少问题。这是树可以到达的最深程度（可能问的问题总数），即使使用更少的问题也可以得到一些结果。例如，使用前面的图，一些结果在1个问题后就可以得到，一些在2个问题后。因此，该决策树的**深度**为2。
- en: Decision tree advantages
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树优点
- en: 'The following are some advantages from using decision trees:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树的一些优点如下：
- en: Easy interpretation.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易解释。
- en: Straightforward, self-explanatory visualizations.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观、易于理解的可视化。
- en: Can be easily reproduced.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以轻松复制。
- en: Can handle both numeric and categorical data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理数值和分类数据。
- en: Perform well on very large datasets.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非常大的数据集上表现良好。
- en: Normally are very fast.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常运行非常快。
- en: Depth-wise, the location of the tree allows easy visualization of which features
    are important. The importance is denoted by the depth of the tree.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度上，树的位置允许轻松可视化哪些特征很重要。重要性由树的深度表示。
- en: Decision tree disadvantages
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树缺点
- en: 'The following are some disadvantages to using decision trees:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树的一些缺点如下：
- en: At each node, the algorithm needs to determine the correct choice. The best
    choice at one node may not necessarily be the best choice for the entire tree.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个节点，算法需要确定正确的选择。一个节点的最佳选择可能并不一定是整棵树的最佳选择。
- en: If a tree is deep, it can be prone to what is known as overfitting.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果树很深，它可能会倾向于所谓的过拟合。
- en: Decision trees can memorize the training set.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树可以记住训练集。
- en: When should we use a decision tree?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们应该在什么情况下使用决策树？
- en: 'The following are some examples of when to use a decision tree:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用决策树的一些例子：
- en: When you want a simple and explainable model
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你想要一个简单且可解释的模型时
- en: When your model should be non-parametric
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的模型应该是非参数时
- en: When you don’t want to worry about feature selection
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你不想担心特征选择时
- en: Random forests
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: We have talked about decision trees, and now it’s time to discuss random forests.
    Very basically, a random forest is a collection of decision trees. In random forests,
    a fraction of the number of total rows and features are selected at random to
    train on. A decision tree is then built upon this subset. This collection will
    then have the results aggregated into a single result.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了决策树，现在是时候讨论随机森林了。非常基本地说，随机森林是一系列决策树的集合。在随机森林中，随机选择了一部分总行数和特征数来训练。然后在这个子集上构建决策树。这个集合的结果将汇总成一个单一的结果。
- en: Random forests can also reduce bias and variance. How do they do this? By training
    on different data samples, or by using a random subset of features. Let’s take
    an example. Let’s say we have 30 features. A random forest might only use 10 of
    these features. That leaves 20 features unused, but some of those 20 features
    might be important. Remember that a random forest is a collection of decision
    trees. Therefore, in each tree, if we utilize 10 features, over time most if not
    all of our features would have been included anyway simply because of the law
    of averages. So, it is this inclusion that helps limit our error due to bias and
    variance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林还可以减少偏差和方差。它们是如何做到这一点的？通过在不同的数据样本上训练，或者通过使用特征的随机子集。让我们举一个例子。假设我们有30个特征。随机森林可能只使用其中的10个特征。这留下了20个未使用的特征，但其中一些20个特征可能很重要。记住，随机森林是一系列决策树的集合。因此，在每一棵树中，如果我们利用10个特征，随着时间的推移，大多数甚至所有我们的特征都会被包括在内，这仅仅是因为平均法则。所以，正是这种包含帮助我们限制由于偏差和方差引起的错误。
- en: For large datasets, the number of trees can grow quite large, sometimes into
    the tens of thousands and more, depending on the number of features you are using,
    so you need to be careful regarding performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，树的数量可能会非常大，有时达到数万甚至更多，这取决于你使用的特征数量，因此你需要注意性能。
- en: 'Here is a diagram of what a random forest might look like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个随机森林可能看起来像的图示：
- en: '![](img/fef193f7-c28b-492e-838e-7352b2518c59.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fef193f7-c28b-492e-838e-7352b2518c59.png)'
- en: Random forest advantages
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林的优点
- en: 'The following are some advantages from using random forests:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用随机森林的一些优点：
- en: More robust than just a single decision tree
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比单个决策树更稳健
- en: Random forests contain many decision trees and are therefore able to limit overfitting
    and error
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林包含许多决策树，因此能够限制过拟合和错误
- en: Depth-wise, the location shows which features contribute to the classification
    or regression as well as their relative importance
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度方向上，位置显示了哪些特征对分类或回归有贡献以及它们的相对重要性
- en: Can be used for both regression and classification
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用于回归和分类
- en: Default parameters can be sufficient
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认参数可能足够
- en: Fast to train
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度快
- en: Random forest disadvantages
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林的缺点
- en: 'The following are some disadvantages to using random forests:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用随机森林的一些缺点：
- en: Random forests need to be done in parallel in order to increase speed
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林需要并行处理以提高速度
- en: Predictions can be slow to create once trained
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦训练完成，预测可能创建得较慢
- en: More accuracy requires more trees, and this can result in a slower model
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的精度需要更多的树，这可能会导致模型变慢
- en: When should we use a random forest?
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们应该在什么情况下使用随机森林？
- en: 'The following are some examples of when to use random forests:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用随机森林的一些例子：
- en: When model interpretation is not the most important criterion. Interpretation
    will not be as easy as a single tree.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型解释不是最重要的标准时。解释可能不会像单棵树那样简单。
- en: When model accuracy is most important.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型精度是最重要的。
- en: When you want robust classification, regression, and feature selection analysis.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你想要稳健的分类、回归和特征选择分析时。
- en: To prevent overfitting.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了防止过拟合。
- en: Image classification.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类。
- en: Recommendation engines.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎。
- en: SharpLearning
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SharpLearning
- en: Let’s now turn our attention to an incredible open source package, SharpLearning.
    **SharpLearning** is an excellent machine learning framework for individuals to
    learn about many aspects of machine learning, including decision trees and random
    forests as we described in the preceding sections. Let’s spend a few minutes getting
    familiar with a few things before we dive into some code samples and example applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向一个令人难以置信的开源包，SharpLearning。**SharpLearning** 是一个出色的机器学习框架，个人可以从中了解许多机器学习的方面，包括我们在前几节中描述的决策树和随机森林。在我们深入研究代码示例和示例应用之前，让我们花几分钟时间熟悉一些事情。
- en: Terminology
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语
- en: 'Throughout this chapter you are going to see the following terms used. Here
    is the context for what each of them means:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将看到以下术语的使用。以下是每个术语的含义背景：
- en: '**Learner**: This refers to a machine learning algorithm.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习器**：这指的是一个机器学习算法。'
- en: '**Model**: This refers to a machine learning model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：这指的是一个机器学习模型。'
- en: '**Hyper-parameters**: These are the parameters used to adjust and regulate
    (hopefully) the machine learning model.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：这些是用于调整和调节（希望是）机器学习模型的参数。'
- en: '**Targets**: These are more commonly referred to as a dependent variable. In
    most notations, this will be *y*. These are the values that we are attempting
    to model.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：这些更常见地被称为因变量。在大多数记号中，这将表示为 *y*。这些是我们试图建模的值。'
- en: '**Observations**: These are the feature matrix, which contains all the information
    we currently have about the targets. In most notations, this will be *x*.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观测值**：这些是特征矩阵，其中包含我们目前关于目标的所有信息。在大多数记号中，这将表示为 *x*。'
- en: 'Throughout most of our examples we will be focusing on two namespaces within
    SharpLearning. They are:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的大多数示例中，我们将关注 SharpLearning 中的两个命名空间。它们是：
- en: '`SharpLearning.DecisionTrees`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SharpLearning.DecisionTrees`'
- en: '`SharpLearning.RandomForest`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SharpLearning.RandomForest`'
- en: With that behind us, let’s start digging into SharpLearning and show you a few
    concepts relative to how it works.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们开始深入研究 SharpLearning，向您展示一些与它的工作方式相关的基本概念。
- en: Loading and saving models
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和保存模型
- en: SharpLearning makes it very easy to load and save models to disk. This is a
    very important part of a machine learning library and SharpLearning is among the
    easiest to implement.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SharpLearning 使将模型加载和保存到磁盘变得非常容易。这是机器学习库的一个重要部分，而 SharpLearning 是最容易实现的部分之一。
- en: All models in SharpLearning have a `Save` and a `Load` method. These methods
    do the heavy lifting of saving and loading a model for us.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SharpLearning 中的所有模型都有一个 `Save` 和一个 `Load` 方法。这些方法为我们完成了保存和加载模型的繁重工作。
- en: 'As an example, here we will save a model that we learned to disk:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里我们将保存一个我们学习到的模型到磁盘：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we want to load this model back in, we simply use the `Load` method:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想重新加载此模型，我们只需使用 `Load` 方法：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Yep, it’s that easy and simple to load and save your data models. It is also
    possible for you to save models using serialization. This will allow us to choose
    between XML and a Binary format. Another very nice design feature of SharpLearning
    is that serializing models allows us to serialize to the `IPredictorModel` interface.
    This makes replacing your models much easier, if each conforms to that interface.
    Here’s how we would do that:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，加载和保存数据模型既简单又容易。您还可以使用序列化来保存模型。这将使我们能够在 XML 和二进制格式之间进行选择。SharpLearning 的另一个非常棒的设计特性是，序列化模型允许我们将其序列化为
    `IPredictorModel` 接口。这使得如果每个模型都符合该接口，替换模型变得更加容易。以下是这样做的方法：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '| **Algorithm** | **Train Error** | **Test Error** |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **训练误差** | **测试误差** |'
- en: '| --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RegressionDecisionTreeLearner(default) | 0.0518 | 0.4037 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 回归决策树学习器（默认） | 0.0518 | 0.4037 |'
- en: And there you have it, instant training and testing errors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如此一来，立即得到训练和测试误差。
- en: When reporting the performance of your model, you should always use the test
    error even if the training error is lower, since that is an estimate of how well
    the model generalizes to new data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当报告您模型的性能时，您应该始终使用测试误差，即使训练误差更低，因为那是模型对新数据泛化能力的估计。
- en: 'Now, let''s talk for a second about **hyperparameters**. Hyperparameters are
    parameters that affect the learning process of the machine learning algorithm.
    You can adjust them to tune the process and improve performance and reliability.
    At the same time, you can also incorrectly adjust parameters and have something
    that does not work as intended. Let''s look at a few things that can happen with
    an incorrectly tuned hyperparameter:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈**超参数**。超参数是影响机器学习算法学习过程的参数。你可以调整它们来调整过程并提高性能和可靠性。同时，你也可以错误地调整参数，得到一个不符合预期的工作结果。让我们看看一个错误调整的超参数可能会发生的一些事情：
- en: If the model is too complex, you can end up with what is known as high variance,
    or **Overfitting**
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型过于复杂，你可能会得到所谓的**高方差**，或称为**过拟合**
- en: If the model ends up being too simple, you will end up with what is known as
    high bias, or **Underfitting**
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型最终变得过于简单，你将得到所谓的**高偏差**，或称为**欠拟合**
- en: 'For those who have not done so, manually tuning hyperparameters, a process
    that happens in almost every use case, can take a considerable amount of your
    time. As the number of hyperparameters increases with the model, the tuning time
    and effort increase as well. The best way around this is to use an optimizer and
    let the work happen for you. To this end, SharpLearning can be a huge help to
    us due to the numerous optimizers that are available for it. Here is a list of
    just some of them:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些还没有这样做的人来说，手动调整超参数，这个过程几乎在每一个用例中都会发生，可能会占用你相当多的时间。随着模型超参数数量的增加，调整时间和努力也会增加。解决这个问题最好的办法是使用一个优化器，让它为你完成工作。为此，SharpLearning可以为我们提供巨大的帮助，因为它提供了许多可用的优化器。以下是一些其中的例子：
- en: Grid search
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Random search
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Particle swarm (which we will talk about in [Chapter 7](34e7d6b8-85b9-42ff-865b-86f158138320.xhtml),
    *Replacing Back Propagation with PSO*)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粒子群（我们将在[第7章](34e7d6b8-85b9-42ff-865b-86f158138320.xhtml)中讨论，*用PSO替换反向传播*）
- en: Bayesian optimization
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Globalized bounded nelder mead
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全球化边界内尔德梅德
- en: Let’s start with an example.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从例子开始。
- en: 'Let’s create a learner and use the default parameters, which, more than likely,
    will be good enough. Once we find our parameters and create the learner, we need
    to create the model. We then will predict the training and test set. Once all
    of that is complete, we will measure the error on the test set and record it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个学习器并使用默认参数，这些参数很可能已经足够好了。一旦我们找到参数并创建学习器，我们需要创建模型。然后我们将预测训练集和测试集。一旦所有这些都完成，我们将测量测试集上的误差并记录下来：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And here is our test set error
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的测试集误差
- en: '| **Algorithm** | **Test Error** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **测试误差** |'
- en: '| --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RegressionSquareLossGradientBoostLearner (default) | 0.4984 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 回归平方损失梯度提升学习器（默认） | 0.4984 |'
- en: 'With that part complete, we now have our baseline established. Let’s use a
    `RandomSearchOptimizer` to tune the hyperparameters to see if we can get any better
    results. To do this we need to establish the bounds of the hyperparameters, so
    our optimizer knows how to tune. Let’s look at how we do this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 那部分完成之后，我们现在已经建立了基线。让我们使用`RandomSearchOptimizer`来调整超参数，看看我们是否能得到更好的结果。为此，我们需要确定超参数的范围，这样我们的优化器就知道如何调整。让我们看看我们是如何做到这一点的：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Did you notice that we used a Logarithmic transform for the learning rate?
    Do you know why we did this? The answer is: to ensure that we had a more even
    distribution across the entire range of values. We have a large range difference
    between our minimum and maximum values (0.02 -> 0.2), so the logarithmic transform
    will be best.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到我们使用了学习率的对数变换吗？你知道为什么这样做吗？答案是：确保在整个值域上有一个更均匀的分布。我们的最小值和最大值之间有一个很大的范围差异（0.02
    -> 0.2），因此对数变换将是最好的。
- en: 'We now need a *validation set* to help us measure how well the model generalizes
    to unseen data during our optimization. To do this, we will need to further split
    the training data. To do this, we are going to leave our current test set out
    of the optimization process. If we don’t, we risk getting a positive bias on our
    final error estimate, and that will not be what we want:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要一个**验证集**来帮助我们衡量模型在优化过程中对未见数据的泛化能力。为此，我们需要进一步分割训练数据。为此，我们将把当前的测试集排除在优化过程之外。如果不这样做，我们可能会在最终的误差估计上产生正偏差，而这并不是我们想要的：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'One more thing that the optimizer will need is an objective function. The function
    will take a double array as input (containing the set of hyperparameters) and
    return an `OptimizerResult` that contains the validation error and the corresponding
    set of hyperparameters:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器还需要另一件事，那就是目标函数。该函数将接受一个double数组作为输入（包含超参数集）并返回一个`OptimizerResult`，其中包含验证错误和相应的超参数集：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once this `objective` function has been defined, we can now create and run
    the optimizer to find the best set of parameters. Let’s start out by running our
    optimizer for 30 iterations and trying out 30 different sets of hyperparameters:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了`objective`函数，我们现在可以创建并运行优化器以找到最佳参数集。让我们先运行我们的优化器30次迭代，并尝试30个不同的超参数集：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we run this, our optimizer should find the best set of hyperparameters.
    Let’s see what it finds:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行这个，我们的优化器应该找到最佳的超参数集。让我们看看它找到了什么：
- en: '`Trees`: 277'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Trees`: 277'
- en: '`learningRate`: 0.035'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`学习率`: 0.035'
- en: '`maximumTreeDepth`: 15'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maximumTreeDepth`: 15'
- en: '`subSampleRatio`: 0.838'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subSampleRatio`: 0.838'
- en: '`featuresPrSplit`: 4'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featuresPrSplit`: 4'
- en: 'Progress. Now that we have a set of best hyperparameters, which were measured
    on our validation set, we can create a learner with these parameters and learn
    a new model using the entire dataset:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 进度。现在我们已经有一组在验证集上测量的最佳超参数，我们可以使用这些参数创建一个学习器，并使用整个数据集学习一个新的模型：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With our final set of hyperparameters now intact, we pass these to our learner
    and are able to reduce the test error significantly. For us to do that manually
    would have taken us an eternity and beyond!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了最终的超参数集，我们将这些传递给我们的学习器，能够显著降低测试错误。如果我们手动做这件事，将花费我们无尽的时光和更多！
- en: '| **Algorithm** | **Test Error** |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **测试错误** |'
- en: '| --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RegressionSquareLossGradientBoostLearner (default) | 0.4984 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 回归平方损失梯度提升学习器（默认） | 0.4984 |'
- en: '| RegressionSquareLossGradientBoostLearner (Optimizer) | 0.3852 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 回归平方损失梯度提升学习器（优化器） | 0.3852 |'
- en: Example code and applications
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例代码和应用
- en: In the next few sections, we are going to look at some code samples without
    all the verbosity. This will be pure C# code so it should be something easily
    understood by all.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将查看一些没有所有冗余的代码示例。这将完全是C#代码，所以它应该是所有人都容易理解的。
- en: 'Let’s take a quick look at how we can use SharpLearning to predict observations.
    I’ll show you an entire code sample without the verbosity:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下我们如何使用SharpLearning来预测观察值。我将向您展示一个完整的代码示例，而不包含冗余：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Saving a model
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存模型
- en: 'Here is a code sample that will show you how easy it is to save a model:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个代码示例，将向您展示保存模型是多么容易：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Mean squared error regression metric
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差回归指标
- en: '**Mean squared error** (**MSE**) is a metric that measures the average of the
    squares of the errors. More concretely, it measures the average distance between
    the estimated values and what is estimated. A mean squared error is always non-negative,
    and values that are closer to zero are considered more acceptable. SharpLearning
    makes it incredibly easy to calculate this error metric, as depicted in the following
    code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（MSE）是一个衡量误差平方平均值的指标。更具体地说，它衡量估计值与估计值之间的平均距离。均方误差始终为非负值，接近零的值被认为是更可接受的。SharpLearning使得计算此误差指标变得极其简单，如下面的代码所示：'
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: F1 score
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1分数
- en: To talk about an f1 score we must first talk about *Precision* and *Recall*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要谈论f1分数，我们必须首先谈论*精确度*和*召回率*。
- en: '**Precision** is the ratio of correctly predicted positive observations divided
    by the total predicted positive observations. Less formally, of all the people
    that said they were coming, how many came?'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**是正确预测的正观察值与总预测正观察值的比率。不太正式地说，在所有说他们会来的人中，有多少人真的来了？'
- en: '**Recall** (sensitivity) is the ratio of correctly predicted positive observations
    to all observations in total.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**回忆**（灵敏度）是正确预测的正观察值与总观察值的比率。'
- en: '**F1 score** is then the weighted average of Precision and Recall.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1分数**是精确度和召回率的加权平均值。'
- en: 'Here’s how we calculate an f1 score using SharpLearning:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用SharpLearning计算f1分数的方法：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Optimizations
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: 'The following is how you can use the *Particle Swarm Optimizer* to return the
    result that best minimizes the provided function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用*粒子群优化器*返回最佳最小化提供的函数的结果的方法：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is how you can use the *Grid Search Optimizer* to optimize by
    trying all combinations of the provided parameters:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用*网格搜索优化器*通过尝试提供的参数的所有组合来优化的方法：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is how you can use the *Random Search Optimizer* to initialize
    random parameters between the min and max provided. The result that best minimizes
    the provided function will be returned:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用 *随机搜索优化器* 在提供的最小值和最大值之间初始化随机参数的方法。将返回最小化提供函数的最佳结果：
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Sample application 1
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本应用 1
- en: 'With all this knowledge under our belt, let’s go ahead and write our first
    sample program. The program itself is very simple and meant to show how easy it
    is to implement such techniques in your applications with a minimal amount of
    code. To show you exactly what I mean, here is what the output of the program
    looks like:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握所有这些知识之后，让我们继续编写我们的第一个样本程序。程序本身非常简单，旨在展示如何以最少的代码实现这些技术。为了展示我的意思，以下是程序输出的样子：
- en: '![](img/a5da7152-6cf1-4d74-a8e3-939463ceb358.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5da7152-6cf1-4d74-a8e3-939463ceb358.jpg)'
- en: The code
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'Here is the code that implements our sample program and produces the previous
    output. As you can see, the code is very simple and everything in here (save the
    shuffling of indices) is code we have already walked through before. We’ll keep
    the verbosity to a minimum so that you can concentrate on the code itself. This
    sample will read in our data, parse it into observations and target samples, and
    then create a learner using 1,000 trees. From there we will use the learner to
    learn and create our model. Once this is complete we will calculate our mean squared
    error metric and display it on the screen:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是实现我们的样本程序并产生之前输出的代码。如您所见，代码非常简单，这里面的所有代码（除了索引的洗牌）都是我们之前已经介绍过的。我们将保持详尽性到最小，以便您能专注于代码本身。这个样本将读取我们的数据，将其解析成观察和目标样本，然后使用1,000棵树创建一个学习器。然后我们将使用这个学习器来学习和创建我们的模型。一旦完成，我们将计算均方误差指标并在屏幕上显示：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Sample application 2 – wine quality
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本应用 2 – 葡萄酒质量
- en: 'In our next application, we are going to use our knowledge to determine the
    most important features for wine based upon the model that is created. Here is
    what our output will look like when we complete it:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个应用程序中，我们将使用我们的知识来确定基于创建的模型，葡萄酒最重要的特征。以下是完成它后我们的输出将看起来像什么：
- en: '![](img/fe7ac6eb-9a76-4d5c-b02a-ad46e51e49a0.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe7ac6eb-9a76-4d5c-b02a-ad46e51e49a0.png)'
- en: The code
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'Here is the code for our application. As always, we first load and parse our
    data into observations and target sample sets. Since this is a regression sample,
    we’ll use a 70/30 split of our data sample: 70% for training, 30% for testing.
    From there, we create our random forest learner and create our model. After this,
    we calculate our training and test errors and print out the feature importance
    in order of importance, as found by our model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们应用程序的代码。和往常一样，我们首先将数据加载并解析成观察和目标样本集。由于这是一个回归样本，我们将数据样本分成70/30的比例：70%用于训练，30%用于测试。然后，我们创建我们的随机森林学习者和模型。之后，我们计算训练和测试错误，并按模型找到的重要性顺序打印出特征重要性：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about decision trees and random forests. We also
    learned how to use the open source framework, SharpLearn, to add these powerful
    features to our applications. In the next chapter, we are going to learn about
    facial and motion detection and show you how you can enable your application with
    this exciting technology! You’ll meet Frenchie, my pet French Bulldog, who will
    demonstrate most of the samples we will show. Also, we have a guest poser you
    will just have to see!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了决策树和随机森林。我们还学习了如何使用开源框架SharpLearn将这些强大的功能添加到我们的应用程序中。在下一章中，我们将学习关于面部和动作检测的内容，并展示您如何使用这项令人兴奋的技术来启用您的应用程序！您将见到我的宠物法国斗牛犬Frenchie，他将演示我们将展示的大多数样本。此外，我们还有一位嘉宾，您绝对会想见一见！
- en: References
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://github.com/mdabros/SharpLearning](https://github.com/mdabros/SharpLearning)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/mdabros/SharpLearning](https://github.com/mdabros/SharpLearning)'
