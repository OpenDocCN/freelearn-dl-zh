- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Image Captioning with Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Transformer 进行图像标题生成
- en: 'Transformer models changed the playing field for many NLP problems. They have
    redefined the state of the art by a significant margin, compared to the previous
    leaders: RNN-based models. We have already studied Transformers and understood
    what makes them tick. Transformers have access to the whole sequence of items
    (e.g. a sequence of tokens), as opposed to RNN-based models that look at one item
    at a time, making them well-suited for sequential problems. Following their success
    in the field of NLP, researchers have successfully used Transformers to solve
    computer vision problems. Here we will learn how to use Transformers to solve
    a multi-modal problem involving both images and text: image captioning.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型改变了许多 NLP 问题的解决方式。与之前的主流模型 RNN 模型相比，它们通过显著的优势重新定义了当前的技术水平。我们已经研究过
    Transformer，并理解了它们的工作原理。Transformer 可以访问整个序列的所有项（例如，一个 token 序列），而 RNN 模型一次只查看一个项，这使得
    Transformer 更适合解决序列问题。在 NLP 领域取得成功之后，研究人员已经成功地将 Transformer 应用于计算机视觉问题。在这里，我们将学习如何使用
    Transformer 来解决一个涉及图像和文本的多模态问题：图像标题生成。
- en: Automated image captioning, or image annotation, has a wide variety of applications.
    One of the most prominent applications is image retrieval in search engines. Automated
    image captioning can be used to retrieve all the images belonging to a certain
    class (for example, a cat) as per the user’s request. Another application can
    be in social media where, when an image is uploaded by a user, the image is automatically
    captioned so that the user can either refine the generated caption or post it
    as it is.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自动图像标题生成，或称图像注释，具有广泛的应用场景。一个最突出的应用是搜索引擎中的图像检索。自动图像标题生成可以用于根据用户的请求，检索属于某一特定类别（例如，猫）的所有图像。另一个应用可能是在社交媒体中，当用户上传一张图像时，图像会自动生成标题，用户可以选择修改生成的标题或直接发布原始标题。
- en: In this chapter, we will learn to caption images using machine learning, where
    a model is trained to generate a sequence of tokens (i.e. a caption) when given
    an image. We will first understand how Transformer models are used in computer
    vision, and then extend our understanding to solve the problem of generating captions
    for images. For generating captions for images, we will use a popular dataset
    for image captioning tasks known as **Microsoft Common Objects in Context** (**MS-COCO**).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用机器学习为图像生成标题，训练一个模型，在给定图像时生成一个 token 序列（即标题）。我们将首先了解 Transformer
    模型如何在计算机视觉中应用，然后扩展我们的理解，解决为图像生成标题的问题。为了生成图像标题，我们将使用一个广泛应用于图像标题生成任务的流行数据集，称为 **Microsoft
    Common Objects in Context**（**MS-COCO**）。
- en: 'Solving this will require two Transformer models: one to generate an image
    representation and the other to generate the relevant caption. Once the image
    representation is generated, it will be fed as one of the inputs to the text-based
    Transformer model. The text-based Transformer model will be trained to predict
    the next token in the caption given the current caption, at a given time step.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题需要两个 Transformer 模型：一个用于生成图像表示，另一个用于生成相关的标题。一旦图像表示生成完成，它将作为其中一个输入传递给基于文本的
    Transformer 模型。基于文本的 Transformer 模型将被训练以预测给定当前标题的情况下，在特定时间步长下标题中下一个 token。
- en: 'We will generate three datasets: training, validation, and testing datasets.
    We use the training dataset to train the model and the validation set to monitor
    performance during training. Finally we use the test dataset to generate captions
    for a set of unseen images.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成三个数据集：训练集、验证集和测试集。我们使用训练集来训练模型，验证集用于在训练过程中监控模型表现，最后使用测试集为一组未见过的图像生成标题。
- en: 'Looking at the image caption generation pipeline at a very high level, we have
    two main components:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从非常高层次来看图像标题生成流程，我们有两个主要组件：
- en: A pretrained Vision Transformer model that takes in an image and produces a
    1D hidden representation of the image
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个预训练的视觉 Transformer 模型，它接受图像并生成该图像的 1D 隐藏表示
- en: A text-based Transformer decoder model that can decode the hidden image representation
    to a series of token IDs
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个基于文本的 Transformer 解码器模型，它可以将隐藏的图像表示解码成一系列 token ID
- en: We will use a pretrained Transformer model to generate image representations.
    Known as the Vision Transformer (ViT), it has been trained on the ImageNet dataset
    and has delivered great performance on the ImageNet classification task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个预训练的 Transformer 模型来生成图像表示。这个模型被称为视觉 Transformer（ViT），它已经在 ImageNet 数据集上进行了训练，并且在
    ImageNet 分类任务中取得了优异的表现。
- en: 'Specifically, this chapter will cover the following main topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论以下主要主题：
- en: Getting to know the data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解数据
- en: Downloading the data
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载数据
- en: Processing and tokenizing data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理和标记数据
- en: Defining a `tf.data.Dataset`
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 `tf.data.Dataset`
- en: The machine learning pipeline for image caption generation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕生成的机器学习流程
- en: Implementing the model with TensorFlow
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现模型
- en: Training the model
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Evaluating the results quantitatively
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定量评估结果
- en: Evaluating the model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Captions generated for test images
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为测试图像生成的字幕
- en: Getting to know the data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解数据
- en: 'Let’s first understand the data we are working with both directly and indirectly.
    There are two datasets we will rely on:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们了解我们所使用的数据，既包括直接使用的，也包括间接使用的。我们将依赖两个数据集：
- en: The ILSVRC ImageNet dataset ([http://image-net.org/download](http://image-net.org/download))
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ILSVRC ImageNet 数据集 ([http://image-net.org/download](http://image-net.org/download))
- en: The MS-COCO dataset ([http://cocodataset.org/#download](http://cocodataset.org/#download))
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MS-COCO 数据集 ([http://cocodataset.org/#download](http://cocodataset.org/#download))
- en: We will not engage the first dataset directly, but it is essential for caption
    learning. This dataset contains images and their respective class labels (for
    example, cat, dog, and car). We will use a CNN that is already trained on this
    dataset, so we do not have to download and train on this dataset from scratch.
    Next we will use the MS-COCO dataset, which contains images and their respective
    captions. We will directly learn from this dataset by mapping the image to a fixed-size
    feature vector, using the Vision Transformer, and then map this vector to the
    corresponding caption using a text-based Transformer (we will discuss this process
    in detail later).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会直接使用第一个数据集，但它对字幕学习至关重要。这个数据集包含图像及其相应的类别标签（例如，猫、狗和汽车）。我们将使用一个已经在这个数据集上训练好的
    CNN，因此我们无需从头开始下载和训练该数据集。接下来我们将使用 MS-COCO 数据集，它包含图像及其相应的字幕。我们将通过将图像映射到一个固定大小的特征向量，使用
    Vision Transformer，然后使用基于文本的 Transformer 将该向量映射到相应的字幕（我们稍后会详细讨论这一过程）。
- en: ILSVRC ImageNet dataset
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ILSVRC ImageNet 数据集
- en: 'ImageNet is an image dataset that contains a large set of images (~1 million)
    and their respective labels. These images belong to 1,000 different categories.
    This dataset is very expressive and contains almost all the objects found in the
    images we want to generate captions for. *Figure 11.1* shows some of the classes
    available in the ImageNet dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet 是一个包含大量图像（约 100 万张）及其相应标签的图像数据集。这些图像属于 1,000 个不同的类别。该数据集非常具有表现力，几乎包含了我们想为其生成字幕的所有图像中的对象。*图
    11.1* 展示了 ImageNet 数据集中一些可用的类别：
- en: '![ILSVRC ImageNet dataset](img/B14070_11_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![ILSVRC ImageNet 数据集](img/B14070_11_01.png)'
- en: 'Figure 11.1: A small sample of the ImageNet dataset'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：ImageNet 数据集的一个小样本
- en: ImageNet is a good dataset to train on, in order to obtain image encodings that
    are required for caption generation. We say we use this dataset indirectly because
    we will use a pretrained Transformer that is trained on this dataset. Therefore,
    we will not be downloading, nor training the model on this dataset, by ourselves.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet 是一个很好的训练数据集，用于获取生成字幕所需的图像编码。我们说我们间接使用这个数据集，因为我们将使用一个在这个数据集上预训练的 Transformer。因此，我们自己不会下载或在这个数据集上训练模型。
- en: The MS-COCO dataset
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MS-COCO 数据集
- en: 'Now we will move on to the dataset that we will actually be using, which is
    called **MS-COCO** (short for **Microsoft - Common Objects in Context**). We will
    use the training dataset from the year 2014 and the validation set from 2017\.
    We use datasets belonging to different times to avoid using large datasets for
    this exercise. As described earlier, this dataset consists of images and their
    respective descriptions. The dataset is quite large (for example, the training
    dataset consists of ~120,000 samples and can measure over 15 GB). Datasets are
    updated every year, and a competition is then held to recognize the team that
    achieves state-of-the-art performance. Using the full dataset is important when
    the objective is to achieve state-of-the-art performance. However, in our case,
    we want to learn a reasonable model that is able to suggest what is in an image
    generally. Therefore, we will use a smaller dataset (~40,000 images and ~200,000
    captions) to train our model. *Figure 11.2* includes some of the samples available:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转向我们实际使用的数据集，它被称为**MS-COCO**（即**Microsoft - Common Objects in Context**的缩写）。我们将使用2014年的训练数据集和2017年的验证数据集。我们使用不同时期的数据集，以避免在本练习中使用大型数据集。如前所述，该数据集包含图像及其相应的描述。数据集非常庞大（例如，训练数据集包含约120,000个样本，大小超过15GB）。数据集每年更新一次，并举行竞赛，表彰那些在此数据集上取得最先进成绩的团队。在目标是达到最先进的性能时，使用完整数据集很重要。然而，在我们这种情况中，我们希望学习一个合理的模型，能够一般性地推测图像中有什么。因此，我们将使用较小的数据集（约40,000张图像和约200,000个描述）来训练我们的模型。*图
    11.2*展示了可用的一些样本：
- en: '![](img/B14070_11_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_02.png)'
- en: 'Figure 11.2: A small sample of the MS-COCO dataset'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：MS-COCO 数据集的小样本
- en: For learning with and testing our end-to-end image caption generation model,
    we will use the 2017 validation dataset, provided on the official MS-COCO dataset
    website.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和测试我们的端到端图像描述生成模型，我们将使用2017年的验证数据集，数据集可以从官方的MS-COCO数据集网站获取。
- en: '**Note**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: In practice, you should use separate datasets for testing and validation, to
    avoid data leakage during testing. Using the same data for validation and testing
    can lead the model to incorrectly represent its generalizability to the real world.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，您应使用单独的数据集进行测试和验证，以避免在测试过程中数据泄露。使用相同的数据进行验证和测试可能导致模型错误地表示其在现实世界中的泛化能力。
- en: 'In *Figure 11.3*, we can see some of the images found in the validation set.
    These are some hand-picked examples from the validation set representing a variety
    of different objects and scenes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 11.3*中，我们可以看到验证集中一些图像的样本。这些是从验证集中精心挑选的例子，代表了各种不同的物体和场景：
- en: '![The MS-COCO dataset](img/B14070_11_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![MS-COCO 数据集](img/B14070_11_03.png)'
- en: 'Figure 11.3: Unseen images that we will use to test the image caption generation
    capability of our algorithm'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：我们将用来测试算法生成图像描述能力的未见图像
- en: Downloading the data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载数据
- en: 'The MS-COCO dataset we will be using is quite large. Therefore, we will manually
    download these datasets. To do that, follow the instructions below:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 MS-COCO 数据集相当大。因此，我们将手动下载这些数据集。为此，请按照以下说明操作：
- en: Create a folder called `data` in the `Ch11-Image-Caption-Generation` folder
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Ch11-Image-Caption-Generation`文件夹中创建一个名为`data`的文件夹
- en: Download the 2014 Train images set ([http://images.cocodataset.org/zips/train2014.zip](http://images.cocodataset.org/zips/train2014.zip))
    containing 83K images (`train2014.zip`)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 2014 年的训练图像集（[http://images.cocodataset.org/zips/train2014.zip](http://images.cocodataset.org/zips/train2014.zip)），该集包含
    83K 张图像（`train2014.zip`）
- en: Download the 2017 Val images set ([http://images.cocodataset.org/zips/val2017.zip](http://images.cocodataset.org/zips/val2017.zip))
    containing 5K images (`val2017.zip`)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 2017 年的验证图像集（[http://images.cocodataset.org/zips/val2017.zip](http://images.cocodataset.org/zips/val2017.zip)），该集包含
    5K 张图像（`val2017.zip`）
- en: Download the annotation sets for 2014 (`annotations_trainval2014.zip`) ([http://images.cocodataset.org/annotations/annotations_trainval2014.zip](http://images.cocodataset.org/annotations/annotations_trainval2014.zip))
    and 2017 (`annotations_trainval2017.zip`) ([http://images.cocodataset.org/annotations/annotations_trainval2017.zip](http://images.cocodataset.org/annotations/annotations_trainval2017.zip))
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 2014 年（`annotations_trainval2014.zip`）（[http://images.cocodataset.org/annotations/annotations_trainval2014.zip](http://images.cocodataset.org/annotations/annotations_trainval2014.zip)）和2017年（`annotations_trainval2017.zip`）（[http://images.cocodataset.org/annotations/annotations_trainval2017.zip](http://images.cocodataset.org/annotations/annotations_trainval2017.zip)）的注释集
- en: Copy the downloaded zip files to the `Ch11-Image-Caption-Generation/data` folder
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下载的压缩文件复制到`Ch11-Image-Caption-Generation/data`文件夹中
- en: Extract the zip files using the **Extract to** option so that it unzips the
    content within a sub-folder
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **Extract to** 选项解压缩 zip 文件，使其在子文件夹内解压缩内容
- en: 'Once you complete the above steps, you should have the following subfolders:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完成上述步骤后，你应该会有以下子文件夹：
- en: '`data/train2014` – Contains the training images'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/train2014` – 包含训练图像'
- en: '`data/annotations_trainval2014` – Contains the captions of the training images'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/annotations_trainval2014` – 包含训练图像的标题'
- en: '`data/val2017` – Contains the validation images'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/val2017` – 包含验证图像'
- en: '`data/annotations_trainval2017` – Contains the captions of the validation images'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/annotations_trainval2017` – 包含验证图像的标题'
- en: Processing and tokenizing data
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理和标记化
- en: 'With the data downloaded and placed in the correct folders, let’s define the
    directories containing the required data:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据下载并放入正确的文件夹后，让我们定义包含所需数据的目录：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we have defined the directories containing training and testing images
    as well as the file paths of the JSON files that contain the captions of the training
    and testing images.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们定义了包含训练和测试图像的目录，以及包含训练和测试图像标题的 JSON 文件路径。
- en: Preprocessing data
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'As the next step, let’s split the training set in to train and validation sets.
    We will use 80% of the original set as training data and 20% as the validation
    data (randomly chosen):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是将训练集分割为训练集和验证集。我们将使用原始数据集的 80% 作为训练数据，20% 作为验证数据（随机选择）：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can print the dataset sizes and see what we ended up with:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印数据集的大小，看看我们得到了什么：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will print:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s read the captions and create a pandas DataFrame using them. Our DataFrame
    will have four important columns:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们读取标题并使用它们创建一个 pandas DataFrame。我们的 DataFrame 将包含四个重要的列：
- en: '`image_id` – Identifies an image (used to generate the file path)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_id` – 标识图像（用于生成文件路径）'
- en: '`image_filepath` – File location of the image identified by `image_id`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_filepath` – 由 `image_id` 标识的图像文件位置'
- en: '`caption` – Original caption'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caption` – 原始标题'
- en: '`preprocessed_caption` – Caption after some simple preprocessing'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocessed_caption` – 经简单预处理后的标题'
- en: 'First we will load the data in the JSON file and get the data into a DataFrame:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载 JSON 文件中的数据，并将其导入到 DataFrame 中：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The data we’re looking for in the file is found under a key called `"annotations"`.
    Under `"``annotations"` we have a list of dictionaries each having the `image_id`,
    `id`, and `caption`. The function `pd.json_normalize()` takes in the loaded data
    and converts that to a `pd.DataFrame`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在文件中寻找的数据位于一个名为 `"annotations"` 的键下。在 `"annotations"` 下，我们有一个字典列表，每个字典包含 `image_id`、`id`
    和 `caption`。函数 `pd.json_normalize()` 接受加载的数据并将其转换为 `pd.DataFrame`。
- en: We then create the column called `image_filepath` by prefixing the root directory
    path to the `image_id` and appending the extension `.jpg`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过将根目录路径前缀加到 `image_id` 上，并附加扩展名 `.jpg` 来创建名为 `image_filepath` 的列。
- en: 'We will only keep the data points where the `image_filepath` values are in
    the training images we stored in `train_filepaths`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只保留 `image_filepath` 值在我们存储在 `train_filepaths` 中的训练图像中的数据点：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now define a function called `preprocess_captions()` that processes the
    original caption:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义一个名为 `preprocess_captions()` 的函数，用来处理原始标题：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the above code, we:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们：
- en: Added two special tokens, `[START]` and `[END]`, to denote the start and the
    end of each caption respectively
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了两个特殊标记 `[START]` 和 `[END]`，分别表示每个标题的开始和结束
- en: Converted the captions to lowercase
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标题转换为小写
- en: Removed everything that is not a word, character, or space
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除所有非单词、字符或空格的内容
- en: 'We then call this function on the training dataset:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在训练数据集上调用这个函数：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then follow a similar process for both validation and test data:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对验证数据和测试数据执行类似的过程：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s check the data in `training_captions_df` (*Figure 11.4*):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 `training_captions_df` 中的数据（*图 11.4*）：
- en: '![](img/B14070_11_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_04.png)'
- en: 'Figure 11.4: Data contained in training_captions_df'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：training_captions_df 中的数据
- en: This data shows important information such as where the image is located in
    the file structure, the original caption, and the preprocessed caption.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据展示了重要信息，例如图像在文件结构中的位置、原始标题和预处理后的标题。
- en: 'Let’s also analyze some statistics about the images. We will take a small sample
    of the first 1,000 images from the training dataset and look at image sizes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也分析一些关于图像的统计信息。我们将从训练数据集中取出前 1,000 张图像的小样本，并查看图像的大小：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will produce *Figure 11.5*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生 *图 11.5*：
- en: '![](img/B14070_11_05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_05.png)'
- en: 'Figure 11.5: Statistics about the size of the images in the training dataset'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：训练数据集中图像大小的统计信息
- en: 'We can see that most images have a resolution of 640x640\. We will later need
    to resize images to 224x224 to match the model’s input requirements. We’ll also
    look at our vocabulary size:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到大多数图像的分辨率为640x640。稍后我们需要将图像调整为224x224，以匹配模型的输入要求。我们还可以查看词汇表大小：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This prints:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This tells us that 3,629 words occur at least 25 times in our train dataset.
    We use this as our vocabulary size.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，有3,629个词在训练数据集中至少出现了25次。我们将这个作为词汇表的大小。
- en: Tokenizing data
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词数据
- en: 'Since we are developing Transformer models, we need a robust tokenizer similar
    to the ones used by popular models like BERT. Hugging Face’s `tokenizers` library
    provides us with a range of tokenizers that are easy to use. Let’s understand
    how we can use one of these tokenizers for our purpose. You can import it using:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在开发Transformer模型，我们需要一个强大的分词器，类似于BERT等流行模型使用的分词器。Hugging Face的`tokenizers`库为我们提供了一系列易于使用的分词器。让我们了解如何使用这些分词器之一来满足我们的需求。你可以通过以下方式导入它：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, let’s define the `BertWordPieceTokenizer`. We will pass the following
    arguments when doing so:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义`BertWordPieceTokenizer`。我们在定义时将传递以下参数：
- en: '`unk_token` – Defines a token to be used for out-of-vocabulary words'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` – 定义一个用于表示词汇表外（OOV）词汇的标记'
- en: '`clean_text` – Whether to perform simple preprocessing steps to clean text'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_text` – 是否进行简单的预处理步骤以清理文本'
- en: '`lowercase` – Whether to lowercase the text'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lowercase` – 是否将文本转换为小写'
- en: 'These arguments can be seen in the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些参数：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the tokenizer defined, we can call the `train_from_iterator()` function
    to train the tokenizer on our dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好分词器后，我们可以调用`train_from_iterator()`函数来在我们的数据集上训练分词器：
- en: tokenizer.train_from_iterator(
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: tokenizer.train_from_iterator(
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `train_from_iterator()` function takes in several arguments:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_from_iterator()`函数接受多个参数：'
- en: '`iterator` – An iterable that produces a string (containing the caption) as
    one item.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterator` – 一个可迭代对象，每次产生一个字符串（包含标题）。'
- en: '`vocab_size` – Size of the vocabulary.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` – 词汇表的大小。'
- en: '`special_tokens` – Special tokens that will be used in our data. Specifically
    we use `[PAD]` (to denote padding), `[UNK]` (to denote OOV tokens), `[START]`
    (to denote the start), and `[END]` (to denote the end). These tokens will get
    assigned lower IDs starting from 0.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens` – 我们数据中将使用的特殊标记。具体来说，我们使用`[PAD]`（表示填充），`[UNK]`（表示OOV标记），`[START]`（表示开始）和`[END]`（表示结束）。这些标记将从0开始分配较低的ID。'
- en: 'Once the tokenizer is trained, we can use it to convert strings of text to
    sequences of tokens. Let’s convert a few example sentences to sequences of tokens
    using the trained tokenizer:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分词器训练完成，我们可以使用它将文本字符串转换为标记序列。让我们使用训练好的分词器将几个示例句子转换为标记序列：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will print:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can see how the tokenizer has learned its own vocabulary and is tokenizing
    string sentences. The words that contain `##` in front mean they must be combined
    with the previous token (without spaces) to get the final result. For example,
    the final string from the tokens `''image''`, `''``cap''` and `''##tion''` is
    `''image caption''`. Let’s see which IDs the special tokens we defined are mapped
    to:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到分词器如何学习自己的词汇，并且正在对字符串句子进行分词。前面带有`##`的词汇表示它们必须与前面的标记（无空格）结合，才能得到最终结果。例如，来自标记`'image'`，`'cap'`和`'##tion'`的最终字符串是`'image
    caption'`。让我们看看我们定义的特殊标记映射到哪些ID：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now let’s look at how we can define a TensorFlow data pipeline using the processed
    data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用处理过的数据定义一个TensorFlow数据管道。
- en: Defining a tf.data.Dataset
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个tf.data.Dataset
- en: 'Now let’s look at how we can create a `tf.data.Dataset` using the data. We
    will first write a few helper functions. Namely, we’ll define:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用数据创建`tf.data.Dataset`。我们将首先编写一些辅助函数。也就是说，我们将定义：
- en: '`parse_image()` to load and process an image from a `filepath`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_image()`用来加载和处理来自`filepath`的图像'
- en: '`generate_tokenizer()` to generate a tokenizer trained on the data passed to
    the function'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`generate_tokenizer()`来生成一个基于传入数据训练的分词器
- en: 'First let’s discuss the `parse_image()` function. It takes three arguments:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们讨论一下`parse_image()`函数。它需要三个参数：
- en: '`filepath` – Location of the image'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filepath` – 图像的位置'
- en: '`resize_height` – Height to resize the image to'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resize_height` – 调整图像高度的目标值'
- en: '`resize_width` – Width to resize the image to'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resize_width` – 调整图像宽度的目标值'
- en: 'The function is defined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数定义如下：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We are mostly relying on `tf.image` functions to load and process the image.
    This function specifically:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要依赖`tf.image`函数来加载和处理图像。这个函数具体来说：
- en: Reads the image from the `filepath`
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`filepath`读取图像
- en: Decodes the bytes in the JPEG image to a `uint8` tensor and converts to a `float32
    dtype` tensor.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码JPEG图像中的字节为`uint8`张量，并转换为`float32 dtype`张量。
- en: 'By the end of these steps, we’ll have an image whose pixel values are between
    0 and 1\. Next, we:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤结束后，我们将得到一个像素值介于0和1之间的图像。接下来，我们：
- en: Resize the image to a given height and width
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整为给定的高度和宽度
- en: Finally normalize the image so that the pixel values are between -1 and 1 (as
    required by the ViT model we’ll be using)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，将图像归一化，使得像素值介于-1和1之间（这符合我们将要使用的ViT模型的要求）
- en: 'With that we define the second helper function. This function encapsulates
    the functionality of the `BertWordPieceTokenizer` we have discussed previously:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们定义了第二个辅助函数。这个函数封装了我们之前讨论过的`BertWordPieceTokenizer`的功能：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With that we can define our main data function to generate the TensorFlow data
    pipeline:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以定义我们的主数据函数，以生成TensorFlow数据管道：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This function takes the following arguments:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受以下参数：
- en: '`image_captions_df` – A pandas DataFrame containing image file paths and processed
    captions'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_captions_df` – 一个包含图像文件路径和处理过的标题的pandas DataFrame'
- en: '`tokenizer` – An optional tokenizer that will be used to tokenize the captions'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` – 可选的分词器，用于对标题进行分词'
- en: '`n_vocab` – The vocabulary size'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_vocab` – 词汇表大小'
- en: '`pad_length` – The length to pad captions'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_length` – 填充标题的长度'
- en: '`batch_size` – Batch size to batch the data'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` – 批处理数据时的批量大小'
- en: '`training` – Whether the data pipeline should be run in training mode or not.
    In training mode, we shuffle data whereas otherwise, we do not'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` – 数据管道是否应该以训练模式运行。在训练模式下，我们会打乱数据，反之则不会'
- en: First this function generates a tokenizer if a new tokenizer has not been passed.
    Next we create a column called “`caption_token_ids`" in our DataFrame, which is
    created by calling the `encode_batch()` function of the tokenizer on the `preprocessed_caption`
    column. We then perform padding on the `caption_token_ids` column. We add the
    `[PAD]` token ID if a caption is shorter than `pad_length`, or truncate it if
    it’s longer. We then create a `tf.data.Dataset` using the `from_tensor_slices()`
    function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果没有传递新的分词器，该函数会生成一个分词器。接下来，我们在我们的DataFrame中创建一个名为“`caption_token_ids`”的列，这个列是通过调用分词器的`encode_batch()`函数对`preprocessed_caption`列进行编码而生成的。然后我们对`caption_token_ids`列进行填充。如果标题的长度小于`pad_length`，我们将添加`[PAD]`令牌ID；如果标题长度超过`pad_length`，则进行截断。然后我们使用`from_tensor_slices()`函数创建一个`tf.data.Dataset`。
- en: 'Each sample in this dataset will be a dictionary with the key `image_filepath`
    and `caption_token_ids` and values containing corresponding values. Once we do
    this, we have the ingredients to get the actual data. We will call the `tf.data.Dataset.map()`
    function to:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集中的每个样本将是一个字典，字典的键为`image_filepath`和`caption_token_ids`，值为相应的值。一旦我们完成这个步骤，就拥有了获取实际数据的基础。我们将调用`tf.data.Dataset.map()`函数来：
- en: Call `parse_image()` on each `image_filepath` to produce the actual image
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个`image_filepath`调用`parse_image()`以生成实际图像
- en: Return all caption token IDs, except the last, as inputs
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回所有标题令牌ID，除了最后一个，作为输入
- en: A range from 0 to the number of tokens, representing position of each input
    token ID (used to get positional embeddings for the Transformer)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围从0到令牌的数量，表示每个输入令牌ID的位置（用于获取Transformer的位置信息嵌入）
- en: Return all caption token IDs as the targets
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回所有的标题令牌ID作为目标
- en: 'Let’s understand what the inputs and outputs are going to look like for an
    example. Say you have the caption *a brown bear*. Here’s how the inputs and outputs
    going to look for our Transformer decoder (*Figure 11.6*):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解输入和输出的样子。假设你有一个标题 *a brown bear*。下面是我们Transformer解码器中输入和输出的样子（*图11.6*）：
- en: '![](img/B14070_11_06.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_06.png)'
- en: 'Figure 11.6: How inputs and targets are organized for the model'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：模型输入和目标的组织方式
- en: 'Finally, if in training mode, we shuffle the dataset using a `buffer_size`
    of 10 times the batch size. Then we batch the dataset using the `batch_size` provided
    when calling the function. Let’s call this function on our training dataset to
    see what we get:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果处于训练模式，我们使用`batch_size`的10倍作为`buffer_size`来打乱数据集。然后我们使用在调用函数时提供的`batch_size`来批处理数据集。让我们在我们的训练数据集上调用这个函数，看看得到的结果：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Which will output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here we can see the inputs and outputs organized into a nested tuple. It has
    the format `((image, input caption token IDs, position IDs), target caption token
    IDs)`. For example, we have produced a data pipeline with a batch size of 2, a
    pad length of 10, and a vocabulary size of 4,000\. We can see the image batch
    has the shape [2, 224, 224, 3], input caption token IDs and the position IDs have
    the shape [2, 11], and finally, the target caption token IDs are of shape [2,
    12]. It is important to note that we use an additional buffer for padding length
    to incorporate the `[START]` and `[END]` tags. Therefore, the resulting tensors
    use a caption length of 12 (i.e. 10+2). The most important thing to note here
    is the length of the input and target captions. Input captions have one item less
    than the target captions as shown by the lengths. This is because, the first item
    in our input captions would be the image feature vector. This brings the length
    of input tokens to be equal to the length of target tokens.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到输入和输出被组织成一个嵌套的元组。它的格式是`((image, input caption token IDs, position
    IDs), target caption token IDs)`。例如，我们已经生成了一个批量大小为2、填充长度为10、词汇表大小为4,000的数据管道。我们可以看到，图像批次的形状是[2,
    224, 224, 3]，输入标题的token ID和位置ID的形状是[2, 11]，最后，目标标题的token ID的形状是[2, 12]。需要注意的是，我们使用了一个额外的缓冲区来处理填充长度，以包含`[START]`和`[END]`标签。因此，最终得到的张量使用了标题长度为12（即10+2）。这里最重要的是注意输入和目标标题的长度。输入标题比目标标题少一个项目，正如长度所示。这是因为输入标题中的第一个项目是图像特征向量。这使得输入tokens的长度等于目标tokens的长度。
- en: With the data pipeline out of the way, we will discuss the mechanics of the
    model we’ll be using.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决了数据管道之后，我们将讨论我们将使用的模型的机制。
- en: The machine learning pipeline for image caption generation
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像标题生成的机器学习流程
- en: 'Here we will look at the image caption generation pipeline at a very high level
    and then discuss it piece by piece until we have the full model. The image caption
    generation framework consists of two main components:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从一个非常高层次的角度来看图像标题生成流程，然后逐步分析，直到我们拥有完整的模型。图像标题生成框架由两个主要部分组成：
- en: A pretrained Vision Transformer model to produce an image representation
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预训练的视觉Transformer模型，用于生成图像表示。
- en: A text-based decoder model that can decode the image representation to a series
    of token IDs. This uses a text tokenizer to convert tokens to token IDs and vice
    versa
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基于文本的解码器模型，可以将图像表示解码为一系列的token ID。该模型使用文本分词器将tokens转换为token ID，反之亦然。
- en: Though the Transformer models were initially used for text-based NLP problems,
    they have out-grown the domain of text data and have been used in other areas
    such as image data and audio data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Transformer模型最初是用于基于文本的NLP问题，但它们已经超越了文本数据领域，应用于图像数据和音频数据等其他领域。
- en: Here we will be using one Transformer model that can process image data and
    another that can process text data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一个可以处理图像数据的Transformer模型和一个可以处理文本数据的Transformer模型。
- en: Vision Transformer (ViT)
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉Transformer（ViT）
- en: First, let’s look at the Transformer generating the encoded vector representations
    of images. We will be using a pretrained **Vision Transformer** (**ViT**) to achieve
    this. This model has been trained on the ImageNet dataset we discussed above.
    Let’s understand the architecture of this model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下生成图像编码向量表示的Transformer模型。我们将使用一个预训练的**视觉Transformer**（**ViT**）来实现这一点。该模型已经在我们之前讨论过的ImageNet数据集上进行了训练。接下来，让我们了解一下该模型的架构。
- en: 'Originally, the ViT was proposed in the paper *An Image is Worth 16X16 Words:
    Transformers for Image Recognition at Scale* by Dosovitskiy et al ([https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)).
    This can be considered the first substantial step toward adapting Transformers
    for computer vision problems. This model is called the Vision Transformer model.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '最初，ViT是由Dosovitskiy等人在论文《*An Image is Worth 16X16 Words: Transformers for Image
    Recognition at Scale*》中提出的（[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)）。这可以被视为将Transformer应用于计算机视觉问题的第一个重要步骤。这个模型被称为视觉Transformer模型。'
- en: The idea is to decompose an image into small patches of 16x16 and consider each
    as a separate token. Each image path is flattened to a 1D vector and their position
    is encoded by a positional encoding mechanism similar to the original Transformer.
    But images are 2D structures; is it enough to have 1D positional information,
    and not 2D positional information? The authors argue that a 1D positional encoding
    was adequate and 2D positional encoding did not provide a significant boost. Once
    the image is broken into patches of 16x16 and flattened, each image can be presented
    as a sequence of tokens, just like a textual input sequence (*Figure 11.7*).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的思路是将图像分解为16x16的小块，并将每一块视为一个独立的标记。每个图像块被展平为一个1D向量，并通过类似于原始Transformer的位置信息编码机制对其位置进行编码。但图像是二维结构；仅使用1D位置信息而没有2D位置信息是否足够呢？作者认为，1D位置信息足够，而2D位置信息并未带来显著的提升。一旦图像被分解为16x16的块并展平，每张图像就可以像文本输入序列一样表示为一系列标记（*图11.7*）。
- en: Then the model is pretrained in a self-supervised fashion, using a vision dataset
    called JFT-300M ([https://paperswithcode.com/dataset/jft-300m](https://paperswithcode.com/dataset/jft-300m)).
    The paper proposes an elegant way to train the ViT in a semi-supervised fashion
    using image data. Similar to how NLP problems represent a unit of text as a token,
    a token is a patch of an image (i.e. a sequence of continuous values where values
    are normalized pixels). Then the ViT is pretrained to predict the mean 3-bit RGB
    color of a given image patch. Each channel (i.e. red, green, and blue) is represented
    with 3 bits (each bit having a value of 0 or 1), which gives 512 possibilities
    or classes. In other words, for a given image, patches (similar to how tokens
    are treated in NLP) are masked randomly (using the same approach as BERT), and
    the model is asked to predict the mean 3-bit RGB color of that image patch.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型以自监督的方式进行预训练，使用名为JFT-300M的视觉数据集（[https://paperswithcode.com/dataset/jft-300m](https://paperswithcode.com/dataset/jft-300m)）。该论文提出了一种优雅的方式，使用图像数据对ViT进行半监督训练。类似于NLP问题中将文本单元表示为标记的方式，图像的一个“标记”是图像的一块（即一系列连续的值，这些值是标准化的像素）。然后，ViT被预训练以预测给定图像块的平均3位RGB颜色。每个通道（即红色、绿色和蓝色）用3位表示（每个位的值为0或1），这提供了512种可能性或类别。换句话说，对于给定的图像，图像块（类似于NLP中的标记）会被随机遮蔽（采用与BERT相同的方法），然后模型被要求预测该图像块的平均3位RGB颜色。
- en: After pretraining, the model can be fine-tuned for a task-specific problem by
    fitting a classification or a regression head on top of the ViT, just like BERT.
    The ViT also has the `[CLS]` token at the beginning of the sequence, which will
    be used as the input representation for downstream vision models that are plugged
    on top of the ViT.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 经过预训练后，模型可以通过在ViT上添加分类头或回归头进行任务特定问题的微调，就像BERT一样。ViT在序列的开头也有`[CLS]`标记，它将作为下游视觉模型的输入表示，这些视觉模型被接入ViT之上。
- en: '*Figure 11.7* illustrates the mechanics of the ViT:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.7*展示了ViT的机制：'
- en: '![](img/B14070_11_07.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_07.png)'
- en: 'Figure 11.7: The Vision Transformer model'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：视觉Transformer模型
- en: The model we’ll be using here originated from the paper *How to train your ViT?
    Data, Augmentation, and Regularization in Vision Transformers* by Steiner et al
    ([https://arxiv.org/pdf/2106.10270.pdf](https://arxiv.org/pdf/2106.10270.pdf)).
    It proposes several variants of the ViT model. Specifically, we will use the ViT-S/16
    architecture. ViT-S is the second smallest ViT model with 12 layers and a hidden
    output dimensionality of 384; in total it has 22.2M parameters. The number 16
    here means that the model is trained on image patches of 16x16\. The model has
    been fine-tuned using the ImageNet dataset we discussed earlier. We will use the
    feature extractor part of the model for the purpose of image captioning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用的模型源自Steiner等人的论文*如何训练你的ViT？视觉Transformer中的数据、增强和正则化*（[https://arxiv.org/pdf/2106.10270.pdf](https://arxiv.org/pdf/2106.10270.pdf)）。该论文提出了几种ViT模型的变体。具体来说，我们将使用ViT-S/16架构。ViT-S是第二小的ViT模型，包含12层，隐藏输出维度为384；总共拥有22.2M个参数。这里的数字16意味着该模型是在16x16的图像块上进行训练的。该模型已通过我们之前讨论的ImageNet数据集进行了微调。我们将使用模型的特征提取部分进行图像字幕生成。
- en: Text-based decoder Transformer
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于文本的解码器Transformer
- en: 'The text-based decoder’s primary purpose is to predict the next token in the
    sequence given the previous tokens. This decoder is mostly similar to the BERT
    we used in the previous chapter. Let’s refresh our memory on what the Transformer
    model is composed of. The Transformer consists of several stacked layers. Each
    layer has:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的解码器的主要目的是根据前面的标记预测序列中的下一个标记。这个解码器与我们在上一章使用的BERT大致相同。让我们回顾一下Transformer模型的组成部分。Transformer由多个堆叠的层组成。每一层都有：
- en: A self-attention layer – Generates a hidden representation for each token position
    by taking in the input token and attending to the tokens at other positions in
    the sequence
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力层 – 通过接收输入标记并关注序列中其他位置的标记，为每个标记位置生成一个隐藏表示
- en: A fully connected subnetwork – Generates an element-wise non-linear hidden representation
    by propagating the self-attention layer’s output through two fully connected layers
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个全连接子网络 – 通过将自注意力层的输出通过两个全连接层传播，生成逐元素的非线性隐藏表示
- en: 'In addition to these, the network uses residual connections and layer normalization
    techniques to enhance performance. When speaking of inputs, the model uses two
    types of input embeddings to inform the model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，网络还使用残差连接和层归一化技术来提高性能。谈到输入时，模型使用两种类型的输入嵌入来告知模型：
- en: Token embeddings – Each token is represented with an embedding vector that is
    jointly trained with the model
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记嵌入 – 每个标记都用一个与模型共同训练的嵌入向量表示
- en: Position embeddings – Each token position is represented by an ID and a corresponding
    embedding for that position
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置嵌入 – 每个标记位置通过一个ID和该位置的相应嵌入来表示
- en: Compared to the BERT we used in the previous chapter, a key difference in our
    model is how we use the self-attention mechanism. When using BERT, the self-attention
    layer was able to pay attention in a bidirectional manner (i.e. pay attention
    to tokens on both sides of the current input). However, in the decoder-based model,
    it can only pay attention to the tokens to the left of the current token. In other
    words, the attention mechanism only has access to inputs seen up to the current
    input.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章使用的BERT相比，我们模型中的一个关键区别是自注意力机制的使用方式。使用BERT时，自注意力层能够以双向方式进行注意（即同时关注当前输入两侧的标记）。然而，在基于解码器的模型中，它只能关注当前标记左侧的标记。换句话说，注意力机制只能访问当前输入之前看到的输入。
- en: Putting everything together
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'Let’s now learn how to put the two models together. We will use the following
    procedure to train the model end to end:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来学习如何将这两个模型结合起来。我们将使用以下程序来训练端到端模型：
- en: We generate the image encoding via the ViT model. It generates a single representation
    of 384 items for an image.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过ViT模型生成图像编码。它为图像生成384个项目的单一表示。
- en: This representation, along with all the caption tokens except the last, goes
    into the decoder as the inputs.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个表示连同所有标题标记（除了最后一个）一起作为输入送入解码器。
- en: Given the current input token, the decoder predicts the next token. At the end
    of this process we will have the full image caption.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定当前输入标记，解码器预测下一个标记。在这个过程结束时，我们将得到完整的图像标题。
- en: Another alternative for connecting the ViT and the text decoder models is by
    providing direct access to the ViT’s full sequence of encoder outputs as a part
    of the attention mechanism of the decoder. In this work, not to overcomplicate
    our discussion, we only use a single output from the ViT model as an input to
    the decoder.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 将ViT和文本解码器模型连接的另一个替代方法是通过提供直接访问ViT完整序列的编码器输出作为解码器注意力机制的一部分。在这项工作中，为了不让讨论过于复杂，我们仅使用ViT模型的一个输出作为解码器的输入。
- en: Implementing the model with TensorFlow
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现模型
- en: 'We will now implement the model we just studied. First let’s import a few things:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将实现刚才学习的模型。首先让我们导入一些内容：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Implementing the ViT model
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现ViT模型
- en: Next, we are going to download the pretrained ViT model from TensorFlow Hub.
    We will be using a model submitted by Sayak Paul. The model is available at [https://tfhub.dev/sayakpaul/vit_s16_fe/1](https://tfhub.dev/sayakpaul/vit_s16_fe/1).
    You can see other Vision Transformer models available at [https://tfhub.dev/sayakpaul/collections/vision_transformer/1](https://tfhub.dev/sayakpaul/collections/vision_transformer/1).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从TensorFlow Hub下载预训练的ViT模型。我们将使用Sayak Paul提交的模型。该模型可在[https://tfhub.dev/sayakpaul/vit_s16_fe/1](https://tfhub.dev/sayakpaul/vit_s16_fe/1)找到。你还可以查看其他Vision
    Transformer模型，网址是[https://tfhub.dev/sayakpaul/collections/vision_transformer/1](https://tfhub.dev/sayakpaul/collections/vision_transformer/1)。
- en: '[PRE25]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then define an input layer to input images and pass that to the `image_encoder`
    to get the final feature vector for that image:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一个输入层来输入图像，并将其传递给`image_encoder`以获取该图像的最终特征向量：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can look at the size of the final image representation by running:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下代码来查看最终图像表示的大小：
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'which will output:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码将输出：
- en: '[PRE28]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next we will look at the details of how to implement the text-based Transformer
    model, which will take in the image representation to generate the image caption.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将详细了解如何实现基于文本的Transformer模型，它将输入图像表示以生成图像描述。
- en: Implementing the text-based decoder
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基于文本的解码器
- en: Here we will implement a Transformer decoder model from the ground up. This
    is different from how we used Transformer models before, where we downloaded a
    pretrained model and used them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从头开始实现一个Transformer解码器模型。这与我们之前使用Transformer模型的方式不同，当时我们下载了一个预训练模型并使用它。
- en: 'Before we implement the model itself, we are going to implement two custom
    Keras layers: one for the self-attention mechanism and the other one to capture
    the functionality of a single layer in the Transformer model. Let’s start with
    the self-attention layer.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现模型之前，我们将实现两个自定义的Keras层：一个用于自注意力机制，另一个用于捕获Transformer模型中单层的功能。我们从自注意力层开始。
- en: Defining the self-attention layer
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义自注意力层
- en: 'Here we define the self-attention layer using the Keras subclassing API:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Keras子类化API定义自注意力层：
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here we have to populate the logic for three functions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须填充三个函数的逻辑：
- en: '`__init__()` and `__build__()` – Define various hyperparameters and layer initialization
    specific logic'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__()` 和 `__build__()` – 定义特定于层初始化的各种超参数和逻辑'
- en: '`call()` – Computations that need to happen when the layer is called'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call()` – 在调用该层时需要进行的计算'
- en: You can see that we define the dimensionality of the attention output, `d`,
    as an argument to the `__init__()` method. Next in the `__build__()` method, we
    define three weight matrices, `Wq`, `Wk`, and `Wv`. If you remember our discussion
    from the previous chapter, these represent the weights of the query, key, and
    value respectively.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们将注意力输出的维度`d`作为参数传递给`__init__()`方法。接着，在`__build__()`方法中，我们定义了三个权重矩阵，`Wq`、`Wk`和`Wv`。如果你记得我们在上一章的讨论，这些分别代表查询、键和值的权重。
- en: 'Finally, in the `call` method we have the logic. It takes four inputs: query,
    key, value inputs, and an optional mask for values. We then compute the latent
    `q`, `k`, and `v` by multiplying with the corresponding weight matrices `Wq`,
    `Wk`, and `Wv`. To compute attention, we will be using the out-of-the-box layer
    `tf.keras.layers.Attention`. We used a similar layer to compute the Bahdanau attention
    mechanism in *Chapter 9**, Sequence-to-Sequence Learning – Neural Machine Translation*.
    The `tf.keras.layers.Attention()` layer has several arguments. One that we care
    about here is setting `causal=True`.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`call`方法中我们有逻辑。它接受四个输入：查询、键、值输入和一个可选的值掩码。然后，我们通过与相应的权重矩阵`Wq`、`Wk`和`Wv`相乘来计算潜在的`q`、`k`和`v`。为了计算注意力，我们将使用现成的层`tf.keras.layers.Attention`。我们在*第9章*《序列到序列学习——神经机器翻译*》中使用了类似的层来计算Bahdanau注意力机制。`tf.keras.layers.Attention()`层有几个参数，其中一个我们关心的是设置`causal=True`。
- en: 'By doing this, we are instructing the layer to mask the tokens to the right
    of the current token. This essentially prevents the decoder from leaking information
    about future tokens. Next, the layer takes in the following arguments during the
    call:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是指示该层将当前令牌右侧的令牌进行屏蔽。这本质上防止了解码器泄漏关于未来令牌的信息。接下来，层在调用过程中会接受以下参数：
- en: '`inputs` – A list of inputs containing the query, value, and key in that order'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs` – 包含查询、值和键的输入列表，按此顺序排列'
- en: '`mask` – A list of two items containing the masks for the query and value'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask` – 包含查询和值的掩码的两个项的列表'
- en: Finally it returns the output of the attention layer `h`. Next, we will implement
    the computations of a Transformer layer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后返回注意力层的输出 `h`。接下来，我们将实现Transformer层的计算。
- en: Defining the Transformer layer
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义Transformer层
- en: 'With the self-attention layer, let’s capture the computations of a single Transformer
    layer in the following class. It uses self-attention, fully connected layers,
    and other optimization techniques to compute the output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自注意力层，我们可以捕捉单个Transformer层中的计算过程。它使用自注意力、全连接层和其他优化技术来计算输出：
- en: '[PRE30]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `TransformerDecoderLayer` performs the following steps:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransformerDecoderLayer` 执行以下步骤：'
- en: Using the given input, the layer computes a multi-head attention output. A multi-head
    attention output is generated by computing attention outputs with several smaller
    heads and concatenating those outputs to a single output (`h1`).
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定的输入，该层计算多头注意力输出。多头注意力输出通过计算几个较小头的注意力输出并将这些输出连接到单个输出 (`h1`)。
- en: Next we add the original input `x` to `h1` to form a residual connection, (`h1_add`).
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们将原始输入 `x` 加到 `h1` 中形成残差连接 (`h1_add`)。
- en: This is followed by a layer normalization step that normalizes (`h1_norm`).
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着进行层归一化步骤来归一化 (`h1_norm`)。
- en: '`h1_norm` goes through a fully connected layer to produce `h2_1`.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`h1_norm` 经过全连接层生成 `h2_1`。'
- en: '`h2_1` goes through another fully connected layer to produce `h2_2`.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`h2_1` 经过另一个全连接层生成 `h2_2`。'
- en: Then we create another residual connection by adding `h1` and `h2_2` to produce
    `h2_add`.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过将 `h1` 和 `h2_2` 相加创建另一个残差连接来产生 `h2_add`。
- en: Finally we perform layer normalization to produce `h2_norm`, which is the final
    output of this custom layer.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行层归一化以生成 `h2_norm`，这是此自定义层的最终输出。
- en: Defining the full decoder
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义完整的解码器
- en: 'With all the utility layers implemented, we can implement the text decoder.
    We will define two input layers. The first takes in a sequence of tokens as the
    input and the second takes in a sequence of positions (0-index based) to denote
    the position of each token. You can see that both layers are defined such that
    they can take in an arbitrary length sequence as an input. This will serve an
    important purpose as we will see later during inference:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有实用程序层实现后，我们可以实现文本解码器。我们将定义两个输入层。第一个接受一个令牌序列作为输入，第二个接受一个序列位置（基于0索引），以表示每个令牌的位置。您可以看到，两个层都被定义为能够接受任意长度的序列作为输入。这将在推理过程中起重要作用：。
- en: '[PRE31]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next we define the embeddings. Our embedding vectors will have a length of
    384 to match the ViT model’s output dimensionality. We defined two embedding layers:
    the token embedding layer and the positional embedding layer:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义嵌入。我们的嵌入向量长度为384，以匹配ViT模型的输出维度。我们定义了两个嵌入层：token嵌入层和位置嵌入层：
- en: '[PRE32]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The token embedding layer works just as we have seen several times. It produces
    an embedding vector for each token in the sequence. We mask inputs with `ID 0`
    as they represent the padded tokens. Next let’s understand how we can implement
    the positional embeddings:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌嵌入层的工作方式与我们多次见过的一样。它为序列中的每个令牌生成一个嵌入向量。我们用 `ID 0` 掩盖输入，因为它们表示填充的令牌。接下来让我们了解如何实现位置嵌入：
- en: '[PRE33]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We have already discussed how positional embeddings are calculated. The original
    Transformer paper uses the following equations to generate positional embeddings:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何计算位置嵌入。原始Transformer论文使用以下方程式生成位置嵌入：
- en: '![](img/B14070_11_001.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_001.png)'
- en: '![](img/B14070_11_002.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_002.png)'
- en: 'Here *pos* denotes the position in the sequence and *i* denotes the *i*^(th)
    feature dimension (`0< i<d_model`). Even-numbered features use a sine function,
    where odd-numbered features use a cosine function. Computing this as a layer requires
    some effort. Let’s slowly break down the logic. First we compute the following
    two tensors (let’s refer to them with `x` and `y` for ease):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *pos* 表示序列中的位置，*i* 表示第 *i*^(th) 个特征维度 (`0< i<d_model`)。偶数特征使用正弦函数，奇数特征使用余弦函数。计算这一层需要一些工作。让我们慢慢分解这个逻辑。首先我们计算以下两个张量（为了方便我们用
    `x` 和 `y` 表示）：
- en: '[PRE34]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We use the `tf.where(cond, x, y)` function to select values element-wise from
    `x` and `y` using a Boolean matrix `cond` of the same size. For a given position,
    if `cond` is `True`, select `x`, and if `cond` is `False`, select `y`. Here we
    use the condition as `pos%2 == 0`, which provides `True` for even positions and
    `False` for odd positions.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`tf.where(cond, x, y)`函数，根据与`cond`的布尔矩阵的大小相同，按元素从`x`和`y`中选择值。对于某一位置，如果`cond`为`True`，选择`x`；如果`cond`为`False`，选择`y`。这里我们使用条件`pos%2
    == 0`，这会对偶数位置返回`True`，对奇数位置返回`False`。
- en: In order to make sure we produce tensors with correct shapes, we utilize the
    broadcasting capabilities of TensorFlow.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们生成形状正确的张量，我们利用了TensorFlow的广播能力。
- en: 'Let’s understand a little bit how broadcasting has helped. Take the computation:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微了解一下广播是如何帮助的。来看一下计算：
- en: '[PRE35]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here we need a `[batch size, time steps, d_model]`-sized output. `tf.expand_dims(x,
    axis=-1)` produces a `[batch size, time steps, 1]`-sized output. `10000**(2*tf.reshape(tf.range(d_model,
    dtype='float32'),[1,1, -1])/d_model)` produces a `[1, 1, d_model]`-sized output.
    Dividing the first output by the second provides us with a tensor of size `[batch
    size, time steps, d_model]`. This is because the broadcasting capability of TensorFlow
    allows it to perform operations between arbitrary-sized dimensions and dimensions
    of size 1\. You can imagine TensorFlow copying the dimension of size 1, n many
    times to perform an operation with an n-sized dimension. But in reality it does
    this more efficiently.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要一个`[batch size, time steps, d_model]`大小的输出。`tf.expand_dims(x, axis=-1)`会生成一个`[batch
    size, time steps, 1]`大小的输出。`10000**(2*tf.reshape(tf.range(d_model, dtype='float32'),[1,1,
    -1])/d_model)`会生成一个`[1, 1, d_model]`大小的输出。将第一个输出除以第二个输出，得到一个大小为`[batch size, time
    steps, d_model]`的张量。这是因为TensorFlow的广播能力允许它在任意大小的维度和大小为1的维度之间执行操作。你可以想象TensorFlow将大小为1的维度复制n次，以执行与n大小维度的操作。但实际上，它这样做得更高效。
- en: 'Once the token and positional embeddings are computed. We add them element-wise
    to get the final embeddings:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦令牌和位置嵌入被计算出来，我们将它们按元素相加，得到最终的嵌入：
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If you remember, the first input to the decoder is the image feature vector
    followed by caption tokens. Therefore, we need to concatenate `image_features`
    (produced by the ViT) with the `embed_out` to get the full sequence of inputs:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，解码器的第一个输入是图像特征向量，后面跟着字幕令牌。因此，我们需要将`image_features`（由ViT产生）与`embed_out`拼接起来，得到完整的输入序列：
- en: '[PRE37]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then we define four Transformer decoder layers and compute the hidden output
    of those layers:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义四个Transformer解码器层，并计算这些层的隐藏输出：
- en: '[PRE38]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We use a `Dense` layer having `n_vocab` output nodes and a *softmax* activation
    to compute the final output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个`Dense`层，具有`n_vocab`个输出节点，并采用*softmax*激活函数来计算最终输出：
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, we define the full model. It takes in:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义完整的模型。它接受以下输入：
- en: '`image_input` – A batch of images of size 224x224x3'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_input` – 一批224x224x3大小的图像'
- en: '`caption_input` – The token IDs of the caption (except the last token)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caption_input` – 字幕的令牌ID（不包括最后一个令牌）'
- en: '`position_input` – A batch of position IDs representing each token position'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_input` – 表示每个令牌位置的一批位置ID'
- en: 'And gives `final_out` as the output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 并将`final_out`作为输出：
- en: '[PRE40]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we have defined the full model (*Figure 11.8*):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了完整的模型（*图11.8*）：
- en: '![](img/B14070_11_08.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_08.png)'
- en: 'Figure 11.8: Code references overlaid on the illustration of the full model'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：代码参考叠加在完整模型的插图上
- en: Training the model
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now that the data pipeline and the model are defined, training it is quite
    easy. First let’s define a few parameters:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据管道和模型已定义，训练它就非常容易了。首先定义一些参数：
- en: '[PRE41]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We use a vocabulary size of 4,000 and a batch size of 96\. To speed up the
    training we’ll only use 60% of training data and 20% of validation data. However,
    you could increase these to get better results. Then we get the tokenizer trained
    on the full training dataset:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用4,000的词汇量和96的批量大小。为了加速训练，我们只使用60%的训练数据和20%的验证数据。然而，你可以增加这些数据以获得更好的结果。然后我们得到在完整训练数据集上训练的分词器：
- en: '[PRE42]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next we define the BLEU metric. This is the same BLEU computation from *Chapter
    9*, *Sequence-to-Sequence Learning – Neural Machine Translation*, with some minor
    differences. Therefore, we will not repeat the discussion here.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义BLEU指标。这与*第9章*《序列到序列学习——神经机器翻译*中的BLEU计算相同，仅有一些小的差异。因此，我们在这里不再重复讨论。
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Sample the smaller set of validation data outside the training loop to keep
    the set constant:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环外采样较小的验证数据集，以保持数据集的恒定：
- en: '[PRE44]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next we train the model for 5 epochs:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练模型5个周期：
- en: '[PRE45]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In each iteration, we generate a `train_dataset` and a `valid_dataset`. Note
    that the training set is sampled randomly in each epoch, resulting in different
    data points, while the validation set is constant. Also note that we are passing
    the previously generated tokenizer as an argument to the data pipeline function.
    We call the `full_model.fit()` function with the train dataset to train it for
    a single epoch within the loop. Finally we iterate through the batches of the
    validation dataset and compute loss, accuracy, and BLEU values for each batch.
    Then we print out the mean value of those batch metrics. The output looks like
    below:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们会生成`train_dataset`和`valid_dataset`。注意，训练集在每个周期内会随机采样，导致不同的数据点，而验证集是固定的。还要注意，我们将先前生成的tokenizer作为参数传递给数据管道函数。我们在循环中使用`full_model.fit()`函数，并用训练数据集对其进行单次训练。最后，我们遍历验证数据集的批次，计算每个批次的损失、准确率和BLEU值。然后，我们输出这些批次指标的平均值。输出结果如下所示：
- en: '[PRE46]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s go through the results. We can see that the training loss and validation
    losses have more or less gone down consistently. We have a training and validation
    accuracy of ~80%. Finally, the `valid_bleu` score is around 0.10\. You can see
    the state of the art for a few models here: [https://paperswithcode.com/sota/image-captioning-on-coco](https://paperswithcode.com/sota/image-captioning-on-coco).
    You can see that a BLEU-4 score of 39 has been reached by the UNIMO model. It
    is important to note that, in reality, our BLEU score is higher than what’s reported
    here. This is because each image has multiple captions. And when computing the
    BLEU score with multiple references, you compute BLEU for each and take the max.
    We have only considered one caption per image when computing the BLEU score. Additionally,
    our model was far less complicated and trained on a small fraction of the data
    available. If you would like to increase model performance, you can expose the
    full training set, and experiment with larger ViT models and data augmentation
    techniques to improve performance.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果。我们可以看到，训练损失和验证损失大致持续下降。我们的训练和验证准确率大约为80%。最后，`valid_bleu`得分约为0.10。你可以在这里看到一些模型的最新技术：[https://paperswithcode.com/sota/image-captioning-on-coco](https://paperswithcode.com/sota/image-captioning-on-coco)。可以看到，UNIMO模型达到了39的BLEU-4分数。值得注意的是，实际上我们的BLEU得分比这里报告的要高。这是因为每张图片有多个描述。在计算多个参考的BLEU得分时，你需要对每个描述计算BLEU，并取最大值。而我们在计算BLEU得分时只考虑了每张图片的一个描述。此外，我们的模型要简单得多，且只在一小部分可用数据上进行了训练。如果你希望提高模型性能，可以尝试使用完整的训练集，并实验更大的ViT模型和数据增强技术来提高表现。
- en: Next let’s discuss some of the different metrics used to measure the quality
    of sequences in the context of image captioning.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们讨论在图像描述的上下文中，衡量序列质量的不同指标。
- en: Evaluating the results quantitatively
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定量评估结果
- en: 'There are many different techniques for evaluating the quality and the relevancy
    of the captions generated. We will briefly discuss several such metrics we can
    use to evaluate the captions. We will discuss four metrics: BLEU, ROGUE, METEOR,
    and CIDEr.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 评估生成的描述质量和相关性有许多不同的技术。我们将简要讨论几种可用于评估描述的指标。我们将讨论四个指标：BLEU、ROUGE、METEOR和CIDEr。
- en: All these measures share a key objective, to measure the adequacy (the meaning
    of the generated text) and fluency (the grammatical correctness of text) of the
    generated text. To calculate all these measures, we will use a candidate sentence
    and a reference sentence, where a candidate sentence is the sentence/phrase predicted
    by our algorithm and the reference sentence is the true sentence/phrase we want
    to compare with.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些指标共享一个关键目标，即衡量生成文本的适应性（生成文本的意义）和流畅性（文本的语法正确性）。为了计算这些指标，我们将使用候选句子和参考句子，其中候选句子是我们算法预测的句子/短语，而参考句子是我们要与之比较的真实句子/短语。
- en: BLEU
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLEU
- en: '**Bilingual Evaluation Understudy** (**BLEU**) was proposed by Papineni and
    others in *BLEU: A Method for Automatic Evaluation of Machine Translation*, *Proceedings
    of the 40*^(th) *Annual Meeting of the Association for Computational Linguistics
    (ACL)*, *Philadelphia*, *July (2002): 311-318*. It measures the n-gram similarity
    between reference and candidate phrases, in a position-independent manner. This
    means that a given n-gram from the candidate is present anywhere in the reference
    sentence and is considered to be a match. BLEU calculates the n-gram similarity
    in terms of precision. BLEU comes in several variations (BLEU-1, BLEU-2, BLEU-3,
    and so on), denoting the value of *n* in the n-gram.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**双语评估替代法** (**BLEU**) 是由 Papineni 等人在 *BLEU: A Method for Automatic Evaluation
    of Machine Translation* 中提出的，*《第40届计算语言学协会年会论文集（ACL）》*，*费城*，*2002年7月：311-318*。它通过一种与位置无关的方式来度量参考句子和候选句子之间的
    n-gram 相似度。这意味着候选句子中的某个 n-gram 出现在参考句子的任何位置都被认为是匹配的。BLEU 计算 n-gram 相似度时使用精确度。BLEU
    有多个变种（BLEU-1、BLEU-2、BLEU-3 等），表示 n-gram 中 *n* 的值。'
- en: '![](img/B14070_11_003.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_003.png)'
- en: 'Here, *Count(n-gram)* is the number of total occurrences of a given n-gram
    in the candidate sentence. *Count*[clip] *(n-gram)* is a measure that calculates
    *Count(n-gram)* for a given n-gram and clips that value by a maximum value. The
    maximum value for an n-gram is calculated as the number of occurrences of that
    n-gram in the reference sentence. For example, consider these two sentences:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Count(n-gram)* 是候选句子中给定 n-gram 的总出现次数。*Count*[clip] *(n-gram)* 是一种度量方法，用于计算给定
    n-gram 的 *Count(n-gram)* 值，并根据最大值进行裁剪。n-gram 的最大值是通过该 n-gram 在参考句子中的出现次数来计算的。例如，考虑以下两句话：
- en: 'Candidate: **the** the the the the the the'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选短语：**the** the the the the the the
- en: 'Reference: **the** cat sat on **the** mat'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献：**the** cat sat on **the** mat
- en: '*Count(“the”) = 7*'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Count(“the”) = 7*'
- en: '*Count* [clip] *(“the”)=2*'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*Count* [clip] *(“the”)=2*'
- en: 'Note that the entity, ![](img/B14070_11_004.png), is a form of precision. In
    fact, it is called the modified n-gram precision. When multiple references are
    present, the BLEU is considered to be the maximum:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，实体 ![](img/B14070_11_004.png) 是精确度的一种表现形式。事实上，它被称为修正后的 n-gram 精确度。当存在多个参考文献时，BLEU
    被认为是最大值：
- en: '![](img/B14070_11_005.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_005.png)'
- en: However, the modified n-gram precision tends to be higher for smaller candidate
    phrases because this entity is divided by the number of n-grams in the candidate
    phrase. This means that this measure will incline the model to produce shorter
    phrases. To avoid this, a penalty term, *BP*, is added to the preceding term that
    penalizes short candidate phrases as well. BLEU possesses several limitations
    such as BLEU ignores synonyms when calculating the score and does not consider
    recall, which is also an important metric to measure accuracy. Furthermore, BLEU
    appears to be a poor choice for certain languages. However, this is a simple metric
    that has been found to correlate well with human judgment as well in most situations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，修正后的 n-gram 精确度对于较小的候选短语倾向于较高，因为这个实体是由候选短语中的 n-gram 数量来划分的。这意味着这种度量会使模型倾向于生成较短的短语。为了避免这种情况，增加了一个惩罚项
    *BP*，它对短候选短语也进行惩罚。BLEU 存在一些局限性，比如在计算分数时忽略了同义词，且没有考虑召回率，而召回率也是衡量准确性的一个重要指标。此外，BLEU
    对某些语言来说似乎不是一个理想选择。然而，这是一个简单的度量方法，通常在大多数情况下与人工评判具有较好的相关性。
- en: ROUGE
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROUGE
- en: '**Recall-Oriented Understudy for Gisting Evaluations** (**ROUGE**), proposed
    by Chin-Yew Lin in *ROUGE: A Package for Automatic Evaluation of Summaries*, *Proceedings
    of the Workshop on Text Summarization Branches Out (2004)*, can be identified
    as a variant of BLEU, and uses recall as the basic performance metric. The ROUGE
    metric looks like the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '**面向召回的摘要评估替代法** (**ROUGE**)，由 Chin-Yew Lin 在 *ROUGE: A Package for Automatic
    Evaluation of Summaries* 中提出，*《文本摘要分支扩展研讨会论文集（2004）》*，可以被视为 BLEU 的一种变体，且使用召回率作为基本的性能评估标准。ROUGE
    度量公式如下：'
- en: '![](img/B14070_11_006.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_006.png)'
- en: 'Here, ![](img/B14070_11_007.png) is the number of n-grams from candidates that
    were present in the reference, and ![](img/B14070_11_008.png) is the total n-grams
    present in the reference. If there exist multiple references, *ROUGE-N* is calculated
    as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B14070_11_007.png) 是候选词组中出现在参考文献中的 n-gram 数量，而 ![](img/B14070_11_008.png)
    是参考文献中出现的总 n-gram 数量。如果存在多个参考文献，*ROUGE-N* 的计算公式如下：
- en: '![](img/B14070_11_009.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_009.png)'
- en: Here, *ref*[i] is a single reference from the pool of available references.
    There are numerous variants of the ROUGE measure that introduce various improvements
    to the standard ROUGE metric. ROUGE-L computes the score based on the longest
    common subsequence found between the candidate and reference sentence pairs. Note
    that the longest common subsequence does not need to be continuous in this case.
    Next, ROUGE-W calculates the score based on the longest common subsequence, which
    is penalized by the amount of fragmentation present within the subsequence. ROUGE
    also suffers from limitations such as not considering precision in the calculations
    of the score.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*ref*[i]是来自可用参考集合中的一个单一参考。ROUGE度量有多个变种，针对标准ROUGE度量进行改进。ROUGE-L通过计算候选句子与参考句子对之间找到的最长公共子序列来得分。需要注意的是，在这种情况下，最长公共子序列不需要是连续的。接下来，ROUGE-W根据最长公共子序列进行计算，并根据子序列中的碎片化程度进行惩罚。ROUGE也存在一些局限性，比如在得分计算中没有考虑精度。
- en: METEOR
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: METEOR
- en: '**Metric for Evaluation of Translation with Explicit Ordering** (**METEOR**),
    proposed by Michael Denkowski and Alon Lavie in *Meteor Universal: Language Specific
    Translation Evaluation for Any Target Language*, *Proceedings of the Ninth Workshop
    on Statistical Machine Translation (2014): 376-380*, is a more advanced evaluation
    metric that performs alignments for a candidate and a reference sentence. METEOR
    is different from BLEU and ROUGE in the sense that METEOR takes the position of
    words into account. When computing similarities between a candidate sentence and
    a reference sentence, the following cases are considered as matches:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**翻译评估度量标准（Explicit Ordering Translation Evaluation Metric）**（**METEOR**），由Michael
    Denkowski和Alon Lavie提出，见于*Meteor Universal: Language Specific Translation Evaluation
    for Any Target Language*，*第九届统计机器翻译研讨会论文集（2014）：376-380*，是一种更先进的评估度量标准，它对候选句子和参考句子进行对齐。METEOR与BLEU和ROUGE的不同之处在于，METEOR考虑了单词的顺序。在计算候选句子与参考句子之间的相似性时，以下情况被视为匹配：'
- en: '**Exact**: The word from the candidate exactly matches the word from the reference
    sentence'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全匹配**：候选句子中的单词与参考句子中的单词完全匹配'
- en: '**Stem**: A stemmed word (for example, *walk* of the word *walked*) matches
    the word from the reference sentence'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干**：一个词干化后的词（例如，*walk*是词*walked*的词干）与参考句子中的单词匹配'
- en: '**Synonym**: The word from a candidate sentence is a synonym for the word from
    the reference sentence'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词**：候选句子中的单词是参考句子中单词的同义词'
- en: 'To calculate the METEOR score, the matches between a reference sentence and
    a candidate sentence can be shown, as in *Figure 11.10*, with the help of a table.
    Then, precision (*P*) and recall (*R*) values are calculated based on the number
    of matches, present in the candidate and reference sentences. Finally, the harmonic
    mean of *P* and *R* is used to compute the METEOR score:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算METEOR得分，可以通过表格展示参考句子与候选句子之间的匹配情况，如*图11.10*所示。然后，基于候选句子和参考句子中匹配项的数量，计算精度（*P*）和召回率（*R*）值。最后，使用*P*和*R*的调和均值来计算METEOR得分：
- en: '![](img/B14070_11_010.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_010.png)'
- en: 'Here, ![](img/B14070_11_015.png), ![](img/B14070_11_016.png), and ![](img/B14070_11_017.png)are
    tunable parameters, and *frag* penalizes fragmented matches, in order to prefer
    candidate sentences that have fewer gaps in matches as well as those that closely
    follow the order of words of the reference sentence. The *frag* is calculated
    by looking at the number of crosses in the final unigram mapping (*Figure 11.9*):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_11_015.png)、![](img/B14070_11_016.png)和![](img/B14070_11_017.png)是可调参数，*frag*会惩罚碎片化的匹配，以偏好那些匹配中间隙较少且单词顺序与参考句子接近的候选句子。*frag*是通过观察最终单一词映射中的交叉数来计算的（*图11.9*）：
- en: '![METEOR](img/B14070_11_09.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![METEOR](img/B14070_11_09.png)'
- en: 'Figure 11.9: Different possible alignments for two strings'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：两串字符串的不同对齐方式
- en: For example, we can see that the left side has 7 crosses, whereas the right
    side has 10 crosses, which means the right-side alignment will be more penalized
    than the left side.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到左侧有7个交叉，而右侧有10个交叉，这意味着右侧的对齐会比左侧更受惩罚。
- en: '![METEOR](img/B14070_11_10.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![METEOR](img/B14070_11_10.png)'
- en: 'Figure 11.10: The METEOR word matching table'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：METEOR单词匹配表
- en: You can see that we denoted matches between the candidate sentence and the reference
    sentence in circles and ovals. For example, we denote exact matches with a solid
    black circle, synonyms with a dashed hollow circle, and stemmed matches with dotted
    circles.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们用圆圈和椭圆表示候选句子与参考句子之间的匹配。例如，我们用实心黑色圆圈表示完全匹配，用虚线空心圆圈表示同义词匹配，用点状圆圈表示词干匹配。
- en: METEOR is computationally more complex, but has often been found to correlate
    with human judgment more than BLEU, suggesting that METEOR is a better evaluation
    metric than BLEU.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: METEOR在计算上更复杂，但通常被发现与人工评判的相关性高于BLEU，表明METEOR是比BLEU更好的评估指标。
- en: CIDEr
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIDEr
- en: '**Consensus-based Image Description Evaluation** (**CIDEr**), proposed by Ramakrishna
    Vedantam and others in *CIDEr: Consensus-based Image Description Evaluation*,
    *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, *2015*, is
    another measure that evaluates the consensus of a candidate sentence to a given
    set of reference statements. CIDEr is defined to measure the grammaticality, saliency,
    and accuracy (that is, precision and recall) of a candidate sentence.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于共识的图像描述评估**（**CIDEr**），由Ramakrishna Vedantam等人在《*CIDEr: 基于共识的图像描述评估*》，*IEEE计算机视觉与模式识别会议（CVPR）*，*2015*中提出，是另一种评估候选句子与给定参考句子集合共识度的衡量标准。CIDEr旨在衡量候选句子的语法正确性、显著性和准确性（即精度和召回率）。'
- en: 'First, CIDEr weighs each n-gram found in both the candidate and reference sentences
    by means of TF-IDF, so that more common n-grams (for example, the words *a* and
    *the*) will have a smaller weight, whereas rare words will have a higher weight.
    Finally, CIDEr is calculated as the cosine similarity between the vectors formed
    by TF-IDF-weighted n-grams found in the candidate sentence and the reference sentence:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，CIDEr通过TF-IDF加权候选句子和参考句子中出现的每个n-gram，因此更常见的n-gram（例如，单词*a*和*the*）的权重较小，而稀有单词的权重较大。最后，CIDEr通过计算候选句子和参考句子中TF-IDF加权n-gram向量之间的余弦相似度来得出：
- en: '![](img/B14070_11_011.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_011.png)'
- en: Here, *cand* is the candidate sentence, *ref* is the set of reference sentences,
    *ref*[j] is the *j*^(th) sentence of *ref*, and *m* is the number of reference
    sentences for a given candidate. Most importantly, ![](img/B14070_11_012.png)
    is the TF-IDF values calculated for all the n-grams in the candidate sentence
    and formed as a vector. ![](img/B14070_11_013.png) is the same vector for the
    reference sentence, *ref*[i]. ![](img/B14070_11_014.png) denotes the magnitude
    of the vector.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*cand*是候选句子，*ref*是参考句子的集合，*ref*[j]是*ref*的第*j*^(个)句子，*m*是给定候选句子的参考句子数量。最重要的是，![](img/B14070_11_012.png)是为候选句子中的所有n-gram计算的TF-IDF值，并将其作为向量。![](img/B14070_11_013.png)是参考句子*ref*[i]的相同向量。![](img/B14070_11_014.png)表示该向量的大小。
- en: Overall, it should be noted that there is no clear-cut winner that is able to
    perform well across all the different tasks that are found in NLP. These metrics
    are significantly task-dependent and should be carefully chosen depending on the
    task. Here we’ll be using the BLEU score for our model.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，应该注意的是，没有一个明确的赢家能够在自然语言处理中的所有任务上表现出色。这些指标在很大程度上依赖于任务，并应根据具体任务谨慎选择。在这里，我们将使用BLEU分数来评估我们的模型。
- en: Evaluating the model
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: With the model trained, let’s test the model on our unseen test dataset. Testing
    logic is almost identical to the validation logic we discussed earlier during
    model training. Therefore we will not repeat our discussion here.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好模型后，让我们在未见过的测试数据集上测试模型。测试逻辑与我们在模型训练过程中讨论的验证逻辑几乎相同。因此，我们不会在这里重复讨论。
- en: '[PRE47]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This will output:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE48]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Great, we can see the model is showing a similar performance to what it did
    on the validation data. This means our model has not overfitted data, and should
    perform reasonably well in the real world. Let’s now generate captions for a few
    sample images.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们可以看到模型在测试数据上的表现与在验证数据上的表现相似。这意味着我们的模型没有过拟合，并且在现实世界中应该表现得相当不错。现在，让我们为一些示例图像生成描述。
- en: Captions generated for test images
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为测试图像生成的描述
- en: With the help of metrics such as accuracy and BLEU, we have ensured our model
    is performing well. But, one of the most important tasks a trained model has to
    perform is generating outputs for new data. We will learn how we can use our model
    to generate actual captions. Let’s first understand how we can generate captions
    at a conceptual level. It’s quite straightforward to generate the image representation
    using an image. The tricky part is adapting the text decoder to generate captions.
    As you can imagine, the decoder inference needs to work in a different setting
    than the training. This is because at inference we don’t have caption tokens to
    input to the model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用诸如准确率和 BLEU 之类的度量标准，我们确保了我们的模型表现良好。但训练好的模型最重要的任务之一是为新数据生成输出。我们将学习如何使用模型生成实际的标题。首先，让我们从概念上理解如何生成标题。通过使用图像来生成图像表示是非常直接的。棘手的部分是调整文本解码器以生成标题。正如你所想，解码器推理需要在与训练不同的环境下工作。这是因为在推理时，我们没有标题令牌可以输入模型。
- en: The way we predict with our model is by starting with the image and a starting
    caption that has the single token `[START]`. We feed these two inputs to the model
    to generate the next token. We then combine the new token with the current input
    and predict the next token. We keep going this way until we reach a certain number
    of steps or the model outputs `[END]` (*Figure 11.11*). If you remember, we developed
    the model in a way that it can accept an arbitrary length token sequence. This
    is extremely helpful during inference as at each time step, the length of the
    sequence increases.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模型进行预测的方式是从图像和一个包含单一令牌`[START]`的起始标题开始。我们将这两个输入传递给模型，以生成下一个令牌。然后，我们将新令牌与当前输入结合，预测下一个令牌。我们会一直这么进行，直到达到一定的步数，或者模型输出`[END]`（*图
    11.11*）。如果你还记得，我们以这样一种方式开发了模型，使得它能够接受任意长度的令牌序列。这在推理过程中非常有用，因为在每个时间步长，序列的长度都会增加。
- en: '![](img/B14070_11_11.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_11.png)'
- en: 'Figure 11.11: How the decoder of the trained model is used to generate a new
    caption for a given image'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：训练好的模型的解码器如何生成给定图像的新标题
- en: 'We will choose a small dataset of 10 samples from the test dataset and generate
    captions:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从测试数据集中选择一个包含 10 个样本的小数据集，并生成标题：
- en: '[PRE49]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next let’s define a function called `generate_captions()`. This function takes
    in:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个名为`generate_captions()`的函数。这个函数接收：
- en: '`model` – The trained model'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` – 训练好的模型'
- en: '`image_input` – A batch of input images'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_input` – 一批输入图像'
- en: '`tokenizer` – Trained tokenizer'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` – 训练好的分词器'
- en: '`n_samples` – Number of samples in the batch'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_samples` – 批次中的样本数量'
- en: 'As we can see in the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示：
- en: '[PRE50]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This function starts with a single caption token ID. The ID 2 maps to the token
    `[START]`. We predict for 30 steps or if the last token is `[END]` (mapped to
    token ID 3). We generate position inputs for the batch of data by creating a range
    sequence from 0 to *i* and repeating that `n_sample` times across the batch dimension.
    We then predict the token probabilities by feeding the inputs to the model.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数以一个单一的标题令牌 ID 开始。ID 2 映射到令牌`[START]`。我们预测 30 步，或者当最后一个令牌是`[END]`（映射到令牌 ID
    3）时停止。我们通过创建一个从 0 到 *i* 的范围序列，并在批次维度上重复 `n_sample` 次，为数据批次生成位置输入。然后，我们将输入传递给模型，以预测令牌的概率。
- en: 'We can now use this function to generate captions:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这个函数生成标题：
- en: '[PRE51]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s now visualize the captions side by side with the image inputs. Additionally,
    we’ll show the ground truth captions:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将标题与图像输入并排显示。另外，我们还将展示真实的标题：
- en: '[PRE52]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You will get a plot similar to the following. The images *sampled* will be
    randomly sampled every time it runs. The results of this run can be seen in *Figure
    11.12*:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个类似于下图的图表。每次运行时，*采样的*图像会被随机采样。此次运行的结果可以在*图 11.12*中看到：
- en: '![](img/B14070_11_12.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_11_12.png)'
- en: 'Figure 11.12: Captions generated on a sample of test data'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12：在测试数据样本上生成的标题
- en: We can see that our model does a good job of generating captions. In general,
    we can see that the model can identify objects and activities portrayed in the
    images. It is also important to remember that each of our images has multiple
    captions associated with it. Therefore, the predicted captions do not necessarily
    need to match the ground truth caption shown in the image.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的模型在生成标题方面表现得很好。总的来说，我们可以看到模型能够识别图像中展示的物体和活动。同样需要记住的是，每张图像都有多个与之关联的标题。因此，预测的标题不一定需要与图像中的真实标题完全匹配。
- en: Summary
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we focused on a very interesting task that involves generating
    captions for given images. Our image-captioning model was one of the most complex
    models in this book, which included the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于一个非常有趣的任务——为给定的图像生成描述。我们的图像描述模型是本书中最复杂的模型之一，包含以下内容：
- en: A vision Transformer model that produces an image representation
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个生成图像表示的视觉 Transformer 模型
- en: A text-based Transformer decoder
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基于文本的 Transformer 解码器
- en: Before we began with the model, we analyzed our dataset to understand various
    characteristics such as image sizes and the vocabulary size. Then we understood
    how we can use a tokenizer to tokenize captions strings. We then used this knowledge
    to build a TensorFlow data pipeline.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始模型之前，我们分析了数据集，以理解各种特征，例如图像大小和词汇量大小。接着我们了解了如何使用分词器对描述字符串进行分词。然后，我们利用这些知识构建了一个
    TensorFlow 数据管道。
- en: We discussed each component in detail. The Vision Transformer (ViT) takes in
    an image and produces a hidden representation of that image. Specifically, the
    ViT breaks an image into a sequence of 16x16 patches of pixels. After that, it
    treats each patch as a token embedding to the Transformer (along with positional
    information) to produce a representation of each patch. It also incorporates the
    [CLS] token at the beginning to provide a holistic representation of the image.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细讨论了每个组件。视觉 Transformer (ViT) 接收一张图像并生成该图像的隐藏表示。具体来说，ViT 将图像拆分成一系列 16x16
    像素的小块。之后，它将每个小块作为一个 token 嵌入传递给 Transformer（包括位置编码信息），以生成每个小块的表示。它还在开头加入了[CLS]
    token，用来提供图像的整体表示。
- en: Next the text decoder takes in the image representation along with caption tokens
    as inputs. The objective of the decoder becomes to predict the next token at each
    time step. We were able to reach a BLEU-4 score of just above 0.10 for the validation
    dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，文本解码器将图像表示和描述 token 作为输入。解码器的目标是在每个时间步预测下一个 token。我们在验证数据集上达到了 BLEU-4 得分略高于
    0.10。
- en: Thereafter, we discussed several different metrics (BLEU, ROUGE, METEOR, and
    CIDEr), which we can use to quantitatively evaluate the generated captions, and
    we saw that as we ran our algorithm through the training data, the BLEU-4 score
    increased over time. Additionally, we visually inspected the generated captions
    and saw that our ML pipeline progressively gets better at captioning images.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们讨论了几种不同的评价指标（BLEU、ROUGE、METEOR 和 CIDEr），这些指标可以用来定量评估生成的图像描述，并且我们看到，在将算法应用于训练数据时，BLEU-4
    得分随着时间的推移而增加。此外，我们还通过视觉检查生成的描述，发现我们的机器学习管道在逐渐提高图像描述的准确性。
- en: Next, we evaluated our model on the test dataset and validated that it demonstrates
    similar performance on test data as expected. Finally, we learned how we can use
    the trained model to generate captions for unseen images.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在测试数据集上评估了我们的模型，并验证了其在测试数据上的表现与预期相符。最后，我们学习了如何使用训练好的模型为未见过的图像生成描述。
- en: We have reached the end of the book. We have covered many different topics in
    natural language and discussed state-of-the-art models and techniques that help
    us to solve problems.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 本书已经结束。我们涵盖了许多自然语言处理的不同主题，并讨论了有助于解决问题的先进模型和技术。
- en: In the Appendix, we will discuss some mathematical concepts related to machine
    learning, followed by an explanation of how to use the visualization tool TensorBoard
    to visualize word vectors.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录中，我们将讨论与机器学习相关的一些数学概念，并解释如何使用可视化工具 TensorBoard 来可视化词向量。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](Chapter_11.xhtml)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](Chapter_11.xhtml)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](Chapter_11.xhtml)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，与超过 1000 名成员一起学习，访问链接：[https://packt.link/nlp](Chapter_11.xhtml)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
