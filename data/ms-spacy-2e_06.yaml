- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Utilizing spaCy with Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 spaCy 与转换器一起工作
- en: '**Transformers** are the latest hot topic in NLP. The goal of this chapter
    is to learn how to use transformers to improve the performance of trainable components
    in spaCy.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换器**是 NLP 中的最新热门话题。本章的目标是学习如何使用转换器来提高 spaCy 中可训练组件的性能。'
- en: First, you will learn about transformers and transfer learning. Next, you’ll
    learn more about spaCy trainable components and how to train a component, introducing
    spaCy’s **config.cfg** files and spaCy’s CLI. Then, you will learn about the architectural
    details of the commonly used Transformer architecture – **Bidirectional Encoder**
    **Representations from Transformers** ( **BERT** ) and its successor, RoBERTa.
    Finally, you’ll train the **TextCategorizer** component to classify texts using
    a transformer layer to improve accuracy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将了解转换器和迁移学习。接下来，你将更深入地了解 spaCy 可训练组件以及如何训练一个组件，介绍 spaCy 的 **config.cfg**
    文件和 spaCy 的 CLI。然后，你将了解常用 Transformer 架构的架构细节——**双向编码器** **Transformer 表示**（**BERT**）及其继任者
    RoBERTa。最后，你将训练 **TextCategorizer** 组件，使用转换器层来提高准确率对文本进行分类。
- en: By the end of this chapter, you will be able to prepare data for training and
    fine-tune your own spaCy components. Because of the way spaCy is designed; while
    doing that, you’ll be following software engineering best practices. You’ll also
    have a solid basis of how transformers work, which will be useful when we work
    with **large** **language models** ( **LLMs** ) in [*Chapter 7*](B22441_07.xhtml#_idTextAnchor102)
    . You’ll be able to build state-of-the-art NLP pipelines with just a few lines
    of code with the power of Transformer models and transfer learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够准备训练数据并微调你自己的 spaCy 组件。由于 spaCy 的设计方式；在这样做的时候，你将遵循软件工程的最佳实践。你还将对转换器的工作原理有一个坚实的基础，这在我们与
    **大型** **语言模型**（**LLMs**）在 [*第 7 章*](B22441_07.xhtml#_idTextAnchor102) 一起工作时将非常有用。你将能够仅用几行代码就利用
    Transformer 模型和迁移学习构建最先进的 NLP 管道。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Model training and transfer learning with spaCy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 spaCy 进行模型训练和迁移学习
- en: Classifying text with spaCy pipelines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 spaCy 管道对文本进行分类
- en: Working with spaCy **config.cfg** files
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 spaCy **config.cfg** 文件一起工作
- en: Preparing training data to fine-tune models with spaCy
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练数据以使用 spaCy 微调模型
- en: Using Hugging Face’s Transformer for downstream tasks with spaCy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 的 Transformer 进行 spaCy 的下游任务
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The dataset and the chapter code can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和章节代码可以在 [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    找到。
- en: We will use the **pandas** library to manipulate the datasets and also install
    the **spacy-transformers** library to work with the **transformer** spaCy component.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 **pandas** 库来操作数据集，并安装 **spacy-transformers** 库以与 **transformer** spaCy
    组件一起工作。
- en: Transformers and transfer learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换器和迁移学习
- en: 'A milestone in NLP happened in 2017 with the release of the research paper
    *Attention Is All You Need* , by Vaswani et al. ( [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    ), which introduced a brand-new machine learning idea and architecture – **transformers**
    . Transformers in NLP is a fresh idea that aims to solve sequential modeling tasks
    and targets some problems introduced by **Long Short-Term Memory** ( **LSTM**
    ) architecture. Here’s how the paper explains how transformers work:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年，随着 Vaswani 等人发表的研究论文 *Attention Is All You Need* 的发布，自然语言处理领域发生了一个里程碑事件（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)），该论文介绍了一种全新的机器学习思想和架构——**转换器**。NLP
    中的转换器是一个新颖的想法，旨在解决序列建模任务，并针对 **长短期记忆**（**LSTM**）架构引入的一些问题。以下是论文如何解释转换器的工作原理：
- en: “ *The Transformer is the first transduction model relying entirely on self-attention
    to compute representations of its input and output without using sequence aligned
    RNNs* *or convolution.* ”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “*Transformer 是第一个完全依赖自注意力来计算其输入和输出表示的转换模型，不使用序列对齐 RNN 或卷积*”
- en: '**Transduction** in this context means transforming input to output by transforming
    input words and sentences into vectors. Typically, a transformer is trained on
    a huge corpus. Then, in our downstream tasks, we use these vectors as they carry
    information regarding the word semantics, sentence structure, and sentence semantics.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，**转换**意味着通过将输入单词和句子转换为向量来转换输入到输出。通常，一个变压器在一个巨大的语料库上训练。然后，在我们的下游任务中，我们使用这些向量，因为它们携带有关词语义、句子结构和句子语义的信息。
- en: Before transformers, the cool kid in the NLP world was the **word vector** technique.
    A word vector is basically a dense number representation of a word. What’s surprising
    about these vectors is that semantically similar words have similar word vectors.
    Word vectors such as **GloVe** and **FastText** vectors are already trained on
    the Wikipedia corpus and can be used for semantic similarity calculations. The
    **Token.similarity()** , **Span.similarity()** , and **Doc.similarity()** methods
    all use word vectors to predict how similar these containers are. This is a simple
    example usage of **transfer learning** , where we are using the knowledge from
    the texts (the knowledge extracted from words in the word vectors training) to
    solve a new problem (the similarity problem). Transformers are more powerful because
    they are designed to understand language in context, something that word vectors
    can’t do. We’ll learn more about this in the *BERT* section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器之前，NLP世界中的热门技术是**词向量**技术。词向量基本上是一个词的密集数字表示。这些向量令人惊讶的地方在于，语义相似的词具有相似的词向量。例如，**GloVe**和**FastText**向量已经在维基百科语料库上进行了训练，可以用于语义相似度计算。**Token.similarity()**、**Span.similarity()**和**Doc.similarity()**方法都使用词向量来预测这些容器之间的相似度。这是一个简单的迁移学习示例用法，其中我们使用文本中的知识（从词向量训练中提取的词的知识）来解决新问题（相似度问题）。变压器更强大，因为它们被设计成能够理解上下文中的语言，这是词向量无法做到的。我们将在*BERT*部分了解更多关于这一点。
- en: '*Transformer* is the name of the model architecture, but Hugging Face Transformers
    is also the name of the Hugging Face set of APIs and tools to easily download
    and train state-of-the-art pretrained models. Hugging Face Transformers offer
    thousands of pre-trained models to perform NLP tasks, such as text classification,
    text summarization, question answering, machine translation, and natural language
    generation in more than 100 languages. The goal is to make state-of-the-art NLP
    accessible to everyone.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer**是模型架构的名称，但Hugging Face Transformers也是Hugging Face提供的一套API和工具，用于轻松下载和训练最先进的预训练模型。Hugging
    Face Transformers提供了数千个预训练模型，用于执行NLP任务，如文本分类、文本摘要、问答、机器翻译以及超过100种语言的自然语言生成。目标是让最先进的NLP技术对每个人可访问。'
- en: In this chapter, we are going to use the transformers models to apply a form
    of transfer learning to improve the accuracy of a downstream task – in our case,
    the text classification task. We’ll do that using spaCy’s **transformer** component
    from the **spacy-transformers** package in conjunction with the **textcat** component
    to increase the accuracy of the pipeline. With spaCy, there is also the option
    to use predictions directly from an existing Hugging Face model. To do that, you
    can use the wrappers from the **spacy-huggingface-pipelines** package. We will
    see how to do that in [*Chapter 11*](B22441_11.xhtml#_idTextAnchor143) .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用变压器模型来应用一种迁移学习形式，以提高下游任务的准确性——在我们的案例中，是文本分类任务。我们将通过结合使用spaCy的**transformer**组件（来自**spacy-transformers**包）和**textcat**组件来实现这一点，以提高管道的准确性。使用spaCy，还有直接使用现有Hugging
    Face模型的预测选项。为此，你可以使用**spacy-huggingface-pipelines**包中的封装器。我们将在[*第11章*](B22441_11.xhtml#_idTextAnchor143)中看到如何做到这一点。
- en: Alright, now that you know what transformers are, let’s go ahead and learn more
    about the machine learning concepts behind the technique.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在你已经知道了什么是变压器，让我们继续学习更多关于该技术背后的机器学习概念。
- en: From LSTMs to Transformers
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从LSTMs到Transformers
- en: Before transformers, LSTM neural network cells were the go-to solution for modeling
    text. LSTMs are a variation of a **recurrent neural network** ( **RNN** ) cell.
    RNN is a special neural network architecture that can process sequential data
    in steps. In usual neural networks, we assume that all the inputs and outputs
    are independent of each other. The problem is that this way of modeling is not
    true for text data. Every word’s presence depends on the neighbor’s words. For
    example, during a machine translation task, we predict a word by considering all
    the words we predicted before. RNNs address this situation by capturing information
    about the past sequence elements and holding them in memory (called **hidden state**
    ).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer出现之前，LSTM神经网络单元是用于建模文本的常用解决方案。LSTM是**循环神经网络**（**RNN**）单元的一种变体。RNN是一种特殊的神经网络架构，可以分步骤处理序列数据。在通常的神经网络中，我们假设所有输入和输出都是相互独立的。问题是这种建模方式并不适用于文本数据。每个单词的存在都依赖于其相邻的单词。例如，在机器翻译任务中，我们通过考虑之前预测的所有单词来预测一个单词。RNN通过捕获关于过去序列元素的信息并将它们保持在内存中（称为**隐藏状态**）来解决这种情况。
- en: 'LSTMs were created to fix some computational problems of RNNs. RNNs have the
    problem of forgetting some data back in the sequence, as well as some numerical
    stability issues due to chain multiplications called vanishing and exploding gradients.
    An LSTM cell is slightly more complicated than an RNN cell, but the logic of computation
    is the same: we feed one input word at each time step and LSTM outputs a value
    at each time step.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是为了解决RNN的一些计算问题而创建的。RNN有一个问题，就是会忘记序列中的一些数据，以及由于链式乘法导致的梯度消失和爆炸等数值稳定性问题。LSTM单元比RNN单元稍微复杂一些，但计算逻辑是相同的：我们每个时间步输入一个单词，LSTM在每个时间步输出一个值。
- en: LSTMs are better than vanilla RNNs, but they have some shortcomings too. The
    LSTM architecture sometimes has difficulties with learning long text. Statistical
    dependencies in a long text can be difficult to represent by an LSTM because,
    as the time steps pass, the LSTM can forget some of the words that were processed
    at earlier time steps. Also, the nature of LSTMs is sequential. We process one
    word at each time step. This means parallelizing the learning process is impossible;
    we must process it sequentially. Not allowing parallelization creates a performance
    bottleneck.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM比传统的RNN更好，但它们也有一些缺点。LSTM架构有时在学习长文本时会有困难。长文本中的统计依赖关系可能很难用LSTM表示，因为随着时间步的推移，LSTM可能会忘记之前处理的一些单词。此外，LSTM的本质是顺序的。我们每个时间步处理一个单词。这意味着学习过程的并行化是不可能的；我们必须顺序处理。不允许并行化造成了一个性能瓶颈。
- en: 'Transformers address these problems by not using recurrent layers at all. The
    Transformer architecture consists of two parts – an input encoder (called the
    **Encoder** ) block on the left, and the output decoder (called the **Decoder**
    ) block on the right. The following diagram is taken from the original paper,
    *Attention Is All You Need* ( [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    ), and exhibits the transformer architecture:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer通过完全不使用循环层来解决这些问题。Transformer架构由两部分组成——左侧的输入编码器块（称为**编码器**）和右侧的输出解码器块（称为**解码器**）。以下图表来自原始论文《Attention
    Is All You Need》（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)），展示了Transformer架构：
- en: '![Figure 6.1 – Transformer architecture from the paper “Attention is All You
    Need”](img/B22441_06_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 来自论文“Attention is All You Need”的Transformer架构](img/B22441_06_01.jpg)'
- en: Figure 6.1 – Transformer architecture from the paper “Attention is All You Need”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 来自论文“Attention is All You Need”的Transformer架构
- en: The preceding architecture is used for a machine translation task; hence, the
    input is a sequence of words from the source language, and the output is a sequence
    of words in the target language. The encoder generates a vector representation
    of the input words and passes them to the decoder (the word vector transfer is
    represented by the arrow from the encoder block in the direction of the decoder
    block). The decoder takes these input word vectors, transforms the output words
    into word vectors, and finally, generates the probability of each output word
    (labeled in *Figure 6* *.1* as **Output Probabilities** ).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构用于机器翻译任务；因此，输入是源语言的单词序列，输出是目标语言的单词序列。编码器生成输入单词的向量表示，并将它们传递给解码器（编码器块指向解码器块的箭头表示词向量转移）。解码器接收这些输入单词向量，将输出单词转换为单词向量，并最终生成每个输出单词的概率（在*图6.1*中标记为**输出概率**）。
- en: The innovation transformers bring lies in the **Multi-Head Attention** block.
    This block creates a dense representation for each word by using a self-attention
    mechanism. The self-attention mechanism relates each word in the input sentence
    to the other words in the input sentence. The word embedding of each word is calculated
    by taking a weighted average of the other words’ embeddings. This way, the importance
    of each word in the input sentence is calculated, so the architecture focuses
    its attention on each input word in turn.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器带来的创新在于**多头注意力**块。该块通过使用自注意力机制为每个单词创建一个密集表示。自注意力机制将输入句子中的每个单词与句子中的其他单词相关联。每个单词的词嵌入是通过取其他单词嵌入的加权平均来计算的。这样，就可以计算出输入句子中每个单词的重要性，因此架构依次关注每个输入单词。
- en: The following diagram illustrates the mechanism of self-attention in a Transformer
    model. It shows how the input words on the left-hand side attend to the input
    word “it” on the right-hand side. The color gradient in the diagram represents
    the level of relevance each word has with respect to “it.” Words with darker,
    more intense colors, such as “The animal,” have higher relevance, while words
    with lighter shades, like “didn't” or “too tired,” have lower relevance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示说明了Transformer模型中自注意力的机制。它展示了左侧的输入单词如何关注右侧的输入单词“它”。图中的颜色渐变表示每个单词与“它”的相关程度。颜色较深、较鲜艳的单词，如“动物”，相关性较高，而颜色较浅的单词，如“没”或“太累了”，相关性较低。
- en: This visualization demonstrates that the Transformer can precisely determine
    that the pronoun “it” refers to “The animal” in this sentence. Such a capability
    allows Transformers to resolve complex semantic dependencies and relationships
    within a sentence, showcasing their powerful ability to understand context and
    meaning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可视化演示了Transformer可以精确地确定这个句子中的代词“它”指的是“动物”。这种能力使得Transformer能够解决句子中的复杂语义依赖和关系，展示了它们理解上下文和意义的能力。
- en: '![Figure 6.2 – Illustration of the self-attention mechanism](img/B22441_06_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 自注意力机制的说明](img/B22441_06_02.jpg)'
- en: Figure 6.2 – Illustration of the self-attention mechanism
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 自注意力机制的说明
- en: Later on in the chapter, we are going to learn about a famous transformer model
    called **BERT** , so don’t worry if all this content seems too abstract now. We
    will learn how to use transformers through a text classification use case, but
    before using transformers, we need to see how to tackle the text classification
    problem with spaCy. Let’s do that in the next section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将学习一个名为**BERT**的著名Transformer模型，所以如果现在所有这些内容看起来太抽象，请不要担心。我们将通过文本分类用例学习如何使用Transformer，但在使用Transformer之前，我们需要了解如何使用spaCy解决文本分类问题。让我们在下一节中这样做。
- en: Text classification with spaCy
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy进行文本分类
- en: spaCy models are very successful for general NLP purposes, such as understanding
    a sentence’s syntax, splitting a paragraph into sentences, and extracting entities.
    However, sometimes, we work on very specific domains that spaCy pre-trained models
    didn’t learn how to handle.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy模型在通用NLP目的上非常成功，例如理解句子的句法，将段落分割成句子，以及提取实体。然而，有时我们处理的是非常具体的领域，而spaCy预训练模型并没有学会如何处理这些领域。
- en: For example, X (formerly Twitter) text contains many non-regular words, such
    as hashtags, emoticons, and mentions. Also, X sentences are usually just phrases,
    not full sentences. Here, it’s entirely reasonable that spaCy’s POS tagger performs
    in a substandard manner as the POS tagger is trained on full, grammatically correct
    English sentences.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，X（以前是 Twitter）文本包含许多非正规词，如标签、表情符号和提及。此外，X 句子通常只是短语，而不是完整的句子。在这里，spaCy 的 POS
    标记器以不标准的方式表现是完全可以理解的，因为 POS 标记器是在完整的、语法正确的英语句子上训练的。
- en: Another example is the medical domain. It contains many entities, such as drug,
    disease, and chemical compound names. These entities are not expected to be recognized
    by spaCy’s NER model because it has no disease or drug entity labels. NER does
    not know anything about the medical domain at all.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是医疗领域。它包含许多实体，如药物、疾病和化学化合物名称。这些实体不被期望被 spaCy 的 NER 模型识别，因为它没有疾病或药物实体标签。NER
    完全不知道医疗领域。
- en: 'In this chapter, we’ll be working with the *Amazon Fine Food Reviews* dataset
    ( [https://www.kaggle.com/snap/amazon-fine-food-reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)
    ). This dataset contains customer reviews about fine food sold on Amazon ( *J.
    McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating
    dimensions with review text. RecSys, 2013* , [https://dl.acm.org/doi/abs/10.1145/2507157.2507163](https://dl.acm.org/doi/abs/10.1145/2507157.2507163)
    ). Reviews include user and product information, user ratings, and text. We want
    to classify these reviews as positive or negative. As this is a specific domain
    problem, spaCy doesn’t know how to classify it (yet). To teach the pipeline how
    to do that, we will use **TextCategorizer** , a trainable component to classify
    text. We’ll do that in the next section.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 *Amazon Fine Food Reviews* 数据集（[https://www.kaggle.com/snap/amazon-fine-food-reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)）。这个数据集包含了关于在亚马逊上销售的精致食品的客户评论（*J.
    McAuley 和 J. Leskovec. 隐藏因素和隐藏主题：通过评论文本理解评分维度。RecSys，2013*，[https://dl.acm.org/doi/abs/10.1145/2507157.2507163](https://dl.acm.org/doi/abs/10.1145/2507157.2507163)）。评论包括用户和产品信息、用户评分和文本。我们希望将这些评论分类为正面或负面。由于这是一个特定领域的问题，spaCy（目前）不知道如何进行分类。为了教会管道如何做到这一点，我们将使用
    **TextCategorizer**，这是一个可训练的文本分类组件。我们将在下一节中这样做。
- en: Training the TextCategorizer component
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 TextCategorizer 组件
- en: '**TextCategorizer** is an optional and trainable pipeline component to predict
    categories over a whole document. To train it, we need to provide examples and
    their class labels. *Figure 6* *.3* shows exactly where the **TextCategorizer**
    component lies in the NLP pipeline; this component comes after the essential components.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**TextCategorizer** 是一个可选的可训练管道组件，用于预测整个文档的类别。要训练它，我们需要提供示例及其类别标签。*图 6* *.3*
    显示了 **TextCategorizer** 组件在 NLP 管道中的确切位置；该组件位于基本组件之后。'
- en: '![Figure 6.3 – TextCategorizer in the NLP pipeline](img/B22441_06_03.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – NLP 管道中的 TextCategorizer](img/B22441_06_03.jpg)'
- en: Figure 6.3 – TextCategorizer in the NLP pipeline
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – NLP 管道中的 TextCategorizer
- en: 'A neural network architecture lies behind spaCy’s **TextCategorizer** component,
    which provides us with user-friendly and end-to-end approaches to training the
    classifier. This means we don’t have to deal directly with the neural network
    architecture. **TextCategorizer** is available in two flavors: single-label classifier
    ( **textcat** ) and multilabel classifier ( **textcat_multilabel** ). As the name
    suggests, a multilabel classifier can predict more than one class. A single-label
    classifier predicts only one class for each example and classes are mutually exclusive.
    The predictions of the component are saved in **doc.cats** as a dictionary, where
    the key is the name of the category, and the value is a score between 0 and 1
    (inclusive).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的 **TextCategorizer** 组件背后是一个神经网络架构，它为我们提供了用户友好且端到端的训练分类器的途径。这意味着我们不必直接处理神经网络架构。**TextCategorizer**
    有两种形式：单标签分类器（**textcat**）和多标签分类器（**textcat_multilabel**）。正如其名所示，多标签分类器可以预测多个类别。单标签分类器对每个示例只预测一个类别，且类别互斥。组件的预测结果以字典形式保存在
    **doc.cats** 中，其中键是类别的名称，值是介于 0 和 1（包含）之间的分数。
- en: To understand how to use the **TextCategorizer** component, it is helpful to
    learn how to train a deep model in general. Let’s do that in the next section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何使用 **TextCategorizer** 组件，学习如何一般地训练深度模型是有帮助的。让我们在下一节中这样做。
- en: Training a deep learning model
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练深度学习模型
- en: To train a neural network, we need to configure the model parameters and provide
    the training examples. Each prediction of the neural network is a sum of its weight
    values; hence, the training procedure adjusts the weights of the neural network
    with our examples. If you want to learn more about how neural networks function,
    you can read the excellent guide at [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)
    .
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个神经网络，我们需要配置模型参数并提供训练示例。神经网络的每个预测都是其权重值的总和；因此，训练过程通过我们的示例调整神经网络的权重。如果你想了解更多关于神经网络如何工作的信息，你可以阅读优秀的指南[http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)。
- en: In the training procedure, we’ll go over the training set several times and
    show each example several times. Each iteration is called an **epoch** . At each
    iteration, we also shuffle the training data to prevent the model from learning
    patterns specific to the order of the examples. This shuffling of training data
    helps to ensure that the model generalizes well to unseen data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将多次遍历训练集，并多次展示每个示例。每次迭代被称为**epoch**。在每次迭代中，我们还会洗牌训练数据，以防止模型学习到与示例顺序相关的特定模式。这种训练数据的洗牌有助于确保模型能够很好地泛化到未见过的数据。
- en: In each epoch, the training code updates the weights of the neural network through
    incremental updates. These incremental updates are usually applied by dividing
    the data of each epoch into **mini-batches** . A **loss** is calculated by comparing
    the actual label with the neural network’s current output. **Optimizers** are
    functions that update the neural network weights subject to that loss. **Gradient
    descent** is the name of the algorithm used to find the direction and the rate
    to update the network parameters. The optimizers work by iteratively updating
    the model parameters in the direction of the gradient that reduces the loss. That
    is the training process in a nutshell.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch中，训练代码通过增量更新来更新神经网络的权重。这些增量更新通常通过将每个epoch的数据分成**小批量**来应用。通过比较实际标签与神经网络当前输出，计算出一个**损失**。**优化器**是更新神经网络权重以适应该损失的函数。**梯度下降**是用于找到更新网络参数的方向和速率的算法。优化器通过迭代更新模型参数，使其朝着减少损失的方向移动。这就是简而言之的训练过程。
- en: If you ever had to train a deep learning model using PyTorch or TensorFlow,
    you’re likely familiar with the often-challenging nature of the process. spaCy
    uses Thinc, a lightweight deep learning library with a functional programming
    API for composing models. With Thinc, we can switch between PyTorch, TensorFlow,
    and MXNet models without changing the code (and without having to code with these
    libraries directly).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用PyTorch或TensorFlow训练过深度学习模型，你可能会熟悉这个过程经常具有挑战性的性质。spaCy使用Thinc，这是一个轻量级的深度学习库，具有功能编程API，用于组合模型。使用Thinc，我们可以在不更改代码的情况下（并且无需直接使用这些库进行编码）在PyTorch、TensorFlow和MXNet模型之间切换。
- en: The Thinc conceptual model is a little different from the other neural network
    libraries. To train spaCy models, we’ll have to learn about Thinc’s configuration
    system. We will do that in the next sections of this chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Thinc概念模型与其他神经网络库略有不同。为了训练spaCy模型，我们需要了解Thinc的配置系统。我们将在本章的下一节中介绍这一点。
- en: Summarizing the training process, we need to gather and prepare the data, define
    the optimizer to update the weights for each mini-batch, split the data into mini-batches,
    and shuffle each mini-batch for training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总结训练过程，我们需要收集和准备数据，定义优化器以更新每个小批量的权重，将数据分成小批量，并对每个小批量进行洗牌以进行训练。
- en: 'We haven’t touched on the phase of gathering and preparing the data yet. spaCy’s
    **Example** container holds the information for one training instance. It stores
    two **Doc** objects: one for holding the reference label and one for holding the
    predictions of the pipeline. Let''s learn how to build the **Example** objects
    from our training data in the next section.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有涉及到收集和准备数据的阶段。spaCy的**示例**容器包含一个训练实例的信息。它存储了两个**Doc**对象：一个用于存储参考标签，另一个用于存储管道的预测。让我们在下一节学习如何从我们的训练数据中构建**示例**对象。
- en: Preparing the data for spaCy trainable components
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为spaCy可训练组件准备数据
- en: To create a dataset for training, we need to construct Example objects. Example
    objects can be created using the **Example.from_dict()** method with a Doc reference
    and a dictionary of gold-standard annotations. For the **TextCategorizer** component,
    the annotation name for **Example** should be a **cat** dictionary of label/value
    pairs indicating how relevant the category is for the text.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建用于训练的数据集，我们需要构建**Example**对象。可以使用带有Doc引用和金标准注释字典的**Example.from_dict()**方法来创建**Example**对象。对于**TextCategorizer**组件，**Example**的注释名称应该是表示文本类别相关性的**cat**标签/值对字典。
- en: 'Each review of the dataset we’ll be working on can be either positive or negative.
    Here is an example of a review:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的数据集中的每个评论可以是正面或负面的。以下是一个评论示例：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The **Example.from_dict()** method takes **Doc** as the first parameter and
    **Dict[str, Any]** as the second parameter. For our classification use case, the
    **Doc** will be the review text and **Dict[str, Any]** will be **cat** dictionary
    with the labels and the correct classification. Let''s build **Example** for the
    previous review:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**Example.from_dict()**方法将**Doc**作为第一个参数，将**Dict[str, Any]**作为第二个参数。对于我们的分类用例，**Doc**将是评论文本，**Dict[str,
    Any]**将是带有标签和正确分类的**cat**字典。让我们为之前的评论构建**Example**：'
- en: 'First, let’s load a blank English pipeline:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载一个空的英文管道：
- en: '[PRE1]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s create a **Doc** to wrap the review text and create the **cats**
    dictionary with the correct labels:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个**Doc**来封装评论文本，并创建带有正确标签的**cats**字典：
- en: '[PRE2]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, let’s create an **Example** object:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个**Example**对象：
- en: '[PRE3]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this chapter, we are only fine-tuning the **TextCategorizer** component,
    but with spaCy, you can also train other trainable components such as **Tagger**
    or **DependencyParser** . The process of creating the **Example** objects is the
    same; the only thing that differs is the type of annotation for each. Here are
    some examples of different annotations (you can find the full list at [https://spacy.io/api/data-formats#dict-input](https://spacy.io/api/data-formats#dict-input)
    ):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仅微调**TextCategorizer**组件，但使用spaCy，你也可以训练其他可训练组件，如**Tagger**或**DependencyParser**。创建**Example**对象的过程是相同的；唯一不同的是每个对象的注释类型。以下是一些不同注释的示例（完整的列表可以在[https://spacy.io/api/data-formats#dict-input](https://spacy.io/api/data-formats#dict-input)找到）：
- en: '**text** : Raw text'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**text**：原始文本'
- en: '**cats** : Dictionary of **label** / **value** pairs indicating how relevant
    a certain text category is for the text'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cats**：表示特定文本类别对文本相关性的**label** / **value**对字典'
- en: '**tags** : List of fine-grained POS tags'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tags**：细粒度POS标签列表'
- en: '**deps** : List of string values indicating the dependency relation of a token
    to its head'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**deps**：表示标记与其头标记之间的依赖关系的字符串值列表'
- en: 'The **amazon_food_reviews.csv** file has a sample of 4,000 rows of the original
    *Amazon Fine Food Reviews* dataset. We will take 80% of these rows for training
    and use the other 20% for testing. Let’s create the array with all the training
    examples:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**amazon_food_reviews.csv**文件是原始*Amazon Fine Food Reviews*数据集的4,000行样本。我们将从中取80%用于训练，其余20%用于测试。让我们创建包含所有训练示例的数组：'
- en: 'First, let’s download the dataset:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们下载数据集：
- en: '[PRE4]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s load and split 80% of the dataset for training:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载并分割80%的数据集用于训练：
- en: '[PRE5]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, let’s create the train examples and store them in a list:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建训练示例并将它们存储在一个列表中：
- en: '[PRE6]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that you know how to create the examples to feed the training data, we can
    go ahead and write the training script.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何创建示例来提供训练数据，我们可以继续编写训练脚本。
- en: Creating the training script
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建训练脚本
- en: The recommended way of training the models is using the **spacy train** command
    with spaCy’s CLI; we *shouldn’t* be writing our own training scripts. In this
    section, we will write our own training script for learning purposes. We’ll learn
    how to properly train the models using the CLI in the next sections of this chapter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的推荐方法是使用spaCy的**spacy train**命令和spaCy的CLI；我们**不应该**编写自己的训练脚本。在本节中，我们将为了学习目的编写自己的训练脚本。我们将在本章的下一节学习如何使用CLI正确训练模型。
- en: Let’s review the steps to train a deep learning model. At each epoch, we shuffle
    the training data and *update the weights of the neural network* through i *ncremental
    updates* using *optimizer functions* . spaCy offers methods to create all of these
    steps of the training loop.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下训练深度学习模型的步骤。在每个epoch中，我们通过使用**optimizer functions**进行**incremental updates**来随机打乱训练数据并更新神经网络的权重。spaCy提供了创建训练循环中所有这些步骤的方法。
- en: 'Our goal is to train the **TextCategorizer** component, so the first step is
    to create it and add it to the pipeline. Since this is a trainable component,
    we need to initialize it providing the **Examples** . We also need to provide
    the current **nlp** object. Here is the code to create and initialize the component,
    using the list we’ve created in the previous code block:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练**TextCategorizer**组件，因此第一步是创建它并将其添加到管道中。由于这是一个可训练的组件，我们需要提供**Examples**来初始化它。我们还需要提供当前的**nlp**对象。以下是创建和初始化组件的代码，使用我们在前面的代码块中创建的列表：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We pass the whole **TRAIN_EXAMPLES** list as a **lambda** function. The next
    step is to define the optimizer to update the model weights. spaCy’s **Language**
    class has a **resume_training()** method that creates and returns an optimizer.
    By default, it returns the **Adam** optimizer, and we will stick to it here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整个**TRAIN_EXAMPLES**列表作为一个**lambda**函数传递。下一步是定义优化器以更新模型权重。spaCy的**Language**类有一个**resume_training()**方法，它创建并返回一个优化器。默认情况下，它返回**Adam**优化器，我们将坚持使用它。
- en: We’re ready to define the training loop. For each epoch, we go over training
    examples one by one and update the weights of **textcat** . We go over the data
    for 40 epochs. spaCy’s **util.minibatch** function iterates over batches of items.
    The **size** parameter defines the batch size. I have a GPU with enough memory
    so I'm dividing the data into groups of 200 rows.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好定义训练循环。对于每个epoch，我们逐个遍历训练示例并更新**textcat**的权重。我们遍历数据40个epochs。spaCy的**util.minibatch**函数遍历项目批次。**size**参数定义了批次大小。我有一个内存足够的GPU，所以我将数据分成200行的组。
- en: Important note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you’re running the code and run into a “GPU out of memory” error, you may
    try to decrease the **size** parameter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行代码并遇到“GPU out of memory”错误，你可以尝试减小**size**参数。
- en: 'With the loop through the training data in place, the next step is to finally
    compute the difference between the model prediction and the correct labels and
    update the weights accordingly. The **update** method of the **Language** class
    handles that. We will provide the data and a dictionary to update the loss so
    we can keep track of it and the optimizer we’ve created previously. The following
    code defines the full training loop:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据循环到位后，下一步是最终计算模型预测与正确标签之间的差异，并相应地更新权重。**Language**类的**update**方法处理这一点。我们将提供数据和字典来更新损失，这样我们就可以跟踪它和之前创建的优化器。以下代码定义了完整的训练循环：
- en: 'Initialize the pipeline and the component:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化管道和组件：
- en: '[PRE8]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create the optimizer:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建优化器：
- en: '[PRE9]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the training loop:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练循环：
- en: '[PRE10]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And there you have it, your first trained spaCy component. If everything is
    okay, the model is learning and the losses should be decreasing. At every 10 epochs,
    we print the losses to check if this is happening. Let’s predict the categories
    of some unseen reviews to have a quick glance at how the model is behaving:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，你就有了一个第一个训练好的spaCy组件。如果一切正常，模型正在学习，损失应该会减少。在每10个epoch后，我们打印损失以检查这是否发生。让我们预测一些未见过的评论的分类，快速看一下模型的行为：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Figure 6* *.4* shows the results:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.4* 显示了结果：'
- en: '![Figure 6.4 – Categories of the review examples](img/B22441_06_04.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 评论示例的分类](img/B22441_06_04.jpg)'
- en: Figure 6.4 – Categories of the review examples
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 评论示例的分类
- en: The model is right for the first two examples, but the last one is clearly a
    negative review and the model classifies it to be positive. We can see that the
    review has some very objective indicators of a bad review such as the passage
    **this is the worst** . Maybe if we add more information about the context of
    words like transformers do, we can increase the performance of the model. Let’s
    try this in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型对前两个例子是正确的，但最后一个显然是负面评论，而模型将其分类为正面。我们可以看到，评论中包含一些非常客观的负面评论指标，例如段落**这是最糟糕的**。也许如果我们添加更多关于像transformers这样的单词上下文的信息，我们可以提高模型的表现。让我们在下一节尝试一下。
- en: Using Hugging Face transformers in spaCy
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在spaCy中使用Hugging Face transformers
- en: In this chapter, we are going to use spaCy’s **transformer** component from
    **spacy-transformers** in conjunction with the **textcat** component to increase
    the accuracy of the pipeline. This time, we will create the pipeline using spaCy’s
    **config.cfg** system, which is the recommended way to train the spaCy components.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用spaCy的**transformer**组件（来自**spacy-transformers**）与**textcat**组件结合使用，以提高管道的准确性。这次，我们将使用spaCy的**config.cfg**系统创建管道，这是训练spaCy组件的推荐方式。
- en: Let’s first get to know the **Transformer** component.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解 **Transformer** 组件。
- en: The Transformer component
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 组件
- en: The **Transformer** component is provided by the **spacy-transformers** package.
    With the **Transformer** component, we can use transformer models to improve the
    accuracy of our tasks. The component supports all models that are available via
    the Hugging Face **transformers** library. In this chapter, we are going to use
    the RoBERTa model. We'll learn more about this model in the next sections of this
    chapter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer** 组件由 **spacy-transformers** 包提供。使用 **Transformer** 组件，我们可以使用
    transformer 模型来提高我们任务的准确性。该组件支持通过 Hugging Face **transformers** 库可用的所有模型。在本章中，我们将使用
    RoBERTa 模型。我们将在本章的下一节中了解更多关于这个模型的信息。'
- en: '**Transformer** adds a **Doc._.trf_data** attribute to the **Doc** objects.
    These transformer tokens can be shared with the other pipeline components. In
    this chapter, we are going to use the tokens of the RoBERTa model as part of the
    **TextCategorizer** component. But first, let’s use the RoBERTa model without
    **TextCategorizer** to see how it works. The **Transformers** component allows
    us to use a lot of different architectures. To use the **roberta-base** model
    from Hugging Face, we need to use the **spacy-transformers.TransformerModel.v3**
    architecture. This is how we do it:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer** 为 **Doc** 对象添加一个 **Doc._.trf_data** 属性。这些 transformer 标记可以与其他管道组件共享。在本章中，我们将使用
    RoBERTa 模型的标记作为 **TextCategorizer** 组件的一部分。但首先，让我们使用没有 **TextCategorizer** 的 RoBERTa
    模型来查看它是如何工作的。**Transformers** 组件允许我们使用许多不同的架构。要使用 Hugging Face 的 **roberta-base**
    模型，我们需要使用 **spacy-transformers.TransformerModel.v3** 架构。这就是我们这样做的方式：'
- en: 'Import the libraries and load a blank model:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并加载一个空白模型：
- en: '[PRE12]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the architecture we want to use with the **Transformer** component.
    The **Transformer** component accepts a **model** config to set the Thinc model
    wrapping the transformer. We set the architecture to **spacy-transformers.TransformerModel.v3**
    and the model to **roberta-base** :'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **Transformer** 组件定义我们想要使用的架构。**Transformer** 组件接受一个 **model** 配置来设置包装 transformer
    的 Thinc 模型。我们将架构设置为 **spacy-transformers.TransformerModel.v3** 并将模型设置为 **roberta-base**：
- en: '[PRE13]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Add the component to the pipeline, initialize it, and print the vectors:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将组件添加到管道中，初始化它，并打印向量：
- en: '[PRE14]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result is a **FullTransformerBatch** object that holds a batch of input
    and output objects for the transformer model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含 transformer 模型的输入和输出对象的批次的 **FullTransformerBatch** 对象。
- en: Cool, now we need to use this model output together with the **TextCategorizer**
    component. We'll do that using the **config.cfg** file, so first we need to learn
    how to work with this configuration system.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，现在我们需要使用这个模型输出与 **TextCategorizer** 组件一起使用。我们将使用 **config.cfg** 文件来完成，所以首先我们需要学习如何与这个配置系统一起工作。
- en: spaCy’s configuration system
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: spaCy 的配置系统
- en: 'spaCy v3.0 introduces configuration files. These files are used to include
    all settings and hyperparameters for training pipelines. Under the hood, the training
    config uses the configuration system provided by the Thinc library. As the spaCy
    documentation points out, some of the main advantages and features of spaCy’s
    training config are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy v3.0 引入了配置文件。这些文件用于包含训练管道的所有设置和超参数。在底层，训练配置使用 Thinc 库提供的配置系统。正如 spaCy
    文档所指出的，spaCy 训练配置的一些主要优点和功能如下：
- en: '**Structured sections** : The config is grouped into sections, and nested sections
    are defined using the **.** notation. For example, **[components.textcat]** defines
    the settings for the pipeline’s **TextCategorizer** component.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化部分**：配置被分组到部分中，嵌套部分使用 **.** 符号定义。例如，**[components.textcat]** 定义了管道的 **TextCategorizer**
    组件的设置。'
- en: '**Interpolation** : If you have hyperparameters or other settings used by multiple
    components, define them once and reference them as variables.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插值**：如果您有多个组件使用的超参数或其他设置，请定义一次，并作为变量引用。'
- en: '**Reproducibility with no hidden defaults** : The config file is the “single
    source of truth” and includes all settings.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无隐藏默认值的可重复性**：配置文件是“单一事实来源”并包含所有设置。'
- en: '**Automated checks and validation** : When you load a config, spaCy checks
    whether the settings are complete and whether all values have the correct types.
    This lets you catch potential mistakes early. In your custom architectures, you
    can use Python type hints to tell the config which types of data to expect.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Automated checks and validation**：当你加载一个配置时，spaCy会检查设置是否完整，以及所有值是否具有正确的类型。这让你能够及早捕捉到潜在的错误。在你的自定义架构中，你可以使用Python类型提示来告诉配置期望哪些类型的数据。'
- en: 'The config is divided into sections and subsections, indicated by the square
    brackets and dot notation. For example, **[components]** is a section, and **[components.textcat]**
    is a subsection. The main top-level sections of a config file are as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 配置被分为多个部分和子部分，由方括号和点符号表示。例如，**[components]**是一个部分，而**[components.textcat]**是一个子部分。配置文件的主要顶级部分如下：
- en: '**paths** : Paths to data and other assets. Reused across the config as variables
    (e.g., **${paths.train}** ) and can be overwritten on the CLI.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**paths**：数据和其它资产的路由。在配置中作为变量重用（例如，**${paths.train}**），并且可以在命令行界面（CLI）上覆盖。'
- en: '**system** : Settings related to system and hardware. Reused across the config
    as variables (e.g., **${system.seed}** ) and can be overwritten on the CLI.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**system**：与系统和硬件相关的设置。在配置中作为变量重用（例如，**${system.seed}**），并且可以在命令行界面（CLI）上覆盖。'
- en: '**nlp** : Definition of the **nlp** object, its tokenizer, and processing pipeline
    component names.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nlp**：**nlp**对象、其分词器和处理流程组件名称的定义。'
- en: '**components** : Definitions of the pipeline components and their models.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**components**：流程组件及其模型的定义。'
- en: '**training** : Settings and controls for the training and evaluation process.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**training**：训练和评估过程的设置和控制。'
- en: '**pretraining** : Optional settings and controls for the language model pretraining.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pretraining**：语言模型预训练的可选设置和控制。'
- en: '**initialize** : Data resources and arguments passed to components when **nlp.initialize
    ()** is called before training (but not at runtime).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**initialize**：在训练前调用**nlp.initialize()**时传递给组件的数据资源和参数（但不是在运行时）。'
- en: We now know how to train a deep learning model and how to define the configuration
    for this training using Thinc as part of the spaCy training process. This configuration
    system is very handy for maintaining and reproducing NLP pipelines, and they are
    not only for training but also for building the pipelines when we don’t need to
    train the components. The spaCy config system really shines when combined with
    the spaCy CLI.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何训练一个深度学习模型，以及如何使用Thinc作为spaCy训练过程的一部分来定义此训练的配置。这个配置系统对于维护和重现NLP流程非常有用，并且不仅限于训练，还包括在我们不需要训练组件时构建流程。当与spaCy
    CLI结合使用时，spaCy配置系统表现得尤为出色。
- en: Training the TextCategorizer with a config file
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用配置文件训练TextCategorizer
- en: In this section, we will use spaCy’s CLI to fine-tune the classification pipeline.
    As usual, the first thing you need to do to train the model is prepare the data.
    The recommended way of preparing data for training with spaCy is using the **DocBin**
    container instead of creating the **Example** objects as we did earlier. **DocBin**
    container packs a collection of **Doc** objects for binary serialization.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用spaCy的命令行界面（CLI）来微调分类流程。通常，训练模型的第一步是准备数据。使用spaCy进行训练时，推荐的方式是使用**DocBin**容器，而不是像之前那样创建**Example**对象。**DocBin**容器打包了一系列**Doc**对象，以便进行二进制序列化。
- en: 'To create the training data with **DocBin** , we will create the **Doc** objects
    using the text of the reviews and add the **doc.cats** attribute accordingly.
    The process is pretty straightforward as we just need to use the **DocBin.add()**
    method to add a **Doc** annotation for serialization:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用**DocBin**创建训练数据，我们将使用评论的文本创建**Doc**对象，并相应地添加**doc.cats**属性。这个过程相当直接，我们只需要使用**DocBin.add()**方法添加一个**Doc**注释以进行序列化：
- en: 'First, we load and split the data as we did previously:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们像之前一样加载数据并进行分割：
- en: '[PRE15]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we create a **DocBin** object and, inside the **for** loop, we create
    the **Doc** objects and add them to **DocBin** :'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建一个**DocBin**对象，并在**for**循环内部创建**Doc**对象并将它们添加到**DocBin**：
- en: '[PRE16]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we save the **DocBin** object to disk:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将**DocBin**对象保存到磁盘：
- en: '[PRE17]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will also need to create a **dev** test set (it will be used in the training),
    so let’s create a function to convert the datasets:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要创建一个**dev**测试集（它将在训练中使用），因此让我们创建一个函数来转换数据集：
- en: '[PRE18]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we have all the data prepared in the recommended way to train the models.
    We will use these **.spacy** files in a minute; let’s first learn how to use the
    spaCy CLI.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经以推荐的方式准备好了所有数据以训练模型。我们将在一分钟内使用这些 **.spacy** 文件；让我们首先学习如何使用 spaCy CLI。
- en: spaCy’s CLI
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: spaCy 的 CLI
- en: With spaCy’s CLI, you can perform spaCy operations using the command line. Using
    the command line is important because we can create and automate the execution
    of the pipelines, making sure that every time we run the pipeline, it will follow
    the same steps.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 的 CLI，您可以通过命令行执行 spaCy 操作。使用命令行很重要，因为我们可以创建和自动化管道的执行，确保每次运行管道时，它都会遵循相同的步骤。
- en: spaCy’s CLI provides commands for training pipelines, converting data, debugging
    your config files, evaluating the models, and so on. You can type **python -m
    spacy --help** to see the list of all CLI commands.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的 CLI 提供了用于训练管道、转换数据、调试配置文件、评估模型等命令。您可以通过输入 **python -m spacy --help**
    来查看所有 CLI 命令的列表。
- en: The **spacy train** command trains or updates a spaCy pipeline. It requires
    data in spaCy’s binary format but you can also convert data from other formats
    using the **spacy convert** command. The config file should include all settings
    and hyperparameters used during training. We can override settings using command-line
    options. For instance, **--training.batch_size 128** overrides the value of **"batch_size"**
    in the **"[** **training]"** block.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**spacy train** 命令用于训练或更新一个 spaCy 管道。它需要 spaCy 的二进制格式数据，但您也可以使用 **spacy convert**
    命令将数据从其他格式转换过来。配置文件应包含训练过程中使用的所有设置和超参数。我们可以使用命令行选项来覆盖设置。例如，**--training.batch_size
    128** 会覆盖 **"[** **training]"** 块中 **"batch_size"** 的值。'
- en: 'We will use the **spacy init config** CLI command to create the configuration
    file. The information in the configuration file includes the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 **spacy init config** CLI 命令来创建配置文件。配置文件中的信息包括以下内容：
- en: The paths for the converted datasets
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换数据集的路径
- en: A seed number and the GPU config
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个种子数和 GPU 配置
- en: How to create the **nlp** object
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建 **nlp** 对象
- en: How to build the components we will use
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建我们将使用的组件
- en: How to do the training itself
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进行训练本身
- en: For training, it is important to *make all the settings explicit* . We don´t
    want hidden defaults because they can make the pipelines hard to reproduce. That’s
    part of the design of the configuration files.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，*明确所有设置* 非常重要。我们不希望有隐藏的默认值，因为它们可以使管道难以重现。这是配置文件设计的一部分。
- en: 'Let’s create a training configuration that doesn’t use the **Transformer**
    component to see the proper way of training a model:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个不使用 **Transformer** 组件的训练配置，以了解训练模型的正确方式：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This command creates a configuration file named **config_without_transformer.cfg**
    using an English model and with a **TextCategorizer** component, with all the
    other settings defined by default.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令使用英语模型和一个 **TextCategorizer** 组件创建一个名为 **config_without_transformer.cfg**
    的配置文件，并默认定义了所有其他设置。
- en: Inside the file, in the **paths** section, we should point to the **train**
    and **dev** data paths. Then, in the **system** section, we set the random seed.
    spaCy uses CuPy for GPU support. CuPy provides a NumPy-compatible interface for
    GPU arrays. The **gpu_allocator** parameter sets the library for CuPy to route
    GPU memory allocation to and the values can be either **pytorch** or **tensorflow**
    . This avoids the memory problems when using CuPy together with one of these libraries,
    but since it's now our case, here we can leave it set to **null** .
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件中，在 **paths** 部分中，我们应该指向 **train** 和 **dev** 数据路径。然后，在 **system** 部分中，我们设置随机种子。spaCy
    使用 CuPy 来支持 GPU。CuPy 为 GPU 数组提供了一个与 NumPy 兼容的接口。**gpu_allocator** 参数设置 CuPy 将
    GPU 内存分配路由到哪个库，其值可以是 **pytorch** 或 **tensorflow**。这避免了在使用 CuPy 与这些库之一一起使用时出现的内存问题，但由于现在的情况，我们可以将其设置为
    **null**。
- en: In the **nlp** section, we specify the model we’ll use and define the components
    of the pipeline, which is just **textcat** for now. In the **components** section,
    we need to specify how to initialize the component, so we set the **factory =
    "textcat"** parameter in the **component.textcat** subsection. **textcat** is
    the name of the registered function to create the **TextCategorizer** component.
    You can see all the available config parameters at [https://spacy.io/api/data-formats#config](https://spacy.io/api/data-formats#config)
    .
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **nlp** 部分，我们指定我们将使用的模型并定义管道的组件，目前只是 **textcat**。在 **components** 部分，我们需要指定如何初始化组件，因此我们在
    **component.textcat** 子部分中设置了 **factory = "textcat"** 参数。**textcat** 是创建 **TextCategorizer**
    组件的注册函数的名称。您可以在 [https://spacy.io/api/data-formats#config](https://spacy.io/api/data-formats#config)
    上看到所有可用的配置参数。
- en: 'With the config all set, we can run the **spacy train** command. The result
    of this run is a new pipeline, so you need to specify a path to save it. Here
    is the full command to run the training process:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 配置设置完成后，我们可以运行 **spacy train** 命令。这次运行的输出是一个新的管道，因此您需要指定一个路径来保存它。以下是运行训练过程的完整命令：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This command trains the pipeline using the configuration file we’ve created
    and points to the **train.spacy** and **dev.spacy** data. *Figure 6* *.5* shows
    the training output.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令使用我们创建的配置文件训练管道，并指向 **train.spacy** 和 **dev.spacy** 数据。*图 6* *.5* 展示了训练输出。
- en: '![Figure 6.5 – Training output](img/B22441_06_05.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 训练输出](img/B22441_06_05.jpg)'
- en: Figure 6.5 – Training output
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 训练输出
- en: '**E** indicates the epoch, and you can also see the loss and the score for
    each optimization step. The best model is saved at **pipeline_without_transformer/model-last**
    . Let’s load it and check the results of the previous examples:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**E** 表示时代，你还可以看到每个优化步骤的损失和分数。最佳模型保存在 **pipeline_without_transformer/model-last**
    。让我们加载它并检查前述示例的结果：'
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Figure 6* *.6* shows the results.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6* *.6* 展示了结果。'
- en: '![Figure 6.6 – Categories of the review examples using the new pipeline](img/B22441_06_06.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 使用新流程的审查示例类别](img/B22441_06_06.jpg)'
- en: Figure 6.6 – Categories of the review examples using the new pipeline
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 使用新流程的审查示例类别
- en: Now, the model is incorrect for the first two examples and correct for the third
    one. Let’s see whether we can improve that using the **transformers** component.
    Before doing that, now is a good time to learn the internals of one of the most
    influential transformer models, **BERT** . Then, we’ll learn more about its successor,
    **RoBERTa** , the model we’ll use in this chapter’s classification use case.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型对前两个示例是不正确的，对第三个示例是正确的。让我们看看我们是否可以使用 **transformers** 组件来改进这一点。在这样做之前，现在是学习最有影响力的变压器模型之一，**BERT**
    的内部结构的好时机。然后，我们将了解其继任者，**RoBERTa**，这是我们将在本章分类用例中使用的模型。
- en: BERT and RoBERTa
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 和 RoBERTa
- en: 'In this section, we’ll explore the most influential and commonly used Transformer
    model, BERT. BERT is introduced in Google’s 2018 research paper; you can read
    it here: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)
    .'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨最有影响力和最常用的 Transformer 模型，BERT。BERT 在 Google 2018 年的研究论文中被介绍；您可以在以下链接中阅读：[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)。
- en: 'What does BERT do exactly? To understand what BERT outputs, let’s dissect the
    name:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 究竟做了什么？为了理解 BERT 的输出，让我们剖析一下这个名字：
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: BERT is essentially a trained transformer encoder stack. Input into BERT is
    a sentence, and the output is a sequence of word vectors. The difference between
    BERT and previous word vector techniques is that BERT’s word vectors are contextual,
    which means that a vector is assigned to a word based on the input sentence.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 实质上是一个训练好的变压器编码器堆栈。BERT 的输入是一个句子，输出是一个单词向量的序列。BERT 与之前的词向量技术之间的区别在于，BERT
    的词向量是上下文相关的，这意味着一个向量是根据输入句子分配给一个单词的。
- en: 'Word vectors such as GloVe are context-free, meaning that the word vector for
    a word is always the same independent of the sentence it is used in. The following
    diagram explains this problem:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 GloVe 的词向量是上下文无关的，这意味着一个单词的词向量在句子中使用时总是相同的，不受句子上下文的影响。以下图表解释了这个问题：
- en: '![Figure 6.7 – Word vector for the word “bank”](img/B22441_06_07.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – “bank”单词的词向量](img/B22441_06_07.jpg)'
- en: Figure 6.7 – Word vector for the word “bank”
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – “bank”单词的词向量
- en: Here, even though the word *bank* has two completely different meanings in these
    two sentences, the word vectors are the same. Each word has only one vector and
    vectors are saved to a file following training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，尽管这两个句子中的单词 *bank* 有两种完全不同的含义，但词向量是相同的。每个单词只有一个向量，并且向量在训练后保存到文件中。
- en: 'On the contrary, BERT word vectors are dynamic. BERT can generate different
    word vectors for the same word depending on the input sentence. The following
    diagram shows the word vectors generated by BERT:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，BERT 词向量是动态的。BERT 可以根据输入句子为同一单词生成不同的词向量。以下图表显示了 BERT 生成的词向量：
- en: '![Figure 6.8 – Two distinct word vectors generated by BERT for the same word,
    “bank,” in two different contexts](img/B22441_06_08.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – BERT 在两个不同语境下为同一单词“bank”生成的两个不同的词向量](img/B22441_06_08.jpg)'
- en: Figure 6.8 – Two distinct word vectors generated by BERT for the same word,
    “bank,” in two different contexts
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – BERT 在两个不同语境下为同一单词“bank”生成的两个不同的词向量
- en: How does BERT generate these word vectors? In the next section, we’ll explore
    the details of the BERT architecture.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是如何生成这些词向量的？在下一节中，我们将探讨 BERT 架构的细节。
- en: BERT architecture
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT 架构
- en: 'BERT is a transformer encoder stack, which means that several encoder layers
    are stacked on top of each other. The first layer initializes the word vectors
    randomly, and then each encoder layer transforms the output of the previous encoder
    layer. The paper introduces two model sizes for BERT: BERT Base and BERT Large.
    The following diagram shows the BERT architecture:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 是一个变压器编码器堆叠，这意味着几个编码器层堆叠在一起。第一层随机初始化词向量，然后每个编码器层将前一个编码器层的输出进行转换。论文介绍了两种
    BERT 模型大小：BERT Base 和 BERT Large。以下图表显示了 BERT 架构：
- en: '![Figure 6.9 – BERT Base and Large architectures, having 12 and 24 encoder
    layers, respectively](img/B22441_06_09.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – BERT Base 和 Large 架构，分别有 12 和 24 个编码器层](img/B22441_06_09.jpg)'
- en: Figure 6.9 – BERT Base and Large architectures, having 12 and 24 encoder layers,
    respectively
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – BERT Base 和 Large 架构，分别有 12 和 24 个编码器层
- en: Both BERT models have a huge number of encoder layers. BERT Base has 12 encoder
    layers and BERT Large has 24 encoder layers. The dimensions of the resulting word
    vectors are different too; BERT Base generates word vectors of size 768 and BERT
    Large generates word vectors of size 1024.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 BERT 模型都有大量的编码器层。BERT Base 有 12 个编码器层，BERT Large 有 24 个编码器层。生成的词向量维度也不同；BERT
    Base 生成 768 大小的词向量，BERT Large 生成 1024 大小的词向量。
- en: 'The following diagram exhibits a high-level overview of BERT inputs and outputs
    (ignore the CLS token for now; you’ll learn about it in the *BERT input* *format*
    section):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 BERT 输入和输出的高级概述（现在忽略 CLS 标记；你将在 *BERT 输入* *格式* 部分学习有关它的内容）：
- en: '![Figure 6.10 – BERT model input word and output word vectors](img/B22441_06_10.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – BERT 模型输入词和输出词向量](img/B22441_06_10.jpg)'
- en: Figure 6.10 – BERT model input word and output word vectors
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – BERT 模型输入词和输出词向量
- en: In the preceding diagram, we can see a high-level overview of BERT inputs and
    outputs. BERT input must be in a special format and include some special tokens,
    such as CLS in *Figure 6* *.10* . In the next section, you’ll learn about the
    details of the BERT input format.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到 BERT 输入和输出的高级概述。BERT 输入必须以特殊格式，并包含一些特殊标记，如 *图 6.10* 中的 CLS。在下一节中，你将了解
    BERT 输入格式的细节。
- en: BERT input format
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT 输入格式
- en: To understand how BERT generates the output vectors, we need to know the BERT
    input data format. BERT input format can represent a single sentence, as well
    as a pair of sentences. For tasks such as question answering and semantic similarity,
    we input two sentences to the model in a single sequence of tokens.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 BERT 如何生成输出向量，我们需要了解 BERT 输入数据格式。BERT 输入格式可以表示单个句子，也可以表示一对句子。对于问答和语义相似度等任务，我们将两个句子作为一个标记序列输入到模型中。
- en: 'BERT works with a class of special tokens and a special tokenization algorithm
    called **WordPiece** . The main special tokens are **[CLS]** , **[SEP]** , and
    **[PAD]** :'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 与一类特殊标记和一种称为 **WordPiece** 的特殊标记化算法一起工作。主要的特殊标记是 **[CLS]**，**[SEP]** 和
    **[PAD]**：
- en: The first special token of BERT is [ **CLS** ]. The first token of every input
    sequence has to be [ **CLS** ]. We use this token in classification tasks as an
    aggregate of the input sentence. We ignore this token in non-classification tasks.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 的第一个特殊标记是 [ **CLS** ]。每个输入序列的第一个标记必须是 [ **CLS** ]。我们在分类任务中使用此标记作为输入句子的汇总。在非分类任务中，我们忽略此标记。
- en: '[ **SEP** ] means a sentence separator. If the input is a single sentence,
    we place this token at the end of the sentence. If the input is two sentences,
    then we use this token to separate two sentences. Hence, for a single sentence,
    the input looks like [ **CLS** ] sentence [ **SEP** ], and for two sentences,
    the input looks like [ **CLS** ] sentence1 [ **SEP** ] sentence2 [ **SEP** ].'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ **SEP** ] 表示句子分隔符。如果输入是一个单独的句子，我们将此标记放置在句子的末尾。如果输入是两个句子，则使用此标记来分隔两个句子。因此，对于单个句子，输入看起来像
    [ **CLS** ] 句子 [ **SEP** ]，而对于两个句子，输入看起来像 [ **CLS** ] 句子1 [ **SEP** ] 句子2 [ **SEP**
    ]。'
- en: '[ **PAD** ] is a special token meaning padding. BERT receives sentences of
    a fixed length; hence, we pad the short sentences before feeding them to BERT.
    The maximum length of tokens we can feed to BERT is 512.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ **PAD** ] 是一个特殊标记，表示填充。BERT 接收固定长度的句子；因此，我们在将句子输入到 BERT 之前对其进行填充。我们可以输入到
    BERT 中的标记的最大长度是 512。'
- en: BERT tokenizes the words using WordPiece tokenization. A “word piece” is literally
    a piece of a word. The WordPiece algorithm breaks words down into several sub-words.
    The idea is to break down complex/long tokens into simpler tokens. For example,
    the word **playing** is tokenized as **play** and **##ing** . A **##** character
    is placed before every word piece to indicate that this token is not a word from
    the language’s vocabulary but that it’s a word piece.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 使用 WordPiece 分词对单词进行分词。一个“词片”字面上就是一个单词的一部分。WordPiece 算法将单词分解成几个子词。其想法是将复杂/长的标记分解成更简单的标记。例如，单词
    **playing** 被分词为 **play** 和 **##ing**。一个 **##** 字符放置在每个词片之前，以指示此标记不是语言词汇中的单词，而是词片。
- en: 'Let’s look at some more examples:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看更多的例子：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: By doing that, we represent the language vocabulary more compactly, grouping
    common sub-words. WordPiece tokenization creates wonders on rare/unseen words,
    as these words are broken down into their sub-words.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们更紧凑地表示语言词汇，将常见的子词分组。WordPiece 分词在罕见/未见过的单词上创造了奇迹，因为这些单词被分解成它们的子词。
- en: After tokenizing the input sentence and adding the special tokens, each token
    is converted to its ID. After that, we feed the sequence of token IDs to BERT.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入句子进行分词并添加特殊标记后，每个标记被转换为它的 ID。之后，我们将标记 ID 的序列输入到 BERT。
- en: 'To summarize, this is how we transform a sentence into a BERT input format:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这是我们将句子转换为 BERT 输入格式的方法：
- en: '![Figure 6.11 – Transforming an input sentence into BERT input format](img/B22441_06_11.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 将输入句子转换为 BERT 输入格式](img/B22441_06_11.jpg)'
- en: Figure 6.11 – Transforming an input sentence into BERT input format
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 将输入句子转换为 BERT 输入格式
- en: This tokenization process is crucial for transformer models because it allows
    the model to handle out-of-vocabulary words and helps in generalization. For example,
    the model can learn that the suffix *ness* in words such as *happiness* and *sadness*
    has a specific meaning and can use this knowledge for new words that also have
    this suffix.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词过程对于转换器模型至关重要，因为它允许模型处理词汇表外的单词，并有助于泛化。例如，模型可以学习到像 *happiness* 和 *sadness*
    这样的单词中 *ness* 后缀具有特定的含义，并且可以使用这种知识来处理具有相同后缀的新单词。
- en: BERT is trained on a large unlabeled Wiki corpus and a huge book corpus. As
    stated in Google Research’s BERT GitHub repository, [https://github.com/google-research/bert](https://github.com/google-research/bert)
    , they trained a large model (12-layer to 24-layer Transformer) on a large corpus
    (Wikipedia + BookCorpus) for a long time (1M update steps).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在一个大型未标记的 Wiki 语料库和庞大的书籍语料库上训练。如 Google Research 的 BERT GitHub 仓库 [https://github.com/google-research/bert](https://github.com/google-research/bert)
    中所述，他们在一个大型语料库（维基百科 + BookCorpus）上长时间（1M 更新步骤）训练了一个大型模型（12层到24层的转换器）。
- en: 'BERT is trained with two training methods: **masked language modeling** ( **MLM**
    ) and **next sentence prediction** ( **NSP** ). **Language modeling** is the task
    of predicting the next token given the sequence of previous tokens. For example,
    given the sequence of the words *Yesterday I visited a* , a language model can
    predict the next token as one of the tokens such as *church* , *hospital* , *school*
    , and so on. **Masked language modeling** is a kind of language modeling where
    we mask a percentage of the tokens randomly by replacing them with a **[MASK]**
    token. We expect MLM to predict the masked words.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 使用两种训练方法进行训练：**掩码语言模型**（**MLM**）和**下一个句子预测**（**NSP**）。**语言模型**是预测给定先前标记序列的下一个标记的任务。例如，给定单词序列
    *Yesterday I visited a*，语言模型可以预测下一个标记为诸如 *church*、*hospital*、*school* 等标记之一。**掩码语言模型**是一种语言模型，其中我们通过用
    **掩码标记**随机替换一定比例的标记来掩码。我们期望 MLM 能够预测掩码的单词。
- en: 'The masked language model data preparation in BERT is implemented as follows.
    First, 15 of the input tokens are chosen at random. Then, the following happens:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BERT 中的掩码语言模型数据准备如下。首先，随机选择 15 个输入标记。然后，发生以下情况：
- en: 80% of the tokens chosen are replaced with **[MASK]**
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所选择的标记中有 80% 被替换为 **粗体标记**。
- en: 10% of the tokens chosen are replaced with another token from the vocabulary
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所选择的标记中有 10% 被替换为词汇表中的另一个标记
- en: The remaining 10% are left unchanged
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的 10% 保持不变
- en: 'A training example sentence for MLM looks like the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: MLM 的一个训练示例句子如下：
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: NSP is the task of predicting the next sentence given an input sentence. In
    this approach, we feed two sentences to BERT and expect BERT to predict the order
    of the sentences, more specifically, whether the second sentence is the sentence
    that comes after the first sentence.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: NSP 是根据输入句子预测下一个句子的任务。在这个方法中，我们向 BERT 输入两个句子，并期望 BERT 预测句子的顺序，更具体地说，是否第二个句子是跟在第一个句子后面的句子。
- en: 'Let’s make an example input to NSP. We’ll feed two sentences separated by the
    **[SEP]** token as input:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个 NSP 的示例输入。我们将以 **[SEP]** 标记分隔的两个句子作为输入：
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this example, the second sentence can follow the first sentence; hence, the
    predicted label is **IsNext** . How about this example?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第二句话可以跟在第一句话后面；因此，预测的标签是 **IsNext**。这个例子怎么样？
- en: '[PRE26]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This example pair of sentences generates the **NotNext** label, as they are
    not contextually or semantically related.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这对句子示例生成的是 **NotNext** 标签，因为它们在上下文或语义上不相关。
- en: Both these training techniques allow the model to learn complex concepts about
    the language. Transformers are the basis of LLMs. LLMs are revolutionizing the
    NLP world, mostly because of their capacity for understanding context.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种训练技术都允许模型学习关于语言的复杂概念。Transformer 是 LLM 的基础。LLM 正在改变 NLP 世界，这主要是因为它们理解上下文的能力。
- en: Now that you know the BERT architecture, the details of the input format, and
    training data preparation, you have a solid basis to understand how LLMs work.
    Getting back to our classification use case, we’ll work with a successor of BERT,
    a model called RoBERTa. Let’s learn about RoBERTa in the next section.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 BERT 架构、输入格式的细节以及训练数据准备，你有了理解 LLM 的工作原理的坚实基础。回到我们的分类用例，我们将使用 BERT 的一个后继模型，即
    RoBERTa。让我们在下一节中了解 RoBERTa。
- en: RoBERTa
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RoBERTa
- en: The RoBERTa model was proposed at [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)
    . It builds on BERT and the key difference between them is in the data preparation
    and training.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 模型在 [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)
    中被提出。它建立在 BERT 的基础上，它们之间的关键区别在于数据准备和训练。
- en: BERT does the token masking once during data preprocessing, which results in
    the same mask for each training instance in every epoch. RoBERTa uses *dynamic
    masking* , where they generate the masking patterns every time we feed a sequence
    to the model. They also removed the NSP because they found that it matches or
    slightly improves downstream task performance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在数据预处理期间进行一次标记掩码，这导致每个训练实例在每个 epoch 中都有相同的掩码。RoBERTa 使用 *动态掩码*，每次我们向模型输入一个序列时，它们都会生成掩码模式。他们还移除了
    NSP，因为他们发现它匹配或略微提高了下游任务性能。
- en: RoBERTa also uses larger batch sizes than BERT and a larger vocabulary size,
    from a vocabulary size of 30K to a vocabulary containing 50K sub-word units. The
    paper is a very good read to understand design decisions that impact transformer
    models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 也比 BERT 使用更大的批量大小和更大的词汇量，从 30K 的词汇量到包含 50K 子词单元的词汇量。这篇论文是一篇非常好的读物，可以了解影响
    Transformer 模型的设计决策。
- en: Now that we know how BERT and RoBERTa work, it’s time to finally use RoBERTa
    in our text classification pipeline.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了BERT和RoBERTa的工作原理，现在是时候最终在我们的文本分类管道中使用RoBERTa了。
- en: Training the TextCategorizer with a transformer
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用转换器训练TextCategorizer
- en: To work with the **transformer** component for downstream tasks, we need to
    tell spaCy how to connect the component output with the other pipeline components.
    We’ll do that with the **spacy-transformers.TransformerModel.v3** and the **spacy-transformers.TransformerListener.v1**
    layers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要与**transformer**组件进行下游任务，我们需要告诉spaCy如何将组件输出与其他管道组件连接起来。我们将使用**spacy-transformers.TransformerModel.v3**和**spacy-transformers.TransformerListener.v1**层来完成这项工作。
- en: In spaCy, we have different model architectures, which are functions that wire
    up Thinc **Model** instances. Both **TransformerModel** and the **TransformerListener**
    model are Transformer layers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，我们有不同的模型架构，这些是连接Thinc **Model**实例的函数。**TransformerModel**和**TransformerListener**模型都是转换器层。
- en: The **spacy-transformers.TransformerModel.v3** layer loads and wraps the transformer
    models from the Hugging Face Transformers library. It works with any transformer
    that has pretrained weights and has a PyTorch implementation. The **spacy-transformers.TransformerListener.v1**
    layer takes a list of **Doc** objects as input and uses the **TransformerModel**
    layer to produce a list of two-dimensional arrays as output.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**spacy-transformers.TransformerModel.v3**层加载并包装了来自Hugging Face Transformers库的转换器模型。它与任何具有预训练权重和PyTorch实现的转换器一起工作。**spacy-transformers.TransformerListener.v1**层接受一个**Doc**对象列表作为输入，并使用**TransformerModel**层生成一个二维数组列表作为输出。'
- en: Now that you know the spaCy layer concept, it is time to revisit the **TextCategorizer**
    component. In spaCy, the **TextCategorizer** component has different model architecture
    layers. Typically, each architecture accepts sublayers as arguments. By default,
    the **TextCategorizer** component uses the **spacy.TextCatEnsemble.v2** layer,
    which is a stacked ensemble of a linear bag-of-words model and a neural network
    model. We used this layer when we trained the pipeline using the configuration
    file.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了spaCy层概念，是时候回顾**TextCategorizer**组件了。在spaCy中，**TextCategorizer**组件有不同的模型架构层。通常，每个架构接受子层作为参数。默认情况下，**TextCategorizer**组件使用**spacy.TextCatEnsemble.v2**层，这是一个线性词袋模型和神经网络模型的堆叠集成。我们在使用配置文件训练管道时使用了这个层。
- en: 'To finish our journey of this chapter, we will change the neural network layer
    of **TextCatEnsemble** from the default **spacy.Tok2Vec.v2** layer and use the
    RoBERTa transformer model. We will do this by creating a new configuration file:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本章的旅程，我们将**TextCatEnsemble**的神经网络层从默认的**spacy.Tok2Vec.v2**层更改为RoBERTa转换器模型。我们将通过创建一个新的配置文件来完成这项工作：
- en: '[PRE27]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This command created a **config_transformer.cfg** file optimized for accuracy
    and for training on a GPU. *Figure 6* *.12* shows the output of the command.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建了一个针对准确性和GPU训练优化的**config_transformer.cfg**文件。*图6.12*显示了命令的输出。
- en: '![Figure 6.12 – Creating the new training configuration that uses RoBERTa](img/B22441_06_12.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 创建使用RoBERTa的新训练配置](img/B22441_06_12.jpg)'
- en: Figure 6.12 – Creating the new training configuration that uses RoBERTa
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 创建使用RoBERTa的新训练配置
- en: 'Now, we can train the pipeline pointing to this configuration file, train the
    model, and make the predictions, just as we did in the previous section:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过指向此配置文件来训练管道，训练模型，并做出预测，就像我们在上一节中所做的那样：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This time, we do the training using a GPU, so we set the **gpu_id** parameter.
    *Figure 6* *.13* shows the results using this new trained model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们使用GPU进行训练，因此设置了**gpu_id**参数。*图6.13*显示了使用这个新训练模型的结果。
- en: '![Figure 6.13 – Categories of the review examples using the pipeline with transformer](img/B22441_06_13.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 使用转换器管道的评论示例类别](img/B22441_06_13.jpg)'
- en: Figure 6.13 – Categories of the review examples using the pipeline with transformer
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 使用转换器管道的评论示例类别
- en: Now, the model classifies reviews correctly. Nice, isn’t it?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型能够正确地分类评论。不错，不是吗？
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: It’s fair to say that this chapter is one of the most important chapters of
    the book. Here, we learned about transfer learning and transformers, and how to
    use the spaCy configuration system to train the **TextCategorizer** component.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，这一章是本书最重要的章节之一。在这里，我们学习了迁移学习和转换器，以及如何使用spaCy配置系统来训练**TextCategorizer**组件。
- en: With the knowledge of how to prepare the data to train spaCy components and
    the knowledge of how to use the configuration files to define the training settings,
    you are now able to fine-tune any spaCy trainable component. That´s a huge step,
    congratulations!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解如何准备数据以训练 spaCy 组件以及如何使用配置文件来定义训练设置的知识，你现在能够微调任何 spaCy 可训练组件。这是一个巨大的进步，恭喜你！
- en: In this chapter, you learned about language models. In the next chapter, you
    will learn how to use LLMs, which are the most powerful NLP models today.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了关于语言模型的内容。在下一章中，你将学习如何使用 LLMs，它们是目前最强大的 NLP 模型。
- en: 'Part 3: Customizing and Integrating NLP Workflows'
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：定制和集成 NLP 工作流程
- en: This final section focuses on creating tailored NLP solutions and integrating
    spaCy with other tools and platforms. You’ll explore how to leverage LLMs, train
    custom models, and integrate spaCy projects with web applications to build end-to-end
    solutions.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍创建定制的 NLP 解决方案以及将 spaCy 与其他工具和平台集成。你将了解如何利用大型语言模型（LLMs）、训练自定义模型以及将 spaCy
    项目与网络应用程序集成以构建端到端解决方案。
- en: 'This part has the following chapters:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 7*](B22441_07.xhtml#_idTextAnchor102) , *Enhancing NLP Tasks Using
    LLMs with spacy-llm*'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B22441_07.xhtml#_idTextAnchor102) ，*使用 spacy-llm 增强NLP任务*'
- en: '[*Chapter 8*](B22441_08.xhtml#_idTextAnchor109) , *Training an NER Component
    with Your Own Data*'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B22441_08.xhtml#_idTextAnchor109) ，*使用您自己的数据训练 NER 组件*'
- en: '[*Chapter 9*](B22441_09.xhtml#_idTextAnchor123) , *Creating End-to-End spaCy
    Workflows with Weasel*'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B22441_09.xhtml#_idTextAnchor123) ，*使用 Weasel 创建端到端 spaCy 工作流程*'
- en: '[*Chapter 10*](B22441_10.xhtml#_idTextAnchor134) , *Training an Entity Linker
    Model with spaCy*'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B22441_10.xhtml#_idTextAnchor134) ，*使用 spaCy 训练实体链接器模型*'
- en: '[*Chapter 11*](B22441_11.xhtml#_idTextAnchor143) , *Integrating spaCy with
    Third-Party Libraries*'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B22441_11.xhtml#_idTextAnchor143) ，*将 spaCy 与第三方库集成*'
