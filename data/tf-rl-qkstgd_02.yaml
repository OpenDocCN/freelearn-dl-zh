- en: Temporal Difference, SARSA, and Q-Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差、SARSA 和 Q-Learning
- en: In the previous chapter, we looked at the basics of RL. In this chapter, we
    will cover **temporal difference** (**TD**) learning, SARSA, and Q-learning, which
    were very widely used algorithms in RL before deep RL became more common. Understanding
    these older-generation algorithms is essential if you want to master the field,
    and will also lay the foundation for delving into deep RL. We will therefore spend
    this chapter looking at examples using these older generation algorithms. In addition,
    we will also code some of these algorithms using Python. We will not be using
    TensorFlow for this chapter, as the problems do not involve any deep neural networks
    under study. However, this chapter will lay the groundwork for more advanced topics
    that we will cover in the subsequent chapters, and will also be our first coding
    experience of an RL algorithm. Specifically, this chapter will be our first deep
    dive into a standard RL algorithm, and how you can use it to train RL agents for
    a specific task. It will also be our first hands-on effort at RL, including both
    theory and practice.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了强化学习的基础知识。在本章中，我们将介绍**时间差**（**TD**）学习、SARSA 和 Q-learning，这些都是深度强化学习（deep
    RL）普及之前广泛使用的算法。理解这些旧一代算法对你掌握该领域非常重要，并且为深入研究深度强化学习奠定基础。因此，我们将在本章中通过使用这些旧一代算法的示例来进行学习。另外，我们还会使用
    Python 编写这些算法的代码。本章不会使用 TensorFlow，因为这些问题不涉及任何深度神经网络。然而，本章将为后续章节涉及的更高级主题奠定基础，并且是我们第一次亲手实现强化学习算法的编码实践。本章将是我们第一次深入探讨标准强化学习算法，并学习如何用它来训练智能体完成特定任务。它还将是我们第一次实践强化学习，包括理论与实践相结合。
- en: 'Some of the topics that will be covered in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖的部分主题如下：
- en: Understanding TD learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 TD 学习
- en: Learning SARSA
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 SARSA
- en: Understanding Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Q-learning
- en: Cliff walking with SARSA and Q-learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SARSA 和 Q-learning 进行悬崖行走
- en: Grid world with SARSA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SARSA 的网格世界
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Knowledge of the following will help you to better understand the concepts
    presented in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下知识将帮助你更好地理解本章所讲的概念：
- en: Python (version 2 or 3)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（版本 2 或 3）
- en: NumPy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: TensorFlow (version 1.4 or higher)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本 1.4 或更高版本）
- en: Understanding TD learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 TD 学习
- en: We will first learn about TD learning. This is a very fundamental concept in
    RL. In TD learning, the learning of the agent is attained by experience. Several
    trial episodes are undertaken of the environment, and the rewards accrued are
    used to update the value functions. Specifically, the agent will keep an update
    of the state-action value functions as it experiences new states/actions. The
    Bellman equation is used to update this state-action value function, and the goal
    is to minimize the TD error. This essentially means the agent is reducing its
    uncertainty of which action is the optimal action in a given state; it gains confidence
    on the optimal action in a given state by lowering the TD error.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将学习 TD 学习。这是强化学习中的一个非常基础的概念。在 TD 学习中，智能体的学习通过经验获得。智能体通过多次与环境的互动来进行试探，所获得的奖励用于更新值函数。具体来说，智能体会随着经验的积累，持续更新状态-动作值函数。贝尔曼方程被用来更新这一状态-动作值函数，目标是最小化
    TD 错误。这本质上意味着智能体正在减少对给定状态下最优动作的不确定性；它通过降低 TD 错误来提高对最优动作的信心。
- en: Relation between the value functions and state
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值函数与状态之间的关系
- en: The value function is an agent's estimate of how good a given state is. For
    instance, if a robot is near the edge of a cliff and may fall, that state is bad
    and must have a low value. On the other hand, if the robot/agent is near its final
    goal, that state is a good state to be in, as the rewards they will soon receive
    are high, and so that state will have a higher value.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数是智能体对给定状态好坏的估计。例如，如果一个机器人靠近悬崖边缘，可能会掉下去，那么这个状态就是坏的，值应该很低。另一方面，如果机器人/智能体接近最终目标，那么这个状态就是一个好状态，因为它们很快会获得高奖励，因此该状态的值会较高。
- en: 'The value function, *V*, is updated after reaching a *s[t] *state and receiving
    a *r[t]* reward from the environment. The simplest TD learning algorithm is called
    *TD(0)* and performs an update using the following equation where *α* is the learning
    rate and *0 ≤ α ≤ 1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数 *V* 在到达 *s[t]* 状态并从环境中获得 *r[t]* 奖励后会被更新。最简单的 TD 学习算法叫做 *TD(0)*，它使用以下方程进行更新，其中
    *α* 是学习率，*0 ≤ α ≤ 1*：
- en: '![](img/5af87999-0b2c-43b2-bb23-aaff79a6b0e9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5af87999-0b2c-43b2-bb23-aaff79a6b0e9.png)'
- en: Note that in some reference papers or books, the preceding formula will have
    *r[t]* instead of *r[t+1]*. This is just a difference in convention and is not
    an error; *r[t+1]* here denotes the reward received from *s[t]* stateand transitioning
    to *s[t+1]*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在某些参考论文或书籍中，上述公式可能会使用 *r[t]* 替代 *r[t+1]*。这只是约定上的差异，并不是错误；此处的 *r[t+1]* 表示从
    *s[t]* 状态获得的奖励，并且过渡到 *s[t+1]*。
- en: 'There is also another TD learning variant called *TD(λ)* that used eligibility
    traces *e(s),* which are a record of visiting a state. More formally, we perform
    a *TD(λ)* update as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种名为 *TD(λ)* 的 TD 学习变体，它使用可达性踪迹 *e(s)*，即访问状态的记录。更正式地说，我们按照以下方式执行 *TD(λ)* 更新：
- en: '![](img/79783f9a-d161-4dae-9041-68ded414b74a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79783f9a-d161-4dae-9041-68ded414b74a.png)'
- en: 'The eligibility traces are given by the following equation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可达性踪迹由以下公式给出：
- en: '![](img/dcb459a2-8c15-4304-8038-4473ebaef015.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcb459a2-8c15-4304-8038-4473ebaef015.png)'
- en: Here, *e(s) = 0* at *t = 0*. For each step the agent takes, the eligibility
    trace decreases by *γλ* for all states, and is incremented by *1* for the state
    visited in the current time step. Here, *0 ≤ λ ≤ 1*, and it is a parameter that
    decides how much of the credit from a reward is to be assigned to distant states.
    Next, we will look at the theory behind our next two RL algorithms, SARSA and
    Q-learning, both of which are quite popular in the RL community.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*e(s) = 0* 当 *t = 0*。对于代理所采取的每一步，可达性踪迹会对所有状态减少 *γλ*，并在当前时间步访问的状态上增加 *1*。这里，*0
    ≤ λ ≤ 1*，它是一个决定奖励的影响程度分配给远程状态的参数。接下来，我们将探讨我们下两个 RL 算法（SARSA 和 Q-learning）的理论，这两个算法在
    RL 社区中都非常流行。
- en: Understanding SARSA and Q-Learning
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 SARSA 和 Q-Learning
- en: In this section, we will learn about SARSA and Q-Learning and how can they are
    coded with Python. Before we go further, let's find out what SARSA and Q-Learning
    are. SARSA is an algorithm that uses the state-action Q values to update. These
    concepts are derived from the computer science field of dynamic programming, while Q-learning
    is an off-policy algorithm that was first proposed by Christopher Watkins in 1989,
    and is a widely used RL algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习 SARSA 和 Q-Learning，并探讨如何使用 Python 编写它们。在深入了解之前，让我们先弄清楚什么是 SARSA 和
    Q-Learning。SARSA 是一种使用状态-动作 Q 值进行更新的算法。这些概念源自计算机科学中的动态规划领域，而 Q-learning 是一种离政策算法，最早由
    Christopher Watkins 在 1989 年提出，并且是广泛使用的强化学习算法。
- en: Learning SARSA
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 SARSA
- en: '**SARSA** is another on-policy algorithm that was very popular, particularly
    in the 1990s. It is an extension of TD-learning, which we saw previously, and
    is an on-policy algorithm. SARSA keeps an update of the state-action value function,
    and as new experiences are encountered, this state-action value function is updated
    using the Bellman equation of dynamic programming. We extend the preceding TD
    algorithm to state-action value function, *Q(s[t],a[t])*, and this approach is
    called SARSA:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**SARSA** 是另一种非常流行的在政策算法，特别是在 1990 年代。它是 TD-learning 的扩展，我们之前已经看到过，并且是一个在政策算法。SARSA
    会更新状态-动作值函数，并且随着新经验的获得，这个状态-动作值函数会使用动态规划中的 Bellman 方程进行更新。我们将前述的 TD 算法扩展到状态-动作值函数
    *Q(s[t],a[t])*，这个方法被称为 SARSA：'
- en: '![](img/54c7d075-6c67-403c-8d81-bb5c4e19cb39.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54c7d075-6c67-403c-8d81-bb5c4e19cb39.png)'
- en: Here, from a given state *s[t]*, we take action *a[t]*, receive a reward *r[t+1]*,
    transition to a new state *s[t+1]*, and thereafter take an action *a[t+1]* that
    then continues on and on. This quintuple (*s[t]*, *a[t]*, *r[t+1]*, *s[t+1]*,
    *a[t+1]*) gives the algorithm the name SARSA. SARSA is an on-policy algorithm,
    as the same policy is updated as was used to estimate Q. For exploration, you
    can use, say, *ε*-greedy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，从给定的状态 *s[t]* 开始，我们采取行动 *a[t]*，获得奖励 *r[t+1]*，过渡到一个新的状态 *s[t+1]*，然后继续采取行动
    *a[t+1]*，如此循环下去。这个五元组 (*s[t]*, *a[t]*, *r[t+1]*, *s[t+1]*, *a[t+1]*) 赋予了算法 SARSA
    这个名字。SARSA 是一个在政策算法，因为它更新的是与估计 Q 时使用的相同策略。为了探索，你可以使用例如 *ε*-贪心策略。
- en: Understanding Q-learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Q-learning
- en: Q-learning is an off-policy algorithm that was first proposed by Christopher
    Watkins in 1989, and is a widely used RL algorithm. Q-learning, such as SARSA,
    keeps an update of the state-action value function for each state-action pair,
    and recursively updates it using the Bellman equation of dynamic programming as
    new experiences are collected. Note that it is an off-policy algorithm as it uses
    the state-action value function evaluated at the action, which will maximize the
    value. Q-learning is used for problems where the actions are discrete – for example,
    if we have the actions move north, move south, move east, move west, and we are
    to decide the optimum action in a given state, then Q-learning is applicable in
    such settings.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种脱离策略的算法，最早由Christopher Watkins在1989年提出，是一种广泛使用的强化学习算法。与SARSA一样，Q学习会保持每个状态-动作对的状态-动作值函数的更新，并使用动态编程的贝尔曼方程递归更新它，随着新经验的收集。请注意，它是一种脱离策略的算法，因为它使用的是评估过的状态-动作值函数，这个值将最大化。Q学习适用于动作是离散的情况——例如，如果我们有向北、向南、向东、向西的动作，并且我们要在某个给定状态下决定最优的动作，那么Q学习在这种设置下是适用的。
- en: 'In the classical Q-learning approach, the update is given as follows, where
    the max is performed over actions, that is, we choose the action a corresponding to
    the maximum value of Q at state *s[t+1]*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的Q学习方法中，更新方式如下，其中对动作执行max操作，即我们选择与状态*s[t+1]*对应的Q值最大值的动作a：
- en: '![](img/3536e18c-a044-4baa-a15b-c7703069b46a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3536e18c-a044-4baa-a15b-c7703069b46a.png)'
- en: The *α* is the learning rate, which is a hyper-parameter that the user can specify.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*是学习率，这是一个超参数，用户可以指定。'
- en: Before we code the algorithms in Python, let's find out what kind of problems
    will be considered.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们用Python编写算法之前，让我们先了解一下将考虑哪些类型的问题。
- en: Cliff walking and grid world problems
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 悬崖行走和网格世界问题
- en: Let's consider cliff walking and grid world problems. First, we will introduce
    these problems to you, then we will proceed on to the coding part. For both problems,
    we consider a rectangular grid with `nrows` (number of rows) and `ncols` (number
    of columns). We start from one cell to the south of the bottom left cell, and
    the goal is to reach the destination, which is one cell to the south of the bottom
    right cell.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑悬崖行走和网格世界问题。首先，我们将介绍这些问题，然后进入编码部分。对于这两个问题，我们考虑一个具有`nrows`（行数）和`ncols`（列数）的矩形网格。我们从底部左侧单元格正下方的一个单元格开始，目标是到达底部右侧单元格正下方的目标单元格。
- en: Note that the start and destination cells are not part of the `nrows` x `ncols`
    grid of cells. For the cliff walking problem, the cells to the south of the bottom
    row of cells, except for the start and destination cells, form a cliff where,
    if the agent enters, the episode ends with catastrophic fall into the cliff. Likewise,
    if the agent tries to leave the left, top, or right boundaries of the grid of
    cells, it is placed back in the same cell, that is, it is equivalent to taking
    no action.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，起点和终点单元格不属于`nrows` x `ncols`的网格单元格。在悬崖行走问题中，除了起点和终点单元格外，位于底部行单元格以南的单元格构成了悬崖，如果智能体进入这些单元格，剧集将以灾难性地掉入悬崖结束。同样，如果智能体试图离开网格单元格的左边、上边或右边边界，它将被放回到同一单元格，也就是说，它相当于没有采取任何行动。
- en: For the grid world problem, we do not have a cliff, but we have obstacles inside
    the grid world. If the agent tries to enter any of these obstacle cells, it is
    bounced back to the same cell from which it came. In both these problems, the
    goal is to find the optimum path from the start to the destination.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网格世界问题，我们没有悬崖，但网格世界内有障碍物。如果智能体试图进入任何这些障碍物单元格，它将被弹回到它原本所在的单元格。在这两个问题中，目标是找到从起点到终点的最优路径。
- en: So, let's dive on in!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，接下来我们深入研究吧！
- en: Cliff walking with SARSA
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SARSA进行悬崖行走
- en: 'We will now learn how to code the aforementioned equations in Python and implement
    the cliff walking problem with SARSA. First, let''s import the `numpy`, `sys`,
    and `matplotlib` packages in Python. If you have not used these packages in the
    past, there are several Packt books on these topics to help you come up to speed.
    Type the following command to install the required packages in a Linux Terminal:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习如何在Python中编码上述方程，并使用SARSA实现悬崖行走问题。首先，让我们导入Python中的`numpy`、`sys`和`matplotlib`包。如果你以前没有使用过这些包，市面上有几本关于这些主题的Packt书籍可以帮助你尽快掌握。请在Linux终端中输入以下命令来安装所需的包：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will now summarize the code involved to solve the grid world problem. In
    a Terminal, use your favorite editor (for example, gedit, emacs, or vi) to code
    the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将总结解决网格世界问题所涉及的代码。在终端中，使用你喜欢的编辑器（例如 gedit、emacs 或 vi）编写以下代码：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will use a 3 x 12 grid for the cliff walking problem, that is, `3` rows
    and `12` columns. We also have `4` actions to take at any cell. You can go north,
    east, south, or west:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个 3 x 12 的网格来解决悬崖行走问题，即有 `3` 行和 `12` 列。我们还有 `4` 个动作可以在每个单元格中执行。你可以向北、向东、向南或向西移动：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will consider `100000` episodes in total. For exploration, we will use ε-greedy
    with a value of *ε* = `0.1.` We will consider a constant value for *ε*, although
    the interested reader is encouraged to consider variable values for *ε* as well
    with its value slowly annealed to zero over the course of the episodes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑总共 `100000` 次回合。为了进行探索，我们将使用 ε-贪婪策略，*ε* 的值为 `0.1`。我们将考虑一个常数值的 *ε*，尽管有兴趣的读者可以考虑使用变化的
    *ε* 值，并在回合过程中将其逐渐衰减至零。
- en: 'The learning rate, *α*, is chosen as `0.1`, and the discount factor *γ* = `0.95`
    is used, which are typical values for this problem:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 *α* 选择为 `0.1`，折扣因子 *γ* 选择为 `0.95`，这些是解决该问题的典型值：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will next assign values for the rewards. For any normal action that does
    not fall into the cliff, the reward is `-1`; if the agent falls down the cliff,
    the reward is `-100`; for reaching the destination, the reward is also `-1`. Feel
    free to explore other values for these rewards later, and investigate its effect
    on the final Q values and path taken from start to destination:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为奖励分配值。对于任何不掉入悬崖的正常动作，奖励是 `-1`；如果代理掉入悬崖，奖励是 `-100`；到达目的地的奖励也是 `-1`。稍后可以探索这些奖励的其他值，并研究其对最终
    Q 值以及从起点到终点路径的影响：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Q values for state-action pairs are initialized to zero. We will use a
    NumPy array for Q, which is `nrows` x `ncols` x `nact`, that is, we have a `nact`
    number of entries for each cell, and we have `nrows` x `ncols` total number of
    cells:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 状态-动作对的 Q 值初始化为零。我们将使用一个 NumPy 数组来存储 Q 值，形状为 `nrows` x `ncols` x `nact`，即每个单元格有
    `nact` 个条目，整个网格有 `nrows` x `ncols` 个单元格：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will define a function to make the agent go to the start location, which
    has (*x, y*) co-ordinates of (`x=0`, `y=nrows`):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个函数，使代理回到起始位置，该位置的 (*x, y*) 坐标为（`x=0`，`y=nrows`）：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we define a function to take a random action, where we define `a = 0`
    for moving `top/north`, `a = 1` for moving `right/east`, `a = 2` for moving `bottom/south`,
    and `a = 4` for moving `left/west`. Specifically, we will use NumPy''s `random.randint()`
    function, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来执行随机动作，在该函数中我们定义 `a = 0` 表示向 `top/north` 移动，`a = 1` 表示向 `right/east`
    移动，`a = 2` 表示向 `bottom/south` 移动，`a = 4` 表示向 `left/west` 移动。具体来说，我们将使用 NumPy 的
    `random.randint()` 函数，如下所示：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will now define the `move` function, which will take a given (*x, y*) location
    of the agent and the current action, `a`, and then will perform the action. It
    will return the new location of the agent after taking the action, (*x1, y1*),
    as well as the state of the agent, which we define as `state = 0` for the agent
    to be `OK` after taking the action; `state = 1` for reaching the destination;
    and `state = 2` for falling into the cliff. If the agent leaves the domain through
    the left, top, or right, it is sent back to the same grid (equivalent to taking
    no action):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义 `move` 函数，该函数将接受代理的给定位置 (*x, y*) 和当前的动作 `a`，然后执行该动作。它将返回执行该动作后代理的新位置
    (*x1, y1*) 以及代理的状态，我们定义 `state = 0` 表示代理在执行动作后 `OK`；`state = 1` 表示到达目的地；`state
    = 2` 表示掉入悬崖。如果代理通过左侧、顶部或右侧离开网格，它将被送回到同一网格（等同于没有执行任何动作）：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will next define the `exploit` function, which will take the (*x, y*) location
    of the agent and take the greedy action based on the Q values, that is, it will
    take the `a` action that has the highest Q value at that (*x, y*) location. We
    will do this using NumPy''s `np.argmax()`. If we are at the start location, we
    go north (`a = 0`); if we are one step away from the destination, we go south
    (`a = 2`):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义 `exploit` 函数，该函数将接受代理的 (*x, y*) 位置，并根据 Q 值采取贪婪的动作，即选择在该 (*x, y*) 位置具有最高
    Q 值的 `a` 动作。如果我们处于起始位置，我们将向北移动（`a = 0`）；如果我们距离目的地一步之遥，我们将向南移动（`a = 2`）：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will perform the Bellman update using the following `bellman()` function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下 `bellman()` 函数执行贝尔曼更新：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will then define a function to explore or exploit, depending on a random
    number less than *ε*, the parameter we use in the ε-greedy exploration strategy.
    For this, we will use NumPy''s `np.random.uniform()`, which will output a real
    random number between zero and one:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们将定义一个函数，根据随机数是否小于 *ε* 来选择探索或利用，这个参数是我们在 ε-贪心探索策略中使用的。为此，我们将使用 NumPy 的 `np.random.uniform()`，它将输出一个介于零和一之间的实数随机数：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We now have all the functions required for the cliff walking problem. So, we
    will loop over the episodes and for each episode we start at the starting location,
    then explore or exploit, then we move the agent one step depending on the action.
    Here is the Python code for this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了解决悬崖行走问题所需的所有函数。因此，我们将对回合进行循环，在每个回合中，我们从起始位置开始，接着进行探索或利用，然后根据动作移动智能体一步。以下是这部分的
    Python 代码：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We perform the Bellman update based on the rewards received; note that this
    is based on the equations presented earlier in this chapter in the theory section.
    We stop the episode if we reach the destination or fall down the cliff; if not,
    we continue the exploration or exploitation strategy for one more step, and this
    goes on and on. The `state` variable in the following code takes the `1` value
    for reaching the destination, takes the value `2` for falling down the cliff,
    and is `0` otherwise:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据所获得的奖励执行贝尔曼更新；请注意，这基于本章理论部分之前呈现的方程。如果我们到达目的地或掉下悬崖，我们就停止该回合；如果没有，我们继续进行一次探索或利用策略，然后继续下去。以下代码中的`state`变量取值为`1`表示到达目的地，取值为`2`表示掉下悬崖，否则为`0`：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code will complete all the episodes, and we now have the converged
    values of Q. We will now plot this using `matplotlib` for each of the actions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将完成所有回合，现在我们已经得到了收敛的 Q 值。接下来，我们将使用 `matplotlib` 绘制每个动作的 Q 值：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we will do a path planning using the preceding converged Q values.
    That is, we will plot the exact path the agent will take from start to finish
    using the final converged Q values. For this, we will create a variable called
    `path`, and store values for it tracing the `path`. We will then plot it using
    `matplotlib` as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用之前收敛的 Q 值进行路径规划。也就是说，我们将绘制出智能体从起点到终点的准确路径，使用最终收敛的 Q 值。为此，我们将创建一个名为`path`的变量，并为其存储追踪`路径`的值。然后，我们将使用
    `matplotlib` 来绘制它，如下所示：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That's it. We have completed the coding required for the cliff walking problem
    with SARSA. We will now view the results. In the following screenshot, we present
    the Q values for each of the actions (going north, east, south, or west) at each
    of the locations in the grid. As per the legend, yellow represents high Q values
    and violet represents low Q values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们已经完成了使用 SARSA 解决悬崖行走问题所需的编码。现在我们来查看结果。在下面的屏幕截图中，我们展示了在网格中每个位置的每个动作（向北、向东、向南或向西）的
    Q 值。如图例所示，黄色表示高 Q 值，紫色表示低 Q 值。
- en: 'SARSA clearly tries to avoid the cliff by choosing to not go south when the
    agent is just one step to the north of the cliff, as is evident from the large
    negative Q values for the south action:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从 SARSA 的结果来看，它明显地通过选择不向南走来避免掉进悬崖，尤其是在智能体正好位于悬崖北侧时，南方动作的负 Q 值很大：
- en: '![](img/505d9930-d182-4645-a9a4-014be72a8096.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/505d9930-d182-4645-a9a4-014be72a8096.png)'
- en: 'Figure 1: Q values for the cliff walking problem using SARSA'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用 SARSA 进行悬崖行走问题的 Q 值
- en: 'We will next plot the path taken by the agent from start to finish in the following
    screenshot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将绘制智能体从起点到终点的路径，见下图：
- en: '![](img/65270e16-dedb-4ed8-88e7-d5c04a3c5497.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65270e16-dedb-4ed8-88e7-d5c04a3c5497.png)'
- en: 'Figure 2: Path taken by the agent using SARSA'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 SARSA 的智能体路径
- en: The same cliff walking problem will now be investigated using Q-learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 Q-learning 来研究相同的悬崖行走问题。
- en: Cliff walking with Q-learning
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q-learning 进行悬崖行走
- en: We will now repeat the same cliff walking problem, albeit using Q-learning in
    lieu of SARSA. Most of the code is the same as was used for SARSA, except for
    a few differences that will be summarized here. Since Q-learning uses a greedy
    action selection strategy, we will use a function for this as follows to compute
    the value of the maximum value of Q at a given location. Most of the code is the
    same as in the previous section, so we will only specify the changes to be made.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将重复相同的悬崖行走问题，尽管这次我们使用 Q-learning 来代替 SARSA。大部分代码与 SARSA 相同，除了几个总结的差异。由于
    Q-learning 使用贪心策略选择动作，我们将使用以下函数来计算给定位置的最大 Q 值。大部分代码与前一节相同，因此我们只指定需要更改的部分。
- en: Now, let's code cliff walking with Q-learning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用Q-learning编写悬崖行走代码。
- en: 'The `max_Q()` function is defined as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_Q()`函数定义如下：'
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will compute the Q value at the new state using the `max_Q()` function defined
    previously:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用先前定义的`max_Q()`函数来计算新状态下的Q值：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In addition, the action choosing whether it is exploration or exploitation
    is done inside the `while` loop as we choose actions greedily when exploiting:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，选择探索还是利用的动作是在`while`循环内完成的，因为在利用时我们会贪婪地选择动作：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'That''s it for coding Q-learning. We will now apply this to solve the cliff
    walking problem and present the Q values for each of the actions, and the path
    traced by the agent to go from start to finish, which are shown in the following
    screenshots:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是编写Q-learning代码的全部内容。我们将应用它来解决悬崖行走问题，并展示每个动作的Q值以及代理从起点到终点所走的路径，具体内容将在以下截图中显示：
- en: '![](img/f5369a08-d1bf-4c24-a6cd-402953bf2d65.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5369a08-d1bf-4c24-a6cd-402953bf2d65.png)'
- en: 'Figure 3: Q values for the cliff walking problem using Q-learning'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用Q-learning进行悬崖行走问题的Q值
- en: 'As evident, the path traced is now different for Q-learning vis-à-vis SARSA.
    Since Q-learning is a greedy strategy, the agent now takes a path close to the
    cliff at the bottom of the following screenshot (*Figure 4*), as it is the shortest
    path:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，对于Q-learning和SARSA，路径有所不同。由于Q-learning是一种贪婪策略，代理现在选择接近悬崖的路径（见下图*图4*），因为它是最短路径：
- en: '![](img/1129e881-0d54-4ac3-9dc7-bf086e65ba38.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1129e881-0d54-4ac3-9dc7-bf086e65ba38.png)'
- en: 'Figure 4: Path traced by the agent for the cliff walking problem using Q-learning'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用Q-learning解决悬崖行走问题时，代理所走的路径
- en: On the other hand, since SARSA is more far-sighted, and so chooses the safe
    but longer path that is the top row of cells (see *Figure 2*).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于SARSA更具远见，因此它选择了安全但较长的路径，即位于顶部的单元格行（见*图2*）。
- en: Our next problem is the grid world problem, where we must navigate a grid. We
    will code this in SARSA.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个问题是网格世界问题，我们必须在一个网格中进行导航。我们将在SARSA中实现这个问题。
- en: Grid world with SARSA
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SARSA的网格世界
- en: We will next consider the grid world problem, and we will use SARSA to solve
    it. We will introduce obstacles in place of a cliff. The goal of the agent is
    to navigate the grid world from start to destination by avoiding the obstacles.
    We will store the coordinates of the obstacle cells in the `obstacle_cells` list,
    where each entry is the (*x, y*) coordinate of the obstacle cell.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将考虑网格世界问题，并使用SARSA来解决它。我们将在悬崖位置引入障碍物。代理的目标是从起点导航到目标，同时避开障碍物。我们将在`obstacle_cells`列表中存储障碍物单元格的坐标，每个条目表示障碍物单元格的(*x,
    y*)坐标。
- en: 'Here is a summary of the steps involved in this task:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此任务涉及的步骤总结：
- en: Most of the code is the same as previously used, the differences will be summarized
    here
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大部分代码与之前使用的相同，区别将在此总结
- en: Place obstacles in the grid
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网格中放置障碍物
- en: The `move()` function has to also look for obstacles in the grid
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`move()`函数还需要在网格中查找障碍物'
- en: Plot Q values and the path traced by the agent
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制Q值以及代理所走的路径
- en: 'Here, we will start coding the algorithm in Python:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将开始用Python编写算法：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `move()` function will now change, as we have to also look for obstacles.
    If the agent ends up in one of the obstacle cells it is pushed back to where it
    came from, as shown in the following code snippet:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`move()`函数现在将发生变化，因为我们还需要检查障碍物。如果代理最终进入某个障碍物单元格，它将被推回到原来的位置，如以下代码片段所示：'
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'That''s it for coding grid world with SARSA. The Q-values and the path taken
    are shown in following diagrams:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是用SARSA编写网格世界代码的全部内容。Q值和所走的路径将在以下图表中显示：
- en: '![](img/6fea0ff6-2606-4c75-9d97-0e2b04560463.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fea0ff6-2606-4c75-9d97-0e2b04560463.png)'
- en: 'Figure 5: Q-values for each of the actions for the grid world problem using
    SARSA'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：使用SARSA解决网格世界问题时，每个动作的Q值
- en: 'As we can see in the following diagram, the agent navigates around the obstacles
    to reach its destination:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，代理绕过障碍物到达目的地：
- en: '![](img/ce653ae1-9920-48d1-87d8-7de42b1930c7.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce653ae1-9920-48d1-87d8-7de42b1930c7.png)'
- en: 'Figure 6: Path traced by the agent for the grid world problem using SARSA'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用SARSA解决网格世界问题时代理所走的路径
- en: Grid world with Q-learning is not a straightforward problem to attempt, as the
    greedy strategy used will not enable the agent to avoid repeated actions easily
    at a given state. Convergence is typically very slow, and so it will be avoided
    for now.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Q-learning解决网格世界问题并非易事，因为采用的贪婪策略无法轻易避免代理在某一状态下重复执行相同动作。通常，收敛速度非常慢，因此暂时避免使用。
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we looked at the concept of TD. We also learned about our
    first two RL algorithms: Q-learning and SARSA. We saw how you can code these two
    algorithms in Python and use them to solve the cliff walking and grid world problems.
    These two algorithms give us a good understanding of the basics of RL and how
    to transition from theory to code. These two algorithms were very popular in the
    1990s and early 2000s, before deep RL gained prominence. Despite that, Q-learning
    and SARSA still find use in the RL community today.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们探讨了时序差分（TD）概念。我们还学习了前两个RL算法：Q学习和SARSA。我们看到如何用Python编写这两个算法，并用它们解决悬崖行走和网格世界问题。这两个算法使我们对RL的基础有了良好的理解，并展示了如何从理论过渡到代码。这两个算法在1990年代和2000年代初期非常流行，在深度强化学习崭露头角之前。尽管如此，Q学习和SARSA在RL社区中至今仍然被使用。
- en: In the next chapter, we will look at the use of deep neural networks in RL that
    gives rise to deep RL. We will see a variant of Q-learning called **Deep Q-Networks**
    (**DQNs**) that will use a neural network instead of a tabular state-action value
    function, which we saw in this chapter. Note that only problems with small number
    of states and actions are suited to Q-learning and SARSA. When we have a large
    number of states and/or actions, we encounter what is called as the Curse of Dimensionality,
    where a tabular approach will be unfeasible due to excessive memory use; in these
    problems, DQN is better suited, and will be the crux of the next chapter.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨深度神经网络在强化学习（RL）中的应用，进而形成深度强化学习（deep RL）。我们将看到Q学习的一个变种，称为**深度Q网络**（**DQNs**），它将使用神经网络代替我们在本章中看到的表格型状态-动作值函数。请注意，只有状态和动作数量较少的问题才适合Q学习和SARSA。当状态和/或动作数量较多时，我们会遇到所谓的“维度灾难”（Curse
    of Dimensionality），在这种情况下，由于过度的内存使用，表格方法变得不可行；在这些问题中，DQN更为适用，并且将是下一章的核心内容。
- en: Further reading
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Reinforcement Learning: an Introduction* by *Richard Sutton and Andrew Barto*,
    2018'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习：导论* 由 *理查德·萨顿 和 安德鲁·巴尔托* 编著，2018年'
