- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: TensorFlow is at the center of developing **Machine Learning** (**ML**) solutions.
    It is an ecosystem that can support all of the different stages in the life cycle
    of an ML project, from the early prototyping up until the productionization of
    the model. TensorFlow provides various reusable building blocks, allowing you
    to build not just the simplest but also the most complex deep neural networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是开发**机器学习**（**ML**）解决方案的核心。它是一个生态系统，可以支持 ML 项目生命周期中的各个阶段，从早期的原型设计到模型的生产化。TensorFlow
    提供了各种可重用的构建模块，允许您构建不仅仅是最简单的，甚至是最复杂的深度神经网络。
- en: Who this book is for
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适用对象
- en: This book is aimed at novice - to intermediate-level users of TensorFlow. The
    reader may be from academia doing cutting-edge research on ML or an industry practioner
    using ML in their job. You will get the most benefit from this book if you have
    some basic familiarity with TensorFlow (or a similar framework like Pytorch) already.
    This will help you to grasp the concepts and use cases discussed in the book quicker.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书面向 TensorFlow 初学者到中级用户。读者可能来自学术界，从事机器学习的前沿研究，或者来自工业界，将机器学习应用于工作中。如果您已经对 TensorFlow（或类似的框架如
    Pytorch）有一些基础了解，您将从本书中获得最大的收益。这将帮助您更快地掌握本书讨论的概念和使用案例。
- en: What this book covers
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概述
- en: '*Chapter 1*, *Introduction to Natural Language Processing*, explains what natural
    language processing is and the kinds of tasks it may entail. We then discuss how
    an NLP task is solved using traditional methods. This paves the way to discuss
    how deep learning is used in NLP and what the benefits are. Finally, we discuss
    the installation and usage of the technical tools in this book.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一章*，*自然语言处理简介*，解释了自然语言处理是什么，以及它可能涉及的任务类型。然后我们讨论了如何使用传统方法解决 NLP 任务。这为讨论如何在
    NLP 中使用深度学习及其优势铺平了道路。最后，我们讨论了本书中使用的技术工具的安装和使用。'
- en: '*Chapter 2*, *Understanding TensorFlow 2*, provides you with a sound guide
    to writing programs and running them in TensorFlow 2\. This chapter will first
    offer an in-depth explanation of how TensorFlow executes a program. This will
    help you to understand the TensorFlow execution workflow and feel comfortable
    with TensorFlow terminology. Next, we will discuss various building blocks in
    TensorFlow and useful operations that are available. We will finally discuss how
    all this knowledge of TensorFlow can be used to implement a simple neural network
    to classify images of handwritten digits.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二章*，*理解 TensorFlow 2*，为您提供了编写程序和在 TensorFlow 2 中运行程序的完整指南。本章将首先深入解释 TensorFlow
    如何执行程序。这将帮助您理解 TensorFlow 的执行流程，并熟悉 TensorFlow 的术语。接下来，我们将讨论 TensorFlow 中的各种构建模块和可用的有用操作。最后，我们将讨论如何利用所有这些
    TensorFlow 知识来实现一个简单的神经网络，用于分类手写数字图像。'
- en: '*Chapter 3*, *Word2vec – Learning Word Embeddings*, introduces Word2vec—a method
    to learn numerical representations of words that reflect the semantics of the
    words. But before diving straight into Word2vec techniques, we first discuss some
    classical approaches used to represent words, such as one-hot-encoded representations,
    and the **Term Frequency-Inverse Document Frequency** (**TF-IDF**) frequency method.
    Following this, we will move on to a modern tool forlearning word vectors known
    as Word2vec, which uses a neural network to learn word representations. We will
    discuss two popular Word2vec variants: skip-gram and the **Continuous Bag-of-Words**
    (**CBOW**) model. Finally, we will visualize the word representations learned
    using a dimensionality reduction technique to map the vectors to a more interpretable
    two-dimensional surface.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*第三章*，*Word2vec - 学习词嵌入*，介绍了 Word2vec——一种学习反映单词语义的数值表示方法。但在直接进入 Word2vec 技术之前，我们首先讨论了一些用于表示单词的经典方法，如独热编码表示法，以及**词频-逆文档频率**（**TF-IDF**）频率方法。接下来，我们将介绍一种现代的学习词向量的工具，即
    Word2vec，它利用神经网络学习单词表示。我们将讨论两种流行的 Word2vec 变体：skip-gram 和**连续袋词模型**（**CBOW**）。最后，我们将使用降维技术可视化所学习到的单词表示，将向量映射到更易于解释的二维平面。'
- en: '*Chapter 4*, *Advanced Word Vector Algorithms*, starts with a more recent word
    embedding learning technique known as GloVe, which incorporates both global and
    local statisticsin text data to find word vectors. Next, we will learn about one
    of the modern, more sophisticated techniques for generating dynamic word representations
    based on the context of a word, known as ELMo.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*第4章*，*高级词向量算法*，介绍了一种较新的词嵌入学习技术——GloVe，它结合了文本数据中的全局和局部统计信息来寻找词向量。接下来，我们将学习一种现代的、更复杂的技术，即基于词语上下文生成动态词表示的技术，称为
    ELMo。'
- en: '*Chapter 5*, *Sentence Classification with Convolutional Neural Networks*,
    introduces you to **Convolutional Neural Networks** (**CNNs**). CNNs are a powerful
    family of deep models that can leverage the spatial structure of an input to learn
    from data. In other words, a CNN can process images in their two-dimensional form,
    whereas a multilayer perceptron needs the image to be unwrapped to a one-dimensional
    vector. We will first discuss various operations that are undergone in CNNs, such
    as the convolution and pooling operations, in detail. Then, we will see an example
    where we will learn to classify images of clothes with a CNN. Then, we will transition
    into an application of CNNs in NLP. More precisely, we will be investigating how
    to apply a CNN to classify sentences, where the task is to classify if a sentence
    is about a person, location, object, and so on.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*第5章*，*卷积神经网络的句子分类*，介绍了**卷积神经网络**（**CNNs**）。CNNs 是一类强大的深度学习模型，它能够利用输入数据的空间结构进行学习。换句话说，CNN
    可以处理二维形式的图像，而多层感知机则需要将图像展开成一维向量。我们将首先详细讨论 CNN 中涉及的各种操作，例如卷积操作和池化操作。接下来，我们将通过一个例子，学习如何使用
    CNN 对衣物图像进行分类。然后，我们将进入 CNN 在自然语言处理（NLP）中的应用。更准确地说，我们将研究如何将 CNN 应用于句子分类任务，其中任务是将句子分类为与人、地点、物体等相关。'
- en: '*Chapter 6*, *Recurrent Neural Networks*, focuses on introducing **Recurrent
    Neural Networks** (**RNNs**) and using RNNs for language generation. RNNs are
    different from feed-forward neural networks (for example, CNNs) as RNNs have memory.
    The memory is stored as a continuously updated system state. We will start with
    a representation of a feed-forward neural network and modify that representation
    to learn from sequences of data instead of individual data points. This process
    will transform the feed-forward network to an RNN. This will be followed by a
    technical description of the exact equations used for computations within the
    RNN. Next, we will discuss the optimization process of RNNs that is used to update
    the RNN’s weights. Thereafter we will iterate through different types of RNNs
    such as one-to-one RNNs and one-to-many RNNs. We will then discuss a popular application
    of RNNs, which is to identify named entities in text (for example, Person name,
    Organization, and so on). Here, we’ll be using a basic RNN model to learn.Next,
    we will enhance our model further by incorporating embeddings at different scales
    (for example, token embeddings and character embeddings). The token embeddings
    are generated through an embedding layer, where the character embeddings are generated
    using a CNN. We will then analyze the new model’s performance on the named entity
    recognition task.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6章*，*递归神经网络*，重点介绍了**递归神经网络**（**RNNs**）及其在语言生成中的应用。RNN 与前馈神经网络（例如 CNNs）不同，因为
    RNN 具有记忆。该记忆以持续更新的系统状态形式存储。我们将从前馈神经网络的表示开始，并修改该表示，使其能够从数据序列中学习，而非单个数据点。这个过程将把前馈网络转化为
    RNN。接着，我们将详细描述 RNN 内部用于计算的精确方程。然后，我们将讨论用于更新 RNN 权重的 RNN 优化过程。随后，我们将遍历不同类型的 RNN，例如一对一
    RNN 和一对多 RNN。接下来，我们将讨论 RNN 的一个流行应用，即识别文本中的命名实体（例如人名、组织名等）。在这里，我们将使用一个基础的 RNN 模型进行学习。然后，我们将通过在不同尺度（例如标记嵌入和字符嵌入）中引入嵌入来进一步增强我们的模型。标记嵌入通过嵌入层生成，而字符嵌入则通过
    CNN 生成。最后，我们将分析新模型在命名实体识别任务中的表现。'
- en: '*Chapter 7*, *Understanding* *Long Short-Term Memory Networks*, discusses **Long
    Short-Term Memory networks** ( **LSTMs**) by initially providing an intuitive
    explanation of how these models work and progressively diving into the technical
    details required to implement them on your own. Standard RNNs suffer from the
    crucial limitation of the inability to persist long-term memory. However, advanced
    RNN models (for example, LSTMs and **Gated Recurrent Units** (**GRUs**)) have
    been proposed, which can remember sequences for a large number of time steps.
    We will also examine how exactly LSTMs alleviate the problem of persisting long-term
    memory (this is known as the vanishing gradient problem). We will then discuss
    several modifications that can be used to improve LSTM models further, such as
    predicting several time steps ahead at once and reading sequences both forward
    and backward. Finally, we will discuss several variants of LSTM models such as
    GRUs and LSTMs with peephole connections.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*第7章*，*理解长短期记忆网络*，讨论了**长短期记忆网络**（**LSTM**），首先通过直观的解释让你理解这些模型是如何工作的，然后逐步深入技术细节，帮助你自己实现它们。标准的RNN模型存在一个重要的局限性——无法保持长期记忆。然而，已经提出了先进的RNN模型（例如LSTM和**门控循环单元**（**GRU**）），它们能够记住多个时间步的序列。我们还将探讨LSTM是如何缓解长期记忆保持问题的（这被称为梯度消失问题）。接着，我们将讨论几种可以进一步改进LSTM模型的修改方法，例如一次性预测多个时间步并且同时读取前后序列。最后，我们将讨论LSTM模型的几种变体，例如GRU和带窥视连接的LSTM。'
- en: '*Chapter 8*, *Applications of LSTM – Generating Text*, explains how to implement
    the LSTMs, GRUs, and LSTMs with peephole connections discussed in *Chapter 7*,
    *Understanding* *Long Short-Term Memory Networks*. Furthermore, we will compare
    the performance of these extensions both qualitatively and quantitatively. We
    will also discuss how to implement some of the extensions examined in *Chapter
    7*, *Understanding* *Long Short-Term Memory Networks*, such as predicting several
    time steps ahead (known as beam search) and using word vectors as inputs instead
    of one-hot-encoded representations.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*第8章*，*LSTM的应用——生成文本*，解释了如何实现*第7章*中讨论的LSTM、GRU和带窥视连接的LSTM，*理解长短期记忆网络*。此外，我们还将从定性和定量两个方面比较这些扩展的性能。我们还将讨论如何实现*第7章*中考察的一些扩展，例如预测多个时间步（即束搜索），以及使用词向量作为输入，而不是使用独热编码表示。'
- en: '*Chapter 9*, *Sequence-to-Sequence Learning – Neural Machine Translation*,
    discusses machine translation, which has gained a lot of attention both due to
    the necessity of automating translation and the inherent difficulty of the task.
    We start the chapter with a brief historical flashback explaining how machine
    translation was implemented in the early days. This discussion ends with an introduction
    to **Neural Machine Translation** (**NMT**) systems. We will see how well current
    NMT systems are doing compared to old systems (such as statistical machine translation
    systems), which will motivate us to learn about NMT systems. Afterward, we will
    discuss the concepts underpinning the design of NMT systems and continue with
    the technical details. Then, we will discuss the evaluation metric we use to evaluate
    our system. Following this, we will investigate how we can implement an English-to-German
    translator from scratch. Next, we will learn about ways to improve NMT systems.
    We will look at one of those extensions in detail, called the attention mechanism.
    The attention mechanism has become essential in sequence-to-sequence learning
    problems.Finally, we will compare the performance improvement obtained with the
    attention mechanism and analyze the reasons behind the performance gain. This
    chapter concludes with a section on how the same concept of NMT systems can be
    extended to implement chatbots. Chatbots are systems that can communicate with
    humans and are used to fulfill various customer requests.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*第9章*，*序列到序列学习—神经机器翻译*，讨论了机器翻译，这一领域由于自动化翻译的必要性以及任务本身的固有难度而引起了大量关注。我们从简短的历史回顾开始，解释了机器翻译在早期是如何实现的。这一讨论以对**神经机器翻译**（**NMT**）系统的介绍结束。我们将看到当前的NMT系统与老旧系统（如统计机器翻译系统）相比表现如何，这将激励我们进一步学习NMT系统。接下来，我们将讨论支撑NMT系统设计的基本概念，并继续讲解技术细节。然后，我们将讨论用于评估系统的评估指标。接下来，我们将研究如何从零开始实现一个英德翻译器。然后，我们将学习如何改进NMT系统。我们将详细介绍其中的一种扩展，即注意力机制。注意力机制已成为序列到序列学习问题中的关键因素。最后，我们将比较应用注意力机制后性能的提升，并分析性能提升的原因。本章的最后部分将讲解如何将NMT系统的相同概念扩展应用于聊天机器人。聊天机器人是能够与人类进行交流的系统，广泛用于满足各种客户需求。'
- en: '*Chapter 10*, *Transformers*, discusses Transformers, the latest breakthrough
    in the domain of NLP which have outperformed many other previous state-of-the-art
    models. In this chapter, we will use the Hugging Face Transformers library to
    use pre-trained models for downstream tasks with ease. In this chapter, we will
    learn about the Transformer architecture in depth. This discussion will lead into
    a popular Transformer model called BERT, which we will use to solve a problem
    of question answering. We will discuss specific components found in BERT to effectively
    use it for the application. Next, we will train the model on a popular question-answer
    dataset known as SQUAD. Finally, we will evaluate the model on a test dataset
    and use the trained model to generate answers for unseen questions.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*第10章*，*Transformer*，讨论了Transformer，这一在自然语言处理领域的最新突破，已超越了许多先前的先进模型。在本章中，我们将使用Hugging
    Face的Transformers库，轻松地利用预训练模型进行下游任务。在本章中，我们将深入了解Transformer架构。接下来，我们将介绍一种流行的Transformer模型，称为BERT，使用它来解决问题解答任务。我们将讨论BERT中一些特定的组件，以便有效地将其应用于实践。然后，我们将在一个流行的问答数据集SQUAD上训练模型。最后，我们将对模型进行评估，并使用训练好的模型为未见过的问题生成答案。'
- en: '*Chapter 11*, *Image Captioning with Transformers*, looks at another exciting
    application, where Transformers are used to generate captions (that is, descriptions)
    for images. This application is interesting because it shows us how to combine
    two different types of models as well as how to learn with multimodal data (for
    example, images and text). Here, we will use a pre-trained Vision Transformer
    model that generates a rich hidden representation for a given image. This representation,
    along with caption tokens, is fed to a text-based Transformer model. The text-based
    Transformer predicts the next caption token, given previous caption tokens. Once
    the model is trained, we will evaluate the captions generated by our model, both
    qualitatively and quantitatively. We will also discuss some of the popular metrics
    used to measure the quality of sequences such as image captions.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 11 章*，*使用 Transformers 进行图像标题生成*，探讨了另一种激动人心的应用，使用 Transformers 生成图像的标题（即描述）。这个应用有趣之处在于，它展示了如何结合两种不同类型的模型，以及如何使用多模态数据（例如图像和文本）进行学习。在这里，我们将使用一个预训练的
    Vision Transformer 模型，该模型为给定的图像生成丰富的隐藏表示。这个表示与标题令牌一起输入到基于文本的 Transformer 模型中。基于文本的
    Transformer 根据之前的标题令牌预测下一个标题令牌。一旦模型训练完成，我们将定性和定量地评估模型生成的标题。我们还将讨论一些用于衡量序列质量（如图像标题）的常用指标。'
- en: '*Appendix* *A:* *Mathematical Foundations and Advanced TensorFlow*, introduces
    various mathematical data structures (for example, matrices) and operations (for
    example, a matrix inverse). We will also discuss several important concepts in
    probability. Finally, we will walk you through a guide aimed at teaching you to
    use TensorBoard to visualize word embeddings. TensorBoard is a handy visualization
    tool that is shipped with TensorFlow. This can be used to visualize and monitor
    various variables in your TensorFlow client.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录 A：* *数学基础与高级 TensorFlow*，介绍了各种数学数据结构（例如矩阵）和运算（例如矩阵求逆）。我们还将讨论概率中的一些重要概念。最后，我们将引导你学习如何使用
    TensorBoard 可视化词嵌入。TensorBoard 是一个随 TensorFlow 提供的实用可视化工具，可以用来可视化和监控 TensorFlow
    客户端中的各种变量。'
- en: To get the most out of this book
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何最大化本书的学习效果
- en: To get the most out of this book you need a basic understanding of TensorFlow
    or a similar framework such as PyTorch. Familiarity obtained through basic TensorFlow
    tutorials that are freely available in the web should suffice to get started on
    this book.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化本书的学习效果，你需要对 TensorFlow 或类似框架（如 PyTorch）有基本了解。通过网上免费提供的基础 TensorFlow 教程获得的熟悉程度应足以开始阅读本书。
- en: A basic knowledge of mathematics, including an understanding of n-dimensional
    tensors, matrix multiplication, and so on, will also prove invaluable throughout
    this book. Finally, you need an enthusiasm for learning about cutting edge machine
    learning that is setting the stage for modern NLP solutions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，基本的数学知识，包括对 n 维张量、矩阵乘法等的理解，将在学习过程中极为宝贵。最后，你需要对学习前沿的机器学习技术充满热情，这些技术正为现代自然语言处理解决方案奠定基础。
- en: Download the example code files
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: The code bundle for the book is hosted on GitHub at https://github.com/thushv89/packt_nlp_tensorflow_2\.
    We also have other code bundles from our rich catalog of books and videos available
    at https://github.com/PacktPublishing/. Check them out!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包托管在 GitHub 上，网址是 https://github.com/thushv89/packt_nlp_tensorflow_2。我们还提供了其他来自我们丰富图书和视频目录的代码包，地址是
    https://github.com/PacktPublishing/。赶快去看看吧！
- en: Download the color images
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: https://static.packt-cdn.com/downloads/9781838641351_ColorImages.pdf.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个包含本书中使用的截图/图表的彩色图像的 PDF 文件。你可以在此下载：https://static.packt-cdn.com/downloads/9781838641351_ColorImages.pdf。
- en: Conventions used
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了多种文本约定。
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    For example: “After running the `pip install` command, you should have Jupyter
    Notebook available in the Conda environment.”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`CodeInText`：表示文本中的代码词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 用户名。例如：“运行
    `pip install` 命令后，你应该能在 Conda 环境中使用 Jupyter Notebook。”'
- en: 'A block of code is set as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一段代码按以下方式设置：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出均按以下方式书写：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Bold**: Indicates a new term or an important word. Words that you see on
    the screen (such as in menus or dialog boxes) also appear in the text like this
    , for example: “The feature that builds this computational graph automatically
    in TensorFlow is known as **AutoGraph**.”'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示一个新术语或重要的词汇。你在屏幕上看到的词汇（例如在菜单或对话框中）也会像这样出现在文本中，例如：“在 TensorFlow 中自动构建计算图的功能被称为**AutoGraph**。”'
- en: Warnings or important notes appear like this.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要说明将以这种形式出现。
- en: Tips and tricks appear like this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士和技巧将以这种形式出现。
- en: Get in touch
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们欢迎读者的反馈。
- en: '**General feedback**: Email `feedback@packtpub.com`, and mention the book’s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：发送电子邮件至`feedback@packtpub.com`，并在邮件主题中提及书名。如果你对本书的任何方面有疑问，请通过`questions@packtpub.com`与我们联系。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book we would
    be grateful if you would report this to us. Please visit http://www.packtpub.com/submit-errata,
    select your book, click on the Errata Submission Form link, and enter the details.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：虽然我们已经尽力确保内容的准确性，但错误仍然可能发生。如果你在本书中发现错误，请向我们报告。请访问 http://www.packtpub.com/submit-errata，选择你的书籍，点击“勘误提交表格”链接并填写详细信息。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果你在互联网上发现任何我们作品的非法复制品，请提供该位置地址或网站名称。请通过`copyright@packtpub.com`与我们联系，并附上该材料的链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit http://authors.packtpub.com.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有兴趣成为作者**：如果你在某个领域具有专长，并且有兴趣编写或参与撰写一本书，请访问 http://authors.packtpub.com。'
- en: Share your thoughts
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享你的想法
- en: Once you’ve read *Natural Language Processing with TensorFlow, Second Edition*,
    we’d love to hear your thoughts! Please [click here to go straight to the Amazon
    review page](https://packt.link/r/1838641351) for this book and share your feedback.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你读完了*《使用 TensorFlow 进行自然语言处理（第二版）》*，我们非常希望听到你的想法！请[点击这里直接进入该书的亚马逊评论页面](https://packt.link/r/1838641351)并分享你的反馈。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论对我们以及技术社区都非常重要，能帮助我们确保提供优质的内容。
