- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Advanced Word Vector Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级词向量算法
- en: 'In *Chapter 3*, *Word2vec – Learning Word Embeddings*, we introduced you to
    Word2vec, the basics of learning word embeddings, and the two common Word2vec
    algorithms: skip-gram and CBOW. In this chapter, we will discuss several other
    word vector algorithms:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*Word2vec – 学习词向量*中，我们介绍了Word2vec、学习词向量的基础知识，以及两个常见的Word2vec算法：skip-gram和CBOW。在本章中，我们将讨论其他几种词向量算法：
- en: GloVe – Global Vectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe – 全局向量
- en: ELMo – Embeddings from Language Models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELMo – 来自语言模型的嵌入
- en: Document classification with ELMo
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ELMo进行文档分类
- en: First, you will learn a word embedding learning technique known as **Global
    Vectors** (**GloVe**) and the specific advantages that GloVe has over skip-gram
    and CBOW.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将学习一种词嵌入学习技术，称为**全局向量**（**GloVe**），以及GloVe相对于skip-gram和CBOW的具体优势。
- en: You will also look at a recent approach for representing language called **Embeddings
    from Language Models** (**ELMo**). ELMo has an edge over other algorithms as it
    is able to disambiguate words, as well as capture semantics. Specifically, ELMo
    generates “contextualized” word representations, by using a given word along with
    its surrounding words, as opposed to treating word representations independently,
    as in skip-gram or CBOW.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将学习一种最近的语言表示方法，称为**来自语言模型的嵌入**（**ELMo**）。与其他算法相比，ELMo具有优势，因为它能够消除词义歧义并捕捉语义。具体来说，ELMo生成的是“上下文化”的单词表示，它通过使用给定单词及其周围的单词，而不是像skip-gram或CBOW那样独立地处理单词表示。
- en: Finally, we will solve an exciting use-case of document classification using
    our newly founded ELMo vectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将解决一个使用我们新创建的ELMo向量进行文档分类的令人兴奋的应用案例。
- en: GloVe – Global Vectors representation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GloVe – 全局向量表示
- en: One of the main limitations of skip-gram and CBOW algorithms is that they can
    only capture local contextual information, as they only look at a fixed-length
    window around a word. There’s an important part of the puzzle missing here as
    these algorithms do not look at global statistics (by global statistics we mean
    a way for us to see all the occurrences of words in the context of another word
    in a text corpus).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram和CBOW算法的主要限制之一是它们只能捕捉局部上下文信息，因为它们只看一个固定长度的窗口围绕单词。因此，缺少了解决这个问题的重要部分，因为这些算法并不查看全局统计信息（全局统计信息是指我们查看一个单词在文本语料库中与另一个单词的上下文中的所有出现情况的一种方法）。
- en: 'However, we have already studied a structure that could contain this information
    in *Chapter 3*, *Word2vec – Learning Word Embeddings*: the co-occurrence matrix.
    Let’s refresh our memory on the co-occurrence matrix, as GloVe uses the statistics
    captured in the co-occurrence matrix to compute vectors.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们已经在*第3章*，*Word2vec – 学习词向量*中学习过一种可以包含这些信息的结构：共现矩阵。让我们回顾一下共现矩阵，因为GloVe使用共现矩阵中捕捉到的统计信息来计算向量。
- en: 'Co-occurrence matrices encode the context information of words, but they require
    maintaining a V × V matrix, where V is the size of the vocabulary. To understand
    the co-occurrence matrix, let’s take two example sentences:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 共现矩阵编码了单词的上下文信息，但它们需要维护一个V × V的矩阵，其中V是词汇表的大小。为了理解共现矩阵，假设我们有两个例句：
- en: '*Jerry and Mary are friends*.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jerry and Mary are friends*。'
- en: '*Jerry buys flowers for Mary*.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jerry buys flowers for Mary*。'
- en: 'If we assume a context window of size 1, on each side of a chosen word, the
    co-occurrence matrix will look like the following (we only show the upper triangle
    of the matrix, as the matrix is symmetric):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设上下文窗口的大小为1，即在所选单词的每一侧，那么共现矩阵将如下所示（我们只显示矩阵的上三角部分，因为矩阵是对称的）：
- en: '|  | **Jerry** | **and** | **Mary** | **are** | **friends** | **buys** | **flowers**
    | **for** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | **Jerry** | **and** | **Mary** | **are** | **friends** | **buys** | **flowers**
    | **for** |'
- en: '| **Jerry** | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **Jerry** | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
- en: '| **and** |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **and** |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| **Mary** |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **Mary** |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
- en: '| **are** |  |  |  | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **are** |  |  |  | 0 | 1 | 0 | 0 | 0 |'
- en: '| **friends** |  |  |  |  | 0 | 0 | 0 | 0 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **friends** |  |  |  |  | 0 | 0 | 0 | 0 |'
- en: '| **buys** |  |  |  |  |  | 0 | 1 | 0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **buys** |  |  |  |  |  | 0 | 1 | 0 |'
- en: '| **flowers** |  |  |  |  |  |  | 0 | 1 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **flowers** |  |  |  |  |  |  | 0 | 1 |'
- en: '| **for** |  |  |  |  |  |  |  | 0 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **for** |  |  |  |  |  |  |  | 0 |'
- en: We can see that this matrix shows us how a word in a corpus is related to any
    other word, hence it contains global statistics about the corpus. That said, what
    are some of the advantages of having a co-occurrence matrix, as opposed to seeing
    just the local context?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个矩阵展示了语料库中一个词与其他任何词的关系，因此它包含了关于语料库的全局统计信息。也就是说，拥有共现矩阵相较于仅仅看到局部上下文有什么优势呢？
- en: It provides you with additional information about the characteristics of the
    words. For example, if you consider the sentence “the cat sat on the mat,” it
    is difficult to say if “the” is a special word that appears in the context of
    words such as “cat” or “mat.” However, if you have a large-enough corpus and a
    co-occurrence matrix, it’s very easy to see that “the” is a frequently occurring
    stop word.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为你提供了关于词语特性的额外信息。例如，如果你考虑句子“the cat sat on the mat”，就很难判断“the”是否是一个特殊的词，出现在像“cat”或“mat”这样的词的上下文中。然而，如果你有足够大的语料库和共现矩阵，就很容易看出“the”是一个频繁出现的停用词。
- en: The co-occurrence matrix recognizes the repeating usages of contexts or phrases,
    whereas in the local context this information is ignored. For example, in a large
    enough corpus, “New York” will be a clear winner, showing that the two words appear
    in the same context many times.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共现矩阵识别了上下文或短语的重复使用，而在局部上下文中这些信息则被忽略。例如，在足够大的语料库中，“New York”将明显成为赢家，表明这两个词在同一上下文中出现了很多次。
- en: It is important to keep in mind that Word2vec algorithms use various techniques
    to approximately inject some word co-occurrence patterns, while learning word
    vectors. For example, the sub-sampling technique we used in the previous chapter
    (i.e. sampling lower-frequency words more) helps to detect and avoid stop words.
    But they introduce additional hyperparameters and are not as informative as the
    co-occurrence matrix.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 需要牢记的是，Word2vec算法使用各种技术来大致注入一些词汇共现模式，同时学习词向量。例如，我们在上一章使用的子采样技术（即更频繁地采样低频词）有助于识别和避免停用词。但它们引入了额外的超参数，并且不如共现矩阵那样富有信息。
- en: Using global statistics to come up with word representations is not a new concept.
    An algorithm known as **Latent Semantic Analysis** (**LSA**) has been using global
    statistics in its approach.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全局统计信息来生成词表示并不是一个新概念。一个叫做**潜在语义分析**（**LSA**）的算法已经在其方法中使用了全局统计信息。
- en: LSA is used as a document analysis technique that maps words in the documents
    to something known as a **concept**, a common pattern of words that appears in
    a document. Global matrix factorization-based methods efficiently exploit the
    global statistics of a corpus (for example, co-occurrence of words in a global
    scope), but have been shown to perform poorly at word analogy tasks. On the other
    hand, context window-based methods have been shown to perform well at word analogy
    tasks, but do not utilize global statistics of the corpus, leaving space for improvement.
    GloVe attempts to get the best of both worlds—an approach that efficiently leverages
    global corpus statistics while optimizing the learning model in a context window-based
    manner similar to skip-gram or CBOW.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LSA作为一种文档分析技术，将文档中的词映射到所谓的**概念**，即在文档中出现的常见词模式。基于全局矩阵分解的方法有效地利用语料库的全局统计信息（例如，词语在全局范围内的共现），但在词汇类比任务中表现较差。另一方面，基于上下文窗口的方法在词汇类比任务中表现较好，但没有利用语料库的全局统计信息，因此有改进的空间。GloVe试图兼顾这两者的优点——一种既高效利用全局语料库统计信息，又像skip-gram或CBOW那样通过上下文窗口优化学习模型的方法。
- en: 'GloVe, a new technique for learning word embeddings was introduced in the paper
    “GloVe: Global Vectors for Word Representation” by Pennington et al. ([https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)).
    GloVe attempts to bridge the gap of missing global co-occurrence information in
    Word2vec algorithms. The main contribution of GloVe is a new cost function (or
    an objective function) that uses the valuable statistics available in the co-occurrence
    matrix. Let’s first understand the motivation behind the GloVe method.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'GloVe，一种用于学习词嵌入的新技术，已在Pennington等人的论文《GloVe: Global Vectors for Word Representation》中提出（[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)）。GloVe旨在弥补Word2vec算法中缺失的全局共现信息。GloVe的主要贡献是提出了一种新的成本函数（或目标函数），该函数利用了共现矩阵中可用的宝贵统计信息。让我们首先理解GloVe方法背后的动机。'
- en: Understanding GloVe
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GloVe
- en: 'Before looking at the implementation details of GloVe, let’s take time to understand
    the concepts governing the computations in GloVe. To do so, let’s consider an
    example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看 GloVe 的实现细节之前，让我们先花些时间理解 GloVe 中计算的基本概念。为此，我们来看一个例子：
- en: Consider word *i*=Ice and *j*=Steam
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑单词 *i*=Ice 和 *j*=Steam
- en: Define an arbitrary probe word *k*
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个任意的探针词 *k*
- en: Define ![](img/B14070_04_001.png) to be the probability of words *i* and *k*
    occurring close to each other, and ![](img/B14070_04_002.png) to be the words
    *j* and *k* occurring together
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 ![](img/B14070_04_001.png) 为单词 *i* 和 *k* 在一起出现的概率，![](img/B14070_04_002.png)
    为单词 *j* 和 *k* 一起出现的概率
- en: Now let’s look at how the ![](img/B14070_04_003.png) entity behaves with different
    values for *k*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 ![](img/B14070_04_003.png) 实体在不同 *k* 值下的表现。
- en: 'For *k* = “Solid” , it is highly likely to appear with *i*, thus, ![](img/B14070_04_001.png)
    will be high. However, *k* would not often appear along with *j* causing a low
    ![](img/B14070_04_002.png). Therefore, we get the following expression:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *k* = “Solid”，它很可能与 *i* 一起出现，因此 ![](img/B14070_04_001.png) 会较高。然而，*k* 不太会与
    *j* 一起出现，导致 ![](img/B14070_04_002.png) 较低。因此，我们得到以下表达式：
- en: '![](img/B14070_04_006.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_006.png)'
- en: 'Next, for *k* = “gas”, it is unlikely to appear in the close proximity of *i*
    and therefore will have a low ![](img/B14070_04_001.png); however, since *k* highly
    correlates with *j*, the value of ![](img/B14070_04_002.png) will be high. This
    leads to the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于 *k* = “gas”，它不太可能与 *i* 紧密相邻出现，因此会有一个较低的 ![](img/B14070_04_001.png)；然而，由于
    *k* 与 *j* 高度相关，![](img/B14070_04_002.png) 的值将会较高。这导致了以下情况：
- en: '![](img/B14070_04_009.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_009.png)'
- en: 'Now, for words such as *k* = “water”, which has a strong relationship with
    both *i* and *j*, or *k* = “Fashion”, which *i* and *j* both have minimal relevance
    to, we get this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于像 *k* = “water” 这样的单词，它与 *i* 和 *j* 都有很强的关系，或者对于 *k* = “Fashion” 这样的单词，它与
    *i* 和 *j* 都没有太多相关性，我们得到如下结果：
- en: '![](img/B14070_04_010.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_010.png)'
- en: 'If you assume we have learned sensible word embeddings for these words, these
    relationships can be visualized in a vectors space to understand why the ratio
    ![](img/B14070_04_003.png) behaves this way (see *Figure 4\. 1*). In the figure
    below, the solid arrow shows the distance between the words (*i, j*), whereas
    the dashed lines express the distance between the words, (*i, k*) and (*j, k*).
    These distances can then be associated with the probability values we discussed.
    For example, when *i* = “ice” and *k* = “solid”, we expect their vectors to have
    a shorter distance between them (i.e. more frequently co-occurring). Therefore,
    we can associate distance between (*i, k*) as the inverse of ![](img/B14070_04_001.png)
    (i.e. ![](img/B14070_04_013.png)) due to the definition of ![](img/B14070_04_001.png).
    This diagram shows how these distances vary as the probe word *k* changes:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果假设我们已经为这些单词学习了合理的词向量，这些关系可以在向量空间中可视化，从而理解为何比率 ![](img/B14070_04_003.png) 会有这样的行为（见
    *图 4.1*）。在下图中，实心箭头表示单词 (*i, j*) 之间的距离，而虚线则表示单词 (*i, k*) 和 (*j, k*) 之间的距离。这些距离可以与我们讨论的概率值关联起来。例如，当
    *i* = “ice”和 *k* = “solid” 时，我们期望它们的向量之间的距离较短（即更频繁地共同出现）。因此，由于 ![](img/B14070_04_001.png)
    的定义，我们可以将 (*i, k*) 之间的距离与 ![](img/B14070_04_001.png) 的倒数关联起来（即 ![](img/B14070_04_013.png)）。该图展示了随着探针词
    *k* 的变化，这些距离是如何变化的：
- en: '![](img/B14070_04_01.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_01.png)'
- en: 'Figure 4.1: How the entities P_ik and P_jk behave as the probe word changes
    in proximity to the words i and j'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：当探针词变化时，P_ik 和 P_jk 实体如何随着与单词 *i* 和 *j* 的接近度而变化
- en: 'It can be seen that the ![](img/B14070_04_003.png) entity, which is calculated
    by measuring the frequency of two words appearing close to each other, behaves
    in different ways as the relationship between the three words changes. As a result,
    it becomes a good candidate for learning word vectors. Therefore, a good starting
    point for defining the loss function will be as shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，![](img/B14070_04_003.png) 实体是通过测量两个单词紧密出现的频率来计算的，当三个单词之间的关系发生变化时，它的表现也会有所不同。因此，它成为了学习词向量的一个不错的候选对象。因此，定义损失函数的一个好的起点将如下所示：
- en: '![](img/B14070_04_016.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_016.png)'
- en: 'Here, *F* is some function and *w* and ![](img/B14070_04_017.png) are two different
    embedding spaces we’ll be using. In other words, the words ![](img/B14070_04_018.png)
    and ![](img/B14070_04_019.png) are looked up from one embedding space, whereas
    the probe word ![](img/B14070_04_020.png) is looked up from another. From this
    point, the original paper goes through the derivation meticulously to reach the
    following loss function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*F* 是某个函数，*w* 和 ![](img/B14070_04_017.png) 是我们将使用的两个不同的嵌入空间。换句话说，词汇 ![](img/B14070_04_018.png)
    和 ![](img/B14070_04_019.png) 是从一个嵌入空间中查找的，而探测词 ![](img/B14070_04_020.png) 则是从另一个嵌入空间中查找的。从这一点开始，原始论文仔细地进行了推导，以得到以下损失函数：
- en: '![](img/B14070_04_021.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_021.png)'
- en: We will not go through the derivation here, as that’s out of scope for this
    book. Rather we will use the derived loss function and implement the algorithm
    with TensorFlow. If you need a less mathematically dense explanation of how we
    can derive this cost function, please refer to the author-written article at [https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里不会深入推导，因为这超出了本书的范围。我们将直接使用已推导出的损失函数，并通过 TensorFlow 实现该算法。如果你需要一个较少数学密集的解释，了解我们是如何推导该成本函数的，请参考作者撰写的文章：[https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)。
- en: Here, ![](img/B14070_04_022.png) is defined as ![](img/B14070_04_023.png), if
    ![](img/B14070_04_024.png), else 1, where ![](img/B14070_04_025.png) is the frequency
    with which the word *j* appeared in the context of the word *i*. ![](img/B14070_04_034.png)
    is a hyperparameter we set. Remember that we defined two embedding spaces ![](img/B14070_04_026.png)
    and ![](img/B14070_04_017.png) in our loss function. ![](img/B14070_04_028.png)
    and ![](img/B14070_04_029.png) represent the word embedding and the bias embedding
    for the word *i* obtained from embedding space ![](img/B14070_04_026.png), respectively.
    And, ![](img/B14070_04_031.png) and ![](img/B14070_04_032.png) represent the word
    embedding and bias embedding for word *j* obtained from embedding space ![](img/B14070_04_017.png),
    respectively. Both these embeddings behave similarly except for the randomization
    at the initialization. At the evaluation phase, these two embeddings are added
    together, leading to improved performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B14070_04_022.png) 被定义为 ![](img/B14070_04_023.png)，如果 ![](img/B14070_04_024.png)，否则为
    1，其中 ![](img/B14070_04_025.png) 是词 *j* 在词 *i* 的上下文中出现的频率。![](img/B14070_04_034.png)
    是我们设置的一个超参数。记住，我们在损失函数中定义了两个嵌入空间 ![](img/B14070_04_026.png) 和 ![](img/B14070_04_017.png)。![](img/B14070_04_028.png)
    和 ![](img/B14070_04_029.png) 分别表示从嵌入空间 ![](img/B14070_04_026.png) 中获得的词 *i* 的词嵌入和偏置嵌入。而
    ![](img/B14070_04_031.png) 和 ![](img/B14070_04_032.png) 则分别表示从嵌入空间 ![](img/B14070_04_017.png)
    中获得的词 *j* 的词嵌入和偏置嵌入。这两种嵌入的行为类似，除了初始化时的随机化。在评估阶段，这两个嵌入将被加在一起，从而提高性能。
- en: Implementing GloVe
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 GloVe
- en: In this subsection, we will discuss the steps for implementing GloVe. The full
    code is available in the `ch4_glove.ipynb` exercise file located in the `ch4`
    folder.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将讨论实现 GloVe 的步骤。完整代码可以在 `ch4_glove.ipynb` 练习文件中找到，该文件位于 `ch4` 文件夹内。
- en: 'First, we’ll define the hyperparameters as we did in the previous chapter:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义超参数，就像在上一章中做的那样：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The hyperparameters you define here are the same hyperparameters we defined
    in the previous chapter. We have a batch size, embedding size, window size, the
    number of epochs, and, finally, a set of held-out validation word IDs that we
    will print the most similar words to.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里定义的超参数与我们在上一章中定义的超参数相同。我们有一个批量大小、嵌入维度、窗口大小、训练轮数，最后，还有一组保留的验证词 ID，用来打印最相似的词。
- en: 'We will then define the model. First, we will import a few things we will need
    down the line:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将定义模型。首先，我们将导入一些在后续代码中需要用到的库：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The model is going to have two input layers: `word_i` and `word_j`. They represent
    a batch of context words and a batch of target words (or a batch of positive skip-grams):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将有两个输入层：`word_i` 和 `word_j`。它们分别表示一批上下文词和一批目标词（或一批正样本跳字）：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note how the shape is defined. The shape is defined as an empty tuple. This
    means the final shape of `word_i` and `word_j` would be `[None]`, meaning it will
    take a vector of an arbitrary number of elements as the input.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意形状是如何定义的。形状被定义为空元组。这意味着 `word_i` 和 `word_j` 的最终形状将是 `[None]`，意味着它将接受一个任意元素数量的向量作为输入。
- en: 'Next, we are going to define the embedding layers. There will be four embedding
    layers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义嵌入层。将会有四个嵌入层：
- en: '`embeddings_i` – The context embedding layer'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embeddings_i` – 上下文嵌入层'
- en: '`embeddings_j` – The target embedding layer'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embeddings_j` – 目标嵌入层'
- en: '`b_i` – The context embedding bias'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b_i` – 上下文嵌入偏置'
- en: '`b_j` – The target embedding bias'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b_j` – 目标嵌入偏置'
- en: 'The following code defines these:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了这些内容：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we are going to compute the output. The output of this model will be:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算输出。这个模型的输出将是：
- en: '![](img/B14070_04_035.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_035.png)'
- en: 'As you can see, that’s a portion of our final loss function. We have all the
    right ingredients to compute this result:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这就是我们最终损失函数的一部分。我们拥有所有正确的元素来计算这个结果：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: First we will use the `tensorflow.keras.layers.Dot` layer to compute the dot
    product batch-wise between the context embedding lookup (`embeddings_i`) and the
    target embedding lookup (`embeddings_j`). For example, the two inputs to the `Dot`
    layer will be of size `[batch size, embedding size]`. After the dot product, the
    output `ij_dot` will be `[batch size, 1]`, where `ij_dot[k]` will be the dot product
    between `embeddings_i[k, :]` and `embeddings_j[k, :]`. Then we simply add `b_i`
    and `b_j` (which has shape `[None, 1]`) element-wise to `ij_dot`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `tensorflow.keras.layers.Dot` 层来计算上下文嵌入查找（`embeddings_i`）和目标嵌入查找（`embeddings_j`）之间的点积。举例来说，`Dot`
    层的两个输入将是 `[batch size, embedding size]` 的大小。经过点积后，输出 `ij_dot` 的形状将是 `[batch size,
    1]`，其中 `ij_dot[k]` 将是 `embeddings_i[k, :]` 和 `embeddings_j[k, :]` 之间的点积。然后，我们只需将
    `b_i` 和 `b_j`（其形状为 `[None, 1]`）逐元素加到 `ij_dot` 上。
- en: 'Finally, the model is defined as taking `word_i` and `word_j` as inputs and
    outputting `pred`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型被定义为以 `word_i` 和 `word_j` 作为输入，并输出 `pred`：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we are going to do something quite important.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一些相当重要的操作。
- en: We have to devise a way to compute the complex loss function defined above,
    using various components/functionality available in a model. First let’s revisit
    the loss function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须设计一种方法，使用模型中可用的各种组件/功能来计算上面定义的复杂损失函数。首先，让我们重新审视损失函数。
- en: '![](img/B14070_04_021.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_021.png)'
- en: where,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '![](img/B14070_04_023.png), if ![](img/B14070_04_024.png), else 1.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B14070_04_023.png)，如果 ![](img/B14070_04_024.png)，否则为 1。'
- en: 'Although it looks complex, we can use already existing loss functions and other
    functionality to implement the GloVe loss. You can abstract this loss function
    into three components as shown in the image below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看起来很复杂，我们可以利用现有的损失函数和其他功能来实现 GloVe 损失。你可以将这个损失函数抽象为下图所示的三个组件：
- en: '![](img/B14070_04_02.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_02.png)'
- en: 'Figure 4.2: The breakdown of the GloVe loss function showing how predictions,
    targets, and weights interact with each other to compute the final loss'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：GloVe 损失函数的分解，展示了预测值、目标值和权重是如何相互作用以计算最终损失的
- en: 'Therefore, if sample weights are denoted by ![](img/B14070_04_039.png), predictions
    are denoted by ![](img/B14070_04_040.png), and true targets are denoted by ![](img/B14070_04_041.png),
    then we can write the loss as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果样本权重用 ![](img/B14070_04_039.png) 表示，预测值用 ![](img/B14070_04_040.png) 表示，真实目标用
    ![](img/B14070_04_041.png) 表示，那么我们可以将损失函数写为：
- en: '![](img/B14070_04_042.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_042.png)'
- en: 'This is simply a weighted mean squared loss. Therefore, we will use `"mse"`
    as the loss for our model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是一个加权均方损失。因此，我们将使用`"mse"`作为我们模型的损失函数：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will later see how we can feed in sample weights to the model to complete
    the loss function. So far, we have defined different components of the GloVe algorithm
    and compiled the model. Next, we are going to have a look at how data can be generated
    to train the GloVe model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会看到如何将样本权重输入到模型中，以完成损失函数。到目前为止，我们已经定义了 GloVe 算法的不同组件，并编译了模型。接下来，我们将看看如何生成数据来训练
    GloVe 模型。
- en: Generating data for GloVe
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 GloVe 生成数据
- en: The dataset we will be using is the same as the dataset from the previous chapter.
    To recap, we will be using the BBC news articles dataset available at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    It contains 2225 news articles belonging to 5 topics, business, entertainment,
    politics, sport, and tech, which were published on the BBC website between 2004
    and 2005.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集与上一章使用的数据集相同。为了回顾一下，我们将使用BBC新闻文章数据集，网址为[http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html)。该数据集包含2225篇新闻文章，属于5个主题：商业、娱乐、政治、体育和科技，均发表于2004至2005年间的BBC网站。
- en: 'Let’s now generate the data. We will be encapsulating the data generation in
    a function called `glove_data_generator()`. As the first step, let us write a
    function signature:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成数据。我们将数据生成封装在一个名为`glove_data_generator()`的函数中。第一步，让我们编写一个函数签名：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The function takes several arguments:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受多个参数：
- en: '`sequences` (`List[List[int]]`) – a list of a list of word IDs. This is the
    output generated by tokenizer’s `texts_to_sequences()` function.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`List[List[int]]`）– 一个包含单词ID列表的列表。这是由分词器的`texts_to_sequences()`函数生成的输出。'
- en: '`window_size` (`int`) – Window size for the context.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_size`（`int`）– 上下文窗口大小。'
- en: '`batch_size` (`int`) – Batch size.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`）– 批量大小。'
- en: '`vocab_size` (`int`) – Vocabulary size.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`）– 词汇表大小。'
- en: '`cooccurrence_matrix` (`scipy.sparse.lil_matrix`) – A sparse matrix containing
    co-occurrences of words.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cooccurrence_matrix`（`scipy.sparse.lil_matrix`）– 一个稀疏矩阵，包含单词的共现。'
- en: '`x_max` (`int`) – Hyperparameter used by GloVe to compute sample weights.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_max`（`int`）– GloVe用于计算样本权重的超参数。'
- en: '`alpha` (`float`) – Hyperparameter used by GloVe to compute sample weights.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`（`float`）– GloVe用于计算样本权重的超参数。'
- en: '`seed` – The random seed.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` – 随机种子。'
- en: 'It also has several outputs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包含若干输出：
- en: A batch of (target, context) word ID tuples
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一批（目标，上下文）单词ID元组
- en: The corresponding ![](img/B14070_04_043.png) values for the (target, context)
    tuples
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应的 ![](img/B14070_04_043.png) 值，适用于（目标，上下文）元组
- en: Sample weights (i.e. ![](img/B14070_04_044.png)) values for the (target, context)
    tuples
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本权重（即 ![](img/B14070_04_044.png)）值，适用于（目标，上下文）元组
- en: 'First we will shuffle the order of news articles:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将打乱新闻文章的顺序：
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will create the sampling table, so that we can use sub-sampling to
    avoid over-sampling common words (e.g. stop words):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建采样表，以便可以使用子采样避免过度采样常见词汇（例如停用词）：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With that, for every sequence (i.e. list of word IDs) representing an article,
    we generate positive skip-grams. Note how we are keeping `negative_samples=0.0`
    as, unlike skip-gram or CBOW algorithms, GloVe does not rely on negative candidates:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，对于每个序列（即表示文章的单词ID列表），我们生成正向skip-gram。请注意，我们将`negative_samples=0.0`，因为与skip-gram或CBOW算法不同，GloVe不依赖于负样本：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With that, we first break down the skip-gram tuples into two lists, one containing
    targets and the other containing context words, and convert them to NumPy arrays
    subsequently:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们首先将skip-gram元组拆分成两个列表，一个包含目标，另一个包含上下文单词，并随后将其转换为NumPy数组：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then index the positions given by the (target, context) word pairs, from
    the co-occurrence matrix to retrieve the corresponding ![](img/B14070_04_025.png)
    values, where (*i,j*) represents a (target, context) pair:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从共现矩阵中索引（目标，上下文）单词对所给出的位置信息，以检索相应的 ![](img/B14070_04_025.png) 值，其中（*i,j*）表示（目标，上下文）对：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we compute a corresponding ![](img/B14070_04_043.png) (denoted by `log_x_ij`)
    and ![](img/B14070_04_044.png) (denoted by `sample_weights`):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算相应的 ![](img/B14070_04_043.png)（记作`log_x_ij`）和 ![](img/B14070_04_044.png)（记作`sample_weights`）：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If a code is not chosen, a random seed is set. Afterward, all of `context`,
    `targets`, `log_x_ij`, and `sample_weights` are shuffled while maintaining the
    correspondence of elements between the arrays:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未选择代码，则设置一个随机种子。之后，`context`、`targets`、`log_x_ij` 和 `sample_weights` 将被打乱，同时保持数组元素之间的对应关系：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally, we iterate through batches of the data we created above. Each batch
    will consist of
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们迭代通过我们上面创建的数据批次。每个批次将包含
- en: A batch of (target, context) word ID tuples
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一批（目标，上下文）单词ID元组
- en: The corresponding ![](img/B14070_04_043.png) values for the (target, context)
    tuples
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应的 ![](img/B14070_04_043.png) 值，适用于（目标，上下文）元组
- en: Sample weights (i.e. ![](img/B14070_04_044.png)) values for the (target, context)
    tuples
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本权重（即 ![](img/B14070_04_044.png)）值，适用于（目标，上下文）元组
- en: in that order.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 按此顺序。
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that the data is ready to be pumped in, let’s discuss the final piece of
    the puzzle: training the model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好输入，我们来讨论最后一个步骤：训练模型。
- en: Training and evaluating GloVe
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与评估GloVe
- en: 'Training the model is effortless, as we have all the components to train the
    model. As the first step, we will reuse the `ValidationCallback` we created in
    *Chapter 3*, *Word2vec – Learning Word Embeddings*. To recap, `ValidationCallback`
    is a Keras callback. Keras callbacks give you a way to execute some important
    operation(s) at the end of every training iteration, epoch, prediction step, etc.
    Here we are using the callback to perform a validation step at the end of every
    epoch. Our callback would take a list of word IDs intended as the validation words
    (held out in `valid_term_ids`), the model containing the embedding matrix, and
    a tokenizer to decode word IDs. Then it will compute the most similar top-k words
    for every word in the validation word set and print that as the output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型是轻而易举的，因为我们拥有所有训练模型所需的组件。第一步，我们将重用在*第3章*中创建的`ValidationCallback`，即*Word2vec
    – 学习词嵌入*。回顾一下，`ValidationCallback`是一个Keras回调。Keras回调让你能够在每次训练迭代、周期、预测步骤等结束时执行一些重要操作。在这里，我们使用回调在每个周期结束时执行验证步骤。我们的回调将接受一个词ID的列表（作为验证词，存放在`valid_term_ids`中），包含嵌入矩阵的模型，以及一个解码词ID的tokenizer。然后，它将计算验证词集中的每个词的最相似的top-k词，并将其作为输出：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should get a sensible-looking output once the model has finished training.
    Here are some of the cherry-picked results:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，你应该能得到一个合乎预期的输出。以下是一些精心挑选的结果：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can see that words like “months,” “weeks,” and “years” are grouped together.
    Numbers like “5bn,” “8bn,” and “2bn” are grouped together as well. “Deutsche”
    is surrounded by “Austria’s” and “Austria.” Finally, we will save the embeddings
    to the disk. We will combine weights and the bias of each context and target vector
    space to a single array, where the last column of the array will represent the
    bias and save it to the disk:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，“months”，“weeks”和“years”等词被分到了一组。像“5bn”，“8bn”和“2bn”这样的数字也被分到了一组。“Deutsche”被“Austria’s”和“Austria”围绕。最后，我们将词嵌入保存到磁盘。我们将每个上下文和目标向量空间的权重与偏置合并为一个数组，其中数组的最后一列表示偏置，并将其保存到磁盘：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will save embeddings as pandas DataFrames. First we get all the words sorted
    by their IDs. We subtract 1 to discount the reserved word ID 0 as we’ll add that
    manually, in the following line. Note that, word ID 0 will not show up in `tokenizer.index_word`.
    Next we get the required layers by name (namely, `context_embedding`, `target_embedding`,
    `context_embedding_bias` and `target_embedding_bias`). Once we have the layers
    we can use the `get_weights()` function to retrieve weights.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将词嵌入保存为pandas DataFrame。首先，我们按ID对所有词进行排序。我们减去1，以去除保留的词ID 0，因为我们将在下一行手动添加它。请注意，词ID
    0不会出现在`tokenizer.index_word`中。接下来，我们按名称获取所需的层（即`context_embedding`、`target_embedding`、`context_embedding_bias`和`target_embedding_bias`）。一旦获取到这些层，我们可以使用`get_weights()`函数来获取权重。
- en: In this section, we looked at GloVe, another word embedding learning technique.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了GloVe，这是一种词嵌入学习技术。
- en: The main advantage of GloVe over the Word2vec techniques discussed in *Chapter
    3*, *Word2vec – Learning Word Embeddings*, is that it pays attention to both global
    and local statistics of the corpus to learn embeddings. As GloVe is able to capture
    the global information about words, it tends to give better performance, especially
    when the corpus size increases. Another advantage is that, unlike in Word2vec
    techniques, GloVe does not approximate the cost function (for example, Word2vec
    using negative sampling), but calculates the true cost. This leads to better and
    easier optimization of the loss.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe相对于*第3章*中讨论的Word2vec技术的主要优点在于，它关注语料库的全局和局部统计信息来学习嵌入。由于GloVe能够捕捉到词的全局信息，它通常能提供更好的性能，尤其是在语料库规模增大时。另一个优点是，与Word2vec技术不同，GloVe并不近似代价函数（例如，Word2vec使用负采样），而是计算真正的代价。这使得损失的优化更加高效和容易。
- en: In the next section, we are going to look at one more word vector algorithm
    known as **Embeddings from Language Models** (**ELMo**).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍另一个词向量算法，称为**来自语言模型的嵌入**（**ELMo**）。
- en: ELMo – Taking ambiguities out of word vectors
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELMo – 消除词向量中的歧义
- en: 'So far, we’ve looked at word embedding algorithms that can give only a unique
    representation of the words in the vocabulary. However, they will give a constant
    representation for a given word, no matter how many times you query. Why would
    this be a problem? Consider the following two phrases:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了只能为词汇中的每个单词提供唯一表示的词嵌入算法。然而，它们会为给定的单词提供恒定的表示，无论你查询多少次。这为什么是个问题呢？请考虑以下两个短语：
- en: '*I went to the bank to deposit some money*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*我去银行存了一些钱*'
- en: and
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '*I walked along the river bank*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*我沿着河岸走*'
- en: Clearly, the word “bank” is used in two totally different contexts. If you use
    a vanilla word vector algorithm (e.g. skip-gram), you can only have one representation
    for the word “bank”, and it is probably going to be muddled between the concept
    of a financial institution and the concept of walkable edges along a river, depending
    on the references to this word found in the corpus it’s trained on. Therefore,
    it is more sensible to provide embeddings for a word while preserving and leveraging
    the context around it. This is exactly what ELMo is striving for.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，单词“bank”在两个完全不同的语境中使用。如果你使用普通的词向量算法（例如 skip-gram），你只能为单词“bank”提供一个表示，并且这个表示可能会在金融机构的概念和可以走的河岸边缘的概念之间混淆，具体取决于它在语料库中的引用。因此，更合理的做法是为一个词提供嵌入，同时保留并利用它周围的上下文。这正是
    ELMo 所努力实现的目标。
- en: 'Specifically, ELMo takes in a sequence, as opposed to a single token, and provides
    contextualized representations for each token in the sequence. *Figure 4.3* depicts
    various components encompassing the model. The first thing to understand is that
    ELMo is a complicated beast! There are lots of neural network models orchestrating
    in ELMo to produce the output. Particularly, the model uses:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，ELMo 处理的是一系列输入，而不是单一的词汇，并为序列中每个词提供上下文化的表示。*图 4.3* 展示了涵盖该模型的不同组件。首先需要理解的是，ELMo
    是一个复杂的系统！在 ELMo 中，许多神经网络模型相互协调以产生输出。特别地，模型使用：
- en: A character embedding layer (an embedding vector for each character).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个字符嵌入层（每个字符的嵌入向量）。
- en: A **convolutional neural network** (**CNN**) – a CNN consists of many convolutional
    layers followed by an optional fully connected classification layer.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **卷积神经网络** (**CNN**)——CNN 由许多卷积层和可选的全连接分类层组成。
- en: A convolution layer takes in a sequence of inputs (e.g. sequence of characters
    in a word) and moves a window of weights over the input to generate a latent representation.
    We will discuss CNNs in detail in the coming chapters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层接收一系列输入（例如单词中的字符序列），并在输入上移动一个加权窗口来生成潜在表示。我们将在后续章节中详细讨论 CNN。
- en: Two bi-directional LSTM layers – an LSTM is a type of model that is used to
    process time-series data. Given a sequence of inputs (e.g. sequence of word vectors),
    an LSTM goes from one input to the other, on the time dimension, and produces
    an output at each position. Unlike fully connected networks, LSTMs have memory,
    meaning the output at the current position will be affected by what the LSTM has
    seen in the past. We will discuss LSTMs in detail in the coming chapters.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个双向 LSTM 层——LSTM 是一种用于处理时间序列数据的模型。给定一系列输入（例如词向量序列），LSTM 会沿着时间维度从一个输入处理到另一个输入，并在每个位置产生一个输出。与全连接网络不同，LSTM
    具有记忆功能，这意味着当前位点的输出会受到 LSTM 过去见过的数据的影响。我们将在后续章节中详细讨论 LSTM。
- en: The specifics of these different components are outside the scope of this chapter.
    They will be discussed in detail in the coming chapters. Therefore, do not worry
    if you do not understand the exact mechanisms of the sub-components shown here
    (*Figure 4.3*).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同组件的具体细节超出了本章的讨论范围。它们将在后续章节中详细讨论。因此，如果你不理解这里展示的子组件的具体机制，也不必担心（*图 4.3*）。
- en: '![](img/B14070_04_03.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_03.png)'
- en: 'Figure 4.3: Different components of the ELMo model. Token embeddings are generated
    using a type of neural network known as a CNN. These token embeddings are fed
    to an LSTM model (that can process time-series data). The output of the first
    LSTM model is fed to a second LSTM model to generate a latent contextualized representation
    for each token'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：ELMo 模型的不同组件。词嵌入是通过一种名为 CNN 的神经网络生成的。这些词嵌入被输入到 LSTM 模型中（该模型可以处理时间序列数据）。第一个
    LSTM 模型的输出被输入到第二个 LSTM 模型，以生成每个词的潜在上下文化表示。
- en: We can download a pretrained ELMo model from TensorFlow Hub ([https://tfhub.dev](https://tfhub.dev)).
    TF Hub is a repository for various pretrained models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从 TensorFlow Hub ([https://tfhub.dev](https://tfhub.dev)) 下载预训练的 ELMo 模型。TF
    Hub 是各种预训练模型的存储库。
- en: It hosts models for tasks such as image classification, text classification,
    text generation, etc. You can go to the site and browse various available models.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 它托管了用于图像分类、文本分类、文本生成等任务的模型。你可以访问该网站并浏览各种可用的模型。
- en: Downloading ELMo from TensorFlow Hub
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 TensorFlow Hub 下载 ELMo
- en: The ELMo model we will be using is found at [https://tfhub.dev/google/elmo/3](https://tfhub.dev/google/elmo/3).
    It has been trained on a very large corpus of text to solve a task known as language
    modeling. In language modeling, we try to predict the next word given the previous
    sequence of tokens. We will learn more about language modeling in the coming chapters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 ELMo 模型位于 [https://tfhub.dev/google/elmo/3](https://tfhub.dev/google/elmo/3)。它已经在一个非常大的文本语料库上进行了训练，以解决称为语言建模的任务。在语言建模中，我们试图根据先前的标记序列预测下一个单词。在接下来的章节中，我们将更多地了解语言建模。
- en: 'Before downloading the model, let’s set the following environment variables:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载模型之前，让我们设置以下环境变量：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`TF_FORCE_GPU_ALLOW_GROWTH` allows TensorFlow to allocate GPU memory on-demand
    as opposed to allocating all GPU memory at once. `TFHUB_CACHE_DIR` sets the directory
    where the models will be downloaded. We will first import TensorFlow Hub:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`TF_FORCE_GPU_ALLOW_GROWTH` 允许 TensorFlow 根据需要分配 GPU 内存，而不是一次性分配所有 GPU 内存。
    `TFHUB_CACHE_DIR` 设置模型下载的目录。我们首先导入 TensorFlow Hub：'
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, as usual, we will clear any running TensorFlow sessions by running the
    following code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，像往常一样，我们将通过运行以下代码清除任何正在运行的 TensorFlow 会话：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we will download the ELMo model. You can employ two ways to download
    pretrained models from TF Hub and use them in our code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将下载 ELMo 模型。你可以使用两种方式从 TF Hub 下载预训练模型并在我们的代码中使用它们：
- en: '`hub.load(<url>, **kwargs)` – Recommended way for downloading and using TensorFlow
    2-compatible models'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub.load(<url>, **kwargs)` – 推荐的下载和使用 TensorFlow 2 兼容模型的方式'
- en: '`hub.KerasLayer(<url>, **kwargs)` – This is a workaround for using TensorFlow
    1-based models in TensorFlow 2'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hub.KerasLayer(<url>, **kwargs)` – 这是在 TensorFlow 2 中使用基于 TensorFlow 1 的模型的一种解决方案'
- en: 'Unfortunately, ELMo has not been ported to TensorFlow 2 yet. Therefore, we
    will use the `hub.KerasLayer()` as the workaround to load ELMo in TensorFlow 2:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，ELMo 还没有移植到 TensorFlow 2。因此，我们将使用 `hub.KerasLayer()` 作为在 TensorFlow 2 中加载
    ELMo 的解决方法：
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note that we are providing two arguments, `signature` and `signature_outputs_as_dict`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在提供两个参数，`signature` 和 `signature_outputs_as_dict`：
- en: '`signature` (`str`) – Can be `default` or `tokens`. The default signature accepts
    a list of strings, where each string will be converted to a list of tokens internally.
    The tokens signature takes in inputs as dictionary having two keys. Namely, `tokens`
    (a list of list of tokens. Each list of tokens is a single phrase/sentence and
    includes padding tokens to bring them to a fixed length) and “`sequence_len`"
    (the length of each list of tokens, to determine the padding length).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`signature` (`str`) – 可以是 `default` 或 `tokens`。默认签名接受字符串列表，其中每个字符串将在内部转换为标记列表。标记签名接受输入为具有两个键的字典。即
    `tokens`（标记列表的列表。每个标记列表是一个短语/句子，包括填充标记以将其调整为固定长度）和 "`sequence_len`"（每个标记列表的长度，以确定填充长度）。'
- en: '`signature_outputs_as_dict` (`bool`) – When set to `true`, it will return all
    the outputs defined in the provided signature.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`signature_outputs_as_dict` (`bool`) – 当设置为 `true` 时，将返回提供的签名中定义的所有输出。'
- en: Now that we have understood the components of ELMo and downloaded it from TensorFlow
    Hub, let’s see how we can process input data for ELMo.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 ELMo 的组成部分，并从 TensorFlow Hub 下载了它，让我们看看如何处理 ELMo 的输入数据。
- en: Preparing inputs for ELMo
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备 ELMo 的输入
- en: Here we will define a function that will convert a given list of strings to
    the format ELMo expects the inputs to be in. Remember that we set the signature
    of ELMo to be `tokens`. An example input to the signature `"tokens"` would look
    as follows.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将定义一个函数，将给定的字符串列表转换为 ELMo 期望输入的格式。请记住，我们将 ELMo 的签名设置为 `tokens`。签名 `"tokens"`
    的示例输入如下。
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let’s take a moment to process what the input comprises. First it has the key
    `tokens`, which has a list of tokens. Each list of tokens can be thought of as
    a sentence. Note how padding is added to the end of the short sentence to match
    the length. This is important as, otherwise, the model will throw an error as
    it can’t convert arbitrary-length sequences to a tensor. Next we have `sequence_len`,
    which is a list of integers. Each integer specifies the true length of each sequence.
    Note how the second element says 3, to match the actual tokens present in the
    second sequence.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间处理输入的组成部分。首先，它有一个关键字`tokens`，其中包含一系列令牌。每个令牌列表可以看作是一个句子。注意短句子的末尾如何添加填充以匹配长度。这很重要，否则模型会抛出错误，因为它无法将任意长度的序列转换为张量。接下来我们有`sequence_len`，它是一个整数列表。每个整数指定每个序列的真实长度。注意第二个元素为3，以匹配第二个序列中实际存在的令牌。
- en: 'Given a list of strings, we can write a function to do this transformation
    for us. That’s what the `format_text_for_elmo()` function will do for us. Let’s
    sink our teeth into the specifics:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个字符串列表，我们可以编写一个函数来为我们执行这个转换。这就是`format_text_for_elmo()`函数的作用。让我们深入了解具体细节：
- en: '[PRE24]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We first create two lists, `token_inputs` and `token_lengths`, to contain individual
    tokens and their respective lengths. Next we go through each string in `texts`,
    and get the individual tokens using the `tf.keras.preprocessing.text.text_to_word_sequence()`
    function. While doing so, we will calculate the maximum token length we have observed
    so far. After iterating through the sequences, we check if the maximum length
    inferred from the inputs is different to `max_len` (if specified). If so, we will
    use `max_len_inferred` as the maximum length. This is important, because if you
    do otherwise, you may unnecessarily lengthen the inputs by defining a large value
    for `max_len`. Not only that, the model will raise an error like the one below
    if you do so.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建两个列表，`token_inputs`和`token_lengths`，用于包含单个令牌及其各自的长度。接下来，我们遍历`texts`中的每个字符串，使用`tf.keras.preprocessing.text.text_to_word_sequence()`函数获取单个令牌。在此过程中，我们将计算迄今为止观察到的最大令牌长度。遍历完所有序列后，我们检查从输入推断出的最大长度是否与`max_len`（如果指定）不同。如果不同，我们将使用`max_len_inferred`作为最大长度。这一点很重要，因为如果你不这样做，可能会通过为`max_len`定义一个大值来不必要地延长输入长度。不仅如此，如果你这么做，模型将抛出像下面这样的错误。
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Once the proper maximum length is found, we will go through the sequences and
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到适当的最大长度，我们将遍历序列并
- en: If it is longer than `max_len`, truncate the sequence
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它比`max_len`长，则截断序列。
- en: If it is shorter than `max_len`, add tokens until it reaches `max_len`
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它比`max_len`短，则添加令牌直到达到`max_len`。
- en: 'Finally, we will convert them to `tf.Tensor` objects using the `tf.constant`
    construct. For example, you can call this function with:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`tf.constant`构造将它们转换为`tf.Tensor`对象。例如，你可以使用以下方式调用该函数：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We will now see how ELMo can be used to generate embeddings for the prepared
    inputs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看到如何使用ELMo为准备好的输入生成嵌入。
- en: Generating embeddings with ELMo
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ELMo生成嵌入
- en: 'Once the input is prepared, generating embeddings is quite easy. First we will
    transform the inputs to the stipulated format of the ELMo layer. Here we are using
    some example titles from the BBC dataset:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入准备好，生成嵌入就非常简单。首先，我们将把输入转换为ELMo层规定的格式。这里我们使用BBC数据集中的一些示例标题：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, simply pass the `elmo_inputs` to the `elmo_layer` as the input and get
    the result:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，只需将`elmo_inputs`传递给`elmo_layer`作为输入，并获取结果：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s now print the results and their shapes with the following line:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用以下代码打印结果及其形状：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will print out:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As you can see, the model returns 6 different outputs. Let’s go through them
    one by one:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，模型返回了6个不同的输出。让我们逐一查看：
- en: '`sequence_len` – The same input we provided containing the lengths of the sequences
    in the input'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_len` – 我们提供的相同输入，包含输入中各个序列的长度'
- en: '`word_emb` – The token embeddings obtained via the CNN layer in the ELMo model.
    We got a vector of size 512 for all sequence positions (i.e. 6) and for all rows
    in the batch (i.e. 5).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_emb` – 通过ELMo模型中的CNN层获得的令牌嵌入。我们为所有序列位置（即6）和批次中的所有行（即5）得到了一个大小为512的向量。'
- en: '`lstm_output1` – The contextualized representations of tokens obtained via
    the first LSTM layer'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lstm_output1` – 通过第一个LSTM层获得的令牌的上下文化表示'
- en: '`lstm_output2` – The contextualized representations of tokens obtained via
    the second LSTM layer'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lstm_output2` – 通过第二个LSTM层获得的令牌的上下文化表示'
- en: '`default` – The mean embedding vector obtained by averaging all of the `lstm_output1`
    and `lstm_output2` embeddings'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default` – 通过对所有的`lstm_output1`和`lstm_output2`嵌入进行平均得到的平均嵌入向量'
- en: '`elmo` – The weighted sum of all of `word_emb`, `lstm_output1`, and `lstm_output2`,
    where weights are a set of task-specific trainable parameters that will be jointly
    trained during the task-specific training'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elmo` – 所有`word_emb`、`lstm_output1`和`lstm_output2`的加权和，其中权重是一组任务特定的可训练参数，将在任务特定训练期间一起训练'
- en: What we are interested in here is the `default` output. That would give us a
    very good representation of what’s contained in the document.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里关注的是`default`输出。它将为我们提供文档内容的非常好的表示。
- en: '**Other word embedding techniques**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他词嵌入技术**'
- en: Apart from the word embedding techniques we discussed here, there are a few
    notable widely used word embedding techniques. We will discuss a few of those
    here.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在这里讨论的词嵌入技术外，还有一些著名的广泛使用的词嵌入技术。我们将在此讨论其中一些。
- en: '**FastText**'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastText**'
- en: FastText ([https://fasttext.cc/](https://fasttext.cc/)), introduced in the paper
    “Enriching Word Vectors with Subword Information” by Bojanowski et al. ([https://arxiv.org/pdf/1607.04606.pdf](https://arxiv.org/pdf/1607.04606.pdf)),
    introduces a technique where word embeddings are computed by considering the sub-components
    of a word. Specifically, they compute the word embedding as a summation of embeddings
    of *n*-grams of the word for several values of *n*. In the paper, they use *3*
    <= *n* <=*6*. For example, for the word “banana,” the tri-grams (*n*=3) would
    be `['ban', 'ana', 'nan', 'ana']`. This leads to robust embeddings that can withstand
    common problems of text, such as spelling mistakes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: FastText ([https://fasttext.cc/](https://fasttext.cc/))，由Bojanowski等人于论文《Enriching
    Word Vectors with Subword Information》提出 ([https://arxiv.org/pdf/1607.04606.pdf](https://arxiv.org/pdf/1607.04606.pdf))，介绍了一种通过考虑词语的子组件来计算词嵌入的技术。具体来说，他们将词嵌入计算为词的*n*-gram嵌入的总和，*n*取多个值。在论文中，他们使用了*3*
    <= *n* <= *6*。例如，对于单词“banana”，三元组（*n*=3）为`['ban', 'ana', 'nan', 'ana']`。这使得嵌入变得更加健壮，能够抵抗文本中的常见问题，比如拼写错误。
- en: '**Swivel embeddings**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**Swivel 嵌入**'
- en: 'Swivel embeddings, introduced by the paper “*Swivel: Improving Embeddings by
    Noticing What’s Missing*” by Shazeer et al. ([https://arxiv.org/pdf/1602.02215.pdf](https://arxiv.org/pdf/1602.02215.pdf)),
    tries to blend GloVe and skip-grams with negative sampling. One of the critical
    limitations of GloVe is that it only uses information about positive contexts.
    Therefore, the method is not penalized for trying to create similar vectors of
    words that have not been observed together. But the negative sampling used in
    skip-grams directly tackles this problem. The biggest innovation of Swivel is
    a loss function that incorporates unobserved word pairs. As an added benefit,
    it can also be trained in a distributed environment.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'Swivel 嵌入，由Shazeer等人于论文《*Swivel: Improving Embeddings by Noticing What’s Missing*》提出
    ([https://arxiv.org/pdf/1602.02215.pdf](https://arxiv.org/pdf/1602.02215.pdf))，尝试将GloVe和跳字模型与负采样结合。GloVe的一个关键限制是它仅使用有关正向上下文的信息。因此，该方法不会因尝试创建未曾一起出现的单词的相似向量而受到惩罚。而跳字模型中使用的负采样则直接解决了这个问题。Swivel的最大创新是包含未观察到的词对的损失函数。作为额外的好处，它还可以在分布式环境中进行训练。'
- en: '**Transformer models**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer模型**'
- en: Transformers are a type of model that has reimagined the way we think about
    NLP problems. The Transformer model was initially introduced in the paper “*Attention
    is all you need*” by Vaswani ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    This model has many different embeddings within it and, like ELMo, can generate
    an embedding per token by processing a sequence of text. We will talk about Transformer
    models in detail in later chapters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型是一类重新定义我们思考NLP问题方式的模型。Transformer模型最初由Vaswani在论文《*Attention is
    all you need*》中提出 ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))。该模型内部有许多不同的嵌入，像ELMo一样，它可以通过处理文本序列为每个标记生成嵌入。我们将在后续章节中详细讨论Transformer模型。
- en: We have discussed all the bells and whistles required to confidently use the
    ELMo model. Next we will classify documents using ELMo, in which ELMo will generate
    document embeddings as inputs to a classification model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了使用ELMo模型所需的所有细节。接下来，我们将使用ELMo进行文档分类，在此过程中，ELMo将生成文档嵌入作为分类模型的输入。
- en: Document classification with ELMo
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ELMo进行文档分类
- en: Although Word2vec gives a very elegant way of learning numerical representations
    of words, learning word representations alone is not convincing enough to realize
    the power of word vectors in real-world applications.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Word2vec提供了一种非常优雅的学习词语数值表示的方法，但仅仅学习词表示并不足以令人信服地展示词向量在实际应用中的强大功能。
- en: Word embeddings are used as the feature representation of words for many tasks,
    such as image caption generation and machine translation. However, these tasks
    involve combining different learning models such as **Convolutional Neural Networks**
    (**CNNs**) and **Long Short-Term Memory** (**LSTM**) models or two LSTM models
    (the CNN and LSTM models will be discussed in more detail in later chapters).
    To understand a real-world usage of word embeddings let’s stick to a simpler task—document
    classification.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入被用作许多任务中词语的特征表示，比如图像标题生成和机器翻译。然而，这些任务涉及结合不同的学习模型，如**卷积神经网络**（**CNNs**）和**长短期记忆**（**LSTM**）模型，或者两个LSTM模型（CNN和LSTM模型将在后续章节中详细讨论）。为了理解词嵌入在实际应用中的使用，我们可以从一个更简单的任务——文档分类开始。
- en: Document classification is one of the most popular tasks in NLP. Document classification
    is extremely useful for anyone who is handling massive collections of data such
    as those for news websites, publishers, and universities. Therefore, it is interesting
    to see how learning word vectors can be adapted to a real-world task such as document
    classification by means of embedding entire documents instead of words.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分类是自然语言处理（NLP）中最流行的任务之一。文档分类对处理海量数据的人员非常有用，例如新闻网站、出版商和大学。因此，看看如何通过嵌入整个文档而不是词语，将学习到的词向量应用于像文档分类这样的实际任务是非常有趣的。
- en: This exercise is available in the `Ch04-Advance-Word-Vectors` folder (`ch4_document_classification.ipynb`).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习可在`Ch04-Advance-Word-Vectors`文件夹下找到（`ch4_document_classification.ipynb`）。
- en: Dataset
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'For this task, we will use an already-organized set of text files. These are
    news articles from the BBC. Every document in this collection belongs to one of
    the following categories: *Business*, *Entertainment*, *Politics*, *Sports*, or
    *Technology*.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用一组已组织好的文本文件。这些是来自BBC的新闻文章。该数据集中每篇文档都属于以下类别之一：*商业*、*娱乐*、*政治*、*体育*或*技术*。
- en: 'Here are a couple of brief snippets from the actual data:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自实际数据的几个简短片段：
- en: '*Business*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*商业*'
- en: '*Japan narrowly escapes recession*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*日本勉强避免衰退*'
- en: '*Japan’s economy teetered on the brink of a technical recession in the three
    months to September, figures show.*'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*日本经济在截至九月的三个月里，勉强避免了技术性衰退，数据显示。*'
- en: '*Revised figures indicated growth of just 0.1% - and a similar-sized contraction
    in the previous quarter. On an annual basis, the data suggests annual growth of
    just 0.2%,...*'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*修正后的数据显示增长仅为0.1%——而前一季度也出现了类似规模的收缩。按年计算，数据表明年增长仅为0.2%，...*'
- en: First, we will download the data and load the data into memory. We will use
    the same `download_data()` function to download the data. Then we will slightly
    modify the `read_data()` function to not only return a list of articles, where
    each article is a string, but also to return a list of filenames, where each filename
    corresponds to the file the article was stored in. The filenames will subsequently
    help us to create the labels for our classification model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将下载数据并将其加载到内存中。我们将使用相同的`download_data()`函数来下载数据。然后，我们会稍微修改`read_data()`函数，使其不仅返回文章列表（每篇文章是一个字符串），还返回文件名列表，其中每个文件名对应存储该文章的文件。文件名随后将帮助我们为分类模型创建标签。
- en: '[PRE32]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We will then create and fit a tokenizer on the data, as we have done before.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将像之前一样在数据上创建并拟合一个分词器。
- en: '[PRE33]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As the next step, we will create labels. Since we are training a classification
    model, we need both inputs and labels. Our inputs will be document embeddings
    (we will see how to compute them soon), and the targets will be a label ID between
    0 and 4\. Each class we mentioned above (e.g. business, tech, etc.) will be assigned
    to a separate category. Since the filename includes the category as a folder,
    we can leverage the filename to generate a label ID.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建标签。由于我们正在训练一个分类模型，因此我们需要输入和标签。我们的输入将是文档嵌入（我们很快会看到如何计算它们），而目标将是一个介于0和4之间的标签ID。我们上面提到的每个类别（例如，商业、技术等）将被分配到一个单独的类别中。由于文件名包括作为文件夹的类别，因此我们可以利用文件名生成标签ID。
- en: 'We will use the pandas library to create the labels. First we will convert
    the list of filenames to a pandas Series object using:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pandas库来创建标签。首先，我们将文件名列表转换为pandas的Series对象，方法如下：
- en: '[PRE34]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'An example entry in this series could look like `data/bbc/tech/127.txt`. Next,
    we will split each item on the “/” character, which will return a list `[''data'',
    ''bbc'', ''tech'', ''127.txt'']`. We will also set `expand=True`. `expand=True`
    will transform our Series object to a DataFrame by turning each item in the list
    of tokens into a separate column of a DataFrame. In other words, our `pd.Series`
    object will become an `[N, 4]`-sized `pd.DataFrame` with one token in each column,
    where `N` is the number of files:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该系列中的一个示例条目可能类似于`data/bbc/tech/127.txt`。接下来，我们将按“/”字符分割每个条目，这将返回一个列表`['data',
    'bbc', 'tech', '127.txt']`。我们还会设置`expand=True`。`expand=True`将通过将列表中的每个项目转换为`DataFrame`的单独列，来把我们的Series对象转换成DataFrame。换句话说，我们的`pd.Series`对象将变成一个形状为`[N,
    4]`的`pd.DataFrame`，每一列包含一个token，其中`N`是文件的数量：
- en: '[PRE35]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the resulting data, we only care about the third column, which has the category
    of a given article (e.g. `tech`). Therefore, we will discard the rest of the data
    and only keep that column:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的数据中，我们只关心第三列，它包含了给定文章的类别（例如`tech`）。因此，我们将丢弃其他数据，仅保留这一列：
- en: '[PRE36]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we will map the string label to an integer ID using the pandas `map()`
    function as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用pandas的`map()`函数将字符串标签映射到整数ID，方法如下：
- en: '[PRE37]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This will result in something like:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致如下结果：
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'What we did here can be written as just one line by chaining the sequence of
    commands to a single line:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的，可以通过将一系列命令链式写成一行来实现：
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'With that, we move on to the next important step, i.e. splitting the data into
    train/test subsets. When training a supervised model, we generally need three
    datasets:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入另一个重要步骤，即将数据拆分为训练集和测试集。在训练一个监督学习模型时，我们通常需要三个数据集：
- en: A training set – This is the dataset the model will be trained on.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集 —— 这是模型将要训练的 数据集。
- en: A validation set – This will be used during the training to monitor model performance
    (e.g. signs of overfitting).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集 —— 这个数据集将在训练过程中用于监控模型性能（例如，防止过拟合的迹象）。
- en: A testing set – This will be not exposed to the model at any time during the
    model training. It will only be used after the model training to evaluate the
    model on unseen data.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集 —— 该数据集在模型训练过程中不会暴露给模型。它只会在模型训练完成后，用于评估模型在未见数据上的表现。
- en: 'In this exercise, we will only use the training set and the testing set. This
    will help us to keep our conversation more focused on embeddings and keep the
    discussion about the downstream classification model simple. Here we will use
    67% of the data as training data and use 33% of data as testing data. Data will
    be split randomly:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们只使用训练集和测试集。这将帮助我们将讨论聚焦于嵌入部分，并简化关于下游分类模型的讨论。这里我们将67%的数据作为训练数据，33%的数据作为测试数据。数据将随机拆分：
- en: '[PRE40]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now we have a training dataset to train the model and a test dataset to test
    it on unseen data. We will now see how we can generate document embeddings from
    token or word embeddings.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练数据集用于训练模型，以及一个测试数据集用于在未见数据上进行测试。接下来我们将展示如何从标记或单词嵌入生成文档嵌入。
- en: Generating document embeddings
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成文档嵌入
- en: Let’s first remind ourselves how we stored embeddings for skip-gram, CBOW, and
    GloVe algorithms. *Figure 4.4* depicts how these look in a `pd.DataFrame` object.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下我们如何存储skip-gram、CBOW和GloVe算法的嵌入。*图4.4*展示了这些嵌入在`pd.DataFrame`对象中的样子。
- en: '![](img/B14070_04_04.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_04.png)'
- en: 'Figure 4.4: A snapshot of the context embeddings of the skip-gram algorithm
    we saved to the disk. You can see below it says that it has 128 columns (i.e.
    the embedding size)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：保存到磁盘的skip-gram算法上下文嵌入的快照。你可以看到下方显示它有128列（即嵌入维度）。
- en: 'ELMo embeddings are an exception to this. Since ELMo generates contextualized
    representations for all tokens in a sequence, we have stored the mean embedding
    vectors resulting from averaging all the generated vectors:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo嵌入是一个例外。由于ELMo为序列中的所有token生成上下文表示，因此我们存储了通过对所有生成的向量取平均得到的均值嵌入向量：
- en: '![](img/B14070_04_05.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_05.png)'
- en: 'Figure 4.5: A snapshot of ELMo vectors. ELMo vectors have 1024 elements'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：ELMo向量的快照。ELMo向量有1024个元素。
- en: 'To compute the document embeddings from skip-gram, CBOW, and GloVe embeddings,
    let us write the following function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从skip-gram、CBOW和GloVe嵌入计算文档嵌入，让我们编写以下函数：
- en: '[PRE41]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `generate_document_embeddings()` function takes the following arguments:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_document_embeddings()`函数接受以下参数：'
- en: '`texts` – A list of strings, where each string represents an article'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`texts` – 一个字符串列表，其中每个字符串代表一篇文章'
- en: '`filenames` – A list of filenames corresponding to the articles in `texts`'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filenames` – 一个文件名列表，对应于`texts`中的文章'
- en: '`tokenizer` – A tokenizer that can process `texts`'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` – 一个可以处理`texts`的分词器'
- en: '`embeddings` – The embeddings as a `pd.DataFrame`, where each row represents
    a word vector, indexed by the corresponding token'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embeddings` – 以`pd.DataFrame`格式表示的嵌入，其中每一行代表一个词向量，按对应的标记索引'
- en: The function first preprocesses the texts by converting the strings to sequences,
    and then back to a list of strings. This helps us to use the built-in preprocessing
    functionalities of the tokenizer to clean the text. Next, each preprocessed string
    is split by the space character to return a list of tokens. Then we index all
    the positions in the embeddings matrix that corresponds to all the tokens in the
    text. Finally, the mean vector is computed for the document by computing the mean
    of all the chosen embedding vectors.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先通过将字符串转换为序列，然后再转换回字符串列表来预处理文本。这帮助我们利用分词器的内置预处理功能来清理文本。接下来，将每个预处理的字符串按空格字符拆分，以返回一个词元列表。然后，我们索引嵌入矩阵中所有与文本中所有词元对应的位置。最后，通过计算所有选择的嵌入向量的均值来计算文档的均值向量。
- en: 'With that, we can load the embeddings from different algorithms (skip-gram,
    CBOW, and GloVe), and compute the document embeddings. Here we will only show
    the process for the skip-gram algorithm. But you can easily extend it to the other
    algorithms, as they have similar inputs and outputs:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以加载来自不同算法（skip-gram、CBOW和GloVe）的嵌入，并计算文档嵌入。这里我们仅展示skip-gram算法的过程。但你可以轻松扩展到其他算法，因为它们有类似的输入和输出：
- en: '[PRE42]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now we will see how we can leverage the generated document embedding to train
    a classifier.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何利用生成的文档嵌入来训练分类器。
- en: Classifying documents with document embeddings
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用文档嵌入进行文档分类
- en: 'We will be training a simple multi-class (or a multinomial) logistic regression
    classifier on this data. The logistic regression model will look as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此数据上训练一个简单的多类（或多项式）逻辑回归分类器。逻辑回归模型将如下所示：
- en: '![](img/B14070_04_06.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_06.png)'
- en: 'Figure 4.6: This diagram depicts the multinomial logistic regression model.
    The model takes in an embedding vector and outputs a probability distribution
    over different available classes'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：该图描述了多项式逻辑回归模型。模型接受一个嵌入向量，并输出不同类别的概率分布
- en: It’s a very simple model with a single layer, where the input is the embedding
    vector (e.g. a 128-element-long vector), and the output is a 5-node softmax layer
    that will output the likelihood of the input belonging to each category, as a
    probability distribution.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的模型，只有一层，其中输入是嵌入向量（例如，一个包含128个元素的向量），输出是一个5节点的softmax层，该层会输出输入属于每个类别的可能性，作为一个概率分布。
- en: 'We will be training several models, as opposed to a single run. This will give
    us a more consistent result on the performance of the model. To implement the
    model, we’ll be using a popular general-purpose machine learning library called
    scikit-learn ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)).
    In each run, a multi-class logistic regression classifier is created with the
    `sklearn.linear_model.LogisticRegression` object. Additionally, in each run:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练多个模型，而不是仅仅一次运行。这将为我们提供一个更一致的模型性能结果。为了实现这个模型，我们将使用一个流行的通用机器学习库，名为scikit-learn（[https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)）。在每次运行中，都会创建一个多类逻辑回归分类器，使用`sklearn.linear_model.LogisticRegression`对象。此外，在每次运行中：
- en: The model is trained on the training inputs and targets
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型在训练输入和目标上进行训练
- en: The model predicts the class (a value from 0 to 4) for each test input, where
    the class of an input is the one that has the maximum probability from all classes
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型为每个测试输入预测类别（一个从0到4的值），其中输入的类别是所有类别中具有最大概率的类别
- en: The model computes the test accuracy using the predicted classes and true classes
    of the test set
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用测试集的预测类别和真实类别来计算测试准确度
- en: 'The code looks like the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下所示：
- en: '[PRE43]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'By setting `multi_class=''multinomial''`, we are making sure it’s a multi-class
    logistic regression model (or a softmax classifier). This will output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`multi_class='multinomial'`，我们确保这是一个多类逻辑回归模型（或softmax分类器）。这将输出：
- en: '[PRE44]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When you follow the procedure for all the skip-gram, CBOW, GloVe, and ELMo
    algorithms, you will see a result similar to the following. This is a box plot
    diagram. However, as performance is quite similar between trials, you won’t see
    much variation present in the diagram:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当你按步骤操作所有 skip-gram、CBOW、GloVe 和 ELMo 算法时，你会看到类似以下的结果。这是一个箱线图。然而，由于各次实验的表现相似，因此图表中不会出现太多变化。
- en: '![](img/B14070_04_07.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_04_07.png)'
- en: 'Figure 4.7: Box plot interpreting performance on document classification for
    different models. We can see that ELMo is a clear-cut winner, where GloVe performs
    the worst'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：不同模型在文档分类中的性能箱线图。我们可以看到，ELMo 是明显的赢家，而 GloVe 的表现最差。
- en: We can see that skip-gram achieves around 86% accuracy, followed closely by
    CBOW, which achieves on-par performance. Surprisingly GloVe achieves performance
    far below the skip-gram and CBOW, around 66% accuracy.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，skip-gram 达到了大约 86% 的准确率，紧随其后的是 CBOW，二者的表现相当。令人惊讶的是，GloVe 的表现远低于 skip-gram
    和 CBOW，准确率约为 66%。
- en: This could be pointing to a limitation of the GloVe loss function. Unlike, skip-gram
    and CBOW, which are considered both positive (observed) and negative (unobserved)
    target and context pairs, GloVe only focuses on observed pairs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能指向 GloVe 损失函数的一个限制。与 skip-gram 和 CBOW 不同，后者同时考虑正样本（已观察到）和负样本（未观察到）的目标和上下文对，而
    GloVe 只关注已观察到的对。
- en: This could be hurting GloVe’s ability to generate effective representations
    of words. Finally, ELMo achieves the best, which is around 98% accuracy. But it
    is important to keep in mind that ELMo has been trained on a much larger dataset
    than the BBC dataset, thus it is not fair to compare ELMo with other models just
    on this number.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会影响 GloVe 生成有效词表示的能力。最终，ELMo 达到了最佳性能，准确率大约为 98%。但需要注意的是，ELMo 是在比 BBC 数据集更大规模的数据集上训练的，因此仅根据这个数字将
    ELMo 与其他模型进行比较是不公平的。
- en: 'In this section, you learned how we can extend word embeddings turned to document
    embeddings and how these can be used in a downstream classifier model to classify
    documents. First, you learned about word embeddings using a selected algorithm
    (e.g. skip-gram, CBOW, and GloVe). Then we created document embeddings by averaging
    the word embeddings of all the words found in that document. This was the case
    for the skip-gram, CBOW, and GloVe algorithms. In the case of the ELMo algorithm,
    we were able to infer document embeddings straight from the model. Later we used
    these document embeddings to classify some BBC news articles that fall into these
    categories: entertainment, tech, politics, business, and sports.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你学习了如何将词嵌入扩展为文档嵌入，并且如何将这些嵌入用于下游分类模型进行文档分类。首先，你了解了使用选定算法（例如 skip-gram、CBOW
    和 GloVe）进行词嵌入。然后我们通过对文档中所有单词的词嵌入进行平均来创建文档嵌入。这适用于 skip-gram、CBOW 和 GloVe 算法。在 ELMo
    算法的情况下，我们能够直接从模型中推断出文档嵌入。随后，我们使用这些文档嵌入对一些 BBC 新闻文章进行分类，这些文章属于以下类别：娱乐、科技、政治、商业和体育。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed GloVe—another word embedding learning technique.
    GloVe takes the current Word2vec algorithms a step further by incorporating global
    statistics into the optimization, thus increasing the performance.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 GloVe——另一种词嵌入学习技术。GloVe 通过将全局统计信息纳入优化，进一步提升了当前 Word2Vec 算法的性能。
- en: Next, we learned about a much more advanced algorithm known as ELMo (which stands
    for Embeddings from Language Models). ELMo provides contextualized representations
    of words by looking at a word within a sentence or a phrase, not by itself.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了一个更为先进的算法，叫做 ELMo（即来自语言模型的嵌入）。ELMo 通过查看单词在句子或短语中的上下文，而不是孤立地看待单词，提供了上下文化的词表示。
- en: Finally, we discussed a real-world application of using word embeddings—document
    classification. We showed that word embeddings are very powerful and allow us
    to classify related documents with a simple multi-class logistic regression model
    reasonably well. ELMo performed the best out of skip-gram, CBOW, and GloVe, due
    to the vast amount of data it has been trained on.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了词嵌入的一个实际应用——文档分类。我们展示了词嵌入非常强大，并且允许我们用一个简单的多类逻辑回归模型相当好地分类相关文档。由于 ELMo
    在大量数据上进行了训练，因此其表现优于 skip-gram、CBOW 和 GloVe。
- en: In the next chapter, we will move on to discussing a different family of deep
    networks that are more powerful in exploiting spatial information present in data,
    known as **Convolutional Neural Networks** (**CNNs**).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论另一类深度网络，它们在利用数据中存在的空间信息方面更强大，称为**卷积神经网络**（**CNNs**）。
- en: Precisely, we will see how CNNs can be used to exploit the spatial structure
    of sentences to classify them into different classes.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将看到如何利用 CNNs 来挖掘句子的空间结构，将其分类到不同的类别中。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人交流，和超过1000名成员一起学习，网址为：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
