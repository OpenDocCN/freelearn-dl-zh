- en: 2\. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 神经网络
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter starts with an introduction to biological neurons; we see how an
    artificial neural network is inspired by biological neural networks. We will examine
    the structure and inner workings of a simple single-layer neuron called a perceptron
    and learn how to implement it in TensorFlow. We will move on to building multilayer
    neural networks to solve more complex multiclass classification tasks and discuss
    the practical considerations of designing a neural network. As we build deep neural
    networks, we will move on to Keras to build modular and easy-to-customize neural
    network models in Python. By the end of this chapter, you'll be adept at building
    neural networks to solve complex problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从介绍生物神经元开始，看看人工神经网络如何受生物神经网络的启发。我们将研究一个简单的单层神经元（称为感知器）的结构和内部工作原理，并学习如何在TensorFlow中实现它。接着，我们将构建多层神经网络来解决更复杂的多类分类任务，并讨论设计神经网络时的实际考虑。随着我们构建深度神经网络，我们将转向Keras，在Python中构建模块化且易于定制的神经网络模型。到本章结束时，你将能熟练地构建神经网络来解决复杂问题。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we learned how to implement basic mathematical concepts
    such as quadratic equations, linear algebra, and matrix multiplication in TensorFlow.
    Now that we have learned the basics, let's dive into **Artificial Neural Networks**
    (**ANNs**), which are central to artificial intelligence and deep learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何在TensorFlow中实现基本的数学概念，如二次方程、线性代数和矩阵乘法。现在，我们已经掌握了基础知识，让我们深入了解**人工神经网络**（**ANNs**），它们是人工智能和深度学习的核心。
- en: 'Deep learning is a subset of machine learning. In supervised learning, we often
    use traditional machine learning techniques, such as support vector machines or
    tree-based models, where features are explicitly engineered by humans. However,
    in deep learning, the model explores and identifies the important features of
    a labeled dataset without human intervention. ANNs, inspired by biological neurons,
    have a layered representation, which helps them learn labels incrementally—from
    the minute details to the complex ones. Consider the example of image recognition:
    in a given image, an ANN would just as easily identify basic details such as light
    and dark areas as it would identify more complex structures such as shapes. Though
    neural network techniques are tremendously successful at tasks such as identifying
    objects in images, how they do so is a black box, as the features are learned
    implicitly. Deep learning techniques have turned out to be powerful at tackling
    very complex problems, such as speech/image recognition, and hence are used across
    industry in building self-driving cars, Google Now, and many more applications.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集。在监督学习中，我们经常使用传统的机器学习技术，如支持向量机或基于树的模型，其中特征是由人工明确设计的。然而，在深度学习中，模型会在没有人工干预的情况下探索并识别标记数据集中的重要特征。人工神经网络（ANNs），受生物神经元的启发，具有分层表示，这有助于它们从微小的细节到复杂的细节逐步学习标签。以图像识别为例：在给定的图像中，ANN能够轻松识别诸如明暗区域这样的基本细节，也能识别更复杂的结构，如形状。尽管神经网络技术在识别图像中的物体等任务中取得了巨大成功，但它们的工作原理是一个黑箱，因为特征是隐式学习的。深度学习技术已经证明在解决复杂问题（如语音/图像识别）方面非常强大，因此被广泛应用于行业，如构建自动驾驶汽车、Google
    Now和许多其他应用。
- en: Now that we know the importance of deep learning techniques, we will take a
    pragmatic step-by-step approach to understanding a mix of theory and practical
    considerations in building deep-learning-based solutions. We will start with the
    smallest component of a neural network, which is an artificial neuron, also referred
    to as a perceptron, and incrementally increase the complexity to explore **Multi-Layer
    Perceptrons** (**MLPs**) and advanced models such as **Recurrent Neural Networks**
    (**RNNs**) and **Convolutional Neural Networks** (**CNNs**).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了深度学习技术的重要性，我们将采取一种务实的逐步方法，结合理论和实际考虑来理解构建基于深度学习的解决方案。我们将从神经网络的最小组件——人工神经元（也称为感知器）开始，逐步增加复杂性，探索**多层感知器**（**MLPs**）以及更先进的模型，如**递归神经网络**（**RNNs**）和**卷积神经网络**（**CNNs**）。
- en: Neural Networks and the Structure of Perceptrons
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与感知器的结构
- en: A neuron is a basic building block of the human nervous system, which relays
    electric signals across the body. The human brain consists of billions of interconnected
    biological neurons, and they are constantly communicating with each other by sending
    minute electrical binary signals by turning themselves on or off. The general
    meaning of a neural network is a network of interconnected neurons. In the current
    context, we are referring to ANNs, which are actually modeled on a biological
    neural network. The term artificial intelligence is derived from the fact that
    natural intelligence exists in the human brain (or any brain for that matter),
    and we humans are trying to simulate this natural intelligence artificially. Though
    ANNs are inspired by biological neurons, some of the advanced neural network architectures,
    such as CNNs and RNNs, do not actually mimic the behavior of a biological neuron.
    However, for ease of understanding, we will begin by drawing an analogy between
    the biological neuron and an artificial neuron (perceptron).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是人类神经系统的基本构建块，它在全身传递电信号。人脑由数十亿个相互连接的生物神经元组成，它们通过开启或关闭自己，不断发送微小的电二进制信号互相通信。神经网络的普遍含义是相互连接的神经元的网络。在当前的背景下，我们指的是人工神经网络（ANNs），它们实际上是基于生物神经网络的模型。人工智能这一术语源自于自然智能存在于人脑（或任何大脑）这一事实，而我们人类正在努力模拟这种自然智能。尽管人工神经网络受到生物神经元的启发，但一些先进的神经网络架构，如卷积神经网络（CNNs）和递归神经网络（RNNs），并没有真正模仿生物神经元的行为。然而，为了便于理解，我们将首先通过类比生物神经元和人工神经元（感知机）来开始。
- en: 'A simplified version of a biological neuron is represented in *Figure 2.1*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的简化版本在*图2.1*中表示：
- en: '![Figure 2.1: Biological neuron'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1：生物神经元'
- en: '](img/B15385_02_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_01.jpg)'
- en: 'Figure 2.1: Biological neuron'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：生物神经元
- en: 'This is a highly simplified representation. There are three main components:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个高度简化的表示。它有三个主要组件：
- en: The dendrites, which receive the input signals
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树突，接收输入信号
- en: The cell body, where the signal is processed in some form
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细胞体，信号在其中以某种形式进行处理
- en: The tail-like axon, through which the neuron transfers the signal out to the
    next neuron
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尾部状的轴突，通过它神经元将信号传递到下一个神经元
- en: 'A perceptron can also be represented in a similar way, although it is not a
    physical entity but a mathematical model. *Figure 2.2* shows a high-level representation
    of an artificial neuron:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机也可以用类似的方式表示，尽管它不是一个物理实体，而是一个数学模型。*图2.2*展示了人工神经元的高级表示：
- en: '![Figure 2.2: Representation of an artificial neuron'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2：人工神经元的表示'
- en: '](img/B15385_02_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_02.jpg)'
- en: 'Figure 2.2: Representation of an artificial neuron'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：人工神经元的表示
- en: 'In an artificial neuron, as in a biological one, there is an input signal.
    The central node conflates all the signals and fires the output signal if it is
    above a certain threshold. A more detailed representation of a perceptron is shown
    in *Figure 2.3*. Each component of this perceptron is explained in the sections
    that follow:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经元中，和生物神经元一样，有一个输入信号。中央节点将所有信号合并，如果信号超过某个阈值，它将触发输出信号。感知机的更详细表示在*图2.3*中展示。接下来的章节将解释这个感知机的每个组件：
- en: '![Figure 2.3: Representation of a perceptron'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3：感知机的表示'
- en: '](img/B15385_02_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_03.jpg)'
- en: 'Figure 2.3: Representation of a perceptron'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：感知机的表示
- en: 'A perceptron has the following components:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机有以下组成部分：
- en: Input layer
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Weights
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重
- en: Bias
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置
- en: Net input function
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络输入函数
- en: Activation function
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Let's look at these components and their TensorFlow implementations in detail
    by considering an `OR` table dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑一个`OR`表格数据集，详细查看这些组件及其TensorFlow实现。
- en: Input Layer
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入层
- en: Each example of input data is fed through the input layer. Referring to the
    representation shown in *Figure 2.3*, depending on the size of the input example,
    the number of nodes will vary from *x*1 to *x*m. The input data can be structured
    data (such as a CSV file) or unstructured data, such as an image. These inputs,
    *x*1 to *x*m, are called features (`m` refers to the number of features). Let's
    illustrate this with an example.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入数据的示例都会通过输入层传递。参考*图2.3*中的表示，根据输入示例的大小，节点的数量将从*x*1到*x*m不等。输入数据可以是结构化数据（如CSV文件）或非结构化数据，如图像。这些输入，*x*1到*x*m，被称为特征（`m`表示特征的数量）。我们通过一个例子来说明这一点。
- en: 'Let''s say the data is in the form of a table as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据以如下表格形式呈现：
- en: '![Figure 2.4: Sample input and output data – OR table'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4：样本输入和输出数据——OR 表'
- en: '](img/B15385_02_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_04.jpg)'
- en: 'Figure 2.4: Sample input and output data – OR table'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：样本输入和输出数据——OR 表
- en: 'Here, the inputs to the neuron are the columns *x*1 and *x*2, which correspond
    to one row. At this point, it may be difficult to comprehend, but for now, accept
    it that the data is fed one row at a time in an iterative manner during training.
    We will represent the input data and the true labels (output `y`) with the TensorFlow
    `Variable` class as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，神经元的输入是列 *x*1 和 *x*2，它们对应于一行。此时可能很难理解，但暂时可以接受这样一个事实：在训练过程中，数据是以迭代的方式，一次输入一行。我们将使用
    TensorFlow `Variable` 类如下表示输入数据和真实标签（输出 `y`）：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Weights
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重
- en: 'Weights are associated with each neuron, and the input features dictate how
    much influence each of the input features should have in computing the next node.
    Each neuron will be connected to all the input features. In the example, since
    there were two inputs (*x*1 and *x*2) and the input layer is connected to one
    neuron, there will be two weights associated with it: *w*1 and *w*2\. A weight
    is a real number; it can be positive or negative and is mathematically represented
    as `Variable` class as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 权重与每个神经元相关联，输入特征决定了每个输入特征在计算下一个节点时应有的影响力。每个神经元将与所有输入特征连接。在这个例子中，由于有两个输入（*x*1
    和 *x*2），且输入层与一个神经元连接，所以会有两个与之相关联的权重：*w*1 和 *w*2。权重是一个实数，可以是正数或负数，数学上表示为 `Variable`
    类，如下所示：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Weights would be of the following dimension: *number of input features × output
    size*.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的维度如下：*输入特征的数量 × 输出大小*。
- en: Bias
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏置
- en: In *Figure 2.3*, bias is represented by *b*, which is called additive bias.
    Every neuron has one bias. When *x* is zero, that is, no information is coming
    from the independent variables, then the output should be biased to just *b*.
    Like the weights, the bias also a real number, and the network has to learn the
    bias value to get the correct predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 2.3*中，偏置由 *b* 表示，称为加性偏置。每个神经元都有一个偏置。当 *x* 为零时，也就是说没有来自自变量的信息输入时，输出应该仅为 *b*。像权重一样，偏置也是一个实数，网络必须学习偏置值才能得到正确的预测结果。
- en: 'In TensorFlow, bias is the same size as the output size and can be represented
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，偏置与输出大小相同，可以表示如下：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Net Input Function
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 净输入函数
- en: 'The net input function, also commonly referred to as the input function, can
    be described as the sum of the products of the inputs and their corresponding
    weights plus the bias. Mathematically, it is represented as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 净输入函数，也常被称为输入函数，可以描述为输入与其对应权重的乘积之和，再加上偏置。数学上表示如下：
- en: '![Figure 2.5: Net input function in mathematical form'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：数学形式的净输入函数'
- en: '](img/B15385_02_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_05.jpg)'
- en: 'Figure 2.5: Net input function in mathematical form'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：数学形式的净输入函数
- en: 'Here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '*x*i: input data—*x*1 to *x*m'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*i：输入数据——*x*1 到 *x*m'
- en: '*w*i: weights—*w*1 to *w*m'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*i：权重——*w*1 到 *w*m'
- en: '*b*: additive bias'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*：加性偏置'
- en: 'As you can see, this formula involves inputs and their associated weights and
    biases. This can be written in vectorized form, and we can use matrix multiplication,
    which we learned about in *Chapter 1*, *Building Blocks of Deep Learning*. We
    will see this when we start the code demo. Since all the variables are numbers,
    the result of the net input function is just a number, a real number. The net
    input function can be easily implemented using the TensorFlow `matmul` functionality
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个公式涉及到输入及其相关的权重和偏置。可以以向量化的形式写出，我们可以使用矩阵乘法，这是我们在*第1章*《深度学习基础》中学过的内容。我们将在开始代码演示时看到这一点。由于所有变量都是数字，净输入函数的结果只是一个数字，一个实数。净输入函数可以通过
    TensorFlow 的 `matmul` 功能轻松实现，如下所示：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`W` stands for weight, `X` stands for input, and `B` stands for bias.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`W` 代表权重，`X` 代表输入，`B` 代表偏置。'
- en: Activation Function (G)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数（G）
- en: The output of the net input function (`z`) is fed as input to the activation
    function. The activation function squashes the output of the net input function
    (`z`) into a new output range depending on the choice of activation function.
    There are a variety of activation functions, such as sigmoid (logistic), ReLU,
    and tanh. Each activation function has its own pros and cons. We will take a deep
    dive into activation functions later in the chapter. For now, we will start with
    a sigmoid activation function, also known as a logistic function. With the sigmoid
    activation function, the linear output `z` is squashed into a new output range
    of (0,1). The activation function provides non-linearity between layers, which
    gives neural networks the ability to approximate any continuous function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 网络输入函数（`z`）的输出作为输入传递给激活函数。激活函数将网络输入函数（`z`）的输出压缩到一个新的输出范围，具体取决于激活函数的选择。有各种激活函数，如
    Sigmoid（逻辑函数）、ReLU 和 tanh。每个激活函数都有其优缺点。我们将在本章稍后深入探讨激活函数。现在，我们先从 Sigmoid 激活函数开始，也叫逻辑函数。使用
    Sigmoid 激活函数时，线性输出`z`被压缩到一个新的输出范围（0,1）。激活函数在层与层之间提供了非线性，这使得神经网络能够逼近任何连续函数。
- en: 'The mathematical equation of the sigmoid function is as follows, where *G(z)*
    is the sigmoid function and the right-hand equation details the derivative with
    respect to *z*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数的数学公式如下，其中*G(z)*是Sigmoid函数，右边的公式详细说明了关于*z*的导数：
- en: '![Figure 2.6: Mathematical form of the sigmoid function'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6：Sigmoid 函数的数学形式'
- en: '](img/B15385_02_06.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_06.jpg)'
- en: 'Figure 2.6: Mathematical form of the sigmoid function'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：Sigmoid 函数的数学形式
- en: 'As you can see in *Figure 2.7*, the sigmoid function is a more or less S-shaped
    curve with values between 0 and 1, no matter what the input is:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*图 2.7*中所见，Sigmoid 函数是一个大致为S形的曲线，其值介于0和1之间，无论输入是什么：
- en: '![Figure 2.7: Sigmoid curve'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7：Sigmoid 曲线'
- en: '](img/B15385_02_07.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_07.jpg)'
- en: 'Figure 2.7: Sigmoid curve'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：Sigmoid 曲线
- en: And if we set a threshold (say `0.5`), we can convert this into a binary output.
    Any output greater than or equal to `.5` is considered `1`, and any value less
    than `.5` is considered `0`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设定一个阈值（比如`0.5`），我们可以将其转换为二进制输出。任何大于或等于`.5`的输出被视为`1`，任何小于`.5`的值被视为`0`。
- en: 'Activation functions such as sigmoid are provided out of the box in TensorFlow.
    A sigmoid function can be implemented in TensorFlow as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了现成的激活函数，如 Sigmoid。可以如下在 TensorFlow 中实现 Sigmoid 函数：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have seen the structure of a perceptron and its code representation
    in TensorFlow, let's put all the components together to make a perceptron.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了感知机的结构以及其在 TensorFlow 中的代码表示，让我们把所有组件结合起来，构建一个感知机。
- en: Perceptrons in TensorFlow
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 中的感知机
- en: 'In TensorFlow, a perceptron can be implemented just by defining a simple function,
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，可以通过定义一个简单的函数来实现感知机，如下所示：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At a very high level, we can see that the input data passes through the net
    input function. The output of the net input function is passed to the activation
    function, which, in turn, gives us the predicted output. Now, let''s look at each
    line of the code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个非常高的层次上，我们可以看到输入数据通过网络输入函数。网络输入函数的输出会传递给激活函数，激活函数反过来给我们预测的输出。现在，让我们逐行看一下代码：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output of the net input function is stored in `z`. Let's see how we got
    that result by breaking it down further into two parts, that is, the matrix multiplication
    part contained in `tf.matmul` and the addition contained in `tf.add`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 网络输入函数的输出存储在`z`中。让我们通过进一步分解，将结果分为两部分来看，即`tf.matmul`中的矩阵乘法部分和`tf.add`中的加法部分。
- en: 'Let''s say we''re storing the result of the matrix multiplication of `X` and
    `W` in a variable called `m`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将`X`和`W`的矩阵乘法结果存储在一个名为`m`的变量中：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s consider how we got that result. For example, let''s say `X` is
    a row matrix, like [ X1 X2 ], and `W` is a column matrix, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一下如何得到这个结果。例如，假设`X`是一个行矩阵，如[ X1 X2 ]，而`W`是一个列矩阵，如下所示：
- en: '![Figure 2.8: Column matrix'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：列矩阵'
- en: '](img/B15385_02_08.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_08.jpg)'
- en: 'Figure 2.8: Column matrix'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：列矩阵
- en: 'Recall from the previous chapter that `tf.matmul` will perform matrix multiplication.
    So, the result is this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下上一章提到的，`tf.matmul`会执行矩阵乘法。因此，结果是这样的：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And then, we add the output, `m`, to the bias, `B`, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将输出`m`与偏置`B`相加，如下所示：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that what we do in the preceding step is the same as the mere addition
    of the two variables `m` and `b`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在前一步所做的与简单地将两个变量 `m` 和 `b` 相加是一样的：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Hence, the final output is:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的输出是：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`z` would be the output of the net input function.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`z` 将是净输入函数的输出。'
- en: 'Now, let''s consider the next line:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑下一行：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we learned earlier, `tf.sigmoid` is a readily available implementation of
    the sigmoid function. The net input function's output (`z`) computed in the previous
    line is fed as input to the sigmoid function. The result of the sigmoid function
    is the output of the perceptron, which is in the range of 0 to 1\. During training,
    which will be explained later in the chapter, we will feed the data in batches
    to this function, which will calculate the predicted values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前学到的，`tf.sigmoid` 是 Sigmoid 函数的现成实现。前一行计算的净输入函数的输出（`z`）作为输入传递给 Sigmoid
    函数。Sigmoid 函数的结果就是感知器的输出，范围在 0 到 1 之间。在训练过程中，稍后将在本章中解释，我们将数据批量地输入到这个函数中，函数将计算预测值。
- en: 'Exercise 2.01: Perceptron Implementation'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.01：感知器实现
- en: 'In this exercise, we will implement the perceptron in TensorFlow for an `OR`
    table. Let''s set the input data in TensorFlow and freeze the design parameters
    of perceptron:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将为 `OR` 表格实现一个感知器。在 TensorFlow 中设置输入数据并冻结感知器的设计参数：
- en: 'Let''s import the necessary package, which, in our case, is `tensorflow`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入必要的包，在我们的案例中是 `tensorflow`：
- en: '[PRE13]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Set the input data and labels of the `OR` table data in TensorFlow:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中设置 `OR` 表格数据的输入数据和标签：
- en: '[PRE14]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see in the output, we will have a 4 × 2 matrix of input data:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你在输出中看到的，我们将得到一个 4 × 2 的输入数据矩阵：
- en: '[PRE15]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will set the actual labels in TensorFlow and use the `reshape()` function
    to reshape the `y` vector into a 4 × 1 matrix:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在 TensorFlow 中设置实际的标签，并使用 `reshape()` 函数将 `y` 向量重塑为一个 4 × 1 的矩阵：
- en: '[PRE16]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is a 4 × 1 matrix, as follows:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出是一个 4 × 1 的矩阵，如下所示：
- en: '[PRE17]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now let's design parameters of a perceptron.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们设计感知器的参数。
- en: '*Number of neurons (units) = 1*'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*神经元数量（单位） = 1*'
- en: '*Number of features (inputs) = 2 (number of examples × number of features)*'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*特征数量（输入） = 2（示例数量 × 特征数量）*'
- en: 'The activation function will be the sigmoid function, since we are doing binary classification:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 激活函数将是 Sigmoid 函数，因为我们正在进行二元分类：
- en: '[PRE18]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, `X.shape[1]` will equal `2` (since the indices start
    with zero, `1` refers to the second index, which is `2`).
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上面的代码中，`X.shape[1]` 将等于 `2`（因为索引是从零开始的，`1` 指的是第二个索引，它的值是 `2`）。
- en: 'Define the connections weight matrix in TensorFlow:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义连接权重矩阵：
- en: '[PRE19]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The weight matrix would essentially be a columnar matrix as shown in the following
    figure. It will have the following dimension: *number of features (columns) ×
    output size*:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重矩阵本质上将是一个列矩阵，如下图所示。它将具有以下维度：*特征数量（列数） × 输出大小*：
- en: '![Figure 2.9: A columnar matrix'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.9：列矩阵](img/B15385_02_09.jpg)'
- en: '](img/B15385_02_09.jpg)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_09.jpg)'
- en: '[PRE20]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now create the variable for the bias:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建偏置的变量：
- en: '[PRE21]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There is only one bias per neuron, so in this case, the bias is just one number
    in the form of a single-element array. However, if we had a layer of 10 neurons,
    then it would be an array of 10 numbers—1 for each neuron.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个神经元只有一个偏置，因此在这种情况下，偏置只是一个单元素数组中的一个数字。然而，如果我们有一个包含 10 个神经元的层，那么它将是一个包含 10 个数字的数组——每个神经元对应一个。
- en: 'This will result in a 0-row matrix with a single element like this:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致一个零行矩阵，包含一个单一元素，如下所示：
- en: '[PRE22]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now that we have the weights and bias, the next step is to perform the computation
    to get the net input function, feed it to the activation function, and then get
    the final output. Let''s define a function called `perceptron` to get the output:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了权重和偏置，下一步是执行计算，得到净输入函数，将其输入到激活函数中，然后得到最终输出。让我们定义一个名为 `perceptron` 的函数来获取输出：
- en: '[PRE23]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be a 4 × 1 array that contains the predictions by our perceptron:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是一个 4 × 1 的数组，包含我们感知器的预测结果：
- en: '[PRE24]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, the predictions are not quite accurate. We will learn how to
    improve the results in the sections that follow.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，预测结果并不十分准确。我们将在接下来的章节中学习如何改进结果。
- en: Note
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3feF7MO](https://packt.live/3feF7MO).
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/3feF7MO](https://packt.live/3feF7MO)。
- en: You can also run this example online at [https://packt.live/2CkMiEE](https://packt.live/2CkMiEE).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2CkMiEE](https://packt.live/2CkMiEE)上运行这个示例。你必须执行整个Notebook才能获得预期的结果。
- en: In this exercise, we implemented a perceptron, which is a mathematical implementation
    of a single artificial neuron. Keep in mind that it is just the implementation
    of the model; we have not done any training. In the next section, we will see
    how to train the perceptron.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们实现了一个感知机，它是单个人工神经元的数学实现。请记住，这只是模型的实现，我们还没有进行任何训练。在下一节中，我们将看到如何训练感知机。
- en: Training a Perceptron
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练感知机
- en: 'To train a perceptron, we need the following components:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个感知机，我们需要以下组件：
- en: Data representation
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据表示
- en: Layers
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层
- en: Neural network representation
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络表示
- en: Loss function
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: Optimizer
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器
- en: Training loop
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'In the previous section, we covered most of the preceding components: the `perceptron()`,
    which uses a linear layer and a sigmoid layer to perform predictions. What we
    did in the previous section using input data and initial weights and biases is
    called **forward propagation**. The actual neural network training involves two
    stages: forward propagation and backward propagation. We will explore them in
    detail in the next few steps. Let''s look at the training process at a higher
    level:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讲解了大部分前面的组件：`perceptron()`，它使用线性层和sigmoid层来执行预测。我们在前一节中使用输入数据和初始权重与偏差所做的工作被称为**前向传播**。实际的神经网络训练涉及两个阶段：前向传播和反向传播。我们将在接下来的几步中详细探讨它们。让我们从更高的层次来看训练过程：
- en: A training iteration where the neural network goes through all the training
    examples is called an Epoch. This is one of the hyperparameters to be tweaked
    in order to train a neural network.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络遍历所有训练样本的训练迭代被称为一个Epoch。这是需要调整的超参数之一，以便训练神经网络。
- en: In each pass, a neural network does forward propagation, where data travels
    from the input to the output. As seen in *Exercise 2.01*, *Perceptron Implementation*,
    inputs are fed to the perceptron. Input data passes through the net input function
    and the activation function to produce the predicted output. The predicted output
    is compared with the labels or the ground truth, and the error or loss is calculated.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一轮传递中，神经网络都会进行前向传播，其中数据从输入层传输到输出层。正如在*练习 2.01*，*感知机实现*中所看到的，输入被馈送到感知机。输入数据通过网络输入函数和激活函数，生成预测输出。预测输出与标签或真实值进行比较，计算误差或损失。
- en: In order to make a neural network learn, learning being the adjustment of weights
    and biases in order to make correct predictions, there needs to be a **loss function**,
    which will calculate the error between an actual label and the predicted label.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了让神经网络学习（即调整权重和偏差以做出正确预测），需要有一个**损失函数**，它将计算实际标签和预测标签之间的误差。
- en: To minimize the error in the neural network, the training loop needs an **optimizer**,
    which will minimize the loss on the basis of a loss function.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化神经网络中的误差，训练循环需要一个**优化器**，它将基于损失函数来最小化损失。
- en: Once the error is calculated, the neural network then sees which nodes of the
    network contributed to the error and by how much. This is essential in order to
    make the predictions better in the next epoch. This way of propagating the error
    backward is called **backward propagation** (backpropagation). Backpropagation
    uses the chain rule from calculus to propagate the error (the error gradient)
    in reverse order until it reaches the input layer. As it propagates the error
    back through the network, it uses gradient descent to make fine adjustments to
    the weights and biases in the network by utilizing the error gradient calculated before.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦计算出误差，神经网络就会查看网络中哪些节点对误差产生了影响，以及影响的程度。这对于在下一轮训练中提高预测效果至关重要。这个向后传播误差的方式被称为**反向传播**（backpropagation）。反向传播利用微积分中的链式法则，以反向顺序传播误差（误差梯度），直到达到输入层。在通过网络反向传播误差时，它使用梯度下降法根据之前计算的误差梯度对网络中的权重和偏差进行微调。
- en: This cycle continues until the loss is minimized.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环会持续进行，直到损失最小化。
- en: 'Let''s implement the theory we have discussed in TensorFlow. Revisit the code
    in *Exercise 2.01*, *Perceptron Implementation,* where the perceptron we created
    just did one forward pass. We got the following predictions, and we saw that our
    perceptron had not learned anything:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 TensorFlow 中实现我们讨论过的理论。回顾一下 *练习 2.01*，*感知机实现*，在该练习中，我们创建的感知机只进行了一个前向传播。我们得到了以下预测结果，并且发现我们的感知机没有学到任何东西：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In order to make our perceptron learn, we need additional components, such as
    a training loop, a loss function, and an optimizer. Let's see how to implement
    these components in TensorFlow.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的感知机学习，我们需要一些额外的组件，例如训练循环、损失函数和优化器。让我们看看如何在 TensorFlow 中实现这些组件。
- en: Perceptron Training Process in TensorFlow
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 中的感知机训练过程
- en: 'In the next exercise, when we train our model, we will use a **Stochastic Gradient
    Descent** (**SGD**) optimizer to minimize the loss. There are a few more advanced
    optimizers available and provided by TensorFlow out of the box. We will look at
    the pros and cons of each of them in later sections. The following code will instantiate
    a stochastic gradient descent optimizer using TensorFlow:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，当我们训练模型时，我们将使用 **随机梯度下降**（**SGD**）优化器来最小化损失。TensorFlow 提供了一些更高级的优化器。我们将在后续部分讨论它们的优缺点。以下代码将使用
    TensorFlow 实例化一个随机梯度下降优化器：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `perceptron` function takes care of the forward propagation. For the backpropagation
    of the error, we have used an optimizer. `Tf.optimizers.SGD` creates an instance
    of an optimizer. SGD will update the parameters of the networks—weights and biases—on
    each example from the input data. We will discuss the functioning of the gradient
    descent optimizer in greater detail later in this chapter. We will also discuss
    the significance of the `0.01` parameter, which is known as the learning rate.
    The learning rate is the magnitude by which SGD takes a step in order to reach
    the global optimum of the loss function. The learning rate is another hyperparameter
    that needs to be tweaked in order to train a neural network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`perceptron` 函数负责前向传播。对于误差的反向传播，我们使用了一个优化器。`Tf.optimizers.SGD` 创建了一个优化器实例。SGD
    会在每个输入数据的示例上更新网络的参数——权重和偏置。我们将在本章后续部分更详细地讨论梯度下降优化器的工作原理。我们还会讨论 `0.01` 参数的意义，该参数被称为学习率。学习率是
    SGD 为了达到损失函数的全局最优解而采取的步伐的大小。学习率是另一个超参数，需要调节以训练神经网络。'
- en: 'The following code can be used to define the epochs, training loop, and loss
    function:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可用于定义训练周期、训练循环和损失函数：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Inside the training loop, the loss is calculated using the loss function, which
    is defined as a lambda function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环中，损失是通过损失函数计算的，损失函数被定义为一个 lambda 函数。
- en: 'The `tf.nn.sigmoid_cross_entropy_with_logits` function calculates the loss
    value of each observation. It takes two parameters: `Labels = y` and `logit =
    perceptron(x)`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.nn.sigmoid_cross_entropy_with_logits` 函数计算每个观测值的损失值。它接受两个参数：`Labels = y`
    和 `logit = perceptron(x)`。'
- en: '`perceptron(X)` returns the predicted value, which is the result of the forward
    propagation of the input, `x`. This is compared with the corresponding label value
    stored in `y`. The mean value is calculated using `Tf.reduce_mean`, and the magnitude
    is taken. The sign is ignored using the `abs` function. `Optimizer.minimize` takes
    the loss value and adjusts the weights and bias as a part of the backward propagation
    of the error.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`perceptron(X)` 返回预测值，这是输入 `x` 的前向传播结果。这个结果与存储在 `y` 中的相应标签值进行比较。使用 `Tf.reduce_mean`
    计算平均值，并取其大小。使用 `abs` 函数忽略符号。`Optimizer.minimize` 会根据损失值调整权重和偏置，这是误差反向传播的一部分。'
- en: The forward propagation is executed again with the new values of weights and
    bias. And this forward and backward process continues for the number of iterations
    we define.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的权重和偏置值再次执行前向传播。这个前向和反向过程会持续进行，直到我们定义的迭代次数结束。
- en: During the backpropagation, the weights and biases are updated only if the loss
    is less than the previous cycle. Otherwise, the weights and biases remain unchanged.
    In this way, the optimizer ensures that even though it loops through the required
    number of iterations, it only stores the values of `w` and `b` for which the loss
    is minimal.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，只有当损失小于上一个周期的损失时，权重和偏置才会被更新。否则，权重和偏置保持不变。通过这种方式，优化器确保尽管它会执行所需的迭代次数，但只会存储那些损失最小的
    `w` 和 `b` 值。
- en: We have set the number of epochs for the training to 1,000 iterations. There
    is no rule of thumb for setting the number of epochs since the number of epochs
    is a hyperparameter. But how do we know when training has taken place successfully?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练的轮数设置为1,000次迭代。设置训练轮数没有固定的经验法则，因为轮数是一个超参数。那么，我们如何知道训练是否成功呢？
- en: 'When we can see that the values of weights and biases have changed, we can
    conclude the training has taken place. Let''s say we used a training loop for
    the `OR` data we saw in *Exercise 2.01*, *Perceptron Implementation*, we would
    see weights somewhat equal to the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到权重和偏置的值发生变化时，我们可以得出结论，训练已经发生。假设我们使用了*练习 2.01*中的`OR`数据进行训练，并应用了感知机实现，我们会看到权重大致等于以下值：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And the bias would be something like this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置值可能是这样的：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'When the network has learned, that is, the weights and biases have been updated,
    we can see whether it is making accurate predictions using `accuracy_score` from
    the `scikit-learn` package. We can use it to measure the accuracy of the predictions
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络已经学习，即权重和偏置已经更新时，我们可以使用`scikit-learn`包中的`accuracy_score`来查看它是否做出了准确的预测。我们可以通过如下方式来测量预测的准确性：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, `accuracy_score` takes two parameters—the label values (`y`) and the predicted
    values (`ypred`)—and measures the accuracy. Let's say the result is `1.0`. This
    means the perceptron is 100% accurate.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`accuracy_score`接收两个参数——标签值（`y`）和预测值（`ypred`）——并计算准确率。假设结果是`1.0`，这意味着感知机的准确率为100%。
- en: In the next exercise, we will train our perceptron to perform a binary classification.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将训练感知机来执行二分类任务。
- en: 'Exercise 2.02: Perceptron as a Binary Classifier'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：感知机作为二分类器
- en: 'In the previous section, we learned how to train a perceptron. In this exercise,
    we will train our perceptron to approximate a slightly more complicated function.
    We will be using randomly generated external data with two classes: class `0`
    and class `1`. Our trained perceptron should be able to classify the random numbers
    based on their class:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何训练感知机。在本练习中，我们将训练感知机来近似一个稍微复杂一些的函数。我们将使用随机生成的外部数据，数据有两个类别：类别`0`和类别`1`。我们训练后的感知机应该能够根据类别来分类这些随机数：
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The data is in a CSV file called `data.csv`. You can download the file from
    GitHub by visiting [https://packt.live/2BVtxIf](https://packt.live/2BVtxIf).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在名为`data.csv`的CSV文件中。你可以通过访问[https://packt.live/2BVtxIf](https://packt.live/2BVtxIf)从GitHub下载该文件。
- en: 'Import the required libraries:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE31]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Apart from `tensorflow`, we will need `pandas` to read the data from the CSV
    file, `confusion_matrix` and `accuracy_score` to measure the accuracy of our perceptron
    after the training, and `matplotlib` to visualize the data.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了`tensorflow`，我们还需要`pandas`来从CSV文件读取数据，`confusion_matrix`和`accuracy_score`来衡量训练后感知机的准确性，以及`matplotlib`来可视化数据。
- en: 'Read the data from the `data.csv` file. It should be in the same path as the
    Jupyter Notebook file in which you are running this exercise''s code. Otherwise,
    you will have to change the path in the code before executing it:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`data.csv`文件中读取数据。该文件应与运行此练习代码的Jupyter Notebook文件在同一路径下。否则，在执行代码之前你需要更改路径：
- en: '[PRE32]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Examine the data:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据：
- en: '[PRE33]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output will be as follows:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.10: Contents of the DataFrame'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.10：DataFrame 的内容'
- en: '](img/B15385_02_10.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_10.jpg)'
- en: 'Figure 2.10: Contents of the DataFrame'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.10：DataFrame 的内容
- en: As you can see, the data has three columns. `x1` and `x2` are the features,
    and the `label` column contains the labels `0` or `1` for each observation. The
    best way to see this kind of data is through a scatter plot.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，数据有三列。`x1`和`x2`是特征，而`label`列包含每个观测的标签`0`或`1`。查看这种数据的最佳方式是通过散点图。
- en: 'Visualize the data by plotting it using `matplotlib`:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`绘制图表来可视化数据：
- en: '[PRE34]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output will be as follows:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.11: Scatter plot of external data'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.11：外部数据的散点图'
- en: '](img/B15385_02_11.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_11.jpg)'
- en: 'Figure 2.11: Scatter plot of external data'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.11：外部数据的散点图
- en: This shows the two distinct classes of the data shown by the two different shapes.
    Data with the label `0` is represented by a star, while data with the label `1`
    is represented by a triangle.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显示了数据的两个不同类别，通过两种不同的形状来表示。标签为`0`的数据用星号表示，而标签为`1`的数据用三角形表示。
- en: 'Prepare the data. This step is not unique to neural networks; you must have
    seen it in regular machine learning as well. Before submitting the data to a model
    for training, you split it into features and labels:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据。这一步骤不仅仅是神经网络特有的，你在常规机器学习中也一定见过。在将数据提交给模型进行训练之前，你需要将数据分割为特征和标签：
- en: '[PRE35]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`x_input` contains the features, `x1` and `x2`. The values at the end convert
    it into matrix format, which is what is expected as input when the tensors are
    created. `y_label` contains the labels in matrix format.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x_input`包含特征`x1`和`x2`。末尾的值将其转换为矩阵格式，这是创建张量时所期望的输入格式。`y_label`包含矩阵格式的标签。'
- en: 'Create TensorFlow variables for features and labels and typecast them to `float`:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建TensorFlow变量用于特征和标签，并将它们转换为`float`类型：
- en: '[PRE36]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The rest of the code is for the training of the perceptron, which we saw in
    *Exercise 2.01*, *Perceptron Implementation*:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩下的代码是用来训练感知器的，我们在*练习 2.01*，*感知器实现*中看过：
- en: '[PRE37]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The `#` symbol in the code snippet above denotes a code comment. Comments are
    added into code to help explain specific bits of logic.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码片段中的`#`符号表示代码注释。注释被添加到代码中以帮助解释特定的逻辑部分。
- en: 'Display the values of `weight` and `bias` to show that the perceptron has been trained:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示`weight`和`bias`的值，以展示感知器已经被训练过：
- en: '[PRE38]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE39]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Pass the input data to check whether the perceptron classifies it correctly:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入数据传递进去，检查感知器是否正确分类：
- en: '[PRE40]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Round off the output to convert it into binary format:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输出结果进行四舍五入，转换成二进制格式：
- en: '[PRE41]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Measure the accuracy using the `accuracy_score` method, as we did in the previous
    exercise:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`accuracy_score`方法来衡量准确性，正如我们在之前的练习中所做的那样：
- en: '[PRE42]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE43]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The perceptron gives 100% accuracy.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该感知器给出了100%的准确率。
- en: The confusion matrix helps to get the performance measurement of a model. We
    will plot the confusion matrix using the `scikit-learn` package.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵帮助评估模型的性能。我们将使用`scikit-learn`包来绘制混淆矩阵。
- en: '[PRE44]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output will be as follows:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将如下所示：
- en: '[PRE45]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: All the numbers are along the diagonal, that is, 12 values corresponding to
    class 0 and 9 values corresponding to class 1 are properly classified by our trained
    perceptron (which has achieved 100% accuracy).
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有的数字都位于对角线上，即，12个值对应于类别0，9个值对应于类别1，这些都被我们训练好的感知器正确分类（该感知器已达到100%的准确率）。
- en: Note
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gJ73bY](https://packt.live/3gJ73bY).
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要查看这个具体部分的源代码，请参考[https://packt.live/3gJ73bY](https://packt.live/3gJ73bY)。
- en: You can also run this example online at [https://packt.live/2DhelFw](https://packt.live/2DhelFw).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，访问[https://packt.live/2DhelFw](https://packt.live/2DhelFw)。你必须执行整个Notebook才能得到预期的结果。
- en: In this exercise, we trained our perceptron into a binary classifier, and it
    has done pretty well. In the next exercise, we will see how to create a multiclass
    classifier.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将感知器训练成了一个二分类器，并且表现得相当不错。在下一个练习中，我们将看到如何创建一个多分类器。
- en: Multiclass Classifier
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多分类器
- en: A classifier that can handle two classes is known as a **binary classifier**,
    like the one we saw in the preceding exercise. A classifier that can handle more
    than two classes is known as a **multiclass classifier**. We cannot build a multiclass
    classifier with a single neuron. Now we move from one neuron to one layer of multiple
    neurons, which is required for multiclass classifiers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以处理两类的分类器被称为**二分类器**，就像我们在之前的练习中看到的那样。一个可以处理多于两类的分类器被称为**多分类器**。我们无法使用单一神经元来构建多分类器。现在我们从一个神经元转变为一个包含多个神经元的层，这对于多分类器是必需的。
- en: A single layer of multiple neurons can be trained to be a multiclass classifier.
    Some of the key points are detailed here. You need as many neurons as the number
    of classes; that is, for a 3-class classifier, you need 3 neurons; for a 10-class
    classifier you need 10 neurons, and so on.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一层多个神经元可以被训练成一个多分类器。这里详细列出了一些关键点。你需要的神经元数量等于类别的数量；也就是说，对于一个3类的分类器，你需要3个神经元；对于一个10类的分类器，你需要10个神经元，依此类推。
- en: As we saw in binary classification, we used sigmoid (logistic layer) to get
    predictions in the range of 0 to 1\. In multiclass classification, we use a special
    type of activation function called the **Softmax** activation function to get
    probabilities across each class that sums to 1\. With the sigmoid function in
    a multiclass setting, the probabilities do not necessarily add up to 1, so Softmax
    is preferred.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在二分类中看到的，我们使用sigmoid（逻辑层）来获取0到1范围内的预测。在多类分类中，我们使用一种特殊类型的激活函数，称为**Softmax**激活函数，以获得每个类别的概率，总和为1。使用sigmoid函数进行多类分类时，概率不一定加起来为1，因此更倾向使用Softmax。
- en: Before we implement the multiclass classifier, let's explore the Softmax activation function.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现多类分类器之前，让我们先探索Softmax激活函数。
- en: The Softmax Activation Function
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax 激活函数
- en: 'The Softmax function is also known as the **normalized exponential function**.
    As the word **normalized** suggests, the Softmax function normalizes the input
    into a probability distribution that sums to 1\. Mathematically, it is represented
    as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数也被称为**归一化指数函数**。正如**归一化**一词所暗示的，Softmax函数将输入归一化为一个总和为1的概率分布。从数学角度来看，它表示为：
- en: '![Figure 2.12: Mathematical form of the Softmax function'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12：Softmax函数的数学形式'
- en: '](img/B15385_02_12.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_12.jpg)'
- en: 'Figure 2.12: Mathematical form of the Softmax function'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：Softmax函数的数学形式
- en: To understand what Softmax does, let's use TensorFlow's built-in `softmax` function
    and see the output.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Softmax的作用，让我们使用TensorFlow内置的`softmax`函数并查看输出。
- en: 'So, for the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对于以下代码：
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output will be:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果将是：
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As you can see in the output, the `values` input is mapped to a probability
    distribution that sums to 1\. Note that `7` (the highest value in the original
    input values) received the highest weight, `0.824637055`. This is what the Softmax
    function is mainly used for: to focus on the largest values and suppress values
    that are below the maximum value. Also, if we sum the output, it adds up to ~
    1.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，输出中`values`输入被映射到一个概率分布，且总和为1。注意，`7`（原始输入值中的最大值）获得了最高的权重，`0.824637055`。这正是Softmax函数的主要用途：专注于最大值，并抑制低于最大值的值。此外，如果我们对输出求和，结果将接近1。
- en: 'Illustrating the example in more detail, let''s say we want to build a multiclass
    classifier with 3 classes. We will need 3 neurons connected to a Softmax activation
    function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 详细说明该示例，假设我们想构建一个包含3个类别的多类分类器。我们将需要连接到Softmax激活函数的3个神经元：
- en: '![Figure 2.13: Softmax activation function used in a multiclass classification
    setting'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13：在多类分类设置中使用的Softmax激活函数'
- en: '](img/B15385_02_13.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_13.jpg)'
- en: 'Figure 2.13: Softmax activation function used in a multiclass classification
    setting'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：在多类分类设置中使用的Softmax激活函数
- en: 'As seen in *Figure 2.13*, `x`1, `x`2, and `x`3 are the input features, which
    go through the net input function of each of the three neurons, which have the
    weights and biases (`W`i, j and `b`i) associated with it. Lastly, the output of
    the neuron is fed to the common Softmax activation function instead of the individual
    sigmoid functions. The Softmax activation function spits out the probabilities
    of the 3 classes: `P1`, `P2`, and `P3`. The sum of these three probabilities will
    add to 1 because of the Softmax layer.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 2.13*所示，`x`1、`x`2和`x`3是输入特征，它们经过每个神经元的网络输入函数，这些神经元具有与之相关的权重和偏置（`W`i，j 和
    `b`i）。最后，神经元的输出被送入通用的Softmax激活函数，而不是单独的sigmoid函数。Softmax激活函数输出3个类别的概率：`P1`、`P2`和`P3`。由于Softmax层的存在，这三个概率的总和将为1。
- en: 'As we saw in the previous section, Softmax highlights the maximum value and
    suppresses the rest of the values. Suppose a neural network is trained to classify
    the input into three classes, and for a given set of inputs, the output is class
    2; then it would say that `P2` has the highest value since it is passed through
    a Softmax layer. As you can see in the following figure, `P2` has the highest
    value, which means the prediction is correct:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一部分看到的，Softmax突出最大值并抑制其余的值。假设一个神经网络被训练来将输入分类为三个类别，对于给定的输入集，输出为类别2；那么它会说`P2`具有最高值，因为它经过了Softmax层。如下面的图所示，`P2`具有最高值，这意味着预测是正确的：
- en: '![Figure 2.14: Probability P2 is the highest'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14：概率P2最大'
- en: '](img/B15385_02_14.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_14.jpg)'
- en: 'Figure 2.14: Probability P2 is the highest'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：概率P2最大
- en: 'An associated concept is one-hot encoding. As we have three different classes,
    `class1`, `class2`, and `class3`, we need to encode the class labels into a format
    that we can work with more easily; so, after applying one-hot encoding, we would
    see the following output:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 相关概念是独热编码。由于我们有三个不同的类别，`class1`、`class2`和`class3`，我们需要将类别标签编码为便于操作的格式；因此，应用独热编码后，我们会看到如下输出：
- en: '![Figure 2.15: One-hot encoded data for three classes'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：三个类别的独热编码数据'
- en: '](img/B15385_02_15.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_15.jpg)'
- en: 'Figure 2.15: One-hot encoded data for three classes'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：三个类别的独热编码数据
- en: 'This makes the results quick and easy to interpret. In this case, the output
    that has the highest value is set to 1, and all others are set to 0\. The one-hot
    encoded output of the preceding example would be like this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以使结果快速且容易解释。在这种情况下，值最高的输出被设置为1，所有其他值设置为0。上述例子的独热编码输出将如下所示：
- en: '![ Figure 2.16: One-hot encoded output probabilities'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16：独热编码的输出概率'
- en: '](img/B15385_02_16.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_16.jpg)'
- en: 'Figure 2.16: One-hot encoded output probabilities'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16：独热编码的输出概率
- en: The labels of the training data also need to be one-hot encoded. And if they
    have a different format, they need to be converted into one-hot-encoded format
    before training the model. Let's do an exercise on multiclass classification with
    one-hot encoding.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的标签也需要进行独热编码。如果它们格式不同，则需要在训练模型之前将其转换为独热编码格式。让我们进行一次关于独热编码的多类分类练习。
- en: 'Exercise 2.03: Multiclass Classification Using a Perceptron'
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.03：使用感知机进行多类分类
- en: 'To perform multiclass classification, we will be using the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)),
    which has 3 classes of 50 instances each, where each class refers to a type of
    Iris. We will have a single layer of three neurons using the Softmax activation
    function:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行多类分类，我们将使用鸢尾花数据集（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)），该数据集包含3个类别，每个类别有50个实例，每个类别代表一种鸢尾花。我们将使用一个包含三个神经元的单层，采用Softmax激活函数：
- en: Note
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can download the dataset from GitHub using this link: [https://packt.live/3ekiBBf](https://packt.live/3ekiBBf).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过这个链接从GitHub下载数据集：[https://packt.live/3ekiBBf](https://packt.live/3ekiBBf)。
- en: 'Import the required libraries:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE48]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: You must be familiar with all of these imports as they were used in the previous
    exercise, except for `get_dummies`. This function converts a given label data
    into the corresponding one-hot-encoded format.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该熟悉所有这些导入，因为它们在前一个练习中已使用过，除了`get_dummies`。此函数将给定的标签数据转换为相应的独热编码格式。
- en: 'Load the `iris.csv` data:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`iris.csv`数据：
- en: '[PRE49]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s examine the first five rows of the data:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看数据的前五行：
- en: '[PRE50]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output will be as follows:'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.17: Contents of the DataFrame'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.17：DataFrame的内容'
- en: '](img/B15385_02_17.jpg)'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_17.jpg)'
- en: 'Figure 2.17: Contents of the DataFrame'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.17：DataFrame的内容
- en: 'Visualize the data by using a scatter plot:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用散点图可视化数据：
- en: '[PRE51]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The resulting plot will be as follows. The *x* axis denotes the sepal length
    and the *y* axis denotes the sepal width. The shapes in the plot represent the
    three species of Iris, setosa (star), versicolor (triangle), and virginica (circle):'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果图如下所示。*x*轴表示花萼长度，*y*轴表示花萼宽度。图中的形状表示三种鸢尾花的品种，setosa（星形）、versicolor（三角形）和virginica（圆形）：
- en: '![Figure 2.18: Iris data scatter plot'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.18：鸢尾花数据散点图'
- en: '](img/B15385_02_18.jpg)'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_18.jpg)'
- en: 'Figure 2.18: Iris data scatter plot'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.18：鸢尾花数据散点图
- en: There are three classes, as can be seen in the visualization, denoted by different shapes.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如可视化所示，共有三个类别，用不同的形状表示。
- en: 'Separate the features and the labels:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征和标签分开：
- en: '[PRE52]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`values` will transform the features into matrix format.'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`values`将把特征转换为矩阵格式。'
- en: 'Prepare the data by doing one-hot encoding on the classes:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对类别进行独热编码来准备数据：
- en: '[PRE53]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`get_dummies(y)` will convert the labels into one-hot-encoded format.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`get_dummies(y)`将把标签转换为独热编码格式。'
- en: 'Create a variable to load the features and typecast it to `float32`:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个变量来加载特征，并将其类型转换为`float32`：
- en: '[PRE54]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Implement the `perceptron` layer with three neurons:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用三个神经元实现`感知机`层：
- en: '[PRE55]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The code looks very similar to the single perceptron implementation. Only the
    `Number_of_units` parameter is set to `3`. Therefore, the weight matrix will be
    4 x 3 and the bias matrix will be 1 x 3.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码看起来与单一感知机实现非常相似。只是将`Number_of_units`参数设置为`3`。因此，权重矩阵将是4 x 3，偏置矩阵将是1 x 3。
- en: 'The other change is in the activation function:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个变化是在激活函数中：
- en: '`Output=tf.nn.softmax(x)`'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Output=tf.nn.softmax(x)`'
- en: We are using `softmax` instead of `sigmoid`.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用的是`softmax`而不是`sigmoid`。
- en: 'Create an instance of the `optimizer`. We will be using the `Adam` optimizer.
    At this point, you can think of `Adam` as an improved version of gradient descent
    that converges faster. We will cover it in detail later in the chapter:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`optimizer`实例。我们将使用`Adam`优化器。在这一点上，你可以将`Adam`视为一种改进版的梯度下降法，它收敛速度更快。我们将在本章稍后详细讲解：
- en: '[PRE56]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Define the training function:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练函数：
- en: '[PRE57]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Again, the code looks very similar to the single-neuron implementation except
    for the loss function. Instead of `sigmoid_cross_entropy_with_logits`, we use
    `softmax_cross_entropy_with_logits`.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次说明，代码看起来与单神经元实现非常相似，唯一的不同是损失函数。我们使用的是`softmax_cross_entropy_with_logits`，而不是`sigmoid_cross_entropy_with_logits`。
- en: 'Run the training for `1000` iterations:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行训练`1000`次迭代：
- en: '[PRE58]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Print the values of the weights to see if they have changed. This is also an
    indication that our perceptron is learning:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印权重值以查看它们是否发生了变化。这也是我们感知器在学习的一个标志：
- en: '[PRE59]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output shows the learned weights of our perceptron:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出显示我们感知器学习到的权重：
- en: '[PRE60]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To test the accuracy, we feed the features to predict the output and then calculate
    the accuracy using `accuracy_score`, like in the previous exercise:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试准确率，我们将特征输入以预测输出，然后使用`accuracy_score`计算准确率，就像在前面的练习中一样：
- en: '[PRE61]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出为：
- en: '[PRE62]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: It has given 98% accuracy, which is pretty good.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它的准确率达到了98%，非常不错。
- en: Note
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Dhes3U](https://packt.live/2Dhes3U).
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/2Dhes3U](https://packt.live/2Dhes3U)。
- en: You can also run this example online at [https://packt.live/3iJJKkm](https://packt.live/3iJJKkm).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址是[https://packt.live/3iJJKkm](https://packt.live/3iJJKkm)。你必须执行整个Notebook才能得到期望的结果。
- en: In this exercise, we performed multiclass classification using our perceptron.
    Let's do a more complex and interesting case study of the handwritten digit recognition
    dataset in the next section.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们使用感知器进行了多类分类。接下来，我们将进行一个更复杂、更有趣的手写数字识别数据集的案例研究。
- en: MNIST Case Study
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST案例研究
- en: Now that we have seen how to train a single neuron and a single layer of neurons,
    let's take a look at more realistic data. MNIST is a famous case study. In the
    next exercise, we will create a 10-class classifier to classify the MNIST dataset.
    However, before that, you should get a good understanding of the MNIST dataset.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了如何训练单个神经元和单层神经网络，接下来让我们看看更现实的数据。MNIST是一个著名的案例研究。在下一个练习中，我们将创建一个10类分类器来分类MNIST数据集。不过，在那之前，你应该对MNIST数据集有一个充分的了解。
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) refers
    to the modified dataset that the team led by Yann LeCun worked with at NIST. This
    project was aimed at handwritten digit recognition using neural networks.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改版国家标准与技术研究院**（**MNIST**）是指由Yann LeCun领导的团队在NIST使用的修改数据集。这个项目的目标是通过神经网络进行手写数字识别。'
- en: 'We need to understand the dataset before we get into writing the code. The
    MNIST dataset is integrated into the TensorFlow library. It consists of 70,000
    handwritten images of the digits 0 to 9:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写代码之前，我们需要了解数据集。MNIST数据集已经集成到TensorFlow库中。它包含了70,000张手写数字0到9的图像：
- en: '![Figure 2.19: Handwritten digits'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.19：手写数字'
- en: '](img/B15385_02_19.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_19.jpg)'
- en: 'Figure 2.19: Handwritten digits'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19：手写数字
- en: When we say images, you might think these are JPEG files, but they are not.
    They are actually stored in the form of pixel values. As far as the computer is
    concerned, an image is a bunch of numbers. These numbers are pixel values ranging
    from 0 to 255\. The dimension of each of these images is 28 x 28\. The images
    are stored in the form of a 28 x 28 matrix, each cell containing real numbers
    ranging from 0 to 255\. These are grayscale images (commonly known as black and
    white). 0 indicates white and 1 indicates complete black, and values in between
    indicate a certain shade of gray. The MNIST dataset is split into 60,000 training
    images and 10,000 test images.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到图像时，你可能会认为它们是JPEG文件，但实际上它们不是。它们是以像素值的形式存储的。从计算机的角度来看，图像就是一堆数字。这些数字是从0到255之间的像素值。这些图像的维度是28
    x 28。图像是以28 x 28矩阵的形式存储的，每个单元包含从0到255之间的实数。这些是灰度图像（通常称为黑白图像）。0表示白色，1表示完全黑色，中间的值表示不同深浅的灰色。MNIST数据集分为60,000张训练图像和10,000张测试图像。
- en: Each image has a label associated with it ranging from 0 to 9\. In the next
    exercise, let's build a 10-class classifier to classify the handwritten MNIST
    images.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图片都有一个标签，标签范围从0到9。下一次练习中，我们将构建一个10类分类器来分类手写的MNIST图片。
- en: 'Exercise 2.04: Classifying Handwritten Digits'
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习2.04：分类手写数字
- en: 'In this exercise, we will build a single-layer 10-class classifier consisting
    of 10 neurons with the Softmax activation function. It will have an input layer
    of 784 pixels:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建一个由10个神经元组成的单层10类分类器，采用Softmax激活函数。它将有一个784像素的输入层：
- en: 'Import the required libraries and packages just like we did in the earlier
    exercise:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库和包，就像我们在前面的练习中做的那样：
- en: '[PRE63]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create an instance of the MNIST dataset:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建MNIST数据集的实例：
- en: '[PRE64]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the MNIST dataset''s `train` and `test` data:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集的`train`和`test`数据：
- en: '[PRE65]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Normalize the data:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行归一化：
- en: '[PRE66]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Flatten the 2-dimensional images into row matrices. So, a 28 × 28 pixel gets
    flattened to `784` using the `reshape` function:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二维图像展平为行矩阵。因此，一个28 × 28像素的图像将被展平为`784`，使用`reshape`函数：
- en: '[PRE67]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create a `Variable` with the features and typecast it to `float32`:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`Variable`，并将其类型转换为`float32`：
- en: '[PRE68]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Create a one-hot encoding of the labels and transform it into a matrix:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建标签的独热编码并将其转换为矩阵：
- en: '[PRE69]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Create the single-layer neural network with `10` neurons and train it for `1000` iterations:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含`10`个神经元的单层神经网络，并训练`1000`次：
- en: '[PRE70]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Prepare the test data to measure the accuracy:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备测试数据以评估准确率：
- en: '[PRE71]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Run the predictions by passing the test data through the network:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将测试数据传入网络来进行预测：
- en: '[PRE72]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Calculate the accuracy:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算准确率：
- en: '[PRE73]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The predicted accuracy is:'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测的准确率是：
- en: '[PRE74]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Note
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3efd7Yh](https://packt.live/3efd7Yh).
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 若要访问此部分的源代码，请参考[https://packt.live/3efd7Yh](https://packt.live/3efd7Yh)。
- en: You can also run this example online at [https://packt.live/2Oc83ZW](https://packt.live/2Oc83ZW).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，地址是[https://packt.live/2Oc83ZW](https://packt.live/2Oc83ZW)。你必须执行整个Notebook才能获得期望的结果。
- en: In this exercise, we saw how to create a single-layer multi-neuron neural network
    and train it as a multiclass classifier.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们展示了如何创建一个单层多神经元神经网络，并将其训练为一个多类分类器。
- en: The next step is to build a multilayer neural network. However, before we do
    that, we must learn about the Keras API, since we use Keras to build dense neural
    networks.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建一个多层神经网络。然而，在此之前，我们必须了解Keras API，因为我们使用Keras来构建密集神经网络。
- en: Keras as a High-Level API
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras作为高级API
- en: In TensorFlow 1.0, there were several APIs, such as Estimator, Contrib, and
    layers. In TensorFlow 2.0, Keras is very tightly integrated with TensorFlow, and
    it provides a high-level API that is user-friendly, modular, composable, and easy
    to extend in order to build and train deep learning models. This also makes developing
    code for neural networks much easier. Let's see how it works.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 1.0中，有多个API，比如Estimator、Contrib和layers。而在TensorFlow 2.0中，Keras与TensorFlow紧密集成，提供了一个用户友好的、高级的API，具有模块化、可组合且易于扩展的特性，可以用来构建和训练深度学习模型。这也使得开发神经网络代码变得更加简单。让我们来看它是如何工作的。
- en: 'Exercise 2.05: Binary Classification Using Keras'
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习2.05：使用Keras进行二分类
- en: 'In this exercise, we will implement a very simple binary classifier with a
    single neuron using the Keras API. We will use the same `data.csv` file that we
    used in *Exercise 2.02*, *Perceptron as a Binary Classifier*:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用Keras API实现一个非常简单的二分类器，只有一个神经元。我们将使用与*练习 2.02*、*感知机作为二分类器*中相同的`data.csv`文件：
- en: Note
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be downloaded from GitHub by accessing the following GitHub
    link: https://packt.live/2BVtxIf.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过访问以下GitHub链接进行下载：https://packt.live/2BVtxIf。
- en: 'Import the required libraries:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE75]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: In the code, `Sequential` is the type of Keras model that we will be using because
    it is very easy to add layers to it. `Dense` is the type of layer that will be
    added. These are the regular neural network layers as opposed to the convolutional
    layers or pooling layers that will be used later on.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在代码中，`Sequential`是我们将使用的Keras模型类型，因为它非常容易向其中添加层。`Dense`是将要添加的层类型。这些是常规的神经网络层，而不是稍后将使用的卷积层或池化层。
- en: 'Import the data:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据：
- en: '[PRE76]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Inspect the data:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据：
- en: '[PRE77]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The following will be the output:'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 2.20: Contents of the DataFrame'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.20：DataFrame的内容'
- en: '](img/B15385_02_20.jpg)'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_20.jpg)'
- en: 'Figure 2.20: Contents of the DataFrame'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.20：DataFrame的内容
- en: 'Visualize the data using a scatter plot:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用散点图可视化数据：
- en: '[PRE78]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The resulting plot is as follows, with the *x* axis denoting `x1` values and
    the y-axis denoting `x2` values:'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的图形如下，*x*轴表示`x1`值，y轴表示`x2`值：
- en: '![Figure 2.21: Scatter plot of the data'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.21：数据的散点图'
- en: '](img/B15385_02_21.jpg)'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_21.jpg)'
- en: 'Figure 2.21: Scatter plot of the data'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.21：数据的散点图
- en: 'Prepare the data by separating the features and labels and setting the `tf` variables:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分离特征和标签并设置`tf`变量来准备数据：
- en: '[PRE79]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Create a neural network model consisting of a single layer with a neuron and
    a sigmoid activation function:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个神经网络模型，由一个神经元和一个sigmoid激活函数组成：
- en: '[PRE80]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The parameters in `mymodel.add(Dense())` are as follows: `units` is the number
    of neurons in the layer; `input_dim` is the number of features, which in this
    case is `2`; and `activation` is `sigmoid`.'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`mymodel.add(Dense())`中的参数如下：`units`是该层神经元的数量；`input_dim`是特征的数量，在此案例中为`2`；`activation`是`sigmoid`。'
- en: 'Once the model is created, we use the `compile` method to pass the additional
    parameters that are needed for training, such as the type of the optimizer, the
    loss function, and so on:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，我们使用`compile`方法传入训练所需的额外参数，如优化器类型、损失函数等：
- en: '[PRE81]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In this case, we are using the `adam` optimizer, which is an enhanced version
    of the gradient descent optimizer, and the loss function is `binary_crossentropy`,
    since this is a binary classifier.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个案例中，我们使用了`adam`优化器，这是梯度下降优化器的增强版，损失函数是`binary_crossentropy`，因为这是一个二分类器。
- en: The `metrics` parameter is almost always set to `['accuracy']`, which is used
    to display information such as the number of epochs, the training loss, the training
    accuracy, the test loss, and the test accuracy during the training process.
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`metrics`参数几乎总是设置为`[''accuracy'']`，用于显示如训练轮数、训练损失、训练准确度、测试损失和测试准确度等信息。'
- en: 'The model is now ready to be trained. However, it is a good idea to check the
    configuration of the model by using the `summary` function:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已准备好进行训练。然而，使用`summary`函数检查模型配置是个好主意：
- en: '[PRE82]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output will be as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.22: Summary of the sequential model'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.22：顺序模型摘要'
- en: '](img/B15385_02_22.jpg)'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_22.jpg)'
- en: 'Figure 2.22: Summary of the sequential model'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.22：顺序模型摘要
- en: 'Train the model by calling the `fit()` method:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fit()`方法来训练模型：
- en: '[PRE83]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'It takes the features and labels as the data parameters along with the number
    of epochs, which in this case is `1000`. The model will start training and will
    continuously provide the status as shown here:'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它接受特征和标签作为数据参数，并包含训练的轮数，在此案例中为`1000`。模型将开始训练，并会持续显示状态，如下所示：
- en: '![Figure 2.23: Model training logs using Keras'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.23：使用Keras的模型训练日志'
- en: '](img/B15385_02_23.jpg)'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_23.jpg)'
- en: 'Figure 2.23: Model training logs using Keras'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.23：使用Keras的模型训练日志
- en: 'We will evaluate our model using Keras''s `evaluate` functionality:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Keras的`evaluate`功能来评估模型：
- en: '[PRE84]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output is as follows:'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE85]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: As you can see, our Keras model is able to train well, as our accuracy is 100%.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，我们的Keras模型训练得非常好，准确率达到了100%。
- en: Note
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZVV1VY](https://packt.live/2ZVV1VY).
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定章节的源代码，请参考[https://packt.live/2ZVV1VY](https://packt.live/2ZVV1VY)。
- en: You can also run this example online at [https://packt.live/38CzhTc](https://packt.live/38CzhTc).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址是[https://packt.live/38CzhTc](https://packt.live/38CzhTc)。你必须执行整个笔记本才能得到期望的结果。
- en: In this exercise, we have learned how to build a perceptron using Keras. As
    you have seen, Keras makes the code more modular and more readable, and the parameters
    easier to tweak. In the next section, we will see how to build a multilayer or
    deep neural network using Keras.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们学习了如何使用 Keras 构建感知机。正如你所见，Keras 使代码更加模块化、更具可读性，而且参数调整也更为简便。在下一节中，我们将学习如何使用
    Keras 构建多层或深度神经网络。
- en: Multilayer Neural Network or Deep Neural Network
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层神经网络或深度神经网络
- en: 'In the previous example, we developed a single-layer neural network, often
    referred to as a shallow neural network. A diagram of this follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们开发了一个单层神经网络，通常称为浅层神经网络。其示意图如下所示：
- en: '![Figure 2.24: Shallow neural network'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.24：浅层神经网络](img/B15385_02_24.jpg)'
- en: '](img/B15385_02_24.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_24.jpg)'
- en: 'Figure 2.24: Shallow neural network'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.24：浅层神经网络
- en: 'One layer of neurons is not sufficient to solve more complex problems, such
    as face recognition or object detection. You need to stack up multiple layers.
    This is often referred to as creating a deep neural network. A diagram of this
    follows:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 一层神经元不足以解决更复杂的问题，如人脸识别或物体检测。你需要堆叠多个层，这通常被称为创建深度神经网络。其示意图如下所示：
- en: '![Figure 2.25: Deep neural network'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.25：深度神经网络](img/B15385_02_28.jpg)'
- en: '](img/B15385_02_25.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_26.jpg)'
- en: 'Figure 2.25: Deep neural network'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.25：深度神经网络
- en: Before we jump into the code, let's try to understand how this works. Input
    data is fed to the neurons in the first layer. It must be noted that every input
    is fed to every neuron in the first layer, and every neuron has one output. The
    output from each neuron in the first layer is fed to every neuron in the second
    layer. The output of each neuron in the second layer is fed to every neuron in
    the third layer, and so on.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入代码之前，让我们试着理解一下这个过程是如何工作的。输入数据被馈送到第一层的神经元。需要注意的是，每个输入都会馈送到第一层的每个神经元，并且每个神经元都有一个输出。第一层每个神经元的输出会被馈送到第二层的每个神经元，第二层每个神经元的输出会被馈送到第三层的每个神经元，依此类推。
- en: 'That is why this kind of network is also referred to as a dense neural network
    or a fully connected neural network. There are other types of neural networks
    with different workings, such as CNNs, but that is something we will discuss in
    the next chapter. There is no set rule about the number of neurons in each layer.
    This is usually determined by trial and error in a process known as hyperparameter
    tuning (which we''ll learn about later in the chapter). However, when it comes
    to the number of neurons in the last layers, there are some restrictions. The
    configuration of the last layer is determined as follows:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种网络也被称为密集神经网络或全连接神经网络。还有其他类型的神经网络，其工作原理不同，比如卷积神经网络（CNN），但这些内容我们将在下一章讨论。每一层中神经元的数量没有固定规则，通常通过试错法来确定，这个过程叫做超参数调优（我们将在本章后面学习）。然而，在最后一层神经元的数量上，是有一些限制的。最后一层的配置如下所示：
- en: '![Figure 2.26: Last layer configuration'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.26：最后一层配置](img/B15385_02_26.jpg)'
- en: '](img/B15385_02_26.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_26.jpg)'
- en: 'Figure 2.26: Last layer configuration'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26：最后一层配置
- en: ReLU Activation Function
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU 激活函数
- en: One last thing to do before we implement the code for deep neural networks is
    learn about the ReLU activation function. This is one of the most popular activation
    functions used in multilayer neural networks.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现深度神经网络的代码之前，最后需要了解一下 ReLU 激活函数。这是多层神经网络中最常用的激活函数之一。
- en: '**ReLU** is a shortened form of **Rectified Linear Unit**. The output of the
    ReLU function is always a non-negative value that is greater than or equal to
    0:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU** 是 **Rectified Linear Unit（修正线性单元）**的缩写。ReLU 函数的输出总是一个非负值，且大于或等于 0：'
- en: '![Figure 2.27: ReLU activation function'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.27：ReLU 激活函数](img/B15385_02_27.jpg)'
- en: '](img/B15385_02_27.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_27.jpg)'
- en: 'Figure 2.27: ReLU activation function'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.27：ReLU 激活函数
- en: 'The mathematical expression for ReLU is:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的数学表达式是：
- en: '![Figure 2.28: ReLU activation function'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.28：ReLU 激活函数](img/B15385_02_27.jpg)'
- en: '](img/B15385_02_28.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_25.jpg)'
- en: 'Figure 2.28: ReLU activation function'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.28：ReLU 激活函数
- en: ReLU converges much more quickly than the sigmoid activation function, and therefore
    it is by far the most widely used activation function. ReLU is used in almost
    every deep neural network. It is used in all the layers except the last layer,
    where either sigmoid or Softmax is used.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 收敛速度比 sigmoid 激活函数快得多，因此它是目前最广泛使用的激活函数。几乎所有的深度神经网络都使用 ReLU。它被应用于除最后一层外的所有层，最后一层则使用
    sigmoid 或 Softmax。
- en: 'The ReLU activation function is provided by TensorFlow out of the box. To see
    how it is implemented, let''s give some sample input values to a ReLU function
    and see the output:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数是 TensorFlow 内置提供的。为了了解它是如何实现的，我们给 ReLU 函数输入一些示例值，看看输出：
- en: '[PRE86]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE87]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: As you can see, all the positive values are retained, and the negative values
    are suppressed to zero. Let's use this ReLU activation function in the next exercise
    to do a multilayer binary classification task.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有正值都被保留，负值被压制为零。接下来我们将在下一个练习中使用这个 ReLU 激活函数来完成多层二分类任务。
- en: 'Exercise 2.06: Multilayer Binary Classifier'
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.06：多层二分类器
- en: In this exercise, we will implement a multilayer binary classifier using the
    `data.csv` file that we used in *Exercise 2.02*, *Perceptron as a Binary Classifier*.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用在*练习 2.02*中使用的`data.csv`文件来实现一个多层二分类器，*感知机作为二分类器*。
- en: 'We will build a binary classifier with a deep neural network of the following
    configuration. There will be an input layer with 2 nodes and 2 hidden layers,
    the first with 50 neurons and the second with 20 neurons, and lastly a single
    neuron to do the final prediction belonging to any binary class:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个深度神经网络二分类器，配置如下：输入层有 2 个节点，包含 2 个隐藏层，第一个层有 50 个神经元，第二个层有 20 个神经元，最后是一个神经元，用于进行最终的二分类预测：
- en: Note
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be downloaded from GitHub using the following link: https://packt.live/2BVtxIf
    .'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过以下链接从 GitHub 下载：https://packt.live/2BVtxIf .
- en: 'Import the required libraries and packages:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库和包：
- en: '[PRE88]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Import and inspect the data:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入并检查数据：
- en: '[PRE89]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The output is as follows:'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.29: The first five rows of the data'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.29：数据的前五行'
- en: '](img/B15385_02_29.jpg)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_29.jpg)'
- en: 'Figure 2.29: The first five rows of the data'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.29：数据的前五行
- en: 'Visualize the data using a scatter plot:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用散点图可视化数据：
- en: '[PRE90]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The resulting output is as follows, with the *x* axis showing `x1` values and
    the *y* axis showing `x2` values:'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果输出如下，*x* 轴显示 `x1` 值，*y* 轴显示 `x2` 值：
- en: '![Figure 2.30: Scatter plot for given data'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.30：给定数据的散点图'
- en: '](img/B15385_02_30.jpg)'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_30.jpg)'
- en: 'Figure 2.30: Scatter plot for given data'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.30：给定数据的散点图
- en: 'Prepare the data by separating the features and labels and setting the `tf` variables:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分离特征和标签并设置 `tf` 变量来准备数据：
- en: '[PRE91]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Build the `Sequential` model:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 `Sequential` 模型：
- en: '[PRE92]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Here are a couple of points to consider. We provide the input details for the
    first layer, then use the ReLU activation function for all the intermediate layers,
    as discussed earlier. Furthermore, the last layer has only one neuron with a sigmoid
    activation function for binary classifiers.
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是几个需要考虑的要点。我们提供了第一层的输入细节，然后对所有中间层使用 ReLU 激活函数，如前所述。此外，最后一层只有一个神经元，并且使用 sigmoid
    激活函数来进行二分类。
- en: 'Provide the training parameters using the `compile` method:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `compile` 方法提供训练参数：
- en: '[PRE93]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Inspect the `model` configuration using the `summary` function:'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `summary` 函数检查 `model` 配置：
- en: '[PRE94]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output will be as follows:'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.31: Deep neural network model summary using Keras'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.31：使用 Keras 深度神经网络模型总结'
- en: '](img/B15385_02_31.jpg)'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_31.jpg)'
- en: 'Figure 2.31: Deep neural network model summary using Keras'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.31：使用 Keras 深度神经网络模型总结
- en: In the model summary, we can see that there are a total of `1191` parameters—weights
    and biases—to learn across the hidden layers to the output layer.
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在模型总结中，我们可以看到，总共有 `1191` 个参数——权重和偏置——需要在隐藏层到输出层之间进行学习。
- en: 'Train the model by calling the `fit()` method:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `fit()` 方法训练模型：
- en: '[PRE95]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Notice that, in this case, the model reaches 100% accuracy within `50` epochs,
    unlike the single-layer model, which needed about 1,000 epochs:'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，模型在 `50` 个 epoch 内达到了 100% 的准确率，而单层模型大约需要 1,000 个 epoch：
- en: '![Figure 2.32: Multilayer model train logs'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.32：多层模型训练日志'
- en: '](img/B15385_02_32.jpg)'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_32.jpg)'
- en: 'Figure 2.32: Multilayer model train logs'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.32：多层模型训练日志
- en: 'Let''s evaluate the model''s performance:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们评估模型的性能：
- en: '[PRE96]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The output is as follows:'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE97]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Our model has now been trained and demonstrates 100% accuracy.
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的模型现在已经训练完成，并且展示了 100% 的准确率。
- en: Note
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZUkM94](https://packt.live/2ZUkM94).
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/2ZUkM94](https://packt.live/2ZUkM94)。
- en: You can also run this example online at [https://packt.live/3iKsD1W](https://packt.live/3iKsD1W).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/3iKsD1W](https://packt.live/3iKsD1W) 在线运行这个示例。你必须执行整个
    Notebook 才能获得期望的结果。
- en: In this exercise, we learned how to build a multilayer neural network using
    Keras. This is a binary classifier. In the next exercise, we will build a deep
    neural network for a multiclass classifier with the MNIST dataset.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何使用 Keras 构建一个多层神经网络。这是一个二分类器。在下一个练习中，我们将为 MNIST 数据集构建一个深度神经网络，用于多类分类器。
- en: 'Exercise 2.07: Deep Neural Network on MNIST Using Keras'
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.07：使用 Keras 在 MNIST 上实现深度神经网络
- en: 'In this exercise, we will perform a multiclass classification by implementing
    a deep neural network (multi-layer) for the MNIST dataset where our input layer
    comprises 28 × 28 pixel images flattened to 784 input nodes followed by 2 hidden
    layers, the first with 50 neurons and the second with 20 neurons. Lastly, there
    will be a Softmax layer consisting of 10 neurons since we are classifying the
    handwritten digits into 10 classes:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过实现一个深度神经网络（多层）来对 MNIST 数据集进行多类分类，其中输入层包含 28 × 28 的像素图像，展平成 784 个输入节点，后面有
    2 个隐藏层，第一个隐藏层有 50 个神经元，第二个隐藏层有 20 个神经元。最后，会有一个 Softmax 层，包含 10 个神经元，因为我们要将手写数字分类为
    10 个类别：
- en: 'Import the required libraries and packages:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库和包：
- en: '[PRE98]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Load the MNIST data:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据：
- en: '[PRE99]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '`train_features` has the training images in the form of 28 x 28 pixel values.'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`train_features` 包含的是 28 x 28 像素值形式的训练图像。'
- en: '`train_labels` has the training labels. Similarly, `test_features` has the
    test images in the form of 28 x 28 pixel values. `test_labels` has the test labels.'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`train_labels` 包含训练标签。类似地，`test_features` 包含 28 x 28 像素值形式的测试图像，`test_labels`
    包含测试标签。'
- en: 'Normalize the data:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行归一化：
- en: '[PRE100]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: The pixel values of the images range from 0-255\. We need to normalize the values
    by dividing them by 255 so that the range goes from 0 to 1.
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像的像素值范围为 0-255。我们需要通过将它们除以 255 来对这些值进行归一化，使其范围从 0 到 1。
- en: 'Build the `sequential` model:'
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 `sequential` 模型：
- en: '[PRE101]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: There are couple of points to note. The first layer in this case is not actually
    a layer of neurons but a `Flatten` function. This flattens the 28 x 28 image into
    a single array of `784`, which is fed to the first hidden layer of `50` neurons.
    The last layer has `10` neurons corresponding to the 10 classes with a `softmax`
    activation function.
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有几点需要注意。首先，在这个案例中，第一层实际上并不是一层神经元，而是一个 `Flatten` 函数。它将 28 x 28 的图像展平成一个包含 `784`
    的一维数组，这个数组会输入到第一个隐藏层，隐藏层有 `50` 个神经元。最后一层有 `10` 个神经元，对应 10 个类别，并使用 `softmax` 激活函数。
- en: 'Provide training parameters using the `compile` method:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `compile` 方法提供训练参数：
- en: '[PRE102]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Note
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The loss function used here is different from the binary classifier. For a
    multiclass classifier, the following loss functions are used: `sparse_categorical_crossentropy`,
    which is used when the labels are not one-hot encoded, as in this case; and, `categorical_crossentropy`,
    which is used when the labels are one-hot encoded.'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里使用的损失函数与二分类器不同。对于多分类器，使用以下损失函数：`sparse_categorical_crossentropy`，当标签未经过 one-hot
    编码时使用，如本例所示；以及 `categorical_crossentropy`，当标签已进行 one-hot 编码时使用。
- en: 'Inspect the model configuration using the `summary` function:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `summary` 函数检查模型配置：
- en: '[PRE103]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The output is as follows:'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.33: Deep neural network summary'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.33：深度神经网络摘要](img/B15385_02_33.jpg)'
- en: '](img/B15385_02_33.jpg)'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_33.jpg)'
- en: 'Figure 2.33: Deep neural network summary'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.33：深度神经网络摘要
- en: In the model summary, we can see that there are a total of 40,480 parameters—weights
    and biases—to learn across the hidden layers to the output layer.
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在模型摘要中，我们可以看到总共有 40,480 个参数——权重和偏差——需要在隐藏层到输出层之间进行学习。
- en: 'Train the model by calling the `fit` method:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `fit` 方法来训练模型：
- en: '[PRE104]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'The output will be as follows:'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.34: Deep neural network training logs'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.34：深度神经网络训练日志](img/B15385_02_34.jpg)'
- en: '](img/B15385_02_34.jpg)'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_34.jpg)'
- en: 'Figure 2.34: Deep neural network training logs'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.34：深度神经网络训练日志
- en: 'Test the model by calling the `evaluate()` function:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `evaluate()` 函数来测试模型：
- en: '[PRE105]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The output will be:'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE106]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Now that the model is trained and tested, in the next few steps, we will run
    the prediction with some images selected randomly.
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在模型已经训练并测试完成，在接下来的几个步骤中，我们将用一些随机选择的图像进行预测。
- en: 'Load a random image from a test dataset. Let''s locate the 200th image:'
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从测试数据集中加载一张随机图像。我们选择第200张图像：
- en: '[PRE107]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Let''s see the shape of the image using the following command:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用以下命令查看图像的形状：
- en: '[PRE108]'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The output is:'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE109]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: We can see that the shape of the image is 28 x 28\. However, the model expects
    3-dimensional input. We need to reshape the image accordingly.
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到图像的形状是28 x 28。然而，模型期望的是三维输入。我们需要相应地重塑图像。
- en: 'Use the following code to reshape the image:'
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码来重塑图像：
- en: '[PRE110]'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Let''s call the `predict()` method of the model and store the output in a variable
    called `result`:'
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们调用模型的`predict()`方法，并将输出存储在一个名为`result`的变量中：
- en: '[PRE111]'
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '`result` has the output in the form of 10 probability values, as shown here:'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`result`以10个概率值的形式输出，如下所示：'
- en: '[PRE112]'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'The position of the highest value will be the prediction. Let''s use the `argmax`
    function we learned about in the previous chapter to find out the prediction:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最高值的位置就是预测结果。让我们使用在上一章中学到的`argmax`函数来查找预测结果：
- en: '[PRE113]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'In this case, it is `3`:'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，它是`3`：
- en: '[PRE114]'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'In order to check whether the prediction is correct, we check the label of
    the corresponding image:'
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了检查预测是否正确，我们检查相应图像的标签：
- en: '[PRE115]'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Again, the value is `3`:'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次，值是`3`：
- en: '[PRE116]'
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'We can also visualize the image using `pyplot`:'
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以使用`pyplot`可视化图像：
- en: '[PRE117]'
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The output will be as follows:'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.35: Test image visualized'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.35：测试图像可视化'
- en: '](img/B15385_02_35.jpg)'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_02_35.jpg)'
- en: 'Figure 2.35: Test image visualized'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.35：测试图像可视化
- en: And this shows that the prediction is correct.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明预测是正确的。
- en: Note
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2O5KRgd](https://packt.live/2O5KRgd).
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参考[https://packt.live/2O5KRgd](https://packt.live/2O5KRgd)。
- en: You can also run this example online at [https://packt.live/2O8JHR0](https://packt.live/2O8JHR0).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2O8JHR0](https://packt.live/2O8JHR0)上在线运行这个示例。你必须执行整个Notebook才能获得期望的结果。
- en: In this exercise, we created a multilayer multiclass neural network model using
    Keras to classify the MNIST data. With the model we built, we were able to correctly
    predict a random handwritten digit.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们使用Keras创建了一个多层多类神经网络模型，用于对MNIST数据进行分类。通过我们构建的模型，我们能够正确预测一个随机的手写数字。
- en: Exploring the Optimizers and Hyperparameters of Neural Networks
  id: totrans-557
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索神经网络的优化器和超参数
- en: Training a neural network to get good predictions requires tweaking a lot of
    hyperparameters such as optimizers, activation functions, the number of hidden
    layers, the number of neurons in each layer, the number of epochs, and the learning
    rate. Let's go through each of them one by one and discuss them in detail.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络以获得良好的预测结果需要调整许多超参数，例如优化器、激活函数、隐藏层的数量、每层神经元的数量、训练轮次和学习率。让我们逐一讨论每一个超参数，并详细解释它们。
- en: Gradient Descent Optimizers
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降优化器
- en: In an earlier section titled *Perceptron Training Process in TensorFlow*, we
    briefly touched upon the gradient descent optimizer without going into the details
    of how it works. This is a good time to explore the gradient descent optimizer
    in a little more detail. We will provide an intuitive explanation without going
    into the mathematical details.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前名为*TensorFlow中的感知机训练过程*的部分中，我们简要提到了梯度下降优化器，但没有深入讨论其工作原理。现在是时候稍微详细了解一下梯度下降优化器了。我们将提供一个直观的解释，而不涉及数学细节。
- en: 'The gradient descent optimizer''s function is to minimize the loss or error.
    To understand how gradient descent works, you can think of this analogy: imagine
    a person at the top of a hill who wants to reach the bottom. At the beginning
    of the training, the loss is large, like the height of the hill''s peak. The functioning
    of the optimizer is akin to the person descending the hill to the valley at the
    bottom, or rather, the lowest point of the hill, and not climbing up the hill
    that is on the other side of the valley.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降优化器的作用是最小化损失或误差。为了理解梯度下降是如何工作的，可以这样类比：想象一个人站在山顶，想要到达山脚。训练开始时，损失很大，就像山顶的高度。优化器的工作就像这个人从山顶走到山谷底部，或者说，走到山的最低点，而不是走到山的另一边。
- en: Remember the learning rate parameter that we used while creating the optimizer?
    That can be compared to the size of the steps the person takes to climb down the
    hill. If these steps are large, it is fine at the beginning since the person can
    climb down faster, but once they near the bottom, if the steps are too large,
    the person crosses over to the other side of the valley. Then, in order to climb
    back down to the bottom of the valley, the person will try to move back but will
    move over to the other side again. This results in going back and forth without
    reaching the bottom of the valley.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在创建优化器时使用的学习率参数吗？它可以与人们下坡时采取的步伐大小进行比较。如果这些步伐很大，刚开始时是没问题的，因为这样可以更快地下坡，但一旦接近山谷底部，如果步伐过大，就会跨过山谷的另一边。然后，为了重新下到山谷底部，这个人会尝试回到原地，但又会再次跨越到另一边。结果就是在两边来回移动，始终无法到达山谷底部。
- en: On the other hand, if the person takes very small steps (a very small learning
    rate), they will take forever to reach the bottom of the valley; in other words,
    the model will take forever to converge. So, finding a learning rate that is neither
    too small nor too big is very important. However, unfortunately, there is no rule
    of thumb to find out in advance what the right value should be—we have to find
    it by trial and error.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果一个人采取非常小的步伐（非常小的学习率），他们将永远无法到达山谷底部；换句话说，模型将永远无法收敛。因此，找到一个既不太小也不太大的学习率非常重要。然而，不幸的是，目前没有一种经验法则可以提前知道正确的值应该是多少——我们只能通过试验和错误来找到它。
- en: 'There are two main types of gradient-based optimizers: batch and stochastic
    gradient descent. Before we jump into them, let''s recall that one epoch means
    a training iteration where the neural network goes through all the training examples:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度基优化器主要有两种类型：批量梯度下降和随机梯度下降。在我们深入讨论这两种之前，先回顾一下一个训练周期（epoch）的含义：它表示神经网络遍历所有训练样本的一次训练迭代。
- en: In an epoch, when we reduce the loss across all the training examples, it is
    called **batch gradient descent**. This is also known as **full batch gradient
    descent**. To put it simply, after going through a full batch, we take a step
    to adjust the weights and biases of the network to reduce the loss and improve
    the predictions. There is a similar form of it called mini-batch gradient descent,
    where we take steps, that is, we adjust weights and biases, after going through
    a subset of the full dataset.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个训练周期内，当我们减少所有训练样本的损失时，这就是**批量梯度下降**。它也被称为**全批量梯度下降**。简单来说，遍历完一个完整批次后，我们会采取一步来调整网络的权重和偏置，以减少损失并改善预测。还有一种类似的方法叫做小批量梯度下降，它是在遍历数据集的一个子集后调整权重和偏置的过程。
- en: In contrast to batch gradient descent, when we take a step at one example per
    iteration, we have **stochastic gradient descent** (**SGD**). The word *stochastic*
    tells us there is randomness involved here, which, in this case, is the batch
    that is randomly selected.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与批量梯度下降不同，当我们每次迭代只取一个样本时，就有了**随机梯度下降**（**SGD**）。*随机*一词告诉我们这里涉及随机性，在这种情况下，就是随机选择的批量。
- en: Though SGD works relatively well, there are advanced optimizers that can speed
    up the training process. They include SGD with momentum, Adagrad, and Adam.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机梯度下降（SGD）相对有效，但还有一些先进的优化器可以加速训练过程。它们包括带动量的SGD、Adagrad和Adam。
- en: The Vanishing Gradient Problem
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度问题
- en: In the *Training a Perceptron* section, we learned about the forward and backward
    propagation of neural networks. When a neural network performs forward propagation,
    the error gradient is calculated with respect to the true label, and backpropagation
    is performed to see which parameters (the weights and biases) of the neural network
    have contributed to the error and the extent to which they have done so. The error
    gradient is propagated from the output layer to the input layer to calculate gradients
    with respect to each parameter, and in the last step, the gradient descent step
    is performed to adjust the weights and biases according to the calculated gradient.
    As the error gradient is propagated backward, the gradients calculated at each
    parameter become smaller and smaller as it advances to the lower (initial) layers.
    This decrease in the gradients means that the changes to the weights and biases
    become smaller and smaller. Hence, our neural network struggles to find the global
    minimum and does not give good results. This is called the vanishing gradient
    problem. The problem happens with the use of the sigmoid (logistic) function as
    an activation function, and hence we use the ReLU activation function to train
    deep neural network models to avoid gradient complications and improve the results.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 在*感知机训练*部分，我们了解了神经网络的前向传播和反向传播。当神经网络进行前向传播时，误差梯度是相对于真实标签计算的，之后进行反向传播，查看神经网络的哪些参数（权重和偏置）对误差的贡献以及贡献的大小。误差梯度从输出层传播到输入层，计算每个参数的梯度，最后一步是执行梯度下降步骤，根据计算得到的梯度调整权重和偏置。随着误差梯度的反向传播，计算得出的每个参数的梯度逐渐变小，直到更低（初始）层次。这种梯度的减小意味着权重和偏置的变化也变得越来越小。因此，我们的神经网络很难找到全局最小值，并且结果不好。这就是所谓的梯度消失问题。这个问题出现在使用sigmoid（逻辑）函数作为激活函数时，因此我们使用ReLU激活函数来训练深度神经网络模型，以避免梯度问题并改善结果。
- en: Hyperparameter Tuning
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: 'Like any other model training process in machine learning, it is possible to
    perform hyperparameter tuning to improve the performance of the neural network
    model. One of the parameters is the learning rate. The other parameters are as
    follows:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 像机器学习中的其他模型训练过程一样，我们可以进行超参数调优，以提高神经网络模型的性能。其中一个参数是学习率。其他参数如下：
- en: '**Number of epochs**: Increasing the number of epochs generally increases the
    accuracy and lowers the loss'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代次数**：增加迭代次数通常能提高准确性并降低损失'
- en: '**Number of layers**: Increasing the number of layers increases the accuracy,
    as we saw in the exercises with MNIST'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层数**：增加层数可以提高准确性，正如我们在MNIST练习中看到的那样'
- en: '**Number of neurons per layer**: This also increases the accuracy'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每层的神经元数量**：这也会提高准确性'
- en: And once again, there is no way to know in advance what the right number of
    layers or the right number of neurons per layer is. This has to be figured out
    by trial and error. It has to be noted that the larger the number of layers and
    the larger the number of neurons per layer, the greater the computational power
    required. Therefore, we start with the smallest possible numbers and slowly increase
    the number of layers and neurons.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们无法事先知道正确的层数或每层神经元的数量。必须通过试错法来找出。需要注意的是，层数越多，每层的神经元数量越多，所需的计算能力就越大。因此，我们从最小的数字开始，逐步增加层数和神经元数量。
- en: Overfitting and Dropout
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合与Dropout
- en: 'Neural networks with complex architectures and too many parameters tend to
    fit on all the data points, including noisy labels, leading to the problem of
    overfitting and neural networks that are not able to generalize well on unseen
    datasets. To tackle this issue, there is a technique called **dropout**:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络具有复杂的架构和过多的参数，容易在所有数据点上进行过拟合，包括噪声标签，从而导致过拟合问题，并使神经网络无法在未见过的数据集上进行良好的泛化。为了解决这个问题，有一种技术叫做**dropout**：
- en: '![Figure 2.36: Dropout illustrated'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.36：Dropout 说明'
- en: '](img/B15385_02_36.jpg)'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_02_36.jpg)'
- en: 'Figure 2.36: Dropout illustrated'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.36：Dropout 说明
- en: In this technique, a certain number of neurons are deactivated randomly during
    the training process. The number of neurons to be deactivated is provided as a
    parameter in the form of a percentage. For example, `Dropout = .2` means 20% of
    the neurons in that layer will be randomly deactivated during the training process.
    The same neurons are not deactivated more than once, but a different set of neurons
    is deactivated in each epoch. During testing, however, all the neurons are activated.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 在该技术中，训练过程中会随机停用一定数量的神经元。要停用的神经元数量通过百分比的形式作为参数提供。例如，`Dropout = .2` 表示该层中 20%
    的神经元将在训练过程中随机停用。相同的神经元不会被多次停用，而是在每个训练周期中停用不同的神经元。然而，在测试过程中，所有神经元都会被激活。
- en: 'Here is an example of how we can add `Dropout` to a neural network model using Keras:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何使用 Keras 将 `Dropout` 添加到神经网络模型中的示例：
- en: '[PRE118]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: In this case, a dropout of 20% is added to `Hidden Layer2`. It is not necessary
    for the dropout to be added to all layers. As a data scientist, you can experiment
    and decide what the `dropout` value should be and how many layers need it.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`Hidden Layer2` 添加了 20% 的 dropout。并不需要将 dropout 添加到所有层。作为数据科学家，您可以进行实验并决定
    `dropout` 值应该是多少，以及需要多少层。
- en: Note
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'A more detailed explanation of dropout can be found in the paper by Nitish
    Srivastava et al. available here: [http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 dropout 的更详细解释可以在 Nitish Srivastava 等人的论文中找到，点击此链接即可查看：[http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)。
- en: As we have come to the end of this chapter, let's test what we have learned
    so far with the following activity.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入本章的尾声，让我们通过以下活动来测试目前为止学到的内容。
- en: 'Activity 2.01: Build a Multilayer Neural Network to Classify Sonar Signals'
  id: totrans-588
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动 2.01：构建一个多层神经网络来分类声纳信号
- en: 'In this activity, we will use the Sonar dataset ([https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))),
    which has patterns obtained by bouncing sonar signals off a metal cylinder at
    various angles and under various conditions. You will build a neural network-based
    classifier to classify between sonar signals bounced off a metal cylinder (the
    Mine class), and those bounced off a roughly cylindrical rock (the Rock class).
    We recommend using the Keras API to make your code more readable and modular,
    which will allow you to experiment with different parameters easily:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们将使用 Sonar 数据集（[https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))），该数据集通过将声纳信号以不同角度和条件反射到金属圆柱体上获得。您将构建一个基于神经网络的分类器，用于区分从金属圆柱体反射回来的声纳信号（矿物类）和从大致圆柱形的岩石反射回来的声纳信号（岩石类）。我们建议使用
    Keras API 来使您的代码更加易读和模块化，这样您可以轻松地尝试不同的参数：
- en: Note
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the sonar dataset from this link [https://packt.live/31Xtm9M](https://packt.live/31Xtm9M).
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下链接下载 sonar 数据集：[https://packt.live/31Xtm9M](https://packt.live/31Xtm9M)。
- en: The first step is to understand the data so that you can figure out whether
    this is a binary classification problem or a multiclass classification problem.
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是理解数据，以便您能确定这是二分类问题还是多分类问题。
- en: 'Once you understand the data and the type of classification that needs to be
    done, the next step is network configuration: the number of neurons, the number
    of hidden layers, which activation function to use, and so on.'
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您理解了数据和需要执行的分类类型，下一步就是网络配置：神经元的数量、隐藏层的数量、使用哪个激活函数等等。
- en: 'Recall the network configuration steps that we''ve covered so far. Let''s just
    reiterate a crucial point, the activation function part: for the output (the last)
    layer, we use sigmoid to do binary classification and Softmax to do multiclass classification.'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回顾我们迄今为止讨论的网络配置步骤。让我们再强调一个关键点——激活函数部分：对于输出（最后一层），我们使用 sigmoid 进行二分类，使用 Softmax
    进行多分类。
- en: Open the `sonar.csv` file to explore the dataset and see what the target variables
    are.
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `sonar.csv` 文件以探索数据集，并查看目标变量是什么。
- en: Separate the input features and the target variables.
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离输入特征和目标变量。
- en: 'Preprocess the data to make it neural network-compatible. Hint: one-hot encoding.'
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行预处理，使其兼容神经网络。提示：one-hot 编码。
- en: Define a neural network using Keras and compile it with the right loss function.
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras 定义一个神经网络，并使用正确的损失函数进行编译。
- en: Print out a model summary to verify the network parameters and considerations.
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出模型总结以验证网络参数和 注意事项。
- en: You are expected to get an accuracy value above 95% by designing a proper multilayer
    neural network using these steps.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 你应通过使用这些步骤设计一个合适的多层神经网络，以期获得95%以上的准确率。
- en: Note
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 390.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤、解决方案和额外评论已在第390页呈现。
- en: Summary
  id: totrans-603
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started off by looking at biological neurons and then moved
    on to artificial neurons. We saw how neural networks work and took a practical
    approach to building single-layer and multilayer neural networks to solve supervised
    learning tasks. We looked at how a perceptron works, which is a single unit of
    a neural network, all the way to a deep neural network capable of performing multiclass
    classification. We saw how Keras makes it very easy to create deep neural networks
    with a minimal amount of code. Lastly, we looked at practical considerations to
    take into account when building a successful neural network, which involved important
    concepts such as gradient descent optimizers, overfitting, and dropout.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先研究了生物神经元，然后转向人工神经元。我们了解了神经网络的工作原理，并采用实践的方法构建了单层和多层神经网络来解决监督学习任务。我们研究了感知机的工作原理，它是神经网络的一个单元，直到可以进行多类分类的深度神经网络。我们看到Keras如何使得使用最少的代码轻松创建深度神经网络。最后，我们研究了在构建成功的神经网络时需要考虑的实际因素，这涉及了诸如梯度下降优化器、过拟合和Dropout等重要概念。
- en: In the next chapter, we will go to the next level and build a more complicated
    neural network called a CNN, which is widely used in image recognition.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将进入更高层次，构建一个更复杂的神经网络，称为CNN，它广泛应用于图像识别。
