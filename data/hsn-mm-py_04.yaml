- en: Parameter Learning Using Maximum Likelihood
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用最大似然估计进行参数学习
- en: In the previous chapter, we discussed the state inference in the case of a **Hidden
    Markov Model** (**HMM**). We tried to predict the next state for an HMM using
    the information of previous state transitions. But in each cases, we had assumed
    that we already knew the transition and emission probabilities of the model. But
    in real-life problems, we usually need to learn these parameters from our observations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了**隐马尔可夫模型**（**HMM**）中的状态推断。我们尝试使用前一个状态转移的信息预测HMM的下一个状态。但在每种情况下，我们假设我们已经知道模型的转移和发射概率。然而，在实际问题中，我们通常需要从观察数据中学习这些参数。
- en: 'In this chapter, we will try to estimate the parameters of our HMM model through
    data gathered from observations. We will be covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试通过从观察中收集到的数据来估计我们的HMM模型的参数。我们将覆盖以下主题：
- en: Maximum likelihood learning, with examples
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大似然学习，带有示例
- en: Maximum likelihood learning in HMMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HMM中的最大似然学习
- en: Expectation maximization algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望最大化算法
- en: The Baum-Welch algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baum-Welch算法
- en: Maximum likelihood learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然学习
- en: 'Before diving into learning about **maximum likelihood estimation** (**MLE**)
    in HMMs, let''s try to understand the basic concepts of MLE in generic cases.
    As the name suggests, MLE tries to select the parameters of the model that maximizes
    the likelihood of observed data. The likelihood for any model with given parameters
    is defined as the probability of getting the observed data, and can be written
    as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解HMM中的**最大似然估计**（**MLE**）之前，我们先来理解一下MLE在一般情况下的基本概念。顾名思义，MLE试图选择最大化观察数据似然性的模型参数。对于给定参数的任何模型，似然性定义为获取观察数据的概率，可以写成如下形式：
- en: '![](img/b83fac20-1c71-4634-a46f-dac1aaf79d54.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b83fac20-1c71-4634-a46f-dac1aaf79d54.png)'
- en: 'Here, *D={D[1], D[2], D[3], …, D[n]}* is the observed data, and *θ* is the
    set of parameters governing our model. In most cases, for simplicity, we assume
    that the datapoints are **independent and identically distributed** (**IID**). With
    that assumption, we can simplify the definition of our likelihood function as
    follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*D={D[1], D[2], D[3], …, D[n]}*是观察数据，*θ*是控制我们模型的参数集。在大多数情况下，为了简便起见，我们假设数据点是**独立同分布**（**IID**）的。在这个假设下，我们可以将似然函数的定义简化为如下形式：
- en: '![](img/57cd5871-7d15-439f-8668-48a8c5630604.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57cd5871-7d15-439f-8668-48a8c5630604.png)'
- en: Here, we have used the multiplication rule for independent random variables
    to decompose the joint distribution into product over individual datapoint.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了独立随机变量的乘法规则，将联合分布分解为单个数据点的乘积。
- en: 'Coming back to MLE, MLE tries to find the value of *θ* for which the value
    of *P(D|θ)* is at a maximum. So, basically we now have an optimization problem
    at hand:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回到最大似然估计，MLE试图找到使得*P(D|θ)*值最大化的*θ*值。所以，基本上我们现在面临的是一个优化问题：
- en: '![](img/3d3b5257-b933-46d8-b328-5a1f5f08da60.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d3b5257-b933-46d8-b328-5a1f5f08da60.png)'
- en: In the next couple of subsections, we will try to apply MLE to some simple examples
    to understand it better.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个小节中，我们将尝试将最大似然估计（MLE）应用于一些简单的示例，以便更好地理解它。
- en: MLE in a coin toss
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬币投掷中的MLE
- en: Let's assume that we want to learn a model of a given coin using observations
    obtained from tossing it. Since a coin can only have two outcomes, heads or tails,
    it can be modeled using a single parameter. Let's say we define the parameter
    as *θ*, which is the probability of getting heads when the coin is tossed. The
    probability of getting tails will automatically be *1-θ* because getting either
    heads or tails are mutually exclusive events.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想通过投掷硬币获得的观察数据来学习一个硬币模型。由于硬币只有两种结果：正面或反面，因此可以用一个参数来建模。假设我们将该参数定义为*θ*，即投掷硬币时获得正面的概率。获得反面的概率将自动为*1-θ*，因为正面和反面是互斥事件。
- en: 'We have our model ready, so let''s move on to computing the likelihood function
    of this model. Let''s assume that we are given some observations of coin tosses
    as *D={H,H,T,H,T,T}*. For the given data we can write our likelihood function
    as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好了模型，接下来我们来计算该模型的似然函数。假设我们得到了一些硬币投掷的观察数据，*D={H,H,T,H,T,T}*。对于给定的数据，我们可以将似然函数写成如下形式：
- en: '![](img/b32bf448-9d8a-4656-b29d-bf6641bb8652.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b32bf448-9d8a-4656-b29d-bf6641bb8652.png)'
- en: 'Now, we would like to find the value of *θ* that would maximize *P(D|θ)*. For
    that, we take the derivative of our likelihood function, equate it to *0*, and
    then solve it for *θ*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望找到能够最大化*P(D|θ)*的*θ*值。为此，我们需要对我们的似然函数进行求导，将其等于*0*，然后求解*θ*：
- en: '![](img/e2c3b80e-bf69-4d12-873b-18a90aea69d6.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2c3b80e-bf69-4d12-873b-18a90aea69d6.png)'
- en: Therefore, our MLE estimator learned that the probability of getting heads on
    tossing the coin is *0.5*. Looking at our observations, we would expect the same
    probability as we have an equal number of heads and tails in our observed data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的MLE估计器学到抛硬币得到正面的概率是*0.5*。根据我们的观察，由于正反面出现的次数相等，我们也预期这个概率应该是相同的。
- en: 'Let''s now try to write code to learn the parameter *θ* for our model. But
    as we know that finding the optimal value can run into numerical issues on a computer,
    is there a possible way to avoid that and directly be able to compute *θ[MLE]*?
    If we look closely at our likelihood equation, we realize that we can write a
    generic formula for the likelihood for this model. If we assume that our data
    has *n* heads and *m* tails, we can write the likelihood as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试编写代码，来学习模型的参数*θ*。但是，如我们所知，在计算机上寻找最优值时可能会遇到数值问题，是否有可能避免这些问题，直接计算*θ[MLE]*呢？如果我们仔细观察我们的似然方程，我们会发现我们可以为这个模型写出一个通用的似然公式。如果我们假设我们的数据中有*n*个正面和*m*个反面，我们可以将似然写成如下形式：
- en: '![](img/1c241c3f-451d-4b92-9ec5-a9a6576f5673.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c241c3f-451d-4b92-9ec5-a9a6576f5673.png)'
- en: 'Now, we can actually find *θ[MLE]* in a closed-form using this likelihood function
    and avoid relying on any numerical method to compute the optimum value:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实际上可以使用这个似然函数找到*θ[MLE]*的封闭形式解，避免依赖任何数值方法来计算最优值：
- en: '![](img/51def384-ba97-4278-8d00-35a663574cc5.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51def384-ba97-4278-8d00-35a663574cc5.png)'
- en: 'We can see that we have been able to find a closed form solution for the MLE
    solution to *θ*. Now, coding this up would be to simply compute the preceding
    formula as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们已经能够找到一个关于MLE解*θ*的封闭形式解。现在，编写代码就是简单地计算前面的公式，如下所示：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let''s try out our function for different datapoints:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试对不同的数据点应用我们的函数：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The outputs are as we expect, but one of the drawbacks of the MLE approach
    is that it is very sensitive to randomness in our data which, in some cases, might
    lead it to learn the wrong parameters. This is especially true in a case when
    the dataset is small in size. For example, let''s say that we toss a fair coin
    three times and we get heads in each toss. The MLE approach, in this case, would
    learn the value of *θ* to be 1, which is not correct since we had a fair coin.
    The output is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如我们预期，但MLE方法的一个缺点是它对数据中的随机性非常敏感，在某些情况下，这可能导致其学习到错误的参数。尤其是在数据集较小时，这种情况尤为明显。例如，假设我们抛一枚公平的硬币三次，每次都得到正面。在这种情况下，MLE方法会学习到*θ*的值为1，这显然是不正确的，因为我们抛的是一枚公平的硬币。输出结果如下：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In [Chapter 5](b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml), *Parameter Inference
    using Bayesian Approach*, we will try to solve this problem of MLE by starting
    with a prior distribution over the parameters, and it modifies its prior as it
    sees more and more data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml)《使用贝叶斯方法进行参数推断》中，我们将尝试通过从参数的先验分布开始来解决这个MLE问题，并且随着数据量的增加，它会调整其先验分布。
- en: MLE for normal distributions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正态分布的最大似然估计（MLE）
- en: 'In the previous section, we had a model with a single parameter. In this section,
    we will apply the same concepts to a slightly more complex model. We will try
    to learn the parameters of a normal distribution (also known as the **Gaussian
    distribution**) from a given observed data. As we know, the normal distribution
    is parametrized by its mean and standard deviation and the distribution is given
    as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了一个包含单个参数的模型。在本节中，我们将把相同的概念应用到一个稍微复杂的模型中。我们将尝试从给定的观测数据中学习正态分布的参数（也称为**高斯分布**）。如我们所知，正态分布由其均值和标准差来参数化，分布的公式如下：
- en: '![](img/5f1a73b2-ed97-47e4-8352-840dedaf7627.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f1a73b2-ed97-47e4-8352-840dedaf7627.png)'
- en: Here, *µ* is the mean and *σ* is the standard deviation of the normal distribution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*µ*是正态分布的均值，*σ*是标准差。
- en: As we discussed earlier, for estimating parameters using MLE we would need some
    observed data, which, in this case, we are assuming to be coming from a normal
    distribution (or that it can be approximated using a normal distribution). Let's
    assume that we have some observed data: *X = {x[1], x[2],...,x[N]}*. We want to
    estimate the parameters *μ* (mean) and *σ²* (variance) for our model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，使用MLE来估计参数时，我们需要一些观测数据，在本例中，我们假设这些数据来自正态分布（或者可以使用正态分布来逼近）。假设我们有一些观测数据：*X
    = {x[1], x[2], ..., x[N]}*。我们想要估计我们的模型的参数*μ*（均值）和*σ²*（方差）。
- en: 'We will follow the same steps as we took in the previous section. We will start
    by defining the likelihood function for the normal distribution. The likelihood
    is the probability of the data being observed, given the parameters. So, given
    the observed data, we can state the likelihood function as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循与上一节相同的步骤。我们将首先定义正态分布的似然函数。似然函数是给定参数的情况下，数据被观测到的概率。所以，给定观测数据，我们可以将似然函数写成如下形式：
- en: '![](img/62dc53b8-f2d5-49fb-b859-d3f8595ab12e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62dc53b8-f2d5-49fb-b859-d3f8595ab12e.png)'
- en: 'One issue that we usually run into while trying to work with the product of
    small numbers is that the number can get too small for the computer to work with.
    To avoid running into this issue, we instead work with the log-likelihood instead
    of the simple likelihood. Since log is an increasing function, the maximum of
    the log-likelihood function would be for the same value of parameters as it would
    have been for the likelihood function. The log-likelihood can be defined as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试处理小数值的乘积时，我们通常会遇到一个问题，那就是数字可能变得太小，计算机无法处理。为了避免遇到这个问题，我们转而使用对数似然，而不是简单的似然。由于对数是一个递增函数，最大对数似然函数的值与似然函数的最大值对应的参数是一样的。对数似然可以定义如下：
- en: '![](img/90870fda-be88-45f8-ad5f-82fb52efdefa.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90870fda-be88-45f8-ad5f-82fb52efdefa.png)'
- en: 'We can then find the values of *μ[MLE]* and *σ[MLE]* that maximize the log-likelihood
    function by taking partial derivatives with respect to each of the variables,
    equating it to *0,* and solving the equation. To get the mean value, we need to
    take the partial derivative of the log-likelihood function with respect to *μ *while
    keeping *σ* as constant, and set it to *0**,* which gives us the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对每个变量分别求偏导数，令其等于*0*，并解方程来找到最大化对数似然函数的*μ[MLE]*和*σ[MLE]*的值。为了得到均值，我们需要对对数似然函数关于*μ*求偏导数，同时保持*σ*为常数，并设其为*0*，得到以下结果：
- en: '![](img/e649282d-ea2b-49fb-9c3d-c810b4f0c286.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e649282d-ea2b-49fb-9c3d-c810b4f0c286.png)'
- en: 'Similarly, the MLE of standard deviation *σ²*can be computed by the partial
    derivative of the log-likelihood function with *σ²* while keeping *μ* constant,
    equating it to *0,* and then solving for *σ²*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，标准差*σ²*的MLE可以通过对对数似然函数关于*σ²*求偏导数，同时保持*μ*为常数，令其等于*0*，然后解出*σ²*：
- en: '![](img/9bd38c12-3bb6-433f-83f2-4fd95a074cf6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bd38c12-3bb6-433f-83f2-4fd95a074cf6.png)'
- en: 'As we can see, we have again been able to derive a closed-form solution for
    the MLE and thus wouldn''t need to rely on numerical methods while coding it up.
    Let''s try to code this up and check if our MLE approach has learnt the correct
    parameters:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们再次能够得出一个闭式解，因此在编程时无需依赖数值方法。让我们尝试编写代码，检查我们的MLE方法是否学习到了正确的参数：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have our learning function ready, so we can now generate some data from
    a known distribution and check if our function is able to learn the same parameters
    from the generated data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好学习函数，现在可以从已知分布生成一些数据，并检查我们的函数是否能够从生成的数据中学习到相同的参数：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example, we can see that the learned values are not very accurate.
    This is because of the problem with the MLE being too sensitive to the observed
    datapoints, as we discussed in the previous section. Let''s try to run this same
    example with more observed data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到学到的值并不是非常准确。这是因为MLE对于观测数据点过于敏感，正如我们在上一节中讨论的那样。让我们尝试使用更多的观测数据运行这个相同的例子：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this case, with more data, we can see that the learned values are much closer
    to our original values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用更多的数据，我们可以看到学到的值与原始值更接近。
- en: MLE for HMMs
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型的最大似然估计（MLE）
- en: Having a basic understanding of MLE, we can now move on to applying these concepts
    to the case of HMMs. In the next few subsections, we will see two possible scenarios
    of learning in HMMs, namely, supervised learning and unsupervised learning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本了解MLE的基础上，我们现在可以将这些概念应用于HMM的情况。在接下来的几个小节中，我们将看到HMM学习的两种可能场景，即监督学习和无监督学习。
- en: Supervised learning
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'In the case of supervised learning, we use the data generated by sampling the
    process that we are trying to model. If we are trying to parameterize our HMM
    model using simple discrete distributions, we can simply apply the MLE to compute
    the transition and emission distributions by counting the number of transitions
    from any given state to another state. Similarly, we can compute the emission
    distribution by counting the output states from different hidden states. Therefore
    the transition and emission probabilities can be computed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的情况下，我们使用通过对我们试图建模的过程进行采样生成的数据。如果我们试图使用简单的离散分布来参数化我们的HMM模型，我们可以直接应用最大似然估计（MLE）通过统计从任何给定状态到其他状态的转移次数来计算转移和发射分布。类似地，我们可以通过统计不同隐状态下的输出状态来计算发射分布。因此，转移和发射概率可以通过以下方式计算：
- en: '![](img/0ebe8117-16f0-4638-a4c4-2235101afd0e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ebe8117-16f0-4638-a4c4-2235101afd0e.png)'
- en: Here, *T(i,j)* is the transition probability from state *i* to state *j*. And
    *E(i,s) *is the emission probability of getting state *s* from state *i*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*T(i,j)* 是从状态 *i* 到状态 *j* 的转移概率。而 *E(i,s)* 是从状态 *i* 获取状态 *s* 的发射概率。
- en: 'Let''s take a very simple example to make this clearer. We want to model the
    weather and whether or not it would rain over a period of time. Also, we assume
    that the weather can take three possible states:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个非常简单的例子来使这个概念更加清晰。我们希望建模天气以及是否会在一段时间内下雨。同时，我们假设天气可以有三种可能的状态：
- en: '*Sunny (S)*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*晴天 (S)*'
- en: '*Cloudy (C)*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多云 (C)*'
- en: '*Windy (W)*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有风 (W)*'
- en: 'And the *Rain* variable can have two possible states; *that it rained (R)*
    or *that it didn''t rain (NR)*. An HMM model would look something like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 *雨天* 变量有两个可能的状态：*下雨 (R)* 或 *不下雨 (NR)*。一个HMM模型大致是这样的：
- en: 'And let''s say we have some observed data for this which looks something like *D={(S,NR),
    (S,NR), (C,NR), (C,R), (C,R), (W,NR), (S,NR), (W,R), (C,NR)}*. Here, the first
    element of each datapoint represents the observed weatherthat day and the second
    element represents whether it rained or not that day. Now, using the formulas
    that we derived earlier, we can easily compute the transition and emission probabilities.
    We will start with computing the transition probability from *S* to *S*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些观察数据，如 *D={(S,NR), (S,NR), (C,NR), (C,R), (C,R), (W,NR), (S,NR), (W,R),
    (C,NR)}*。在这里，每个数据点的第一个元素表示当天观察到的天气，而第二个元素表示当天是否下雨。现在，使用我们之前推导出的公式，我们可以轻松地计算转移和发射概率。我们将从计算
    *S* 到 *S* 的转移概率开始：
- en: '![](img/33c63d6c-3e65-4819-9696-899a449302b0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33c63d6c-3e65-4819-9696-899a449302b0.png)'
- en: 'Similarly, we can compute the transition probabilities for all the other combinations
    of states:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算所有其他状态组合的转移概率：
- en: '![](img/db4f5682-7f09-4d02-a7b4-d7149798095c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db4f5682-7f09-4d02-a7b4-d7149798095c.png)'
- en: 'And, hence, we have our complete transition probability over all the possible
    states of the weather. We can represent it in tabular form to look nicer:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们就得到了天气的所有可能状态下的完整转移概率。我们可以以表格形式表示它，使其看起来更清晰：
- en: '|  | **Sunny(S)** | **Cloudy(C)** | **Windy(W)** |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | **晴天(S)** | **多云(C)** | **有风(W)** |'
- en: '| **Sunny(S)** | 0.33 | 0.33 | 0.33 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **晴天(S)** | 0.33 | 0.33 | 0.33 |'
- en: '| **Cloudy(C)** | 0 | 0.66 | 0.33 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **多云(C)** | 0 | 0.66 | 0.33 |'
- en: '| **Windy(W)** | 0.5 | 0.5 | 0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **有风 (W)** | 0.5 | 0.5 | 0 |'
- en: 'Table 1: Transition probability for the weather model'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：天气模型的转移概率
- en: 'Now, coming to computing the emission probability, we can again just follow
    the formula derived previously:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，计算发射概率时，我们可以再次按照之前推导的公式进行：
- en: '![](img/710bbee7-885a-4fb0-8f3c-8fddb48e341c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/710bbee7-885a-4fb0-8f3c-8fddb48e341c.png)'
- en: 'Similarly, we can compute all the other values in the distribution:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算分布中所有其他值：
- en: '![](img/2b4d9790-4cd0-403b-861a-927dcdc42baf.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b4d9790-4cd0-403b-861a-927dcdc42baf.png)'
- en: 'And hence our emission probability can be written in tabular form as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的发射概率可以写成表格形式如下：
- en: '|  | **Sunny(S)** | **Cloudy(C)** | **Windy(W)** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | **晴天(S)** | **多云(C)** | **有风(W)** |'
- en: '| **Rain (R)** | 0 | 0.5 | 0.5 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **雨天 (R)** | 0 | 0.5 | 0.5 |'
- en: '| **No Rain (NR)** | 1 | 0.5 | 0.5 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **无雨 (NR)** | 1 | 0.5 | 0.5 |'
- en: 'Table 2: Emission probability for the weather model'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：天气模型的发射概率
- en: In the previous example, we saw how we can compute the parameters of an HMM
    using MLE and some simple computations. But, because in this case we had assumed
    the transition and emission probabilities as simple discrete conditional distribution,
    the computation was much easier. With more complex cases, we will need to estimate
    more parameters than we did in the previous section in the case of the normal
    distribution.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们看到如何使用最大似然估计（MLE）和一些简单计算来计算HMM的参数。但由于在此案例中，我们假设了转移和发射概率是简单的离散条件分布，因此计算更为简便。在更复杂的情况下，我们需要估计比在前一节中正态分布情况更多的参数。
- en: Code
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'Let''s now try to code up the preceding algorithm:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试编写前述算法的代码：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s generate some data and try learning the parameters using the preceding
    function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些数据，并尝试使用之前的函数学习参数：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Unsupervised learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In the previous section, we saw how we can use supervised learning in a case
    where we have all the variables observed, including the hidden variables. But
    that is usually not the case with real-life problems. For such cases, we use unsupervised
    learning to estimate the parameters of the model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到当所有变量（包括隐藏变量）都被观察到时，如何使用监督学习。但在实际问题中，这种情况通常并不成立。对于这种情况，我们使用无监督学习来估计模型的参数。
- en: 'The two main learning algorithms used for this are the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用于此目的的两种主要学习算法如下：
- en: The Viterbi learning algorithm
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维特比学习算法
- en: The Baum-Welch algorithm
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baum-Welch 算法
- en: We will discuss these in the next couple of subsections.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几个子节中讨论这些内容。
- en: Viterbi learning algorithm
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维特比学习算法
- en: The Viterbi learning algorithm (not to be confused with the Viterbi algorithm
    for state estimation) takes a set of training observations *O^r*, with *1≤r≤R,* and
    estimates the parameters of a single HMM by iteratively computing Viterbi alignments.
    When used to initialize a new HMM, the Viterbi segmentation is replaced by a uniform
    segmentation (that is, each training observation is divided into *N* equal segments)
    for the first iteration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 维特比学习算法（与用于状态估计的维特比算法不同）接收一组训练观测值 *O^r*，其中 *1≤r≤R*，并通过迭代计算维特比对齐来估计单个HMM的参数。当用于初始化一个新的HMM时，维特比分割被均匀分割替代（即，每个训练观测值被划分为
    *N* 等分）用于第一次迭代。
- en: 'Other than the first iteration on a new model, each training sequence *O* is
    segmented using a state alignment procedure which results from maximizing:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在新模型的第一次迭代中，每个训练序列 *O* 都使用状态对齐过程进行分割，该过程是通过最大化以下公式得到的：
- en: '![](img/a9db8ff8-70d9-4d24-b6cb-b6b75107d1c1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9db8ff8-70d9-4d24-b6cb-b6b75107d1c1.png)'
- en: 'for *1<i<N* where:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *1<i<N*，其中：
- en: '![](img/4b6267fd-58d5-4263-b1a1-73900e2f01ea.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b6267fd-58d5-4263-b1a1-73900e2f01ea.png)'
- en: 'And the initial conditions are given by:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 初始条件由以下给出：
- en: '![](img/1892e2c6-f317-44fa-9d68-6d88fa772b0c.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1892e2c6-f317-44fa-9d68-6d88fa772b0c.png)'
- en: '![](img/d8b3a899-783f-4721-bbad-d2e6f3b07fd0.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8b3a899-783f-4721-bbad-d2e6f3b07fd0.png)'
- en: 'for *1<j<N*. And, in the discrete case, the output probability ![](img/67e61a73-a5d6-43a1-886d-1273ee4fec13.png)is
    defined as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *1<j<N*。在离散情况下，输出概率 ![](img/67e61a73-a5d6-43a1-886d-1273ee4fec13.png) 定义为：
- en: '![](img/d8b2df7f-45e2-48bd-9844-2685f99504f1.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8b2df7f-45e2-48bd-9844-2685f99504f1.png)'
- en: where *S* is the total number of streams, *v[s](O[st])* is the output given,
    the input *O[st,]* and *P[js][v]* is the probability of state *j* to give an output
    *v*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *S* 是流的总数，*v[s](O[st])* 是给定的输出，输入 *O[st]*，以及 *P[js][v]* 是状态 *j* 给定输出 *v*
    的概率。
- en: 'If *A[ij]* represents the total number of transitions from state *i* to state
    *j* in performing the preceding maximizations, then the transition probabilities
    can be estimated from the relative frequencies:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *A[ij]* 表示在执行上述最大化时，从状态 *i* 到状态 *j* 的总转移次数，则可以从相对频率中估计转移概率：
- en: '![](img/6d90b2a9-b716-4397-a074-b8f018b14a28.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d90b2a9-b716-4397-a074-b8f018b14a28.png)'
- en: 'The sequence of states that maximizes *∅[N](T)* implies an alignment of training
    data observations with states. Within each state, a further alignment of observations
    to mixture components is made. Usually, two mechanisms can be used for this, for
    each state and each output stream:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化 *∅[N](T)* 的状态序列意味着训练数据观测与状态的对齐。在每个状态内，还会进一步将观测值对齐到混合成分。通常，对于每个状态和每个输出流，可以使用两种机制：
- en: Use clustering to allocate each observation *O[st]* to one of *M[s]* clusters
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聚类将每个观测值 *O[st]* 分配给一个 *M[s]* 集群
- en: Associate each observation *O[st]* with the mixture component with the highest
    probability
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个观测值 *O[st]* 与具有最高概率的混合成分关联
- en: In either case, the net result is that every observation is associated with
    a single unique mixture component. This association can be represented by the
    indicator function ![](img/54848bcf-e5b3-42ce-a7d8-7afedce1e71c.png), which is
    *1* if ![](img/746625b8-e57b-4c27-b113-12cd1284871f.png) is associated with a
    mixture component *m* of stream *s* of state *j,* and is zero otherwise.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，最终结果是每个观察值都与一个唯一的混合成分相关联。这个关联可以通过指示函数 ![](img/54848bcf-e5b3-42ce-a7d8-7afedce1e71c.png)表示，当 ![](img/746625b8-e57b-4c27-b113-12cd1284871f.png)与状态为*j*的流*s*的混合成分*m*相关联时，值为*1*，否则为零。
- en: 'The means and variances are then estimated by computing simple means:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 均值和方差随后通过计算简单的均值来估计：
- en: '![](img/785f9fb8-df5e-4cdc-b8be-d0ea5d87eea2.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/785f9fb8-df5e-4cdc-b8be-d0ea5d87eea2.png)'
- en: '![](img/f00c98ac-1363-4bda-8bee-58585d1e2345.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f00c98ac-1363-4bda-8bee-58585d1e2345.png)'
- en: 'And the mixture weights are based on the number of observations allocated to
    each component:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 混合权重基于分配给每个成分的观察数量：
- en: '![](img/862c8580-5b51-48fc-bdd2-75bcb49eda3c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/862c8580-5b51-48fc-bdd2-75bcb49eda3c.png)'
- en: The Baum-Welch algorithm (expectation maximization)
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Baum-Welch算法（期望最大化）
- en: The **expectation maximization** (**EM**) algorithm (known as **Baum-Welch**
    when applied to HMMs) is an iterative method used to find the maximum likelihood
    or **maximum a posteriori** (**MAP**) estimates of parameters in statistical models,
    where the model depends on unobserved latent variables. The EM iteration alternates
    between performing an **expectation** (**E**) step, which creates a function for
    the expectation of the log-likelihood evaluated using the current estimate for
    the parameters, and a **maximization **(**M**) step, which computes parameters
    maximizing the expected log-likelihood found on the *E* step. These parameter
    estimates are then used to determine the distribution of the latent variables
    in the next *E* step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**（**EM**）算法（在应用于HMM时称为**Baum-Welch**）是一种迭代方法，用于求解统计模型中依赖于未观察到的潜在变量的参数的最大似然估计或**最大后验估计**（**MAP**）。EM迭代在执行**期望**（**E**）步骤和**最大化**（**M**）步骤之间交替进行，**E**步骤创建一个期望的对数似然函数，该函数使用当前的参数估计进行评估，**M**步骤则计算最大化**E**步骤找到的期望对数似然的参数。这些参数估计随后用于确定在下一个**E**步骤中潜在变量的分布。'
- en: 'The EM algorithm starts with initial value of parameters (*θ^(old)*). In the
    *E *step, we take these parameters and find the posterior distribution of latent
    variables *P(Z|X,θ^(old))*. We then use this posterior distribution to evaluate
    the expectation of the logarithm of the complete data likelihood function, as
    a function of the parameters *θ*, to give the function *Q(θ,θ^(old)),* defined
    by the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法从参数的初始值（*θ^(old)*）开始。在**E**步骤中，我们取这些参数并找到潜在变量的后验分布*P(Z|X,θ^(old))*。然后，我们使用这个后验分布来评估完整数据似然函数对数的期望，作为参数*θ*的函数，得到函数*Q(θ,θ^(old))*，其定义如下：
- en: '![](img/b835312c-23b2-4ddd-b44c-87b037870f51.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b835312c-23b2-4ddd-b44c-87b037870f51.png)'
- en: 'Let''s introduce some terms that can help us in the future. *γ(Z[n])* to denote
    the marginal posterior distribution of a latent variable:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们引入一些术语，这些术语可以帮助我们将来使用。*γ(Z[n])*表示潜在变量的边际后验分布：
- en: '![](img/e473d34c-bc1d-425f-b461-ebd6bf8a8246.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e473d34c-bc1d-425f-b461-ebd6bf8a8246.png)'
- en: '*ξ(z[n-1], z[n])* denoting the marginal posterior distribution of two successive
    latent variables:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*ξ(z[n-1], z[n])*表示两个连续潜在变量的边际后验分布：'
- en: '![](img/936a734c-181c-4c4b-969b-c593f6048656.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/936a734c-181c-4c4b-969b-c593f6048656.png)'
- en: Thus, for each value of *n*, we can store *γ(Z[n])* as a vector of *K* non-negative
    numbers that sum to *1,* and, similarly we can use a *K×K* matrix of non-negative
    numbers that sum to *1* to save *ξ(z[n-1], z[n])*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个*n*值，我们可以将 *γ(Z[n])* 存储为一个*K*维的非负数向量，且这些数的和为*1*，同样我们可以使用一个*K×K*的非负数矩阵来存储*ξ(z[n-1],
    z[n])*，其和也为*1*。
- en: 'As we have discussed in previous chapters, the latent variable *z[n]* can be
    represented as *K* dimensional binary variable where *z[nk] = 1* when *z[n]* is
    in state *k*. We can also use it to denote the conditional probability of *z[nk] =
    1,* and similarly *ξ(z[n-1], j, z[nk])* to denote the conditional probability
    of *zn-1*, *j = 1,* and *z[nk] = 1*. As the expectation of a binary random variable
    is just the probability of its value being *1*, we can state the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中讨论的，潜在变量*z[n]*可以表示为*K*维的二元变量，当*z[n]*处于状态*k*时，*z[nk] = 1*。我们还可以用它表示*z[nk]
    = 1*的条件概率，同样，*ξ(z[n-1], j, z[nk])*可以表示*z[n-1]*、*j = 1*和*z[nk] = 1*的条件概率。由于二元随机变量的期望就是其值为*1*的概率，我们可以得出以下结论：
- en: '![](img/29aa84f0-6709-4d8e-9eed-2fdbb9d243aa.png)![](img/cace6a8f-3baa-4e7a-b215-cd7d51a9db15.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29aa84f0-6709-4d8e-9eed-2fdbb9d243aa.png)![](img/cace6a8f-3baa-4e7a-b215-cd7d51a9db15.png)'
- en: 'As we discussed in the previous chapter, the joint probability distribution
    of an HMM can be represented as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章讨论的那样，HMM的联合概率分布可以表示如下：
- en: '![](img/ad2a4b0e-4746-4014-994f-7107eadce813.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad2a4b0e-4746-4014-994f-7107eadce813.png)'
- en: 'Thus we can write the data likelihood function as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将数据似然函数写成如下形式：
- en: '![](img/9185967c-2c6d-4a48-a76a-a43632f416ae.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9185967c-2c6d-4a48-a76a-a43632f416ae.png)'
- en: In the *E* step we try to evaluate the quantities* γ(z[n])* and *ξ(z[n-1], z[n])* efficiently. For
    efficient computation of these two terms we can use either a forward backward
    algorithm or the Viterbi algorithm as discussed in the previous chapter. And in
    the *M* step, we try to maximize the value of *Q(θ, θ^(old))* with respect to
    the parameters *θ={A, π, Φ}* in which we treat *γ(z[n])* and *ξ(z[n-1], z[n])* as
    constants.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在*E*步骤中，我们尝试有效地评估量*γ(z[n])*和*ξ(z[n-1], z[n])*。为了高效地计算这两个项，我们可以使用前向后向算法或维特比算法，正如上一章所讨论的那样。在*M*步骤中，我们尝试对*Q(θ,
    θ^(old))* 关于参数*θ={A, π, Φ}*进行最大化，其中我们将*γ(z[n])*和*ξ(z[n-1], z[n])*视为常数。
- en: 'In doing so, we get the MLE values of the parameters as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们得到了参数的MLE值，如下所示：
- en: '![](img/77ebd97f-a907-45e7-a0f9-03dedef7cabc.png)![](img/84bef755-e69e-4bb8-98a4-7397e9e18821.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77ebd97f-a907-45e7-a0f9-03dedef7cabc.png)![](img/84bef755-e69e-4bb8-98a4-7397e9e18821.png)'
- en: 'If we assume the emission distribution to be a normal distribution such that ![](img/dcabab1e-ffd3-473c-a5e6-8d5c9e8f1fd3.png),
    then the maximization of *Q(θ, θ^(old))* with respect to *Φ[k]* would result in the
    following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设发射分布是正态分布，令 ![](img/dcabab1e-ffd3-473c-a5e6-8d5c9e8f1fd3.png)，那么对*Q(θ,
    θ^(old))* 关于*Φ[k]*的最大化将得到如下结果：
- en: '![](img/2920ee83-9f17-4334-a247-16c18f4f1a89.png)![](img/b922047a-e242-4346-b85e-478a777dfef6.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2920ee83-9f17-4334-a247-16c18f4f1a89.png)![](img/b922047a-e242-4346-b85e-478a777dfef6.png)'
- en: The EM algorithm must be initialized by choosing starting values for *π* and
    *A*, which should, of course, be non-negative and should add up to *1*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法必须通过选择*π*和*A*的初始值来初始化，这些值当然应该是非负的，并且总和为*1*。
- en: Code
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: The algorithms for parameter estimation look quite complex but `hmmlearn`, a
    Python package for working with HMMs, has great implementations for it. `hmmlearn`
    is also hosted on PyPI so it can be installed directly using `pip:pip install
    hmmlearn`. For the code example, we will take an example of stock price prediction
    by learning a Gaussian HMM on stock prices. This example has been taken from the
    examples page of `hmmlearn`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 参数估计的算法看起来相当复杂，但`hmmlearn`，一个用于处理HMM的Python包，提供了很好的实现。`hmmlearn`也托管在PyPI上，因此可以直接通过`pip:pip
    install hmmlearn`安装。对于代码示例，我们将以通过学习股票价格的高斯HMM进行股票价格预测为例。这个示例来自`hmmlearn`的示例页面。
- en: 'For the example, we also need the `matplotlib` and `datetime` packages which
    can also be installed using `pip`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们还需要安装`matplotlib`和`datetime`包，这些也可以通过`pip`安装：
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Coming to the code, we should start by importing all the required packages:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 进入代码部分，我们应从导入所有必要的包开始：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will fetch our stock price data from Yahoo! Finance:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从Yahoo! Finance获取我们的股票价格数据：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we define a Gaussian HMM model and learn the parameters for our data:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个高斯HMM模型，并为我们的数据学习参数：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now print out our learned parameters:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以打印出我们学到的参数：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can also plot our hidden states over time:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制出隐藏状态随时间变化的图：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![](img/518698e7-beb2-4ed5-a1f0-2ce16f0b085c.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/518698e7-beb2-4ed5-a1f0-2ce16f0b085c.png)'
- en: 'Figure 1: Plot of hidden states over time'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：隐藏状态随时间变化的图
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we introduced algorithms for doing parameter estimation of
    a given HMM model. We started by looking into the basics of MLE and then applied
    the concepts to HMMs. For HMM training, we looked into two different scenarios:
    supervised training, when we have the observations for the hidden states, and
    unsupervised training, when we only have the output observations.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了用于估计给定HMM模型参数的算法。我们从MLE的基本概念入手，然后将这些概念应用于HMM。对于HMM的训练，我们讨论了两种不同的场景：有监督训练，当我们有隐藏状态的观测值时；以及无监督训练，当我们只有输出观测值时。
- en: We also talked about the problems with estimation using MLE. In the next chapter,
    we will introduce algorithms for doing parameter estimation using the Bayesian
    approach, which tries to solve these issues.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了使用MLE进行估计时遇到的问题。在下一章中，我们将介绍使用贝叶斯方法进行参数估计的算法，该方法试图解决这些问题。
