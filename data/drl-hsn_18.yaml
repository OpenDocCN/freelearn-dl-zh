- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Advanced Exploration
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级探索
- en: In this chapter, we will talk about the topic of exploration in reinforcement
    learning (RL). It has been mentioned several times in the book that the exploration/exploitation
    dilemma is a fundamental thing in RL and very important for efficient learning.
    However, in the previous examples, we used quite a trivial approach to exploring
    the environment, which was, in most cases, 𝜖-greedy action selection. Now it’s
    time to go deeper into the exploration subfield of RL, as more complicated environments
    might require much better exploration strategies than 𝜖-greedy approach.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论强化学习（RL）中的探索主题。书中多次提到，探索/利用困境是强化学习中的一个基本问题，对于高效学习非常重要。然而，在之前的例子中，我们使用了一种相当简单的探索环境的方法，即大多数情况下的
    𝜖-greedy 行动选择。现在是时候深入探讨强化学习中的探索子领域，因为更复杂的环境可能需要比 𝜖-greedy 方法更好的探索策略。
- en: 'More specifically, we will cover the following key topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们将涵盖以下关键主题：
- en: Why exploration is such a fundamental topic in RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么探索是强化学习中如此基本的话题
- en: The effectiveness of the epsilon-greedy (𝜖-greedy) approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 𝜖-greedy 方法的有效性
- en: Alternatives and how they work in different environments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替代方法及其在不同环境中的工作原理
- en: We will implement the methods described to solve a toy, but still challenging,
    problem called MountainCar. This will allow us to better understand the methods,
    the way they could be implemented, and their behavior. After that, we will try
    to tackle a harder problem from the Atari suite.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现所描述的方法，解决一个名为 MountainCar 的玩具问题，尽管它依然具有挑战性。这将帮助我们更好地理解这些方法、它们如何实现以及它们的行为。之后，我们将尝试解决一个来自
    Atari 套件的更难问题。
- en: Why exploration is important
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么探索很重要
- en: In this book, lots of environments and methods have been discussed, and in almost
    every chapter, exploration was mentioned. Very likely, you’ve already got ideas
    about why it’s important to explore the environment effectively, so I’m just going
    to discuss the main reasons.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讨论了许多环境和方法，几乎每一章都提到了探索。很可能你已经对为什么有效地探索环境很重要有了一些想法，所以我将只讨论主要的原因。
- en: Before that, it might be useful to agree on the term “effective exploration.”
    In theoretical RL, a strict definition of this exists, but the high-level idea
    is simple and intuitive. Exploration is effective when we don’t waste time in
    states of the environment that have already been seen by and are familiar to the
    agent. Rather than taking the same actions again and again, the agent needs to
    look for a new experience. As we’ve already discussed, exploration has to be balanced
    by exploitation, which is the opposite and means using our knowledge to get the
    best reward in the most efficient way. Let’s now quickly discuss why we might
    be interested in effective exploration in the first place.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，定义“有效探索”可能会有帮助。在理论强化学习中，已有严格的定义，但高层次的概念既简单又直观。当我们不再浪费时间在已经被智能体见过并且熟悉的环境状态中时，探索就是有效的。智能体不应一遍遍做相同的动作，而是需要寻找新的经验。正如我们之前讨论过的，探索必须与利用相平衡，后者是相反的概念，指的是利用我们的知识以最有效的方式获得最好的奖励。现在让我们快速讨论一下为什么我们最初会对有效探索感兴趣。
- en: First, good exploration of the environment might have a fundamental influence
    on our ability to learn a good policy. If the reward is sparse and the agent obtains
    a good reward on some rare conditions, it might experience a positive reward only
    once in many episodes, so the ability of the learning process to explore the environment
    effectively and fully might bring more samples with a good reward that the method
    could learn from.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，良好的环境探索可能对我们学习良好策略的能力产生根本性影响。如果奖励稀疏，且智能体只有在某些罕见条件下才能获得良好的奖励，那么它可能在许多回合中只会经历一次正奖励，因此学习过程有效且充分地探索环境的能力，可能会带来更多能够从中学习到的良好奖励样本。
- en: In some cases, which are very frequent in practical applications of RL, a lack
    of good exploration might mean that the agent will never experience a positive
    reward at all, which makes everything else useless. If you have no good samples
    to learn from, you can have the most efficient RL method, but the only thing it
    will learn is that there is no way to get a good reward. This is the case for
    lots of practically interesting problems around us. For instance, we will take
    a closer look at the MountainCar environment later in the chapter, which has trivial
    dynamics, but due to a sparsity of rewards, is quite tricky to solve.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些情况下，这种情况在强化学习的实际应用中非常常见，缺乏良好的探索可能意味着代理根本无法体验到正向奖励，这样其他一切就变得无用。如果你没有好的样本来学习，你可以拥有最有效的强化学习方法，但它唯一能学到的就是没有办法获得好的奖励。这正是许多实际中有趣的问题的情况。例如，我们将在本章稍后详细了解MountainCar环境，它的动力学非常简单，但由于奖励稀疏，解决起来相当棘手。
- en: On the other hand, even if the reward is not sparse, effective exploration increases
    the training speed due to better convergence and training stability. This happens
    because our sample from the environment becomes more diverse and requires less
    communication with the environment. As a result, our RL method has the chance
    to learn a better policy in a shorter time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，即使奖励不是稀疏的，有效的探索也能提高训练速度，因为它有助于更好的收敛性和训练稳定性。这是因为我们从环境中采样变得更加多样化，且与环境的通信需求减少。因此，我们的强化学习方法有机会在更短的时间内学习到更好的策略。
- en: What’s wrong with 𝜖-greedy?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 𝜖-greedy有什么问题吗？
- en: Throughout the book, we have used the 𝜖-greedy exploration strategy as a simple,
    but still acceptable, approach to exploring the environment. The underlying idea
    behind 𝜖-greedy is to take a random action with the probability of 𝜖; otherwise,
    (with 1 −𝜖 probability) we act according to the policy (greedily). By varying
    the hyperparameter 0 ≤𝜖 ≤ 1, we can change the exploration ratio. This approach
    was used in most of the value-based methods described in the book. Quite a similar
    idea was used in policy-based methods, when our network returns the probability
    distribution over actions to take. To prevent the network from becoming too certain
    about actions (by returning a probability of 1 for a specific action and 0 for
    others), we added the entropy loss, which is just the entropy of the probability
    distribution multiplied by some hyperparameter. In the early stages of the training,
    this entropy loss pushes our network toward taking random actions (by regularizing
    the probability distribution). But in later stages, when we have explored the
    environment enough and our reward is relatively high, the policy gradient dominates
    over this entropy regularization. But this hyperparameter requires tuning to work
    properly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在全书中，我们使用了𝜖-greedy探索策略作为一种简单但仍然可接受的环境探索方法。𝜖-greedy背后的基本思想是以𝜖的概率采取随机动作；否则，（以1
    −𝜖的概率）我们按照策略（贪婪地）执行动作。通过调整超参数0 ≤𝜖 ≤ 1，我们可以改变探索的比例。这种方法在本书中描述的大多数基于值的方法中都有使用。类似的思想也被应用于基于策略的方法，当我们的网络返回一个动作的概率分布时。为了防止网络对动作变得过于确定（通过为某个特定动作返回1的概率，为其他动作返回0的概率），我们添加了熵损失，它实际上是概率分布的熵乘以某个超参数。在训练的早期阶段，这个熵损失推动我们的网络采取随机动作（通过正则化概率分布）。但在后期，当我们足够探索了环境且奖励相对较高时，策略梯度就主导了这种熵正则化。但是，这个超参数需要调整才能正常工作。
- en: 'At a high level, both approaches are doing the same thing: to explore the environment,
    we introduce randomness into our actions. However, recent research shows that
    this approach is very far from being ideal:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，两种方法做的事情是相同的：为了探索环境，我们将随机性引入到我们的动作中。然而，最近的研究表明，这种方法距离理想状态还有很大差距：
- en: In the case of value iteration methods, random actions taken in some pieces
    of our trajectory introduce bias into our Q-value estimation. The Bellman equation
    assumes that the Q-value for the next state is obtained from the action with the
    largest Q. In other words, the rest of the trajectory is supposed to be from our
    optimal behavior. But with 𝜖-greedy, we might take not the optimal action, but
    just a random action, and this piece of the trajectory will be stored in the replay
    buffer for a long time, until our 𝜖 is decayed and old samples are pushed from
    the buffer. Before that happens, we will learn wrong Q-values.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在值迭代方法中，轨迹中的某些随机动作会引入偏差，影响我们对Q值的估计。贝尔曼方程假设下一个状态的Q值是通过选择Q值最大的动作来获得的。换句话说，轨迹的其余部分应来自我们的最优行为。然而，使用𝜖-贪婪策略时，我们可能不会选择最优动作，而是随机选择一个动作，这段轨迹将会长期保存在回放缓冲区中，直到我们的𝜖值衰减并且旧样本被从缓冲区中删除。在此之前，我们将学习到错误的Q值。
- en: With random actions injected into our trajectory, our policy changes with every
    step. With the frequency defined by the value of 𝜖 or the entropy loss coefficient,
    our trajectory constantly switches from a random policy to our current policy.
    This might lead to poor state space coverage in situations when multiple steps
    are needed to reach some isolated areas in the environment’s state space.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着随机动作的注入，我们的策略在每一步都会发生变化。根据𝜖值或熵损失系数定义的频率，我们的轨迹会不断地在随机策略和当前策略之间切换。这可能导致在需要多个步骤才能到达环境状态空间中某些孤立区域时，状态空间的覆盖不充分。
- en: 'To illustrate the last issue, let’s consider a simple example taken from the
    paper by Strehl and Littman called An analysis of model-based interval estimation
    for Markov decision processes, which was published in 2008 [SL08]. The example
    is called “River Swim” and it models a river that the agent needs to cross. The
    environment contains six states and two actions: left and right. States 1 and
    6 are on the river’s opposite sides and states 2 to 5 are in the water.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明最后一个问题，让我们考虑一个简单的例子，取自Strehl和Littman的论文《基于模型的区间估计分析：马尔可夫决策过程》，该论文于2008年发表[SL08]。这个例子称为“River
    Swim”，它模拟了一个智能体需要跨越的河流。环境包含六个状态和两个动作：左移和右移。状态1和状态6位于河流的两侧，状态2到状态5位于水中。
- en: 'Figure [18.1](#x1-330004r1) shows the transition diagram for the first two
    states, 1 and 2:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图[18.1](#x1-330004r1)显示了前两个状态（状态1和状态2）的转移图：
- en: '![123pppppp ====== 001000.6.4.0.6.355 ](img/B22150_18_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![123pppppp ====== 001000.6.4.0.6.355 ](img/B22150_18_01.png)'
- en: 'Figure 18.1: Transitions for the first two states of the River Swim environment'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1：River Swim环境的前两个状态的转移
- en: In the first state (the circle with the label “1”), the agent stands on the
    ground of the riverbank. The only action is right (shown in solid lines), which
    means entering the river and swimming against the current to state 2\. But the
    current is strong, and our right action from state 1 succeeds only with a probability
    of 60% (the solid line from state 1 to state 2). With a probability of 40%, the
    current keeps us in state 1 (the solid line connecting state 1 to itself).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个状态（标有“1”的圆圈）中，智能体站在河岸上。唯一的动作是右移（通过实线表示），意味着进入河流并逆流游泳到达状态2。然而，水流很强，从状态1向右游泳的动作成功的概率只有60%（从状态1到状态2的实线）。以40%的概率，水流将我们留在状态1（连接状态1与自身的实线）。
- en: 'In the second state (the circle with the label “2”), we have two actions: left,
    which is shown by the dotted line connecting states 2 and 1 (this action always
    succeeds, as the current flushes us back to the riverbank), and right (dashed
    lines), which means swimming against the current to state 3\. As before, swimming
    against the current is hard, so the probability of getting from state 2 to state
    3 is just 35% (the dashed line connecting states 2 and 3). With a probability
    of 60%, our left action ends up in the same state (the curved dashed line connecting
    state 2 to itself). But sometimes, despite our efforts, our left action ends up
    in state 1, which happens with a 5% probability (the curved dashed line connecting
    states 2 and 1).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个状态（标有“2”的圆圈）中，我们有两个动作：左移，通过虚线连接状态2和状态1（该动作总是成功的，因为当前的流水会将我们冲回河岸），以及右移（虚线），意味着逆流游泳到达状态3。如前所述，逆流游泳很困难，因此从状态2到状态3的概率仅为35%（连接状态2和状态3的虚线）。以60%的概率，我们的左移动作最终会停留在同一状态（连接状态2和状态2的弯曲虚线）。但有时，尽管我们努力，左移动作最终会使我们回到状态1，这种情况发生的概率为5%（连接状态2和状态1的弯曲虚线）。
- en: 'As I’ve said, there are six states in River Swim, but the transitions for states
    3, 4, and 5 are identical to those for state 2\. The last state, 6, is similar
    to state 1, so there is only one action available there: left, meaning to swim
    back. In Figure [18.2](#x1-330006r2), you can see the full transition diagram
    (which is just clones of the diagram we’ve already seen, where right action transitions
    are shown as solid lines, and left action transitions are dotted lines):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所说，River Swim 有六个状态，但状态 3、4 和 5 的转换与状态 2 相同。最后一个状态 6 与状态 1 相似，因此在该状态下只有一个动作可用：左，即游回去。在图
    [18.2](#x1-330006r2) 中，你可以看到完整的转换图（这只是我们之前见过的图的克隆，右转动作的转换用实线表示，左转动作的转换用虚线表示）：
- en: '![123456ppppppppppppppppppp = = = = = = = = = = = = = = = = = = = 0010001000100010001.6.4.0.6.3.0.6.3.0.6.3.0.6.355555555
    ](img/B22150_18_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![123456ppppppppppppppppppp = = = = = = = = = = = = = = = = = = = 0010001000100010001.6.4.0.6.3.0.6.3.0.6.3.0.6.355555555
    ](img/B22150_18_02.png)'
- en: 'Figure 18.2: The full transition diagram for the River Swim environment'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2：River Swim 环境的完整转换图
- en: In terms of the reward, the agent gets a small reward of 1 for the transition
    between states 1 to 5, but it gets a very high reward of 1,000 for getting into
    state 6, which acts as compensation for all the efforts of swimming against the
    current.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 就奖励而言，代理在状态 1 到状态 5 之间的转换获得 1 的小奖励，但进入状态 6 时会获得 1,000 的高奖励，作为对逆流游泳所有努力的补偿。
- en: 'Despite the simplicity of the environment, its structure creates a problem
    for the 𝜖-greedy strategy being able to fully explore the state space. To check
    this, I implemented a very simple simulation of this environment, which you will
    find in Chapter18/riverswim.py. The simulated agent always acts randomly (𝜖 =
    1) and the result of the simulation is the frequency of various state visits.
    The number of steps the agent can take in one episode is limited to 10, but this
    can be changed using the command line. We won’t go over the entire code here;
    you can refer to it in the GitHub repository. Here, let’s look at the results
    of the experiments:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管环境本身很简单，但其结构为 𝜖-贪婪策略能够完全探索状态空间带来了问题。为了检查这一点，我实现了这个环境的一个非常简单的模拟，你可以在 Chapter18/riverswim.py
    中找到它。模拟中的代理总是随机行动（𝜖 = 1），模拟结果是各种状态访问的频率。代理在一个回合中可以采取的步数限制为 10，但可以通过命令行进行更改。我们不会在这里详细讲解整个代码；你可以在
    GitHub 仓库中查看。现在，我们来看一下实验结果：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding output, each line shows the state number and the number of
    times it was visited during the simulation. With default command-line options,
    the simulation of 100 steps (10 episodes) was performed. As you can see, the agent
    never reached state 6 and was only in state 5 once. By increasing the number of
    episodes, the situation became a bit better, but not much:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的输出中，每一行显示了状态编号以及在模拟过程中访问该状态的次数。使用默认的命令行选项，进行了 100 步（10 个回合）的模拟。正如你所看到的，代理从未到达状态
    6，并且仅在状态 5 中出现过一次。通过增加回合数，情况稍有改善，但并没有太大变化：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With 10 times more episodes simulated, we still didn’t visit state 6, so the
    agent had no idea about the large reward there.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟了 10 倍回合后，我们仍然没有访问状态 6，因此代理完全不知道那里有如此高的奖励。
- en: 'Only with 10,000 episodes simulated were we able to get to state 6, but only
    five times, which is 0.05% of all the steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在模拟了 10,000 个回合后，我们才成功到达状态 6，但仅仅 5 次，占所有步骤的 0.05%：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Therefore, it’s not very likely that the training will be efficient, even with
    the best RL method. Also, we had only six states in this example. Imagine how
    inefficient it will be with 20 or 50 states, which is not that unlikely; for example,
    in Atari games, there might be hundreds of decisions to be made before something
    interesting happens. If you want to, you can experiment with the riverswim.py
    tool, which allows you to change the random seed, the number of steps in the episode,
    the total number of steps, and even the number of states in the environment.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使采用最好的强化学习方法，训练的效率也不太可能很高。此外，在这个例子中，我们只有六个状态。想象一下，如果有 20 或 50 个状态，效率会低到什么程度，而这并非不可能；例如，在
    Atari 游戏中，可能需要做出数百个决策才能发生一些有趣的事情。如果你愿意，可以使用 riverswim.py 工具进行实验，工具允许你更改随机种子、回合中的步数、总步数，甚至环境中的状态数。
- en: This simple example illustrates the issue with random actions in exploration.
    By acting randomly, our agent does not try to actively explore the environment;
    it just hopes that random actions will bring something new to its experience,
    which is not always the best thing to do.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子说明了在探索中随机动作的问题。通过随机行动，我们的智能体并没有积极地去探索环境，它只是希望随机动作能为其经验带来一些新东西，但这并不总是最好的做法。
- en: Let’s now discuss more efficient approaches to the exploration problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论一些更高效的探索方法。
- en: Alternative ways of exploration
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索的替代方法
- en: In this section, we will provide you with an overview of a set of alternative
    approaches to the exploration problem. This won’t be an exhaustive list of approaches
    that exist, but rather will provide an outline of the landscape.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为您提供一组探索问题的替代方法的概述。这并不是现有方法的详尽列表，而是提供一个领域概况。
- en: 'We’re going to explore the following three approaches to exploration:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索以下三种探索方法：
- en: Randomness in the policy, when stochasticity is added to the policy that we
    use to get samples. The method in this family is noisy networks, which we have
    already covered in Chapter [8](ch012.xhtml#x1-1240008).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略中的随机性，当我们在获取样本时向所使用的策略中添加随机性。本方法家族中的方法是噪声网络，我们在第[8](ch012.xhtml#x1-1240008)章中已经讲过。
- en: 'Count-based methods, which keep track of the number of times the agent has
    seen the particular state. We will check two methods: the direct counting of states
    and the pseudo-count method.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于计数的方法，它们记录智能体在特定状态下出现的次数。我们将检查两种方法：直接计数状态和伪计数方法。
- en: Prediction-based methods, which try to predict something from the state and
    from the quality of the prediction. We can make judgements about the familiarity
    of the agent with this state. To illustrate this approach, we will take a look
    at the policy distillation method, which has shown state-of-the-art results on
    hard-exploration Atari games like Montezuma’s Revenge.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于预测的方法，它们尝试根据状态和预测的质量来预测某些内容。我们可以判断智能体对该状态的熟悉程度。为了说明这种方法，我们将通过观察策略蒸馏方法来进行说明，该方法在像《蒙特祖玛的复仇》这样的难度较大的
    Atari 游戏中取得了最先进的成果。
- en: Before implementing these methods, let’s try and understand them in greater
    detail.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现这些方法之前，让我们尝试更详细地理解它们。
- en: Noisy networks
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声网络
- en: Let’s start with an approach that is already familiar to us. We covered the
    method called noisy networks in Chapter [8](ch012.xhtml#x1-1240008), when we referred
    to Hessel et al. [[Hes+18](#)] and discussed deep Q-network (DQN) extensions.
    The idea is to add Gaussian noise to the network’s weights and learn the noise
    parameters (mean and variance) using backpropagation, in the same way that we
    learn the model’s weights. In that chapter, this simple approach gave a significant
    boost in Pong training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个我们已经熟悉的方法开始。我们在第[8](ch012.xhtml#x1-1240008)章中提到过噪声网络方法，当时我们提到 Hessel 等人[[Hes+18](#)]并讨论了深度
    Q 网络（DQN）的扩展。其思路是向网络的权重中添加高斯噪声，并通过反向传播来学习噪声参数（均值和方差），这与我们学习模型的权重的方式相同。在那一章中，这种简单的方法显著提升了
    Pong 游戏的训练效果。
- en: At a high level, this might look very similar to the 𝜖-greedy approach, but
    Fortunato et al. [[For+17](#)] claimed a difference. The difference lies in the
    way we apply stochasticity to the network. In 𝜖-greedy, randomness is added to
    the actions. In noisy networks, randomness is injected into part of the network
    itself (several fully connected layers close to the output), which means adding
    stochasticity to our current policy. In addition, parameters of the noise might
    be learned during the training, so the training process might increase or decrease
    this policy randomness if needed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次看，这可能看起来与𝜖-贪婪方法非常相似，但Fortunato等人[[For+17](#)]声称存在差异。这个差异在于我们如何将随机性应用到网络中。在𝜖-贪婪方法中，随机性是添加到动作中的。而在噪声网络中，随机性被注入到网络的部分（接近输出的几个全连接层），这意味着将随机性添加到我们当前的策略中。此外，噪声的参数可能会在训练过程中学习，因此训练过程可能会根据需要增加或减少这种策略的随机性。
- en: According to the paper, the noise in noisy layers needs to be sampled from time
    to time, which means that our training samples are not produced by our current
    policy, but by the ensemble of policies. With this, our exploration becomes directed,
    as random values added to the weights produce a different policy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据论文，噪声层中的噪声需要不时进行采样，这意味着我们的训练样本不是由当前策略生成的，而是由多个策略的集成生成的。这样一来，我们的探索变得有针对性，因为加到权重上的随机值会产生不同的策略。
- en: Count-based methods
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于计数的方法
- en: This family of methods is based on the intuition to visit states that have not
    been explored before. In simple cases, when the state space is not very large
    and different states are easily distinguishable from each other, we just count
    the number of times we have seen the state or state + action and prefer to get
    to the states for which this count is low.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类方法基于一个直觉：访问那些之前没有被探索过的状态。在简单的情况下，当状态空间不太大并且不同的状态很容易区分时，我们只需计算看到状态或状态+动作的次数，并倾向于前往那些计数较低的状态。
- en: 'This could be implemented as an extra reward, not obtained from the environment
    but from the visit count of the state. In the literature, such a reward is called
    an intrinsic reward. In this context, the reward from the environment is called
    an extrinsic reward. One of the options to formulate such a reward is to use the
    bandits exploration approach: ![√-1--- N˜(s)](img/eq70.png). Here, Ñ(s) is a count
    or pseudo-count of times we have seen the state, s, and value c defines the weight
    of the intrinsic reward.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以作为额外的奖励来实现，这种奖励不是来自环境，而是来自状态的访问次数。在文献中，这种奖励被称为内在奖励。在这个语境中，环境中的奖励被称为外在奖励。制定这种奖励的一种方式是使用强盗探索方法：![√-1---
    N˜(s)](img/eq70.png)。这里，Ñ(s)是我们看到状态s的次数或伪计数，值c定义了内在奖励的权重。
- en: If the number of states is small, like in the tabular learning case (we discussed
    it in Chapter [5](ch009.xhtml#x1-820005)), we can just count them. In more difficult
    cases, when there are too many states, some transformation of the states needs
    to be introduced, like the hashing function or some embeddings of the states (we’ll
    discuss this later in the chapter in more detail).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态的数量很少，比如在表格学习的情况下（我们在第[5](ch009.xhtml#x1-820005)章讨论过），我们可以直接对其进行计数。在更困难的情况下，当状态太多时，需要引入一些对状态的转换，例如哈希函数或某些状态的嵌入（我们稍后会在本章更详细地讨论）。
- en: For pseudo-count methods, Ñ(s) is factorized into the density function and the
    total number of states visited, given by Ñ(s) = ρ(x)n(x), where ρ(x) is a “density
    function,” representing the likelihood of the state x and approximated by a neural
    network. There are several different methods for how to do this, but they might
    be tricky to implement, so we won’t deal with complex cases in this chapter. If
    you’re curious, you can refer to the paper by Georg Ostrovski et al. called Count-based
    exploration with neural density models [[Ost+17](#)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于伪计数方法，Ñ(s)被分解为密度函数和访问的状态总数，给定Ñ(s) = ρ(x)n(x)，其中ρ(x)是“密度函数”，表示状态x的可能性，并通过神经网络进行近似。有几种不同的方法可以做到这一点，但它们可能很难实现，所以我们在本章不会讨论复杂的情况。如果你感兴趣，可以参考Georg
    Ostrovski等人发表的《基于计数的探索与神经密度模型》[[Ost+17](#)]。
- en: A special case of introducing the intrinsic reward is called curiosity-driven
    exploration, when we don’t take the reward from the environment into account at
    all. In that case, the training and exploration is driven 100% by the novelty
    of the agent’s experience. Surprisingly, this approach might be very efficient
    not only in discovering new states in the environment but also in learning quite
    good policies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 引入内在奖励的一个特殊情况叫做好奇心驱动的探索，当我们完全不考虑来自环境的奖励时。在这种情况下，训练和探索完全由智能体经验的新颖性驱动。令人惊讶的是，这种方法可能非常有效，不仅能发现环境中的新状态，还能学习出相当不错的策略。
- en: Prediction-based methods
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于预测的方法
- en: The third family of exploration methods is based on another idea of predicting
    something from the environment data. If the agent can make accurate predictions,
    it means the agent has been in this situation enough and it isn’t worth exploring
    it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类探索方法基于从环境数据中预测某些东西的另一个想法。如果智能体能够做出准确的预测，意味着智能体已经在这种情况下经历了足够多的时间，因此不值得再去探索它。
- en: But if something unusual happens and our prediction is significantly off, it
    might mean that we need to pay attention to the state that we’re currently in.
    There are many different approaches to doing this, but in this chapter, we will
    discuss how to implement this approach, as proposed by Burda et al. in 2018 in
    the paper called Exploration by random network distillation [[Bur+18](#)]. The
    authors were able to reach state-of-the-art results in so-called hard-exploration
    games in Atari.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果发生了一些不寻常的情况，且我们的预测偏差很大，这可能意味着我们需要关注当前所处的状态。做这件事有很多不同的方式，但在本章中，我们将讨论如何实现这一方法，正如Burda等人在2018年提出的《通过随机网络蒸馏进行探索》一文中所提出的那样[[Bur+18](#)]。作者们在所谓的硬探索游戏中（如Atari）达到了最先进的结果。
- en: 'The approach used in the paper is quite simple: we add the intrinsic reward,
    which is calculated from the ability of one neural network (NN) (which is being
    trained) to predict the output from another randomly initialized (untrained) NN.
    The input to both NNs is the current observation, and the intrinsic reward is
    proportional to the mean squared error (MSE) of the prediction.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中使用的方法非常简单：我们添加了内在奖励，该奖励通过一个神经网络（NN）（正在训练中）从另一个随机初始化（未训练）神经网络预测输出的能力来计算。两个神经网络的输入是当前的观察值，内在奖励与预测的均方误差（MSE）成正比。
- en: MountainCar experiments
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MountainCar 实验
- en: In this section, we will try to implement and compare the effectiveness of different
    exploration approaches on a simple, but still challenging, environment, which
    could be classified as a “classical RL” problem that is very similar to the familiar
    CartPole problem. But in contrast to CartPole, the MountainCar problem is quite
    challenging from an exploration point of view.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将尝试在一个简单但仍具有挑战性的环境中实现并比较不同探索方法的效果，这个环境可以归类为一个“经典强化学习”问题，与我们熟悉的 CartPole
    问题非常相似。但与 CartPole 相比，MountainCar 问题在探索角度上要困难得多。
- en: The problem’s illustration is shown in Figure [18.3](#x1-335002r3) and it consists
    of a small car starting from the bottom of the valley. The car can move left and
    right, and the goal is to reach the top of the mountain on the right.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的示意图如图 [18.3](#x1-335002r3) 所示，图中有一辆小车从山谷的底部开始。汽车可以向左或向右移动，目标是到达右侧山顶。
- en: '![PIC](img/file265.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file265.png)'
- en: 'Figure 18.3: The MountainCar environment'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3：MountainCar 环境
- en: The trick here is in the environment’s dynamics and the action space. To reach
    the top, the actions need to be applied in a particular way to swing the car back
    and forth to speed it up. In other words, the agent needs to apply the actions
    for several time steps to make the car go faster and eventually reach the top.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的诀窍在于环境的动态和动作空间。为了到达山顶，动作需要以特定的方式应用，使汽车前后摆动以加速。换句话说，智能体需要在多个时间步骤内应用动作，使汽车加速并最终到达山顶。
- en: Obviously, this coordination of actions is not something that is easy to achieve
    with just random actions, so the problem is hard from the exploration point of
    view and very similar to our River Swim example.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种动作协调并不是通过随机动作轻松实现的，因此从探索的角度来看，这个问题很难，且与我们的 River Swim 示例非常相似。
- en: 'In Gym, this environment has the name MountainCar-v0 and it has a very simple
    observation and action space. The observations are just two numbers: the first
    one gives the horizontal position of the car and the second value is the car’s
    velocity. The action could be 0, 1, or 2, where 0 means pushing the car to the
    left, 1 applies no force, and 2 pushes the car to the right. The following is
    a very simple illustration of this in Python REPL:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gym 中，这个环境的名称是 MountainCar-v0，且它有一个非常简单的观察和动作空间。观察值只有两个数字：第一个数字表示汽车的水平位置，第二个数字表示汽车的速度。动作可以是
    0、1 或 2，其中 0 表示将汽车推向左侧，1 表示不施加任何力量，2 表示将汽车推向右侧。以下是一个在 Python REPL 中非常简单的示意：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, in every step, we get the reward of -1, so the agent needs to
    learn how to get to the goal as soon as possible to get as little total negative
    reward as possible. By default, the number of steps is limited to 200, so if we
    haven’t reached the goal (which happens most of the time), our total reward is
    −200.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在每一步中，我们获得的奖励是-1，因此智能体需要学习如何尽快到达目标，以便获得尽可能少的总负奖励。默认情况下，步数限制为200，所以如果我们没有达到目标（这通常是发生的情况），我们的总奖励就是−200。
- en: DQN + 𝜖-greedy
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN + 𝜖-greedy
- en: The first method that we will use is our traditional 𝜖-greedy approach to exploration.
    It is implemented in the source file Chapter18/mcar_dqn.py. I won’t include the
    source code here, as it is already familiar to you. This program implements various
    exploration strategies on top of the DQN method, allowing us to select between
    them using the -p command-line option. To launch the normal 𝜖-greedy method, the
    option -p egreedy needs to be passed. During the training, we are decreasing 𝜖
    from 1.0 to 0.02 for the first 10⁵ training steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的第一个方法是我们传统的 𝜖-greedy 探索方法。它在源文件 Chapter18/mcar_dqn.py 中实现。我不会在这里包含源代码，因为你已经很熟悉它了。这个程序在
    DQN 方法的基础上实现了各种探索策略，允许我们通过 -p 命令行选项在它们之间进行选择。要启动正常的 𝜖-greedy 方法，需要传递 -p egreedy
    选项。在训练过程中，我们将 𝜖 从 1.0 降低到 0.02，持续进行 10⁵ 步训练。
- en: The training is quite fast; it takes just two to three minutes to do 10⁵ training
    steps. But from the charts shown in Figure [18.4](#x1-336002r4) and Figure [18.5](#x1-336003r5),
    it is obvious that during those 10⁵ steps, which was 500 episodes, we didn’t reach
    the goal state even once. That’s really bad news, as our 𝜖 has decayed, so we
    will do no more exploration in the future.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练速度相当快；进行 10⁵ 步训练只需两到三分钟。但从图 [18.4](#x1-336002r4) 和图 [18.5](#x1-336003r5) 中展示的图表可以明显看出，在这
    10⁵ 步（即 500 回合）中，我们一次都没有达到目标状态。这是个坏消息，因为我们的 𝜖 已经衰减，意味着我们在未来不会进行更多的探索。
- en: '![PIC](img/B22150_18_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_18_04.png)'
- en: 'Figure 18.4: The reward (left) and steps (right) during the DQN training with
    the 𝜖-greedy strategy'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.4：在 DQN 训练过程中使用 𝜖-greedy 策略时的奖励（左）和步数（右）
- en: '![PIC](img/B22150_18_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_18_05.png)'
- en: 'Figure 18.5: Epsilon (left) and loss (right) during the training'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5：训练过程中 𝜖（左）和损失（右）的变化
- en: 'The 2% of random actions that we still perform are just not enough because
    it requires dozens of coordinated steps to reach the top of the mountain (the
    best policy on MountainCar has a total reward of around −80). We can now continue
    our training for millions of steps, but the only data we will get from the environment
    will be episodes, which will take 200 steps with −200 total reward. This illustrates
    once more how important exploration is. Regardless of the training method we have,
    without proper exploration, we might just fail to train. So, what should we do?
    If we want to stay with 𝜖-greedy, the only option for us is to explore for longer
    (by changing the speed of 𝜖 decrease). You can experiment with the hyperparameters
    of the -p egreedy mode, but I went to the extreme and implemented the -p egreedy-long
    hyperparameter set. In this regime, we keep 𝜖 = 1.0 until we reach at least one
    episode with a total reward better than −200\. Once this has happened, we start
    training the normal way, decreasing 𝜖 from 1.0 to 0.02 for subsequent 10⁶ frames.
    As we don’t do training, during the initial exploration phase, it normally runs
    5 to 10 times faster. To start the training in this mode, we use the following
    command line: ./mcar_dqn.py -n t1 -p egreedy-long.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然执行的 2% 随机动作远远不够，因为达到山顶需要数十步协调的动作（MountainCar 上的最佳策略总奖励大约为 -80）。现在我们可以继续训练几百万步，但我们从环境中获得的数据将只是回合，每回合需要
    200 步，且总奖励为 -200。这再次说明了探索的重要性。无论我们使用什么训练方法，如果没有适当的探索，我们可能根本无法训练成功。那么，我们应该怎么做呢？如果我们想继续使用
    𝜖-greedy，唯一的选择就是进行更长时间的探索（通过调整 𝜖 衰减的速度）。你可以尝试调整 -p egreedy 模式的超参数，但我走到了极端，实施了
    -p egreedy-long 超参数集。在这个方案中，我们将 𝜖 保持为 1.0，直到至少有一个回合的总奖励超过 -200。完成这个目标后，我们开始正常训练，将
    𝜖 从 1.0 降低到 0.02，持续训练 10⁶ 帧。在初始探索阶段，由于没有进行训练，这个过程通常会比正常训练快 5 到 10 倍。要在这种模式下开始训练，我们使用以下命令行：./mcar_dqn.py
    -n t1 -p egreedy-long。
- en: Unfortunately, even with this improvement of 𝜖-greedy, it still failed to solve
    the environment due to its complexity. I left this version to run for five hours,
    but after 500k episodes, it still hadn’t faced even a single example of the goal,
    so I gave up. Of course, you could try it for a longer period.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使改进了 𝜖-greedy 策略，仍然由于环境的复杂性未能解决问题。我让这个版本运行了五个小时，但在 500k 个回合后，它仍然没有遇到过一次目标，所以我放弃了。当然，你可以尝试更长时间。
- en: DQN + noisy networks
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN + 噪声网络
- en: 'To apply the noisy networks approach to our MountainCar problem, we just need
    to replace one of two layers in our network with the NoisyLinear class, so our
    architecture will become as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将噪声网络方法应用于我们的 MountainCar 问题，我们只需将网络中的两层之一替换为 NoisyLinear 类，最终架构如下：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The only difference between the NoisyLinear class and the version from Chapter [8](ch012.xhtml#x1-1240008)
    is that this version has an explicit method, sample_noise(), to update the noise
    tensors, so we need to call this method on every training iteration; otherwise,
    the noise will be constant during the training. This modification is needed for
    future experiments with policy-based methods, which require the noise to be constant
    during the relatively long period of trajectories. In any case, the modification
    is simple, and we just need to call this method from time to time. In the case
    of the DQN method, it is called on every training iteration. As in Chapter [8](ch012.xhtml#x1-1240008),
    the implementation of NoisyLinear is taken from the TorchRL library. The code
    is the same as before, so to activate the noisy networks, you need to run the
    training with the -p noisynet command line.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`NoisyLinear` 类与第[8章](ch012.xhtml#x1-1240008)版本的唯一区别在于，此版本有一个显式的方法 `sample_noise()`
    来更新噪声张量，因此我们需要在每次训练迭代时调用此方法；否则，噪声将在训练过程中保持不变。这个修改是为了未来与基于策略的方法进行实验所需的，这些方法要求噪声在相对较长的轨迹期间保持恒定。无论如何，这个修改很简单，我们只需不时调用这个方法。在
    DQN 方法中，它会在每次训练迭代时被调用。和第[8章](ch012.xhtml#x1-1240008)一样，`NoisyLinear` 的实现来自于 TorchRL
    库。代码与之前相同，所以要激活噪声网络，你需要使用 -p noisynet 命令行来运行训练。'
- en: 'In Figure [18.6](#x1-337010r6), you can see the plots for the three hours of
    training:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[18.6](#x1-337010r6)中，你可以看到三小时训练的图表：
- en: '![PIC](img/B22150_18_06.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_18_06.png)'
- en: 'Figure 18.6: The training reward (left) and test steps (right) on DQN with
    noisy networks exploration'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.6：DQN 带噪声网络探索的训练奖励（左）和测试步骤（右）
- en: As you can see, the training process wasn’t able to reach the mean test reward
    of −130 (as required in the code), but after just 7k training steps (20 minutes
    of training), we discovered the goal state, which is great progress in comparison
    to 𝜖-greedy, which didn’t find a single instance of the goal state after 5 hours
    of trial and error.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，训练过程未能达到代码中要求的平均测试奖励 -130，但在仅仅 7k 训练步（20 分钟训练）后，我们发现了目标状态，相比于 𝜖-greedy
    方法在经过 5 小时的试错后依然没有找到任何目标状态，这已是很大的进步。
- en: From the test steps chart (on the right of Figure [18.6](#x1-337010r6)) we can
    see that there are some tests with less than 100 steps, which is very close to
    the optimal policy. But they were not often enough to push mean test reward below
    the −130 level.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从测试步骤图（图[18.6](#x1-337010r6)右侧）中我们可以看到，有一些测试的步骤数不到 100 步，这非常接近最优策略。但这些测试次数不足以将平均测试奖励推低到
    -130 以下。
- en: DQN + state counts
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN + 状态计数
- en: The last exploration technique that we will apply to the DQN method is count-based.
    As our state space is just two floating-point values, we will discretize the observation
    by rounding values to three digits after the decimal point, which should provide
    enough precision to distinguish different states from each other but still group
    similar states together. For every individual state, we will keep the count of
    times we have seen this state before and use that to give an extra reward to the
    agent. For an off-policy method, it might not be the best idea to modify rewards
    during the training, but we will examine the effect.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用于 DQN 方法的最后一种探索技术是基于计数的。由于我们的状态空间只有两个浮点值，我们将通过将数值四舍五入到小数点后三位来离散化观察，这应该能够提供足够的精度来区分不同的状态，但仍能将相似的状态聚集在一起。对于每个单独的状态，我们将记录该状态出现的次数，并利用这个计数为智能体提供额外的奖励。对于一个离策略方法来说，在训练过程中修改奖励可能不是最好的做法，但我们将会考察其效果。
- en: As before, I’m not going to provide the full source code; I will just emphasize
    the differences from the base version. First, we apply the wrapper to the environment
    to keep track of the counters and calculate the intrinsic reward value. You will
    find the code for the wrapper is in the lib/common.py module and it is shown here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如同之前一样，我不会提供完整的源代码；我只会强调与基础版本的不同之处。首先，我们为环境应用包装器，以跟踪计数器并计算内在奖励值。你可以在 lib/common.py
    模块中找到包装器的代码，下面是它的展示。
- en: 'Let us look at the constructor first:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看一下构造函数：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the constructor, we take the environment we want to wrap, the optional hash
    function to be applied to the observations, and the scale of the intrinsic reward.
    We also create the container for our counters, which will map the hashed state
    into the number of times we have seen it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们传入要包装的环境、可选的哈希函数（用于观察结果）以及固有奖励的规模。我们还创建了一个计数器容器，它将哈希后的状态映射为我们看到该状态的次数。
- en: 'Then, we define the helper function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义辅助函数：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function will calculate the intrinsic reward value of the state. It applies
    the hash to the observation, updates the counter, and calculates the reward using
    the formula we have already seen.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将计算状态的固有奖励值。它对观察结果应用哈希，更新计数器，并使用我们已经看到的公式计算奖励。
- en: 'The last method of the wrapper is responsible for the environment step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 包装器的最后一个方法负责环境的步骤：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here we call the helper function to get the reward and return the sum of the
    extrinsic and intrinsic reward components.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们调用辅助函数来获取奖励，并返回外部奖励和固有奖励组件的总和。
- en: 'To apply the wrapper, we need to pass to it the hashing function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这个包装器，我们需要将哈希函数传递给它：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Three digits are probably too many, so you can experiment with a different way
    of hashing states.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 三位数字可能太多了，所以你可以尝试使用另一种方式来哈希状态。
- en: To start the training, pass -p counts to the training program. In Figure [18.7](#x1-338021r7),
    you can see the charts with training and testing rewards. As the training environment
    is wrapped in our PseudoCountReward wrapper, values during training are higher
    than testing.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，请将 -p counts 参数传递给训练程序。在图 [18.7](#x1-338021r7) 中，你可以看到带有训练和测试奖励的图表。由于训练环境被我们包装在伪计数奖励包装器中，因此训练期间的值高于测试期间的值。
- en: '![PIC](img/B22150_18_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_18_07.png)'
- en: 'Figure 18.7: The training reward (left) and test rewards (right) on DQN with
    pseudo-count reward bonus'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.7：DQN 训练奖励（左）和测试奖励（右），带伪计数奖励加成
- en: As you can see, we weren’t able to get -130 average test reward using this method,
    but were very close to it. It took it just 10 minutes to discover the goal state,
    which is also quite impressive.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们未能通过这种方法获得 -130 的平均测试奖励，但我们非常接近。它只用了 10 分钟就发现了目标状态，这也是相当令人印象深刻的。
- en: PPO method
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO 方法
- en: 'Another set of experiments that we will conduct with our MountainCar problem
    is related to the on-policy method Proximal Policy Optimization (PPO), which we
    covered in Chapter [16](ch020.xhtml#x1-29000016). There are several motivations
    for this choice:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 MountainCar 问题上进行的另一组实验与在线策略方法 Proximal Policy Optimization（PPO）相关，我们在第
    [16](ch020.xhtml#x1-29000016) 章中讨论过。选择这个方法的动机有几个：
- en: First, as you saw in the DQN method + noisy networks case, when good examples
    are rare, DQNs have trouble adapting to them quickly. This might be solved by
    increasing the replay buffer size and switching to the prioritized buffer, or
    we could try on-policy methods, which adjust the policy immediately according
    to the obtained experience.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，正如你在 DQN 方法 + 噪声网络的案例中看到的那样，当好的示例很少时，DQN 在快速适应这些示例方面会遇到困难。这可以通过增加重放缓冲区的大小并切换到优先缓冲区来解决，或者我们可以尝试使用在线策略方法，这些方法根据获得的经验立即调整策略。
- en: Another reason for choosing this method is the modification of the reward during
    the training. Count-based exploration and policy distillation introduce the intrinsic
    reward component, which might change over time. The value-based methods might
    be sensitive to the modification of the underlying reward as, basically, they
    will need to relearn values during the training. On-policy methods shouldn’t have
    any problems with that, as an increase of the reward just puts more emphasis on
    a sample with higher reward in terms of the policy gradient.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择这个方法的另一个原因是训练过程中奖励的修改。基于计数的探索和策略蒸馏引入了固有奖励组件，这个组件可能会随着时间的推移而变化。基于值的方法可能对基础奖励的修改比较敏感，因为它们基本上需要在训练过程中重新学习值。而在线策略方法不应该有任何问题，因为奖励的增加只是使具有较高奖励的样本在策略梯度中更加重要。
- en: Finally, it’s just interesting to check our exploration strategies on both families
    of RL methods.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，检查我们的探索策略在两种RL方法家族中的表现是很有趣的。
- en: To implement this approach, in the file Chapter18/mcar_ppo.py, we have a PPO
    implementation combined with various exploration strategies applied to MountainCar.
    The code is not very different from the PPO from Chapter [16](ch020.xhtml#x1-29000016),
    so I’m not going to repeat it here. To start the normal PPO without extra exploration
    tweaks, you should run the command ./mcar_ppo.py -n t1 -p ppo. In this version,
    nothing specifically is done to perform exploration – we purely rely on random
    weights initialization in the beginning of the training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这种方法，在文件Chapter18/mcar_ppo.py中，我们有一个结合了各种探索策略的PPO实现，应用于MountainCar。代码与第[16](ch020.xhtml#x1-29000016)章中的PPO实现差别不大，所以我不会在这里重复。要启动没有额外探索调整的普通PPO，你应该运行命令`./mcar_ppo.py
    -n t1 -p ppo`。在这个版本中，没有专门做探索的操作——我们完全依赖于训练开始时的随机权重初始化。
- en: 'As a reminder, PPO is in the policy gradient methods family, which limits Kullback-Leibler
    divergence between the old and new policy during the training, avoiding dramatic
    policy updates. Our network has two heads: the actor and the critic. The actor
    network returns the probability distribution over our actions (our policy) and
    the critic estimates the value of the state. The critic is trained using MSE loss,
    while the actor is driven by the PPO surrogate objective we discussed in Chapter [16](ch020.xhtml#x1-29000016).
    In addition to those two losses, we regularize the policy by applying entropy
    loss scaled by the hyperparameter β. There is nothing new here so far. The following
    is the PPO network structure:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，PPO属于策略梯度方法家族，在训练过程中限制旧策略与新策略之间的Kullback-Leibler散度，避免了剧烈的策略更新。我们的网络有两个部分：演员和评论家。演员网络返回我们行为的概率分布（我们的策略），评论家估计状态的价值。评论家使用均方误差（MSE）损失进行训练，而演员则由我们在第[16](ch020.xhtml#x1-29000016)章讨论的PPO代理目标驱动。除了这两种损失，我们通过应用由超参数β缩放的熵损失来对策略进行正则化。到目前为止没有什么新内容。以下是PPO网络结构：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'I stopped the training after three hours, as it showed no improvements. The
    goal state was found after an hour and 30k episodes. The charts in Figure [18.8](#x1-339015r8)
    show the reward dynamics during the training:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我在训练了三个小时后停止了训练，因为没有看到任何改进。目标状态在一个小时和30k轮次后找到了。图[18.8](#x1-339015r8)中的图表展示了训练过程中的奖励动态：
- en: '![PIC](img/B22150_18_08.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_18_08.png)'
- en: 'Figure 18.8: The training reward (left) and test rewards (right) on plain PPO'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.8：在普通PPO上的训练奖励（左）和测试奖励（右）
- en: As the PPO result wasn’t very impressive, let’s try to extend it with extra
    exploration tricks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PPO的结果并不十分令人印象深刻，让我们尝试通过额外的探索技巧来扩展它。
- en: PPO + Noisy Networks
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO + 噪声网络
- en: As with the DQN method, we can apply the noisy networks exploration approach
    to our PPO method. To do that, we need to replace the output layer of the actor
    with the NoisyLinear layer. Only the actor network needs to be affected because
    we would like to inject the noisiness only into the policy and not into the value
    estimation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与DQN方法类似，我们可以将噪声网络探索方法应用到PPO方法中。为此，我们需要用NoisyLinear层替换演员网络的输出层。只有演员网络需要受到影响，因为我们只希望将噪声注入到策略中，而不是价值估计中。
- en: 'There is one subtle nuance related to the application of noisy networks: where
    the random noise needs to be sampled. In Chapter [8](ch012.xhtml#x1-1240008),
    when you first met noisy networks, the noise was sampled on every forward() pass
    of the NoisyLinear layer. According to the original research paper, this is fine
    for off-policy methods, but for on-policy methods, it needs to be done differently.
    Indeed, when we train on-policy, we obtain the training samples produced by our
    current policy and calculate the policy gradient, which should push the policy
    toward improvement. The goal of noisy networks is to inject randomness, but as
    we have discussed, we prefer directed exploration over just a random change of
    policy after every step. With that in mind, our random component in the NoisyLinear
    layer needs to be updated not after every forward() pass, but much less frequently.
    In my code, I resampled the noise on every PPO batch, which was 2,048 transitions.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个微妙的细节与噪声网络的应用有关：即随机噪声需要在哪个地方进行采样。在第[8章](ch012.xhtml#x1-1240008)中，当你首次接触噪声网络时，噪声是在每次
    `forward()` 通过 NoisyLinear 层时进行采样的。根据原始研究论文，对于离策略方法，这是可以的，但对于在策略方法，它需要以不同的方式进行。实际上，当我们进行在策略训练时，我们获得的是当前策略产生的训练样本，并计算策略梯度，这应该推动策略朝着改进的方向前进。噪声网络的目标是注入随机性，但正如我们所讨论的，我们更倾向于有针对性的探索，而不是在每一步之后就随机地改变策略。考虑到这一点，NoisyLinear
    层中的随机成分不需要在每次 `forward()` 传递之后更新，而应该更少的频率进行更新。在我的代码中，我在每个 PPO 批次（即 2,048 次转换）时重新采样噪声。
- en: 'As before, I trained PPO+NoisyNets for 3 hours. But in this case, the goal
    state was found after 30 minutes and 18k episodes, which is a better result. In
    addition, according to the train steps count, the training process was able to
    drive the car in the optimal way a couple of times (with a step count less than
    100). But these successes did not lead to the optimal policy at the end. The charts
    in Figure [18.9](#x1-340003r9) show the reward dynamics during the training:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我训练了 PPO+NoisyNets 3 小时。但在这种情况下，目标状态在 30 分钟和 18k 回合后就被找到，这是一个更好的结果。此外，根据训练步数统计，训练过程成功地让小车以最优方式行驶了几次（步数小于
    100）。但是，这些成功并没有导致最终的最优策略。图 [18.9](#x1-340003r9) 中的图表展示了训练过程中的奖励动态：
- en: '![PIC](img/B22150_18_09.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_18_09.png)'
- en: 'Figure 18.9: The training reward (left) and test rewards (right) on PPO with
    Noisy Networks'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.9：PPO 使用噪声网络的训练奖励（左）和测试奖励（右）
- en: PPO + state counts
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO + 状态计数
- en: In this case, exactly the same count-based approach with three-digit hashing
    is implemented for the PPO method and can be triggered by passing -p counts to
    the training process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用三位数哈希的基于计数的方式在 PPO 方法中得到了完全相同的实现，并可以通过在训练过程中传递 -p counts 来触发。
- en: 'In my experiments, the method was able to solve the environment (get an average
    reward higher than -130) in 1.5 hours, and it required 61k episodes. The following
    is the final part of the console output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的实验中，该方法能够在 1.5 小时内解决环境问题（获得平均奖励高于 -130），并且需要 61k 回合。以下是控制台输出的最后部分：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you can see from the plots in Figure [18.10](#x1-341011r10), the training
    discovered the goal state after 23k episodes. It took another 40k episodes to
    polish the policy to the optimal count of steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [18.10](#x1-341011r10) 所示，从图表中可以看到，训练在 23k 回合后发现了目标状态。之后又花了 40k 回合来优化策略，达到了最优步数：
- en: '![PIC](img/B22150_18_10.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_18_10.png)'
- en: 'Figure 18.10: The training reward (left) and test rewards (right) on PPO with
    pseudo-count reward bonus'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.10：PPO 使用伪计数奖励加成的训练奖励（左）和测试奖励（右）
- en: PPO + network distillation
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO + 网络蒸馏
- en: As the final exploration method in our MountainCar experiment, I implemented
    the network distillation method proposed by Burda et al. [[Bur+18](#)]. In this
    method, two extra NNs are introduced. Both need to map the observation into one
    number, in the same way that our value head does. The difference is in the way
    they are used. The first NN is randomly initialized and kept untrained. This will
    be our reference NN. The second one is trained to minimize the MSE loss between
    the second and the first NN. In addition, the absolute difference between the
    outputs of the NNs is used as the intrinsic reward component.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们 MountainCar 实验中的最终探索方法，我实现了 Burda 等人提出的网络蒸馏方法[[Bur+18](#)]。在这种方法中，引入了两个额外的神经网络（NN）。这两个网络都需要将观察值映射为一个数字，方式与我们的价值头部相同。不同之处在于它们的使用方式。第一个神经网络是随机初始化并保持未训练的，这将成为我们的参考神经网络。第二个神经网络经过训练，以最小化第二个和第一个神经网络之间的均方误差（MSE）损失。此外，神经网络输出之间的绝对差异作为内在奖励组件。
- en: The idea behind this is that the better the agent has explored some state, the
    better our second (trained) NN will predict the output of the first (untrained)
    one. This will lead to a smaller intrinsic reward being added to the total reward,
    which will decrease the policy gradient assigned to the sample.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的想法是，代理程序探索某些状态得越好，我们第二个（训练过的）神经网络就越能预测第一个（未经训练的）神经网络的输出。这将导致将较小的内在奖励添加到总奖励中，从而减少样本分配的策略梯度。
- en: In the paper, the authors suggested training separate value heads to predict
    separate intrinsic and extrinsic reward components, but for this example, I decided
    to keep it simple and just added both rewards in the wrapper, the same way that
    we did in the counter-based exploration method. This minimizes the number of modifications
    in the code.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者建议训练单独的价值头来预测内在和外在奖励成分，但在这个例子中，我决定保持简单，只是在包装器中添加了这两个奖励，就像我们在基于计数的探索方法中所做的那样。这样可以最小化代码的修改数量。
- en: 'In terms of those extra NN architectures, I did a small experiment and tried
    several architectures for both NNs. The best results were obtained with the reference
    NN having three layers and the trained NN having just one layer. This helps to
    prevent the overfitting of the trained NN, as our observation space is not very
    large. Both NNs are implemented in the MountainCarNetDistillery class in the lib/ppo.py
    module:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关于那些额外的神经网络架构，我做了一个小实验，并尝试了两个神经网络的几种架构。最佳结果是参考神经网络具有三层，训练神经网络只有一层。这有助于防止训练神经网络的过拟合，因为我们的观察空间并不是很大。两个神经网络都实现在lib/ppo.py模块的MountainCarNetDistillery类中：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Besides the forward() method, which returns the output from both NNs, the class
    includes two helper methods for intrinsic reward calculation and for getting the
    loss between two NNs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了返回两个神经网络输出的forward()方法外，该类还包括两个帮助方法，用于计算内在奖励和获取两个神经网络之间的损失。
- en: 'To start the training, the argument -p distill needs to be passed to the mcar_ppo.py
    program. In my experiment, 33k episodes were required to solve the problem, which
    is almost two times less than Noisy Networks. As discussed in earlier chapters,
    there might be some bugs and inefficiencies in my implementation, so you’re welcome
    to modify it to make it faster and more efficient:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，需要将参数 -p distill 传递给mcar_ppo.py程序。在我的实验中，解决问题需要33k个周期，比噪声网络少了近两倍。正如早些时候讨论的那样，我的实现中可能存在一些错误和低效性，因此欢迎您修改以使其更快更高效：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The plots with the training and testing rewards are shown in Figure [18.11](#x1-342040r11).
    In Figure [18.12](#x1-342041r12), the total loss and distillation loss are shown.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 显示有关训练和测试奖励的图表如图18.11所示。在图18.12中，显示了总损失和蒸馏损失。
- en: '![PIC](img/B22150_18_11.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_18_11.png)'
- en: 'Figure 18.11: The training reward (left) and test rewards (right) on PPO with
    network distillation'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 18.11: PPO与网络蒸馏的训练奖励（左）和测试奖励（右）'
- en: '![PIC](img/B22150_18_12.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_18_12.png)'
- en: 'Figure 18.12: The total loss (left) and distillation loss (right)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 18.12: 总损失（左）和蒸馏损失（右）'
- en: As before, due to the intrinsic reward component, the training episodes have
    a higher reward on the plots. From the distillation loss plot, it is clear that
    before the agent discovered the goal state, everything was boring and predictable,
    but once it had figured out how to end the episode earlier than 200 steps, the
    loss grew significantly.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，由于内在奖励成分的存在，训练周期在图中有更高的奖励。从蒸馏损失图中可以明显看出，在代理程序发现目标状态之前，一切都是无聊和可预测的，但一旦它找出如何在200步之前结束这一情况，损失就显著增加。
- en: Comparison of methods
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法比较
- en: 'To simplify the comparison of the experiments we’ve done on MountainCar, I
    put all the numbers into the following table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们在MountainCar上进行的实验比较，我将所有数字放入以下表格中：
- en: '| Method | Goal state found | Solved |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Method | 找到目标状态 | 解决 |'
- en: '|  | Episodes | Time | Episodes | Time |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | Episodes | Time | Episodes | Time |'
- en: '| DQN + 𝜖-greedy | x | x | x | x |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| DQN + 𝜖-greedy | x | x | x | x |'
- en: '| DQN + noisy nets | 8k | 15 min | x | x |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| DQN + noisy nets | 8k | 15 min | x | x |'
- en: '| PPO | 40k | 60 min | x | x |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| PPO | 40k | 60 min | x | x |'
- en: '| PPO + noisy nets | 20k | 30 min | x | x |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| PPO + noisy nets | 20k | 30 min | x | x |'
- en: '| PPO + counts | 25k | 36 min | 61k | 90 min |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| PPO + counts | 25k | 36 min | 61k | 90 min |'
- en: '| PPO + distillation | 16k | 36 min | 33k | 84 min |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| PPO + distillation | 16k | 36 min | 33k | 84 min |'
- en: 'Table 18.1: Summary of experiments'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 18.1: 实验总结'
- en: As you can see, both DQN and PPO with exploration extensions are able to solve
    the MountainCar environment. Concrete method selection is up to you and your concrete
    situation, but it is important to be aware of the different approaches to the
    exploration you might use.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，带有探索扩展的DQN和PPO都能够解决MountainCar环境。具体方法的选择取决于你和你具体的情况，但重要的是要意识到你可能会使用不同的探索方法。
- en: Atari experiments
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari实验
- en: The MountainCar environment is a nice and fast way to experiment with exploration
    methods, but to conclude the chapter, I’ve included Atari versions of the DQN
    and PPO methods with the exploration tweaks we described to check a more complicated
    environment.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: MountainCar环境是一个非常好的快速实验探索方法，但为了总结这一章，我包含了带有我们描述过的探索调整的DQN和PPO方法的Atari版本，以便检查一个更复杂的环境。
- en: As the primary environment, I’ve used Seaquest, which is a game where the submarine
    needs to shoot fish and enemy submarines, and save aquanauts. This game is not
    as famous as Montezuma’s Revenge, but it still might be considered as medium-hard
    exploration because, to continue the game, you need to control the level of oxygen.
    When it becomes low, the submarine needs to rise to the surface for some time.
    Without this, the episode will end after 560 steps and with a maximum reward of
    20\. But once the agent learns how to replenish the oxygen, the game might continue
    almost infinitely and bring to the agent a 10k-100k score. Surprisingly, traditional
    exploration methods struggle with discovering this; normally, training gets stuck
    at 560 steps, after which the oxygen runs out and the submarine dies.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 作为主要环境，我使用了Seaquest，这是一个潜艇需要击败鱼类和敌人潜艇，并拯救水下宇航员的游戏。这个游戏没有像《蒙特祖玛的复仇》那么有名，但它仍然可以算作是中等难度的探索，因为要继续游戏，你需要控制氧气的水平。当氧气变低时，潜艇需要升到水面一段时间。如果没有这个操作，游戏将在560步后结束，且最大奖励为20。然而，一旦智能体学会如何补充氧气，游戏几乎可以无限继续，并为智能体带来10k-100k的分数。令人惊讶的是，传统的探索方法在发现这一点时有困难；通常，训练会在560步时卡住，之后氧气耗尽，潜艇就会死掉。
- en: The negative aspect of Atari is that every experiment requires at least half
    a day of training to check the effect, so my code and hyperparameters are very
    far from being the best, but they might be useful as a starting point for your
    own experiments. Of course, if you discover a way to improve the code, please
    share your findings on GitHub.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Atari的一个负面方面是每次实验至少需要半天的训练才能检查效果，因此我的代码和超参数距离最佳状态还有很大差距，但它们可能作为你自己实验的起点是有用的。当然，如果你发现了改进代码的方法，请在GitHub上分享你的发现。
- en: 'As before, there are two program files: atari_dqn.py, which implements the
    DQN method with 𝜖-greedy and noisy networks exploration, and atari_ppo.py, which
    is the PPO method with optional noisy networks and the network distillation method.
    To switch between hyperparameters, the command-line option -p needs to be used.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，有两个程序文件：atari_dqn.py，实现了带有𝜖-贪婪和噪声网络探索的DQN方法；atari_ppo.py，实现了PPO方法，带有可选的噪声网络和网络蒸馏方法。要在超参数之间切换，需要使用命令行选项-p。
- en: In the following sections, let us look at the results that I got from a few
    runs of the code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，让我们看看我通过几次代码运行得到的结果。
- en: DQN + 𝜖-greedy
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN + 𝜖-贪婪
- en: 'In comparison to other methods tried on Atari, 𝜖-greedy was the best, which
    might be surprising, as it gave s the worst results in the MountainCar experiment
    earlier in this chapter. But this happens quite often in reality and can lead
    to new directions of research and even breakthroughs. After 13 hours of training,
    it was able to reach an average reward of 18 with a maximum reward of 25\. According
    to the chart showing the number of steps, just a few episodes were able to discover
    how to get the oxygen so, maybe with more training, this method can break the
    560-step boundary. In Figure [18.13](#x1-345002r13), the plots with the average
    reward and number of steps are shown:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他在Atari上尝试过的方法相比，𝜖-贪婪表现最好，这可能会让人感到惊讶，因为它在本章前面的MountainCar实验中给出了最差的结果。但这在现实中是很常见的，并且可能会带来新的研究方向，甚至突破。经过13小时的训练，它能够达到18的平均奖励，最大奖励为25。根据显示步骤数的图表，只有少数几个回合能够发现如何获取氧气，因此，或许经过更多的训练，这种方法可以突破560步的限制。在图[18.13](#x1-345002r13)中，显示了平均奖励和步骤数的图表：
- en: '![PIC](img/B22150_18_13.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_18_13.png)'
- en: 'Figure 18.13: The average training reward (left) and count of steps (right)
    on DQN with 𝜖-greedy'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.13：DQN与𝜖-贪婪的平均训练奖励（左）和步骤数（右）
- en: DQN + noisy networks
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN + 噪声网络
- en: 'Noisy networks combined with DQN showed worse results — after 6 hours of training,
    it was able to reach a reward of 6\. In Figure [18.14](#x1-346002r14), the plots
    are shown:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 带有噪声网络的DQN表现更差——经过6小时的训练，它的奖励值只能达到6。在图[18.14](#x1-346002r14)中，显示了相关的图表：
- en: '![PIC](img/B22150_18_14.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_18_14.png)'
- en: 'Figure 18.14: The average training reward (left) and count of steps (right)
    on DQN with noisy networks'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.14：在带有噪声网络的DQN上的平均训练奖励（左）和步数（右）
- en: PPO
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO
- en: PPO experiments were much worse — all the combinations (vanilla PPO, noisy networks,
    and network distillation) showed no reward progress and were able to reach an
    average reward of 4\. This is a bit surprising, as experiments with the same code
    in the previous edition of the book were able to get better results. This might
    be an indication of some subtle bugs in the code or in the training environment
    I used. Feel free to experiment with these methods by yourself!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: PPO实验的表现更差——所有的组合（原生PPO、噪声网络和网络蒸馏）都没有奖励进展，平均奖励只能达到4。这有点令人惊讶，因为在本书的上一版中，使用相同代码的实验能获得更好的结果。这可能表明代码或我使用的训练环境中存在一些细微的bug。你可以自己尝试这些方法！
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we discussed why 𝜖-greedy exploration is not the best in some
    cases and checked alternative modern approaches for exploration. The topic of
    exploration is much wider and lots of interesting methods are left uncovered,
    but I hope you were able to get an overall impression of the new methods and the
    way they should be implemented and used in your own problems.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了为什么𝜖-贪心探索在某些情况下不是最佳方法，并检查了现代的替代探索方法。探索的主题要广泛得多，还有很多有趣的方法未被涉及，但我希望你能够对这些新方法以及它们如何在自己的问题中实施和使用有一个整体的印象。
- en: 'In the next chapter, we’ll take a look at another approach to the exploration
    in complex enviroments: RL with human feedback (RLHF).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一种在复杂环境中探索的方法：带有人工反馈的强化学习（RLHF）。
