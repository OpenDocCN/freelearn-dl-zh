- en: Deep Learning for Robotics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器人学中的深度学习
- en: 'So far, we''ve learned how to build an intelligent chatbot, which can play
    board games just as a human does, and glean insights from stock market data. In
    this chapter, we''re going to move on to what many people in the general public
    imagine **Artificial Intelligence** (**AI**) to be: self-learning robots. In [Chapter
    8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning*, you
    learned all about reinforcement learning and how to use those methods for basic
    tasks. In this chapter, we''ll learn how to apply those methods to robotic motion.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何构建一个智能聊天机器人，它可以像人类一样玩棋盘游戏，并从股市数据中提取洞察。在本章中，我们将进入许多人眼中认为 **人工智能**
    (**AI**) 所代表的领域：自我学习的机器人。在 [第 8 章](91114074-444f-4201-98ef-e510210380f2.xhtml)，*强化学习*
    章节中，您学习了强化学习的所有内容，以及如何利用这些方法处理基本任务。在本章中，我们将学习如何将这些方法应用于机器人的运动控制。
- en: In this chapter, we'll be using GPUs to help train these powerful algorithms.
    If you don't have a GPU- enabled computer, it's recommended that you use either
    AWS or Google Cloud to give you some more computing power.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 GPU 来帮助训练这些强大的算法。如果您的计算机没有 GPU 支持，建议您使用 AWS 或 Google Cloud 来提供更多的计算能力。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Setting up your environment
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置您的环境
- en: Setting u a deep deterministic policy gradients model
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置深度确定性策略梯度模型
- en: The actor-critic network
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员-评论家网络
- en: DDPG and its implementation
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG 及其实现
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll be working with TensorFlow and the OpenAI gym environment,
    so you''ll need the following programs installed on your computer:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 TensorFlow 和 OpenAI gym 环境，因此您需要在计算机上安装以下程序：
- en: TensorFlow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: OpenAI gym
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI gym
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Traditional robotics, known as **Robotics Process Automation**, is the process
    of automating physical tasks that would normally be done by a human. Much like
    the term **machine learning** covers a variety of methods and approaches, including
    deep learning approaches; robotics covers a wide variety of techniques and methods.
    In general, we can break these approaches down into two categories: **traditional
    approaches** and **AI approaches**.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统机器人技术，被称为 **机器人过程自动化**，是自动化物理任务的过程，这些任务通常由人类完成。类似于 **机器学习** 这一术语涵盖了多种方法和方法论，包括深度学习方法；机器人学涵盖了各种各样的技术和方法。一般来说，我们可以将这些方法分为两类：**传统方法**
    和 **人工智能方法**。
- en: 'Traditional robotic control programming takes a few steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器人控制编程通常需要以下几个步骤：
- en: '**Measurement**: The robot receives data from its sensors regarding actions
    to take for a given task.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测量**：机器人从传感器接收数据，以确定为完成特定任务应该采取的行动。'
- en: '**Inference**: The orientation of the robot is relative to its environment
    from the data received in the sensors.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理**：机器人相对于其环境的方位是基于传感器接收到的数据。'
- en: '**Modeling**: Models what the robot must do at each state of action to complete
    an action.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建模**：在每个动作状态下建模机器人必须做什么以完成一个动作。'
- en: '**Control**: Codes the low-level controls, such as the steering mechanism,
    that the model will use to control the robot''s movement.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**控制**：编写低级控制代码，例如模型用于控制机器人运动的转向机制。'
- en: '**Deployment of the model**: Checks how the model works in the actual surroundings
    for which it has been created.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型部署**：检查模型在实际环境中如何工作，这些环境是为其创建的。'
- en: In traditional robotic development, these methods are hardcoded. With deep learning,
    however, we can create algorithms that learn actions from end to end, thereby
    eliminating step *2* through *4* in this process, and significantly cutting down
    on the time it takes to develop a successful robot. What's even more important
    is the ability of deep learning techniques to generalize; instead of having to
    program motions for variations of a given task, we can teach our algorithms to
    learn a general response to that task. In recent years, this AI approach to robotics
    has made breakthroughs in the field and allowed for significantly more advanced
    robots.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器人开发中，这些方法是硬编码的。然而，通过深度学习，我们可以创建能够从头到尾学习动作的算法，从而消除该过程中的第*2*到第*4*步，并显著缩短开发成功机器人所需的时间。更重要的是，深度学习技术具有泛化能力；我们无需为特定任务的变化编程动作，而是可以教会算法学习对该任务的普适响应。近年来，这种人工智能方法在机器人领域取得了突破，允许开发出更为先进的机器人。
- en: Due to a lack of consistency of parts and control systems in the robotics market,
    designing and creating a physical robot can be a difficult task. In February 2018,
    OpenAI added virtual simulated robotic arms to its training environments, which
    opened the door for a myriad of new applications and development. These virtual
    environments use a physical environment simulator to allow us to immediately begin
    testing our robot control algorithms without the need to procure expensive and
    disparate parts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器人市场中零部件和控制系统缺乏一致性，设计和创建物理机器人可能是一个艰巨的任务。2018年2月，OpenAI在其训练环境中加入了虚拟模拟机器人手臂，开启了无数新应用和开发的大门。这些虚拟环境使用物理环境模拟器，允许我们立即开始测试机器人控制算法，而无需采购昂贵且不一致的零部件。
- en: Setting up your environment
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: 'We''ll be utilizing the gym environment from OpenAI that we learned about in
    [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning,*
    to create an intelligent robotic arm. OpenAI created a virtual environment based
    on Fetch Robotic Arms, which created the first fully virtualized test space for
    robotics algorithms:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用OpenAI的gym环境，这是我们在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)《强化学习》中学习的内容，来创建一个智能机器人手臂。OpenAI基于Fetch机器人手臂创建了一个虚拟环境，成为第一个完全虚拟化的机器人算法测试空间：
- en: '![](img/fb3a5c98-5a77-4e1c-a6ae-9a115da7ecaa.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb3a5c98-5a77-4e1c-a6ae-9a115da7ecaa.png)'
- en: 'You should have these environments already installed on your computer from
    when we installed gym in[ Chapter 11](4fc1bf80-b3a2-4615-9057-bfff14c0508b.xhtml),
    *Deep Learning for Finance*. We''ll just need to add two more packages to get
    this robotics environment up and running:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经安装了这些环境，因为我们在[第11章](4fc1bf80-b3a2-4615-9057-bfff14c0508b.xhtml)《金融深度学习》中安装了gym环境。我们只需再添加两个包来让这个机器人环境运行起来：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Both `cmake` and `openmpi` are designed to help with the computational efficiency
    ...
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`cmake`和`openmpi`都是为提高计算效率而设计的...'
- en: MuJoCo physics engine
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MuJoCo物理引擎
- en: 'MuJoCo is a virtual environment that simulates real physical environments for
    testing intelligent algorithms. It''s the environment that the leading AI researchers
    at Google DeepMind, OpenAI, and others use to teach virtual robots tasks, virtual
    humans, and spiders to run, among other tasks. Google in particular made quite
    a splash in the news in 2017 when they published a video of a virtual human that
    taught itself to run and jump based on reinforcement learning techniques:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MuJoCo是一个虚拟环境，模拟现实物理环境，用于测试智能算法。这是Google DeepMind、OpenAI等领先AI研究者用来教虚拟机器人执行任务、虚拟人类以及蜘蛛等任务的环境。特别是Google，在2017年发布了一段虚拟人类自学跑步和跳跃的视频，引起了广泛关注，展示了强化学习技术的应用：
- en: '![](img/2b8cd775-1697-4521-85fc-92fa0e93c3bf.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b8cd775-1697-4521-85fc-92fa0e93c3bf.png)'
- en: 'MuJoCo is a paid licensed program, but they allow for 30-day free trials which
    we will use to complete our tasks. If you are a student, you can obtain a perpetual
    license of MuJoCo for free for your own personal projects. There are a few steps
    to go through to obtain MuJoCo and set up your environment for AI applications:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MuJoCo是一个付费许可程序，但它提供30天的免费试用，我们将使用这个试用期来完成我们的任务。如果你是学生，可以免费获得MuJoCo的永久许可证，用于个人项目。以下是获取MuJoCo并设置AI应用环境的几个步骤：
- en: Download the MuJoCo binary files from their website
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从MuJoCo网站下载二进制文件
- en: Sign up for a free trial of MuJoCo
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册MuJoCo的免费试用
- en: Place your license key in the `~|.mujoco|mjkey.txt` folder
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的许可证密钥放入`~|.mujoco|mjkey.txt`文件夹中
- en: Install the Python package for MuJoCo
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装MuJoCo的Python包
- en: This may seem a bit cumbersome, but it does give us access to the most cutting-edge
    simulator that is used in the AI world. If you're having issues with any of these
    steps, we'll be keeping an up-to-date help document on the GitHub repository for
    this book. With that, let's walk through these steps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有些繁琐，但它确实让我们能够使用AI界最前沿的模拟器。如果你在这些步骤中遇到问题，我们将在本书的GitHub仓库中保持最新的帮助文档。现在，让我们一起来完成这些步骤。
- en: Downloading the MuJoCo binary files
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载MuJoCo二进制文件
- en: 'First things first, let''s download the MuJoCo program itself. Navigate to
    [https://www.roboti.us/index.html](https://www.roboti.us/index.html) and download
    the [`mjpro150`](https://www.roboti.us/index.htm) file that corresponds to your
    operating system[.](https://www.roboti.us/index.htm) You can also do this through
    the command line with the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载 MuJoCo 程序本身。访问 [https://www.roboti.us/index.html](https://www.roboti.us/index.html)
    并下载与您的操作系统对应的[`mjpro150`](https://www.roboti.us/index.htm) 文件。您也可以通过命令行使用以下代码来下载：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Signing up for a free trial of MuJoCo
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册 MuJoCo 的免费试用
- en: 'Go through the following steps to sign-up for a free trial of MuJoCo:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤注册 MuJoCo 的免费试用：
- en: 'Once you''ve downloaded the binary files for MuJoCo, navigate to [https://www.roboti.us/license.html](https://www.roboti.us/license.html)
    to sign up for a free trial. You should see the following prompt box for signing
    up for MuJoCo:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载完 MuJoCo 的二进制文件后，访问 [https://www.roboti.us/license.html](https://www.roboti.us/license.html)
    注册免费试用。您应该会看到以下注册 MuJoCo 的提示框：
- en: '![](img/c26c32f7-64f1-4a17-8b30-bd3ae6565e27.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c26c32f7-64f1-4a17-8b30-bd3ae6565e27.png)'
- en: 'See those blue links to the right of the computer ID box? You''ll need to download
    the one that corresponds to your operating system. This will generate a key for
    MuJoCo to keep track of your computer and its trial. If you are using macOS, you
    can download and get the key with the following code:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看计算机 ID 框右侧的蓝色链接吗？您需要下载与您的操作系统对应的链接。这将生成一个 MuJoCo 密钥，用于跟踪您的计算机和试用。如果您使用的是 macOS，可以通过以下代码下载并获取密钥：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you are on a Linux machine, you can download your MuJoCo key with the following
    code:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用的是 Linux 机器，可以通过以下代码下载 MuJoCo 密钥：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once you have your key, place it in the prompt box. You should receive an email
    with a license that will enable you to use MuJoCo.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您获得了密钥，将其放入提示框中。您应该会收到一封包含许可证的电子邮件，这将使您能够使用 MuJoCo。
- en: Configuring your MuJoCo files
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 MuJoCo 文件
- en: 'Next, you''ll need to configure your access files. You should receive your
    license key from MuJoCo in an email. Once you do, place it in the following folder
    so the program can access it:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要配置您的访问文件。您应该会通过电子邮件收到来自 MuJoCo 的许可证密钥。一旦收到，请将其放入以下文件夹，以便程序能够访问：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Installing the MuJoCo Python package
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 MuJoCo Python 包
- en: 'Lastly, we need to install the MuJoCo Python package that will allow Python
    to speak to the program. We can do that easily with a `pip install`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要安装 MuJoCo Python 包，这将允许 Python 与该程序进行交互。我们可以通过简单的 `pip install` 来完成：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should now have access to the most powerful virtualized environment for
    robotics testing. If you've had issues with any part of this process, remember
    that you can always access an up-to-date issues page from this book's GitHub repository.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该可以访问最强大的机器人测试虚拟环境。如果在此过程中遇到任何问题，请记得您可以随时通过本书的 GitHub 仓库访问最新的故障排除页面。
- en: Setting up a deep deterministic policy gradients model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置深度确定性策略梯度模型
- en: 'In [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, we learned about how to use policy optimization methods for continuous
    action spaces. Policy optimization methods learn directly by optimizing a policy
    from actions taken in their environment, as explained in the following diagram:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)《*强化学习*》中，我们学习了如何使用策略优化方法处理连续动作空间。策略优化方法通过优化环境中采取的行动直接学习，如下图所示：
- en: '![](img/3d215981-be18-4299-b4d8-0431ce0fe32b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d215981-be18-4299-b4d8-0431ce0fe32b.png)'
- en: Remember, policy gradient methods are **off**-**policy**, meaning that their
    behavior in a certain moment is not necessarily reflective of the policy they
    are abiding by. These policy gradient algorithms utilize **policy iteration**,
    where they evaluate the given policy and follow the policy gradient in order to
    learn an optimal policy. ...
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，策略梯度方法是**离策略**的，这意味着它们在某一时刻的行为不一定反映它们所遵循的策略。这些策略梯度算法使用**策略迭代**，评估给定的策略并跟随策略梯度来学习最优策略。...
- en: Experience replay buffer
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验重放缓冲区
- en: Next, let's create the experience replay buffer as we did in [Chapter 11](4fc1bf80-b3a2-4615-9057-bfff14c0508b.xhtml),
    *Deep Learning for Finance,* when we looked at creating deep learning models for
    game playing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们像在[第11章](4fc1bf80-b3a2-4615-9057-bfff14c0508b.xhtml)《*金融深度学习*》中一样创建经验重放缓冲区，在那一章中，我们研究了如何为游戏创建深度学习模型。
- en: Hindsight experience replay
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事后经验重放
- en: 'To improve on robotic movement, OpenAI researchers released a paper on a technique
    known as **Hindsight experience replay** (**HER**) in 2018 to attempt to overcome
    issues that arise when training reinforcement learning algorithms in **sparse
    environments**. Recall [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, choosing an appropriate reward for an agent can make or break the performance
    of that agent. Anecdotes such as the following often appear as the result of bad
    reward functions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善机器人运动，OpenAI 的研究人员于 2018 年发布了一篇关于一种名为 **后视经验回放**（**HER**）的技术的论文，旨在克服在 **稀疏环境**
    中训练强化学习算法时出现的问题。回想一下 [第 8 章](91114074-444f-4201-98ef-e510210380f2.xhtml)，*强化学习*，为一个智能体选择合适的奖励可以决定该智能体的表现成败。以下这种轶事通常是由于不良奖励函数的结果：
- en: A friend is training a simulated robot arm to reach toward a point above a table.
    It turns out the point was defined with respect to the table, and the table wasn’t
    anchored to anything. The policy learned to slam the table really hard, making
    the table ...
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个朋友正在训练一个模拟的机器人手臂，让它伸向桌子上方的一个点。结果发现，这个点是相对于桌子定义的，而桌子并没有固定在任何地方。学习到的策略是猛烈地撞击桌子，导致桌子...
- en: The actor–critic network
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论员网络
- en: DDPG models rely on actor-critic frameworks, which are used to learn policies
    without having to calculate the value function again and again.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 模型依赖于演员-评论员框架，该框架用于学习策略，而不需要一遍又一遍地计算价值函数。
- en: Actor-critic models are essential frameworks in reinforcement learning. They
    are the basis and inspiration for **Generative Adversarial Networks** (**GANs**),
    which we learned about in [Chapter 7](719394b6-6058-4ac6-89f3-c2ec26563e7a.xhtml),
    *Generative Models*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论员模型是强化学习中的基础框架。它们是 **生成对抗网络**（**GANs**）的基础和灵感来源，后者我们在 [第 7 章](719394b6-6058-4ac6-89f3-c2ec26563e7a.xhtml)
    *生成模型* 中学到过。
- en: 'As you may have guessed,these actor-critic models consist of two parts:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，这些演员-评论员模型由两个部分组成：
- en: '**The actor**: Estimates the policy function'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动者**：估计策略函数'
- en: '**The critic**: Estimates the value function'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论员**：估计价值函数'
- en: 'The process works exactly as it sounds; the actor model tries to emulate an
    action, and the critic model criticizes the actor model to help it improve its
    performance. Let''s take our robotics use case; the goal of robotic motion is
    to create a robot that simulates human ability. In this case, the actor would
    want to observe human actions to emulate this, and the critic would help guide
    the robotic actor to be more human. With actor-critic models, we simply take this
    paradigm and translate it into mathematical formulas. Altogether, the actor-critic
    process looks as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程如字面所示；演员模型尝试模仿一个动作，而评论员模型则批评演员模型以帮助其改善表现。让我们以机器人的应用为例；机器人运动的目标是创建一个模拟人类能力的机器人。在这种情况下，演员会想观察人类的动作以进行模仿，评论员则帮助引导机器人演员变得更像人类。通过演员-评论员模型，我们只是将这一范式转换成数学公式。总体来说，演员-评论员过程如下所示：
- en: '![](img/86427f1d-e4a3-4957-8519-fecbacd3f36d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86427f1d-e4a3-4957-8519-fecbacd3f36d.png)'
- en: Each of these networks has its own loss function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些网络都有自己的损失函数。
- en: The actor
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行动者
- en: 'The actor network is also called the **target policy network**. The actor is
    trained by utilizing gradient descent that has been mini-batched. Its loss is
    defined with the following function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络也被称为 **目标策略网络**。演员通过使用小批量的梯度下降进行训练。它的损失由以下函数定义：
- en: '![](img/9825ba57-196d-4759-be16-82828f514c27.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9825ba57-196d-4759-be16-82828f514c27.png)'
- en: Here, *s* represents a state that has been sampled from the replay buffer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*s* 表示从回放缓冲区采样的状态。
- en: The critic
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评论员
- en: The output of the critic is the estimate of the action-value function Q (^(s,a)),
    and as such you might see the critic network sometimes called the **action**-**value
    function approximator**. Its job is to help the actor appropriately approximate
    the action-value function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员的输出是动作价值函数 Q (^(s,a)) 的估计值，因此你可能会看到评论员网络有时被称为 **动作**-**价值函数近似器**。它的工作是帮助行动者适当地近似动作价值函数。
- en: 'The critic model works very similarly to the Q-function approximator that we
    saw in [Chapter 10](11b4608a-b8e2-4bea-bef1-8b2569c35a71.xhtml), *Deep Learning
    for Game Playing*. The critic produces a **temporal**-**difference** (**TD**)
    error, which it uses to update its gradients. The TD error helps the algorithm
    reduce the variance that occurs from trying to make predictions off of highly
    correlated data. DDPG utilizes a target network, just as the Deep Q-network did
    in [Chapter 10](11b4608a-b8e2-4bea-bef1-8b2569c35a71.xhtml), *Deep Learning for
    Game Playing,* only the targets are computed by utilizing the outputs of the actor:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员模型与我们在[第10章](11b4608a-b8e2-4bea-bef1-8b2569c35a71.xhtml)《游戏学习深度学习》中看到的Q函数近似器非常相似。评论员生成一个**时间差异**（**TD**）误差，用于更新其梯度。TD误差帮助算法减少由高度相关数据预测时出现的方差。DDPG利用目标网络，就像在[第10章](11b4608a-b8e2-4bea-bef1-8b2569c35a71.xhtml)《游戏学习深度学习》中一样，只不过目标是通过利用演员的输出计算出来的：
- en: '![](img/8d38eac6-84d6-4842-84bd-d38721b70e20.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d38eac6-84d6-4842-84bd-d38721b70e20.png)'
- en: 'This target network generates targets for the TD error calculations, and acts
    as a regularizer. Let''s break this down:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标网络为TD误差计算生成目标，并充当正则化器。我们来分解一下：
- en: '![](img/bc93632b-c702-4cef-9135-b5f59d3ded32.png)represents our TD error.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/bc93632b-c702-4cef-9135-b5f59d3ded32.png)代表了我们的TD误差。'
- en: r^i represent the reward received from a certain action.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: r^i代表从某个动作中获得的奖励。
- en: '![](img/753c7672-5f60-43fe-99dd-e9aa4975f052.png) collectively represents the
    target ![](img/40dd12d0-0706-4931-aece-ceefb112ba0d.png)for the actor and critic
    models. Recall that![](img/f57fbccf-3122-44dc-ba49-4ad5bbb6f6cb.png) (gamma) represents
    a **discount factor**. Recall that the discount factor can be any value between
    0 and 1 that represents the relative importance between a current reward and a
    future reward. So, our target then becomes the output of the actor/critic models
    multiplied by the mixing parameter.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/753c7672-5f60-43fe-99dd-e9aa4975f052.png)共同代表了演员和评论员模型的目标！[](img/40dd12d0-0706-4931-aece-ceefb112ba0d.png)。回忆一下！[](img/f57fbccf-3122-44dc-ba49-4ad5bbb6f6cb.png)（gamma）代表了**折扣因子**。记住，折扣因子可以是介于0和1之间的任何值，表示当前奖励和未来奖励之间的相对重要性。所以，我们的目标变成了演员/评论员模型输出与混合参数的乘积。'
- en: '![](img/5cc11c26-1a2b-4cfb-995c-7c8024c86a8e.png) represents the target of
    the actor network; it''s saying that the target ![](img/257aded6-196e-4062-b400-18fa67ad6c49.png)is
    a function of state *s*, given a particular policy ![](img/5772391f-fe8a-4070-aee2-5386cbc0eb53.png).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/5cc11c26-1a2b-4cfb-995c-7c8024c86a8e.png)代表演员网络的目标；它表示目标![](img/257aded6-196e-4062-b400-18fa67ad6c49.png)是状态*s*的函数，给定一个特定的策略![](img/5772391f-fe8a-4070-aee2-5386cbc0eb53.png)。'
- en: Likewise, we then say that the output of the actor network is dependent on the
    critic network by representing that critic network by its weights ![](img/44017cfb-fbe4-425e-9c48-31e532cf8bfc.png).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，我们也可以说，演员网络的输出依赖于评论员网络，表示评论员网络的权重为![](img/44017cfb-fbe4-425e-9c48-31e532cf8bfc.png)。
- en: 'The critic tries to minimize its own loss function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员尝试最小化它自己的损失函数：
- en: '![](img/c5aca07d-2976-4120-8432-d0f760179f86.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5aca07d-2976-4120-8432-d0f760179f86.png)'
- en: Deep Deterministic Policy Gradients
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: There is a natural extension of **Deep Deterministic Policy Gradients** (**DDPG**)
    by replacing the feedforward neural networks used for approximating the actor
    and the critic with recurrent neural networks. This extension is called the **recurrent
    deterministic policy gradient** algorithm (**RDPG**) and is discussed in the f
    paper N. Heess, J. J. Hunt, T. P. Lillicrap and D. Silver. *Memory-based control
    with recurrent neural networks*. 2015.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度确定性策略梯度**（**DDPG**）有一个自然扩展，通过将用于近似演员和评论员的前馈神经网络替换为递归神经网络。这个扩展称为**递归确定性策略梯度**算法（**RDPG**），并在N.
    Heess、J. J. Hunt、T. P. Lillicrap和D. Silver的论文中进行了讨论。*基于记忆的控制与递归神经网络*，2015年。'
- en: The recurrent critic and actor are trained using **backpropagation through time** (**BPTT**).
    For readers who are interested in it, the paper can be downloaded from [https://arxiv.org/abs/1512.04455](https://arxiv.org/abs/1512.04455).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 循环评论员和演员通过**时间反向传播**（**BPTT**）进行训练。对有兴趣的读者，论文可以从[https://arxiv.org/abs/1512.04455](https://arxiv.org/abs/1512.04455)下载。
- en: Implementation of DDPG
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG的实现
- en: This section will show you how to implement the actor-critic architecture using
    TensorFlow. The code structure is almost the same as the DQN implementation that
    was shown in the previous chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将展示如何使用TensorFlow实现演员-评论员架构。代码结构几乎与上一章展示的DQN实现相同。
- en: 'The `ActorNetwork` is a simple MLP that takes the observation state as its
    input:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`ActorNetwork`是一个简单的多层感知机（MLP），它将观察状态作为输入：'
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The constructor requires four arguments: `input_state`, `output_dim`, `hidden_layers`,
    and `activation`. `input_state` is a tensor for the observation state. `output_dim` is
    the dimension of the action space. `hidden_layers` specifies the number of the
    hidden layers and the number of units for each layer. `activation` indicates the
    activation function for the output layer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数需要四个参数：`input_state`、`output_dim`、`hidden_layers`和`activation`。`input_state`是一个表示观察状态的张量。`output_dim`是动作空间的维度。`hidden_layers`指定了隐藏层的数量和每层的单元数。`activation`表示输出层的激活函数。
- en: 'The `CriticNetwork` is also a MLP, which is enough for the classic control
    tasks:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`CriticNetwork`也是一个多层感知机（MLP），对于经典控制任务来说已经足够：'
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The network takes the state and the action as its inputs. It first maps the
    state into a hidden feature representation and then concatenates this representation
    with the action, followed by several hidden layers. The output layer generates
    the Q-value that corresponds to the inputs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将状态和动作作为输入。它首先将状态映射到一个隐藏的特征表示，然后将该表示与动作拼接，接着通过若干隐藏层。输出层生成与输入相对应的Q值。
- en: 'The actor-critic network combines the actor network and the critic network
    together:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家网络将演员网络和评论家网络结合在一起：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The constructor requires six arguments, as follows: `input_dim` and `action_dim` are
    the dimensions of the state space and the action space, respectively. `critic_layers` and `actor_layers` specify
    the hidden layers of the critic network and the actor network.  `actor_activation` indicates
    the activation function for the output layer of the actor network. `scope` is
    the scope name used for the `scope` TensorFlow variable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数需要六个参数，分别是：`input_dim`和`action_dim`是状态空间和动作空间的维度。`critic_layers`和`actor_layers`指定评论家网络和演员网络的隐藏层。`actor_activation`表示演员网络输出层的激活函数。`scope`是用于`scope`
    TensorFlow变量的作用域名称。
- en: The constructor first creates an instance of the `self.actor_network` actor
    network with an input of `self.x,` where `self.x` represents the current state.
    It then creates an instance of the critic network using the following as the inputs: `self.actor_network.get_output_layer()` as
    the output of the actor network and `self.x` as the current state. Given these
    two networks, the constructor calls `self._build()` to build the loss functions
    for the actor and critic that we discussed previously. The actor loss is `-tf.reduce_mean(value)`,
    where `value` is the Q-value computed by the critic network. The critic loss is `0.5
    * tf.reduce_mean(tf.square((value - self.y)))`, where `self.y` is a tensor for
    the predicted target value computed by the target network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数首先创建一个`self.actor_network`演员网络的实例，输入为`self.x`，其中`self.x`表示当前状态。然后，它使用以下输入创建一个评论家网络的实例：`self.actor_network.get_output_layer()`作为演员网络的输出，`self.x`作为当前状态。给定这两个网络，构造函数调用`self._build()`来构建我们之前讨论的演员和评论家的损失函数。演员损失是`-tf.reduce_mean(value)`，其中`value`是评论家网络计算的Q值。评论家损失是`0.5
    * tf.reduce_mean(tf.square((value - self.y)))`，其中`self.y`是目标网络计算的预测目标值的张量。
- en: 'The class `ActorCriticNet` provides the functions for calculating the action
    and the Q-value given the current state, that is, `get_action` and `get_value`.
    It also provides `get_action_value`, which computes the `state-action value` function
    given the current state and the action taken by the agent:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`ActorCriticNet`类提供了在给定当前状态的情况下计算动作和Q值的函数，即`get_action`和`get_value`。它还提供了`get_action_value`，该函数在给定当前状态和代理采取的动作的情况下计算`状态-动作值`函数：'
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Because DPG has almost the same architecture as DQN, the implementations of
    the replay memory and the optimizer are not shown in this chapter. For more details,
    you can refer to the previous chapter or visit our GitHub repository ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)).
    By combining these modules together, we can implement the `DPG` class for the
    deterministic policy gradient algorithm:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因为DPG的架构几乎与DQN相同，所以在本章中没有展示回放记忆和优化器的实现。更多细节，请参考上一章或访问我们的GitHub仓库（[https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)）。通过将这些模块结合在一起，我们可以实现用于确定性策略梯度算法的`DPG`类：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, `config` includes all the parameters of DPG, for example, batch size
    and learning rate for training. The `task` is an instance of a certain classic
    control task. In the constructor, the replay memory, Q-network, target network,
    and optimizer are initialized by calling the `_init_modules` function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`config`包含DPG的所有参数，例如训练的批次大小和学习率。`task`是某个经典控制任务的实例。在构造函数中，通过调用`_init_modules`函数初始化了回放内存、Q网络、目标网络和优化器：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `choose_action` function selects an action based on the current estimate
    of the actor-critic network and the observed state.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`choose_action`函数根据演员-评论家网络的当前估计和观察到的状态选择一个动作。'
- en: Note that a Gaussian noise controlled by `epsilon` is added for exploration.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，受`epsilon`控制的高斯噪声被加入以进行探索。
- en: The `play` function submits an action into the simulator and returns the feedback
    from the simulator. The `update_target_network` function updates the target network
    from the current actor-critic network.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`play`函数将动作提交给模拟器，并返回模拟器的反馈。`update_target_network`函数根据当前的演员-评论家网络更新目标网络。'
- en: 'To begin the training process, the following function can be called:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练过程，可以调用以下函数：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In each episode, it calls `replay_memory.phi` to get the current state and calls
    the `choose_action` function to select an action based on the current state. This
    action is submitted into the simulator by calling the `play` function, which returns
    the corresponding reward, next state, and termination signal. Then, the `(current
    state, action, reward, termination)` transition is stored into the replay memory.
    For every `update_interval` step (`update_interval = 1` ,by default), the actor-critic
    network is trained with a batch of transitions that are randomly sampled from
    the replay memory. For every `time_between_two_copies` step, the target network is
    updated and the weights of the Q-network are saved to the hard disk.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个回合中，它调用`replay_memory.phi`来获取当前状态，并调用`choose_action`函数根据当前状态选择一个动作。这个动作通过调用`play`函数提交到模拟器中，模拟器返回相应的奖励、下一个状态和终止信号。然后，`(当前状态,
    动作, 奖励, 终止)`转换被存储到回放内存中。每当执行`update_interval`步（默认`update_interval = 1`）时，演员-评论家网络会使用从回放内存中随机采样的一批转换进行训练。每经过`time_between_two_copies`步，目标网络会更新，Q网络的权重将被保存到硬盘中。
- en: 'After the training step, the following function can be called for evaluating
    the performance of our trained agent:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤后，可以调用以下函数来评估我们训练的智能体的性能：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we expanded upon the knowledge that we obtained about in [Chapter
    8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning*, to learn
    about DDPG, HER, and how to combine these methods to create a reinforcement learning
    algorithm that independently controls a robotic arm.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们扩展了在[第8章](91114074-444f-4201-98ef-e510210380f2.xhtml)中获得的有关*强化学习*的知识，学习了DDPG、HER，以及如何将这些方法结合起来创建一个能够独立控制机器人手臂的强化学习算法。
- en: The Deep Q network that we used to solve game challenges worked in discrete
    spaces; when building algorithms for more fluid motion tasks such as robots or
    self-driving cards, we need a class of algorithms that can handle continuous action
    spaces. For this, use policy gradient methods, which learn a policy from a set
    of actions directly. We can improve this learning by using an experience replay
    buffer, which stores positive past experiences so that they may be sampled during
    training ...
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来解决游戏挑战的深度Q网络在离散空间中工作；当构建适用于更流畅运动任务（如机器人或自动驾驶汽车）的算法时，我们需要一种能够处理连续动作空间的算法。为此，使用策略梯度方法，它直接从一组动作中学习一个策略。我们可以通过使用经验回放缓冲区来改进这一学习过程，它存储了过去的正向经验，以便在训练过程中进行采样……
- en: References
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Python Reinforcement Learning Projects*, Sean Saito, Yang Wenzhuo, and Rajalingappaa
    Shanmugamani, [https://www.packtpub.com/big-data-and-business-intelligence/python-reinforcement-learning-projects](https://www.packtpub.com/big-data-and-business-intelligence/python-reinforcement-learning-projects).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python强化学习项目*，Sean Saito，Yang Wenzhuo，和Rajalingappaa Shanmugamani，[https://www.packtpub.com/big-data-and-business-intelligence/python-reinforcement-learning-projects](https://www.packtpub.com/big-data-and-business-intelligence/python-reinforcement-learning-projects)。'
