- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: AI for Logistics – Robots in a Warehouse
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物流中的人工智能——仓库中的机器人
- en: It's time for the next step on our AI journey. I told you at the beginning of
    this book that AI has tremendous value to bring to transport and logistics, with
    self-driving delivery vehicles that speed up logistical processes. They're a huge
    boost to the economy through the e-commerce industry.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是我们人工智能之旅的下一步了。书的开头我曾告诉你，人工智能在运输和物流方面具有巨大的价值，特别是自动驾驶配送车辆，它们能加速物流流程。这些技术通过电子商务产业为经济带来了巨大的推动。
- en: 'In this new chapter, we''ll build an AI for just that kind of application.
    The model we''ll use for this will, of course, be Q-learning (we''re saving deep
    Q-learning for the self-driving car). Q-learning is a simple, but powerful, AI
    model that can optimize the flows of movement in a warehouse, which is the real-world
    problem you''ll solve here. In order to facilitate this journey, you''ll work
    on an environment you''re already familiar with: the maze we saw in the previous
    chapter.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将为这种应用构建一个人工智能。我们将使用的模型当然是Q学习（我们将深度Q学习留给自动驾驶汽车）。Q学习是一个简单但强大的人工智能模型，可以优化仓库中的移动流，这是你将在这里解决的现实问题。为了便于这一旅程，你将使用一个你已经熟悉的环境：我们在上一章看到的迷宫。
- en: 'The difference is that, this time, the maze will actually be the warehouse
    of a business. It could be any business: an e-commerce business, a retail business,
    or any business that sells products to customers and that has a warehouse to store
    large amounts of products to be sold.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于，这次迷宫实际上是某个企业的仓库。它可以是任何一种企业：电子商务企业、零售企业，或者任何一个销售产品给顾客并拥有仓库来存储大量产品的企业。
- en: 'Let''s have a look again at this maze, now a warehouse:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 再看看这个迷宫，现在它变成了一个仓库：
- en: '![](img/B14110_08_01.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_01.png)'
- en: 'Figure 1: The warehouse'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：仓库
- en: 'Inside this warehouse, the products are stored in 12 different locations, labeled
    by the following letters from **A** to **L**:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个仓库里，产品存放在12个不同的位置，用 **A** 到 **L** 的字母标记：
- en: '![](img/B14110_08_02.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_02.png)'
- en: 'Figure 2: Locations in the warehouse'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：仓库中的位置
- en: 'When orders are placed by customers, a robot moves around the warehouse to
    collect the products for delivery. That will be your AI! Here''s what it looks
    like:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户下订单时，机器人会在仓库内移动，收集待配送的产品。那就是你的人工智能！它长这样：
- en: '![](img/B14110_08_03.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_03.png)'
- en: 'Figure 3: Warehouse robot'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：仓库机器人
- en: 'The 12 locations are all connected to a computer system, which ranks in real
    time the product collection priorities for these 12 locations. As an example,
    let''s say that at a specific time, *t*, it returns the following ranking:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这12个位置都连接到计算机系统，该系统实时对这些12个位置的产品收集优先级进行排序。举个例子，假设在某个特定时间 *t*，它返回如下排序：
- en: '![](img/B14110_08_04.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_04.png)'
- en: 'Figure 4: Top priority locations'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：最高优先级的位置
- en: Location **G** has priority 1, which means it's the top priority, as it contains
    a product that must be collected and delivered immediately. Our robot must move
    to location **G** by the shortest route depending on where it is. Our goal is
    to actually build an AI that will return that shortest route, wherever the robot
    is.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 位置 **G** 排在第一位，这意味着它是最高优先级的，因为它包含一个必须立即收集并配送的产品。我们的机器人必须根据当前位置通过最短的路线到达位置 **G**。我们的目标实际上是构建一个人工智能，它能返回这条最短路线，无论机器人在哪里。
- en: But we could do even better. Here, locations **K** and **L** are in the top
    3 priorities. Hence, it would be great to implement an option for our robot to
    go via some intermediary locations before reaching its final top priority location.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们可以做得更好。这里，位置 **K** 和 **L** 排在前三位。因此，为我们的机器人实现一个选项，允许它通过一些中介位置再到达最终的最高优先级位置，将是非常有意义的。
- en: The way the system computes the priorities of the locations is outside the scope
    of this case study. The reason for this is that there can be many ways, from simple
    rules or algorithms, to deterministic computations, to machine learning, to compute
    these priorities. But most of these ways would not be AI as we know it today.
    What we really want to focus on in this exercise is the core AI, encompassing
    Reinforcement Learning and Q-learning. We can just say for the purposes of this
    example that location **G** is the top priority because one of the most loyal
    platinum-level customers of the company placed an urgent order of a product stored
    in location **G**, which therefore must be delivered as soon as possible.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 系统如何计算位置的优先级超出了本案例研究的范围。原因是，计算这些优先级的方式有很多种，从简单的规则或算法，到确定性计算，再到机器学习。但其中大多数方式都不是我们今天所说的AI。我们在这次练习中真正想要关注的是核心AI，包括强化学习和Q学习。为了本例的目的，我们可以简单地说，位置**G**是最高优先级的，因为公司的一个最忠实的铂金级客户下了一个紧急订单，产品存放在位置**G**，因此必须尽快交付。
- en: In conclusion, our mission is to build an AI that will always take the shortest
    route to the top priority location, whatever the location it starts from, and
    have the option to go by an intermediary location which is in the top three priorities.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的使命是构建一个AI，它始终会选择从任意起点出发，走最短的路线到达优先级最高的位置，并且可以选择经过一个位于前三个优先级中的中介位置。
- en: Building the environment
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建环境
- en: 'When building an AI, the first thing we always have to do is define the environment.
    Defining an environment always requires the following three elements:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建AI时，我们首先需要做的是定义环境。定义环境总是需要以下三个元素：
- en: Defining the states
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义状态
- en: Defining the actions
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义动作
- en: Defining the rewards
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义奖励
- en: These three elements have already been defined in the previous chapter on Q-learning,
    but let's quickly remind ourselves what they are.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个元素在前一章关于Q学习的内容中已经定义过了，但我们来快速回顾一下它们是什么。
- en: The states
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态
- en: The state, at a specific time *t*, is the location where the robot is at that
    time *t*. However, remember, you have to encode the location names so that our
    AI can do the math.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定时刻*t*，状态是机器人在该时刻*t*所在的位置。不过，请记住，你需要对位置名称进行编码，这样我们的AI才能进行计算。
- en: 'At the risk of disappointing you, given all the crazy hype about AI, let''s
    remain realistic and understand that Q-learning is nothing more than a bunch of
    math equations; just like any other AI model. Let''s make the encoding integers
    start at 0, simply because indexes in Python start at 0:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免让你失望，尽管有关于AI的疯狂炒作，我们还是要保持现实，理解Q学习不过是一堆数学公式；就像其他任何AI模型一样。我们让编码的整数从0开始，仅仅是因为Python中的索引从0开始：
- en: '![](img/B14110_08_05.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_05.png)'
- en: 'Figure 5: Location to state mapping'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：位置到状态的映射
- en: The actions
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作
- en: 'The actions are the next possible destinations to which the robot can go. You
    can encode these destinations with the same indexes as the states. Hence, the
    total list of actions that the AI can perform is the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动作是机器人可以前往的下一个可能目的地。你可以用与状态相同的索引对这些目的地进行编码。因此，AI可以执行的动作总列表如下：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The rewards
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励
- en: Remember, when in a specific location, there are some actions that the robot
    cannot perform. For example, if the robot is in location **J**, it can perform
    the actions 5, 8, and 10, but it cannot perform the other actions. You can specify
    that by attributing a reward of 0 to the actions it cannot perform, and a reward
    of 1 to the actions it can perform.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在特定位置时，机器人无法执行某些动作。例如，如果机器人处于位置**J**，它可以执行动作5、8和10，但无法执行其他动作。你可以通过为无法执行的动作赋予奖励0，为可以执行的动作赋予奖励1来指定这一点。
- en: 'That brings you to building the following matrix of rewards:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导你构建以下奖励矩阵：
- en: '![](img/B14110_08_06.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_06.png)'
- en: 'Figure 6: Rewards matrix'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：奖励矩阵
- en: AI solution refresher
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI解决方案回顾
- en: 'It never hurts to get a little refresher of a model before implementing it!
    Let''s remind ourselves of the steps of the Q-learning process; this time, adapting
    it to your new problem. Let''s welcome Q-learning back on stage:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施模型之前，回顾一下模型总是有益的！让我们回顾一下Q学习过程的步骤；这一次，我们将其调整到你的新问题上。让我们欢迎Q学习重新登场：
- en: Initialization (first iteration)
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始化（第一次迭代）
- en: 'For all pairs of states *s* and actions *a*, the Q-values are initialized to
    0:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有状态*s*和动作*a*的组合，Q值初始化为0：
- en: '![](img/Image15036.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image15036.png)'
- en: Next iterations
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下一步迭代
- en: 'At each iteration *t* ≥ 1, the AI will repeat the following steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，*t* ≥ 1，AI 将重复以下步骤：
- en: It selects a random state ![](img/B14110_08_002.png) from the possible states:![](img/B14110_08_003.png)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从可能的状态中选择一个随机状态！[](img/B14110_08_002.png)：![](img/B14110_08_003.png)
- en: It performs a random action ![](img/B14110_08_004.png) that can lead to a next
    possible state, that is, such that ![](img/B14110_08_005.png):![](img/B14110_08_006.png)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它执行一个随机动作 ![](img/B14110_08_004.png)，这个动作可能导致下一个可能的状态，即： ![](img/B14110_08_005.png)：![](img/B14110_08_006.png)
- en: It reaches the next state ![](img/B14110_08_007.png) and gets the reward ![](img/B14110_08_008.png).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它到达下一个状态 ![](img/B14110_08_007.png)，并获得奖励 ![](img/B14110_08_008.png)。
- en: It computes the temporal difference ![](img/B14110_08_009.png):![](img/B14110_08_010.png)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它计算时间差异！[](img/B14110_08_009.png)：![](img/B14110_08_010.png)
- en: It updates the Q-value by applying the Bellman equation:![](img/B14110_08_011.png)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它通过应用贝尔曼方程来更新 Q 值：![](img/B14110_08_011.png)
- en: We repeat these steps over 1,000 iterations. Why 1,000? The choice of 1,000
    comes from my experimentation with this particular environment. I chose a number
    that's large enough for the Q-values to converge over the training. 100 wasn't
    large enough, but 1,000 was. Usually, you can just pick a very large number, for
    example, 5,000, and you will get convergence (that is, the Q-values will no longer
    update). However, that depends on the complexity of the problem. If you are dealing
    with a much more complex environment, for example, if you had hundreds of locations
    in the warehouse, you'd need a much higher number of training iterations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 1,000 次迭代中重复这些步骤。为什么是 1,000？选择 1,000 是因为我在这个特定环境中的实验结果。我选择了一个足够大的数字，使得
    Q 值能够在训练过程中收敛。100 次迭代不够大，但 1,000 次足够了。通常，你可以选择一个非常大的数字，例如 5,000，这样你就会看到收敛（即 Q
    值不再更新）。不过，这取决于问题的复杂性。如果你正在处理一个更复杂的环境，例如仓库中有数百个位置，你可能需要更多的训练迭代次数。
- en: That's the whole process. Now, you're going to implement it in Python from scratch!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是整个过程。现在，你将从零开始在 Python 中实现它！
- en: Are you ready? Let's do this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？我们开始吧。
- en: Implementation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Alright, let's smash this. But first, try to smash this yourself without me.
    Of course, this is a journey we'll take together, but I really don't mind if you
    take some steps ahead of me. The faster you become independent in AI, the sooner
    you'll do wonders with it. Try to implement the Q-learning process mentioned previously,
    exactly as it is. It's okay if you don't implement everything; what matters is
    that you try.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们来挑战这个。不过首先，试着自己先搞定它，别等我来。虽然这是我们一起走的旅程，但如果你能提前走几步，我也不介意。你在 AI 上变得独立的速度越快，你就越能发挥它的奇迹。试着按照之前提到的
    Q-learning 过程来实现，完全按照原样来。即使你没有实现所有内容也没关系，重要的是你要尝试。
- en: That's enough coaching; no matter how successful you were, let's go through
    the solution.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经是足够的指导了；不管你多么成功，还是让我们一起看看解决方案吧。
- en: 'First, start by importing the libraries that you''ll use in this implementation.
    There''s only one needed this time: the `numpy` library, which offers a practical
    way of working with arrays and mathematical operations. Give it the shortcut `np`.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，开始导入你将在这个实现中使用的库。这次只需要一个库：`numpy` 库，它提供了一种方便的方式来处理数组和数学运算。给它起个快捷名称 `np`。
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, set the parameters of your model. These include the discount factor *γ*
    and the learning rate ![](img/B14110_08_0311.png), which are the only parameters
    of the Q-learning model. Give them the values of `0.75` and `0.9` respectively,
    which I've arbitrarily picked but are usually a good choice. These are decent
    values to start with if you don't know what to use. However, you'll get the same
    result with similar values.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，设置模型的参数。这些包括折扣因子 *γ* 和学习率！[](img/B14110_08_0311.png)，它们是 Q-learning 模型的唯一参数。分别赋值为
    `0.75` 和 `0.9`，这两个值是我随便选的，但通常是一个不错的选择。如果你不知道该使用什么，这些值是一个不错的起点。不过，你也可以使用类似的值，结果会相同。
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The two previous code sections were simply the introductory sections, before
    you really start to build your AI model. The next step is to start the first part
    of our implementation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个代码部分只是引导部分，在你真正开始构建 AI 模型之前。下一步是开始我们实现的第一部分。
- en: Try to remember what you have to do now, as a first general step of building
    an AI.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试着记住你需要做的事情，这是构建 AI 的第一个通用步骤。
- en: You build the environment!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要构建环境！
- en: 'I just wanted to highlight that, once again; it''s really compulsory. The environment
    will be the first part of your code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是想再次强调一下，这真的很重要。环境将是你代码的第一部分：
- en: Part 1 – Building the environment
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一部分 – 构建环境
- en: 'Let''s look at the whole structure of this implementation so that you can take
    a step back already. Your code will be structured in three parts:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个实现的整体结构，这样你可以稍微退后一步。你的代码将分为三部分：
- en: '**Part 1** – Building the environment'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一部分** – 构建环境'
- en: '**Part 2** – Building the AI solution with Q-learning (training)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二部分** – 使用 Q 学习构建 AI 解决方案（训练）'
- en: '**Part 3** – Going into production (inference)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三部分** – 进入生产（推理）'
- en: 'Let''s start with part 1\. For that, you define the states, the actions, and
    the rewards. Begin by defining the states, with a Python dictionary mapping the
    location''s names (in letters from A to L) into the states (in indexes from 0
    to 11). Call this dictionary `location_to_state`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一部分开始。首先，定义状态、动作和奖励。首先定义状态，使用一个 Python 字典将位置名称（从 A 到 L 的字母）映射到状态（从 0 到
    11 的索引）。将这个字典命名为`location_to_state`：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, define the actions with a simple list of indexes from 0 to 11\. Remember
    that each action index corresponds to the next location where that action leads
    to:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，用一个简单的从 0 到 11 的索引列表定义动作。记住，每个动作索引对应于该动作引导到的下一个位置：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, define the rewards, by creating a matrix of rewards where the rows
    correspond to the current states ![](img/B14110_08_012.png), the columns correspond
    to the actions ![](img/B14110_08_013.png) leading to the next state , and the
    cells contain the rewards ![](img/B14110_08_015.png). If a cell contains a 1,
    that means the AI can perform the action ![](img/B14110_08_016.png) from the current
    state, ![](img/B14110_08_018.png) to reach the next state . If a cell ![](img/B14110_08_020.png)
    contains a 0, that means the AI cannot perform the action ![](img/B14110_08_021.png)
    from the current state ![](img/B14110_08_022.png) to reach any next state ![](img/B14110_08_023.png).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，定义奖励，通过创建一个奖励矩阵，其中行对应当前状态 ![](img/B14110_08_012.png)，列对应导致下一个状态的动作 ![](img/B14110_08_013.png)，单元格包含奖励
    ![](img/B14110_08_015.png)。如果一个单元格包含 1，意味着 AI 可以从当前状态 ![](img/B14110_08_018.png)
    执行动作 ![](img/B14110_08_016.png)，到达下一个状态 ![](img/B14110_08_020.png)。如果一个单元格 ![](img/B14110_08_023.png)
    包含 0，意味着 AI 无法从当前状态 ![](img/B14110_08_022.png) 执行动作 ![](img/B14110_08_021.png)
    到达任何下一个状态 ![](img/B14110_08_023.png)。
- en: Now, you might remember this very important question, the answer of which is
    at the heart of Reinforcement Learning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能记得这个非常重要的问题，它的答案是强化学习的核心。
- en: How will you let the AI know that it has to go to that top priority location
    **G**?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你将如何让 AI 知道它必须去那个优先级最高的位置 **G**？
- en: Everything works with the reward.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都与奖励相关。
- en: I must insist, again, that you remember this. If you attribute a high reward
    to location **G**, then the AI, through the Q-learning process, will learn to
    catch that high reward in the most efficient way because it is larger than the
    rewards of getting to the other locations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须再次强调，记住这一点。如果你给 **G** 位置分配一个高奖励，那么 AI 通过 Q 学习过程将学会以最有效的方式获取这个高奖励，因为它比到达其他位置的奖励要大。
- en: 'Remember this very important rule: the AI, when it is powered by Q-learning
    (or deep Q-learning, as you''ll soon learn), will always learn to reach the highest
    reward by the quickest route that does not penalize the AI with negative rewards.
    That''s why the trick to reach location **G** is simply to attribute it a higher
    reward than the other locations.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个非常重要的规则：当 AI 使用 Q 学习（或者你很快会学到的深度 Q 学习）时，它总是会学习通过最快的路径达到最高奖励，同时避免因负奖励而惩罚
    AI。这就是为什么达到 **G** 位置的诀窍仅仅是给它赋予比其他位置更高的奖励。
- en: Start by manually putting a high reward, which can be any high number as long
    as it is larger than 1, inside the cell corresponding to location **G**; location
    **G** is the top priority location where the robot has to go in order to collect
    the products.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先手动放入一个高奖励，任何大于 1 的数字都可以，只要它大于 1，放在对应 **G** 位置的单元格中；**G** 位置是机器人必须去的优先级最高的位置，以便收集产品。
- en: Since location **G** has encoded index state 6, put a `1000` reward in the cell
    of row 6 and column 6\. Later on, we will improve your solution by implementing
    an automatic way of going to the top priority location, without having to manually
    update the matrix of rewards and leaving it initialized with 0s and 1s just as
    it should be. For now, here's your matrix of rewards, including the manual update.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 **G** 位置的编码索引状态为 6，因而在第 6 行第 6 列的单元格中放入一个 `1000` 的奖励。稍后，我们将通过实现一种自动去往优先级最高位置的方法来改进你的解决方案，无需手动更新奖励矩阵，并且保持初始化为
    0 和 1，这正是它应有的样子。目前，这是你的奖励矩阵，包括手动更新部分。
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That completes this first part. Now, let's begin the second part of your implementation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了第一部分。现在，让我们开始第二部分的实现。
- en: Part 2 – Building the AI Solution with Q-learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二部分 – 使用Q学习构建AI解决方案
- en: To build your AI solution, follow the Q-learning algorithm exactly as it was
    provided previously. If you had any trouble when you tried implementing Q-learning
    on your own, now is your chance for revenge. Literally, all that's about to follow
    is only and exactly the same Q-learning process translated into code.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建你的AI解决方案，按照之前提供的Q学习算法逐步进行。如果你在自己实现Q学习时遇到了困难，现在是复仇的时刻了。接下来的一切都完全是之前的Q学习过程翻译成代码。
- en: Now you've got that in your mind, try coding it on your own again. You can do
    it!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经在脑海中理解了，试着再次独立编写代码吧。你能做到的！
- en: Congratulations if you tried, no matter how it came out. Next, let's check if
    you got it right.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你尝试了，不管结果如何。接下来，让我们检查一下你是否做对了。
- en: First, initialize all the Q-values by creating your matrix of Q-values full
    of 0s, in which the rows correspond to the current states ![](img/B14110_08_012.png),
    the columns correspond to the actions ![](img/B14110_08_013.png) leading to the
    next state ![](img/B14110_08_014.png), and the cells contain the Q-values ![](img/B14110_08_027.png).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过创建一个全是0的Q值矩阵来初始化所有的Q值，其中行对应当前状态 ![](img/B14110_08_012.png)，列对应到达下一个状态的动作
    ![](img/B14110_08_013.png)，单元格包含Q值 ![](img/B14110_08_027.png)。
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then implement the Q-learning process with a for loop over 1,000 iterations,
    repeating the exact same steps of the Q-learning process 1,000 times.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，用一个循环执行Q学习过程，进行1,000次迭代，重复Q学习过程中的每一个步骤1,000次。
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now you''ve reached the first really exciting step of the journey. You''re
    actually ready to launch the Q-learning process and get your final Q-values. Execute
    the whole code you''ve implemented so far, and visualize the Q-values with the
    following simple print statements:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经到达了旅程中的第一个真正令人兴奋的步骤。你实际上已经准备好启动Q学习过程并获得最终的Q值。执行你到目前为止实现的全部代码，并通过以下简单的打印语句来可视化Q值：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here''s what I got:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我得到的结果：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you''re working on Spyder in Anaconda, then for more visual clarity you
    can even check the matrix of Q-values directly in Variable Explorer, by double-clicking
    on Q. Then, to get the Q-values as integers, you can click on **Format** and enter
    a float formatting of `%.0f`. You get the following, which is a bit clearer since
    you can see the indexes of the rows and columns in your Q matrix:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Anaconda的Spyder中工作，那么为了更清晰地看到，你甚至可以直接在变量浏览器中查看Q值矩阵，只需双击Q。然后，要将Q值显示为整数，可以点击**格式**并输入浮动格式`%.0f`。这样你就会得到如下结果，这样更清晰，因为你可以看到Q矩阵中行和列的索引：
- en: '![](img/B14110_08_07.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_07.png)'
- en: 'Figure 7: Matrix of Q-values'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Q值矩阵
- en: Now that you have your matrix of Q-values, you're ready to go into production—you
    can move on to the third part of the implementation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了Q值矩阵，你可以进入生产阶段了——可以继续进行实现的第三部分。
- en: Part 3 – Going into production
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三部分 – 进入生产阶段
- en: In other words, you're going into inference mode! In this part, you'll compute
    the optimal path from any starting location to any ending top priority location.
    The idea here is to implement a `route` function, that takes as inputs a starting
    location and an ending location and that returns as output the shortest route
    inside a Python list. The starting location corresponds to wherever our autonomous
    warehouse robot is at a given time, and the ending location corresponds to where
    the robot has to go as a top priority.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你现在进入了推理模式！在这一部分，你将计算从任何起始位置到任何终极目标位置的最佳路径。这里的思路是实现一个`route`函数，它接受起始位置和终止位置作为输入，并返回一个包含最短路径的Python列表。起始位置对应我们自主仓库机器人在某一时刻的位置，终止位置对应机器人必须去的优先位置。
- en: 'Since you''ll want to input the locations with their names (in letters), as
    opposed to their states (in indexes), you''ll need a dictionary that maps the
    location states (in indexes) to the location names (in letters). That''s the first
    thing to do here in this third part, using a trick to invert your previous dictionary,
    `location_to_state`, since you simply want to get the exact inverse mapping from
    this dictionary:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你需要输入的是位置的名称（用字母表示），而不是位置的状态（用索引表示），你需要一个字典，将位置的状态（用索引表示）映射到位置的名称（用字母表示）。这是第三部分的第一个任务，使用一个技巧来反转你之前的字典`location_to_state`，因为你只需要从这个字典中获取精确的反向映射：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, please focus— if the dots haven't perfectly connected in your mind, now
    is the time when they will. I'll show you the exact steps of how the robot manages
    to figure out the shortest route.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请集中注意力——如果这些点在你的脑海中还没有完全连接，现在是它们连接的时候了。我将向你展示机器人是如何计算出最短路径的准确步骤。
- en: 'Your robot is going to go from location **E** to location **G**. Here''s the
    explanation of exactly how it does that—I''ll enumerate the different steps of
    the process. Follow along on the matrix of Q-values as I explain:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器人将从位置 **E** 移动到位置 **G**。以下是它是如何做到的解释——我将列出过程的不同步骤。请在我讲解时关注 Q 值矩阵：
- en: The AI starts at the starting location **E**.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 从起始位置 **E** 开始。
- en: The AI gets the state of location **E**, which according to your `location_to_state`
    mapping is ![](img/B14110_08_028.png).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 获得位置 **E** 的状态，根据你的 `location_to_state` 映射，这是 ![](img/B14110_08_028.png)。
- en: On the row of index ![](img/B14110_08_028.png) in our matrix of Q-values, the
    AI chooses the column that has the maximum Q-value (`703`).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的 Q 值矩阵的索引行 ![](img/B14110_08_028.png) 中，AI 选择具有最大 Q 值（`703`）的列。
- en: This column has index 8, so the AI performs the action of index 8, which leads
    it to the next state ![](img/B14110_08_030.png).
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该列的索引为 8，因此 AI 执行索引 8 的操作，这将引导它进入下一个状态 ![](img/B14110_08_030.png)。
- en: The AI gets the location of state 8, which according to our `state_to_location`
    mapping is location **I**. Since the next location is location **I**, **I** is
    appended to the AI's list containing the optimal path.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 获取状态 8 的位置，根据我们的 `state_to_location` 映射，这是位置 **I**。由于下一个位置是位置 **I**，**I**
    被添加到 AI 的包含最佳路径的列表中。
- en: Then, starting from the new location **I**, the AI repeats the same previous
    five steps until it reaches our final destination, location **G**.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从新位置 **I** 开始，AI 重复之前的五个步骤，直到到达最终目的地位置 **G**。
- en: 'That''s it! That''s exactly what you have to implement. You have to generalize
    this to any starting and ending locations, and the best way to do that is through
    a function taking two inputs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这正是你需要实现的内容。你需要将其通用化到任何起始和结束位置，最好的方法是通过一个接受两个输入的函数来实现：
- en: '`starting_location`: The location at which the AI starts'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`starting_location`: AI 启动的起始位置'
- en: '`ending_location`: The top priority location to which it has to go'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ending_location`: 它必须到达的最高优先级位置'
- en: and returning the optimal route. Since we're talking about a route, you can
    call that function `route()`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 并返回最佳路径。因为我们在谈论路径，你可以调用该函数 `route()`。
- en: An important thing to understand inside this `route()` function is that since
    you don't know how many locations the AI will have to go through between the starting
    and ending locations, you have to make a `while` loop which will repeat the 5-step
    process described previously, and that will stop as soon as it reaches the top
    priority end location.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `route()` 函数中，理解的一个重要点是，由于你不知道 AI 在起始位置和结束位置之间需要经过多少个位置，你必须创建一个 `while` 循环，重复之前描述的
    5 步骤，并在到达最高优先级的结束位置时停止。
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Congratulations! Your AI is now ready. Not only does it have the training process
    implemented, but also the code to run in inference mode. The only thing that's
    not great so far is that you still have to manually update the matrix of rewards;
    but no worries, we'll get to that later on. Before we get to that, let's first
    check that you have an intermediary victory here, and then we can get to work
    on improvements.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你的 AI 现在准备好了。它不仅实现了训练过程，还有推理模式下运行的代码。到目前为止，唯一不太完美的是你仍然需要手动更新奖励矩阵；但别担心，我们稍后会解决这个问题。在我们解决这个问题之前，先检查一下你是否已经在这里获得了阶段性的胜利，然后我们可以继续进行改进。
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That''s perfect—I ran the code twice when testing it to go from E to G, which
    is why you see the two preceding outputs. The two possible optimal paths were
    returned: one passing by F, and the other one passing by K.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 完美——在测试时，我运行了两次代码从 E 到 G，这就是为什么你会看到前两个输出的原因。返回了两条可能的最佳路径：一条经过 F，另一条经过 K。
- en: That's a good start. You have a first version of your AI model that functions
    well. Now let's improve your AI, and take it to the next level.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个良好的开端。你有了第一版运作良好的 AI 模型。现在，让我们改进你的 AI，将它提升到更高的水平。
- en: You can improve the AI in two ways. Firstly, by automating the reward attribution
    to the top priority location so that you don't have to do it manually. Secondly,
    by adding a feature that gives the AI the option to go by an intermediate location
    before going to the top priority location—that intermediate location should be
    in the top three priority locations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过两种方式改进人工智能。首先，通过自动化奖励分配到最高优先级的位置，这样你就不必手动进行操作。其次，添加一个功能，让人工智能在前往最高优先级位置之前可以选择经过一个中间位置——该中间位置应当位于前三个优先级位置之内。
- en: In our top priority locations ranking, the second top priority location is location
    K. Therefore, in order to optimize the warehouse flows, your autonomous warehouse
    robot must go via location K to collect products on its way to the top priority
    location G. One way to do this is to have the option to go by an intermediate
    location in the process of your `route()` function. This is exactly what you'll
    implement as a second improvement.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最高优先级位置排名中，第二高优先级的位置是位置K。因此，为了优化仓库流动，你的自动仓库机器人必须经过位置K，收集产品，然后再前往最高优先级位置G。实现这一目标的一种方法是，在`route()`函数的过程中选择经过一个中间位置。这正是你将作为第二个改进来实现的内容。
- en: First, let's implement the first improvement, the one that automates the reward
    attribution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实现第一个改进，即自动化奖励分配。
- en: Improvement 1 – Automating reward attribution
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改进1——自动化奖励分配
- en: The way to do this is in three steps.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的方法分为三个步骤。
- en: '**Step 1**: Go back to the original matrix of rewards, as it was before with
    only 1s and 0s. Part 1 of the code becomes the following, and will be included
    in the final code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步**：返回到最初的奖励矩阵，像之前那样只包含1和0。代码的第1部分变成如下，并将被包含在最终代码中：'
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 2**: In part 2 of the code, make a copy (call it `R_new`) of your rewards
    matrix, inside which the `route()` function can automatically update the reward
    in the cell of the ending location.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2步**：在代码的第2部分，复制一份你的奖励矩阵（称之为`R_new`），在其中`route()`函数可以自动更新结束位置单元格的奖励。'
- en: 'Why do you have to make a copy? Because you have to keep the original matrix
    of rewards initialized with 1s and 0s for future modifications when you want to
    go to a new priority location. So, how will the `route()` function automatically
    update the reward in the cell of the ending location? That''s an easy one: since
    the ending location is one of the inputs of the `route()` function, then by using
    your `location_to_state` dictionary, you can very easily find that cell and update
    its reward to `1000`. Here''s how you do that:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么需要复制？因为你必须保持原始奖励矩阵初始化为1和0，以便将来在你需要前往新的优先级位置时进行修改。那么，`route()`函数如何自动更新结束位置单元格的奖励呢？这个很简单：由于结束位置是`route()`函数的输入之一，因此通过使用你的`location_to_state`字典，你可以非常容易地找到该单元格并将其奖励更新为`1000`。下面是如何实现的：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step 3**: You must include the whole Q-learning algorithm (including the
    initialization step) inside the `route()` function, right after we make that update
    of the reward in your copy (`R_new`) of the rewards matrix. In your previous implementation,
    the Q-learning process happened on the original version of the rewards matrix.
    Now that original version needs to stay as it is, that is, initialized to 1s and
    0s only. Therefore, you must include the Q-learning process inside the `route()`
    function, and make it happen on your copy of the rewards matrix `R_new`, instead
    of the original rewards matrix `R`. Here''s how you do that:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3步**：你必须将整个Q学习算法（包括初始化步骤）包含在`route()`函数中，在我们更新奖励矩阵复制（`R_new`）中的奖励后执行。在你之前的实现中，Q学习过程是在原始奖励矩阵上进行的。现在，原始版本需要保持原样，即仅初始化为1和0。因此，你必须将Q学习过程包含在`route()`函数中，并使其在奖励矩阵的复制`R_new`上执行，而不是原始奖励矩阵`R`。下面是如何实现的：'
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Perfect; part 2 is now ready! Here''s part 2 of the final code in full:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 完美；第2部分已经准备好！以下是第2部分的最终完整代码：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If you execute this new code several times with the start and end points of
    **E** and **G**, you'll get the same two possible optimal paths as before. You
    can also play around with the `route()` function and try out different starting
    and ending points. Try it out!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你多次执行这个新代码，起点和终点分别为**E**和**G**，你将得到与之前相同的两个可能的最优路径。你还可以尝试`route()`函数，尝试不同的起点和终点。试试看吧！
- en: Improvement 2 – Adding an intermediate goal
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改进2——添加中间目标
- en: Now, let's tackle the second improvement. There are three possible solutions
    to the problem of adding the option to go by the intermediate location **K**,
    the second top priority location. When you see them, you'll understand what I
    meant when I told you that everything in Reinforcement Learning works by the rewards.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来解决第二个改进问题。添加一个通过中间位置**K**的选项问题有三种可能的解决方案。当你看到它们时，你会明白我之前说过的“强化学习的一切都是通过奖励来工作的”是什么意思。
- en: 'Only one of the solutions works from every starting point, but I''d like to
    give you all three solutions to help reinforce your intuition. To help with that,
    here''s a reminder of our warehouse layout:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个解决方案适用于每一个起始点，但我想给你三个解决方案，以帮助你加深直觉。为了帮助你理解，这里是我们的仓库布局的提醒：
- en: '![](img/B14110_08_08.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_08_08.png)'
- en: 'Figure 8: Locations in the warehouse'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：仓库中的位置
- en: '**Solution 1**: Give a high reward to the action leading from location **J**
    to location **K**. This high reward must be larger than 1, and below 1,000\. It
    must be larger than 1 so that the Q-learning process favors the action leading
    from **J** to **K**, as opposed to the action leading from **J** to **F**, which
    has a reward of 1\. It must also be below 1,000 so that the highest reward stays
    on the top priority location, to make sure the AI ends up there. For example,
    in your rewards matrix you can give a high reward of `500` to the cell in the
    row of index 9 and the column of index 10, since that cell corresponds to the
    action leading from location **J** (state index 9) to location **K** (state index
    10). That way, your AI robot will always go by location **K** when going from
    location **E** to location **G**. Here''s how the matrix of rewards would look
    in that case:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案 1**：给从位置**J**到位置**K**的动作一个高奖励。这个高奖励必须大于1，且小于1,000。它必须大于1，这样Q学习过程才会偏向于从**J**到**K**的动作，而不是从**J**到**F**的动作，因为后者的奖励为1。它还必须小于1,000，这样最高的奖励才能保持在最高优先级的位置，确保AI最终到达那里。例如，在你的奖励矩阵中，你可以给第9行第10列的单元格一个高奖励`500`，因为该单元格对应的是从位置**J**（状态索引9）到位置**K**（状态索引10）的动作。这样，你的AI机器人在从位置**E**到位置**G**时，就会始终经过位置**K**。在这种情况下，奖励矩阵的样子应该是：'
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This solution does not work in every case, and actually only works for starting
    points **E**, **I**, and **J**. That's because the `500` weight can only affect
    the decision of the AI as to whether or not it should go from **J** to **K**;
    it doesn't change how likely it is for the AI to go to **J** in the first place.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案并不适用于所有情况，实际上它只适用于起始点**E**、**I**和**J**。这是因为`500`的权重只能影响AI是否应该从**J**到**K**的决策；它不会改变AI在最初选择是否去**J**的可能性。
- en: '**Solution 2**: Give a bad reward to the action leading from location **J**
    to location **F**. This bad reward just has to be below 0\. By punishing this
    action with a bad reward, the Q-learning process will never favor the action leading
    from **J** to **F**. For example, in your rewards matrix, you can give a bad reward
    of `-500` to the cell in the row of index 9 and the column of index 5, since that
    cell corresponds to the action leading from location **J** (state index 9) to
    location **F** (state index 5). That way, your autonomous warehouse robot will
    never go from location **J** to location **F** on its way to location **G**. Here''s
    how the matrix of rewards would look in that case:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案 2**：给从位置**J**到位置**F**的动作一个差奖励。这个差奖励必须小于0。通过惩罚这个动作，Q学习过程就不会偏向于从**J**到**F**的动作。例如，在你的奖励矩阵中，你可以给第9行第5列的单元格一个差奖励`-500`，因为该单元格对应的是从位置**J**（状态索引9）到位置**F**（状态索引5）的动作。这样，你的自动化仓库机器人在前往位置**G**时，就永远不会从位置**J**到位置**F**。在这种情况下，奖励矩阵的样子应该是：'
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This solution does not work in every case, and actually only works for starting
    points **E**, **I**, and **J**. Just as in solution 1, that's because the `-500`
    weight can only affect the decision of the AI as to whether or not it should go
    from **J** to **F**; it doesn't change how likely it is for the AI to go to **J**
    in the first place.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案并不适用于所有情况，实际上它只适用于起始点**E**、**I**和**J**。和解决方案1一样，这是因为`-500`的权重只能影响AI是否应该从**J**到**F**的决策；它不会改变AI最初选择是否去**J**的可能性。
- en: '**Solution 3**: Make an additional `best_route()` function, taking as inputs
    the three starting, intermediary, and ending locations, which will call your previous
    `route()` function twice; the first time from the starting location to the intermediary
    location, and a second time from the intermediary location to the ending location.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**方案 3**：新增一个 `best_route()` 函数，接受三个输入：起始地点、中介地点和结束地点，该函数将调用你之前的 `route()`
    函数两次；第一次从起始地点到中介地点，第二次从中介地点到结束地点。'
- en: The first two solutions are easy to implement manually, but tricky to implement
    automatically. It is easy to automatically get the index of the intermediary location
    via which you want the AI to go, but it's difficult to get the index of the location
    that leads to that intermediary location, since it depends on the starting location
    and ending location. If you try to implement either the first or second solution,
    you'll see what I mean. Besides, solutions 1 and 2 do not work as global solutions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个方案可以手动轻松实现，但自动实现起来则比较棘手。你可以轻松地自动获得中介地点的索引，但很难得到导致 AI 到达该中介地点的路径，因为它依赖于起点和终点。如果你尝试实现第一个或第二个方案，你就会明白我的意思。此外，方案
    1 和方案 2 并不能作为全局解决方案。
- en: Only solution 3 guarantees that the AI will visit an intermediate location before
    going to the final location.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 只有方案 3 能保证 AI 在前往最终地点之前会经过一个中介地点。
- en: 'Accordingly, we''ll implement solution 3, which can be coded in just two extra
    lines of code, and which I included in *Part 3 – Going into production*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将实现方案 3，该方案只需额外增加两行代码，我已在 *第三部分 – 进入生产阶段* 中包含了该代码：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Easy, right? Sometimes, the best solutions are the simplest ones. That''s definitely
    the case here. As you can see, included in Part 3 is the code that runs the ultimate
    test. This test will be successful if the AI goes through location **K** while
    taking the shortest route from location **E** to location **G**. To test it, execute
    this whole new code as many times as you want; you''ll always get the same, expected
    output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，对吧？有时候，最佳的解决方案往往是最简单的。这里的情况就是如此。如你所见，第三部分包括了运行最终测试的代码。如果 AI 能在从地点 **E**
    到地点 **G** 的最短路径上经过 **K** 地点，那么这个测试就会成功。你可以多次执行这段全新的代码，你总会得到相同的预期输出：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Congratulations! You've developed a fully functional AI, powered by Q-learning,
    which solves an optimization problem for logistics. Using this AI robot, we can
    now go from any location to any new top priority location, while optimizing our
    paths to collect products in a second priority intermediary location. Not bad!
    If you get bored with logistics, feel free to imagine yourself back in the maze,
    and try the `best_route()` function with whatever starting and ending points you
    would like, so you can see how flexible the AI you've created is. Have fun with
    it! And, of course, you have the full code available for you on the GitHub page.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经开发出了一个完全功能的 AI，采用 Q 学习算法，能够解决物流优化问题。通过这个 AI 机器人，我们现在可以从任何地点出发，前往任何新的最高优先级地点，同时优化路径以便在第二优先级的中介位置收集产品。做得不错！如果你对物流有些厌倦，可以随时想象自己回到迷宫中，并使用
    `best_route()` 函数，随意设置起点和终点，看看你创建的 AI 有多么灵活。玩得开心！当然，你也可以在 GitHub 页面上获取完整的代码。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you've implemented a Q-learning solution to a business problem.
    You had to find the best route to a certain location in your warehouse. Not only
    have you done that, but you've also implemented additional code that allowed your
    AI to make as many intermediary stops as you wanted. Based on the obtained rewards,
    your AI was able to find the best route going through these stops. That was Q-learning
    for warehouse robots. Now, let's move on to deep Q-learning!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你实现了一个 Q 学习解决方案，用于解决业务问题。你需要找到通往仓库某个位置的最佳路线。不仅如此，你还实现了额外的代码，允许你的 AI 按照需要进行多个中介停靠点。基于获得的奖励，AI
    能够找到经过这些停靠点的最佳路线。这就是用于仓库机器人的 Q 学习方法。接下来，让我们进入深度 Q 学习！
