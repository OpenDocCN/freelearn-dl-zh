- en: Application of Deep Learning in Text Mining
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在文本挖掘中的应用
- en: 'The current chapter we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Performing preprocessing of textual data and extraction of sentiments
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行文本数据的预处理和情感提取
- en: Analyzing documents using tf-idf
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用tf-idf分析文档
- en: Performing sentiment prediction using LSTM network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM网络进行情感预测
- en: Application using text2vec examples
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用text2vec示例的应用
- en: Performing preprocessing of textual data and extraction of sentiments
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行文本数据的预处理和情感提取
- en: In this section, we will use Jane Austen's bestselling novel Pride and Prejudice,
    published in 1813, for our textual data preprocessing analysis. In R, we will
    use the `tidytext` package by Hadley Wickham to perform tokenization, stop word
    removal, sentiment extraction using predefined sentiment lexicons, **term frequency
    - inverse document frequency** (**tf-idf**) matrix creation, and to understand
    pairwise correlations among *n*-grams.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用简·奥斯汀的畅销小说《傲慢与偏见》（1813年出版）进行文本数据预处理分析。在R中，我们将使用Hadley Wickham的`tidytext`包进行分词、去除停用词、使用预定义的情感词典进行情感提取、**词频-逆文档频率**（**tf-idf**）矩阵创建，并理解*n*-grams之间的配对相关性。
- en: In this section, instead of storing text as a string or a corpus or a **document
    term matrix** (**DTM**), we process them into a tabular format of one token per
    row.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们不将文本存储为字符串、语料库或**文档词频矩阵**（**DTM**），而是将其处理成每行一个标记的表格格式。
- en: How to do it...
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Here is how we go about preprocessing:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们进行预处理的步骤：
- en: 'Load the required packages:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的包：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the `Pride and Prejudice` dataset. The `line_num` attribute is analogous
    to the line number printed in the book:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载《傲慢与偏见》数据集。`line_num`属性与书中打印的行号相似：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, perform tokenization to restructure the one-string-per-row format to a
    one-token-per-row format. Here, the token can refer to a single word, a group
    of characters, co-occurring words (*n*-grams), sentences, paragraphs, and so on.
    Currently, we will tokenize sentence into singular words:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，执行分词操作，将每行一个字符串的格式重新构建为每行一个标记的格式。这里，标记可以是单个单词、一组字符、共现词（*n*-grams）、句子、段落等。目前，我们将句子分词为单个单词：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, remove the commonly occurring words such as *the*, *and*, *for*, and
    so on using the `stop words` removal corpus:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用`stop words`去除语料库去除常见词，如*the*、*and*、*for*等：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Extract the most common textual words used:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取最常用的文本单词：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Visualize the top 10 common occurring words, as shown in the following figure:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化最常出现的前10个词，如下图所示：
- en: '![](img/00139.gif)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00139.gif)'
- en: Top 10 common words
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 前10个常见词
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, extract sentiments at a higher level (that is positive or negative) using
    the `bing` lexicon.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用`bing`词典提取更高层次的情感（即正面或负面）。
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Visualize the sentiments across small sections (150 words) of text, as shown
    in the following figure:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化文本小节（150个词）中的情感变化，如下图所示：
- en: '![](img/00088.gif)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00088.gif)'
- en: Distribution of the number of positive and negative words across sentences of
    150 words each
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每个150个词的句子中正面和负面词的分布
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now extract sentiments at a granular level (namely positive, negative, anger,
    disgust, surprise, trust, and so on.) using the `nrc` lexicon:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用`nrc`词典提取更细粒度的情感（即正面、负面、愤怒、厌恶、惊讶、信任等）：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Visualize the variation across different sentiments defined, as shown in the
    following figure:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化不同情感定义的变化，如下图所示：
- en: '![](img/00121.gif)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.gif)'
- en: Variation across different types of sentiments
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型情感的变化
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Extract the most occurring positive and negative words based on the `bing`
    lexicon, and visualize them as shown in the following figure:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于`bing`词典提取最常出现的正面和负面词，并如图所示可视化它们：
- en: '![](img/00015.gif)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.gif)'
- en: Top 10 positive and negative words in the novel Pride and Prejudice
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 《傲慢与偏见》中正面和负面词的前10名
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Generate a sentiment word cloud as shown in the following figure:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成如下图所示的情感词云：
- en: '![](img/00127.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpeg)'
- en: Word cloud of positive and negative words
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正面和负面词的词云
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now analyze sentiments across the chapters of the book:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在分析整本书各章节的情感：
- en: 'Extract the chapters, and perform tokenization:'
  id: totrans-46
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取章节并执行分词：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Extract the set `positive` and `negative` words from the `bing` lexicon:'
  id: totrans-48
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`bing`词典中提取`positive`和`negative`词集：
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Get the count of words for each chapter:'
  id: totrans-50
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每章的词频：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Extract the ratio of positive and negative words:'
  id: totrans-52
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取正面和负面词的比例：
- en: '[PRE15]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Generate a sentiment flag for each chapter based on the proportion of positive
    and negative words:'
  id: totrans-54
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于每章正面和负面词汇的比例，为每章生成情感标志：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As mentioned earlier, we have used Jane Austen's famous novel *Pride and Prejudice*
    in this section, detailing the steps involved in tidying the data, and extracting
    sentiments using (publicly) available lexicons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本节使用了简·奥斯汀的著名小说《傲慢与偏见》，详细介绍了数据整理的步骤，并使用（公开的）词典提取情感。
- en: Steps 1 and 2 show the loading of the required `cran` packages and the required
    text. Steps 3 and 4 perform unigram tokenization and stop word removal. Steps
    5 and 6 extract and visualize the top 10 most occurring words across all the 62
    chapters. Steps 7 to 12 demonstrate high and granular-level sentiments using two
    widely used lexicons `bing` and `nrc`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1和步骤2显示了所需的`cran`包和所需文本的加载。步骤3和步骤4执行 unigram 分词和停用词移除。步骤5和步骤6提取并可视化所有62章中出现次数最多的前10个单词。步骤7到12展示了使用两个广泛使用的情感词典`bing`和`nrc`进行的高层次和细粒度情感分析。
- en: Both the lexicons contains a list of widely used English words that are tagged
    to sentiments. In `bing`, each word is tagged to one of the high level binary
    sentiments (positive or negative), and in `nrc`, each word is tagged to one of
    the granular-level multiple sentiments (positive, negative, anger, anticipation,
    joy, fear, disgust, trust, sadness, and surprise).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个词典都包含了一些广泛使用的英文单词，并将其标记为不同的情感。在`bing`词典中，每个单词都被标记为高层次的二元情感（积极或消极），而在`nrc`词典中，每个单词都被标记为细粒度的多种情感之一（积极、消极、愤怒、期待、快乐、恐惧、厌恶、信任、悲伤和惊讶）。
- en: Each 150-word-long sentence is tagged to a sentiment, and the same has been
    shown in the figure showing the *Distribution of number of positive and negative
    words across sentences of 150 words each*. In step 13, chapter-wise sentiment
    tagging is performed using maximum occurrence of positive or negative words from
    the `bing` lexicon. Out of 62 chapters, 52 have more occurrences of positive lexicons,
    and 10 have more occurrences of negative lexicons.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个150个单词的句子都被标记为一个情感，并且这一点已在图中展示，图中显示了*每个150个单词的句子中正面和负面词汇的分布*。在步骤13中，使用`bing`词典中正面或负面词汇的最大出现频率对每一章进行情感标记。在62章中，有52章正面词汇出现次数更多，而10章负面词汇出现次数更多。
- en: Analyzing documents using tf-idf
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 tf-idf 分析文档
- en: In this section, we will learn how to analyze documents quantitatively. A simple
    way is to look at the distribution of unigram words across the document and their
    frequency of occurrence, also termed as **term frequency** (**tf**). The words
    with higher frequency of occurrence generally tend to dominate the document.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何定量分析文档。一种简单的方法是查看文档中 unigram 单词的分布及其出现频率，也称为**词频**（**tf**）。出现频率较高的单词通常会主导文档内容。
- en: However, one would disagree in case of generally occurring words such as the,
    is, of, and so on. Hence, these are removed by stop word dictionaries. Apart from
    these stop words, there might be some specific words that are more frequent with
    less relevance. Such kinds of words are penalized using their **inverse document
    frequency** (**idf**) values. Here, the words with higher frequency of occurrence
    are penalized.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于像“the”、“is”、“of”等常见单词，人们往往会有所不同的看法。因此，这些词会通过停用词字典被移除。除此之外，可能还有一些特定的单词，它们频繁出现但相关性较低。这类单词会通过其**逆文档频率**（**idf**）值进行惩罚。这里，出现频率较高的单词会被惩罚。
- en: The statistic tf-idf combines these two quantities (by multiplication) and provides
    a measure of importance or relevance of each word for a given document across
    multiple documents (or a corpus).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf统计量结合了这两个量（通过乘法），并提供了一个衡量给定文档在多个文档（或语料库）中每个单词的重要性或相关性的标准。
- en: In this section, we will generate a tf-idf matrix across chapters of the book
    *Pride and Prejudice*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将生成《傲慢与偏见》一书中各章节的 tf-idf 矩阵。
- en: How to do it...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Here is how we go about analyzing documents using tf-idf:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用 tf-idf 分析文档：
- en: Extract the text of all 62 chapters in the book `Pride and Prejudice`. Then,
    return chapter-wise occurrence of each word. The total words in the book are approx
    1.22M.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取《傲慢与偏见》一书中所有62章的文本。然后，返回每个单词按章节出现的次数。该书的总词汇量约为1.22M。
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Calculate the rank of words such that the most frequently occurring words have
    lower ranks. Also, visualize the term frequency by rank, as shown in the following
    figure:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算单词的排名，使得出现频率较高的单词排名较低。同时，按排名可视化词频，如下图所示：
- en: '![](img/00016.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: This figure shows lower ranks for words with higher term-frequency (ratio) value
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了具有较高词频（比率）值的单词的排名较低
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Calculate the `tf-idf` value for each word using the `bind_tf-idf` function:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`bind_tf-idf`函数计算每个单词的`tf-idf`值：
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Extract and visualize the top 15 words with higher values of tf-idf, as shown
    in the following figure:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取并可视化tf-idf值较高的前15个单词，如下图所示：
- en: '![](img/00018.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00018.jpeg)'
- en: tf-idf values of top 15 words
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: tf-idf值的前15个单词
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As mentioned earlier, one can observe that the tf-idf scores of very common
    words such as *the* are close to zero and those of fewer occurrence words such
    as the proper noun *Austen* is close to one.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，可以观察到，非常常见的单词，如*the*，其tf-idf值接近零，而出现较少的单词，如专有名词*Austen*，其tf-idf值接近一。
- en: Performing sentiment prediction using LSTM network
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM网络进行情感预测
- en: In this section, we will use LSTM networks to perform sentiment analysis. Along
    with the word itself, the LSTM network also accounts for the sequence using recurrent
    connections, which makes it more accurate than a traditional feed-forward neural
    network.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用LSTM网络进行情感分析。与传统的前馈神经网络不同，LSTM网络不仅考虑单词本身，还通过递归连接考虑序列，这使得它比传统的神经网络更准确。
- en: Here, we shall use the `movie reviews` dataset `text2vec` from the `cran` package.
    This dataset consists of 5,000 IMDb movie reviews, where each review is tagged
    with a binary sentiment flag (positive or negative).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用`cran`包中的`movie reviews`数据集`text2vec`。该数据集包含5000条IMDb电影评论，每条评论都标记有二元情感标志（正面或负面）。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here is how you can proceed with sentiment prediction using LSTM:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何使用LSTM进行情感预测的方法：
- en: 'Load the required packages and movie reviews dataset:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的包和电影评论数据集：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Extract the movie reviews and labels as a dataframe and matrix respectively.
    In movie reviews, add an additional attribute `"Sno"` denoting the review number.
    In the labels matrix, add an additional attribute related to `negative flag`.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取电影评论和标签，分别作为数据框和矩阵。在电影评论中，添加一个额外的属性`"Sno"`表示评论编号。在标签矩阵中，添加与`negative flag`相关的附加属性。
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Extract all the unique words across the reviews, and get their count of occurrences
    (*n*). Also, tag each word with a unique integer (`orderNo`). Thus, each word
    is encoded using a unique integer, which shall be later used in the LSTM network.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取所有评论中的独特单词，并获取它们的出现次数（*n*）。同时，给每个单词标记一个唯一整数（`orderNo`）。因此，每个单词都使用唯一整数进行编码，之后将用于LSTM网络。
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, assign the tagged words back to the reviews based on their occurrences:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，根据单词的出现情况将标记的单词重新分配给评论：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Using the outcome of step 4, create a list of reviews with each review transformed
    into a set of encoded numbers representing those words:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第4步的结果，创建一个评论列表，将每条评论转换为表示单词的编码数字集合：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In order to facilitate equal-length sequences to the LSTM network, let's restrict
    the review length to 150 words. In other words, reviews longer than 150 words
    will be truncated to the first 150, whereas shorter reviews will be made 150 words
    long by prefixing with the required number of zeroes. Thus, we now add in a new
    word **0**.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便将等长序列输入LSTM网络，我们将限制评论长度为150个单词。换句话说，超过150个单词的评论将被截断为前150个单词，而短于150个单词的评论将通过在前面添加必要数量的零填充为150个单词。因此，我们现在添加一个新的单词**0**。
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now split the 5,000 reviews into training and testing reviews using a 70:30
    split ratio. Also, bind the list of train and test reviews row wise into a matrix
    format, with rows representing reviews and columns representing the position of
    a word:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将这5000条评论按70:30的比例拆分为训练集和测试集。同时，将训练集和测试集评论按行合并成矩阵格式，行表示评论，列表示单词的位置：
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Similarly, also split the labels into train and test accordingly:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样地，也将标签根据情况拆分为训练集和测试集：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Reset the graph, and start an interactive TensorFlow session:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置图，并启动交互式TensorFlow会话：
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define model parameters such as size of input pixels (`n_input`), step size
    (`step_size`), number of hidden layers (`n.hidden`), and number of outcome classes
    (`n.classes`):'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型参数，如输入像素的大小（`n_input`）、步长（`step_size`）、隐藏层的数量（`n.hidden`）和输出类别的数量（`n.classes`）：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define training parameters such as learning rate (`lr`), number of inputs per
    batch run (`batch`), and number of iterations (`iteration`):'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练参数，如学习率(`lr`)、每批次输入的数量(`batch`)和迭代次数(`iteration`)：
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Based on the RNN and LSTM functions defined in [Chapter 6](part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Recurrent Neural Networks,* from the section *Run the optimization post initializing
    a session using global variables initializer*.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于[第6章](part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab)《*循环神经网络*》中定义的RNN和LSTM函数，来自《*使用全局变量初始化器运行优化*》部分。
- en: '[PRE32]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the reduction in training errors across iterations as shown in the following
    figure:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练误差在各次迭代中的减少情况，如下图所示：
- en: '![](img/00118.gif)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00118.gif)'
- en: Distribution of sentiment prediction error of training dataset
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集的情感预测误差分布
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Get the error of test data:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取测试数据的误差：
- en: '[PRE34]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In steps 1 to 8, the movie reviews dataset is loaded, processed, and transformed
    into a set of train and test matrices, which can be directly used to train an
    LSTM network. Steps 9 to 14 are used to run LSTM using TensorFlow, as described
    in [Chapter 6](part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab), *Recurrent
    Neural Networks*. The figure *Distribution of sentiment prediction error of training
    dataset* shows the decline in training errors across 500 iterations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1到第8步中，加载、处理并转换电影评论数据集为一组训练和测试矩阵，可以直接用于训练LSTM网络。第9到第14步用于运行使用TensorFlow的LSTM，如[第6章](part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab)《*循环神经网络*》中所描述。图表《*训练数据集情感预测误差分布*》显示了在500次迭代中训练误差的下降。
- en: Application using text2vec examples
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用text2vec示例的应用
- en: In this section, we will analyze the performance of logistic regression on various
    examples of `text2vec`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析逻辑回归在各种`text2vec`示例中的性能。
- en: How to do it...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here is how we apply `text2vec`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何应用`text2vec`的方式：
- en: 'Load the required packages and dataset:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的包和数据集：
- en: '[PRE35]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Function to perform Lasso logistic regression, and return the train and test
    `AUC` values:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行Lasso逻辑回归的函数，并返回训练和测试的`AUC`值：
- en: '[PRE36]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Split the movies review data into train and test in an 80:20 ratio:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将电影评论数据按80:20比例划分为训练集和测试集：
- en: '[PRE37]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Generate a DTM of all vocabulary words (without any stop word removal), and
    asses its performance using Lasso logistic regression:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成所有词汇词的DTM（不去除任何停用词），并使用Lasso逻辑回归评估其性能：
- en: '[PRE38]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Perform pruning using a list of stop words, and then assess the performance
    using Lasso logistic regression:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用停用词列表进行修剪，然后使用Lasso逻辑回归评估性能：
- en: '[PRE39]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Generate a DTM using *n*-grams (uni and bigram words), and then assess the
    performance using Lasso logistic regression:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*n*-gram（单词单元和二元词组）生成DTM，然后使用Lasso逻辑回归评估性能：
- en: '[PRE40]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Perform feature hashing, and then asses the performance using Lasso logistic
    regression:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行特征哈希，然后使用Lasso逻辑回归评估性能：
- en: '[PRE41]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using tf-idf transformation on full vocabulary DTM, assess the performance
    using Lasso logistic regression:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完整词汇DTM上使用tf-idf转换，使用Lasso逻辑回归评估性能：
- en: '[PRE42]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How it works...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Steps 1 to 3 loads necessary packages, datasets, and functions required to assess
    different examples of `text2vec`. Logistic regression is implemented using the
    `glmnet` package with L1 penalty (Lasso regularization). In step 4, a DTM is created
    using all the vocabulary words present in the train movie reviews, and the test
    `auc` value is 0.918\. In step 5, the train and test DTMs are pruned using stop
    words and frequency of occurrence.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第1到第3步加载评估不同`text2vec`示例所需的必要包、数据集和函数。逻辑回归使用`glmnet`包实现，并采用L1惩罚（Lasso正则化）。在第4步，使用训练集中的所有词汇创建DTM，测试`auc`值为0.918。在第5步，通过停用词和词频修剪训练和测试DTM。
- en: The test `auc` value is observed as 0.916, not much decrease compared to using
    all the vocabulary words. In step 6, along with single words (or uni-grams), bi-grams
    are also added to the vocabulary. The test `auc` value increases to 0.928\. Feature
    hashing is then performed in step 7, and the test `auc` value is 0.895\. Though
    the `auc` value reduced, hashing is meant to improve run-time performance of larger
    datasets. Feature hashing is widely popularized by Yahoo. Finally, in step 8,
    we perform tf-idf transformation, which returns a test `auc` value of 0.907.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 测试`auc`值观察到为0.916，与使用所有词汇时相比没有太大下降。在第6步，除了单个词（或单元语法），还将二元语法（bi-grams）添加到词汇中。测试`auc`值增加到0.928。接着，在第7步进行特征哈希，测试`auc`值为0.895。尽管`auc`值有所降低，但哈希旨在提高大数据集的运行时性能。特征哈希是由Yahoo广泛推广的。最后，在第8步，进行tf-idf转换，返回测试`auc`值为0.907。
