- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: What to Do If the System Isn’t Working
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果系统不工作该怎么办
- en: In this chapter, we will discuss how to improve systems. If the original model’s
    first round of training fails to produce a satisfactory performance or the real-world
    scenario that the system addresses undergoes changes, we need to modify something
    to enhance the system’s performance. In this chapter, we will discuss techniques
    such as adding new data and changing the structure of an application, while at
    the same time ensuring that new data doesn’t degrade the performance of the existing
    system. Clearly, this is a big topic, and there is a lot of room to explore how
    to improve the performance of **natural language understanding** (**NLU**) systems.
    It isn’t possible to cover all the possibilities here, but this chapter should
    give you a good perspective on the most important options and techniques that
    can improve system performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何改进系统。如果原始模型的第一轮训练未能产生令人满意的性能，或者系统所解决的真实世界场景发生变化，我们需要修改一些内容以增强系统的性能。在本章中，我们将讨论如添加新数据和更改应用程序结构的技术，同时确保新数据不会降低现有系统的性能。显然，这是一个大课题，我们有很多探索提高**自然语言理解**（**NLU**）系统性能的空间。在这里不可能涵盖所有可能性，但本章应该能给您一个关于最重要的可以提高系统性能的选项和技术的良好视角。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: Figuring out that a system isn’t working
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弄清楚系统不起作用
- en: Fixing accuracy problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复准确性问题
- en: Moving on to deployment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入部署阶段
- en: Problems after deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署后的问题
- en: The first step is to find out that a system isn’t working as well as desired.
    This chapter will include a number of examples of tools that can help with this.
    We will start by listing the software requirements needed to run these examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是弄清楚系统是否运行得不如预期。本章将包括一些有助于此的工具示例。我们将从列出运行这些示例所需的软件要求开始。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following data and software to run the examples in this
    chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下数据和软件来运行本章的示例：
- en: Our usual development environment – that is, Python 3 and Jupyter Notebook
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通常的开发环境——即Python 3和Jupyter Notebook
- en: The TREC dataset
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TREC数据集
- en: The Matplotlib and Seaborn packages, which we will use to display graphical
    charts
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用于显示图表的Matplotlib和Seaborn包
- en: pandas and NumPy for numerical manipulation of data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于数据的数值操作的pandas和NumPy
- en: The BERT NLU system, previously used in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193)
    and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以前在[*第11章*](B19005_11.xhtml#_idTextAnchor193)和[*第13章*](B19005_13.xhtml#_idTextAnchor226)中使用的BERT自然语言理解系统
- en: The Keras machine learning library, for working with BERT
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于与BERT一起工作的Keras机器学习库
- en: NLTK, which we will use for generating new data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK，我们将用它来生成新数据
- en: An OpenAI API key which we will use to access the OpenAI tools
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用于访问OpenAI工具的OpenAI API密钥
- en: Figuring out that a system isn’t working
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弄清楚系统不起作用
- en: Figuring out whether a system isn’t working as well as it should be is important,
    both during initial development as well as during ongoing deployment. We’ll start
    by looking at poor performance during initial development.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清楚系统是否运行得不如预期同样重要，无论是在初步开发阶段还是在持续部署期间。我们将从初步开发阶段的性能不佳开始。
- en: Initial development
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始开发
- en: The primary techniques we will use to determine that our system isn’t working
    as well as we'd like are the evaluation techniques we learned about in [*Chapter
    13*](B19005_13.xhtml#_idTextAnchor226). We will apply those in this chapter. We
    will also use confusion matrices to detect specific classes that don’t work as
    well as the other classes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中学到的评估技术来确定我们的系统是否达到了我们想要的效果。我们将在本章中应用这些技术。我们还将使用混淆矩阵来检测不如其他类别表现好的特定类别。
- en: It is always a good idea to look at the dataset at the outset and check the
    balance of categories because unbalanced data is a common source of problems.
    Unbalanced data does not necessarily mean that there will be accuracy problems,
    but it’s valuable to understand our class balance at the beginning. That way,
    we will be prepared to address accuracy issues caused by class imbalance as system
    development progresses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时查看数据集并检查类别的平衡总是一个好主意，因为不平衡的数据是问题的常见来源。不平衡的数据并不一定意味着会出现准确性问题，但了解我们的类别平衡在开始阶段是有价值的。这样，我们将能够在系统开发进展中准备好解决由类别不平衡引起的准确性问题。
- en: Checking category balance
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查类别平衡
- en: For our data exploration in this chapter, we will use the **Text Retrieval Conference**
    (**TREC**) dataset, which is a commonly used multi-class classification dataset
    and can be downloaded from Hugging Face ([https://huggingface.co/datasets/trec](https://huggingface.co/datasets/trec)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的数据探索，我们将使用**文本检索大会**（**TREC**）数据集，这是一个常用的多类别分类数据集，可以从Hugging Face下载（[https://huggingface.co/datasets/trec](https://huggingface.co/datasets/trec)）。
- en: Dataset citations
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: '*Learning Question Classifiers*, Li, Xin and Roth, Dan, *{COLING} 2002:* *The
    19th International Conference on Computational Linguistics*, 2002, [https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习问题分类器*，李欣和丹·罗斯，*{COLING} 2002:* *第十九届国际计算语言学大会*，2002，[https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150)'
- en: )
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '*Toward Semantics-Based Answer Pinpointing*, Hovy, Eduard and Gerber, Laurie
    and Hermjakob, Ulf and Lin, Chin-Yew and Ravichandran, Deepak, *Proceedings of
    the First International Conference on Human Language Technology Research*, 2001,
    [https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*面向语义的答案定位*，霍维（Eduard Hovy）、格伯（Laurie Gerber）、赫姆贾科布（Ulf Hermjakob）、林进伟（Chin-Yew
    Lin）和拉维昌德兰（Deepak Ravichandran），*第一届国际人类语言技术研究大会论文集*，2001，[https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069)'
- en: )
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: The dataset consists of 5,452 training examples of questions that users might
    ask of a system and 500 test examples. The goal of the classification task is
    to identify the general topic of a question as the first step in answering it.
    The question topics are organized into two levels, consisting of six broad categories
    and 50 more specific subcategories that fall under the broader topics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含5,452个用户可能向系统提问的训练样本和500个测试样本。分类任务的目标是识别问题的一般主题，这是回答问题的第一步。问题主题分为两个级别，包括六个广泛类别和50个更具体的子类别，这些子类别属于更广泛的主题。
- en: 'We will be working with the broad categories, which are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理广泛类别，具体如下：
- en: Abbreviation (`ABBR`)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩写 (`ABBR`)
- en: Description (`DESC`)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述 (`DESC`)
- en: Entity (`ENTY`)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体 (`ENTY`)
- en: Human (`HUM`)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类 (`HUM`)
- en: Location (`LOC`)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置 (`LOC`)
- en: Number (`NUM`)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字 (`NUM`)
- en: One important task at the beginning is to find out how many documents are in
    each class. We want to see whether all of the classes have enough texts for effective
    training and whether no classes are significantly more or less common than the
    others.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，一个重要任务是找出每个类别中有多少文档。我们希望了解各个类别是否都有足够的文本进行有效训练，以及没有类别明显多于或少于其他类别。
- en: 'So far in this book, we have seen many ways to load datasets. One of the easiest
    ways to load a dataset is based on data being organized into folders, with separate
    folders for each class. Then, we can load the dataset with the `tf.keras.utils.text_dataset_from_directory()`
    function, which we used several times in previous chapters, and see the class
    names. This is shown in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中我们已经看到许多加载数据集的方法。加载数据集的最简单方法之一是将数据组织成文件夹，每个类别都有单独的文件夹。然后，我们可以使用`tf.keras.utils.text_dataset_from_directory()`函数加载数据集，这是我们在前几章中多次使用的，并查看类别名称。如下代码所示：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can then count the number of files in each class and display them in a bar
    graph with this code, using the `matplotlib` and `seaborn` graphics libraries:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用以下代码统计每个类别中文件的数量，并利用`matplotlib`和`seaborn`图形库将它们显示为条形图：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'While this code prints out the count of texts in each class as text output,
    it is also very helpful to see the totals as a bar graph. We can use the graphics
    libraries to create this graph:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这段代码将每个类别中文本的数量输出为文本，但将总数以条形图的形式呈现也非常有帮助。我们可以使用图形库来创建该图：
- en: '![Figure 14.1 – Coarse-grained class counts in the TREC data](img/B19005_14_01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – TREC数据中的粗粒度类别计数](img/B19005_14_01.jpg)'
- en: Figure 14.1 – Coarse-grained class counts in the TREC data
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – TREC数据中的粗粒度类别计数
- en: As *Figure 14**.1* shows, the `DESC` class is much smaller than the others,
    and it is possible that there will be accuracy problems with this class. There
    are ways to address this situation, which is one of the main topics of this chapter,
    but for now, we won’t make any changes until we see that this actually causes
    a problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图14.1*所示，`DESC`类的数量远小于其他类别，可能会出现该类别的准确性问题。解决这一问题的方法是本章的主要话题之一，但目前为止，我们不会做出任何更改，直到发现这一问题确实导致了困难。
- en: Doing initial evaluations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行初步评估
- en: Once we have done this initial exploration, we will want to try training one
    or more initial models for the data and evaluate them using some of the techniques
    we learned in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成了初步的探索，我们将尝试为数据训练一个或多个初始模型，并使用我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中学到的一些技术来评估它们。
- en: 'For this exploration, we will use the BERT-based training process that was
    covered in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), so we won’t duplicate
    that here. However, there are a few changes in the model that we need to make
    because we are now working with a *categorical* classification problem (six classes),
    rather than a binary classification problem (two classes), and it is worth pointing
    these out. We can see the new model definition in the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个探索中，我们将使用在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中介绍的基于BERT的训练过程，因此我们不会在这里重复。然而，由于我们现在处理的是一个*类别*分类问题（六个类别），而不是二分类问题（两个类别），因此模型中有一些变化需要注意。我们可以在下面的代码中看到新的模型定义：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The two changes that are needed in the model definition for the categorical
    task are in the final layer, which has six outputs, corresponding to the six classes,
    and a softmax activation function, as opposed to the sigmoid activation function
    that we used for binary problems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型定义中，需要为类别任务进行的两个更改是最终层，它有六个输出，分别对应六个类别，并且使用了softmax激活函数，而不是我们在二分类问题中使用的sigmoid激活函数。
- en: 'The other changes that are needed for categorical data are changes in the loss
    function and the metrics, as shown in the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 处理类别数据所需的其他更改是损失函数和指标的更改，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we will define the categorical loss and metrics functions. Other metrics
    are available, but we will just look at accuracy here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将定义类别损失和评估指标函数。虽然还有其他指标可以使用，但我们这里只看准确率。
- en: After training the model, as we did in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    we can look at the final scores. If the model does not meet the overall performance
    expectations for the application using the metrics that have been chosen, you
    can try different hyperparameter settings, or you can try other models. This was
    the process we followed in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), where
    we compared the performance of three different models on the movie review data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之后，正如我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中所做的那样，我们可以查看最终得分。如果模型未能满足应用程序的整体性能预期，使用所选指标，你可以尝试不同的超参数设置，或者尝试其他模型。这是我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中遵循的过程，我们比较了三种不同模型在电影评论数据上的表现。
- en: Keeping in mind that larger models are likely to have better performance, you
    can try increasing the size of the models. There is a limit to this strategy –
    at some point, the larger models will become very slow and unwieldy. You might
    also see that the payoff from larger and larger models becomes smaller, and performance
    levels off. This probably means that increasing the size of the models will not
    solve the problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，较大的模型可能会有更好的表现，你可以尝试增加模型的规模。这个策略是有限的——到某个时候，较大的模型会变得非常慢且难以使用。你也可能会看到，越来越大的模型带来的回报变得越来越小，性能趋于平稳。这可能意味着，增加模型的大小并不能解决问题。
- en: There are many different possibilities to look at different hyperparameter settings.
    This is, in general, a huge search space that can’t be fully explored, but there
    are some heuristics that you can use to find settings that could improve your
    results. Looking at the training history charts of loss and accuracy changes over
    epochs should give you a good idea of whether additional training epochs are likely
    to be helpful. Different batch sizes, learning rates, optimizers, and dropout
    layers can also be explored.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的可能性可以考虑不同的超参数设置。通常来说，这是一个巨大的搜索空间，无法完全探索，但你可以使用一些启发式方法来找到可能改善结果的设置。查看训练历史图表中损失和准确率随训练轮次变化的情况，可以帮助你判断是否需要更多的训练轮次。还可以探索不同的批量大小、学习率、优化器和丢弃层。
- en: Another strategy to diagnose system performance is to look at the data itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 诊断系统性能的另一种策略是直接查看数据本身。
- en: One initial evaluation we can do is a more fine-grained check for weak classes,
    by looking at the probabilities of the classifications for a large number of items
    in the dataset. We will look at this in the next section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行的一项初步评估是通过查看数据集中大量项目的分类概率，进行更细致的弱类别检查。我们将在下一节讨论这个问题。
- en: Checking for weak classes
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查弱类
- en: 'Low probabilities for a class of items are a sign that a system is not able
    to classify items with high confidence and has a good chance of making errors.
    To check for this, we can use the model to predict the classification of a subset
    of our data and look at the average scores, as shown in the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类别项的低概率是系统无法以高信心对项进行分类并且有很大可能会出错的标志。为了检查这一点，我们可以使用模型预测数据子集的分类，并查看平均得分，如下代码所示：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code goes through a subset of the TREC training data, predicts each item’s
    class, saves the predicted class in the `classification` variable, and then adds
    it to the `scores` list for the predicted class.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码遍历了TREC训练数据的一个子集，预测每个项的类别，将预测的类别保存在`classification`变量中，然后将其添加到预测类的`scores`列表中。
- en: 'The final step in the code is to iterate through the scores list and print
    the length and average score for each class. The results are shown in *Table 14.1*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后一步是遍历得分列表，并打印每个类别的项数和平均得分。结果显示在*表 14.1*中：
- en: '| **Class** | **Number** **of items** | **Average score** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **项数** | **平均得分** |'
- en: '| `ABBR` | 792 | 0.9070532 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `ABBR` | 792 | 0.9070532 |'
- en: '| `DESC` | 39 | 0.8191106 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `DESC` | 39 | 0.8191106 |'
- en: '| `HUM` | 794 | 0.8899161 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `HUM` | 794 | 0.8899161 |'
- en: '| `ENTY` | 767 | 0.9638871 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `ENTY` | 767 | 0.9638871 |'
- en: '| `LOC` | 584 | 0.9767452 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `LOC` | 584 | 0.9767452 |'
- en: '| `NUM` | 544 | 0.9651737 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `NUM` | 544 | 0.9651737 |'
- en: Table 14.1 – The number of items and the average score for each class
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14.1 – 各个类别的项数和平均得分
- en: 'We can see from *Table 14.1* that the number of items and average probabilities
    of the class predictions vary quite a bit. As you will recall from the counts
    we did in *Figure 14**.1*, we were already concerned about the `DESC` class because
    it was so small relative to the other classes. We can investigate this a bit further
    by looking at the predicted classifications of the individual items in each class
    with the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从*表 14.1*中可以看出，各个类预测的项数和平均概率差异较大。正如你从*图 14.1*中所回忆的那样，我们已经对`DESC`类感到担忧，因为它相对于其他类非常小。我们可以通过以下代码进一步调查每个类中单独项的预测分类：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s look at the histograms for the `DESC` and `LOC` classes, which are at
    the extreme ends of the set of average scores. The `LOC` class is shown in *Figure
    14**.2*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下`DESC`和`LOC`类的直方图，它们位于平均得分的极端。`LOC`类显示在*图 14.2*中：
- en: '![Figure 14.2 – The distribution of probability scores for the LOC class](img/B19005_14_2.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2 – LOC 类的概率得分分布](img/B19005_14_2.jpg)'
- en: Figure 14.2 – The distribution of probability scores for the LOC class
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 – LOC 类的概率得分分布
- en: We can see in *Figure 14**.2* that not only is the average probability very
    high (which we saw in *Table 14.1* as well) but there also are very few probabilities
    under `LOC` class. This class is likely to be very accurate in the deployed application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 14.2*中看到，不仅平均概率非常高（我们在*表 14.1*中也看到了这一点），而且在`LOC`类下的概率也很少。这个类在部署应用中可能非常准确。
- en: There is a second, less obvious advantage to classes that show the pattern in
    *Figure 14**.2*. In a deployed interactive application, we don’t want a system
    to give users answers that it’s not very confident of. This is because they’re
    more likely to be wrong, which would mislead the users. For that reason, developers
    should define a *threshold* probability score, which an answer has to exceed before
    the system provides that answer to the user.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 显示*图 14.2*中的模式的类有一个第二个、不太明显的优点。在部署的交互式应用程序中，我们不希望系统给用户提供它不太自信的答案。这是因为这些答案更可能是错误的，会误导用户。因此，开发人员应该定义一个*阈值*概率得分，只有当答案超过这个阈值时，系统才会提供该答案给用户。
- en: If the probability is lower than the threshold, the system should respond to
    the user that it doesn’t know the answer. The value of the threshold has to be
    set by the developer, based on the trade-off between the risk of giving users
    wrong answers and the risk of annoying users by saying *I don’t know* too frequently.
    *In* *Figure 14**.2* we can see that if we set the threshold to **0.9**, the system
    will not have to say *I don’t know* very often, which will improve user satisfaction
    with the system.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果概率低于阈值，系统应向用户反馈“不知道答案”。阈值的设定应由开发人员基于给用户错误答案的风险和频繁说*我不知道*而让用户感到烦恼的风险之间的权衡来决定。在*图
    14.2*中我们可以看到，如果将阈值设置为**0.9**，系统就不需要频繁说*我不知道*，这将提升用户对系统的满意度。
- en: 'Let’s contrast *Figure 14**.2* with a histogram for the `DESC` class, which
    we can see in *Figure 14**.3*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将*图14.2*与`DESC`类别的直方图进行对比，后者可以在*图14.3*中看到：
- en: '![Figure 14.3 – The distribution of probability scores for the DESC class](img/B19005_14_03.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图14.3 – DESC类别的概率得分分布](img/B19005_14_03.jpg)'
- en: Figure 14.3 – The distribution of probability scores for the DESC class
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 – DESC类别的概率得分分布
- en: '*Figure 14**.3* shows many probability scores less than `DESC` class will be
    problematic in deployment.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.3*显示了许多小于`DESC`类别的概率得分，这在部署时可能会成为问题。'
- en: 'A confusion matrix, such as the one we reviewed in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    can also help detect underperforming classes. We can generate a confusion matrix
    for the TREC data with the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中回顾的混淆矩阵，也可以帮助检测表现不佳的类别。我们可以使用以下代码生成TREC数据的混淆矩阵：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code generates the predicted classes from the test data (represented in
    the `predicted_classes` variable) and compares them to the true classes (represented
    in the `y-test` variable). We can use the scikit-learn `confusion_matrix` function
    to display the confusion matrix as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码从测试数据（由`predicted_classes`变量表示）生成预测的类别，并将其与真实类别（由`y-test`变量表示）进行比较。我们可以使用scikit-learn的`confusion_matrix`函数来显示混淆矩阵，如下所示：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see the resulting confusion matrix in *Figure 14**.4*. The confusion
    matrix tells us how often each class was predicted to be each other class, including
    itself:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图14.4*中看到生成的混淆矩阵。混淆矩阵告诉我们每个类别被预测为其他类别（包括自身）的频率：
- en: '![Figure 14.4 – The confusion matrix for the TREC test set](img/B19005_14_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图14.4 – TREC测试集的混淆矩阵](img/B19005_14_04.jpg)'
- en: Figure 14.4 – The confusion matrix for the TREC test set
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 – TREC测试集的混淆矩阵
- en: The correct predictions can be seen on the main diagonal. For example, `ABBR`
    was correctly predicted as `ABBR` *137* times. We can also see the prediction
    errors for each class. The most frequent error was incorrectly classifying `ENTY`
    as `ABBR` *11* times. In this particular example, we don’t see a lot of evidence
    that specific classes get confused with each other, although there is a tendency
    for `ENTY` to be confused with `ABBR`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的预测可以在主对角线看到。例如，`ABBR`被正确预测为`ABBR` *137*次。我们还可以看到每个类别的预测错误。最常见的错误是将`ENTY`错误分类为`ABBR`
    *11*次。在这个特定的例子中，我们没有看到很多证据表明某些类别会互相混淆，尽管`ENTY`与`ABBR`之间存在一定的混淆倾向。
- en: 'Finally, we can look at the classification report to see the `precision`, `recall`,
    and `F1` scores for each class, as well as the overall averages for the entire
    test set. The recall scores in the classification report for `DESC` and `ENTY`
    are somewhat lower than the other recall scores, which reflects the fact that
    some of the items in those classes are incorrectly recognized as `ABBR`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看分类报告，了解每个类别的`precision`、`recall`和`F1`得分，以及整个测试集的平均得分。在分类报告中，`DESC`和`ENTY`的召回率低于其他类别，这反映出这些类别中的一些项被错误地识别为`ABBR`：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s worth pointing out at this point that the decision of whether the system
    is *good enough* really depends on the application and the developer’s decision.
    In some applications, it’s better to give the user some result, even if it might
    be wrong, while in other applications, it’s important for every result to be correct,
    even if the system has to say *I don’t know* almost all the time. Going back to
    the ideas of precision and recall that we covered in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    another way of putting this is to say that in some applications, recall is more
    important, and in other cases, precision is more important.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的是，系统是否*足够好*的决定实际上取决于应用场景和开发者的决策。在某些应用中，即使结果可能是错误的，向用户提供一些结果更为重要；而在其他应用中，确保每个结果都是正确的很重要，即使系统几乎总是需要说*我不知道*。回到我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中讨论的精准度和召回率的概念，换句话说，在某些应用中，召回率更为重要，而在其他情况下，精准度更为重要。
- en: If we want to improve the performance of the TREC application, the next step
    is to decide how to address our performance concerns and improve overall accuracy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想提高TREC应用程序的性能，下一步是决定如何解决我们的性能问题并提高整体准确性。
- en: Fixing accuracy problems
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修复准确性问题
- en: In this section, we will look at fixing performance problems through two strategies.
    The first one involves issues that can be addressed by changing data, and the
    second strategy involves issues that require restructuring the application. Generally,
    changing the data is easier, and it is a better strategy if it is important to
    keep the structure of the application the same – that is, we don’t want to remove
    classes or introduce new classes. We’ll start by discussing changing the data
    and then discuss restructuring the application.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将通过两种策略来解决性能问题。第一种策略涉及通过更改数据来解决的问题，第二种策略则是涉及需要重构应用程序的问题。通常来说，更改数据较为容易，而且如果保持应用程序结构不变很重要——即我们不想删除类或引入新类——那么这是一种更好的策略。我们将先讨论更改数据，然后再讨论重构应用程序。
- en: Changing data
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改数据
- en: Changing data can greatly improve the performance of your system; however, you
    won’t always have this option. For example, you might not have control over the
    dataset if you work with a standard dataset that you intend to compare to other
    researchers’ work. You can’t change the data if you are in that situation because
    if you do, your system’s performance won’t be comparable to that of other researchers.
    If your system’s performance isn’t satisfactory but you can’t change the data,
    the only options are to improve the algorithms by using a different model or adjusting
    the hyperparameters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更改数据可以大大提升系统性能；然而，你并不总是能有这个选择。例如，如果你使用的是标准数据集并打算将其与其他研究者的工作进行比较，那么你可能无法控制数据集。在这种情况下，你不能更改数据，因为这样一来，你的系统性能就无法与其他研究者的成果进行比较。如果你的系统性能不令人满意，但又无法更改数据，那么唯一的选择就是通过使用不同的模型或调整超参数来改进算法。
- en: On the other hand, if you work on an application where you do have control over
    a dataset, changing data can be a very effective way to improve your system.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你处理的是可以控制的数据集的应用，那么更改数据可以是提升系统性能的一个非常有效的方法。
- en: Many performance issues are the result of not having enough data, either overall,
    or in specific classes. Other performance issues can be due to annotation errors
    . We’ll start with a brief discussion of annotation errors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 许多性能问题是由于数据不足，无论是整体数据不足还是某些特定类别的数据不足。其他性能问题则可能由于标注错误导致。我们将先简要讨论标注错误。
- en: Annotation errors
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标注错误
- en: It is possible that the poor performance of systems in supervised learning applications
    is due to annotation errors. Another way of putting this is to say that the supervision
    of data was wrong, and the system was trained to do the wrong thing. Perhaps an
    annotator accidentally assigned some data to the wrong class. If the data is training
    data, data in the wrong class will make the model less accurate, or if the data
    is test data, the item would be scored incorrectly because the model was wrong.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 系统在监督学习应用中性能不佳，可能是由于标注错误。换句话说，数据的监督是错误的，系统被训练去做错误的事情。也许标注员不小心将某些数据分配到了错误的类别。如果这些数据是训练数据，那么错误类别的数据会使得模型的准确性降低；如果数据是测试数据，模型会因错误而给出不正确的评分。
- en: Checking for occasional annotation errors by reviewing the annotation of every
    item in the dataset can be very time-consuming, and it is not likely to improve
    the system much. This is because if the dataset is large enough, this kind of
    sporadic error is unlikely to have much of an impact on the quality of the overall
    system. However, if you suspect that annotation errors are causing problems, a
    simple check for low-confidence items can be helpful without requiring every annotation
    to be checked. This can be done by using a variation of the code we used in the
    *Checking for weak classes* section to check for weak classes. In that code, we
    predicted the class of each item in the dataset, kept track of its probabilities
    (scores), and averaged the probabilities of all the items in the class. To modify
    the code to look instead for individual items with low probabilities, you could
    record each item and its probability individually, and then look for low-probability
    items in the final list. You are encouraged to try this exercise for yourself.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看数据集中的每个项目的标注来检查偶尔出现的标注错误可能非常耗时，而且不太可能对系统的改进产生太大影响。这是因为如果数据集足够大，这种零星的错误不太可能对整体系统的质量产生显著影响。然而，如果你怀疑标注错误正在导致问题，检查低置信度项的简单方法可能会有所帮助，而不需要检查每个标注。你可以使用我们在*检查弱类别*部分使用的代码变体来检查弱类别。在那段代码中，我们预测了数据集中每个项的类别，跟踪了其概率（得分），并计算了该类别中所有项的概率平均值。要修改代码以查找低概率的单个项，你可以单独记录每个项目及其概率，然后在最终列表中查找低概率项。鼓励你自己尝试这个练习。
- en: On the other hand, it is also possible that data contains not only occasional
    mistakes but also systematic annotation errors. Systematic errors might be due
    to differences in the annotators’ understanding of the meanings of the classes,
    leading to the similar items being assigned to different classes by different
    annotators. Ideally, these kinds of errors can be avoided, or at least reduced,
    by preparing clear annotation guidelines for annotators before the annotation
    process begins, or even by giving them training classes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据中也可能不仅包含偶尔的错误，还包括系统性的标注错误。系统性错误可能是由于标注者对类别含义的理解存在差异，导致不同标注者将相似的项分配到不同的类别。理想情况下，通过在标注过程开始之前为标注者准备明确的标注指南，或者通过为他们提供培训课程，可以避免或至少减少这类错误。
- en: Tools such as the *kappa* statistic, which was mentioned in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    can measure divergent annotations among annotators. If the kappa statistic shows
    that there is a lot of divergence across annotators, some of the data might need
    to be re-annotated using clarified guidelines. It can also happen that it is impossible
    to get annotators to agree because the decisions that the annotators have to make
    are inherently too subjective for people to agree on, no matter how much guidance
    they are given. This is a sign that the problem is not really suitable for NLU
    in the first place because there might not be a real correct classification for
    this data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 像*kappa*统计量这样的工具，可以衡量标注者之间的标注差异。如果kappa统计量显示标注者之间存在较大差异，则可能需要重新标注某些数据，并使用明确的指南进行标注。也可能发生无法让标注者达成一致的情况，因为标注者必须做出的决策本质上过于主观，无论提供多少指导，都难以达成一致。这表明问题本身可能不适合自然语言理解（NLU），因为此数据可能没有一个真正正确的分类。
- en: However, assuming that we do have a problem with objective classifications,
    in addition to addressing annotation errors, we can also improve system performance
    by creating a more balanced dataset. To do this, we will first look at adding
    and removing existing data from classes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们确实存在客观分类问题，除了处理标注错误外，我们还可以通过创建更平衡的数据集来提高系统性能。为此，我们将首先考虑从各个类别中添加和移除现有数据。
- en: Adding and removing existing data from classes
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从类别中添加和移除现有数据
- en: Unbalanced amounts of data in different classes are a common situation that
    can lead to poor model performance. The main reason that a dataset can be unbalanced
    is that this imbalance represents the actual situation in the application domain.
    For example, an application that is supposed to detect online hate speech will
    most likely encounter many more examples of non-hate speech than actual hate speech,
    but it is nevertheless important to find instances of hate speech, even if they
    are rare. Another example of a naturally unbalanced dataset would be a banking
    application where we find many more utterances about checking account balances
    than utterances about changing account addresses. Changing the address on an account
    just doesn’t happen very often compared to checking balances.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类别之间的数据不平衡是常见的情况，可能导致模型性能较差。数据集不平衡的主要原因是这种不平衡代表了应用领域中的实际情况。例如，应该检测在线仇恨言论的应用程序，很可能会遇到比实际仇恨言论更多的非仇恨言论，但找到仇恨言论仍然很重要，尽管它们比较稀少。另一个自然不平衡数据集的例子是银行应用程序，我们会发现有关支票账户余额的言论远多于更改账户地址的言论。与检查余额相比，账户地址的更改发生得非常少。
- en: There are several ways to make the sizes of the classes more even.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以使各类的大小更加均衡。
- en: 'Two common approaches are to duplicate data in the smaller classes or remove
    data from the larger classes. Adding data is called **oversampling** and removing
    data is called **undersampling**. The obvious approach to oversampling is to randomly
    copy some of the data instances and add them to the training data. Similarly,
    you can undersample by randomly removing instances from the classes that are too
    large. There are also other more sophisticated approaches to undersampling and
    oversampling, and you can find many online discussions about these topics – here,
    for example: [https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data](https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data).
    However, we will not review these here because they can become quite complex.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 两种常见的方法是复制小类中的数据或从大类中删除数据。添加数据称为**过采样**，而删除数据称为**欠采样**。过采样的明显方法是随机复制一些数据实例并将它们添加到训练数据中。同样，你也可以通过随机删除过多类中的实例来进行欠采样。还有其他更复杂的欠采样和过采样方法，你可以在网上找到很多关于这些话题的讨论
    —— 例如，在这里：[https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data](https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data)。然而，我们在这里不做进一步讨论，因为它们可能会变得相当复杂。
- en: Undersampling and oversampling can be helpful, but you should understand that
    they have to be used thoughtfully. For example, in the TREC dataset, trying to
    undersample the five frequent classes so that they have no more instances than
    the `DESC` class would require throwing out hundreds of instances from the larger
    classes, along with the information that they contain. Similarly, oversampling
    a small class such as `DESC` so that it contains the same number of instances
    as the larger classes means that there will be many duplicate instances of the
    `DESC` texts. This could result in overfitting the examples in `DESC` and consequently
    make it hard for the model to generalize to new test data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样和过采样可以是有帮助的，但你应该明白它们需要谨慎使用。例如，在TREC数据集中，试图将五个频繁出现的类别进行欠采样，使它们不超过`DESC`类的实例数量，这将需要丢弃来自大类的数百个实例，并且这些实例包含了有价值的信息。同样，将一个小类如`DESC`进行过采样，使其实例数与大类相同，意味着`DESC`文本将有许多重复实例。这可能导致模型在`DESC`示例上发生过拟合，从而使模型难以对新测试数据进行泛化。
- en: It is easy to see that while undersampling and oversampling can potentially
    be useful, they are not automatic solutions. They are probably most helpful when
    classes are not extremely different in size and where there are plenty of examples,
    even in the smallest classes. You should also keep in mind that the classes don’t
    have to be exactly balanced for a system to perform well.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，虽然欠采样和过采样可能有用，但它们并不是自动的解决方案。当类别之间的大小差异不极端，并且即使是最小的类别中也有大量示例时，它们可能最为有效。你还应该记住，系统表现良好并不意味着类别必须完全平衡。
- en: Another approach to adding data is to create new data, which we will discuss
    in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种添加数据的方法是创建新数据，我们将在下一节中讨论。
- en: Generating new data
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成新数据
- en: 'If your dataset has underrepresented classes, or is too small overall, you
    can also add generated data to the entire dataset or just to the smaller classes.
    We will look at the following three ways to do this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集有代表性不足的类别，或者总体数据集过小，你也可以将生成的数据添加到整个数据集中，或仅添加到较小的类别中。我们将探讨以下三种方法来实现这一点：
- en: Generating new data from rules
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从规则生成新数据
- en: Generating new data from LLMs
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从大语言模型（LLMs）生成新数据
- en: Using crowdworkers to get new data
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人群工人获取新数据
- en: Generating new data from rules
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从规则生成新数据
- en: 'One way to create new data is to write rules to generate new examples of data,
    based on the data that you already have. The `restaurant search` class. You could
    write a `parse` library:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新数据的一种方式是编写规则，基于已有的数据生成新的数据示例。例如，`restaurant search` 类别。你可以编写一个 `parse` 库：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the rules in an NLTK CFG can be any context-free rules; they don’t
    have to correspond to actual linguistic categories. For example, we could have
    called the last Adj_Cuisine instead. We might want to do this if we want to be
    able to generate sentences with other adjectives, such as `good` or `low-priced`.
    The rule names and the rules themselves don’t matter to the NLTK CFG package;
    the only thing that matters is that the CFG is written in the syntax that the
    NLTK CFG package expects. The names and the rules can be any rules that you find
    convenient to generate new examples.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，NLTK CFG 中的规则可以是任何上下文无关规则；它们不必对应实际的语言学类别。例如，我们本可以将最后的规则命名为 Adj_Cuisine。如果我们希望能够生成带有其他形容词的句子，比如
    `good` 或 `low-priced`，我们可能会这么做。规则名称和规则本身对 NLTK CFG 包没有影响；唯一重要的是 CFG 必须以 NLTK CFG
    包所期望的语法书写。名称和规则可以是任何便于生成新示例的规则。
- en: 'The last two lines in the preceding code will generate 10 examples of sentences
    from this grammar, with the following result:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的最后两行将从该语法生成 10 个句子示例，结果如下：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you want to generate all of the possible sentences from these rules, you
    will leave out the parameter, `n=10`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想生成这些规则的所有可能句子，可以省略参数 `n=10`。
- en: This is a fast way to generate a lot of sentences, but as you can see, the sentences
    are quite repetitious. This is because the NLTK `generate` method will produce
    every possible sentence that the grammar covers. Adding a lot of repetitious sentences
    to your training set could skew the model to these kinds of sentences, which in
    turn might make it harder for the model to recognize more varied restaurant search
    sentences. One approach to getting a wider variety of sentences from an NLTK CFG
    would be to write a broader grammar, generate all the sentences it covers, and
    then randomly select a subset of the generated sentences to add to the training
    set.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成大量句子的快速方法，但正如你所看到的，句子非常重复。这是因为 NLTK `generate` 方法会生成语法所覆盖的每个可能的句子。将大量重复句子添加到训练集中可能会使模型倾向于这些句子，这反而可能使得模型更难识别更为多样化的餐厅查询句子。从
    NLTK CFG 获取更广泛句子的一种方法是编写更广泛的语法，生成它覆盖的所有句子，然后随机选择一部分生成的句子加入训练集。
- en: Using **large language models** (**LLMs**) to generate new examples is another
    useful and easy option, which we will discuss in the following section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **大语言模型**（**LLMs**）生成新示例是另一个有用且简单的选择，我们将在接下来的章节中讨论这个选项。
- en: Generating new data from LLMs
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从大语言模型（LLMs）生成新数据
- en: 'Online LLMs such as ChatGPT are another very good way to get more training
    data because you can simply ask them to generate the appropriate training data.
    For example, let’s say ChatGPT was given the following prompt:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在线 LLM，如 ChatGPT，是获取更多训练数据的另一种非常好的方式，因为你可以简单地要求它们生成适当的训练数据。例如，假设 ChatGPT 收到以下提示：
- en: '*generate 20 requests to find local restaurants of different cuisines and*
    *price ranges*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成 20 条请求，查找不同菜系和* *价格范围的本地餐厅*'
- en: 'ChatGPT ([chat.openai.com/chat](https://chat.openai.com/chat)) would produce
    the following answer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT ([chat.openai.com/chat](https://chat.openai.com/chat)) 将生成以下答案：
- en: '![Figure 14.5 – The ChatGPT-generated restaurant query data](img/B19005_14_05.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5 – ChatGPT 生成的餐厅查询数据](img/B19005_14_05.jpg)'
- en: Figure 14.5 – The ChatGPT-generated restaurant query data
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – ChatGPT 生成的餐厅查询数据
- en: (For brevity, not all the results are shown.)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: （为了简洁，未显示所有结果。）
- en: 'You can see that these sentences are much less repetitious than the ones from
    NLTK’s `generate` method. In the initial ChatGPT prompt, you can also restrict
    the question style – for example, you could ask for generated questions in an
    informal style. This results in informal sentences such as the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这些句子比 NLTK 的 `generate` 方法生成的句子要少很多重复。在最初的 ChatGPT 提示中，你还可以限制问题的风格——例如，你可以要求生成的查询问题采用非正式风格。这样就会生成像以下这样的非正式句子：
- en: '![Figure 14.6 – ChatGPT-generated restaurant query data in an informal style](img/B19005_14_06.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.6 – ChatGPT 生成的餐厅查询数据，风格为非正式](img/B19005_14_06.jpg)'
- en: Figure 14.6 – ChatGPT-generated restaurant query data in an informal style
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 – ChatGPT 生成的餐厅查询数据，风格为非正式
- en: You can also control the variation among the responses by changing the *temperature*
    parameter, which is available in the ChatGPT API. Temperature settings vary between
    zero and two. A temperature setting of zero means that the responses will be less
    varied, and a higher setting means that they will be more varied.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过改变 ChatGPT API 中的 *temperature* 参数来控制回应之间的变化性。温度设置的范围从零到二不等。温度为零时，回应的变化性较小；温度较高时，回应的变化性较大。
- en: Within ChatGPT, a low temperature means that the generation model chooses the
    next word for a response among the higher probability words, and a high temperature
    means that the model will select the next word from among the words with lower
    probabilities. The result with a higher temperature setting will include more
    varied responses, but some of them might not make sense. *Figure 14**.7* shows
    how to set the temperature in code directly from the API.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 中，低温度意味着生成模型会从高概率的词汇中选择下一个词，而高温度意味着模型会从概率较低的词汇中选择下一个词。高温度设置的结果会产生更多样化的回应，但其中一些回应可能不太合理。*图
    14.7* 展示了如何直接从 API 中的代码设置温度。
- en: '![Figure 14.7 – Setting the GPT temperature using the OpenAI API](img/B19005_14_07.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7 – 使用 OpenAI API 设置 GPT 温度](img/B19005_14_07.jpg)'
- en: Figure 14.7 – Setting the GPT temperature using the OpenAI API
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 – 使用 OpenAI API 设置 GPT 温度
- en: The code in *Figure 14**.7* sets the value of the `temperature` parameter to
    `1.5`, which results in a fairly diverse set of responses. You can see these at
    the bottom of *Figure 14**.7*. The code also sets the `model` parameter to use
    the `gpt-3.5-turbo` model and sets the message to send in the `messages` parameter.
    If you are interested in experimenting with other GPT API calls, you can find
    other API parameters in the OpenAI API documentation at [https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.7* 中的代码将 `temperature` 参数的值设置为 `1.5`，这会生成相当多样化的响应。你可以在 *图 14.7* 的底部看到这些响应。该代码还将
    `model` 参数设置为使用 `gpt-3.5-turbo` 模型，并在 `messages` 参数中设置要发送的消息。如果你有兴趣尝试其他 GPT API
    调用，可以在 OpenAI API 文档中找到其他 API 参数，网址为 [https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference)。'
- en: Note that you will need to set the `openai.api_key` variable at line 3 to your
    own OpenAI user key to run this code, since the OpenAI API is a paid service.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你需要在第 3 行设置 `openai.api_key` 变量为你自己的 OpenAI 用户密钥才能运行此代码，因为 OpenAI API 是一项付费服务。
- en: If you use an LLM to generate data, be sure to check the results and decide
    whether the responses represent the kind of examples that your users would really
    say and, hence, should be included in your training data. For example, some of
    the informal requests in *Figure 14**.6* might be more informal than many users
    would say to a chatbot.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 LLM 来生成数据，请务必检查结果，并决定这些回应是否代表用户可能实际说出的例子，因此是否应包含在你的训练数据中。例如，*图 14.6* 中的一些非正式请求可能比许多用户对聊天机器人的提问更为随意。
- en: A final way to add new data to underrepresented classes is to hire crowdworkers
    to create more data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种向数据不足的类别添加新数据的方法是雇佣众包工作者来创建更多数据。
- en: Using crowdworkers to get new data
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用众包工作者获取新数据
- en: Getting more data from crowdworkers is time-consuming and possibly expensive,
    depending on how much data you need and how complicated it is. Nevertheless, it
    would be an option if you didn't get enough data using other methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从众包工作者获取更多数据既费时又可能昂贵，具体取决于你需要多少数据以及数据的复杂程度。然而，如果使用其他方法未能获得足够的数据，这仍然是一个可行的选项。
- en: Of course, all of these methods we have outlined in this section (using rules,
    using LLMs, and using crowdworkers) can be combined – not all of the new training
    data has to come from the same place.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们在本节中概述的所有方法（使用规则、使用LLMs和使用众包工人）是可以组合使用的——所有的新训练数据不必都来自同一个地方。
- en: Another approach similar to changing the data is to change the application itself,
    which we will discuss in the next section.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类似于改变数据的方法是改变应用程序本身，我们将在下一节讨论这一点。
- en: Restructuring an application
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重构一个应用程序
- en: In some cases, the best solution to classes that are not being predicted well
    is to restructure an application. As in the case of changing the data, you won’t
    always have the option to do this if this is a standard dataset that the research
    community uses to compare work among different labs, as the application has to
    have the same structure as that used by other researchers for the results to be
    comparable.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，预测不良的类的最佳解决方案是重构应用程序。与改变数据一样，如果这是一个研究社区用于不同实验室之间比较工作的标准数据集，你可能没有这个选项，因为应用程序必须与其他研究者使用的结构相同，才能确保结果具有可比性。
- en: If you do have control over the application structure, you can add, remove,
    or combine classes that don’t perform well. This can greatly improve the overall
    application performance. Let’s start by looking at an artificial example of an
    application that needs restructuring, and the different ways that this restructuring
    might be done.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实能够控制应用程序结构，你可以添加、删除或合并表现不佳的类。这可以大大提高整体应用程序性能。我们先通过一个需要重构的应用程序的人工示例来看看，这种重构可能采取的不同方式。
- en: Visualizing the need for class restructuring
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化重构类的需求
- en: Visualizing datasets can often provide immediate insight into potential performance
    problems. We can visualize how similar classes in a dataset are to each other
    in a couple of ways.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据集通常可以立即提供潜在性能问题的洞察。我们可以通过几种方式可视化数据集中相似类之间的关系。
- en: First, confusion matrices such as the one in *Figure 14**.4* are a good source
    of information about which classes are similar to each other and consequently
    get confused for each other. We saw immediately from *Figure 14**.4* that `ENTY`
    and `DESC` were quite often confused with `ABBR`. We might want to add data to
    those classes, as discussed in the previous section, or we could also consider
    restructuring the application, which we will discuss next.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像*图14.4*中的混淆矩阵是了解哪些类彼此相似并因此容易互相混淆的好信息来源。我们从*图14.4*中可以立刻看到，`ENTY`和`DESC`经常与`ABBR`混淆。我们可能需要增加这些类的数据，如前一节所述，或者我们也可以考虑重构应用程序，我们将在接下来的部分讨论这个问题。
- en: A second visualization technique is to use the topic modeling techniques that
    we saw in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), to see problems with
    the application structure.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种可视化技术是使用我们在[*第12章*](B19005_12.xhtml#_idTextAnchor217)中看到的主题建模技术，来查看应用程序结构的问题。
- en: '*Figure 14**.8* shows how an artificially constructed dataset of four classes
    might look if we clustered them based on the [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)
    tools, **Sentence Bert** and **BERTopic**. We can immediately see that there are
    some problems with the classes in this dataset.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.8*展示了如果我们基于[*第12章*](B19005_12.xhtml#_idTextAnchor217)中的**Sentence Bert**和**BERTopic**工具对四个类进行聚类，人工构建的数据集可能是什么样子。我们可以立即看到该数据集中类的分布存在一些问题。'
- en: "![Figure 14.\uFEFF8 – Unsupervised clustering of four classes with artificially\
    \ generated data](img/B19005_14_08.jpg)"
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图14.8 - 使用人工生成数据对四个类进行无监督聚类](img/B19005_14_08.jpg)'
- en: Figure 14.8 – Unsupervised clustering of four classes with artificially generated
    data
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8 - 使用人工生成数据对四个类进行无监督聚类
- en: First of all, the instances of **Class 1**, represented by circles, seem to
    cluster into two different classes, one centered around the point (*0.5, 0.5*)
    and the other centered around the point (0*.5*, *1.75*). It seems unlikely that
    these clusters should both be grouped into the same class if they are actually
    that different. **Class 1** should probably be split, and the instances currently
    assigned to Class 1 should be assigned to at least two, and possibly three, new
    classes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，**Class 1**的实例，用圆圈表示，似乎聚集成了两个不同的类，一个集中在点(*0.5, 0.5*)，另一个集中在点(0.5, 1.75*)。如果这些聚类真的如此不同，它们不应该都被归为同一类。因此，**Class
    1**可能需要拆分，目前分配给Class 1的实例应该被分配到至少两个，甚至可能是三个新类。
- en: '**Class 2**, represented by squares, and **Class 3**, represented by triangles,
    seem problematic. They are not completely mixed together, but they are not completely
    separate either. Some of the instances of both classes are likely to be misclassified
    because of their similarity to the other class. If you see classes such as **Class
    3** and **Class 4**, with this kind of overlap, consider merging the classes if
    they appear to be similar in meaning (if they aren’t similar in meaning, consider
    adding more data to one or both classes). Finally, **Class 4**, represented by
    stars, is compact and doesn’t overlap with any other classes. It shouldn’t require
    any adjustments.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**Class 2**（用方形表示）和**Class 3**（用三角形表示）似乎存在问题。它们并不是完全混合在一起，但也没有完全分开。由于它们之间的相似性，可能有些实例会被错误分类。如果你看到像**Class
    3**和**Class 4**这样的类，存在这种重叠情况，考虑是否应合并这些类，特别是它们在意义上看起来相似（如果它们在意义上不相似，考虑向一个或两个类中添加更多数据）。最后，**Class
    4**（用星形表示）是紧凑的，并且没有与其他类重叠，应该不需要任何调整。'
- en: Let’s now take a look at three restructuring options – merging classes, dividing
    classes, and introducing new classes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下三种重构选项——合并类、划分类和引入新类。
- en: Merging classes
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合并类
- en: Classes that are much smaller than the rest of the classes can be merged with
    other semantically similar classes, especially if they are frequently confused
    with those classes. This can be a good strategy because, in many real applications,
    there are classes that simply don’t occur very often, but unlike the hate speech
    example mentioned earlier, it isn’t always critical to be able to tell the difference
    between the original classes. Of course, this is only possible with a multi-class
    problem – that is, a problem with more than two classes – since merging the classes
    in a binary (two-class) problem will put everything in one class and leave us
    with nothing to classify.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 比其他类小得多的类可以与其他语义上相似的类合并，特别是当它们经常与这些类混淆时。这可能是一种很好的策略，因为在许多实际应用中，有些类的出现频率非常低，但与前面提到的仇恨言论示例不同，通常并不是必须能区分这些原始类。当然，这只有在多类问题的情况下才可行——即，有两个以上类的问题——因为在二分类（两个类）问题中合并类会把所有内容归为一个类，导致没有东西可以分类。
- en: In some cases, merging classes can be accomplished by adding all the data from
    one class to the data of the other class, which is the simplest restructuring
    that we can do. A slightly more complex merger of classes can be done if the new
    structure is more complicated – for example, if it involves adding slots. For
    example, it is possible that classes such as **Class 2** and **Class 3** in *Figure
    14**.8* are actually not different enough to be worth trying to separate.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，通过将一个类的所有数据添加到另一个类的数据中，可以实现类的合并，这是我们能做的最简单的重构。如果新的结构更加复杂——例如，涉及添加插槽——那么可以进行稍微复杂一点的类合并。例如，像**Class
    2**和**Class 3**在*Figure 14**.8*中，实际上可能没有足够的不同，无法值得尝试分开。
- en: As an example, suppose we work on a generic personal assistant application,
    with classes such as `play music`, `find a restaurant`, `get weather forecast`,
    `find a bookstore`, and `find a bank`. It might turn out that `find a restaurant`
    has much more data than `find a bookstore`, and as a result, `find a bookstore`
    is often confused with `find a restaurant`. In that case, it would be worth considering
    whether all the `find a` classes should be merged into one larger class. This
    class could be called `local business search`, with `bookstore`, `restaurant`,
    and `bank` being treated as slots, as discussed in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们在开发一个通用的个人助手应用程序，类如`play music`、`find a restaurant`、`get weather forecast`、`find
    a bookstore`和`find a bank`。可能出现的情况是，`find a restaurant`的数据比`find a bookstore`多得多，结果导致`find
    a bookstore`常常与`find a restaurant`混淆。在这种情况下，值得考虑是否应该将所有的`find a`类合并为一个更大的类。这个类可以被命名为`local
    business search`，其中`bookstore`、`restaurant`和`bank`被视为插槽，正如在[*Chapter 9*](B19005_09.xhtml#_idTextAnchor173)中讨论的那样。
- en: Another strategy is to separate classes such as **Class 1** in *Figure 14**.5*
    into two different classes. The next section discusses dividing classes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是将像**Class 1**在*Figure 14**.5*中那样的类分成两个不同的类。下一节将讨论划分类。
- en: Dividing classes
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 划分类
- en: Sometimes it makes sense to divide a large class into separate smaller classes
    if there appear to be systematic differences between two or more groups within
    the class. Tools such as BERTopic can help suggest names for new classes if the
    new name isn’t obvious from looking at the instances in each group. Unfortunately,
    dividing a class into new classes isn’t as easy as merging classes because the
    examples in the new classes will need to be annotated with their new names. Although
    re-annotation is more work, dividing and re-annotating classes is necessary if
    you have to divide a large class into more meaningful new classes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，如果一个大类别内部出现了系统性的差异，可能有意义将这个大类别划分为若干个较小的类别。像BERTopic这样的工具可以帮助建议新类别的名称，如果通过查看每组中的实例不能直观地得出新名称。不幸的是，将一个类别划分为新类别并不像合并类别那么容易，因为新类别中的实例需要用新的名称重新标注。虽然重新标注工作量更大，但如果你需要将一个大类别划分为更有意义的新类别，那么划分和重新标注类别是必要的。
- en: Introducing an "other" class
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入“其他”类别
- en: Introducing an `other` class is a variant of the strategy of merging classes.
    If there are several small classes that don’t really have enough training data
    to be reliably classified, it can sometimes be useful to group them together in
    an `other` class – that is, a class that contains items that don’t fit into any
    of the other classes. One type of application that this approach can be useful
    for is call routing in a telephone self-service application.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 引入`其他`类别是合并类别策略的一个变体。如果有几个小类别，它们没有足够的训练数据来可靠地进行分类，那么有时将它们归为一个`其他`类别是有用的——即，包含那些不符合任何其他类别的项目的类别。这种方法对某些应用场景很有用，例如电话自助服务应用中的呼叫路由。
- en: In these applications, there can sometimes be hundreds of destinations where
    a call can be routed. In nearly every application of this kind, there are some
    infrequent classes for which there is much less data than other classes. Sometimes,
    it is best to not try to identify these classes because trying to do so accurately
    will be difficult with the small amounts of data available. A better strategy
    would be to group them together into an `other` class. It still might be hard
    to identify items in the `other` category because the items it contains will not
    be very similar, but it will keep them from interfering with the overall application
    accuracy. How items in the `other` class are handled depends on the specific application’s
    goals, but options include handling them manually (for example, with a human call
    center agent) or simply telling users that the system can’t handle their question.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些应用中，有时会有数百个目的地可以路由电话。在几乎每个此类应用中，都有一些不常见的类别，它们的数据量远少于其他类别。有时，最好不要尝试识别这些类别，因为在可用的数据量很少的情况下，准确地识别它们将非常困难。一个更好的策略是将它们归为一个`其他`类别。尽管在`其他`类别中的项目可能很难识别，因为它包含的项目之间差异很大，但它可以防止这些项目干扰整体应用的准确性。如何处理`其他`类别中的项目取决于具体应用的目标，常见的处理方式包括人工处理（例如，通过人工呼叫中心代理）或直接告诉用户系统无法处理他们的问题。
- en: After the accuracy issues that were identified during initial development have
    been identified and addressed, it is time to deploy the system.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始开发过程中识别并解决准确性问题后，就可以部署系统了。
- en: Moving on to deployment
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向部署阶段过渡
- en: If we’ve fixed the performance issues we’ve discussed so far, we will have trained
    a model that meets our performance expectations, and we can move on to deployment,
    when the system is installed and does the task that it was designed for. Like
    any software, a deployed NLU model can have problems with system and hardware
    issues, such as network issues, scalability, and general software problems. We
    won’t discuss these kinds of problems because they aren’t specific to NLU.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经解决了到目前为止讨论的性能问题，那么我们就已经训练了一个符合我们性能预期的模型，可以进入部署阶段，在这个阶段，系统会被安装并执行其设计的任务。像任何软件一样，已部署的自然语言理解（NLU）模型可能会遇到系统和硬件问题，例如网络问题、可扩展性问题以及一般的软件问题。我们不会讨论这些问题，因为它们并不特定于NLU。
- en: The next section will cover considerations to address NLU performance problems
    that occur after deployment.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将讨论解决部署后NLU性能问题的考虑事项。
- en: Problems after deployment
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署后的问题
- en: After an NLU system is developed and put into place in an application, it still
    requires monitoring. Once the system has reached an acceptable level of performance
    and has been deployed, it can be tempting to leave it alone and assume that it
    doesn’t need any more attention, but this is not the case. At the very least,
    the deployed system will receive a continuous stream of new data that can be challenging
    to the existing system if it is different from the training data in some way.
    On the other hand, if it is not different, it can be used as new training data.
    Clearly, it is better to detect performance problems from internal testing than
    to learn about them from negative customer feedback.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 开发并在应用中实施NLU系统后，仍需要进行监控。一旦系统达到可接受的性能水平并已部署，就可能会诱使人们不再关注它并假设它不需要进一步关注，但事实并非如此。至少，部署的系统将接收到持续的新数据流，如果这些数据与训练数据有所不同，则可能对现有系统构成挑战。另一方面，如果数据没有变化，则可以用作新的训练数据。显然，最好通过内部测试检测性能问题，而不是从负面客户反馈中了解这些问题。
- en: At a high level, we can think of new performance problems as either being due
    to a change in the system itself, or due to a change in the deployment context.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们可以将新的性能问题看作是由系统本身的变化或部署环境的变化所导致的。
- en: Changes in system performance due to system changes should be detected by testing
    before the new system is deployed. This kind of testing is very similar to the
    kind of testing that has to be done for any software deployment, so we won’t cover
    it in any detail. Degradation in performance can be detected by versioning the
    system and running an evaluation with a fixed set of data and metrics after every
    change. This is useful to both detect decreases in performance but also to document
    improvements in performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于系统变更导致的系统性能变化应在新系统部署之前进行测试检测。这种测试与任何软件部署所需的测试非常相似，因此我们不会详细讨论它。通过对系统进行版本控制，并在每次变更后使用固定的数据集和指标进行评估，可以检测到性能下降。这不仅有助于检测性能下降，还有助于记录性能改进。
- en: As with any machine-learning-based system, new data can cause problems with
    an NLU system because it is different in some significant way from the training
    data. These kinds of differences are frequently due to changes in the deployment
    context.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何基于机器学习的系统一样，新数据可能会对NLU系统造成问题，因为它在某些重要方面与训练数据不同。这些差异通常是由部署环境的变化引起的。
- en: What do we mean by changes in the **deployment context**? The deployment context
    refers to everything about the application, except for the NLU system itself.
    Specifically, it can include the users, their demographics, their geographical
    locations, the backend information that’s being provided, and even events in the
    world such as weather. Any of these can change the characteristics of texts that
    the application processes. These changes alter the correspondence between the
    training data and the new data being processed, which will lead to a decrease
    in performance.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的**部署环境的变化**是指应用程序的所有内容，除了NLU系统本身。具体来说，它可能包括用户、其人口统计信息、地理位置、提供的后端信息，甚至世界上的事件，比如天气。这些因素中的任何一个都可以改变应用程序处理的文本特征。这些变化会改变训练数据和正在处理的新数据之间的对应关系，从而导致性能下降。
- en: Some changes in the deployment context can be predicted. For example, if a company
    introduces a new product, this will introduce new vocabulary that a customer support
    chatbot, voice assistant, or email router needs to recognize, since, after all,
    we expect customers to be talking about it. It is a best practice to perform an
    evaluation on new data after changes like the introduction of a new product occurs,
    and decide whether the system should be retrained with additional data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一些部署环境的变化是可以预测的。例如，如果公司推出新产品，这将引入新的词汇，客户支持聊天机器人、语音助手或电子邮件路由器需要识别，毕竟我们期望客户讨论它。在类似引入新产品的变化后，最佳实践是对新数据进行评估，并决定是否应使用额外数据对系统进行重新训练。
- en: On the other hand, some changes can’t be predicted – for example, the COVID-19
    pandemic introduced a lot of new vocabulary and concepts that medical or public
    health NLU applications needed to be trained on. Because some deployment context
    changes can’t be predicted, it is a good idea to periodically perform an evaluation
    using new data coming in from the deployment.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有些变化是无法预测的——例如，COVID-19疫情引入了大量新的词汇和概念，医学或公共卫生领域的NLU应用需要对此进行训练。由于某些部署环境的变化是无法预见的，因此定期使用从部署中获得的新数据进行评估是一个好主意。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned about a number of important strategies to
    improve the performance of NLU applications. You first learned how to do an initial
    survey of the data and identify possible problems with the training data. Then,
    you learned how to find and diagnose problems with accuracy. We then described
    different strategies to improve performance – specifically, adding data and restructuring
    the application. The final topic we covered was a review of problems that can
    occur in deployed applications and how they can be addressed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经学习了许多提高自然语言理解（NLU）应用性能的重要策略。首先，你学习了如何对数据进行初步调查并识别训练数据中的潜在问题。然后，你学习了如何发现并诊断准确性方面的问题。接着，我们介绍了不同的性能提升策略——具体来说，包括增加数据和重构应用程序。最后，我们讨论了已部署应用程序中可能出现的问题及其解决方法。
- en: In the final chapter, we will provide an overview of the book and a look to
    the future. We will discuss where there is potential for improvement in the state
    of the art of NLU performance, as well as faster training, more challenging applications,
    and what we can expect from NLU technology as the new LLMs become more widely
    used.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将概述本书内容并展望未来。我们将讨论在NLU性能的最新技术中有哪些提升的潜力，以及更快速的训练、更具挑战性的应用，并探讨随着新一代大型语言模型（LLM）被广泛应用，NLU技术将会有什么样的发展。
