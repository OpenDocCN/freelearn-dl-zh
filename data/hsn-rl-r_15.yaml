- en: Exploring Deep Reinforcement Learning Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度强化学习方法
- en: '**Neural networks** (**NNs**) are exceptionally effective at getting good characteristics
    for highly structured data. We could then represent our Q function with a neural
    network, which takes the status and action as input and outputs (gives) the corresponding
    Q value. **Deep reinforcement learning** (**DRL**) methods use deep neural networks
    to approximate any of the following reinforcement learning components: value function,
    policy, and model.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**（**NNs**）在获取高度结构化数据的良好特征方面非常有效。然后，我们可以用神经网络来表示我们的Q函数，该神经网络以状态和动作作为输入，并输出（给出）相应的Q值。**深度强化学习**（**DRL**）方法使用深度神经网络来逼近以下任何强化学习组件：价值函数、策略和模型。'
- en: In this chapter, we will deal with DRL gradually. First, we will learn the basic
    concepts of artificial neural networks and see how to apply them by taking a practical
    example. Later, we will see how to apply these concepts to reinforcement learning
    to improve the performance of the algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将逐步学习DRL。首先，我们将学习人工神经网络的基本概念，并通过实际示例看到如何应用它们。随后，我们将看到如何将这些概念应用于强化学习，以提高算法的性能。
- en: By the end of this chapter, we will learn the fundamentals of artificial neural
    networks, how to apply feedforward neural network methods to your data, and how
    neural network algorithms work. We will understand the basic concepts that deep
    neural networks use to approximate reinforcement learning components and we will
    learn how to implement a deep Q network using R. Finally, we will learn how to
    implement a deep recurrent Q network using R.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将学习人工神经网络的基本原理，如何将前馈神经网络方法应用于你的数据，以及神经网络算法如何工作。我们将理解深度神经网络用来逼近强化学习组件的基本概念，并将学习如何使用R实现深度Q网络。最后，我们将学习如何使用R实现深度递归Q网络。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Introducing neural network basic concepts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍神经网络基本概念
- en: Managing feed-forward neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理前馈神经网络
- en: Neural network for regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于回归的神经网络
- en: Approaching DRL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近DRL
- en: Deep recurrent Q-networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度递归Q网络
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用：
- en: '[http://bit.ly/35szk1D](http://bit.ly/35szk1D)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/35szk1D](http://bit.ly/35szk1D)'
- en: Introducing neural network basic concepts
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍神经网络基本概念
- en: '**Artificial neural networks** (**ANNs**) are mathematical models whose purpose
    is to try to simulate some typical human brain activities such as pattern recognition,
    language comprehension, image perception, and so on. The architecture of an ANN
    is composed of a system of nodes, which refer to the neurons of a human brain,
    interconnected between them by weighted connections, which simulate synapses between
    neurons. The output of the network is updated iteratively through the link weights
    up to the convergence. The data collected in the experimental fields are provided
    at the input level and the network result is provided by the output level. The
    input nodes represent the independent or predictive variables necessary to predict
    the dependent variables that represent the output neurons.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**）是数学模型，其目的是尝试模拟一些典型的人类大脑活动，例如模式识别、语言理解、图像感知等。一个ANN的架构由一组节点组成，这些节点指代人类大脑中的神经元，节点之间通过加权连接相互连接，模拟神经元之间的突触。网络的输出通过连接权重迭代更新，直到收敛。实验领域收集的数据在输入层提供，网络结果在输出层提供。输入节点代表预测输出神经元所需的独立或预测变量。'
- en: Neural networks offer a very powerful set of tools that can solve problems in
    the field of classification, regression, and non-linear control. In addition to
    having a high processing speed, neural networks can learn the solution from a
    certain set of examples. In many applications, this allows us to circumvent the
    need to develop a model of the physical processes underlying the problem, which
    can often be difficult, if not impossible, to find.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络提供了一套非常强大的工具，可以解决分类、回归和非线性控制领域的问题。除了具有较高的处理速度外，神经网络还能够从一组示例中学习解决方案。在许多应用中，这使得我们能够绕过开发物理过程模型的需求，而这一模型往往难以找到，甚至不可能找到。
- en: ANNs try to emulate the behavior of biological neurons. Let's see how.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs尝试模拟生物神经元的行为。我们来看看具体如何实现的。
- en: Biological neural networks
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物神经网络
- en: The inspiration for neural networks derives from studies on information-processing
    mechanisms in the biological nervous system, the human brain; in fact, much of
    the research on neural networks has precisely the purpose of investigating these
    mechanisms. An artificial neural network is made up of many neurons or simple
    processors. An artificial neuron mimics the characteristics of a biological neuron—every
    cell in the human nervous system can receive, process, and transmit electrical
    signals.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的灵感来源于对生物神经系统信息处理机制的研究，特别是人类大脑；事实上，神经网络的研究正是为了探究这些机制。人工神经网络由许多神经元或简单的处理单元组成。人工神经元模拟生物神经元的特性——人类神经系统中的每个细胞都能够接收、处理并传输电信号。
- en: 'It consists of four basic parts, namely, the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 它由四个基本部分组成，分别是：
- en: Body cell
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细胞体
- en: Synapses
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 突触
- en: Axon
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轴突
- en: Dendrites
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树突
- en: The dendrites receive electrical information from other neurons through the
    synapses and transmit them to the body of the cell. Here, they are added together
    and, if the total excitation exceeds a threshold limit, the cell reacts by passing
    the signal to another cell through the axon.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 树突通过突触接收来自其他神经元的电信号，并将其传输到细胞体。在这里，这些信号被加在一起，如果总的兴奋度超过了某个阈值，细胞就会通过轴突将信号传递给其他细胞。
- en: 'The following diagram shows the structure of a biological neuron:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个生物神经元的结构：
- en: '![](img/73296bdf-c324-4417-a0c0-dc8611f99d3a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73296bdf-c324-4417-a0c0-dc8611f99d3a.png)'
- en: When the signal reaches the synapse, it causes the release of chemicals called
    neurotransmitters, which enter the bodies of other neurons. Depending on the type
    of synapse, which can be excitatory or inhibitory, these substances respectively
    increase or decrease the probability that the next neuron becomes active. At each
    synapse, a weight is associated, which determines the type and magnitude of the
    exciter or inhibitor effect. Hence, each neuron carries out a weighted sum of
    the inputs coming from the other neurons and, if this sum exceeds a certain threshold,
    the neuron is activated.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当信号到达突触时，它会引起一种叫做神经递质的化学物质的释放，这些化学物质进入其他神经元的细胞体。根据突触的类型（可以是兴奋性或抑制性），这些物质分别增加或减少下一个神经元被激活的概率。在每个突触处，都会有一个权重值与之相关，这个权重决定了兴奋或抑制效应的类型和大小。因此，每个神经元都会对来自其他神经元的输入进行加权求和，如果这个求和结果超过了某个阈值，神经元就会被激活。
- en: Each operation performed by the neuron has a millisecond duration, so it represents
    a relatively slow processing system. However, the entire network has a very large
    number of neurons and synapses that can operate in parallel and simultaneously,
    making the actual processing power very high. Furthermore, the biological neural
    network has a high tolerance to inaccurate or even wrong information; it has the
    capacity for learning and generalization, which makes it so efficient in identification
    and classification operations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元执行的操作持续时间为毫秒级，因此它代表了一个相对较慢的处理系统。然而，整个网络拥有大量的神经元和突触，这些神经元和突触能够并行并同时操作，从而使得实际的处理能力非常强大。此外，生物神经网络对不准确甚至错误的信息具有很高的容忍度；它具有学习和归纳的能力，这使得它在识别和分类操作中表现得非常高效。
- en: 'The functioning of neurons regulates the activities of the brain, which is
    a naturally optimized machine for solving complex problems. Its structure, made
    of simple elements, has evolved over time in the direction of improving its capabilities:
    there is no central control, and all areas of the brain contribute together to
    the realization of a task or the solution of a problem in a contributory way.
    If one part of the brain stops working, it continues to perform its tasks, perhaps
    not with the same performance. The brain is fault-tolerant; its performance slowly
    degrades in proportion to the destruction of its neurons.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的功能调控着大脑的活动，大脑是一个自然优化的解决复杂问题的机器。它由简单的元素构成，经过时间的演化，朝着提升其能力的方向发展：大脑没有中央控制，各个区域共同协作完成任务或解决问题。如果大脑的某一部分停止工作，它仍然能够继续执行任务，尽管可能没有原来的表现那么好。大脑具有容错能力；随着神经元的破坏，其性能会缓慢下降。
- en: Artificial neural networks
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: In a similar way to a biological neuron, an artificial neuron receives various
    stimuli in input, each of which is the output of another neuron. Each input is
    then multiplied by a corresponding weight and added to the others to determine
    the level of neuron activation by another function.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于生物神经元，人工神经元接收来自不同神经元的输入刺激。每个输入会乘以一个相应的权重，并与其他输入相加，进而通过另一个函数决定神经元的激活水平。
- en: 'The architecture of a neural network is characterized by the distinction between
    input neurons and output neurons, the number of layers of synapses (or neurons),
    and the presence of feedback connections, as shown in the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的架构特点在于输入神经元与输出神经元的区分、突触（或神经元）层数以及反馈连接的存在，如下图所示：
- en: '![](img/3f591bb8-d3f4-4be8-bbb0-be0b3f1a1306.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f591bb8-d3f4-4be8-bbb0-be0b3f1a1306.png)'
- en: When an input vector (stimulus) is applied to the input neurons of the neural
    network, the signals travel in parallel along the connections through the internal
    nodes, up to the output and hence produce the response of the neural network.
    In the simplest formulation, each node processes only the local information, does
    not know the overall purpose of the processing, and has no memory. The response
    and behavior of the network depend intrinsically on its architecture and the value
    of artificial synapses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入向量（刺激）作用于神经网络的输入神经元时，信号沿着连接以并行方式在内部节点之间传播，直到输出并产生神经网络的响应。在最简单的模型中，每个节点仅处理局部信息，不知道整体处理目标，并且没有记忆。网络的响应和行为本质上取决于其架构和人工突触的值。
- en: 'In some cases, a single layer of synapses is not sufficient to learn the desired
    association between input and output patterns: in these cases, it is necessary
    to use multi-layer networks that possess internal neurons and more than one layer
    of synapses. These networks are called deep neural networks. The response of such
    a network is obtained by calculating the activation of a layer of neurons at a
    time proceeding gradually from the internal nodes toward the exit nodes.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，单一的突触层不足以学习输入和输出模式之间的期望关联：在这种情况下，需要使用多层网络，具有内部神经元和多个突触层。这些网络被称为深度神经网络。这样的网络响应是通过逐层计算神经元的激活值，逐渐从内部节点向输出节点推进得到的。
- en: 'An artificial neural network goal is simply the computation of the outputs
    of all of the neurons, through a deterministic calculation. Basically, ANN is
    a set of mathematical function approximations. The following elements are essential
    in an ANN architecture:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的目标仅仅是通过确定性计算计算所有神经元的输出。基本上，ANN是一组数学函数逼近。以下元素是ANN架构中至关重要的：
- en: Layers
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层
- en: Weights
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重
- en: Bias
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置
- en: Activation function
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: In the following sections, we will dive deeper into these concepts.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将深入探讨这些概念。
- en: Layers types
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层类型
- en: 'We already introduced the architecture of an artificial neural network in the
    *Artificial neural networks* section, and we have been able to analyze a scheme
    in which different types of neurons were highlighted. In that scheme, it is possible
    to identify a structure in layers. In fact, we can easily identify an input layer,
    a middle layer (named hidden layer), and an output layer, as shown in the following
    diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*人工神经网络*部分介绍了人工神经网络的架构，并且能够分析一个突出不同类型神经元的方案。在该方案中，可以识别出一个层次结构。事实上，我们可以轻松地识别出输入层、一个中间层（称为隐藏层）和输出层，如下图所示：
- en: '![](img/625dcdbc-3f1d-4829-b703-2cbfaf8b798c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/625dcdbc-3f1d-4829-b703-2cbfaf8b798c.png)'
- en: In the previous diagram, it is possible to identify the simplest of architectures
    that includes an input layer, a single hidden layer, and an output layer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中，可以识别出最简单的架构，包括输入层、一个隐藏层和输出层。
- en: Each layer has its own task that it performs through the action of the neurons
    it contains. The input layer is intended to introduce the initial data into the
    system for further processing by the subsequent layers. From the input level,
    the workflow of the artificial neural network begins.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都有自己的任务，通过它所包含的神经元的作用来执行。输入层的作用是将初始数据输入系统，以便后续层进一步处理。从输入层开始，人工神经网络的工作流程启动。
- en: In the input layer, artificial neurons have a different role to play in some
    *passive* way because they do not receive information from previous levels. In
    general, they receive a series of inputs and introduce the information into the
    system for the first time. This level then sends the data to the next levels,
    where the neurons receive weighted inputs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入层，人工神经元扮演着不同的角色，因为它们不会接收来自前一层的信息。在一般情况下，它们接收一系列输入，并将信息首次引入系统。然后，这一层将数据传递给下一层，在那里神经元接收加权输入。
- en: The hidden layer in an artificial neural network is interposed between input
    levels and output levels. The neurons of the hidden layer receive a set of weighted
    inputs and produce an output according to the indications received from an activation
    function. It represents the essential part of the entire network, as it is here
    that the magic of transforming the input data into output responses takes place.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络中的隐藏层位于输入层和输出层之间。隐藏层的神经元接收一组加权输入，并根据从激活函数接收到的指示产生输出。它代表了整个网络的核心部分，因为正是在这里，输入数据转化为输出响应的“魔法”发生。
- en: 'Hidden levels can operate in many ways. In some cases, the inputs are weighted
    randomly; in others they are calibrated through an iterative process. In general,
    the neuron of the hidden layer functions as a biological neuron in the brain:
    it takes its probabilistic input signals, processes them, and converts them into
    an output corresponding to the axon of the biological neuron.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层可以以多种方式操作。在某些情况下，输入是随机加权的；在其他情况下，它们通过迭代过程进行调整。通常，隐藏层的神经元类似于大脑中的生物神经元：它接收其概率输入信号，对其进行处理，并将其转换为对应于生物神经元轴突的输出。
- en: Finally, the output layer produces certain outputs for the model. Although they
    are made in a way very similar to other artificial neurons in the neural network,
    the type and number of neurons in the output layer depend on the type of response
    the system must provide. For example, if we are designing a neural network for
    the classification of an object, the output layer will consist of a single node
    that will provide us with this value. In fact, the output of this node must simply
    provide a positive or negative indication of the presence or absence of the target
    in the input data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层为模型生成特定的输出。虽然它们的生成方式与神经网络中的其他人工神经元非常相似，但输出层中神经元的类型和数量取决于系统必须提供的响应类型。例如，如果我们正在设计一个用于对象分类的神经网络，则输出层将由一个节点组成，该节点将为我们提供此值。事实上，该节点的输出只需提供一个正面或负面的指示，表示目标在输入数据中是否存在。
- en: Weights and biases
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重和偏置
- en: In an artificial neural network, the conversion of an input into an output takes
    place thanks to the contribution of the weights of the connections. In linear
    regression, the slope is multiplied by the input to provide the output. The same
    argument can be made for weights in a neural network. In fact, they represent
    numerical parameters that specify the contribution of each neuron to the final
    result. For example, if the inputs are *x[1]*, *x[2]*, and *x[3]*, the synaptic
    weights to be applied to these are indicated as *w[1]*, *w[2]*, and *w[3]*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络中，输入转换为输出依赖于连接权重的贡献。在线性回归中，斜率与输入相乘以提供输出。对于神经网络中的权重，也可以做出相同的论点。实际上，权重是表示每个神经元对最终结果贡献的数值参数。例如，如果输入是
    *x[1]*、*x[2]* 和 *x[3]*，则应用于这些输入的突触权重分别表示为 *w[1]*、*w[2]* 和 *w[3]*。
- en: 'In this assumption, we can represent the output returned by the neuron through
    the following formula:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种假设下，我们可以通过以下公式表示神经元返回的输出：
- en: '![](img/4256d04d-bce5-4e1b-88ed-11f17e638b6d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4256d04d-bce5-4e1b-88ed-11f17e638b6d.png)'
- en: In the previous formula, *i* is the number of inputs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*i* 是输入的数量。
- en: In the preceding formula, the matrix multiplication defines a weighted sum.
    To this weighted sum, it is necessary to add the bias that can be compared to
    the added intercept in a linear equation. The bias is, therefore, an additional
    parameter that is used to adjust the output of each neuron.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，矩阵乘法定义了加权和。为了这个加权和，必须加上偏置，这可以与线性方程中的加截距进行比较。因此，偏置是一个额外的参数，用于调整每个神经元的输出。
- en: 'The processing done by a neuron is hence denoted as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的处理过程因此表示如下：
- en: '![](img/c0f19659-6821-499f-8683-42de46ad1f66.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0f19659-6821-499f-8683-42de46ad1f66.png)'
- en: 'The output is adjusted by the activation function. The output of neurons in
    a level will represent the input of the next level, as shown in the following
    diagram:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出由激活函数进行调整。某一层神经元的输出将代表下一层的输入，如下图所示：
- en: '![](img/29ed19a0-afef-442c-8bab-2f7cb2683f12.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29ed19a0-afef-442c-8bab-2f7cb2683f12.png)'
- en: The meaning of this scheme is that we are giving the input signal (*x[i]*) a
    weight (*w[i]*), which is a real number that reproduces the natural synapse. When
    the value *w[i]* is greater than zero, the channel is called **excitatory**; if
    the value is less than zero, the channel is inhibitory. The absolute value of
    *w[i]* represents the strength of the connection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方案的含义是，我们给输入信号（*x[i]*）赋予一个权重（*w[i]*），它是一个实数，模拟了自然突触。当* w[i]*的值大于零时，通道被称为**兴奋性**；如果值小于零，则通道为抑制性。*w[i]*的绝对值表示连接的强度。
- en: Activation functions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'The activation function plays a crucial role in processing system output. The
    activation function represents a mathematical function that converts the input
    into output and defines the process based on neural networks. Without the contribution
    of the activation function, a neural network is trivialized to a simple linear
    function. In a linear function, the conversion from input to output is realized
    through a direct proportionality, as shown in the following example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在处理系统输出中起着至关重要的作用。激活函数表示一种数学函数，它将输入转换为输出，并根据神经网络定义处理过程。如果没有激活函数的贡献，神经网络将简化为一个简单的线性函数。在线性函数中，从输入到输出的转换通过直接的比例关系实现，如下所示：
- en: '![](img/aa07f9f6-1e5d-4eda-9545-da04c1ead7e7.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa07f9f6-1e5d-4eda-9545-da04c1ead7e7.png)'
- en: 'Simply, a linear function is a polynomial of the first degree, then a straight
    line. In the real world, most problems are non-linear and complex in nature. To
    deal with non-linear problems, it is necessary to use the activation functions.
    Nonlinear functions are high degree polynomial functions, as shown in the following
    example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，线性函数是一阶多项式，即一条直线。在现实世界中，大多数问题都是非线性且复杂的。为了解决非线性问题，必须使用激活函数。非线性函数是高阶多项式函数，如下所示：
- en: '![](img/66702e7f-863e-4487-aa2f-756cbdbf85f8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66702e7f-863e-4487-aa2f-756cbdbf85f8.png)'
- en: It is a non-linear function that contains a factor of complexity. The activation
    functions add the non-linearity property to neural networks and characterize them
    as approximators of universal functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个包含复杂度因子的非线性函数。激活函数为神经网络添加了非线性特性，并将其表征为通用函数的逼近器。
- en: 'There are many activation functions available for a neural network to use.
    The following are the most used:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可使用许多激活函数。以下是最常用的几种：
- en: '**Sigmoid**: This function is represented by a sigmoid curve, typical for its
    S shape. This is the most used activation function. Its action is to transform
    the input into a value between 0 and 1\. In this way, the model takes on a logistical
    nature.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：该函数由S形的Sigmoid曲线表示。它是最常用的激活函数。它的作用是将输入转换为0和1之间的值。这样，模型呈现出一种逻辑性质。'
- en: '**Unit step**: This function transforms the input into 0 if the argument is
    negative and 1 if the argument is positive. In this way, the output takes on a
    binary nature. These activation functions are used for binary schemes.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单位阶跃**：该函数将输入转换为0（如果参数为负数），或1（如果参数为正数）。这样，输出具有二进制性质。这些激活函数用于二进制方案。'
- en: '**Hyperbolic tangent**: It is a non-linear function, defined in the range of
    values (-1, 1). These functions are interesting because they allow the neuron
    to have a continuous output, which allows a probabilistic interpretation.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双曲正切**：它是一个非线性函数，定义在值的范围内（-1, 1）。这些函数很有趣，因为它们允许神经元具有连续的输出，从而可以进行概率解释。'
- en: '**Rectified Linear Unit** (**ReLU**): It is a function with linear characteristics
    for parts of the existence domain that will output the input directly if is positive;
    otherwise, it will output zero. The range of output is between 0 and infinity.
    ReLU finds applications in computer vision and speech recognition using deep neural
    networks.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整流线性单元**（**ReLU**）：它是一个具有线性特性的函数，对于存在域的部分，如果输入是正数，则直接输出输入值；否则，输出为零。其输出范围介于0和无限大之间。ReLU在计算机视觉和语音识别中通过深度神经网络得到应用。'
- en: Managing feedforward neural networks
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理前馈神经网络
- en: When the flow passes from the input layer to the hidden layers and therefore
    to the output layer, we talk about feed-forward propagation. In this case, the
    transfer function is applied to each hidden level. Hence, the value of the activation
    function is propagated to the next level. The next layer can be another hidden
    layer or the output layer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据流从输入层传递到隐含层，再到输出层时，我们称之为前馈传播。在这种情况下，转移函数应用于每一层隐藏层。因此，激活函数的值会传播到下一层。下一层可以是另一层隐藏层，或者是输出层。
- en: The term **feedforward** is used to indicate the networks in which each node
    receives connections only from the lower layers. These networks emit a response
    for each input pattern but fail to capture the possible temporal structure of
    the input information or to exhibit endogenous temporal dynamics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈**一词用于表示每个节点仅接收来自低层的连接的网络。这些网络会对每个输入模式发出响应，但无法捕捉输入信息可能的时间结构或展示内生的时间动态。'
- en: 'Now let''s move on to a crucial topic for neural networks: neural network training.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入神经网络的一个关键话题：神经网络训练。
- en: Neural network training
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: 'To choose the input values for which a neuron turns on or off, the network
    is trained. This is a crucial step in the realization of the model, which consists
    of training the neural network to generalize the information, starting from a
    set of inputs corresponding to known outputs. The performances of the network
    depend very much on the information presented to them: they must be representative
    of what the network must learn. Training is a fundamental part of building a neural
    network and the examples to be used (training set) must be carefully chosen.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择使神经元开启或关闭的输入值，需要训练网络。这是实现模型中的关键步骤，旨在训练神经网络以从一组对应已知输出的输入中进行泛化。网络的性能在很大程度上取决于呈现给它们的信息：这些信息必须代表网络需要学习的内容。训练是构建神经网络的基础，所使用的示例（训练集）必须仔细选择。
- en: 'We shall take a step-by-step approach to understand the neural network training
    with a single hidden layer. Let''s take the input layer has one neuron and the
    output will solve a binary classification problem (predict 0 or 1). Here''s a
    list of all the steps for training a network:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取逐步的方式来理解具有单一隐藏层的神经网络训练。假设输入层有一个神经元，输出将解决一个二分类问题（预测0或1）。以下是训练网络的所有步骤列表：
- en: Load the input as a matrix.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入加载为矩阵。
- en: Use random values to initialize weights and biases. This step must be done only
    at the beginning, then just update them.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化权重和偏置。这一步仅在开始时进行，之后只需更新它们。
- en: Repeat the following steps from 4 to 9 for each epoch, until convergence.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个周期重复步骤 4 到 9，直到收敛。
- en: Send the inputs to the network.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入传递到网络中。
- en: Estimate the output from the input layer, through the hidden layer(s), to the
    output layer.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入层通过隐藏层(s)到输出层，估计输出。
- en: Estimate the error at the outputs.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计输出的误差。
- en: Adopt the output error to calculate error signals for previous layers.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采用输出误差来计算前面层的误差信号。
- en: Adopt the error signals to calculate weight changes.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采用误差信号计算权重变化。
- en: Use the weight changes to update them.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用权重变化来更新权重。
- en: '*Steps 4* and *5* are forward propagation and *steps 6* through *9* are backpropagation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4* 和 *5* 是前向传播，*步骤 6* 到 *9* 是反向传播。'
- en: 'In general, the most used method to teach a network to generalize through the
    adjustment of neuron weights (*w[i]*) is to follow the **delta rule**, which consists
    of comparing the network outputs with the desired values: subtract the two values
    and the difference is used to update all of the weights of the inputs that have
    different values of zero.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，教会网络通过调整神经元权重 (*w[i]*) 来进行泛化的最常用方法是遵循**增量规则**，该规则通过比较网络输出与期望值：将两个值相减，差值用于更新所有输入的权重，其中这些输入的值不为零。
- en: 'The process is iterated until convergence is reached. The following diagram
    shows a graph of the net weight adjustment procedure:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程会迭代，直到收敛为止。下图显示了净重调整过程的图示：
- en: '![](img/82ab85ec-fe9a-4657-8e9c-216e5cc8a8af.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82ab85ec-fe9a-4657-8e9c-216e5cc8a8af.png)'
- en: 'In practice, the algorithm compares the inputs with the outputs: the difference
    between the weighted input values and the output or expected values is calculated
    and the difference (error) is used to recalculate all the input weights. The procedure
    is repeated until the error between input and output becomes close to zero.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，算法将输入与输出进行比较：计算加权输入值与输出或期望值之间的差异，并利用该差异（误差）重新计算所有输入权重。该过程会反复进行，直到输入和输出之间的误差接近于零。
- en: In the following section, we will apply neural networks to solve a regression
    problem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将应用神经网络来解决回归问题。
- en: Neural network for regression
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归神经网络
- en: Regression analysis is the starting point in data science; in fact, they are
    the most well-understood models in numerical simulation. Regression models are
    easily interpreted as they are based on solid mathematical bases—think of matrix
    algebra. Linear regression allows us to derive a mathematical formula representative
    of the corresponding model. Therefore, these techniques are extremely easy to
    understand.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析是数据科学的起点；事实上，它们是数值模拟中最为理解透彻的模型。回归模型易于解释，因为它们基于坚实的数学基础——可以想象为矩阵代数。线性回归允许我们推导出一个数学公式来代表相应的模型。因此，这些技术极易理解。
- en: Regression analysis is a statistical process aimed at identifying the relationship
    between a set of independent variables (explanatory variables) and the dependent
    variable (response variable). With this technique, it is possible to establish
    how the value of the response variable changes when the explanatory variable is
    varied.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析是一种统计过程，旨在识别一组自变量（解释变量）与因变量（响应变量）之间的关系。通过这项技术，可以确定当解释变量变化时，响应变量的值如何变化。
- en: In the following paragraphs, an example of a regression predictive modeling
    problem is proposed to understand how to solve it with neural networks. The Boston
    dataset will be used as a data source; the median values of owner-occupied homes
    are predicted for the test data. The dataset describes 12 numerical properties
    of houses in Boston suburbs and is concerned with modeling the price of houses
    in those suburbs in thousands of dollars. As such, this is a regression predictive
    modeling problem because the output is a continuous variable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，提出了一个回归预测建模问题的示例，旨在理解如何通过神经网络来解决。将使用波士顿数据集作为数据来源；预测测试数据中业主自住住宅的中位数价格。该数据集描述了波士顿郊区房屋的12个数值特征，目的是建模这些郊区房屋的价格（以千美元为单位）。因此，这是一个回归预测建模问题，因为输出是一个连续变量。
- en: 'Remember that regression and classification are both related to the forecast:
    in classification, we try to predict the output by grouping it into classes (categorical
    variable) while, in regression, we try to predict the output value in a continuous
    way (continuous variable).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，回归与分类都与预测有关：在分类中，我们试图通过将输出分组为类别（分类变量）来预测输出，而在回归中，我们试图以连续的方式（连续变量）预测输出值。
- en: The Boston dataset input attributes include features such as crime rate, the
    proportion of non-retail business acres, and chemical concentrations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集的输入属性包括犯罪率、非零售商业区土地比例和化学物质浓度等特征。
- en: 'To get the data, we draw on the large collection of data available in the UCI
    Machine Learning Repository at the following link: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 获取数据时，我们利用 UCI 机器学习库中的大量数据集，数据链接如下：[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。
- en: 'The number of instances and the number of variables are shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 实例数量和变量数量如下所示：
- en: 'Number of instances: 506'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例数量：506
- en: 'Number of variables: 13 continuous variables (including the class attribute,
    `medv`) and 1 binary-valued attribute'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量数量：13 个连续变量（包括类属性 `medv`）和 1 个二值属性
- en: 'All the variables are shown in the following list:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所有变量如下所示：
- en: '`crimper`: Capita crime rate by town'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crimper`：每个城镇的人均犯罪率'
- en: '`zn`: Proportion of residential land zoned for lots over 25,000 square feet'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zn`：规划为大于 25,000 平方英尺地块的住宅用地比例'
- en: '`indus`: Proportion of non-retail business acres per town'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indus`：每个城镇的非零售商业区土地比例'
- en: '`chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chas`：查尔斯河虚拟变量（= 1 如果地块与河流相邻；否则为 0）'
- en: '`nox`: Nitric oxides concentration (parts per 10 million)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nox`：氮氧化物浓度（每千万分之一）'
- en: '`rm`: Average number of rooms per dwelling'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rm`：每个住宅的平均房间数'
- en: '`age`: Proportion of owner-occupied units built before 1940'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`：1940年之前建造的房主自有住房比例'
- en: '`dis`: Weighted distances to five Boston employment centers'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dis`：到波士顿五个就业中心的加权距离'
- en: '`rad`: Index of accessibility to radial highways'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rad`：径向公路可达性指数'
- en: '`tax`: Full-value property-tax rate per $10,000'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tax`：每$10,000的房产税率'
- en: '`ptratio`: Pupil-teacher ratio by town'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ptratio`：每个城镇的师生比例'
- en: '`lstat`: Percent lower status of the population'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lstat`：人口中低社会经济地位的比例'
- en: '`medv`: Median value of owner-occupied homes in $1,000s'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`medv`：房主自有住宅的中位数价值（以千美元为单位）'
- en: In the previous list, `medv` represents the response variable, and the other
    thirteen variables are the predictors. Our goal is to develop a regression model
    that simulates the variation of the `medv` value. The model should be able to
    identify the relationship between the first thirteen columns and the response
    variable `medv`, if it exists.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的列表中，`medv`是响应变量，其他13个变量是预测变量。我们的目标是开发一个回归模型，用以模拟`medv`值的变化。该模型应该能够识别前13列与响应变量`medv`之间的关系（如果存在的话）。
- en: This dataset is already provided with R libraries (MASS), so we do not have
    to worry about retrieving the data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已经通过R库（MASS）提供，因此我们无需担心获取数据。
- en: 'First, we have to get the data. To do this, as we said, we can use the MASS
    libraries:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要获取数据。正如我们所说的那样，我们可以使用MASS库：
- en: 'Let''s load the library:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载所需的库：
- en: '[PRE0]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To install a new library, you need to use the `install.packages()` function.
    This feature installs the packages. It is necessary to pass a vector of names
    and a destination library, after which the command downloads the packages from
    the repositories and installs them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装一个新的库，需要使用`install.packages()`函数。这个功能安装所需的包。需要传递一个包含名称的向量和目标库，之后该命令将从仓库下载并安装这些包。
- en: 'Let''s now concern ourselves with making the experiment reproducible:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们关注如何使实验具有可重复性：
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `set.seed()` command makes the example reproducible, in the sense that all
    of the random numbers generated will always be the same, for each simulation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`set.seed()`命令使得示例可重复，即每次模拟生成的所有随机数始终相同。'
- en: 'We can now load the dataset:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以加载数据集了：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will only use the variables necessary for our analysis:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将只使用对分析有用的变量：
- en: '[PRE3]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The dataframe also includes the names of the variables as they are in the original
    dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框还包括原始数据集中变量的名称。
- en: Let's start by taking a look at the data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看数据。
- en: Exploratory analysis
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性分析
- en: We now perform an exploratory analysis to see how the data is distributed and
    to extract the preliminary knowledge. Let's start by checking the dataset using
    the `str()` function. This function returns a tight summary of the internal structure
    of an R object.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进行探索性分析，看看数据是如何分布的，并提取初步知识。让我们首先使用`str()`函数检查数据集。这个函数返回一个紧凑的R对象内部结构总结。
- en: 'Ideally, only one line for each basic structure is displayed:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，每个基本结构只显示一行：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following results are returned:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果如下：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So, we got the confirmation that it was 506 observations of 13 variables: 11
    numericals and 2 integers. Now, to obtain a brief summary of the dataset, we can
    use the `summary()` function, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们确认数据集包含506个观测值和13个变量：11个数值型和2个整数型。现在，为了获得数据集的简要总结，我们可以使用`summary()`函数，如下所示：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `summary()` function returns a series of data statistics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary()`函数返回一系列数据统计信息。'
- en: 'The results are shown here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示如下：
- en: '![](img/f1d52e33-2f77-4138-9fae-ba4d6be8d2ce.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1d52e33-2f77-4138-9fae-ba4d6be8d2ce.png)'
- en: The analysis of the results shows that the variables have different intervals.
    When the predictors have very different extreme values, the weight on the response
    variables by the character with extreme values may be prevalent. This can affect
    the accuracy of the forecast. Hence, we may need to scale values under different
    features such that they fall under a common range. Through this statistical procedure,
    it is possible to compare identical variables belonging to different distributions
    and also different variables or variables expressed in different units.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 结果分析表明，变量的范围不同。当预测变量的极端值差异很大时，具有极端值的特征对响应变量的权重可能会占主导地位，这会影响预测的准确性。因此，我们可能需要对不同特征下的值进行缩放，以便它们落在一个共同的范围内。通过这一统计过程，可以比较属于不同分布的相同变量，也可以比较以不同单位表示的变量。
- en: Remember, it is good practice to rescale the data before training a regression
    algorithm. Using the rescaling technique, data units are eliminated; this allows
    us to easily compare data from different locations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在训练回归算法之前，重新缩放数据是一个好习惯。使用重新缩放技术可以消除数据单位，这使得我们可以轻松比较来自不同位置的数据。
- en: 'To rescale the data, we will use the min-max method to get all the scaled data
    in the range [0, 1]. The formula to achieve this is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重新缩放数据，我们将使用最小-最大方法，将所有数据缩放到[0, 1]范围内。实现这一点的公式如下：
- en: '![](img/312c9d15-12ff-45cc-940a-992681a690fb.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/312c9d15-12ff-45cc-940a-992681a690fb.png)'
- en: 'First, we need to calculate the minimum and maximum values of each column in
    the database. We will use the `apply()` function that applies a function to the
    values of a matrix:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要计算数据库中每一列的最小值和最大值。我们将使用`apply()`函数将函数应用于矩阵的值：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Three arguments have been passed: the first specifies the data set on which
    to apply the function (`InputData`). The second argument specifies the indexes
    on which the function (2) will be applied. Being a matrix, 1 specifies the rows
    and 2 specifies the columns. The third argument specifies the function to be applied,
    in our case, the `max()` function.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 三个参数已经传递：第一个指定要应用函数的数据集（`InputData`）。第二个参数指定函数（2）将应用的索引。由于是矩阵，1指定行，2指定列。第三个参数指定要应用的函数，在我们的案例中是`max()`函数。
- en: 'Now, we will calculate the minimums for each column:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算每一列的最小值：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we need to apply the `scale ()` function to normalize the data. The `scale
    ()` function centers and/or resizes the columns of a numeric matrix, as shown
    here:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要应用`scale()`函数来归一化数据。`scale()`函数对数值矩阵的列进行中心化和/或缩放，如下所示：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To confirm the normalization of the data, let''s apply the `summary()` function
    again:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认数据是否已归一化，我们再次应用`summary()`函数：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following results are printed:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出以下结果：
- en: '![](img/907c7fbc-9270-456b-81ff-41f777c3e596.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/907c7fbc-9270-456b-81ff-41f777c3e596.png)'
- en: 'Let''s go into our exploratory analysis. We can do it by making a boxplot of
    the variables, as shown here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始我们的探索性分析。我们可以通过绘制变量的箱型图来进行，如下所示：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following plot is printed:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出以下图表：
- en: '![](img/8e7c11ff-e4e7-448c-bea5-079223d09348.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e7c11ff-e4e7-448c-bea5-079223d09348.png)'
- en: The previous diagram clearly shows that some variables have anomalous values.
    For example, the variable crim shows the greatest number of outliers. Outliers
    are numerically different from the rest of the collected data. Statistics obtained
    from variables containing anomalous values may return incorrect information.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表清楚地显示了一些变量具有异常值。例如，变量crim显示了最多的离群值。离群值在数值上与其余数据明显不同。含有异常值的变量所获得的统计数据可能会返回错误的信息。
- en: Training the network
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Before training the network, we must split the data. We will start with data
    splitting, subdividing data into exactly two subsets of a specified ratio for
    training and validation. This technique is particularly useful when you have a
    very large dataset. In this case, the dataset is divided into two partitions:
    training and test. The training set is used to train the model, while the test
    set will provide us with a significant performance estimate. This method is very
    advantageous when using slow methods and needing a quick approximation of performance.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络之前，我们必须先拆分数据。我们将从数据拆分开始，将数据按照指定的比例划分为训练集和验证集。在数据集非常大的情况下，这种技术尤其有用。在这种情况下，数据集被划分为两个部分：训练集和测试集。训练集用于训练模型，而测试集则为我们提供了显著的性能估计。当使用缓慢的算法并且需要快速估算性能时，这种方法非常有利。
- en: 'The following example divides the dataset so that 70 percent is used to train
    a neural network model and the remaining 30 percent is used to evaluate model
    performance:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将数据集划分为70%的数据用于训练神经网络模型，其余30%的数据用于评估模型性能：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The code in the first line subdivides the 70:30 data, meaning to use 70 percent
    of the data to train the network and the remaining 30 percent to test the network.
    In the second and third row, the data of the dataframe named `DataScaled` is subdivided
    into two new dataframes called `TrainData` and `TestData`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行的代码将70:30的数据进行拆分，即使用70%的数据来训练网络，剩下的30%用于测试网络。在第二行和第三行中，名为`DataScaled`的数据框被拆分为两个新数据框，分别称为`TrainData`和`TestData`。
- en: 'Now, we need to set the formula we will use to build the neural network:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要设置构建神经网络时使用的公式：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the previous piece of code, we first retrieve all variable names through
    the `names()` function. Next, we create the formula we will use to build the network.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先通过`names()`函数获取所有变量名。接下来，我们创建用于构建网络的公式。
- en: The `neuralnet()` function uses formulas in a compact symbolic form. The `~`
    operator defines the model. For example, the **formula y ~** model is interpreted
    as meaning that the answer y is modeled by a predictor specified symbolically
    by the model. This model consists of a series of terms separated by `+` operators.
    Each term is a variable name separated from others by `:` operators.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet()`函数使用紧凑的符号形式的公式。`~`操作符定义了模型。例如，**公式y ~**模型的意思是，输出y由符号方式指定的预测变量建模。该模型由一系列通过`+`操作符分隔的项组成，每项是通过`:`操作符分隔的变量名。'
- en: 'Then, we will use the `neuralnet` library to build and train the network. Let''s
    load the library:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`neuralnet`库来构建和训练网络。让我们加载该库：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `neuralnet` library is used to train neural networks using backpropagation,
    resilient backpropagation (RPROP) with or without weight backtracking, or the
    modified globally convergent version (GRPROP). The following table gives some
    information about this package:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet`库用于使用反向传播、弹性反向传播（RPROP）以及带或不带权重回溯的训练神经网络，或者使用修改后的全局收敛版本（GRPROP）。以下表格提供了该包的一些信息：'
- en: '| Package | `neuralnet` |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 包 | `neuralnet` |'
- en: '| Date | 2019-02-07 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2019-02-07 |'
- en: '| Version | 1.44.2 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 1.44.2 |'
- en: '| Title | Training of Neural Networks |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 神经网络的训练 |'
- en: '| Author | Stefan Fritsch, Frauke Guenther, Marvin N. Wright, Marc Suling,
    Sebastian M. Mueller |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | Stefan Fritsch, Frauke Guenther, Marvin N. Wright, Marc Suling, Sebastian
    M. Mueller |'
- en: 'The following lists the most useful functions contained in this package:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了该包中最有用的函数：
- en: '`neuralnet`: Training of neural networks'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neuralnet`：神经网络的训练。'
- en: '`compute`: Computation of a given neural network for given covariate vectors'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compute`：计算给定协变量向量的神经网络结果。'
- en: '`prediction`: Summarizes the output of the neural network, the data and the
    fitted values of `glm` objects (if available)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction`：总结神经网络的输出、数据以及`glm`对象的拟合值（如果可用）。'
- en: '`plot.nn`: The plot method for neural networks'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot.nn`：神经网络的绘图方法。'
- en: 'Now, we can build and train the network. At first, we have to choose the number
    of neurons, and to do this, we need to know the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建和训练网络。首先，我们必须选择神经元的数量，为此，我们需要了解以下内容：
- en: The choice of a layer on a few neurons will cause a high error; this is because
    the predictive factors could be too complex.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择较少神经元的层会导致较高的误差；这是因为预测因子可能过于复杂。
- en: On the contrary, too many neurons overload training data and do not allow generalization.
    The number of neurons in each hidden layer should be a number between the input
    size and the output layer, for example, an average.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，过多的神经元会使训练数据过载，无法进行泛化。每个隐藏层中的神经元数量应介于输入层和输出层之间，例如取平均值。
- en: The number of neurons in each hidden layer should not exceed twice the number
    of incoming neurons.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层中的神经元数量不应超过输入神经元数量的两倍。
- en: 'We choose to set ten neurons in the hidden layer. Do not worry—the best choice
    is obtained with experience:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择在隐藏层设置十个神经元。别担心——最佳选择通过经验得出：
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The hidden argument specifies the number of neurons for each hidden layer. The
    `linear.output` argument performs a regression if `linear.output=TRUE` or a classification
    if `linear.output=FALSE`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`hidden`参数指定每个隐藏层的神经元数量。`linear.output`参数执行回归（如果`linear.output=TRUE`）或分类（如果`linear.output=FALSE`）。'
- en: 'To produce result summaries of the results of the model, we use the `summary()`
    function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成模型结果的汇总，我们使用`summary()`函数：
- en: '[PRE16]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following results are returned:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Three features are displayed for each component of the neural network model:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经网络模型组件显示三个特征：
- en: '**Length:** This feature specifies how many elements of this type are contained
    in it.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长度：** 此功能指定该类型包含的元素数量。'
- en: '**Class:** This feature returns a specific indication on the component class.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类：** 此功能返回有关组件类别的具体指示。'
- en: '**Mode:** This feature describes the type of component (numeric, list, function,
    logical, and so on).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式：** 此功能描述了组件的类型（数字、列表、函数、逻辑等）。'
- en: 'The `plot()` function draws a graph indicating the neural network architecture
    with layers, nodes, and weights on each connection:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot()`函数绘制一张图，显示神经网络的架构，包括各层、节点和每个连接的权重：'
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The neural network plot is shown in the following diagram:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络图如下所示：
- en: '![](img/40121ab6-a594-487d-b78a-296fd0071878.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40121ab6-a594-487d-b78a-296fd0071878.png)'
- en: In the previous diagram, the black lines represent the connections between each
    layer; also, the weight values on each connection are printed. The blue lines
    show the added bias in each step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，黑线表示各层之间的连接；此外，每个连接上的权重值也被打印出来。蓝线显示了每一步中添加的偏置。
- en: Neural network model evaluation
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型评估
- en: 'Now, we can use the network to make predictions. For this, we had set aside
    30% of the data in the `TestData` dataframe. It is time to use it:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用网络进行预测。为此，我们已经将30%的数据保留在`TestData`数据框中。现在是时候使用它了：
- en: '[PRE19]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How can we figure whether the forecasts performed by the network are accurate?
    We can use the **mean squared error** (**MSE**) as a measure of how far our predictions
    are from the real data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何判断网络的预测是否准确呢？我们可以使用**均方误差**（**MSE**）作为衡量我们预测与真实数据之间差距的标准。
- en: 'In the first part of the algorithm, we have normalized the data. To compare
    the data we need to step back and return to the original data. Once the values
    of the dataset are restored, we can calculate the MSE through the following equation:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法的第一部分，我们对数据进行了归一化。为了比较数据，我们需要回退并返回到原始数据。一旦数据集的值被恢复，我们就可以通过以下方程计算MSE：
- en: '![](img/5799058f-14d5-4c07-a5d0-9a460b9f8f81.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5799058f-14d5-4c07-a5d0-9a460b9f8f81.png)'
- en: 'The following code performs an MSE calculation:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行了MSE计算：
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we have results, but what do we compare them with? To compare the results
    with another model, we can construct a linear regression model. Then, we elaborate
    on a linear regression model by applying the `lm()` function. This function is
    used to process linear regression models, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了结果，但我们该与什么进行比较呢？为了与另一个模型进行比较，我们可以构建一个线性回归模型。然后，我们通过应用`lm()`函数来建立线性回归模型。这个函数用于处理线性回归模型，如下所示：
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To produce a summary of the results of the model, we can use once again the
    `summary()` function as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成模型结果的总结，我们可以再次使用`summary()`函数，如下所示：
- en: '[PRE22]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following results are returned:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是返回的结果：
- en: '![](img/1f091fae-3580-4f36-b6ea-949a7583f484.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f091fae-3580-4f36-b6ea-949a7583f484.png)'
- en: 'Now, we will calculate the MSE for the model based on multiple regression:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算基于多重回归的模型的MSE：
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can compare the results of both models:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以比较两个模型的结果：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following results are printed:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是打印出的结果：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: From the comparison between the two models (neural network model versus linear
    regression model), the neural network wins (19.4 versus 34.8).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从两个模型（神经网络模型与线性回归模型）的比较中可以看出，神经网络获胜（19.4 对 34.8）。
- en: In the following section, we will see how it is possible to develop a DRL model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到如何开发一个DRL模型。
- en: Approaching DRL
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接近DRL
- en: In [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml), *Temporal Difference
    Learning*, we addressed a practical example of using Q-learning to address a vehicle
    routing problem. In that case, the estimates of the value function were made using
    a table, in which each box represents a state or a state-action pair. The use
    of a table to represent the value function allows the creation of simple algorithms.
    Under Markovian environmental conditions, this table allows us to accurately estimate
    the value function since it assigns the expected performance during the iterations
    of the policies to every possible configuration from the environment. The use
    of the table, however, also leads to limitations. These methods apply only to
    environments with a reduced number of states and actions. The problem is not limited
    to the large amount of memory required to store the table, but above all, to the
    large amount of data and time required to accurately estimate each state-action
    pair. In other words, the main problem is generalization.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第七章](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml)，*时间差分学习*中，我们展示了一个实际的例子，使用Q-learning来解决车辆路径规划问题。在那个案例中，值函数的估计是通过一个表格完成的，其中每个格子代表一个状态或一个状态-动作对。使用表格表示值函数能够创建简单的算法。在马尔可夫环境下，这个表格允许我们准确地估计值函数，因为它为环境的每一个可能配置分配了在策略迭代过程中预期的表现。然而，使用表格也带来了局限性。这些方法仅适用于状态和动作数目较少的环境。问题不仅限于存储表格所需的大量内存，更主要的是准确估计每个状态-动作对所需的大量数据和时间。换句话说，主要问题在于泛化能力。
- en: 'To solve this problem, we can adopt a method based on the combination of reinforcement
    learning methods with function approximation methods. The following diagram shows
    a deep Q-learning scheme:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以采用一种基于强化学习方法与函数逼近方法结合的方案。下图展示了一个深度Q学习方案：
- en: '![](img/0470eccc-f797-4e9e-af06-899bceb5684b.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0470eccc-f797-4e9e-af06-899bceb5684b.png)'
- en: The term deep Q-learning refers to a method of reinforcement learning that adopts
    a neural network as an approximation of a value function. It, therefore, represents
    an evolution of the basic Q-learning method, since the action-state table is replaced
    by a neural network, to approximate the optimal value function.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: “深度Q学习”这一术语指的是一种强化学习方法，它采用神经网络作为价值函数的逼近。因此，它代表了基本Q学习方法的进化，因为动作-状态表被神经网络所取代，用以逼近最优价值函数。
- en: This is an innovative approach compared to those seen in the previous chapters.
    So far, the input of the algorithm has provided both the state and the action
    to provide the expected return. Deep Q-learning revolutionizes the structure,
    as it only requires the state of the environment as an input and provides all
    of the status-action values, as there are actions that can be performed in the
    environment.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与前几章看到的方法相比，一种创新的 approach。到目前为止，算法的输入提供了状态和动作，以便提供期望的回报。深度Q学习彻底革新了结构，因为它只需要环境的状态作为输入，并提供所有的状态-动作值，因为在环境中可以执行的动作是多样的。
- en: Q-learning is an algorithm widely used in reinforcement learning. Initially,
    it was considered an unstable algorithm when used with neural networks and therefore
    its use was limited to tasks and problems that involved limited dimensional spaces
    of states. The Q-learning algorithms and techniques can be used with DNNs. These
    algorithms have shown excellent performance.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种在强化学习中广泛使用的算法。最初，当它与神经网络一起使用时，被认为是不稳定的算法，因此其使用被限制在涉及有限维度状态空间的任务和问题中。Q学习算法和技术可以与深度神经网络（DNN）一起使用。这些算法已展示出卓越的性能。
- en: The Deep Q-learning or Deep Q-Network (DQN) is a reinforcement learning method
    for the approximation of the function. It represents an evolution of the Q-learning
    method where the action-state table is replaced by a neural network. In this algorithm,
    therefore, the learning does not consist of updating the table but consists of
    adjusting the weights of the neurons that make up the network. This update takes
    place using the backpropagation technique, which we have had the opportunity to
    learn more about in the *Neural network training* section of this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习或深度Q网络（DQN）是一种用于函数逼近的强化学习方法。它代表了Q学习方法的进化，其中动作-状态表被神经网络取代。在这个算法中，学习不再是更新表格，而是调整构成网络的神经元的权重。这个更新过程是通过反向传播技术进行的，正如我们在本章的*神经网络训练*部分中所学到的。
- en: 'The learning of the value function is therefore based on the modification of
    the weights using the following function:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，价值函数的学习是基于使用以下函数来修改权重：
- en: '![](img/4da55eb9-4a0c-4890-b0eb-9912439c6861.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4da55eb9-4a0c-4890-b0eb-9912439c6861.png)'
- en: 'In the previous equation, the two terms take on the following meaning:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，两个项具有以下含义：
- en: '*L[t]* is the loss function.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L[t]* 是损失函数。'
- en: '![](img/b29ae214-7f75-417a-93ee-075f338d8a92.png)is the optimal expected return.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/b29ae214-7f75-417a-93ee-075f338d8a92.png)是最优的预期回报。'
- en: '![](img/bbc4d93f-23fa-43a5-8ab4-56bdc487feb9.png)is the estimated value from
    the network.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/bbc4d93f-23fa-43a5-8ab4-56bdc487feb9.png) 是网络的估计值。'
- en: The errors calculated by the loss function will be propagated backward in the
    network using a backward step (backpropagation), following the gradient descent
    logic. In fact, the gradient indicates the direction of the greatest growth of
    a function; moving in the opposite direction, we reduce the error to the maximum.
    Policy behavior is given by an e-greedy approach to ensure enough exploration.
    The key aspect of DQN is the use of the experience replay. With this technique,
    the agent's experience is taken at every time step *t* and saved in a dataset
    called replay memory.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数计算的误差将通过反向步骤（反向传播）在网络中进行传播，遵循梯度下降的逻辑。实际上，梯度指示了函数增长的最大方向；朝相反方向移动，我们将误差降至最小。策略行为通过e-贪婪方法给出，以确保足够的探索。DQN的关键方面是经验回放的使用。通过这种技术，代理的经验在每个时间步*
    t * 时被捕获并保存在一个名为回放记忆的数据集中。
- en: The training is carried out through a mini-batch technique, that is, by taking
    a sub-set of samples of experiences randomly extracted from the replay memory.
    In this way, past experiences are used to update the network. Furthermore, the
    sub-set chosen randomly by the replay memory allows interrupting the strong correlation
    between successive experiences, hence reducing the variance between updates.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 训练通过小批量技术进行，即从回放记忆中随机提取一部分经验样本。通过这种方式，过去的经验被用来更新网络。此外，回放记忆随机选择的子集可以打断连续经验之间的强相关性，从而减少更新之间的方差。
- en: 'Following is the algorithm in pseudo-code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是伪代码中的算法：
- en: '[PRE26]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This algorithm can be implemented using R and the libraries available for neural
    networks and reinforcement learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法可以使用R和可用于神经网络及强化学习的库来实现。
- en: Let's now see an advanced example of DRL.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一个高级的深度强化学习（DRL）示例。
- en: Deep recurrent Q-networks
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度递归Q网络
- en: In the last section, *Approaching DRL*, we have already said that deep Q-learning
    adopts a neural network as an approximation of a value function. However, this
    method has limited memory and relies on the possibility of perceiving the state
    of the environment at each decision point. To overcome this problem, we can add
    recurrence to a **deep Q-network** (**DQN**) by replacing the first level fully
    connected neural network with a recurring LSTM. In this way, the **deep recurrent
    Q-network** (**DRQN**) model is obtained.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节《*接近深度强化学习*》中，我们已经提到深度Q学习使用神经网络作为价值函数的近似。然而，这种方法有着有限的内存，并且依赖于在每个决策点感知环境状态的可能性。为了克服这个问题，我们可以通过用递归LSTM替换第一级全连接神经网络，向**深度Q网络**（**DQN**）中添加递归。通过这种方式，得到了**深度递归Q网络**（**DRQN**）模型。
- en: Let's start with the recurrent neural networks.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从递归神经网络开始。
- en: Recurrent neural networks
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: A **recurrent neural network** (**RNN**) is a neural model in which a bidirectional
    flow of information is present. In other words, while the propagation of signals
    in feedforward networks takes place only in a continuous manner in one direction
    from inputs to outputs, recurrent networks are different. In recurrent networks,
    this propagation can also occur from a neural layer following a previous one,
    between neurons belonging to the same layer, or even between a neuron and itself.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）是一种神经网络模型，其中存在双向信息流。换句话说，在前馈网络中，信号的传播仅以单向连续的方式从输入到输出进行，而递归网络则不同。在递归网络中，这种传播不仅可以发生在一个神经层向后传播到前一个神经层，还可以在同一层中的神经元之间，甚至在神经元与其自身之间发生。'
- en: 'A recurring network will decide things at a particular time which will affect
    the decision it will take immediately after. Recurrent networks have two sources
    of input: the present and the recent past. This information is combined to determine
    how to respond to the new data. Recurrent networks differ from feedforward networks
    in that they add feedback linked to past decisions. This functionality gives the
    recurring networks a memory to perform tasks that feedforward networks cannot
    do.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 递归网络将在特定的时刻做出决定，这些决定会影响它随后的决策。递归网络有两个输入来源：当前的状态和最近的过去。这些信息被结合起来以确定如何对新数据作出反应。递归网络与前馈网络的不同之处在于，它们加入了与过去决策相关的反馈。这一功能使得递归网络具有记忆，能够执行前馈网络无法完成的任务。
- en: Access to memory occurs through the content rather than by address or location.
    One approach to this is that the memory content is the pattern of activations
    on the nodes of an RNN. The idea is to start the network with an activation scheme
    that is a partial or noisy representation of the requested memory content and
    that the network stabilizes on the required content.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 内存的访问是通过内容进行的，而不是通过地址或位置。一种方法是，内存内容是RNN节点上激活的模式。其理念是，从一个部分或噪声表示的激活模式开始网络，作为请求的内存内容的表示，然后网络将稳定到所需的内容。
- en: 'An RNN is a class of neural networks where there is at least one feedback connection
    between neurons that form a directed cycle. A typical RNN with connections between
    the output layer and the hidden layer is represented in the following diagram:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一类神经网络，其中至少存在一个神经元之间的反馈连接，形成一个有向循环。一个典型的具有输出层和隐藏层连接的RNN如下图所示：
- en: '![](img/e7b4b40d-7067-4397-942b-a9e87dae5960.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7b4b40d-7067-4397-942b-a9e87dae5960.png)'
- en: 'In the recurring network shown in the preceding diagram, both the input level
    and the output level are used to define the weights of the hidden level. Ultimately,
    we can think of RNNs as a variant of ANNs: these variants can be characterized
    by a different number of hidden levels and different trends of the data flow.
    RNNs are characterized by different trends in the flow of data, in fact, the connections
    between the neurons form a cycle. Recurrent neural networks can use internal memory
    for their processing, as they have connections between hidden levels that propagate
    over time to learn sequences.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面图示的递归网络中，输入层和输出层都用来定义隐藏层的权重。最终，我们可以将 RNN 看作是 ANN 的一种变体：这些变体可以通过不同数量的隐藏层和不同的数据流趋势来特征化。RNN
    的特点是数据流的不同趋势，事实上，神经元之间的连接形成了一个循环。递归神经网络可以利用内部记忆进行处理，因为它们在隐藏层之间有连接，这些连接会随时间传播，学习序列数据。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the world of DRL. To start, we learned the basic
    concepts of neural networks. We understood the concepts of layers, nodes, biases
    and transfer functions. In a nutshell, we understood how the architecture of a
    fully connected neural network is structured. Later, we applied the acquired skills,
    building a neural network to solve a regression problem. Then, we learned what
    is meant by DRL and how neural networks are used to approximate the value function.
    Finally, we analyzed a further form of DRL in which the neural network is replaced
    by a recurring network. These are the DRQNs that have proven to be particularly
    efficient.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了 DRL 的世界。首先，我们学习了神经网络的基本概念。我们理解了层、节点、偏置和传递函数的概念。简而言之，我们了解了全连接神经网络的架构如何构建。随后，我们应用所学的技能，构建了一个神经网络来解决回归问题。接着，我们学习了
    DRL 的含义以及神经网络如何用来逼近价值函数。最后，我们分析了另一种形式的 DRL，其中神经网络被递归网络所替代。这些被称为 DRQN 的网络已被证明特别高效。
- en: In the next chapter, we will explore the Keras model using TensorFlow as a backend
    engine. We will learn how to use Keras to set a multilayer perceptron model. Then,
    we will learn how to use DRL to balance a cart pole system.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探索使用 TensorFlow 作为后端引擎的 Keras 模型。我们将学习如何使用 Keras 设置一个多层感知器模型。接着，我们将学习如何使用
    DRL 平衡一个小车摆系统。
