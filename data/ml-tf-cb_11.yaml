- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Reinforcement Learning with TensorFlow and TF-Agents
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 和 TF-Agents 进行强化学习
- en: 'TF-Agents is a library for **reinforcement learning** (**RL**) in **TensorFlow**
    (**TF**). It makes the design and implementation of various algorithms easier
    by providing a number of modular components corresponding to the core parts of
    an RL problem:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: TF-Agents 是一个用于 **强化学习**（**RL**）的 **TensorFlow**（**TF**）库。通过提供多个模块化组件，TF-Agents
    使得各种算法的设计和实现变得更加简单，这些组件对应 RL 问题的核心部分：
- en: An agent operates in an **environment** and learns by processing signals received
    every time it chooses an action. In TF-Agents, an environment is typically implemented
    in Python and wrapped in a TF wrapper to enable efficient parallelization.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个智能体在 **环境** 中操作，并通过处理每次选择动作时收到的信号来进行学习。在 TF-Agents 中，环境通常用 Python 实现，并用 TF
    包装器封装，以便高效并行化。
- en: A **policy** maps an observation from the environment into a distribution over
    actions.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略** 将环境中的观测映射为动作的分布。'
- en: A **driver** executes a policy in an environment for a specified number of steps
    (also called **episodes**).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动器** 在环境中执行策略，经过指定的步数（也叫 **回合**）。'
- en: A **replay buffer** is used to store experience (agent trajectories in action
    space, along with associated rewards) of executing a policy in an environment;
    the buffer content is queried for a subset of trajectories during training.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回放缓冲区** 用于存储在环境中执行策略的经验（智能体在动作空间中的轨迹，以及相关的奖励）；训练时，会查询缓冲区中的一部分轨迹。'
- en: 'The basic idea is to cast each of the problems we discuss as a RL problem,
    and then map the components into TF-Agents counterparts. In this chapter, we will
    show how TF-Agents can be used to solve some simple RL problems:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路是将我们讨论的每个问题转化为一个 RL 问题，然后将各个组件映射到 TF-Agents 对应的部分。在本章中，我们将展示如何使用 TF-Agents
    来解决一些简单的 RL 问题：
- en: The GridWorld problem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GridWorld 问题
- en: The OpenAI Gym environment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym 环境
- en: Multi-armed bandits for content personalization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂强盗问题用于内容个性化
- en: 'The best way to start our demonstration of RL capabilities in TF-Agents is
    with a toy problem: GridWorld is a good choice due to its intuitive geometry and
    easy-to-interpret action but, despite this simplicity, it constitutes a proper
    objective, where we can investigate the optimal paths an agent takes to achieve
    the goal.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 展示 TF-Agents 中强化学习能力的最佳方式是通过一个玩具问题：GridWorld 是一个很好的选择，因为它具有直观的几何结构和易于理解的动作，尽管如此，它仍然是一个合适的目标，我们可以研究智能体为达成目标而采取的最佳路径。
- en: GridWorld
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GridWorld
- en: The code in this section is adapted from [https://github.com/sachag678](https://github.com/sachag678).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码改编自 [https://github.com/sachag678](https://github.com/sachag678)。
- en: We begin by demonstrating the basic TF-Agents functionality in the GridWorld
    environment. RL problems are best studied in the context of either games (where
    we have a clearly defined set of rules and fully observable context), or toy problems
    such as GridWorld. Once the basic concepts are clearly defined in a simplified
    but non-straightforward environment, we can move to progressively more challenging
    situations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示在 GridWorld 环境中基本的 TF-Agents 功能。RL 问题最好在游戏（我们有一套明确的规则和完全可观察的环境）或像 GridWorld
    这样的玩具问题中研究。一旦基本概念在一个简化但不简单的环境中得到了清晰的定义，我们就可以转向逐步更具挑战性的情境。
- en: 'The first step is to define a GridWorld environment: this is a 6x6 square board,
    where the agent starts at (0,0), the finish is at (5,5), and the goal of the agent
    is to find the path from the start to the finish. Possible actions are moves up/down/left/right.
    If the agent lands on the finish, it receives a reward of 100, and the game terminates
    after 100 steps if the end was not reached by the agent. An example of the GridWorld
    "map" is provided here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义一个 GridWorld 环境：这是一个 6x6 的方形棋盘，智能体从 (0,0) 开始，终点在 (5,5)，智能体的目标是找到从起点到终点的路径。可能的动作包括上/下/左/右移动。如果智能体到达终点，它将获得
    100 的奖励，并且如果智能体在 100 步内没有到达终点，游戏将结束。这里提供了一个 GridWorld “地图”的示例：
- en: '![Chart, box and whisker chart  Description automatically generated](img/B16254_11_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, box and whisker chart  Description automatically generated](img/B16254_11_01.png)'
- en: 'Figure 11.1: The GridWorld "map"'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：GridWorld “地图”
- en: Now we understand what we're working with, let's build a model to find its way
    around the GridWorld from **(0,0)** to **(5,5)**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了要处理的内容，让我们构建一个模型，从 **(0,0)** 找到通向 **(5,5)** 的路径。
- en: How do we go about it?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们该如何进行？
- en: 'As usual, we begin by loading the necessary libraries:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先加载必要的库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: TF-Agents is a library under active development, so, despite our best efforts
    to keep the code up to date, certain imports might need to be modified by the
    time you are running this code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TF-Agents 是一个积极开发中的库，因此尽管我们尽力保持代码更新，但在你运行这段代码时，某些导入可能需要修改。
- en: 'A crucial step is defining the environment that our agent will be operating
    in. Inheriting from the `PyEnvironment` class, we specify the `init` method (action
    and observation definitions), conditions for resetting/terminating the state,
    and the mechanics for moving:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键步骤是定义智能体将要操作的环境。通过继承`PyEnvironment`类，我们指定`init`方法（动作和观察定义）、重置/终止状态的条件以及移动机制：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have the following preliminary setup:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下初步设置：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We begin by creating the environments and wrapping them to ensure that they
    terminate after 100 steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建环境并将其封装，以确保它们在 100 步后终止：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For this recipe, we will be using a **Deep Q-Network** (**DQN**) agent. This
    means that we need to define the network and the associated optimizer first:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用**深度 Q 网络**（**DQN**）智能体。这意味着我们需要首先定义网络及其关联的优化器：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As indicated above, the TF-Agents library is under active development. The
    current version works with TF > 2.3, but it was originally written for TensorFlow
    1.x. The code used in this adaptation was developed using a previous version,
    so for the sake of backward compatibility, we require a less-than-elegant workaround,
    such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，TF-Agents 库正在积极开发中。目前版本适用于 TF > 2.3，但它最初是为 TensorFlow 1.x 编写的。此改编中使用的代码是基于先前版本开发的，因此为了兼容性，我们需要使用一些不太优雅的解决方法，例如以下代码：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the agent:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 定义智能体：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As a next step, we create the replay buffer and replay observer. The former
    is used for storing the (action, observation) pairs for training:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是创建回放缓冲区和回放观察者。前者用于存储训练用的（动作，观察）对：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then create a dataset from our buffer so that it can be iterated over:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从回放缓冲区创建数据集，以便可以进行迭代：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The final bit of preparation involves creating a driver that will simulate
    the agent in the game and store the (state, action, reward) tuples in the replay
    buffer, along with storing a number of metrics:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的准备工作是创建一个驱动程序，模拟游戏中的智能体，并将（状态，动作，奖励）元组存储在回放缓冲区中，同时还需要存储若干个度量：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Having finished the preparatory groundwork, we can run the driver, draw experience
    from the dataset, and use it to train the agent. For monitoring/logging purposes,
    we print the loss and average return at specific intervals:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 完成准备工作后，我们可以运行驱动程序，从数据集中获取经验，并用它来训练智能体。为了监控/记录，我们会在特定间隔打印损失和平均回报：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the code executes successfully, you should observe output similar to the
    following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码成功执行，你应该看到类似以下的输出：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'While detailed, the output of the training routine is not that well suited
    for reading by a human. However, we can visualize the progress of our agent instead:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练过程的输出很详细，但并不适合人类阅读。不过，我们可以通过可视化来观察智能体的进展：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Which will deliver us the following graph:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供以下图表：
- en: '![](img/B16254_11_02.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_11_02.png)'
- en: 'Figure 11.2: Average episode length over number of episodes'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：每集平均长度与集数的关系
- en: 'The graph demonstrates the progress in our model: after the first 4,000 episodes,
    there is a massive drop in the average episode length, indicating that it takes
    our agent less and less time to reach the ultimate objective.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图表展示了我们模型的进展：在前 4000 集后，平均每集时长出现大幅下降，表明我们的智能体在达到最终目标时所需时间越来越少。
- en: See also
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: Documentation for customized environments can be found at [https://www.tensorflow.org/agents/tutorials/2_environments_tutorial](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有关自定义环境的文档可以参考[https://www.tensorflow.org/agents/tutorials/2_environments_tutorial](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial)。
- en: 'RL is a huge field and even a basic introduction is beyond the scope of this
    book, but for those interested in learning more, the best recommendation is the
    classic Sutton and Barto book: [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一个庞大的领域，甚至是一个简单的介绍也超出了本书的范围。但对于那些有兴趣了解更多的人，最好的推荐是经典的 Sutton 和 Barto
    书籍：[http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)
- en: CartPole
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 倾斜摆杆（CartPole）
- en: 'In this section, we will make use of Open AI Gym, a set of environments containing
    non-trivial elementary problems that can be solved using RL approaches. We''ll
    use the CartPole environment. The objective of the agent is to learn how to keep
    a pole balanced on a moving cart, with possible actions including a movement to
    the left or to the right:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Open AI Gym，这是一个包含可以通过强化学习方法解决的非平凡基础问题的环境集。我们将使用 CartPole 环境。智能体的目标是学习如何在移动的小车上保持一根杆子平衡，可能的动作包括向左或向右移动：
- en: '![](img/B16254_11_03.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_11_03.png)'
- en: 'Figure 11.3: The CartPole environment, with the black cart balancing a long
    pole'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：CartPole 环境，黑色小车平衡着一根长杆
- en: Now we understand our environment, let's build a model to balance a pole.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了环境，接下来让我们构建一个模型来平衡一个杆子。
- en: How do we go about it?
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们该如何进行呢？
- en: 'We begin by installing some prerequisites and importing the necessary libraries.
    The installation part is mostly required to ensure that we can generate visualizations
    of the trained agent''s performance:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装一些前提条件并导入必要的库。安装部分主要是为了确保我们能够生成训练智能体表现的可视化效果：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As before, there are some hyperparameters of our toy problem that we define:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们定义了一些我们玩具问题的超参数：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we proceed with function definitions for our problem. Start by computing
    the average return for a policy in our environment over a fixed period (measured
    by the number of episodes):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续定义我们问题的函数。首先计算一个策略在环境中固定时间段内的平均回报（以回合数为衡量标准）：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Boilerplate code for collecting a single step and the associated data aggregation
    are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 收集单个步骤及相关数据聚合的样板代码如下：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If a picture is worth a thousand words, then surely a video must be even better.
    In order to visualize the performance of our agent, we need a function that renders
    the actual animation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一张图片值千言万语，那么视频一定更好。为了可视化我们智能体的表现，我们需要一个渲染实际动画的函数：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With the preliminaries out of the way, we can now proceed to actually setting
    up our environment:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步工作完成后，我们可以开始真正地设置我们的环境：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the CartPole environment, the following applies:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CartPole 环境中，适用以下内容：
- en: 'An observation is an array of four floats:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个观察是一个包含四个浮动数值的数组：
- en: The position and velocity of the cart
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小车的位置和速度
- en: The angular position and velocity of the pole
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆子的角位置和速度
- en: The reward is a scalar float value
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是一个标量浮动值
- en: 'An action is a scalar integer with only two possible values:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个动作是一个标量整数，只有两个可能的值：
- en: 0 — "move left"
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 — "向左移动"
- en: 1 — "move right"
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 — "向右移动"
- en: 'As before, split the training and evaluation environments and apply the wrappers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，分开训练和评估环境，并应用包装器：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the network forming the backbone of the learning algorithm in our agent:
    a neural network predicting the expected returns of all actions (commonly referred
    to as Q-values in RL literature) given an observation of the environment as input:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 定义构成我们智能体学习算法基础的网络：一个神经网络，根据环境的观察作为输入，预测所有动作的预期回报（通常在强化学习文献中称为 Q 值）：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With this, we can instantiate a DQN agent:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以实例化一个 DQN 智能体：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set up the policies – the main one used for evaluation and deployment, and
    the secondary one that is utilized for data collection:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 设置策略——主要用于评估和部署的策略，以及用于数据收集的次要策略：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In order to have an admittedly not very sophisticated comparison, we will also
    create a random policy (as the name suggests, it acts randomly). This demonstrates
    an important point, however: a policy can be created independently of an agent:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行一个不算很复杂的比较，我们还将创建一个随机策略（顾名思义，它是随机执行的）。然而，这展示了一个重要的观点：策略可以独立于智能体创建：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To get an action from a policy, we call the `policy.action(time_step)` method.
    The `time_step` contains the observation from the environment. This method returns
    a policy step, which is a named tuple with three components:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从策略中获得一个动作，我们调用 `policy.action(time_step)` 方法。`time_step` 包含来自环境的观察。这个方法返回一个策略步骤，这是一个包含三个组件的命名元组：
- en: '**Action**: the action to be taken (move left or move right)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：要执行的动作（向左移动或向右移动）'
- en: '**State**: used for stateful (RNN-based) policies'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：用于有状态（基于 RNN）的策略'
- en: '**Info**: auxiliary data, such as the log probabilities of actions:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息**：辅助数据，例如动作的对数概率：'
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The replay buffer tracks the data collected from the environment, which is
    used for training:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区跟踪从环境中收集的数据，这些数据用于训练：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For most agents, `collect_data_spec` is a named tuple called **Trajectory**,
    containing the specs for observations, actions, rewards, and other items.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数代理，`collect_data_spec` 是一个命名元组，称为**轨迹**（Trajectory），它包含关于观察、动作、奖励以及其他项的规格。
- en: 'We now make use of our random policy to explore the environment:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在利用随机策略来探索环境：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The replay buffer can now be accessed by an agent by means of a pipeline. Since
    our DQN agent needs both the current and the next observation to calculate the
    loss, the pipeline samples two adjacent rows at a time (`num_steps = 2`):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代理可以通过管道访问重放缓冲区。由于我们的DQN代理需要当前观察和下一次观察来计算损失，因此管道一次会采样两行相邻数据（`num_steps =
    2`）：
- en: '[PRE27]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'During the training part, we switch between two steps, collecting data from
    the environment and using it to train the DQN:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练部分，我们在两个步骤之间切换：从环境中收集数据并用它来训练DQN：
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'A (partial) output of the code block is given here. By way of a quick reminder,
    `step` is the iteration in the training process, `loss` is the value of the loss
    function in the deep network driving the logic behind our agent, and `Average
    Return` is the reward at the end of the current run:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块的（部分）输出如下。快速回顾一下，`step` 是训练过程中的迭代次数，`loss` 是深度网络中驱动代理逻辑的损失函数值，`Average Return`
    是当前运行结束时的奖励：
- en: '[PRE29]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Each iteration consists of 200 time steps and keeping the pole up gives a reward
    of 1, so our maximum reward per episode is 200:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代包含200个时间步，保持杆子竖立会得到1分奖励，因此每一轮的最大奖励为200：
- en: '![](img/B16254_11_04.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_11_04.png)'
- en: 'Figure 11.4: Average return over number of iterations'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：每次迭代的平均回报
- en: As you can see from the preceding graph, the agent takes about 10 thousand iterations
    to discover a successful policy (with some hits and misses, as the U-shaped pattern
    of reward in that part demonstrates). After that, the reward stabilizes and the
    algorithm is able to successfully complete the task each time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以看出，代理大约需要1万次迭代才能发现一个成功的策略（其中有一些波动，正如奖励曲线中的U形模式所示）。之后，奖励趋于稳定，算法能够每次成功完成任务。
- en: 'We can also observe the performance of our agents in a video. As regards the
    random policy, you can try the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过视频观察代理的表现。关于随机策略，您可以尝试以下操作：
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And as regards the trained one, you can try the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 而关于已经训练的策略，您可以尝试以下操作：
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: See also
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Open AI Gym environment documentation can be found at [https://gym.openai.com/](https://gym.openai.com/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Open AI Gym环境文档可以在[https://gym.openai.com/](https://gym.openai.com/)找到。
- en: MAB
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAB
- en: In probability theory, a **multi-armed bandit** (**MAB**) problem refers to
    a situation where a limited set of resources must be allocated between competing
    choices in such a manner that some form of long-term objective is maximized. The
    name originated from the analogy that was used to formulate the first version
    of the model. Imagine we have a gambler facing a row of slot machines who has
    to decide which ones to play, how many times, and in what order. In RL, we formulate
    it as an agent that wants to balance exploration (acquisition of new knowledge)
    and exploitation (optimizing decisions based on experience already acquired).
    The objective of this balancing is the maximization of a total reward over a period
    of time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，**多臂赌博机**（**MAB**）问题指的是一种情境，其中有限的资源必须在多个竞争的选择之间分配，以最大化某种形式的长期目标。这个名称来源于用于制定模型第一版的类比。假设我们有一个赌徒，面前是一排老虎机，他必须决定选择哪些老虎机进行游戏，玩多少次，以及以什么顺序进行。在强化学习（RL）中，我们将其表述为一个代理（agent），该代理希望平衡探索（获得新知识）和开发（基于已经获得的经验优化决策）。这种平衡的目标是在一段时间内最大化总奖励。
- en: 'An MAB is a simplified RL problem: an action taken by the agent does not influence
    the subsequent state of the environment. This means that there is no need to model
    state transitions, credit rewards to past actions, or plan ahead to get to rewarding
    states. The goal of an MAB agent is to determine a policy that maximizes the cumulative
    reward over time.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: MAB是一个简化的强化学习问题：代理采取的动作不会影响环境的后续状态。这意味着不需要建模状态转移，不需要为过去的动作分配奖励，也不需要提前规划以到达奖励状态。MAB代理的目标是确定一个策略，使得随时间推移能够最大化累积奖励。
- en: 'The main challenge is an efficient approach to the exploration-exploitation
    dilemma: if we always try to exploit the action with the highest expected rewards,
    there is a risk we miss out on better actions that could have been uncovered with
    more exploration.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战是有效应对探索与利用的难题：如果我们总是尝试利用期望奖励最高的动作，就有可能错过那些通过更多探索可以发现的更好的动作。
- en: The setup used in this example is adapted from the Vowpal Wabbit tutorial at
    [https://vowpalwabbit.org/tutorials/cb_simulation.html](https://vowpalwabbit.org/tutorials/cb_simulation.html).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中使用的设置来源于Vowpal Wabbit教程，网址为[https://vowpalwabbit.org/tutorials/cb_simulation.html](https://vowpalwabbit.org/tutorials/cb_simulation.html)。
- en: 'In this section, we will simulate the problem of personalizing online content:
    Tom and Anna go to a website at different times of the day and are shown an article.
    Tom likes politics in the morning and music in the afternoon, while Anna prefers
    sport or politics in the morning and politics in the afternoon. Casting the problem
    in MAB terms, this means the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟个性化在线内容的问题：Tom和Anna在一天中的不同时间访问网站并查看一篇文章。Tom早上喜欢政治，下午喜欢音乐，而Anna早上喜欢体育或政治，下午喜欢政治。将这个问题用多臂赌博机（MAB）术语表述，这意味着：
- en: The context is a pair {user, time of day}
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文是一个包含{用户，时间段}的对。
- en: Possible actions are news topics {politics, sport, music, food}
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的动作是新闻主题{政治、体育、音乐、食物}。
- en: The reward is 1 if a user is shown content they find interesting at this time,
    and 0 otherwise
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果用户在此时看到他们感兴趣的内容，则奖励为1，否则为0。
- en: The objective is to maximize the reward measured through the **clickthrough
    rate** (**CTR**) of the users.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最大化通过用户的**点击率**（**CTR**）衡量的奖励。
- en: How do we go about it?
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们该怎么做呢？
- en: 'As usual, we begin by loading the necessary packages:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们首先加载必要的包：
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then define some hyperparameters that will be used later:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来定义一些超参数，这些参数将在后续使用：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The first function we need is a context sampler to generate observations coming
    from the environment. Since we have two users and two parts of the day, it comes
    down to generating two-element binary vectors:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的第一个函数是一个上下文采样器，用于生成来自环境的观察值。由于我们有两个用户和一天中的两个时间段，实际上是生成两元素二进制向量：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we define a generic function for calculating the reward per arm:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个通用函数来计算每个臂的奖励：
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can use the function to define the rewards per arm. They reflect the set
    of preferences described at the beginning of this recipe:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用该函数来定义每个臂的奖励。这些奖励反映了在本食谱开头描述的偏好集：
- en: '[PRE36]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The final part of our function''s setup involves a calculation of the optimal
    rewards for a given context:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们函数设置的最后一部分涉及计算给定上下文的最优奖励：
- en: '[PRE37]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For the sake of this example, we assume that the environment is stationary;
    in other words, the preferences do not change over time (which does not need to
    be the case in a practical scenario, depending on your time horizon of interest):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本示例的目的，我们假设环境是静态的；换句话说，偏好在时间上没有变化（这在实际场景中不一定成立，取决于你关注的时间范围）：
- en: '[PRE38]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We are now ready to instantiate an agent implementing a bandit algorithm. We
    use a predefined `LinUCB` class; as usual, we define the observation (two elements
    representing the user and the time of day), time step, and action specification
    (one of four possible types of content):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备实例化一个实现赌博机算法的代理。我们使用预定义的`LinUCB`类；像往常一样，我们定义观察值（两个元素，表示用户和时间），时间步长和动作规范（四种可能内容类型之一）：
- en: '[PRE39]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A crucial component of the MAB setup is regret, which is defined as the difference
    between an actual reward collected by the agent and the expected reward of an
    **oracle policy**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博机（MAB）设置的一个关键组成部分是遗憾，它被定义为代理实际获得的奖励与**oracle策略**的期望奖励之间的差异：
- en: '[PRE40]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can now commence the training of our agent. We run the trainer loop for
    `num_iterations` and execute `steps_per_loop` in each step. Finding the appropriate
    values for those parameters is usually about striking a balance between the recent
    nature of updates and training efficiency:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始训练我们的代理。我们运行训练循环`num_iterations`次，每次执行`steps_per_loop`步。找到这些参数的合适值通常是要在更新的时效性和训练效率之间找到平衡：
- en: '[PRE41]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can visualize the results of our experiment by plotting the regret (negative
    reward) over subsequent iterations of the algorithm:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制后续迭代中的遗憾（负奖励）来可视化实验结果：
- en: '[PRE42]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Which will plot the following graph for us:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们绘制出以下图表：
- en: '![](img/B16254_11_05.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_11_05.png)'
- en: 'Figure 11.5: Performance of a trained UCB agent over time'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：训练过的UCB代理随时间的表现
- en: As the preceding graph demonstrates, after an initial period of learning (indicating
    a spike in regret around iteration 30), the agent keeps getting better at serving
    the desired content. There is a lot of variation going on, which shows that even
    in a simplified setting – two users – efficient personalization remains a challenge.
    Possible avenues of improvement could involve longer training or adapting a DQN
    agent so that more sophisticated logic can be employed for prediction.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，在初始学习阶段（在第30次迭代附近的遗憾出现峰值），代理会不断改进，提供期望的内容。过程中有很多变化，表明即便在一个简化的环境下——两个用户——高效个性化仍然是一个挑战。改进的可能方向包括更长时间的训练，或者调整DQN代理，使其能够采用更复杂的逻辑来进行预测。
- en: See also
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'An extensive collection of bandits and related environments can be found in
    the *TF-Agents documentation repository*: [https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的强盗算法及其环境的广泛集合可以在*TF-Agents文档库*中找到：[https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2)。
- en: 'Readers interested in contextual multi-armed bandits are encouraged to follow
    the relevant chapters from the book by *Sutton and Barto*: [https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对上下文多臂强盗问题感兴趣的读者可以参考*Sutton和Barto*的书中的相关章节：[https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)。
