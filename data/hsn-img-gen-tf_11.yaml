- en: '*Chapter 8*: Self-Attention for Image Generation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：用于图像生成的自注意力机制'
- en: You may have heard about some popular **Natural Language Processing** (**NLP**)
    models, such as the Transformer, BERT, or GPT-3\. They all have one thing in common
    – they all use an architecture known as a transformer that is made up of self-attention
    modules.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过一些流行的**自然语言处理**（**NLP**）模型，比如Transformer、BERT或GPT-3。它们有一个共同点——它们都使用一种叫做transformer的架构，而该架构由自注意力模块组成。
- en: Self-attention is gaining widespread adoption in computer vision, including
    classification tasks, which makes it an important topic to master. As we will
    learn in this chapter, self-attention helps us to capture important features in
    the image without using deep layers for large effective receptive fields. StyleGAN
    is great for generating faces, but it will struggle to generate images from ImageNet.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制在计算机视觉中得到了广泛应用，包括分类任务，这使其成为一个重要的学习主题。正如我们将在本章中学习的，自注意力帮助我们捕捉图像中的重要特征，而无需使用深层网络来获得大范围的感受野。StyleGAN在生成人脸方面表现优秀，但在从ImageNet生成图像时会遇到困难。
- en: In a way, faces are easy to generate, as eyes, noses, and lips all have similar
    shapes and are in similar positions across various faces. In contrast, the 1,000
    classes of ImageNet contain varied objects (dogs, trucks, fish, and pillows, for
    instance) and backgrounds. Therefore, the discriminator must be more effective
    at capturing the distinct features of various objects. This is where self-attention
    comes into play. Using that, with conditional batch normalization and spectral
    normalization, we will implement a **Self-Attention GAN** (**SAGAN**) to generate
    images based on given class labels.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，生成面孔是容易的，因为眼睛、鼻子和嘴唇的形状相似，并且在各种面孔中位置也相似。相比之下，ImageNet的1000个类别包含了各种各样的物体（例如狗、卡车、鱼和枕头）和背景。因此，判别器必须更加有效地捕捉各种物体的不同特征。这正是自注意力发挥作用的地方。通过自注意力、条件批量归一化和谱归一化，我们将实现一个**自注意力生成对抗网络**（**SAGAN**），以根据给定的类别标签生成图像。
- en: After that, we will use the SAGAN as a base to create a BigGAN. We will add
    orthogonal regularization and change the method of doing class embedding. BigGANs
    can generate high-definition images without using ProGAN-like architecture, and
    they are considered to be state-of-the-art models in class labels conditioning
    image generation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们将以SAGAN为基础，创建一个BigGAN。我们将添加正交正则化，并改变类别嵌入的方法。BigGAN能够在不使用类似ProGAN架构的情况下生成高分辨率图像，并且被认为是在类别标签条件下生成图像的最先进模型。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Spectral normalization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱归一化
- en: Self-attention modules
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力模块
- en: Building a SAGAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建SAGAN
- en: Implementing BigGAN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现BigGAN
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The Jupyter notebooks can be found here ([https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter08)):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本可以在这里找到（[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter08)）：
- en: '`ch8_sagan.ipynb`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch8_sagan.ipynb`'
- en: '`ch8_big_gan.ipynb`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch8_big_gan.ipynb`'
- en: Spectral normalization
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谱归一化
- en: Spectral normalization is an important method to stabilize GAN training and
    it has been used in a lot of recent state-of-the-art GANs. Unlike batch normalization
    or other normalization methods that normalize the activation, spectral normalization
    normalizes the weights instead. The aim of spectral normalization is to limit
    the growth of the weights, so the networks adhere to the 1-Lipschitz constraint.
    This has proved effective in stabilizing GAN training, as we learned in [*Chapter
    3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*, Generative Adversarial Network*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 谱归一化是稳定GAN训练的一个重要方法，已经在许多最近的先进GAN中得到了应用。与批量归一化或其他归一化方法不同，谱归一化是对权重进行归一化，而不是激活。谱归一化的目的是限制权重的增长，使得网络遵守1-Lipschitz约束。正如我们在[*第3章*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*，生成对抗网络*中所学到的那样，这对于稳定GAN训练已被证明非常有效。
- en: We will revise WGANs to give us a better understanding of the idea behind spectral
    normalization. The WGAN discriminator (also known as the critic) needs to keep
    its prediction to small numbers to meet the 1-Lipschtiz constraint. WGANs do this
    by naively clipping the weights to the range of [-0.01, 0.01].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新审视 WGAN，以便更好地理解光谱归一化背后的思想。WGAN 判别器（也称为评论家）需要将其预测值保持在小的范围内，以满足 1-Lipschitz
    限制。WGAN 通过简单地将权重裁剪到 [-0.01, 0.01] 范围内来实现这一点。
- en: This is not a reliable method as we need to fine-tune the clipping range, which
    is a hyperparameter. It would be nice if there was a systematic way to enforce
    the 1-Lipschitz constraint without the use of hyperparameters, and spectral normalization
    is the tool we need for that. In essence, spectral normalization normalizes the
    weights by dividing by their spectral norms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一种可靠的方法，因为我们需要微调裁剪范围，这本身是一个超参数。如果能有一种系统化的方法在不使用超参数的情况下强制执行 1-Lipschitz 限制，那就太好了，而光谱归一化就是我们需要的工具。从本质上讲，光谱归一化通过除以权重的光谱范数来对权重进行归一化。
- en: Understanding spectral norm
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解光谱范数
- en: 'We will go over some linear algebra to roughly explain what spectral norm is.
    You may have learned about eigenvalues and eigenvectors in matrix theory with
    the following equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要回顾一些线性代数内容，以大致解释什么是光谱范数。你可能已经在矩阵理论中学习过特征值和特征向量，公式如下：
- en: '![](img/Formula_08_001.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_001.jpg)'
- en: Here *A* is a square matrix, *v* is the eigenvector, and the *lambda* is its
    eigenvalue.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *A* 是一个方阵，*v* 是特征向量，*lambda* 是其特征值。
- en: 'We''ll try to understand the terms using a simple example. Let''s say that
    *v* is a vector of position *(x, y)* and *A* is a linear transformation as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个简单的例子来理解这些术语。假设 *v* 是一个位置向量 *(x, y)*，而 *A* 是如下所示的线性变换：
- en: '![](img/Formula_08_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_002.jpg)'
- en: 'If we multiply *A* with *v*, we''ll get a new position with a change of direction
    as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 *A* 与 *v* 相乘，我们将得到一个新的位置，并且方向发生变化，如下所示：
- en: '![](img/Formula_08_003.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_003.jpg)'
- en: Eigenvectors are vectors that do not change their directions when *A* is applied
    to them. Instead, they are only scaled by the scalar eigenvalues denoted as lambda.
    There can be multiple eigenvector-eigenvalue pairs. The square root of the largest
    eigenvalue is the spectral norm of the matrix. For a non-square matrix, we will
    need to use a mathematical algorithm such as **singular value decomposition**
    (**SVD**) to calculate the eigenvalues, which can be computationally costly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是指在 *A* 被应用时其方向不发生变化的向量。它们只是通过标量特征值 *lambda* 进行缩放。可能会有多个特征向量-特征值对。最大特征值的平方根就是矩阵的光谱范数。对于非方阵，我们需要使用数学算法，如
    **奇异值分解** (**SVD**) 来计算特征值，这在计算上可能会很昂贵。
- en: Therefore, a power iteration method is employed to speed up the calculation
    and make it feasible for neural network training. Let's jump in to implement spectral
    normalization as a weight constraint in TensorFlow.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，采用幂迭代方法来加速计算，并使其在神经网络训练中变得可行。让我们跳过实现光谱归一化作为 TensorFlow 中的权重约束。
- en: Implementing spectral normalization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现光谱归一化
- en: The mathematic algorithm of spectral normalization as given by T. Miyato et
    al., 2018, in the *Spectral Normalization For Generative Adversarial Networks*
    paper may appear complex. However, as usual, the software implementation is simpler
    than what the mathematics looks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由 T. Miyato 等人于2018年在《生成对抗网络中的光谱归一化》论文中给出的光谱归一化数学算法看起来可能很复杂。然而，像往常一样，软件实现比数学公式更简单。
- en: 'The following are the steps to perform spectral normalization:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行光谱归一化的步骤：
- en: The weights in the convolutional layer form a 4-dimensional-tensor, so the first
    step is to reshape it into a 2D matrix of *W*, where we keep the last dimension
    of the weight. Now the weight has the shape *(H×W, C)*.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层中的权重形成一个四维张量，因此第一步是将其重塑为一个二维矩阵 *W*，其中保持权重的最后一个维度。现在，权重的形状为 *(H×W, C)*。
- en: Initialize a vector *u* with N(0,1).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个向量 *u*，其分布为 N(0,1)。
- en: 'In a `for` loop, calculate the following:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `for` 循环中，计算如下内容：
- en: a) Calculate *V = (W*T*) U* with matrix transpose and matrix multiplication.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算 *V = (W*T*) U*，使用矩阵转置和矩阵乘法。
- en: b) Normalize *V* with its L2 norm, that is, *V = V/||V||*2.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 归一化 *V*，使用其 L2 范数，即 *V = V/||V||*2。
- en: c) Calculate *U = WV*.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算 *U = WV*。
- en: d) Normalize *U* with its L2 norm, that is, *U = U/||U||*2.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 归一化 *U*，使用其 L2 范数，即 *U = U/||U||*2。
- en: Calculate the spectral norm as *U*T*W V*.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算光谱范数为 *U*T*W V*。
- en: Finally, divide the weights by the spectral norm.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将权重除以光谱范数。
- en: 'The full code is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码如下：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The number of iterations is a hyperparameter, and I found `5` to be sufficient.
    Spectral normalization can also be implemented to have a variable to remember
    the vector `u` rather than starting from random values. This should reduce the
    number of iterations to `1`. We can now apply spectral normalization by using
    it as a kernel constraint when defining layers, as in `Conv2D(3, 1, kernel_constraint=SpectralNorm())`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数是一个超参数，我发现`5`次就足够了。也可以实现谱归一化，使用一个变量来记住向量`u`，而不是从随机值开始。这应该将迭代次数减少到`1`。我们现在可以通过将其作为定义层时的卷积核约束来应用谱归一化，如`Conv2D(3,
    1, kernel_constraint=SpectralNorm())`。
- en: Self-attention modules
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注意力模块
- en: Self-attention modules became popular with the introduction of an NLP model
    known as the Transformer. In NLP applications such as language translation, the
    model often needs to read sentences word by word to understand them before producing
    the output. The neural network used prior to the advent of the Transformer was
    some variant on the **recurrent neural network** (**RNN**), such as **long short-term
    memory** (**LSTM**). The RNN has internal states to remember words as it reads
    a sentence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模块随着一种名为Transformer的NLP模型的出现而变得流行。在语言翻译等NLP应用中，该模型通常需要逐字读取句子以理解其含义，然后生成输出。Transformer出现之前使用的神经网络是某种变体的**递归神经网络**（**RNN**），如**长短期记忆网络**（**LSTM**）。RNN具有内部状态，在读取句子时记住单词。
- en: One drawback of that is that when the number of words increases, the gradients
    for the first words vanish. That is to say, the words at start of the sentence
    become less important gradually as the RNN reads more words.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个缺点是，当单词数量增加时，前面单词的梯度会消失。也就是说，句子开始部分的单词随着RNN读取更多单词而逐渐变得不那么重要。
- en: The Transformer does things differently. It reads all the words at once and
    weights the importance of each individual word. Therefore, more attention is given
    to words that are more important, and hence the name **attention**. Self-attention
    is a cornerstone of state-of-the-art NLP models such as BERT and GPT-3\. However,
    NLP is not in the scope of this book. We will now look at the details of how self-attention
    works in CNN.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的做法不同。它一次性读取所有单词，并加权每个单词的重要性。因此，更重要的单词会获得更多的关注，这也是**注意力**这一名称的由来。自注意力是最先进的NLP模型（如BERT和GPT-3）的基石。然而，本书不涉及NLP的内容。现在我们将看一下自注意力在CNN中的工作细节。
- en: Self-attention for computer vision
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉中的自注意力
- en: CNNs are mainly made up of convolutional layers. For a convolutional layer with
    a kernel size of 3×3, it will only look at 3×3=9 features in the input activation
    to compute each output feature. It will not look at pixels outside of this range.
    To capture the pixels outside of this range, we could increase the kernel size
    slightly to, say, 5×5 or 7×7, but that is still small compared to the feature
    map size.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CNN主要由卷积层构成。对于一个3×3大小的卷积层，它只会查看输入激活中的3×3=9个特征来计算每个输出特征。它不会查看超出这个范围的像素。为了捕捉这个范围外的像素，我们可以稍微增加卷积核的大小，例如使用5×5或7×7，但这相对于特征图的大小仍然较小。
- en: We will have to move down one network layer for the convolutional kernel's receptive
    field to be large enough to capture what we want. As with RNNs, the relative importance
    of the input features fades as we move down through the network layers. Thus,
    we can use self-attention to look at every pixel in the feature map and work on
    what we should pay attention to.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要下移一个网络层，使得卷积核的感受野足够大，以捕获我们想要的特征。与RNN一样，随着我们向下通过网络层，输入特征的相对重要性会逐渐减弱。因此，我们可以使用自注意力来查看特征图中的每个像素，并专注于我们应该关注的部分。
- en: 'We will now look at how the self-attention mechanism works. The first step
    of self-attention is to project each input feature into three vectors known as
    the **key**, **query**, and **value**. We don''t see these terms a lot in computer
    vision literature, but I thought it would be good to teach you about them so that
    you can better understand general self-attention-, Transformer-, or NLP-related
    literature. The following figure illustrates how attention maps are generated
    from a query:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下自注意力机制是如何工作的。自注意力的第一步是将每个输入特征映射到三个向量上，分别被称为**key**、**query**和**value**。在计算机视觉文献中我们不常看到这些术语，但我认为向你介绍它们会帮助你更好地理解与自注意力、Transformer或NLP相关的文献。下图展示了如何从查询生成注意力图：
- en: '![Figure 8.1 – Illustration of an attention map. (Source: H. Zhang et al.,
    2019,'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 注意力图的示意图。（来源：H. Zhang等，2019年，'
- en: '"Self-Attention Generative Adversarial Networks," https://arxiv.org/abs/1805.08318)](img/B14538_08_01.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: “自注意力生成对抗网络”，https://arxiv.org/abs/1805.08318)](img/B14538_08_01.jpg)
- en: 'Figure 8.1 – Illustration of an attention map. (Source: H. Zhang et al., 2019,
    "Self-Attention Generative Adversarial Networks," https://arxiv.org/abs/1805.08318)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 注意力图的示意图。（来源：H. Zhang 等，2019年，《自注意力生成对抗网络》，https://arxiv.org/abs/1805.08318）
- en: On the left is an image with queries marked with dots. The next five images
    show the attention maps given by the queries. The first attention map on the top
    queries one eye of the rabbit; the attention map has more white (indicating areas
    of high importance) around both the eyes and close to complete darkness (for low
    importance) in other areas.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是标有点的查询图像。接下来的五张图展示了这些查询给出的注意力图。顶部的第一个注意力图查询了兔子的一个眼睛；这个注意力图在两个眼睛周围有更多的白色区域（表示高重要性），而在其他区域则接近完全黑暗（表示低重要性）。
- en: 'Now, we''ll now go over the technical terms of key, query, and value one by
    one:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将逐一讲解关键字、查询和数值的技术术语：
- en: A **value** is a representation of the input features. We don't want the self-attention
    module to look at every single pixel as this will be too computationally expensive
    and unnecessary. Instead, we are more interested in the local regions of the input
    activation. Therefore, the value has reduced dimensions from the input features,
    both in terms of the activation map size (for example, it may be downsampled to
    have smaller height and width) and the number of channels. For convolutional layer
    activations, the channel number is reduced by using a 1x1 convolution, and the
    spatial size is reduced by max-pooling or average pooling.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值**是输入特征的表示。我们不希望自注意力模块查看每个像素，因为这样计算开销过大且不必要。相反，我们更关注输入激活的局部区域。因此，值的维度相较于输入特征有所减少，既包括激活图的大小（例如，可能会降采样以使高度和宽度更小），也包括通道数。对于卷积层的激活，通道数通过使用1x1卷积进行减少，空间大小则通过最大池化或平均池化来缩小。'
- en: '**Keys and queries** are used to compute the importance of the features of
    the self-attention map. To calculate an output feature at location *x*, we take
    query at location *x* and compare it with the key at all locations. To illustrate
    more on this, let''s say we have an image of a portrait.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键字和查询**用于计算自注意力图中特征的重要性。为了计算位置 *x* 的输出特征，我们取位置 *x* 的查询，并将其与所有位置的关键字进行比较。为了进一步说明这一点，假设我们有一张人像图片。'
- en: When the network is processing one eye of the portrait, it will take its query,
    which has a semantic meaning of *eye*, and check that with the keys of other areas
    of the portrait. If one of the other areas' keys is *eye*, then we know we have
    found the other eye, and it certainly is something we want to pay attention to
    so that we can match the eye color.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当网络处理人像中的一只眼睛时，它会拿取其查询（具有*眼睛*的语义意义），并与人像其他区域的关键字进行比对。如果其他区域的某个关键字是*眼睛*，那么我们就知道找到了另一只眼睛，这显然是我们需要关注的内容，以便匹配眼睛颜色。
- en: To put that into an equation, for feature *0*, we calculate a vector of *q0
    × k0,  q0 × k1,  q0 × k2* and so on to *q0 × kN-1*. The vectors are then normalized
    using softmax so they all sum up to *1.0*, which is our attention score. This
    is used as a weight to perform element-wise multiplication of the value, to give
    the attention outputs.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将其转化为方程，对于特征 *0*，我们计算一个向量 *q0 × k0, q0 × k1, q0 × k2*，依此类推，直到 *q0 × kN-1*。然后，通过softmax对这些向量进行归一化，使它们的总和为
    *1.0*，这就是我们的注意力分数。这个分数作为权重，用来执行数值的逐元素乘法，从而给出注意力输出。
- en: 'The SAGAN self-attention module is based on the non-local block (X. Wang et
    al., 2018, *Non-local Neural Networks*, [https://arxiv.org/abs/1711.07971](https://arxiv.org/abs/1711.07971)),
    which was originally designed for video classification. The authors experimented
    with different ways of implementing self-attention before settling on the current
    architecture. The following diagram shows the attention module in SAGAN, where
    **theta** **θ**, **phi** **φ**, and **g** correspond to key, query, and value:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SAGAN自注意力模块基于非局部块（X. Wang 等，2018年，*Non-local Neural Networks*， [https://arxiv.org/abs/1711.07971](https://arxiv.org/abs/1711.07971)），该模块最初是为视频分类设计的。作者在确定当前架构之前，尝试了不同的自注意力实现方式。以下图展示了SAGAN中的注意力模块，其中**theta**
    **θ**，**phi** **φ**和**g**分别对应于关键字、查询和数值：
- en: '![Figure 8.2 – Self-attention module architecture in SAGAN'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – SAGAN中的自注意力模块架构'
- en: '](img/B14538_08_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_08_02.jpg)'
- en: Figure 8.2 – Self-attention module architecture in SAGAN
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – SAGAN中的自注意力模块架构
- en: Most computation in deep learning is vectorized for speed performance, and it
    is no different for self-attention. If we ignore the batch dimension for simplicity,
    the activations after 1×1 convolution will have a shape of (H, W, C). The first
    step is to reshape it into a 2D matrix with a shape of (H×W, C) and use the matrix
    multiplication between *θ* and *φ* to calculate the attention map. In the self-attention
    module used in SAGAN, there is another 1×1 convolution that is used to restore
    the channel number to the input channel, followed by scaling with learnable parameters.
    Furthermore, this is made into a residual block.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的大多数计算都是矢量化的，以提高速度性能，自注意力也不例外。为简单起见，如果忽略批处理维度，则1×1卷积后的激活形状为(H, W, C)。第一步是将其重塑为形状为(H×W,
    C)的二维矩阵，并使用*θ*和*φ*之间的矩阵乘法来计算注意力图。在SAGAN中使用的自注意力模块中，还有另一个1×1卷积，用于将通道数恢复到输入通道，并使用可学习参数进行缩放。此外，这被制成一个残差块。
- en: Implementing a self-attention module
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自注意力模块
- en: 'We will first define all the 1×1 convolutional layers and weights in the custom
    layer''s `build()`. Please note that we use a spectral normalization function
    as the kernel constraint for the convolutional layers as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在自定义层的`build()`函数中定义所有的1×1卷积层和权重。请注意，我们使用谱归一化函数作为卷积层的核约束，具体如下：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are a few things to note here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点需要注意：
- en: The internal activation can have reduced dimensions to make the computation
    run faster. The reduced numbers were obtained by the SAGAN authors by experimentation.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部激活可以减少维度以提高计算速度。减少的数值是通过SAGAN作者的实验得到的。
- en: After every convolution layer, the activation (H, W, C) is reshaped into two-
    dimensional matrix with the shape (HW, C). We can then use matrix multiplication
    on the matrices.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层之后，激活(H, W, C)被重塑为形状为(HW, C)的二维矩阵。然后我们可以对矩阵进行矩阵乘法。
- en: 'The following is the `call()` function of the layer to perform the self-attention
    operations. We will first calculate `theta`, `phi`, and `g`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是该层执行自注意操作的`call()`函数。我们首先计算`theta`、`phi`和`g`：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will then calculate the attention map as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算注意力图如下：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we multiply attention map with the query `g` and proceed to produce
    the final output:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将注意力图与查询`g`相乘，并继续生成最终输出：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With the spectral normalization and self-attention layers written, we can now
    use them to build a SAGAN.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有了谱归一化和自注意力层后，我们现在可以使用它们来构建SAGAN。
- en: Building a SAGAN
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建SAGAN
- en: 'The SAGAN has a simple architecture that looks like DCGAN''s. However, it is
    a class-conditional GAN that uses class labels to both generate and discriminate
    between images. In the following figure, each image on each row is generated from
    different class labels:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: SAGAN具有类似DCGAN的简单架构。然而，它是一个条件生成对抗网络，使用类标签来生成和区分图像。在下图中，每一行的每个图像都是从不同类标签生成的：
- en: '![Figure 8.3 – Images generated by a SAGAN by using different class labels.
    (Source: A. Brock et al., 2018, "Large Scale GAN Training for High Fidelity Natural
    Image Synthesis," https://arxiv.org/abs/1809.11096)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3 – 使用不同类标签生成的SAGAN图像。（来源：A. Brock等人，2018年，“用于高保真自然图像合成的大规模GAN训练”，https://arxiv.org/abs/1809.11096）'
- en: '](img/B14538_08_03.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_08_03.jpg)'
- en: 'Figure 8.3 – Images generated by a SAGAN by using different class labels. (Source:
    A. Brock et al., 2018, "Large Scale GAN Training for High Fidelity Natural Image
    Synthesis," https://arxiv.org/abs/1809.11096)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 使用不同类标签生成的SAGAN图像。（来源：A. Brock等人，2018年，“用于高保真自然图像合成的大规模GAN训练”，https://arxiv.org/abs/1809.11096）
- en: In this example, we will use the `CIFAR10` dataset, which contains 10 classes
    of images with a resolution of 32x32\. We will deal with the conditioning part
    later. Now, let's first complete the simplest part – the generator.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用`CIFAR10`数据集，该数据集包含10类32x32分辨率的图像。稍后我们将处理条件部分。现在，让我们首先完成最简单的部分 – 生成器。
- en: Building a SAGAN generator
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建SAGAN生成器
- en: 'At a high level, the SAGAN generator doesn''t look very different from other
    GAN generators: it takes noise as input and goes through a dense layer, followed
    by multiple levels of upsampling and convolution blocks, to achieve the target
    image resolution. We start with 4×4 resolution and use three upsampling blocks
    to reach the final resolution of 32×32, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，SAGAN生成器与其他GAN生成器差别不大：它将噪声作为输入，经过一个全连接层，然后是多个上采样和卷积块，最终达到目标图像分辨率。我们从4×4分辨率开始，使用三个上采样块，最终达到32×32的分辨率，具体如下：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Despite using different activation dimensions within the self-attention module,
    its output has the same shape as the input. Thus, this can be inserted anywhere
    after a convolutional layer. However, it may be overkill to put it at 4×4 resolution
    when the kernel size is 3×3\. So, the self-attention layer is inserted only once
    in the SAGAN generator at a higher spatial resolution stage to make the most out
    of the self-attention layer. The same goes for the discriminator, where the self-attention
    layer is placed at the lower layer when the spatial resolution is higher.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在自注意力模块中使用了不同的激活维度，其输出的形状与输入相同。因此，它可以被插入到卷积层之后的任何位置。然而，当卷积核大小为3×3时，将其插入到4×4的分辨率可能有些过度。因此，在SAGAN生成器中，自注意力层仅在较高空间分辨率的阶段插入，以充分利用自注意力层。同样，对于判别器，当空间分辨率较高时，自注意力层会放置在较低的层次。
- en: That's all for the generator, if we're doing unconditional image generation.
    We will need to feed the class labels into the generator so it can create images
    from the given classes. At the beginning of [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*, we learned about some common ways of conditioning
    on labels, but the SAGAN uses a more advanced way; that is, it encodes the class
    label into learnable parameters in batch normalization. We introduced **conditional
    batch normalization** in [*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*,
    Style Transfer*, and we will now implement it for the SAGAN.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是生成器的全部内容，如果我们进行的是无条件图像生成。我们需要将类别标签输入生成器，这样它才能根据给定的类别生成图像。在[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)《图像到图像的转换》中，我们学习了一些常见的标签条件化方法，但SAGAN使用了一种更先进的方式；即，它将类别标签编码为批量归一化中的可学习参数。我们在[*第5章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)《风格迁移》中介绍了**条件批量归一化**，现在我们将在SAGAN中实现它。
- en: Conditional batch normalization
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件批量归一化
- en: 'Throughout much of this book, we have been complaining about the drawback of
    using batch normalization in GANs. In `CIFAR10`, there are 10 classes: 6 of them
    are animals (bird, cat, deer, dog, frog, and horse) and 4 of them are vehicles
    (airplane, automobile, ship, and truck). Obviously, they look very different –the
    vehicles tend to have hard and straight edges, while the animals tend to have
    curvier edges and softer textures.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的很多章节中，我们一直在抱怨GAN中使用批量归一化的缺点。在`CIFAR10`中，有10个类别：其中6个是动物（鸟、猫、鹿、狗、青蛙和马），4个是交通工具（飞机、汽车、船和卡车）。显然，它们看起来差异很大——交通工具通常有硬直的边缘，而动物的边缘则更弯曲且质感较软。
- en: As we have learned regarding style transfer, the activation statistics dictate
    the image style. Therefore, mixing the batch statistics can create images that
    look a bit like an animal and a bit like a vehicle – for example, a car-shaped
    cat. This is because batch normalization uses only one gamma and one beta for
    an entire batch that's made up of different classes. The problem is resolved if
    we have a gamma and a beta for each of the styles (classes), and that is exactly
    what conditional batch normalization is about. It has one gamma and one beta for
    each class, so there are 10 betas and 10 gammas per layer for the 10 classes in
    `CIFAR10`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在风格迁移中所学到的，激活统计量决定了图像的风格。因此，混合批量统计量可以生成看起来既像动物又像交通工具的图像——例如，一只形状像汽车的猫。这是因为批量归一化对一个包含不同类别的整个批次仅使用一个gamma和一个beta。如果我们对每个风格（类别）都有一个gamma和一个beta，那么这个问题就得到了解决，这正是条件批量归一化的核心所在。它为每个类别提供一个gamma和一个beta，因此在`CIFAR10`的每一层中，对于10个类别来说，共有10个betas和10个gammas。
- en: 'We can now construct the variables required by the conditional batch normalization
    as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以按照以下方式构造条件批量归一化所需的变量：
- en: A gamma and a beta with a shape of *(10, C)*, where *C* is the activation channel
    number.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个形状为*(10, C)*的gamma和beta，其中*C*是激活通道数。
- en: A moving mean and variance with a shape of *(1, 1, 1, C)*. In training, the
    mean and variance are calculated from a minibatch. During inference, we use the
    moving averaged values accumulated in training. They are shaped so that the arithmetic
    operation is broadcast to the N, H, and W dimensions.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有形状*(1, 1, 1, C)*的移动均值和方差。在训练中，均值和方差是从一个小批量中计算得出的；在推理时，我们使用在训练过程中累积的移动平均值。它们的形状会使得算术操作可以广播到N、H和W维度。
- en: 'The following is the code for conditional batch normalization:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是条件批量归一化的代码：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we run the conditional batch normalization, we retrieve the correct `beta`
    and `gamma` for the labels. This is done using `tf.gather(self.beta, labels)`,
    which is conceptually equivalent to `beta = self.beta[labels]`, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行条件批量归一化时，我们会为标签检索正确的`beta`和`gamma`值。这是通过`tf.gather(self.beta, labels)`实现的，在概念上等同于`beta
    = self.beta[labels]`，如下所示：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Apart from that, the rest of the code is identical to batch normalization.
    Now, we can place the conditional batch normalization in the residual block for
    the generator:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，其余的代码与批量归一化相同。现在，我们可以将条件批量归一化放置在生成器的残差块中：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the runtime code for the forward pass of conditional batch
    normalization:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是条件批量归一化的前向传递运行时代码：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The residual block for the discriminator looks similar to the one for the generator
    but with a couple of differences, as listed here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的残差块与生成器的类似，但有一些差异，具体列举如下：
- en: There is no normalization.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里没有归一化。
- en: Downsampling happens inside the residual block with average pooling.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下采样发生在残差块内部，采用平均池化方式。
- en: Therefore, we won't be showing the code for the discriminator's residual block.
    We can now proceed to the final building block – the discriminator.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不会展示判别器残差块的代码。我们现在可以继续讨论最后的构建块——判别器。
- en: Building the discriminator
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建判别器
- en: The discriminator uses the self-attention layer as well, and it is placed near
    the input layers to capture the large activation map. As it is a conditional GAN,
    we will also use the label in the discriminator to make sure that the generator
    is producing the correct images matching the classes. The general approach to
    incorporating label information is to first project the label into the embedding
    space and then use the embedding at either the input layer or any internal layer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器也使用自注意力层，并将其放置在靠近输入层的位置，以捕捉较大的激活图。由于它是一个条件生成对抗网络（cGAN），我们还将在判别器中使用标签，以确保生成器生成的图像与类别匹配。将标签信息纳入模型的一般方法是，首先将标签投影到嵌入空间，然后在输入层或任何内部层使用该嵌入。
- en: 'There are two common methods of merging the embedding with the activation –
    **concatenation** and **element-wise multiplication**. The SAGAN uses architecture
    that''s similar to the projection model by T. Miyato and M. Koyama''s *cGANs with
    Projection Discriminator*, as shown at the bottom right of the following figure:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见的将嵌入与激活合并的方法——**连接**和**逐元素乘法**。SAGAN使用的架构类似于T. Miyato和M. Koyama在其2018年《带投影判别器的cGANs》中提出的投影模型，正如下图右下方所示：
- en: '![Figure 8.4 – Comparison of several common ways of incorporating labels as
    conditions in a discriminator. (d) is the one used in the SAGAN. (Redrawn from
    T. Miyato and M. Koyama''s, 2018 "cGANs with Projection Discriminator," https://arxiv.org/abs/1802.05637)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4 – 比较几种常见的将标签作为条件整合到判别器中的方法。（d）是SAGAN中使用的方法。（改图自T. Miyato和M. Koyama的2018年《带投影判别器的cGANs》，https://arxiv.org/abs/1802.05637）'
- en: '](img/B14538_08_04.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_08_04.jpg)'
- en: Figure 8.4 – Comparison of several common ways of incorporating labels as conditions
    in a discriminator. (d) is the one used in the SAGAN. (Redrawn from T. Miyato
    and M. Koyama's, 2018 "cGANs with Projection Discriminator," https://arxiv.org/abs/1802.05637)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 比较几种常见的将标签作为条件整合到判别器中的方法。（d）是SAGAN中使用的方法。（改图自T. Miyato和M. Koyama的2018年《带投影判别器的cGANs》，https://arxiv.org/abs/1802.05637）
- en: 'The label is first projected into embedding space, and then we perform element-wise
    multiplication with activation just before the dense layer (**ψ** in the diagram).
    The result then adds to the dense layer output to give the final prediction as
    follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 标签首先被投影到嵌入空间，然后我们与激活值进行逐元素乘法运算，紧接着在密集层（**ψ**图示中）之前执行。这一结果会加到密集层的输出上，从而给出最终的预测，如下所示：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With the models defined, we can now train the SAGAN.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义完成后，我们可以开始训练SAGAN。
- en: Training the SAGAN
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练SAGAN
- en: We will use the standard GAN training pipeline. The loss function is `CIFAR10`
    has small images of size 32×32, the training is relatively stable and quick. The
    original SAGAN was designed for an image resolution of 128×128, but this resolution
    is still small compared to other training sets that we have used. In the next
    section, we will look at some improvements made to the SAGAN for training on bigger
    datasets with bigger image sizes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用标准的GAN训练流程。损失函数是`CIFAR10`，它包含了32×32大小的小图像，训练相对稳定且快速。原始的SAGAN是为128×128的图像分辨率设计的，但与我们使用的其他训练集相比，这个分辨率仍然较小。在下一部分，我们将讨论一些对SAGAN的改进，以便在更大的数据集和更大图像尺寸上进行训练。
- en: Implementing BigGAN
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现BigGAN
- en: 'The BigGAN is an improved version of the SAGAN. The BigGAN ups the image resolution
    significantly from 128×128 to 512×512, and it does it without progressive growth
    of layers! The following are some sample images generated by BigGAN:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: BigGAN是SAGAN的改进版本。BigGAN显著提高了图像分辨率，从128×128提升到512×512，而且在没有层次逐步增长的情况下完成了这一点！以下是一些由BigGAN生成的示例图像：
- en: '![Figure 8.5 – Class-conditioned samples generated by BigGAN at 512x512 (Source:
    A. Brock et al., 2018, "Large Scale GAN Training for High Fidelity Natural Image
    Synthesis," https://arxiv.org/abs/1809.11096)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.5 – BigGAN在512x512分辨率下生成的类别条件样本（来源：A. Brock等人，2018年，“大规模GAN训练用于高保真自然图像合成”，https://arxiv.org/abs/1809.11096）'
- en: '](img/B14538_08_05.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_08_05.jpg)'
- en: 'Figure 8.5 – Class-conditioned samples generated by BigGAN at 512x512 (Source:
    A. Brock et al., 2018, "Large Scale GAN Training for High Fidelity Natural Image
    Synthesis," https://arxiv.org/abs/1809.11096)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – BigGAN在512x512分辨率下生成的类别条件样本（来源：A. Brock等人，2018年，“大规模GAN训练用于高保真自然图像合成”，https://arxiv.org/abs/1809.11096）
- en: BigGAN is considered the state-of-the-art class-conditional GAN. We'll now look
    into the changes and modify the SAGAN code to make ourselves a BigGAN.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: BigGAN被认为是最先进的类别条件GAN。接下来我们将查看这些变化，并修改SAGAN代码，打造我们的BigGAN。
- en: Scaling GANs
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展GAN
- en: Older GANs tend to use small batch sizes as that would produce better-quality
    images. Now we know that the quality problem was caused by the batch statistics
    used in batch normalization, and this is addressed by using other normalization
    techniques. Still, the batch size has remained small as it is physically limited
    by GPU memory constraints.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 较早的GAN通常使用小批量大小，因为这能产生更高质量的图像。现在我们知道，质量问题是由批量归一化中使用的批量统计数据引起的，而这一问题通过使用其他归一化技术得以解决。尽管如此，批量大小仍然较小，因为它受限于GPU内存的物理约束。
- en: 'However, being part of Google has its perks: the DeepMind team, who created
    the BigGAN, had all the resources they needed. Through experimentation, they found
    that scaling up GANs helps in producing better results. In BigGAN training, the
    batch size used was eight times that of the SAGAN; the convolutional channel numbers
    are also 50% higher. This is where the name BigGAN came from: bigger proved to
    be better.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为谷歌的一部分有其优势：创建BigGAN的DeepMind团队拥有他们所需的所有资源。通过实验，他们发现，扩大GAN的规模有助于产生更好的结果。在BigGAN训练中，使用的批量大小是SAGAN的八倍；卷积通道的数量也提高了50%。这就是BigGAN名称的来源：更大证明更好。
- en: 'As a matter of fact, the bulking up of the SAGAN is the main contributor to
    BigGAN''s superior performance, as summarized in the following table:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，SAGAN的增强是BigGAN性能优越的主要原因，具体总结在下表中：
- en: '![Figure 8.6 – Improvement in Frechet Inception Distance (FID) and Inception
    Score (IS) by adding features to the SAGAN baseline. The Configurations column
    shows the features added to the configuration in the previous row. The numbers
    in brackets show the improvement on the preceding row'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6 – 通过向SAGAN基线添加特征，改善了Frechet Inception Distance（FID）和Inception Score（IS）。配置列显示了前一行配置中添加的特征。括号中的数字表示相较前一行的改进。'
- en: '](img/B14538_08_06.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_08_06.jpg)'
- en: Figure 8.6 – Improvement in Frechet Inception Distance (FID) and Inception Score
    (IS) by adding features to the SAGAN baseline. The Configurations column shows
    the features added to the configuration in the previous row. The numbers in brackets
    show the improvement on the preceding row
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 通过向SAGAN基线添加特征，改善了Frechet Inception Distance（FID）和Inception Score（IS）。配置列显示了前一行配置中添加的特征。括号中的数字表示相较前一行的改进。
- en: The table shows the BigGAN's performance when trained on ImageNet. The **Frechet
    Inception Distance** (**FID**) measures the class variety (the lower the better),
    while the **Inception Score** (**IS**) indicates the image quality (the higher
    the better). On the left is the configuration of the network, starting with the
    SAGAN baseline and adding new features row by row. We can see that the biggest
    improvement came from increasing the batch size. This makes sense for improving
    the FID, as a batch size of 2,048 is larger than the class size of 1,000, making
    the GAN less likely to overfit to the small number of classes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示了BigGAN在ImageNet上训练时的表现。**Frechet Inception Distance**（**FID**）衡量类别多样性（值越低越好），而**Inception
    Score**（**IS**）表示图像质量（值越高越好）。左侧是网络的配置，从SAGAN基准开始，逐行增加新特性。我们可以看到，最大的改进来自于增加批量大小。这对于提高FID是有意义的，因为批量大小为2,048大于1,000的类别数，使得GAN不太容易过拟合到较少的类别。
- en: Increasing the channel size also resulted in significant improvement. The other
    three features add only a small improvement. Therefore, if you don't have multiple
    GPUs that can fit a large network and batch size, then you should stick to the
    SAGAN. If you do have such GPUs or just want to know about the feature upgrades,
    then let's crack on!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 增加通道大小也带来了显著的改进。其他三个特性只带来了小幅改进。因此，如果你没有多个可以适应大型网络和批量大小的GPU，那么你应该坚持使用SAGAN。如果你确实有这样的GPU，或者只是想了解这些功能的升级，那我们就继续吧！
- en: Skipping latent vectors
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳过潜在向量
- en: Traditionally, the latent vector *z* goes into the first dense layer of the
    generator, followed by a sequence of convolutional and upsampling layers. Although
    the StyleGAN also has a latent vector that goes only to the first layer of its
    generator, it has another source of random noise that goes into every resolution
    of the activation map. This allows the control of style at different resolution
    levels.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，潜在向量*z*进入生成器的第一个密集层，然后依次经过卷积层和上采样层。虽然StyleGAN也有一个潜在向量，仅输入到其生成器的第一层，但它有另一个来源的随机噪声，输入到每个分辨率的激活图中。这允许在不同的分辨率级别上控制风格。
- en: 'By merging the two ideas, the BigGAN split the latent vector into chunks, where
    each of them goes to different residual blocks in the generator. Later, we will
    see how that concatenates with the class label for conditional batch normalization.
    In addition to the default BigGAN, there is another configuration known as BigGAN-deep
    that is four times deeper. The following diagram shows their difference in concatenating
    labels and input noise. We will implement the BigGAN on the left:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这两个思想合并，BigGAN将潜在向量分割成若干块，每块分别输入到生成器中的不同残差块。稍后我们将看到这如何与类别标签一起拼接，用于条件批归一化。除了默认的BigGAN外，还有一种配置称为BigGAN-deep，其深度是默认版本的四倍。下图显示了它们在拼接标签和输入噪声时的差异。我们将实现左侧的BigGAN：
- en: '![Figure 8.7 – Two configurations of the generator (Redrawn from A. Brock et
    al., 2018, "Large Scale GAN Training for High Fidelity Natural Image Synthesis,"
    https://arxiv.org/abs/1809.11096)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.7 – 生成器的两种配置（重绘自 A. Brock 等人，2018年，"Large Scale GAN Training for High
    Fidelity Natural Image Synthesis," https://arxiv.org/abs/1809.11096）'
- en: '](img/B14538_08_07.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_08_07.jpg)'
- en: Figure 8.7 – Two configurations of the generator (Redrawn from A. Brock et al.,
    2018, "Large Scale GAN Training for High Fidelity Natural Image Synthesis," https://arxiv.org/abs/1809.11096)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 生成器的两种配置（重绘自 A. Brock 等人，2018年，"Large Scale GAN Training for High Fidelity
    Natural Image Synthesis," https://arxiv.org/abs/1809.11096）
- en: We will now look at how BigGAN reduces the size of the embedding in conditional
    batch normalization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来看看BigGAN如何在条件批归一化中减少嵌入的大小。
- en: Shared class embedding
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享类别嵌入
- en: In the SAGAN's conditional batch normalization, there is a matrix of the shape
    [class number, channel number] for each beta and gamma in every layer. When the
    number of classes and channels increases, the weight size goes up rapidly too.
    When trained on the 1,000-class ImageNet with 1,024-channel convolutional layers,
    this will create over 1 million variables in one normalization layer alone!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在SAGAN的条件批归一化中，每一层的每个beta和gamma都有一个形状为[class number, channel number]的矩阵。当类别数和通道数增加时，权重的大小也会迅速增加。在1,000类的ImageNet上训练时，使用1,024通道的卷积层，这将导致单个归一化层中有超过100万个变量！
- en: Therefore, instead of having a weight matrix of 1,000×1,024, the BigGAN first
    projects the class into an embedding of smaller dimensions, for example, 128,
    that is shared across all layers. Within the conditional batch normalization,
    dense layers are used to map the class embedding and noise into betas and gammas.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，BigGAN不是使用一个1,000×1,024的权重矩阵，而是首先将类别投影到一个较小维度的嵌入中，例如128，且该嵌入在所有层中共享。在条件批量归一化中，全连接层用于将类别嵌入和噪声映射到betas和gammas。
- en: 'The following code snippet shows the first two layers in the generator:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了生成器中的前两层：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The latent vector with dimensions of 128 is first split into four equal parts,
    for the dense layer and the residual blocks at three resolutions. The label is
    projected into a shared embedding that concatenates with the `z` chunk and goes
    into residual blocks. The residual blocks are unchanged from the SAGAN, but we''ll
    make some small modifications to the conditional batch normalization in the following
    code. Instead of declaring variables for gamma and beta, we now generate from
    class labels via dense layers. As usual, we will first define the required layers
    in `build()` as shown here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 具有128维的潜在向量首先被拆分为四个相等的部分，用于全连接层和三个分辨率的残差块。标签被投影到一个共享的嵌入中，该嵌入与`z`块连接并进入残差块。残差块与SAGAN中的保持不变，但我们将在以下代码中对条件批量归一化进行一些小的修改。我们现在通过全连接层从类别标签中生成，而不是声明gamma和beta的变量。像往常一样，我们将首先在`build()`中定义所需的层，如下所示：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At runtime, we will use dense layers to generate `beta` and `gamma` from the
    shared embedding. Then, they will be used like normal batch normalization. The
    code snippet for the dense layer parts are shown here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，我们将使用全连接层从共享的嵌入生成`beta`和`gamma`。然后，它们将像常规批量归一化一样使用。全连接层部分的代码片段如下所示：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We added dense layers to predict `beta` and `gamma` from the latent vector and
    label embedding. That replaces the large weight variables.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了全连接层，从潜在向量和标签嵌入中预测`beta`和`gamma`，这取代了较大的权重变量。
- en: Orthogonal regularization
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正交正则化
- en: '**Orthogonality** is used extensively in the BigGAN to initialize weights and
    as a weight regularizer. A matrix is said to be orthogonal if multiplication with
    its transpose will produce an identity matrix. An **identity matrix** is a matrix
    with one in the diagonal elements and zero in all other places. Orthogonality
    is a good property because the norm of a matrix doesn''t change if it is multiplied
    by an orthogonal matrix.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**正交性**在BigGAN中被广泛应用于初始化权重和作为权重正则化器。一个矩阵被称为正交的，如果它与其转置相乘会产生单位矩阵。**单位矩阵**是一个对角线元素为1，其他位置元素为0的矩阵。正交性是一个好的特性，因为如果一个矩阵与正交矩阵相乘，它的范数不会发生变化。'
- en: 'In a deep neural network, the repeated matrix multiplication can result in
    exploding or vanishing gradients. Therefore, maintaining orthogonality can improve
    training. The equation for original orthogonal regularization is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，重复的矩阵乘法可能导致梯度爆炸或消失。因此，保持正交性可以改善训练。原始正交正则化的公式如下：
- en: '![](img/Formula_08_004.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_004.jpg)'
- en: 'Here *W* is the weight reshaped as a matrix and beta is a hyperparameter. As
    this regularization was found to be limiting, the BigGAN uses a different variant:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*是重塑为矩阵的权重，beta是一个超参数。由于这种正则化被发现具有局限性，BigGAN使用了不同的变种：
- en: '![](img/Formula_08_005.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_005.jpg)'
- en: In this variant, *(1 – I)* removes the diagonal elements, which are dot products
    of the filters. This removes the constraint on the filter's norm and aims to minimize
    the pairwise cosine similarity between the filters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个变种中，*(1 – I)*去除了对角线元素，它们是滤波器的点积。这样去除了滤波器范数的约束，并旨在最小化滤波器之间的成对余弦相似度。
- en: 'Orthogonality is closely related to spectral normalization, and both can co-exist
    in a network. We implemented spectral normalization as a kernel constraint, where
    the weights are modified directly. Weight regularization calculates the loss from
    the weights and adds the loss to other losses for backpropagation, hence regularizing
    the weights in an indirect way. The following code shows how to write a custom
    regularizer in TensorFlow:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正交性与谱归一化密切相关，两者可以在网络中共存。我们将谱归一化实现为核约束，其中权重被直接修改。权重正则化通过计算权重的损失并将损失添加到其他损失中进行反向传播，从而以间接方式对权重进行正则化。以下代码展示了如何在TensorFlow中编写自定义正则化器：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then assign the *kernel initializer*, *kernel constraint*, and *kernel
    regularizer* to the convolution and dense layers. However, adding them to each
    of the layers can make the code look long and cluttered. To avoid this, we can
    put them into a dictionary and pass them as keyword arguments (`kwargs`) into
    the Keras layers as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将*核初始化器*、*核约束*和*核正则化器*分配给卷积层和全连接层。然而，将它们添加到每一层中会使代码显得冗长且杂乱。为了避免这种情况，我们可以将它们放入字典中，并作为关键字参数（`kwargs`）传递到Keras层中，方法如下：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As we mentioned earlier, orthogonal regularization has the smallest effect in
    improving image quality. The beta value of *1e-4* was obtained numerically, and
    you might need to tune it for your dataset.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，正交正则化对提升图像质量的影响最小。*1e-4*的β值是通过数值计算得到的，您可能需要根据您的数据集调整它。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about an important network architecture known as
    self-attention. The effectiveness of the convolutional layer is limited by its
    receptive field, and self-attention helps to capture important features including
    activations that are spatially-distanced from conventional convolutional layers.
    We have learned how to write a custom layer to insert into a SAGAN. The SAGAN
    is a state-of-the-art class-conditional GAN. We also implemented conditional batch
    normalization to learn different learnable parameters specific to each class.
    Finally, we looked at the bulked-up version of the SAGAN known as the BigGAN,
    which trumps SAGAN's performance significantly in terms of both image resolution
    and class variation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一个重要的网络架构——自注意力机制。卷积层的有效性受其感受野的限制，而自注意力机制有助于捕捉包括与传统卷积层空间上距离较远的激活在内的重要特征。我们学习了如何编写自定义层并将其插入到SAGAN中。SAGAN是一种最先进的类别条件GAN。我们还实现了条件批量归一化，以学习特定于每个类别的不同可学习参数。最后，我们研究了SAGAN的强化版本，即BigGAN，它在图像分辨率和类别变化方面大大超越了SAGAN的表现。
- en: We have now learned about most, if not all, of the important GANs for image
    generation. In recent years, two major components have gained popularity in the
    GAN world – they are AdaIN for the StyleGAN as covered in [*Chapter 7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)*,
    High Fidelity Face Generation* and self-attention for the SAGAN. The Transformer
    is based on self-attention and has revolutionized NLP, and it's starting to make
    its way into computer vision. Therefore, it is now a good time to learn about
    attention-based generative models as this may be how future GANs look. In the
    next chapter, we will use what we learned about image generation at the end of
    this chapter to generate a deepfake video.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了大多数重要的生成对抗网络（GAN）用于图像生成的内容。如果不是全部的话。近年来，GAN领域的两个主要组件开始受到关注——它们分别是用于StyleGAN的AdaIN，如在[*第7章*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)中讨论的，*高保真面部生成*，以及SAGAN中的自注意力机制。Transformer基于自注意力机制，并已在自然语言处理（NLP）领域引发了革命，它也开始进入计算机视觉领域。因此，现在是学习基于注意力的生成模型的好时机，因为未来的GAN可能会采用这种方法。在下一章中，我们将运用本章末尾关于图像生成的知识来生成一段深度伪造视频。
