- en: Appendix 2 – Assessments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录2 – 评估
- en: The following are the answers to the questions mentioned at the end of each
    chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每章结尾提到的问题的答案。
- en: Chapter 1 – Fundamentals of Reinforcement Learning
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 – 强化学习基础
- en: In supervised and unsupervised learning, the model (agent) learns based on the
    given training dataset, whereas, in **reinforcement learning** (**RL**), the agent
    learns by directly interacting with the environment. Thus RL is essentially an
    interaction between the agent and its environment.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在监督学习和无监督学习中，模型（智能体）基于给定的训练数据集进行学习，而在**强化学习**（**RL**）中，智能体通过直接与环境互动进行学习。因此，RL本质上是智能体与环境之间的互动。
- en: The environment is the world of the agent. The agent stays within the environment.
    For instance, in the chess game, the chessboard is the environment since the chess
    player (agent) learns to play chess within the chessboard (environment). Similarly,
    in the Super Mario Bros game, the world of Mario is called the environment.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境是智能体的世界。智能体存在于环境中。例如，在国际象棋游戏中，棋盘就是环境，因为棋手（智能体）在棋盘（环境）中学习如何下棋。同样，在超级马里奥兄弟游戏中，马里奥的世界就是环境。
- en: The deterministic policy maps the state to one particular action, whereas the
    stochastic policy maps the state to the probability distribution over an action
    space.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定性策略将状态映射到一个特定的动作，而随机策略则将状态映射到动作空间上的概率分布。
- en: The agent interacts with the environment by performing actions, starting from
    the initial state until they reach the final state. This agent-environment interaction
    starting from the initial state until the final state is called an episode.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 智能体通过执行动作与环境进行互动，从初始状态开始，直到到达最终状态。这个从初始状态到最终状态的智能体-环境交互过程称为一个回合。
- en: The discount factor helps us in preventing the return reaching up to infinity
    by deciding how much importance we give to future rewards and immediate rewards.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 折扣因子帮助我们通过决定给未来奖励和即时奖励的权重，防止回报达到无穷大。
- en: The value function (value of a state) is the expected return of the trajectory
    starting from that state whereas the Q function (the Q value of a state-action
    pair) is the expected return of the trajectory starting from that state and action.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 价值函数（一个状态的价值）是从该状态开始的轨迹的期望回报，而Q函数（状态-动作对的Q值）是从该状态和动作开始的轨迹的期望回报。
- en: In a deterministic environment, we can be sure that when an agent performs an
    action *a* in state *s*, then it always reaches state ![](img/B15558_12_016.png).
    In a stochastic environment, we cannot say that by performing some action *a*
    in state *s*, the agent always reaches state ![](img/B15558_12_016.png) because
    there will be some randomness associated with the stochastic environment.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在确定性环境中，我们可以确定当智能体在状态*s*下执行动作*a*时，它总是到达状态 ![](img/B15558_12_016.png)。在随机环境中，我们无法确定通过在状态*s*下执行某个动作*a*，智能体总是到达状态
    ![](img/B15558_12_016.png)，因为随机环境中会有一些随机性。
- en: Chapter 2 – A Guide to the Gym Toolkit
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 – 健身工具包指南
- en: The Gym toolkit provides a variety of environments for training the RL agent
    ranging from classic control tasks to Atari game environments. We can train our
    RL agent to learn in these simulated environments using various RL algorithms.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gym工具包提供了各种环境，用于训练RL智能体，从经典控制任务到Atari游戏环境。我们可以使用各种RL算法训练我们的RL智能体，让它在这些模拟环境中学习。
- en: We can create a Gym environment using the `make` function. The `make` function
    requires the environment ID as a parameter.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`make`函数创建一个Gym环境。`make`函数需要环境ID作为参数。
- en: We learned that the action space consists of all the possible actions in the
    environment. We can obtain the action space by using `env.action_space`.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们知道，动作空间包含环境中所有可能的动作。我们可以通过使用`env.action_space`来获取动作空间。
- en: We can visualize the Gym environment using the `render()` function.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`render()`函数来可视化Gym环境。
- en: Some classic control environments offered by Gym include the cart pole balancing
    environment, the pendulum, and the mountain car environment.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gym提供的一些经典控制环境包括平衡杆环境、摆钟环境和山地车环境。
- en: We can generate an episode by selecting an action in each state using the `step()`
    function.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过在每个状态中选择一个动作来生成一个回合，使用`step()`函数。
- en: The state space of the Atari environment will be either the game screen's pixel
    values or the RAM of the Atari machine.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Atari环境的状态空间将是游戏屏幕的像素值或Atari机器的RAM。
- en: We can record the agent's gameplay using the Monitor wrapper. It takes three
    parameters—the environment, the directory where we want to save our recordings,
    and the force option.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用Monitor包装器记录智能体的游戏过程。它需要三个参数——环境、我们希望保存记录的目录以及强制选项。
- en: Chapter 3 – The Bellman Equation and Dynamic Programming
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 – 贝尔曼方程与动态规划。
- en: The Bellman equation states that the value of a state can be obtained as a sum
    of the immediate reward and the discounted value of the next state. Similar to
    the Bellman equation of the value function, the Bellman equation of the Q function
    states that the Q value of a state-action pair can be obtained as a sum of the
    immediate reward and the discounted Q value of the next state-action pair.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝尔曼方程表示，一个状态的值可以通过当前奖励和下一个状态的折现值之和来获得。与值函数的贝尔曼方程类似，Q函数的贝尔曼方程表示，状态-动作对的Q值可以通过当前奖励和下一个状态-动作对的折现Q值之和来获得。
- en: The Bellman expectation equation gives the Bellman value and Q functions whereas
    the Bellman optimality equation gives the optimal Bellman value and Q functions.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝尔曼期望方程给出了贝尔曼值函数和Q函数，而贝尔曼最优性方程给出了最优的贝尔曼值函数和Q函数。
- en: The value function can be derived from the Q function as ![](img/B15558_19_003.png).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值函数可以从Q函数中推导出来，如下图所示：![](img/B15558_19_003.png)。
- en: The Q function can be derived from the value function as ![](img/B15558_19_004.png).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q函数可以从值函数中推导出来，如下图所示：![](img/B15558_19_004.png)。
- en: 'In the value iteration method, we perform the following steps:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在值迭代方法中，我们执行以下步骤：
- en: Compute the optimal value function by taking maximum over Q function, that is,
    ![](img/B15558_03_088.png)
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对Q函数进行最大化计算最优值函数，即：![](img/B15558_03_088.png)。
- en: Extract the optimal policy from the computed optimal value function
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从计算出的最优值函数中提取最优策略。
- en: 'In the policy iteration method, we perform the following steps:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在策略迭代方法中，我们执行以下步骤：
- en: Initialize the random policy
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化随机策略。
- en: Compute the value function using the given policy
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定策略计算值函数。
- en: Extract a new policy using the value function obtained from *step 2*
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*步骤2*中获得的值函数提取新策略。
- en: If the extracted policy is the same as the policy used in *step 2* then stop,
    else send the extracted new policy to *step 2* and repeat *steps 2* to *4*
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提取的策略与*步骤2*中使用的策略相同，则停止，否则将提取的新策略发送到*步骤2*并重复*步骤2*至*步骤4*。
- en: In the value iteration method, first, we compute the optimal value function
    by taking the maximum over the Q function iteratively. Once we find the optimal
    value function then we will use it to extract the optimal policy. In the policy
    iteration method, we will try to compute the optimal value function using the
    policy iteratively. Once we have found the optimal value function then the policy
    that was used to create the optimal value function will be extracted as the optimal
    policy.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在值迭代方法中，首先，我们通过对Q函数进行迭代最大化来计算最优值函数。一旦找到最优值函数，我们将使用它来提取最优策略。在策略迭代方法中，我们将尝试通过策略迭代的方式计算最优值函数。一旦我们找到了最优值函数，创建该最优值函数的策略将被提取为最优策略。
- en: Chapter 4 – Monte Carlo Methods
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 – 蒙特卡罗方法。
- en: In the Monte Carlo method, we approximate the value of a state by taking the
    average return of a state across *N* episodes instead of taking the expected return.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在蒙特卡罗方法中，我们通过取*N*次实验中某个状态的回报平均值来近似该状态的值，而不是取预期回报。
- en: To compute the value function using the dynamic programming method, we need
    to know the model dynamics, and when we don't know the model dynamics, we use
    model-free methods. The Monte Carlo method is a model-free method meaning that
    it doesn't require the model dynamics (transition probability) to compute the
    value function.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用动态规划方法计算值函数，我们需要了解模型动态。当我们不知道模型动态时，我们使用无模型方法。蒙特卡罗方法是一种无模型方法，意味着它不需要模型动态（转移概率）来计算值函数。
- en: In a prediction task, we evaluate the given policy by predicting the value function
    or Q function, which helps us to understand the expected return an agent would
    get if it used the given policy. However, in a control task, our goal is to find
    the optimal policy and are not given any policy as input, so we start by initializing
    a random policy and try to find the optimal policy iteratively.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预测任务中，我们通过预测值函数或Q函数来评估给定的策略，这有助于我们理解智能体在使用给定策略时所能获得的预期回报。然而，在控制任务中，我们的目标是找到最优策略，并且不会给定任何策略作为输入，因此我们从初始化一个随机策略开始，并尝试通过迭代的方式找到最优策略。
- en: In the MC prediction method, the value of a state and value of a state-action
    pair can be computed by just taking the average return of the state and an average
    return of state-action pair across several episodes respectively.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在MC预测方法中，状态值和状态-动作对的值可以通过分别取多个回合中的状态的平均回报和状态-动作对的平均回报来计算。
- en: In first-visit MC, we compute the return only for the first time the state is
    visited in the episode. In every-visit MC, we compute the return every time the
    state is visited in the episode.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在首次访问MC中，我们仅计算状态首次在回合中被访问时的回报。在每次访问MC中，我们每次访问状态时都会计算回报。
- en: When the environment is non-stationary, we don't have to take the return of
    the state from all the episodes and compute the average. As the environment is
    non-stationary, we can ignore returns from earlier episodes and use only the returns
    from the latest episodes for computing the average. Thus, we can compute the value
    of the state using the incremental mean.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当环境是非平稳的时，我们不需要从所有回合中取状态的回报并计算平均值。由于环境是非平稳的，我们可以忽略早期回合的回报，仅使用最新回合的回报来计算平均值。这样，我们可以使用增量平均值计算状态的值。
- en: In the on-policy method, we generate episodes using one policy and also improve
    the same policy iteratively to find the optimal policy, while with the off-policy
    Monte Carlo control method, we use two different policies for generating the episode
    (the behavior policy) and for finding the optimal policy (the target policy).
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，我们使用一个策略生成回合，并且迭代地改进同一个策略来找到最优策略；而在基于策略外的蒙特卡罗控制方法中，我们使用两种不同的策略生成回合（行为策略）和寻找最优策略（目标策略）。
- en: An epsilon-greedy policy is one where we select a random action (exploration)
    with probability epsilon, and we select the best action (exploitation) with probability
    1-epsilon.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: epsilon-贪心策略是指我们以概率epsilon选择一个随机动作（探索），并以概率1-epsilon选择最佳动作（利用）。
- en: Chapter 5 – Understanding Temporal Difference Learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 – 理解时间差分学习
- en: Unlike the Monte Carlo method, the **Temporal Difference** (**TD**) learning
    method makes use of bootstrapping so that we don't have to wait until the end
    of the episode to compute the value of a state.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与蒙特卡罗方法不同，**时间差分**（**TD**）学习方法利用自举技术，这样我们就不需要等到回合结束才能计算状态的值。
- en: The TD learning algorithm takes the benefits of both the dynamic programming
    and the Monte Carlo methods into account. That is, just like the dynamic programming
    method, we perform bootstrapping so that we don't have to wait till the end of
    an episode to compute the state value or Q value and just like the Monte Carlo
    method, it is a model-free method, and so it does not require the model dynamics
    of the environment to compute the state value or Q value.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD学习算法结合了动态规划和蒙特卡罗方法的优点。也就是说，像动态规划方法一样，我们执行自举操作，这样我们就不需要等到回合结束才能计算状态值或Q值；同时，像蒙特卡罗方法一样，它是一种无模型的方法，因此它不需要环境的模型动态来计算状态值或Q值。
- en: The TD error can be defined as the difference between the target value and predicted
    value.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD误差可以定义为目标值与预测值之间的差异。
- en: The TD learning update rule is given as ![](img/B15558_05_010.png).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD学习更新规则如图所示 ![](img/B15558_05_010.png)。
- en: In a TD prediction task, given a policy, we estimate the value function using
    the given policy. So, we can say what the expected return an agent can obtain
    in each state if it acts according to the given policy.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在TD预测任务中，给定一个策略，我们使用该策略估计值函数。因此，我们可以说，代理在每个状态下如果按照给定策略行动，期望获得的回报是多少。
- en: '**SARSA** is an on-policy TD control algorithm and it stands for **State-Action-Reward-State-Action**.
    The update rule for computing the Q function using SARSA is given as ![](img/B15558_18_045.png).'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**SARSA**是一种基于策略的TD控制算法，代表**状态-动作-奖励-状态-动作**。使用SARSA计算Q函数的更新规则如图所示 ![](img/B15558_18_045.png)。'
- en: SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy
    policy for selecting an action in the environment and also to compute the Q value
    of the next state-action pair, whereas Q learning is an off-policy algorithm meaning
    that we use an epsilon-greedy policy for selecting an action in the environment,
    but to compute the Q value of the next state-action pair we use a greedy policy.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SARSA是基于策略的算法，这意味着我们使用单一的epsilon-贪心策略来选择环境中的动作，并计算下一个状态-动作对的Q值，而Q学习是基于策略外的算法，意味着我们使用epsilon-贪心策略选择环境中的动作，但计算下一个状态-动作对的Q值时，我们使用贪心策略。
- en: Chapter 6 – Case Study – The MAB Problem
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 – 案例研究 – 多臂强盗问题
- en: The **Multi-Armed Bandit** (**MAB**) problem is one of the classic problems
    in RL. A MAB is a slot machine where we pull the arm (lever) and get a payout
    (reward) based on some probability distribution. A single slot machine is called
    a one-armed bandit, and when there are multiple slot machines, it is called a
    MAB or *k*-armed bandit, where *k* denotes the number of slot machines.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多臂强盗**（**MAB**）问题是强化学习中的经典问题之一。多臂强盗就像一个老虎机，我们拉动臂（杠杆），并根据某种概率分布获得奖励（回报）。单臂老虎机称为单臂强盗，当有多个老虎机时，称为多臂强盗或*k*臂强盗，其中*k*表示老虎机的数量。'
- en: With the epsilon-greedy policy, we select the best arm with probability 1-epsilon,
    and we select the random arm with probability epsilon.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在epsilon-贪心策略中，我们以1-epsilon的概率选择最佳臂，并以epsilon的概率选择随机臂。
- en: In softmax exploration, the arm will be selected based on the probability. However,
    in the initial rounds we will not know the correct average reward of each arm,
    so selecting the arm based on the probability of average reward will be inaccurate
    in the initial rounds. So to avoid this we introduce a new parameter called *T*.
    *T* is called the temperature parameter.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在软最大探索中，臂会根据概率进行选择。然而，在最初的回合中，我们不知道每个臂的正确平均奖励，因此基于平均奖励的概率来选择臂在初期会不准确。为了避免这种情况，我们引入了一个新的参数叫做*T*，*T*称为温度参数。
- en: The upper confidence bound is computed as ![](img/B15558_19_009.png).
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上置信区间计算为![](img/B15558_19_009.png)。
- en: When the value of ![](img/B15558_19_010.png) is higher than ![](img/B15558_19_011.png),
    then we will have a high probability closer to 1 than 0\.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当![](img/B15558_19_010.png)的值大于![](img/B15558_19_011.png)时，我们将获得一个接近1的高概率，而不是接近0的低概率。
- en: 'The steps involved in the Thomson sampling method are as follows:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thomson采样方法的步骤如下：
- en: Initialize the beta distribution with alpha and beta set to equal values for
    all the *k* arms
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化贝塔分布时，将所有的*k*臂的alpha和beta设置为相等的值。
- en: Sample a value from the beta distribution of all the *k* arms
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所有*k*臂的贝塔分布中采样一个值
- en: Pull the arm whose sampled value is high
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拉动采样值较高的臂
- en: If we win the game then update the alpha value of the distribution as ![](img/B15558_19_012.png)
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们赢得了游戏，则将分布的alpha值更新为![](img/B15558_19_012.png)
- en: If we lose the game then update the beta value of the distribution as ![](img/B15558_19_013.png)
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们输了游戏，则将分布的beta值更新为![](img/B15558_19_013.png)
- en: Repeat *steps 2* to *5* for several numbers of rounds
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*到*5*若干回合
- en: With contextual bandits, we take actions based on the state of the environment
    and the state holds the context. Contextual bandits are widely used for personalizing
    content according to the user's behavior. They are also used to solve the cold-start
    problems faced in recommendation systems.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上下文强盗时，我们会根据环境的状态采取行动，状态持有上下文。上下文强盗广泛应用于根据用户行为个性化内容推荐。它们也用于解决推荐系统中面临的冷启动问题。
- en: Chapter 7 – Deep Learning Foundations
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 – 深度学习基础
- en: The activation function is used to introduce non-linearity to neural networks.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活函数用于引入神经网络的非线性。
- en: The softmax function is basically a generalization of the sigmoid function.
    It is usually applied to the final layer of the network and while performing multi-class
    classification tasks. It gives the probabilities of each class being output and
    thus, the sum of softmax values will always equal 1.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax函数基本上是sigmoid函数的推广。它通常应用于网络的最后一层，并在进行多类分类任务时使用。它给出了每个类别的输出概率，因此，softmax值的总和将始终等于1。
- en: The epoch specifies the number of times the neural network sees our whole training
    data. So, we can say one epoch is equal to one forward pass and one backward pass
    for all training samples.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Epoch指定神经网络看到整个训练数据的次数。因此，我们可以说一个epoch等于所有训练样本的一个前向传播和一个反向传播。
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various **Natural Language Processing** (**NLP**) tasks, such
    as language translation, sentiment analysis, text generation, and so on.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）广泛应用于涉及序列数据的场景，如时间序列、文本、音频、语音、视频、天气等。它们已广泛用于各种**自然语言处理**（**NLP**）任务，如语言翻译、情感分析、文本生成等。
- en: While backpropagating the RNN, we multiply the weights and derivative of the
    tanh function at every time step. When we multiply smaller numbers at every step
    while moving backward, our gradient becomes infinitesimally small and leads to
    a number that the computer can't handle; this is called the vanishing gradient
    problem.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播RNN时，我们在每个时间步骤上乘以权重和tanh函数的导数。当我们在每一步向后传播时，如果每次乘上较小的数字，我们的梯度会变得极小，导致计算机无法处理的数值；这就是所谓的消失梯度问题。
- en: The pooling layer reduces spatial dimensions by keeping only the important features.
    The different types of pooling operation include max pooling, average pooling,
    and sum pooling.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化层通过仅保留重要特征来减少空间维度。不同类型的池化操作包括最大池化、平均池化和求和池化。
- en: Suppose, we want our GAN to generate handwritten digits. First, we will take
    a dataset containing a collection of handwritten digits; say, the MNIST dataset.
    The generator learns the distribution of images in our dataset. It learns the
    distribution of handwritten digits in our training set. We feed random noise to
    the generator and it will convert the random noise into a new handwritten digit
    similar to the one in our training set. The goal of the discriminator is to perform
    a classification task. Given an image, it classifies it as real or fake; that
    is, whether the image is from the training set or has been generated by the generator.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们希望GAN生成手写数字。首先，我们将获取一个包含手写数字集合的数据集，比如MNIST数据集。生成器学习数据集中图像的分布。它学习训练集中手写数字的分布。我们将随机噪声输入生成器，它会将随机噪声转化为与训练集中的手写数字相似的新数字。判别器的目标是执行分类任务。给定一张图像，判别器将其分类为真实或伪造；即判断该图像是来自训练集还是由生成器生成的。
- en: Chapter 8 – A Primer on TensorFlow
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 – TensorFlow入门
- en: A TensorFlow session is used to execute computational graphs with operations
    on the node and tensors to its edges.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow会话用于执行计算图，其中包含节点上的操作和连接到边缘的张量。
- en: Variables are the containers used to store values. Variables will be used as
    input to several other operations in the computational graph. We can think of
    placeholders as variables, where we only define the type and dimension, but will
    not assign the value. Values for the placeholders will be fed at runtime. We feed
    the data to the computational graphs using placeholders. Placeholders are defined
    with no values.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量是用来存储值的容器。变量将作为输入，传递给计算图中的其他操作。我们可以将占位符看作是变量，其中我们只定义类型和维度，但不会赋予值。占位符的值将在运行时提供。我们使用占位符将数据输入到计算图中。占位符在定义时没有值。
- en: TensorBoard is TensorFlow's visualization tool that can be used to visualize
    the computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it would become confusing when we have to debug the model.
    As we can visualize the computational graph in TensorBoard, we can easily understand,
    debug, and optimize such complex models. It also supports sharing.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorBoard是TensorFlow的可视化工具，可以用来可视化计算图。它还可以用于绘制各种定量指标以及一些中间计算的结果。当我们训练一个非常深的神经网络时，调试模型时可能会感到困惑。由于我们可以在TensorBoard中可视化计算图，因此我们可以轻松理解、调试和优化这些复杂的模型。它还支持共享功能。
- en: Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping.
    Unlike the graph mode, where we need to construct a graph every time to perform
    any operation, eager execution follows the imperative programming paradigm, where
    any operation can be performed immediately without having to create a graph, just
    like we do in Python.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow中的急切执行方式更符合Python编程风格，并且支持快速原型开发。与图模式不同，图模式下我们每次进行操作时都需要构建一个计算图，而急切执行遵循命令式编程范式，任何操作都可以立即执行，而无需创建图，就像我们在Python中一样。
- en: 'Building a model in Keras involves four important steps:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中构建模型包含四个重要步骤：
- en: Defining the model
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型
- en: Compiling the model
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型
- en: Fitting the model
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Evaluating the model
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack of one above another.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 功能模型提供的灵活性比顺序模型更大。例如，在功能模型中，我们可以轻松地将任何一层连接到另一层，而在顺序模型中，每一层都堆叠在另一个之上。
- en: Chapter 9 – Deep Q Network and Its Variants
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 – 深度Q网络及其变种
- en: When the environment consists of a large number of states and actions, it will
    be very expensive to compute the Q value of all possible state-action pairs in
    an exhaustive fashion. So, we use a deep Q network for approximating the Q function.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当环境由大量状态和动作组成时，采用穷举的方式计算所有可能的状态-动作对的Q值会非常昂贵。因此，我们使用深度Q网络来近似Q函数。
- en: We use a buffer called the replay buffer to collect the agent's experience and
    based on this experience, we train our network. The replay buffer is usually implemented
    as a queue structure (first in, first out) rather than a list. So, if the buffer
    is full and the new experience comes in, we remove the old experience and add
    the new experience into the buffer.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用一个叫做回放缓冲区（replay buffer）的缓存来收集智能体的经验，并基于这些经验来训练我们的网络。回放缓冲区通常实现为队列结构（先进先出），而不是列表。因此，如果缓冲区已满并且有新的经验进入，我们会删除旧的经验，并将新的经验添加到缓冲区中。
- en: When the target and predicted values depend on the same parameter ![](img/B15558_12_006.png),
    it will cause instability in the mean squared error and the network will learn
    poorly. It also causes a lot of divergence during training. So, we use a target
    network.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当目标值和预测值依赖于相同的参数！[](img/B15558_12_006.png)时，会导致均方误差的不稳定，网络将学习得很差。它还会在训练过程中导致很多发散问题。因此，我们使用目标网络。
- en: Unlike with DQNs, in double DQNs, we compute the target value using two Q functions.
    One Q function parameterized by the main network parameter ![](img/B15558_12_006.png)
    selects the action that has the maximum Q value, and the other Q function parameterized
    by the target network parameter ![](img/B15558_12_025.png) computes the Q value
    using the action selected by the main network.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与DQN不同，在双DQN中，我们使用两个Q函数来计算目标值。一个Q函数由主网络的参数！[](img/B15558_12_006.png)进行参数化，选择具有最大Q值的动作，另一个Q函数由目标网络的参数！[](img/B15558_12_025.png)进行参数化，计算由主网络选择的动作的Q值。
- en: A transition with a high TD error implies that the transition is not correct
    and so we need to learn more about that transition to minimize the error. A transition
    with a low TD error implies that the transition is already good. We can always
    learn more from our mistakes rather than only focusing on what we are already
    good at, right? Similarly, we can learn more from the transitions with a high
    TD error than those with a low TD error. Thus, we can assign a higher priority
    to the transitions with a high TD error and a lower priority to transitions that
    got a low TD error.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个具有较高TD误差的过渡意味着该过渡是不正确的，因此我们需要更多地学习这个过渡以减少误差。一个具有较低TD误差的过渡意味着该过渡已经很好。我们总是能从错误中学到更多，而不仅仅是关注我们已经做得好的部分，对吧？同样，我们可以从具有较高TD误差的过渡中学到更多，而不是从那些具有较低TD误差的过渡中学到的东西。因此，我们可以对具有较高TD误差的过渡赋予更高的优先级，而对那些误差较低的过渡赋予较低的优先级。
- en: The advantage function can be defined as the difference between the Q function
    and the value function.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优势函数可以定义为Q函数和价值函数之间的差异。
- en: The LSTM layer is in the DQN so that we can retain information about the past
    states as long as it is required. Retaining information about the past states
    helps us when we have the problem of **Partially Observable Markov Decision Processes**
    (**POMDPs**).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM层在DQN中用于保持对过去状态的信息，直到不再需要。保持过去状态的信息对我们解决**部分可观察马尔可夫决策过程**（**POMDPs**）中的问题非常有帮助。
- en: Chapter 10 – Policy Gradient Method
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 – 策略梯度方法
- en: In the value-based method, we extract the optimal policy from the optimal Q function
    (Q values).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，我们从最优Q函数（Q值）中提取最优策略。
- en: It is difficult to compute optimal policy using the value-based method when
    our action space is continuous. So, we use the policy-based method. In the policy-based
    method, we compute the optimal policy without the Q function.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们的动作空间是连续时，使用基于价值的方法计算最优策略是困难的。因此，我们使用基于策略的方法。在基于策略的方法中，我们无需Q函数即可计算最优策略。
- en: In the policy gradient method, we select actions based on the action probability
    distribution given by the network and if we win the episode, that is, if we get
    a high return, then we assign high probabilities to all the actions of the episode,
    else we assign low probabilities to all the actions of the episode.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在策略梯度方法中，我们根据网络给出的动作概率分布来选择动作。如果我们赢得了这一轮，也就是说，如果我们获得了较高的回报，那么我们会给该轮的所有动作分配较高的概率，否则我们会给该轮的所有动作分配较低的概率。
- en: The policy gradient is computed as ![](img/B15558_19_017.png).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略梯度的计算公式是！[](img/B15558_19_017.png)。
- en: Reward-to-go is basically the return of the trajectory starting from the state
    *s*[t]. It is computed as ![](img/B15558_10_126.png).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回报到达（Reward-to-go）基本上是从状态 *s*[t] 开始的轨迹的回报。它的计算公式为 ![](img/B15558_10_126.png)。
- en: The policy gradient with the baseline function is a policy gradient method that
    uses the baseline function to reduce the variance in the return.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带基线函数的策略梯度方法是一种使用基线函数来减少回报方差的策略梯度方法。
- en: The baseline function *b* gives us the expected return from the state the agent
    is in, then subtracting *b* on every step will reduce the variance in the return.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基线函数 *b* 给出的是智能体所在状态的期望回报，然后在每一步减去 *b* 可以减少回报的方差。
- en: Chapter 11 – Actor-Critic Methods – A2C and A3C
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 – 演员-评论家方法 – A2C 和 A3C
- en: The actor-critic method is one of the most popular algorithms in deep RL. Several
    modern deep RL algorithms are designed based on the actor-critic method. The actor-critic
    method lies at the intersection of value-based and policy-based methods. That
    is, it takes advantage of both value-based and policy-based methods.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员-评论家方法是深度强化学习中最流行的算法之一。许多现代深度强化学习算法是基于演员-评论家方法设计的。演员-评论家方法位于基于价值的方法与基于策略的方法的交汇处。也就是说，它同时利用了基于价值和基于策略的方法。
- en: In the actor-critic method, the actor computes the optimal policy and the critic
    evaluates the policy computed by the actor network by estimating the value function.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在演员-评论家方法中，演员计算最优策略，评论家通过估计价值函数来评估演员网络计算的策略。
- en: In the policy gradient method with baseline, first, we generate complete episodes
    (trajectories), and then we update the parameter of the network, whereas, in the
    actor-critic method, we update the network parameter at every step of the episode.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在带基线的策略梯度方法中，我们首先生成完整的回合（轨迹），然后更新网络的参数；而在演员-评论家方法中，我们在每一步的回合中更新网络参数。
- en: In the actor network, we compute the gradient as ![](img/B15558_19_019.png)**.**
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在演员网络中，我们计算梯度为 ![](img/B15558_19_019.png)**.**
- en: In **advantage actor-critic** (**A2C**), we compute the policy gradient with
    the advantage function and the advantage function is the difference between the Q
    function and the value function, that is, *Q*(*s*, *a*) – *V*(*s*).
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**优势演员-评论家**（**A2C**）方法中，我们使用优势函数来计算策略梯度，优势函数是Q函数与价值函数之间的差异，也就是 *Q*(*s*, *a*)
    – *V*(*s*)。
- en: The word asynchronous implies the way A3C works. That is, instead of having
    a single agent that tries to learn the optimal policy, here, we have multiple
    agents that interact with the environment. Since we have multiple agents interacting
    with the environment at the same time, we provide copies of the environment to
    every agent so that each agent can interact with its own copy of the environment.
    So, all these multiple agents are called worker agents and we have a separate
    agent called the global agent. All the worker agents report to the global agent
    asynchronously and the global agent aggregates the learning.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “异步”一词暗示了A3C的工作方式。也就是说，A3C不是通过单个智能体来学习最优策略，而是有多个智能体与环境互动。由于多个智能体同时与环境互动，我们为每个智能体提供环境的副本，使得每个智能体都能与自己专属的环境副本互动。因此，这些多个智能体被称为工作智能体，而我们还有一个单独的智能体叫做全局智能体。所有工作智能体都异步地向全局智能体报告，全局智能体则聚合学习结果。
- en: In A2C, we can have multiple worker agents, each interacting with its own copies
    of the environment, and all the worker agents perform the synchronous updates,
    unlike A3C where the worker agents perform asynchronous updates.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在A2C中，我们可以有多个工作智能体，每个智能体与自己的环境副本进行互动，所有工作智能体都进行同步更新，不像A3C那样工作智能体进行异步更新。
- en: Chapter 12 – Learning DDPG, TD3, and SAC
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 – 学习DDPG、TD3和SAC
- en: DDPG consists of an actor and critic. The actor is a policy network and uses
    the policy gradient method for learning the optimal policy. The critic is a DQN
    and it evaluates the action produced by the actor.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG由演员和评论家组成。演员是一个策略网络，使用策略梯度方法来学习最优策略。评论家是一个DQN，它评估演员产生的动作。
- en: The critic is basically a DQN. The goal of the critic is to evaluate the action
    produced by the actor network. The critic evaluates the action produced by the
    actor using the Q value computed by the DQN.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评论家基本上是一个DQN。评论家的目标是评估演员网络产生的动作。评论家使用DQN计算的Q值来评估演员产生的动作。
- en: The key features of TD3 includes clipped double Q learning, delayed policy updates,
    and target policy smoothing.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD3的关键特性包括裁剪双Q学习、延迟的策略更新和目标策略平滑。
- en: Instead of using one critic network, we use two main critic networks for computing
    the Q value and we use two target critic networks for computing the target value.
    We compute two target Q values using two target critic networks and use the minimum
    value out of these two while computing the loss. This helps in preventing the
    overestimation of the target Q value.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不使用单一的评论者网络，而是使用两个主要评论者网络来计算Q值，同时使用两个目标评论者网络来计算目标值。我们使用两个目标评论者网络计算两个目标Q值，并在计算损失时取这两个值中的最小值。这有助于防止目标Q值的过高估计。
- en: The DDPG method produces different target values even for the same action, thus
    the variance of the target value will be high even for the same action, so we
    reduce this variance by adding some noise to the target action.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG方法即使在相同的动作下也会产生不同的目标值，因此即使对于相同的动作，目标值的方差也会很高，因此我们通过向目标动作中加入一些噪声来减少这种方差。
- en: In the SAC method, we use a slightly modified version of the objective function
    with the entropy term as ![](img/B15558_19_020.png) and it is often called **maximum
    entropy RL** or **entropy regularized RL**. Adding an entropy term is also often
    referred to as an entropy bonus.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SAC方法中，我们使用带有熵项的目标函数的略微修改版本，表示为：![](img/B15558_19_020.png)，它通常被称为**最大熵RL**或**熵正则化RL**。添加熵项也常常被称为熵奖励。
- en: The role of the critic network is to evaluate the policy produced by the actor.
    Instead of using only the Q function to evaluate the actor's policy, the critic
    in SAC uses both the Q function and the value function.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评论者网络的作用是评估由行为者生成的策略。在SAC中，评论者不仅使用Q函数来评估行为者的策略，还使用价值函数。
- en: Chapter 13 – TRPO, PPO, and ACKTR Methods
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章 – TRPO、PPO 和 ACKTR 方法
- en: The trust region implies the region where our actual function *f*(*x*) and approximated
    function ![](img/B15558_13_038.png) are close together. So, we can say that our
    approximation will be accurate if our approximated function ![](img/B15558_13_038.png)
    is in the trust region.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信任区域指的是实际函数*f*(*x*)和近似函数![](img/B15558_13_038.png)之间接近的区域。因此，我们可以说，如果我们的近似函数![](img/B15558_13_038.png)位于信任区域内，那么我们的近似将是准确的。
- en: TRPO is a policy gradient algorithm, and it acts as an improvement to policy
    gradient with baseline. TRPO tries to make a large policy update while imposing
    a KL constraint that the old policy and the new policy should not vary from each
    other too much. TRPO guarantees monotonic policy improvement, guaranteeing that
    there will always be a policy improvement on every iteration.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TRPO是一种策略梯度算法，是对带基准的策略梯度的改进。TRPO试图进行一次大规模的策略更新，同时施加KL约束，确保旧策略和新策略之间的差异不会过大。TRPO保证了单调的策略改进，确保每次迭代都会有策略的改进。
- en: Just like gradient descent, conjugate gradient descent also tries to find the
    minimum of the function; however, the search direction of conjugate gradient descent
    will be different from gradient descent and conjugate gradient descent attains
    convergence in *N* iterations.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像梯度下降一样，共轭梯度下降也试图找到函数的最小值；然而，共轭梯度下降的搜索方向与梯度下降不同，并且共轭梯度下降在*N*次迭代内达到收敛。
- en: The update rule of TRPO is given as ![](img/B15558_13_240.png).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TRPO的更新规则如图所示：![](img/B15558_13_240.png)。
- en: PPO acts as an improvement to the TRPO algorithm and is simple to implement.
    Similar to TRPO, PPO ensures that the policy updates are in the trust region.
    But unlike TRPO, PPO does not use any constraint in the objective function.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PPO是对TRPO算法的改进，并且实现简单。与TRPO类似，PPO确保策略更新位于信任区域内。但与TRPO不同，PPO在目标函数中不使用任何约束。
- en: In the PPO clipped method, in order to ensure that the policy updates are in
    the trust region, that is, the new policy is not far away from the old policy,
    PPO adds a new function called the clipping function, which ensures that the new
    and old policies are not far away from each other.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在PPO裁剪方法中，为了确保策略更新位于信任区域，即新策略不远离旧策略，PPO添加了一个新的函数，称为裁剪函数，确保新旧策略不会相差太远。
- en: K-FAC approximates the Fisher information matrix as a block diagonal matrix
    where each block contains the derivatives. Then each block is approximated as
    a Kronecker product of two matrices, which is known as Kronecker factorization.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K-FAC将费舍尔信息矩阵近似为一个块对角矩阵，其中每个块包含导数。然后，每个块被近似为两个矩阵的克罗内克积，这被称为克罗内克因式分解。
- en: Chapter 14 – Distributional Reinforcement Learning
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 – 分布式强化学习
- en: In a distributional RL, instead of selecting an action based on the expected
    return, we select the action based on the distribution of the return, which is
    often called the value distribution or return distribution.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分布式强化学习中，我们不是根据期望回报选择动作，而是根据回报的分布来选择动作，这通常被称为价值分布或回报分布。
- en: In categorical DQN, we feed the state and support of the distribution as the
    input and the network returns the probabilities of the value distribution.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分类 DQN 中，我们将状态和分布的支持作为输入，网络返回价值分布的概率。
- en: The authors of the categorical DQN suggest that it will be efficient to choose
    the number of support *N* as 51 and so the categorical DQN is also known as the
    C51 algorithm.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类 DQN 的作者建议，选择支持的数量 *N* 为 51 会更高效，因此分类 DQN 也被称为 C51 算法。
- en: Inverse CDF is also known as the quantile function. Inverse CDF as the name
    suggests is the inverse of the cumulative distribution function. That is, in CDF,
    given the support *x*, we obtain the cumulative probability ![](img/B15558_12_056.png),
    whereas in inverse CDF, given cumulative probability ![](img/B15558_12_056.png),
    we obtain the support *x*.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逆累积分布函数（Inverse CDF）也被称为分位数函数。顾名思义，逆累积分布函数是累积分布函数的逆函数。也就是说，在 CDF 中，给定支持 *x*，我们得到累积概率
    ![](img/B15558_12_056.png)，而在逆 CDF 中，给定累积概率 ![](img/B15558_12_056.png)，我们得到支持
    *x*。
- en: In a categorical DQN, along with the state, we feed the fixed support at equally
    spaced intervals as an input to the network, and it returns the non-uniform probabilities.
    However, in a QR-DQN, along with the state, we feed the fixed uniform probabilities
    as an input to the network and it returns the support at variable locations (unequally
    spaced support).
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分类 DQN 中，我们将固定支持（在均匀间隔下）与状态一起输入到网络中，网络返回非均匀的概率。然而，在 QR-DQN 中，我们将固定的均匀概率与状态一起输入到网络中，网络返回在变量位置的支持（不均匀间隔的支持）。
- en: 'The D4PG is similar to DDPG except for the following:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D4PG 类似于 DDPG，区别如下：
- en: We use a distributional DQN in the critic network instead of using the regular
    DQN to estimate the Q values.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在评论网络中使用分布式 DQN，而不是使用常规的 DQN 来估计 Q 值。
- en: We calculate *N*-step returns in the target instead of calculating one-step
    returns.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算 *N* 步回报作为目标，而不是计算一步回报。
- en: We use prioritized experience replay and add importance to the gradient updates
    in the critic network.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用优先经验回放，并在评论网络中对梯度更新赋予重要性。
- en: Instead of using one actor, we use *L* independent actors, each of which acts
    in parallel, collecting experience and storing the experience in the replay buffer.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不使用一个演员，而是使用 *L* 个独立的演员，每个演员并行行动，收集经验并将经验存储在回放缓冲区中。
- en: Chapter 15 – Imitation Learning and Inverse RL
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章 – 模仿学习与逆强化学习
- en: One of the simplest and most naive ways to perform imitation learning is by
    treating an imitation learning task as a supervised learning task. First, we collect
    a set of expert demonstrations, then we train a classifier to perform the same
    action performed by the expert in a particular state. We can view this as a big
    multiclass classification problem and train our agent to perform the action performed
    by the expert in the respective state.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行模仿学习最简单和最直观的方法之一是将模仿学习任务视为监督学习任务。首先，我们收集一组专家示范，然后我们训练一个分类器，执行专家在特定状态下执行的相同动作。我们可以将其视为一个大的多类分类问题，并训练我们的智能体在相应状态下执行专家所执行的动作。
- en: In DAgger, we aggregate the dataset over a series of iterations and train the
    classifier on the aggregated dataset.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DAgger 中，我们通过一系列迭代来聚合数据集，并在聚合后的数据集上训练分类器。
- en: In DQfD, we fill the replay buffer with expert demonstrations and pre-train
    the agent. Note that these expert demonstrations are used only for pretraining
    the agent. Once the agent is pre-trained, the agent will interact with the environment
    and gather more experience and make use of it for learning. Thus DQfD consists
    of two phases, which are pre-training and training.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DQfD 中，我们用专家示范填充回放缓冲区，并对智能体进行预训练。请注意，这些专家示范仅用于智能体的预训练。一旦智能体完成预训练，它将与环境交互，收集更多经验并利用这些经验进行学习。因此，DQfD
    包括两个阶段：预训练和训练。
- en: IRL is used when it is hard to design the reward function. In RL, we try to
    find the optimal policy given the reward function but in IRL, we try to learn
    the reward function given the expert demonstrations. Once we have derived the
    reward function from the expert demonstrations using IRL, then we can use the
    reward function to train our agent to learn the optimal policy using any RL algorithm.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IRL用于当设计奖励函数很困难时。在RL中，我们尝试在给定奖励函数的情况下找到最优策略，但在IRL中，我们尝试根据专家演示学习奖励函数。一旦我们通过IRL从专家演示中推导出奖励函数，就可以使用这个奖励函数来训练我们的代理，使用任何强化学习算法来学习最优策略。
- en: We can represent the state with a feature vector *f*. Let's say we have a state
    *s*; then its feature vector can be defined as *f*[s].
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以用特征向量*f*表示状态。假设我们有一个状态*s*，那么它的特征向量可以定义为*f*[s]。
- en: In GAIL, the role of the generator is to generate a policy by learning the occupancy
    measure of the expert policy, and the role of the discriminator is to classify
    whether the generated policy is from the expert policy or the agent policy. So,
    we train the generator using TRPO. The discriminator is basically a neural network
    that tells us whether the policy generated by the generator is the expert policy
    or the agent policy.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GAIL中，生成器的角色是通过学习专家策略的占用度量来生成一个策略，而判别器的角色是分类生成的策略是来自专家策略还是来自代理策略。因此，我们使用TRPO训练生成器。判别器基本上是一个神经网络，用于判断生成器生成的策略是专家策略还是代理策略。
- en: Chapter 16 – Deep Reinforcement Learning with Stable Baselines
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章 – 使用Stable Baselines进行深度强化学习
- en: Stable Baselines is an improved implementation of OpenAI Baselines. Stable Baselines
    is a high-level library that is easier to use than OpenAI Baselines, and it also
    includes state-of-the-art deep RL algorithms along with offering several useful
    features.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Stable Baselines是OpenAI Baselines的改进版。Stable Baselines是一个更易于使用的高级库，提供了最新的深度强化学习算法，并且还包含了几个有用的功能。
- en: We can save the agent as `agent.save()` and load the trained agent as `agent.load()`.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过`agent.save()`保存代理，并通过`agent.load()`加载已训练的代理。
- en: We generally train our agent in a single environment per step but with Stable
    Baselines, we can train our agent in multiple environments per step. This helps
    our agent to learn quickly. Now, our states, actions, reward, and done will be
    in the form of a vector since we are training our agent in multiple environments.
    So, we call this a vectorized environment.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通常在每个步骤中只在一个环境中训练我们的代理，但使用Stable Baselines，我们可以在每个步骤中在多个环境中训练代理。这有助于我们的代理更快地学习。现在，我们的状态、动作、奖励和完成标志将以向量的形式呈现，因为我们在多个环境中训练代理。因此，我们称之为向量化环境。
- en: In SubprocVecEnv, we run each environment in a different process, whereas in
    DummyVecEnv, we run each environment in the same process.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SubprocVecEnv中，我们在不同的进程中运行每个环境，而在DummyVecEnv中，我们在同一个进程中运行每个环境。
- en: With Stable Baselines, it is easier to view the computational graph of our model
    in TensorBoard. In order to do that, we just need to pass the directory where
    we need to store our log files while instantiating the agent.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Stable Baselines，查看我们模型的计算图在TensorBoard中变得更容易。为了做到这一点，我们只需在实例化代理时传递需要存储日志文件的目录。
- en: With Stable Baselines, we can easily record a video of our agent using the `VecVideoRecorder`
    module.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Stable Baselines，我们可以轻松地通过`VecVideoRecorder`模块录制代理的视频。
- en: Chapter 17 – Reinforcement Learning Frontiers
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章 – 强化学习前沿
- en: Meta learning produces a versatile AI model that can learn to perform various
    tasks without having to train them from scratch. We train our meta-learning model
    on various related tasks with a few data points, so for a new but related task,
    it can make use of the learning obtained from the previous tasks and we don't
    have to train it from scratch.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 元学习生成了一种多功能的AI模型，它可以学习执行各种任务，而无需从头开始训练。我们在多个相关任务上用少量数据点训练我们的元学习模型，因此对于一个新的但相关的任务，它可以利用从前一个任务中获得的学习成果，我们不必从头开始训练它。
- en: '**Model-Agnostic Meta Learning** (**MAML**) is one of the most popularly used
    meta-learning algorithms and it has created a major breakthrough in meta-learning
    research. The basic idea of MAML is to find a better initial model parameter so
    that with good initial parameters, the model can learn quickly on new tasks with
    fewer gradient steps.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型无关元学习**（**MAML**）是最常用的元学习算法之一，它在元学习研究中取得了重大突破。MAML的基本思路是找到更好的初始模型参数，这样通过好的初始参数，模型可以在新任务上快速学习，且只需较少的梯度更新步骤。'
- en: In the outer loop of MAML, we update the model parameter as ![](img/B15558_19_026.png)
    and it is known as a meta objective.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 MAML 的外循环中，我们更新模型参数，如 ![](img/B15558_19_026.png)，它被称为元目标。
- en: The meta training set basically acts as a training set in the outer loop and
    is used to update the model parameter in the outer loop.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 元训练集基本上充当外循环中的训练集，用于更新外循环中的模型参数。
- en: In hierarchical RL, we decompose a large problem into small subproblems in a
    hierarchy. The different methods used in hierarchical RL include state-space decomposition,
    state abstraction, and temporal abstraction.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次化强化学习中，我们将一个大问题分解为层次结构中的小子问题。层次化强化学习中使用的不同方法包括状态空间分解、状态抽象和时间抽象。
- en: With an imagination augmented agent, before taking any action in an environment,
    the agent imagines the consequences of taking the action and if they think the
    action will provide a good reward, they will perform the action.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用增强想象力的智能体，在采取任何行动之前，智能体会想象采取该行动的后果，如果他们认为该行动会带来好的奖励，他们就会执行这个行动。
