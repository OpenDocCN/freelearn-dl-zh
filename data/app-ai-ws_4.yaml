- en: 4\. An Introduction to Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 决策树简介
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces you to two types of supervised learning algorithms in
    detail. The first algorithm will help you classify data points using decision
    trees, while the other algorithm will help you classify data points using random
    forests. Furthermore, you'll learn how to calculate the precision, recall, and
    F1 score of models, both manually and automatically. By the end of this chapter,
    you will be able to analyze the metrics that are used for evaluating the utility
    of a data model and classify data points based on decision trees and random forest
    algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍两种类型的监督学习算法。第一个算法将帮助你使用决策树对数据点进行分类，而另一个算法则帮助你使用随机森林对数据点进行分类。此外，你还将学习如何手动和自动计算模型的精确度、召回率和F1分数。到本章结束时，你将能够分析用于评估数据模型效用的指标，并基于决策树和随机森林算法对数据点进行分类。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous two chapters, we learned the difference between regression
    and classification problems, and we saw how to train some of the most famous algorithms.
    In this chapter, we will look at another type of algorithm: tree-based models.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们学习了回归问题与分类问题的区别，并且我们看到了如何训练一些最著名的算法。本章中，我们将介绍另一种算法类型：基于树的模型。
- en: Tree-based models are very popular as they can model complex non-linear patterns
    and they are relatively easy to interpret. In this chapter, we will introduce
    you to decision trees and the random forest algorithms, which are some of the
    most widely used tree-based models in the industry.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的模型非常流行，因为它们可以建模复杂的非线性模式，并且相对容易理解。本章中，我们将介绍决策树和随机森林算法，这些是行业中最广泛使用的基于树的模型之一。
- en: Decision Trees
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree has leaves, branches, and nodes. Nodes are where a decision
    is made. A decision tree consists of rules that we use to formulate a decision
    (or prediction) on the prediction of a data point.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有叶子节点、分支和节点。节点是做出决策的地方。决策树由我们用来制定决策（或预测）数据点的规则组成。
- en: Every node of the decision tree represents a feature, while every edge coming
    out of an internal node represents a possible value or a possible interval of
    values of the tree. Each leaf of the tree represents a label value of the tree.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的每个节点代表一个特征，而从内部节点出来的每条边代表树的一个可能值或值区间。树的每个叶子节点代表树的一个标签值。
- en: This may sound complicated, but let's look at an application of this.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有些复杂，但让我们来看一个应用实例。
- en: 'Suppose we have a dataset with the following features and the response variable
    is determining whether a person is creditworthy or not:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，具有以下特征，并且响应变量是判断一个人是否有信用：
- en: '![Figure 4.1: Sample dataset to formulate the rules'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1：用于制定规则的示例数据集'
- en: '](img/B16060_04_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_01.jpg)'
- en: 'Figure 4.1: Sample dataset to formulate the rules'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：用于制定规则的示例数据集
- en: 'A decision tree, remember, is just a group of rules. Looking at the dataset
    in *Figure 4.1*, we can come up with the following rules:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，决策树只是一些规则的集合。查看*图 4.1*中的数据集，我们可以得出以下规则：
- en: All people with house loans are determined as creditworthy.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有有房贷的人都被确定为有信用的人。
- en: If debtors are employed and studying, then loans are creditworthy.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果债务人有工作并且在学习，那么贷款是有信用的。
- en: People with income above 75,000 a year are creditworthy.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年收入超过75,000的人是有信用的。
- en: At or below 75,000 a year, people with car loans and who are employed are creditworthy.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年收入在75,000以下的、有车贷并且有工作的人员是有信用的。
- en: 'Following the order of the rules we just defined, we can build a tree, as shown
    in *Figure 4.2* and describe one possible credit scoring method:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们刚才定义的规则顺序，我们可以建立一棵树，如*图 4.2*所示，并描述一种可能的信用评分方法：
- en: '![Figure 4.2: Decision tree for the loan type'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：贷款类型的决策树'
- en: '](img/B16060_04_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_02.jpg)'
- en: 'Figure 4.2: Decision tree for the loan type'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：贷款类型的决策树
- en: First, we determine the loan type. House loans are automatically creditworthy
    according to the first rule. Study loans are described by the second rule, resulting
    in a subtree containing another decision on employment. Since we have covered
    both house and study loans, there are only car loans left. The third rule describes
    an income decision, while the fourth rule describes a decision on employment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们确定贷款类型。根据第一个规则，房屋贷款自动为信用良好。学习贷款由第二个规则描述，结果是一个包含另一个关于就业的决策的子树。由于我们已经涵盖了房屋贷款和学习贷款，因此只剩下汽车贷款。第三个规则描述了收入决策，而第四个规则描述了关于就业的决策。
- en: Whenever we must score a new debtor to determine whether they are creditworthy,
    we have to go through the decision tree from top to bottom and observe the true
    or false value at the bottom.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们必须对一个新的债务人进行评分，以确定其是否具有信用时，我们必须从决策树的顶部到底部进行遍历，并观察底部的真假值。
- en: Obviously, a model based on seven data points is highly inaccurate because we
    can't generalize rules that simply do not match reality. Therefore, rules are
    often determined based on large amounts of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，基于七个数据点的模型准确性非常低，因为我们无法推广那些与现实完全不符的规则。因此，规则通常是基于大量数据来确定的。
- en: This is not the only way that we can create a decision tree. We can build decision
    trees based on other sequences of rules, too. Let's extract some other rules from
    the dataset in *Figure 4.1*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是构建决策树的唯一方法。我们还可以基于其他规则的顺序来构建决策树。让我们从*图 4.1*的数据集中提取一些其他规则。
- en: '**Observation 1**: Notice that individual salaries that are greater than 75,000
    are all creditworthy.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察 1**：注意到所有收入超过 75,000 的个人都是信用良好的。'
- en: '`Income > 75,000 => CreditWorthy` is true.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`收入 > 75,000 => 信用良好` 这个规则成立。'
- en: Rule 1 classifies four out of seven data points (IDs C, E, F, G); we need more
    rules for the remaining three data points.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 规则 1 将七个数据点中的四个（ID C、E、F、G）分类；对于剩下的三个数据点，我们需要更多的规则。
- en: '**Observation 2**: Out of the remaining three data points, two are not employed.
    One is employed (ID D) and is creditworthy. With this, we can claim the following
    rule:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察 2**：在剩下的三个数据点中，有两个没有就业。其中一个是就业的（ID D），并且是信用良好的。由此，我们可以得出以下规则：'
- en: '`Income <= 75,000`, the following holds true: `Employed == true => CreditWorthy`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`收入 <= 75,000` 时，以下情况成立：`在职 == true => 信用良好`。'
- en: 'Note that with this second rule, we can also classify the remaining two data
    points (IDs A and B) as not creditworthy. With just two rules, we accurately classified
    all the observations from this dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用这个第二个规则，我们也可以将剩下的两个数据点（ID A 和 B）分类为不信用良好。通过这两个规则，我们准确地分类了该数据集中的所有观察点：
- en: '![Figure 4.3: Decision tree for income'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3：收入的决策树'
- en: '](img/B16060_04_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_03.jpg)'
- en: 'Figure 4.3: Decision tree for income'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：收入的决策树
- en: The second decision tree is less complex. At the same time, we cannot overlook
    the fact that the model says, *employed people with a lower income are less likely
    to pay back their loans*. Unfortunately, there is not enough training data available
    (there are only seven observations in this example), which makes it likely that
    we'll end up with false conclusions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个决策树比较简单。同时，我们不能忽视这样一个事实：模型表示，*低收入的在职人员更可能无法偿还贷款*。不幸的是，训练数据不足（这个例子中只有七个观察点），这使得我们很可能得出错误的结论。
- en: Overfitting is a frequent problem in decision trees when making a decision based
    on a few data points. This decision is rarely representative.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是决策树中常见的问题，当我们仅基于少量数据点做出决策时，这个决策通常不能代表整体情况。
- en: Since we can build decision trees in any possible order, it makes sense to define
    an efficient way of constructing a decision tree. Therefore, we will now explore
    a measure for ordering the features in the decision process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以按照任何可能的顺序构建决策树，因此定义一种高效的决策树构建方式是有意义的。因此，我们将探讨一种用于在决策过程中排序特征的度量方法。
- en: Entropy
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵
- en: In information theory, entropy measures how randomly distributed the possible
    values of an attribute are. The higher the degree of randomness is, the higher
    the entropy of the attribute.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息论中，熵衡量一个属性的可能值分布的随机程度。随机程度越高，属性的熵值越高。
- en: Entropy is the highest possibility of an event. If we know beforehand what the
    outcome will be, then the event has no randomness. So, entropy is **zero**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是事件的最高可能性。如果我们事先知道事件的结果，那么这个事件就没有随机性。因此，熵值为**零**。
- en: We use entropy to order the splitting of nodes in the decision tree. Taking
    the previous example, which rule should we start with? Should it be `Income <=
    75000` or `is employed`? We need to use a metric that can tell us that one specific
    split is better than the other. A good split can be defined by the fact it clearly
    split the data into two homogenous groups. One of these metrics is information
    gain, and it is based on entropy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用熵来排序决策树中节点的分裂。以之前的例子为例，我们应该从哪个规则开始？是`Income <= 75000`还是`is employed`？我们需要使用一个度量标准来告诉我们哪个特定的分裂比另一个更好。一个好的分裂可以通过它清楚地将数据分成两个同质的组来定义。一个这样的度量是信息增益，它基于熵。
- en: 'Here is the formula for calculating entropy:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是计算熵的公式：
- en: '![Figure 4.4: Entropy formula'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4：熵公式'
- en: '](img/B16060_04_04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_04.jpg)'
- en: 'Figure 4.4: Entropy formula'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：熵公式
- en: '*p*i represents the probability of one of the possible values of the target
    variable occurring. So, if this column has *n* different unique values, then we
    will have the probability for each of them *([p*1*, p*2*, ..., p*n*])* and apply
    the formula.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*i表示目标变量某一可能值发生的概率。所以，如果这一列有*n*个不同的唯一值，那么我们将为每个值计算概率*([p*1*, p*2*, ..., p*n*])*并应用公式。'
- en: To manually calculate the entropy of a distribution in Python, we can use the
    `np.log2` and `np.dot()` methods from the NumPy library. There is no function
    in `numpy` to automatically calculate entropy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中手动计算分布的熵，我们可以使用NumPy库中的`np.log2`和`np.dot()`方法。在`numpy`中没有自动计算熵的函数。
- en: 'Have a look at the following example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的例子：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The probabilities are given as a NumPy array or a regular list on *line 2*:
    *p*i.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 概率以NumPy数组或常规列表的形式给出，在*第2行*：*p*i。
- en: 'We need to create a vector of the negated values of the distribution in *line
    3*: - *p*i.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个包含*第3行*分布中取反值的向量：- *p*i。
- en: In *line 4*, we must take the base two logarithms of each value in the distribution
    list: logi pi.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4行*，我们必须对分布列表中的每个值取以2为底的对数：logi pi。
- en: 'Finally, we calculate the sum with the scalar product, also known as the dot
    product of the two vectors:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过标量积来计算总和，也称为两个向量的点积：
- en: '![Figure 4.5: Dot product of two vectors'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5：两个向量的点积'
- en: '](img/B16060_04_05.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_05.jpg)'
- en: 'Figure 4.5: Dot product of two vectors'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：两个向量的点积
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You learned about the dot product for the first time in *Chapter 2*, *An Introduction
    to Regression*. The dot product of two vectors is calculated by multiplying the
    *i*th coordinate of the first vector by the *i*th coordinate of the second vector,
    for each *i*. Once we have all the products, we sum the values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你第一次在*第二章*《回归分析导论》中学习了点积。两个向量的点积是通过将第一个向量的第*i*个坐标与第二个向量的第*i*个坐标相乘来计算的，针对每个*i*。一旦我们得到所有的乘积，就将它们求和：
- en: '*np.dot([1, 2, 3], [4, 5, 6])*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*np.dot([1, 2, 3], [4, 5, 6])*'
- en: This results in 1*4 + 2*5 + 3*6 = 32.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到1*4 + 2*5 + 3*6 = 32。
- en: In the next exercise, we will be calculating entropy on a small sample dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将计算一个小样本数据集的熵。
- en: 'Exercise 4.01: Calculating Entropy'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.01：计算熵
- en: 'In this exercise, we will calculate the entropy of the features in the dataset
    in *Figure 4.6*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将计算数据集中特征的熵，如*图4.6*所示：
- en: '![Figure 4.6: Sample dataset to formulate the rules'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.6：用于制定规则的样本数据集'
- en: '](img/B16060_04_06.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_06.jpg)'
- en: 'Figure 4.6: Sample dataset to formulate the rules'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：用于制定规则的样本数据集
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset file can also be found in our GitHub repository:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件也可以在我们的GitHub仓库中找到：
- en: '[https://packt.live/2AQ6Uo9](https://packt.live/2AQ6Uo9).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.live/2AQ6Uo9](https://packt.live/2AQ6Uo9)。'
- en: We will calculate entropy for the `Employed`, `Income`, `LoanType`, and `LoanAmount` features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为`Employed`、`Income`、`LoanType`和`LoanAmount`特征计算熵。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: 'Import the `numpy` package as `np`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`包并命名为`np`：
- en: '[PRE1]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a function called `entropy()` that receives an array of probabilities
    and then returns the calculated entropy value, as shown in the following code snippet:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`entropy()`的函数，它接收一个概率数组并返回计算得到的熵值，如下代码片段所示：
- en: '[PRE2]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will calculate the entropy of the `Employed` column. This column contains
    only two possible values: `true` or `false`. The `true` value appeared four times
    out of seven rows, so its probability is `4/7`. Similarly, the probability of
    the `false` value is `3/7` as it appeared three times in this dataset.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将计算 `Employed` 列的熵。该列仅包含两个可能的值：`true` 或 `false`。在七行数据中，`true` 出现了四次，因此其概率为
    `4/7`。类似地，`false` 的概率为 `3/7`，因为它在数据集中出现了三次。
- en: 'Use the `entropy()` function to calculate the entropy of the `Employed` column
    with the probabilities `4/7` and `3/7`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `entropy()` 函数计算 `Employed` 列的熵，概率分别为 `4/7` 和 `3/7`：
- en: '[PRE3]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should get the following output:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE4]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This value is quite close to zero, which means the groups are quite homogenous.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个值接近零，意味着这些组是相当同质的。
- en: 'Now, use the `entropy()` function to calculate the entropy of the `Income`
    column with its corresponding list of probabilities:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用 `entropy()` 函数计算 `Income` 列的熵及其对应的概率列表：
- en: '[PRE5]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should get the following output:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Compared to the `Employed` column, the entropy for `Income` is higher. This
    means the probabilities of this column are more spread.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 `Employed` 列相比，`Income` 的熵较高。这意味着该列的概率分布更为分散。
- en: 'Use the `entropy` function to calculate the entropy of the `LoanType` column
    with its corresponding list of probabilities:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `entropy` 函数计算 `LoanType` 列的熵及其对应的概率列表：
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should get the following output:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This value is higher than 0, so the probabilities for this column are quite
    spread.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个值大于 0，因此该列的概率分布较为分散。
- en: 'Let''s use the `entropy` function to calculate the entropy of the `LoanAmount`
    column with its corresponding list of probabilities:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `entropy` 函数计算 `LoanAmount` 列的熵及其对应的概率列表：
- en: '[PRE9]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should get the following output:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The entropy for `LoanAmount` is quite high, so its values are quite random.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`LoanAmount` 的熵较高，因此其值相当随机。'
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37T8DVz](https://packt.live/37T8DVz).
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/37T8DVz](https://packt.live/37T8DVz)。
- en: You can also run this example online at [https://packt.live/2By7aI6](https://packt.live/2By7aI6).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址为 [https://packt.live/2By7aI6](https://packt.live/2By7aI6)。你必须执行整个
    Notebook 才能得到预期的结果。
- en: Here, you can see that the `Employed` column has the lowest entropy among the
    four different columns because it has the least variation in terms of values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到 `Employed` 列的熵在四个不同的列中是最低的，因为它的值变化最小。
- en: By completing this exercise, you've learned how to manually calculate the entropy
    for each column of a dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个练习后，你已经学会了如何手动计算数据集每一列的熵。
- en: Information Gain
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息增益
- en: When we partition the data points in a dataset according to the values of an
    attribute, we reduce the entropy of the system.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们根据某个属性的值对数据集中的数据点进行划分时，我们减少了系统的熵。
- en: 'To describe information gain, we can calculate the distribution of the labels.
    Initially, in *Figure 4.1*, we had five creditworthy and two not creditworthy
    individuals in our dataset. The entropy belonging to the initial distribution
    is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述信息增益，我们可以计算标签的分布。在 *图 4.1* 中，我们的数据集包含五个可信和两个不可信的个体。初始分布的熵如下：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s see what happens if we partition the dataset based on whether the loan
    amount is greater than 15,000 or not:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果根据贷款金额是否大于 15,000 来划分数据集，会发生什么：
- en: In group 1, we get one data point belonging to the 15,000 loan amount. This
    data point is not creditworthy.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组 1 中，我们得到一个属于 15,000 贷款金额的数据点。这个数据点是不可信的。
- en: In group 2, we have five creditworthy individuals and one non-creditworthy individual.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组 2 中，我们有五个可信的个体和一个不可信的个体。
- en: The entropy of the labels in each group is as follows.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组中标签的熵如下：
- en: 'For group 1, we have the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组 1，我们有如下情况：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For group 2, we have the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组 2，我们有如下情况：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To calculate the information gain, let''s calculate the weighted average of
    the group entropies:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算信息增益，让我们计算组熵的加权平均值：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, to find the information gain, we need to calculate the difference between
    the original entropy (`H_label`) and the one we just calculated:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了找到信息增益，我们需要计算原始熵（`H_label`）与我们刚刚计算出的熵之间的差异：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By splitting the data with this rule, we gain a little bit of information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个规则拆分数据，我们获得了一些信息。
- en: When creating the decision tree, on each node, our job is to partition the dataset
    using a rule that maximizes the information gain.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建决策树时，在每个节点，我们的任务是使用能够最大化信息增益的规则来划分数据集。
- en: We could also use Gini Impurity instead of entropy-based information gain to
    construct the best rules for splitting decision trees.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用基尼不纯度代替基于熵的信息增益来构建最佳的决策树拆分规则。
- en: Gini Impurity
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基尼不纯度
- en: 'Instead of entropy, there is another widely used metric that can be used to
    measure the randomness of a distribution: Gini Impurity.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了熵之外，还有另一种广泛使用的度量标准可以用来衡量分布的随机性：基尼不纯度。
- en: 'Gini Impurity is defined as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度的定义如下：
- en: '![Figure 4.7: Gini Impurity'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7：基尼不纯度'
- en: '](img/B16060_04_07.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_07.jpg)'
- en: 'Figure 4.7: Gini Impurity'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：基尼不纯度
- en: '*p*i here represents the probability of one of the possible values of the target
    variable occurring.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*i 代表目标变量可能值之一发生的概率。'
- en: Entropy may be a bit slower to calculate because of the usage of the logarithm.
    Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的计算可能稍微慢一些，因为涉及到对数运算。另一方面，基尼不纯度在衡量随机性时的精确度较低。
- en: Note
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Some programmers prefer Gini Impurity because you don''t have to calculate
    with logarithms. Computation-wise, none of the solutions are particularly complex,
    and so both can be used. When it comes to performance, the following study concluded
    that there are often just minimal differences between the two metrics: [https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一些程序员更倾向于使用基尼不纯度，因为不需要进行对数计算。从计算的角度来看，这两种方法都不算特别复杂，因此可以使用其中任意一种。在性能方面，以下研究得出的结论是，这两种度量标准之间的差异通常非常小：[https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)。
- en: With this, we have learned that we can optimize a decision tree by splitting
    the data based on information gain or Gini Impurity. Unfortunately, these metrics
    are only available for discrete values. What if the label is defined on a continuous
    interval such as a price range or salary range?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们已经了解到，可以通过基于信息增益或基尼不纯度来优化决策树。不幸的是，这些度量标准仅适用于离散值。如果标签是在一个连续区间内定义的，比如价格范围或薪资范围，该怎么办呢？
- en: We have to use other metrics. You can technically understand the idea behind
    creating a decision tree based on a continuous label, which was about regression.
    One metric we can reuse in this chapter is the mean squared error. Instead of
    Gini Impurity or information gain, we have to minimize the mean squared error
    to optimize the decision tree. As this is a beginner's course, we will omit this
    metric.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须使用其他度量标准。理论上，你可以理解基于连续标签创建决策树的思路，那就是回归。我们在本章中可以重用的一个度量标准是均方误差。与基尼不纯度或信息增益不同，我们必须最小化均方误差以优化决策树。由于这是一个初学者课程，我们将省略这一度量标准。
- en: In the next section, we will discuss the exit condition for a decision tree.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论决策树的退出条件。
- en: Exit Condition
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 退出条件
- en: We can continuously split the data points according to more and more specific
    rules until each leaf of the decision tree has an entropy of zero. The question
    is whether this end state is desirable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据越来越具体的规则持续地拆分数据点，直到决策树的每个叶子节点的熵为零。问题是，这种最终状态是否是理想的。
- en: Often, this is not what we expect, because we risk overfitting the model. When
    our rules for the model are too specific and too nitpicky, and the sample size
    that the decision was made on is too small, we risk making a false conclusion,
    thus recognizing a pattern in the dataset that simply does not exist in real life.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这不是我们所期望的，因为我们可能会面临过拟合模型的风险。当模型的规则过于具体且过于挑剔，而做出决策的样本量又过小，我们就有可能得出错误的结论，从而在数据集中识别出一个实际上并不存在于现实中的模式。
- en: For instance, if we spin a roulette wheel three times and we get 12, 25, and
    12, this concludes that every odd spin resulting in the value 12 is not a sensible
    strategy. By assuming that every odd spin equals 12, we find a rule that is exclusively
    due to random noise.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们转动轮盘三次，得到的结果分别是12、25和12，这就得出结论：每次奇数次转动结果为12的策略并不明智。假设每次奇数次转动结果都是12，我们就发现了一个完全由随机噪音引起的规则。
- en: Therefore, posing a restriction on the minimum size of the dataset that we can
    still split is an exit condition that works well in practice. For instance, if
    you stop splitting as soon as you have a dataset that's lower than 50, 100, 200,
    or 500 in size, you avoid drawing conclusions on random noise, and so you minimize
    the risk of overfitting the model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对我们仍然可以分割的数据集的最小大小施加限制是一个在实际中效果良好的退出条件。例如，如果在数据集小于50、100、200或500时就停止分割，就可以避免对随机噪音得出结论，从而最大限度地减少过拟合模型的风险。
- en: Another popular exit condition is the maximum restriction on the depth of the
    tree. Once we reach a fixed tree depth, we classify the data points in the leaves.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的退出条件是树的最大深度限制。一旦达到固定的树深度，我们就在叶子节点对数据点进行分类。
- en: Building Decision Tree Classifiers Using scikit-learn
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn构建决策树分类器
- en: We have already learned how to load data from a `.csv` file, how to apply preprocessing
    to data, and how to split data into training and testing datasets. If you need
    to refresh yourself on this knowledge, you can go back to the previous chapters,
    where you can go through this process in the context of regression and classification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何从`.csv`文件加载数据，如何对数据进行预处理，以及如何将数据拆分为训练集和测试集。如果你需要复习这些知识，可以回到前面的章节，在回归和分类的背景下重新学习这一过程。
- en: 'Now, we will assume that a set of training features, training labels, testing
    features, and testing labels have been given as a return value of the `scikit-learn
    train-test-split` call:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们假设已经通过`scikit-learn train-test-split`调用返回了一组训练特征、训练标签、测试特征和测试标签：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code snippet, we used `train_test_split` to split the dataset
    (features and labels) into training and testing sets. The testing set represents
    10% of the observation (`test_size=0.1`). The `random_state` parameter is used
    to get reproducible results.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`train_test_split`将数据集（特征和标签）拆分为训练集和测试集。测试集占观测数据的10%（`test_size=0.1`）。`random_state`参数用于获取可重复的结果。
- en: We will not focus on how we got these data points because this process is exactly
    the same as in the case of regression and classification.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会专注于如何获得这些数据点，因为这个过程与回归和分类的情况完全相同。
- en: 'It''s time to import and use the decision tree classifier of scikit-learn:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候导入并使用scikit-learn的决策树分类器了：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We set one optional parameter in `DecisionTreeClassifier`, that is, `max_depth`,
    to limit the depth of the decision tree.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`DecisionTreeClassifier`中设置了一个可选参数，即`max_depth`，用于限制决策树的深度。
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read the official documentation for the full list of parameters: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以阅读官方文档，获取参数的完整列表：[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)。
- en: 'Some of the more important parameters are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些更重要的参数：
- en: '`criterion`: Gini stands for Gini Impurity, while entropy stands for information
    gain. This will define which measure will be used to assess the quality of a split
    at each node.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`: Gini代表基尼不纯度，entropy代表信息增益。这将定义在每个节点上用于评估分割质量的度量标准。'
- en: '`max_depth`: This is the parameter that defines the maximum depth of the tree.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 这是定义树的最大深度的参数。'
- en: '`min_samples_split`: This is the minimum number of samples needed to split
    an internal node.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`: 这是分割内部节点所需的最小样本数。'
- en: You can also experiment with all the other parameters that were enumerated in
    the documentation. We will omit them in this section.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试文档中列出的所有其他参数。我们将在本节中省略它们。
- en: 'Once the model has been built, we can use the decision tree classifier to predict
    data:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型构建完成，我们就可以使用决策树分类器进行数据预测：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You will build a decision tree classifier in the activity at the end of this
    section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本节末尾的活动中构建一个决策树分类器。
- en: Performance Metrics for Classifiers
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器的性能评估指标
- en: 'After splitting the training and testing data, the decision tree model has
    a `score` method to evaluate how well testing data is classified by the model
    (also known as the accuracy score). We learned how to use the `score` method in
    the previous two chapters:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在拆分训练数据和测试数据之后，决策树模型有一个 `score` 方法，用来评估模型对测试数据分类的效果（也称为准确率）。我们在前两章中学习了如何使用 `score`
    方法：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The return value of the `score` method is a number that's less than or equal
    to 1\. The closer we get to 1, the better our model is.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`score` 方法的返回值是一个小于或等于 1 的数字。我们越接近 1，模型就越好。'
- en: Now, we will learn about another way to evaluate the model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习另一种评估模型的方法。
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Feel free to use this method on the models you constructed in the previous chapter
    as well.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将此方法应用于你在上一章构建的模型。
- en: 'Suppose we have one test feature and one test label:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个测试特征和一个测试标签：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s use the previous creditworthy example and assume we trained a decision
    tree and now have its predictions:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前的信用评分示例，假设我们训练了一个决策树，现在有了它的预测结果：
- en: '![Figure 4.8: Sample dataset to formulate the rules'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8：用于制定规则的示例数据集'
- en: '](img/B16060_04_08.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_08.jpg)'
- en: 'Figure 4.8: Sample dataset to formulate the rules'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：用于制定规则的示例数据集
- en: Our model, in general, made good predictions but had few errors. It incorrectly
    predicted the results for IDs `A`, `D`, and `E`. Its accuracy score will be 4
    / 7 = 0.57.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型一般来说做出了很好的预测，但也有少数错误。它错误地预测了 ID `A`、`D` 和 `E` 的结果。它的准确率得分将是 4 / 7 = 0.57。
- en: 'We will use the following definitions to define some metrics that will help
    you evaluate how good your classifier is:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下定义来定义一些度量标准，帮助你评估分类器的好坏：
- en: '`Creditworthy` column, in our example) and the corresponding predictions both
    have the value `Yes`. In our example, IDs `C`, `F`, and `G` will fall under this category.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Creditworthy` 列（在我们的示例中）和相应的预测值都是 `Yes`。在我们的示例中，ID `C`、`F` 和 `G` 将属于这一类别。'
- en: '`No`. Only ID `B` will be classified as true negative.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`No`。只有 ID `B` 会被分类为真正负类。'
- en: '`Yes` but the true label is actually `No`. This will be the case for ID `A`.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Yes`，但真实标签实际上是 `No`。这种情况适用于 ID `A`。'
- en: '`No` but the true label is actually `Yes`, such as for IDs `D` and `E`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`No`，但真实标签实际上是 `Yes`，例如 ID `D` 和 `E`。'
- en: 'Using the preceding four definitions, we can define four metrics that describe
    how well our model predicts the target variable. The `#( X )` symbol denotes the
    number of values in `X`. Using technical terms, `#( X )` denotes the cardinality
    of `X`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面四个定义，我们可以定义四个度量标准，用来描述我们的模型如何预测目标变量。`#( X )` 符号表示 `X` 中的值的数量。从技术术语上讲，`#(
    X )` 表示 `X` 的基数：
- en: '**Definition (Accuracy)**: *#( True Positives ) + #( True Negatives ) / #(
    Dataset )*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（准确率）**：*#（真正例）+ #(真正负类) / #(数据集)*'
- en: Accuracy is a metric that's used for determining how many times the classifier
    gives us the correct answer. This is the first metric we used to evaluate the
    score of a classifier.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是一个用来衡量分类器给出正确答案次数的指标。这是我们用来评估分类器得分的第一个度量标准。
- en: In our previous example (*Figure 4.8*), the accuracy score will be TP + TN /
    total = (3 + 1) / 7 = 4/7.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的示例中（*图 4.8*），准确率得分将是 TP + TN / 总数 = (3 + 1) / 7 = 4/7。
- en: 'We can use the function provided by scikit-learn to calculate the accuracy
    of a model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 scikit-learn 提供的函数来计算模型的准确率：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Definition (Precision)**: *#TruePositives / (#TruePositives + #FalsePositives)*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（精度）**：*#真正例 / (#真正例 + #假正例)*'
- en: Precision centers around values that our classifier found to be positive. Some
    of these results are true positive, while others are false positive. High precision
    means that the number of false positive results is very low compared to the true
    positive results. This means that a precise classifier rarely makes a mistake
    when finding a positive result.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 精度关注的是分类器认为是正类的值。这些结果中，有些是真正例，而有些是假正例。高精度意味着假正例的数量相对于真正例非常低。这意味着一个精确的分类器在找到正类时很少犯错。
- en: '**Definition (Recall)**: *#True Positives / (#True Positives + #False Negatives)*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（召回率）**：*#真正例 / (#真正例 + #假负例)*'
- en: Recall centers around values that are positive among the test data. Some of
    these results are found by the classifier. These are the true positive values.
    Those positive values that are not found by the classifier are false negatives.
    A classifier with a high recall value finds most of the positive values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率关注的是测试数据中正类值的情况。分类器找到的这些结果是正例（True Positive）。那些分类器未找到的正类值是假阴性（False Negative）。一个召回率高的分类器能够找到大多数正类值。
- en: 'Using our previous example (*Figure 4.8*), we will get the following measures:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前的示例（*图 4.8*），我们将得到以下度量：
- en: Precision = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度 = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8
- en: Recall = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率 = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667
- en: With these two measures, we can easily see where our model is performing better
    or worse. In this example, we know it tends to misclassify false negative cases.
    These measures are more granular than the accuracy score, which only gives you
    an overall score.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这两个度量，我们可以轻松看到我们的模型在哪些地方表现更好或更差。在这个例子中，我们知道它倾向于误分类假阴性案例。这些度量比准确率分数更为细致，准确率分数仅提供一个整体分数。
- en: The F1 score is a metric that combines precision and recall scores. Its value
    ranges between 0 and 1\. If the F1 score equals 1, it means the model is perfectly
    predicting the right outcomes. On the other hand, an F1 score of 0 means the model
    cannot predict the target variable accurately. The advantage of the F1 score is
    that it considers both false positives and false negatives.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是一个结合精确度和召回率的度量。它的值范围在 0 到 1 之间。如果 F1 分数为 1，则表示模型完美地预测了正确的结果。另一方面，F1 分数为
    0 则表示模型无法准确预测目标变量。F1 分数的优点是它考虑了假阳性和假阴性。
- en: 'The formula for calculating the F1 score is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 F1 分数的公式如下：
- en: '![Figure 4.9: Formula to calculate the F1 score'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9：计算 F1 分数的公式'
- en: '](img/B16060_04_09.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_09.jpg)'
- en: 'Figure 4.9: Formula to calculate the F1 score'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：计算 F1 分数的公式
- en: 'As a final note, the scikit-learn package also provides a handy function that
    can show all these measures in one go: `classification_report()`. A classification
    report is useful to check the quality of our predictions:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要指出的是，scikit-learn 包还提供了一个非常实用的函数，可以一次性显示所有这些度量：`classification_report()`。分类报告有助于检查我们预测的质量：
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the next exercise, we will be practicing how to calculate these scores manually.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将练习如何手动计算这些分数。
- en: 'Exercise 4.02: Precision, Recall, and F1 Score Calculation'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.02：精确度、召回率和 F1 分数计算
- en: In this exercise, we will calculate the precision, recall value, and the F1
    score of two different classifiers on a simulated dataset.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将计算两个不同分类器在模拟数据集上的精确度、召回率和 F1 分数。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件。
- en: 'Import the `numpy` package as `np` using the following code:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入`numpy`包，并将其命名为`np`：
- en: '[PRE28]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a `numpy` array called `real_labels` that contains the values [`True,
    True, False, True, True]`. This list will represent the true values of the target
    variable for our simulated dataset. Print its content:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`real_labels`的`numpy`数组，包含值[`True, True, False, True, True`]。该列表表示我们模拟数据集的目标变量的真实值。打印其内容：
- en: '[PRE29]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The expected output will be as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE30]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create a `numpy` array called `model_1_preds` that contains the values `[True,
    False, False, False, False]`. This list will represent the predicted values of
    the first classifier. Print its content:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_preds`的`numpy`数组，包含值`[True, False, False, False, False]`。该列表表示第一个分类器的预测值。打印其内容：
- en: '[PRE31]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The expected output will be as follows:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE32]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create another `numpy` array called `model_2_preds` that contains the values
    `[True, True, True, True, True]`. This list will represent the predicted values
    of the first classifier. Print its content:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个名为`model_2_preds`的`numpy`数组，包含值`[True, True, True, True, True]`。该列表表示第一个分类器的预测值。打印其内容：
- en: '[PRE33]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The expected output will be as follows:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE34]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create a variable called `model_1_tp_cond` that will find the true positives
    for the first model:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_tp_cond`的变量，用来找到第一个模型的真正例：
- en: '[PRE35]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The expected output will be as follows:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE36]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a variable called `model_1_tp` that will get the number of true positives
    for the first model by summing `model_1_tp_cond`:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_tp`的变量，通过求和`model_1_tp_cond`来获取第一个模型的真正例数量：
- en: '[PRE37]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The expected output will be as follows:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE38]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There is only `1` true positive case for the first model.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个模型只有`1`个真实阳性案例。
- en: 'Create a variable called `model_1_fp` that will get the number of false positives
    for the first model:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_fp`的变量，用于获取第一个模型的假阳性数量：
- en: '[PRE39]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The expected output will be as follows:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE40]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: There is no false positive for the first model.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个模型没有假阳性。
- en: 'Create a variable called `model_1_fn` that will get the number of false negatives
    for the first model:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_fn`的变量，用于获取第一个模型的假阴性数量：
- en: '[PRE41]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The expected output will be as follows:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE42]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The first classifier presents `3` false negative cases.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个分类器有`3`个假阴性案例。
- en: 'Create a variable called `model_1_precision` that will calculate the precision
    for the first model:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_precision`的变量，用于计算第一个模型的精度：
- en: '[PRE43]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The expected output will be as follows:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE44]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The first classifier has a precision score of `1`, so it didn't predict any
    false positives.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个分类器的精度得分为`1`，因此它没有预测出任何假阳性。
- en: 'Create a variable called `model_1_recall` that will calculate the recall for
    the first model:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_recall`的变量，用于计算第一个模型的召回率：
- en: '[PRE45]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The expected output will be as follows:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE46]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The recall score for the first model is only `0.25`, so it is predicting quite
    a lot of false negatives.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个模型的召回率仅为`0.25`，因此它预测了相当多的假阴性。
- en: 'Create a variable called `model_1_f1` that will calculate the F1 score for
    the first model:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_f1`的变量，用于计算第一个模型的F1分数：
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The expected output will be as follows:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As expected, the F1 score is quite low for the first model.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如预期所示，第一个模型的F1分数相当低。
- en: 'Create a variable called `model_2_tp` that will get the number of true positives
    for the second model:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_2_tp`的变量，用于获取第二个模型的真实阳性数量：
- en: '[PRE49]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The expected output will be as follows:'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE50]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: There are `4` true positive cases for the second model.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个模型有`4`个真实阳性案例。
- en: 'Create a variable called `model_2_fp` that will get the number of false positives
    for the second model:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_2_fp`的变量，用于获取第二个模型的假阳性数量：
- en: '[PRE51]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The expected output will be as follows:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE52]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: There is only one false positive for the second model.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个模型只有一个假阳性。
- en: 'Create a variable called `model_2_fn` that will get the number of false negatives
    for the second model:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_2_fn`的变量，用于获取第二个模型的假阴性数量：
- en: '[PRE53]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The expected output will be as follows:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE54]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There is no false negative for the second classifier.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个分类器没有假阴性。
- en: 'Create a variable called `model_2_precision` that will calculate precision
    for the second model:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_2_precision`的变量，用于计算第二个模型的精度：
- en: '[PRE55]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The expected output will be as follows:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE56]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The precision score for the second model is quite high: `0.8`. It is not making
    too many mistakes regarding false positives.'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个模型的精度得分相当高：`0.8`。它在假阳性方面并没有犯太多错误。
- en: 'Create a variable called `model_2_recall` that will calculate recall for the
    second model:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_2_recall`的变量，用于计算第二个模型的召回率：
- en: '[PRE57]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The expected output will be as follows:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE58]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In terms of recall, the second classifier did a great job and didn't misclassify
    observations to false negatives.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在召回率方面，第二个分类器表现出色，没有将任何观测值错误分类为假阴性。
- en: 'Create a variable called `model_2_f1` that will calculate the F1 score for
    the second model:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_2_f1`的变量，用于计算第二个模型的F1分数：
- en: '[PRE59]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The expected output will be as follows:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE60]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The F1 score is quite high for the second model.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个模型的F1分数相当高。
- en: Note
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3evqbtu](https://packt.live/3evqbtu).
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3evqbtu](https://packt.live/3evqbtu)。
- en: You can also run this example online at [https://packt.live/2NoxLdo](https://packt.live/2NoxLdo).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2NoxLdo](https://packt.live/2NoxLdo)在线运行此示例。你必须执行整个笔记本以获得预期的结果。
- en: In this exercise, we saw how to manually calculate the precision, recall, and
    F1 score for two different models. The first classifier has excellent precision
    but bad recall, while the second classifier has excellent recall and quite good
    precision.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们展示了如何手动计算两个不同模型的精度、召回率和F1分数。第一个分类器具有优秀的精度，但召回率较差，而第二个分类器具有优秀的召回率和相当不错的精度。
- en: Evaluating the Performance of Classifiers with scikit-learn
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 评估分类器的性能
- en: 'The scikit-learn package provides some functions for automatically calculating
    the precision, recall, and F1 score for you. You will need to import them first:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 包提供了一些函数，用于自动计算精度、召回率和 F1 分数。你需要先导入这些函数：
- en: '[PRE61]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'To get the precision score, you will need to get the predictions from your
    model, as shown in the following code snippet:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得精度分数，你需要从模型中获取预测结果，如下所示的代码片段：
- en: '[PRE62]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Calculating the `recall_score` can be done like so:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 `recall_score` 可以这样进行：
- en: '[PRE63]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Calculating the `f1_score` can be done like so:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 `f1_score` 可以这样进行：
- en: '[PRE64]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In the next section, we will learn how to use another tool, called the confusion
    matrix, to analyze the performance of a classifier.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何使用另一个工具——混淆矩阵，来分析分类器的性能。
- en: The Confusion Matrix
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'Previously, we learned how to use some calculated metrics to assess the performance
    of a classifier. There is another very interesting tool that can help you evaluate
    the performance of a multi-class classification model: the confusion matrix.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们学习了如何使用一些计算指标来评估分类器的性能。还有另一个非常有趣的工具可以帮助你评估多类分类模型的性能：混淆矩阵。
- en: A confusion matrix is a square matrix where the number of rows and columns equals
    the number of distinct label values (or classes). In the columns of the matrix,
    we place each test label value. In the rows of the matrix, we place each predicted
    label value.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个方阵，其中行数和列数等于不同标签值（或类别）的数量。在矩阵的列中，我们放置每个测试标签值。在矩阵的行中，我们放置每个预测标签值。
- en: 'A confusion matrix looks like this:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下所示：
- en: '![Figure 4.10: Sample confusion matrix'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10：示例混淆矩阵'
- en: '](img/B16060_04_10.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_10.jpg)'
- en: 'Figure 4.10: Sample confusion matrix'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：示例混淆矩阵
- en: 'In the preceding example, the first row of the confusion matrix is showing
    us that the model is doing the following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，混淆矩阵的第一行展示了模型的表现：
- en: Correctly predicting class A `88` times
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确预测 A 类 `88` 次
- en: Predicting class A when the true value is B `3` times
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在真实值为 B 时预测 A 类 `3` 次
- en: Predicting class A when the true value is C `2` times
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在真实值为 C 时预测 A 类 `2` 次
- en: We can also see the scenario where the model is making a lot of mistakes when
    it is predicting C while the true value is A (16 times). A confusion matrix is
    a powerful tool to quickly and easily spot which classes your model is performing
    well or badly for.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，当模型预测 C 类时，真实值为 A（16 次）的错误情况。混淆矩阵是一个强大的工具，可以快速轻松地发现模型在某些类别上表现良好或不良。
- en: 'The scikit-learn package provides a function to calculate and display a confusion matrix:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 包提供了一个函数来计算并显示混淆矩阵：
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In the next activity, you will be building a decision tree that will classify
    cars as unacceptable, acceptable, good, and very good for customers.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，你将构建一个决策树模型，用来将汽车分类为不可接受、可接受、良好和非常好四个类别，以便客户使用。
- en: 'Activity 4.01: Car Data Classification'
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.01：汽车数据分类
- en: 'In this activity, you will build a reliable decision tree model that''s capable
    of aiding a company in finding cars that clients are likely to buy. We will be
    assuming that the car rental agency is focusing on building a lasting relationship
    with its clients. Your task is to build a decision tree model that classifies
    cars into one of four categories: unacceptable, acceptable, good, and very good.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，你将构建一个可靠的决策树模型，帮助公司找到客户可能购买的汽车。我们假设租车公司正在专注于与客户建立长期关系。你的任务是构建一个决策树模型，将汽车分类为不可接受、可接受、良好和非常好四个类别。
- en: Note
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/2V95I6h](https://packt.live/2V95I6h).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件也可以在我们的 GitHub 仓库中找到：[https://packt.live/2V95I6h](https://packt.live/2V95I6h)。
- en: 'The dataset for this activity can be accessed here: [https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的数据集可以在这里访问：[https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)。
- en: Citation – *Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository*.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 引用 – *Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository*。
- en: 'It is composed of six different features: `buying`, `maintenance`, `doors`,
    `persons`, `luggage_boot`, and `safety`. The target variable ranks the level of
    acceptability for a given car. It can take four different values: `unacc`, `acc`,
    `good`, and `vgood`.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 它由六个不同的特征组成：`buying`、`maintenance`、`doors`、`persons`、`luggage_boot`和`safety`。目标变量对给定汽车的接受度进行排名。它可以取四个不同的值：`unacc`、`acc`、`good`和`vgood`。
- en: 'The following steps will help you complete this activity:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此活动：
- en: Load the dataset into Python and import the necessary libaries.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集加载到Python中并导入必要的库。
- en: Perform label encoding with `LabelEncoder()` from scikit-learn.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn中的`LabelEncoder()`进行标签编码。
- en: Extract the `label` variable using `pop()` from pandas.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pop()`从pandas中提取`label`变量。
- en: Now, separate the training and testing data with `train_test_spit()` from scikit-learn.
    We will use 10% of the data as test data.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用scikit-learn中的`train_test_split()`将训练数据和测试数据分开。我们将使用10%的数据作为测试数据。
- en: Build the decision tree classifier using `DecisionTreeClassifier()` and its
    methods, `fit()` and `predict()`.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DecisionTreeClassifier()`及其方法`fit()`和`predict()`构建决策树分类器。
- en: Check the score of our model based on the test data with `score()`.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`score()`检查基于测试数据的模型评分。
- en: Create a deeper evaluation of the model using `classification_report()` from
    scikit-learn.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn中的`classification_report()`对模型进行更深入的评估。
- en: 'Expected output:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出：
- en: '![Figure 4.11: Output showing the expected classification report'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11：显示预期分类报告的输出'
- en: '](img/B16060_04_11.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_11.jpg)'
- en: 'Figure 4.11: Output showing the expected classification report'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：显示预期分类报告的输出
- en: Note
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 353.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第353页找到。
- en: In the next section we will be looking at Random Forest Classifier.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将研究随机森林分类器。
- en: Random Forest Classifier
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: 'If you think about the name random forest classifier, it can be explained as
    follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一下随机森林分类器这个名称，可以这样解释：
- en: A forest consists of multiple trees.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一片森林由多棵树组成。
- en: These trees can be used for classification.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些树可以用于分类。
- en: Since the only tree we have used so far for classification is a decision tree,
    it makes sense that the random forest is a forest of decision trees.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于到目前为止我们唯一使用的分类树是决策树，因此可以理解随机森林是一片决策树的森林。
- en: The random nature of the trees means that our decision trees are constructed
    in a randomized manner.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的随机性意味着我们的决策树是以随机化的方式构建的。
- en: Therefore, we will base our decision tree construction on information gain or
    Gini Impurity.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将根据信息增益或基尼不纯度来构建决策树。
- en: Once you understand these basic concepts, you essentially know what a random
    forest classifier is all about. The more trees you have in the forest, the more
    accurate prediction is going to be. When performing prediction, each tree performs
    classification. We collect the results, and the class that gets the most votes
    wins.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了这些基本概念，你就基本上知道随机森林分类器的内容了。森林中的树木越多，预测的准确性就越高。在执行预测时，每棵树都执行分类。我们收集结果，获得最多投票的类别获胜。
- en: Random forests can be used for regression as well as for classification. When
    using random forests for regression, instead of counting the most votes for a
    class, we take the average of the arithmetic mean (average) of the prediction
    results and return it. Random forests are not as ideal for regression as they
    are for classification, though, because the models that are used to predict values
    are often out of control, and often return a wide range of values. The average
    of these values is often not too meaningful. Managing the noise in a regression
    exercise is harder than in classification.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以用于回归和分类。在使用随机森林进行回归时，我们不再统计某个类别的最多投票数，而是取预测结果的算术平均值（平均数）并返回。尽管随机森林在分类中的表现非常理想，但在回归中的效果不如分类，因为用于预测值的模型通常是失控的，且经常返回一个范围较广的值。这些值的平均值往往意义不大。管理回归中的噪声比分类更为困难。
- en: Random forests are often better than one simple decision tree because they provide
    redundancy. They treat outlier values better and have a lower probability of overfitting
    the model. Decision trees seem to behave great as long as you are using them on
    the data that was used when creating the model. Once you use them to predict new
    data, random forests lose their edge. Random forests are widely used for classification
    problems, whether it be customer segmentation for banks or e-commerce, classifying
    images, or medicine. If you own an Xbox with Kinect, your Kinect device contains
    a random forest classifier to detect your body.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通常比简单的决策树表现更好，因为它们提供了冗余性。它们能更好地处理异常值，并且具有较低的过拟合模型的概率。决策树在使用训练数据时表现得很好，但一旦用于预测新数据，随机森林则失去其优势。随机森林广泛应用于分类问题，无论是银行或电子商务的客户细分、图像分类，还是医学领域。如果你拥有一台带有Kinect的Xbox，你的Kinect设备就包含了一个随机森林分类器，用于检测你的身体。
- en: Random forest is an ensemble algorithm. The idea behind ensemble learning is
    that we take an aggregated view of a decision of multiple agents that potentially
    have different weaknesses. Due to the aggregated vote, these weaknesses cancel
    out, and the majority vote likely represents the correct result.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成算法。集成学习的思想是，我们通过多个可能有不同弱点的代理的决策来获得一个综合视角。由于集体投票，这些弱点会被抵消，大多数投票结果很可能代表正确的结果。
- en: Random Forest Classification Using scikit-learn
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行随机森林分类
- en: 'As you may have guessed, the scikit-learn package provides an implementation
    of the `RandomForest` classifier with the `RandomForestClassifier` class. This
    class provides the exact same methods as all the scikit-learn models you have
    seen so far – you need to instantiate a model, then fit it with the training set
    with `.fit()`, and finally make predictions with `.predict()`:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，scikit-learn包提供了`RandomForest`分类器的实现，使用的是`RandomForestClassifier`类。这个类提供了与迄今为止所有scikit-learn模型完全相同的方法——你需要实例化一个模型，然后使用`.fit()`方法对其进行训练，最后使用`.predict()`方法进行预测：
- en: '[PRE66]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In the next section, we will be looking at the parameterization of the random
    forest classifier.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看随机森林分类器的参数化。
- en: The Parameterization of the Random Forest Classifier
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林分类器的参数化
- en: 'We will be considering a subset of the possible parameters, based on what we
    already know, which is based on the description of constructing random forests:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于已有的知识，考虑可能的参数子集，这些知识来源于构建随机森林的描述：
- en: '`n_estimators`: The number of trees in the random forest. The default value
    is 10.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：随机森林中的树木数量。默认值为10。'
- en: '`criterion`: Use Gini or entropy to determine whether you use Gini Impurity
    or information gain using the entropy in each tree. This will be used to find
    the best split at each node.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`：使用Gini或熵来确定在每棵树中是使用Gini不纯度还是信息增益。它将用于找到每个节点的最佳分裂。'
- en: '`max_features`: The maximum number of features considered in any tree of the
    forest. Possible values include an integer. You can also add some strings such
    as `sqrt` for the square root of the number of features.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：每棵树考虑的最大特征数。可能的值包括整数。你还可以添加一些字符串，如`sqrt`，表示特征数量的平方根。'
- en: '`max_depth`: The maximum depth of each tree.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：每棵树的最大深度。'
- en: '`min_samples_split`: The minimum number of samples in the dataset in a given
    node to perform a split. This may also reduce the tree''s size.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：在给定节点中，数据集中的最小样本数，以执行分裂。这也可以减少树的大小。'
- en: '`bootstrap`: A Boolean that indicates whether to use bootstrapping on data
    points when constructing trees.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`：一个布尔值，指示在构建树时是否对数据点使用自助法。'
- en: Feature Importance
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: A random forest classifier gives you information on how important each feature
    in the data classification process is. Remember, we used a lot of randomly constructed
    decision trees to classify data points. We can measure how accurately these data
    points behave, and we can also see which features are vital when it comes to decision-making.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器为你提供了每个特征在数据分类过程中有多重要的信息。记住，我们使用了许多随机构建的决策树来对数据点进行分类。我们可以衡量这些数据点的表现准确性，同时也能看到在决策过程中哪些特征是至关重要的。
- en: 'We can retrieve the array of feature importance scores with the following query:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下查询来检索特征重要性得分的数组：
- en: '[PRE67]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this six-feature classifier, the fourth and sixth features are clearly a
    lot more important than any other features. The third feature has a very low importance
    score.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个包含六个特征的分类器中，第四个和第六个特征显然比其他特征更重要。第三个特征的重要性得分非常低。
- en: Feature importance scores come in handy when we have a lot of features and we
    want to reduce the feature size to avoid the classifier getting lost in the details.
    When we have a lot of features, we risk overfitting the model. Therefore, reducing
    the number of features by dropping the least significant ones is often helpful.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性分数在我们有很多特征时特别有用，尤其是当我们想要减少特征数量以避免分类器在细节中迷失时。当我们有大量特征时，我们容易导致模型过拟合。因此，通过剔除最不重要的特征来减少特征数量通常是有帮助的。
- en: Cross-Validation
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Earlier, we learned how to use different metrics to assess the performance of
    a classifier, such as the accuracy, precision, recall, or the F1 score on a training
    and testing set. The objective is to have a high score on both sets that are very
    close to each other. In that case, your model is performant and not prone to overfitting.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们学习了如何使用不同的度量标准来评估分类器的性能，例如在训练集和测试集上的准确率、精确率、召回率或F1分数。目标是在两个数据集上都获得较高的分数，并且这两个分数要非常接近。如果是这样，你的模型就能表现良好，并且不容易过拟合。
- en: The test set is used as a proxy to evaluate whether your model can generalize
    well to unseen data or whether it learns patterns that are only relevant to the
    training set.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集用于作为代理，评估你的模型是否能够很好地泛化到未见过的数据，或者是否仅仅学习了对训练集有意义的模式。
- en: But in the case of having quite a few hyperparameters to tune (such as for `RandomForest`),
    you will have to train a lot of different models and test them on your testing
    set. This kind of defeats the purpose of the testing set. Think of the testing
    set as the final exam that will define whether you pass a subject or not. You
    will not be allowed to pass and repass it over and over.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在需要调整多个超参数的情况下（例如对于`RandomForest`），你需要训练大量不同的模型并在测试集上测试它们。这种做法实际上削弱了测试集的作用。把测试集当作最终考试，决定你是否能通过某一科目。你不会被允许反复通过或重考。
- en: One solution for avoiding using the testing set too much is creating a validation
    set. You will train your model on the training set and use the validation set
    to assess its score according to different combinations of hyperparameters. Once
    you find your best model, you will use the testing set to make sure it doesn't
    overfit too much. This is, in general, the suggested approach for any data science
    project.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过度使用测试集的一个解决方案是创建验证集。你将在训练集上训练模型，并使用验证集根据不同的超参数组合来评估模型的得分。一旦找到最佳模型，你将使用测试集确保它不会过度拟合。这通常是任何数据科学项目推荐的做法。
- en: The drawback of this approach is that you are reducing the number of observations
    for the training set. If you have a dataset with millions of rows, it is not a
    problem. But for a small dataset, this can be problematic. This is where cross-validation
    comes in.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，你减少了用于训练集的观测数据。如果你的数据集包含数百万行，这不是问题。但对于一个小型数据集，这可能会造成问题。这就是交叉验证派上用场的地方。
- en: The following *Figure 4.12*, shows that this is a technique where you create
    multiple splits of the training data. For each split, the training data is separated
    into folds (five, in this example) and one of the folds will be used as the validation
    set while the others will be used for training.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图 4.12*显示了这种技术，在这种技术中，你会创建多个训练数据的分割。对于每个分割，训练数据被分为若干个折叠（本例中为五个），其中一个折叠将作为验证集，其余折叠用于训练。
- en: 'For instance, for the top split, fold 5 will be used for validation and the
    four other folds (1 to 4) will be used to train the model. You will follow the
    same process for each split. After going through each split, you will have used
    the entire training data and the final performance score will be the average of
    all the models that were trained on each split:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在上面的分割中，第五折将用于验证，其余四个折叠（1到4）将用于训练模型。你将对每个分割执行相同的过程。经过每个分割后，你将使用整个训练数据，最终的性能得分将是每个分割上训练的所有模型的平均值：
- en: '![Figure 4.12: Cross-validation example'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12：交叉验证示例'
- en: '](img/B16060_04_12.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_12.jpg)'
- en: 'Figure 4.12: Cross-validation example'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：交叉验证示例
- en: 'With scikit-learn, you can easily perform cross-validation, as shown in the
    following code snippet:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，你可以轻松地执行交叉验证，以下是一个代码示例：
- en: '[PRE68]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '`cross_val_score` takes two parameters:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_val_score`函数接受两个参数：'
- en: '`cv`: Specifies the number of splits.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cv`：指定拆分的次数。'
- en: '`scoring`: Defines which performance metrics you want to use. You can find
    the list of possible values here: [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scoring`：定义你希望使用的性能指标。你可以在这里找到可能的值列表：[https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)。'
- en: In the next section, we will look at a specific variant of `RandomForest`, called
    `extratrees`.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将探讨`RandomForest`的一个特定变体，称为`extratrees`。
- en: Extremely Randomized Trees
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极端随机森林
- en: Extremely randomized trees increase the randomization inside random forests
    by randomizing the splitting rules on top of the already randomized factors in
    random forests.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树通过在已经随机化的随机森林因素基础上随机化分裂规则，从而增加了随机森林内部的随机化程度。
- en: 'Parameterization is like the random forest classifier. You can see the full
    list of parameters here: [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化方式类似于随机森林分类器。你可以在这里查看完整的参数列表：[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)。
- en: 'The Python implementation is as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Python实现如下：
- en: '[PRE69]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: In the following activity, we will be optimizing the classifier built in *Activity
    4.01*, *Car Data Classification*.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的活动中，我们将优化在*活动 4.01*中构建的分类器，*汽车数据分类*。
- en: 'Activity 4.02: Random Forest Classification for Your Car Rental Company'
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.02：为你的汽车租赁公司进行随机森林分类
- en: In this activity, you will optimize your classifier so that you satisfy your
    clients more when selecting future cars for your car fleet. We will be performing
    random forest and extreme random forest classification on the car dealership dataset
    that you worked on in the previous activity of this chapter.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，你将优化分类器，以便在为你的车队选择未来的汽车时，更好地满足客户需求。我们将对你在本章前一个活动中使用的汽车经销商数据集进行随机森林和极端随机森林分类。
- en: 'The following steps will help you complete this activity:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此活动：
- en: Follow *Steps 1 - 4* of the previous *Activity 4.01*, *Car Data Classification*.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照之前*活动 4.01*的*步骤 1 - 4*，*汽车数据分类*。
- en: Create a random forest using `RandomForestClassifier`.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`RandomForestClassifier`创建一个随机森林。
- en: Train the models using `.fit()`.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.fit()`训练模型。
- en: Import the `confusion_matrix` function to find the quality of the `RandomForest`.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`confusion_matrix`函数来评估`RandomForest`的质量。
- en: Print the classification report using `classification_report()`.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`classification_report()`打印分类报告。
- en: Print the feature importance with `.feature_importance_`.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.feature_importance_`打印特征重要性。
- en: Repeat *Steps 2 to 6* with an `extratrees` model.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`extratrees`模型重复*步骤 2 到 6*。
- en: 'Expected output:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出：
- en: '[PRE70]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Note
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 357.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第357页找到。
- en: By completing this activity, you've learned how to fit the `RandomForest` and
    `extratrees` models and analyze their classification report and feature importance.
    Now, you can try different hyperparameters on your own and see if you can improve
    their results.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本活动后，你已经学会了如何拟合`RandomForest`和`extratrees`模型，并分析它们的分类报告和特征重要性。现在，你可以尝试不同的超参数，看看能否提高结果。
- en: Summary
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to use decision trees for prediction. Using
    ensemble learning techniques, we created complex reinforcement learning models
    to predict the class of an arbitrary data point.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用决策树进行预测。通过集成学习技术，我们创建了复杂的强化学习模型来预测任意数据点的类别。
- en: Decision trees proved to be very accurate on the surface, but they were prone
    to overfitting the model. Random forests and extremely randomized trees reduce
    overfitting by introducing some random elements and a voting algorithm, where
    the majority wins.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在表面上表现得非常准确，但它们容易导致模型过拟合。随机森林和极端随机树通过引入一些随机元素和投票算法来减少过拟合，其中多数投票获胜。
- en: Beyond decision trees, random forests, and extremely randomized trees, we also
    learned about new methods for evaluating the utility of a model. After using the
    well-known accuracy score, we started using the precision, recall, and F1 score
    metrics to evaluate how well our classifier works. All of these values were derived
    from the confusion matrix.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 除了决策树、随机森林和极端随机树之外，我们还了解了评估模型效用的新方法。在使用了众所周知的准确率评分后，我们开始使用精确率、召回率和F1评分指标来评估我们的分类器的表现。所有这些数值都是从混淆矩阵中得出的。
- en: In the next chapter, we will describe the clustering problem and compare and
    contrast two clustering algorithms.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将描述聚类问题，并比较和对比两种聚类算法。
