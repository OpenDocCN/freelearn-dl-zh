- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Transformers and Their Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换器和它们的用途
- en: In this chapter, we will learn about transformers and how to apply them to perform
    various NLP tasks. Typical tasks in the NLP domain involve loading and processing
    data so that it can be used downstream seamlessly. Once the data is read, another
    task is that of transforming the data into a form that the various models can
    use. Once the data is transformed into the requisite format, we use it to perform
    the actual tasks, such as classification, text generation, and language translation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解转换器及其如何应用于执行各种NLP任务。NLP领域的典型任务涉及加载和处理数据，以便它可以无缝地用于下游。一旦数据被读取，另一个任务是转换数据，使其以各种模型可以使用的形式。一旦数据被转换成所需的格式，我们就用它来执行实际的任务，如分类、文本生成和语言翻译。
- en: 'Here is a list of the recipes in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是本章中的菜谱列表：
- en: Loading a dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Tokenizing the text in your dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据集中的文本进行分词
- en: Using the tokenized text to perform classification with Transformer models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分词后的文本通过转换器模型进行分类
- en: Using different Transformer models based on different requirements
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据不同的需求使用不同的转换器模型
- en: Generating text by taking a cue from an initial starting sentence
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过参考初始起始句子生成文本
- en: Translating text between different languages using pre-trained Transformer models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的转换器模型在不同语言之间翻译文本
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is in the `Chapter08` folder in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节的代码位于书籍GitHub仓库的`Chapter08`文件夹中（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08)）。
- en: As in previous chapters, the packages required for the chapter are part of the
    `poetry` environment. Alternatively, you can install all the packages using the
    `requirements.txt` file.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，本章所需的包是`poetry`环境的一部分。或者，您也可以使用`requirements.txt`文件安装所有包。
- en: Loading a dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: In this recipe, we will learn how to load a public dataset and work with it.
    We will use the `RottenTomatoes` dataset for this recipe as an example. This dataset
    contains ratings and reviews for movies. Please refer to the link at [https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset](https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset)
    for more information about the dataset
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何加载公共数据集并与之交互。我们将使用`RottenTomatoes`数据集作为本菜谱的示例。这个数据集包含了电影的评分和评论。请参考以下链接获取更多关于数据集的信息：[https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset](https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset)
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As part of this chapter, we will use the libraries from the `HuggingFace` site
    ([huggingface.co](http://huggingface.co)). For this recipe, we will use the dataset
    package. You can use the `8.1_Transformers_dataset.ipynb` notebook from the code
    site if you need to work from an existing notebook.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的一部分，我们将使用来自`HuggingFace`网站（[huggingface.co](http://huggingface.co)）的库。对于这个菜谱，我们将使用数据集包。如果您需要从一个现有的笔记本开始工作，可以使用代码网站上的`8.1_Transformers_dataset.ipynb`笔记本。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this recipe, you will load the `RottenTomatoes` dataset from the `HuggingFace`
    site using the dataset package. This package will download the dataset for you
    if it does not exist. For any subsequent runs, it will use the downloaded dataset
    from the cache if it was downloaded previously.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，您将使用数据集包从`HuggingFace`网站加载`RottenTomatoes`数据集。如果数据集不存在，该包会为您下载它。对于任何后续运行，如果之前已下载，它将使用缓存中的下载数据集。
- en: 'The recipe does the following things:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本菜谱执行以下操作：
- en: Reads in the **RottenTomatoes** dataset
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取**RottenTomatoes**数据集
- en: Describes the features of the dataset
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述数据集的特征
- en: Loads the data from the training split of the dataset
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集的训练分割中加载数据
- en: Samples a few sentences from the dataset and prints them
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中抽取几个句子并打印出来
- en: 'The steps for the recipe are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱的步骤如下：
- en: 'Do the necessary imports to import the necessary types and functions from the
    datasets package:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入，从数据集包导入必要的类型和函数：
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load **"rotten tomatoes"** via the **load_dataset** function and print the
    internal dataset splits. This dataset contains the train, validation, and test
    splits:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过**load_dataset**函数加载**"rotten tomatoes"**并打印内部数据集分割。这个数据集包含训练、验证和测试分割：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the preceding command would be as follows:'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load the dataset and print the attributes of the train split. The **training_data.description**
    describes the dataset details and the **training_data.features** describes the
    features of the dataset. In the output, we can see that the **training_data**
    split contains the features **text**, which is of the type string, and **label**,
    which is of type categorical, with the values **neg** and **pos**:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并打印训练分割的属性。**training_data.description**描述了数据集的详细信息，而**training_data.features**描述了数据集的特征。在输出中，我们可以看到**training_data**分割包含特征**text**，它是字符串类型，以及**label**，它是分类类型，具有**neg**和**pos**的值：
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the command is as follows:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE4]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have loaded the dataset, we will print the first five sentences
    from it. This is just to confirm that we are indeed able to read from the dataset:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经加载了数据集，我们将打印其中的前五个句子。这只是为了确认我们确实能够从数据集中读取：
- en: '[PRE5]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the command is as follows:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 命令的输出如下：
- en: '[PRE6]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tokenizing the text in your dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在您的数据集中对文本进行分词
- en: The components contained within the transformer do not have any intrinsic knowledge
    of the words that it processes. Instead, the tokenizer only uses the token identifiers
    for the words that it processes. In this recipe, we will learn how to transform
    the text in your dataset into a representation that can be used by the models
    for downstream tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器内部包含的组件对其处理的单词没有任何内在知识。相反，分词器只使用它处理的单词的标记标识符。在这个食谱中，我们将学习如何将您的数据集中的文本转换为可以由模型用于下游任务的表示。
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As part of this recipe, we will use the `AutoTokenizer` module from the transformers
    package. You can use the `8.2_Basic_Tokenization.ipynb` notebook from the code
    site if you need to work from an existing notebook.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个食谱的一部分，我们将使用来自transformers包的`AutoTokenizer`模块。如果您需要从一个现有的笔记本中工作，可以使用代码网站的`8.2_Basic_Tokenization.ipynb`笔记本。
- en: How to do it...
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: In this recipe, you will continue from the previous example of using the `RottenTomatoes`
    dataset and sampling a few sentences from it. We will then encode the sampled
    sentences into tokens and their respective representations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，您将继续使用之前的`RottenTomatoes`数据集示例，并从中采样几个句子。然后我们将将这些采样句子编码成标记及其相应的表示。
- en: 'The recipe does the following things:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱做了以下事情：
- en: Loads a few sentences into memory
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一些句子加载到内存中
- en: Instantiates a tokenizer and tokenizes the sentences
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化一个分词器并对句子进行分词
- en: Converts the token IDs generated from the previous step back into the tokens
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将前一步生成的标记ID转换回标记
- en: 'The steps for the recipe are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 食谱的步骤如下：
- en: 'Do the necessary imports to import the necessary **AutoTokenizer** module from
    the **transformers** library:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入以导入来自**transformers**库的必要的**AutoTokenizer**模块：
- en: '[PRE7]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We initialize a sentence array consisting of three sentences that we will use
    for this example. These sentences are of different lengths and have a good combination
    of the same and different words. This will allow us to understand how the tokenized
    representation varies for each of them:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化一个包含三个句子的句子数组，我们将使用这个例子。这些句子的长度不同，并且有很好的相同和不同单词的组合。这将使我们能够了解分词表示如何因每个句子而异：
- en: '[PRE8]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Instantiate a tokenizer of the **bert-base-cased** type. This tokenizer is
    case-sensitive. This means that the words star and STAR will have different tokenized
    representations:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个**bert-base-cased**类型的分词器。这个分词器是区分大小写的。这意味着单词star和STAR将会有不同的分词表示：
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we tokenize all the sentences in the **sentences** array. We
    call the tokenizer constructor and pass it the **sentences** array as an argument
    followed by printing the **tokenized_output** instance returned by the constructor
    function. This object is a dictionary of three items:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将**sentences**数组中的所有句子进行分词。我们调用分词器构造函数，并将**sentences**数组作为参数传递，然后打印构造函数返回的**tokenized_output**实例。此对象是一个包含三项的字典：
- en: '**input_ids**: These are the numerical token identifiers that are assigned
    to each token.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input_ids**：这些是分配给每个标记的数值标记标识符。'
- en: '**token_type_ids**: These IDs define the type of tokens that are contained
    in the sentences.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**token_type_ids**：这些ID定义了句子中包含的标记的类型。'
- en: '**attention_mask**: These define the attention values for each token in the
    input. This mask determines what tokens are paid attention to when downstream
    tasks are performed. These values are floats and can vary from 0 (no attention)
    to 1 (full attention).'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**attention_mask**：这些定义了输入中每个标记的注意力值。这个掩码决定了在执行下游任务时哪些标记会被关注。这些值是浮点数，可以从0（无注意力）到1（完全注意力）变化。'
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this step, we take the input IDs of the first sentence and convert them
    back into tokens:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将第一句话的输入ID转换回标记：
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Converting them into tokens returns the following output:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将它们转换为标记返回以下输出：
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In addition to the original tokens, the tokenizer adds the `[CLS]` and `[SEP]`.
    These tokens were added for the training tasks that were performed to train BERT.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了原始标记外，分词器还添加了`[CLS]`和`[SEP]`。这些标记是为了训练BERT所执行的训练任务而添加的。
- en: Now that we have learned about the internal representation of the text used
    by the transformer internally, let us learn how we can classify a piece of text
    into different categories.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经了解了transformer内部使用的文本的内部表示，让我们学习如何将一段文本分类到不同的类别中。
- en: Classifying text
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对文本进行分类
- en: In this recipe, we will use the `RottenTomatoes` dataset and classify the review
    texts for sentiment. We will classify the test split of the dataset and evaluate
    the results of the classifier against the true labels in the test split of the
    dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`RottenTomatoes`数据集并对评论文本进行情感分类。我们将对数据集的测试分割进行分类，并评估分类器对测试分割中真实标签的结果。
- en: Getting ready
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.3_Classification_And_Evaluation.ipynb` notebook from
    the code site if you need to work from an existing notebook.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个菜谱的一部分，我们将使用来自transformers包的pipeline模块。如果你需要从一个现有的笔记本中工作，可以使用代码网站上的`8.3_Classification_And_Evaluation.ipynb`笔记本。
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: In this recipe, you will continue from the previous example using the `RottenTomatoes`
    dataset and sample a few sentences from it. We will then classify a small subset
    of five sentences for sentiment classification and demonstrate the results on
    this smaller subset. We will then perform inference on the whole test split of
    the dataset and evaluate the results of the classification.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，你将使用`RottenTomatoes`数据集并从中抽取几个句子。然后我们将对五个句子的一个小子集进行情感分类，并在这个较小的子集上展示结果。然后我们将对数据集的整个测试分割进行推理并评估分类结果。
- en: 'The recipe does the following things:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱执行以下操作：
- en: Loads the **RottenTomatoes** dataset and prints the first five sentences from
    it
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载**RottenTomatoes**数据集并打印其中的前五句话
- en: Instantiates a pipeline with a pre-trained Roberta model that has been trained
    on the same dataset for sentiment analysis
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化一个使用在相同数据集上训练的预训练Roberta模型进行情感分析的管道
- en: Performs inference (or sentiment prediction) on the whole test split of the
    dataset using the pipeline
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道在整个数据集的测试分割上执行推理（或情感预测）
- en: Evaluates the results of the inference
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估推理结果
- en: 'The steps for the recipe are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱的步骤如下：
- en: 'Do the necessary imports to import the required packages and modules:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入以导入所需的包和模块：
- en: '[PRE13]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this step, we probe for the presence of a **Compute Unified Device Architecture**
    (**CUDA**) compatible device (or **Graphics Processing Unit** (**GPU**)) present
    in the system. If such a device is present, our model will be loaded on it. This
    accelerates the training and inference performance of a model if it is supported.
    However, if such a device is not present, the **Central Processing Unit** (**CPU**)
    will be used. We also load the **RottenTomatoes** dataset and select the first
    five sentences from it. This is to ensure we are indeed able to read the data
    present in the dataset:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们检查系统中是否存在兼容**Compute Unified Device Architecture**（**CUDA**）的设备（或**Graphics
    Processing Unit**（**GPU**））。如果存在这样的设备，我们的模型将加载到它上面。如果支持，这将加速模型的训练和推理性能。然而，如果不存在这样的设备，将使用**Central
    Processing Unit**（**CPU**）。我们还加载了**RottenTomatoes**数据集并从中选择了前五句话。这是为了确保我们确实能够读取数据集中存在的数据：
- en: '[PRE14]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Initialize the pipeline for sentiment analysis via a pipeline. A pipeline is
    an abstraction that allows us to easily use models or inference tasks without
    having to write the code to piece them together. We load the **roberta-base-rotten-tomatoes**
    model from **textattack**, which has been trained on this dataset. In the following
    segment, we use the pipeline for the sentiment analysis task and set a specific
    model to be used for this task:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过管道初始化用于情感分析的管道。管道是一种抽象，它允许我们轻松使用模型或推理任务，而无需编写将它们拼接在一起的代码。我们从**textattack**加载了**roberta-base-rotten-tomatoes**模型，该模型已经在这个数据集上进行了训练。在接下来的段落中，我们使用管道进行情感分析任务，并设置用于此任务的具体模型：
- en: '[PRE15]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In this step, we generate the predictions for the small subset of sentences
    that we selected in step 2\. Using the pipeline object to generate predictions
    is as easy as just passing it a series of sentences. If you are running this example
    on a machine without a compatible CUDA device, this step might take a little time:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们为在步骤2中选择的句子小集合生成预测。使用管道对象生成预测就像传递一系列句子一样简单。如果你在没有兼容CUDA设备的机器上运行此示例，这一步可能需要一点时间：
- en: '[PRE16]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this step, we iterate through our sentences and check the predictions for
    our sentences. We print the actual and generated predictions along with the sentence
    text for the five sentences. The actual labels are read from the dataset, whereas
    the predictions are generated via the pipeline object:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们遍历我们的句子并检查句子的预测结果。我们打印出实际和生成的预测结果，以及五个句子的文本。实际标签是从数据集中读取的，而预测是通过管道对象生成的：
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have validated the pipeline and its results, let’s generate the
    inference for the whole test set and generate the evaluation measures of this
    particular model. Load the complete test split for the **RottenTomatoes** dataset:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然我们已经验证了管道及其结果，让我们为整个测试集生成推理，并生成这个特定模型的评估指标。加载**RottenTomatoes**数据集的完整测试分割：
- en: '[PRE18]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this step, we initialize an evaluator object that can be used to perform
    the inference along with evaluating the results of the classification. It can
    also be used to present an easy-to-read summary of the evaluation results:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一个评估器对象，它可以用来执行推理并评估分类的结果。它还可以用来展示易于阅读的评估结果摘要：
- en: '[PRE19]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In this step, we call the **compute** method on the **evaluator** instance.
    This triggers the inference and the evaluation using the same pipeline instance
    that we initialized in step 4\. It returns the evaluation metrics of **accuracy**,
    **precision**, **recall**, and **f1**, along with some performance metrics related
    to inference:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们在**评估器**实例上调用**compute**方法。这触发了使用我们在步骤4中初始化的相同管道实例进行的推理和评估。它返回**准确度**、**精确度**、**召回率**和**f1**的评估指标，以及一些与推理相关的性能指标：
- en: '[PRE20]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this step, we print the results of the evaluation. Of note are the **precision**,
    **recall**, and **f1** values. An **f1** of **0.88**, observed in this case, is
    an indicator of the very good efficacy of the classifier, though it could always
    be improved further:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们打印出评估的结果。值得注意的是**精确度**、**召回率**和**f1**值。在这个案例中观察到的**f1**值为**0.88**，这是分类器非常有效率的指标，尽管它总是可以进一步改进：
- en: '[PRE21]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this recipe, we used a pre-trained classifier to classify the data on a dataset.
    The dataset and model were both for sentiment analysis. There are cases where
    we can use classifiers that are trained on a different class of data but can still
    be used as is. This saves us from having to train a classifier of our own and
    repurpose a model that already exists. We will learn about this use case in the
    next recipe.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用预训练的分类器对一个数据集上的数据进行分类。数据集和模型都是用于情感分析的。有些情况下，我们可以使用在另一类数据上训练的分类器，但仍然可以直接使用。这使我们免去了训练自己的分类器并重新利用现有模型的麻烦。我们将在下一个菜谱中了解这个用例。
- en: Using a zero-shot classifier
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用零样本分类器
- en: In this recipe, we will classify a sentence using a zero-shot classifier. There
    are instances where we do not have the luxury of training a classifier from scratch
    or using a model that has been trained as per the labels of our data. **Zero-shot
    classification** can be used in such scenarios for any team to get up and running
    quickly. The zero in the terminology means that the classifier has not seen any
    data (zero samples precisely) from the target dataset that will be used for inference.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用零样本分类器对句子进行分类。有些情况下，我们没有从头开始训练分类器或使用按照我们数据标签训练的模型的奢侈。**零样本分类**可以在这种场景下帮助任何团队快速启动。术语中的“零”意味着分类器没有看到目标数据集用于推理的任何数据（精确到零样本）。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.4_Zero_shot_classification.ipynb` notebook from the
    code site if you need to work from an existing notebook.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个菜谱的一部分，我们将使用来自transformers包的管道模块。如果您需要从一个现有的笔记本中工作，可以使用代码网站上的`8.4_Zero_shot_classification.ipynb`笔记本。
- en: How to do it...
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this recipe, we will use a couple of sentences and classify them. We will
    use our own set of labels for these sentences. We will use the `facebook/bart-large-mnli`
    model for this recipe. This model is suitable for the task of zero-shot classification.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用几个句子并将它们进行分类。我们将为这些句子使用我们自己的标签集。我们将使用`facebook/bart-large-mnli`模型来完成这个菜谱。这个模型适合零样本分类的任务。
- en: 'The recipe does the following things:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱执行以下操作：
- en: Initializes a pipeline based on a zero-shot classification model
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于零样本分类模型初始化一个管道
- en: Uses the pipeline to classify a sentence into a custom set of user-defined labels
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道将句子分类到用户自定义的标签集中
- en: Prints the results of the classification with the classes and their associated
    probabilities
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印分类的结果，包括类别及其相关的概率
- en: 'The steps for the recipe are as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱的步骤如下：
- en: 'Do the necessary imports and identify the compute device, as described in the
    previous classification recipe:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入并识别计算设备，如前一个分类菜谱中所述：
- en: '[PRE22]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In this step, we initialize a pipeline instance with the **facebook/bart-large-mnli**
    model. We have chosen this particular model for our example, but other models
    can also be used – available on the **HuggingFace** site:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们使用**facebook/bart-large-mnli**模型初始化一个管道实例。我们选择了这个特定的模型作为我们的示例，但也可以使用其他模型——可在**HuggingFace**网站上找到：
- en: '[PRE23]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Use the pipeline instance to classify a sentence into a given set of candidate
    labels. The labels provided in the example are completely novel and have been
    defined by us. The model was not trained on examples with these labels. The classification
    output is stored in the **result** variable, which is a dictionary. This dictionary
    has the **''sequence''**, **''labels''**, and **''scores''** keys. The **''sequence''**
    element stores the original sentence passed to the classifier. The **''labels''**
    element stores the labels for the classes, but the ordering is different than
    what we passed in the arguments. The **''scores''** element stores the probabilities
    of the classes and corresponds to the same ordering in the **''labels''** element.
    The last argument in this call is **device**. If there is a CUDA-compatible device
    present in the system, it will be used:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用管道实例将句子分类到给定的一组候选标签中。示例中提供的标签完全是新颖的，并且是由我们定义的。模型没有在具有这些标签的示例上进行训练。分类输出存储在**result**变量中，它是一个字典。这个字典有**'sequence'**、**'labels'**和**'scores'**键。**'sequence'**元素存储传递给分类器的原始句子。**'labels'**元素存储类别的标签，但其顺序与我们传递的参数不同。**'scores'**元素存储类别的概率，并与**'labels'**元素中的相同顺序相对应。这个调用中的最后一个参数是**device**。如果系统中存在兼容CUDA的设备，它将被使用：
- en: '[PRE24]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We print the sequence, followed by printing each label and its associated probability.
    Note that the order of the labels has changed from the initial input that we specified
    in the previous step. The function call reorders the labels based on the descending
    order of the label probability:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印序列，然后打印每个标签及其相关的概率。请注意，标签的顺序已经从我们在上一步中指定的初始输入中改变。函数调用根据标签概率的降序重新排序标签：
- en: '[PRE25]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We run a zero-shot classification on a different sentence and print the results
    for it. This time, we emit a result that picks the class with the highest probability
    and prints the result:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对不同的句子运行零样本分类，并打印其结果。这次，我们发出一个选择概率最高的类别的结果并打印出来：
- en: '[PRE26]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: So far, we have used the transformer and some pre-trained models to generate
    token IDs and classifications. These recipes have used the encoder part of the
    transformer. The encoder generates a representation of the text, which is then
    used by a classifier head in front of it to generate classification labels. However,
    the transformer has another component, called the decoder. A decoder uses a given
    representation of text and generates subsequent text. In the next recipe, we will
    learn more about the decoder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了转换器和一些预训练模型来生成标记ID和分类。这些菜谱已经使用了转换器的编码器部分。编码器生成文本的表示，然后由其前面的分类头使用以生成分类标签。然而，转换器还有一个名为解码器的另一个组件。解码器使用给定的文本表示并生成后续文本。在下一个菜谱中，我们将更多地了解解码器。
- en: Generating text
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成文本
- en: In this recipe, we will use a **generative transformer model** to generate text
    from a given seed sentence. One such model to generate text is the GPT-2 model,
    which is an improved version of the original **General Purpose Transformer** (**GPT**)
    model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在此菜谱中，我们将使用一个 **生成式转换器模型**从给定的种子句子生成文本。一个用于生成文本的模型是 GPT-2 模型，它是原始 **通用转换器**（**GPT**）模型的改进版本。
- en: Getting ready
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.5_Transformer_text_generation.ipynb` notebook from
    the code site if you need to work from an existing notebook.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为此菜谱的一部分，我们将使用来自 transformers 包的管道模块。如果您需要从一个现有的笔记本中工作，可以使用代码站点中的 `8.5_Transformer_text_generation.ipynb`
    笔记本。
- en: How to do it...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this recipe, we will start with an initial seed sentence and use the GPT-2
    model to generate text based on the given seed sentence. We will also tinker with
    certain parameters to improve the quality of the generated text.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在此菜谱中，我们将从一个初始种子句子开始，使用 GPT-2 模型根据给定的种子句子生成文本。我们还将调整某些参数以提高生成文本的质量。
- en: 'The recipe does the following things:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱执行以下操作：
- en: It initializes a starting sentence from which a continuing sentence will be
    generated
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个起始句子，后续句子将从该句子生成。
- en: It initializes a GPT-2 model as part of a pipeline and uses it to generate five
    sentences as part of the parameters passed to the generator method
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个作为管道一部分的 GPT-2 模型，并使用它来生成五个句子，作为传递给生成方法的参数。
- en: It prints the results of the generation
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它打印了生成的结果。
- en: 'The steps for the recipe are as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 菜谱的步骤如下：
- en: 'Do the necessary imports and identify the compute device, as described in the
    previous classification recipe:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入并识别计算设备，如前一个分类菜谱中所述：
- en: '[PRE27]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Initialize a seed input sentence based on which the subsequent text will be
    generated. Our goal here is to use the GPT-2 decoder to hypothetically generate
    the text that follows it based on the generation parameters:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据后续文本生成的种子输入句子初始化。我们的目标是使用 GPT-2 解码器根据生成参数假设性地生成后续文本：
- en: '[PRE28]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In this step, we initialize a text-generation pipeline with the **''gpt-2''**
    model. This model is based on a **large language model** (**LLM**) that was trained
    using a large text corpus. The last argument in this call is **device**. If there
    is a CUDA-compatible device present in the system, it will be used:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们使用 **'gpt-2'** 模型初始化一个文本生成管道。该模型基于一个使用大量文本语料库训练的 **大型语言模型**（**LLM**）。此调用中的最后一个参数是
    **device**。如果系统中存在兼容CUDA的设备，它将被使用：
- en: '[PRE29]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Generate the continuing sequence for the seed sentence and store the results.
    The parameters of note used in the call other than the seed text are as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为种子句子生成后续序列并存储结果。调用中除种子文本之外需要注意的参数如下：
- en: '**max_length**: The maximum length of the generated sentence, including the
    length of the seed sentence.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_length**：生成句子的最大长度，包括种子句子的长度。'
- en: '**num_return_sequences**: The number of generated sequences to return.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_return_sequences**：返回的生成序列的数量。'
- en: '**num_beams**: This parameter controls the quality of the generated sequence.
    A higher number generally results in improved quality of the generated sequence
    but also slows down the generation. We encourage you to try out different values
    for this parameter based on the quality requirements of the generated sequence.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_beams**：此参数控制生成序列的质量。较高的数值通常会导致生成序列的质量提高，但也会减慢生成速度。我们鼓励您根据生成序列的质量要求尝试不同的此参数值。'
- en: '[PRE30]'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Print the generated sentences:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印生成的句子：
- en: '[PRE31]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There’s more…
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: As we can see in the preceding example, the generated output was rudimentary,
    repetitive, grammatically incorrect, or perhaps incoherent. There are different
    techniques that we can use to improve the generated output.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，生成的输出是基本的、重复的、语法错误的，或者可能是不连贯的。我们可以使用不同的技术来改进生成的输出。
- en: We will use the `no_repeat_ngram_size` parameter this time to generate the text.
    We will set the value of this parameter to `2`. This instructs the generator to
    not repeat bi-grams.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这次将使用`no_repeat_ngram_size`参数来生成文本。我们将此参数的值设置为`2`。这指示生成器不要重复二元组。
- en: 'We will change the line in *step 4* to the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*步骤4*中的行更改为以下内容：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As we can see in the following output, the sentences have reduced repetition,
    but some of them are still incoherent:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下输出所示，句子重复性减少，但其中一些仍然不连贯：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: To improve the coherency, we can use another technique to include the next word
    from a set of words that have the highest likelihood of being the next word. We
    use the `top_k` parameter and set its value to `50`. This instructs the generator
    to sample the next word from the top 50 words, arranged according to their probabilities.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高连贯性，我们可以使用另一种技术来包含一组单词中最有可能成为下一个单词的下一个单词。我们使用`top_k`参数并将其值设置为`50`。这指示生成器从根据其概率排列的前50个单词中采样下一个单词。
- en: 'We change the line in *step 4* to the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*步骤4*中的行更改为以下内容：
- en: '[PRE34]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can also combine the `top_k` parameter with the `top_p` parameter. This
    instructs the generator to select the next word from the set of words that have
    a probability higher than this defined value. Adding this parameter with a value
    of `0.8` yields the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将`top_k`参数与`top_p`参数结合使用。这指示生成器从具有高于此定义值的概率的单词集中选择下一个单词。将此参数与值为`0.8`的参数结合使用会产生以下输出：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we can see, the addition of additional parameters to the generator continues
    to improve the generated output.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，向生成器添加额外的参数继续提高生成的输出质量。
- en: 'As a final example, let us generate a longer output sequence by changing the
    line in *step 4* to the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的例子，让我们通过将*步骤4*中的行更改为以下内容来生成更长的输出序列：
- en: '[PRE36]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As we can see, the generated output, however fictitious, is more coherent and
    readable. We encourage you to experiment with different mixes of parameters and
    their respective values to improve the generated output based on their use cases.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，生成的输出，尽管有些虚构，但更加连贯和易读。我们鼓励您尝试不同的参数组合及其相应值，以根据其用例改进生成的输出。
- en: Please note that the output returned by the model might differ a bit from what
    this example has shown. This happens because the internal language model is probabilistic
    in nature. The next word is sampled from a distribution that contains words that
    have a probability larger than what we defined in our parameters for generation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型返回的输出可能略有不同于本例所示。这是因为内部语言模型本质上具有概率性。下一个单词是从包含概率大于我们在生成参数中定义的单词的分布中采样的。
- en: In this recipe, we used the decoder module of the transformer to generate text,
    given a seed sentence. There are use cases where an encoder and decoder are used
    together to generate text. We will learn about this in the next recipe.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了transformer的解码器模块来生成文本，给定一个种子句子。在某些用例中，编码器和解码器一起使用来生成文本。我们将在下一个示例中了解这一点。
- en: Language translation
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言翻译
- en: In this recipe, we will use transformers for language translation. We will use
    the **Google Text-To-Text Transfer Transformer** (**T5**) model. This model is
    an end-to-end model that uses both the encoder and decoder components of the transformer
    model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用transformers进行语言翻译。我们将使用**Google Text-To-Text Transfer Transformer**（**T5**）模型。这是一个端到端模型，它使用transformer模型的编码器和解码器组件。
- en: Getting ready
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: As part of this recipe, we will use the pipeline module from the transformers
    package. You can use the `8.6_Language_Translation_with_transformers.ipynb` notebook
    from the code site if you need to work from an existing notebook.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本示例的一部分，我们将使用transformers包中的pipeline模块。如果您需要从一个现有的笔记本中工作，可以使用代码网站上的`8.6_Language_Translation_with_transformers.ipynb`笔记本。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this recipe, you will initialize a seed sentence in English and translate
    it to French. The T5 model expects the input format to encode the information
    about the language translation task along with the seed sentence. In this case,
    the encoder uses the input in the source language and generates a representation
    of the text. The decoder uses this representation and generates text for the target
    language. The T5 model is trained specifically for this task, in addition to many
    others. If you are running on a machine that does not have a CUDA-compatible device,
    it might take some time for the recipe steps to be executed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，你将初始化一个英语种子句子并将其翻译成法语。T5模型期望输入格式编码有关语言翻译任务的信息以及种子句子。在这种情况下，编码器使用源语言中的输入并生成文本的表示。解码器使用这个表示并为目标语言生成文本。T5模型专门为此任务以及其他许多任务进行了训练。如果你在没有任何CUDA兼容设备的机器上运行，食谱步骤的执行可能需要一些时间。
- en: 'The recipe does the following things:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该食谱执行以下操作：
- en: It initializes the **Google t5-base** model and tokenizer
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化了**Google t5-base**模型和标记器
- en: It initializes a seed sentence in English that will be translated into French
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个英语种子句子，该句子将被翻译成法语
- en: It tokenizes the seed sentence along with the task specification to translate
    the seed sentence into French
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将种子句子以及翻译任务规范进行标记化，以便将种子句子翻译成法语
- en: It generates the translated tokens, decodes them into the target language (French),
    and prints them
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它生成翻译后的标记，将它们解码成目标语言（法语），并打印出来
- en: 'The steps for the recipe are as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该食谱的步骤如下：
- en: 'Do the necessary imports and identify the compute device, as described in the
    previous classification recipe:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入并识别计算设备，如前一个分类食谱中所述：
- en: '[PRE37]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Initialize a tokenizer and model instance with the **t5-base** model from Google.
    We use the **model_max_length** parameter of **200**. Feel free to experiment
    with higher values if your seed sentence is longer than 200 words. We also load
    the model onto the device that was identified for computation in step 1:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自谷歌的**t5-base**模型初始化一个标记器和模型实例。我们使用**model_max_length**参数为**200**。如果你的种子句子超过200个单词，可以自由地尝试更高的值。我们还把模型加载到第1步中确定的用于计算的设备上：
- en: '[PRE38]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Initialize a seed sequence that you want to translate:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个你想要翻译的种子序列：
- en: '[PRE39]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Tokenize the input sequence. The tokenizer specifies the source and the target
    language as part of its input encoding. This is done by appending the “**translate
    English to French:**” text to the input seed sequence. We load these token IDs
    into the device that is used for computation. It is a requirement for both the
    model and the token IDs to be on the same device:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记输入序列。标记器将其源语言和目标语言作为其输入编码的一部分进行指定。这是通过将“**翻译英语到法语：**”文本附加到输入种子句子中实现的。我们将这些标记ID加载到用于计算的设备上。模型和标记ID必须在同一设备上，这是两者的要求：
- en: '[PRE40]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Translate the source language token IDs to the target language token IDs via
    the model. The model uses the encoder-decoder architecture to convert the input
    token IDs to the output token IDs:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型将源语言标记ID转换为目标语言标记ID。该模型使用编码器-解码器架构将输入标记ID转换为输出标记ID：
- en: '[PRE41]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Decode the text from the token IDs to the target language tokens. We use the
    tokenizer to convert the output token IDs to the target language tokens:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本从标记ID解码成目标语言标记。我们使用标记器将输出标记ID转换为目标语言标记：
- en: '[PRE42]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Print the translated output:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印翻译后的输出：
- en: '[PRE43]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In conclusion, this chapter introduced the concept of transformers, along with
    some of its basic applications. The next chapter will focus on how we can use
    the different NLP techniques to understand text better.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本章介绍了transformers的概念，以及一些基本应用。下一章将重点介绍我们如何使用不同的NLP技术更好地理解文本。
