- en: Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: In the previous chapters, we learned about mapping input to a target—where,
    the input and output values are provided. In this chapter, we will be learning
    about reinforcement learning, where the objective that we want to achieve and
    the environment that we operate in are provided, but not any input or output mapping.
    The way in which reinforcement learning works is that we generate input values
    (the state in which the agent is) and the corresponding output values (the reward
    the agent achieves for taking certain actions in a state) by taking random actions
    at the start and gradually learning from the generated input data (actions in
    a state) and output values (rewards achieved by taking certain actions).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了将输入映射到目标—其中输入和输出值是已知的。在本章中，我们将学习强化学习，其中目标和操作环境是已知的，但没有输入或输出的映射。强化学习的工作原理是，我们通过开始时随机采取行动，逐步从生成的输入数据（状态中的行动）和输出数据（通过采取某些行动所获得的奖励）中学习，生成输入值（智能体的状态）和相应的输出值（智能体在状态中采取某些行动所获得的奖励）。
- en: 'In this chapter, we will cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将覆盖以下内容：
- en: The optimal action to take in a simulated game with a non-negative reward
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个模拟游戏中采取的最佳行动，且奖励为非负值
- en: The optimal action to take in a state in a simulated game
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模拟游戏中的状态下采取的最佳行动
- en: Q-learning to maximize rewards when playing Frozen Lake
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning 在玩《Frozen Lake》时最大化奖励
- en: Deep Q-learning to balance a cart pole
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q学习来平衡一个推车杆
- en: Deep Q-learning to play the Space Invaders game
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q学习来玩《Space Invaders》游戏
- en: The optimal action to take in a simulated game with a non-negative reward
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一个模拟游戏中采取的最佳行动，且奖励为非负值
- en: In this section, we will understand the way in which we can take the right action
    for a simulated game. Note that this exercise will primarily help you to grasp
    how reinforcement learning works.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将理解如何在模拟游戏中采取正确的行动。请注意，这个练习将主要帮助你掌握强化学习的工作原理。
- en: Getting ready
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let's define the environment we are operating in this simulated setting.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一下我们在这个模拟环境中操作的环境。
- en: You have three boxes, on which two players are playing a game. Player 1 marks
    a box with 1 and player 2 marks one with 2\. The player who is able to mark two
    consecutive boxes wins.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你有三个盒子，两个玩家正在玩游戏。玩家1标记一个盒子为1，玩家2标记一个盒子为2。能够标记两个连续盒子的玩家获胜。
- en: 'The empty board for this game looks as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该游戏的空白棋盘如下所示：
- en: '![](img/a6def8c7-0658-4359-b45a-8cae86613db7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6def8c7-0658-4359-b45a-8cae86613db7.png)'
- en: 'For the problem we just defined, only player 1 has an opportunity to win the
    game. The possible scenarios in which player 1 wins are either of the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们刚刚定义的问题，只有玩家1有机会赢得游戏。玩家1赢得游戏的可能情境如下：
- en: '![](img/129d43fc-ab45-4bc1-acc2-63b404e4cd47.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/129d43fc-ab45-4bc1-acc2-63b404e4cd47.png)'
- en: From the problem setting, the intuitive way in which player 1 wins is when player
    1 chooses the middle box. This way, irrespective of which box is chosen by player
    2, player 1 will win on their subsequent move.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从问题设置中来看，玩家1获胜的直观方式是选择中间的盒子。这样，无论玩家2选择哪个盒子，玩家1都会在下一步获胜。
- en: While the first step for player 1 is intuitive for us, in the next section we
    will learn about how an agent can automatically figure out the optimal first move.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于玩家1来说，第一步是直观的，但在接下来的章节中，我们将学习一个智能体如何自动找出最佳的第一步行动。
- en: 'The strategy that we will adopt to solve this problem is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取的策略来解决这个问题如下：
- en: We initialize an empty board
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们初始化一个空白棋盘
- en: Player 1 chooses a box randomly
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家1随机选择一个盒子
- en: Player 2 chooses a box randomly from the remaining 2 boxes
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家2从剩下的两个盒子中随机选择一个
- en: 'Depending on the box player 1 is left with, we update the reward for player
    1:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据玩家1剩下的盒子，我们更新玩家1的奖励：
- en: If player 1 is able to place 1s in two consecutive boxes, he is a winner and
    will a reward of 1
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果玩家1能够在两个连续盒子中放置1，他将成为赢家，并获得1的奖励
- en: Otherwise, player 1 will get a reward of 0
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，玩家1将获得0的奖励
- en: Repeat the preceding exercise 100 times, where the game is played and we store
    a reward for the given sequence of moves
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复前面的练习100次，每次进行游戏并为给定的动作序列存储奖励
- en: Now, we will go ahead and calculate the average reward for the various first
    moves that were taken
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们将继续计算各种第一步行动的平均奖励
- en: The box that was chosen in the first move, that has the highest average reward
    over 100 iterations is the optimal first move for player 1
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一次移动中选择的框，经过100次迭代后，具有最高平均奖励的是玩家1的最佳首次行动。
- en: How to do it...
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The strategy defined above is coded as follows (the code file is available
    as `Finding_optimal_policy.ipynb` in GitHub):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略的代码如下（代码文件可在GitHub上的`Finding_optimal_policy.ipynb`找到）：
- en: 'Define the game environment and the function to play the game:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义游戏环境和执行游戏的函数：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we are initializing an empty board with zero values and
    playing a random move named `samp`. Player 1 takes the first move and then player
    2 takes their turn, followed by player 1\. We fill up the empty board in this
    manner.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们初始化了一个空的棋盘，所有单元格值为零，并进行了一次名为`samp`的随机移动。玩家1先行，然后是玩家2，接着是玩家1。我们以这种方式填充空棋盘。
- en: 'Define a function to calculate the reward at the end of a game:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来计算游戏结束时的奖励：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Play the game `100` times:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩游戏`100`次：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Calculate the reward for choosing a certain first move:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算选择某一首次行动的奖励：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When you repeat the preceding code for multiple options for the first move,
    you will notice that the average reward is highest when occupying the second square.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对多种首次行动选项重复运行前面的代码时，你会注意到当占据第二个方格时，平均奖励最高。
- en: The optimal action to take in a state in a simulated game
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在模拟游戏中，状态下采取的最佳行动
- en: 'In the previous scenario, we considered a simplistic case where there is a
    reward when the objective is achieved. In this scenario, we will complicate game
    by having negative rewards too. However, the objective remains the same: maximizing the reward
    in the given problem setting where the environment has both positive and negative
    rewards.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的场景中，我们考虑了一个简化的情况，即当目标达成时会获得奖励。在此场景中，我们将通过引入负奖励来使游戏更加复杂。然而，目标仍然不变：在给定环境中最大化奖励，该环境同时包含正奖励和负奖励。
- en: Getting ready
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The environment we are working on is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理的环境如下：
- en: '![](img/77011aa8-e3e7-41cc-9bf4-452707112100.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77011aa8-e3e7-41cc-9bf4-452707112100.png)'
- en: 'We start at the cell with **S** in it and our objective is to reach the cell
    where the reward is **+1**. In order to maximize the chances of achieving the reward,
    we will be using Bellman''s equation, which calculates the value of each cell
    in the preceding grid as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从包含**S**的单元格开始，目标是到达奖励为**+1**的单元格。为了最大化获得奖励的机会，我们将使用贝尔曼方程来计算前面网格中每个单元格的值，如下所示：
- en: '*Value of current cell = reward of moving from the current cell to next cell
    + discount factor * value of next cell*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*当前单元格的值 = 从当前单元格移动到下一个单元格的奖励 + 折扣因子 * 下一个单元格的值*'
- en: Additionally, in the current problem, the reward for moving to any cell other
    than the cell with a reward of **+1** is *0*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在当前问题中，除了奖励为**+1**的单元格外，移动到任何其他单元格的奖励都是*0*。
- en: The discount factor can be thought of as the energy expended moving from one
    cell to another. Thus, a cell that is far away from the rewarding cell will have
    a lower value compared to other cells in the current problem setting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子可以被视为从一个单元格移动到另一个单元格时所消耗的能量。因此，在当前问题设置中，远离奖励单元格的单元格值较低。
- en: Once we calculate the value of each cell, we move to the cell that has the highest
    value of all the cells that an agent could move to.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出每个单元格的值，我们就会移动到具有所有可能移动的单元格中最高值的单元格。
- en: 'The strategy that we''ll adopt to calculate the value of each cell is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的计算每个单元格值的策略如下：
- en: Initialize an empty board.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个空棋盘。
- en: Define the possible actions that an agent could take in a cell.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义代理在一个单元格内可能采取的行动。
- en: Define the state that an agent will be in, for the action the agent takes in
    the current cell.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义代理在当前单元格内采取行动时所处的状态。
- en: Calculate the value of the current state, which depends on the reward for moving
    to the next state, as well as the value of the next state.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算当前状态的值，该值依赖于移到下一个状态的奖励以及下一个状态的值。
- en: Update the cell value of the current state based on the earlier calculation.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于之前的计算，更新当前状态的单元格值。
- en: Additionally, store the action taken in the current state to move to the next
    state.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，存储当前状态下采取的行动，以便移动到下一个状态。
- en: Note that, in the initial iterations, the values of cells that are far away
    from the end goal remain zero, while the values of cells that are adjacent to
    the end state rise.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，在初期迭代中，距离终点较远的格子的值保持为零，而与终点相邻的格子的值则上升。
- en: As we iterate the previous steps multiple times, we will be in a position to
    update the cell values and, thus, in a position to decide the optimal route for
    the agent to follow.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着我们多次迭代前面的步骤，我们将能够更新格子的值，从而决定代理应该遵循的最优路径。
- en: How to do it...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In this section, we''ll code the strategy that we laid out in the previous
    section (the code file is available as `Finding_optimal_policy.ipynb` in GitHub):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写在前一节中规划的策略（代码文件在GitHub上的`Finding_optimal_policy.ipynb`中可以找到）：
- en: 'Initialize an empty board:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空的棋盘：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the actions that can be taken in different states—where `D` represents
    moving down, `R` is right, `L` is left, and `U` is moving up:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义在不同状态下可以采取的行动——其中`D`代表向下移动，`R`代表向右，`L`代表向左，`U`代表向上：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the function that extracts the next state given the current state and
    the action taken in the current state:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，根据当前状态和在当前状态下采取的行动来提取下一个状态：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Initialize the lists where the state, action, and rewards are appended:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化列表，用于附加状态、行动和奖励：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Execute 100 actions at most in an episode (an episode is an instance of a game)
    where random action is taken in a cell (state) and calculate the value of the current
    state based on the reward for moving to the next state, as well as the value of
    next state.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个回合中最多执行100次行动（一个回合是游戏的一次实例），在每个格子（状态）中随机采取行动，并根据移动到下一个状态的奖励以及下一个状态的值来计算当前状态的值。
- en: 'Repeat the above exercise for `100` iterations (episodes/games) and calculate
    the value of each cell:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重复以上练习`100`次迭代（回合/游戏），并计算每个格子的值：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code, we are taking random actions in a state and then calculating
    the next state for the action taken in the current state:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们在一个状态中采取随机行动，然后计算该行动对应的下一个状态：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code, we are updating the value of a state:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在更新一个状态的值：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding results in the following final cell state values across all the
    cells:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的结果在所有格子中的最终状态值如下：
- en: '![](img/695f027e-2888-4b08-b8ca-dba64348fc4e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/695f027e-2888-4b08-b8ca-dba64348fc4e.png)'
- en: Based on the preceding output, the agent can take take either the right action
    or the down action at the start of the game (where the agent starts from the top-left
    corner). However, if the agent takes the down action in the first step, it is
    better off taking the *r**ight* action in the next step, as the cell state value
    is higher for the state to the right compared to the state that is above the current
    cell state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的输出，代理可以在游戏开始时采取右侧的行动或向下的行动（代理从左上角开始）。然而，如果代理在第一步采取向下的行动，那么在下一步它更适合采取*向右*的行动，因为右侧的状态值高于当前状态上方的状态值。
- en: There's more...
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Imagine the environment (the cells and their corresponding rewards) looks as
    follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设环境（各个格子及其对应的奖励）如下所示：
- en: '![](img/a7b18107-739a-4819-b47b-2a8998b62a92.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7b18107-739a-4819-b47b-2a8998b62a92.png)'
- en: 'The actions that could be taken at different states areas follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同状态下可以采取的行动如下：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The cell state values in various cells after iterating through the game multiple
    times is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在多次游戏迭代后，各个格子的状态值如下所示：
- en: '![](img/96dfeacf-52d9-47f3-928c-275ff5b32948.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96dfeacf-52d9-47f3-928c-275ff5b32948.png)'
- en: From the preceding results, we can see that the agent is better off taking an
    action down from the top-left corner than moving to its right, as the cell state
    value is higher for the cell state that is below the starting cell.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的结果来看，我们可以看到，代理在左上角采取向下的行动要比向右移动更好，因为下方格子的状态值高于起始格子上方的状态值。
- en: Q-learning to maximize rewards when playing Frozen Lake
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Q学习最大化Frozen Lake游戏中的奖励
- en: So far, in the previous sections, we have been taking random actions in a given
    state. Additionally, we have also been defining the environment and calculating
    the next state, actions, and the reward for a move via code. In this section,
    we will leverage OpenAI's Gym package to navigate through the Frozen Lake environment.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前面的章节中，我们一直在给定的状态下采取随机行动。此外，我们还通过代码定义了环境，并计算了每一步的下一个状态、行动和奖励。在本节中，我们将利用OpenAI的Gym包来在Frozen
    Lake环境中进行导航。
- en: Getting ready
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The Frozen Lake environment looks as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结湖环境如下所示：
- en: '![](img/2f4064cc-944d-4d94-ab94-0a5c8b963098.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f4064cc-944d-4d94-ab94-0a5c8b963098.png)'
- en: The agent starts from the **S** state and the goal is to reach the **G** state
    by avoiding the **H** state as far as possible.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 代理从**S**状态开始，目标是通过尽量避开**H**状态，最终到达**G**状态。
- en: In the preceding environment, there are 16 possible states that an agent can
    be in. Additionally, the agent can take four possible actions (move up, down,
    right, or left).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的环境中，代理可以处于16个可能的状态中。此外，代理可以采取四个可能的动作（向上、向下、向右或向左）。
- en: We'll define a q-table where there are 16 rows corresponding to the 16 states
    and four columns corresponding to the four actions that can be taken in each state.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个q表格，其中有16行对应16种状态，4列对应每种状态下可以采取的四个动作。
- en: 'In the previous section, we learned that:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习到：
- en: '*Value of action taken in a state = reward + discount factor * value of the
    best possible action taken in the next state*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*在一个状态下采取的动作的价值 = 奖励 + 折扣因子 * 下一个状态中采取的最佳可能动作的价值*'
- en: 'We''ll modify the preceding formula as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改前面的公式如下：
- en: '*Value of action taken in a state = value of action taken in a state + 1*(reward
    + discount factor* value of the best possible action taken in the next state - value
    of action taken in a state)*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*在一个状态下采取的动作的价值 = 在该状态下采取的动作的价值 + 1 * (奖励 + 折扣因子 * 下一个状态中采取的最佳可能动作的价值 - 在该状态下采取的动作的价值)*'
- en: Finally, we'll replace the 1 with the learning rate, so that a value update
    of an action in a state does not change drastically. This is similar to the effect
    of having learning rates in neural networks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把1替换为学习率，这样状态中动作的值更新就不会发生剧烈变化。这类似于神经网络中使用学习率的效果。
- en: '*Value of action taken in a state = value of action taken in a state + learning
    rate*(reward + discount factor* value of the best possible action taken in the
    next state - value of action taken in a state)*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*在一个状态下采取的动作的价值 = 在该状态下采取的动作的价值 + 学习率 * (奖励 + 折扣因子 * 下一个状态中采取的最佳可能动作的价值 - 在该状态下采取的动作的价值)*'
- en: From the preceding, we can now update the q-table so that we can identify the
    optimal action that can be taken in different states.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的内容，我们现在可以更新q表格，以便能够识别在不同状态下可以采取的最佳动作。
- en: 'The strategy that we''ll adopt to solve this case study is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下策略来解决这个案例研究：
- en: Register the environment in OpenAI's Gym
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OpenAI的Gym中注册环境
- en: Initialize a zero array q-table shaped as 16 x 4
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个零数组q表格，形状为16 x 4
- en: 'Employ an exploration-versus-exploitation approach in choosing an action in
    a given state:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择给定状态下的动作时，采用探索与利用的平衡方法：
- en: So far, we have merely explored possible overall actions as we randomly chose
    an action in a given state.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅仅是探索了可能的整体动作，因为我们在给定状态下随机选择了一个动作。
- en: In this section, we will explore the initial iterations as we are not sure of
    the optimal action to take during the initial few episodes of the game.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索初始的几次迭代，因为我们在游戏的前几个回合中并不确定应该采取的最佳动作。
- en: However, as we learn more about the game, we exploit what we have learned in
    terms of possible actions to take while still taking random actions (with decreasing
    frequency as the number of episodes increases).
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，随着我们对游戏了解的深入，我们将利用已经学到的关于可能采取的动作的知识，同时仍然会随机采取一些动作（随着回合数的增加，随机动作的频率会逐渐减少）。
- en: 'In a given episode:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一回合中：
- en: Choose an action depending on whether we try and explore or exploit
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据我们是尝试探索还是利用，选择一个动作
- en: Identify the new state and reward, and check whether the game is over by taking
    the action chosen in the previous step
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定新的状态和奖励，并检查通过采取上一步选择的动作，游戏是否结束
- en: Initialize a learning rate parameter and discount factor
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化学习率参数和折扣因子
- en: Update the value of taking the preceding action in a state in the q-table by
    using the formula discussed earlier
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前面讨论的公式，通过更新q表格中在某一状态下采取的前一个动作的价值
- en: Repeat the preceding steps until the game is over
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，直到游戏结束
- en: Additionally, repeat the preceding steps for 1,000 different games
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，重复前面的步骤，进行1,000场不同的游戏
- en: Check the q-table to identify the optimal action to take in a given state
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看q表格，找出在给定状态下应采取的最佳动作
- en: Plot the path of an agent as it takes the actions in a state as per the q-table
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据q表格绘制代理在状态中采取动作的路径
- en: How to do it...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'In this section, we will code the strategy we discussed earlier (the code file
    is available as `Frozen_Lake_with_Q_Learning.ipynb` in GitHub):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将编写我们之前讨论的策略（代码文件在GitHub上作为`Frozen_Lake_with_Q_Learning.ipynb`提供）：
- en: 'Import the relevant packages:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    It supports teaching agents everything from walking to playing games such as Pong and Pinball.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Gym是一个开发和比较强化学习算法的工具包。它支持教智能体从行走到玩游戏（如Pong和Pinball）等所有任务。
- en: More about Gym can be found at: [https://gym.openai.com/](https://gym.openai.com/).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于Gym的信息可以在这里找到：[https://gym.openai.com/](https://gym.openai.com/)。
- en: 'Register the environment:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册环境：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create the environment:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建环境：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Inspect the created environment:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查创建的环境：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/eb411224-67a2-44b0-921e-610d1061e23a.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb411224-67a2-44b0-921e-610d1061e23a.png)'
- en: 'The preceding step renders (prints) the environment:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤呈现（打印）环境：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The preceding code provides the number of state action pairs in the environment.
    In our case, given that it is a 4 x 4 grid, we have a total of 16 states. Thus,
    we have a total of 16 observations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码提供了环境中的状态-动作对的数量。在我们的例子中，考虑到它是一个4x4的网格，我们总共有16个状态。因此，我们有16个观察。
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code defines the number of actions that can be taken in a state
    in the environment:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码定义了在环境中可以在某个状态下执行的动作数量：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code samples an action from the possible set of actions:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从可能的动作集合中采样一个动作：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code takes the action and generates the new state and the reward
    of the action, flags whether the game is done, and provides additional information
    for the step:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码执行动作并生成新的状态和该动作的奖励，标记游戏是否结束，并为步骤提供附加信息：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding code resets the environment so that the agent is back to the starting
    state.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码重置了环境，使得智能体回到起始状态。
- en: 'Initialize the q-table:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化q表：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have initialized it to a shape of (16, 4) as there are 16 states and 4 possible
    actions in each state.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其初始化为(16, 4)的形状，因为有16个状态，每个状态有4个可能的动作。
- en: 'Run multiple iterations of playing a game:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行多个游戏回合：
- en: 'Initialize hyper-parameters:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化超参数：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Play multiple episodes of the game:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 玩多个游戏回合：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the code below, we are defining the action to be taken. If `eps` (which is
    a random number generated between 0 to 1) is less than 0.5, we explore; otherwise,
    we exploit (to consider the best action in a q-table)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们定义了要采取的动作。如果`eps`（它是一个在0到1之间生成的随机数）小于0.5，我们进行探索；否则，我们进行利用（即考虑q表中的最佳动作）
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the code below, we are fetching the new state and the reward, and flag whether
    the game is done by taking the action in the given step:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们获取新的状态和奖励，并通过在给定步骤中采取动作来标记游戏是否结束：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the code below, we are updating the q-table based on the action taken in
    a state. Additionally, we are also updating the state with the new state obtained
    after taking action in the current state:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们根据在某个状态下采取的动作更新q表。此外，我们还在当前状态下采取动作后，使用新状态更新状态：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the following code, as the game is over, we proceed to a new episode of the game.
    However, we ensure that the randomness factor (`eps`), which is used in deciding
    whether we are going for exploration or exploitation, is updated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，由于游戏已结束，我们将继续进行新的游戏回合。然而，我们确保更新随机性因子（`eps`），该因子用于决定我们是进行探索还是利用。
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once we have built the q-table, we now deploy the agent to maneuver in line
    with the optimal actions suggested by the q-table:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们构建了q表，我们现在就可以部署智能体，让其根据q表建议的最优动作来操作：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding gives the optimal path that the agent has to traverse to reach
    the end goal.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码给出了智能体必须经过的最优路径，以到达最终目标。
- en: Deep Q-learning to balance a cart pole
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习平衡推车杆
- en: In the previous sections, we learned about taking an action based on q-table
    values. However, arriving at an optimal value is time-consuming, as the agent
    would have to play multiple times to arrive at the optimal q-table.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的部分，我们学习了如何基于q表的值采取行动。然而，得到最优值是一个耗时的过程，因为智能体需要多次游戏才能得到最优的q表。
- en: In this section, we will learn about using a neural network so that we can arrive
    at the optimal values faster than what we achieved when we used Q-learning.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何使用神经网络，这样我们就能比使用Q学习时更快地得到最优值。
- en: Getting ready
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this exercise, we will register the cart-pole environment where the possible
    actions are to move either right or left so that we balance the pole. Additionally
    the cart position, cart velocity, pole angle, and pole velocity at the tip is
    the information we have about the states.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将注册一个推车-杆环境，可能的行动是向右或向左移动，以保持杆的平衡。此外，推车的位置、推车速度、杆的角度和杆尖端的速度是我们关于状态的信息。
- en: 'The rules of this game can be found here: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的规则可以在此找到：[https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)。
- en: A pole is attached to a cart by an un-actuated joint, and the cart moves along
    a frictionless track. The system is controlled by applying a force of +1 or -1
    to the cart. The pendulum starts upright, and the goal is to prevent it from falling
    over. A reward of +1 is provided for every time-step that the pole remains upright.
    The episode ends when the pole is more than 15 degrees from the vertical, or the
    cart moves more than 2.4 units from the center.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 杆通过一个不带驱动的关节连接到推车上，推车沿着一个无摩擦的轨道移动。该系统通过对推车施加+1或-1的力量来控制。摆杆从直立开始，目标是防止其倒下。每当杆保持直立时，都会提供+1的奖励。当杆与竖直方向的夹角超过15度，或者推车离中心超过2.4个单位时，回合结束。
- en: In order to balance the cart-pole, we'll adopt the same strategy as we adopted
    in the previous section. However, the difference in deep q-learning is that we'll
    use a neural network to help us predict the optimal action that the agent needs
    to take.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平衡推车-杆，我们将采用与上一部分相同的策略。然而，深度Q学习的不同之处在于，我们将使用神经网络来帮助我们预测代理需要采取的最佳行动。
- en: 'The way in which we train the neural network is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练神经网络的方式如下：
- en: 'We''ll store the information on state values, the action taken, and the reward
    achieved:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将存储状态值、所采取的行动和获得的奖励的信息：
- en: The reward will be 1 if the game does not end (is not over) and 0 otherwise.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果游戏没有结束（未结束），奖励为1，否则为0。
- en: Initially, the model predicts based on randomly initialized weights, where the
    output layer of the model has two nodes that correspond to the new state's values
    for the two possible actions.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始时，模型基于随机初始化的权重进行预测，其中模型的输出层有两个节点，分别对应两个可能行动的新状态值。
- en: The new state value will be based on the action that maximizes the value of
    new state
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新状态值将基于最大化新状态值的行动。
- en: If the game is not over, we update the current state's value with the sum of
    the reward and the product of the maximum state value of the new state and the
    discount factor.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果游戏未结束，我们将通过当前状态的奖励与新状态的最大状态值和折扣因子的乘积之和来更新当前状态的值。
- en: 'We''ll now override the value of the action from the updated current state''s
    value that we obtained previously:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在将覆盖先前获得的更新后的当前状态值来更新行动的值：
- en: If the action taken in the current step is wrong (that is, the game is over)
    the value of the action in the current state will be 0.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前步骤采取的行动是错误的（即游戏结束），那么当前状态下该行动的值为0。
- en: Otherwise, the value of the target in the current step is a positive number.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，当前步骤中目标的值为正数。
- en: This way, we are letting the model figure out the right action to take.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样，我们让模型自己找出该采取的正确行动。
- en: Additionally, we can consider this a way to specify that the action is wrong
    when the reward is zero. However, given that we are not sure whether it is the
    right action when the reward is 1, we'll just update it for the action we took
    and leave the new state's value (if we take the other action) untouched.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们可以认为当奖励为零时，行动是错误的。然而，由于我们无法确定当奖励为1时它是否为正确行动，因此我们只更新我们采取的行动，并保持新状态的值（如果我们采取另一个行动）不变。
- en: We append the state values to the input array, and also the values of taking
    one or an other action in the current state as the output array.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将状态值附加到输入数组，并且将采取某个行动时在当前状态下的值作为输出数组。
- en: We fit the model that minimizes the mean-squared error for the preceding data
    points.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们拟合模型，最小化前述数据点的均方误差。
- en: Finally, we keep reducing the exploration over an increasing number of episodes.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在逐渐增加的回合数中不断减少探索。
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We''ll code the strategy we discussed earlier as follows (the code file is
    available as `Deep_Q_learning_to_balance_a_cart_pole.ipynb` in GitHub):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按如下方式编码我们之前讨论的策略（代码文件可在GitHub的`Deep_Q_learning_to_balance_a_cart_pole.ipynb`中找到）：
- en: 'Create the environment and store the action size and state size in variables:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建环境并将动作大小和状态大小存储在变量中：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A cart-pole environment looks as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一个倒立摆环境如下所示：
- en: '![](img/2a3a72ea-30b2-4b46-b545-908bc873ef5a.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a3a72ea-30b2-4b46-b545-908bc873ef5a.png)'
- en: 'Import the relevant packages:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包：
- en: '[PRE30]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define a model:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the lists that need to be appended:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义需要附加的列表：
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define a function that replays the game:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来重玩游戏：
- en: '[PRE33]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we are defining a function that takes the neural network
    model, batch size, and epsilon (the parameter that signifies whether we'll explore
    or exploit). We are fetching a random sample of the size of `batch_size`. Note
    that you will learn about memory structure (which comprises state, action, reward,
    and `next_state`) in the next step. If the game is not done, we are updating the
    reward for taking the action that is taken; otherwise, the target will be 0 (as
    the reward would be 0 when the game is over).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个函数，该函数接收神经网络模型、批次大小和epsilon（表示我们是探索还是开发的参数）。我们获取一个大小为`batch_size`的随机样本。请注意，你将在下一步了解记忆结构（包括状态、动作、奖励和`next_state`）。如果游戏未结束，我们将更新采取的动作的奖励；否则，目标奖励将为0（因为游戏结束时奖励为0）。
- en: Additionally, the model predicts the value of taking a certain action (as the
    model has 2 nodes in the output—where each provides the output of taking one action
    over the other). The function returns the updated model and the coefficient of
    exploration/exploitation (epsilon).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型预测采取某个动作的价值（因为模型在输出层有2个节点，每个节点分别输出采取其中一个动作的结果）。该函数返回更新后的模型和探索/开发系数（epsilon）。
- en: 'Play the game over multiple episodes and append the scores obtained by the agent.
    Additionally, ensure that the actions taken by the agent are dictated by the model
    based on the epsilon value:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多个回合中玩游戏，并附加代理获得的得分。此外，确保代理采取的行动是根据epsilon值由模型决定的：
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the preceding code, we are playing a total of 200 episodes where we are
    resetting the environment at the start of the episode. Additionally, we are reshaping
    the state so that it can be passed to the neural network model:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们总共进行了200个回合的游戏，并且在每回合开始时重置环境。此外，我们将状态重塑，以便可以传递给神经网络模型：
- en: '[PRE35]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the preceding step, we are taking an action based on the exploration parameter
    (epsilon), where we take a random action (`env.actionspace.sample()`) in certain
    cases, and leverage the model''s predictions in other cases:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们根据探索参数（epsilon）采取行动，在某些情况下我们采取随机行动（`env.actionspace.sample()`），而在其他情况下，我们利用模型的预测：
- en: '[PRE36]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the preceding step, we are performing an action and extracting the next
    state, the reward, and the information about whether the game is over. Additionally,
    we are overwriting the reward value with -10 if the game is over (which means
    the agent made an incorrect move). Further, we are extracting the next state and
    appending it into the memory. This way, we are creating a dataset for the model
    to be trained on, where the model takes the current state and reward to calculate
    the reward for one of the two possible actions:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们执行了一个动作并提取了下一个状态、奖励以及游戏是否结束的信息。此外，如果游戏结束，我们将奖励值重置为-10（这意味着代理做出了错误的动作）。进一步地，我们提取下一个状态并将其附加到记忆中。这样，我们为模型创建了一个数据集，模型利用当前状态和奖励计算两个可能动作之一的奖励：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the preceding code, if the game is done, we  append the score (the number
    of steps taken during the game); otherwise, we update the model. Additionally,
    we are updating the model only when memory has as many data points as the pre-defined
    batch size.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，如果游戏结束，我们将得分（游戏过程中采取的步数）附加到列表中；否则，我们更新模型。此外，只有当内存中的数据点数量达到预定义的批次大小时，我们才会更新模型。
- en: 'Plotting the scores over increasing epochs looks as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练轮数增加，得分的变化如下所示：
- en: '![](img/c8b05e8e-9ac2-4b2c-94cc-ee1ca482baad.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8b05e8e-9ac2-4b2c-94cc-ee1ca482baad.png)'
- en: Deep Q-learning to play Space Invaders game
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度Q学习来玩Space Invaders游戏
- en: In the previous section, we used Deep Q-learning to play the Cart-Pole game.
    In this section, we will leverage Deep Q-learning to play Space Invaders, which
    is a more complex environment than Cart-Pole.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用深度Q学习来玩倒立摆游戏。在本节中，我们将利用深度Q学习来玩Space Invaders，这是一个比倒立摆更复杂的环境。
- en: 'A sample screenshot of the Space Invaders game looks as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Space Invaders 游戏的截图如下所示：
- en: '![](img/61dfcc9b-3271-482d-aef3-0eaebddf9a93.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61dfcc9b-3271-482d-aef3-0eaebddf9a93.png)'
- en: source: https://gym.openai.com/envs/SpaceInvaders-v0/
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 来源： [https://gym.openai.com/envs/SpaceInvaders-v0/](https://gym.openai.com/envs/SpaceInvaders-v0/)
- en: The objective of this exercise is to maximize the score obtained in a single
    game.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的目标是最大化单场游戏中获得的分数。
- en: Getting ready
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'The strategy that we''ll adopt to build an agent that is able to maximize the score
    is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略是构建一个能够最大化分数的智能体，具体如下：
- en: Initialize the environment of the *Space Invaders-Atari2600* game.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化*Space Invaders-Atari2600*游戏环境。
- en: 'Preprocess the image frame:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像帧进行预处理：
- en: Remove pixels that do not necessarily impact the action prediction
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除那些不一定会影响动作预测的像素。
- en: For example, pixels below the location of the player
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，玩家位置下方的像素
- en: Normalize the input image.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化输入图像。
- en: Resize the image before passing it to the neural network model
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将图像传递给神经网络模型之前调整图像大小
- en: Stack frames as required by the Gym environment
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照Gym环境的要求堆叠帧
- en: 'Let the agent play the game over multiple episodes:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让智能体在多个回合中玩游戏：
- en: During the initial episodes, we'll have high exploration which decays over increasing
    episodes.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在初期回合中，我们会有较高的探索度，而随着回合的增加，探索度逐渐衰减。
- en: The action that needs to be taken in a state depends on the value of the exploration
    coefficient.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某一状态下需要采取的动作取决于探索系数的值。
- en: Store the game state and the corresponding reward for the action taken in a
    state in memory.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将游戏状态和对应的奖励（基于在该状态下采取的动作）存储在记忆中。
- en: Update the model depending on the reward that was received in the previous episodes.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据前几个回合获得的奖励来更新模型。
- en: How to do it...
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The strategy that we discussed earlier is coded as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的策略编码如下：
- en: 'Download the ROM that contains the Space Invaders game and also install the
    `retro` package:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载包含《太空入侵者》游戏的ROM，并安装`retro`包：
- en: '[PRE38]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create the environment and extract the observation space:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建环境并提取观察空间：
- en: '[PRE39]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Build a function that preprocesses the frame (image/screenshot of the Space
    Invaders game):'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个处理帧（图像/《太空入侵者》游戏的截图）预处理的函数：
- en: '[PRE40]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Build a function that stacks frames given a state:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个根据状态堆叠帧的函数：
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Initialize the model hyperparameters:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型的超参数：
- en: '[PRE42]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Build a function that samples data from the total memory:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个从总记忆中采样数据的函数：
- en: '[PRE50]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Build a function that returns the action that the agent needs to take:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个返回智能体需要采取的动作的函数：
- en: '[PRE52]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Build a function that fine-tunes the model:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个微调模型的函数：
- en: '[PRE53]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define the neural network model:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络模型：
- en: '[PRE54]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'A summary of model is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/4a623396-afa7-491b-96da-6853a83906e6.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a623396-afa7-491b-96da-6853a83906e6.jpg)'
- en: 'Loop through multiple episodes and keep playing the game while updating the
    model:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环多个回合并持续玩游戏，同时更新模型：
- en: '[PRE55]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Plot the rewards obtained over increasing episodes:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制随着回合增加获得的奖励：
- en: '[PRE56]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](img/3f3345e3-c0b6-49bc-9009-b2f000a06908.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f3345e3-c0b6-49bc-9009-b2f000a06908.png)'
- en: From this, we can see that the model has learned to score over 800 in some episodes.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以看出，模型已经学会在某些回合中得分超过800分。
