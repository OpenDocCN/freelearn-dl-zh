- en: Predicting Diabetes with Multilayer Perceptrons
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多层感知器预测糖尿病
- en: In the first chapter, we went through the inner workings of a neural network,
    how to build our own neural network using Python libraries such as Keras, as well
    as the end-to-end machine learning workflow. In this chapter, we will apply what
    we have learned to build a **multilayer perceptron** (**MLP**) that can predict
    whether a patient is at risk of diabetes. This marks the first neural network
    project that we will build from scratch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讲解了神经网络的内部工作原理，如何使用Python库（如Keras）构建自己的神经网络，以及端到端的机器学习工作流程。在本章中，我们将应用所学的内容，构建一个**多层感知器**（**MLP**），用于预测患者是否有糖尿病风险。这是我们从零开始构建的第一个神经网络项目。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the problem that we're trying to tackle—diabetes mellitus
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解我们要解决的问题——糖尿病
- en: How AI is being used in healthcare today, and how AI will continue to transform
    healthcare
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能今天在医疗保健中的应用，以及人工智能将如何继续变革医疗保健
- en: An in-depth analysis of the diabetes mellitus dataset, including data visualization
    using Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对糖尿病数据集的深入分析，包括使用Python进行数据可视化
- en: Understanding MLPs, and the model architecture that we will use
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解MLP，以及我们将使用的模型架构
- en: A step-by-step guide to implement and train an MLP with Keras
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras实现并训练MLP的逐步指南
- en: Analysis of our results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果分析
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The key Python libraries required for this chapter are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的关键Python库如下：
- en: matplotlib 3.0.2
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib 3.0.2
- en: pandas 0.23.4
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 0.23.4
- en: Keras 2.2.4
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 2.2.4
- en: NumPy 1.15.2
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 1.15.2
- en: seaborn 0.9.0
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: seaborn 0.9.0
- en: scikit-learn 0.20.2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 0.20.2
- en: To download the dataset required for this project, please refer to the instructions
    at [https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/Chapter02/how_to_download_the_dataset.txt](https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter2/how_to_download_the_dataset.txt).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载本项目所需的数据集，请参考[https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/Chapter02/how_to_download_the_dataset.txt](https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter2/how_to_download_the_dataset.txt)中的说明。
- en: The code for this chapter can be found in the GitHub repository for the book
    at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python)。
- en: 'To download the code into your computer, you may run the following `git clone`
    command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要将代码下载到您的计算机中，可以运行以下`git clone`命令：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the process is complete, there will be a folder titled `Neural-Network-Projects-with-Python` .
    Enter the folder by running this command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 完成该过程后，将会有一个名为`Neural-Network-Projects-with-Python`的文件夹。运行以下命令进入该文件夹：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要在虚拟环境中安装所需的Python库，请运行以下命令：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that you should have installed Anaconda on your computer first before
    running this command. To enter the virtual environment, run the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要先在计算机上安装Anaconda，然后才能运行此命令。要进入虚拟环境，请运行以下命令：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Navigate to the `Chapter02` folder by running the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令进入`Chapter02`文件夹：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following files are located in the folder:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文件位于该文件夹中：
- en: '`main.py`: This is the main code for the neural network.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main.py`：这是神经网络的主代码。'
- en: '`utils.py`: This file contains auxiliary utility code that will help us in
    the implementation of our neural network.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`utils.py`：此文件包含辅助工具代码，帮助我们实现神经网络。'
- en: '`visualize.py`: This file contains code for exploratory data analysis and data
    visualization.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`visualize.py`：此文件包含用于探索性数据分析和数据可视化的代码。'
- en: 'To run the code for the neural network, simply execute the `main.py` file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行神经网络的代码，只需执行`main.py`文件：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To recreate the data visualizations covered in this chapter, execute the `visualize.py`
    file:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要重新创建本章中介绍的数据可视化内容，请执行`visualize.py`文件：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Diabetes – understanding the problem
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 糖尿病 – 理解问题
- en: Diabetes is a chronic medical condition that is associated with elevated blood
    sugar levels in the body. Diabetes often leads to cardiovascular disease, stroke,
    kidney damage, and long-term damage to the extremities (that is, limbs and eyes).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病是一种与体内血糖水平升高相关的慢性疾病。糖尿病常导致心血管疾病、中风、肾脏损害以及四肢（即四肢和眼睛）的长期损伤。
- en: It is estimated that there are 415 million people in the world suffering from
    diabetes, with up to 5 million deaths every year attributed to diabetes-related
    complications. In the United States, diabetes is estimated to be the seventh highest
    cause of death. Clearly, diabetes is a cause of concern to the wellbeing of modern
    society.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 全球估计有4.15亿人患有糖尿病，每年约有500万人死于糖尿病相关并发症。在美国，糖尿病被认为是第七大死亡原因。显然，糖尿病已经成为现代社会福祉的一个严重问题。
- en: 'Diabetes can be divided into two subtypes: type 1 and type 2\. Type 1 diabetes
    results from the body''s inability to produce sufficient insulin. Type 1 diabetes
    is relatively rare compared to type 2 diabetes, and it only accounts for approximately
    5% of diabetes. Unfortunately, the exact cause of type 1 diabetes is unknown and
    therefore, it is difficult to prevent the onset of type 1 diabetes.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病可以分为两种亚型：1型和2型。1型糖尿病是由于身体无法产生足够的胰岛素所导致的。与2型糖尿病相比，1型糖尿病相对较为罕见，仅占糖尿病患者的约5%。不幸的是，1型糖尿病的确切原因尚不清楚，因此很难预防1型糖尿病的发生。
- en: Type 2 diabetes results from the body's gradual resistance to insulin. Type
    2 diabetes is the prevalent form of diabetes in the world, and it is caused by
    excessive body weight, irregular exercise, and a poor diet. Fortunately, the onset
    of type 2 diabetes can be prevented and reversed if diagnosed early.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2型糖尿病是由于身体对胰岛素的逐渐抵抗引起的。2型糖尿病是全球最为常见的糖尿病类型，其主要原因包括超重、不规律的运动和不良的饮食习惯。幸运的是，如果早期诊断，2型糖尿病是可以预防和逆转的。
- en: One of the barriers for early detection and diagnosis of diabetes is that the
    early stages of diabetes are often non-symptomatic. People who are on the path
    to diabetes (also known as prediabetes) often do not know that they have diabetes
    until it is too late.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 早期检测和诊断糖尿病的障碍之一是糖尿病的早期阶段往往没有明显症状。处于糖尿病前期（即预糖尿病）的人们通常在为时已晚时才意识到自己已经患有糖尿病。
- en: 'How can we use machine learning to address this problem? If we have a labeled
    dataset that contains some vital measurements of patients (for example, age and
    blood insulin level), as well as a true label indicating the onset of diabetes
    in the patient sometime after the measurements were taken, then we can train a
    neural network (machine learning classifier) on this data and use it to make predictions
    on new patients:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何利用机器学习解决这一问题？如果我们拥有一个标注的数据集，其中包含一些患者的重要指标（例如，年龄和血液胰岛素水平），以及标明患者在这些指标采集后某个时刻是否发生糖尿病的真实标签，那么我们可以基于这些数据训练神经网络（机器学习分类器），并将其应用于对新患者的预测：
- en: '![](img/e97e9572-4f07-4f7b-85e8-a5659671a799.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e97e9572-4f07-4f7b-85e8-a5659671a799.png)'
- en: In the next section, we'll briefly explore how AI is transforming healthcare.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将简要探讨人工智能如何改变医疗行业。
- en: AI in healthcare
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医疗行业中的人工智能
- en: Beyond predicting diabetes using machine learning, the field of healthcare,
    in general, is ripe for disruption by AI. According to a study by Accenture, the
    market for AI in healthcare is set for explosive growth, with an estimated compound
    annual growth rate of 40% by 2021\. This significant growth is driven by a proliferation
    of AI and tech companies in healthcare.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用机器学习预测糖尿病，整个医疗健康领域也正迎来人工智能的颠覆。根据埃森哲的一项研究，预计到2021年，人工智能在医疗行业的市场将迎来爆炸性增长，年复合增长率预计为40%。这一显著增长主要受到人工智能和科技公司在医疗领域扩展的推动。
- en: Apple's chief executive officer, Tim Cook, believes that Apple can make significant
    contributions in healthcare. Apple's vision for disrupting healthcare can be exemplified
    by its developments in wearable technology. In 2018, Apple announced a new generation
    of smartwatches with active monitoring of cardiovascular health. Apple's smartwatches
    can now conduct electrocardiography in real time, and even warn you when your
    heart rate becomes abnormal, which is an early sign of cardiovascular failure.
    Apple's smartwatches also collect accelerometer and gyroscope measurements to
    predict in real time if a significant fall has occurred. Clearly, the impact of
    AI on healthcare will be far-reaching.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果公司首席执行官蒂姆·库克（Tim Cook）相信，苹果能够在医疗保健领域做出重大贡献。苹果对颠覆医疗保健的愿景可以通过其在可穿戴技术方面的进展来体现。2018年，苹果发布了新一代智能手表，具有主动监测心血管健康的功能。苹果的智能手表现在可以实时进行心电图检查，甚至在心率异常时发出警告，这是心血管衰竭的早期征兆。苹果的智能手表还收集加速度计和陀螺仪的数据，以实时预测是否发生了重大跌倒。显然，人工智能对医疗保健的影响将是深远的。
- en: 'The value of AI in healthcare is not in replacing physicians and other healthcare
    workers, but rather to augment their activities. AI has the potential to support
    healthcare workers throughout a patient''s journey and to assist healthcare workers
    in discovering insights into a patient''s wellbeing using data. According to experts,
    AI in healthcare will see the most growth in the following areas:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在医疗保健中的价值不在于取代医生和其他医疗工作者，而在于增强他们的工作。人工智能有潜力在患者整个就医过程中支持医疗工作者，并通过数据帮助医疗工作者发现有关患者健康的见解。专家认为，人工智能在医疗保健领域的增长将集中在以下几个领域：
- en: '![](img/90889624-341c-4fa6-80dd-ded51086ff81.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90889624-341c-4fa6-80dd-ded51086ff81.png)'
- en: Automated diagnosis
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化诊断
- en: Let's zoom in on automated diagnosis as that is the area of concern for this
    project. Experts believe that AI will greatly augment the way medical diagnosis
    is conducted. At the moment, most medical diagnosis is performed by skilled medical
    experts. In the case of medical diagnosis through images (such as X-rays and MRI
    scans), skilled radiologists are required to provide their expertise in the diagnostic
    process. These skilled medical professionals go through years of rigorous training
    before being certified, and there is a shortage of these medical experts in certain
    countries, which contributes to poor outcomes. The role of AI is to augment these
    experts and to offload low-level routine diagnosis, which can be done by an AI
    agent with a high degree of accuracy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们聚焦于自动化诊断，因为这是本项目关注的领域。专家们认为，人工智能将大大增强医学诊断的方式。目前，大多数医学诊断是由熟练的医疗专家进行的。在影像学诊断（如X光和MRI扫描）中，需要熟练的放射科医生提供诊断专业知识。这些经验丰富的医疗专家需要经过多年严格的培训才能获得认证，且某些国家的这类专家稀缺，导致了较差的诊疗效果。人工智能的作用是增强这些专家的能力，并卸载低级的日常诊断工作，这些工作可以由人工智能高效且准确地完成。
- en: This ties back to our original problem statement; using AI to predict which
    patients are at risk of diabetes. As we shall see, we can use machine learning
    and neural networks to make this prediction. In this chapter, we will design and
    implement an MLP that can predict the onset of diabetes using machine learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们最初的问题陈述相呼应：使用人工智能预测哪些患者有糖尿病风险。正如我们将看到的，我们可以使用机器学习和神经网络来进行这一预测。在本章中，我们将设计并实现一个多层感知器（MLP），利用机器学习来预测糖尿病的发生。
- en: The diabetes mellitus dataset
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 糖尿病数据集
- en: The dataset that we will be using for this project comes from the Pima Indians Diabetes
    dataset, as provided by the National Institute of Diabetes and Digestive and Kidney
    Diseases (and hosted by Kaggle).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于本项目的数据集来自库马印第安人糖尿病数据集，由美国国家糖尿病、消化与肾脏疾病研究所提供（并由Kaggle托管）。
- en: The Pima Indians are a group of native Americans living in Arizona, and they
    are a highly studied group of people due to their genetic predisposition to diabetes.
    It is believed that the Pima Indians carry a gene that allows them to survive
    long periods of starvation. This thrifty gene allowed the Pima Indians to store
    in their bodies whatever glucose and carbohydrates they may eat, which is genetically advantageous
    in an environment where famines were common.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 库马印第安人是生活在亚利桑那州的一群美洲土著人，由于他们遗传上容易患糖尿病，这群人得到了大量的研究。人们认为，库马印第安人携带一种基因，使得他们能够在长时间饥荒中生存。这种节俭基因使得库马印第安人能够将他们摄入的葡萄糖和碳水化合物储存在体内，这在经常发生饥荒的环境中具有遗传优势。
- en: However, as society modernized and the Pima Indians began to change their diet
    to one of processed food, the rate of type 2 diabetes among them began to increase
    as well. Today, the incidence of type 2 diabetes among the Pima Indians is the
    highest in the world. This makes them a highly studied group of people, as researchers
    attempt to find the genetic link of diabetes among the Pima Indians.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着社会的现代化，皮马印第安人开始改变饮食，转向加工食品，他们的2型糖尿病发病率也开始上升。如今，皮马印第安人中2型糖尿病的发病率是世界最高的。这使得他们成为一个高度研究的群体，研究人员试图找到皮马印第安人中糖尿病的遗传关联。
- en: The Pima Indians diabetes dataset consists of diagnostic measurements collected
    from a sample of female Pima Indians, along with a label indicating whether the
    patient developed diabetes within five years of the initial measurement. In the
    next section, we'll perform exploratory data analysis on the Pima Indians diabetes
    dataset to uncover important insights about the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 皮马印第安人糖尿病数据集包含从一组女性皮马印第安人样本中收集的诊断测量数据，以及一个标签，指示患者是否在初次测量后的五年内发展为糖尿病。在下一部分中，我们将对皮马印第安人糖尿病数据集进行探索性数据分析，以揭示数据中的重要洞察。
- en: Exploratory data analysis
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'Let''s dive into the dataset to understand the kind of data we are working
    with. We import the dataset into pandas:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入数据集，以了解我们正在处理的数据类型。我们将数据集导入 pandas：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s take a quick look at the first five rows of the dataset by calling the
    `df.head()` command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看数据集的前五行，通过调用`df.head()`命令：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We get the following output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/8f948292-21c1-42bc-9e9f-f44b34b89f5d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f948292-21c1-42bc-9e9f-f44b34b89f5d.png)'
- en: 'It looks like there are nine columns in the dataset, which are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来数据集有九列，具体如下：
- en: '`Pregnancies`: Number of previous pregnancies'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pregnancies`：之前怀孕的次数'
- en: '`Glucose`: Plasma glucose concentration'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Glucose`：血浆葡萄糖浓度'
- en: '`BloodPressure`: Diastolic blood pressure'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BloodPressure`：舒张压'
- en: '`SkinThickness`: Skin fold thickness measured from the triceps'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SkinThickness`：从三头肌测量的皮肤褶皱厚度'
- en: '`Insulin` : Blood serum insulin concentration'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Insulin`：血清胰岛素浓度'
- en: '`BMI`: Body mass index'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BMI`：身体质量指数'
- en: '`DiabetesPedigreeFunction`: A summarized score that indicates the genetic predisposition
    of the patient for diabetes, as extrapolated from the patient''s family record
    for diabetes'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DiabetesPedigreeFunction`：一个总结性的分数，表示患者的糖尿病遗传易感性，基于患者的家族糖尿病记录推算得出'
- en: '`Age`: Age in years'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Age`：年龄（以年为单位）'
- en: '`Outcome`: The target variable we are trying to predict, `1` for patients that
    developed diabetes within five years of the initial measurement, and `0` otherwise'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Outcome`：我们试图预测的目标变量，`1`表示患者在初次测量后的五年内发展为糖尿病，`0`表示其他情况'
- en: 'Let''s start by visualizing the distribution of the nine variables in the dataset.
    We can do this by plotting a histogram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从可视化数据集中的九个变量的分布开始。我们可以通过绘制直方图来实现：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We get the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/5a73eaea-dfa6-4131-954b-2163e03e0476.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a73eaea-dfa6-4131-954b-2163e03e0476.png)'
- en: The histogram provides some interesting insights into the data. From the histogram
    for `Age`, we can see that most of the data was collected from young people, with
    the most common age group between 20-30 years old. We can also see that the distribution
    for `BMI`, `BloodPressure`, and `Glucose` concentration is normally distributed
    (that is, a bell curve shape), which is what we we expect when we collect such
    statistics from a population. However, note that the tail of the `Glucose` concentration
    distribution shows some rather extreme values. It appears that there are people
    with plasma `Glucose` concentration that is almost 200\. On the opposite end of
    the distribution, we can see that there are people with 0 values for `BMI`, `BloodPressure`,
    and `Glucose`. Logically, we know that it is not possible to have a 0 value for
    these measurements. Are these missing values? We shall explore more in the next
    section on data preprocessing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图提供了一些关于数据的有趣洞察。从`Age`的直方图中，我们可以看到大多数数据来自年轻人，最常见的年龄段是在20至30岁之间。我们还可以看到，`BMI`、`BloodPressure`和`Glucose`浓度的分布呈正态分布（即钟形曲线），这也是我们在从一个群体中收集这些统计数据时所期望的。然而，注意到`Glucose`浓度的分布尾部出现了一些极端值。看起来有些人的血浆`Glucose`浓度接近200。分布的另一端，我们可以看到有些人`BMI`、`BloodPressure`和`Glucose`的值为0。从逻辑上讲，我们知道这些测量值不可能为0。这些是缺失值吗？我们将在下一部分的数据预处理章节中进一步探讨。
- en: If we look at the distribution for the number of previous `Pregnancies`, we
    can see some outliers as well. We can see that some patients had more than 15
    previous pregnancies. While that may not be entirely surprising, we should keep
    such outliers in mind when we do our analysis, as it can skew our results.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 `Pregnancies`（怀孕次数）这一变量的分布，我们也可以看到一些异常值。我们可以看到有些患者曾有超过 15 次的怀孕历史。虽然这可能并不完全令人惊讶，但在分析时我们应考虑这些异常值，因为它们可能会影响我们的结果。
- en: The distribution of outcome shows that approximately 65% of the population belongs
    to class 0 (no diabetes), while the remaining 35% belongs to class 1 (diabetes).
    When building a machine learning classifier, we should always keep in mind the
    distribution of classes in our training data. In order to ensure that our machine
    learning classifier works well in the real world, we should ensure that the distribution
    of classes in our training data mirrors that of the real world. In this case,
    the distribution of the classes does not match those in the real world, as it
    is estimated by the **World Health Organization** (**WHO**) that only 8.5% of
    the world population suffers from diabetes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果分布显示，大约 65% 的人群属于类别 0（没有糖尿病），而其余 35% 属于类别 1（糖尿病）。在构建机器学习分类器时，我们应该始终牢记训练数据中类别的分布。为了确保我们的机器学习分类器在现实世界中表现良好，我们应该确保训练数据中的类别分布与现实世界相匹配。在这种情况下，类别的分布与现实世界并不匹配，因为据**世界卫生组织**（**WHO**）估计，全球仅有
    8.5% 的人口患有糖尿病。
- en: We do not need to worry about the distribution of classes in our training data
    for this project, as we are not going to deploy our classifier in the real world.
    Nevertheless, it is a good practice for data scientists and machine learning engineers
    to check the distribution of classes in the training data, in order to ensure
    strong model performance in the real world.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们不需要担心训练数据中类别的分布，因为我们并不会将分类器部署到现实世界中。然而，对于数据科学家和机器学习工程师来说，检查训练数据中类别的分布是一种良好的做法，以确保模型在现实世界中的表现。
- en: Lastly, it is important to note that the variables are on different scales.
    For example, the `DiabetesPedigreeFunction` variable ranges from 0 to ~2.5, while
    the `Insulin` variable ranges from 0 to ~800\. This difference in scale can cause
    problems in training our neural network, as variables with larger scales tend
    to dominate variables with smaller scales. In the next section on data preprocessing,
    we will look at how we can standardize the variables.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是要注意这些变量的尺度不同。例如，`DiabetesPedigreeFunction` 变量的范围是从 0 到 ~2.5，而 `Insulin`
    变量的范围是从 0 到 ~800。尺度的差异可能会导致训练神经网络时出现问题，因为尺度较大的变量往往会主导尺度较小的变量。在接下来的数据预处理部分，我们将讨论如何对变量进行标准化。
- en: We can also plot a density plot to investigate the relationship between each
    variable and the target variable. To do so, we will use seaborn. seaborn is a
    Python data visualization library based on matplotlib.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制密度图来调查每个变量与目标变量之间的关系。为此，我们将使用 seaborn。seaborn 是一个基于 matplotlib 的 Python
    数据可视化库。
- en: 'The following code snippet shows how to plot a density plot for each variable.
    To visualize the difference in distribution between diabetics and non-diabetics,
    we will also plot them separately on each plot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何为每个变量绘制密度图。为了可视化糖尿病患者与非糖尿病患者之间分布的差异，我们还将在每个图上分别绘制它们：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We''ll get the output shown in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到如下截图所示的输出：
- en: '![](img/f2b68949-1ee2-47e3-ab61-844dd4106287.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2b68949-1ee2-47e3-ab61-844dd4106287.png)'
- en: 'The following screenshot shows the output in continuation to the preceding
    one:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示的是接续前一个截图的输出：
- en: '![](img/a7258813-1cf1-476d-99c4-133d3bd8112b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7258813-1cf1-476d-99c4-133d3bd8112b.png)'
- en: The preceding density plots look complicated, but let's focus on each individual
    plot and see what insights can we gain. If we look at the plot for the `Glucose` variable,
    we can see that among the non-diabetics (solid line), the curve has a normal distribution
    centered around the value 100\. This tells us that among non-diabetics, most people
    have a blood glucose value of 100 mg/dL. On the other hand, if we look at the
    `Diabetics` (dashed line), the curve is wider and is centered around a value of
    150\. This tells us that diabetics tends to have a wider range of blood glucose
    value, and the average blood glucose value is around 150 mg/dL. Therefore, there
    is a significant difference in blood glucose values for diabetes vs non-diabetics. 
    A similar analysis can also be made for the variable `BMI` and `Age`. In other
    words, the `Glucose`, `BMI`, and `Age` variables are strong predictors for diabetes.
    People with diabetes tend to have higher blood glucose level, higher BMI, and
    are older.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的密度图看起来比较复杂，但让我们集中分析每个单独的图，看看能从中获得哪些洞察。如果我们查看`Glucose`变量的图，我们可以看到，在非糖尿病患者（实线）中，曲线呈正态分布，集中在值100左右。这告诉我们，在非糖尿病患者中，大多数人的血糖值为100
    mg/dL。另一方面，如果我们查看糖尿病患者（虚线），曲线较宽，集中在值150左右。这告诉我们，糖尿病患者的血糖值范围较宽，且平均血糖值大约为150 mg/dL。因此，糖尿病与非糖尿病患者之间的血糖值有显著差异。类似的分析也可以用于`BMI`和`Age`变量。换句话说，`Glucose`、`BMI`和`Age`变量是糖尿病的强预测因子。糖尿病患者往往有更高的血糖值、更高的BMI，并且年纪较大。
- en: On the other hand, we can see that for variables such as `BloodPressure` and
    `SkinThickness`, there is no significant difference in the distribution between
    diabetics and non-diabetics. The two groups of people tend to have similar blood
    pressure and skin thickness values. Therefore, `BloodPressure` and `SkinThickness` are
    poorer predictors for diabetes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以看到，对于像`BloodPressure`和`SkinThickness`这样的变量，糖尿病患者和非糖尿病患者在分布上没有显著差异。这两组人群的血压和皮肤厚度值相似。因此，`BloodPressure`和`SkinThickness`是糖尿病预测的较差指标。
- en: Data preprocessing
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: In the previous section, *Exploratory data analysis*, we have discovered that
    there are 0 values in certain columns, which indicates missing values. We have
    also seen that the variables have different scales, which can negatively impact
    model performance. In this section, we will perform data preprocessing to handle
    these issues.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节*探索性数据分析*中，我们发现某些列中存在`0`值，表明存在缺失值。我们还看到这些变量的尺度不同，这可能会对模型性能产生负面影响。在本节中，我们将进行数据预处理以处理这些问题。
- en: Handling missing values
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'First, let''s call the `isnull()` function to check whether there are any missing
    values in the dataset:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们调用`isnull()`函数来检查数据集中是否有缺失值：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll see the following output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](img/fb03262d-f2ea-4a91-95e9-040c8af8ad6b.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb03262d-f2ea-4a91-95e9-040c8af8ad6b.png)'
- en: 'It seems like there are no missing values in the dataset, but are we sure?
    Let''s get a statistical summary of the dataset to investigate further:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来数据集中没有缺失值，但我们确定吗？让我们获取数据集的统计摘要来进一步调查：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/3802ada8-f2b4-427d-b5c6-da3f14ce1010.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3802ada8-f2b4-427d-b5c6-da3f14ce1010.png)'
- en: We can see that there are `768` rows of data, and the `Pregnancies`, `Glucose`,
    `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` columns have a minimum
    value of `0`. This doesn't quite make sense. The measurements for `Glucose`, `BloodPressure`,
    `SkinThickness`, `Insulin`, and `BMI` should never be `0`. This is an indication
    that there are missing values in our dataset. The values were probably recorded
    as `0` due to certain issues during data collection. Perhaps the equipment was
    faulty, or the patient was unwilling to have their measurements taken.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据集中有`768`行数据，且`Pregnancies`、`Glucose`、`BloodPressure`、`SkinThickness`、`Insulin`和`BMI`列的最小值为`0`。这不太合理。`Glucose`、`BloodPressure`、`SkinThickness`、`Insulin`和`BMI`的测量值不应为`0`。这表明数据集中存在缺失值。值被记录为`0`可能是由于数据收集过程中出现了一些问题。也许设备故障，或者患者不愿意接受测量。
- en: 'In any case, we need to handle these `0` values. Let''s take a look at how
    many `0` values are there in each column to understand the extent of the problem:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们需要处理这些`0`值。让我们看一下每列中有多少`0`值，以了解问题的严重程度：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the following result:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![](img/fd47ea3c-fd73-48bc-a01d-aa3a51c52f9a.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd47ea3c-fd73-48bc-a01d-aa3a51c52f9a.png)'
- en: In the `Insulin` column, there are `374` rows with `0` values. That is almost
    half of the data that we have! Clearly, we cannot discard these rows with `0`
    values as that will cause a significant drop in model performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Insulin`列中，有`374`行值为`0`。这几乎占据了我们数据的一半！显然，我们不能丢弃这些`0`值的行，因为那样会导致模型性能显著下降。
- en: 'There are several techniques to handle these missing values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以处理这些缺失值：
- en: Remove (discard) any rows with missing values.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除（丢弃）任何包含缺失值的行。
- en: Replace the missing values with the mean/median/mode of the non-missing values.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用非缺失值的均值/中位数/众数来替换缺失值。
- en: Predict the actual values using a separate machine learning model.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个单独的机器学习模型预测实际值。
- en: Since the missing values comes from continuous variables such as `Glucose`,
    `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI`, we will replace the missing
    values with the mean of the non-missing values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺失值来自于连续变量，如`Glucose`、`BloodPressure`、`SkinThickness`、`Insulin`和`BMI`，我们将用非缺失值的均值来替换缺失值。
- en: 'First, let''s replace the `0` values in the `Glucose`, `BloodPressure`, `SkinThickness`,
    `Insulin`, and `BMI` columns with `NaN`. This way, pandas will understand that
    these values are invalid:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将`Glucose`、`BloodPressure`、`SkinThickness`、`Insulin`和`BMI`列中的`0`值替换为`NaN`。这样，pandas就能理解这些值是无效的：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let''s confirm that the `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`,
    and `BMI` columns no longer contain `0` values:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们确认`Glucose`、`BloodPressure`、`SkinThickness`、`Insulin`和`BMI`列中不再包含`0`值：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We get the following result:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '![](img/028c2dfd-94bf-4def-a37e-d2949e70e9cc.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/028c2dfd-94bf-4def-a37e-d2949e70e9cc.png)'
- en: Note that we did not modify the `Pregnancies` column as `0` values in that column
    (that is, `0` previous pregnancies) are perfectly valid.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有修改`Pregnancies`列，因为该列中的`0`值（即没有怀孕）是完全有效的。
- en: 'Now, let''s replace the `NaN` values with the mean of the non-missing values.
    We can do this using the handy `fillna()` function in pandas:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将`NaN`值替换为非缺失值的均值。我们可以使用pandas中便捷的`fillna()`函数来完成此操作：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Data standardization
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Data standardization is another important technique in data preprocessing. The
    goal of data standardization is to transform the numeric variables so that each
    variable has zero mean and unit variance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化是数据预处理中的另一项重要技术。数据标准化的目标是将数值变量转换，使得每个变量的均值为0，方差为1。
- en: Standardization of variables as a preprocessing step is a requirement for many
    machine learning algorithms. In neural networks, it is important to standardize
    the data in order to ensure that the backpropagation algorithm works as intended.
    Another positive effect of data standardization is that it shrinks the magnitude
    of the variables, transforming them to a scale that is more proportional.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预处理步骤的变量标准化是许多机器学习算法的要求。在神经网络中，标准化数据非常重要，以确保反向传播算法按预期工作。数据标准化的另一个积极效果是它缩小了变量的量级，将它们转换为更加成比例的尺度。
- en: As we have seen earlier, variables such as `Insulin` and `DiabetesPedigreeeFunction` have
    vastly different scales; the maximum value for `Insulin` is `846` while the maximum
    value for `DiabetesPedigreeeFunction` is only `2.42`. With such different scales,
    the variable with the greater scale tends to dominate when training the neural
    network, causing the neural network to inadvertently place more emphasis on the
    variable with a greater scale.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，像`Insulin`和`DiabetesPedigreeeFunction`这样的变量具有非常不同的量纲；`Insulin`的最大值为`846`，而`DiabetesPedigreeeFunction`的最大值仅为`2.42`。在如此不同的量纲下，量纲较大的变量在训练神经网络时往往会占主导地位，从而导致神经网络无意中对量纲较大的变量给予更多的关注。
- en: 'To standardize our data, we can use the `preprocessing` class from scikit-learn.
    Let''s import the `preprocessing` class from scikit-learn and use it to scale
    our data:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化数据，我们可以使用来自scikit-learn的`preprocessing`类。让我们从scikit-learn导入`preprocessing`类，并使用它来对数据进行缩放：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Since the object returned by the `preprocessing.scale()` function is no longer
    a pandas DataFrame, let''s convert it back:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`preprocessing.scale()`函数返回的对象不再是pandas DataFrame，我们需要将其转换回：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Lastly, since we do not want to scale the `Outcome` column (which is the target
    variable that we are trying to predict) let''s use the original `Outcome` column:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于我们不想对`Outcome`列（即我们要预测的目标变量）进行标准化，因此我们将使用原始的`Outcome`列：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s take a look at the mean, standard deviation and the max of each of the
    transformed variables:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看每个转换后变量的均值、标准差和最大值：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get the following result:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![](img/09b7c567-5add-4a81-a9b0-69fcec4b1cb7.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09b7c567-5add-4a81-a9b0-69fcec4b1cb7.png)'
- en: We can see that the scale of each variable is now a lot closer to one another.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每个变量的尺度现在更接近彼此。
- en: Splitting the data into training, testing, and validation sets
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据分为训练集、测试集和验证集
- en: 'The last step in data preprocessing is to split the data into training, testing,
    and validation sets:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的最后一步是将数据分为训练集、测试集和验证集：
- en: '**Training set**: The neural network will be trained on this subset of the
    data.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：神经网络将在这个数据子集上进行训练。'
- en: '**Validation set**: This set of data allows us to perform hyperparameter tuning
    (that is, tuning the number of hidden layers) using an unbiased source of data.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：这一数据集允许我们使用无偏的数据源进行超参数调优（即调节隐藏层的数量）。'
- en: '**Testing set**: The final evaluation of the neural network will be based on
    this subset of the data.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：神经网络的最终评估将基于这个数据子集。'
- en: The purpose of splitting the data into training, testing, and validation sets
    is to avoid overfitting and to provide an unbiased source of data for evaluating
    model performance. Typically, we will use the training and validation set to tune
    and improve our model. The validation set can be used for early stopping of training,
    that is, we continue to train our neural network only to the point where model
    performance on the validation set stops improving. This allows us to avoid overfitting
    the neural network.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为训练集、测试集和验证集的目的是避免过拟合，并为评估模型性能提供一个无偏的数据来源。通常，我们会使用训练集和验证集来调整和改进我们的模型。验证集可以用于训练的早停，即我们只在验证集上的模型性能停止提高时继续训练神经网络。这可以帮助我们避免神经网络的过拟合。
- en: The testing set is also known as the holdout dataset, as the neural network
    will never be trained using it. Instead, we will use the testing set to evaluate
    the model at the end. This provides us with an accurate reflection of the real-world
    performance of our model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集也被称为保留数据集，因为神经网络永远不会使用它进行训练。相反，我们将在最后使用测试集来评估模型。这为我们提供了一个准确反映模型在实际世界中表现的标准。
- en: How do we decide the proportion of each split? The competing concerns, in this
    case, is that if we allocate most of the data for training purposes, model performance
    will increase at the detriment of our ability to avoid overfitting. Similarly,
    if we allocate most of the data for validation and testing purposes, model performance
    will decrease as there might be insufficient data for training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何决定每个分割的比例？在这种情况下，竞争的关注点是，如果我们将大部分数据分配用于训练，模型的性能会提升，但可能会牺牲我们避免过拟合的能力。同样，如果我们将大部分数据分配用于验证和测试，模型性能会下降，因为可能没有足够的数据用于训练。
- en: 'As a general rule of thumb, we should split the original data into 80% training
    and 20% testing, and then to split the training data into 80% training and 20%
    validation again. The following diagram illustrates this process:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一般经验法则是，我们应将原始数据分割为80%的训练集和20%的测试集，然后将训练数据再分割为80%的训练集和20%的验证集。下图展示了这一过程：
- en: '![](img/537a32d0-a0c4-45ff-bb2f-ad5c7a2b8c8d.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/537a32d0-a0c4-45ff-bb2f-ad5c7a2b8c8d.png)'
- en: One important point to note is that the splitting of data must be done at random.
    If we were to use a non-random method of splitting the data (for example, the
    first 80% of rows go to the **Training Set** and the last 20% of rows go to the
    **Testing** **Set**), we could potentially be introducing bias into our training
    and testing set. For example, the original data could be sorted in chronological
    order, so a non-random method of splitting the data could mean that our model
    is only trained on data from a certain date, which is highly biased and would
    not work as well in the real world.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的要点是，数据的分割必须是随机的。如果我们使用非随机的方法进行数据分割（例如，将前80%的行分配到**训练集**，将后20%的行分配到**测试集**），可能会引入偏差。例如，原始数据可能按时间顺序排序，因此使用非随机的分割方法可能意味着我们的模型只会在某个日期的数据上进行训练，这样的偏差非常大，并且在现实世界中表现不佳。
- en: The `train_test_split` function from scikit-learn allows us to randomly split
    a dataset easily.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_test_split`函数来自scikit-learn，它可以轻松地实现数据集的随机分割。'
- en: 'First, let''s separate the dataset into `X` (input features) and `y` (target
    variable):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将数据集分为`X`（输入特征）和`y`（目标变量）：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, make the first split to split the data into the training set (80%) and
    the testing set (20%) according to the preceding diagram:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据前面的图示，第一次拆分将数据划分为训练集（80%）和测试集（20%）：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, make the second split to create the final training set and the validation
    set:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，进行第二次拆分，创建最终的训练集和验证集：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: MLPs
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP
- en: Now that we have completed exploratory data analysis and data preprocessing,
    let's turn our attention towards designing the neural network architecture. In
    this project, we will be using MLPs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了探索性数据分析和数据预处理，接下来我们将把注意力转向神经网络架构的设计。在这个项目中，我们将使用MLP。
- en: An MLP is a class of feedforward neural network, and it distinguishes itself
    from the single-layer perceptron that we've discussed in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, by having at least one hidden layer, with each
    layer activated by a non-linear activation function. This multilayer neural network
    architecture and non-linear activation allows MLPs to produce non-linear decision
    boundaries, which is crucial in multi-dimensional real-world datasets such as
    the Pima Indians Diabetes dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: MLP是一类前馈神经网络，它与我们在[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)《机器学习与神经网络101》中讨论的单层感知器的区别在于，MLP至少有一个隐藏层，每个层都通过非线性激活函数进行激活。这样的多层神经网络架构和非线性激活函数使得MLP能够产生非线性决策边界，这在像Pima印度糖尿病数据集这样的多维真实世界数据集中至关重要。
- en: Model architecture
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'The model architecture of the MLP can be represented graphically as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的模型架构可以图示如下：
- en: '![](img/9a44666b-3e19-42d5-ad40-06a4efa666d0.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a44666b-3e19-42d5-ad40-06a4efa666d0.png)'
- en: As discussed in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, we can use an arbitrary number of hidden layers
    in our MLP. For this project, we will use two hidden layers in our MLP.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)《机器学习与神经网络101》中讨论的那样，我们可以在MLP中使用任意数量的隐藏层。对于这个项目，我们将在MLP中使用两个隐藏层。
- en: Input layer
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入层
- en: Each node in the **input layer** (illustrated by the circles in the pink rectangle)
    refers to each feature (that is, column) in the dataset. Since there are eight
    features in the Pima Indians dataset, there should be eight nodes in the input
    layer of our MLP.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**（由粉色矩形中的圆圈表示）中的每个节点代表数据集中的每个特征（即每一列）。由于Pima印度数据集中有八个特征，因此在我们的MLP的输入层中应有八个节点。'
- en: Hidden layers
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层
- en: 'The next layer after the input layer is known as a **hidden ****layer. **As
    we have seen in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, the hidden layer takes the input layer and
    applies a **non-linear activation function** to it. Mathematically, we can represent
    the function of the hidden layer as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层之后的下一层称为**隐藏层**。正如我们在[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)《机器学习与神经网络101》中所看到的，隐藏层接收输入层并应用**非线性激活函数**。从数学角度来看，我们可以将隐藏层的函数表示如下：
- en: '![](img/25203d3f-47b4-486b-ad64-302c7facf89d.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25203d3f-47b4-486b-ad64-302c7facf89d.png)'
- en: '![](img/62638abb-569a-4dbd-bf4e-9e1fa0344ad9.png) refers to the input passed
    from the previous layer, ![](img/9b29f759-2464-42fc-9031-6b3eeb8df7f3.png) refers
    to the non-linear activation function, ![](img/0a1c0ce4-c44a-4d72-88c5-7c474955aea7.png) are
    the weights, and ![](img/862211f8-85fc-4504-8046-f965be7fcc17.png) refers to the
    biases.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/62638abb-569a-4dbd-bf4e-9e1fa0344ad9.png)表示来自前一层的输入，![](img/9b29f759-2464-42fc-9031-6b3eeb8df7f3.png)表示非线性激活函数，![](img/0a1c0ce4-c44a-4d72-88c5-7c474955aea7.png)是权重，![](img/862211f8-85fc-4504-8046-f965be7fcc17.png)表示偏置。'
- en: To keep things simple, we will only use two hidden layers in our model for this
    project. Increasing the number of hidden layers tends to increase the model complexity
    and training time. For this project, two hidden layers will suffice, as we shall
    see later when we look at the model performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们在这个项目中仅使用两个隐藏层。增加隐藏层的数量往往会增加模型的复杂性和训练时间。对于这个项目来说，两个隐藏层就足够了，稍后我们将在查看模型性能时看到这一点。
- en: Activation functions
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: When designing the neural network model architecture, we also need to decide
    what activation functions to use for each layer. Activation functions have an
    important role to play in neural networks. You can think of activation functions
    as *transformers* in neural networks; they take an input value, transform the
    input value, and pass the transformed value to the next layer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计神经网络模型架构时，我们还需要决定为每一层使用哪些激活函数。激活函数在神经网络中扮演着重要角色。你可以把激活函数看作是神经网络中的*变换器*；它们接受一个输入值，变换这个输入值，并将变换后的值传递给下一层。
- en: In this project, we will use the **rectified linear unit** (**ReLU**) and the
    **sigmoid** as our activation functions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们将使用**修正线性单元**（**ReLU**）和**sigmoid**作为我们的激活函数。
- en: ReLU
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU
- en: As a general rule of thumb, ReLU is always used as the activation function for
    our intermediate hidden layers (that is, non-output layer). In 2011, it was proved
    by researchers that ReLU is superior to all previously used activation functions
    for training **deep neural networks** (**DNNs**). Today, ReLU is the most popular
    choice of activation function for DNNs, and it has become a default choice for
    activation functions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，ReLU 总是作为中间隐藏层（即非输出层）的激活函数使用。2011年，研究人员证明，ReLU 在训练**深度神经网络**（**DNNs**）方面优于所有以前使用的激活函数。如今，ReLU
    已成为 DNNs 中最受欢迎的激活函数选择，并且已成为默认的激活函数选择。
- en: 'Mathematically, we can represent ReLU as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，我们可以将 ReLU 表示如下：
- en: '![](img/f8264954-9c83-4950-847d-0305e48abd66.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8264954-9c83-4950-847d-0305e48abd66.png)'
- en: 'What the ReLU function does is to simply consider only the non-negative portion
    of the original *![](img/ffe50dd3-9cfb-411f-a82c-ca05ef032f78.png)*, and to treat
    the negative portion as *0*. The following graph illustrates this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数的作用是仅考虑原始函数的非负部分，*![](img/ffe50dd3-9cfb-411f-a82c-ca05ef032f78.png)*，并将负部分视为*0*。下图说明了这一点：
- en: '![](img/e635eb09-90bd-4f41-a000-6eb6326e29a7.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e635eb09-90bd-4f41-a000-6eb6326e29a7.png)'
- en: Sigmoid activation function
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数
- en: 'For the final output layer, we need an activation function that makes a prediction
    on the class of the label. For this project, we are making a simple binary prediction
    on the class: 1 for patients with onset of diabetes and 0 for patients without
    the onset of diabetes. The sigmoid activation function is ideal for binary classification
    problems.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最终的输出层，我们需要一个激活函数来预测标签的类别。在本项目中，我们做的是一个简单的二分类预测：1 表示患有糖尿病的患者，0 表示没有糖尿病的患者。Sigmoid
    激活函数非常适合二分类问题。
- en: 'Mathematically, we can represent the sigmoid activation function as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，我们可以将 sigmoid 激活函数表示如下：
- en: '![](img/9f9e96de-1353-4b51-b79c-39283c052647.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9e96de-1353-4b51-b79c-39283c052647.png)'
- en: 'Although this looks complicated, the underlying function is actually pretty
    simple. The **Sigmoid Activation Function** simply takes a value and squashes
    it between **0** and **1**:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来很复杂，但其底层函数实际上非常简单。**Sigmoid 激活函数**只是将一个值压缩到**0**和**1**之间：
- en: '![](img/849c7c78-1e4f-4d31-8fc0-b56464c5b3b4.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/849c7c78-1e4f-4d31-8fc0-b56464c5b3b4.png)'
- en: If the transformed value ![](img/a4e7a8d0-50e1-41cf-a819-9f40fc9c3e42.png) is
    greater than **0.5**, then we classify it as class **1**. Similarly, if the transformed
    value is less than **0.5**, we classify it as class **0**. The **Sigmoid Activation
    Function** allows us to take an input value and outputs a binary class (**1**
    or **0**), which is exactly what we require for this project (that is, to predict
    whether a person has diabetes or not).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果变换后的值 ![](img/a4e7a8d0-50e1-41cf-a819-9f40fc9c3e42.png) 大于 **0.5**，那么我们将其分类为
    **1** 类。类似地，如果变换后的值小于 **0.5**，我们将其分类为 **0** 类。**Sigmoid 激活函数**允许我们接受一个输入值，并输出一个二进制分类（**1**
    或 **0**），这正是本项目所需要的（即预测一个人是否患有糖尿病）。
- en: Model building in Python using Keras
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中使用 Keras 构建模型
- en: We're finally ready to build and train our MLP in Keras.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好在 Keras 中构建并训练我们的 MLP。
- en: Model building
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建
- en: As we mentioned in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, the `Sequential()` class in Keras allows us
    to construct a neural network like Lego, stacking layers on top of one another.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml)《机器学习与神经网络101》中提到的，Keras
    中的 `Sequential()` 类允许我们像搭积木一样构建神经网络，将各层叠加在一起。
- en: 'Let''s create a new `Sequential()` class:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的 `Sequential()` 类：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Next, let's stack our first hidden layer. The first hidden will have 32 nodes,
    and the input dimensions will be 8 (because there are 8 columns in `X_train`).
    Notice that for the very first hidden layer, we need to indicate the input dimensions.
    Subsequently, Keras will take care of the size compatibility of other hidden layers
    automatically.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们堆叠第一个隐藏层。第一个隐藏层将有32个节点，输入维度为8（因为`X_train`有8列）。注意，对于第一个隐藏层，我们需要指定输入维度。之后，Keras会自动处理其他隐藏层的尺寸兼容性。
- en: Another point to note is that we have arbitrarily decided on the number of nodes
    for the first hidden layer. This variable is a hyperparameter that should be carefully
    selected through trial and error. In this project, we will skip hyperparameter
    tuning and just use 32 as the number of nodes since it does not necessarily make
    much of a difference for this simple dataset.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的点是，我们随意决定了第一个隐藏层的节点数量。这个变量是一个超参数，应该通过反复试验来仔细选择。在这个项目中，我们跳过了超参数调优，直接使用32作为节点数量，因为对于这个简单数据集来说，它不会带来太大差异。
- en: 'Let''s add the first hidden layer:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加第一个隐藏层：
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `activation` function used is `relu`, as discussed in the previous section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的`activation`函数是`relu`，如前一节所讨论。
- en: Next, let's stack on the second hidden layer. Adding more hidden layers increases
    the complexity of our model, but can sometimes cause the model to overfit. For
    this project, we will use two hidden layers only, as that is sufficient to produce
    a satisfactory model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们堆叠第二个隐藏层。添加更多的隐藏层会增加模型的复杂度，但有时可能导致模型过拟合。对于这个项目，我们只使用两个隐藏层，因为这足以生成一个令人满意的模型。
- en: 'Let''s add our second hidden layer:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加第二个隐藏层：
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Finally, finish off the MLP by adding the output layer. This layer has only
    one single node, as we're dealing with binary classification here. The `activation`
    function used is the `sigmoid` function, and it *squashes* the output between
    0 and 1 (binary output).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过添加输出层来完成MLP。这个层只有一个节点，因为我们在做二分类任务。使用的`activation`函数是`sigmoid`函数，它将输出值*压缩*在0和1之间（二值输出）。
- en: 'Now we add the output layer as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们按照如下方式添加输出层：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Model compilation
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型编译
- en: Before we start training our model, we need to define the parameters of the
    training process, which is done via the `compile` method.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练模型之前，我们需要定义训练过程的参数，这可以通过`compile`方法来完成。
- en: 'There are three different parameters we need to define for the training process:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们需要定义三个不同的参数：
- en: '**Optimizer**: Let''s use the `adam` optimizer, which is a popular optimizer
    in Keras. For most datasets, the `adam` optimizer will work well without much
    tuning.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：我们使用`adam`优化器，它是Keras中常用的优化器。对于大多数数据集，`adam`优化器通常无需过多调节就能良好运行。'
- en: '**Loss function**: We''ll use `binary_crossentropy` as our `loss` function
    since the problem at hand is a binary classification problem.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：我们将使用`binary_crossentropy`作为我们的`loss`函数，因为我们面临的是一个二分类问题。'
- en: '**Metrics**: We''ll use `accuracy` (that is, the percentage of correctly classified
    samples) as our evaluation metric.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标**：我们将使用`accuracy`（即正确分类样本的百分比）作为我们的评估指标。'
- en: 'Then, we can run the `compile()` function as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按照以下方式运行`compile()`函数：
- en: '[PRE28]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Model training
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'To train our MLP model defined in earlier steps, let''s call the `fit` function.
    Let''s train our model for `200` iterations:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们在前面步骤中定义的MLP模型，让我们调用`fit`函数。我们将训练模型`200`次迭代：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We get the following result:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '![](img/e09e058e-ecec-4ed8-a802-082d4a211bb3.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e09e058e-ecec-4ed8-a802-082d4a211bb3.png)'
- en: As we can see, the loss decreases and the accuracy increases over each epoch,
    as the learning algorithm continuously updates the weights and biases in the MLP
    according to the training data. Note that the accuracy shown in the preceding
    screenshot refers to the accuracy based on the training data. In the next section,
    we will take a look at the performance of the MLP based on the held out testing
    data, as well as some other important metrics.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，随着每个周期的进行，损失值在减少，准确率在增加，因为学习算法根据训练数据不断更新MLP中的权重和偏差。请注意，前述截图中显示的准确率是基于训练数据的。在下一节中，我们将看看基于测试数据的MLP表现以及其他一些重要指标。
- en: Results analysis
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果分析
- en: Having successfully trained our MLP, let's evaluate our model based on the testing
    accuracy, confusion matrix, and **receiver operating characteristic** (**ROC**)
    curve.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 成功训练了MLP后，让我们基于测试准确率、混淆矩阵和**接收者操作特征**（**ROC**）曲线来评估模型。
- en: Testing accuracy
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试准确度
- en: 'We can evaluate our model on the training set and testing set using the `evaluate()`
    function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`evaluate()`函数评估我们的训练集和测试集模型表现：
- en: '[PRE30]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We get the following result:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '![](img/1751887a-189c-42ad-9d5e-b18dbab8bed2.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1751887a-189c-42ad-9d5e-b18dbab8bed2.png)'
- en: The accuracy is 91.85% and 78.57% on the training set and testing set respectively.
    The difference in accuracy between the training and testing set isn't surprising
    since the model was trained on the training set. In fact, by training the model
    over more iterations, we can achieve 100% accuracy on the training set, but that
    would not be desirable as it just means that we are overfitting our model. The
    testing accuracy should always be used to evaluate the real-world performance
    of our model, as the testing set represents real-world data that the model has
    never seen before.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集和测试集上的准确率分别为91.85%和78.57%。训练集和测试集之间的准确率差异并不令人惊讶，因为模型是在训练集上进行训练的。事实上，通过多次迭代训练模型，我们可以在训练集上达到100%的准确率，但那并不理想，因为这意味着我们过拟合了模型。测试准确度应该始终用于评估我们模型的实际表现，因为测试集代表了模型之前从未见过的真实世界数据。
- en: The testing accuracy of 78.57% is pretty impressive for our simple MLP with
    just two hidden layers. What this means is that given the eight measurements from
    a new patient (glucose, blood pressure, insulin, and so on), our MLP is able to
    predict with ~80% accuracy whether that patient will develop diabetes within the
    next five years. In essence, we have developed our first AI agent!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 78.57%的测试准确率对于我们这个仅有两个隐藏层的简单MLP来说相当令人印象深刻。这意味着，给定来自一位新患者的八个测量值（如血糖、血压、胰岛素等），我们的MLP能够以约80%的准确率预测该患者是否会在未来五年内发展为糖尿病。从本质上讲，我们已经开发出了我们的第一个AI代理！
- en: Confusion matrix
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: The confusion matrix is a useful visualization tool that provides analysis on
    the true negative, false positive, false negative, and true positives made by
    our model. Beyond a simple accuracy metric, we should also look at the confusion
    matrix to understand the performance of the model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个有用的可视化工具，它提供了关于模型所做的真负例、假正例、假负例和真正例的分析。除了简单的准确度指标外，我们还应查看混淆矩阵，以了解模型的表现。
- en: 'The definition of true negative, false positive, false negative, and true positives
    are as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 真负例、假正例、假负例和真正例的定义如下：
- en: '**True negative**: Actual class is negative (no diabetes), and the model predicted
    negative (no diabetes)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例**：实际类别为负（无糖尿病），模型预测为负（无糖尿病）'
- en: '**False positive**: Actual class is negative (no diabetes), but the model predicted
    positive (diabetes)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例**：实际类别为负（无糖尿病），但模型预测为正（糖尿病）'
- en: '**False negative**: Actual class is positive (diabetes), but the model predicted
    negative (no diabetes)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例**：实际类别为正（糖尿病），但模型预测为负（无糖尿病）'
- en: '**True positive**: Actual class is positive (diabetes), and the model predicted
    positive (diabetes)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例**：实际类别为正（糖尿病），模型预测为正（糖尿病）'
- en: Clearly, we want our false positive and false negative numbers to be as low
    as possible, and for the true negative and true positive numbers to be as high
    as possible.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望假正例和假负例的数量尽可能低，而真负例和真正例的数量尽可能高。
- en: 'We can construct a confusion matrix using the `confusion``_matrix` class from
    `sklearn`, using `seaborn` for the visualization:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sklearn`中的`confusion``_matrix`类构建混淆矩阵，并使用`seaborn`进行可视化：
- en: '[PRE31]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And the result is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](img/059c6fc4-a341-4a29-9f3d-2b7f7a574261.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/059c6fc4-a341-4a29-9f3d-2b7f7a574261.png)'
- en: From the preceding confusion matrix, we can see that most predictions are true
    negatives and true positives (as indicated by the 78.57% test accuracy in the
    previous section). The remaining 19 predictions are false negatives and 14 other
    predictions are false positives, which are undesirable.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的混淆矩阵中，我们可以看到大多数预测是**真负例**和**真正例**（如上一节中提到的78.57%的测试准确率所示）。其余的19个预测为**假负例**，另有14个预测为**假正例**，这些都是不理想的。
- en: For diabetes prediction, a false negative is perhaps more damaging than a false
    positive. A false negative means telling the patient that they will not develop
    diabetes within the next five years, when in fact they would. Therefore, when
    we evaluate the performance of different models for predicting the onset of diabetes,
    a model with a lower false negative is more desirable.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于糖尿病预测来说，假阴性可能比假阳性更具破坏性。假阴性意味着告诉病人他们在接下来的五年内不会得糖尿病，然而事实上他们会得。因此，当我们评估不同模型在预测糖尿病发生方面的表现时，假阴性较少的模型更为理想。
- en: ROC curve
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC 曲线
- en: 'For classification tasks, we should also look at the ROC curve to evaluate
    our model. The ROC curve is a plot with the **True Positive Rate** (**TPR**) on
    the *y* axis and the **False Positive Rate** (**FPR**) on the *x *axis. TPR and
    FPR are defined as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，我们也应该查看 ROC 曲线来评估我们的模型。ROC 曲线是一个图表，**真正阳性率**（**TPR**）位于 *y* 轴，**假阳性率**（**FPR**）位于
    *x* 轴。TPR 和 FPR 定义如下：
- en: '![](img/0a6b9f5f-8686-4ac2-bda5-1f4914f1429d.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a6b9f5f-8686-4ac2-bda5-1f4914f1429d.png)'
- en: '![](img/78951486-57b2-4acb-b924-600aae8dc499.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78951486-57b2-4acb-b924-600aae8dc499.png)'
- en: 'When we analyze the ROC curve, we look at the **area under the curve** (**AUC**)
    to evaluate the performance of the model that produced the curve. A large AUC
    indicates that the model is able to differentiate the respective classes with
    high accuracy, while a low AUC indicates that the model makes poor, often wrong
    predictions. A ROC curve that lies on the diagonal indicates that the model does
    no better than random. The following diagram illustrates this:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们分析 ROC 曲线时，我们会查看**曲线下的面积**（**AUC**）来评估生成该曲线的模型的性能。较大的 AUC 表明模型能够高准确率地区分不同类别，而较小的
    AUC 则表明模型预测不佳，常常出错。位于对角线上的 ROC 曲线表明模型的表现与随机猜测没有区别。以下图示说明了这一点：
- en: '![](img/5b37e12b-9118-4dec-9a60-0a6391164a21.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b37e12b-9118-4dec-9a60-0a6391164a21.png)'
- en: 'Let''s plot the ROC curve for our model and analyze its performance. As always,
    scikit-learn provides a useful `roc_curve` class to help us do this. But first,
    let''s get the predicted probabilities of each class using the `predict()` function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型的 ROC 曲线并分析其性能。和往常一样，scikit-learn 提供了一个有用的 `roc_curve` 类来帮助我们实现这一点。但首先，让我们使用
    `predict()` 函数获取每个类别的预测概率：
- en: '[PRE32]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, run the `roc_curve` function in order to get the corresponding false
    positive rate and true positive rate for the ROC curve:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行 `roc_curve` 函数，以获得对应的假阳性率和真正阳性率，用于绘制 ROC 曲线：
- en: '[PRE33]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now plot the values on a plot using matplotlib:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用 matplotlib 绘制图表中的值：
- en: '[PRE34]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We get the following result:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![](img/d5ccd4e6-20e1-47b5-8ae1-3448b2931bd7.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5ccd4e6-20e1-47b5-8ae1-3448b2931bd7.png)'
- en: From the preceding **ROC Curve**, we can see that the model performs rather
    well, close to the model **ROC Curve** shown in the preceding diagram. This shows
    that our model is able to differentiate samples of different classes, making good
    predictions.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的**ROC 曲线**中，我们可以看到该模型表现相当不错，接近前面图示中的**ROC 曲线**。这表明我们的模型能够有效地区分不同类别的样本，做出准确预测。
- en: Further improvements
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步改进
- en: At this point, it is worth wondering if it is possible to further improve the
    performance of our model. How can we further improve the accuracy of our model
    and/or improve the false negative and false positive rate?
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，值得思考是否有可能进一步提高我们模型的性能。我们如何才能进一步提高模型的准确性和/或改善假阴性率和假阳性率？
- en: In general, any limitation in performance is usually due to the lack of strong
    features in the dataset, rather than the complexity of the neural network used.
    The Pima Indians Diabetes dataset only consists of eight features, and it can
    be argued that these features alone are insufficient to really predict the onset
    of diabetes.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，性能上的任何局限性通常是由于数据集中缺乏强特征，而不是所使用神经网络的复杂性。皮马印第安人糖尿病数据集仅包含八个特征，可以说单靠这些特征不足以真正预测糖尿病的发生。
- en: One way to increase the number of features we provide to the model is via **feature
    engineering**. Feature engineering is the process of using one's domain knowledge
    of the problem to create new features for the machine learning algorithm. Feature
    engineering is one of the most important aspects of data science. In fact, many
    past winners of Kaggle competitions have credited their success to feature engineering,
    and not just tuning of the machine learning model. However, feature engineering
    is a double-edged sword and must be done carefully. Adding inappropriate features
    may create noise for our machine learning model, affecting the performance of
    our model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 增加我们提供给模型的特征数量的一种方式是通过**特征工程**。特征工程是利用领域知识为机器学习算法创建新特征的过程。特征工程是数据科学中最重要的方面之一。事实上，许多过去的Kaggle竞赛获胜者将他们的成功归功于特征工程，而不仅仅是机器学习模型的调优。然而，特征工程是一把双刃剑，必须谨慎进行。添加不合适的特征可能会为我们的机器学习模型带来噪声，进而影响模型的性能。
- en: On the opposite spectrum, we may also consider removing features in order to
    improve model performance. This is known as **feature selection**. Feature selection
    is used when we believe that the original dataset contains too much noise, and
    removing the noisy features (features that are not strong predictors) may improve
    model performance. One popular way to do feature selection is to use decision
    trees.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在相反的方向上，我们也可以考虑去除某些特征以提高模型的性能。这被称为**特征选择**。当我们认为原始数据集包含太多噪声时，可以使用特征选择，去除那些噪声特征（即不强的预测特征），以改善模型性能。使用决策树是进行特征选择的常见方法。
- en: Decision trees are a separate class of machine learning models with a tree-like
    data structure. Decision trees are useful as they calculate and rank the most
    important features according to certain statistical criteria. We can first fit
    the data using the decision tree, and then use the output from the decision tree
    to remove features that are deemed unimportant, before providing the reduced dataset
    to our neural network. Again, feature selection is a double-edged sword that can
    potentially affect model performance.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一类具有树状数据结构的独立机器学习模型。决策树的优点在于它根据某些统计标准计算并排序最重要的特征。我们可以首先使用决策树拟合数据，然后利用决策树的输出去除被认为不重要的特征，最后将减少后的数据集输入神经网络。再次强调，特征选择是一把双刃剑，可能会影响模型的性能。
- en: Although feature engineering and feature selection were not done in this project,
    we will see it being used in other projects in later chapters, as we gradually
    take on more challenging problems.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本项目中没有进行特征工程和特征选择，但我们将在后续章节的其他项目中看到它们的应用，因为我们将逐步处理更具挑战性的问题。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have designed and implemented an MLP that is capable of
    predicting the onset of diabetes with ~80% accuracy.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们设计并实现了一个多层感知机（MLP），该模型能够以约80%的准确率预测糖尿病的发生。
- en: We first performed exploratory data analysis where we looked at the distribution
    of each variable, as well as the relationship between each variable and the target
    variable. We then performed data preprocessing to remove missing data and we also
    standardized our data such that each variable has a mean of 0 with unit standard
    deviation. Finally, we split our original data randomly into a training set, a
    validation set, and a testing set.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行了探索性数据分析，查看了每个变量的分布情况，以及各个变量与目标变量之间的关系。然后，我们进行了数据预处理，去除了缺失数据，并对数据进行了标准化处理，使每个变量的均值为0，标准差为1。最后，我们将原始数据随机划分为训练集、验证集和测试集。
- en: We then looked at the architecture of the MLP that we used, which consists of
    2 hidden layers, with 32 nodes in the first hidden layer and 16 nodes in the second
    hidden layer. We then implemented this MLP in Keras using the sequential model,
    which allows us to stack layers on one another. We then trained our MLP using
    the training set, where Keras used the Adam optimizer algorithm to modify the
    weights and biases in the neural network over 200 iterations, gradually improving
    model's accuracy.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们查看了我们使用的MLP的架构，该网络包含2个隐藏层，第一个隐藏层有32个节点，第二个隐藏层有16个节点。然后，我们在Keras中使用顺序模型实现了这个MLP，顺序模型允许我们将各层叠加在一起。接下来，我们使用训练集训练了我们的MLP，在200次迭代中，Keras使用Adam优化算法逐步调整神经网络中的权重和偏置，逐渐提高模型的准确性。
- en: Finally, we evaluated our model using metrics such as the testing accuracy,
    confusion matrix, and ROC curve. We saw the importance of looking at metrics such
    as false negatives and false positives when evaluating our model, and how false
    negatives and false positives are important metrics, especially for a classifier
    that predicts the onset of diabetes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用测试准确率、混淆矩阵和 ROC 曲线等指标评估了我们的模型。我们看到了在评估模型时，查看假阴性和假阳性等指标的重要性，以及假阴性和假阳性在预测糖尿病发作的分类器中特别重要。
- en: This concludes the chapter on using a simple MLP to predict the onset of diabetes.
    In the next chapter, [Chapter 3](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml),
    *Predicting Taxi Fares with Deep Feedforward Networks*, we will use a more complicated
    dataset that utilizes temporal and geolocation information to make predictions
    of taxi fares.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了使用简单的 MLP 预测糖尿病发作的章节。在下一章节，[第 3 章](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml)，*使用深度前馈网络预测出租车费用*，我们将使用一个更复杂的数据集，该数据集利用时间和地理位置信息来预测出租车费用。
- en: Questions
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How do we plot a histogram of each variable in a pandas DataFrame, and why are
    histograms useful?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何绘制 pandas DataFrame 中每个变量的直方图？为什么直方图很有用？
- en: We can plot a histogram by calling the `df.hist()` function built into a pandas
    DataFrame class. A histogram provides an accurate representation of the distribution
    of our numerical data.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用 `df.hist()` 函数来绘制直方图，该函数内置于 pandas DataFrame 类中。直方图提供了数值数据分布的准确表示。
- en: How do we check for missing values (NaN values) in a pandas DataFrame?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何检查 pandas DataFrame 中的缺失值（NaN 值）？
- en: We can call the `df.isnull().any()` function to easily check whether there are
    any null values in each column of the dataset.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用 `df.isnull().any()` 函数，轻松检查数据集中每一列是否有缺失值。
- en: Besides NaN values, what other kinds of missing values could appear in a dataset?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了 NaN 值，还有哪些类型的缺失值可能出现在数据集中？
- en: Missing values can also appear in the form of 0 values. Missing values are often
    recorded as 0 in a dataset due to certain issues during data collection—perhaps
    the equipment was faulty, or there are other issues hindering data collection.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值也可能以 0 值的形式出现。由于数据收集过程中可能出现某些问题，缺失值常常以 0 记录在数据集中——可能是设备故障，或者存在其他阻碍数据收集的问题。
- en: Why is it crucial to remove missing values in a dataset before training a neural
    network with it?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在用数据训练神经网络之前，删除数据集中的缺失值是至关重要的？
- en: Neural networks are unable to handle NaN values. Neural networks require all
    of their inputs to be numerical due to the kind of mathematical operations they
    perform during forward and back propagation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络无法处理 NaN 值。由于神经网络在前向传播和反向传播过程中执行的数学运算，所有输入都必须是数值型的。
- en: What does data standardization do, and why is it important to perform data standardization
    before training a neural network with the data?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据标准化做了什么？为什么在用数据训练神经网络之前进行数据标准化很重要？
- en: The goal of data standardization is to transform the numeric variables so that
    each variable has zero mean and unit variance. When training neural networks,
    it is important to ensure that the data has been standardized. This ensures that
    features with a larger scale does not dominate features with a smaller scale when
    training a neural network.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化的目标是将数值变量转换，使得每个变量的均值为零，方差为单位。当训练神经网络时，确保数据已经标准化非常重要。这能确保在训练神经网络时，规模较大的特征不会主导规模较小的特征。
- en: How do we split our dataset to ensure unbiased evaluation of model performance?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何拆分数据集以确保模型性能的无偏评估？
- en: Before training a neural network, we should split our dataset into a training
    set, validation set, and testing set. The neural network will be trained on the
    training set, while the validation set allows us to perform hyperparameter tuning
    using an unbiased source of data. Finally, the testing set provides an unbiased
    source of data to evaluate the performance of the neural network.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络之前，我们应该将数据集分为训练集、验证集和测试集。神经网络将在训练集上训练，而验证集允许我们使用无偏的数据源进行超参数调整。最后，测试集提供了一个无偏的数据源，用于评估神经网络的性能。
- en: What are the characteristic features of the model architecture of a MLP?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLP 模型架构的特征是什么？
- en: MLPs are feedforward neural networks, and they have at least one hidden layer,
    with each layer activated by a non-linear activation function. This multilayer
    neural network architecture and non-linear activation allows MLPs to produce non-linear
    decision boundaries.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: MLP（多层感知机）是前馈神经网络，至少有一层隐藏层，每一层都由非线性激活函数激活。这个多层神经网络架构和非线性激活使得MLP能够产生非线性的决策边界。
- en: What is the purpose of activation functions in neural networks?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中激活函数的作用是什么？
- en: Activation functions performs a non-linear transformation on the weights and
    biases before passing it to the next layer. The most popular and effective activation
    function between hidden layers is the ReLU activation function.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数对权重和偏差进行非线性转换，然后再传递到下一层。最常见且有效的隐藏层激活函数是ReLU激活函数。
- en: What is a suitable loss function to use when training our neural network for
    a binary classification problem?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练神经网络进行二分类问题时，应该使用什么样的损失函数？
- en: The binary cross entropy is the most appropriate loss function to use when training
    our neural network for a binary classification problem.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵是训练神经网络进行二分类问题时最合适的损失函数。
- en: What does a confusion matrix represent, and how can we use it to evaluate the
    performance of our neural network?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵代表什么，我们如何利用它来评估神经网络的性能？
- en: A confusion matrix provides values on the true negative, false positive, false
    negative, and true positives made by our neural network. Beyond a simple accuracy
    metric, the confusion matrix allows us to drill down into the kind of mistakes
    made by our neural network (false positives and false negatives).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵提供了我们神经网络在真正负类、假正类、假负类和真正正类上的值。除了简单的准确性指标外，混淆矩阵还可以让我们深入了解神经网络所犯的错误类型（假正类和假负类）。
