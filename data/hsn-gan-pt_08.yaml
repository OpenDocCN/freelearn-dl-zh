- en: Image-to-Image Translation and Its Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像到图像翻译及其应用
- en: 'In this chapter, we will push the label-based image generation to the next
    level: we will use pixel-wise labeling to perform image-to-image translation and
    transfer image styles.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将把基于标签的图像生成推向一个新水平：我们将使用像素级标注来执行图像到图像的翻译，并传输图像风格。
- en: You will learn how to use pixel-wise label information to perform image-to-image
    translation with pix2pix and translate high-resolution images with pix2pixHD.
    Following this, you will learn how to perform style transfer between unpaired
    image collections with CycleGAN.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学习如何使用像素级标签信息来执行pix2pix的图像到图像翻译，并使用pix2pixHD来翻译高分辨率图像。随后，您将学习如何在未成对的图像集合之间执行风格转移与CycleGAN。
- en: By the end of this chapter, combined with the knowledge from the previous chapter,
    you will have grasped the core methodology of using image-wise and pixel-wise
    label information to improve the quality, or manipulate the attributes, of generated
    images. You will also know how to flexibly design model architectures to accomplish
    your goals, including generating larger images or transferring textures between
    different styles of images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章末，结合前一章的知识，您将掌握使用基于图像和像素级标签信息来提高生成图像质量或操作生成图像属性的核心方法论。您还将学会如何灵活设计模型架构以实现您的目标，包括生成更大的图像或在不同风格的图像之间传输纹理。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Using pixel-wise labels to translate images with pix2pix
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用像素级标签来执行与pix2pix的图像翻译
- en: Pix2pixHD – high-resolution image translation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pix2pixHD – 高分辨率图像翻译
- en: CycleGAN – image-to-image translation from unpaired collections
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN – 从未成对的集合进行图像到图像的翻译
- en: Using pixel-wise labels to translate images with pix2pix
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用像素级标签来执行与pix2pix的图像翻译
- en: In the previous chapter, we learned how to use auxiliary information such as
    labels and attributes to improve the quality of images that are generated by GANs.
    The labels we used in the previous chapter were image-wise, which means that each
    image has only one or several labels. Labels can be assigned to specific pixels,
    which are known as pixel-wise labels. Pixel-wise labels are playing an increasingly
    important role in the realm of deep learning. For example, one of the most famous
    online image classification contests, the **ImageNet Large Scale Visual Recognition
    Challenge** (**ILSVRC**, [http://www.image-net.org/challenges/LSVRC](http://www.image-net.org/challenges/LSVRC/)),
    is no longer being hosted since its last event in 2017, whereas object detection
    and segmentation challenges such as COCO ([http://cocodataset.org](http://cocodataset.org))
    are receiving more attention.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用标签和属性等辅助信息来提高GAN生成的图像质量。我们在上一章中使用的标签是基于图像的，这意味着每个图像只有一个或几个标签。标签可以分配给特定的像素，这些像素被称为像素级标签。像素级标签在深度学习领域中的作用越来越重要。例如，最著名的在线图像分类比赛之一，**ImageNet大规模视觉识别挑战**（**ILSVRC**，[http://www.image-net.org/challenges/LSVRC](http://www.image-net.org/challenges/LSVRC/)），自2017年最后一次活动后不再举办，而像COCO（[http://cocodataset.org](http://cocodataset.org)）这样的对象检测和分割挑战却越来越受到关注。
- en: An iconic application of pixel-wise labeling is semantic segmentation. **Semantic
    segmentation** (or image/object segmentation) is a task in which every pixel in
    the image must belong to one object. The most promising application of semantic
    segmentation is autonomous cars (or self-driving cars). If each and every pixel
    that's captured by the camera that's mounted on the self-driving car is correctly
    classified, all of the objects in the image will be easily recognized, which makes
    it much easier for the vehicle to properly analyze the current environment and
    make the right decision upon whether it should, for example, turn or slow down
    to avoid other vehicles and pedestrians. To understand more about semantic segmentation,
    please refer to the following link: [https://devblogs.nvidia.com/image-segmentation-using-digits-5](https://devblogs.nvidia.com/image-segmentation-using-digits-5).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级标注的一个标志性应用是语义分割。**语义分割**（或图像/对象分割）是一个任务，其中图像中的每个像素必须属于一个对象。语义分割最有前途的应用是自动驾驶汽车。如果自动驾驶汽车上安装的摄像头捕获的每个像素都被正确分类，图像中的所有对象都将容易被识别，这使得车辆能够更轻松地分析当前环境并做出正确的决策，例如转弯或减速以避开其他车辆和行人。要了解更多关于语义分割的信息，请参考以下链接：[https://devblogs.nvidia.com/image-segmentation-using-digits-5](https://devblogs.nvidia.com/image-segmentation-using-digits-5)。
- en: Transforming the original color image into a segmentation map (as shown in the
    following diagram) can be considered as an image-to-image translation problem,
    which is a much larger field and includes style transfer, image colorization,
    and more. Image **style transfer** is about moving the iconic textures and colors
    from one image to another, such as combining your photo with a Vincent van Gogh
    painting to create a unique artistic portrait of you. **Image colorization** is
    a task where we feed a 1-channel grayscale image to the model and let it predict
    the color information for each pixel, which leads to a 3-channel color image.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始彩色图像转换为分割图（如下图所示）可以视为图像到图像的翻译问题，这是一个更广泛的领域，包括风格迁移、图像上色等。**图像风格迁移**是指将一种图像中的标志性纹理和颜色转移到另一张图像上，例如将你的照片与文森特·梵高的画作结合，创作一幅独特的艺术肖像。**图像上色**是一个任务，我们将一张单通道的灰度图像输入模型，让它预测每个像素的颜色信息，从而得到一张三通道的彩色图像。
- en: GANs can be used in image-to-image translation as well. In this section, we
    will use a classic image-to-image translation model, pix2pix, to transform images
    from one domain to another. Pix2pix was proposed by Phillip Isola, Jun-Yan Zhu,
    and Tinghui Zhou, et. al. in their paper *Image-to-Image Translation with Conditional
    Adversarial Networks*. Pix2pix was designed to learn of the connections between
    paired collections of images, for example, transforming an aerial photo taken
    by a satellite into a regular map, or a sketch image into a color image, and vice
    versa.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GANs也可以用于图像到图像的翻译。在这一节中，我们将使用经典的图像到图像翻译模型pix2pix，将图像从一个领域转换到另一个领域。Pix2pix由Phillip
    Isola、Jun-Yan Zhu、Tinghui Zhou等人在他们的论文《*带条件对抗网络的图像到图像翻译*》中提出。Pix2pix的设计旨在学习成对图像集合之间的关系，例如将卫星拍摄的航拍图像转换为普通地图，或者将素描图像转换为彩色图像，反之亦然。
- en: The authors of the paper have kindly provided the full source code for their
    work, which runs perfectly on PyTorch 1.3. The source code is also well organized.
    Therefore, we will use their code directly in order to train and evaluate the
    pix2pix model and learn how to organize our models in a different way.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者友好地提供了他们工作的完整源代码，该代码在PyTorch 1.3上运行完美。源代码也很好地组织起来。因此，我们将直接使用他们的代码来训练和评估pix2pix模型，并学习如何以不同的方式组织我们的模型。
- en: 'First, open a Terminal and download the code for this section using the following
    command. This is also available under the `pix2pix` directory in this chapter''s
    code repository:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开一个终端并使用以下命令下载本节的代码。此代码也可以在本章代码库中的`pix2pix`目录下找到：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, `install` the prerequisites to be able to visualize the results during
    training:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`安装`必要的前提条件，以便在训练过程中能够可视化结果：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Generator architecture
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器架构
- en: 'The architecture of the generator network of pix2pix is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix生成器网络的架构如下：
- en: '![](img/6b0e72d5-fb06-4e4f-9642-65a97cfe2f52.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b0e72d5-fb06-4e4f-9642-65a97cfe2f52.png)'
- en: Generator architecture of pix2pix
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix的生成器架构
- en: Here, we assume that both the input and output data are 3-channel 256x256 images. In
    order to illustrate the generator structure of pix2pix, feature maps are represented
    by colored blocks and convolution operations are represented by gray and blue
    arrows, in which gray arrows are convolution layers for reducing the feature map
    sizes and blue arrows are for doubling the feature map sizes. Identity mapping
    (including skip connections) is represented by black arrows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设输入和输出数据都是三通道的256x256图像。为了说明pix2pix的生成器结构，特征图用彩色块表示，卷积操作用灰色和蓝色箭头表示，其中灰色箭头表示用于减少特征图大小的卷积层，蓝色箭头表示用于加倍特征图大小的卷积层。身份映射（包括跳跃连接）用黑色箭头表示。
- en: We can see that the first half layers of this network gradually transform the
    input image into 1x1 feature maps (with wider channels) and the last half layers
    transform these very small feature maps into an output image with the same size
    of the input image. It compresses the input data into much lower dimensions and
    changes them back to their original dimensions. Therefore, this U-shaped kind
    of network structure is often known as U-Net. There are also many skip connections
    in the U-Net that connect the mirrored layers in order to help information (including
    details coming from previous layers in the forward pass and gradients coming from
    the latter layers in the backward pass) flow through the network. Without these
    skip connections, the network is also known as an encoder-decoder model, meaning
    that we stack a decoder at the end of an encoder.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个网络的前半部分逐步将输入图像转换为1x1的特征图（并且通道更宽），后半部分将这些非常小的特征图转换为与输入图像相同大小的输出图像。它将输入数据压缩为较低的维度，然后再将其恢复为原始维度。因此，这种U形的网络结构通常被称为U-Net。U-Net中还有许多跳跃连接，它们连接镜像层，以帮助信息（包括前向传播中来自先前层的细节和后向传播中来自后续层的梯度）在网络中流动。如果没有这些跳跃连接，这个网络也可以称为编码器-解码器模型，意思是我们在编码器的末尾堆叠了一个解码器。
- en: The pix2pix model is defined in the `models.pix2pix_model.Pix2PixModel` class,
    which is derived from an **abstract base class** (**ABC**) known as `models.base_model.BaseModel`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix模型在`models.pix2pix_model.Pix2PixModel`类中定义，该类继承自一个**抽象基类**（**ABC**），即`models.base_model.BaseModel`。
- en: An **abstract base class** in Python is a class containing at least one **abstract
    method** (that's declared and not implemented). It cannot be instantiated. You
    can only create objects with its subclasses after providing the implementations
    for all the abstract methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的**抽象基类**是一个包含至少一个**抽象方法**（已声明但未实现）的类。它不能被实例化。你只能在提供所有抽象方法的实现后，使用它的子类创建对象。
- en: 'The generator network, `netG`, is created by the `models.networks.define_G`
    method. By default, it takes `''unet_256''` as the `netG` argument value (which
    is specified at line 32 in `models/pix2pix_model.py` and overrides the initialized
    value, that is, `"resnet_9blocks"`, at line 34 in `options/base_options.py`).
    Therefore, `models.networks.UnetGenerator` is used to create the U-Net. In order
    to show how the U-Net is created in a recursive manner, we replace the arguments
    with their actual values, as shown in the following code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络`netG`是由`models.networks.define_G`方法创建的。默认情况下，它使用`'unet_256'`作为`netG`参数值（该值在`models/pix2pix_model.py`的第32行指定，并覆盖了`options/base_options.py`中第34行初始化的值，即`"resnet_9blocks"`）。因此，`models.networks.UnetGenerator`被用来创建U-Net。为了展示U-Net是如何递归创建的，我们将参数替换为它们的实际值，如下代码所示：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'At the fourth line in the preceding code snippet, the innermost block is defined,
    which creates the layers in the middle of the U-Net. The innermost block is defined
    as follows. Note that the following code should be treated as pseudocode since
    it''s simply to show you how different blocks are designed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段的第4行，定义了最内层的块，它创建了U-Net中间部分的层。最内层的块定义如下。请注意，以下代码应视为伪代码，因为它只是用来展示不同块是如何设计的：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `nn.Conv2d` layer in `down` transforms 2x2 input feature maps into 1x1 ones
    (because `kernel_size`=4 and `padding`=1), and the `nn.ConvTranspose2d` layer
    transforms them back so that they're 2x2 in size.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`down`中的`nn.Conv2d`层将2x2的输入特征图转换为1x1的特征图（因为`kernel_size`=4且`padding`=1），然后`nn.ConvTranspose2d`层将它们转置回2x2大小。'
- en: Remember the calculation formula of the output size for `nn.Conv2d` and `nn.ConvTranspose2d`?
    The output size of the convolution is ![](img/1539e7fa-afb0-43b4-8182-da522a169eb1.png),
    while the output size of the transposed convolution is ![](img/9050dcbb-74e6-46ec-b1e7-764b92864e6b.png).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 记得`nn.Conv2d`和`nn.ConvTranspose2d`的输出大小计算公式吗？卷积的输出大小是![](img/1539e7fa-afb0-43b4-8182-da522a169eb1.png)，而转置卷积的输出大小是![](img/9050dcbb-74e6-46ec-b1e7-764b92864e6b.png)。
- en: In the forward pass, it concatenates the output with a skip connection (that
    is, the input *x* itself) along the depth channel, which doubles the number of
    channels (and leads to the first 1,024-channel feature maps in the preceding diagram).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，它将输出与跳跃连接（即输入 *x* 本身）沿深度通道拼接，这样就将通道数翻倍（并导致前面图中第一个1,024通道的特征图）。
- en: When designing complex networks, it's been observed that the concatenation of
    the feature maps from two branches is better than their sum because the concatenation
    reserves more information. Of course, this concatenation costs a little more memory
    as well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计复杂网络时，已观察到将两个分支的特征图进行拼接比它们的求和效果更好，因为拼接保留了更多的信息。当然，拼接也会稍微增加内存的开销。
- en: 'Then, the rest of the layers are built recursively, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，其他层按递归方式构建，如下所示：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although in `models.networks.UnetGenerator`, the `unet_block` object is recursively
    passed as a `submodule` to a new `unet_block`, thanks to the compact design to
    the implementation of tensors, the actual modules will be created and saved on
    memory properly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在`models.networks.UnetGenerator`中，`unet_block`对象作为`submodule`递归传递给新的`unet_block`，但由于张量实现的紧凑设计，实际模块会正确地在内存中创建和保存。
- en: 'Finally, the first and last layers (which can be seen in the outermost block)
    are defined as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第一层和最后一层（可以在最外层块中看到）定义如下：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: All the convolution kernels in the generator network are initialized based on
    a normal distribution with a mean of 0 and a standard deviation of 0.02\. The
    scale factors in all the batch normalization layers are initialized based on the normal
    distribution with a mean of 1 and a standard deviation of 0.02.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络中的所有卷积核都基于均值为0、标准差为0.02的正态分布进行初始化。所有批量归一化层中的缩放因子则基于均值为1、标准差为0.02的正态分布进行初始化。
- en: Discriminator architecture
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器架构
- en: 'The architecture of the discriminator network of pix2pix is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix的判别器网络架构如下：
- en: '![](img/496267dd-f9f8-4c85-a1ad-de0fb427a0ed.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/496267dd-f9f8-4c85-a1ad-de0fb427a0ed.png)'
- en: Discriminator architecture of pix2pix
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix的判别器架构
- en: A pair of samples (one from each collection) are concatenated along the depth
    channel, and this 6-channel image is treated as the actual input of the discriminator
    network. The discriminator network maps the 6-channel 256x256 image to a 1-channel
    30x30 image, which is used to calculate the discriminator loss.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一对样本（每个集合中各取一个）沿深度通道进行拼接，这个6通道的图像被视为判别器网络的实际输入。判别器网络将6通道256x256的图像映射为1通道30x30的图像，用于计算判别器损失。
- en: 'The discriminator network, `netD`, is created by the `models.networks.define_G`
    method. By default, it takes `"basic"` as the argument value of `netD`, which
    is defined at line 33 in `options/base_options.py`. The `models.networks.NLayerDiscriminator`
    module, which has `n_layer=3`, is initialized so that it can serve as the discriminator
    network. Again, we''ve simplified the code so that it''s easy to read. You may
    refer to the full code in the `models/networks.py` file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络`netD`是通过`models.networks.define_G`方法创建的。默认情况下，它将`"basic"`作为`netD`的参数值，这在`options/base_options.py`的第33行中定义。`models.networks.NLayerDiscriminator`模块（其`n_layer=3`）被初始化以作为判别器网络。为了简化代码，便于阅读，我们已经对代码进行了简化。你可以参考`models/networks.py`文件中的完整代码：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we provide a short snippet so that we can print the sizes of all the
    feature maps if the model is created, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们提供了一段简短的代码，以便在模型创建时打印所有特征图的大小，如下所示：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can replace the line `return self.model(input)` with the following code
    to check the feature map sizes in all the layers (including the normalization
    and activation function layers):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将`return self.model(input)`这一行替换为以下代码，以检查所有层中的特征图大小（包括归一化和激活函数层）：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Alternatively, you can always use TensorBoard or other tools, which we will
    introduce in the last chapter of this book, so that you can easily examine the
    architectures of your models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以始终使用TensorBoard或其他工具，我们将在本书的最后一章介绍这些工具，以便你可以轻松查看模型的架构。
- en: The discriminator network creates a 30x30 feature map to represent the loss.
    This kind of architecture is called **PatchGAN**, which means that every small
    image patch in the original image is mapped to a pixel in the final loss map.
    A big advantage of PatchGAN is that it can handle the arbitrary sizes of input
    images as long as the labels have been transformed so that they're the same size
    as the loss map. It also evaluates the quality of the input image according to
    the quality of the local patches, rather than their global property. Here, we
    will show you how the size of the image patch (that is, 70) is calculated.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络生成一个30x30的特征图来表示损失。这样的架构被称为**PatchGAN**，意味着原始图像中的每个小图像块都会映射到最终损失图中的一个像素。PatchGAN的一个大优点是，它可以处理任意大小的输入图像，只要标签已经转换为与损失图相同的大小。它还根据局部图像块的质量评估输入图像的质量，而不是根据其全局属性。这里，我们将展示如何计算图像块的大小（即70）。
- en: First, let's consider a single convolution layer with a kernel size of `k` and
    a stride size of `s`. For each pixel in the output feature map, its value is only
    determined by a small patch of pixels in the input image, whose size is the same
    as the convolution kernel.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑一个单独的卷积层，卷积核大小为`k`，步幅为`s`。对于输出特征图中的每个像素，其值仅由输入图像中与卷积核大小相同的小块像素决定。
- en: 'When there are more than two convolution layers, the size of the input patch
    is calculated with the following formula:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当卷积层超过两个时，输入补丁的大小可以通过以下公式计算：
- en: '![](img/ed0ae0fe-f0b9-4825-999a-41e71a2e3bf7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed0ae0fe-f0b9-4825-999a-41e71a2e3bf7.png)'
- en: 'Therefore, the size of the input patch corresponding to a single pixel in the
    output feature map in each layer of the discriminator network can be obtained:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以获得判别器网络中每一层输出特征图中单个像素对应的输入补丁大小：
- en: '5th layer (k=4, s=1): Input patch size is 4 (which is the size of the kernel)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五层（k=4，s=1）：输入补丁的大小是4（即卷积核的大小）
- en: '4th layer (k=4, s=1): Input patch size is 4+1*(4-1)=7'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四层（k=4，s=1）：输入补丁的大小是 4+1*(4-1)=7
- en: '3rd layer (k=4, s=2): Input patch size is 4+2*(7-1)=16'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层（k=4，s=2）：输入补丁的大小是 4+2*(7-1)=16
- en: '2nd layer (k=4, s=2): Input patch size is 4+2*(16-1)=34'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层（k=4，s=2）：输入补丁的大小是 4+2*(16-1)=34
- en: '1st layer (k=4, s=2): Input patch size is 4+2*(34-1)=70'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层（k=4，s=2）：输入补丁的大小是 4+2*(34-1)=70
- en: This means that all of these 70x70 overlapping image patches are transformed
    by convolution layers into individual pixels in the 30x30 loss map. Any pixel
    outside this 70x70 image patch has no influence over the corresponding pixel in
    the loss map.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着所有这些70x70重叠的图像块会通过卷积层转换为30x30损失图中的独立像素。任何超出这个70x70图像块的像素都不会对损失图中的相应像素产生影响。
- en: Training and evaluation of pix2pix
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pix2pix的训练和评估
- en: 'The training of pix2pix is very similar to conditional GANs, which we introduced
    in the previous chapter. When training the discriminator network, a pair of real
    data and a label should be mapped to 1, whereas a pair of generated data and a
    label (that fake data is generated from) is mapped to 0\. When training the generator
    network, the gradients are passed through both of the discriminator and generator
    networks when the parameters in the generator network are updated. This generated
    data and the label should be mapped to 1 by the discriminator network. The major
    difference is that the labels are image-wise in CGAN and are pixel-wise in pix2pix.
    This process is described in the following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix的训练与我们在前一章中介绍的条件GAN非常相似。在训练判别器网络时，一对真实数据和标签应映射为1，而一对生成的数据和标签（该假数据是由生成器生成的）应映射为0。当训练生成器网络时，梯度会通过判别器和生成器网络，当生成器网络中的参数被更新时。这些生成的数据和标签应由判别器网络映射为1。主要的区别是，条件GAN中的标签是基于图像的，而pix2pix中的标签是基于像素的。这个过程在下面的图示中进行了说明：
- en: '![](img/7ccb49b6-c6c5-415b-a0a2-65129f267e48.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ccb49b6-c6c5-415b-a0a2-65129f267e48.png)'
- en: Basic training process of image-wise and pixel-wise labeled GANs. A* and B*
    denote real samples. Networks in red boxes are actually updated.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图像级和像素级标签GAN的基本训练过程。A* 和 B* 表示真实样本。红框中的网络实际上会被更新。
- en: 'Note that, when training pix2pix, in order to let the generated samples be
    as similar to the real ones as possible, an additional term is added to the loss
    function when training the generator network, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在训练pix2pix时，为了使生成的样本尽可能与真实样本相似，在训练生成器网络时，损失函数中会额外添加一个项，具体如下：
- en: '![](img/cd1117ed-2fbc-4293-add1-c65349fd6b54.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd1117ed-2fbc-4293-add1-c65349fd6b54.png)'
- en: Here, ![](img/bd348fa5-fff8-4911-bc8d-58bb0795875f.png) represents the L1-loss
    between the generated samples and the real ones from the paired collection. The
    purpose of the L1-loss is to reserve the low-frequency information in the images
    for better image quality.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/bd348fa5-fff8-4911-bc8d-58bb0795875f.png)表示生成样本与来自配对集合的真实样本之间的L1损失。L1损失的目的是保留图像中的低频信息，以获得更好的图像质量。
- en: It is worth mentioning that using L1-norm or L2-norm alone will generate blurry
    or blocky images. A short explanation of this can be found here: [https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry](https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry).
    It is also common to use them as regularization terms in the traditional image
    restoration methods, in which the gradients of the restored images control the
    sharpness. If you are interested in the roles of L1-loss and L2-loss in the field
    of image processing, feel free to check out the famous paper, *Total variation
    blind deconvolution* by Tony F. Chan and C.K. Wong in 1998.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，仅使用L1范数或L2范数将会生成模糊或块状的图像。对此的简短解释可以在此找到：[https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry](https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry)。在传统的图像恢复方法中，常常将它们用作正则化项，其中恢复图像的梯度控制图像的清晰度。如果你对L1损失和L2损失在图像处理领域的作用感兴趣，可以查看Tony
    F. Chan和C.K. Wong于1998年发表的著名论文《*Total variation blind deconvolution*》。
- en: 'Now, we can define the training procedure of pix2pix, as follows (pseudocode):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义pix2pix的训练过程，如下所示（伪代码）：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `Pix2PixModel` class serves a similar purpose to the `Model` class in `build_gan.py`
    from the previous chapter, which creates the generator and discriminator networks,
    defines their optimizers, and controls the training procedures of the networks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pix2PixModel`类的作用类似于前一章`build_gan.py`中的`Model`类，用于创建生成器和判别器网络，定义它们的优化器，并控制网络的训练过程。'
- en: Now, let's download some images and train the pix2pix model to perform image-to-image
    translation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们下载一些图像并训练pix2pix模型来执行图像到图像的转换。
- en: 'Run the `datasets/download_pix2pix_dataset.sh` script to download the dataset
    files, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`datasets/download_pix2pix_dataset.sh`脚本以下载数据集文件，具体如下：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Alternatively, you can go to [http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)
    to download the dataset files manually and extract them to any location you like
    (for example, an external hard drive such as `/media/john/HouseOfData/image_transfer/maps`).
    The maps dataset file is approximately 239 MB in size and contains a few more
    than 1,000 images in the collections of the train, validation, and test sets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以访问[http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)手动下载数据集文件，并将其解压到你喜欢的任何位置（例如，外部硬盘`/media/john/HouseOfData/image_transfer/maps`）。地图数据集文件大约为239
    MB，包含在训练集、验证集和测试集中的1,000多张图像。
- en: Note that collection A in the maps dataset contains satellite photos and that
    collection B contains map images, which is opposite to what was shown in the diagrams
    in the previous subsections.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，地图数据集中的集合A包含卫星照片，而集合B包含地图图像，这与前面小节中所展示的图示相反。
- en: 'Next, open a Terminal and run the following script to start training. Make
    sure you modify the `dataroot` argument so that it specifies your own location.
    You may also try other datasets and change `direction` from `BtoA` to `AtoB` to
    change the translation direction between two image collections:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开终端并运行以下脚本以开始训练。确保修改`dataroot`参数以指定你自己的位置。你也可以尝试其他数据集，并将`direction`从`BtoA`更改为`AtoB`，以更改两个图像集合之间的转换方向：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For the first time of training, you may encounter an error stating `Could not
    connect to Visdom server`. This is because the training script calls the `Visdom`
    module to dynamically update the generated results so that we can monitor the
    training process via web browsers. You can manually open the `checkpoints/maps_pix2pix/web/index.html`
    file with your favorite browser to keep an eye on the generated images as the
    model is being trained. Note that there is a chance that closing the `index.html`
    page in the web browser could cause the training process to freeze.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次训练时，你可能会遇到错误提示`Could not connect to Visdom server`。这是因为训练脚本调用了`Visdom`模块来动态更新生成结果，以便我们通过网页浏览器监控训练过程。你可以手动打开`checkpoints/maps_pix2pix/web/index.html`文件，用你喜欢的浏览器来查看生成的图像，随着模型的训练进行监控。请注意，关闭浏览器中的`index.html`页面可能会导致训练过程冻结。
- en: It takes about 6.7 hours to finish 200 epochs of training and costs about 1,519
    MB GPU memory on a GTX 1080Ti graphics card.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完成200个训练周期大约需要6.7小时，并且在GTX 1080Ti显卡上大约需要1,519 MB的GPU内存。
- en: 'The results are also saved in the `checkpoints/maps_pix2pix/web/images` directory.
    The images that are generated by doing this are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果也保存在`checkpoints/maps_pix2pix/web/images`目录中。通过此方法生成的图像如下所示：
- en: '![](img/a9938fd6-2b4d-4da7-b15e-33470c8bf30c.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9938fd6-2b4d-4da7-b15e-33470c8bf30c.png)'
- en: Generated images by pix2pix
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由pix2pix生成的图像
- en: As we can see, the generated satellite photos look pretty convincing on their
    own. Compared to real satellite photos, they do a good job of organizing the trees
    along the trails in the park.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，生成的卫星照片看起来相当可信。与真实的卫星照片相比，它们在公园小径两旁的树木组织方面做得相当好。
- en: 'In this section, we managed to translate and generate 256 x 256 images. In
    the next section, we will learn how to generate high-resolution images with an
    upgraded version of pix2pix: pix2pixHD.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们成功地生成了256x256的图像。在下一节中，我们将学习如何使用pix2pix的升级版本：pix2pixHD生成高分辨率图像。
- en: Pix2pixHD – high-resolution image translation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pix2pixHD - 高分辨率图像转换
- en: Pix2pixHD was proposed by Ting-Chun Wang, Ming-Yu Liu, and Jun-Yan Zhu, et.
    al. in their paper, *High-Resolution Image Synthesis and Semantic Manipulation
    with Conditional GANs*, which was an upgraded version of the pix2pix model. The
    biggest improvement of pix2pixHD over pix2pix is that it supports image-to-image
    translation at 2,048x1,024 resolution and with high quality.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2pixHD由Ting-Chun Wang、Ming-Yu Liu和Jun-Yan Zhu等人在他们的论文*《高分辨率图像合成与语义操作与条件GANs》*中提出，是pix2pix模型的升级版。pix2pixHD相比于pix2pix的最大改进在于，它支持2,048x1,024分辨率的图像到图像的转换，并且具有更高的质量。
- en: Model architecture
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: 'To make this happen, they designed a two-stage approach to gradually train
    and refine the networks, as shown in the following diagram. First, a lower resolution
    image of 1,024x512 is generated by a generator network, ![](img/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png),
    called the **global generator** (the red box). Second, the image is enlarged by
    a generator network, ![](img/1d7444a7-949b-4f66-871d-8ecd2f72d759.png), called
    the **local enhancer network** so that it''s around 2,048x1,024 in size (the black
    box). It is also viable to put another local enhancer network at the end to generate
    4,096x2,048 images. Note that the last feature map in ![](img/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png)
    is also inserted into ![](img/1d7444a7-949b-4f66-871d-8ecd2f72d759.png) (before
    the residual blocks) via an element-wise sum to introduce more global information
    into higher resolution images:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，他们设计了一个两阶段的方案，逐步训练并优化网络，如下图所示。首先，一个分辨率为1,024x512的低分辨率图像由生成器网络生成，![](img/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png)，称为**全局生成器**（红框）。其次，该图像由另一个生成器网络放大，![](img/1d7444a7-949b-4f66-871d-8ecd2f72d759.png)，称为**局部增强网络**，使其大小接近2,048x1,024（黑框）。也可以在最后加入另一个局部增强网络来生成4,096x2,048的图像。请注意，![](img/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png)中的最后一个特征图也通过逐元素求和的方式插入到![](img/1d7444a7-949b-4f66-871d-8ecd2f72d759.png)（在残差块之前），以便将更多的全局信息引入到更高分辨率的图像中：
- en: '![](img/084d449a-d919-449f-9768-40daf14dfb27.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/084d449a-d919-449f-9768-40daf14dfb27.png)'
- en: Architecture of the generator model in pix2pixHD (image retrieved from the paper
    by T. C. Wang, et. al., 2018)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pixHD中生成器模型的架构（图片摘自T. C. Wang等人2018年的论文）
- en: 'The discriminator network in pix2pixHD is also designed in a multi-scale fashion.
    Three identical discriminator networks work on different image scales (original
    size, 1/2 size, and 1/4 size) and their loss values are added together. It is
    reported by the authors that, without multi-scale design in the discriminator,
    repeated patterns are often observed in the generated images. Also, an additional
    term, called the **feature matching loss**, is added to the final discriminator
    loss, as shown the following formula:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pixHD中的判别器网络也是以多尺度的方式设计的。三个相同的判别器网络分别在不同的图像尺度（原始大小、1/2大小和1/4大小）上工作，并且它们的损失值会加在一起。作者报告称，如果判别器没有多尺度设计，生成的图像中常常会出现重复的模式。此外，一个额外的项，称为**特征匹配损失**，被添加到最终的判别器损失中，如下公式所示：
- en: '![](img/fecc2df3-fec9-4df6-bf1a-0cc22388f685.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fecc2df3-fec9-4df6-bf1a-0cc22388f685.png)'
- en: Here, ![](img/8df2c128-fd0c-4a48-b0eb-e61089bf1d48.png) measures the L1-loss
    between the feature maps of the generated and real images at multiple layers in
    the discriminator networks. It forces the generator to approximate the real data
    at different scales, thereby generating more realistic images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/8df2c128-fd0c-4a48-b0eb-e61089bf1d48.png) 测量生成图像与真实图像在判别器网络多个层次上特征图的
    L1 损失。它迫使生成器在不同尺度上近似真实数据，从而生成更为逼真的图像。
- en: Sometimes, several objects with the same label may find their way together,
    which makes it difficult for the generator to correctly distinguish these objects.
    It would help if the generator knew which pixels belong to which object compared
    to which class they belong to. Therefore, in pix2pixHD, an **instance boundary
    map** (which is a binary map denoting the boundaries of all the objects) is channel-wise concatenated
    to the semantic label map before it's fed into the generator. Similarly, the instance
    boundary map is also concatenated to the semantic label map and the image (the
    generated one or the real one), before being fed into the discriminator.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，几个具有相同标签的物体可能会被识别为同一物体，这会导致生成器难以正确区分这些物体。如果生成器能知道哪些像素属于哪个物体，而不是它们属于哪个类别，会有所帮助。因此，在
    pix2pixHD 中，**实例边界图**（这是一个二值图，表示所有物体的边界）会按通道方式与语义标签图连接，然后再输入到生成器中。类似地，实例边界图也会与语义标签图和图像（生成的或真实的）连接，然后一起输入到判别器中。
- en: Furthermore, in order to make it easier to manipulate the attributes of the
    generated images, pix2pixHD uses an additional **encoder** to extract features
    from the real images and performs instance-wise average pooling (averages all
    the pixels in one object and then broadcasts back to these pixels) on the features.
    These features are also part of the input to the generator. K-means clustering
    is performed on the features of all the objects in each class, and several available
    textures or colors can be chosen for the objects during inference.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了更方便地操作生成图像的属性，pix2pixHD 使用了一个额外的 **编码器** 来从真实图像中提取特征，并对特征进行实例级的平均池化（对每个物体的所有像素进行平均，然后将结果广播回这些像素）。这些特征也是生成器的输入之一。对每个类别中所有物体的特征进行
    K-means 聚类，并在推理过程中为物体选择几种可用的纹理或颜色。
- en: We will not dive deep into the specific architecture designs of pix2pixHD since
    the main structure of its source code is similar to pix2pix. You can check out
    the source code if you're interested.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨 pix2pixHD 的具体架构设计，因为其源代码的主要结构与 pix2pix 类似。如果你感兴趣，可以查看源代码。
- en: Model training
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: The training of pix2pixHD is both time- and memory-consuming. It requires about
    24 GB GPU memory to train 2,048x1,024 images. Therefore, we will only train on
    a 1,024x512 resolution in order to fit this on a single graphic card.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pixHD 的训练既耗时又耗内存。训练 2,048x1,024 图像大约需要 24 GB 的 GPU 内存。因此，我们将只在 1,024x512
    分辨率下进行训练，以便将其适配到单张显卡上。
- en: 'NVIDIA has already open-sourced the full source code of pix2pixHD for PyTorch.
    All we need to do is download the source code and dataset to produce our own high
    resolution synthesized images. Let''s do this now:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 已经将完整的 pix2pixHD 源代码开源，支持 PyTorch。我们需要做的就是下载源代码和数据集，来生成我们自己的高分辨率合成图像。现在就开始吧：
- en: Install the prerequisites (dominate and apex). We previously installed the `dominate` library. **Apex** is
    a mixed precision and distributed training library that's developed by NVIDIA.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装先决条件（dominate 和 apex）。我们之前已经安装了 `dominate` 库。**Apex** 是由 NVIDIA 开发的一个混合精度和分布式训练库。
- en: Use **Automatic Mixed Precision** (**AMP**) to reduce the GPU memory consumption
    (or even the training time) during training by replacing the standard floating-point
    values with lower bit floats.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **自动混合精度** (**AMP**) 在训练过程中通过将标准浮点值替换为较低位数的浮点数来减少 GPU 内存消耗（甚至是训练时间）。
- en: 'Open a Terminal in Ubuntu and type in the following scripts to install `apex`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Ubuntu 中打开终端，输入以下脚本来安装 `apex`：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Download the source code of pix2pixHD (also available under the code repository
    for this chapter):'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 pix2pixHD 的源代码（本章的代码库中也可以找到）：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Use the **Cityscapes** dataset to train the pix2pixHD model. It is available
    at [https://www.cityscapes-dataset.com](https://www.cityscapes-dataset.com) and
    you'll need to register first before being granted access to the download links.
    We need to download the `gtFine_trainvaltest.zip` *(*241 MB*)* and `leftImg8bit_trainvaltest.zip` *(*11
    GB*)* files for this experiment.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **Cityscapes** 数据集来训练 pix2pixHD 模型。它可以在 [https://www.cityscapes-dataset.com](https://www.cityscapes-dataset.com)
    获取，您需要先注册才能获得下载链接。我们需要下载 `gtFine_trainvaltest.zip`（241 MB） 和 `leftImg8bit_trainvaltest.zip`（11
    GB） 文件来进行此次实验。
- en: 'When the download is finished, we need to reorganize the images so that the
    training script can pick up the images correctly:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载完成后，我们需要重新组织图像，以便训练脚本可以正确地获取图像：
- en: Put all the image files in the `gtFine/train/*` folders that end with `_gtFine_instanceIds.png` into
    the `datasets/cityscapes/train_inst` directory.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有以 `_gtFine_instanceIds.png` 结尾的图像文件放入 `gtFine/train/*` 文件夹中，并将其放入 `datasets/cityscapes/train_inst`
    目录中。
- en: Put all the image files in the `gtFine/train/*` folders that end with `_gtFine_labelIds.png` into
    the `datasets/cityscapes/train_label` directory.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有以 `_gtFine_labelIds.png` 结尾的图像文件放入 `gtFine/train/*` 文件夹中，并将其放入 `datasets/cityscapes/train_label`
    目录中。
- en: Put all the image files in the `leftImg8bit/train/*` folders that end with `_leftImg8bit.png` into
    the `datasets/cityscapes/train_img` directory.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有以 `_leftImg8bit.png` 结尾的图像文件放入 `leftImg8bit/train/*` 文件夹中，并将其放入 `datasets/cityscapes/train_img`
    目录中。
- en: The test and validation sets can be ignored since we only need the training
    images. There should be 2,975 images in each of the training folders.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以忽略测试和验证集，因为我们只需要训练图像。每个训练文件夹中应该有2,975张图像。
- en: 'Run `scripts/train_512p.sh` to start the training process or simply type the
    following in the Terminal:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `scripts/train_512p.sh` 来启动训练过程，或者在终端中简单地输入以下内容：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: All the intermediate results (arguments taken, generated images, logging information,
    and model files) will be saved in the `checkpoints/label2city_512p` folder. You
    can always check the `checkpoints/label2city_512p/web/index.html` file in your
    favorite browser or directly check out the images in the `checkpoints/label2city_512p/web/images`
    folder to monitor the training process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所有中间结果（采用的参数、生成的图像、日志信息和模型文件）都将保存在 `checkpoints/label2city_512p` 文件夹中。您可以随时在您喜爱的浏览器中检查
    `checkpoints/label2city_512p/web/index.html` 文件，或直接查看 `checkpoints/label2city_512p/web/images`
    文件夹中的图像，以监视训练过程。
- en: 'Here are the results after 35 epochs of training (about 20 hours):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过35个周期（约20小时）的训练后，这里是结果：
- en: '![](img/655ba229-1df2-4a56-92a5-579539a25957.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/655ba229-1df2-4a56-92a5-579539a25957.png)'
- en: Generated image after 35 epochs of training by pix2pixHD
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过35个周期的 pix2pixHD 训练后生成的图像
- en: Here, we can see that the model has already figured out where to put vehicles,
    trees, buildings, and pedestrians based on the label information from the instance
    map, although the objects themselves still have much to improve on in terms of
    appearance. It is interesting to observe that the model is trying to put road
    lines in the correct positions and that the badge of the car that the images have
    been captured from has an almost perfect reflection on the front hood (which makes
    sense since the badge and the hood appear in every image).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到模型已经根据实例地图中的标签信息找到了放置车辆、树木、建筑物和行人的位置，尽管这些对象在外观上仍有很大改进空间。有趣的是观察到模型试图将道路线放置在正确的位置，并且从被捕捉图像的汽车前盖上的徽章具有几乎完美的反射（这是有道理的，因为徽章和前盖在每张图像中都出现）。
- en: 'If you are willing to wait long enough (approximately 110 hours), the results
    are pretty impressive:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够等待足够长的时间（大约110小时），结果将会非常令人印象深刻：
- en: '![](img/fd1df132-f082-48aa-9526-74cc7e3e8450.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1df132-f082-48aa-9526-74cc7e3e8450.png)'
- en: Generated image by pix2pixHD (images retrieved from https://github.com/NVIDIA/pix2pixHD)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由 pix2pixHD 生成的图像（从 https://github.com/NVIDIA/pix2pixHD 检索的图像）
- en: It costs about 8,077 MB GPU memory to train on a 1,024x512 resolution. When
    AMP is enabled (trained with `--fp16`), the GPU memory consumption starts with
    7,379 MB at first and gradually increases to 7,829 MB after a few epochs, which
    is indeed lower than before. However, the training time is almost half as long
    than it is without AMP. Therefore, you should go without AMP for now, until its
    performance is improved in the future.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在1,024x512分辨率上训练大约需要8,077 MB的GPU内存。当启用AMP（使用 `--fp16` 训练）时，GPU内存消耗从一开始的7,379
    MB逐渐增加到几个周期后的7,829 MB，这确实比以前低。然而，训练时间几乎减少了一半。因此，目前应该不使用AMP，直到未来其性能得到改进。
- en: CycleGAN – image-to-image translation from unpaired collections
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CycleGAN – 来自无配对图像集的图像到图像翻译
- en: You may have noticed that, when training pix2pix, we need to determine a direction
    (`AtoB` or `BtoA`) that the images are translated to. Does this mean that, if
    we want to freely translate from image set A to image set B and vice versa, we
    need to train two models separately? Not with CycleGAN, we say!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在训练pix2pix时，我们需要确定一个方向（`AtoB`或`BtoA`），即图像要被翻译到的方向。这是否意味着，如果我们想要在图像集A和图像集B之间自由地进行翻译，我们需要分别训练两个模型呢？我们要说的是，使用CycleGAN就不需要！
- en: CycleGAN was proposed by Jun-Yan Zhu, Taesung Park, and Phillip Isola, et. al.
    in their paper, *Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
    Networks*. It is a bidirectional generative model based on unpaired image collections.
    The core idea of CycleGAN is built on the assumption of cycle consistency, which
    means that if we have two generative models, G and F, that translate between two
    sets of images, X and Y, in which Y=G(X) and X=F(Y), we can naturally assume that
    F(G(X)) should be very similar to X and G(F(Y)) should be very similar to Y. This
    means that we can train two sets of generative models at the same time that can
    freely translate between two sets of images.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN是由Jun-Yan Zhu、Taesung Park、Phillip Isola等人提出的，发表在他们的论文《*使用循环一致性对抗网络进行无配对图像到图像的翻译*》中。它是一个基于无配对图像集的双向生成模型。CycleGAN的核心思想基于循环一致性假设，这意味着如果我们有两个生成模型G和F，它们在两个图像集X和Y之间进行翻译，其中Y=G(X)，X=F(Y)，我们可以自然地假设F(G(X))应该与X非常相似，而G(F(Y))应该与Y非常相似。这意味着我们可以同时训练两组生成模型，它们可以在两个图像集之间自由翻译。
- en: CycleGAN is specifically designed for unpaired image collections, which means
    that the training samples are not necessarily strictly paired like they were in
    the previous sections when we looked at pix2pix and pix2pixHD (for example, semantic
    segmentation maps versus street views from the same perspective, or regular maps
    versus satellite photos of the same location). This makes CycleGAN more than just
    an image-to-image translation tool. It unlocks the potential to **transfer style**
    from any kind of images to your own images, for example, turning apples into oranges,
    horses into zebras, photos into oil paintings, and vice versa. Here, we'll perform
    image-to-image translation on landscape photos and Vincent van Gogh's paintings
    as an example to show you how CycleGAN is designed and trained.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN专门为无配对图像集设计，这意味着训练样本不一定像我们在前几节中看到的pix2pix和pix2pixHD那样严格配对（例如，从相同视角的语义分割图与街景，或者常规地图与同一地点的卫星照片）。这使得CycleGAN不仅仅是一个图像到图像的翻译工具。它解锁了从任何类型的图像到你自己图像的**风格迁移**的潜力，例如，将苹果变成橙子、马变成斑马、照片变成油画，反之亦然。在这里，我们将使用风景照片和文森特·梵高的油画作为示例，展示CycleGAN是如何设计和训练的。
- en: Note that, in this section, the code layout is similar to CGAN in the previous
    chapter. The full source code is available under the code repository for this
    chapter. The models are defined in `cyclegan.py`, the training process is defined
    in `build_gan.py`, and the main entry is located at `main.py`. The source code
    is based on the implementation provided by [https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN). It
    is worth mentioning that our implementation trains 1.2x faster and costs 28% less
    GPU memory than that implementation. Also, in the source code of pix2pix, which
    can be found in the first section of this chapter, an implementation of CycleGAN
    was provided. You may choose whichever implementation you like since there isn't
    much of a difference between the two.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本节中，代码布局与上一章中的CGAN类似。完整的源代码可以在本章的代码库中找到。模型在`cyclegan.py`中定义，训练过程在`build_gan.py`中定义，主入口位于`main.py`。源代码基于[https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)提供的实现。值得一提的是，我们的实现训练速度比该实现快1.2倍，并且GPU内存占用减少了28%。此外，在本章的第一部分，你可以找到pix2pix的源代码，其中也提供了CycleGAN的实现。你可以选择任何一个实现，因为两者之间没有太大区别。
- en: Cycle consistency-based model design
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于循环一致性的模型设计
- en: Two pairs of generator and discriminator networks are used, with each being
    responsible for a translation direction. In order to understand why CycleGAN is
    designed as such, we need to understand how the cycle consistency is constructed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两对生成器和判别器网络，每对负责一个翻译方向。为了理解为什么CycleGAN是这样设计的，我们需要了解循环一致性是如何构建的。
- en: In the following diagram, the generator, ![](img/f940b35c-4e1c-481b-94a6-ca18e1505cea.png)maps
    sample A to sample B and its performance is measured by the discriminator, ![](img/8cb29fbf-9cbb-4552-8a33-f05dd7f5090e.png).
    At the same time, another generator, ![](img/844e96ce-5649-43b2-a58c-d543f0629aa1.png), is
    trained to map sample B back to sample A, whose performance is measured by the
    discriminator, ![](img/999a4fc7-a3ca-4056-a923-716408abd75c.png). In this process,
    the distance between a generated sample, ![](img/ec2bfd57-c769-41de-aba9-61400cf32047.png), and
    the corresponding original real sample, ![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png), tells
    us whether a cycle consistency exists in our model, as shown in the dotted box
    in the following diagram. The distance between ![](img/deac1805-46a7-4b5c-a2df-42904c879e9a.png) and ![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png) is
    measured by the **cycle consistency loss**, which takes the form of the L1-norm.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，生成器，![](img/f940b35c-4e1c-481b-94a6-ca18e1505cea.png)将样本A映射为样本B，其性能由判别器，![](img/8cb29fbf-9cbb-4552-8a33-f05dd7f5090e.png)来衡量。与此同时，另一个生成器，![](img/844e96ce-5649-43b2-a58c-d543f0629aa1.png)，被训练来将样本B映射回样本A，其性能由判别器，![](img/999a4fc7-a3ca-4056-a923-716408abd75c.png)来衡量。在这个过程中，生成样本，![](img/ec2bfd57-c769-41de-aba9-61400cf32047.png)与对应的真实样本，![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png)之间的距离，告诉我们模型中是否存在循环一致性，如下图中的虚线框所示。![](img/deac1805-46a7-4b5c-a2df-42904c879e9a.png)与![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png)之间的距离通过**循环一致性损失**来衡量，该损失形式为L1范数。
- en: 'Besides the traditional **adversarial loss** (distance between ![](img/b1979e0b-0650-450b-b9ec-aea0e7b059dc.png) and
    1), the **identity loss** (which means that ![](img/58282974-6e86-4511-a954-dddbfcfaec32.png) should
    be very close to ![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png) itself) is
    also added to help maintain the color style of the images:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的**对抗损失**（![](img/b1979e0b-0650-450b-b9ec-aea0e7b059dc.png)与1之间的距离），还加入了**恒等损失**（意味着![](img/58282974-6e86-4511-a954-dddbfcfaec32.png)应该与![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png)非常接近），以帮助保持图像的颜色风格：
- en: '![](img/dc7bfabe-2c11-425e-9ebb-793ad0ad6e00.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc7bfabe-2c11-425e-9ebb-793ad0ad6e00.png)'
- en: The calculation of loss in CycleGAN. A* and B* denote real samples. Networks
    denoted by red boxes are updated while training the generators.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN中的损失计算。A*和B*表示真实样本。由红框表示的网络在训练生成器时会更新。
- en: 'The two generator networks, ![](img/f940b35c-4e1c-481b-94a6-ca18e1505cea.png)and, are
    identical. The architecture of the generator network can be seen in the following
    diagram. The 256x256 input image is downsampled by multiple convolution layers
    to 64x64, processed by nine successive residual blocks, and finally upsampled
    by convolutions back to 256x256:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个生成器网络，![](img/f940b35c-4e1c-481b-94a6-ca18e1505cea.png)和，完全相同。生成器网络的架构可以在下图中看到。256x256的输入图像经过多个卷积层下采样为64x64，然后经过九个连续的残差块处理，最后通过卷积再次上采样回256x256：
- en: '![](img/53aa9cf0-ebf0-43f4-a5b4-c1ddd3a6b58d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53aa9cf0-ebf0-43f4-a5b4-c1ddd3a6b58d.png)'
- en: Generator architecture in CycleGAN
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN中的生成器架构
- en: 'We''ll start the code with a blank file named `cyclegan.py`, as we mentioned
    previously. Let''s start with the imports:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一个名为`cyclegan.py`的空文件开始，如之前所提到的。让我们从导入开始：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we''ll create the code for the definition of the residual block, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建定义残差块的代码，如下所示：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can define the generator network, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义生成器网络，如下所示：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you may have noticed, here, we used `torch.nn.InstanceNorm2d` instead of `torch.nn.BatchNorm2d`.
    The former normalization layer is more suitable for style transfer.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所注意到的，在这里我们使用了`torch.nn.InstanceNorm2d`而不是`torch.nn.BatchNorm2d`。前者的归一化层更适合风格迁移。
- en: 'Similarly, two identical discriminator networks are used in CycleGAN and their
    relationship can be seen in the following diagram:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，CycleGAN中使用了两个相同的判别器网络，它们的关系可以在下图中看到：
- en: '![](img/eb67f62f-54fd-4654-916a-6ee30cc817ab.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb67f62f-54fd-4654-916a-6ee30cc817ab.png)'
- en: Relationship between two discriminator networks in CycleGAN. Networks denoted
    by red boxes are updated during training.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN中两个判别器网络之间的关系。由红框表示的网络在训练时会更新。
- en: The architecture of the discriminator network is almost the same as it is in
    pix2pix (which is called PatchGAN), except that the input image has a depth channel
    of 3, instead of 6, and `torch.nn.BatchNorm2d` is replaced with `torch.nn.InstanceNorm2d`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的架构几乎与pix2pix中的架构相同（该架构称为PatchGAN），唯一不同的是输入图像的深度通道为3，而不是6，并且`torch.nn.BatchNorm2d`被替换成了`torch.nn.InstanceNorm2d`。
- en: 'The code for the definition of the discriminator network is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络定义的代码如下：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, let's learn how the model can be trained and evaluated.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何训练和评估模型。
- en: Model training and evaluation
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练与评估
- en: 'Now, we''ll create the `build_gan.py` file. As usual, we''ll begin with the
    imports:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建`build_gan.py`文件。像往常一样，我们从导入开始：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We''ll need a function to initialize the weights:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来初始化权重：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we will create the `Model` class:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建`Model`类：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The training processes for the generator and discriminator networks were shown
    previously. Here, we will dive into the implementation of `build_gan.train()`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 之前展示了生成器和判别器网络的训练过程。在这里，我们将深入探讨`build_gan.train()`的实现。
- en: 'First, we need to train the generator networks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要训练生成器网络：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we need to train the discriminator networks:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要训练判别器网络：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The last variable, `d_loss`, is simply for logging and has been omitted here.
    You can refer to the source code file for this chapter if you want to find out
    more about logging printing and image exporting:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的变量`d_loss`仅用于日志记录，这里已省略。如果你想了解更多关于日志打印和图像导出的内容，可以参考本章的源代码文件：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, as suggested in the paper, we update the discriminators by randomly picking
    an image from the history of generated images, rather than the fake samples in
    real-time. The history of generated images is maintained by the `ImageBuffer`
    class, which is defined as follows. Copy the `utils.py` file from the previous
    chapter and add the `ImageBuffer` class to it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，正如论文中建议的那样，我们通过随机从生成的图像历史中选择一张图像来更新判别器，而不是实时从假样本中选择。生成图像的历史由`ImageBuffer`类维护，定义如下。请将上一章中的`utils.py`文件复制过来，并将`ImageBuffer`类添加到其中：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We also need to write a custom dataset reader that picks up unpaired images
    from separate folders. Place the following content into a new file called `datasets.py`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要编写一个自定义数据集读取器，从不同的文件夹中提取未配对的图像。将以下内容放入一个名为`datasets.py`的新文件中：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The shapes of the paintings and photos are not always square. Therefore, we
    need to crop 256x256 patches from the original images. We preprocess the data
    (**data augmentation**) in `main.py`. Here, we''re only showing a part of the
    code. You can find the rest of the code in the `main.py` file:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 画作和照片的形状并不总是方形的。因此，我们需要从原始图像中裁剪 256x256 的补丁。我们在`main.py`中预处理数据（**数据增强**）。这里我们只展示了一部分代码，你可以在`main.py`文件中找到其余部分：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Don''t forget to adjust the argument parsing for CycleGAN. Remember, you should
    change the `--data_dir` default so that it matches your own setup, so be sure
    to include the following on the command line:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记调整 CycleGAN 的参数解析。记住，你应该更改`--data_dir`的默认值，使其与自己的设置匹配，因此请确保在命令行中包含以下内容：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, it's time to download the datasets and start having fun! Go to [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets) to
    manually download the dataset files. Alternatively, you can use the `datasets/download_cyclegan_dataset.sh`
    script that's located in the source code of pix2pix to download the `vangogh2photo.zip`
    file, which is about 292 MB in size and contains 400 Van Gogh paintings and 7,038
    photos (6,287 in train and 751 in test). When the download is finished, extract
    the images to a folder (for example, an external hard drive such as `/media/john/HouseOfData/image_transfer`).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候下载数据集并开始享受了！访问[https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets)手动下载数据集文件。或者，你可以使用位于
    pix2pix 源代码中的`datasets/download_cyclegan_dataset.sh`脚本下载`vangogh2photo.zip`文件，该文件约为
    292 MB，包含 400 张梵高画作和 7,038 张照片（其中 6,287 张用于训练，751 张用于测试）。下载完成后，将图像提取到一个文件夹中（例如，外部硬盘`/media/john/HouseOfData/image_transfer`）。
- en: 'Open a Terminal and type the following script to start training:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端并键入以下脚本以开始训练：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It takes about 10 hours to train CycleGAN for 20 epochs and costs about 4,031
    MB GPU memory on a GTX 1080Ti graphics card. Some of the results can be seen in
    the following image. Here, we can see that the style transfer capability of CycleGAN
    is pretty amazing. You can also check out this site to learn about more applications
    of CycleGAN: [https://junyanz.github.io/CycleGAN](https://junyanz.github.io/CycleGAN):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 CycleGAN 20 个周期大约需要 10 小时，并且在 GTX 1080Ti 显卡上消耗大约 4,031 MB 的 GPU 内存。以下图像中可以看到一些结果。在这里，我们可以看到
    CycleGAN 的风格迁移能力非常惊人。你还可以访问这个网站，了解更多关于 CycleGAN 的应用：[https://junyanz.github.io/CycleGAN](https://junyanz.github.io/CycleGAN)：
- en: '![](img/74b56dc8-0c58-4e0d-81cb-ed1689b9868e.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74b56dc8-0c58-4e0d-81cb-ed1689b9868e.png)'
- en: 'Generated images by CycleGAN. Top two rows: Painting to photo; Bottom two rows:
    Photo to painting.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 生成的图像。上面两行：从画作到照片；下面两行：从照片到画作。
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have been getting familiar with image generation for several chapters now.
    Although it is always challenging and fulfilling to successfully train GANs to
    generate amazing images, we should recognize that GANs can also be used to fix
    things and restore images.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在几个章节中熟悉了图像生成技术。尽管成功训练 GAN 生成惊艳图像总是充满挑战和成就感，但我们也应该认识到，GAN 还可以用来修复问题并恢复图像。
- en: In the next chapter, we will explore the generative power of GANs to address
    some of the challenging problems in image restoration.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨 GAN 的生成能力，以解决图像修复中的一些挑战性问题。
- en: Furthering reading
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Le J. (May 3, 2018) *How to do Semantic Segmentation using Deep learning*. Retrieved
    from [https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef](https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Le J. (2018年5月3日) *如何使用深度学习进行语义分割*。取自 [https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef](https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef)。
- en: Rainy J. (Feb 12, 2018) *Stabilizing neural style-transfer for video*. Retrieved
    from [https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rainy J. (2018年2月12日) *为视频稳定化神经风格转移*。取自 [https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42)。
- en: Isola P, Zhu JY, Zhou T, Efros A. (2017) *Image-to-Image Translation with Conditional
    Adversarial Networks*. CVPR.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Isola P, Zhu JY, Zhou T, Efros A. (2017) *基于条件对抗网络的图像到图像翻译*。CVPR。
- en: Agustinus K. (Feb 9, 2017) *Why does L2 reconstruction loss yield blurry images?* Retrieved
    from [https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry](https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry).
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Agustinus K. (2017年2月9日) *为什么 L2 重建损失会导致模糊图像？* 取自 [https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry](https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry)。
- en: 'Chan T F, Wong C K. (1998) *Total Variation Blind Deconvolution. IEEE Transactions
    on Image Processing*. 7(3): 370-375.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Chan T F, Wong C K. (1998) *总变分盲去卷积。IEEE 图像处理学报*。7(3): 370-375。'
- en: Wang T C, Liu M Y, Zhu J Y, et. al. (2018) *High-Resolution Image Synthesis
    and Semantic Manipulation with Conditional GANs*. CVPR.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang T C, Liu M Y, Zhu J Y, 等. (2018) *基于条件 GAN 的高分辨率图像合成与语义操作*。CVPR。
- en: Zhu J Y, Park T, Isola P, et. al. (2017) *Unpaired Image-to-Image Translation
    using Cycle-Consistent Adversarial Networks*. ICCV.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhu J Y, Park T, Isola P, 等. (2017) *使用循环一致对抗网络的无配对图像到图像翻译*。ICCV。
