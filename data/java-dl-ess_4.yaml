- en: Chapter 4. Dropout and Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：Dropout 和卷积神经网络
- en: 'In this chapter, we continue to look through the algorithms of deep learning.
    The pre-training that was taken into both DBN and SDA is indeed an innovative
    method, but deep learning also has other innovative methods. Among these methods,
    we''ll go into the details of the particularly eminent algorithms, which are the
    following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续深入探讨深度学习的算法。DBN 和 SDA 所采用的预训练方法的确是一种创新方法，但深度学习也有其他创新方法。在这些方法中，我们将详细介绍一些特别突出的算法，具体如下：
- en: The dropout learning algorithm
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 学习算法
- en: Convolutional neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Both algorithms are necessary to understand and master deep learning, so make
    sure you keep up.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种算法对于理解和掌握深度学习都非常重要，所以一定要跟上进度。
- en: Deep learning algorithms without pre-training
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 没有预训练的深度学习算法
- en: In the previous chapter, you learned that layer-wise training with pre-training
    was a breakthrough for DBN and SDA. The reason why these algorithms need pre-training
    is because an issue occurs where an output error gradually vanishes and doesn't
    work well in neural networks with simple piled-up layers (we call this the vanishing
    gradient problem). The deep learning algorithm needs pre-training whether you
    want to improve the existing method or reinvent it—you might think of it like
    that.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学到了层级训练与预训练是 DBN 和 SDA 的突破性进展。这些算法需要预训练的原因是，因为在神经网络中，由于简单堆叠的层次结构，会出现一个问题，输出误差会逐渐消失，在这样的网络中效果不好（我们称之为梯度消失问题）。无论是想改进现有方法，还是重新发明方法，深度学习算法都需要预训练——你可以这么理解。
- en: However, actually, the deep learning algorithms in this chapter don't have a
    phase of pre-training, albeit in the deep learning algorithm without pre-training,
    we can get a result with higher precision and accuracy. Why is such a thing possible?
    Here is a brief reason. Let's think about why the vanishing gradient problem occurs—remember
    the equation of backpropagation? A delta in a layer is distributed to all the
    units of a previous layer by literally propagating networks backward. This means
    that in the network where all units are tied densely, the value of an error backpropagated
    to each unit becomes small. As you can see from the equations of backpropagation,
    the gradients of the weight are obtained by the multiplication of the weights
    and deltas among the units. Hence, the more terms we have, the more dense the
    networks are and the more possibilities we have for underflow. This causes the
    vanishing gradient problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上，本章中的深度学习算法并没有预训练阶段，尽管在没有预训练的深度学习算法中，我们依然可以得到更高精度和准确度的结果。为什么会发生这种情况呢？这里给出一个简短的原因。让我们思考一下为什么会出现梯度消失问题——记得反向传播的公式吗？一个层中的
    delta 会通过字面上的反向传播网络分布到前一层的所有单元。这意味着在所有单元都紧密连接的网络中，反向传播到每个单元的误差值会变得很小。从反向传播的公式中可以看出，权重的梯度是通过单元之间权重和
    delta 的乘积得到的。因此，我们的网络中项数越多，网络越密集，发生下溢的可能性就越大。这就导致了梯度消失问题。
- en: Therefore, we can say that if the preceding problems can be avoided without
    pre-training, a machine can learn properly with deep neural networks. To achieve
    this, we need to arrange how to connect the networks. The deep learning algorithm
    in this chapter is a method that puts this contrivance into practice using various
    approaches.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说，如果能够避免上述问题而不使用预训练，那么机器就能够在深度神经网络中正常学习。为了实现这一点，我们需要安排如何连接网络。本章中的深度学习算法就是通过各种方法将这种构思付诸实践的。
- en: Dropout
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: 'If there''s a problem with the network being tied densely, just force it to
    be sparse. Then the vanishing gradient problem won''t occur and learning can be
    done properly. The algorithm based on such an idea is the **dropout** algorithm.
    Dropout for deep neural networks was introduced in *Improving neural networks
    by preventing co adaptation of feature detectors* (Hinton, et. al. 2012, [http://arxiv.org/pdf/1207.0580.pdf](http://arxiv.org/pdf/1207.0580.pdf))
    and refined in *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*
    (Srivastava, et. al. 2014, [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)).
    In dropout, some of the units are, literally, forcibly dropped while training.
    What does this mean? Let''s look at the following figures—firstly, neural networks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '如果网络过于密集导致问题，只需强制其变得稀疏。这样，消失梯度问题就不会发生，学习也可以正常进行。基于这种思想的算法就是**dropout**算法。针对深度神经网络的dropout算法首次提出于*通过防止特征检测器共同适应来改进神经网络*（Hinton等，2012，[http://arxiv.org/pdf/1207.0580.pdf](http://arxiv.org/pdf/1207.0580.pdf)），并在*Dropout:
    A Simple Way to Prevent Neural Networks from Overfitting*（Srivastava等，2014，[https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)）中得到了完善。在dropout中，一些单元在训练时被强制“丢弃”。这是什么意思呢？让我们看一下以下图示——首先是神经网络：'
- en: '![Dropout](img/B04779_04_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_01.jpg)'
- en: 'There is nothing special about this figure. It is a standard neural network
    with one input layer, two hidden layers, and one output layer. Secondly, the graphical
    model can be represented as follows by applying dropout to this network:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图没有什么特别之处。它是一个标准的神经网络，包含一个输入层、两个隐藏层和一个输出层。其次，通过向该网络应用dropout，图形模型可以表示为以下方式：
- en: '![Dropout](img/B04779_04_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_02.jpg)'
- en: Units that are dropped from the network are depicted with cross signs. As you
    can see in the preceding figure, dropped units are interpreted as non-existent
    in the network. This means we need to change the structure of the original neural
    network while the dropout learning algorithm is being applied. Thankfully, applying
    dropout to the network is not difficult from a computational standpoint. You can
    simply build a general deep neural network first. Then the dropout learning algorithm
    can be applied just by adding a dropout mask—a simple binary mask—to all the units
    in each layer. Units with the value of 0 in the binary mask are the ones that
    are dropped from the network.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络中丢弃的单元用叉号表示。如前图所示，丢弃的单元在网络中被解释为不存在。这意味着在应用dropout学习算法时，我们需要改变原始神经网络的结构。幸运的是，从计算角度来看，应用dropout到网络并不困难。你可以先构建一个常规的深度神经网络。然后，通过向每一层的所有单元添加一个dropout掩码——一个简单的二进制掩码——来应用dropout学习算法。在二进制掩码中，值为0的单元就是从网络中被丢弃的单元。
- en: This may remind you of DA (or SDA) discussed in the previous chapter because
    DA and dropout look similar at first glance. Corrupting input data in DA also
    adds binary masks to the data when implemented. However, there are two remarkably
    different points between them. First, while it is true that both methods have
    the process of adding masks to neurons, DA applies the mask only to units in the
    input layer, whereas dropout applies it to units in the hidden layer. Some of
    the dropout algorithms apply masks to both the input layer and the hidden layer,
    but this is still different from DA. Second, in DA, once the corrupt input data
    is generated, the data will be used throughout the whole training epochs, but
    in dropout, the data with different masks will be used in each training epoch.
    This indicates that a neural network of a different shape is trained in each iteration.
    Dropout masks will be generated in each layer in each iteration according to the
    probability of dropout.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会让你想起上一章讨论过的DA（或SDA），因为DA和dropout乍一看非常相似。DA在实现时也会将二进制掩码添加到数据中来破坏输入数据。然而，它们之间有两个显著的不同点。首先，虽然这两种方法都有向神经元添加掩码的过程，但DA只将掩码应用于输入层的单元，而dropout则将其应用于隐藏层的单元。某些dropout算法会同时向输入层和隐藏层应用掩码，但这与DA仍然不同。第二，在DA中，一旦生成了损坏的输入数据，这些数据将在整个训练周期中使用，而在dropout中，每个训练周期使用的数据都会有不同的掩码。这意味着每次迭代时训练的神经网络形状都会不同。dropout掩码会根据dropout的概率在每一层中生成。
- en: You might have a question—can we train the model even if the shape of the network
    is different in every step? The answer is yes. You can think of it this way—the
    network is well trained with dropout because it puts more weights on the existing
    neurons to reflect the characteristics of the input data. However, dropout has
    a single demerit, that is, it requires more training epochs than other algorithms
    to train and optimize the model, which means it takes more time until it is optimized.
    Another technique is introduced here to reduce this problem. Although the dropout
    algorithm itself was invented earlier, it was not enough for deep neural networks
    to gain the ability to generalize and get high precision rates just by using this
    method. With one more technique that makes the network even more sparse, we achieve
    deep neural networks to get higher accuracy. This technique is the improvement
    of the activation function, which we can say is a simple yet elegant solution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会有一个问题——即使每一步网络的形状不同，我们也能训练这个模型吗？答案是肯定的。你可以这样理解——通过 dropout 方法，网络能够得到很好的训练，因为它将更多的权重放在现有的神经元上，以反映输入数据的特征。然而，dropout
    有一个缺点，那就是它需要比其他算法更多的训练周期来训练和优化模型，这意味着它需要更多的时间才能优化。这里介绍了一种新的技术来减少这个问题。尽管 dropout
    算法本身是在较早之前就被发明的，但仅仅使用这个方法并不足以让深度神经网络具备泛化能力并获得高精度率。通过再加入一种使网络更加稀疏的技术，我们得以让深度神经网络获得更高的准确性。这项技术就是对激活函数的改进，可以说这是一种简单而优雅的解决方案。
- en: All of the methods of neural networks explained so far utilize the sigmoid function
    or hyperbolic tangent as an activation function. You might get great results with
    these functions. However, as you can see from the shape of them, these curves
    saturate and kill the gradients when the input values or error values at a certain
    layer are relatively large or small.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止解释的所有神经网络方法都使用了 sigmoid 函数或双曲正切作为激活函数。使用这些函数可能会取得不错的结果。然而，正如你从它们的曲线形状中可以看到的，当某一层的输入值或误差值相对较大或较小时，这些曲线会饱和并导致梯度消失。
- en: 'One of the activation functions introduced to solve this problem is the **rectifier**.
    A unit-applied rectifier is called a **Rectified Linear Unit** (**ReLU**). We
    can call the activation function itself ReLU. This function is described in the
    following equation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的激活函数之一是**整流器**。一个应用了整流器的单元叫做**整流线性单元**（**ReLU**）。我们可以将激活函数本身称为 ReLU。这个函数可以用以下方程描述：
- en: '![Dropout](img/B04779_04_15.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_15.jpg)'
- en: 'The function can be represented by the following figure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数可以通过以下图形表示：
- en: '![Dropout](img/B04779_04_03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_03.jpg)'
- en: 'The broken line in the figure is the function called a **softplus function**,
    the derivative of it is logistic function, which can be described as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的虚线是一个被称为**softplus 函数**的函数，它的导数是逻辑斯蒂函数，可以描述如下：
- en: '![Dropout](img/B04779_04_16.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_16.jpg)'
- en: 'This is just for your information: we have the following relations that a smooth
    approximation to the rectifier. As you can see from the figure above, since the
    rectifier is far simpler than the sigmoid function and hyperbolic tangent, you
    can easily guess that the time cost will reduce when it is applied to the deep
    learning algorithm. In addition, because the derivative of the rectifier—which
    is necessary when calculating backpropagation errors—is also simple, we can, additionally,
    shorten the time cost. The equation of the derivative can be represented as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅供参考：我们有以下关系，作为整流器的平滑近似。如你从上面的图中所见，由于整流器比 sigmoid 函数和双曲正切函数简单得多，你可以很容易猜到，当它应用于深度学习算法时，时间成本会降低。此外，由于整流器的导数——在计算反向传播误差时是必需的——也很简单，因此我们还可以进一步缩短时间成本。导数的方程可以表示如下：
- en: '![Dropout](img/B04779_04_17.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_17.jpg)'
- en: Since both the rectifier and the derivative of it are very sparse, we can easily
    imagine that the neural networks will be also sparse through training. You may
    have also noticed that we no longer have to worry about gradient saturations because
    we don't have the causal curves that the sigmoid function and hyperbolic tangent
    contain anymore.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整流器及其导数都非常稀疏，我们可以轻松想象，通过训练，神经网络也将变得更加稀疏。你可能还注意到，我们不再需要担心梯度饱和问题，因为我们不再有 sigmoid
    函数和双曲正切包含的因果曲线。
- en: 'With the technique of dropout and the rectifier, a simple deep neural network
    can learn a problem without pre-training. In terms of the equations used to implement
    the dropout algorithm, they are not difficult because they are just simple methods
    of adding dropout masks to multi-layer perceptrons. Let''s look at them in order:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用dropout技术和修正函数（rectifier），一个简单的深度神经网络可以在没有预训练的情况下学习一个问题。就实现dropout算法所用的方程式而言，它们并不复杂，因为它们只是将dropout掩码添加到多层感知机中的简单方法。我们按顺序来看一下它们：
- en: '![Dropout](img/B04779_04_18.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_18.jpg)'
- en: 'Here, ![Dropout](img/B04779_04_19.jpg) denotes the activation function, which
    is, in this case, the rectifier. You see, the previous equation is for units in
    the hidden layer without dropout. What the dropout does is just apply the mask
    to them. It can be represented as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![Dropout](img/B04779_04_19.jpg)表示激活函数，在本例中是修正函数。你看，前面的方程式是没有dropout的隐藏层单元的方程式。dropout做的就是将掩码应用到这些单元上。它可以表示如下：
- en: '![Dropout](img/B04779_04_20.jpg)![Dropout](img/B04779_04_21.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_20.jpg)![Dropout](img/B04779_04_21.jpg)'
- en: 'Here, ![Dropout](img/B04779_04_22.jpg) denotes the probability of dropout,
    which is generally set to 0.5\. That''s all for forward activation. As you can
    see from the equations, the term of the binary mask is the only difference from
    the ones of general neural networks. In addition, during backpropagation, we also
    have to add masks to the delta. Suppose we have the following equation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![Dropout](img/B04779_04_22.jpg)表示dropout的概率，通常设置为0.5。前向激活到此为止。从方程式可以看出，二进制掩码项是与一般神经网络的唯一不同之处。此外，在反向传播过程中，我们还需要将掩码添加到delta中。假设我们有以下方程式：
- en: '![Dropout](img/B04779_04_23.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_23.jpg)'
- en: 'With this, we can define the delta as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以按如下方式定义delta：
- en: '![Dropout](img/B04779_04_24.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_24.jpg)'
- en: 'Here, ![Dropout](img/B04779_04_25.jpg) denotes the evaluation function (these
    equations are the same as we mentioned in [Chapter 2](ch02.html "Chapter 2. Algorithms
    for Machine Learning – Preparing for Deep Learning"), *Algorithms for Machine
    Learning – Preparing for Deep Learning*). We get the following equation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![Dropout](img/B04779_04_25.jpg)表示评估函数（这些方程式与我们在[第2章](ch02.html "第2章 机器学习算法
    – 准备深度学习")中提到的相同，*机器学习算法 – 准备深度学习*）。我们得到了以下方程式：
- en: '![Dropout](img/B04779_04_26.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_26.jpg)'
- en: 'Here, the delta can be described as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，delta可以按如下方式描述：
- en: '![Dropout](img/B04779_04_27.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_27.jpg)'
- en: 'Now we have all the equations necessary for implementation, let''s dive into
    the implementation. The package structure is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了实现所需的所有方程式，接下来让我们深入到实现中。包结构如下：
- en: '![Dropout](img/B04779_04_04.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_04.jpg)'
- en: 'First, what we need to have is the rectifier. Like other activation functions,
    we implement it in `ActivationFunction.java` as `ReLU`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要的是修正函数（rectifier）。像其他激活函数一样，我们在`ActivationFunction.java`中实现它，命名为`ReLU`：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Also, we define `dReLU` as the derivative of the rectifier:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们将`dReLU`定义为修正函数的导数：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Accordingly, we updated the constructor of `HiddenLayer.java` to support `ReLU`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，我们更新了`HiddenLayer.java`的构造函数，以支持`ReLU`：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let''s have a look at `Dropout.java`. In the source code, we''ll build
    the neural networks of two hidden layers, and the probability of dropout is set
    to 0.5:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下`Dropout.java`。在源代码中，我们将构建一个包含两层隐藏层的神经网络，并且dropout的概率设置为0.5：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The constructor of `Dropout.java` can be written as follows (since the network
    is just a simple deep neural network, the code is also simple):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout.java`的构造函数可以按如下方式编写（由于该网络只是一个简单的深度神经网络，代码也非常简单）：'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As explained, now we have the `HiddenLayer` class with `ReLU` support, we can
    use `ReLU` as the activation function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，现在我们有了支持`ReLU`的`HiddenLayer`类，我们可以使用`ReLU`作为激活函数。
- en: 'Once a model is built, what we do next is train the model with dropout. The
    method for training is simply called `train`. Since we need some layer inputs
    when calculating the backpropagation errors, we define the variable called `layerInputs`
    first to cache their respective input values:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型构建完成，接下来我们要做的就是用dropout训练模型。训练的方法简单地称为`train`。由于在计算反向传播误差时，我们需要一些层的输入，因此我们首先定义一个叫做`layerInputs`的变量来缓存它们各自的输入值：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, `X` is the original training data. We also need to cache the dropout
    masks for each layer for backpropagation, so let''s define it as `dropoutMasks`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`X`是原始训练数据。我们还需要为每一层缓存dropout掩码以进行反向传播，因此我们将其定义为`dropoutMasks`：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Training begins in a forward activation fashion. Look how we apply the dropout
    masks to the value; we merely multiply the activated values and binary masks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 训练以前向激活的方式开始。看看我们如何将dropout掩码应用到值上；我们只是将激活值与二进制掩码相乘：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The dropout method is defined in `Dropout.java` as well. As explained in the
    equation, this method returns the values following the Bernoulli distribution:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout.java` 中也定义了dropout方法。如公式所示，该方法返回遵循伯努利分布的值：'
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After forward propagation through the hidden layers, training data is forward
    propagated in the output layer of the logistic regression. Then, in the same way
    as the other neural networks algorithm, the deltas of each layer are going back
    through the network. Here, we apply the cached masks to the delta so that its
    values are backpropagated in the same network:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过隐藏层的前向传播之后，训练数据会在逻辑回归的输出层进行前向传播。然后，像其他神经网络算法一样，每一层的误差会通过网络反向传播。在这里，我们将缓存的掩码应用于误差，这样它的值就会在相同的网络中进行反向传播：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After the training comes the test phase. But before we apply the test data
    to the tuned model, we need to configure the weights of the network. Dropout masks
    can''t be simply applied to the test data because when masked, the shape of each
    network will be differentiated, and this may return different results because
    a certain unit may have a significant effect on certain features. Instead, what
    we do is smooth the weights of the network, which means we simulate the network
    where whole units are equally masked. This can be done using the following equation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，进入测试阶段。但在将测试数据应用于已经调优的模型之前，我们需要配置网络的权重。不能简单地将dropout掩码应用于测试数据，因为掩码会改变每个网络的形状，这可能会导致不同的结果，因为某个特定的单元可能对某些特征有显著的影响。相反，我们做的是平滑网络的权重，这意味着我们模拟一个全体单元都被掩盖的网络。这个过程可以通过以下公式完成：
- en: '![Dropout](img/B04779_04_28.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](img/B04779_04_28.jpg)'
- en: 'As you can see from the equation, all the weights are multiplied by the probability
    of non-dropout. We define the method for this as `pretest`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如公式所示，所有权重都与非dropout的概率相乘。我们为此定义的方法是`pretest`：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We have to call this method once before the test. Since the network is a general
    multi-layered neural network, what we need to do for the prediction is just perform
    forward activation through the network:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在测试前调用一次此方法。由于网络是一个通用的多层神经网络，对于预测，我们所需要做的就是通过网络执行前向激活：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Compared to DBN and SDA, the dropout MLP is far simpler and easier to implement.
    It suggests the possibility that with a mixture of two or more techniques, we
    can get higher precision.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与DBN和SDA相比，dropout MLP要简单得多，且更容易实现。它暗示着，通过混合两种或更多技术，我们可以获得更高的精度。
- en: Convolutional neural networks
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: All the machine learning/deep learning algorithms you have learned about imply
    that the type of input data is one-dimensional. When you look at a real-world
    application, however, data is not necessarily one-dimensional. A typical case
    is an image. Though we can still convert two-dimensional (or higher-dimensional)
    data into a one-dimensional array from the standpoint of implementation, it would
    be better to build a model that can handle two-dimensional data as it is. Otherwise,
    some information embedded in the data, such as positional relationships, might
    be lost when flattened to one dimension.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你学到的所有机器学习/深度学习算法都假定输入数据是一维的。然而，现实世界中的数据不一定是一维的。一个典型的例子是图像。尽管我们仍然可以从实现的角度将二维（或更高维度）数据转换为一维数组，但最好是构建一个可以处理二维数据的模型。否则，当数据被展平为一维时，数据中某些嵌入的信息，例如位置关系，可能会丢失。
- en: 'To solve this problem, an algorithm called **Convolutional Neural Networks**
    (**CNN**) was proposed. In CNN, features are extracted from two-dimensional input
    data through convolutional layers and pooling layers (this will be explained later),
    and then these features are put into general multi-layer perceptrons. This preprocessing
    for MLP is inspired by human visual areas and can be described as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，提出了一种叫做**卷积神经网络**（**CNN**）的算法。在CNN中，特征通过卷积层和池化层（稍后会解释）从二维输入数据中提取，然后这些特征会输入到普通的多层感知器（MLP）中。MLP的这种预处理灵感来自人类的视觉区域，可以描述如下：
- en: Segment the input data into several domains. This process is equivalent to a
    human's receptive fields.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入数据划分为多个领域。这个过程相当于人类的感受野。
- en: Extract the features from the respective domains, such as edges and position
    aberrations.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各自的领域中提取特征，如边缘和位置畸变。
- en: With these features, MLP can classify data accordingly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些特征，MLP可以相应地进行数据分类。
- en: 'The graphical model of CNN is not similar to that of other neural networks.
    Here is a briefly outlined example of CNN:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的图形模型与其他神经网络不同。以下是CNN的简要示例：
- en: '![Convolutional neural networks](img/B04779_04_05.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络](img/B04779_04_05.jpg)'
- en: You may not fully understand what CNN is just from the figure. Moreover, you
    might feel that CNN is relatively complicated and difficult to understand. But
    you don't have to worry about that. It is a fact that CNN has a complicated graphical
    model and has unfamiliar terminologies such as convolution and pooling, which
    you don't hear about in other deep learning algorithms. However, when you look
    at the model step by step, there's nothing too difficult to understand. CNN consists
    of several types of layers specifically adjusted for image recognition. Let's
    look at each layer one by one in the next subsection. In the preceding figure,
    there are two convolution and pooling (**Subsampling**) layers and fully connected
    multi-layer perceptrons in the network. We'll see what the convolutional layers
    do first.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能从这幅图中无法完全理解CNN是什么。此外，你可能会觉得CNN相对复杂，难以理解。但你不需要担心这一点。CNN的图形模型确实复杂，且包含诸如卷积和池化等不熟悉的术语，这些术语在其他深度学习算法中是听不到的。然而，当你一步步地看模型时，其实并没有什么难以理解的地方。CNN由几种专门调整的层组成，旨在进行图像识别。我们将在下一小节中逐一介绍每一层。在前面的图中，网络中有两个卷积层和池化（**下采样**）层，以及全连接的多层感知机。我们首先来看卷积层的作用。
- en: Convolution
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: 'Convolutional layers literally perform convolution, which means applying several
    filters to the image to extract features. These filters are called **kernels**,
    and convolved images are called **feature maps**. Let''s see the following image
    (decomposed to color values) and kernel:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层实际上执行卷积操作，这意味着应用多个滤波器到图像上以提取特征。这些滤波器称为**核**，卷积后的图像称为**特征图**。我们来看一下下面的图像（已分解为颜色值）和核：
- en: '![Convolution](img/B04779_04_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![卷积](img/B04779_04_06.jpg)'
- en: 'With these, what is done with convolution is illustrated as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些，卷积所做的操作如下所示：
- en: '![Convolution](img/B04779_04_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![卷积](img/B04779_04_07.jpg)'
- en: 'The kernel slides across the image and returns the summation of its values
    within the kernel as a multiplication filter. You might have noticed that you
    can extract many kinds of features by changing kernel values. Suppose you have
    kernels with values as described here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 核在图像上滑动，并将核内值的和作为乘法滤波器返回。你可能已经注意到，通过改变核值，你可以提取多种特征。假设你有如下所述的核值：
- en: '![Convolution](img/B04779_04_08.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![卷积](img/B04779_04_08.jpg)'
- en: You see that the kernel on the left extracts the edges of the image because
    it accentuates the color differences, and the one on the right blurs the image
    because it degrades the original values. The great thing about CNN is that in
    convolutional layers, you don't have to set these kernel values manually. Once
    initialized, CNN itself will learn the proper values through the learning algorithm
    (which means parameters trained in CNN are the weights of kernels) and can classify
    images very precisely in the end.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到左边的核提取了图像的边缘，因为它强调了颜色差异，而右边的核则模糊了图像，因为它降低了原始值。CNN的一个伟大之处在于，在卷积层中，你不需要手动设置这些核值。一旦初始化，CNN本身将通过学习算法自动学习合适的值（这意味着在CNN中训练的参数是核的权重），最终能够非常精确地分类图像。
- en: Now, let's think about why neural networks with convolutional layers (kernels)
    can predict with higher precision rates. The key here is the **local receptive
    field**. In most layers in neural networks except CNN, all neurons are fully connected.
    This even causes slightly different data, for example, one-pixel parallel data
    would be regarded as completely different data in the network because this data
    is propagated to different neurons in hidden layers, whereas humans can easily
    understand they are the same. With fully connected layers, it is true that neural
    networks can recognize more complicated patterns, but at the same time they lack
    the ability to generalize and lack flexibility. In contrast, you can see that
    connections among neurons in convolutional layers are limited to their kernel
    size, making the model more robust to translated images. Thus, neural networks
    with their receptive fields limited locally are able to acquire **translation
    invariance** when kernels are optimized.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们思考一下，为什么具有卷积层（卷积核）的神经网络能够以更高的精度进行预测。关键在于**局部感受野**。在除了CNN之外的大多数神经网络层中，所有神经元都是完全连接的。这甚至导致稍微不同的数据，比如说一像素的平行数据会被认为是完全不同的数据，因为这些数据会传播到隐藏层的不同神经元，而人类则很容易理解它们是相同的。通过完全连接层，神经网络确实能够识别更复杂的模式，但与此同时，它们缺乏泛化能力和灵活性。相比之下，你可以看到卷积层中神经元之间的连接仅限于它们的卷积核大小，这使得模型在处理平移过的图像时更具鲁棒性。因此，具有局部感受野的神经网络在优化卷积核时能够获得**平移不变性**。
- en: 'Each kernel has its own values and extracts respective features from the image.
    Please bear in mind that the number of feature maps and the number of kernels
    are always the same, which means if we have 20 kernels, we have also twenty feature
    maps, that is, convolved images. This can be confusing, so let''s explore another
    example. Given a gray-scaled **image** and twenty **kernels**, how many **feature
    maps** are there? The answer is twenty. These twenty images will be propagated
    to the next layer. This is illustrated as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积核都有自己的值，并从图像中提取相应的特征。请记住，特征图的数量与卷积核的数量始终相同，这意味着如果我们有20个卷积核，我们就有20个特征图，也就是卷积后的图像。这可能会令人困惑，所以我们来探讨另一个例子。假设有一张灰度**图像**和20个**卷积核**，那么有多少个**特征图**呢？答案是二十。这二十张图像将被传播到下一层。如下所示：
- en: '![Convolution](img/B04779_04_09.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![卷积](img/B04779_04_09.jpg)'
- en: 'So, how about this: suppose we have a 3-channeled image (for example, an RGB
    image) and the number of kernels is twenty, how many feature maps will there be?
    The answer is, again, twenty. But this time, the process of convolution is different
    from the one with gray-scaled, that is 1-channeled, images. When the image has
    multiple channels, kernels will be adapted separately for each channel. Therefore,
    in this case, we will have a total of 60 convolved images first, composed of twenty
    mapped images for each of the 3 channels. Then, all the convolved images originally
    from the same image will be combined into one feature map. As a result, we will
    have twenty feature maps. In other words, images are decomposed into different
    channeled data, applied kernels, and then combined into mixed-channeled images
    again. You can easily imagine from the flow in the preceding diagram that when
    we apply a kernel to a multi-channeled image to make decomposed images, the same
    kernel should be applied. This flow can be seen in the following figure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们有一张3通道的图像（例如，RGB图像），且卷积核的数量为20，那么会有多少个特征图呢？答案仍然是20。但这次，卷积过程与灰度（即单通道）图像有所不同。当图像有多个通道时，卷积核将分别适应每个通道。因此，在这种情况下，我们首先会得到60个卷积后的图像，分别由3个通道的每个通道对应20张映射后的图像组成。然后，所有原本来自同一图像的卷积图像将合并成一个特征图。因此，我们将得到20个特征图。换句话说，图像被分解成不同通道的数据，应用卷积核后再重新合并成混合通道的图像。你可以从前面图示的流程中轻松地想象，当我们将卷积核应用于多通道图像以生成分解后的图像时，应该对每个通道应用相同的卷积核。这个流程如下图所示：
- en: '![Convolution](img/B04779_04_10.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![卷积](img/B04779_04_10.jpg)'
- en: Computationally, the number of kernels is represented with the dimension of
    the weights' tensor. You'll see how to implement this later.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算角度来看，卷积核的数量由权重张量的维度表示。稍后你会看到如何实现这一点。
- en: Pooling
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化
- en: What pooling layers do is rather simple compared to convolutional layers. They
    actually do not train or learn by themselves but just downsample images propagated
    from convolutional layers. Why should we bother to do downsampling? You might
    think it may lose some significant information from the data. But here, again,
    as with convolutional layers, this process is necessary to make the network keep
    its translation invariance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积层相比，池化层的工作相对简单。它们实际上不会自己训练或学习，而只是对从卷积层传播过来的图像进行下采样。为什么我们需要进行下采样？你可能认为这样会丢失一些重要的信息。但在这里，再次像卷积层一样，这个过程是必要的，以便让网络保持平移不变性。
- en: 'There are several ways of downsampling, but among them, max-pooling is the
    most famous. It can be represented as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样的方法有很多，但其中最大池化是最著名的。它可以表示为：
- en: '![Pooling](img/B04779_04_11.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![池化](img/B04779_04_11.jpg)'
- en: In a max-pooling layer, the input image is segmented into a set of non-overlapping
    sub-data and the maximum value is output from each data. This process not only
    keeps its translation invariance but also reduces the computation for the upper
    layers. With convolution and pooling, CNN can acquire robust features from the
    input.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大池化层中，输入图像被分割成一组不重叠的子数据，每个数据输出最大值。这个过程不仅保持了其平移不变性，还减少了上层的计算。通过卷积和池化，CNN可以从输入中获取鲁棒的特征。
- en: Equations and implementations
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方程和实现
- en: 'Now we know what convolution and max-pooling are, let''s describe the whole
    model with equations. We''ll use the figure of convolution below in equations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了卷积和最大池化的基本概念，接下来我们用方程来描述整个模型。我们将在下面的方程中使用卷积的图示：
- en: '![Equations and implementations](img/B04779_04_12.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_12.jpg)'
- en: 'As shown in the figure, if we have an image with a size of ![Equations and
    implementations](img/B04779_04_29.jpg) and kernels with a size of ![Equations
    and implementations](img/B04779_04_30.jpg), the convolution can be represented
    as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，如果我们有一个大小为![方程和实现](img/B04779_04_29.jpg)的图像和大小为![方程和实现](img/B04779_04_30.jpg)的卷积核，那么卷积可以表示为：
- en: '![Equations and implementations](img/B04779_04_31.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_31.jpg)'
- en: 'Here, ![Equations and implementations](img/B04779_04_32.jpg) is the weight
    of the kernel, that is, the model parameter. Just bear in mind we''ve described
    each summation from 0, not from 1, so you get a better understanding. The equation,
    however, is not enough when we think about multi-convolutional layers because
    it does not have the information from the channel. Fortunately, it''s not difficult
    because we can implement it just by adding one parameter to the kernel. The extended
    equation can be shown as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![方程和实现](img/B04779_04_32.jpg)是卷积核的权重，也就是模型的参数。只需记住，我们从0开始描述每个求和，而不是从1开始，这样你能更好地理解。可是，考虑到多卷积层时，这个方程还不够，因为它没有包含来自通道的信息。幸运的是，这并不难，因为我们只需在卷积核中加入一个参数就可以实现。扩展后的方程可以表示为：
- en: '![Equations and implementations](img/B04779_04_33.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_33.jpg)'
- en: Here, ![Equations and implementations](img/B04779_04_34.jpg) denotes the channel
    of the image. If the number of kernels is ![Equations and implementations](img/B04779_04_35.jpg)
    and the number of channels is ![Equations and implementations](img/B04779_04_36.jpg),
    we have ![Equations and implementations](img/B04779_04_37.jpg). Then, you can
    see from the equation that the size of the convolved image is ![Equations and
    implementations](img/B04779_04_38.jpg).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![方程和实现](img/B04779_04_34.jpg)表示图像的通道。如果核的数量是![方程和实现](img/B04779_04_35.jpg)，通道的数量是![方程和实现](img/B04779_04_36.jpg)，那么我们有![方程和实现](img/B04779_04_37.jpg)。然后，从方程中可以看出，卷积图像的大小是![方程和实现](img/B04779_04_38.jpg)。
- en: 'After the convolution, all the convolved values will be activated by the activation
    function. We''ll implement CNN with the rectifier—the most popular function these
    days—but you may use the sigmoid function, the hyperbolic tangent, or any other
    activation functions available instead. With the activation, we have:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积后，所有卷积后的值将通过激活函数进行激活。我们将使用Rectifier（当下最流行的激活函数）来实现CNN，但你也可以使用Sigmoid函数、双曲正切函数或任何其他可用的激活函数。经过激活后，我们得到：
- en: '![Equations and implementations](img/B04779_04_39.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_39.jpg)'
- en: Here, ![Equations and implementations](img/B04779_04_40.jpg) denotes the bias,
    the other model parameter. You can see that ![Equations and implementations](img/B04779_04_40.jpg)
    doesn't have subscripts of ![Equations and implementations](img/B04779_04_41.jpg)
    and ![Equations and implementations](img/B04779_04_42.jpg), that is, we have ![Equations
    and implementations](img/B04779_04_43.jpg), a one-dimensional array. Thus, we
    have forward-propagated the values of the convolutional layer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![公式与实现](img/B04779_04_40.jpg)表示偏置，即另一个模型参数。你可以看到，![公式与实现](img/B04779_04_40.jpg)没有![公式与实现](img/B04779_04_41.jpg)和![公式与实现](img/B04779_04_42.jpg)的下标，也就是说，我们有![公式与实现](img/B04779_04_43.jpg)，这是一个一维数组。这样，我们就完成了卷积层的前向传播。
- en: 'Next comes the max-pooling layer. The propagation can simply be written as
    follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是最大池化层。传播过程可以简单地写成如下形式：
- en: '![Equations and implementations](img/B04779_04_44.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![公式与实现](img/B04779_04_44.jpg)'
- en: Here, ![Equations and implementations](img/B04779_04_45.jpg) and ![Equations
    and implementations](img/B04779_04_46.jpg) are the size of pooling filter and
    ![Equations and implementations](img/B04779_04_47.jpg). Usually, ![Equations and
    implementations](img/B04779_04_45.jpg) and ![Equations and implementations](img/B04779_04_46.jpg)
    are set to the same value of 2 ~ 4.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![公式与实现](img/B04779_04_45.jpg)和![公式与实现](img/B04779_04_46.jpg)是池化滤波器的大小，而![公式与实现](img/B04779_04_47.jpg)表示池化层的输出。通常，![公式与实现](img/B04779_04_45.jpg)和![公式与实现](img/B04779_04_46.jpg)都设置为相同的值，通常为2~4。
- en: These two layers, the convolutional layer and the max-pooling layer, tend to
    be arrayed in this order, but you don't necessarily have to follow it. You can
    put two convolutional layers before max-pooling, for example. Also, while we put
    the activation right after the convolution, sometimes it is set after the max-pooling
    instead of the convolution. For simplicity, however, we'll implement CNN with
    the order and sequence of convolution–activation–max-pooling.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这两层——卷积层和最大池化层——通常按此顺序排列，但你不一定必须按这个顺序。你可以在最大池化之前放置两个卷积层。例如，虽然我们将激活函数放在卷积后面，但有时它会被设置在最大池化之后，而不是卷积之后。为了简单起见，我们将按卷积–激活–最大池化的顺序实现CNN。
- en: Tip
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: One important note here is that although the kernel weights will be learned
    from the data, the architecture, the size of kernel, and the size of pooling are
    all parameters.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个重要的提示，虽然核权重将从数据中学习，但架构、核的大小以及池化的大小都是模型参数。
- en: The simple MLP follows after convolutional layers and max-pooling layers to
    classify the data. Here, since MLP can only accept one-dimensional data, we need
    to flatten the downsampled data as preprocessing to adapt it to the input layer
    of MLP. The extraction of features was completed before MLP, so formatting the
    data into one dimension won't be a problem. Thus, CNN can classify the image data
    once the model is optimized. To do this, as with other neural networks, the backpropagation
    algorithm is applied to CNN to train the model. We won't mention the equation
    related to MLP here.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的MLP在卷积层和最大池化层后面，用于分类数据。这里，由于MLP只能接受一维数据，我们需要将下采样后的数据展平，作为预处理步骤，以适配MLP的输入层。在MLP之前已经完成了特征提取，因此将数据格式化为一维数据不会成为问题。因此，一旦模型优化完成，CNN就能够对图像数据进行分类。为此，与其他神经网络一样，反向传播算法被应用于CNN以训练模型。我们在这里不提及与MLP相关的公式。
- en: 'The error from the input layer of MLP is backpropagated to the max-pooling
    layer, and this time it is unflattened to two dimensions to be adapted properly
    to the model. Since the max-pooling layer doesn''t have model parameters, it simply
    backpropagates the error to the previous layer. The equation can be described
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的输入层的误差被反向传播到最大池化层，这时它会被重新展平为二维，以便正确适应模型。由于最大池化层没有模型参数，它只是将误差反向传播到前一层。该公式可以描述如下：
- en: '![Equations and implementations](img/B04779_04_48.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![公式与实现](img/B04779_04_48.jpg)'
- en: 'Here, ![Equations and implementations](img/B04779_04_49.jpg) denotes the evaluation
    function. This error is then backpropagated to the convolutional layer, and with
    it we can calculate the gradients of the weight and the bias. Since the activation
    with the bias comes before the convolution when backpropagating, let''s see the
    gradient of the bias first, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![公式与实现](img/B04779_04_49.jpg)表示评估函数。然后，这个误差被反向传播到卷积层，利用这个误差我们可以计算权重和偏置的梯度。由于偏置与激活函数在反向传播时出现在卷积之前，下面我们先来看偏置的梯度，公式如下：
- en: '![Equations and implementations](img/B04779_04_50.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_50.jpg)'
- en: 'To proceed with this equation, we define the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续进行这个方程，我们定义以下内容：
- en: '![Equations and implementations](img/B04779_04_51.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_51.jpg)'
- en: 'We also define:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义：
- en: '![Equations and implementations](img/B04779_04_52.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_52.jpg)'
- en: 'With these, we get:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些，我们得到：
- en: '![Equations and implementations](img/B04779_04_53.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_53.jpg)'
- en: 'We can calculate the gradient of the weight (kernel) in the same way:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用相同的方式计算权重（卷积核）的梯度：
- en: '![Equations and implementations](img/B04779_04_54.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_54.jpg)'
- en: 'Thus, we can update the model parameters. If we have just one convolutional
    and max-pooling layer, the equations just given are all that we need. When we
    think of multi-convolutional layers, however, we also need to calculate the error
    of the convolutional layers. This can be represented as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以更新模型参数。如果我们只有一个卷积层和一个最大池化层，那么刚才给出的方程就是我们需要的全部。然而，当我们考虑多卷积层时，我们还需要计算卷积层的误差。这个可以表示如下：
- en: '![Equations and implementations](img/B04779_04_55.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_55.jpg)'
- en: 'Here, we get:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们得到：
- en: '![Equations and implementations](img/B04779_04_56.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_56.jpg)'
- en: 'So, the error can be written as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，误差可以写成如下形式：
- en: '![Equations and implementations](img/B04779_04_57.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_57.jpg)'
- en: We have to be careful when calculating this because there's a possibility of
    ![Equations and implementations](img/B04779_04_58.jpg) or ![Equations and implementations](img/B04779_04_59.jpg),
    where there's no element in between the feature maps. To solve this, we need to
    add zero paddings to the top-left edges of them. Then, the equation is simply
    a convolution with the kernel flipped along both axes. Though the equations in
    CNN might look complicated, they are just a pile of summations of each parameter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算这个时，我们必须小心，因为可能会出现![方程和实现](img/B04779_04_58.jpg) 或者![方程和实现](img/B04779_04_59.jpg)，其中特征图之间没有元素。为了解决这个问题，我们需要在它们的左上角添加零填充。然后，方程实际上就是一个卷积，卷积核沿两个轴翻转。尽管卷积神经网络中的方程看起来很复杂，但它们其实只是每个参数的求和。
- en: 'With all the previous equations, we can now implement CNN, so let''s see how
    we do it. The package structure is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有了前面所有的方程，我们现在可以实现 CNN，接下来让我们看看如何实现它。包结构如下：
- en: '![Equations and implementations](img/B04779_04_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_13.jpg)'
- en: '`ConvolutionNeuralNetworks.java` is used to build the model outline of CNN,
    and the exact algorithms for training in the convolutional layers and max-pooling
    layers, forward propagations, and backpropagations are written in `ConvolutionPoolingLayer.java`.
    In the demo, we have the original image size of `12` ![Equations and implementations](img/B04779_04_60.jpg)
    ` 12` with one channel:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConvolutionNeuralNetworks.java` 用于构建 CNN 的模型框架，卷积层和最大池化层的训练算法，以及前向传播和反向传播的具体实现写在
    `ConvolutionPoolingLayer.java` 中。在演示中，我们的原始图像大小为 `12` ![方程和实现](img/B04779_04_60.jpg)
    ` 12`，且只有一个通道：'
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The image will be propagated through two `ConvPoolingLayer` (convolutional
    layers and max-pooling layers). The number of kernels in the first layer is set
    to `10` with the size of `3` ![Equations and implementations](img/B04779_04_60.jpg)
    ` 3` and `20` with the size of `2 ` ![Equations and implementations](img/B04779_04_60.jpg)
    ` 2` in the second layer. The size of the pooling filters are both set to `2 `
    ![Equations and implementations](img/B04779_04_60.jpg) ` 2`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图像将通过两个 `ConvPoolingLayer`（卷积层和最大池化层）进行传播。第一层中的卷积核数量设置为 `10`，大小为 `3` ![方程和实现](img/B04779_04_60.jpg)
    ` 3`，第二层中设置为 `20`，大小为 `2` ![方程和实现](img/B04779_04_60.jpg) ` 2`。池化滤波器的大小都设置为 `2`
    ![方程和实现](img/B04779_04_60.jpg) ` 2`：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After the second max-pooling layer, there are `20` feature maps with the size
    of `2` ![Equations and implementations](img/B04779_04_60.jpg) ` 2`. These maps
    are then flattened to `80` units and will be forwarded to the hidden layer with
    `20` neurons:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个最大池化层之后，特征图的数量为 `20`，大小为 `2` ![方程和实现](img/B04779_04_60.jpg) ` 2`。这些特征图然后被展平为
    `80` 个单元，并将被传递到具有 `20` 个神经元的隐藏层：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then create simple demo data of three patterns with a little noise. We''ll
    leave out the code to create demo data here. If we illustrate the data, here is
    an example of it:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了三个带有少量噪声的简单示范数据。这里我们将省略创建示范数据的代码。如果我们展示数据，示例如下：
- en: '![Equations and implementations](img/B04779_04_14.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![方程和实现](img/B04779_04_14.jpg)'
- en: 'Now let''s build the model. The constructor is similar to other deep learning
    models and rather simple. We construct multi `ConvolutionPoolingLayers` first.
    The size for each layer is calculated in the method:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建模型。构造函数类似于其他深度学习模型，且相对简单。我们首先构建多个`ConvolutionPoolingLayers`。每一层的大小在方法中计算：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When you look at the constructor of the `ConvolutionPoolingLayer` class, you
    can see how the kernel and the bias are defined:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看`ConvolutionPoolingLayer`类的构造函数时，你可以看到核（kernel）和偏置（bias）是如何定义的：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next comes the construction of MLP. Don''t forget to flatten the downsampled
    data when passing through them:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是MLP的构建。传递数据时，别忘了将下采样后的数据展平：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once the model is built, we need to train it. In the `train` method, we cache
    all the forward-propagated data so that we can utilize it when backpropagating:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型构建完成，我们需要训练它。在`train`方法中，我们缓存所有前向传播的数据，以便在反向传播时使用：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`preActivated_X` is defined for convolved feature maps, `activated_X` for activated
    features, and `downsampled_X` for downsampled features. We put and cache the original
    data into `downsampled_X`. The actual training begins with forward propagation
    through convolution and max-pooling:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`preActivated_X`用于卷积特征图，`activated_X`用于激活后的特征，`downsampled_X`用于下采样后的特征。我们将原始数据放入并缓存到`downsampled_X`中。实际的训练从通过卷积和最大池化的前向传播开始：'
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `forward` method of `ConvolutionPoolingLayer` is simple and consists of
    `convolve` and `downsample`. The `convolve` function does the convolution, and
    `downsample` does the max-pooling:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConvolutionPoolingLayer`的`forward`方法很简单，由`convolve`和`downsample`组成。`convolve`函数执行卷积操作，`downsample`执行最大池化操作：'
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The values of `preActivated_X` and `activated_X` are set inside the convolve
    method. You can see that the method simply follows the equations explained previously:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`preActivated_X`和`activated_X`的值是在卷积方法中设置的。你可以看到该方法只是简单地遵循之前解释的方程式：'
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `downsample` method follows the equations as well:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`downsample`方法同样遵循方程式：'
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You might think we've made some mistake here because there are so many `for`
    loops in these methods, but actually there's nothing wrong. As you can see from
    the equations of CNN, the algorithm requires many loops because it has many parameters.
    The code here works well, but practically, you could define and move the part
    of the innermost loops to other methods. Here, to get a better understanding,
    we've implemented CNN with many nested loops so that we can compare the code with
    equations. You can see now that CNN requires a lot of time to get results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为我们在这里犯了些错误，因为这些方法中有很多`for`循环，但实际上并没有问题。正如你从CNN的方程式中看到的，算法需要很多循环，因为它有很多参数。这里的代码是正常工作的，但实际上，你可以将最内层循环的一部分定义并移动到其他方法中。为了更好地理解，我们在这里实现了带有许多嵌套循环的CNN，以便将代码与方程式进行比较。你现在可以看到，CNN需要大量时间才能得到结果。
- en: 'After we downsample the data, we need to flatten it:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行数据下采样后，需要将其展平：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The data is then forwarded to the hidden layer:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数据随后被转发到隐藏层：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Multi-class logistic regression is used in the output layer and the delta is
    then backpropagated to the hidden layer:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层使用多类逻辑回归，然后将误差（delta）反向传播到隐藏层：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `backward` method of `ConvolutionPoolingLayer` is the same as `forward`,
    also simple. Backpropagation of max-pooling is written in `upsample` and that
    of convolution is in `deconvolve`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConvolutionPoolingLayer`的`backward`方法与`forward`方法相同，也很简单。最大池化的反向传播写在`upsample`中，卷积的反向传播写在`deconvolve`中：'
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'What `upsample` does is just transfer the delta to the convolutional layer:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`upsample`的作用只是将误差（delta）传递到卷积层：'
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In `deconvolve`, we need to update the model parameter. Since we train the
    model with mini-batches, we calculate the summation of the gradients first:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在`deconvolve`中，我们需要更新模型参数。由于我们使用小批量训练模型，我们首先计算梯度的总和：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, update the weight and the bias using these gradients:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用这些梯度更新权重和偏置：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Unlike other algorithms, we have to calculate the parameters and delta discretely
    in CNN:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他算法不同，我们必须在CNN中分别计算参数和误差（delta）：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we train the model, so let''s go on to the test part. The method for testing
    or prediction simply does the forward propagation, just like the other algorithms:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们训练模型，接下来进入测试部分。测试或预测的方法简单地进行前向传播，就像其他算法一样：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Congratulations! That's all for CNN. Now you can run the code and see how it
    works. Here, we have CNN with two-dimensional data as input, but CNN can also
    have three-dimensional data if we expand the model. We can expect its application
    in medical fields, for example, finding malignant tumors from 3D-scanned data
    of human brains.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！CNN的内容到此为止。现在你可以运行代码，看看它是如何工作的。在这里，我们使用的是二维数据作为输入的CNN，但如果我们扩展模型，CNN也可以处理三维数据。我们可以预期它在医学领域的应用，例如，从人类大脑的三维扫描数据中发现恶性肿瘤。
- en: The process of convolution and pooling was originally invented by LeCun et al.
    in 1998 ([http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)),
    yet as you can see from the codes, it requires much calculation. We can assume
    that this method might not have been suitable for practical applications with
    computers at the time, not to mention making it deep. The reason CNN has gained
    more attention recently is probably because the power and capacity of computers
    has greatly developed. But still, we can't deny the problem. Therefore, it seems
    practical to use GPU, not CPU, when we have CNN with certain amounts of data.
    Since the implementation to optimize the algorithm to GPU is complicated, we won't
    write the codes here. Instead, in [Chapter 5](ch05.html "Chapter 5. Exploring
    Java Deep Learning Libraries – DL4J, ND4J, and More"), *Exploring Java Deep Learning
    Libraries – DL4J, ND4J, and More* and [Chapter 7](ch07.html "Chapter 7. Other
    Important Deep Learning Libraries"), *Other Important Deep Learning Libraries*,
    you'll see the library of deep learning that is capable of utilizing GPU.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和池化过程最初由LeCun等人在1998年发明（[http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)），然而正如你从代码中看到的，它需要大量计算。我们可以假设，这种方法在当时可能不适合用于计算机的实际应用，更不用说让它变得深度了。CNN近年来受到更多关注的原因可能是计算机的性能和能力得到了极大的发展。但即便如此，我们不能否认这个问题。因此，在我们拥有一定数据量的CNN时，使用GPU而非CPU似乎是实际可行的。由于将算法优化为GPU的实现较为复杂，我们在这里不会编写代码。相反，在[第5章](ch05.html
    "第5章。探索Java深度学习库 – DL4J、ND4J等")，*探索Java深度学习库 – DL4J、ND4J等*和[第7章](ch07.html "第7章。其他重要的深度学习库")，*其他重要的深度学习库*，你将看到能够利用GPU的深度学习库。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned about two deep learning algorithms that don''t
    require pre-training: deep neural networks with dropout and CNN. The key to high
    precision rates is how we make the network sparse, and dropout is one technique
    to achieve this. Another technique is the rectifier, the activation function that
    can solve the problem of saturation that occurred in the sigmoid function and
    the hyperbolic tangent. CNN is the most popular algorithm for image recognition
    and has two features: convolution and max-pooling. Both of these attribute the
    model to acquire translation invariance. If you are interested in how dropout,
    rectifier, and other activation functions contribute to the performance of neural
    networks, the following could be good references: *Deep Sparse Rectifier Neural
    Networks* (Glorot, et. al. 2011, [http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf](http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)),
    *ImageNet Classification with Deep Convolutional Neural Networks* (Krizhevsky
    et. al. 2012, [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)),
    and *Maxout Networks* (Goodfellow et al. 2013, [http://arxiv.org/pdf/1302.4389.pdf](http://arxiv.org/pdf/1302.4389.pdf)).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了两种不需要预训练的深度学习算法：带有丢弃法的深度神经网络和CNN。高精度率的关键在于如何让网络变得稀疏，而丢弃法是实现这一目标的技术之一。另一种技术是修正器，即激活函数，它可以解决sigmoid函数和双曲正切函数中出现的饱和问题。CNN是最流行的图像识别算法，具有两个特点：卷积和最大池化。这两者使得模型具备了平移不变性。如果你对丢弃法、修正器和其他激活函数如何提高神经网络的性能感兴趣，以下的文献可能会是很好的参考：*深度稀疏修正器神经网络*（Glorot等人，2011，[http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf](http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)），*使用深度卷积神经网络进行ImageNet分类*（Krizhevsky等人，2012，[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)），以及*Maxout网络*（Goodfellow等人，2013，[http://arxiv.org/pdf/1302.4389.pdf](http://arxiv.org/pdf/1302.4389.pdf)）。
- en: 'While you now know the popular and useful deep learning algorithms, there are
    still many of them that have not been mentioned in this book. This field of study
    is getting more and more active, and more and more new algorithms are appearing.
    But don''t worry, as all the algorithms are based on the same root: neural networks.
    Now you know the way of thinking required to grasp or implement the model, you
    can fully understand whatever models you encounter.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你现在已经了解了流行且有用的深度学习算法，但仍有许多算法在本书中没有提及。这个研究领域越来越活跃，新的算法层出不穷。但别担心，因为所有算法都基于相同的根基：神经网络。现在你已经掌握了理解或实现模型所需的思维方式，遇到任何模型时，你都能完全理解。
- en: We've implemented deep learning algorithms from scratch so you fully understand
    them. In the next chapter, you'll see how we can implement them with deep learning
    libraries to facilitate our research or applications.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从零开始实现了深度学习算法，以帮助你完全理解它们。在接下来的章节中，你将看到我们如何使用深度学习库来实现这些算法，从而促进我们的研究或应用。
