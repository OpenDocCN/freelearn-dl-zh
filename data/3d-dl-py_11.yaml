- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Mesh R-CNN
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mesh R-CNN
- en: This chapter is dedicated to a state-of-the-art model called Mesh R-CNN, which
    aims to combine two different but important tasks into one end-to-end model. It
    is a combination of the well-known image segmentation model Mask R-CNN and a new
    3D structure prediction model. These two tasks were researched a lot separately.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于介绍一个最先进的模型，称为 Mesh R-CNN，旨在将两项不同但重要的任务结合为一个端到端的模型。它结合了著名的图像分割模型 Mask R-CNN
    和一个新的 3D 结构预测模型。这两项任务以前是单独研究的。
- en: Mask R-CNN is an object detection and instance segmentation algorithm that got
    the highest precision scores in benchmark datasets. It belongs to the R-CNN family
    and is a two-stage end-to-end object detection model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 是一种目标检测和实例分割算法，在基准数据集上获得了最高的精度分数。它属于 R-CNN 家族，是一个两阶段的端到端目标检测模型。
- en: Mesh R-CNN goes beyond the 2D object detection problem and outputs a 3D mesh
    of detected objects as well. If we think of the world, people see in 3D, which
    means the objects are 3D. So, why not have a detection model that outputs objects
    in 3D as well?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Mesh R-CNN 超越了 2D 目标检测问题，同时输出被检测物体的 3D 网格。如果我们认为世界是三维的，人类看到的物体也是三维的。那么，为什么不让一个检测模型同时输出三维物体呢？
- en: In this chapter, we are going to understand how Mesh R-CNN works. Moreover,
    we will dive deeper into understanding different elements and techniques used
    in models such as voxels, meshes, graph convolutional networks, and Cubify operators.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将理解 Mesh R-CNN 的工作原理。此外，我们将深入理解模型中使用的不同元素和技术，如体素、网格、图卷积网络以及 Cubify 运算符。
- en: Next, we will explore the GitHub repository provided by the authors of the Mesh
    R-CNN paper. We will try the demo on our image and visualize the results of the
    prediction.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索 Mesh R-CNN 论文作者提供的 GitHub 仓库。我们将在自己的图像上尝试演示，并可视化预测结果。
- en: Finally, we will discuss how we can reproduce the training and testing of Mesh
    R-CNN and understand the benchmark of the model accuracy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论如何重现 Mesh R-CNN 的训练和测试过程，并理解模型精度的基准。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要内容：
- en: Understanding mesh and voxel structures
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解网格和体素结构
- en: Understanding the structure of the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型的结构
- en: Understanding what a graph convolution is
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解什么是图卷积
- en: Trying the demo of Mesh R-CNN
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试 Mesh R-CNN 的演示
- en: Understanding the training and testing process of the model
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型的训练和测试过程
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To run the example code snippets in this book, ideally, you need to have a computer
    with a GPU. However, running the code snippets with only CPUs is not impossible.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本书中的示例代码，理想情况下，你需要一台配备 GPU 的计算机。然而，仅使用 CPU 运行代码片段也是可能的。
- en: 'The following are the recommended computer configurations:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是推荐的计算机配置：
- en: A GPU from, for example, the NVIDIA GTX series or RTX series with at least 8
    GB of memory
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个来自 NVIDIA GTX 系列或 RTX 系列的 GPU，至少需要 8 GB 的内存
- en: Python 3
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3
- en: The PyTorch library and PyTorch3D libraries
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 库和 PyTorch3D 库
- en: Detectron2
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Detectron2
- en: The Mesh R-CNN repository, which can be found at [https://github.com/facebookresearch/meshrcnn](https://github.com/facebookresearch/meshrcnn)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesh R-CNN 仓库，可以在 [https://github.com/facebookresearch/meshrcnn](https://github.com/facebookresearch/meshrcnn)
    找到
- en: The code snippets for this chapter can be found at [https://github.com/PacktPublishing/3D-Deep-Learning-with-Python](https://github.com/PacktPublishing/3D-Deep-Learning-with-Python).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码片段可以在 [https://github.com/PacktPublishing/3D-Deep-Learning-with-Python](https://github.com/PacktPublishing/3D-Deep-Learning-with-Python)
    找到。
- en: Overview of meshes and voxels
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格和体素概述
- en: As mentioned earlier in this book, meshes and voxels are two different 3D data
    representations. Mesh R-CNN uses both representations to get better quality 3D
    structure predictions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书前面提到的，网格和体素是两种不同的 3D 数据表示方式。Mesh R-CNN 使用这两种表示方式来获得更高质量的 3D 结构预测。
- en: 'A mesh is the surface of a 3D model represented as polygons, where each polygon
    can be represented as a triangle. Meshes consist of vertices connected by edges.
    The edge and vertex connection creates faces that have a commonly triangular shape.
    This representation is good for faster transformations and rendering:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 网格是 3D 模型的表面，以多边形形式表示，每个多边形可以表示为三角形。网格由通过边连接的顶点组成。边和顶点的连接创建了通常呈三角形的面。这种表示方式有助于更快速的变换和渲染：
- en: '![Figure 10.1: Example of a polygon mesh ](img/B18217_10_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1：多边形网格示例](img/B18217_10_001.jpg)'
- en: 'Figure 10.1: Example of a polygon mesh'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：多边形网格示例
- en: 'Voxels are the 3D analogs of 2D pixels. As each image consists of 2D pixels,
    it is logical to use the same idea to represent 3D data. Each voxel is a cube,
    and each object is a group of cubes where some of them are the outer visible parts,
    and some of them are inside the object. It’s easier to visualize 3D objects with
    voxels, but it’s not the only use case. In deep learning problems, voxels can
    be used as input for 3D convolutional neural networks:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 体素是 3D 版本的 2D 像素。每个图像由 2D 像素组成，因此采用相同的思想来表示 3D 数据是合乎逻辑的。每个体素都是一个立方体，而每个物体则由一组体素组成，其中一些是外部可见部分，另一些则位于物体内部。使用体素更容易可视化
    3D 物体，但这并不是唯一的使用场景。在深度学习问题中，体素可以作为 3D 卷积神经网络的输入。
- en: '![Figure 10.2: Example of a voxel ](img/B18217_10_002.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2：体素示例](img/B18217_10_002.jpg)'
- en: 'Figure 10.2: Example of a voxel'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：体素示例
- en: Mesh R-CNN uses both types of 3D data representations. Experiments have shown
    that predicting voxels and then converting them into the mesh, and then refining
    the mesh, helps the network learn better.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Mesh R-CNN 使用两种类型的 3D 数据表示。实验表明，先预测体素，然后将其转换为网格，接着再精细化网格，帮助网络更好地学习。
- en: Next, we’ll look at the Mesh R-CNN architecture to see how the aforementioned
    3D representations of data are created from image input.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看 Mesh R-CNN 架构，了解上述 3D 数据表示是如何从图像输入中创建的。
- en: Mesh R-CNN architecture
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mesh R-CNN 架构
- en: '3D shape detection has captured the interest of many researchers. Many models
    have been developed that have gotten good accuracy, but they mostly focused on
    synthetic benchmarks and isolated objects:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 形状检测吸引了许多研究人员的兴趣。虽然已经开发出了许多模型，取得了不错的精度，但它们大多集中于合成基准和孤立物体的检测：
- en: '![Figure 10.3: 3D object examples of the ShapeNet dataset ](img/B18217_10_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3：ShapeNet 数据集中的 3D 物体示例](img/B18217_10_003.jpg)'
- en: 'Figure 10.3: 3D object examples of the ShapeNet dataset'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：ShapeNet 数据集中的 3D 物体示例
- en: At the same time, 2D object detection and image segmentation problems have had
    rapid advances as well. Many models and architectures solve this problem with
    high accuracy and speed. There are solutions for localizing objects and detecting
    the bounding boxes and masks. One of them is called Mask R-CNN, which is a model
    for object detection and instance segmentation. This model is state-of-the-art
    and has a lot of real-life applications.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，2D 目标检测和图像分割问题也取得了快速进展。许多模型和架构以高精度和高速解决了这一问题。已经有方法可以定位物体并检测边界框和掩码。其中一种被称为
    Mask R-CNN，是一种用于目标检测和实例分割的模型。该模型处于最前沿，并具有许多现实生活中的应用。
- en: 'However, we see the world in 3D. The authors of the Mesh R-CNN paper decided
    to combine these two approaches into a single solution: a model that detects the
    object on a realistic image and outputs the 3D mesh instead of the mask. The new
    model takes a state-of-the-art object detection model, which takes an RGB image
    as input and outputs the class label, segmentation mask, and 3D mesh of the objects.
    The authors have added a new branch to Mask R-CNN that is responsible for predicting
    high-resolution triangle meshes:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们看到的是一个 3D 的世界。Mesh R-CNN 论文的作者决定将这两种方法结合成一个单一的解决方案：一个能够检测真实图像中的物体，并输出 3D
    网格而非掩码的模型。这个新模型采用最先进的目标检测模型，输入为 RGB 图像，输出物体的类别标签、分割掩码和 3D 网格。作者在 Mask R-CNN 中添加了一个新分支，用于预测高分辨率的三角形网格：
- en: '![Figure 10.5: Mesh R-CNN general structure ](img/B18217_10_004.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5：Mesh R-CNN 一般结构](img/B18217_10_004.jpg)'
- en: 'Figure 10.5: Mesh R-CNN general structure'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：Mesh R-CNN 一般结构
- en: 'The authors aimed to create one model that is end-to-end trainable. That is
    why they took the state-of-the-art Mask R-CNN model and added a new branch for
    mesh prediction. Before diving deeper into the mesh prediction part, let’s quickly
    recap Mask R-CNN:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的目标是创建一个可以端到端训练的模型。因此，他们采用了最先进的 Mask R-CNN 模型，并为网格预测添加了一个新分支。在深入研究网格预测部分之前，让我们快速回顾一下
    Mask R-CNN：
- en: '![Figure 10.6: Mask R-CNN structure (Reference: https://arxiv.org/abs/1703.06870)
    ](img/B18217_10_005Redraw.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6：Mask R-CNN 结构（参考文献：[https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)）](img/B18217_10_005Redraw.jpg)'
- en: 'Figure 10.6: Mask R-CNN structure (Reference: https://arxiv.org/abs/1703.06870)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：Mask R-CNN 结构（参考文献：[https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)）
- en: 'Mask R-CNN takes an RGB image as input and outputs bounding boxes, category
    labels, and instance segmentation masks. First, the image passes through the backbone
    network, which is typically based on ResNet – for example, ResNet-50-FPN. The
    backbone network outputs the feature map, which is the input of the next network:
    the **region proposal network** (**RPN**). This network outputs proposals. The
    object classification and mask prediction branches then process the proposals
    and output classes and masks, respectively.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 以 RGB 图像作为输入，并输出边界框、类别标签和实例分割掩码。首先，图像通过主干网络，该网络通常基于 ResNet —— 例如
    ResNet-50-FPN。主干网络输出特征图，作为下一个网络的输入：**区域建议网络**（**RPN**）。该网络输出建议。随后，目标分类和掩码预测分支处理这些建议，并分别输出类别和掩码。
- en: 'This structure of Mask R-CNN is the same for Mesh R-CNN as well. However, in
    the end, a mesh predictor was added. A mesh predictor is a new module that consists
    of two branches: the voxel branch and the mesh refinement branch.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 的这个结构在 Mesh R-CNN 中也是相同的。然而，最终增加了一个网格预测器。网格预测器是一个由两个分支组成的新模块：体素分支和网格精细化分支。
- en: 'The voxel branch takes proposed and aligned features as input and outputs the
    coarse voxel predictions. These are then given as input to the mesh refinement
    branch, which outputs the final mesh. The losses of the voxel branch and mesh
    refinement branch are added to the box and mask losses and the model is trained
    end to end:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 体素分支将提议和对齐后的特征作为输入，并输出粗略的体素预测。然后，这些预测作为输入提供给网格精细化分支，后者输出最终的网格。体素分支和网格精细化分支的损失与框和掩码的损失相加，模型进行端到端的训练：
- en: '![Figure 10.7: Mesh R-CNN architecture ](img/B18217_10_006.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7：Mesh R-CNN 架构](img/B18217_10_006.jpg)'
- en: 'Figure 10.7: Mesh R-CNN architecture'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：Mesh R-CNN 架构
- en: Graph convolutions
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图卷积
- en: Before we look at the structure of the mesh predictor, let’s understand what
    a graph convolution is and how it works.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究网格预测器的结构之前，先了解什么是图卷积以及它是如何工作的。
- en: Early variants of neural networks were adopted for structured Euclidean data.
    However, in the real world, most data is non-Euclidian and has graph structures.
    Recently, many variants of neural networks have started to adapt to graph data
    as well, with one of them being convolutional networks, which are called **graph
    convolutional networks** (**GCNs**).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的早期变体被应用于结构化的欧几里得数据。然而，在现实世界中，大多数数据是非欧几里得的，并具有图结构。近年来，许多神经网络变体已开始适应图数据，其中之一就是卷积网络，称为**图卷积网络**（**GCN**）。
- en: 'Meshes have this graph structure, which is why GCNs are applicable in 3D structure
    prediction problems. The basic operation of a CNN is convolution, which is done
    using filters. We use the sliding window technique for convolution, and the filters
    include weights that the model should learn. GCNs use a similar technique for
    convolution, though the main difference is that the number of nodes can vary,
    and the nodes are unordered:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 网格具有这种图结构，这也是 GCN 可应用于 3D 结构预测问题的原因。CNN 的基本操作是卷积，卷积是通过滤波器进行的。我们使用滑动窗口技术进行卷积，滤波器包括模型应学习的权重。GCN
    使用类似的卷积技术，主要区别在于节点数量可以变化，而且节点是无序的：
- en: '![Figure 10.8: Example of a convolution operation in Euclidian and graph data
    (Source: https://arxiv.org/pdf/1901.00596.pdf) ](img/B18217_10_007Redraw.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8：欧几里得数据和图数据中的卷积操作示例（来源：https://arxiv.org/pdf/1901.00596.pdf）](img/B18217_10_007Redraw.jpg)'
- en: 'Figure 10.8: Example of a convolution operation in Euclidian and graph data
    (Source: https://arxiv.org/pdf/1901.00596.pdf)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：欧几里得数据和图数据中的卷积操作示例（来源：https://arxiv.org/pdf/1901.00596.pdf）
- en: '*Figure 10.9* shows an example of a convolutional layer. The input of the network
    is the graph and adjacency matrix, which represents the edges between the nodes
    in forward propagation. The convolution layer encapsulates information for each
    node by aggregating information from its neighborhood. After that, nonlinear transformation
    is applied. Later, the output of this network can be used in different tasks,
    such as classification:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.9* 显示了一个卷积层的示例。网络的输入是图和邻接矩阵，表示正向传播中节点之间的边。卷积层通过聚合其邻域的信息来封装每个节点的信息。之后，应用非线性变换。随后，该网络的输出可用于不同任务，如分类：'
- en: '![Figure 10.9: Example of a convolutional neural network (Source: https://arxiv.org/pdf/1901.00596.pdf)
    ](img/B18217_10_008Redraw.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9：卷积神经网络示例（来源：https://arxiv.org/pdf/1901.00596.pdf）](img/B18217_10_008Redraw.jpg)'
- en: 'Figure 10.9: Example of a convolutional neural network (Source: https://arxiv.org/pdf/1901.00596.pdf)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：卷积神经网络示例（来源：https://arxiv.org/pdf/1901.00596.pdf）
- en: Mesh predictor
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格预测器
- en: The mesh predictor module aims to detect the 3D structure of an object. It is
    the logical continuation of the `RoIAlign` module, and it is responsible for predicting
    and outputting the final mesh.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 网格预测器模块旨在检测物体的 3D 结构。它是`RoIAlign`模块的逻辑延续，负责预测并输出最终的网格。
- en: As we get 3D meshes from real-life images, we can’t use fixed mesh templates
    with fixed mesh topologies. That is why the mesh predictor consists of two branches.
    The combination of the voxel branch and mesh refinement branch helps reduce the
    issue with fixed topologies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从现实生活中的图像中获得 3D 网格，因此无法使用固定网格模板和固定网格拓扑。这就是为什么网格预测器由两个分支组成。体素分支和网格细化分支的结合有助于减少固定拓扑问题。
- en: The voxel branch is analogous to the mask branch from Mask R-CNN. It takes aligned
    features from `ROIAlign` and outputs a G x G x G grid of voxel occupancy probabilities.
    Next, the Cubify operation is used. It uses a threshold for binarizing voxel occupancy.
    Each occupied voxel is replaced with a cuboid triangle mesh with 8 vertices, 18
    edges, and 12 faces.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 体素分支类似于 Mask R-CNN 中的掩膜分支。它从 `ROIAlign` 获取对齐特征，并输出一个 G x G x G 的体素占据概率网格。接下来，使用
    Cubify 操作。它使用一个阈值来二值化体素占据。每个被占据的体素都被替换为一个有 8 个顶点、18 条边和 12 个面的立方体三角形网格。
- en: The voxel loss is binary cross-entropy, which minimizes the predicted probabilities
    of voxel occupancy with ground truth occupancies.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 体素损失是二元交叉熵，最小化体素占据预测概率与真实占据之间的差异。
- en: 'The mesh refinement branch is a sequence of three different operations: vertex
    alignment, graph convolution, and vertex refinement. Vertex alignment is similar
    to ROI alignment; for each mesh vertex, it yields an image-aligned feature.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 网格细化分支是三个不同操作的序列：顶点对齐、图卷积和顶点细化。顶点对齐类似于 ROI 对齐；对于每个网格顶点，它生成一个图像对齐的特征。
- en: 'Graph convolution takes image-aligned features and propagates information along
    mesh edges. Vertex refinement updates vertex positions. It aims to update vertex
    geometry by keeping the topology fixed:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积采用图像对齐的特征，并沿网格边缘传播信息。顶点细化更新顶点位置。它的目的是在保持拓扑固定的情况下更新顶点几何：
- en: '![Figure 10.10: Mesh refinement branch ](img/B18217_10_009.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图10.10：网格细化分支](img/B18217_10_009.jpg)'
- en: 'Figure 10.10: Mesh refinement branch'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：网格细化分支
- en: As shown in *Figure 10.10*, we can have multiple stages of refinement. Each
    stage consists of vertex alignment, graph convolution, and vertex refinement operations.
    In the end, we get a more accurate 3D mesh.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.10*所示，我们可以进行多阶段的细化。每个阶段包括顶点对齐、图卷积和顶点细化操作。最终，我们得到一个更精确的 3D 网格。
- en: The final important part of the model is the mesh loss function. For this branch,
    chamfer and normal losses are used. However, these techniques need sampled points
    from predicted and ground-truth meshes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最终重要部分是网格损失函数。对于这个分支，使用了 Chamfer 和法向损失。然而，这些技术需要从预测和真实网格中采样点。
- en: 'The following mesh sampling method is used: given vertices and faces, the points
    are uniformly sampled from a probability distribution of the surface of the mesh.
    The probability of each face is proportional to its area.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下网格采样方法：给定顶点和面，从网格表面的概率分布中均匀采样点。每个面的概率与其面积成正比。
- en: Using these sampling techniques, a point cloud from the ground truth, *Q*, and
    a point cloud from the prediction, *P*, are sampled. Next, we calculate *Λ*PQ,
    which is the set of pairs (*p*,*q*) where *q* is the nearest neighbor of *p* in
    *Q*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些采样技术，从真实的点云 *Q* 和预测的点云 *P* 中进行采样。接下来，我们计算 *Λ*PQ，这是成对（*p*，*q*）的集合，其中 *q*
    是 *P* 中 *p* 的最近邻。
- en: 'Chamfer distance is calculated between *P* and *Q*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 *P* 和 *Q* 之间的 Chamfer 距离：
- en: '![](img/Formula_10_001.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_001.jpg)'
- en: 'Next, the absolute normal distance is calculated:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算绝对法向距离：
- en: '![](img/Formula_10_002.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_002.jpg)'
- en: Here, *u*p and *u*q are the units normal to points *p* and *q*, respectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*u*p 和 *u*q 分别是指向 *p* 和 *q* 点的单位法向量。
- en: 'However, only these two losses degenerated meshes. This is why, for high-quality
    mesh production, a shape regularizer was added, which was called edge loss:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只有这两种损失退化了网格。这就是为什么在高质量网格生成中，加入了一个形状正则化器，这被称为边缘损失：
- en: '![](img/Formula_10_003.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_003.jpg)'
- en: 'The final mesh loss is the weighted average of three presented losses: chamfer
    loss, normal loss, and edge loss.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的网格损失是三种损失的加权平均：Chamfer损失、法线损失和边缘损失。
- en: 'In terms of training, two types of experiments were conducted. The first one
    was to check the mesh predictor branch. Here, the ShapeNet dataset was used, which
    includes 55 common categories of classes. This is widely used in benchmarking
    for 3D shape prediction; however, it includes CAD models, which have separate
    backgrounds. Due to this, the mesh predictor model reached state-of-the-art status.
    Moreover, it solves issues regarding objects with holes that previous models couldn’t
    detect well:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练方面，进行了两种类型的实验。第一个实验是检查网格预测分支。在这里，使用了ShapeNet数据集，该数据集包含55种常见的类别。这在3D形状预测的基准测试中被广泛使用；然而，它包含了CAD模型，这些模型有独立的背景。由于这一点，网格预测器模型达到了最先进的状态。此外，它还解决了以前的模型无法很好检测到的有孔物体的问题：
- en: '![Figure 10.11: Mesh predictor on the ShapeNet dataset ](img/B18217_10_010.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图10.11：ShapeNet数据集上的Mesh预测器](img/B18217_10_010.jpg)'
- en: 'Figure 10.11: Mesh predictor on the ShapeNet dataset'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：ShapeNet数据集上的Mesh预测器
- en: 'The third row represents the output of the mesh predictor. We can see that
    it predicts the 3D shape and that it handles the topology and geometry of objects
    very well:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第三行表示网格预测器的输出。我们可以看到它预测了3D形状，并且很好地处理了物体的拓扑和几何结构：
- en: '![Figure 10.12: The output of the end-to-end Mesh R-CNN model ](img/B18217_10_011.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图10.12：端到端Mesh R-CNN模型的输出](img/B18217_10_011.jpg)'
- en: 'Figure 10.12: The output of the end-to-end Mesh R-CNN model'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：端到端Mesh R-CNN模型的输出
- en: The next step is to perform experiments on real-life images. For this, the Pix3D
    dataset was used, which includes 395 unique 3D models placed in 10,069 real-life
    images. In this case, benchmark results are not available, because the authors
    were the first to try this technique. However, we can check the output results
    from the training in *Figure 10.11*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对实际图像进行实验。为此，我们使用了Pix3D数据集，该数据集包含395个独特的3D模型，分布在10,069张真实图像中。在这种情况下，基准结果不可用，因为作者是首批尝试此技术的人。然而，我们可以查看*图10.11*中的训练输出结果。
- en: With that, we have discussed the Mesh R-CNN architecture. Now, we can get hands-on
    and use Mesh R-CNN to find objects in test images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经讨论了Mesh R-CNN的架构。接下来，我们可以动手实践，使用Mesh R-CNN在测试图像中找到物体。
- en: Demo of Mesh R-CNN with PyTorch
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch实现的Mesh R-CNN演示
- en: In this section, we will use the Mesh R-CNN repository to run the demo. We will
    try the model on our image and render the output `.obj` file to see how the model
    predicts the 3D shape. Moreover, we will discuss the training process of the model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Mesh R-CNN的代码库来运行演示。我们将尝试将模型应用到我们的图像上，并渲染输出的`.obj`文件，看看模型是如何预测3D形状的。此外，我们还将讨论模型的训练过程。
- en: 'Installing Mesh R-CNN is pretty straightforward. You need to install Detectron2
    and PyTorch3D first, then build Mesh R-CNN. `Detectron2` is a library from Facebook
    Research that provides state-of-the-art detection and segmentation models. It
    includes Mask R-CNN as well, the model on which Mesh R-CNN was built. You can
    install `detectron2` by running the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Mesh R-CNN非常简单。你需要先安装Detectron2和PyTorch3D，然后构建Mesh R-CNN。`Detectron2`是Facebook
    Research推出的一个库，提供最先进的检测和分割模型。它还包括Mask R-CNN模型，Mesh R-CNN正是基于这个模型构建的。你可以通过运行以下命令来安装`detectron2`：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If this doesn’t work for you, check the website for alternative ways to install
    it. Next, you need to install PyTorch3D, as described earlier in this book. When
    both requirements are ready, you just need to build Mesh R-CNN:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个方法对你不起作用，可以查看官网了解其他安装方式。接下来，你需要按照本书前面的内容安装PyTorch3D。当这两个要求都准备好后，你只需构建Mesh
    R-CNN：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Demo
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示
- en: 'The repository includes a `demo.py` file, which is used to demonstrate how
    the end-to-end process of Mesh R-CNN works. The file is located in `meshrcnn/demo/demo.py`.
    Let’s look at the code to understand how the demo is done. The file includes the
    `VisualizationDemo` class, which consists of two main methods: `run_on_image`
    and `visualize_prediction`. The method names speak for themselves: the first takes
    an image as input and outputs predictions of the model, while the other visualizes
    the detection of the mask, and then saves the final mesh and the image with predictions
    and confidence:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码库包括一个`demo.py`文件，用于演示Mesh R-CNN的端到端过程。该文件位于`meshrcnn/demo/demo.py`。我们来看一下代码，了解演示是如何进行的。该文件包括`VisualizationDemo`类，包含两个主要方法：`run_on_image`和`visualize_prediction`。方法名本身就说明了它们的功能：第一个方法以图像作为输入，并输出模型的预测结果，而第二个方法则可视化遮罩的检测，然后保存最终的网格以及带有预测和置信度的图像：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For the demo, you just need to run the preceding command from the terminal.
    The command has the following parameters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于演示，您只需要从终端运行前面的命令。该命令有以下参数：
- en: '`--config-file` takes the path to the config file, which can be found in the
    `configs` directory'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--config-file`指定配置文件的路径，可以在`configs`目录中找到该文件。'
- en: '`--input` takes the path to the input image'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--input`指定输入图像的路径'
- en: '`--output` takes the path to the directory where predictions should be saved'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--output`指定保存预测结果的目录路径'
- en: '`--onlyhighest`, if `True`, outputs only one mesh and mask that has the highest
    confidence'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--onlyhighest`，如果为`True`，则仅输出具有最高置信度的一个网格和遮罩。'
- en: Now, let’s run and check the output.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行并检查输出。
- en: 'For the demo, we will use the image of the apartment that we used in the previous
    chapter:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于演示，我们将使用上一章中使用的公寓图像：
- en: '![Figure 10.13: The input image for the network ](img/B18217_10_012.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图10.13：网络的输入图像](img/B18217_10_012.jpg)'
- en: 'Figure 10.13: The input image for the network'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：网络的输入图像
- en: 'We give the path to this image to `demo.py`. After prediction, we get the mask
    visualization and mesh of the image. Since we used the `--onlyhighest` argument,
    we only got one mask, which is the prediction of the sofa object. This has an
    88.7% confidence score. The mask prediction is correct – it covers almost the
    entire sofa:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此图像的路径传递给`demo.py`。预测完成后，我们得到该图像的遮罩可视化和网格。由于我们使用了`--onlyhighest`参数，我们只得到了一个遮罩，这是沙发物体的预测，置信度为88.7%。遮罩预测是正确的——它几乎覆盖了整个沙发：
- en: '![Figure 10.14: The output of the demo.py file ](img/B18217_10_013.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图10.14：demo.py文件的输出](img/B18217_10_013.jpg)'
- en: 'Figure 10.14: The output of the demo.py file'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：demo.py文件的输出
- en: Besides the mask, we also got the mesh in the same directory, which is a `.obj`
    file. Now, we need to render images from the 3D object.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 除了遮罩外，我们还在同一目录中得到了网格，它是一个`.obj`文件。现在，我们需要从3D对象渲染图像。
- en: 'The following code is from the `chapt10/viz_demo_results.py` file:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码来自`chapt10/viz_demo_results.py`文件：
- en: 'First, let’s import all the libraries used in the code:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入代码中使用的所有库：
- en: '[PRE3]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we are going to define arguments to run the code:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义运行代码所需的参数：
- en: '[PRE4]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We need input for `path_to_mesh` – that is, the output `.obj` file of `demo.py`.
    We also need to specify the path where the rendered output should be saved, then
    specify the distance from the camera, elevation angle, and rotation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为`path_to_mesh`提供输入——即`demo.py`的输出`.obj`文件。我们还需要指定渲染输出应保存的路径，然后指定相机的距离、俯仰角度和旋转。
- en: 'Next, we must load and initialize the mesh object. First, we must load the
    `.obj` file with the `load_obj` function from `pytorch3d`. Then, we must make
    the vertexes white. We will use the `Meshes` structure from `pytorch3d` to create
    a mesh object:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要加载并初始化网格对象。首先，我们需要使用`pytorch3d`中的`load_obj`函数加载`.obj`文件。然后，我们需要将顶点设置为白色。我们将使用`pytorch3d`中的`Meshes`结构来创建网格对象：
- en: '[PRE5]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next step is to initialize the perspective camera. Then, we need to set
    blend parameters that will be used to blend faces. `sigma` controls opacity, whereas
    `gamma` controls the sharpness of edges:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是初始化透视相机。然后，我们需要设置混合参数，用于混合面。`sigma`控制不透明度，而`gamma`控制边缘的锐利度：
- en: '[PRE6]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we must define settings for rasterization and shading. We will set the
    output image size to 256*256 and set `faces_per_pixel` to 100, which will blend
    100 faces for one pixel. Then, we will use rasterization settings to create a
    silhouette mesh renderer by composing a rasterizer and a shader:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义光栅化和着色的设置。我们将把输出图像的大小设置为256*256，并将`faces_per_pixel`设置为100，这将把100个面混合成一个像素。然后，我们将使用光栅化设置来创建轮廓网格渲染器，方法是组合光栅化器和着色器：
- en: '[PRE7]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We need to create one more `RasterizationSettings` object since we will use
    the Phong renderer as well. It will only need to blend one face per pixel. Again,
    the image output will be 256\. Then, we need to add a point light in front of
    the object. Finally, we need to initialize the Phong renderer:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要创建另一个`RasterizationSettings`对象，因为我们还将使用Phong渲染器。它只需要每个像素混合一个面。再次强调，图像输出将是256。然后，我们需要在物体前面添加一个点光源。最后，我们需要初始化Phong渲染器：
- en: '[PRE8]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we must create the position of the camera based on spheral angles. We
    will use the `look_at_view_transform` function and add the `distance`, `elevation`,
    and `azimuth` parameters that were mentioned previously. Lastly, we must get the
    rendered output from the silhouette and Phong renderer by giving them the mesh
    and camera position as input:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须根据球面角度创建摄像机的位置。我们将使用`look_at_view_transform`函数，并添加之前提到的`distance`、`elevation`和`azimuth`参数。最后，我们必须通过给轮廓和Phong渲染器提供网格和摄像机位置作为输入，来获取渲染输出：
- en: '[PRE9]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The last step is to visualize the results. We will use `matplotlib` to plot
    both rendered images:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是可视化结果。我们将使用`matplotlib`来绘制两个渲染图像：
- en: '[PRE10]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the preceding code will be a `.png` image that will be saved
    in the `save_path` folder given in the arguments. For this parameter and the image
    presented here, the rendered mesh will look like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出将是一个`.png`图像，保存在给定的`save_path`文件夹中。对于此参数和此处呈现的图像，渲染的网格将是这样的：
- en: '![Figure 10.16: Rendered 3D output of the model ](img/B18217_10_014.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.16：模型的渲染3D输出](img/B18217_10_014.jpg)'
- en: 'Figure 10.16: Rendered 3D output of the model'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16：模型的渲染3D输出
- en: As we can see from this angle, the mesh looks very similar to the sofa, not
    counting some defects on not visible parts. You can play with camera position
    and lighting to render an image of the object from another point of view.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，网格与沙发非常相似，忽略不可见部分的某些缺陷。您可以调整摄像机位置和光照，以从另一个角度渲染该物体的图像。
- en: The repository also provides an opportunity to run and reproduce the experiments
    described in the Mesh R-CNN paper. It allows you to run both the Pix3D experiment
    and the ShapeNet experiment.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码库还提供了运行并重现Mesh R-CNN论文中描述的实验的机会。它允许您运行Pix3D实验和ShapeNet实验。
- en: As mentioned earlier, the Pix3D data includes real-life images of different
    IKEA furniture. This data was used to evaluate the whole Mesh R-NN from end to
    end.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Pix3D数据集包含不同IKEA家具的真实生活图像。该数据用于对整个Mesh R-NN进行端到端的评估。
- en: 'To download this data, you need to run the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载这些数据，您需要运行以下命令：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The data contains two splits named S1 and S2 and the repository provides weights
    for both splits. After downloading the data, you can reproduce the training by
    running the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含两个分割，分别是S1和S2，且代码库为这两个分割提供了权重。下载数据后，您可以通过运行以下命令来重现训练过程：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You just need to be careful with the configs. The original model was distributed
    and trained on 8 GB of GPU. If you don’t have that much capacity, it probably
    won’t reach the same accuracy, so you need to tune your hyperparameters for better
    accuracy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要小心配置文件。原始模型是在8 GB GPU上分发并训练的。如果您的GPU容量不足，可能无法达到相同的准确度，因此您需要调整超参数以获得更好的准确度。
- en: 'You can use your trained weights or you can simply run an evaluation on the
    pre-trained models provided by the authors:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用自己训练的权重，或者可以简单地对作者提供的预训练模型进行评估：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding command will evaluate the model for the specified checkpoint file.
    You can find the checkpoints by going to the model’s GitHub repository.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将评估指定检查点文件的模型。您可以通过访问模型的GitHub仓库找到检查点。
- en: 'Next, if you want to run the experiment on ShapeNet, you need to download the
    data, which can be done by running the following command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果您想在ShapeNet上运行实验，您需要下载数据，可以通过运行以下命令来完成：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will download the training, validation, and test sets. The authors have
    also provided the preprocessing code for the ShapeNet dataset. Preprocessing will
    reduce the loading time. The following command will output zipped data, which
    is convenient for training in clusters:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载训练集、验证集和测试集。作者还提供了 ShapeNet 数据集的预处理代码。预处理将减少加载时间。以下命令将输出压缩数据，便于在集群中进行训练：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, to reproduce the experiment, you just need to run the `train_net_shapenet.py`
    file with corresponding configs. Again, be careful when adjusting the training
    process to your hardware capacity:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要重现实验，你只需要运行带有相应配置的 `train_net_shapenet.py` 文件。再次提醒，在调整训练过程时，要小心硬件的能力：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, you can always evaluate your model, or the checkpoints provided by
    the authors, by running the following command:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以通过运行以下命令来评估你的模型，或者作者提供的检查点：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can compare your results with the results provided in the paper. The following
    chart shows the scale-normalized protocol training results that the authors got:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将你的结果与论文中提供的结果进行比较。下图展示了作者得到的尺度归一化协议训练结果：
- en: '![Figure 10.17: The results of evaluation on the ShapeNet dataset ](img/B18217_10_015.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.17：在 ShapeNet 数据集上评估的结果](img/B18217_10_015.jpg)'
- en: 'Figure 10.17: The results of evaluation on the ShapeNet dataset'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17：在 ShapeNet 数据集上评估的结果
- en: The chart includes the category name, number of instances per category, chamfer,
    normal losses, and the F1 scores.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包括了类别名称、每个类别的实例数量、Chamfer 距离、法线损失和 F1 分数。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we presented a new way of looking at the object detection task.
    The 3D world requires solutions that work accordingly, and this is one of the
    first approaches toward that goal. We learned how Mesh R-CNN works by understanding
    the architecture and the structure of the model. We dove deeper into some interesting
    operations and techniques that are used in the model, such as graph convolutional
    networks, Cubify operations, the mesh predictor structure, and more. Finally,
    we learned how this model can be used in practice to detect objects on the image
    that the network has never seen before. We evaluated the results by rendering
    the 3D object.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提出了一种全新的物体检测任务视角。3D 世界需要相应的解决方案，而这是朝着这一目标迈出的首个尝试之一。通过理解架构和模型结构，我们了解了
    Mesh R-CNN 是如何工作的。我们深入探讨了该模型中使用的一些有趣操作和技术，如图卷积网络、Cubify 操作、网格预测器结构等。最后，我们了解了该模型如何在实践中用于检测网络从未见过的图像中的物体。我们通过渲染
    3D 物体来评估结果。
- en: Throughout this book, we have covered 3D deep learning concepts, from the basics
    to more advanced solutions. First, we learned about the various 3D data types
    and structures. Then, we delved into different types of models that solve different
    types of problems such as mesh detection, view synthesis, and more. In addition,
    we added PyTorch 3D to our computer vision toolbox. By completing this book, you
    should be ready to tackle real-world problems related to 3D computer vision and
    much more.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本书贯穿了从基础到更高级的 3D 深度学习概念。首先，我们学习了各种 3D 数据类型和结构。然后，我们深入探讨了不同类型的模型，这些模型解决了不同类型的问题，如网格检测、视图合成等。此外，我们还将
    PyTorch 3D 添加到了计算机视觉工具箱中。通过完成本书的学习，你应该能够应对与 3D 计算机视觉相关的现实世界问题，甚至更多。
