- en: '*Chapter 1*: Getting Started with Image Generation Using TensorFlow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：使用 TensorFlow 开始图像生成'
- en: This book focuses on generating images and videos using unsupervised learning
    with TensorFlow 2\. We assume that you have prior experience in using modern machine
    learning frameworks, such as TensorFlow 1, to build image classifiers with **Convolutional
    Neural Networks** (**CNNs**). Therefore, we will not be covering the basics of
    deep learning and CNNs. In this book, we will mainly use high level Keras APIs
    in TensorFlow 2, which is easy to learn. Nevertheless, we assume that you have
    no prior knowledge of image generation, and we will go through all that is needed
    to help you get started with it. The first aspect that you need to know about
    is **probability distribution**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专注于使用 TensorFlow 2 通过无监督学习生成图像和视频。我们假设你已经有使用现代机器学习框架（如 TensorFlow 1）构建图像分类器的经验，尤其是使用**卷积神经网络**（**CNNs**）。因此，我们不会涉及深度学习和
    CNN 的基础知识。在本书中，我们将主要使用 TensorFlow 2 中的高级 Keras API，学习起来比较简单。不过，我们假设你对图像生成没有任何先验知识，我们会涵盖所有必要的内容，帮助你入门。你首先需要了解的概念是**概率分布**。
- en: Probability distribution is fundamental in machine learning and it is especially
    important in generative models. Don't worry, I assure you that there aren't any
    complex mathematical equations in this chapter. We will first learn what probability
    is and how to use it to generate faces without using any neural networks or complex
    algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布是机器学习中的基础，尤其在生成模型中尤为重要。别担心，我向你保证，本章不会涉及任何复杂的数学公式。我们将首先学习什么是概率，以及如何在不使用神经网络或复杂算法的情况下，使用它生成面孔。
- en: 'That''s right: with the help of only basic math and NumPy code, you''ll learn
    how to create a probabilistic generative model. Following that, you will learn
    how to use TensorFlow 2 to build a **PixelCNN** model in order to generate handwritten
    digits. This chapter is packed with useful information; you will need to read
    this chapter before jumping to any other chapters.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 没错：在只需基本数学和 NumPy 代码的帮助下，你将学习如何创建一个概率生成模型。之后，你将学习如何使用 TensorFlow 2 构建**PixelCNN**模型来生成手写数字。本章内容丰富，包含很多有用的信息；在跳到其他章节之前，你需要先阅读本章。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主要主题：
- en: Understanding probabilities
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解概率
- en: Generating faces with a probabilistic model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用概率模型生成面孔
- en: Building a PixelCNN model from scratch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始构建 PixelCNN 模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code can be found here: [https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter01).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在这里找到：[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter01)。
- en: Understanding probabilities
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解概率
- en: 'You can''t escape the term *probability* in any machine learning literature,
    and it can be confusing as it can have different meanings in different contexts.
    Probability is often denoted as *p* in mathematical equations, and you see it
    everywhere in academic papers, tutorials, and blogs. Although it is a concept
    that is seemingly easy to understand, it can be quite confusing. This is because
    there are multiple different definitions and interpretations depending on the
    context. We will use some examples to clarify things. In this section, we will
    go over the use of probability in the following contexts:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你无法避免在任何机器学习文献中遇到*概率*这个术语，它可能令人困惑，因为在不同的上下文中它有不同的含义。概率通常在数学公式中表示为*p*，你会在学术论文、教程和博客中到处看到它。尽管它看似是一个容易理解的概念，但实际上却相当混乱。这是因为根据上下文的不同，概率有多种不同的定义和解释。我们将通过一些例子来阐明这一点。在本节中，我们将探讨概率在以下几种情境中的应用：
- en: Distribution
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布
- en: Belief
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信念
- en: Probability distribution
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率分布
- en: 'Say we want to train a neural network to classify images of cats and dogs and
    that we found a dataset that contains 600 images of dogs and 400 images of cats.
    As you may be aware, the data will need to be shuffled before being fed into the
    neural network. Otherwise, if it sees only images of the same label in a minibatch,
    the network will get lazy and say all images have the same label without taking
    the effort to look hard and differentiate between them. If we sampled the dataset
    randomly, the probabilities could be written as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想训练一个神经网络来分类猫和狗的图像，并且我们找到了一个数据集，其中包含600张狗的图片和400张猫的图片。如你所知，在将数据输入神经网络之前，数据需要先进行混洗。否则，如果它在一个小批次中只看到同一标签的图像，网络就会懒惰，认为所有图像都有相同的标签，而不费力去区分它们。如果我们随机抽样数据集，那么概率可以写成如下：
- en: '*pdata(dog) = 0.6*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*pdata(dog) = 0.6*'
- en: '*pdata(cat) = 0.4*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*pdata(cat) = 0.4*'
- en: The probabilities here refer to the **data distribution**. In this example,
    this refers to the ratio of the number of cat and dog images to the total number
    of images in the dataset. The probability here is static and will not change for
    a given dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的概率指的是**数据分布**。在这个例子中，指的是猫和狗图像数量与数据集中总图像数量的比例。这里的概率是静态的，对于给定的数据集不会变化。
- en: 'When training a deep neural network, the dataset is usually too big to fit
    into one batch, and we need to break it into multiple minibatches for one epoch.
    If the dataset is well shuffled, then the **sampling distribution** of the minibatches
    will resemble that of the data distribution. If the dataset is unbalanced, where
    some classes have a lot more images from one label than another, then the neural
    network may be biased toward predicting the images it sees more. This is a form
    of **overfitting**. We can therefore sample the data differently to give more
    weight to the less-represented classes. If we want to balance the classes in sampling,
    then the sampling probability becomes as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，数据集通常太大，无法放入一个批次中，我们需要将其分成多个小批次来进行一轮训练。如果数据集经过充分混洗，那么小批次的**抽样分布**将与数据分布相似。如果数据集不平衡，其中某些类别的标签图像远多于其他类别的图像，那么神经网络可能会倾向于预测它看到更多的图像。这就是一种**过拟合**的表现。因此，我们可以通过不同的方式抽样数据，给予较少代表的类别更多权重。如果我们希望在抽样中平衡各个类别，那么抽样概率将如下所示：
- en: '*psample(dog) = 0.5*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*psample(dog) = 0.5*'
- en: '*psample(cat) = 0.5*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*psample(cat) = 0.5*'
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注：
- en: '**Probability distribution** **p(x)** is the probability of the occurrence
    of a data point *x*. There are two common distributions that are used in machine
    learning. **Uniform distribution** is where every data point has the same chances
    of occurrence; this is what people normally imply when they say random sampling
    without specifying the distribution type. **Gaussian distribution** is another
    commonly used distribution. It is so common that people also call it **normal
    distribution**. The probabilities peak at the center (mean) and slowly decay on
    each side. Gaussian distribution also has nice mathematical properties that make
    it a favorite of mathematicians. We will see more of that in the next chapter.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率分布** **p(x)** 是数据点 *x* 出现的概率。在机器学习中，有两种常用的分布。**均匀分布** 是每个数据点有相同出现机会的分布；这是人们通常提到随机抽样时所暗示的，没有指定分布类型的情况。**高斯分布**
    是另一种常用的分布。它如此常见，以至于人们也称之为**正态分布**。概率在中心（均值）处达到峰值，并在两侧逐渐衰减。高斯分布还有一些良好的数学性质，使其成为数学家的最爱。我们将在下一章看到更多内容。'
- en: Prediction confidence
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测置信度
- en: 'After several hundred iterations, the model has finally finished training,
    and I can''t wait to test the new model with an image. The model outputs the following
    probabilities:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在经历了数百次迭代后，模型终于完成了训练，我迫不及待想要用一张图片来测试这个新模型。模型输出以下概率：
- en: '*p(dog) = 0.6*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(dog) = 0.6*'
- en: '*p(cat) = 0.4*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(cat) = 0.4*'
- en: Wait, is the AI telling me that this animal is a mixed-breed with 60% dog genes
    and 40% cat inheritance? Of course not!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，AI是在告诉我这个动物是一个60%狗基因和40%猫遗传的混合种吗？当然不是！
- en: Here, the probabilities no longer refer to distributions; instead, they tell
    us how confident we can be about the predictions, or in other words, how strongly
    we can believe in the output. Now, this is no longer something you quantify by
    counting occurrences. If you are absolutely sure that something is a dog, you
    can put *p(dog) =* *1.0* and *p(cat) = 0.0*. This is known as **Bayesian probability**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，概率不再指的是分布；相反，它们告诉我们我们对预测的信心有多大，换句话说，就是我们对输出结果有多强的信任。现在，这不再是通过计数事件的发生来量化的。如果你完全确定某个东西是狗，你可以设定
    *p(dog) =* *1.0* 和 *p(cat) = 0.0*。这就是所谓的**贝叶斯概率**。
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The traditional statistics approach sees probability as the chances of the occurrence
    of an event, for example, the chances of a baby being a certain sex. There has
    been great debate in the wider statistical field on whether the frequentist or
    Bayesian method is better, which is beyond the scope of this book. However, the
    Bayesian method is probably more important in deep learning and engineering. It
    has been used to develop many important algorithms, including **Kalman filtering**
    to track rocket trajectory. When calculating the projection of a rocket's trajectory,
    the Kalman filter uses information from both the **global positioning system**
    (**GPS**) and **speed sensor**. Both sets of data are noisy, but GPS data is less
    reliable initially (meaning less confidence), and hence this data is given less
    weight in the calculation. We don't need to learn the Bayesian theorem in this
    book; it's enough to understand that probability can be viewed as a confidence
    score rather than as frequency. Bayesian probability has also recently been used
    in searching for hyperparameters for deep neural networks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的统计方法将概率视为某事件发生的可能性，例如，婴儿出生时的性别概率。关于频率主义和贝叶斯方法哪个更好的问题，在更广泛的统计领域中有过激烈的争论，这超出了本书的范围。然而，贝叶斯方法在深度学习和工程中可能更为重要。它已被用于开发许多重要的算法，包括**卡尔曼滤波**，用于追踪火箭轨迹。在计算火箭轨迹的投影时，卡尔曼滤波会同时使用**全球定位系统**（**GPS**）和**速度传感器**的数据。这两组数据都有噪声，但GPS数据起初不太可靠（即置信度较低），因此在计算中给予其较小的权重。我们在本书中不需要学习贝叶斯定理；理解概率可以视为置信度评分而非频率就足够了。贝叶斯概率最近也被用于搜索深度神经网络的超参数。
- en: We have now clarified two main types of probabilities commonly used in general
    machine learning – distribution and confidence. From now on, we will assume that
    probability means probability distribution rather than confidence. Next, we will
    look at a distribution that plays an exceptionally important role in image generation
    – **pixel distribution**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经澄清了在一般机器学习中常用的两种主要概率类型——分布概率和置信度。从现在开始，我们将假设“概率”指的是概率分布，而不是置信度。接下来，我们将讨论在图像生成中起着至关重要作用的一个分布——**像素分布**。
- en: The joint probability of pixels
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 像素的联合概率
- en: Take a look at the following pictures – can you tell whether they are of dogs
    or cats? How do you think the classifier will produce the confidence score?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的图片——你能判断它们是狗还是猫吗？你认为分类器如何生成置信度评分？
- en: '![Figure 1.1 – Pictures of a cat and a dog'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.1 – 一只猫和一只狗的照片'
- en: '](img/B14538_01_01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_01.jpg)'
- en: Figure 1.1 – Pictures of a cat and a dog
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 一只猫和一只狗的照片
- en: Are either of these pictures of dogs or cats? Well, the answer is pretty obvious,
    but at the same time it's not important to what we are going to talk about. When
    you looked at the pictures, you probably thought in your mind that the first picture
    was of a cat and the second picture was of a dog. We see the picture as a whole,
    but that is not what the computer sees. The computer sees **pixels**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些照片是狗还是猫呢？嗯，答案显而易见，但同时，这对我们接下来的讨论并不重要。当你看这些照片时，你可能心里想，第一张照片是猫，第二张是狗。我们看的是整张图片，但这并不是计算机所看到的。计算机看到的是**像素**。
- en: Note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A pixel is the smallest spatial unit in digital image, and it represents a single
    color. You cannot have a pixel where half is black and the other half is white.
    The most commonly used color scheme is 8-bit RGB, where a pixel is made up of
    three channels named R (red), G (green), and B (blue). Their values range from
    0 to 255 (255 being the highest intensity). For example, a black pixel has a value
    of [0, 0, 0], while a white pixel is [255, 255, 255].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像素是数字图像中最小的空间单元，代表一个单一的颜色。你不可能有一个像素，其中一半是黑色，另一半是白色。最常用的颜色方案是 8 位 RGB，其中一个像素由三种通道组成，分别是
    R（红色）、G（绿色）和 B（蓝色）。它们的值范围从 0 到 255（255 为最大强度）。例如，一个黑色像素的值是 [0, 0, 0]，而一个白色像素的值是
    [255, 255, 255]。
- en: The simplest way to describe the **pixel distribution** of an image is by counting
    the number of pixels that have different intensity levels from 0 to 255; you can
    visualize this by plotting a histogram. It is a common tool in digital photography
    to look at a histogram of separate R, G, and B channels to understand the color
    balance. Although this can provide some information to us – for example, an image
    of sky is likely to have many blue pixels, so a histogram may reliably tell us
    something about that – histograms do not tell us how pixels relate to each other.
    In other words, a histogram does not contain spatial information, that is, how
    far a blue pixel is from another blue pixel. We will need a better measure for
    this kind of thing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 描述图像的**像素分布**最简单的方法是通过计算具有不同强度级别（从 0 到 255）的像素数量；你可以通过绘制直方图来可视化这一点。在数字摄影中，一个常用的工具是查看单独的
    R、G 和 B 通道的直方图，以了解色彩平衡。虽然这能提供一些信息给我们——例如，天空图像很可能有很多蓝色像素，因此直方图可以可靠地告诉我们一些相关信息——但是直方图并不能告诉我们像素之间的关系。换句话说，直方图不包含空间信息，也就是，蓝色像素之间的距离是多少。我们需要一个更好的度量来处理这种情况。
- en: Instead of saying *p(x)*, where *x* is a whole image, we can define *x* as *x*1*,
    x*2*, x*3,… *x*n. Now, *p(x)* can be defined as the **joint probability** of pixels
    *p(x*1*, x*2*, x*3*,… x*n*)*, where *n* is the number of pixels and each pixel
    is separated by a comma.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与其说 *p(x)*，其中 *x* 是整个图像，我们可以将 *x* 定义为 *x*1*、x*2*、x*3*，… *x*n。现在，*p(x)* 可以定义为像素的**联合概率**
    *p(x*1*，x*2*，x*3*，… x*n*)*，其中 *n* 是像素的数量，每个像素之间用逗号分隔。
- en: 'We will use the following images to illustrate what we mean by joint probability.
    The following are three images with 2 x 2 pixels that contain binary values, where
    0 is black and 1 is white. We will call the top-left pixel *x*1, the top-right
    pixel *x*2, the bottom-left pixel *x*3, and the bottom-right pixel *x*4:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下图像来说明联合概率的含义。以下是三张具有 2 x 2 像素的二进制值图像，其中 0 表示黑色，1 表示白色。我们将左上角的像素称为 *x*1，右上角的像素称为
    *x*2，左下角的像素称为 *x*3，右下角的像素称为 *x*4：
- en: '![Figure 1.2 – Images with 2 x 2 pixels'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2 – 具有 2 x 2 像素的图像'
- en: '](img/B14538_01_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_02.jpg)'
- en: Figure 1.2 – Images with 2 x 2 pixels
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 具有 2 x 2 像素的图像
- en: 'We first calculate *p(x1 = white)* by counting the number of white *x*1 and
    dividing it by the total number of the image. Then, we do the same for *x*2, as
    follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过计算白色 *x*1 的数量并将其除以图像的总数来计算 *p(x1 = 白色)*。然后，我们对 *x*2 做同样的事情，如下所示：
- en: '*p(x*1 *= white)  = 2 / 3*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x*1 *= 白色) = 2 / 3*'
- en: '*p(x*2 *= white)  = 0 / 3*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x*2 *= 白色) = 0 / 3*'
- en: 'Now we say that *p(x1)* and *p(x2)* are independent of each other because we
    calculated them separately. If we calculate the joint probability where both *x1*
    and *x2* are black, we get the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们说 *p(x1)* 和 *p(x2)* 是相互独立的，因为我们分别计算了它们。如果我们计算两个像素都为黑色的联合概率，我们得到如下结果：
- en: '*p(x*1 *= black, x*2 *= black)  = 0 / 3*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x*1 *= 黑色, x*2 *= 黑色) = 0 / 3*'
- en: 'We can then calculate the complete joint probability of these two pixels as
    follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算这两个像素的完整联合概率，如下所示：
- en: '*p(x*1 *= black, x*2 *= white)  = 0 / 3*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x*1 *= 黑色, x*2 *= 白色) = 0 / 3*'
- en: '*p(x*1 *= white, x*2 *= black)  = 3 / 3*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x*1 *= 白色, x*2 *= 黑色) = 3 / 3*'
- en: '*p(x*1 *= white, x*2 *= white)  = 0 / 3*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x*1 *= 白色, x*2 *= 白色) = 0 / 3*'
- en: We'll need to do the same steps 16 times to calculate the complete joint probability
    of *p(x*1*, x*2*, x*3*, x*4*)*. Now, we could fully describe the pixel distribution
    and use that to calculate the marginal distribution, as in *p(x*1*, x*2*, x*3*)*
    or *p(x*1*)*. However, the calculations required for the joint distribution increase
    exponentially for RGB values where each pixel has 256 x 256 x 256  = 16,777,216
    possibilities. This is where deep neural networks come to the rescue. A neural
    network can be trained to learn a pixel data distribution *P*data. Hence, a neural
    network is our probability model *P*model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要执行相同的步骤16次，以计算完整的联合概率*p(x*1*, x*2*, x*3*, x*4*)*。现在，我们可以完全描述像素分布，并使用该分布来计算边际分布，如*p(x*1*,
    x*2*, x*3*)*或*p(x*1*)*。然而，对于RGB值的联合分布，计算所需的步骤会呈指数增长，因为每个像素有256 x 256 x 256 = 16,777,216种可能性。这时，深度神经网络就派上用场了。神经网络可以训练来学习像素数据分布*P*data。因此，神经网络就是我们的概率模型*P*model。
- en: Important Note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The notations we will use in this book are as follows: *capital X* for the
    dataset, *lowercase x* for image sampled from the dataset, and *lowercase with
    subscript x*i for the pixel.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中将使用的符号如下：*大写X*代表数据集，*小写x*代表从数据集中采样的图像，*带下标x*i代表像素。
- en: The purpose of image generation is to generate an image that has a pixel distribution
    *p(x)* that resembles *p(X)*. For example, an image dataset of oranges will have
    a high probability of lots of occurrences of orange pixels that are distributed
    close to each other in a circle. Therefore, before generating image, we will first
    build a probability model *p*model*(x)* from real data *p*data*(X).* After that,
    we generate images by drawing a sample from *p*model*(x)*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成的目的是生成具有像素分布*p(x)*的图像，这种分布类似于*p(X)*。例如，一个橙子图像数据集将具有很高的概率，出现很多靠近彼此分布的橙色像素。因此，在生成图像之前，我们将首先从真实数据*p*data*(X)*中构建一个概率模型*p*model*(x)*。然后，我们通过从*p*model*(x)*中采样来生成图像。
- en: Generating faces with a probabilistic model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用概率模型生成面孔
- en: Alright, enough mathematics. It is now time to get your hands dirty and generate
    your first image. In this section, we will learn how to generate images by sampling
    from a probabilistic model without even using a neural network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，数学部分到此为止。现在是时候动手生成你的第一张图像了。在这一部分，我们将学习如何通过从概率模型中采样来生成图像，甚至不需要使用神经网络。
- en: Mean faces
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均面孔
- en: 'We will be using the large-scale CelebFaces Attributes (CelebA) dataset created
    by The Chinese University of Hong Kong ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)).
    This can be downloaded directly with Python''s `tensorflow_datasets` module within
    the `ch1_generate_first_image.ipynb` Jupyter notebook, as shown in the following
    code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由香港中文大学创建的大规模CelebFaces Attributes（CelebA）数据集（[http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)）。可以直接通过Python的`tensorflow_datasets`模块在`ch1_generate_first_image.ipynb`
    Jupyter笔记本中下载，如以下代码所示：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The TensorFlow dataset allows us to preview some examples of images by using
    the `tfds.show_examples()` API. The following are some samples of male and female
    celebrities'' faces:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow数据集允许我们通过使用`tfds.show_examples()` API来预览一些图像示例。以下是一些男性和女性名人面孔的样本：
- en: '![Figure 1.3 – Sample images from the CelebA dataset'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3 – 来自CelebA数据集的样本图像'
- en: '](img/B14538_01_03.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_03.jpg)'
- en: Figure 1.3 – Sample images from the CelebA dataset
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 来自CelebA数据集的样本图像
- en: 'As you can see in the figure, there is a celebrity face in every image. Every
    picture is unique, with a variety of genders, poses, expressions, and hairstyles;
    some wear glasses and some don''t. Let''s see how to exploit the probability distribution
    of the images to help us create a new face. We''ll use one of the simplest statistical
    methods – the mean, which means taking an average of the pixels from the images.
    To be more specific, we are averaging the *x*i of every image to calculate the
    *x*i of a new image. To speed up the processing, we''ll use only 2,000 samples
    from the dataset for this task, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，每张图像中都有一个名人的面孔。每张图片都是独一无二的，展示了各种性别、姿势、表情和发型；有些人戴眼镜，有些则没有。我们来看看如何利用图像的概率分布帮助我们创造一个新的面孔。我们将使用最简单的统计方法之一——均值，这意味着取图像像素的平均值。更具体地说，我们是通过平均每张图像的*x*i来计算新图像的*x*i。为了加快处理速度，我们将仅使用数据集中的2,000个样本来完成这项任务，如下所示：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Ta-dah! That is your first generated image, and it looks pretty amazing! I
    initially thought it would look a bit like one of Picasso''s paintings, but it
    turns out that the mean image is quite coherent:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 哇哦！那就是你生成的第一张图像，效果真棒！我最初以为它会看起来有点像毕加索的画作，但事实证明，平均图像其实相当连贯：
- en: '![Figure 1.4 – The mean face'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.4 – 平均面孔](img/B14538_01_04.jpg)'
- en: '](img/B14538_01_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_04.jpg)'
- en: Figure 1.4 – The mean face
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 平均面孔
- en: Conditional probability
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件概率
- en: 'The best thing about the CelebA dataset is that each image is labeled with
    facial attributes as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: CelebA 数据集的最棒之处在于，每张图像都有如下的面部属性标签：
- en: '![Figure 1.5 – 40 attributes in the CelebA dataset in alphabetical order'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.5 – CelebA 数据集中按字母顺序排列的 40 个属性](img/B14538_01_05.jpg)'
- en: '](img/B14538_01_05.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_05.jpg)'
- en: Figure 1.5 – 40 attributes in the CelebA dataset in alphabetical order
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – CelebA 数据集中按字母顺序排列的 40 个属性
- en: 'We are going to use these attributes to generate a new image. Let''s say we
    want to generate a male image. How do we do that? Instead of calculating the probability
    of every image, we use only images that have the `Male` attribute set to `true`.
    We can put it in this way:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些属性生成一张新图像。假设我们想要生成一张男性图像。我们该怎么做呢？我们不通过计算每张图像的概率，而只使用那些将`男性`属性设置为`true`的图像。我们可以这样表达：
- en: '*p(x | y)*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x | y)*'
- en: 'We call this the probability of *x* conditioned on *y*, or more informally
    the probability of *x* given *y*. This is called `Male` attribute, this variable
    is no longer a random probability; every sample will have the `Male` attribute
    and we can be certain that every face belongs to a man. The following figure shows
    new mean faces generated using other attributes as well as `Male`, such as *Male
    + Eyeglasses* and *Male + Eyeglasses + Mustache + Smiling*. Notice that as the
    conditions increase, the number of samples reduces and the mean image also becomes
    noisier:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这为在给定 *y* 条件下 *x* 的概率，或者更非正式地称为在 *y* 条件下 *x* 的概率。这就是`男性`属性，这个变量不再是一个随机概率；每个样本都会有
    `男性` 属性，我们可以确定每个面孔都属于男性。以下图显示了使用其他属性以及`男性`属性生成的新平均面孔，例如 *男性 + 眼镜* 和 *男性 + 眼镜 +
    胡子 + 微笑*。请注意，随着条件的增加，样本数量减少，平均图像也变得更嘈杂：
- en: '![Figure 1.6 – Adding attributes from left to right. (a) Male (b) Male + Eyeglasses
    (c) Male + Eyeglasses + Mustache + Smiling](img/B14538_01_06.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 从左到右添加属性。 (a) 男性 (b) 男性 + 眼镜 (c) 男性 + 眼镜 + 胡子 + 微笑](img/B14538_01_06.jpg)'
- en: Figure 1.6 – Adding attributes from left to right. (a) Male (b) Male + Eyeglasses
    (c) Male + Eyeglasses + Mustache + Smiling
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 从左到右添加属性。 (a) 男性 (b) 男性 + 眼镜 (c) 男性 + 眼镜 + 胡子 + 微笑
- en: 'You could use the Jupyter notebook to generate a new face by using different
    attributes, but not every combination produces satisfactory results. The following
    are some female faces generated with different attributes. The rightmost image
    is an interesting one. I used attributes of `Female`, `Smiling`, `Eyeglasses`,
    and `Pointy_Nose`, but it turns out that people with these attributes tend to
    also have wavy hair, which is an attribute that was excluded in this sample. Visualization
    can be a useful tool to provide insights into your dataset:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Jupyter notebook 通过不同的属性生成新的面孔，但并不是每种组合都会产生令人满意的结果。以下是一些通过不同属性生成的女性面孔。最右边的图像很有趣。我使用了`女性`、`微笑`、`眼镜`和`尖鼻子`等属性，但事实证明，具有这些属性的人通常也有波浪形的头发，而这在这个样本中并未包括。可视化是一个有用的工具，可以为你提供数据集的见解：
- en: '![Figure 1.7 – Female faces with different attributes'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.7 – 不同属性的女性面孔](img/B14538_01_07.jpg)'
- en: '](img/B14538_01_07.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_07.jpg)'
- en: Figure 1.7 – Female faces with different attributes
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 不同属性的女性面孔
- en: Tips
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of using the mean when generating images, you can try to using the median
    as well, which may produce a sharper image. Simply replace `np.mean()` with `np.median()`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像时，你可以尝试使用中位数，而不是使用平均值，这可能会生成更清晰的图像。只需将 `np.mean()` 替换为 `np.median()`。
- en: Probabilistic generative models
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率生成模型
- en: 'There are three main goals that we wish to achieve with image-generation algorithms:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过图像生成算法实现三个主要目标：
- en: Generate images that look like ones in the given dataset.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成看起来像给定数据集中的图像。
- en: Generate a variety of images.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成各种图像。
- en: Control the images being generated.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制生成的图像。
- en: By simply taking the mean of the pixels in an image, we have demonstrated how
    to achieve goals *1* and *3*. However, one limitation is that we could only generate
    one image per condition. That really isn't very effective for an algorithm, generating
    only one image from hundreds or thousands of training images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过取图像中像素的均值，我们就展示了如何实现目标 *1* 和 *3*。然而，一个限制是我们每种条件下只能生成一张图像。对于一个算法来说，这真的不是很有效，只从数百或数千张训练图像中生成一张图像。
- en: 'The following chart shows the distribution of one color channel of an arbitrary
    pixel in the dataset. The *x* mark on the chart is the median value. When we use
    the mean or median of data, we are always sampling the same point, and therefore
    there is no variation in the outcome. Is there a way to generate multiple different
    faces? Yes, we can try to increase the generated image variation by sampling from
    the entire pixel distribution:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了数据集中一个任意像素的一个颜色通道的分布。图表上的 *x* 标记是中位数值。当我们使用数据的均值或中位数时，我们总是在采样同一个点，因此结果不会有变化。有没有办法生成多个不同的面孔？有的，我们可以尝试通过从整个像素分布中进行采样来增加生成图像的变化：
- en: '![Figure 1.8 – The distribution of a pixel''s color channel'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.8 – 像素颜色通道的分布'
- en: '](img/B14538_01_08.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_08.jpg)'
- en: Figure 1.8 – The distribution of a pixel's color channel
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 像素颜色通道的分布
- en: 'A machine learning textbook will probably ask you to first create a probabilistic
    model, `pmodel`, by calculating the joint probability of every single pixel. But
    as the sample space is huge (remember, one RGB pixel can have 16,777,216 different
    values), it is computationally expensive to implement. Also, because this is a
    hands-on book, we will draw pixel samples directly from datasets. To create an
    *x*0 pixel in a new image, we randomly sample from an *x*0 pixel of all images
    in the dataset by running the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一本机器学习教材可能会要求你首先创建一个概率模型 `pmodel`，通过计算每个像素的联合概率。但是，由于样本空间非常庞大（记住，一个RGB像素可以有16,777,216种不同的值），实现起来计算成本非常高。此外，因为这是一本实践书，我们将直接从数据集中提取像素样本。为了在新图像中创建一个
    *x*0 像素，我们通过运行以下代码从数据集中的所有图像的 *x*0 像素中随机采样：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Images were generated using random sampling. Disappointingly, although there
    is some variation between the images, they are not that different from each other,
    and one of our objectives is to be able to generate a variety of faces. Also,
    the images are noticeably noisier than when using the mean. The reason for this
    is that the pixel distribution is independent of each other.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是通过随机采样生成的。令人失望的是，尽管图像之间有一些变化，但它们之间的差异并不大，而我们的目标之一是能够生成多样化的面孔。此外，与使用均值时相比，图像的噪声明显增多。其原因是像素分布是相互独立的。
- en: 'For example, for a given pixel in the lips, we can reasonably expect the color
    to be pink or red, and the same goes for the adjacent pixels. Nevertheless, because
    we are sampling independently from images where faces appear in different locations
    and poses, this results in color discontinuities between pixels, ultimately giving
    this noisy result:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于嘴唇中的一个给定像素，我们可以合理地预计其颜色为粉色或红色，相邻的像素也是如此。然而，由于我们从脸部位于不同位置和姿势的图像中独立采样，这会导致像素之间的颜色不连续，最终产生这种噪声效果：
- en: '![Figure 1.9 – Images generated by random sampling'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9 – 通过随机采样生成的图像'
- en: '](img/B14538_01_09.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_09.jpg)'
- en: Figure 1.9 – Images generated by random sampling
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 通过随机采样生成的图像
- en: Tips
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You may be wondering why the mean face looks smoother than with random sampling.
    Firstly, it is because the distance of the mean between pixels is smaller. Imagine
    a random sampling scenario where one pixel sampled is close to 0 and the next
    one is close to 255\. The mean of these pixels would likely lie somewhere in the
    middle, and therefore the difference between them would be smaller. On the other
    hand, pixels in the backgrounds of pictures tend to have a uniform distribution;
    for example, they could all be part of a blue sky, a white wall, green leaves,
    and so on. As they are distributed rather evenly across the color spectrum, the
    mean value is around [127, 127, 127], which happens to be gray.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么平均面孔看起来比随机采样的更平滑。首先，是因为像素之间的平均距离较小。想象一个随机采样的场景，其中一个像素采样值接近0，而下一个像素接近255。这样，这些像素的平均值可能会位于两者之间，因此它们之间的差异会较小。另一方面，图像背景中的像素通常呈现均匀分布；例如，它们可能都是蓝天的一部分，白色墙壁，绿色树叶，等等。由于这些像素均匀分布在颜色谱上，平均值大约是[127,
    127, 127]，这恰好是灰色。
- en: Parametric modeling
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数化建模
- en: What we just did was use a pixel histogram as our `pmodel`, but there are a
    few shortcomings here. Firstly, due to the large sample space, not every possible
    color exists in our sample distribution. As a result, the generated image will
    never contain any colors that are not present in the dataset. For instance, we
    want to be able to generate the full spectrum of skin tones rather than only one
    very specific shade of brown that exists in the dataset. If you did try to generate
    faces using conditions, you will have found that not every combination of conditions
    is possible. For example, for *Mustache + Sideburns + Heavy_Makeup + Wavy_Hair*,
    there simply wasn't a sample that met those conditions!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才所做的，是使用像素直方图作为我们的`pmodel`，但这里有一些不足之处。首先，由于样本空间很大，并不是每个可能的颜色都存在于我们的样本分布中。因此，生成的图像永远不会包含数据集中不存在的颜色。例如，我们希望能够生成全谱的肤色，而不仅仅是数据集中存在的某个特定的棕色。如果你尝试在条件下生成面孔，你会发现并不是每种条件的组合都是可能的。例如，对于*胡须+鬓角+浓妆+卷发*，根本没有一个样本符合这些条件！
- en: 'Secondly, the sample spaces increase as we increase the size of the dataset
    or the image resolution. This can be solved by having a parameterized model. The
    vertical bar chart in the following figure shows a histogram of 1,000 randomly
    generated numbers:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，随着我们增加数据集的大小或图像分辨率，样本空间会增大。这可以通过使用参数化模型来解决。下图中的垂直条形图显示了1,000个随机生成的数字的直方图：
- en: '![Figure 1.10 – Gaussian histogram and model'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10 – 高斯直方图和模型'
- en: '](img/B14538_01_10.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_10.jpg)'
- en: Figure 1.10 – Gaussian histogram and model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 高斯直方图和模型
- en: 'We can see that there are some bars that don''t have any value. We can fit
    a Gaussian model on the data in which the **Probability Density Function** (**PDF**)
    is plotted as a black line. The PDF equation for a Gaussian distribution is as
    follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有些条形图没有任何数值。我们可以在数据上拟合一个高斯模型，其中**概率密度函数**（**PDF**）以黑线的形式绘制。高斯分布的PDF方程如下：
- en: '![](img/Formula_01_001.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_001.jpg)'
- en: Here, *µ* is the mean and *σ* is the standard deviation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*µ* 是均值，*σ* 是标准差。
- en: We can see that the PDF covers the histogram gap, which means we can generate
    a probability for the missing numbers. This Gaussian model has only two parameters
    – the mean and the standard variation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，PDF覆盖了直方图的空白，这意味着我们可以为缺失的数字生成概率。这个高斯模型只有两个参数——均值和标准差。
- en: The 1,000 numbers can now be condensed to just two parameters, and we can use
    this model to draw as many samples as we wish; we are no longer limited to the
    data we fit the model with. Of course, natural images are complex and could not
    be described by simple models such as a Gaussian model, or in fact any mathematical
    models. This is where neural networks come into play. Now we will use a neural
    network as a parameterized image-generation model where the parameters are the
    network's weights and biases.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这1,000个数字现在可以压缩为仅两个参数，我们可以使用这个模型生成任意数量的样本；我们不再受限于拟合模型的数据。当然，自然图像是复杂的，不能通过简单的模型（如高斯模型）来描述，实际上也不能通过任何数学模型来描述。这就是神经网络发挥作用的地方。现在，我们将使用神经网络作为一个参数化的图像生成模型，其中参数是网络的权重和偏差。
- en: Building a PixelCNN model from scratch
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建PixelCNN模型
- en: 'There are three main categories of deep neural network generative algorithms:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络生成算法的三大类：
- en: '**Generative Adversarial Networks** (**GANs**)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）'
- en: '**Variational Autoencoders** (**VAEs**)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAEs**）'
- en: '**Autoregressive models**'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归模型**'
- en: VAEs will be introduced in the next chapter, and we will use them in some of
    our models. The GAN is the main algorithm we will be using in this book, and there
    are a lot more details about it to come in later chapters. We will introduce the
    lesser-known **autoregressive model** family here and focus on VAEs and GANs later
    in the book. Although it is not so common in image generation, autoregression
    is still an active area of research, with DeepMind's WaveNet using it to generate
    realistic audio. In this section, we will introduce autoregressive models and
    build a **PixelCNN** model from scratch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）将在下一章介绍，我们将在一些模型中使用它们。生成对抗网络（GAN）是本书中主要使用的算法，关于它的更多细节将在后续章节中介绍。在这里，我们将介绍较少人知的**自回归模型**系列，并将在后续章节重点讨论VAE和GAN。尽管在图像生成中不那么常见，自回归仍然是一个活跃的研究领域，DeepMind的WaveNet就使用它来生成逼真的音频。在本节中，我们将介绍自回归模型，并从零开始构建一个**PixelCNN**模型。
- en: Autoregressive models
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归模型
- en: '*Auto* here means *self*, and *regress* in machine learning terminology means
    *predict new values*. Putting them together, autoregressive means we use a model
    to predict new data points based on the model''s past data points.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *Auto* 意味着 *自我*，而在机器学习术语中，*regress* 意味着 *预测新值*。将它们结合起来，autoregressive 意味着我们使用一个模型基于该模型的历史数据点来预测新的数据点。
- en: 'Let''s recall the probability distribution of an image is *p(x)* is joint pixel
    probability *p(x*1*, x*2*,… x*n*)* which is difficult to model due to the high
    dimensionality. Here, we make an assumption that the value of a pixel depends
    only on that of the pixel before it. In other words, a pixel is conditioned only
    on the pixel before it, that is, *p(x*i*) = p(x*i *| x*i-1*) p(x*i-1*)*. Without
    going into the mathematical details, we can approximate the joint probability
    to be the product of conditional probabilities:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下图像的概率分布 *p(x)*，它是联合像素概率 *p(x*1*, x*2*, … x*n*)*，由于维度较高，难以建模。在这里，我们假设一个像素的值仅依赖于其前一个像素的值。换句话说，一个像素仅依赖于它前面的像素，即
    *p(x*i*) = p(x*i *| x*i-1*) p(x*i-1*)*。不深入数学细节，我们可以将联合概率近似为条件概率的乘积：
- en: '*p(x) = p(x*n*, x*n-1*, …, x*2*, x*1*)*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x) = p(x*n*, x*n-1*, …, x*2*, x*1*)*'
- en: '*p(x) = p(x*n *| x*n-1*)... p(x*3 *| x*2*) p(x*2 *| x*1*) p(x*1*)*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x) = p(x*n *| x*n-1*)… p(x*3 *| x*2*) p(x*2 *| x*1*) p(x*1*)*'
- en: 'To give a concrete example, let''s say we have images that contain only a red
    apple in roughly the center of the image, and that the apple is surrounded by
    green leaves. In other words, only two colors are possible: red and green. *x*1
    is the top-left pixel, so *p(x*1*)* is the probability of whether the top-left
    pixel is green or red. If *x*1 is green, then the pixel to its right *p(x*2*)*
    is likely to be green too, as it''s likely to be more leaves. However, it could
    be red, despite the smaller probability.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 举个具体的例子，假设我们有一些图像，图像中大约在中心位置只有一个红色的苹果，并且苹果周围有绿色的叶子。换句话说，只有两种颜色：红色和绿色。*x*1 是左上角的像素，因此
    *p(x*1*)* 是左上角像素是绿色还是红色的概率。如果 *x*1 是绿色，那么它右边的像素 *p(x*2*)* 也很可能是绿色，因为很可能是更多的叶子。然而，尽管概率较小，它也可能是红色。
- en: As we go on, we will eventually hit a red pixel (hooray! We have found our apple!).
    From that pixel onward, it is likely that the next few pixels are more likely
    to be red too. We can now see that this is a lot simpler than having to consider
    all of the pixels together.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续，最终会遇到一个红色像素（太好了！我们找到了苹果！）。从那个像素开始，接下来的几个像素也很可能是红色的。现在我们可以看到，这比必须一起考虑所有像素要简单得多。
- en: PixelRNN
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PixelRNN
- en: '**PixelRNN** was invented by the Google-acquired DeepMind back in 2016\. As
    the name **RNN** (**Recurrent Neural Network**) suggests, this model uses a type
    of RNN called **Long Short-Term Memory** (**LSTM**) to learn an image''s distribution.
    It reads the image one row at a time in a step in the LSTM and processes it with
    a 1D convolution layer, then feeds the activations into subsequent layers to predict
    pixels for that row.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**PixelRNN** 由 Google 收购的 DeepMind 在 2016 年发明。正如 **RNN**（**循环神经网络**）所暗示的那样，该模型使用一种称为
    **长短时记忆**（**LSTM**）的 RNN 来学习图像的分布。它每次读取图像的一行，在 LSTM 中进行一步处理，并用 1D 卷积层处理该行，然后将激活值传递到后续层，预测该行的像素。'
- en: As LSTM is slow to run, it takes a long time to train and generate samples.
    As a result, it fell out of fashion and there has not been much improvement made
    to it since its inception. Thus, we will not dwell on it for long and will instead
    move our attention to a variant, PixelCNN, which was also unveiled in the same
    paper.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LSTM 运行速度较慢，训练和生成样本需要较长时间。因此，它逐渐失去了流行，并且自其诞生以来没有太多改进。因此，我们不会在此过多停留，而是将注意力转向同一篇论文中也被提出的一个变体——PixelCNN。
- en: Building a PixelCNN model with TensorFlow 2
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2 构建 PixelCNN 模型
- en: PixelCNN is made up only of convolutional layers, making it a lot faster than
    PixelRNN. Here, we will implement a simple PixelCNN model for MNIST. The code
    can be found in `ch1_pixelcnn.ipynb`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: PixelCNN 仅由卷积层组成，使其比 PixelRNN 快得多。在这里，我们将实现一个简单的 PixelCNN 模型用于 MNIST。代码可以在 `ch1_pixelcnn.ipynb`
    中找到。
- en: Input and label
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入和标签
- en: 'MNIST consists of 28 x 28 x 1 grayscale images of handwritten digits. It only
    has one channel, with 256 levels to depict the shade of gray:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 由 28 x 28 x 1 的灰度图像组成，展示手写数字。它只有一个通道，使用 256 个灰度级来描绘灰色的阴影：
- en: '![Figure 1.11 – MNIST digit examples'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11 – MNIST 数字示例'
- en: '](img/B14538_01_11.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_11.jpg)'
- en: Figure 1.11 – MNIST digit examples
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – MNIST 数字示例
- en: 'In this experiment, we simplify the problem by casting images into binary format
    with only two possible values: `0` represents black and `1` represents white.
    The code for this is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们通过将图像转换为仅包含两个可能值的二进制格式来简化问题：`0`表示黑色，`1`表示白色。代码如下所示：
- en: '[PRE3]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The function expects two inputs – an image and a label. The first two lines
    of the function cast the image into binary  `float32` format, in other words `0.0`
    or `1.0`. In this tutorial, we will not use the label information; instead, we
    cast the binary image into an integer and return it. We don't have to cast it
    to an integer, but let's just do it to stick to the convention of using an integer
    for labels. To recap, both the input and the label are binary MNIST images of
    28 x 28 x 1; they differ only in data type.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数需要两个输入——一张图像和一个标签。函数的前两行将图像转换为二进制`float32`格式，换句话说，就是`0.0`或`1.0`。在本教程中，我们不使用标签信息；相反，我们将二进制图像转换为整数并返回。我们不必将其转换为整数，但为了遵循标签使用整数的惯例，我们还是这么做。回顾一下，输入和标签都是28
    x 28 x 1的二进制MNIST图像，唯一不同的是数据类型。
- en: Masking
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掩膜
- en: Unlike PixelRNN, which reads row by row, PixelCNN slides a convolutional kernel
    across the image from left to right and from top to bottom. When performing convolution
    to predict the current pixel, a conventional convolution kernel is able to see
    the current input pixel together with the pixels surrounding it, including future
    pixels, and this breaks our conditional probability assumptions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与逐行读取的PixelRNN不同，PixelCNN从左到右、从上到下滑动卷积核遍历图像。在执行卷积以预测当前像素时，传统的卷积核能够看到当前输入像素以及周围的像素，包括未来的像素，这打破了我们的条件概率假设。
- en: To avoid that, we need to make sure that the CNN doesn't cheat to look at the
    pixel it is predicting. In other words, we need to make sure that the CNN doesn't
    see the input pixel *x*i while it is predicting the output pixel *x*i.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们需要确保CNN在预测时不会看到它正在预测的像素。换句话说，我们需要确保CNN在预测输出像素*x*i时，不能看到输入像素*x*i。
- en: 'This is by using a masked convolution, where a mask is applied to the convolutional
    kernel weights before performing convolution. The following diagram shows a mask
    for a 7 x 7 kernel, where the weight from the center onward is 0\. This blocks
    the CNN from seeing the pixel it is predicting (the center of the kernel) and
    all future pixels. This is known as a **type A mask** and is applied only to the
    input layer. As the center pixel is blocked in the first layer, we don''t need
    to hide the center feature anymore in later layers. In fact, we will need to set
    the kernel center to 1 to enable it to read the features from previous layers.
    This is known as type B Mask:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用掩膜卷积实现的，其中在执行卷积之前，掩膜被应用到卷积核权重上。下图展示了一个7 x 7卷积核的掩膜，其中从中心开始的权重为0。这阻止了CNN看到它正在预测的像素（卷积核的中心）以及所有未来的像素。这被称为**类型A掩膜**，仅应用于输入层。由于第一层已经阻止了中心像素，我们在后续层中不再需要隐藏中心特征。实际上，我们需要将卷积核的中心设置为1，以便它能够读取来自前一层的特征。这被称为类型B掩膜：
- en: '![Figure 1.12 – A 7 x 7 kernel mask'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12 – 7 x 7卷积核掩膜'
- en: '](img/B14538_01_12.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_12.jpg)'
- en: 'Figure 1.12 – A 7 x 7 kernel mask (Source: Aäron van den Oord et al., 2016,  Conditional
    Image Generation with PixelCNN Decoders, [https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328))'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12 – 7 x 7卷积核掩膜（来源：Aäron van den Oord 等人，2016年，《使用PixelCNN解码器的条件图像生成》，[https://arxiv.org/abs/1606.05328](https://arxiv.org/abs/1606.05328))
- en: Next, we will learn how to create a custom layer.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何创建自定义层。
- en: Implementing a custom layer
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现自定义层
- en: 'We will now create a custom layer for the masked convolution. We can create
    a custom layer in TensorFlow using model subclassing inherited from the base class,
    `tf.keras.layers.Layer`, as shown in the following code. We will be able to use
    it just like other Keras layers. The following is the basic structure of the custom
    layer class:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为掩膜卷积创建一个自定义层。我们可以通过从基类`tf.keras.layers.Layer`继承模型子类来在TensorFlow中创建自定义层，如下所示。我们将能够像使用其他Keras层一样使用它。以下是自定义层类的基本结构：
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`build()` takes the input tensor''s shape as an argument, and we will use this
    information to create variables of the correct shapes. This function runs only
    once, when the layer is built. We can create a mask by declaring it either as
    a non-trainable variable or as a constant to let TensorFlow know it does not need
    to have gradients to backpropagate:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`build()`将输入张量的形状作为参数，我们将使用这些信息来创建正确形状的变量。此函数只在构建层时运行一次。我们可以通过声明掩膜为非训练变量或常量来创建掩膜，这样TensorFlow就会知道它不需要反向传播的梯度：'
- en: '[PRE5]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`call()` is the forward pass that performs the computation. In this masked
    convolutional layer, we multiply the weight by the mask to zero the lower half
    before performing convolution using the low-level `tf.nn` API:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`call()`是执行计算的前向传播函数。在这个掩蔽卷积层中，我们通过将权重乘以掩膜将下半部分清零，然后使用低级的`tf.nn` API执行卷积操作：'
- en: '[PRE6]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Tips
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: '`tf.keras.layers` is a high-level API that is easy to use without you needing
    to know under-the-hood details. However, sometimes we will need to create custom
    functions using the low-level `tf.nn` API, which requires us to first specify
    or create the tensors to be used.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers`是一个高级API，易于使用，无需了解底层细节。然而，有时我们需要使用低级的`tf.nn` API来创建自定义函数，这要求我们首先指定或创建需要使用的张量。'
- en: Network layers
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络层
- en: 'The PixelCNN architecture is quite straightforward. After the first 7 x 7 `conv2d`
    layer with mask A, there are several layers of residual blocks (see the following
    table) with mask B. To keep the same feature map size of 28 x 28, there is no
    downsampling; for example, the max pooling and padding in these layers is set
    to `SAME`. The top features are then fed into two layers of 1 x 1 convolution
    layers before the output is produced, as seen in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: PixelCNN架构非常简单。经过带有掩膜A的第一个7 x 7 `conv2d`层后，接下来是几层带有掩膜B的残差块（参见下表）。为了保持28 x 28的相同特征图尺寸，这些层没有进行下采样；例如，这些层中的最大池化和填充被设置为`SAME`。然后，顶部特征会被送入两层1
    x 1的卷积层，最终生成输出，如下截图所示：
- en: '![Figure 1.13 – The PixelCNN architecture, showing the layers and output shape'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.13 – PixelCNN架构，展示了层和输出形状](img/B14538_01_13.jpg)'
- en: '](img/B14538_01_13.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_13.jpg)'
- en: Figure 1.13 – The PixelCNN architecture, showing the layers and output shape
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13 – PixelCNN架构，展示了层和输出形状
- en: 'Residual blocks are used in many high-performance CNN-based models and were
    made popular by ResNet, which was invented by Kaiming He et al. in 2015\. The
    following diagram illustrates a variant of residual blocks used in PixelCNN. The
    left path is called a **skip connection path**, which simply passes the features
    from the previous layer. On the right path are three sequential convolutional
    layers with filters of 1 x 1, 3 x 3, and 1 x 1\. This path optimizes the residuals
    of the input features, hence the name **residual net**:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 残差块在许多高性能的基于CNN的模型中都有应用，并且通过ResNet而广为人知，ResNet是由Kaiming He等人在2015年发明的。以下图示展示了PixelCNN中使用的残差块变种。左侧路径被称为**跳跃连接路径**，它简单地传递来自前一层的特征。在右侧路径中，有三个连续的卷积层，过滤器大小为1
    x 1、3 x 3和1 x 1。该路径优化输入特征的残差，因此得名**残差网络**：
- en: '![Figure 1.14 – The residual block where h is the number of filters. (Source:
    Aäron van den Oord et al., Pixel Recurrent Neural Networks)](img/B14538_01_14.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图1.14 – 残差块，其中h表示滤波器的数量。（来源：Aäron van den Oord等人，《Pixel Recurrent Neural
    Networks》）](img/B14538_01_14.jpg)'
- en: 'Figure 1.14 – The residual block where h is the number of filters. (Source:
    Aäron van den Oord et al., Pixel Recurrent Neural Networks)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14 – 残差块，其中h表示滤波器的数量。（来源：Aäron van den Oord等人，《Pixel Recurrent Neural Networks》）
- en: Cross-entropy loss
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: '**Cross-entropy loss**, also known as **log loss**, measures the performance
    of a model, where the output''s probability is between 0 and 1\. The following
    is the equation for binary cross-entropy loss, where there are only two classes,
    labels *y* can be either 0 or 1, and *p(x)* is the model''s prediction. The equation
    is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉熵损失**，也称为**对数损失**，衡量模型的性能，其中输出的概率在0和1之间。以下是二元交叉熵损失的公式，其中只有两个类别，标签*y*可以是0或1，*p(x)*是模型的预测。公式如下：'
- en: '![](img/Formula_01_002.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_002.jpg)'
- en: Let's look at an example where the label is 1, the second term is zero, and
    the first term is the sum of *log p(x)*. The log in the equation is natural log
    (loge) but by convention the base of e is omitted from the equations. If the model
    is confident that *x* belongs to label 1, then *log(1)* is zero. On the other
    hand, if the model wrongly guesses it as label 0 and predicts a low probability
    of *x* being label *1*, say *p(x) = 0.1*. Then *-log (p(x))* becomes higher loss
    of *2.3*. Therefore, minimizing cross-entropy loss will maximize the model's accuracy.
    This loss function is commonly used in classification models but is also popular
    among generative models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，其中标签为1，第二项为零，第一项为*log p(x)*的和。公式中的对数是自然对数（loge），但根据惯例，方程中省略了e的底数。如果模型确信*x*属于标签1，那么*log(1)*为零。另一方面，如果模型错误地将其猜测为标签0，并预测*x*为标签*1*的概率很低，例如*p(x)
    = 0.1*，则*-log(p(x))*将变为较高的损失*2.3*。因此，最小化交叉熵损失将最大化模型的准确性。这个损失函数通常用于分类模型，但在生成模型中也非常流行。
- en: In PixelCNN, the individual image pixel is used as a label. In our binarized
    MNIST, we want to predict whether the output pixel is either 0 or 1, which makes
    this a classification problem with cross-entropy as the loss function.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在PixelCNN中，单个图像像素被用作标签。在我们的二值化MNIST数据集中，我们希望预测输出像素是0还是1，这使得该问题成为一个分类问题，交叉熵作为损失函数。
- en: 'There can be two output types:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 可以有两种输出类型：
- en: Since there can only be 0 or 1 in a binarized image, we can simplify the network
    by using `sigmoid()` to predict the probability of a white pixel, that is, *p(x*i
    *= 1)*. The loss function is binary cross-entropy. This is what we will use in
    our PixelCNN model.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于在二值化图像中只能为0或1，我们可以通过使用`sigmoid()`简化网络，以预测白色像素的概率，即*p(x*i*=1)*。损失函数为二元交叉熵。这就是我们将在PixelCNN模型中使用的损失函数。
- en: Optionally, we could also generalize the network to accept grayscale or RGB
    images. We can use the `softmax()` activation function to produce *N* probabilities
    for each (sub)pixel. *N* will be *2* for binarized images, *256* for grayscale
    images, and *3 x 256* for RGB images. The loss function is sparse categorical
    cross-entropy or categorical cross-entropy if the label is one-hot encoded.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，我们还可以将网络推广以接受灰度或RGB图像。我们可以使用`softmax()`激活函数为每个（子）像素生成*N*个概率。对于二值化图像，*N*将为*2*；对于灰度图像，*N*将为*256*；对于RGB图像，*N*将为*3
    x 256*。如果标签是独热编码，则损失函数为稀疏类别交叉熵或类别交叉熵。
- en: Finally, we are now ready to compile and train the neural network. As seen in
    the following code, we use binary cross-entropy for both `loss` and `metrics`
    and use `RMSprop` as the optimizer. There are many different optimizers to use,
    and their main difference comes in how they adjust the learning rate of individual
    variables based on past statistics. Some optimizers accelerate training but may
    tend to overshoot and not achieve global minima. There is no one best optimizer
    to use in all cases, and you are encouraged to try different ones.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们现在可以准备编译和训练神经网络。如下面的代码所示，我们对`loss`和`metrics`使用二元交叉熵，并将`RMSprop`作为优化器。有许多不同的优化器可以使用，它们的主要区别在于如何根据过去的统计数据调整个别变量的学习率。有些优化器加速了训练，但可能会导致过冲，无法达到全局最小值。没有一种最好的优化器适用于所有情况，因此建议您尝试不同的优化器。
- en: However, the two optimizers that you will see a lot are **Adam** and **RMSprop**.
    The Adam optimizer is a popular choice in image generation for its fast learning,
    while RMSprop is used frequently by Google to produce state-of-the-art models.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您将经常看到的两个优化器是**Adam**和**RMSprop**。Adam优化器因其快速学习而在图像生成中非常流行，而RMSprop则是谷歌频繁使用的一种优化器，用于生成最先进的模型。
- en: 'The following is used to compile and fit the `pixelcnn` model:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于编译和拟合`pixelcnn`模型的代码：
- en: '[PRE7]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we will generate a new image from the preceding model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从前述模型生成一张新图像。
- en: Sample image
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本图像
- en: 'After the training, we can generate a new image using the model by taking the
    following steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以通过以下步骤使用该模型生成新的图像：
- en: Create an empty tensor with the same shape as the input image and fill it with
    zeros. Feed this into the network and get *p(x*1*)*, the probability of the first
    pixel.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个与输入图像形状相同的空张量，并用零填充它。将其输入到网络中，并获得*p(x*1*)*，即第一个像素的概率。
- en: Sample from *p(x*1*)* and assign the sample value to pixel *x*1 in the input
    tensor.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*p(x*1*)*中采样，并将样本值赋给输入张量中的像素*x*1。
- en: Feed the input to the network again and perform step 2 for the next pixel.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入再次送入网络并执行步骤2以处理下一个像素。
- en: Repeat steps 2 and 3 until *x*N has been generated.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，直到*x*N被生成。
- en: One major drawback of the autoregressive model is that it is slow because of
    the need to generate pixel by pixel, which cannot be parallelized. The following
    images were generated by our simple PixelCNN model after 50 epochs of training.
    They don't look quite like proper digits yet, but they're starting to take the
    shape of handwriting strokes. It's quite amazing that we can now generate new
    images out of thin air (that is, with zero-input tensors). Can you generate better
    digits by training the model longer and doing some hyperparameter tuning?
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型的一个主要缺点是它比较慢，因为需要逐像素生成，且无法并行化。以下是我们在训练 50 轮后，由简单的 PixelCNN 模型生成的图像。它们看起来还不像标准的数字，但已经开始呈现出手写笔画的形态。现在我们能够从零输入张量生成新的图像，真是太神奇了（也就是说，我们可以从无到有生成图像）。如果通过训练模型更长时间并进行一些超参数调优，您能生成更好的数字吗？
- en: '![Figure 1.15 – Some images generated by our PixelCNN model'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15 – 由我们的 PixelCNN 模型生成的一些图像'
- en: '](img/B14538_01_15.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_01_15.jpg)'
- en: Figure 1.15 – Some images generated by our PixelCNN model
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15 – 由我们的 PixelCNN 模型生成的一些图像
- en: With that, we have come to the end of the chapter!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经结束了这一章！
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Wow! I think we have learned a lot in this first chapter, from understanding
    pixel probability distribution to using it to build a probabilistic model to generate
    images. We learned how to build custom layers with TensorFlow 2 and use them to
    construct autoregressive PixelCNN models to generate images of handwritten digits.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我觉得我们在这一章学到了很多内容，从理解像素概率分布到使用它构建概率模型生成图像。我们学会了如何使用 TensorFlow 2 构建自定义层，并利用它们构建自回归
    PixelCNN 模型来生成手写数字图像。
- en: In the next chapter, we will learn how to do representation with VAEs. This
    time, we will look at pixels from a whole new perspective. We will train a neural
    network to learn facial attributes, and you'll perform face edits, such as morphing
    a sad-looking girl into a smiling man with a mustache.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用变分自编码器（VAE）进行表示学习。这一次，我们将从全新的角度来看待像素。我们将训练一个神经网络来学习面部特征，并进行面部编辑，例如将一个看起来悲伤的女孩转变成一个带着胡子的微笑男士。
