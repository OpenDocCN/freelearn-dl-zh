- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: 'The Transformer: The Model Behind the Modern AI Revolution'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer：现代人工智能革命的模型背后的秘密
- en: In this chapter, we will discuss the limitations of the models we saw in the
    previous chapter, and how a new paradigm (first attention mechanisms and then
    the transformer) emerged to solve these limitations. This will enable us to understand
    how these models are trained and why they are so powerful. We will discuss why
    this paradigm has been successful and why it has made it possible to solve tasks
    in **natural language processing** (**NLP**) that were previously impossible.
    We will then see the capabilities of these models in practical application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论上一章中看到的模型的局限性，以及如何通过新的范式（首先是注意力机制，然后是transformer）来解决这些局限性。这将使我们能够理解这些模型是如何训练的，以及为什么它们如此强大。我们将讨论为什么这个范式是成功的，以及为什么它使得解决以前不可能的自然语言处理（NLP）任务成为可能。然后我们将看到这些模型在实际应用中的能力。
- en: This chapter will clarify why contemporary LLMs are inherently based on the
    transformer architecture.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将阐明为什么当代的LLM（大型语言模型）本质上基于transformer架构。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Exploring attention and self-attention
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索注意力机制和自注意力
- en: Introducing the transformer model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍transformer模型
- en: Training a transformer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个transformer
- en: Exploring masked language modeling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索掩码语言模型
- en: Visualizing internal mechanisms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化内部机制
- en: Applying a transformer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用transformer
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Most of this code can be run on a CPU, but some parts (fine-tuning and knowledge
    distillation) are preferable to be run on a GPU (one hour of training on a CPU
    versus less than five minutes on a GPU).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码可以在CPU上运行，但某些部分（微调和知识蒸馏）更适宜在GPU上运行（在CPU上训练一小时与在GPU上不到五分钟相比）。
- en: 'The code is written in PyTorch and uses standard libraries for the most part
    (PyTorch, Hugging Face Transformers, and so on), though some snippets come from
    Ecco, a specific library. The code can be found on GitHub: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 代码是用PyTorch编写的，大部分使用标准库（PyTorch、Hugging Face Transformers等），尽管一些片段来自Ecco，一个特定的库。代码可以在GitHub上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2)
- en: Exploring attention and self-attention
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索注意力机制和自注意力
- en: 'In the 1950s, with the beginning of the computer revolution, governments began
    to become interested in the idea of machine translation, especially for military
    applications. These attempts failed miserably, for three main reasons: machine
    translation is more complex than it seems, there was not enough computational
    power, and there was not enough data. Governments concluded that it was a technically
    impossible challenge in the 1960s.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪50年代，随着计算机革命的开始，政府开始对机器翻译的想法产生兴趣，尤其是军事应用方面。这些尝试都以失败告终，主要原因有三个：机器翻译比看起来更复杂，计算能力不足，数据也不足。政府在20世纪60年代得出结论，这是一个技术上不可能的挑战。
- en: 'By the 1990s, two of the three limitations were beginning to be overcome: the
    internet finally allowed for abundant text, and the advent of GPUs finally allowed
    for computational power. The third requirement still had to be met: a model that
    could harness the newfound computational power to handle the complexity of natural
    language.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到了20世纪90年代，三个限制中的两个开始被克服：互联网最终允许有大量的文本，GPU的出现最终允许计算能力。第三个要求仍然需要满足：一个能够利用新发现的计算能力来处理自然语言复杂性的模型。
- en: Machine translation captured the interest of researchers because it is a practical
    problem for which it is easy to evaluate the result (we can easily understand
    whether a translation is good or not). Moreover, we have an abundance of text
    in one language and a counterpart in another. So, researchers tried to adapt the
    previous models to the tasks (RNN, LSTM, and so on). The most commonly used system
    was the **seq2seq model**, where you have an **encoder** and a **decoder**. The
    encoder transforms the sequence into a new succinct representation that should
    preserve the relevant information (a sort of good summary). The decoder receives
    as input this context vector and uses this to transform (translate) this input
    into the output.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译吸引了研究者的兴趣，因为它是一个易于评估结果的实际问题（我们可以很容易地判断翻译是否好）。此外，我们有一种语言的大量文本和另一种语言的对应文本。因此，研究人员试图将先前模型适应到任务（RNN、LSTM等）中。最常用的系统是**seq2seq模型**，其中包含一个**编码器**和一个**解码器**。编码器将序列转换成一种新的简洁表示，应该保留相关信息（一种良好的摘要）。解码器接收这个上下文向量作为输入，并使用它来转换（翻译）这个输入为输出。
- en: '![Figure 2.1 – A seq2seq model with an encoder and a decoder. In one case,
    we take the average of the hidden states (left); in the other case, we use attention
    to identify which hidden state is more relevant for the translation (right) ](img/B21257_02_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 带有编码器和解码器的seq2seq模型。在一种情况下，我们取隐藏状态的平均值（左）；在另一种情况下，我们使用注意力机制来识别哪个隐藏状态对翻译更相关（右）](img/B21257_02_01.jpg)'
- en: Figure 2.1 – A seq2seq model with an encoder and a decoder. In one case, we
    take the average of the hidden states (left); in the other case, we use attention
    to identify which hidden state is more relevant for the translation (right)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 带有编码器和解码器的seq2seq模型。在一种情况下，我们取隐藏状态的平均值（左）；在另一种情况下，我们使用注意力机制来识别哪个隐藏状态对翻译更相关（右）
- en: 'RNNs and derived models have some problems:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RNN及其衍生模型存在一些问题：
- en: '**Alignment**: The length of input and output can be different (for example,
    to translate English to French “*she doesn’t like potatoes*” into “*elle n’aime
    pas les pommes* *de terre*”).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对齐**：输入和输出的长度可以不同（例如，将英语翻译成法语“*她不喜欢土豆*”翻译成“*她不喜欢苹果* *土豆*”）。'
- en: '**Vanishing and exploding gradients**: Problems that arise during training
    so that multiple layers cannot be managed effectively.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失和梯度爆炸**：在训练过程中出现的问题，使得多层无法有效管理。'
- en: '**Non-parallelizability**: Training is computationally expensive and not parallelizable.
    RNNs forget after a few steps.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可并行性**：训练计算成本高且不可并行。RNN在几步之后就会忘记。'
- en: '![Figure 2.2 – Example of issues with alignment: one to many (left) and spurious
    word (right) ](img/B21257_02_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 对齐问题的示例：一对一（左）和虚假词（右）](img/B21257_02_02.jpg)'
- en: 'Figure 2.2 – Example of issues with alignment: one to many (left) and spurious
    word (right)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 对齐问题的示例：一对一（左）和虚假词（右）
- en: '**Attention mechanisms** were initially described to solve the alignment problem,
    as well as to learn the relationships between the various parts of a text and
    the corresponding parts of the translated text.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力机制**最初被描述为解决对齐问题，以及学习文本的各个部分与相应翻译文本部分之间的关系。'
- en: The idea is that instead of passing the hidden state of RNNs, we pass contextual
    information that focuses only on the important parts of the sequence. During decoding
    (translation) for each token, we want to retrieve the corresponding and specific
    information in the other language. Attention determines which tokens in the input
    are important at that moment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是，我们不是传递RNN的隐藏状态，而是传递只关注序列重要部分的上下文信息。在解码（翻译）每个标记时，我们希望检索另一种语言中对应和具体的信息。注意力机制确定在那一刻输入中哪些标记是重要的。
- en: 'The first step is the alignment between the hidden state of the encoder (*h*)
    and the previous decoder output (*s*). The score function can be different: dot
    product or cosine similarity is most commonly used, but it can also be more complex
    functions such as the feedforward neural network layer. This step allows us to
    understand how relevant hidden state encoders are to the translation at that time.
    This step is conducted for all encoder steps.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是编码器（*h*）的隐藏状态和前一个解码器输出（*s*）之间的对齐。得分函数可以不同：点积或余弦相似度是最常用的，但也可以是更复杂的函数，例如前馈神经网络层。这一步使我们能够理解在那一刻隐藏状态编码器与翻译的相关性。这一步会对所有编码器步骤进行。
- en: <mrow><mrow><mrow><mfenced close=")" open="("><mn>1</mn></mfenced><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><mfenced close=")" open="("><mn>1</mn></mfenced><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow>
- en: Right now though, we have a scalar representing the similarity between two vectors
    (*h* and *s*). All these scores are passed into the softmax function that squeezes
    everything between 0 and 1\. This step also serves to assign relative importance
    to each hidden state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们有一个表示两个向量（*h* 和 *s*）之间相似度的标量。所有这些分数都传递到 softmax 函数中，该函数将所有值压缩到 0 和 1 之间。这一步也用于为每个隐藏状态分配相对重要性。
- en: <mrow><mrow><mfenced close=")" open="("><mn>2</mn></mfenced><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mfenced><mo>=</mo><mfrac><mrow><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>)</mo></mrow></mrow></mfrac></mrow></mrow>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mfenced close=")" open="("><mn>2</mn></mfenced><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mfenced><mo>=</mo><mfrac><mrow><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>)</mo></mrow></mrow></mfrac></mrow></mrow>
- en: Finally, we conduct a weighted sum of the various hidden states multiplied by
    the attention score. So, we have a fixed-length context vector capable of giving
    us information about the entire set of hidden states. In simple words, during
    translation, we have a context vector that is dynamically updated and tells us
    how much attention we should give to each part of the input sequence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对各种隐藏状态进行加权求和，乘以注意力分数。因此，我们有一个固定长度的上下文向量，能够提供关于整个隐藏状态集的信息。简单来说，在翻译过程中，我们有一个动态更新的上下文向量，告诉我们应该对输入序列的每个部分给予多少注意力。
- en: <mrow><mrow><mfenced close=")" open="("><mn>3</mn></mfenced><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>∙</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mfenced close=")" open="("><mn>3</mn></mfenced><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>∙</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow>
- en: As you can see from the original article, the model pays different attention
    to the various words in the input during translation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如原文所示，模型在翻译过程中对输入中的不同单词给予不同的注意力。
- en: '![Figure 2.3 – Example of alignment between sentences after training the model
    with attention. Each pixel shows the attention weight between the source word
    and the target word. (https://arxiv.org/pdf/1409.0473)](img/B21257_02_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 使用注意力训练模型后的句子对齐示例。每个像素显示源词和目标词之间的注意力权重。](img/B21257_02_03.jpg)'
- en: Figure 2.3 – Example of alignment between sentences after training the model
    with attention. Each pixel shows the attention weight between the source word
    and the target word. ([https://arxiv.org/pdf/1409.0473](https://arxiv.org/pdf/1409.0473))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 使用注意力训练模型后的句子对齐示例。每个像素显示源词和目标词之间的注意力权重。[https://arxiv.org/pdf/1409.0473](https://arxiv.org/pdf/1409.0473)
- en: 'In addition to solving alignment, the attention mechanism has other advantages:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决对齐问题外，注意力机制还有其他优点：
- en: It reduces the vanishing gradient problem because it provides a shortcut to
    early states.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了梯度消失问题，因为它提供了一个通往早期状态的捷径。
- en: It eliminates the bottleneck problem; the encoder can directly go to the source
    in the translation.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它消除了瓶颈问题；编码器可以直接在翻译中访问源。
- en: It also provides interpretability because we know which words are used for alignment.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还提供了可解释性，因为我们知道哪些词用于对齐。
- en: It definitely improves the performance of the model.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无疑提高了模型的性能。
- en: Its success has given rise to several variants where the scoring function is
    different. One variant in particular, called **self-attention**, has the particular
    advantage that it extracts information directly from the input without necessarily
    needing to compare it with something else.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 它的成功催生了几个变体，其中评分函数不同。特别是其中一个变体，称为**自注意力**，具有特别的优势，即它可以直接从输入中提取信息，而不必一定需要与其他东西进行比较。
- en: The insight behind self-attention is that if we want to look for a book for
    an essay on the French Revolution in a library (query), we don’t need to read
    all the books to find a book on the history of France (value), we just need to
    read the coasts of the books (key). Self-attention, in other words, is a method
    that allows us to search within context to find the representation we need.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的背后洞察是，如果我们想在图书馆（查询）寻找一篇关于法国大革命的论文，我们不需要阅读所有书籍来找到关于法国历史的书籍（值），我们只需要阅读书籍的边缘（键）。换句话说，自注意力是一种允许我们在上下文中搜索以找到所需表示的方法。
- en: '![Figure 2.4 – Self-attention mechanism. Matrix dimensions are included; numbers
    are arbitrary](img/B21257_02_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 自注意力机制。包括矩阵维度；数字是任意的](img/B21257_02_04.jpg)'
- en: Figure 2.4 – Self-attention mechanism. Matrix dimensions are included; numbers
    are arbitrary
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 自注意力机制。包括矩阵维度；数字是任意的
- en: 'Transacting this for a model, given an input we want to conduct a series of
    comparisons between the various components of the sequence (such as tokens) to
    obtain an output sequence (which we can then use for various models or tasks).
    The self-attention equation is as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型来说，给定一个输入，我们想要在序列的各种组件（如标记）之间进行一系列比较以获得输出序列（然后我们可以将其用于各种模型或任务）。自注意力方程如下：
- en: <mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mfenced
    close=")" open="("><mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><mfrac><mrow><mi>Q</mi><mo>∙</mo><msup><mi>k</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mo>∙</mo><mi>V</mi></mrow></mrow>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mfenced
    close=")" open="("><mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><mfrac><mrow><mi>Q</mi><mo>∙</mo><msup><mi>k</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mo>∙</mo><mi>V</mi></mrow></mrow>
- en: You can immediately see that it is derived from the original attention formula.
    We have the dot product to conduct comparisons, and we then exploit the `softmax`
    function to calculate the relative importance and normalize the values between
    0 and 1\. *D* is the size of the sequence; in other words, self-attention is also
    normalized as a function of the length of our sequence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以立即看出，它是从原始注意力公式派生出来的。我们使用点积来进行比较，然后利用`softmax`函数来计算相对重要性和将值归一化到0和1之间。*D*是序列的大小；换句话说，自注意力也是作为我们序列长度的函数进行归一化的。
- en: 'The next step is using `softmax`. Here’s a little refresher on the function
    (how you calculate and how it is implemented more efficiently in Python):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用`softmax`。这里是对该函数（如何计算以及如何在Python中更高效地实现）的一个小复习：
- en: <mrow><mrow><mi>y</mi><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow></mrow>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>y</mi><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow></mrow>
- en: <mrow><mrow><mi>y</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center"
    columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex
    1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")"
    open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto"
    rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline
    baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></mrow>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>p</mi><mi>y</mi><mi>t</mi><mi>h</mi><mi>o</mi><mi>n</mi><mo>:</mo><mi>y</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline
    baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")"
    open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto"
    rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfrac><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac><mo>=</mo><mfrac><msup><mi>e</mi><mi>x</mi></msup><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>)</mo></mrow></mfrac></mrow></mrow>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>p</mi><mi>y</mi><mi>t</mi><mi>h</mi><mi>o</mi><mi>n</mi><mo>:</mo><mi>y</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline
    baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")"
    open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto"
    rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfrac><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac><mo>=</mo><mfrac><msup><mi>e</mi><mi>x</mi></msup><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>)</mo></mrow></mfrac></mrow></mrow>
- en: As we saw in the previous chapter, the dot product can become quite wide as
    the length of the vectors increases. This can lead to inputs that are too large
    in the `softmax` function (this shifts the probability mass in `softmax` to a
    few elements and thus leads to small gradients). In the original article, they
    solved this by normalizing by the square root of *D*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，随着向量长度的增加，点积可以变得相当宽。这可能导致`softmax`函数中的输入太大（这会将`softmax`中的概率质量转移到几个元素上，从而导致梯度很小）。在原始文章中，他们通过将*D*的平方根进行归一化来解决此问题。
- en: '![Figure 2.5 – Self-attention unrolled](img/B21257_02_05.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – 展开的自我注意力](img/B21257_02_05.jpg)'
- en: Figure 2.5 – Self-attention unrolled
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 展开的自我注意力
- en: The real difference is that we use three matrices of weights **Query** (**Q**),
    **Key** (**K**), and **Value** (**V**) that are initially randomly initialized.
    *Q* is the current focus of attention, while *K* informs the model about previous
    inputs, and *V* serves to extract the final input information. So, the first step
    is the multiplication of these three matrices with our input *X* (an array of
    vectors, of which each represents a token).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的区别在于我们使用三个初始随机初始化的权重矩阵：**查询**（**Q**）、**键**（**K**）和**值**（**V**）。*Q*是当前注意力的焦点，*K*向模型提供关于先前输入的信息，而*V*用于提取最终输入信息。因此，第一步是将这三个矩阵与我们的输入*X*（一个向量数组，其中每个向量代表一个标记）相乘。
- en: <mrow><mrow><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi
    mathvariant="bold-italic">W</mi><mi>Q</mi></msup><mo>,</mo><mi mathvariant="bold-italic">K</mi><mo>=</mo><mi
    mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>V</mi></msup><mo>,</mo><mi
    mathvariant="bold-italic">V</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi
    mathvariant="bold-italic">W</mi><mi>V</mi></msup></mrow></mrow>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><mrow><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi
    mathvariant="bold-italic">W</mi><mi>Q</mi></msup><mo>,</mo><mi mathvariant="bold-italic">K</mi><mo>=</mo><mi
    mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>V</mi></msup><mo>,</mo><mi
    mathvariant="bold-italic">V</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi
    mathvariant="bold-italic">W</mi><mi>V</mi></msup></mrow></mrow>'
- en: The beauty of this system is that we can use it to extract more than one representation
    from the same input (after all, we can have multiple questions in a textbook).
    Therefore, since the operations are parallelizable, we can have multi-head attention.
    **Multi-head self-attention** enables the model to simultaneously capture multiple
    types of relationships within the input sequence. This is crucial because a single
    word in a sentence can be contextually related to several other words. During
    training, the *K* and *Q* matrices in each head specialize in modeling different
    kinds of relationships. Each attention head produces an output based on its specific
    perspective, resulting in *n* outputs for *n* heads. These outputs are then concatenated
    and passed through a final linear projection layer to restore the dimensionality
    backt to the original input size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统的美妙之处在于我们可以用它从相同的输入中提取多个表示（毕竟，教科书里可以有多个问题）。因此，由于操作可以并行化，我们可以有多头注意力。**多头自注意力**使得模型能够同时捕捉输入序列中多种类型的关系。这一点至关重要，因为句子中的一个词在上下文中可能与几个其他词相关。在训练过程中，每个头中的*K*和*Q*矩阵专门用于建模不同类型的关系。每个注意力头根据其特定的视角产生输出，从而为*n*个头生成*n*个输出。然后，这些输出被连接起来，并通过最终的线性投影层恢复到原始输入的大小。
- en: '![Figure 2.6 – Multi-head self-attention](img/B21257_02_06.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图2.6 – 多头自注意力](img/B21257_02_06.jpg)'
- en: Figure 2.6 – Multi-head self-attention
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 多头自注意力
- en: 'Self-attention has several advantages:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制具有几个优点：
- en: We can extract different representations for each input.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以为每个输入提取不同的表示。
- en: We can conduct all these computations in parallel and thus with a GPU. Each
    head can be computed independently.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在GPU上并行执行所有这些计算。每个头可以独立计算。
- en: We can use it in models that do not necessarily consist of an encoder and a
    decoder.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在不必要包含编码器和解码器的模型中使用它。
- en: We do not have to wait for different time steps to see the relationship between
    distant word pairs (as in RNN).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不必等待不同时间步来看到远距离词对之间的关系（如在RNN中）。
- en: However, it has a quadratic cost in function of the number of tokens *N*, and
    it has no inherent notion of order.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，它在函数上具有与标记数*N*的二次成本，并且没有固有的顺序概念。
- en: 'Self-attention is computationally expensive. It can be shown that, considering
    a sequence *T* and sequence length *d*, the computation cost and space is quadratic:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力在计算上很昂贵。可以证明，考虑到序列*T*和序列长度*d*，计算成本和空间是二次的：
- en: <mrow><mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mi mathvariant="script">O</mi><mfenced
    close=")" open="("><mrow><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi></mrow></mfenced><mi>s</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi
    mathvariant="script">O</mi><mo>(</mo><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>T</mi><mi>d</mi><mo>)</mo></mrow></mrow></mrow>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mi mathvariant="script">O</mi><mfenced
    close=")" open="("><mrow><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi></mrow></mfenced><mi>s</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi
    mathvariant="script">O</mi><mo>(</mo><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>T</mi><mi>d</mi><mo>)</mo></mrow></mrow></mrow>'
- en: They identified the dot product as the culprit. This computational cost is one
    of the problems of scalability (taking into account that multi-head attention
    is calculated in each block). For this reason, many variations of self-attention
    have been proposed to reduce the computational cost.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将点积识别为罪魁祸首。这种计算成本是可扩展性问题之一（考虑到多头注意力在每个块中计算）。因此，已经提出了许多自注意力的变体来降低计算成本。
- en: Despite the computational cost, self-attention has shown its capability, especially
    when several layers are stacked on top of each other. In the next section, we
    will discuss how this makes the model extremely powerful despite its computational
    cost.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算成本较高，但自注意力已经显示出其能力，尤其是在堆叠多层时。在下一节中，我们将讨论这如何使模型尽管计算成本高，但仍然非常强大。
- en: Introducing the transformer model
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Transformer模型
- en: 'Despite this decisive advance though, several problems remain in machine translation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这个决定性的进步，但在机器翻译中仍存在几个问题：
- en: The model fails to capture the meaning of the sentence and is still error-prone
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型未能捕捉句子的含义，并且仍然存在错误倾向
- en: In addition, we have problems with words that are not in the initial vocabulary
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们还有初始词汇表中没有的单词的问题
- en: Errors in pronouns and other grammatical forms
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代词和其他语法形式中的错误
- en: The model fails to maintain context for long texts
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型未能维持长文本的上下文
- en: It is not adaptable if the domain in the training set and test data is different
    (for example, if it is trained on literary texts and the test set is finance texts)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练集和测试数据中的领域不同（例如，如果它在文学文本上训练，而测试集是金融文本），则不可适应
- en: RNNs are not parallelizable, and you have to compute sequentially
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN不可并行化，必须按顺序计算
- en: Considering these points, Google researchers in 2016 came up with the idea of
    eliminating RNNs altogether rather than improving them. According to the authors
    of the *Attention is All You Need* seminal article; all you need is a model that
    is based on multi-head self-attention. Before going into detail, the transformer
    consists entirely of stacked layers of multi-head self-attention. In this way,
    the model learns a hierarchical and increasingly sophisticated representation
    of the text.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些点，2016年谷歌研究人员提出了完全消除RNN而不是改进它们的想法。根据《注意力即一切》开创性文章的作者；你所需要的是一个基于多头自注意力的模型。在详细说明之前，Transformer模型完全由堆叠的多头自注意力层组成。通过这种方式，模型学习到文本的分层和越来越复杂的表示。
- en: The first step in the process is the transformation of text into numerical vectors
    (tokenization). After that, we have an embedding step to obtain vectors for each
    token. A special feature of the transformer is the introduction of a function
    to record the position of each token in the sequence (self-attention is not position-aware).
    This process is called **positional encoding**. The authors in the article use
    sin and cos alternately with position. This allows the model to know the relative
    position of each token.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的第一步是将文本转换为数值向量（分词）。之后，我们有一个嵌入步骤来获取每个标记的向量。Transformer的一个特殊功能是引入一个函数来记录序列中每个标记的位置（自注意力不是位置感知的）。这个过程被称为**位置编码**。文章中的作者使用正弦和余弦函数交替使用位置。这使得模型能够知道每个标记的相对位置。
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
- en: In the first step, the embedding vectors are summed with the result of these
    functions. This is because self-attention is not aware of word order, but word
    order in a period is important. Thus, the order is directly encoded in the vectors
    it awaits. Note, though, that there are no learnable parameters in this function
    and that for long sequences, it will have to be modified (we will discuss this
    in the next chapter).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，嵌入向量与这些函数的结果相加。这是因为自注意力不知道词序，但一个句子中的词序很重要。因此，顺序直接编码在它等待的向量中。注意，然而，这个函数中没有可学习的参数，并且对于长序列，它将需要修改（我们将在下一章讨论这个问题）。
- en: '![Figure 2.7 – Positional encoding](img/B21257_02_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7 – 位置编码](img/B21257_02_07.jpg)'
- en: Figure 2.7 – Positional encoding
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 位置编码
- en: 'After that, we have a series of transformer blocks in sequence. The **transformer
    block** consists of four elements: multi-head self-attention, feedforward layer,
    residual connections, and layer normalization.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们按顺序有一系列变换器模块。**变换器模块**由四个元素组成：多头自注意力、前馈层、残差连接和层归一化。
- en: '![Figure 2.8 – Flow diagram of the transformer block](img/B21257_02_08.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8 – 变换器模块的流程图](img/B21257_02_08.jpg)'
- en: Figure 2.8 – Flow diagram of the transformer block
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 变换器模块的流程图
- en: The feedforward layer consists of two linear layers. This layer is used to obtain
    a linear projection of the multi-head self-attention. The weights are identifiable
    for each position and are separated. It can be seen as two linear transformations
    with one ReLU activation in between.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈层由两个线性层组成。这一层用于获取多头自注意力的线性投影。权重对于每个位置都是可识别的，并且是分开的。它可以看作是两个线性变换，中间有一个ReLU激活。
- en: <mml:math display="block"><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">x</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>
- en: This adds a step of non-linearity to self-attention. The FFN layer is chosen
    because it is an easily parallelized operation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这为自注意力添加了一个非线性步骤。选择FFN层是因为它是一个易于并行化的操作。
- en: 'Residual connections are connections that pass information between two layers
    without going through the intermediate layer transformation. Initially developed
    in convolutional networks, they allow a shortcut between layers and help the gradient
    pass down to the lower layers. In the transformer, blocks are present for both
    the attention layer and feedforward, where the input is summed with the output.
    Residual connections also have the advantage of making the loss surface smoother
    (this helps the model find a better minimum and not get stuck in a local loss).
    This powerful effect can be seen clearly in *Figure 2**.9*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余连接是两种层之间传递信息而不经过中间层变换的连接。最初在卷积网络中开发，它们允许层之间有捷径，并帮助梯度向下传递到较低层。在Transformer中，存在注意层和前馈的块，其中输入与输出相加。剩余连接还有使损失表面更平滑的优势（这有助于模型找到更好的最小值，而不会陷入局部损失）。这种强大的效果在*图2.9*中可以清楚地看到：
- en: '![Figure 2.9 – Effect of the residual connections on the loss](img/B21257_02_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9 – 剩余连接对损失的影响](img/B21257_02_09.jpg)'
- en: Figure 2.9 – Effect of the residual connections on the loss
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – 剩余连接对损失的影响
- en: Note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 2**.9* is originally from *Visualizing the Loss Landscape of Neural
    Nets* by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein ([https://github.com/tomgoldstein/loss-landscape/tree/master](https://github.com/tomgoldstein/loss-landscape/tree/master)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.9*最初来自Hao Li、Zheng Xu、Gavin Taylor、Christoph Studer和Tom Goldstein的《可视化神经网络损失景观》（[https://github.com/tomgoldstein/loss-landscape/tree/master](https://github.com/tomgoldstein/loss-landscape/tree/master))。'
- en: The residual connection makes the loss surface smoother, which allows the model
    to be trained more efficiently and quickly.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余连接使损失表面更平滑，这允许模型更高效、更快速地训练。
- en: 'Layer normalization is a form of normalization that helps training because
    it keeps the hidden layer values in a certain range (it is an alternative to batch
    normalization). Having taken a single vector, it is normalized in a process that
    takes advantage of the mean and standard deviation. Having calculated the mean
    and standard deviation, the vector is scaled:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化是一种归一化形式，它有助于训练，因为它保持隐藏层值在某个范围内（它是批归一化的替代方案）。在取了一个单向量之后，它通过利用均值和标准差的过程进行归一化。计算了均值和标准差后，向量被缩放：
- en: <mrow><mrow><mi>μ</mi><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mrow></mrow>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>μ</mi><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mrow></mrow>
- en: <mrow><mrow><mover><mi>x</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mi>σ</mi></mfrac></mrow></mrow>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mover><mi>x</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mi>σ</mi></mfrac></mrow></mrow>
- en: In the final transformation, we exploit two parameters that are learned during
    training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终的转换中，我们利用了训练期间学习的两个参数。
- en: <mrow><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>γ</mi><mover><mi>x</mi><mo
    stretchy="true">ˆ</mo></mover><mo>+</mo><mi>β</mi></mrow></mrow>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>γ</mi><mover><mi>x</mi><mo
    stretchy="true">ˆ</mo></mover><mo>+</mo><mi>β</mi></mrow></mrow>
- en: There is a lot of variability during the training, and this can hurt the learning
    of the training. To reduce uninformative variability, we add this normalization
    step, thus normalizing the gradient as well.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中有很多可变性，这可能会损害训练学习。为了减少无信息可变性，我们添加了这个归一化步骤，从而也归一化了梯度。
- en: 'At this point, we can assemble everything into a single block. Consider that
    after embedding, we have as input *X* a matrix of dimension *n x d* (with *n*
    being the number of tokens, and *d* the dimensions of the embedding). This input
    *X* goes into a transformer block and comes out with the same dimensions. This
    process is repeated for all transformer blocks:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以将所有内容组装成一个单独的块。考虑在嵌入之后，我们输入 *X* 是一个维度为 *n x d* 的矩阵（其中 *n* 是标记的数量，*d*
    是嵌入的维度）。这个输入 *X* 进入转换器块，并以相同的维度输出。这个过程会重复应用于所有转换器块：
- en: <mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>
- en: <mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>
- en: 'Some notes on this process are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此过程的几点说明如下：
- en: In some architectures, *LayerNorm* can be after the *FFN* block instead of before
    (whether it is better or not is still debated).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些架构中，*层归一化*可以放在 *FFN* 块之后而不是之前（是否更好仍在讨论中）。
- en: Modern models have up to 96 transformer blocks in series, but the structure
    is virtually identical. The idea is that the model learns an increasingly complex
    representation of the language.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代模型串联的转换器块最多可达96个，但结构实际上完全相同。其理念是模型学习语言越来越复杂的表示。
- en: Starting with the embedding of an input, self-attention allows this representation
    to be enriched by incorporating an increasingly complex context. In addition,
    the model also has information about the location of each token.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从输入的嵌入开始，自注意力允许通过结合越来越复杂的上下文来丰富这种表示。此外，模型还了解每个标记的位置信息。
- en: Absolute positional encoding has the defect of overrepresenting words at the
    beginning of the sequence. Today, there are variants that consider the relative
    position.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对位置编码的缺陷是过度表示序列开头的单词。今天，有考虑相对位置的变体。
- en: 'Once we have “the bricks,” we can assemble them into a functional structure.
    In the original description, the model was structured for machine translation
    and composed of two parts: an encoder (which takes the text to be translated)
    and a decoder (which will produce the translation).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了“积木”，我们就可以将它们组装成一个功能结构。在原始描述中，该模型是为机器翻译而设计的，由两部分组成：一个编码器（它接受要翻译的文本）和一个解码器（它将生成翻译）。
- en: The original transformer is composed of different blocks of transformer blocks
    and structures in an encoder and decoder, as you can see in *Figure 2**.10*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的转换器由编码器和解码器中的不同转换器块和结构组成，如图 *2**.10* 所示。
- en: '![Figure 2.10 – Encoder-decoder structure](img/B21257_02_10.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10 – 编码器-解码器结构](img/B21257_02_10.jpg)'
- en: Figure 2.10 – Encoder-decoder structure
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 编码器-解码器结构
- en: 'The decoder, like the encoder, is composed of an embedding, a positional encoder,
    and a series of transformer blocks. One note is that in the decoder, instead of
    self-attention, we have **cross-attention**. Cross-attention is exactly the same,
    only we take both elements from the encoder and the decoder (because we want to
    condition the generation of the decoder based on the encoder input). In this case,
    the queries come from the encoder and the rest from the decoder. As you can see
    from *Figure 2**.11*, the decoder sequence can be of different sizes, but the
    result is the same:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器，就像编码器一样，由嵌入层、位置编码器和一系列的Transformer块组成。有一点需要注意的是，在解码器中，我们不是使用自注意力，而是使用**交叉注意力**。交叉注意力完全相同，只是我们同时从编码器和解码器中取两个元素（因为我们希望解码器的生成基于编码器输入）。在这种情况下，查询来自编码器，其余来自解码器。正如你从*图2*.11中可以看到的，解码器序列可以有不同的长度，但结果是一样的：
- en: '![Figure 2.11 – Cross-attention](img/B21257_02_11.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – 交叉注意力](img/B21257_02_11.jpg)'
- en: Figure 2.11 – Cross-attention
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 交叉注意力
- en: Input *N* comes from the encoder, while input *M* is from the decoder. In the
    figure, cross-attention is mixing information from the encoder and decoder, allowing
    the decoder to learn from the encoder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输入*N*来自编码器，而输入*M*来自解码器。在图中，交叉注意力正在混合编码器和解码器中的信息，使解码器能够从编码器中学习。
- en: 'Another note on the structure: in the decoder, the first self-attention has
    an additional mask to prevent the model from seeing the future.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关于结构的另一个注意事项：在解码器中，第一个自注意力机制有一个额外的遮蔽层，以防止模型看到未来的信息。
- en: 'This is especially true in the case of QT. In fact, if one wants to predict
    the next word and the model already knows what it is, we have data leakage. To
    compensate for this, we add a mask in which the upper-triangular portion is replaced
    with negative infinity: - ∞.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这在QT的情况下尤其如此。事实上，如果想要预测下一个单词，而模型已经知道它是什么，我们就有了数据泄露。为了补偿这一点，我们添加了一个遮蔽层，其中上三角部分被替换为负无穷大：-
    ∞。
- en: '![Figure 2.12 – Masked attention](img/B21257_02_12.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – 遮蔽注意力](img/B21257_02_12.jpg)'
- en: Figure 2.12 – Masked attention
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 遮蔽注意力
- en: The first transformer consisted of an encoder and decoder, but today there are
    also models that are either encoder-only or decoder-only. Today, for generative
    AI, they are practically all decoder-only. We have our model; now, how can you
    train a system that seems so complex? In the next section, we will see how to
    succeed at training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个Transformer由一个编码器和解码器组成，但今天也有模型要么只有编码器，要么只有解码器。今天，对于生成式AI，它们实际上都是只有解码器。我们有了我们的模型；现在，如何训练这样一个看似复杂的系统呢？在下一节中，我们将看到如何成功地进行训练。
- en: Training a transformer
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个Transformer
- en: How do you train such a complex model? The answer to this question is simpler
    than you might think. The fact that the model can learn through multi-head self-attention
    complex and diverse relationships allows the model to be able to be flexible and
    able to learn complex patterns. It would be too expensive to build examples (or
    find them) to teach these complex relationships to the model. So, we want a system
    that allows the model to learn these relationships on its own. The advantage is
    that if we have a large amount of text available, the model can learn without
    the need for us to curate the training corpus. Thanks to the advent of the internet,
    we have the availability of huge corpora that allow models to see text examples
    of different topics, languages, styles, and more.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何训练这样一个复杂的模型？这个问题的答案比你想象的要简单。模型可以通过多头自注意力学习复杂且多样化的关系，这使得模型能够灵活地学习复杂的模式。构建示例（或找到它们）来教模型这些复杂关系会非常昂贵。因此，我们希望有一个系统，允许模型自己学习这些关系。优势在于，如果我们有大量的文本可用，模型可以在我们不需要精心挑选训练语料库的情况下进行学习。多亏了互联网的出现，我们有了大量语料库，允许模型看到不同主题、语言、风格等的文本示例。
- en: 'Although the original model was a `seq2seq` model, later transformers (such
    as LLMs) were trained as language models, especially in a **self-supervised manner**.
    In language modeling, we consider a sequence of word *s*, and the probability
    of the next word in the sequence *x* is <mml:math><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math>.
    This probability depends on the words up to that point. By the chain rule of the
    probability, we can decompose this probability:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原始模型是一个`seq2seq`模型，但后来的转换器（如LLMs）被训练成语言模型，尤其是在**自监督方式**下。在语言建模中，我们考虑一个单词序列
    *s*，序列中下一个单词 *x* 的概率是 <mml:math><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math>。这个概率取决于该点之前的所有单词。通过概率的链式法则，我们可以分解这个概率：
- en: <mml:math display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
- en: This allows us to calculate the conditional probability of a word from a sequence
    of previous words. The idea is that when we have enough text we can take a sequence
    such as **“to be or not to”** as input and have the model estimate the probability
    for the next word to be **“be,”** <mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>b</mi><mi>e</mi><mo>|</mo><mi>t</mi><mi>o</mi><mi>b</mi><mi>e</mi><mi>o</mi><mi>r</mi><mi>n</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>o</mi><mo>)</mo></mrow></mrow></mrow>.
    Then after the transformer block sequence, we have a layer that conducts a linear
    projection and a **softmax layer** that generates the output. The previous sequence
    is called context; the context length of the first transformers was 512 tokens.
    The model generates an output, which is a probability vector of dimension *V*
    (the model vocabulary), also called a **logit vector**. The projection layer is
    called an **unembedder** (it does reverse mapping) because we have to go from
    a dimension *N* tokens x *D* embedding to 1 x *V*. Since the input and output
    of each transformer block are the same, we could theoretically eliminate blocks
    and attach an unembedder and softmax to any intermediate block. This allows us
    to better interpret the function of each block and its internal representation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够计算从一系列先前单词中一个单词的条件概率。其理念是，当我们拥有足够的文本时，我们可以将诸如**“to be or not to”**这样的序列作为输入，并让模型估计下一个单词为**“be”**的概率，即
    `<mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>b</mi><mi>e</mi><mo>|</mo><mi>t</mi><mi>o</mi><mi>b</mi><mi>e</mi><mi>o</mi><mi>r</mi><mi>n</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>o</mi><mo>)</mo></mrow></mrow></mrow>`.
    然后，在transformer块序列之后，我们有一个执行线性投影的层和一个生成输出的**softmax层**。之前的序列被称为上下文；第一个transformer的上下文长度为512个标记。模型生成一个输出，这是一个维度为*V*（模型词汇表）的概率向量，也称为**logit向量**。投影层被称为**unembedder**（它执行反向映射），因为我们必须从*N*标记
    x *D*嵌入维度映射到1 x *V*。由于每个transformer块的输入和输出相同，理论上我们可以消除块，并将unembedder和softmax附加到任何中间块上。这使我们能够更好地解释每个块的功能及其内部表示。
- en: Once we have this probability vector, we can use self-supervision for training.
    We take a corpus of text (unannotated) and train the model to minimize the difference
    between the probability of the true word in the sequence and the predicted probability.
    To do this, we use **cross-entropy loss** (the difference between the predicted
    and true probability distribution). The predicted probability distribution is
    the logit vector, while the true one is a one-hot encoder vector where it is 1
    for the next word in the sequence and 0 elsewhere.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个概率向量，我们就可以使用自监督进行训练。我们取一个文本语料库（未标注）并训练模型以最小化序列中真实单词的概率与预测概率之间的差异。为此，我们使用**交叉熵损失**（预测概率分布与真实概率分布之间的差异）。预测概率分布是logit向量，而真实的一个是one-hot编码向量，其中序列中的下一个单词为1，其他地方为0。
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced
    close="]" open="[" separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced
    close="]" open="[" separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math>`'
- en: In practice, it is simplified during training simply between the probability
    of the actual predicted word and 1\. The process is iterative for each word in
    the word sequence (and is called teacher forcing). The final loss is the average
    over the entire sequence.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练过程中只是简单地计算实际预测单词的概率和1之间的差异。这个过程对序列中的每个单词都是迭代的（称为教师强制）。最终的损失是整个序列的平均值。
- en: '![Figure 2.13 – Training of the transformer; the loss is the average of the
    loss of all the time steps](img/B21257_02_13.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – transformer的训练；损失是所有时间步长的平均损失](img/B21257_02_13.jpg)'
- en: Figure 2.13 – Training of the transformer; the loss is the average of the loss
    of all the time steps
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – transformer的训练；损失是所有时间步长的平均损失
- en: Since all calculations can be done in parallel in the transformer, we do not
    have to calculate word by word, but we fed the model with the whole sequence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有计算都可以在transformer中并行进行，我们不需要逐词计算，而是将整个序列输入到模型中。
- en: 'Once we have obtained a probability vector, we can choose the probability most
    (**greedy decoding**). Greedy decoding is formally defined as choosing the token
    with the highest probability at each time step:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了一个概率向量，我们可以选择概率最高的（**贪婪解码**）。贪婪解码形式上定义为在每个时间步选择概率最高的标记：
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo><</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mml:math display="block"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo><</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>`'
- en: In fact, it is no longer used because the result is too predictable, generic,
    and repetitive. So, more sophisticated and less deterministic sampling methods
    are used. This sampling process is called decoding (or autoregressive generation
    or causal language modeling, since it is derived from previous word choice). This
    system, in the simplest version, is based either on generating the text of at
    most a predetermined sequence length, or as long as an end-of-sentence token (`<EOS>`)
    is selected.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，它已经不再使用了，因为结果太可预测、太通用和太重复。因此，使用更复杂且确定性较低的采样方法。这个过程被称为解码（或自回归生成或因果语言建模，因为它是从先前的单词选择中派生出来的）。这个系统的最简单版本是基于生成最多预定的序列长度文本，或者直到选择一个句子结束标记（`<EOS>`）。
- en: 'We need to find a way to be able to select tokens while balancing both quality
    and diversity. A model that always chooses the same words will certainly have
    higher quality but will also be repetitive. There are different methods of doing
    the sampling:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到一种方法，在平衡质量和多样性的同时选择标记。一个总是选择相同单词的模型肯定会具有更高的质量，但也会很重复。有几种不同的采样方法：
- en: '**Random sampling**: The model chooses the next token randomly. The sentences
    are strange because the model chooses rare or singular words.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机采样**：模型随机选择下一个标记。句子看起来很奇怪，因为模型选择了罕见或独特的单词。'
- en: '**Top-k sampling**: At each step, we sort the probabilities and choose the
    top *k* most likely words. We renormalize the probability and choose one at random.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-k采样**：在每一步，我们对概率进行排序，并选择概率最高的*k*个单词。我们重新归一化概率并随机选择一个。'
- en: '**Top-p sampling**: This is an alternative in which we keep only a percentage
    of the most likely words.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top-p采样**：这是一种替代方案，我们只保留最可能单词的一定百分比。'
- en: '`softmax`, we divide by a temperature parameter (between 0 and 1). The closer
    *t* is to 0, the closer the probability of the most likely words is to 1 (close
    to greedy sampling). In some cases, we can also have *t* greater than 1 when we
    want a less greedy approach.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`softmax`，我们将除以一个温度参数（介于0和1之间）。当*t*越接近0时，最可能单词的概率就越接近1（接近贪婪采样）。在某些情况下，当我们想要一个不那么贪婪的方法时，*t*也可以大于1。'
- en: So far, we have considered the fixed vocabulary and assumed that each token
    was a word. In general, once the model is trained, there might be some words that
    the model does not know to which a special token, `<UNK>`, is assigned. In transformers
    and LLMs afterward, a way was sought to solve the unknown word problem. For example,
    in the training set, we might have words such as *big*, *bigger*, and *small*
    but not *smaller*. *Smaller* would not be known by the model and would result
    in `<UNK>`. Depending on the training set, the model might have incomplete or
    outdated knowledge. In English, as in other languages, there are definite morphemes
    and grammatical rules, and we would like the tokenizer to be aware. To avoid too
    many `<UNK>` one solution is to think in terms of sub-words (tokens).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑了固定词汇，并假设每个标记都是一个单词。一般来说，一旦模型训练完成，可能会有一些模型不知道的单词，这些单词被分配了一个特殊的标记`<UNK>`。在之后的transformers和LLMs中，寻求了一种解决未知词问题的方法。例如，在训练集中，我们可能有诸如*big*、*bigger*和*small*这样的单词，但没有*smaller*。模型不会知道*smaller*，会导致`<UNK>`。根据训练集，模型可能有不完整或过时的知识。在英语和其他语言中，都有确切的词素和语法规则，我们希望分词器能够意识到这一点。为了避免出现太多的`<UNK>`，一种解决方案是从子词（标记）的角度思考。
- en: One of the most widely used is **Byte-Pair Encoding** (**BPE**). The process
    starts with a list of individual characters. The algorithm then scans the entire
    corpus and begins to merge the symbols that are most frequently found together.
    For example, we have **E** and **R**, and after the first scan, we add a new **ER**
    symbol to the vocabulary. The process continues iteratively to merge and create
    new symbols (longer and longer character strings). Typically, the algorithm stops
    when it has created *N* tokens (with *N* being a predetermined number at the beginning).
    In addition, there is a special end-of-word symbol to differentiate whether the
    token is inside or at the end of a word. Once the algorithm arrives at creating
    a vocabulary, we can segment the corpus with the tokenizer and for each subword,
    we assign an index corresponding to the index in the vocabulary.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的方法是**字节对编码**（**BPE**）。这个过程从一组单个字符开始。算法随后扫描整个语料库，并开始合并最频繁一起出现的符号。例如，我们有**E**和**R**，在第一次扫描后，我们向词汇表中添加一个新的**ER**符号。这个过程会迭代地进行，合并并创建新的符号（越来越长的字符字符串）。通常，算法会在创建了*N*个标记（*N*是开始时预定的一个数字）后停止。此外，还有一个特殊的单词结束符号，用于区分标记是在单词内部还是单词的末尾。一旦算法到达创建词汇表，我们就可以使用分词器对语料库进行分词，并为每个子词分配一个与词汇表中索引相对应的索引。
- en: '![Figure 2.14 – Example of the results of tokenization](img/B21257_02_14.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – 分词结果的示例](img/B21257_02_14.jpg)'
- en: Figure 2.14 – Example of the results of tokenization
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 分词结果的示例
- en: This approach generally causes common words to be present in the model vocabulary
    while rare words are split into subwords. In addition, the model also learns suffixes
    and prefixes, and considers the difference between *app* and the *app#* subword,
    representing a complete word and a subword (*app#* as a subword of *application*).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通常会导致常见词汇出现在模型词汇表中，而罕见词汇则被分割成子词。此外，模型还会学习后缀和前缀，并考虑*app*和*app#*子词之间的区别，代表一个完整的单词和一个子词（*app#*是*application*的子词）。
- en: Exploring masked language modeling
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索掩码语言建模
- en: Although the transformer was revolutionary, the popularization of the transformer
    in the scientific community is also due to the **Bidirectional Encoder Representations
    from Transformers** (**BERT**) model. This is because BERT was a revolutionary
    variant of the transformer that showed the capabilities of this type of model.
    BERT was revolutionary because it was already prospectively designed specifically
    for future applications (such as question answering, summarization, and machine
    translation). In fact, the original transformer analyzes the left-to-right sequence,
    so when the model encounters an entity, it cannot relate it to what is on the
    right of the entity. In these applications, it is important to have context from
    both directions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管transformer具有革命性，但transformer在科学界的普及也归功于**双向编码器表示的Transformer**（**BERT**）模型。这是因为BERT是transformer的一个革命性变体，展示了这种类型模型的能力。BERT之所以革命性，是因为它已经前瞻性地专门为未来的应用（如问答、摘要和机器翻译）进行了设计。事实上，原始的transformer分析的是从左到右的序列，所以当模型遇到一个实体时，它不能将其与实体右侧的内容联系起来。在这些应用中，从两个方向获取上下文非常重要。
- en: '![Figure 2.15 – Difference between a causal and bidirectional language model](img/B21257_02_15.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图2.15 – 因果语言模型和双向语言模型之间的差异](img/B21257_02_15.jpg)'
- en: Figure 2.15 – Difference between a causal and bidirectional language model
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – 因果语言模型和双向语言模型之间的差异
- en: '**Bidirectional encoders** resolve this limitation by allowing the model to
    find relationships over the entire sequence. Obviously, we can no longer use a
    language model to train it (it will be too easy to identify the next word in the
    sequence when you already know the answer) but we have to find a way to be able
    to train a bidirectional model. For clarification, the model reads the entire
    sequence at once and, in this case, consists of the encoder only.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**双向编码器**通过允许模型在整个序列中找到关系来解决这个问题。显然，我们不能再使用语言模型来训练它（当你已经知道答案时，识别序列中的下一个词将变得非常容易），但我们必须找到一种方法来训练双向模型。为了澄清，模型一次性读取整个序列，在这种情况下，只包含编码器。'
- en: 'To try to minimize changes to the structure we use what is called the `<MASK>`).
    In the original training, they masked 15 % of the tokens randomly. Notice that,
    in this case, we do not mask the future because we want the model to be aware
    of the whole context. Also, to better separate the different sentences, we have
    a special token, `[CLS]`, that signals the beginning of an input, and `[SEP]`
    to separate sentences in the input (for example, if we have a question and an
    answer). Otherwise, the structure is the same: we have an embedder, a position
    encoder, different transformer blocks, a linear projection, and a softmax. The
    loss is calculated in the same way; instead of using the next token, we use the
    masked token. The original article introduced two versions of BERT: BERT-BASE
    (12 layers, hidden size with d=768, 12 attention heads, and 110M total parameters)
    and BERT-LARGE (24 layers, hidden size with d=1024, 24 attention heads, and 340M
    total parameters).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽量减少对结构的改变，我们使用所谓的`<MASK>`。在原始训练中，他们随机遮蔽了15%的标记。请注意，在这种情况下，我们没有遮蔽未来，因为我们希望模型意识到整个上下文。此外，为了更好地分隔不同的句子，我们有一个特殊的标记`[CLS]`，它表示输入的开始，以及`[SEP]`来分隔输入中的句子（例如，如果我们有一个问题和答案）。否则，结构是相同的：我们有一个嵌入器、位置编码器、不同的transformer块、线性投影和softmax。损失的计算方式相同；我们不是使用下一个标记，而是使用遮蔽标记。原始文章介绍了BERT的两个版本：BERT-BASE（12层，隐藏大小为d=768，12个注意力头，总参数为110M）和BERT-LARGE（24层，隐藏大小为d=1024，24个注意力头，总参数为340M）。
- en: MLM is a flexible approach because the idea is to corrupt the input and ask
    the model to rebuild. We can mask, but we can also reorder or conduct other transformations.
    The disadvantage of this method is that only 15 percent of the tokens are actually
    used to learn, so the model is highly inefficient.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: MLM是一种灵活的方法，因为其理念是破坏输入并要求模型重建。我们可以进行遮蔽，也可以重新排序或进行其他转换。这种方法的缺点是只有15%的标记实际上被用于学习，因此模型效率非常高。
- en: The training is also highly flexible. For example, the model can be extended
    to `[SEP]` token between sentences). The last layer is a softmax for sentence
    classification; we consider the loss over the categories. This shows how the system
    is flexible and can be adapted to different tasks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 训练也非常灵活。例如，模型可以扩展到句子之间的`[SEP]`标记。最后一层是用于句子分类的softmax；我们考虑类别的损失。这显示了系统的灵活性，可以适应不同的任务。
- en: One final clarification. Until 2024, it was always assumed that these models
    were not capable of generating text. In 2024, two studies showed that by adapting
    the model, you can generate text even with a BERT-like model. For example, in
    this study, they show that one can generate text by exploiting a sequence of [MASK]
    tokens.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后再澄清一点。直到2024年，人们一直认为这些模型无法生成文本。在2024年，两项研究表明，通过调整模型，即使使用BERT类似的模型也可以生成文本。例如，在这项研究中，他们展示了通过利用一系列标记可以生成文本。
- en: '![Figure 2.16 – Text generation with MLM (https://arxiv.org/pdf/2406.04823)](img/B21257_02_16.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16 – 使用MLM进行文本生成](https://arxiv.org/pdf/2406.04823))(img/B21257_02_16.jpg)'
- en: Figure 2.16 – Text generation with MLM ([https://arxiv.org/pdf/2406.04823](https://arxiv.org/pdf/2406.04823))
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 使用MLM进行文本生成([https://arxiv.org/pdf/2406.04823](https://arxiv.org/pdf/2406.04823))
- en: Now that we have seen the two main types of training for a transformer, we can
    better explore what happens inside these models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了transformer的两种主要训练类型，我们可以更好地探索这些模型内部发生的事情。
- en: Visualizing internal mechanisms
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化内部机制
- en: We have seen the inner workings of the transformer, how it can be trained, and
    the main types of models. The beauty of attention is that we can visualize these
    relationships, and in this section, we will see how to do that. We can then visualize
    the relationships within the BERT attention head. As mentioned, in each layer,
    there are several attention heads and each of them learns a different representation
    of the input data. The color intensity indicates a greater weight in the attention
    weights (darker colors indicate weights that are close to 1).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了transformer的内部工作原理，以及它是如何被训练的，以及主要模型类型。注意力的美妙之处在于我们可以可视化这些关系，在本节中，我们将看到如何做到这一点。然后，我们可以可视化BERT注意力头内的关系。如前所述，在每一层中，都有几个注意力头，每个头都学习输入数据的不同表示。颜色强度表示注意力权重中的更大权重（较深的颜色表示接近1的权重）。
- en: 'We can do this using the BERTviz package:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用BERTviz包来完成此操作：
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The visualization is interactive. The code is in the repository. Try running
    it using different phrases and exploring different relationships between different
    words in the phrases. The visualization allows you to explore the different layers
    in the model by taking advantage of the drop-down model. Hovering over the various
    words allows you to see the individual weights of the various heads.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该可视化是交互式的。代码在仓库中。尝试使用不同的短语运行它，并探索短语中不同单词之间的不同关系。可视化允许您通过利用下拉模型来探索模型中的不同层。将鼠标悬停在各个单词上，您可以看到各个头的单个权重。
- en: 'This is the corresponding visualization:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相应的可视化：
- en: '![Figure 2.17 – Visualization of attention between all words in the input](img/B21257_02_17.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17 – 输入中所有单词之间的注意力可视化](img/B21257_02_17.jpg)'
- en: Figure 2.17 – Visualization of attention between all words in the input
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – 输入中所有单词之间的注意力可视化
- en: 'We can also view the various heads of the model at the same time. This allows
    us to see how the various heads model different relationships. This model has
    12 heads for 12 layers, so the model has 144 attention heads and can therefore
    see more than 100 representations for the same sentences (this explains the capacity
    of a model). Moreover, these representations are not completely independent; information
    learned from earlier layers can be used by later layers:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以同时查看模型的各个头。这使我们能够看到各个头如何建模不同的关系。该模型有12个头，对应12层，因此模型有144个注意力头，因此可以查看超过100个相同句子的表示（这解释了模型的能力）。此外，这些表示并不完全独立；早期层学习到的信息可以被后续层使用：
- en: '[PRE1]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important note
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The visualization is interactive. The code is in the repository. Try running
    it using different phrases and exploring different relationships. Here, we have
    the ensemble representation of the various attention heads. Observe how each head
    has a different function and how it models a different representation of the same
    inputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该可视化是交互式的。代码在仓库中。尝试使用不同的短语运行它，并探索不同短语中不同单词之间的关系。在这里，我们有各种注意力头的集成表示。观察每个头如何具有不同的功能以及它如何对相同输入的不同表示进行建模。
- en: 'This is the corresponding visualization:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相应的可视化：
- en: '![Figure 2.18 – Model view of the first two layers](img/B21257_02_18.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图2.18 – 第一层和第二层的模型视图](img/B21257_02_18.jpg)'
- en: Figure 2.18 – Model view of the first two layers
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 – 第一层和第二层的模型视图
- en: Another model that has been fundamental to the current development of today’s
    models is **Generative Pre-Trained Transformer 2** (**GPT-2**). GPT-2 is a causal
    (unidirectional) transformer pre-trained using language modeling on a very large
    corpus of ~40 GB of text data. GPT-2 was specifically trailed to predict the next
    token and to generate text with an input (it generates a token at a time; this
    token is then added to the input sequence to generate the next in an autoregressive
    process). In addition, this is perhaps the first model that has been trained with
    a massive amount of text. In addition, this model consists only of the decoder.
    GPT-2 is a family of models ranging from 12 layers of GPT-2 small to 48 layers
    of GPT-2 XL. Each layer consists of masked self-attention and a feed-forward neural
    network.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个对当今模型发展至关重要的模型是**生成预训练Transformer 2**（**GPT-2**）。GPT-2是在大约40 GB的文本数据语料库上使用语言模型预训练的因果（单向）Transformer。GPT-2专门用于预测下一个标记并生成带有输入的文本（它一次生成一个标记；然后将这个标记添加到输入序列中，以自回归过程生成下一个标记）。此外，这可能是第一个使用大量文本进行训练的模型。此外，这个模型仅由解码器组成。GPT-2是一个模型系列，从12层的GPT-2小型到48层的GPT-2
    XL。每一层由掩码自注意力和前馈神经网络组成。
- en: GPT-2 is generative and trained as a language model so we can give it an input
    judgment and observe the probability for the next token. For example, using “To
    be or not to” as input, the token with the highest probability is “be.”
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2是生成式模型，作为语言模型进行训练，因此我们可以给它一个输入判断并观察下一个标记的概率。例如，使用“To be or not to”作为输入，概率最高的标记是“be”。
- en: '![Figure 2.19 – Probabilities associated with the next token for the GPT-2
    model when probed with the “To be or not to” input sequence](img/B21257_02_19.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图2.19 – 当用“To be or not to”输入序列进行查询时，GPT-2模型与下一个标记相关的概率](img/B21257_02_19.jpg)'
- en: Figure 2.19 – Probabilities associated with the next token for the GPT-2 model
    when probed with the “To be or not to” input sequence
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 – 当用“To be or not to”输入序列进行查询时，GPT-2模型与下一个标记相关的概率
- en: 'Sometimes, it may be necessary to understand which tokens are most important
    to the model to generate the next token. **Gradient X input** is a technique originally
    developed for convolutional networks; at a given time step, we take the output
    probabilities for each token, select the tokens with the highest probability,
    and compute the gradient with respect to the input up to the input tokens. This
    gives us the importance of each token to generate the next token in the sequence
    (the rationale is that small changes in the input tokens with the highest importance
    carry the largest changes in the output). In the figure, we can see the most important
    tokens for the next token in the sequence:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，可能需要了解哪些标记对模型生成下一个标记最为重要。**梯度X输入**是一种最初为卷积网络开发的技巧；在给定的时间步长，我们取每个标记的输出概率，选择概率最高的标记，并计算相对于输入标记的梯度。这给出了每个标记对生成序列中下一个标记的重要性（理由是，输入标记中重要性最高的微小变化会导致输出中最大的变化）。在图中，我们可以看到序列中下一个标记的最重要标记：
- en: '![Figure 2.20 – Gradient X input for the next token in the sequence](img/B21257_02_20.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图2.20 – 序列中下一个标记的梯度X输入](img/B21257_02_20.jpg)'
- en: Figure 2.20 – Gradient X input for the next token in the sequence
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 – 序列中下一个标记的梯度X输入
- en: As mentioned before, there is not only self-attention but also feedforward neural
    network, which plays an important role (it provides a significant portion of the
    parameters in the transformer block, about 66%). Therefore, several works have
    focused on examining the firings of neurons in layers (this technique was also
    originally developed for computer vision).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，除了自注意力机制外，还有前馈神经网络，它在Transformer块中扮演着重要的角色（它提供了参数的很大一部分，大约66%）。因此，一些研究工作集中在检查层中神经元的激活（这项技术最初也是为计算机视觉开发的）。
- en: 'We can follow this activation after each layer, and for each of the tokens,
    we can monitor what their rank (by probability) is after each layer. As we can
    see, the model understands from the first layers which token is the most likely
    to continue a sequence:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以跟踪每一层的激活，并且对于每个标记，我们可以监控它们在每一层后的概率排名。正如我们所见，模型从第一层开始就能理解哪个标记最有可能继续序列：
- en: '![Figure 2.21 – Heatmap of the rank for the top five most likely tokens after
    each layer](img/B21257_02_21.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图2.21 – 每层后五个最有可能的标记的排名热图](img/B21257_02_21.jpg)'
- en: Figure 2.21 – Heatmap of the rank for the top five most likely tokens after
    each layer
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 – 每层之后最有可能的五个标记的排名热图
- en: 'Since there are a considerable number of neurons, it is complex to be able
    to observe them directly. Therefore, one way to investigate these activations
    is to first reduce dimensionality. To avoid negative activations, it is preferred
    to use **Non-Negative Matrix Factorization** (**NMF**) instead of **Principal
    Component Analysis** (**PCA**). The process first captures the activation of neurons
    in the FFNN layers of the model and is then decomposed into some factors (user-chosen
    parameters). Next, we can interactively observe the factors with the highest activation
    when a token has been generated. What we see in the graph is the factor excitation
    for each of the generated tokens:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经元数量相当多，直接观察它们是复杂的。因此，研究这些激活的一种方法是在首先降低维度。为了避免负激活，更倾向于使用**非负矩阵分解**（**NMF**）而不是**主成分分析**（**PCA**）。这个过程首先捕捉模型FFNN层中神经元的激活，然后分解为一些因素（用户选择的参数）。接下来，当生成一个标记时，我们可以交互式地观察具有最高激活的因素。我们在图中看到的是每个生成标记的因素激发：
- en: '![Figure 2.22 – NMF for the activations of the model in generating a sequence](img/B21257_02_22.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图2.22 – 生成序列时模型的激活的NMF](img/B21257_02_22.jpg)'
- en: Figure 2.22 – NMF for the activations of the model in generating a sequence
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 – 生成序列时模型的激活的NMF
- en: We can also conduct this analysis for a single layer. This allows us to analyze
    interesting behaviors within the neurons of a layer (in the image layer 0 of the
    model). In this case, there are certain factors that focus on specific portions
    of the text (beginning, middle, and end). As we mentioned earlier, the model keeps
    track of word order in a sequence due to positional encoding, and this is reflected
    in activation. Other neurons, however, are activated by grammatical structures
    (such as conjunctions, articles, and so on). This indicates to us a specialization
    of what individual neurons in a pattern track and is one of the strength components
    of the transformer. By increasing the number of facts, we can increase the resolution
    and better understand what grammatical and semantic structures the pattern encodes
    in its activations. Moving forward in the structure of the model, we can see that
    layers learn a different representation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以对单层进行这种分析。这使我们能够分析层中神经元（在模型的图像层0）的有趣行为。在这种情况下，有一些因素专注于文本的特定部分（开头、中间和结尾）。正如我们之前提到的，由于位置编码，模型跟踪序列中的词序，这在激活中得到了体现。然而，其他神经元是由语法结构（如连词、冠词等）激活的。这表明我们注意到模式中单个神经元跟踪的专门化，这是transformer的强大组件之一。通过增加事实的数量，我们可以提高分辨率，更好地理解模式在其激活中编码的语法和语义结构。在模型结构的进一步分析中，我们可以看到层学习不同的表示。
- en: '![Figure 2.23 – NMF for the activations of the model in generating a sequence](img/B21257_02_23.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图2.23 – 生成序列时模型的激活的NMF](img/B21257_02_23.jpg)'
- en: Figure 2.23 – NMF for the activations of the model in generating a sequence
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 – 生成序列时模型的激活的NMF
- en: We have seen how to build a transformer and how it works. Now that we know the
    anatomy of a transformer, it is time to see it at work.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何构建transformer以及它是如何工作的。现在我们知道了transformer的解剖结构，是时候看到它的工作状态了。
- en: Applying a transformer
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用transformer
- en: The power of a transformer lies in its ability to be able to learn from an enormous
    amount of text. During this phase of training (called **pre-training**), the model
    learns general rules about the structure of a language. This general representation
    can then be exploited for a myriad of applications. One of the most important
    concepts in deep learning is **transfer learning**, in which we exploit the ability
    of a model trained on a large amount of data for a task different from the one
    it was originally trained for. A special case of transfer learning is **fine-tuning**.
    Fine-tuning allows us to adapt the general knowledge of a model to a particular
    case. One way to do this is to add a set of parameters to a model (at the top
    of it) and then train these parameters by gradient descent for a specific task.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的强大之处在于其能够从大量文本中学习的能力。在这个训练阶段（称为**预训练**），模型学习关于语言结构的通用规则。这种通用表示可以用于众多应用。在深度学习中，最重要的概念之一是**迁移学习**，其中我们利用在大量数据上训练的模型的能力来执行与原始训练任务不同的任务。迁移学习的一个特殊情况是**微调**。微调使我们能够将模型的通用知识适应到特定情况。实现这一目标的一种方法是为模型添加一组参数（在顶部），然后通过梯度下降对这些参数进行特定任务的训练。
- en: The transformer has been trained with large amounts of text and has learned
    semantic rules that are useful in understanding a text. We want to exploit this
    knowledge for a specific application such as sentiment classification. Instead
    of training a model from scratch, we can adapt a pre-trained transformer to classify
    our sentences. In this case, we do not want to destroy the internal representation
    of the model but preserve it. That is why, during fine-tuning, most of the layers
    are frozen (there is no update on the weights). Instead, we just train those one
    or two layers that we add to the top of the model. The idea is to preserve the
    representation and then learn how to use it for our specific task. Those two added
    layers learn precisely how to use the internal representation of the model. To
    give a simple example, let’s imagine we want to learn how to write scientific
    papers. To do that, we don’t have to learn how to write in English again, just
    to adapt our knowledge to this new task.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器已经用大量文本进行训练，并学会了在理解文本中有用的语义规则。我们希望利用这些知识来处理特定应用，如情感分类。而不是从头开始训练模型，我们可以将预训练的变压器适应于对句子进行分类。在这种情况下，我们不想破坏模型的内部表示，而是要保留它。这就是为什么在微调过程中，大多数层都是冻结的（权重没有更新）。相反，我们只训练那些添加到模型顶部的层。想法是保留表示，然后学习如何为我们特定的任务使用它。这两个添加的层精确地学习了如何使用模型的内部表示。为了简单起见，让我们想象我们想要学习如何撰写科学论文。为了做到这一点，我们不需要再次学习如何用英语写作，只需要将我们的知识适应到这个新任务。
- en: 'In BERT, as we mentioned, we add a particular token to the beginning of each
    sequence: a `[CLS]` token. During training or even inference in a bidirectional
    transformer, this token waits for all others in the sequence (if you remember,
    all tokens are connected). This means that the final vector (the one in the last
    layer) is contextualized for each element in the sequence. We can then exploit
    this vector for a classification task. If we have three classes (for example,
    positive, neutral, and negative) we can take the vector for a sequence and use
    softmax to classify.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT中，正如我们提到的，我们在每个序列的开头添加一个特定的标记：一个`[CLS]`标记。在双向变压器的训练或推理过程中，这个标记等待序列中的所有其他标记（如果你还记得，所有标记都是相互连接的）。这意味着最终向量（最后一层的那个）为序列中的每个元素提供了上下文。然后我们可以利用这个向量来进行分类任务。如果我们有三个类别（例如，正面、中立和负面），我们可以取序列的向量并使用softmax进行分类。
- en: <mml:math display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>
- en: The model was not originally trained for sequence classification, so we'd like
    to introduce a learnable matrix to enable class separation. This matrix represents
    a linear transformation and can alternatively be implemented using one or more
    linear layers. We then apply a cross-entropy loss function to optimize these weights.
    This setup follows the standard supervised learning paradigm, where labeled data
    is used to adapt the transformer to a specific task.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型最初并未针对序列分类进行训练，因此我们希望引入一个可学习的矩阵以实现类别分离。这个矩阵代表一个线性变换，也可以通过一个或多个线性层来实现。然后我们应用交叉熵损失函数来优化这些权重。这种设置遵循标准的监督学习范式，其中使用标记数据来适应特定任务。
- en: In this process, we have so far assumed that the remainder of the transformer's
    weights remain frozen. However, as observed in convolutional neural networks,
    even minimal fine-tuning of model parameters can enhance performance. Such updates
    are typically carried out with a very low learning rate.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们迄今为止假设transformer的其余权重保持冻结。然而，正如在卷积神经网络中观察到的，即使是模型参数的最小微调也能提高性能。此类更新通常使用非常低的学习率进行。
- en: We can adapt a pre-trained transformer for new tasks through supervised fine-tuning.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过监督微调来将预训练的transformer适应新任务。
- en: '![Figure 2.24 – Fine-tuning a transformer](img/B21257_02_24.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24 – 微调transformer](img/B21257_02_24.jpg)'
- en: Figure 2.24 – Fine-tuning a transformer
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24 – 微调transformer
- en: In the first step, we are removing the final layer (this is specific to the
    original task). In the second step, we add a random initialized layer and gather
    training examples for the new task. During the fine-tuning, we are presenting
    the model with new examples (in this case, positive and negative reviews). While
    keeping the model frozen (each example is processed by the whole model in the
    forward pass), we update the weight only in the new layer (through backpropagation).
    The model has now learned the new task.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们正在移除最后一层（这针对原始任务而言是特定的）。在第二步中，我们添加一个随机初始化的层，并为新任务收集训练示例。在微调过程中，我们向模型展示新的示例（在这种情况下，是正面和负面的评论）。在保持模型冻结（每个示例在正向传播中由整个模型处理）的同时，我们只更新新层的权重（通过反向传播）。现在，模型已经学会了新任务。
- en: 'Conducting finetuning with Hugging Face is a straightforward process. We can
    use a model such as distill-BERT (a distilled version of BERT) with a few lines
    of code and the dataset we used in the previous chapter. We need to prepare the
    dataset and tokenize it (so that it can be used with a transformer). Hugging Face
    then allows with a simple wrapper that we can train the model. The arguments for
    training are stored in `TrainingArguments`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hugging Face进行微调是一个简单直接的过程。我们可以使用像distill-BERT（BERT的蒸馏版本）这样的模型，用几行代码和前一章中使用的数据集。我们需要准备数据集并将其标记化（以便可以使用transformer）。然后，Hugging
    Face通过一个简单的包装器允许我们训练模型。训练的参数存储在`TrainingArguments`中：
- en: '[PRE2]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Important note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Notice that the process is very similar to training a neural network. In fact,
    the transformer is a deep learning model; for the training, we are using similar
    hyperparameters.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这个过程与训练神经网络非常相似。事实上，transformer是一个深度学习模型；在训练时，我们使用类似的超参数。
- en: In this case, we used only a small fraction of the reviews. The beauty of fine-tuning
    is that we need only a few examples to have a similar (if not better) performance
    than a model trained from scratch.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们只使用了评论的一小部分。微调的美丽之处在于，我们只需要几个示例就能达到与从头开始训练的模型相似（如果不是更好的）的性能。
- en: '![Figure 2.25 – Confusion matrix after fine-tuning](img/B21257_02_25.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图2.25 – 微调后的混淆矩阵](img/B21257_02_25.jpg)'
- en: Figure 2.25 – Confusion matrix after fine-tuning
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 – 微调后的混淆矩阵
- en: BERT’s training was done on 64 TPUs (special hardware for tensor operations)
    for four days; this is beyond the reach of most users. In contrast, fine-tuning
    is possible either on a single GPU or on a CPU. As a result, BERT achieved state-of-the-art
    performance upon its release across a wide array of tasks, including paraphrase
    detection, question answering, and sentiment analysis. Hence, several variants
    such as **RoBERTa** and **SpanBERT** (in this case, we mask an entire span instead
    of a single token with better results) or adapted to specific domains such as
    **SciBERT** were born. However, encoders are not optimal for generative tasks
    (because of mask training) while decoders are.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的训练是在64个TPU（用于张量操作的专用硬件）上进行的，耗时四天；这对大多数用户来说都遥不可及。相比之下，微调可以在单个GPU或CPU上进行。因此，BERT在发布时在包括释义检测、问答和情感分析在内的广泛任务上实现了最先进的性能。因此，诞生了几个变体，如**RoBERTa**和**SpanBERT**（在这种情况下，我们用一个整个跨度而不是单个标记进行掩码，以获得更好的结果），或者针对特定领域进行了调整，如**SciBERT**。然而，编码器对于生成任务来说并不理想（因为掩码训练），而解码器则更合适。
- en: 'To conduct machine translation, the original transformer consisted of an encoder
    and a decoder. A model such as GPT-2 only has a decoder. We can conduct fine-tuning
    in the same way as seen before, we just need to construct the dataset in an optimal
    way for a model that is constituted by the decoder alone. For example, we can
    take a dataset in which we have English and French sentences and build a dataset
    for finetuning as follows: `<sentence in English>` followed by a special `<to-fr>`
    token and then the `<sentence in French>`. The same approach can be used to teach
    summarization to a model, where we insert a special token meaning summarization.
    The model is fine-tuned by conducting the next token prediction (language modeling).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行机器翻译，原始的Transformer模型由编码器和解码器组成。像GPT-2这样的模型只有解码器。我们可以像之前看到的那样进行微调，我们只需要以最佳方式构建一个仅由解码器构成的模型的训练数据集。例如，我们可以取一个包含英语和法语句子的数据集，并构建以下微调数据集：`<英语句子>`后跟一个特殊的`<to-fr>`标记，然后是`<法语句子>`。同样的方法也可以用来教模型进行摘要，其中我们插入一个表示摘要的特殊标记。模型通过进行下一个标记预测（语言建模）进行微调。
- en: '![Figure 2.26 – Fine-tuning of a decoder-only model](img/B21257_02_26.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.26 – 仅解码器模型的微调](img/B21257_02_26.jpg)'
- en: Figure 2.26 – Fine-tuning of a decoder-only model
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26 – 仅解码器模型的微调
- en: Another way to exploit the learned knowledge of a model is to use **knowledge
    distillation**. In the previous section, we used distillGPT-2 which is a distilled
    version of GPT-2\. A distilled model captures knowledge from a much larger model
    without losing significant performance but is much more manageable. Models that
    are trained with a large amount of text learn a huge body of knowledge. All this
    knowledge and skill is often redundant when we need a model for some specific
    task. We are interested in having a model that is very capable for a task, but
    without wanting to deal with a model of billions of parameters. In addition, sometimes
    we do not have enough examples for a model to learn the task from scratch. In
    this case, we can extract knowledge from the larger model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 利用模型学习到的知识的另一种方法是使用**知识蒸馏**。在上一节中，我们使用了distillGPT-2，它是GPT-2的蒸馏版本。蒸馏模型能够从更大的模型中捕获知识，而不会损失显著的性能，但更容易管理。使用大量文本训练的模型会学习到大量的知识。当我们需要针对某些特定任务使用模型时，所有这些知识和技能往往是多余的。我们希望有一个在特定任务上非常强大的模型，但又不想处理数十亿参数的模型。此外，有时我们没有足够的示例让模型从头开始学习任务。在这种情况下，我们可以从更大的模型中提取知识。
- en: '![Figure 2.27 – Generic teacher-student framework for knowledge distillation
    (https://arxiv.org/pdf/2006.05525)](img/B21257_02_27.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.27 – 知识蒸馏的通用师生框架](img/B21257_02_27.jpg)'
- en: Figure 2.27 – Generic teacher-student framework for knowledge distillation ([https://arxiv.org/pdf/2006.05525](https://arxiv.org/pdf/2006.05525))
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.27 – 知识蒸馏的通用师生框架 ([https://arxiv.org/pdf/2006.05525](https://arxiv.org/pdf/2006.05525))
- en: 'Knowledge distillation can be seen as a form of compression, in which we try
    to transfer knowledge from a trained “teacher” model with many parameters to a
    “student” model with fewer parameters. The student model tries to mimic the teacher
    model and achieve the same performances as the teacher model in a task. In such
    a framework, we have three components: the models, knowledge, and algorithm. The
    algorithm can exploit either the teacher’s logits or intermediate activations.
    In the case of the logits, the student tries to mimic the predictions of the teacher
    model, so we try to minimize the difference between the logits produced by the
    teacher and the student. To do this, we use a distillation loss that allows us
    to train the student model.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏可以看作是一种压缩形式，其中我们试图将具有许多参数的已训练“教师”模型中的知识转移到具有较少参数的“学生”模型中。学生模型试图模仿教师模型，并在任务中达到与教师模型相同的性能。在这种框架中，我们有三个组件：模型、知识和算法。算法可以利用教师的logits或中间激活。在logits的情况下，学生试图模仿教师模型的预测，因此我们试图最小化教师和学生产生的logits之间的差异。为此，我们使用一种蒸馏损失，使我们能够训练学生模型。
- en: '![Figure 2.28 – Teacher-student framework for knowledge distillation training
    (https://arxiv.org/pdf/2006.05525)](img/B21257_02_28.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.28 – 知识蒸馏训练的教师-学生框架](https://arxiv.org/pdf/2006.05525)(img/B21257_02_28.jpg)'
- en: Figure 2.28 – Teacher-student framework for knowledge distillation training
    ([https://arxiv.org/pdf/2006.05525](https://arxiv.org/pdf/2006.05525))
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.28 – 知识蒸馏训练的教师-学生框架([https://arxiv.org/pdf/2006.05525](https://arxiv.org/pdf/2006.05525))
- en: For knowledge distillation, the steps are also similar. The first step is data
    preprocessing. For each model, you must remember to choose the model-specific
    tokenizer (although the one from GPT-2 is the most widely used many models have
    different tokenizers). We must then conduct fine-tuning of a model on our task
    (there is no model that is specific to classify reviews). This model will be our
    teacher. The next step is to train a student model. We can also use a pre-trained
    model that is smaller than the teacher (this allows us to be able to use a few
    examples to train it).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于知识蒸馏，步骤也类似。第一步是数据预处理。对于每个模型，你必须记得选择模型特定的分词器（尽管GPT-2的分词器是最广泛使用的，但许多模型有不同的分词器）。然后我们必须在我们的任务上对模型进行微调（没有专门用于分类评论的模型）。这个模型将成为我们的教师。下一步是训练一个学生模型。我们也可以使用一个比教师更小的预训练模型（这使我们能够使用一些示例来训练它）。
- en: 'One important difference is that we now have a specific loss for knowledge
    distillation. This distillation loss calculates the loss between the teacher’s
    logits and the student’s logits. This function typically uses the Kullback-Leibler
    divergence loss to calculate the difference between the two probability distributions
    (Kullback-Leibler divergence is really a measure of the difference between two
    probability distributions). We can define it as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的区别是，我们现在有一个专门用于知识蒸馏的损失。这种蒸馏损失计算教师logits和学生logits之间的损失。这个函数通常使用Kullback-Leibler散度损失来计算两个概率分布之间的差异（Kullback-Leibler散度实际上是两个概率分布之间差异的度量）。我们可以将其定义为以下：
- en: '[PRE3]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At this point, we just have to have a way to train our system. In this case,
    the teacher will be used only in inference while the student model will be trained.
    We will use the teacher’s logits to calculate the loss:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，我们只需要有一种方法来训练我们的系统。在这种情况下，教师将仅在推理中使用，而学生模型将被训练。我们将使用教师的logits来计算损失：
- en: '[PRE4]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As can be seen in the following figure, the performance of the student model
    is similar to the teacher model:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，学生模型的性能与教师模型相似：
- en: '![Figure 2.29 – Confusion matrix for the teacher and student model](img/B21257_02_29.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.29 – 教师和学生模型的混淆矩阵](img/B21257_02_29.jpg)'
- en: Figure 2.29 – Confusion matrix for the teacher and student model
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.29 – 教师和学生模型的混淆矩阵
- en: Fine-tuning and knowledge distillation allow us to be able to use a transformer
    for any supervised task. Fine-tuning allows us to work with datasets that are
    small (and where there are often too few examples to train a model from scratch).
    Knowledge distillation, on the other hand, allows us to get a smaller model (but
    performs as well as a much larger one) when the computational cost is the limit.
    By taking advantage of these techniques, we can tackle any task.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和知识蒸馏使我们能够将转换器用于任何监督任务。微调允许我们处理小数据集（在这些数据集中，通常例子太少以至于无法从头开始训练模型）。另一方面，知识蒸馏允许我们在计算成本是限制的情况下，获得一个更小的模型（但性能与一个更大的模型相当）。通过利用这些技术，我们可以处理任何任务。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the transformer, the model that revolutionized
    NLP and artificial intelligence. Today, all models that have commercial applications
    are derivatives of the transformer, as we learned in this chapter. Understanding
    how it works on a mechanistic level, and how the various parts (self-attention,
    embedding, tokenization, and so on) work together, allows us to understand the
    limitations of modern models. We saw how it works internally in a visual way,
    thus exploring the motive of modern artificial intelligence from multiple perspectives.
    Finally, we saw how we can adapt a transformer to our needs using techniques that
    leverage prior knowledge of the model. Now we can repurpose this process with
    virtually any dataset and any task.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了转换器，这个革命了自然语言处理和人工智能的模型。今天，所有具有商业应用的模型都是转换器的衍生品，正如我们在本章中所学。在机制层面上理解它是如何工作的，以及它的各个部分（自注意力、嵌入、分词等）是如何协同工作的，使我们能够理解现代模型的局限性。我们以直观的方式看到了它是如何内部工作的，从而从多个角度探索了现代人工智能的动机。最后，我们看到了如何使用利用模型先前知识的技巧来调整转换器以满足我们的需求。现在我们可以用几乎任何数据集和任何任务来重新利用这个过程。
- en: Learning how to train a transformer will allow us to understand what happens
    when we take this process to scale. An LLM is a transformer with more parameters
    and that has been trained with more text. This leads to emergent properties that
    have made it so successful, but both its merits and shortcomings lie in the elements
    we have seen.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何训练一个转换器将使我们理解当我们将这个过程扩展到规模时会发生什么。一个大型语言模型（LLM）是一个具有更多参数并且使用更多文本进行训练的转换器。这导致了使其如此成功的涌现特性，但它的优点和缺点都体现在我们所看到的元素中。
- en: In [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042), we will see precisely how
    to obtain an LLM from a transformer. What we have learned in this chapter will
    allow us to see how this step comes naturally.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第三章*](B21257_03.xhtml#_idTextAnchor042)中，我们将精确地了解如何从一个转换器中获得一个LLM。在本章中我们所学的知识将使我们看到这一步骤是如何自然而然地发生的。
