- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Regularization with Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据正则化
- en: Even though there are plenty of regularization methods for models (with each
    model having a unique set of hyperparameters), sometimes, the most effective regularization
    comes from the data itself. Indeed, sometimes, even the most powerful model can’t
    have good performance if the data is not transformed properly beforehand.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多正则化方法可供模型使用（每个模型都有一套独特的超参数），但有时最有效的正则化方法来自于数据本身。事实上，有时即使是最强大的模型，如果数据没有事先正确转换，也无法达到良好的性能。
- en: 'In this chapter, we’ll look at some methods that help regularize models from
    data:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些有助于通过数据正则化模型的方法：
- en: Hashing high cardinality features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希高基数特征
- en: Aggregating features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合特征
- en: Undersampling an imbalanced dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不平衡数据集进行欠采样
- en: Oversampling an imbalanced dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不平衡数据集进行过采样
- en: Resampling imbalanced data with SMOTE
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SMOTE对不平衡数据进行重采样
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will apply several tricks to data, as well as resample
    datasets or download new data via the command line. To do so, you will need the
    following libraries:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将对数据应用几个技巧，并通过命令行重采样数据集或下载新数据。为此，您需要以下库：
- en: NumPy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: pandas
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: scikit-learn
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: imbalanced-learn
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: imbalanced-learn
- en: category_encoders
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: category_encoders
- en: Kaggle API
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle API
- en: Hashing high cardinality features
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈希高基数特征
- en: 'High cardinality features are qualitative features with many possible values.
    High cardinality features may appear in many applications, such as a country in
    a customer database, a phone model in advertising, or vocabulary in NLP applications.
    High cardinality issues can be manifold: not only may they lead to a very highly
    dimensional dataset, but they can also evolve as more and more values become available.
    Indeed, even if the data for the number of countries or vocabulary is arguably
    quite stable, there are new phone models every week, if not every day.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 高基数特征是具有许多可能值的定性特征。高基数特征在许多应用中都会出现，例如客户数据库中的国家、广告中的手机型号，或NLP应用中的词汇。高基数问题可能是多方面的：不仅可能导致非常高维的数据集，而且随着越来越多的值的出现，它们还可能不断发展。事实上，即使国家数量或词汇的数据相对稳定，每周（如果不是每天）也会有新的手机型号出现。
- en: Hashing is a very popular and useful way to deal with such problems. In this
    recipe, we’ll see what it is and how to use it in practice on a dataset to predict
    whether employees will leave a company.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希是一种非常流行且有用的处理这类问题的方法。在本章中，我们将了解它是什么，以及如何在实践中使用它来预测员工是否会离开公司。
- en: Getting started
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: Hashing is a very useful trick in computer science in general, and it is widely
    used in cryptography or blockchain, for example. It is also useful in machine
    learning when dealing with high cardinality features. It does not necessarily
    help with regularization per se, but it can sometimes be a side effect.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希是计算机科学中非常有用的技巧，广泛应用于密码学或区块链等领域。例如，它在处理高基数特征时也在机器学习中非常有用。它本身不一定有助于正则化，但有时它可能是一个副作用。
- en: What is hashing?
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是哈希？
- en: 'Hashing is often used in machine learning at the production level for dealing
    with high cardinality features. High cardinality features tend to have a growing
    number of possible outcomes. This can include things such as a mobile phone model,
    a software version, an item ID, and so on. In such cases, using one-hot encoding
    on high cardinality features may lead to several problems:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希通常在生产环境中用于处理高基数特征。高基数特征往往具有越来越多的可能结果。这些结果可能包括诸如手机型号、软件版本、商品ID等。在这种情况下，使用独热编码（one-hot
    encoding）处理高基数特征可能会导致一些问题：
- en: The required space is not fixed and can’t be controlled
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需空间不是固定的，无法控制
- en: We need to figure out how to encode a new value
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要弄清楚如何编码一个新值
- en: Using hashing instead of one-hot encoding can address these limitations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哈希代替独热编码可以解决这些限制。
- en: 'To do so, we must use a hash function that converts an input into a controlled
    output. One well-known hash function is `md5`. If we apply `md5` to some strings,
    we’ll get the following results:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们必须使用一种哈希函数，它将输入转换为一个可控的输出。一种著名的哈希函数是`md5`。如果我们对一些字符串应用`md5`，我们将得到如下结果：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will look like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As we can see, hashing has several interesting properties:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，哈希具有几个有趣的特性：
- en: No matter the input size, the output size is fixed
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论输入大小如何，输出大小是固定的
- en: Two similar inputs may lead to very different outputs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个相似的输入可能导致非常不同的输出
- en: 'These properties allow hash functions to be very effective when used with high
    cardinality features. All we have to do is this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性使得哈希函数在与高基数特征一起使用时非常有效。我们需要做的就是这样：
- en: Choose a hash function.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个哈希函数。
- en: Define the expected space dimension of the output.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输出的预期空间维度。
- en: Encode our feature with that function.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该函数对我们的特征进行编码。
- en: 'Of course, there are some drawbacks to hashing:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，哈希也有一些缺点：
- en: There may be collisions – two different inputs may have the same output (even
    if this does not necessarily hurt performance if it’s not that severe)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会发生冲突——两个不同的输入可能会有相同的输出（即使这不一定会影响性能，除非冲突非常严重）
- en: We may want similar inputs to have similar outputs (a well-chosen hashing function
    can have such a property)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能希望相似的输入有相似的输出（一个精心选择的哈希函数可以具备这样的特性）
- en: Required installations
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所需的安装
- en: 'We need to do some preparation for this recipe. Since we will download a Kaggle
    dataset, first, we need to install the Kaggle API:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为这个示例做一些准备。由于我们将下载一个Kaggle数据集，首先，我们需要安装Kaggle API：
- en: 'Install the library with `pip`:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pip`安装该库：
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you haven’t already done so, create a Kaggle account at [www.kaggle.com](https://www.kaggle.com).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，创建一个Kaggle账户，访问[www.kaggle.com](https://www.kaggle.com)。
- en: 'Go to your profile page and create your API token by clicking `kaggle.json`
    file to your computer:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到你的个人资料页面并通过点击`kaggle.json`文件将其下载到计算机：
- en: '![Figure 5.1 – Screenshot of the Kaggle website](img/B19629_05_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – Kaggle网站截图](img/B19629_05_01.jpg)'
- en: Figure 5.1 – Screenshot of the Kaggle website
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – Kaggle网站截图
- en: 'You need to move the freshly downloaded `kaggle.json` file to `~/.kaggle` via
    the following command line:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要通过以下命令行将新下载的`kaggle.json`文件移动到`~/.kaggle`：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can now download the dataset with the following command line:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你可以使用以下命令行下载数据集：
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We should now have a file called `aug-train.zip`, which contains the data we
    will use for this recipe. We also need to install the `category_encoders`, `pandas`,
    and `sklearn` libraries with the following command line:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在应该有一个名为`aug-train.zip`的文件，其中包含我们将在这个示例中使用的数据。我们还需要通过以下命令行安装`category_encoders`、`pandas`和`sklearn`库：
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How to do it...
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In this recipe, we will load and quickly prepare the dataset (quickly in the
    sense that more data preparation could lead to better results), and then apply
    a logistic regression model to this classification task. On the selected dataset,
    the `city` feature has 123 possible outcomes, so it can be considered a high cardinality
    feature. Also, we can fairly assume that the production data could contain more
    cities, so the hashing trick would make sense here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将加载并快速准备数据集（快速意味着更多的数据准备可能会带来更好的结果），然后应用逻辑回归模型来处理这个分类任务。在所选数据集上，`city`特征有123个可能的结果，因此可以认为它是一个高基数特征。此外，我们可以合理地假设生产数据可能包含更多的城市，因此哈希技巧在这里是有意义的：
- en: 'Import the required modules, functions, and classes: `pandas` for loading the
    data, `train_test_split` for splitting the data, `StandardScaler` for rescaling
    quantitative features, `HashingEncoder` for encoding qualitative features, and
    `LogisticRegression` as the model:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块、函数和类：`pandas`用于加载数据，`train_test_split`用于划分数据，`StandardScaler`用于重新缩放定量特征，`HashingEncoder`用于编码定性特征，`LogisticRegression`作为模型：
- en: '[PRE6]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Load the dataset with `pd.read_csv()`. Note that we do not need to unzip the
    dataset first since the zip only contains one CSV file – `pandas` will take care
    of that for us:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pd.read_csv()`加载数据集。请注意，我们不需要先解压数据集，因为压缩包只包含一个CSV文件——`pandas`会为我们处理这一切：
- en: '[PRE12]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As we can see, the `city` feature has `123` possible values in the dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，数据集中的`city`特征有`123`个可能的值：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Remove any missing data. We take a very brutal policy here: we remove all the
    features that have a large amount of missing data, and then we remove all the
    rows with remaining missing data. This is not a recommended approach in general
    since we lose a lot of potentially useful information. Since dealing with missing
    data isn’t the subject here, we will take this simplistic approach:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除任何缺失的数据。我们在这里采取非常粗暴的策略：移除所有缺失数据较多的特征，然后移除所有包含剩余缺失数据的行。通常这不是推荐的方法，因为我们丢失了大量潜在有用的信息。由于处理缺失数据不在此讨论范围内，我们将采取这种简化的方法：
- en: '[PRE16]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Split the data into training and test sets with the `train_test_split` function:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数将数据拆分为训练集和测试集：
- en: '[PRE19]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Select and rescale any quantitative features. We will use the standard scaler
    to rescale the selected quantitative features, but any other scaler may work fine
    too:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并重新缩放任何定量特征。我们将使用标准化器来重新缩放所选的定量特征，但其他任何缩放器也可能适用：
- en: '[PRE24]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Select and prepare “regular” qualitative features. Here, we will use the one-hot
    encoder from `scikit-learn`, though we could also apply the hashing trick to those
    features:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并准备“常规”定性特征。在这里，我们将使用`scikit-learn`中的一热编码器，尽管我们也可以对这些特征应用哈希技巧：
- en: '[PRE33]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Encode the high cardinality `''city''` feature with hashing. Since there are
    currently `123` possible values for this feature, we could use only 7 bits to
    encode the whole space of possibilities. This is what is denoted by the `n_components=7`
    parameter. For safety, we could set it to 8 or more bits, to consider a growing
    set of possible cities in the data:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用哈希对高基数的`'city'`特征进行编码。由于当前该特征有`123`个可能的值，我们可以只使用7位来编码整个可能的值空间。这就是`n_components=7`参数所表示的内容。为了安全起见，我们可以将其设置为8位或更多，以考虑数据中可能出现的更多城市：
- en: '[PRE47]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output will look something like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果类似于以下内容：
- en: '[PRE57]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As we can see, all the values are encoded in seven columns, spanning 2^7 = 128
    possible values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有值都被编码成七列，涵盖了2^7 = 128个可能的值。
- en: 'Concatenate all the prepared data:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有准备好的数据连接起来：
- en: '[PRE58]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Instantiate and train the logistic regression model. Here, we will use the
    default hyperparameters proposed by `scikit-learn` for logistic regression:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化并训练逻辑回归模型。在这里，我们将使用`scikit-learn`为逻辑回归提供的默认超参数：
- en: '[PRE62]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Print the accuracy for both the training and test sets using the `.``score()`
    method:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.``score()`方法打印训练集和测试集的准确率：
- en: '[PRE64]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output will look something like this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果类似于以下内容：
- en: '[PRE67]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As we can see, we have an accuracy of about 78% on the test set, with no apparent
    overfitting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们在测试集上的准确率大约为78%，且没有明显的过拟合。
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is possible that adding some features (for example, with feature engineering)
    could help improve the model since the model in itself seems to have no room for
    much improvement based on the fact there is no overfitting.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可能添加一些特征（例如，通过特征工程）有助于改善模型，因为目前模型似乎没有过拟合，且在自身上似乎没有太多提升空间。
- en: See also
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The official documentation of the category encoders library: [https://contrib.scikit-learn.org/category_encoders/](https://contrib.scikit-learn.org/category_encoders/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别编码器库的官方文档：[https://contrib.scikit-learn.org/category_encoders/](https://contrib.scikit-learn.org/category_encoders/)
- en: 'The category encoders page about hashing: [https://contrib.scikit-learn.org/category_encoders/hashing.xhtml](https://contrib.scikit-learn.org/category_encoders/hashing.xhtml)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于哈希的类别编码器页面：[https://contrib.scikit-learn.org/category_encoders/hashing.xhtml](https://contrib.scikit-learn.org/category_encoders/hashing.xhtml)
- en: Aggregating features
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合特征
- en: When you’re looking at high cardinality features, one possible solution is to
    reduce the actual cardinality of that feature. Here, aggregating is one possible
    solution, and it may work very well in some cases. In this recipe, we will explain
    what aggregating is and discuss when we should use it. Once we’ve done that, we
    will apply it.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理高基数特征时，一种可能的解决方案是减少该特征的实际基数。这里，聚合是一个可能的解决方案，并且在某些情况下可能非常有效。在本节中，我们将解释聚合是什么，并讨论何时应该使用它。完成这些后，我们将应用它。
- en: Getting ready
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: When dealing with high cardinality features, one-hot encoding leads to high-dimensionality
    datasets. Because of the so-called curse of dimensionality, the ability for models
    to generalize properly can be a real issue for one-hot encoded high cardinality
    features, even with very large training datasets. Thus, aggregating is a way to
    lower the dimensionality of the one-hot encoding, and then lower the risk of overfitting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理高基数特征时，一热编码会导致高维数据集。由于所谓的维度灾难，即使有非常大的训练数据集，一热编码的高基数特征也可能会导致模型泛化能力不足。因此，聚合是一种降低一热编码维度的方法，从而降低过拟合的风险。
- en: 'There are several ways to aggregate. Let’s, for example, assume that we have
    a database of clients that contains the “phone model” feature, which consists
    of many of the possible phone models (that is, hundreds). There could be at least
    two ways of aggregating such a feature:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以进行聚合。例如，假设我们有一个包含“手机型号”特征的客户数据库，其中包含许多可能的手机型号（即数百种）。至少有两种方法可以对这种特征进行聚合：
- en: '**By occurrence probability**: Any model appearing less than X% in the data
    is considered as “others”'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按出现概率**：在数据中出现少于X%的任何模型被视为“其他”'
- en: '**By a given similarity**: We could gather models by generation, brand, or
    even price'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按给定相似度**：我们可以按生成、品牌甚至价格来聚合模型'
- en: 'These methods have their pros and cons:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法有其优缺点：
- en: '**Pros**: Aggregating by occurrence is simple, works all the time, and does
    not require any subject matter knowledge'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：按出现次数聚合简单、始终有效，并且不需要任何领域知识'
- en: '**Cons**: Aggregating by a given similarity can be more relevant but requires
    knowledge about the feature that may not be available, or it could take too long
    (for example, if there are millions of values)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：按给定相似度聚合可能更相关，但需要对特征有一定了解，而这些知识可能不可得，或者可能需要很长时间（例如，如果有数百万个值的话）'
- en: Note
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Aggregating is also sometimes useful when there is a long tail distribution
    of values for a feature, which means that some values appear a lot, while many
    others appear only a small fraction of the time.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征存在长尾分布时，聚合也有时很有用，这意味着一些值出现得很频繁，而许多其他值则仅偶尔出现。
- en: 'In this recipe, we will apply aggregating to a dataset that contains many cities
    as features but with no information about the city names. This will leave us with
    the only option being to aggregate by occurrence. We will reuse the same dataset
    as in the previous recipe, so we will require the Kaggle API. For that, please
    refer to the previous recipe. Using the Kaggle API, the dataset can be downloaded
    with the following command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将对一个包含许多城市作为特征但没有城市名称信息的数据集进行聚合。这将使我们只剩下按出现次数聚合的选择。我们将重复使用前一个配方中的相同数据集，因此我们将需要Kaggle
    API。为此，请参考前一个配方。通过Kaggle API，可以使用以下命令下载数据集：
- en: '[PRE68]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We will also need the `pandas` and `scikit-learn` libraries, which can be installed
    with the following command:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要`pandas`和`scikit-learn`库，可以通过以下命令安装：
- en: '[PRE69]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: How to do it...
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will use the same dataset as in the previous recipe. To prepare for this
    recipe, we will aggregate the cities based on a given threshold on their occurrences
    in the dataset, and then train and evaluate a model on this data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前一个配方相同的数据集。为了准备这个配方，我们将基于数据集中城市的出现次数聚合城市特征，然后在此数据上训练并评估模型：
- en: 'Import the required modules, functions, and classes: `pandas` for loading the
    data, `train_test_split` for splitting the data, `StandardScaler` for rescaling
    quantitative features, `OneHotEncoder` for encoding qualitative features, and
    `LogisticRegression` as the model:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块、函数和类：`pandas`用于加载数据，`train_test_split`用于划分数据，`StandardScaler`用于重新缩放定量特征，`OneHotEncoder`用于编码定性特征，`LogisticRegression`作为模型：
- en: '[PRE70]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Load the dataset with `pandas`. There’s no need to unzip the file first – this
    is all handled by `pandas`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据集。无需先解压文件——这一切都由`pandas`处理：
- en: '[PRE75]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Remove any missing data. As we did in the previous recipe, we will use a simple
    policy, remove all features that have a large amount of missing data, and then
    remove the rows with missing data:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除任何缺失数据。就像在前一个配方中一样，我们将采用一个简单的策略，删除所有有大量缺失数据的特征，然后删除包含缺失数据的行：
- en: '[PRE76]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Split the data into training and test sets with the `train_test_split` function:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数将数据拆分为训练集和测试集：
- en: '[PRE79]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Rescale any quantitative features with the standard scaler provided by `scikit-learn`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`提供的标准缩放器对任何定量特征进行重新缩放：
- en: '[PRE84]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Now, we must aggregate the `city` feature:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须对`city`特征进行聚合：
- en: '[PRE91]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Prepare the qualitative features with one-hot encoding, including the newly
    aggregated `city` feature:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用独热编码准备定性特征，包括新聚合的`city`特征：
- en: '[PRE101]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Concatenate the quantitative and qualitative features back together:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将定量特征和定性特征重新连接在一起：
- en: '[PRE114]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Instantiate and train the logistic regression model. Here, we will just keep
    the default hyperparameters of the model:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化并训练逻辑回归模型。在这里，我们将保持模型的默认超参数：
- en: '[PRE117]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Compute and print the model’s accuracy on both the training and test sets:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并打印模型在训练集和测试集上的准确率：
- en: '[PRE119]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'The output will look like this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果将如下所示：
- en: '[PRE121]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For this specific case, aggregating did not seem to help much in terms of giving
    us more robust results, but at the very least it has helped the model be less
    unpredictable and robust to new cities.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定情况，聚合似乎并没有显著帮助我们获得更强的结果，但至少它帮助模型变得更不容易预测，且对于新城市更具鲁棒性。
- en: There’s more...
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Since the aggregation code may have looked complicated, let’s take a look at
    what we did.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚合代码可能看起来有些复杂，我们来看看我们做了什么。
- en: 'So, we have the `city` feature, which has many possible values; there’s a frequency
    for each value in the training set. These can be computed with the `.``value_counts(normalize=True)`
    method:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有`city`特征，它有许多可能的值；每个值在训练集中的频率都可以通过`.value_counts(normalize=True)`方法计算：
- en: '[PRE122]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'This will give us the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE123]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'It appears that, in the entire dataset, `city_103` is the value more than 23%
    of the time, while other values such as `city_111` appear less than 1% of the
    time. We will just apply a threshold to those values so that we get the list of
    cities appearing more than the given threshold:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，在整个数据集中，`city_103`的值出现的频率超过23%，而其他值，如`city_111`，出现的频率不到1%。我们只需对这些值应用一个阈值，以便获取出现频率超过给定阈值的城市列表：
- en: '[PRE124]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'This will give us the following output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE125]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Now, all we have to do is get the index (that is, the city name) of all the
    true values. This is exactly what we can do with the following full line:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要做的就是获取所有真实值的索引（即城市名称）。这正是我们可以通过以下完整行来完成的：
- en: '[PRE126]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'This shows the following output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '[PRE127]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: As expected, this returns list of cities that occur more than the threshold.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，这将返回出现频率超过阈值的城市列表。
- en: Undersampling an imbalanced dataset
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对不平衡数据集进行欠采样
- en: 'A typical case in machine learning is what we call an imbalanced dataset. An
    imbalanced dataset simply means that for a given class, some occurrences are much
    more likely than others, hence the lack of balance. There are plenty of cases
    of imbalanced datasets: rare diseases in medicine, customer behavior, and more.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的典型情况是我们所称的“不平衡数据集”。不平衡数据集意味着对于某一类别，某些实例比其他实例更可能出现，从而导致数据的不平衡。不平衡数据集的案例有很多：医学中的罕见疾病、客户行为等。
- en: 'In this recipe, we will propose one possible way to handle imbalanced datasets:
    undersampling. After explaining this process, we will apply it to a credit card
    fraud detection dataset.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们将提出一种可能的处理不平衡数据集的方法：欠采样。在解释这个过程后，我们将其应用于一个信用卡欺诈检测数据集。
- en: Getting ready
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The problem with imbalanced data is that it may bias the results of a machine
    learning model. Let’s assume we’re undertaking a classification task of detecting
    rare diseases present in only 1% of a dataset. A common pitfall with such data
    is to have a model predicting as always healthy as it would still have 99% accuracy.
    So, it would be very likely for a machine learning model to minimize its losses.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据的问题在于，它可能会偏倚机器学习模型的结果。假设我们正在进行一个分类任务，检测数据集中仅占1%的罕见疾病。此类数据的一个常见陷阱是模型总是预测为健康状态，因为这样它仍然可以达到99%的准确率。因此，机器学习模型很可能会最小化其损失。
- en: Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In such situations, other metrics such as the F1-score or **ROC Area Under Curve**
    (**ROC AUC**) are usually more relevant.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，其他指标，如F1分数或**ROC曲线下面积**（**ROC AUC**），通常更为相关。
- en: 'One way to prevent this from happening is to undersample the dataset. More
    specifically, we can undersample the overrepresented class by removing some samples
    of it:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 防止这种情况发生的一种方法是对数据集进行欠采样。更具体来说，我们可以通过删除部分样本来对过度代表的类别进行欠采样：
- en: We keep all the samples of the underrepresented class
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们保留所有欠代表类别的样本
- en: We keep only a subsample of the overrepresented class
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仅保留过度代表类别的子样本
- en: 'By doing this, we can artificially balance the dataset and avoid the pitfalls
    of an imbalanced dataset. For example, let’s say we have a dataset composed of
    the following attributes:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们可以人为地平衡数据集，避免不平衡数据集的陷阱。例如，假设我们有一个由以下属性组成的数据集：
- en: 100 samples with disease
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个带有疾病的样本
- en: 9,900 samples with no disease
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9,900个没有疾病的样本
- en: 'A perfectly balanced undersampling would give us the following results in the
    dataset:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 完全平衡的欠采样将给我们以下数据集中的结果：
- en: 100 samples with disease
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个带有疾病的样本
- en: 100 randomly selected samples with no disease
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个随机选择的没有疾病的样本
- en: Of course, the drawback is that we lose a lot of data in the process.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，缺点是我们在这个过程中会丢失大量数据。
- en: 'For this recipe, we first need to download the dataset. To do so, we will use
    the Kaggle API (refer to the *Hashing high cardinality features* recipe to learn
    how to install it). The dataset can be downloaded with the following command line:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方法，我们首先需要下载数据集。为此，我们将使用Kaggle API（请参考*哈希高基数特征*的方法了解如何安装）。可以使用以下命令行下载数据集：
- en: '[PRE128]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'The following libraries are also needed: `pandas` for loading the data, `scikit-learn`
    for modeling, `matplotlib` for displaying the data, and `imbalanced-learn` for
    undersampling. They can be installed with the following command line:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要以下库：`pandas` 用于加载数据，`scikit-learn` 用于建模，`matplotlib` 用于显示数据，`imbalanced-learn`
    用于欠采样。可以通过以下命令行安装：
- en: '[PRE129]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: How to do it...
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will apply undersampling to a credit card fraud dataset.
    This is a rather extreme case of an imbalanced dataset since only about 0.18%
    of the samples are positive:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇教程中，我们将对信用卡欺诈数据集应用欠采样。这是一个相当极端的类别不平衡数据集的例子，因为其中仅约 0.18% 的样本是正类：
- en: 'Import the required modules, classes, and functions:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块、类和函数：
- en: '`pandas` for data loading and manipulation'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 用于数据加载和处理'
- en: '`train_test_split` for data splitting'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split` 用于数据拆分'
- en: '`StandardScaler` for data rescaling (the dataset holds only quantitative features)'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler` 用于数据重新缩放（数据集仅包含定量特征）'
- en: '`RandomUnderSampler` for undersampling'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomUnderSampler` 用于欠采样'
- en: '`LogisticRegression` for modeling'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegression` 用于建模'
- en: '`roc_auc_score` for displaying the ROC and ROC AUC computations:'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`roc_auc_score` 用于显示 ROC 和 ROC AUC 计算：'
- en: '[PRE130]'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Load the data with `pandas`. We can load the ZIP file directly. We will also
    display the relative amount of each label: we have around 99.8% of regular transactions
    compared to less than 0.18% of fraudulent transactions:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 加载数据。我们可以直接加载 ZIP 文件。我们还将显示每个标签的相对数量：我们有大约 99.8% 的正常交易，而欺诈交易的比例不到
    0.18%：
- en: '[PRE137]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'The output will look like this:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE139]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Split the data into training and test sets. The need to stratify the label
    in such cases can be critical:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集。在这种情况下，确保标签的分层非常关键：
- en: '[PRE140]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Apply random undersampling, up to a 10% sampling strategy. This means we must
    undersample the overrepresented class until there is a 10 to 1 ratio in the class
    balance. We could go up to a 1 to 1 ratio, but this would be at the cost of even
    more dropped data. This ratio is defined by the `sampling_strategy=0.1` parameter.
    We must also set the random state for reproducibility:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用随机欠采样，使用最多 10% 的采样策略。这意味着我们必须对过度表示的类别进行欠采样，直到类别平衡达到 10 比 1 的比例。我们可以做到 1 比
    1 的比例，但这会导致更多的数据丢失。此比例由 `sampling_strategy=0.1` 参数定义。我们还必须设置随机状态以确保可重复性：
- en: '[PRE144]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'This gives us the following output:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE151]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: After undersampling, we end up with `3940` regular transaction samples compared
    to `394` fraudulent transactions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在欠采样后，我们最终得到了 `3940` 个正常交易样本，与 `394` 个欺诈交易样本相比。
- en: 'Rescale the data using a standard scaler:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准缩放器重新缩放数据：
- en: '[PRE152]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Note
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Arguably, we could apply rescaling before resampling. This would give more weight
    to the overrepresented class when rescaling but would not be considered data leakage.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，我们可以在重采样之前应用重新缩放。这将使过度表示的类别在重新缩放时更具权重，但不会被视为数据泄露。
- en: 'Instantiate and train the logistic regression model on the training set:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上实例化并训练逻辑回归模型：
- en: '[PRE156]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Compute the ROC AUC on both the training and test sets. To do so, we need the
    predicted probabilities for each sample, which we can get with the `predict_proba()`
    method, as well as the imported `roc_auc_score()` function:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集和测试集的 ROC AUC。为此，我们需要获取每个样本的预测概率，这可以通过 `predict_proba()` 方法获得，并且需要使用导入的
    `roc_auc_score()` 函数：
- en: '[PRE158]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'This returns the following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下结果：
- en: '[PRE166]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: We got a ROC AUC of about 97% on the test set and close to 99% on the training
    set.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上获得了大约 97% 的 ROC AUC，在训练集上接近 99%。
- en: There’s more...
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'Optionally, we can plot the ROC curves for both the training and test sets.
    To do so, we can use the `roc_curve()` function from `scikit-learn`:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以绘制训练集和测试集的 ROC 曲线。为此，我们可以使用 `scikit-learn` 中的 `roc_curve()` 函数：
- en: '[PRE167]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '![Figure 5.2 – ROC curve for the train and test sets. Plot produced by the
    code](img/B19629_05_02.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 训练集和测试集的 ROC 曲线。图形由代码生成](img/B19629_05_02.jpg)'
- en: Figure 5.2 – ROC curve for the train and test sets. Plot produced by the code
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 训练集和测试集的 ROC 曲线。图形由代码生成
- en: As we can see, while the ROC AUC is very similar for the training and test sets,
    the curve for the test set is a bit lower. This means that, as expected, the model
    is overfitting slightly.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，尽管训练集和测试集的 ROC AUC 非常相似，但测试集的曲线略低。这意味着，正如预期的那样，模型稍微出现了过拟合。
- en: Note that fine-tuning `sampling_strategy` might be helpful to get better results.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，微调 `sampling_strategy` 可能有助于获得更好的结果。
- en: Note
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To optimize the sampling strategy and the model hyperparameters at the same
    time, you can use scikit-learn’s `Pipeline` class.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了同时优化采样策略和模型超参数，可以使用scikit-learn的`Pipeline`类。
- en: See also
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The documentation for `RandomUnderSampler`: [https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomUnderSampler`的文档：[https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml)'
- en: 'The documentation for `Pipeline`: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pipeline`的文档：[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml)'
- en: 'Here’s a great code example of a two-step pipeline hyperparameter optimization:
    [https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml%0D)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个关于双步骤管道超参数优化的优秀代码示例：[https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml%0D)
- en: Oversampling an imbalanced dataset
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对不平衡数据集进行过采样
- en: Another solution when dealing with imbalanced datasets is random oversampling.
    This is the opposite of random undersampling. In this recipe, we’ll learn how
    to use it on the credit card fraud detection dataset.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集的另一种解决方案是随机过采样。这是随机欠采样的对立面。在本配方中，我们将学习如何在信用卡欺诈检测数据集上使用它。
- en: Getting ready
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Random oversampling can be seen as the opposite of random undersampling: the
    idea is to duplicate samples of the underrepresented dataset to rebalance the
    dataset.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过采样可以看作是随机欠采样的对立面：其思想是复制欠代表类别的数据样本，以重新平衡数据集。
- en: 'As for the previous recipe, let’s assume a 1%-99% imbalanced dataset that contains
    the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面的配方一样，假设一个1%-99%不平衡的数据集，其中包含以下内容：
- en: 100 samples with disease
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100个有疾病样本
- en: 9,900 samples with no disease
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9,900个无疾病样本
- en: 'To apply oversampling to this dataset using a 1/1 strategy (so, a perfectly
    balanced dataset), we would need to have 99 duplicates of each sample of the disease
    class. So, the oversampled dataset would need to contain the following:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用1/1策略（即完全平衡的数据集）对这个数据集进行过采样，我们需要对每个疾病类别的样本进行99次复制。因此，过采样后的数据集将需要包含以下内容：
- en: 9,900 samples with disease (100 original samples duplicated 99 times on average)
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9,900个有疾病样本（100个原始样本平均复制99次）
- en: 9,900 samples with no disease
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 9,900个无疾病样本
- en: 'We can easily guess the pros and cons of this method:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松猜测这种方法的优缺点：
- en: '**Pro**: Unlike undersampling, we do not waste any data from the overrepresented
    class, which means our model can be trained on the full picture of the data we
    have'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：与欠采样不同，我们不会丢失过代表类别的任何数据，这意味着我们的模型可以在我们拥有的完整数据上进行训练'
- en: '**Con**: We may have a lot of duplicates of the underrepresented class, leading
    to potential overfitting on this data'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：我们可能会有很多欠代表类别的重复样本，这可能会导致对这些数据的过拟合'
- en: Fortunately, we can choose a rebalancing strategy below 1/1 so that the underrepresented
    data duplication can be limited.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以选择低于1/1的重平衡策略，从而限制数据重复。
- en: For this recipe, we need to download the dataset. If you completed the *Undersampling
    an imbalanced dataset* recipe, you don’t need to do anything else.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们需要下载数据集。如果你已经完成了*Undersampling an imbalanced dataset*配方，那么你无需做其他操作。
- en: 'Otherwise, using the Kaggle API (refer to the *Hashing high cardinality features*
    recipe to learn how to install it), we need to download the dataset via the following
    command line:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，通过使用Kaggle API（请参考*Hashing high cardinality features*配方了解如何安装），我们需要通过以下命令行下载数据集：
- en: '[PRE168]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'The following libraries are also needed: `pandas` for loading the data, `scikit-learn`
    for modeling, `matplotlib` for displaying the data, and `imbalanced-learn` for
    the oversampling part. They can be installed via the following command line:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要以下库：`pandas`用于加载数据，`scikit-learn`用于建模，`matplotlib`用于显示数据，`imbalanced-learn`用于过采样部分。它们可以通过以下命令行安装：
- en: '[PRE169]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: How to do it...
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will apply oversampling to the credit card fraud dataset:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将对信用卡欺诈数据集应用过采样：
- en: 'Import the required modules, classes, and functions:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块、类和函数：
- en: '`pandas` for data loading and manipulation'
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`用于数据加载和处理'
- en: '`train_test_split` for data splitting'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split`用于数据划分'
- en: '`StandardScaler` for data rescaling (the dataset only contains quantitative
    features)'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler`用于数据缩放（数据集仅包含定量特征）'
- en: '`RandomOverSampler` for oversampling'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomOverSampler`用于过采样'
- en: '`LogisticRegression` for modeling'
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegression`用于建模'
- en: '`roc_auc_score` for displaying the ROC and ROC AUC computations:'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`roc_auc_score`用于显示ROC和ROC AUC计算：'
- en: '[PRE170]'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'Load the data with pandas. We can load the ZIP file directly. As we did in
    the previous recipe, we will display the relative amount of each label to remind
    us that we have about 99.8% of regular transactions and less than 0.18% of fraudulent
    transactions:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas加载数据。我们可以直接加载ZIP文件。与前面的小节一样，我们将显示每个标签的相对数量，以提醒我们大约有99.8%的正常交易和不到0.18%的欺诈交易：
- en: '[PRE177]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'This prints the following:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE179]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'Split the data into training and test sets. We must specify stratification
    on the labels to make sure the balance is still the same:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据划分为训练集和测试集。我们必须指定标签的分层抽样，以确保平衡保持不变：
- en: '[PRE180]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'Apply random oversampling with a 10% sampling strategy. This means that we
    oversample the underrepresented class until there is a 10 to 1 ratio in the class
    balance. This ratio is defined by the `sampling_strategy=0.1` parameter. We must
    also set the random state for reproducibility:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用10%的采样策略进行随机过采样。这意味着我们将过采样不足表示的类别，直到类别平衡达到10比1的比例。这个比例由`sampling_strategy=0.1`参数定义。我们还必须设置随机状态以保证可重复性：
- en: '[PRE184]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: 'This outputs the following:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE191]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: After oversampling, we now have `227451` regular transactions in the training
    set (which is unchanged) versus `22745` fraudulent transactions.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在过采样后，我们的训练集中现在有`227451`个正常交易（保持不变），与`22745`个欺诈交易。
- en: Note
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'It is possible to change the sampling strategy. As usual, it is a matter of
    doing a tradeoff: a greater sampling strategy means more duplicated samples for
    more balance, while a smaller sampling strategy means fewer duplicated samples
    but less balance.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 可以更改采样策略。像往常一样，这需要做出取舍：更大的采样策略意味着更多的重复样本来增加平衡，而较小的采样策略则意味着更少的重复样本，但平衡较差。
- en: 'Rescale the data using a standard scaler:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准缩放器对数据进行缩放：
- en: '[PRE192]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: 'Instantiate and train the logistic regression model on the training set:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上实例化并训练逻辑回归模型：
- en: '[PRE196]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: 'Compute the ROC AUC on both the training and test sets. To do so, we need the
    predicted probabilities for each sample, which we can get by using the `predict_proba()`
    method, as well as the imported `roc_auc_score()` function:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集和测试集上的ROC AUC。为此，我们需要每个样本的预测概率，我们可以通过使用`predict_proba()`方法以及导入的`roc_auc_score()`函数来获得：
- en: '[PRE198]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'This returns the following:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下内容：
- en: '[PRE206]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: The results are quite comparable to the ones we obtained with undersampling.
    However, this does not mean that these two techniques are always equal.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与我们通过欠采样获得的结果相当相似。然而，这并不意味着这两种技术总是相等的。
- en: There’s more...
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Optionally, just like we did in the *Undersampling an imbalanced dataset* recipe,
    we can plot the ROC curves for both the training and test sets using the `roc_curve()`
    function from `scikit-learn`:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，正如我们在*欠采样不平衡数据集*一节中所做的那样，我们可以使用`scikit-learn`中的`roc_curve()`函数绘制训练集和测试集的ROC曲线：
- en: '[PRE207]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '![Figure 5.3 – ROC curve for the train and test sets. Plot produced by the
    code](img/B19629_05_03.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 训练集和测试集的ROC曲线。代码生成的图](img/B19629_05_03.jpg)'
- en: Figure 5.3 – ROC curve for the train and test sets. Plot produced by the code
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 训练集和测试集的ROC曲线。代码生成的图
- en: In this case, the ROC AUC curve for the test set is clearly below the one for
    the training set, which means that the model is overfitting slightly.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，测试集的ROC AUC曲线明显低于训练集的曲线，这意味着模型稍微出现了过拟合。
- en: See also
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The documentation for `RandomUnderSampler` is available at [https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomUnderSampler`的文档可以在[https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml)找到。'
- en: Resampling imbalanced data with SMOTE
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SMOTE重新采样不平衡数据
- en: Finally, a more complex solution for dealing with imbalanced datasets is a method
    called SMOTE. After explaining the SMOTE algorithm, we will apply this method
    to the credit card fraud detection dataset.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，处理不平衡数据集的一个更复杂的解决方案是名为SMOTE的方法。在解释SMOTE算法后，我们将应用这种方法来处理信用卡欺诈检测数据集。
- en: Getting ready
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: '**SMOTE** stands for **Synthetic Minority Oversampling TEchnique**. As its
    name suggests, it creates synthetic samples for an underrepresented class. But
    how exactly does it create synthetic data?'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '**SMOTE**代表**Synthetic Minority Oversampling TEchnique**。顾名思义，它为不平衡类创建合成样本。但它究竟如何创建合成数据？'
- en: 'This method uses the k-NN algorithm on the underrepresented class. The SMOTE
    algorithm can be summarized with the following steps:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在不平衡类上使用k-NN算法。SMOTE算法可以通过以下步骤总结：
- en: Randomly pick a sample, ![](img/F_05_001.png), in the minority class.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择少数类中的样本![](img/F_05_001.png)。
- en: Using k-NN, randomly pick one of the k-nearest neighbors of ![](img/F_05_002.png)
    in the minority class. Let’s call this sample ![](img/F_05_003.png).
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k-NN，在少数类中随机选择![](img/F_05_002.png)的k个最近邻之一。我们称此样本为![](img/F_05_003.png)。
- en: 'Compute the new synthetic sample, ![](img/F_05_004.png), with 𝜆 being randomly
    drawn in the [0, 1] range:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新的合成样本，![](img/F_05_004.png)，其中𝜆在[0, 1]范围内随机抽取：
- en: '![Figure 5.4 – Visual representation of SMOTE](img/B19629_05_04.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – SMOTE的视觉表示](img/B19629_05_04.jpg)'
- en: Figure 5.4 – Visual representation of SMOTE
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – SMOTE的视觉表示
- en: 'Compared to random oversampling, this method is more complex since it has one
    hyperparameter: the number of nearest neighbors, *k*, to consider. This method
    also comes with pros and cons:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机过采样相比，此方法更复杂，因为它有一个超参数：考虑的最近邻数*k*。此方法也有其利弊：
- en: '**Pro**: Unlike random oversampling, it limits the risks of overfitting on
    the underrepresented class since samples are not duplicated'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：与随机过采样不同，它限制了在不平衡类上过拟合的风险，因为样本不会重复'
- en: '**Con**: Creating synthetic data is a risky bet; nothing assures you that it
    has a meaning and would ever be likely on real data'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：创建合成数据是一种冒险的赌注；没有任何保证它具有意义，并且可能会在真实数据上成为可能'
- en: To complete this recipe, you will need to download the credit card fraud dataset
    if you haven’t done so already (check out the *Undersampling an imbalanced dataset*
    or *Oversampling an imbalanced dataset* recipe to learn how to do this).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此配方，如果尚未这样做，请下载信用卡欺诈数据集（查看*Undersampling an imbalanced dataset*或*Oversampling
    an imbalanced dataset*配方以了解如何执行此操作）。
- en: 'Using the Kaggle API (refer to the *Hashing high cardinality features* recipe
    to learn how to install it), we have to download the dataset via the following
    command line:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kaggle API（参考*Hashing high cardinality features*配方以了解如何安装它），我们必须通过以下命令行下载数据集：
- en: '[PRE208]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: 'The following libraries are needed: `pandas` for loading the data, `scikit-learn`
    for modeling, `matplotlib` for displaying the data, and `imbalanced-learn` for
    undersampling. They can be installed via the following command line:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 需要以下库：`pandas` 用于加载数据，`scikit-learn` 用于建模，`matplotlib` 用于显示数据，`imbalanced-learn`
    用于欠采样。它们可以通过以下命令行安装：
- en: '[PRE209]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE209]'
- en: How to do it...
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'In this recipe, we will apply SMOTE to the credit card fraud dataset:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将对信用卡欺诈数据集应用SMOTE：
- en: 'Import the required modules, classes, and functions:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块、类和函数：
- en: '`pandas` for data loading and manipulation'
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 用于数据加载和操作'
- en: '`train_test_split` for data splitting'
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split` 用于数据拆分'
- en: '`StandardScaler` for data rescaling (the dataset contains only quantitative
    features)'
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler` 用于数据重新缩放（数据集仅包含定量特征）'
- en: '`SMOTE` for the SMOTE oversampling'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SMOTE` 用于SMOTE过采样'
- en: '`LogisticRegression` for modeling'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LogisticRegression` 用于建模'
- en: '`roc_auc_score` for displaying the ROC and ROC AUC computations:'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`roc_auc_score` 用于显示ROC和ROC AUC计算：'
- en: '[PRE210]'
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: 'Load the data with `pandas`. We can load the ZIP file directly. As we did in
    the previous two recipes, we will display the relative amount of each label. Again,
    we will have about 99.8% of regular transactions and less than 0.18% of fraudulent
    transactions:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据。我们可以直接加载ZIP文件。与前两个配方一样，我们将显示每个标签的相对数量。再次，我们将有大约99.8%的常规交易和少于0.18%的欺诈交易：
- en: '[PRE217]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: 'The output will look like this:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE219]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: 'Split the data into training and test sets. We must specify stratification
    on the labels to make sure the balance is still the same:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和测试集。我们必须在标签上指定分层，以确保平衡保持不变：
- en: '[PRE220]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: 'Apply SMOTE with a 10% sampling strategy with the `sampling_strategy=0.1` parameter.
    By doing this, we will generate synthetic data of the underrepresented class until
    there is a 10 to 1 ratio in the class balance. We must also set the random state
    for reproducibility:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sampling_strategy=0.1`参数应用SMOTE，以10%的采样策略生成不平衡类的合成数据。通过这样做，我们将在类平衡中实现10比1的比例。我们还必须设置随机状态以实现可重现性：
- en: '[PRE224]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: 'With this, we will get the following output:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此方法，我们将得到以下输出：
- en: '[PRE231]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: After oversampling, we now have `227451` regular transactions in the training
    set (which is unchanged) versus `22745` fraudulent transactions, including many
    synthetically generated samples.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在过采样之后，我们在训练集中现在有 `227451` 个常规交易（保持不变），与 `22745` 个欺诈交易，其中包括许多合成生成的样本。
- en: 'Rescale the data using a standard scaler:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准化缩放器对数据进行重缩放：
- en: '[PRE232]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: 'Instantiate and train the logistic regression model on the training set:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上实例化并训练逻辑回归模型：
- en: '[PRE236]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: 'Compute the ROC AUC on both the training and test sets. To do so, we need the
    predicted probabilities for each sample, which we can get by using the `predict_proba()`
    method, as well as the imported `roc_auc_score()` function:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集和测试集上的 ROC AUC。为此，我们需要每个样本的预测概率，这可以通过使用 `predict_proba()` 方法以及导入的 `roc_auc_score()`
    函数来获得：
- en: '[PRE238]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '[PRE244]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE245]'
- en: 'Now, the output should be as follows:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，输出应如下所示：
- en: '[PRE246]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: The results are slightly different from the ones we got for random undersampling
    and oversampling. While the performances on the test set are quite similar, there
    seems to be more overfitting in this case. There are several possible explanations
    for such results, with one of them being that the synthetic samples were not very
    helpful for the model.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与我们在随机欠采样和过采样中得到的结果略有不同。尽管在测试集上的性能非常相似，但在这种情况下似乎有更多的过拟合。对此类结果有几种可能的解释，其中之一是合成样本对模型的帮助不大。
- en: Note
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The results that we got for the resampling strategies on this dataset are not
    necessarily representative of the results we would get on any other dataset. Moreover,
    we had to fine-tune the sampling strategies and models to get a proper performance
    comparison.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个数据集上得到的重采样策略结果不一定能代表我们在其他数据集上得到的结果。此外，我们必须微调采样策略和模型，才能获得合适的性能对比。
- en: There’s more...
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Optionally, we can plot the ROC curves for both the training and test sets
    using the `roc_curve()` function from `scikit-learn`:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以使用 `scikit-learn` 中的 `roc_curve()` 函数绘制训练集和测试集的 ROC 曲线：
- en: '[PRE247]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: 'Here is the plot for it:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图示：
- en: '![Figure 5.5 – ROC curve for the train and test sets after using SMOTE](img/B19629_05_05.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 使用 SMOTE 后的训练集和测试集 ROC 曲线](img/B19629_05_05.jpg)'
- en: Figure 5.5 – ROC curve for the train and test sets after using SMOTE
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 使用 SMOTE 后的训练集和测试集 ROC 曲线
- en: Compared to random undersampling and oversampling, overfitting appears to be
    even clearer here.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机欠采样和过采样相比，这里过拟合似乎更加明显。
- en: See also
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The official documentation for SMOTE can be found at [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE 的官方文档可以在 [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml)
    找到。
- en: It is not recommended that you apply this implementation to categorical features
    as it assumes that a feature value for a sample can be any linear combination
    of values of other samples. This is not true for categorical features.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议将此实现应用于分类特征，因为它假设样本的特征值可以是其他样本特征值的任何线性组合。这对于分类特征来说并不成立。
- en: 'Working implementations for categorical features have also been proposed, including
    the following:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 也有针对分类特征的工作实现，以下是一些例子：
- en: '**SMOTENC**: For working with datasets that contain both categorical and non-categorical
    features: [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml)'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTENC**：用于处理包含分类特征和非分类特征的数据集：[https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml)'
- en: '**SMOTEN**: For working with datasets that contain only categorical features:
    [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml)'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SMOTEN**：用于处理仅包含分类特征的数据集：[https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml)'
