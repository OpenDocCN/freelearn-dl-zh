- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: AI Foundation Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能基础技术
- en: In this chapter, you'll begin your study of AI theory in earnest. You'll start
    with an introduction to a major branch of AI, called Reinforcement Learning, and
    the five principles that underpin every Reinforcement Learning model. Those principles
    will give you the theoretical understanding to make sense of every forthcoming
    AI model in this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将正式开始学习人工智能理论。你将从强化学习这一人工智能的主要分支开始，并学习支撑每一个强化学习模型的五个原则。这些原则将为你提供理论基础，帮助你理解本书中所有即将出现的人工智能模型。
- en: What is Reinforcement Learning?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: When people refer to AI today, some of them think of Machine Learning, while
    others think of Reinforcement Learning. I fall into the second category. I always
    saw Machine Learning as statistical models that have the ability to learn some
    correlations, from which they make predictions without being explicitly programmed.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们今天提到人工智能时，有些人会想到机器学习，而另一些人则会想到强化学习。我属于后者。我一直认为机器学习是具有学习某些关联性的统计模型，它们通过这些关联性进行预测，而无需明确的编程。
- en: 'While this is, in some way, a form of AI, Machine Learning does not include
    the process of taking actions and interacting with an environment like we humans
    do. Indeed, as intelligent human beings, what we constantly keep doing is the
    following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这在某种程度上是一种人工智能的表现，但机器学习并不包含像我们人类那样进行行动和与环境互动的过程。事实上，作为智能的存在，我们不断进行以下的行为：
- en: We observe some input, whether it's what we see with our eyes, what we hear
    with our ears, or what we remember in our memory.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们观察一些输入，不论是通过眼睛看到的，耳朵听到的，还是我们记得的。
- en: These inputs are then processed in our brain.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些输入随后会在我们的大脑中处理。
- en: Eventually, we make decisions and take actions.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，我们做出决策并采取行动。
- en: This process of interacting with an environment is what we are trying to reproduce
    in terms of Artificial Intelligence. And to that extent, the branch of AI that
    works on this is Reinforcement Learning. This is the closest match to the way
    we think; the most advanced form of Artificial Intelligence, if we see AI as the
    science that tries to mimic (or surpass) human intelligence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与环境互动的这一过程是我们试图在人工智能领域重现的内容。为此，人工智能的一个分支——强化学习，致力于此。它是最接近我们思考方式的人工智能形式；如果我们把人工智能看作是试图模仿（或超越）人类智能的科学，那么它就是人工智能中最先进的形式。
- en: Reinforcement Learning also has the most impressive results in business applications
    of AI. For example, Alibaba leveraged Reinforcement Learning to increase its ROI
    in online advertising by 240% without increasing their advertising budget (see
    [https://arxiv.org/pdf/1802.09756.pdf](https://arxiv.org/pdf/1802.09756.pdf),
    page 9, Table 1 last row (DCMAB)). We'll tackle the same industry application
    in this book!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在人工智能的商业应用中也取得了最令人印象深刻的成果。例如，阿里巴巴利用强化学习将其在线广告的投资回报率提高了240%，而无需增加广告预算（参见[https://arxiv.org/pdf/1802.09756.pdf](https://arxiv.org/pdf/1802.09756.pdf)，第9页，表格1最后一行（DCMAB））。我们将在本书中处理相同的行业应用！
- en: The five principles of Reinforcement Learning
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的五个原则
- en: Let's begin building the first pillars of your intuition into how Reinforcement
    Learning works. These are the fundamental principles of Reinforcement Learning,
    which will get you started with the right, solid basics in AI.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始建立你对强化学习如何工作的第一批直觉。这些是强化学习的基本原则，它们将为你在人工智能领域打下坚实的基础。
- en: 'Here are the five principles:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是五个原则：
- en: '**Principle #1**: The input and output system'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原则 #1**：输入与输出系统'
- en: '**Principle #2**: The reward'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原则 #2**：奖励'
- en: '**Principle #3**: The AI environment'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原则 #3**：人工智能环境'
- en: '**Principle #4**: The Markov decision process'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原则 #4**：马尔可夫决策过程'
- en: '**Principle #5**: Training and inference'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原则 #5**：训练与推理'
- en: In the following sections, you can read about each one in turn.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将依次阅读每个内容。
- en: 'Principle #1 – The input and output system'
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '原则 #1 – 输入与输出系统'
- en: The first step is to understand that today, all AI models are based on the common
    principle of inputs and outputs. Every single form of Artificial Intelligence,
    including Machine Learning models, ChatBots, recommender systems, robots, and
    of course Reinforcement Learning models, will take something as input, and will
    return another thing as output.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是理解，今天所有的人工智能模型都基于输入和输出的共同原则。每一种形式的人工智能，包括机器学习模型、聊天机器人、推荐系统、机器人，当然也包括强化学习模型，都会接收某些东西作为输入，并返回另一种东西作为输出。
- en: '![](img/B14110_04_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_04_01.png)'
- en: 'Figure 1: The input and output system'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：输入和输出系统
- en: 'In Reinforcement Learning, these inputs and outputs have a specific name: the
    input is called the state, or input state. The output is the action performed
    by the AI. And in the middle, we have nothing other than a function that takes
    a state as input and returns an action as output. That function is called a policy.
    Remember the name, "policy," because you will often see it in AI literature.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，这些输入和输出有一个特定的名称：输入叫做状态，或输入状态。输出是AI执行的动作。中间的部分，只有一个函数，它以状态为输入，返回动作作为输出。这个函数叫做策略。记住这个名字，“策略”，因为你会在AI文献中经常看到它。
- en: As an example, consider a self-driving car. Try to imagine what the input and
    output would be in that case.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以自动驾驶汽车为例。试着想象一下，在这种情况下输入和输出是什么。
- en: 'The input would be what the embedded computer vision system sees, and the output
    would be the next move of the car: accelerate, slow down, turn left, turn right,
    or brake. Note that the output at any time (*t*) could very well be several actions
    performed at the same time. For instance, the self-driving car can accelerate
    while at the same time turning left. In the same way, the input at each time (*t*)
    can be composed of several elements: mainly the image observed by the computer
    vision system, but also some parameters of the car such as the current speed,
    the amount of gas remaining in the tank, and so on.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是嵌入式计算机视觉系统所看到的内容，输出则是汽车的下一步动作：加速、减速、左转、右转或刹车。请注意，任何时候（*t*），输出可能是多个动作同时执行。例如，自驾车可以在加速的同时左转。同样，每个时刻（*t*）的输入也可以由多个元素组成：主要是计算机视觉系统观察到的图像，但也包括一些汽车参数，如当前速度、油箱剩余的油量等。
- en: 'That''s the very first important principle in Artificial Intelligence: it is
    an intelligent system (a policy) that takes some elements as input, does its magic
    in the middle, and returns some actions to perform as output. Remember that the
    inputs are also called the **states**.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是人工智能的第一个重要原则：它是一个智能系统（一个策略），它接受一些元素作为输入，在中间进行处理，然后返回一些动作作为输出。记住，输入也叫做**状态**。
- en: The next important principle is the reward.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要的原则是奖励。
- en: 'Principle #2 – The reward'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原则#2 – 奖励
- en: Every AI has its performance measured by a reward system. There's nothing confusing
    about this; the reward is simply a metric that will tell the AI how well it does
    over time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个AI的表现都通过奖励系统来衡量。这一点毫不复杂；奖励只是一个度量标准，用来告诉AI它随时间的表现如何。
- en: 'The simplest example is a binary reward: 0 or 1\. Imagine an AI that has to
    guess an outcome. If the guess is right, the reward will be 1, and if the guess
    is wrong, the reward will be 0\. This could very well be the reward system defined
    for an AI; it really can be as simple as that!'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的例子是二元奖励：0或1。想象一个AI需要猜测一个结果。如果猜对了，奖励是1；如果猜错了，奖励是0。这可能就是定义AI奖励系统的方式；实际上，它可以简单到如此！
- en: 'A reward doesn''t have to be binary, however. It can be continuous. Consider
    the famous game of *Breakout*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，奖励不一定是二元的。它可以是连续的。考虑一下著名的*Breakout*游戏：
- en: '![](img/B14110_04_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_04_02.png)'
- en: 'Figure 2: The Breakout game'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Breakout游戏
- en: Imagine an AI playing this game. Try to work out what the reward would be in
    that case. It could simply be the score; more precisely, the score would be the
    accumulated reward over time in one game, and the rewards could be defined as the derivative
    of that score.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个AI在玩这个游戏。试着思考在这种情况下奖励是什么。它可能仅仅是得分；更准确地说，得分就是在一局游戏中随时间积累的奖励，而奖励可以定义为得分的**导数**。
- en: This is one of the many ways we could define a reward system for that game.
    Different AIs will have different reward structures; we will build five rewards
    systems for five different real-world applications in this book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可以为这个游戏定义奖励系统的众多方式之一。不同的AI将有不同的奖励结构；我们将在本书中为五个不同的实际应用构建五个奖励系统。
- en: 'With that in mind, remember this as well: the ultimate goal of the AI will
    always be to maximize the accumulated reward over time.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点：AI的最终目标总是**最大化**随时间积累的奖励。
- en: Those are the first two basic, but fundamental, principles of Artificial Intelligence
    as it exists today; the input and output system, and the reward. The next thing
    to consider is the AI environment.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是人工智能当前存在的前两个基本而根本的原则；输入输出系统和奖励。接下来要考虑的是AI环境。
- en: 'Principle #3 – The AI environment'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原则#3 – AI环境
- en: 'The third principle is what we call an "AI environment." It is a very simple
    framework where you define three things at each time (*t*):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个原则是我们所说的“AI环境”。它是一个非常简单的框架，你需要在每个时间点（*t*）定义三件事：
- en: The input (the state)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入（状态）
- en: The output (the action)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出（动作）
- en: The reward (the performance metric)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励（性能指标）
- en: For each and every single AI based on Reinforcement Learning that is built today,
    we always define an environment composed of the preceding elements. It is, however,
    important to understand that there are more than these three elements in a given
    AI environment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于今天构建的每一个基于强化学习的AI，我们始终定义一个由上述元素组成的环境。然而，理解一个AI环境中不仅仅有这三种元素是很重要的。
- en: For example, if you are building an AI to beat a car racing game, the environment
    will also contain the map and the gameplay of that game. Or, in the example of
    a self-driving car, the environment will also contain all the roads along which
    the AI is driving and the objects that surround those roads. But what you will
    always find in common when building any AI, are the three elements of state, action,
    and reward. The next principle, the Markov decision process, covers how they work
    in practice.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在构建一个AI来打败一款赛车游戏，那么环境还会包含该游戏的地图和游戏玩法。或者，在自动驾驶汽车的例子中，环境还会包含AI行驶的所有道路以及围绕这些道路的物体。但你在构建任何AI时，总是能找到的共同点就是状态、动作和奖励这三个元素。下一个原则，马尔可夫决策过程，讲述了它们在实践中是如何运作的。
- en: 'Principle #4 – The Markov decision process'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '原则 #4 – 马尔可夫决策过程'
- en: 'The Markov decision process, or MDP, is simply a process that models how the
    AI interacts with the environment over time. The process starts at *t* = 0, and
    then, at each next iteration, meaning at *t* = 1, *t* = 2, … *t* = *n* units of
    time (where the unit can be anything, for example, 1 second), the AI follows the
    same format of transition:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程，或MDP，简单来说，是一种模型，描述AI如何随着时间与环境互动。该过程从*t* = 0开始，然后在每个后续的迭代中，也就是在*t* =
    1，*t* = 2，… *t* = *n*单位时间时（单位可以是任何东西，例如1秒），AI遵循相同的过渡格式：
- en: The AI observes the current state, ![](img/B14110_04_001.png).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI观察当前状态，![](img/B14110_04_001.png)。
- en: The AI performs the action, ![](img/B14110_04_002.png).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI执行动作，![](img/B14110_04_002.png)。
- en: The AI receives the reward, ![](img/B14110_04_003.png).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI收到奖励，![](img/B14110_04_003.png)。
- en: The AI enters the following state, ![](img/B14110_04_004.png).
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI进入下一个状态，![](img/B14110_04_004.png)。
- en: 'The goal of the AI is always the same in Reinforcement Learning: it is to maximize
    the accumulated rewards over time, that is, the sum of all the ![](img/B14110_04_005.png)
    received at each transition.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中AI的目标始终是一样的：最大化随着时间积累的奖励总和，也就是在每次过渡时收到的所有![](img/B14110_04_005.png)的总和。
- en: 'The following graphic will help you visualize and remember an MDP better, the
    basis of Reinforcement Learning models:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示将帮助你更好地理解并记住MDP，它是强化学习模型的基础：
- en: '![](img/B14110_04_03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_04_03.png)'
- en: 'Figure 3: The Markov Decision process'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：马尔可夫决策过程
- en: Now four essential pillars are already shaping your intuition of AI. Adding
    a last important one completes the foundation of your understanding of AI. The
    last principle is training and inference; in training, the AI learns, and in inference, it predicts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，四个基本支柱已经在塑造你对AI的直觉。再加上最后一个重要的支柱，就完成了你对AI理解的基础。最后一个原则是训练与推理；在训练阶段，AI进行学习，而在推理阶段，AI进行预测。
- en: 'Principle #5 – Training and inference'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '原则 #5 – 训练与推理'
- en: The final principle you have to understand is the difference between training
    and inference. When building an AI, there is a time for the training mode, and
    a separate time for inference mode. I'll explain what that means starting with
    the training mode.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要理解的最后一个原则是训练和推理之间的区别。构建AI时，有一个阶段是训练模式，另一个阶段是推理模式。我将从训练模式开始解释这意味着什么。
- en: Training mode
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练模式
- en: Now you understand, from the three first principles, that the very first step
    of building an AI is to build an environment in which the input states, the output
    actions, and a system of rewards are clearly defined. From the fourth principle,
    you also understand that inside this environment we will build an AI to interact
    with it, trying to maximize the total reward accumulated over time.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经从前三个原则中理解到，构建AI的第一步是建立一个环境，在这个环境中，输入状态、输出动作和奖励系统需要明确定义。从第四个原则，你也能理解到，在这个环境中我们将构建一个AI来与之交互，尽力最大化随着时间积累的总奖励。
- en: To put it simply, there will be a preliminary (and long) period of time during
    which the AI will be trained to do that. That period of time is called the training;
    we can also say that the AI is in training mode. During that time, the AI tries
    to accomplish a certain goal over and over again until it succeeds. After each
    attempt, the parameters of the AI model are modified in order to do better at
    the next attempt.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，会有一个初步的（且漫长的）时间段，在这段时间里，AI将接受训练以完成目标。这个时间段被称为训练期；我们也可以说AI处于训练模式。在此期间，AI会不断尝试完成某个目标，直到成功为止。在每次尝试后，AI模型的参数都会被调整，以便下次表现得更好。
- en: 'For example, let''s say you''re building a self-driving car and you want it
    to go from point *A* to point *B*. Let''s also imagine that there are some obstacles
    that you want your self-driving car to avoid. Here is how the training process
    happens:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，假设你正在构建一辆自动驾驶汽车，你希望它从*Ａ*点开到*Ｂ*点。我们还假设存在一些障碍物，你希望自动驾驶汽车能够避开它们。训练过程如下：
- en: You choose an AI model, which can be Thompson Sampling (*Chapters 5* and *6*),
    Q-learning (*Chapters 7* and *8*), deep Q-learning (*Chapters 9*, *10,* and *11*)
    or even deep convolutional Q-learning (*Chapters* *12* and *13*).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择一个AI模型，可能是汤普森采样（*第5章*和*第6章*），Q学习（*第7章*和*第8章*），深度Q学习（*第9章*、*第10章*和*第11章*）甚至深度卷积Q学习（*第12章*和*第13章*）。
- en: You initialize the parameters of the model.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你初始化模型的参数。
- en: 'Your AI tries to go from *A* to *B* (by observing the states and performing
    its actions). During this first attempt, the closer it gets to *B*, the higher
    reward you give to the AI. If it fails reaching *B* or hits an obstacle, you give
    the AI a very bad reward. If it manages to reach *B* without hitting any obstacle,
    you give the AI an extremely good reward. It''s just like you would train a dog
    to sit: you give the dog a treat or say "good boy" (positive reward) if the dog sits.
    And you give the dog whatever small punishment you need to if the dog disobeys
    (negative reward). That process is training, and it works the same way in Reinforcement
    Learning.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的AI尝试从*Ａ*到*Ｂ*（通过观察状态并执行其行动）。在第一次尝试中，AI越接近*Ｂ*，你给它的奖励就越高。如果它没能到达*Ｂ*或者碰到障碍，你就给AI一个非常差的奖励。如果它成功到达*Ｂ*且没有撞到任何障碍，你就给它一个极好的奖励。这就像你训练一只狗坐下：如果狗坐下了，你给它奖励或说“好狗”（正面奖励）。如果狗不听话，你就给予它必要的惩罚（负面奖励）。这个过程就是训练，在强化学习中是一样的。
- en: At the end of the attempt (also called an episode), you modify the parameters
    of the model in order to do better next time. The parameters are modified intelligently,
    either iteratively through equations (Q-Learning), or by using Machine Learning
    and Deep Learning techniques such as stochastic gradient descent or backpropagation.
    All these techniques will be covered in this book.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在尝试结束时（也称为一个回合），你会调整模型的参数，以便下次做得更好。这些参数的修改是智能化的，可以通过迭代的方程（Q学习）来完成，或者使用机器学习和深度学习技术，如随机梯度下降或反向传播。所有这些技术将在本书中讲解。
- en: You repeat steps 3 and 4 again, and again, until you reach the desired performance;
    that is, until you have your fully non-dangerous autonomous car!
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要不断重复步骤3和4，直到达到理想的性能；也就是说，直到你拥有一辆完全安全的自动驾驶汽车！
- en: So, that's training. Now, how about inference?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是训练。那么，推理呢？
- en: Inference mode
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理模式
- en: Inference mode simply comes after your AI is fully trained and ready to perform
    well. It will simply consist of interacting with the environment by performing
    the actions to accomplish the goal the AI was trained to achieve before in training
    mode. In inference mode, no parameters are modified at the end of each episode.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 推理模式仅在你的AI完全训练好并准备好表现时开始。在推理模式下，AI只需通过执行行动来与环境互动，以完成在训练模式下曾经训练过的目标。在推理模式中，每个回合结束后，AI的参数不会被修改。
- en: For example, imagine you have an AI company that builds customized AI solutions
    for businesses, and one of your clients asked you to build an AI to optimize the
    flows in a smart grid. First, you'd enter an R&D phase during which you would
    train your AI to optimize these flows (training mode), and as soon as you reached
    a good level of performance, you'd deliver your AI to your client and go into
    production. Your AI would regulate the flows in the smart grid only by observing
    the current states of the grid and performing the actions it has been trained
    to do. That's inference mode.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，假设你拥有一家为企业提供定制AI解决方案的AI公司，且其中一个客户要求你开发一个AI来优化智能电网中的流量。首先，你会进入研发阶段，在这个阶段，你将训练AI来优化这些流量（训练模式），一旦达到了一个良好的性能水平，你就会将AI交付给客户并进入生产阶段。你的AI只通过观察电网的当前状态并执行它被训练过的动作来调节智能电网中的流量。这就是推理模式。
- en: Sometimes, the environment is subject to change, in which case you have to alternate
    fast between training and inference modes so that your AI can adapt to the new
    changes in the environment. An even better solution is to train your AI model
    every day, and go into inference mode with the most recently trained model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，环境可能会发生变化，这时你必须在训练模式和推理模式之间快速切换，以便让你的AI适应环境中的新变化。一个更好的解决方案是每天训练你的AI模型，并使用最新训练的模型进入推理模式。
- en: 'That was the last fundamental principle common to every AI. Congratulations
    – now you already have a solid basic understanding of Artificial Intelligence!
    Since you have that, you are ready to tackle your very first AI model in the next
    chapter: a simple yet very powerful one, still widely used today in business and
    marketing, to solve a problem that has the delightful name of the multi-armed
    bandit problem.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是每个人工智能（AI）共有的最后一个基本原则。恭喜你——现在你已经对人工智能有了坚实的基础理解！既然你掌握了这些，你已经准备好在下一章中处理你的第一个AI模型：一个简单但非常强大的模型，至今在商业和营销中仍被广泛使用，用来解决一个有着迷人名字的多臂赌博机问题。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned the five fundamental principles of Artificial
    Intelligence from a Reinforcement Learning perspective. Firstly, an AI is a system
    that takes an observation (values, images, or any data) as input, and returns
    an action to perform as output (principle #1). Then, there is a reward system
    that helps it measure its performance. The AI will learn through trial and error
    based on the reward it gets over time (principle #2). The input (state), the output
    (action), and the reward system define the AI environment (principle #3). The
    AI interacts with this environment through the Markov decision process (principle
    #4). Finally, in training mode, the AI learns how to maximize its total reward
    by updating its parameters through the iterations, and in inference mode, the
    AI simply performs its actions over full episodes without updating any of its
    parameters – that is to say, without learning (principle #5).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你从强化学习的角度学习了人工智能的五个基本原则。首先，AI是一个系统，它将观察（值、图像或任何数据）作为输入，并返回一个要执行的动作作为输出（原则#1）。然后，有一个奖励系统，帮助它衡量其性能。AI会通过试错法学习，根据它随着时间获得的奖励（原则#2）。输入（状态）、输出（动作）和奖励系统定义了AI环境（原则#3）。AI通过马尔可夫决策过程与这个环境互动（原则#4）。最后，在训练模式下，AI通过迭代更新其参数，学习如何最大化总奖励；而在推理模式下，AI只是执行其动作，贯穿整个过程而不更新任何参数——也就是说，不进行学习（原则#5）。
- en: In the next chapter, you will learn about Thompson Sampling, a simple Reinforcement
    Learning model, and use it to solve the multi-armed bandit problem.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习汤普森采样（Thompson Sampling），一个简单的强化学习模型，并使用它来解决多臂赌博机问题。
