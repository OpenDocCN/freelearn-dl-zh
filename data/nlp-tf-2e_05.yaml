- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Sentence Classification with Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行句子分类
- en: In this chapter, we will discuss a type of neural network known as **Convolutional
    Neural Networks** (**CNNs**). CNNs are quite different from fully connected neural
    networks and have achieved state-of-the-art performance in numerous tasks. These
    tasks include image classification, object detection, speech recognition, and
    of course, sentence classification. One of the main advantages of CNNs is that,
    compared to a fully connected layer, a convolution layer in a CNN has a much smaller
    number of parameters. This allows us to build deeper models without worrying about
    memory overflow. Also, deeper models usually lead to better performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一种叫做 **卷积神经网络**（**CNN**）的神经网络。CNN 与全连接神经网络有很大的不同，并且在许多任务中取得了最先进的性能。这些任务包括图像分类、物体检测、语音识别，当然还有句子分类。CNN
    的主要优势之一是，与全连接层相比，CNN 中的卷积层参数数量要小得多。这使得我们能够构建更深的模型，而不必担心内存溢出。此外，深层模型通常会带来更好的性能。
- en: 'We will introduce you to what a CNN is in detail by discussing different components
    found in a CNN and what makes CNNs different from their fully connected counterparts.
    Then we will discuss the various operations used in CNNs, such as the convolution
    and pooling operations, and certain hyperparameters related to these operations,
    such as filter size, padding, and stride. We will also look at some of the mathematics
    behind the actual operations. After establishing a good understanding of CNNs,
    we will look at the practical side of implementing a CNN with TensorFlow. First,
    we will implement a CNN to classify images and then use a CNN for sentence classification.
    Specifically, we’ll go through the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过讨论 CNN 中的不同组件以及使 CNN 与全连接神经网络不同的特性，详细介绍 CNN。接着，我们将讨论 CNN 中使用的各种操作，如卷积操作和池化操作，以及与这些操作相关的一些超参数，如滤波器大小、填充和步幅。我们还将看看实际操作背后的一些数学原理。在对
    CNN 有了充分的理解后，我们将探讨使用 TensorFlow 实现 CNN 的实际操作。首先，我们将实现一个 CNN 用于图像分类，然后使用 CNN 进行句子分类。具体来说，我们将通过以下几个主题：
- en: Learning the fundamentals of CNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 CNN 的基础知识
- en: Classifying images with CNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行图像分类
- en: Classifying sentences with CNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行句子分类
- en: Introducing CNNs
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 CNN
- en: In this section, you will learn about CNNs. Specifically, you will first get
    an understanding of the sort of operations present in a CNN, such as convolution
    layers, pooling layers, and fully connected layers. Next, we will briefly see
    how all of these are connected to form an end-to-end model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习 CNN。具体来说，你将首先理解 CNN 中存在的各种操作，如卷积层、池化层和全连接层。接下来，我们将简要了解这些操作是如何连接在一起形成一个端到端模型的。
- en: It is important to note that the first use case we’ll be solving with CNNs is
    an image classification task. CNNs were originally used to solve computer vision
    tasks and were adopted for NLP much later. Furthermore, CNNs have a stronger presence
    in the computer vision domain than the NLP domain, making it easier to explain
    the underlying concepts in a vision context. For this reason, we will first learn
    how CNNs are used in computer vision and then move on to NLP.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们将用 CNN 解决的第一个用例是图像分类任务。CNN 最初是用来解决计算机视觉任务的，后来才被应用于自然语言处理（NLP）。此外，CNN
    在计算机视觉领域的应用要比在 NLP 领域更为广泛，这使得在视觉上下文中解释其基本概念更加容易。因此，我们将首先学习 CNN 在计算机视觉中的应用，然后再转向
    NLP。
- en: CNN fundamentals
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN 基础知识
- en: Now, let’s explore the fundamental ideas behind a CNN without delving into too
    much technical detail. A CNN is a stack of layers, such as convolution layers,
    pooling layers, and fully connected layers. We will discuss each of these to understand
    their role in the CNN.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在不深入技术细节的情况下探讨 CNN 背后的基本思想。CNN 是一堆层的堆叠，包括卷积层、池化层和全连接层。我们将讨论每一层，以了解它们在
    CNN 中的作用。
- en: 'Initially, the input is connected to a set of convolution layers. These convolution
    layers slide a patch of weights (sometimes called the convolution window or filter)
    over the input and produce an output by means of the convolution operation. Convolution
    layers use a small number of weights, organized to cover only a small patch of
    input in each layer, unlike fully connected neural networks, and these weights
    are shared across certain dimensions (for example, the width and height dimensions
    of an image). Also, CNNs use the convolution operations to share the weights from
    the output by sliding this small set of weights along the desired dimension. What
    we ultimately get from this convolution operation is illustrated in *Figure 5.1*.
    If the pattern present in a convolution filter is present in a patch of image,
    the convolution will output a high value for that location; if not, it will output
    a low value. Also, by convolving the full image, we get a matrix indicating whether
    a pattern was present or not in a given location. Finally, we will get a matrix
    as the convolution output:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，输入连接到一组卷积层。这些卷积层通过卷积操作，滑动一个权重块（有时称为卷积窗口或滤波器）在输入上并产生输出。卷积层使用少量的权重，这些权重组织成每层仅覆盖输入的小块，这与全连接神经网络不同，这些权重在某些维度（例如图像的宽度和高度）上是共享的。此外，CNN使用卷积操作通过滑动这小部分权重沿着目标维度来共享输出的权重。通过这个卷积操作，我们最终得到的结果如*图
    5.1*所示。如果卷积滤波器中存在的模式在图像的小块中出现，卷积将为该位置输出一个较高的值；如果没有，它将输出一个较低的值。此外，通过对整个图像进行卷积，我们得到一个矩阵，表示在某个位置是否存在该模式。最终，我们会得到一个作为卷积输出的矩阵：
- en: '![CNN fundamentals](img/B14070_05_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CNN基础](img/B14070_05_01.png)'
- en: 'Figure 5.1: What the convolution operation does to an image'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：卷积操作对图像的作用
- en: 'Also, these convolution layers are optionally interleaved with pooling/subsampling
    layers, which reduces the dimensionality of the input. While reducing the dimensionality,
    we make the translation of CNNs invariant, as well as force the CNN to learn with
    less information, leading to better generalization and regularization of the model.
    The dimensionality is reduced by dividing the input into several patches and transforming
    each patch into a single element. For example, such transformations include picking
    the maximum element of a patch or averaging all the values in a patch. We will
    illustrate how pooling can make the translation of CNNs invariant in *Figure 5.2*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些卷积层可以与池化/子采样层交替使用，池化层减少了输入的维度。在减少维度的同时，我们使得卷积神经网络（CNN）的平移不变性得以保持，并且强迫CNN在较少的信息下进行学习，从而提高模型的泛化能力和正则化效果。通过将输入划分为多个小块并将每个小块转换为单个元素，我们可以减少维度。例如，这种转换包括选择一个小块中的最大元素或对一个小块中的所有值进行平均。我们将在*图
    5.2*中展示池化如何使CNN的平移不变性得以保持：
- en: '![CNN fundamentals](img/B14070_05_02.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CNN基础](img/B14070_05_02.png)'
- en: 'Figure 5.2: How the pooling operation helps to make data translation invariant'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：池化操作如何帮助使数据的平移不变
- en: Here, we have the original image and an image slightly translated on the *y*
    axis. We have convolution output for both images, and you can see that the value
    **10** appears at slightly different places in the convolution output. However,
    using max pooling (which takes the maximum value of each thick square), we can
    get the same output at the end. We will discuss these operations in detail later.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有原始图像和稍微在*y*轴上平移的图像。我们为这两幅图像计算了卷积输出，可以看到值**10**在卷积输出中的位置略有不同。然而，使用最大池化（它取每个厚方块的最大值），我们最终可以得到相同的输出。我们将在后续详细讨论这些操作。
- en: Finally, the output is fed to a set of fully connected layers, which then forward
    the output to the final classification/regression layer (for example, sentence/image
    classification). Fully connected layers contain a significant fraction of the
    total number of weights of the CNN, as convolution layers have a small number
    of weights. However, it has been found that CNNs perform better with fully connected
    layers than without them. This could be because convolution layers learn more
    localized features due to their small size, whereas fully connected layers provide
    a global picture of how these localized features should be connected together
    to produce a desirable final output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出被传递给一组全连接层，这些层将输出传递给最终的分类/回归层（例如，句子/图像分类）。全连接层包含了CNN总权重的大部分，因为卷积层的权重较少。然而，研究发现，CNN在有全连接层的情况下表现优于没有全连接层的情况。这可能是因为卷积层由于其较小的尺寸而学习到更多局部特征，而全连接层则提供了这些局部特征应该如何连接以产生理想输出的全局视图。
- en: '*Figure 5.3* shows a typical CNN used to classify images:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.3*展示了一个典型的CNN用于图像分类：'
- en: '![CNN fundamentals](img/B14070_05_03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CNN基础知识](img/B14070_05_03.png)'
- en: 'Figure 5.3: A typical CNN architecture'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：典型的CNN架构
- en: 'As is evident from the figure, CNNs, by design, preserve the spatial structure
    of the inputs during learning. In other words, for a two-dimensional input, a
    CNN will mostly have two-dimensional layers, whereas it will only have fully connected
    layers close to the output layer. Preserving the spatial structure allows CNNs
    to exploit valuable spatial information of the inputs and learn about inputs with
    fewer parameters. The value of spatial information is illustrated in *Figure 5.4*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以明显看出，CNN在设计上能够在学习过程中保持输入的空间结构。换句话说，对于二维输入，CNN通常会有二维的层，而接近输出层时则只有全连接层。保持空间结构使得CNN能够利用输入的宝贵空间信息，并以较少的参数学习输入。空间信息的价值在*图
    5.4*中得到了说明：
- en: '![CNN fundamentals](img/B14070_05_04.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CNN基础知识](img/B14070_05_04.png)'
- en: 'Figure 5.4: Unwrapping an image into a one-dimensional vector loses some of
    the important spatial information'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：将图像展开为一维向量会丧失一些重要的空间信息
- en: As you can see, when a two-dimensional image of a cat is unwrapped to be a one-dimensional
    vector, ears are no longer close to the eyes, and the nose is far away from the
    eyes as well. This means we have destroyed some of the useful spatial information
    during the unwrapping. This is why preserving the two-dimensional nature of the
    inputs is so important.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，当一张猫的二维图像被展开为一维向量时，耳朵不再靠近眼睛，鼻子也远离眼睛。这意味着我们在展开过程中破坏了一些有用的空间信息。这就是保持输入的二维特性如此重要的原因。
- en: The power of CNNs
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN的强大之处
- en: CNNs are a very versatile family of models and have shown a remarkable performance
    in many types of tasks. Such versatility is attributed to the ability of CNNs
    to perform feature extraction and learning simultaneously, leading to greater
    efficiency and generalizability. Let’s discuss a few examples of the utility of
    CNNs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）是一个非常多功能的模型家族，并在许多类型的任务中表现出了卓越的性能。这种多功能性归因于CNN能够同时执行特征提取和学习，从而提高了效率和泛化能力。让我们讨论几个CNN的应用实例。
- en: In the **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) 2020,
    which involved classifying images, detecting objects, and localizing objects in
    an image, CNNs were used to achieve incredible test accuracies. For example, for
    image-classification tasks, its top-1 test accuracy was approximately 90% for
    1,000 different object classes, which means that the CNN was able to correctly
    identify around 900 different objects correctly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）2020中，CNN被用于图像分类、物体检测和物体定位任务，并取得了惊人的测试准确率。例如，在图像分类任务中，CNN的Top-1测试准确率约为90%，涵盖1,000个不同的物体类别，这意味着CNN能够正确识别大约900个不同的物体。
- en: CNNs also have been used for image segmentation. Image segmentation involves
    segmenting an image into different areas. For example, in an urbanscape image
    that includes buildings, a road, vehicles, and passengers, isolating the road
    from the buildings is a segmentation task. Moreover, CNNs have made incredible
    strides, demonstrating their performance in NLP tasks such as sentence classification,
    text generation, and machine translation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 也被用于图像分割。图像分割是指将图像划分为不同的区域。例如，在包含建筑物、道路、车辆和乘客的城市景观图像中，将道路从建筑物中隔离出来就是一个分割任务。此外，CNN
    在自然语言处理（NLP）任务中也取得了显著进展，展现了其在句子分类、文本生成和机器翻译等任务中的表现。
- en: Understanding CNNs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积神经网络（CNN）
- en: Now that we understand the high level concepts governing CNNs, let’s walk through
    the technical details of a CNN. First, we will discuss the convolution operation
    and introduce some terminology, such as filter size, stride, and padding. In brief,
    **filter size** refers to the window size of the convolution operation, **stride**
    refers to the distance between two movements of the convolution window, and **padding**
    refers to the way you handle the boundaries of the input. We will also discuss
    an operation that is known as deconvolution or transposed convolution. Then we
    will discuss the details of the pooling operation. Finally, we will discuss how
    to add fully connected layers, which produce the classification or regression
    output.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了卷积神经网络的高层次概念，让我们来深入了解 CNN 的技术细节。首先，我们将讨论卷积操作并介绍一些术语，比如滤波器大小、步长和填充。简而言之，**滤波器大小**指的是卷积操作的窗口大小，**步长**指的是卷积窗口每次移动的距离，**填充**则指的是处理输入边界的方式。我们还将讨论一种叫做反卷积或转置卷积的操作。然后，我们将讨论池化操作的细节。最后，我们将讨论如何添加全连接层，以生成分类或回归输出。
- en: Convolution operation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积操作
- en: In this section, we will discuss the convolution operation in detail. First,
    we will discuss the convolution operation without stride and padding, then we
    will describe the convolution operation with stride, and then we will discuss
    the convolution operation with padding. Finally, we will discuss something called
    transposed convolution. For all the operations in this chapter, we consider the
    index starting from one, and not from zero.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细讨论卷积操作。首先，我们将讨论没有步长和填充的卷积操作，然后描述有步长的卷积操作，接着讨论有填充的卷积操作。最后，我们将讨论一种叫做转置卷积的操作。对于本章中的所有操作，我们假设索引从1开始，而不是从0开始。
- en: Standard convolution operation
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准卷积操作
- en: 'The convolution operation is a central part of CNNs. For an input of size ![](img/B14070_05_001.png)
    and a weight patch (also known as a *filter* or a *kernel*) of ![](img/B14070_05_002.png),
    where ![](img/B14070_05_003.png), the convolution operation slides the patch of
    weights over the input. Let’s denote the input by `X`, the patch of weights by
    `W`, and the output by `H`. Also, at each location *i, j*, the output is calculated
    as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作是卷积神经网络（CNN）的核心部分。对于一个大小为 ![](img/B14070_05_001.png) 的输入和一个权重块（也称为*滤波器*或*卷积核*）
    ![](img/B14070_05_002.png)，其中 ![](img/B14070_05_003.png)，卷积操作将权重块滑动到输入上。我们用 `X`
    表示输入，`W` 表示权重块，`H` 表示输出。此外，在每个位置 *i, j*，输出按如下公式计算：
- en: '![](img/B14070_05_004.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_004.png)'
- en: 'Here, *x* [i,j], *w* [i,j], and *h*[i,j] denote the value at the *(i,j)*^(th)
    location of *X*, *W*, and *H*, respectively. As already shown by the equation,
    though the input size is ![](img/B14070_05_001.png), the output in this case will
    be ![](img/B14070_05_006.png). Also, *m* is known as the filter size. This means
    the width and height of the output will be slightly less than of the original.
    Let’s look at this through a visualization (see *Figure 5.5*):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x* [i,j]、*w* [i,j] 和 *h*[i,j] 分别表示 *X*、*W* 和 *H* 在 *(i,j)* 位置的值。如方程所示，尽管输入大小为
    ![](img/B14070_05_001.png)，但在这种情况下输出的大小将是 ![](img/B14070_05_006.png)。此外，*m* 被称为滤波器大小。这意味着输出的宽度和高度将略小于原始输入。让我们通过可视化来看这个问题（见
    *图 5.5*）：
- en: '![Standard convolution operation](img/B14070_05_05.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![标准卷积操作](img/B14070_05_05.png)'
- en: 'Figure 5.5: The convolution operation with a filter size (m) = 3, stride =
    1, and no padding'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：卷积操作，滤波器大小 (m) = 3，步长 = 1，并且没有填充
- en: '**Note**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: The output produced by the convolution operation (the rectangle at the top in
    *Figure 5.5*) is sometimes called a **features map**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作产生的输出（*图 5.5* 上方的矩形）有时被称为**特征图**。
- en: Next let’s discuss the stride parameter in convolution.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们讨论卷积中的步长参数。
- en: Convolving with stride
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带步长的卷积
- en: In the preceding example, we shifted the filter by a single step. However, this
    is not mandatory; we can take large steps or strides while convolving the input.
    Therefore, the size of the step is known as the stride.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们通过一步进行滤波器的移动。然而，这并不是强制性的；我们可以在卷积输入时采用较大的步长或步幅。因此，步长的大小被称为步幅。
- en: 'Let’s modify the previous equation to include the *s* [i] and *s* [j] strides:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改之前的公式，加入*s* [i]和*s* [j]步幅：
- en: '![](img/B14070_05_007.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_007.png)'
- en: '![](img/B14070_05_007.1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_007.1.png)'
- en: 'In this case, the output will be smaller as the size of *s*[i] and *s*[j] increases.
    Comparing *Figure 5.5* (*stride = 1*) and *Figure 5.6* (*stride = 2*) illustrates
    the effect of different strides:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，随着*s*[i]和*s*[j]增大，输出会变小。对比*图 5.5*（*步幅 = 1*）和*图 5.6*（*步幅 = 2*）可以说明不同步幅的效果：
- en: '![Convolving with stride](img/B14070_05_06.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![使用步幅进行卷积](img/B14070_05_06.png)'
- en: 'Figure 5.6: The convolution operation with a filter size (m) = 2, stride =
    2, and no padding'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：滤波器大小（m）= 2，步幅 = 2，并且没有填充的卷积操作
- en: As you can see, doing convolution with stride helps to reduce the dimensionality
    of the input similar to a pooling layer. Therefore, sometimes convolution with
    stride is used instead of pooling in the CNNs as it reduces the computational
    complexity. Also note that the dimensionality reduction achieved by stride can
    be tuned or controlled as opposed to the inherent dimensionality reduction from
    the standard convolution operation. We will now discuss another important concept
    in convolution known as padding.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用步幅进行卷积有助于像池化层一样减少输入的维度。因此，有时在卷积神经网络（CNN）中，卷积操作与步幅结合使用，代替池化操作，因为它能减少计算复杂度。还需注意，步幅所实现的维度减小可以进行调整或控制，而标准卷积操作的维度减小是固有的。接下来，我们将讨论卷积中另一个重要的概念——填充。
- en: Convolving with padding
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积与填充
- en: 'The inevitable output size reduction resulting from each convolution (without
    stride) is an undesirable property. This greatly limits the number of layers we
    can have in a network. Also, it is known that deeper networks perform better than
    shallow networks. This should not be confused with the dimensionality reduction
    achieved by stride, as this is a design choice and we can decide to have a stride
    of 1 if necessary. Therefore, padding is used to circumvent this issue. This is
    achieved by padding zeros to the boundary of the input so that the output size
    and the input size are equal. Let’s assume a stride of 1:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每次卷积操作（没有步幅）不可避免地会导致输出尺寸的减小，这是一个不希望出现的属性。这大大限制了网络中可以使用的层数。另外，已知较深的网络比浅层网络表现更好。需要注意的是，这不应与通过步幅实现的维度减小混淆，因为步幅是一个设计选择，如果需要，我们可以决定使用步幅为1。因此，填充被用来绕过这个问题。实现方法是将零填充到输入的边界，使得输出尺寸与输入尺寸相等。假设步幅为1：
- en: '![](img/B14070_05_008.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_008.png)'
- en: 'Here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![](img/B14070_05_009.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_009.png)'
- en: '*Figure 5.7* depicts the result of the padding:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.7* 展示了填充的结果：'
- en: '![Convolving with padding](img/B14070_05_07.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![卷积与填充](img/B14070_05_07.png)'
- en: 'Figure 5.7: The convolution operation with a filter size (m=3), stride (s=1),
    and zero padding'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：滤波器大小（m=3），步幅（s=1），以及零填充的卷积操作
- en: We will now discuss the transposed convolution operation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论转置卷积操作。
- en: Transposed convolution
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转置卷积
- en: Though the convolution operation looks complicated in terms of mathematics,
    it can be simplified to a matrix multiplication. For this reason, we can define
    the transpose of the convolution operation or **deconvolution**, as it is sometimes
    called. However, we will use the term **transposed convolution** as it sounds
    more natural. In addition, deconvolution refers to a different mathematical concept.
    The transposed convolution operation plays an important role in CNNs for the reverse
    accumulation of the gradients during backpropagation. Let’s go through an example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卷积操作在数学上看起来很复杂，但它可以简化为矩阵乘法。因此，我们可以定义卷积操作的转置，或者有时称为**反卷积**。然而，我们将使用**转置卷积**这一术语，因为它听起来更自然。此外，反卷积指的是一个不同的数学概念。转置卷积操作在卷积神经网络（CNN）中起着重要作用，用于反向传播过程中梯度的反向累积。我们将通过一个例子来解释。
- en: For an input of size ![](img/B14070_05_001.png) and a weight patch, or filter,
    of ![](img/B14070_05_002.png), where ![](img/B14070_05_003.png), the convolution
    operation slides the patch of weights over the input. Let’s denote the input by
    *X*, the patch of weights by *W*, and the output by *H*. The output *H* can be
    calculated as a matrix multiplication as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大小为 ![](img/B14070_05_001.png) 的输入和大小为 ![](img/B14070_05_002.png) 的权重块或滤波器，其中
    ![](img/B14070_05_003.png)，卷积操作将权重块滑动在输入上。我们将输入表示为 *X*，权重块表示为 *W*，输出表示为 *H*。输出
    *H* 可以通过以下矩阵乘法计算：
- en: 'Let’s assume ![](img/B14070_05_013.png) and ![](img/B14070_05_014.png) for
    clarity and unwrap the input *X* from left to right, top to bottom, resulting
    in this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/B14070_05_013.png) 和 ![](img/B14070_05_014.png) 为了清晰起见，我们从左到右、从上到下展开输入
    *X*，得到如下结果：
- en: '![](img/B14070_05_015.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_015.png)'
- en: 'Let’s define a new matrix *A* from *W*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 *W* 定义一个新矩阵 *A*：
- en: '![](img/B14070_05_016.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_016.png)'
- en: 'Then, if we perform the following matrix multiplication, we obtain *H*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果我们执行以下矩阵乘法，我们得到 *H*：
- en: '![](img/B14070_05_017.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_017.png)'
- en: Now, by reshaping the output ![](img/B14070_05_018.png) to ![](img/B14070_05_019.png)
    we obtain the convolved output. Now let’s project this result back to *n* and
    *m*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过将输出 ![](img/B14070_05_018.png) 重塑为 ![](img/B14070_05_019.png)，我们得到卷积输出。现在让我们将这个结果投影回
    *n* 和 *m*。
- en: By unwrapping the input ![](img/B14070_05_020.png) to ![](img/B14070_05_021.png)
    and by creating a matrix ![](img/B14070_05_022.png) from *w*, as we showed earlier,
    we obtain ![](img/B14070_05_023.png), which will then be reshaped to ![](img/B14070_05_024.png).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展开输入 ![](img/B14070_05_020.png) 到 ![](img/B14070_05_021.png)，并通过创建矩阵 ![](img/B14070_05_022.png)
    从 *w*，如我们之前所示，我们得到 ![](img/B14070_05_023.png)，然后将其重塑为 ![](img/B14070_05_024.png)。
- en: 'Next, to obtain the transposed convolution, we simply transpose *A* and arrive
    at the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了获得转置卷积，我们只需转置 *A* 并得到如下结果：
- en: '![](img/B14070_05_025.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_025.png)'
- en: Here, ![](img/B14070_05_026.png) is the resultant output of the transposed convolution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B14070_05_026.png) 是转置卷积的结果输出。
- en: We end our discussion about the convolution operation here. We discussed the
    convolution operation, convolution operation with stride, convolution operation
    with padding, and how to calculate the transposed convolution. Next, we will discuss
    the pooling operation in more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里结束卷积操作的讨论。我们讨论了卷积操作、带步幅的卷积操作、带填充的卷积操作以及如何计算转置卷积。接下来，我们将更详细地讨论池化操作。
- en: Pooling operation
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化操作
- en: The pooling operation, which is sometimes known as the subsampling operation,
    was introduced to CNNs mainly for reducing the size of the intermediate outputs
    as well as for making the translation of CNNs invariant. This is preferred over
    the natural dimensionality reduction caused by convolution without padding, as
    we can decide where to reduce the size of the output with the pooling layer, in
    contrast to forcing it to happen every time. Forcing the dimensionality to decrease
    without padding would strictly limit the number of layers we can have in our CNN
    models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作，有时也称为子采样操作，主要是为了减少卷积神经网络（CNN）中间输出的大小，并使得CNN具有平移不变性。与没有填充的卷积引起的自然维度缩减相比，池化操作更为可取，因为我们可以通过池化层来决定输出的大小缩减位置，而不是每次都强制发生。没有填充的情况下强制维度减小会严格限制我们在CNN模型中能使用的层数。
- en: 'We define the pooling operation mathematically in the following sections. More
    precisely, we will discuss two types of pooling: max pooling and average pooling.
    First, however, we will define the notation. For an input of size ![](img/B14070_05_001.png)
    and a kernel (analogous to the filter of a convolution layer) of size ![](img/B14070_05_002.png),
    where ![](img/B14070_05_003.png), the convolution operation slides the patch of
    weights over the input. Let’s denote the input by *X*, the patch of weights by
    *W*, and the output by *H*. Then let us use, *x* [i,j], *w*[i,j], and *h*[i,j]
    to denote the value at the (*i*,*j*)^(th) location of *X*, *W*, and *H*, respectively.
    We will now look at specific implementations of pooling commonly used.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中数学定义池化操作。更准确地说，我们将讨论两种类型的池化：最大池化和平均池化。然而，首先我们将定义符号。对于大小为 ![](img/B14070_05_001.png)
    的输入和大小为 ![](img/B14070_05_002.png) 的卷积核（类似于卷积层的滤波器），其中 ![](img/B14070_05_003.png)，卷积操作将权重块滑动在输入上。我们将输入表示为
    *X*，权重块表示为 *W*，输出表示为 *H*。然后我们使用 *x* [i,j]、*w*[i,j] 和 *h*[i,j] 来表示 *X*、*W* 和 *H*
    中（*i*，*j*）位置的值。接下来，我们将讨论常用的池化实现。
- en: Max pooling
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大池化
- en: 'The max pooling operation picks the maximum element within the defined kernel
    of an input to produce the output. The max pooling operation shifts are windows
    over the input (the middle squares in *Figure 5.8*) and take the maximum at each
    time. Mathematically, we define the pooling equation as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化操作从输入的定义卷积核内选择最大元素生成输出。最大池化操作通过窗口滑动（*图 5.8* 中的中间方块），每次取最大值。数学上，我们将池化公式定义如下：
- en: '![](img/B14070_05_030.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_030.png)'
- en: '![](img/B14070_05_030.1.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_030.1.png)'
- en: '*Figure 5.8* shows this operation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.8* 显示了该操作：'
- en: '![Max pooling](img/B14070_05_08.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![最大池化](img/B14070_05_08.png)'
- en: 'Figure 5.8: The max pooling operation with a filter size of 3, stride of 1,
    and no padding'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：滤波器大小为 3，步长为 1，且无填充的最大池化操作
- en: Next, let’s discuss how to perform max pooling with stride.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何进行带步长的最大池化。
- en: Max pooling with stride
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带步长的最大池化
- en: 'Max pooling with stride is similar to convolution with stride. Here is the
    equation:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 带步长的最大池化与带步长的卷积相似。其公式如下：
- en: '![](img/B14070_05_031.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_031.png)'
- en: '![](img/B14070_05_031.1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_031.1.png)'
- en: '*Figure 5.9* shows the result:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.9* 显示了结果：'
- en: '![Max pooling with stride](img/B14070_05_09.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![带步长的最大池化](img/B14070_05_09.png)'
- en: 'Figure 5.9: The max pooling operation for an input of size (n=4) with a filter
    size of (m=2), stride (s=2), and no padding'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：对大小为 (n=4) 的输入进行最大池化操作，滤波器大小为 (m=2)，步长 (s=2)，且无填充
- en: We will discuss another variant of pooling known as average pooling, below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讨论另一种池化变体——平均池化。
- en: Average pooling
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均池化
- en: 'Average pooling works similar to max pooling, except that instead of only taking
    the maximum, the average of all the inputs falling within the kernel is taken.
    Consider the following equation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 平均池化与最大池化类似，不同之处在于它不仅取最大值，而是取所有落入卷积核内输入的平均值。考虑以下方程：
- en: '![](img/B14070_05_033.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_033.png)'
- en: 'The average pooling operation is shown in *Figure 5.10*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 平均池化操作如 *图 5.10* 所示：
- en: '![Average pooling](img/B14070_05_10.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![平均池化](img/B14070_05_10.png)'
- en: 'Figure 5.10: The average pooling operation for an input of size (n=4) with
    a filter size of (m=2), stride (s=1), and no padding'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：对大小为 (n=4) 的输入进行平均池化操作，滤波器大小为 (m=2)，步长 (s=1)，且无填充
- en: We have so far discussed the operations directly performed on the two-dimensional
    inputs like images. Next we will discuss how they are connected to one-dimensional
    fully connected layers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了直接对二维输入（如图像）执行的操作。接下来我们将讨论它们如何与一维的全连接层连接。
- en: Fully connected layers
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接层
- en: Fully connected layers are a fully connected set of weights from the input to
    the output. These fully connected weights are able to learn global information
    as they are connected from each input to each output. Also, having such layers
    of full connectedness allows us to combine features learned by the convolution
    layers preceding the fully connected layers, globally, to produce meaningful outputs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层是从输入到输出的完全连接的权重集合。这些全连接的权重能够学习全局信息，因为它们从每个输入连接到每个输出。而且，拥有这样的完全连接层使我们能够将前面卷积层学到的特征全局结合起来，生成有意义的输出。
- en: Let’s define the output of the last convolution or pooling layer to be of size
    ![](img/B14070_05_034.png), where *p* is the height of the input, *o* is the width
    of the input, and *d* is the depth of the input. As an example, think of an RGB
    image, which will have a fixed height, fixed width, and a depth of 3 (one depth
    channel for each RGB component).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义最后一个卷积或池化层的输出大小为 ![](img/B14070_05_034.png)，其中 *p* 是输入的高度，*o* 是输入的宽度，*d*
    是输入的深度。举个例子，考虑一个 RGB 图像，其高度和宽度是固定的，深度为 3（每个 RGB 组件都有一个深度通道）。
- en: 'Then, for the initial fully connected layer found immediately after the last
    convolution or pooling layer, the weight matrix will be ![](img/B14070_05_035.png),
    where *height* x *width* x *depth* of the layer output is the number of output
    units produced by that last layer and *m* is the number of hidden units in the
    fully connected layer. Then, during inference (or prediction), we reshape the
    output of the last convolution/pooling layer to be of size ![](img/B14070_05_036.png)
    and perform the following matrix multiplication to obtain *h*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于紧接在最后一个卷积或池化层后的初始全连接层，权重矩阵将是 ![](img/B14070_05_035.png)，其中层输出的*高度* x *宽度*
    x *深度* 是该最后一层产生的输出单元数量，*m* 是全连接层中隐藏单元的数量。然后，在推理（或预测）过程中，我们将最后一个卷积/池化层的输出重新调整为大小为
    ![](img/B14070_05_036.png)，并执行以下矩阵乘法以获得 *h*：
- en: '![](img/B14070_05_037.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_037.png)'
- en: The resultant fully connected layers will behave as in a fully connected neural
    network, where you have several fully connected layers and an output layer. The
    output layer can be a softmax classification layer for a classification problem
    or a linear layer for a regression problem.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果全连接层的行为就像一个全连接神经网络，其中有多个全连接层和一个输出层。输出层可以是一个用于分类问题的 softmax 分类层，或者一个用于回归问题的线性层。
- en: Putting everything together
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将一切结合起来
- en: Now we will discuss how the convolutional, pooling, and fully connected layers
    come together to form a complete CNN.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论卷积层、池化层和全连接层如何结合在一起形成一个完整的 CNN。
- en: As shown in *Figure 5.11*, the convolution, pooling, and fully connected layers
    come together to form an end-to-end learning model that takes raw data, which
    can be high-dimensional (for example, RGB images) and produce meaningful output
    (for example, the class of the object). First, the convolution layers learn the
    spatial features of the images.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.11*所示，卷积层、池化层和全连接层结合在一起，形成一个端到端的学习模型，该模型接受原始数据（可以是高维的，例如 RGB 图像），并产生有意义的输出（例如物体的类别）。首先，卷积层学习图像的空间特征。
- en: 'The lower convolution layers learn low-level features such as differently oriented
    edges present in the images, and the higher layers learn more high-level features
    such as shapes present in the images (for example, circles and triangles) or bigger
    parts of an object (for example, the face of a dog, tail of a dog, and front section
    of a car). The pooling layers in the middle make each of these learned features
    slightly translation invariant. This means that, in a new image, even if the feature
    appears a bit offset compared to the location in which the feature appeared in
    the learned images, the CNN will still recognize that feature. Finally, the fully
    connected layers combine the high-level features learned by the CNN to produce
    global representations that will be used by the final output layer to determine
    the class the object belongs to:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的卷积层学习低级特征，如图像中不同方向的边缘，而较高的层学习更高级的特征，如图像中出现的形状（例如，圆形和三角形）或物体的更大部分（例如，狗的脸、狗的尾巴和汽车的前部）。中间的池化层使这些学习到的特征稍微具有平移不变性。这意味着，在新图像中，即使该特征相对于在学习图像中出现的位置稍微偏移，CNN
    仍然能够识别该特征。最后，全连接层将 CNN 学到的高级特征结合起来，生成全局表示，这些表示将由最终输出层用于确定物体属于哪个类别：
- en: '![Putting everything together](img/B14070_05_11.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![将一切结合起来](img/B14070_05_11.png)'
- en: 'Figure 5.11: Combining convolution layers, pooling layers, and fully connected
    layers to form a CNN'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：结合卷积层、池化层和全连接层形成 CNN
- en: 'With a strong conceptual understanding of a CNN, we will now get started on
    our first use case: classifying images with a CNN model.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 CNN 有了强烈的概念理解后，我们现在将开始我们的第一个用例：使用 CNN 模型进行图像分类。
- en: Exercise – image classification on Fashion-MNIST with CNN
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习 – 使用 CNN 对 Fashion-MNIST 进行图像分类
- en: This will be our first example of using a CNN for a real-world machine learning
    task. We will classify images using a CNN. The reason for not starting with an
    NLP task is that applying CNNs to NLP tasks (for example, sentence classification)
    is not very straightforward. There are several tricks involved in using CNNs for
    such a task. However, originally, CNNs were designed to cope with image data.
    Therefore, let’s start there, and then find our way through to see how CNNs apply
    to NLP tasks in the *Using CNNs for sentence classification* section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是我们第一次使用 CNN 进行实际机器学习任务的示例。我们将使用 CNN 对图像进行分类。不从 NLP 任务开始的原因是，应用 CNN 于 NLP
    任务（例如，句子分类）并不是非常直接。使用 CNN 处理此类任务需要一些技巧。然而，CNN 最初是为应对图像数据而设计的。因此，我们从这里开始，然后逐步探索
    CNN 如何应用于 NLP 任务的 *使用 CNN 进行句子分类* 部分。
- en: About the data
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据
- en: 'In this exercise, we will use a dataset well-known in the computer vision community:
    the Fashion-MNIST dataset. Fashion-MNIST was inspired by the famous MNIST dataset
    ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)). MNIST
    is a database of labeled images of handwritten digits from 0 to 9 (i.e. 10 digits).
    However, due to the simplicity of the MNIST image classification task, test accuracy
    on MNIST is just shy of 100%. At the time of writing, the popular research benchmarking
    site *paperswithcode.com* has published a test accuracy of 99.87% ([https://paperswithcode.com/sota/image-classification-on-mnist](https://paperswithcode.com/sota/image-classification-on-mnist)).
    Because of this, Fashion-MNIST came to life.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用一个在计算机视觉社区中广为人知的数据集：Fashion-MNIST 数据集。Fashion-MNIST 受到著名的 MNIST
    数据集的启发（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）。MNIST
    是一个包含手写数字（0到9，即 10 个数字）标签图像的数据库。然而，由于 MNIST 图像分类任务的简单性，MNIST 的测试准确率几乎接近 100%。截至本文撰写时，流行的研究基准网站
    *paperswithcode.com* 发布了 99.87% 的测试准确率（[https://paperswithcode.com/sota/image-classification-on-mnist](https://paperswithcode.com/sota/image-classification-on-mnist)）。正因如此，Fashion-MNIST
    应运而生。
- en: 'Fashion-MNIST consists of images of clothing garments. Our task is to classify
    each garment into a category (e.g. dress, t-shirt). The dataset contains two sets:
    the training set, and the test set. We will train on the training set and evaluate
    the performance of our model on the unseen test dataset. We will further split
    the training set into two sets: training and validation sets. We will use the
    validation dataset as a continuous performance monitoring mechanism for our model.
    We will discuss the details later, but we will see that we can reach up to approximately
    88% test accuracy without any special regularization or tricks.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion-MNIST 包含衣物图像。我们的任务是将每件衣物分类到一个类别中（例如：连衣裙、T恤）。该数据集包含两个数据集：训练集和测试集。我们将在训练集上进行训练，并在未见过的测试数据集上评估模型的性能。我们还将把训练集分成两个部分：训练集和验证集。我们将使用验证数据集作为模型的持续性能监测机制。我们稍后会详细讨论，但我们会看到，通过简单的训练，模型可以达到大约
    88% 的测试准确率，而无需任何特殊的正则化或技巧。
- en: Downloading and exploring the data
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和探索数据
- en: 'The very first task will be to download and explore the data. To download the
    data, we will simply tap into the `tf.keras.datasets` module, as it provides several
    datasets to be downloaded conveniently through TensorFlow. To see what other datasets
    are available, visit [https://www.tensorflow.org/api_docs/python/tf/keras/datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets).
    The full code for this chapter is available in `ch5_image_classification_fashion_mnist.ipynb`
    in the `Ch05-Sentence-Classification` folder. Simply call the following function
    to download the data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是下载并探索数据。为了下载数据，我们将直接使用 `tf.keras.datasets` 模块，因为它提供了多个数据集，能够通过 TensorFlow
    方便地进行下载。要查看其他可用的数据集，请访问 [https://www.tensorflow.org/api_docs/python/tf/keras/datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)。本章的完整代码位于
    `Ch05-Sentence-Classification` 文件夹中的 `ch5_image_classification_fashion_mnist.ipynb`
    文件里。只需调用以下函数即可下载数据：
- en: '[PRE0]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data will be downloaded to a default cache directory specified by TensorFlow
    (for example: `~/.keras/dataset/fasion_minst`).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据将被下载到 TensorFlow 指定的默认缓存目录（例如：`~/.keras/dataset/fasion_minst`）。
- en: 'We will then see the sizes of the data by printing their shapes:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将通过打印数据的形状来看数据的大小：
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will produce:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生：
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can see that we have 60,000 training images, each of size 28x28, and 10,000
    testing images of the same dimensions. The labels are simple class IDs ranging
    from 0 to 9\. We will also create a variable to contain the class ID to class
    name mapping, which will help us during explorations and post-training analysis:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们有 60,000 张训练图像，每张大小为 28x28，还有 10,000 张相同尺寸的测试图像。标签是简单的类别 ID，范围从 0 到
    9。我们还将创建一个变量来包含类别 ID 到类别名称的映射，这将在探索和训练后分析中帮助我们：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can also plot the images, which will give the following plot of images (*Figure
    5.12*):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制图像，这将生成如下的图像图表（*图 5.12*）：
- en: '![](img/B14070_05_12.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_12.png)'
- en: 'Figure 5.12: An overview of the images found in the Fashion-MNIST dataset'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：Fashion-MNIST 数据集中图像的概览
- en: Finally, we are going to extend `train_images` and `test_images` by adding a
    new dimension (of size 1) to the end of each tensor. Standard implementation of
    the convolution operation in TensorFlow is designed to work on a four-dimensional
    input (i.e. batch, height, width, and channel dimensions).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过在每个张量的末尾添加一个新的维度（大小为 1）来扩展 `train_images` 和 `test_images`。TensorFlow
    中卷积操作的标准实现是针对四维输入设计的（即批次、高度、宽度和通道维度）。
- en: 'Here, the channel dimension is omitted in the images as they are black and
    white images. Therefore, to comply with the dimensional requirement of TensorFlow’s
    convolution operation, we add this additional dimension to the images. This is
    a necessity for using the convolution operation in CNNs. You can do this as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，图像中省略了通道维度，因为它们是黑白图像。因此，为了符合 TensorFlow 卷积操作的维度要求，我们需要在图像中添加这一额外的维度。这是使用
    CNN 中卷积操作的必要条件。你可以按如下方式进行：
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using the indexing and slicing capabilities available in NumPy, you can simply
    add a `None` dimension to the tensor when indexing as above. Let’s now check the
    shapes of the tensors:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NumPy 提供的索引和切片功能，你可以像上面那样简单地通过在索引时添加 `None` 维度来给张量添加新的维度。现在我们来检查张量的形状：
- en: '[PRE5]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这会得到：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s have a crack at implementing a CNN model that can learn from this data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试实现一个可以从这些数据中学习的 CNN 模型。
- en: Implementing the CNN
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 CNN
- en: 'In this subsection, we will look at some important code snippets from the TensorFlow
    implementation of the CNN. The full code is available in `ch5_image_classification_mnist.ipynb`
    in the `Ch05-Sentence-Classification` folder. First, we will define several important
    hyperparameters. The code comments are self-explanatory for the purpose of these
    hyperparameters:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将查看 TensorFlow 实现 CNN 时的一些重要代码片段。完整的代码可在 `Ch05-Sentence-Classification`
    文件夹中的 `ch5_image_classification_mnist.ipynb` 文件中找到。首先，我们将定义一些重要的超参数。代码注释已经自解释，这些超参数的作用如下：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With that, we can start to implement the model. We will find inspiration from
    one of the earliest CNN models, known as LeNet, introduced in the paper *Gradient-Based
    Learning Applied to Document Recognition* by LeCun et al. ([http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)).
    This model will be a great start as it is a simple model yet gives a reasonably
    good performance on the dataset. We will introduce some slight modifications to
    the original model, because the original model operated on a 32x32-sized image,
    whereas in our case, the image is a 28x28-sized image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以开始实现模型了。我们将从最早期的 CNN 模型之一 LeNet 获取灵感，LeNet 在 LeCun 等人的论文《基于梯度的学习应用于文档识别》中提出（[http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)）。这个模型是一个很好的起点，因为它虽然简单，但在数据集上能够取得相当不错的表现。我们将对原始模型做一些微小的修改，因为原始模型处理的是
    32x32 尺寸的图像，而在我们的案例中，图像是 28x28 尺寸的。
- en: 'Let’s go through some quick details of the model. It has the following sequence
    of layers:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下模型的细节。它具有以下层序列：
- en: A convolutional layer with a 5x5 kernel, 1x1 stride, and valid padding
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 5x5 卷积核、1x1 步幅和有效填充的卷积层
- en: A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 2x2 卷积核、2x2 步幅和有效池化的最大池化层
- en: A convolutional layer with a 5x5 kernel, 1x1 stride, and valid pooling
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 5x5 卷积核、1x1 步幅和有效池化的卷积层
- en: A max pooling layer with a 2x2 kernel, 2x2 stride, and valid pooling
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 2x2 卷积核、2x2 步幅和有效池化的最大池化层
- en: A convolutional layer with a 4x4 kernel, 1x1 stride, and valid pooling
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 4x4 卷积核、1x1 步幅和有效池化的卷积层
- en: A layer that flattens the 2D output to a 1D vector
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将 2D 输出展平为 1D 向量的层
- en: A Dense layer with 84 nodes
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 84 个节点的 Dense 层
- en: A final softmax prediction layer with 10 nodes
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最终的 softmax 预测层，包含 10 个节点
- en: Here, all the layers except the last have ReLU (Rectified Linear Unit) activation.
    A convolutional layer in a CNN model generalizes the convolution operation we
    discussed, to work on multi-channel inputs and produce multi-channel outputs.
    Let’s understand what we meant by that. The original convolution operation we
    saw operated on a simple 2D plane with a height *h* and width *w*. Next, the kernel
    moves over the plane while producing a single value at each position. This process
    produces another 2D plane. But in practice, CNN models operate on four-dimensional
    inputs, i.e. an input of size `[batch size, height, width, in channels]`, and
    produce an output that is a four-dimensional, i.e. an output of size `[batch size,
    height, width, out channels]`. To produce this output, the kernel would need to
    be a four-dimensional tensor having the dimensions `[kernel height, kernel width,
    in channels, out channels]`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，除了最后一层外，所有层都使用 ReLU（修正线性单元）激活函数。CNN 模型中的卷积层将我们之前讨论的卷积操作推广到多通道输入，并产生多通道输出。让我们来理解一下这是什么意思。我们看到的原始卷积操作作用于一个简单的二维平面，具有高度
    *h* 和宽度 *w*。接下来，卷积核在平面上移动，每个位置生成一个单一值。这个过程会生成另一个二维平面。但是在实际应用中，CNN 模型处理的是四维输入，即形状为
    `[batch size, height, width, in channels]` 的输入，并生成一个四维输出，即形状为 `[batch size, height,
    width, out channels]` 的输出。为了生成这个输出，卷积核需要是一个四维张量，具有 `[kernel height, kernel width,
    in channels, out channels]` 的维度。
- en: It might not be entirely clear why inputs, outputs, and kernels would be in
    this format. *Figure 5.13* clarifies this.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 可能一开始不太清楚为什么输入、输出和卷积核需要采用这种格式。*图 5.13* 解释了这一点。
- en: '![](img/B14070_05_13.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_13.png)'
- en: 'Figure 5.13: How input and output shapes look for a two-dimensional convolution
    layer'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：二维卷积层的输入和输出形状
- en: 'Below, we will outline the full model. Don’t worry if you don’t understand
    it at first glance. We will go through line by line to understand how the model
    comes to be:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将概述整个模型。如果你一开始没有理解，不用担心。我们会逐行讲解，帮助你理解模型的构建过程：
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The very first thing to notice is that we are using the Keras Sequential API.
    The CNN we are implementing here has a series of layers connected one after the
    other. Therefore, we will use the simplest API possible. We then have our first
    convolutional layer. We have already discussed the convolution operation. Let’s
    take the first line:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，我们使用的是 Keras 的 Sequential API。我们在这里实现的 CNN 模型由一系列层按顺序连接。因此，我们将使用最简单的
    API。接下来是我们第一个卷积层。我们已经讨论过卷积操作。让我们来看第一行：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `tensorflow.keras.layers.Conv2D` layer takes the following argument values
    in that order:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorflow.keras.layers.Conv2D` 层接受如下参数值，顺序如下：'
- en: '`filters` (`int`): This is the number of output filters (i.e. the number of
    out channels).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filters` (`int`): 这是输出滤波器的数量（即输出通道的数量）。'
- en: '`kernel_size` (`Tuple[int]`): This is the (height, width) of the convolution
    kernel.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size` (`Tuple[int]`): 这是卷积核的（高度，宽度）。'
- en: '`strides` (`Tuple[int]`): This denotes the stride on the height and width dimension
    of the input.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides` (`Tuple[int]`): 这个参数表示输入的高度和宽度维度上的步幅。'
- en: '`padding` (`str`): This denotes the type of padding (can be `''``SAME''` or
    `''VALID''`).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`str`): 这个参数表示填充类型（可以是 `''SAME''` 或 `''VALID''`）。'
- en: '`activation` (`str`): The non-linear activation used.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation` (`str`): 使用的非线性激活函数。'
- en: '`input_shape` (`Tuple[int]`): The shape of the input. When defining `input_shape`,
    we do not specify the batch dimension as it’s automatically added.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape` (`Tuple[int]`): 输入的形状。在定义 `input_shape` 时，我们不需要指定批次维度，因为它会自动添加。'
- en: 'Next, we have the first max-pooling layer, which looks as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有第一个最大池化层，其形式如下：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The arguments are quite similar to the ones in `tf.keras.layers.Conv2D`. The
    `pool_size` argument corresponds to the `kernel_size` argument that specifies
    the (height, width) of the pool window. Following a similar pattern, the following
    convolutional and pooling layers are defined. The final convolution layer produces
    a `[batch size, 1, 1, 120]`-sized output. The height and width dimensions are
    equal to 1, because LeNet is designed in a way that the last convolutional kernel
    has the same height and width as the output. Before this input is fed to a fully
    connected layer, we need to flatten this output, such that it has the shape `[batch
    size, 120]`. This is because a standard Dense layer takes a two-dimensional input.
    For that, we use the `tf.keras.layers.Flatten()` layer:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数与`tf.keras.layers.Conv2D`中的参数非常相似。`pool_size`参数对应于`kernel_size`参数，用于指定池窗口的（高度，宽度）。按照类似的模式，以下卷积和池化层被定义。最终的卷积层输出大小为`[batch
    size, 1, 1, 120]`。高度和宽度维度为1，因为LeNet的设计使得最后一个卷积核的高度和宽度与输出相同。在将这个输入送入全连接层之前，我们需要将其展平，使其形状为`[batch
    size, 120]`。这是因为标准的Dense层接受的是二维输入。为此，我们使用`tf.keras.layers.Flatten()`层：
- en: '[PRE11]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, we define two Dense layers as follows.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义两个Dense层如下。
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As the final step, we will compile the model using the sparse categorical cross-entropy
    loss and the Adam optimizer. We will also track the accuracy on the data:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，我们将使用稀疏类别交叉熵损失函数和Adam优化器来编译模型。我们还将跟踪数据上的准确率：
- en: '[PRE13]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the data prepared and the model defined fully, we are good to train our
    model. Model training is as simple as calling one function:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好且模型完全定义后，我们可以开始训练模型。模型训练非常简单，只需调用一个函数：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `tf.keras.layers.Model.fit()` takes many arguments. But let’s only discuss
    the ones we have used here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.Model.fit()`接受许多参数。但我们这里只讨论我们在这里使用的那些：'
- en: '`x` (`np.ndarray` / `tf.Tensor` / other): Takes in a tensor that will act as
    input to the model (implemented as a NumPy array or a TensorFlow tensor). But
    the accepted values are not limited just to tensors. To see the full list, please
    refer to [https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`（`np.ndarray` / `tf.Tensor` / 其他）：接受一个张量，作为模型的输入（实现为NumPy数组或TensorFlow张量）。但是，接受的值不仅限于张量。要查看完整的列表，请参阅[https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)。'
- en: '`y` (`np.ndarray` / `tf.Tensor`): Takes in a tensor that will act as the labels
    (targets) for the model.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`（`np.ndarray` / `tf.Tensor`）：接受一个张量，该张量将作为模型的标签（目标）。'
- en: '`validation_split` (`float`): Setting this argument means a fraction of training
    data (e.g. 0.2 translates to 20%) will be used as validation data.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_split`（`float`）：设置此参数意味着训练数据的一部分（例如，0.2表示20%）将作为验证数据。'
- en: '`epochs` (`int`): The number of epochs to train the model for.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`（`int`）：训练模型的轮数。'
- en: 'You can evaluate the trained model on the test data by calling:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调用以下命令在测试数据上评估训练好的模型：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once run, you’ll see an output as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，你将看到如下输出：
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The model should get up to around 88% accuracy when trained.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后的模型应达到约88%的准确率。
- en: You just finished learning about the functions that we used to create our first
    CNN. You learned to use the functions to implement the CNN structure as well as
    define the loss, minimize the loss, and get predictions for unseen data. We used
    a simple CNN to see if it could learn to classify clothing items. Also, we were
    able to achieve an accuracy above 88% with a reasonably simple CNN. Next, we will
    analyze some of the results produced by the CNN. We will see why the CNN couldn’t
    recognize some of the images correctly.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚学会了我们用来创建第一个CNN的函数。你学会了如何使用这些函数来实现CNN结构、定义损失、最小化损失并获得未见数据的预测。我们使用了一个简单的CNN来看看它是否能够学习分类服装物品。此外，我们成功地用一个相对简单的CNN达到了超过88%的准确率。接下来，我们将分析CNN生成的一些结果。我们将了解为什么CNN没有正确识别一些图像。
- en: Analyzing the predictions produced with a CNN
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析CNN生成的预测结果
- en: Here, we can randomly pick some correctly and incorrectly classified samples
    from the test set to evaluate the learning power of CNNs (see *Figure 5.14*).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以从测试集中随机挑选一些正确和错误分类的样本，以评估CNN的学习能力（见*图5.14*）。
- en: 'We can see that for the correctly classified instances, the CNN is very confident
    about the output, most of the time. This is a good sign that the model is making
    very confident and accurate decisions. However, when we evaluate the incorrectly
    classified examples, we can see that some of them are in fact difficult, and even
    a human can get some of them wrong. For example, for an ankle boot that’s classified
    as a sandal, there is a large black patch that can indicate the presence of straps,
    which makes it more likely to be a sandal (the third image from the right in the
    third row). Also, in the fifth image from the right in the third row, it’s difficult
    to say whether it’s a shirt or a collared t-shirt:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于正确分类的实例，卷积神经网络（CNN）对输出的信心通常非常高。这是一个好兆头，表明模型正在做出非常自信且准确的决策。然而，当我们评估错误分类的实例时，我们可以发现其中一些实例确实很难，甚至人类也可能会犯错。例如，对于一个被分类为凉鞋的
    ankle boot，其上有一个大的黑色补丁，这可能表明有带子，导致它更可能被认为是凉鞋（第三行从右数第三张图）。此外，在第三行从右数第五张图中，很难判断它是衬衫还是有领T恤：
- en: '![](img/B14070_05_14.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_14.png)'
- en: 'Figure 5.14: Fashion-MNIST correctly classified and misclassified instances'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14：Fashion-MNIST 正确分类和错误分类的实例
- en: Using CNNs for sentence classification
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNN 进行句子分类
- en: Though CNNs have mostly been used for computer vision tasks, nothing stops them
    from being used in NLP applications. But as we highlighted earlier, CNNs were
    originally designed for visual content. Therefore, using CNNs for NLP tasks requires
    somewhat more effort. This is why we started out learning about CNNs with a simple
    computer vision problem. CNNs are an attractive choice for machine learning problems
    due to the low parameter count of convolution layers. One such NLP application
    for which CNNs have been used effectively is sentence classification.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CNN 主要用于计算机视觉任务，但没有什么能阻止它们用于 NLP 应用。如前所述，CNN 最初是为视觉内容设计的。因此，使用 CNN 进行 NLP
    任务需要更多的努力。这也是我们从简单的计算机视觉问题开始学习 CNN 的原因。CNN 是机器学习问题的一个有吸引力的选择，因为卷积层的参数数量较少。CNN
    在 NLP 中的一个有效应用是句子分类。
- en: In sentence classification, a given sentence should be classified with a class.
    We will use a question database, where each question is labeled by what the question
    is about. For example, the question “Who was Abraham Lincoln?” will be a question
    and its label will be *Person*. For this we will use a sentence classification
    dataset available at [http://cogcomp.org/Data/QA/QC/](http://cogcomp.org/Data/QA/QC/);
    here you will find several datasets. We are using the set with ~5,500 training
    questions and their respective labels and 500 testing sentences.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子分类中，给定的句子应该被分类到一个类别中。我们将使用一个问题数据库，其中每个问题都按其主题进行标记。例如，问题 “Who was Abraham
    Lincoln?” 将被标记为问题，其标签为 *Person*。为此，我们将使用一个句子分类数据集，数据集可通过 [http://cogcomp.org/Data/QA/QC/](http://cogcomp.org/Data/QA/QC/)
    获取；你将在这里找到多个数据集。我们使用的是包含约5,500个训练问题及其相应标签和500个测试句子的集合。
- en: We will use the CNN network introduced in a paper by Yoon Kim, *Convolutional
    Neural Networks for Sentence Classification*, to understand the value of CNNs
    for NLP tasks. However, using CNNs for sentence classification is somewhat different
    from the Fashion-MNIST example we discussed, because operations (for example,
    convolution and pooling) now happen in one dimension (length) rather than two
    dimensions (height and width). Furthermore, the pooling operations will also have
    a different flavor to the normal pooling operation, as we will see soon. You can
    find the code for this exercise in the `ch5_cnn_sentence_classification.ipynb`
    file in the `Ch5-Sentence-Classification` folder. As the first step, we will understand
    the data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Yoon Kim 在论文《*卷积神经网络用于句子分类*》中介绍的 CNN 网络，来理解 CNN 在自然语言处理（NLP）任务中的价值。然而，使用
    CNN 进行句子分类与我们之前讨论的 Fashion-MNIST 示例有所不同，因为现在的操作（例如卷积和池化）发生在一个维度（长度）中，而不是两个维度（高度和宽度）。此外，池化操作也会与正常的池化操作有所不同，正如我们很快会看到的那样。你可以在
    `Ch5-Sentence-Classification` 文件夹中的 `ch5_cnn_sentence_classification.ipynb` 文件找到这个练习的代码。作为第一步，我们将理解数据。
- en: How data is transformed for sentence classification
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子分类的数据转换方式
- en: Let’s assume a sentence of *p* words. First, we will pad the sentence with some
    special words (if the length of the sentence is < *n*) to set the sentence length
    to *n* words, where ![](img/B14070_05_038.png). Next, we will represent each word
    in the sentence by a vector of size *k*, where this vector can either be a one-hot-encoded
    representation, or Word2vec word vectors learned using skip-gram, CBOW, or GloVe.
    Then a batch of sentences of size *b* can be represented by a ![](img/B14070_05_039.png)
    matrix.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一句话有 *p* 个单词。首先，如果句子的长度小于 *n*，我们将为句子填充一些特殊的单词（将句子长度设置为 *n* 个单词），如 ![](img/B14070_05_038.png)
    所示。接下来，我们将句子中的每个单词表示为一个大小为 *k* 的向量，该向量可以是一个独热编码表示，或者是使用skip-gram、CBOW或GloVe学习的Word2vec词向量。然后，一批大小为
    *b* 的句子可以表示为一个 ![](img/B14070_05_039.png) 矩阵。
- en: 'Let’s walk through an example. Let’s consider the following three sentences:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来演示。让我们考虑以下三句话：
- en: '*Bob and Mary are friends.*'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲍勃和玛丽是朋友。*'
- en: '*Bob plays soccer.*'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲍勃踢足球。*'
- en: '*Mary likes to sing in the choir.*'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*玛丽喜欢在合唱团里唱歌。*'
- en: 'In this example, the third sentence has the most words, so let’s set *n* =
    *7*, which is the number of words in the third sentence. Next, let’s look at the
    one-hot-encoded representation for each word. In this case, there are 13 distinct
    words. Therefore, we get this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第三个句子有最多的单词，因此我们设置 *n* = *7*，即第三个句子中的单词数。接下来，让我们来看一下每个单词的独热编码表示。在这种情况下，有13个不同的单词。因此，我们得到如下表示：
- en: '*Bob*: 1,0,0,0,0,0,0,0,0,0,0,0,0'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*鲍勃*: 1,0,0,0,0,0,0,0,0,0,0,0,0'
- en: '*and*: 0,1,0,0,0,0,0,0,0,0,0,0,0'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*和*: 0,1,0,0,0,0,0,0,0,0,0,0,0'
- en: '*Mary*: 0,0,1,0,0,0,0,0,0,0,0,0,0'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*玛丽*: 0,0,1,0,0,0,0,0,0,0,0,0,0'
- en: 'Also, *k* = *13* for the same reason. With this representation, we can represent
    the three sentences as a three-dimensional matrix of size *3 x 7 x 13*, as shown
    in *Figure 5.15*:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，*k* = *13*，原因相同。使用这种表示，我们可以将三句话表示为一个大小为 *3 x 7 x 13* 的三维矩阵，如 *图5.15* 所示：
- en: '![Data transformation](img/B14070_05_15.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换](img/B14070_05_15.png)'
- en: 'Figure 5.15: A batch of sentences represented as a sentence matrix'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：一批句子表示为句子矩阵
- en: You could also utilize word embeddings instead of one-hot encoding here. Representing
    each word as a one-hot-encoded feature introduces sparsity and wastes computational
    memory. By using embeddings, we are enabling the model to learn more compact and
    powerful word representations than one-hot-encoded representations. This also
    means that ![](img/B14070_04_020.png) becomes a hyperparameter (i.e. the embedding
    size), as opposed to being driven by the size of the vocabulary. This means that,
    in *Figure 5.15*, each column will be a distributed continuous vector, not a combination
    of 0s and 1s.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在这里使用词嵌入代替独热编码。将每个单词表示为独热编码特征会引入稀疏性并浪费计算内存。通过使用词嵌入，我们使得模型能够学习到比独热编码更紧凑、更强大的单词表示。这也意味着
    ![](img/B14070_04_020.png) 成为一个超参数（即嵌入大小），而不是由词汇表的大小驱动。这意味着，在*图5.15*中，每一列将是一个分布式的连续向量，而不是由0和1组成的组合。
- en: We know that one-hot vectors lead to high-dimensional and highly sparse representations
    that are sub-optimal. On the other hand, word vectors give richer representations
    of words. However, learning word vectors is computationally costly. There is another
    alternative called the hashing trick. The beauty of the hashing trick is that
    it is extremely simple but gives a powerful and economical alternative that sits
    between one-hot vectors and word vectors. The idea behind the hashing trick is
    to use a hash function that converts a given token to an integer.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，独热向量会导致高维和高度稀疏的表示，且不理想。另一方面，词向量提供了更丰富的单词表示。然而，学习词向量的计算成本较高。还有一种替代方法叫做哈希技巧。哈希技巧的优点在于它非常简单，但提供了一个强大且经济的替代方案，介于独热向量和词向量之间。哈希技巧背后的想法是使用哈希函数将给定的标记转换为整数。
- en: '*f(<token>)-->hash value*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(<token>)-->哈希值*'
- en: Here *f* is a chosen hash function. Some example popular hash functions are
    SHA ([https://brilliant.org/wiki/secure-hashing-algorithms/](https://brilliant.org/wiki/secure-hashing-algorithms/))
    and MD5 ([https://searchsecurity.techtarget.com/definition/MD5](https://searchsecurity.techtarget.com/definition/MD5)).
    There’s also more advanced hashing such as locality-sensitive hashing ([https://www.pinecone.io/learn/locality-sensitive-hashing/](https://www.pinecone.io/learn/locality-sensitive-hashing/))
    to give out similar IDs for morphologically similar words. You can easily use
    the hashing trick via TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*f*是一个选定的哈希函数。一些常见的哈希函数包括SHA（[https://brilliant.org/wiki/secure-hashing-algorithms/](https://brilliant.org/wiki/secure-hashing-algorithms/)）和MD5（[https://searchsecurity.techtarget.com/definition/MD5](https://searchsecurity.techtarget.com/definition/MD5)）。还有更高级的哈希方法，比如局部敏感哈希（[https://www.pinecone.io/learn/locality-sensitive-hashing/](https://www.pinecone.io/learn/locality-sensitive-hashing/)），可以为形态上相似的词语生成相似的ID。你可以通过TensorFlow（[https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/hashing_trick)）轻松使用哈希技巧。
- en: Implementation – downloading and preparing data
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 – 下载并准备数据
- en: 'First we will download the data from the web. The data download functions are
    provided in the notebook and are simply downloading two files: training and testing
    data (the paths to the files are retained in `train_filename` and `test_filename`).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从网上下载数据。数据下载功能在笔记本中提供，简单地下载了两个文件：训练数据和测试数据（文件路径保存在`train_filename`和`test_filename`中）。
- en: 'If you open these files you will see that they contain a collection of lines
    of text. Each line has the format:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开这些文件，你会看到它们包含一系列文本行。每一行的格式是：
- en: '`<Category>: <sub-category> <question>`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`<Category>: <sub-category> <question>`'
- en: 'There are two pieces of meta information for each question: a category and
    a sub-category. A category is a macro-level classification, where sub-category
    is a finer grain identification of the type of the question. There are six categories
    available: `DESC` (description-related), `ENTY` (entity-related), `HUM` (human-related),
    `ABBR` (abbreviation related), `NUM` (numerical), and `LOC` (location related).
    Each category has several sub-categories associated with them. For example, the
    `ENTY` category is further broken down to animal, currency, events, food, etc.
    For our problem, we will be focusing on high-level classification (i.e. six classes),
    but you could also leverage the same model with minimal changes to classify on
    the sub-category level.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题都有两个元数据：一个类别和一个子类别。类别是宏观分类，子类别则是对问题类型的更细致的划分。共有六个类别：`DESC`（描述相关）、`ENTY`（实体相关）、`HUM`（人类相关）、`ABBR`（缩写相关）、`NUM`（数字相关）和`LOC`（地点相关）。每个类别下有若干子类别。例如，`ENTY`类别进一步细分为动物、货币、事件、食物等。在我们的任务中，我们将专注于高级分类（即六个类别），但你也可以通过最小的修改，利用相同的模型进行子类别层次的分类。
- en: 'Once the files are downloaded, we’ll read the data into the memory. For that,
    we will implement the `read_data()` function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件下载完成，我们将把数据读入内存。为此，我们将实现`read_data()`函数：
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This function simply goes through each line in the file and separates the question,
    category, and sub-category, using the format of each line elucidated above. After
    that, each question, category, and sub-category is written to the lists `questions`,
    `categories`, and `sub_categories` respectively. Finally, the function returns
    these lists. With the `questions`, `categories`, and `sub_categories` available
    for both training and testing data, we will create `pandas` DataFrames for training
    and testing data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数简单地遍历文件中的每一行，并按照上述格式分离问题、类别和子类别。然后，将每个问题、类别和子类别分别写入`questions`、`categories`和`sub_categories`列表。最后，函数返回这些列表。通过为训练和测试数据提供`questions`、`categories`和`sub_categories`，我们将为训练和测试数据创建`pandas`数据框。
- en: '`pandas` DataFrames are an expressive data structure for storing multi-dimensional
    data. A DataFrame can have indices, columns, and values. Each value has a specific
    index and a column. It is quite simple to create a DataFrame:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`数据框是一种用于存储多维数据的表达型数据结构。一个数据框可以有索引、列和数值。每个值都有特定的索引和列。创建一个数据框是相当简单的：'
- en: '[PRE18]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We call the `pd.DataFrame` construct with a dictionary. The keys of the dictionary
    represent columns of the DataFrame, and the values represent the elements in each
    column. Here we create three columns: `question`, `category`, and `sub_category`.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用字典调用 `pd.DataFrame` 构造函数。字典的键表示 DataFrame 的列，值表示每列中的元素。这里我们创建了三个列：`question`、`category`
    和 `sub_category`。
- en: '*Figure 5.16* depicts what the `train_df` looks like.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.16* 展示了 `train_df` 的样子。'
- en: '![](img/B14070_05_16.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_16.png)'
- en: 'Figure 5.16: A sample of data captured in the pandas DataFrame'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：在 pandas DataFrame 中捕获的数据示例
- en: 'We will do a simple shuffle of rows in the training set, to make sure we are
    not introducing any unintentional ordering in the data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对训练集中的行进行简单的洗牌，以确保不会在数据中引入任何无意的顺序：
- en: '[PRE19]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This process will sample 100% of the data from the DataFrame randomly. In other
    words, it will shuffle the order of the rows. From this point onward, we will
    not consider the `sub_category` column. We will first map each class label to
    a class ID:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程将从 DataFrame 中随机抽样 100% 的数据。换句话说，它将打乱行的顺序。从此时起，我们将不再考虑 `sub_category` 列。我们将首先将每个类别标签映射到一个类别
    ID：
- en: '[PRE20]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We first identify the unique values present in the `train_df["category"]`. Then
    we will create a dictionary by mapping from the unique values to a list of numerical
    IDs (0 to 5). The `np.arange()` function can be used to generate a series of integers
    in a specified range (here, the range is from 0 to the length of `unique_cats`).
    This process will give us the following `labels_map`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先识别 `train_df["category"]` 中存在的唯一值。然后，我们将通过将唯一值映射到数字 ID（0 到 5）的列表来创建一个字典。`np.arange()`
    函数可以用来生成一个指定范围内的整数序列（这里，范围是从 0 到 `unique_cats` 的长度）。这个过程将生成以下 `labels_map`。
- en: '`Label->ID mapping: {0: 0, 1: 1, 2: 2, 4: 3, 3: 4, 5: 5}`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`标签->ID 映射：{0: 0, 1: 1, 2: 2, 4: 3, 3: 4, 5: 5}`'
- en: Then we simply apply this mapping to the category column of both the train and
    test DataFrames to convert string labels to numerical labels. The data would look
    as follows, after the transformation (*Figure 5.17*).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们简单地将这个映射应用于训练和测试 DataFrame 的类别列，将字符串标签转换为数字标签。转换后的数据如下所示（*图 5.17*）。
- en: '![](img/B14070_05_17.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_17.png)'
- en: 'Figure 5.17: A sample of data in the DataFrame after mapping categories to
    integers'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17：在将类别映射为整数后，DataFrame 中的数据示例
- en: We create a validation set, stemming from the original training set, to monitor
    model performance while it trains. We will use the `train_test_split()` function
    from the scikit-learn library. 10% of the data will be separated as validation
    data, while 90% is kept as training data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个验证集，源自原始训练集，用于在训练过程中监控模型表现。我们将使用 scikit-learn 库中的`train_test_split()`函数。10%的数据将作为验证数据，其余
    90% 保留作为训练数据。
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This outputs:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can see that approximately 4,900 examples are used as training and the rest
    as validation. In the next section, we will build a tokenizer to tokenize the
    questions and assign individual tokens numerical IDs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大约 4,900 个示例用于训练，剩余的作为验证。在接下来的部分，我们将构建一个分词器来对问题进行分词，并为每个词汇分配数字 ID。
- en: Implementation – building a tokenizer
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 – 构建分词器
- en: 'Moving on, now it’s time to build a tokenizer that can map words to numerical
    IDs:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，到了构建分词器的时刻，它可以将单词映射为数字 ID：
- en: '[PRE23]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here we simply create a `Tokenizer` object and use the `fit_on_texts()` function
    to train it on the training corpus. In this process, the tokenizer will map words
    in the vocabulary to IDs. We will convert all of the train, validation, and test
    inputs to sequences of word IDs. Simply call the `tokenizer.texts_to_sequences()`
    function with a list of strings, where each string represents a question:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们简单地创建一个 `Tokenizer` 对象，并使用 `fit_on_texts()` 函数在训练语料库上训练它。在这个过程中，分词器会将词汇表中的单词映射为
    ID。我们将把训练集、验证集和测试集中的所有输入转换为单词 ID 的序列。只需调用 `tokenizer.texts_to_sequences()` 函数，并传入一个字符串列表，每个字符串代表一个问题：
- en: '[PRE24]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It’s important to understand that we are feeding our model a batch of questions
    at a given time. It is very unlikely that all of the questions have the same number
    of tokens. If all questions do not have the same number of tokens, we cannot form
    a tensor due to the uneven lengths of different questions. To solve this, we have
    to pad shorter sequences with special tokens and truncate sequences longer than
    a specified length. To achieve this we can easily use the `tf.keras.preprocessing.sequence.pad_sequences()`
    function. It would be worthwhile going through the arguments accepted by this
    function:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，我们每次给模型输入一批问题。所有问题的词数不太可能相同。如果所有问题的词数不相同，我们无法形成一个张量，因为问题的长度不一致。为了解决这个问题，我们必须通过特殊符号填充较短的序列，并截断超过指定长度的序列。为了实现这一点，我们可以轻松使用`tf.keras.preprocessing.sequence.pad_sequences()`函数。值得一提的是，我们可以仔细查看该函数所接受的参数：
- en: '`sequences (List[List[int]])` – List of list integers; each list of integers
    is a sequence'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences (List[List[int]])` – 整数列表的列表；每个整数列表是一个序列'
- en: '`maxlen (int)` – The maximum padding length'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxlen (int)` – 最大填充长度'
- en: '`padding (string)` – Whether to pad at the beginning `(pre)` or end `(post)`'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding (string)` – 是否在开头 `(pre)` 或结尾 `(post)` 进行填充'
- en: '`truncating (string)` – Whether to truncate at the beginning `(pre)` or end
    `(post)`'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncating (string)` – 是否在开头 `(pre)` 或结尾 `(post)` 进行截断'
- en: '`value (int)` – What value is to be used for padding (defaults to 0)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value (int)` – 用于填充的值（默认为 0）'
- en: 'Below we use this function to create sequence matrices for training, validation,
    and testing data:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们使用这个函数为训练、验证和测试数据创建序列矩阵：
- en: '[PRE25]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The reason we picked 22 as the sequence length is through a simple analysis.
    The 99% percentile of the sequence lengths of the training corpus is equal to
    22\. Therefore, we have picked that. Another important statistic is that the vocabulary
    size will be approximately 7,880 words. Now we will discuss the model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择 22 作为序列长度的原因是通过简单的分析得出的。训练语料库中序列长度的 99% 百分位数为 22。因此，我们选择了这个值。另一个重要统计信息是词汇表大小大约为
    7,880 个词。接下来我们将讨论模型。
- en: The sentence classification CNN model
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子分类 CNN 模型
- en: Now we will discuss the technical details of the CNN used for sentence classification.
    First, we will discuss how data or sentences are transformed into a preferred
    format that can easily be dealt with by CNNs. Next, we will discuss how the convolution
    and pooling operations are adapted for sentence classification, and finally, we
    will discuss how all these components are connected.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论用于句子分类的 CNN 的技术细节。首先，我们将讨论如何将数据或句子转换为可以方便地由 CNN 处理的首选格式。接下来，我们将讨论如何将卷积和池化操作适应于句子分类，最后，我们将讨论如何将所有这些组件连接起来。
- en: The convolution operation
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积操作
- en: If we ignore the batch size, that is, if we assume that we are only processing
    a single sentence at a time, our data is a ![](img/B14070_05_042.png) matrix,
    where *n* is the number of words per sentence after padding, and *k* is the dimension
    of a single word vector. In our example, this would be *7* x *13*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果忽略批量大小，即假设我们每次只处理一个句子，我们的数据是一个 ![](img/B14070_05_042.png) 矩阵，其中 *n* 是填充后每个句子的单词数，*k*
    是单个词向量的维度。在我们的例子中，这将是 *7* x *13*。
- en: 'Now we will define our convolution weight matrix to be of size ![](img/B14070_05_043.png),
    where *m* is the filter size for a one-dimensional convolution operation. By convolving
    the input *x* of size ![](img/B14070_05_042.png) with a weight matrix *W* of size
    ![](img/B14070_05_043.png), we will produce an output of *h* of size ![](img/B14070_05_046.png)
    as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义卷积权重矩阵，其大小为 ![](img/B14070_05_043.png)，其中 *m* 是一维卷积操作的过滤器大小。通过将输入 *x*
    的大小为 ![](img/B14070_05_042.png) 与大小为 ![](img/B14070_05_043.png) 的权重矩阵 *W* 卷积，我们将得到大小为
    ![](img/B14070_05_046.png) 的输出 *h*，其计算过程如下：
- en: '![](img/B14070_05_047.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_047.png)'
- en: 'Here, *w*[i,j] is the *(i,j)*^(th) element of *W* and we will pad *x* with
    zeros so that *h* is of size ![](img/B14070_05_046.png). Also, we will define
    this operation more simply, as shown here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w*[i,j] 是 *W* 的 *(i,j)*^(th) 元素，我们将使用零填充 *x*，使得 *h* 的大小为 ![](img/B14070_05_046.png)。此外，我们将更简洁地定义这个操作，如下所示：
- en: '![](img/B14070_05_049.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_049.png)'
- en: 'Here, *** defines the convolution operation (with padding) and we will add
    an additional scalar bias *b*. *Figure 5.18* illustrates this operation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，***定义了卷积操作（带填充），并且我们将添加一个额外的标量偏置*b*。*图 5.18*展示了这一操作：
- en: '![The convolution operation](img/B14070_05_18.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![卷积操作](img/B14070_05_18.png)'
- en: 'Figure 5.18: A convolution operation for sentence classification. Convolution
    layers with different kernel widths are used to convolve over the sentence (i.e.
    sequence of tokens)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18：句子分类的卷积操作。使用不同的卷积核宽度的卷积层对句子（即标记序列）进行卷积
- en: Then, to learn a rich set of features, we have parallel layers with different
    convolution filter sizes. Each convolution layer outputs a hidden vector of size
    ![](img/B14070_05_046.png), and we will concatenate these outputs to form the
    input to the next layer of size ![](img/B14070_05_051.png), where *q* is the number
    of parallel layers we will use. The larger *q* is, the better the performance
    of the model.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了学习丰富的特征，我们有并行层，使用不同的卷积过滤器大小。每个卷积层输出大小为 ![](img/B14070_05_046.png) 的隐藏向量，我们将这些输出连接起来，作为下一层的输入，大小为
    ![](img/B14070_05_051.png)，其中*q*是我们将使用的并行层的数量。*q*越大，模型的性能越好。
- en: 'The value of convolving can be understood in the following manner. Think about
    the movie rating learning problem (with two classes, positive or negative), and
    we have the following sentences:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的值可以通过以下方式理解。想象一下电影评分学习问题（有两个类别，正面或负面），我们有以下句子：
- en: '*I like the movie, not too bad*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我喜欢这部电影，还不错*'
- en: '*I did not like the movie, bad*'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我不喜欢这部电影，差劲*'
- en: Now imagine a convolution window of size 5\. Let’s bin the words according to
    the movement of the convolution window.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一个大小为5的卷积窗口。我们将根据卷积窗口的移动来对单词进行分箱。
- en: 'The sentence *I like the movie, not too bad* gives:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 句子*I like the movie, not too bad*给出了：
- en: '*[I, like, the, movie, ‘,’]*'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*[I, like, the, movie, ‘,’]*'
- en: '*[like, the, movie, ‘,’, not]*'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '*[like, the, movie, ‘,’, not]*'
- en: '*[the, movie, ‘,’, not, too]*'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*[the, movie, ‘,’, not, too]*'
- en: '*[movie, ‘,’, not, too, bad]*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*[movie, ‘,’, not, too, bad]*'
- en: 'The sentence *I did not like the movie, bad* gives the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 句子*I did not like the movie, bad*给出了以下结果：
- en: '*[I, did, not, like, the]*'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '*[I, did, not, like, the]*'
- en: '*[did, not ,like, the, movie]*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*[did, not ,like, the, movie]*'
- en: '*[not, like, the, movie, ‘,’]*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*[not, like, the, movie, ‘,’]*'
- en: '*[like, the, movie, ‘,’, bad]*'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '*[like, the, movie, ‘,’, bad]*'
- en: 'For the first sentence, windows such as the following convey that the rating
    is positive:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个句子，像以下的窗口会传达评分为正面：
- en: '*[I, like, the, movie, ‘,’]*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*[I, like, the, movie, ‘,’]*'
- en: '*[movie, ‘,’, not, too, bad]*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*[movie, ‘,’, not, too, bad]*'
- en: 'However, for the second sentence, windows such as the following convey negativity
    in the rating:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于第二个句子，像以下的窗口会传达出负面的评分信息：
- en: '*[did, not, like, the, movie]*'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*[did, not, like, the, movie]*'
- en: We are able to see such patterns that help to classify ratings thanks to the
    preserved spatiality. For example, if you use a technique such as *bag-of-words*
    to calculate sentence representations that lose spatial information, the sentence
    representations of the above two sentences would be highly similar. The convolution
    operation plays an important role in preserving the spatial information of the
    sentences.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够看到这样的模式，它们帮助分类评分，这得益于保留的空间性。例如，如果你使用像*词袋模型*这样的技术来计算句子的表示，这会丢失空间信息，那么上述两个句子的表示将会非常相似。卷积操作在保留句子空间信息方面起着重要作用。
- en: Having *q* different layers with different filter sizes, the network learns
    to extract the rating with different size phrases, leading to an improved performance.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通过具有不同过滤器大小的*q*个层，网络学习如何提取不同大小短语的评分，从而提高性能。
- en: Pooling over time
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间池化
- en: The pooling operation is designed to subsample the outputs produced by the previously
    discussed parallel convolution layers. This is achieved as follows.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作旨在对之前讨论的并行卷积层产生的输出进行下采样。具体实现如下：
- en: 'Let’s assume the output of the last layer *h* is of size ![](img/B14070_05_051.png).
    The pooling over time layer would produce an output *h’* of size ![](img/B14070_05_053.png)
    output. The precise calculation would be as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 假设最后一层的输出*h*的大小是 ![](img/B14070_05_051.png)。时间池化层将生成一个输出*h’*，大小为 ![](img/B14070_05_053.png)。精确的计算如下：
- en: '![](img/B14070_05_054.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_05_054.png)'
- en: Here, ![](img/B14070_05_055.png) and *h*^((i)) is the output produced by the
    *i*^(th) convolution layer and ![](img/B14070_05_056.png) is the set of weights
    belonging to that layer. Simply put, the pooling over time operation creates a
    vector by concatenating the maximum element of each convolution layer.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B14070_05_055.png) 和*h*^((i))是由第*i*层卷积产生的输出，![](img/B14070_05_056.png)是属于该层的权重集。简单来说，时间池化操作通过连接每个卷积层的最大元素来创建一个向量。
- en: 'We will illustrate this operation in *Figure 5.19*:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*图 5.19*中说明这个操作：
- en: '![Pooling over time](img/B14070_05_19.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![时间上的池化](img/B14070_05_19.png)'
- en: 'Figure 5.19: The pooling over time operation for sentence classification'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19：用于句子分类的时间池化操作
- en: 'By combining these operations, we finally arrive at the architecture shown
    in *Figure 5.20*:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些操作，我们最终得到了如*图 5.20*所示的架构：
- en: '![Pooling over time](img/B14070_05_20.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![时间上的池化](img/B14070_05_20.png)'
- en: 'Figure 5\. 20: A sentence classification CNN architecture. The pool of convolution
    layers having different kernel widths produces a set of output sequences. They
    are fed into the Pooling Over Time Layer that produces a compact representation
    of that input. This is finally connected to a classification layer with softmax
    activation'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20：句子分类CNN架构。具有不同核宽度的卷积层池生成一组输出序列。这些序列被送入“时间池化”层，生成该输入的紧凑表示。最后，这些被连接到具有softmax激活的分类层：
- en: Implementation – sentence classification with CNNs
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 – 使用CNN进行句子分类
- en: 'We are off implementing the model in TensorFlow 2\. As a prerequisite, let’s
    import several necessary modules from TensorFlow:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始在TensorFlow 2中实现模型。在此之前，让我们从TensorFlow中导入几个必要的模块：
- en: '[PRE26]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Clear the running session to make sure previous runs are not interfering with
    the current run:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 清除当前运行的会话，以确保之前的运行不会干扰当前的运行：
- en: '[PRE27]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Before we start, we will be using the Functional API from Keras. The reason
    for this is that the model we will be building here cannot be built with the Sequential
    API, due to intricate pathways present in the model. Let’s start off by creating
    an input layer:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将使用Keras的功能性API。这样做的原因是我们将在这里构建的模型不能使用顺序API构建，因为该模型中有复杂的路径。我们先从创建一个输入层开始：
- en: '[PRE28]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The input layer simply takes a batch of `max_seq_length` word IDs. That is,
    a batch of sequences, where each sequence is padded/truncated to a max length.
    We specify the `dtype` as `int32`, since they are word IDs. Next, we define an
    embedding layer, from which we will look up embeddings corresponding to the word
    IDs coming through the `word_id_inputs` layer:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层简单地接收一个`max_seq_length`的单词ID批次。也就是说，接收一批序列，其中每个序列都填充或截断到最大长度。我们将`dtype`指定为`int32`，因为它们是单词ID。接下来，我们定义一个嵌入层，在该层中我们将查找与通过`word_id_inputs`层传入的单词ID对应的嵌入：
- en: '[PRE29]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is a randomly initialized embedding layer. It contains a large matrix
    of size `[n_vocab, 64]`, where each row represents the word vector of the word
    indexed by that row number. The embeddings will be jointly learned with the model,
    while the model is trained on the supervised task. For the next part, we will
    define three different one-dimensional convolution layers with three different
    kernel (filter) sizes of `3`, `4`, and `5`, having 100 feature maps each:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个随机初始化的嵌入层。它包含一个大小为`[n_vocab, 64]`的大矩阵，其中每一行表示由该行编号索引的单词的词向量。嵌入将与模型共同学习，同时在监督任务上训练模型。在下一部分中，我们将定义三个不同的一维卷积层，分别使用三个不同的核（过滤器）大小：`3`、`4`和`5`，每个卷积层有100个特征图：
- en: '[PRE30]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'An important distinction to make here is that we are using one-dimensional
    convolution as opposed to the two-dimensional convolution we used in the earlier
    exercise. However, most of the concepts remain the same. The main difference is
    that, unlike `tf.keras.layers.Conv2D,` which works on four-dimensional inputs,
    `tf.keras.layers.Conv1D` operates on three-dimensional inputs (i.e. inputs with
    shape `[batch size, width, in channels]`). In other words, the convolution kernel
    moves only in one direction over the inputs. Each of these layers produces a `[batch
    size, sentence length, 100]`-sized output. Afterward, these outputs are concatenated
    on the last axis to produce a single tensor:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要做出一个重要区分，我们使用的是一维卷积，而不是之前练习中使用的二维卷积。然而，大多数概念仍然相同。主要的区别在于，`tf.keras.layers.Conv2D`作用于四维输入，而`tf.keras.layers.Conv1D`作用于三维输入（即形状为`[batch
    size, width, in channels]`的输入）。换句话说，卷积核仅沿一个方向在输入上滑动。这些层的每个输出都会产生一个形状为`[batch size,
    sentence length, 100]`的张量。然后，这些输出会在最后一个轴上连接，形成一个单一的张量：
- en: '[PRE31]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Subsequently, the new tensor of size `[batch size, sentence length, 300]` will
    be used to perform the pooling over time operation. We can implement the pooling
    over time operation by defining a one-dimensional max-pooling layer (i.e. `tf.keras.layers.MaxPool1D`)
    with a window as wide as the sequence length. This will produce a single value
    as the output, for each feature map in `conv_out`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，新的张量大小为`[batch size, sentence length, 300]`，将用于执行时间池化操作。我们可以通过定义一个一维最大池化层（即`tf.keras.layers.MaxPool1D`）来实现时间池化操作，其窗口宽度与序列长度相同。这样会为`conv_out`中的每个特征图生成一个单一值作为输出：
- en: '[PRE32]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here we get a `[batch_size, 1, 300]`-sized output after performing the operation.
    Next, we will convert this output to a `[batch_size, 300]`-sized output, by using
    the `tf.keras.layers.Flatten` layer. The Flatten layer simply collapses all the
    dimensions (except the batch dimension) to a single dimension:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们在执行操作后获得了一个`[batch_size, 1, 300]`大小的输出。接下来，我们将使用`tf.keras.layers.Flatten`层将此输出转换为`[batch_size,
    300]`大小的输出。Flatten层将所有维度（除了批次维度）压缩为一个维度：
- en: '[PRE33]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, `flatten_out` is passed to a Dense layer that has `n_classes` (i.e.
    six) nodes as the output and has a softmax activation:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`flatten_out`将传递到一个全连接层，该层具有`n_classes`（即六个）节点作为输出，并且使用softmax激活函数：
- en: '[PRE34]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note the use of the `kernel_regularizer` argument. We can use this argument
    to add any special regularization (e.g. L1 or L2 regularization) to a given layer.
    Finally, we define a model as,
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用了`kernel_regularizer`参数。我们可以使用该参数为给定层添加任何特殊的正则化（例如L1或L2正则化）。最后，我们定义一个模型如下，
- en: '[PRE35]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'and compile the model with the desired loss function, an optimizer, and metrics:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所需的损失函数、优化器和评估指标来编译模型：
- en: '[PRE36]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can view the model by running the following line:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下代码查看模型：
- en: '[PRE37]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: which gives,
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 结果为，
- en: '[PRE38]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Next, we will train the model on the data we already prepared.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在已经准备好的数据上训练模型。
- en: Training the model
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Since we have done the hard yard at the beginning, by making sure the data
    is transformed, training the model is simple. All we need to do is call the `tf.keras.layers.Model.fit()`
    function. However, let’s leverage a few techniques to improve model performance.
    This will be done by leveraging a built-in callback of TensorFlow. The technique
    we’ll be using is known as “decaying the learning rate.” The idea is to reduce
    the learning rate (by some fraction) whenever the model has stopped to improve
    performance. The following callback assists us to do this:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在开始时已经做好了基础工作，确保数据已经转换，因此训练模型非常简单。我们需要做的就是调用`tf.keras.layers.Model.fit()`函数。不过，我们可以通过利用一些技术来提升模型性能。我们将使用TensorFlow内置的回调函数来实现这一点。我们要使用的技术叫做“学习率衰减”。其思想是，当模型停止提高性能时，按某个比例减少学习率。以下回调函数可以帮助我们实现这一点：
- en: '[PRE39]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The parameters can be set as you wish, to control the learning rate reduction.
    Let’s understand the arguments above:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 可以根据需要设置这些参数来控制学习率的减少。让我们理解上面提到的参数：
- en: '`monitor (str)` – Which metric to monitor in order to decay the learning rate.
    We will monitor the validation loss'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitor (str)` – 用于监控的指标，以便衰减学习率。我们将监控验证损失'
- en: '`factor (float)` – By how much to reduce the learning rate. For example, a
    factor of 0.1 means that the learning rate will be reduced by 10 times (e.g. 0.01
    will be stepped down to 0.001)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factor (float)` – 降低学习率的倍数。例如，0.1的因子意味着学习率将减少10倍（例如，0.01将降到0.001）'
- en: '`patience (int)` – How many epochs to wait without an improvement, before reducing
    the learning rate'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patience (int)` – 在没有改进的情况下，等待多少个epoch后才会降低学习率'
- en: '`mode (string)` – Whether to look for an increase or decrease of the metric;
    ‘auto’ means that the direction will be determined by looking at the metric name'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode (string)` – 指定是否寻找指标的增加或减少；`auto`表示方向将根据指标名称确定'
- en: '`min_delta (float)` – How much of an increase/decrease to consider as an improvement'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_delta (float)` – 视为改进的最小增减量'
- en: '`min_lr (float)` – Minimum learning rate (floor)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_lr (float)` – 最小学习率（下限）'
- en: 'Let’s train the model:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型：
- en: '[PRE40]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We will see the accuracy quickly going up and the validation accuracy plateauing
    around 88%. Here’s a snippet of the output produced:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到准确率迅速上升，而验证准确率在88%左右停滞。以下是生成的输出片段：
- en: '[PRE41]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, let’s test the model on the testing dataset:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在测试数据集上测试模型：
- en: '[PRE42]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Evaluating the test data as given in the exercise gives us a test accuracy of
    close to 88% (for 500 test sentences) in this sentence classification task.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 按照练习中给出的测试数据进行评估，我们在这个句子分类任务中获得了接近88%的测试准确率（对于500个测试句子）。
- en: Here we end our discussion about using CNNs for sentence classification. We
    first discussed how one-dimensional convolution operations combined with a special
    pooling operation called *pooling over time* can be used to implement a sentence
    classifier based on the CNN architecture. Finally, we discussed how to use TensorFlow
    to implement such a CNN and saw that it in fact performs well in sentence classification.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们结束了关于使用CNN进行句子分类的讨论。我们首先讨论了如何将一维卷积操作与一种称为*时间池化*的特殊池化操作结合，来实现基于CNN架构的句子分类器。最后，我们讨论了如何使用TensorFlow来实现这样的CNN，并且看到它在句子分类中的确表现良好。
- en: It can be useful to know how the problem we just solved can be useful in the
    real world. Assume that you have a large document about the history of Rome in
    your hand, and you want to find out about Julius Caesar without reading the whole
    document. In this situation, the sentence classifier we just implemented can be
    used as a handy tool to summarize the sentences that only correspond to a person,
    so you don’t have to read the whole document.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 了解我们刚刚解决的问题如何在实际中应用是很有用的。假设你手上有一本关于罗马历史的厚重文档，而你只想了解关于尤利乌斯·凯撒的内容，而不想读完整本书。在这种情况下，我们刚刚实现的句子分类器可以作为一个有用的工具，帮助你总结出与某个人相关的句子，这样你就不必阅读整篇文档了。
- en: Sentence classification can be used for many other tasks as well; one common
    use of this is classifying movie reviews as positive or negative, which is useful
    for automating the computation of movie ratings. Another important application
    of sentence classification can be seen in the medical domain, where it is used
    to extract clinically useful sentences from large documents containing large amounts
    of text.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分类还可以应用于许多其他任务；其中一个常见的应用是对电影评论进行正负面分类，这对于自动化计算电影评分非常有用。句子分类在医学领域也有重要应用，它可以用来从包含大量文本的大型文档中提取临床有用的句子。
- en: Summary
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed CNNs and their various applications. First, we
    went through a detailed explanation of what CNNs are and their ability to excel
    at machine learning tasks. Next we decomposed the CNN into several components,
    such as convolution and pooling layers, and discussed in detail how these operators
    work. Furthermore, we discussed several hyperparameters that are related to these
    operators such as filter size, stride, and padding.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了卷积神经网络（CNN）及其各种应用。首先，我们详细解释了CNN是什么，以及它在机器学习任务中表现优异的能力。接下来，我们将CNN分解成几个组件，如卷积层和池化层，并详细讨论了这些操作符的工作原理。此外，我们还讨论了与这些操作符相关的几个超参数，如滤波器大小、步幅和填充。
- en: Then, to illustrate the functionality of CNNs, we walked through a simple example
    of classifying images of garments. We also did a bit of analysis to see why the
    CNN fails to recognize some images correctly.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，为了说明CNN的功能，我们通过一个简单的例子展示了如何对衣物图像进行分类。我们还进行了一些分析，看看为什么CNN在某些图像识别上出现错误。
- en: Finally, we started talking about how CNNs are applied for NLP tasks. Concretely,
    we discussed an altered architecture of CNNs that can be used to classify sentences.
    We then implemented this particular CNN architecture and tested it on an actual
    sentence classification task.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们开始讨论了卷积神经网络（CNN）如何应用于自然语言处理（NLP）任务。具体来说，我们讨论了一种修改过的CNN架构，可以用于对句子进行分类。然后，我们实现了这一特定的CNN架构，并在实际的句子分类任务中进行了测试。
- en: In the next chapter, we will move on to one of the most popular types of neural
    networks used for many NLP tasks – **Recurrent Neural Networks** (**RNNs**).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将进入一种在许多NLP任务中广泛应用的神经网络类型——**递归神经网络**（**RNNs**）。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的GitHub页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人一起学习，和超过1000名成员共同进步，网址：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
