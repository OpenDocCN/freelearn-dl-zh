- en: Training Deep Prediction Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练深度预测模型
- en: The previous chapters covered a bit of the theory behind neural networks and
    used some neural network packages in R. Now it is time to dive in and look at
    training deep learning models. In this chapter, we will explore how to train and
    build feedforward neural networks, which are the most common type of deep learning
    model. We will use MXNet to build deep learning models to perform classification
    and regression using a retail dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章介绍了神经网络背后的部分理论，并使用了R中的一些神经网络包。现在是时候深入探讨如何训练深度学习模型了。在本章中，我们将探索如何训练和构建前馈神经网络，这是最常见的深度学习模型类型。我们将使用MXNet构建深度学习模型，利用零售数据集进行分类和回归。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Getting started with deep feedforward neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度前馈神经网络入门
- en: Common activation functions – rectifiers, hyperbolic tangent, and maxout
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见激活函数 – 整流器、双曲正切和最大激活
- en: Introduction to the MXNet deep learning library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet深度学习库简介
- en: Use case – Using MXNet for classification and regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用案例 - 使用MXNet进行分类和回归
- en: Getting started with deep feedforward neural networks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度前馈神经网络入门
- en: A deep feedforward neural network is designed to approximate a function, *f()*,
    that maps some set of input variables, *x*, to an output variable, *y*. They are
    called feedforward neural networks because information flows from the input through
    each successive layer as far as the output, and there are no feedback or recursive
    loops (models including both forward and backward connections are referred to
    as recurrent neural networks).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈神经网络旨在逼近一个函数，*f()*，该函数将一组输入变量，*x*，映射到输出变量，*y*。它们被称为前馈神经网络，因为信息从输入层通过每一层依次传递直到输出层，并且没有反馈或递归环路（包含前向和后向连接的模型被称为递归神经网络）。
- en: Deep feedforward neural networks are applicable to a wide range of problems,
    and are particularly useful for applications such as image classification. More
    generally, feedforward neural networks are useful for prediction and classification
    where there is a clearly defined outcome (what digit an image contains, whether
    someone is walking upstairs or walking on a flat surface, the presence/absence
    of disease, and so on).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈神经网络适用于广泛的问题，特别是在图像分类等应用中非常有用。更一般来说，前馈神经网络适用于那些有明确定义结果的预测和分类任务（例如图像包含的数字、某人是否在走楼梯或走平地、是否患有某种疾病等）。
- en: 'Deep feedforward neural networks can be constructed by chaining layers or functions
    together. For example, a network with four hidden layers is shown in the following
    diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈神经网络可以通过将层或函数连接在一起来构建。例如，下面的图示展示了一个包含四个隐藏层的网络：
- en: '![](img/e278f4de-76ae-4249-bfba-ac2915bfdd11.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e278f4de-76ae-4249-bfba-ac2915bfdd11.png)'
- en: 'Figure 4.1: A deep feedforward neural network'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：一个深度前馈神经网络
- en: This diagram of the model is a directed acyclic graph. Represented as a function,
    the overall mapping from the input, *X*, to the output, *Y*, is a multilayered
    function. The first hidden layer is *H**[1]=f^((1))(X, w[1] a[1]**)*, the second
    hidden layer is *H[2]=f^((2))(H[1], w[2] a[2])*, and so on. These multiple layers
    can allow complex functions and transformations to be built up from relatively
    simple ones.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的图示是一个有向无环图。作为一个函数表示，从输入，*X*，到输出，*Y*，的整体映射是一个多层函数。第一隐藏层为 *H[1] = f^((1))(X,
    w[1] a[1])*，第二隐藏层为 *H[2] = f^((2))(H[1], w[2] a[2])*，依此类推。这些多层可以将复杂的函数和变换从相对简单的函数中构建出来。
- en: If sufficient hidden neurons are included in a layer, it can approximate to
    the desired degree of precision with many different types of functions. Feedforward
    neural networks can approximate non-linear functions by applying non-linear transformations
    between layers. These non-linear functions are known as activation functions,
    which we will cover in the next section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在一层中包含足够的隐藏神经元，它可以通过许多不同类型的函数逼近到所需的精度。前馈神经网络可以通过在层之间应用非线性变换来逼近非线性函数。这些非线性函数被称为激活函数，我们将在下一节中讨论。
- en: The weights for each layer will be learned as the model is trained through forward-
    and backward-propagation. Another key piece of the model that must be determined
    is the cost, or loss, function. The two most commonly used cost functions are
    cross-entropy, which is used for classification tasks, and **mean squared error**
    (**MSE**), which is used for regression tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层的权重将在模型通过前向传播和反向传播训练时学习到。另一个必须确定的关键部分是代价函数或损失函数。最常用的两种代价函数是交叉熵，用于分类任务，以及**均方误差**（**MSE**），用于回归任务。
- en: Activation functions
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'The activation function determines the mapping between input and a hidden layer.
    It defines the functional form for how a neuron gets activated. For example, a
    linear activation function could be defined as: *f(x) = x*, in which case the
    value for the neuron would be the raw input, *x*. A linear activation function
    is shown in the top panel of *Figure 4.2*. Linear activation functions are rarely
    used because in practice deep learning models would find it difficult to learn
    non-linear functional forms using linear activation functions. In previous chapters,
    we used the hyperbolic tangent as an activation function, namely *f(x) = tanh(x)*.
    Hyperbolic tangent can work well in some cases, but a potential limitation is
    that at either low or high values, it saturates, as shown in the middle panel
    of the figure  4.2.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数决定了输入与隐藏层之间的映射关系。它定义了神经元激活的函数形式。例如，线性激活函数可以定义为：*f(x) = x*，在这种情况下，神经元的值将是原始输入
    *x*。图 4.2 的顶部面板显示了线性激活函数。线性激活函数很少使用，因为在实践中，深度学习模型使用线性激活函数很难学习非线性的函数形式。在前面的章节中，我们使用了双曲正切作为激活函数，即
    *f(x) = tanh(x)*。双曲正切在某些情况下表现良好，但一个潜在的限制是，在低值或高值时，它会饱和，正如图 4.2 中间面板所示。
- en: Perhaps the most popular activation function currently, and a good first choice
    (Nair, V., and Hinton, G. E. (2010)), is known as a *rectifier*. There are different
    kinds of rectifiers, but the most common is defined by the *f(x) = max(0, x)* function,
    which is known as **relu**. The relu activation is flat below zero and linear
    above zero; an example is shown in Figure 4.2.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可能是最流行的激活函数，也是一个不错的首选（Nair, V., 和 Hinton, G. E.（2010）），它被称为 *修正线性单元*（rectifier）。修正线性单元有不同种类，但最常见的定义为
    *f(x) = max(0, x)* 的函数，这被称为**relu**。relu 激活在零以下是平的，在零以上是线性的；一个示例如图 4.2 所示。
- en: The final type of activation function we will discuss is maxout (Goodfellow,
    Warde­-Farley, Mirza, Courville, and Bengio (2013)). A maxout unit takes the maximum
    value of its input, although as usual, this is after weighting so it is not the
    case that the input variable with the highest value will always win. Maxout activation
    functions seem to work particularly well with dropout.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的最后一种激活函数是 maxout（Goodfellow、Warde­-Farley、Mirza、Courville 和 Bengio（2013））。maxout
    单元取其输入的最大值，尽管和往常一样，这是在加权之后进行的，因此并不是输入变量中值最高的那个总是会胜出。maxout 激活函数似乎与 dropout 特别契合。
- en: 'The relu activation is the most commonly-used activation function and it is
    the default option for the deep learning models in the rest of this book. The
    following graphs for some of the activation functions we have discussed:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: relu 激活是最常用的激活函数，也是本书其余部分深度学习模型的默认选项。以下是我们讨论过的一些激活函数的图示：
- en: '![](img/d1f135e3-40af-4b26-a5f3-0848bfd14096.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1f135e3-40af-4b26-a5f3-0848bfd14096.png)'
- en: 'Figure 4.2: Common activation functions'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：常见的激活函数
- en: Introduction to the MXNet deep learning library
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MXNet 深度学习库简介
- en: The deep learning libraries we will use in this book are MXNet, Keras, and TensorFlow.
    Keras is a frontend API, which means it is not a standalone library as it requires
    a lower-level library in the backend, usually TensorFlow. The advantage of using
    Keras rather than TensorFlow is that it has a simpler interface. We will use Keras
    in later chapters in this book.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将使用的深度学习库有 MXNet、Keras 和 TensorFlow。Keras 是一个前端 API，这意味着它不是一个独立的库，它需要一个底层库作为后端，通常是
    TensorFlow。使用 Keras 而不是 TensorFlow 的优点在于它具有更简单的接口。我们将在本书的后续章节中使用 Keras。
- en: Both MXNet and TensorFlow are multipurpose numerical computation libraries that
    can use GPUs for mass parallel matrix operations. As such, multi-dimensional matrices
    are central to both libraries. In R, we are familiar with the vector, which is
    a one-dimensional array of values of the same type. The R data frame is a two-dimensional
    array of values, where each column can have different types. The R matrix is a
    two-dimensional array of values with the same type. Some machine learning algorithms
    in R require a matrix as input. We saw an example of this in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training
    a Prediction Model*, with the RSNSS package.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 和 TensorFlow 都是多功能的数值计算库，可以利用 GPU 进行大规模并行矩阵运算。因此，多维矩阵是这两个库的核心。在 R 中，我们熟悉向量，它是一个一维的相同类型值的数组。R
    数据框是一个二维数组，其中每一列可以包含不同类型的数据。R 矩阵是一个二维数组，所有值的类型相同。一些机器学习算法在 R 中需要矩阵作为输入。我们在[第 2
    章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)，*训练预测模型*中，使用了 RSNSS 包的例子。
- en: In R, it is unusual to use data structures with more than two dimensions, but
    deep learning uses them extensively. For example, if you have a 32 x 32 color
    image, you could store the pixel values in a 32 x 32 x 3 matrix, where the first
    two dimensions are the width and height, and the last dimension is for the red,
    green, and blue colors. This can be extended further by adding another dimension
    for a collection of images. This is called a batch and allows the processor (CPU/GPU)
    to process multiple images concurrently. The batch size is a hyper-parameter and
    the value selected depends on the size of the input data and memory capacity.
    If our batch size were 64, our matrix would be a 4-dimensional matrix of size
    32 x 32 x 3 x 64 where the first 2 dimensions are the width and height, the third
    dimension is the colors, and the last dimension is the batch size, 64\. The important
    thing to realize is that this is just another way of representing data. In R,
    we would store the same data as a 2-dimensional matrix (or dataframe) with 64
    rows and 32 x 32 x 3 = 3,072 columns. All we are doing is reshaping the data,
    we are not changing it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中，使用多于两维的数据结构并不常见，但深度学习广泛使用它们。例如，如果你有一张 32 x 32 的彩色图像，你可以将像素值存储在一个 32 x
    32 x 3 的矩阵中，其中前两个维度是宽度和高度，最后一个维度代表红色、绿色和蓝色。这可以通过添加一个新的维度来进一步扩展，表示一组图像。这称为一个批次，并允许处理器（CPU/GPU）同时处理多张图像。批次大小是一个超参数，选择的值取决于输入数据的大小和内存容量。如果我们的批次大小为
    64，那么我们的矩阵将是一个 4 维矩阵，大小为 32 x 32 x 3 x 64，其中前两个维度是宽度和高度，第三个维度是颜色，最后一个维度是批次大小 64。需要注意的是，这只是表示数据的另一种方式。在
    R 中，我们会将相同的数据存储为一个 2 维矩阵（或数据框），具有 64 行和 32 x 32 x 3 = 3,072 列。我们所做的仅仅是重新排列数据，并没有改变数据本身。
- en: These n-dimensional matrices, which contain elements of the same type, are the
    cornerstone of using MXNet and TensorFlow. In MXNet, they are referred to as NDArrays.
    In TensorFlow, they are known as **tensors**. These n-dimensional matrices are
    important because they mean that we can feed the data into GPUs more efficiently;
    GPUs can process data in batches more efficiently than processing single rows
    of data. In the preceding example, we use 64 images in a batch, so the deep learning
    library will process input data in chunks of 32 x 32 x 3 x 64.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包含相同类型元素的 n 维矩阵是使用 MXNet 和 TensorFlow 的基石。在 MXNet 中，它们被称为 NDArrays。在 TensorFlow
    中，它们被称为 **张量**。这些 n 维矩阵非常重要，因为它们意味着我们可以更高效地将数据输入到 GPU；GPU 可以比处理单行数据更高效地批量处理数据。在前面的例子中，我们使用了
    64 张图像作为一个批次，因此深度学习库会将输入数据分成 32 x 32 x 3 x 64 的块进行处理。
- en: 'This chapter will use the MXNet deep learning library. MXNet originated at
    Carnegie Mellon University and is heavily supported by Amazon, they choose it
    as their default Deep Learning library in 2016\. In 2017, MXNet was accepted as
    an Apache Incubator project, ensuring that it would remain as open source software.
    Here is a very simple example of an NDArray (matrix) operation in MXNet in R.
    If you have not already installed the MXNet package for R, go back to [Chapter
    1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml), *Getting Started with Deep Learning*,
    for instructions, or use this link: [https://mxnet.apache.org/install/index.html](https://mxnet.apache.org/install/index.html):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用MXNet深度学习库。MXNet起源于卡内基梅隆大学，并得到了亚马逊的强力支持，2016年他们将其作为默认的深度学习库。2017年，MXNet被接受为Apache孵化项目，确保它将保持开源软件。这是一个在MXNet中使用R进行NDArray（矩阵）操作的非常简单的示例。如果你还没有安装MXNet包，请回到[第1章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)，*深度学习入门*，查看安装说明，或者使用此链接：[https://mxnet.apache.org/install/index.html](https://mxnet.apache.org/install/index.html)：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can break down this code line by line:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐行分析这段代码：
- en: Line 1 loads the MXNet package.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1行加载了MXNet包。
- en: Line 2 sets the CPU context. This tells MXNet where to process your computations,
    either on the CPU or on a GPU, if one is available.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2行设置了CPU上下文。这告诉MXNet在哪里处理你的计算，要么是在CPU上，要么是在可用的GPU上。
- en: Line 3 creates a 2-dimensional NDArray of size 2 x 3 where each value is 1.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3行创建了一个大小为2 x 3的二维NDArray，每个值都是1。
- en: Line 4 creates another 2-dimensional NDArray of size 2 x 3\. Each value will
    be 3 because we perform element-wise multiplication and add 1.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4行创建了一个大小为2 x 3的二维NDArray。每个值都会是3，因为我们进行逐元素乘法并加上1。
- en: Line 5 shows that b is an external pointer.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5行显示b是一个外部指针。
- en: Line 6 shows that the class of b is MXNDArray.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6行显示b的类是MXNDArray。
- en: Line 7 displays the results.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7行显示结果。
- en: We can perform mathematical operations, such as multiplication and addition,
    on the `b`variable. However, it is important to realize that, while this behaves
    similarly to an R matrix, it is not a native R object. We can see this when we
    output the type and class of this variable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对`b`变量执行数学运算，如乘法和加法。然而，需要注意的是，虽然这与R矩阵类似，但它并不是一个原生的R对象。当我们输出该变量的类型和类时，就能看到这一点。
- en: When developing deep learning models, there are usually two distinct steps.
    First you create the model architecture and then you train the model. The main
    reason for this is because most deep learning libraries employ symbolic programming
    rather than the imperative programming you are used to. Most of the code you have
    previously written in R is an imperative program, which executes code sequentially.
    For mathematical optimization tasks, such as deep learning, this may not be the
    most efficient method of execution. Most deep learning libraries, including MXNet
    and TensorFlow, use symbolic programming. For symbolic programming, a computation
    graph for the program execution is designed first. This graph is then compiled
    and executed. When the computation graph is generated, the input, output, and
    graph operations are already defined, meaning that the code can be optimized.
    This means that for deep learning, symbolic programs are usually more efficient
    than imperative programs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发深度学习模型时，通常有两个不同的步骤。首先是创建模型架构，然后是训练模型。这样做的主要原因是，大多数深度学习库采用符号化编程，而不是你习惯的命令式编程。你以前在R中编写的大部分代码是命令式程序，它按顺序执行代码。对于数学优化任务，比如深度学习，这可能不是最高效的执行方法。包括MXNet和TensorFlow在内的大多数深度学习库都使用符号化编程。符号化编程首先设计一个计算图，用于程序执行。然后，图会被编译并执行。当计算图生成时，输入、输出和图操作已经定义好了，这意味着代码可以得到优化。这意味着，对于深度学习，符号程序通常比命令式程序更高效。
- en: 'Here is a simple example of the type of optimization using symbolic programs:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用符号程序进行优化的简单例子：
- en: '*M = (M1 * M2) + (M3* M4)*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*M = (M1 * M2) + (M3* M4)*'
- en: 'An imperative program would calculate this as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个命令式程序会按如下方式计算：
- en: '*Mtemp1 = (M1 * M2)*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mtemp1 = (M1 * M2)*'
- en: '*Mtemp2 = (M3* M4)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mtemp2 = (M3* M4)*'
- en: '*M = Mtemp1 + Mtemp2*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*M = Mtemp1 + Mtemp2*'
- en: 'A symbolic program would first create a computation graph, which might look
    like the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个符号化程序会首先创建一个计算图，可能如下所示：
- en: '![](img/2905dc18-fd1f-43ca-8d79-a409fa5c3261.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2905dc18-fd1f-43ca-8d79-a409fa5c3261.png)'
- en: 'Figure 4.3: Example of a computation graph'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：计算图示例
- en: '*M1*, *M2*, *M3*, and *M4* are symbols that need to be operated on. The graph
    shows the dependencies for operations; the *+* operation requires the two preceding
    multiplication operations to be done before it can execute. But there is no dependency
    between the two multiplication steps, so these can be executed in parallel. This
    type of optimization means the code can execute much faster.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*M1*、*M2*、*M3* 和 *M4* 是需要操作的符号。图展示了操作之间的依赖关系；*+* 操作要求先执行两个乘法操作才能执行。但是两个乘法步骤之间没有依赖关系，因此它们可以并行执行。这种优化意味着代码能够更快地执行。'
- en: 'From a coding point of view, this means is that you have two steps in creating
    a deep learning model – first you define the architecture of the model and then
    you train the model. You create *layers *for your deep learning model and each
    layer has symbols that are placeholders. So for example, the first layer is usually:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从编码的角度来看，这意味着创建深度学习模型有两个步骤——首先定义模型的架构，然后训练模型。你为深度学习模型创建*层*，每一层都有作为占位符的符号。例如，第一层通常是：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`data` is a placeholder for the input, which we will insert later. The output
    of each layer feeds into the next layer as input. This might be a convolutional
    layer, a dense layer, an activation layer a dropout layer, and so on. The following
    code example shows how the layers continue to feed into each other; this is taken
    from a full example later in this chapter. Notice how the symbol for each layer
    is used as input in the next layer, this is how the model is built layer after
    layer. The `data1` symbol is passed into the first call to `mx.symbol.FullyConnected`,
    the `fc1` symbol is passed into the first call to `mx.symbol.Activation`, and
    so on.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`data` 是输入的占位符，我们稍后会插入数据。每一层的输出作为下一层的输入。这可能是一个卷积层、一个全连接层、一个激活层、一个丢弃层等。以下代码示例展示了层与层之间如何继续传递数据；这个示例来自本章后面完整的例子。注意每一层的符号如何作为下一层的输入，这就是模型如何一层一层构建的方式。`data1`
    符号传递给第一次调用的 `mx.symbol.FullyConnected`，`fc1` 符号传递给第一次调用的 `mx.symbol.Activation`，依此类推。'
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When you execute this code, it runs instantly as nothing is executed at this
    stage. Eventually, you pass the last layer into a function to train the model.
    In MXNet, this is the `mx.model.FeedForward.create` function. At this stage, the
    computation graph is computed and the model begins to be trained:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行这段代码时，它会立即运行，因为在这个阶段没有任何操作会被执行。最终，你将最后一层传入一个函数来训练模型。在MXNet中，这是 `mx.model.FeedForward.create`
    函数。在这一阶段，计算图会被计算出来，模型开始训练：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is when the deep learning model is created and trained. More information
    on the MXNet architecture is available online; the following links will get you
    started:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习模型被创建并训练的过程。关于MXNet架构的更多信息可以在线找到；以下链接将帮助你入门：
- en: '[https://mxnet.apache.org/tutorials/basic/symbol.html](https://mxnet.apache.org/tutorials/basic/symbol.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mxnet.apache.org/tutorials/basic/symbol.html](https://mxnet.apache.org/tutorials/basic/symbol.html)'
- en: '[https://mxnet.incubator.apache.org/architecture/program_model.html](https://mxnet.incubator.apache.org/architecture/program_model.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mxnet.incubator.apache.org/architecture/program_model.html](https://mxnet.incubator.apache.org/architecture/program_model.html)'
- en: Deep learning layers
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习层
- en: In the earlier code snippets, we saw some layers for a deep learning model,
    including `mx.symbol.FullyConnected`, `mx.symbol.Activation`, and `mx.symbol.Dropout`.
    Layers are how models are constructed; they are computational transformations
    of data. For example, `mx.symbol.FullyConnected` is the first type of layer operation
    we matrix operation we introduced in [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),
    *Getting Started with Deep Learning*. It is *fully connected* because all input
    values are connected to all nodes in the layer. In other deep learning libraries,
    such as Keras, it is called a **dense** layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们看到了一些深度学习模型的层，包括 `mx.symbol.FullyConnected`、`mx.symbol.Activation`
    和 `mx.symbol.Dropout`。层是模型构建的方式；它们是数据的计算变换。例如，`mx.symbol.FullyConnected` 是我们在[第1章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)
    *深度学习入门* 中介绍的矩阵操作的第一种层操作。它是*全连接*的，因为所有输入值都与层中的所有节点相连接。在其他深度学习库中，如Keras，它被称为**密集层**。
- en: 'The `mx.symbol.Activation` layer performs an activation function on the output
    of the previous layer. The `mx.symbol.Dropout` layer performs dropout on the output
    from the previous layer. Other common layer types in MXNet are:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`mx.symbol.Activation` 层对前一层的输出执行激活函数。`mx.symbol.Dropout` 层对前一层的输出执行丢弃操作。MXNet中的其他常见层类型包括：'
- en: '`mxnet.symbol.Convolution`: Performs a convolutional operation that matches
    patterns across the data. It is mostly used in computer vision tasks, which we
    will see in [Chapte](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)[r](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)
    [5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification Using Convolutional
    Neural Networks*. They can also be used for Natural Language Processing, which
    we will see in [Chapter 6](03f666ab-60ce-485a-8090-c158b29ef306.xhtml), *Natural
    Language Processing Using Deep Learning*.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mxnet.symbol.Convolution`：执行卷积操作，匹配数据中的模式。它主要用于计算机视觉任务，我们将在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中看到，*使用卷积神经网络进行图像分类*。它们也可以用于自然语言处理，我们将在[第6章](03f666ab-60ce-485a-8090-c158b29ef306.xhtml)中看到，*使用深度学习进行自然语言处理*。'
- en: '`mx.symbol.Pooling`: Performs pooling on the output from the previous layer.
    Pooling reduces the number of elements by taking the average, or max value, from
    sections of the input. These are commonly used with convolutional layers.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mx.symbol.Pooling`：对前一层的输出进行池化操作。池化通过取输入部分的平均值或最大值来减少元素的数量。这些操作通常与卷积层一起使用。'
- en: '`mx.symbol.BatchNorm`: Used to normalize the weights from the previous layer.
    This is done for the same reason you normalize input data before model-building:
    it helps the model to train better. It also prevents vanishing and exploding gradients
    where gradients get very, very small or very, very large during training. This
    can cause the model to fail to converge, that is, training will fail.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mx.symbol.BatchNorm`：用于对前一层的权重进行归一化。这样做的原因与归一化输入数据类似：有助于模型更好地训练。它还可以防止梯度消失和梯度爆炸问题，避免在训练过程中梯度变得非常小或非常大，这可能导致模型无法收敛，即训练失败。'
- en: '`mx.symbol.SoftmaxOutput`: Calculates a softmax result from the output from
    the previous layer.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mx.symbol.SoftmaxOutput`：从前一层的输出中计算Softmax结果。'
- en: There are recognized patterns for using these layers, for example, an activation
    layer normally follows a fully-connected layer. A dropout layer is usually applied
    after the activation function, but can be between the fully connected layer and
    the activation function. Convolutional layers and pooling layers are often used
    together in image tasks in that order. At this stage, there is no need to try
    to memorize when to use these layers; you will encounter plenty of examples in
    the rest of this book!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些层时有一定的规律，例如，激活层通常跟在全连接层后面。Dropout层通常应用在激活函数之后，但也可以放在全连接层和激活函数之间。卷积层和池化层通常在图像任务中按此顺序一起使用。此时，不必强迫自己记住何时使用这些层；在本书的后续章节中你将遇到大量实例！
- en: If all this seems confusing, take some comfort in knowing that a lot of the
    difficult work in applying these layers is abstracted away from you. In the previous
    chapter, when we built a neural network, we had to manage all the input output
    from the layers. This meant ensuring that the matrix dimensions were correct so
    that the operations worked. Deep Learning libraries, such as MXNet and TensorFlow,
    take care of this for you.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些内容让你感到困惑，可以放心，很多应用这些层的复杂工作已经被抽象出来，不需要你亲自处理。在上一章，我们建立神经网络时，需要管理各层的输入输出。这意味着必须确保矩阵的维度正确，以便操作能够顺利进行。深度学习库，如MXNet和TensorFlow，会为你处理这些问题。
- en: Building a deep learning model
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度学习模型
- en: 'Now that we have covered the basics, let''s look at building our first true
    deep learning model! We will use the `UHI HAR` dataset that we used in [Chapter
    2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training a Prediction Model*.
    The following code does some data preparation: it loads the data and selects only
    the columns that store mean values (those that have the word `mean` in the column
    name). The `y` variables are from 1 to 6; we will subtract one so that the range
    is 0 to 5\. The code for this section is in `Chapter4/uci_har.R`. It requires
    the `UHI HAR` dataset to be in the data folder; download it from [https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) and
    unzip it into the data folder:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了基础知识，让我们来看看如何构建第一个真正的深度学习模型！我们将使用在[第2章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)中使用过的`UHI
    HAR`数据集，*训练预测模型*。以下代码进行了一些数据准备：它加载数据并只选择存储均值的列（那些列名中包含`mean`的）。`y`变量的取值从1到6；我们将减去1，使其范围变为0到5。此部分的代码位于`Chapter4/uci_har.R`中。它需要将`UHI
    HAR`数据集放在数据文件夹中；可以从[https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)下载并解压到数据文件夹中：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will transpose the data and convert it into a matrix. MXNet expects
    the data to be width `x` height rather than height `x` width:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转置数据并将其转换为矩阵。MXNet期望数据的格式为宽度`x`高度，而不是高度`x`宽度：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next step is to define the computation graph. We create a placeholder for
    the data and create two fully connected (or dense) layers followed by relu activations.
    The first layer has 64 nodes and the second layer has 32 nodes. We create a final fully-connected
    layer with six nodes – the number of distinct classes in our y variable. We use
    a softmax activation to convert the numbers from the last six nodes into probabilities
    for each class:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义计算图。我们创建一个占位符来存储数据，并创建两个全连接（或密集）层，后面跟着relu激活函数。第一层有64个节点，第二层有32个节点。然后我们创建一个最终的全连接层，包含六个节点——这是我们的y变量中的不同类别数。我们使用softmax激活函数，将最后六个节点的数值转换为每个类别的概率：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When you run the previous code, nothing actually executes. To train the model,
    we create a `devices` object to indicate where the code should be run, CPU or
    GPU. Then you pass the symbol for last layer (softmax) into the `mx.model.FeedForward.create`
    function. This function has other parameters, which are more properly known as
    hyper-parameters. These include the epochs (`num.round`), which control how many
    times we pass through the data, the learning rate (`learning.rate`), which controls
    how much the gradients are updated during each pass, momentum (`momentum`), which
    is a hyper-parameter that can help the model to train faster, and the weights
    initializer (`initializer`), which controls how the weights and biases for nodes
    are initially set. We also pass in the evaluation metric (`eval.metric`),which
    is how the model is to be evaluated, and a callback function (`epoch.end.callback`),
    which is used to output progress information. When we run the function, it trains
    the model and outputs the progress as per the value we used for the `epoch.end.callback`
    parameter, namely every epoch:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行前面的代码时，实际上什么也不会执行。为了训练模型，我们创建一个`devices`对象，指示代码应该在哪运行，CPU还是GPU。然后将最后一层的符号（softmax）传递给`mx.model.FeedForward.create`函数。这个函数还有其他参数，更恰当地说它们是超参数。包括epochs（`num.round`），它控制我们遍历数据的次数；学习率（`learning.rate`），它控制每次遍历时梯度更新的幅度；动量（`momentum`），这是一个可以帮助模型更快训练的超参数；以及权重初始化器（`initializer`），它控制节点的权重和偏置是如何初始化的。我们还传递了评估指标（`eval.metric`），即模型的评估方式，以及回调函数（`epoch.end.callback`），它用于输出进度信息。当我们运行该函数时，它会训练模型并根据我们在`epoch.end.callback`参数中使用的值输出进度信息，即每个epoch：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we have trained our model, let''s see how it does on the test set:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了模型，让我们看看它在测试集上的表现：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Not bad! We have achieved an accuracy of `87.11%` on our test set.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！我们在测试集上的准确率达到了`87.11%`。
- en: Wait, where are the backward propagation, derivatives, and so on, that we covered
    in previous chapters? The answer to that is deep learning libraries largely manage
    this automatically for you. In MXNet, automatic differentiation is included in
    a package called the autograd package, which differentiates a graph of operations
    with the chain rule. It is one less thing to worry about when building deep learning
    models. For more information, go to [https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html](https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，我们在前面的章节中覆盖的反向传播、导数等在哪里？答案是深度学习库大部分情况下会自动管理这些问题。在 MXNet 中，自动微分包含在一个称为 autograd
    包中，该包使用链式法则对操作图进行微分。在构建深度学习模型时，这是少去的一件烦恼。更多信息，请访问 [https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html](https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html)。
- en: Use case – using MXNet for classification and regression
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Use case – 使用 MXNet 进行分类和回归
- en: 'In this section, we will use a new dataset to create a binary classification
    task. The dataset we will use here is a transactional dataset that is available
    at [https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles).
    This dataset has been made available from dunnhumby, which is perhaps best known
    for its link to the Tesco (a British grocery store) club-card, which is one of
    the largest retail loyalty systems in the world. I recommend the following book,
    which describes how dunnhumby helped Tesco to become the number one retailer by
    applying analytics to their retail loyalty program: *Humby, Clive, Terry Hunt,
    and Tim Phillips. Scoring points. Kogan Page Publishers, 2008*. Even though this
    book is relatively old, it remains one of the best use cases to describe how to
    roll out a business-transformation program based on data analytics.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个新数据集创建一个二分类任务。我们将在这里使用的数据集是一个可在 [https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles)
    获取的交易数据集。这个数据集是由 dunnhumby 提供的，他们因与 Tesco（一家英国超市）会员卡的关联而闻名，该会员卡是世界上最大的零售忠诚系统之一。我推荐阅读以下书籍，描述了
    dunnhumby 如何通过分析他们的零售忠诚计划帮助 Tesco 成为第一零售商：*Humby, Clive, Terry Hunt, and Tim Phillips.
    Scoring points. Kogan Page Publishers, 2008*。尽管这本书相对较旧，但仍然是描述如何基于数据分析推出业务转型计划的最佳案例之一。
- en: Data download and exploration
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据下载和探索
- en: When you go to the preceding link, there are a few different data options; the
    one we will use is called **Let’s Get Sort-of-Real**. This dataset is data for
    over two years for a fictional retail loyalty scheme. The data consists of purchases
    that are linked by basket ID and customer code, that is, we can track transactions
    by customers over time. There are a number of options here, including the full
    dataset, which is 4.3 GB zipped and over 40 GB unzipped. For our first models,
    we will use the smallest dataset, and will download the data titled **All transactions
    for a randomly selected sample of 5,000 customers**; this is 1/100^(th) the size
    of the full database.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问上述链接时，会有几个不同的数据选项；我们将使用的是称为 **Let’s Get Sort-of-Real** 的数据集。这个数据集是一个虚构零售忠诚计划的超过两年的数据。数据包括按篮子ID和顾客代码链接的购买记录，也就是说，我们可以追踪顾客随时间的交易。这里有多种选项，包括完整的数据集，压缩后大小为
    4.3 GB，解压后超过 40 GB。对于我们的第一个模型，我们将使用最小的数据集，并将下载名为 **All transactions for a randomly
    selected sample of 5,000 customers** 的数据；这相当于完整数据库的 1/100 大小。
- en: I wish to thank dunnhumby for releasing this dataset and for allowing us permission
    to use it. One of the problems in deep learning and machine learning in general
    is the lack of large scale real-life datasets that people can practice their skills
    on. When a company makes the effort to release such a dataset, we should appreciate
    the effort and not use the dataset outside the terms and conditions specified.
    Please take the time to read the terms and conditions and use the dataset for
    personal learning purposes only. Remember that any misuse of this dataset (or
    datasets released by other companies) means that companies will be more reluctant
    to make other datasets available in the future.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢 dunnhumby 发布这个数据集，并允许我们使用它。深度学习和机器学习中的一个问题是缺乏大规模真实生活数据集，供人们练习他们的技能。当一家公司努力发布这样一个数据集时，我们应该感谢他们的努力，并且不要在未经授权的情况下使用该数据集。请花时间阅读条款和条件，并仅将数据集用于个人学习目的。请记住，任何对这个数据集（或其他公司发布的数据集）的滥用，都会使公司更不愿意在未来发布其他数据集。
- en: 'Once you have read the terms and conditions and downloaded the dataset to your
    computer, unzip it into a directory called `dunnhumby/in` under the `code` folder.
    Ensure the files are unzipped directly under this folder, and not a sub-directory,
    or you may have to copy them after unzipping the data. The data files are in **comma-delimited**
    (**CSV**) format, with a separate file for each week of data. The files can be
    opened and viewed using a text editor. We will use some of the fields in *Table
    4.1* for our analysis:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完条款和条件并将数据集下载到你的电脑后，请将其解压到`code`文件夹下的`dunnhumby/in`目录中。确保文件直接解压到该文件夹下，而不是子目录中，否则你可能需要在解压后手动复制文件。数据文件采用**逗号分隔**（**CSV**）格式，每周一个独立文件。这些文件可以通过文本编辑器打开查看。我们将在分析中使用*表4.1*中的一些字段：
- en: '| **Field name** | **Description** | **Format** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **字段名称** | **描述** | **格式** |'
- en: '| `BASKET_ID` | Basket ID, or transaction ID. All items in a basket share the
    same `basket_id` value. | Numeric |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `BASKET_ID` | 购物篮ID或交易ID。一个购物篮中的所有商品共享相同的`basket_id`值。 | 数值 |'
- en: '| `CUST_CODE` | Customer Code. This links the transactions/visits to a customer.
    | Char |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `CUST_CODE` | 客户代码。将交易/访问与客户关联。 | 字符 |'
- en: '| `SHOP_DATE` | Date when shopping occurred. Date is specified in the yyyymmdd
    format. | Char |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `SHOP_DATE` | 购物发生的日期。日期采用yyyymmdd格式。 | 字符 |'
- en: '| `STORE_CODE` | Store code. | Char |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `STORE_CODE` | 店铺代码。 | 字符 |'
- en: '| `QUANTITY` | Number of items of the same product bought in this basket. |
    Numeric |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `QUANTITY` | 在该购物篮中购买的相同商品数量。 | 数值 |'
- en: '| `SPEND` | Spend associated to the items bought. | Numeric |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `SPEND` | 与购买的商品相关的消费金额。 | 数值 |'
- en: '| `PROD_CODE` | Product Code. | Char |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE` | 产品代码。 | 字符 |'
- en: '| `PROD_CODE_10` | Product Hierarchy Level 10 Code. | Char |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_10` | 产品层级10代码。 | 字符 |'
- en: '| `PROD_CODE_20` | Product Hierarchy Level 20 Code. | Char |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_20` | 产品层级20代码。 | 字符 |'
- en: '| `PROD_CODE_30` | Product Hierarchy Level 30 Code. | Char |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_30` | 产品层级30代码。 | 字符 |'
- en: '| `PROD_CODE_40` | Product Hierarchy Level 40 Code. | Char |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_40` | 产品层级40代码。 | 字符 |'
- en: 'Table 4.1: Partial data dictionary for transactional dataset'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1：交易数据集的部分数据字典
- en: The data stores details of customer transactions. Every unique item that a person
    purchases in a shopping transaction is represented by one line, and items in a
    transaction will have the same `BASKET_ID` field. A transaction can also be linked
    to a customer using the `CUST_CODE` field. A PDF is included in the ZIP files
    if you want more information on the field types.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据存储了客户交易的详细信息。每个客户在购物交易中购买的独特商品都由一行表示，且同一交易中的商品会有相同的`BASKET_ID`字段。交易还可以通过`CUST_CODE`字段与客户关联。如果你想了解更多字段类型的信息，ZIP文件中包含了一个PDF文件。
- en: We are going to use this dataset for a churn prediction task. A churn prediction
    task is where we predict which customers will return in the next `x` days. Churn
    prediction is used to find customers who are in danger of leaving your program.
    It is used by companies in shopping loyalty schemes, mobile phone subscriptions,
    TV subscriptions, and so on to ensure they maintain enough customers. For most
    companies that rely on revenue from recurring subscriptions, it is more effective
    to spend resources on maintaining their existing customer base than trying to
    acquire new customers. This is because of the high cost of acquiring new customers.
    Also, as time progresses after a customer has left, it is harder to win them back,
    so there is a small window of time in which to send them special offers that may
    entice them to stay.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用该数据集进行客户流失预测任务。流失预测任务是指预测客户在接下来的`x`天内是否会回归。流失预测用于找出那些有可能离开你的服务的客户。购物忠诚计划、手机订阅、电视订阅等公司都会使用流失预测，以确保他们保持足够的客户数量。对于大多数依赖于订阅收入的公司来说，投入资源维持现有客户群比去获取新客户更为有效。这是因为获取新客户的成本较高。此外，随着时间推移，客户一旦流失，再将其吸引回来变得越来越难，因此在这段小的时间窗口内向他们发送特别优惠可能会促使他们留下。
- en: As well as binary classification, we will build a regression model. This will
    predict the amount that a person will spend in the next 14 days. Fortunately,
    we can build a dataset that is suitable for both prediction tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了二分类任务外，我们还将建立一个回归模型。该模型将预测一个人在未来14天内的消费金额。幸运的是，我们可以构建一个适合这两种预测任务的数据集。
- en: 'The data was supplied as 117 CSV files (ignore `time.csv`, which is a lookup
    file). The first step is to perform some basic data exploration to verify that
    the data was downloaded successfully and then perform some basic data quality
    checks. This is an important first step in any analysis: especially when you are
    using an external dataset, you should run some validation checks on the data before
    creating any machine learning models. The `Chapter4/0_Explore.Rmd` script creates
    a summary file and does some exploratory analysis of the data. This is an RMD
    file, so it needs to be run from RStudio. For brevity, and because this book is
    about deep learning and not data processing, I will include just some of the output
    and plots from this script rather than reproducing all the code. You should also
    run the code in this file to ensure the data was imported correctly, although
    it may take a few minutes the first time it runs. Here are some summaries on the
    data from that script:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以117个CSV文件的形式提供（忽略`time.csv`，它是一个查找文件）。第一步是进行一些基本的数据探索，验证数据是否已成功下载，然后执行一些基本的数据质量检查。这是任何分析中的重要第一步：尤其是当你使用外部数据集时，在创建任何机器学习模型之前，你应该对数据运行一些验证检查。`Chapter4/0_Explore.Rmd`脚本创建了一个汇总文件，并对数据进行了探索性分析。这是一个RMD文件，因此需要在RStudio中运行。为了简洁起见，并且因为本书是关于深度学习而非数据处理的，我将只包含部分输出和图表，而不是重现所有的代码。你也应该运行这个文件中的代码，以确保数据正确导入，尽管第一次运行可能需要几分钟时间。以下是该脚本中的一些数据汇总：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we compare this to the website and the PDF, it looks in order. We have over
    2.5 million records, and data for 5,000 customers across 761 stores. The data-exploration
    script also creates some plots to give us a feel for the data. *Figure 4.3* shows
    the sales over the 117 weeks; we see the variety in the data (it is not a flat
    line indicating that each day is different) and there are no gaps indicating missing
    data. There are seasonal patterns, with large peaks toward the end of the calendar
    year, namely the holiday season:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其与网站和PDF进行对比，数据看起来是有序的。我们有超过250万条记录，并且数据来自5,000个客户，跨越761个商店。数据探索脚本还会生成一些图表，帮助我们感受数据。*图4.3*展示了117周内的销售数据；我们可以看到数据的多样性（它不是一条平坦的线，表明每天的数据都是不同的），并且没有缺失数据的间隙。数据中存在季节性模式，特别是在日历年的年底，即假日季节：
- en: '![](img/8165fb22-2fed-4e42-bfbc-46083cd50ba8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8165fb22-2fed-4e42-bfbc-46083cd50ba8.png)'
- en: 'Figure 4.3: Sales plotted over time.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：按时间绘制的销售数据。
- en: The plot in figure 4.3 shows that the data has been imported successfully. The
    data looks consistent and is what we expect for a retail transaction file, we
    do not see any gaps and there is seasonality.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3中的图表表明数据已成功导入。数据看起来一致，符合零售交易文件的预期，我们没有看到任何缺失数据，并且存在季节性波动。
- en: 'For each item a person purchases, there is a product code (`PROD_CODE`) and
    four department codes (`PROD_CODE_10`, `PROD_CODE_20`, `PROD_CODE_30`, `PROD_CODE_40`). We
    will use these department codes in our analysis; the code in `Chapter4/0_Explore.Rmd`
    creates a summary for them. We want to see how many unique values there are for
    each department code, whether the codes represent a hierarchy (each code has at
    most one parent), and whether there are repeated codes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个个人购买的商品，都有一个产品代码（`PROD_CODE`）和四个部门代码（`PROD_CODE_10`、`PROD_CODE_20`、`PROD_CODE_30`、`PROD_CODE_40`）。我们将在分析中使用这些部门代码；`Chapter4/0_Explore.Rmd`中的代码会为它们创建一个汇总。我们希望查看每个部门代码的独特值有多少，代码是否代表了一个层级结构（每个代码最多只有一个父级），以及是否存在重复代码：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We have 4,997 unique product codes with 4 department codes. Our department
    codes go from `PROD_CODE_10`, which has 250 unique codes, to `PROD_CODE_40`, which
    has 9 unique codes. This is a product department code hierarchy, where `PROD_CODE_40` is
    the primary category and `PROD_CODE_10` is the lowest department code in the hierarchy.
    Each code in `PROD_CODE_10`,`PROD_CODE_20`, and`PROD_CODE_30` has only one parent;
    for example, there are no repeating codes, that is, a department code belongs
    in only one super-category. We are not given a lookup file to say what these codes
    represent, but an example of a product code hierarchy for a product might be something
    similar to this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有4,997个独特的产品代码和4个部门代码。我们的部门代码从`PROD_CODE_10`开始，它有250个独特的代码，到`PROD_CODE_40`，它有9个独特的代码。这是一个产品部门代码层级结构，其中`PROD_CODE_40`是主类目，`PROD_CODE_10`是层级中最底层的部门代码。在`PROD_CODE_10`、`PROD_CODE_20`和`PROD_CODE_30`中的每个代码只有一个父级；例如，没有重复的代码，即每个部门代码只属于一个上级分类。我们没有提供查找文件来说明这些代码代表什么，但一个产品代码层级的例子可能类似于下面这样：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To get a sense of these department codes, we can also plot the sales data over
    time by the number of unique product department codes in *Figure 4.4*. This plot
    is also created in `Chapter4/0_Explore.Rmd`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些部门代码，我们还可以根据独特产品部门代码的数量绘制时间序列的销售数据，如*图4.4*所示。这个图表也是在`Chapter4/0_Explore.Rmd`中创建的：
- en: '![](img/6eabed35-9c9b-4357-951d-7fe5c1e250a5.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6eabed35-9c9b-4357-951d-7fe5c1e250a5.png)'
- en: 'Figure 4.4: Unique product codes purchased by date'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：按日期购买的独特产品代码
- en: Note that for this graph, the *y* axis is unique product codes, not sales. This
    data also looks consistent; there are some peaks and dips in the data, but they
    are not as pronounced as in *Figure 4.3*, which is as expected.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于此图，*y*轴表示独特的产品代码，而非销售数据。该数据看起来也很一致；虽然数据中有一些波峰和波谷，但它们不像*图4.3*中那样显著，这是符合预期的。
- en: Preparing the data for our models
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为我们的模型准备数据
- en: Now that we have downloaded and validated the data, we can use it to create
    a dataset for our binary classification and regression model tasks. We want to
    be able to predict which customers will visit the shop in the next two weeks for
    the binary classification task, and how much they will spend in the next two weeks
    for the regression task. The `Chapter4/prepare_data.R` script transforms the raw
    transactional data into a format suitable for machine learning. You need to run
    the code to create the dataset for the models, but you do not have to understand
    exactly how it works. Feel free to skip ahead if you want to focus on the deep
    learning model building.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载并验证了数据，我们可以用它来为我们的二分类和回归模型任务创建数据集。对于二分类任务，我们希望能够预测哪些客户将在接下来的两周内访问商店，对于回归任务，我们希望预测他们将在接下来的两周内消费多少钱。`Chapter4/prepare_data.R`脚本将原始交易数据转换为适合机器学习的格式。你需要运行代码来为模型创建数据集，但不必完全理解它是如何工作的。如果你想专注于深度学习模型的构建，可以跳过这部分内容。
- en: We need to transform our data into a suitable format for prediction tasks. This
    should be a single row for each instance we want to predict. The columns will
    include some fields that are features (`X` variables) and another field that is
    our predictor value (`Y` variable). We want to predict whether a customer returns
    or not and their spend, so our dataset will have a single row per customer with
    features and predictor variables.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据转换为适合预测任务的格式。每个我们要预测的实例应该只有一行。列将包括一些特征字段（`X`变量）和一个预测值字段（`Y`变量）。我们希望预测客户是否回访及他们的消费情况，因此我们的数据集每个客户将有一行，包含特征和预测变量。
- en: 'The first step is to find the cut-off date that separates the variables used
    to predict (`X`) and the variable we will predict for (`Y`). The code looks at
    the data, finds the last transaction date; and then subtracts 13 days from that
    date. This is a cut-off date, we want to predict which customers will spend in
    our shops *on or after* the cut-off date; based on what happens *before* the cut-off
    date. The data before the cut-off date will be used to make our X, or feature
    variables, and sales data on or after the cut-off date will be used to make our
    Y, or predictor variables. The following is that part of the code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是找到一个截止日期，该日期将用于区分用于预测的变量（`X`）和我们将要预测的变量（`Y`）。代码会查看数据，找到最后一个交易日期；然后从该日期减去13天。这个日期就是截止日期，我们希望预测哪些客户将在截止日期*及之后*在我们的商店消费；根据截止日期*之前*发生的情况。截止日期之前的数据将用于生成我们的X或特征变量，截止日期及之后的销售数据将用于生成我们的Y或预测变量。以下是这部分代码：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If this code does not run, the most probable reason is that the source data
    was not saved in the correct location. The dataset must be unzipped into a directory
    called dunnhumby/in under the code folder, that is, at the same level as the chapter
    folders.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此代码无法运行，最可能的原因是源数据未保存在正确的位置。数据集必须解压到名为dunnhumby/in的目录下，位于代码文件夹中，即与章节文件夹处于同一级别。
- en: The last date in our data is `20080706`, which July 7^(th), 2008, and the cut-off
    date is June 23^(rd), 2008\. Although we have data going back to 2006, we will
    only use sales data from 2008\. Any data that is older than six months is unlikely
    to influence a future customer sale. The task is to predict whether a customer
    will return between June 23^(rd), 2008 - July 7^(th), 2008 based on their activity
    before June 23^(rd), 2008.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据中的最后日期是`20080706`，即2008年7月7日，截止日期是2008年6月23日。尽管我们有从2006年开始的数据，但我们只会使用2008年的销售数据。任何超过六个月的数据不太可能影响未来的客户销售。任务是预测客户是否会在2008年6月23日至2008年7月7日之间回访，基于他们在2008年6月23日之前的活动。
- en: 'We now need to create features from our data; so that we can use the spend
    broken down by department code, we will use the `PROD_CODE_40` field. We could
    just group the sales on this department code, but that would give equal weighting
    to spends in Jan 2008 as to spends in June 2008\. We would like to incorporate
    some time factor in our predictor columns. Instead, we will create features on
    a combination of the department code and the week. This will allow our models
    to place more importance on recent activities. First, we group by customer code,
    week, and department code, and create the `fieldName` column. We then pivot this
    data to create our features (`X`) dataset. The cell values in this dataset are
    the sales for that customer (row) and that week-department code (column). Here
    is an example of how the data is transformed for two customers. *Table 4.2* shows
    the sales spend by week and the `PROD_CODE_40` field. *Table 4.3* then uses a
    pivot to create a dataset that has a single row per customer and the aggregate
    fields are now columns with the spend as values:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要从数据中创建特征；为了使用按部门代码拆分的消费数据，我们将使用`PROD_CODE_40`字段。我们可以直接按该部门代码对销售数据进行分组，但这将使得2008年1月和2008年6月的消费权重相等。我们希望在预测变量列中加入时间因素。因此，我们将创建一个部门代码和周组合的特征，这将使我们的模型更加重视近期的活动。首先，我们按客户代码、周和部门代码进行分组，并创建`fieldName`列。然后，我们将这些数据进行透视，创建我们的特征（`X`）数据集。该数据集的单元格值表示该客户（行）和该周-部门代码（列）的销售额。以下是两个客户数据转换后的示例。*表
    4.2*显示了按周和`PROD_CODE_40`字段的销售支出。然后，*表 4.3*通过透视创建了一个数据集，数据集中每个客户只有一行，聚合字段现在作为列，销售额作为值：
- en: '| `CUST_CODE` | `PROD_CODE_40` | `SHOP_WEEK` | `fieldName` | `Sales` |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `CUST_CODE` | `PROD_CODE_40` | `SHOP_WEEK` | `fieldName` | `Sales` |'
- en: '| `cust_001` | D00001 | 200801 | `D00001_200801` | 10.00 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `cust_001` | D00001 | 200801 | `D00001_200801` | 10.00 |'
- en: '| `cust_002` | D00001 | 200801 | `D00001_200801` | 12.00 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `cust_002` | D00001 | 200801 | `D00001_200801` | 12.00 |'
- en: '| `cust_001` | D00015 | 200815 | `D00015_200815` | 15.00 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `cust_001` | D00015 | 200815 | `D00015_200815` | 15.00 |'
- en: '| `cust_001` | D00020 | 200815 | `D00020_200815` | 20.00 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `cust_001` | D00020 | 200815 | `D00020_200815` | 20.00 |'
- en: '| `cust_002` | D00030 | 200815 | `D00030_200815` | 25.00 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `cust_002` | D00030 | 200815 | `D00030_200815` | 25.00 |'
- en: 'Table 4.2: Summary of sales by customer code, department code, and week'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2：按客户代码、部门代码和周汇总的销售数据
- en: '| `CUST_CODE` | `D00001_200801` | `D00015_200815` | `D00020_200815` | `D00030_200815`
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `CUST_CODE` | `D00001_200801` | `D00015_200815` | `D00020_200815` | `D00030_200815`
    |'
- en: '| `cust_001` | 10.00 | 15.00 | 20.00 |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `cust_001` | 10.00 | 15.00 | 20.00 |  |'
- en: '| `cust_002` | 12.00 |  |  | 25.00 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| `cust_002` | 12.00 |  |  | 25.00 |'
- en: 'Table 4.3: Data from Table 4.2 after transformation'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3：表 4.2 转换后的数据
- en: 'Here is the code that does this transformation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行此转换的代码：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The predictor (`Y`) variable is a flag as to whether the customer visited the
    site on the weeks from `200818` to `200819`. We perform a grouping on the data
    after the cut-off date and group the sales by customer, these form the basis of
    our `Y` values. We join the `X` and `Y` datasets, ensuring that we keep all rows
    on the `X` side by doing a left-join. Finally we create a 1/0 flag for our binary
    classification task. When we are done, we see that we have `3933` records in our
    dataset : `1560` customers who did not return and `2373` customers who did. We
    finish by saving our file for the model-building. The following code shows these
    steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 预测变量（`Y`）是一个标志，表示客户是否在`200818`到`200819`周期间访问过该网站。我们在截止日期之后对数据进行分组，并按客户对销售数据进行分组，这些数据将形成我们`Y`值的基础。我们将`X`和`Y`数据集合并，确保通过左连接保留`X`端的所有行。最后，我们为二分类任务创建一个1/0标志。当我们完成时，我们看到数据集中有`3933`条记录：`1560`个未返回的客户和`2373`个已返回的客户。我们通过保存文件来完成模型构建的准备。以下代码展示了这些步骤：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We use the sales data to create our predictor fields, but there were some customer
    attributes that we ignored for this task. These fields included `Customers Price Sensitivity` and
    `Customers Lifestage`. The main reason we did not use these fields was to avoid
    data-leakage. Data-leakage can occur when building prediction models; it occurs
    when some of your fields have values that will not be available or different when
    creating datasets in production. These fields could cause data-leakage because
    we do not know when they were set; it could have been when a customer signs up,
    or it could be a process that runs nightly. If these were created after our cut-off
    date, this would mean that these fields could unfairly predict our `Y` variables.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用销售数据来创建我们的预测字段，但在这项任务中，我们忽略了一些客户属性。这些字段包括`Customers Price Sensitivity`和`Customers
    Lifestage`。我们不使用这些字段的主要原因是避免数据泄漏。数据泄漏是在构建预测模型时可能发生的，它发生在某些字段的值在生产环境中创建数据集时不可用或不同。因为我们不知道这些字段的设置时间，它们可能是在客户注册时创建的，也可能是一个每天运行的过程。如果这些字段是在我们的截止日期之后创建的，这意味着它们可能会不公平地预测我们的`Y`变量。
- en: For example, `Customers Price Sensitivity` has values for `Less Affluent`, `Mid
    Market`, and `Up Market`, which probably are derived from what the customer purchases.
    Therefore, using these fields in a churn-prediction task would result in data-leakage
    if these fields were updated after the cut-off date used to create our dataset
    for our prediction models. A value of `Up Market` for `Customers Price Sensitivity` could
    be strongly linked to return spend, but this value is actually a summary of the
    value it is predicting. Data-leakage is one of the main causes of data models
    performing worse in production as the model was trained with data that is linked
    to the Y variable and can never exist in reality. You should always check for
    data-leakage for time-series tasks and ask yourself whether any field (especially
    lookup attributes) could have been modified after the date used to create the
    model data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`Customers Price Sensitivity`字段有`Less Affluent`、`Mid Market`和`Up Market`等值，这些值可能是从客户的购买行为中推导出来的。因此，在进行流失预测任务时，如果这些字段在创建我们预测模型的数据集后的截止日期之后进行了更新，那么使用这些字段将导致数据泄漏。`Customers
    Price Sensitivity`字段中的`Up Market`值可能与回报支出高度相关，但实际上，这个值是其预测值的总结。数据泄漏是数据模型在生产环境中表现不佳的主要原因之一，因为模型是在包含与Y变量相关的数据上进行训练的，而这些数据在现实中永远不会存在。你应当始终检查时间序列任务中的数据泄漏，并问自己是否有任何字段（特别是查找属性）可能在创建模型数据所使用的日期之后被修改。
- en: The binary classification model
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分类模型
- en: The code from the previous section creates a new file called `predict.csv` in
    the `dunnhumby` folder. This dataset has a single row for each customer with a
    0/1 field indicating whether they visited in the last two weeks and predictor
    variables based on sales data before those two weeks. Now we can proceed to build
    some machine learning models. The `Chapter4/binary_predict.R` file contains the
    code for our first prediction task, binary classification. The first part of the
    code loads the data and creates an array of  predictor variables by including
    all columns except the customer ID, the binary classification predictor variable,
    and the regression predictor variable. The feature columns are all numeric fields
    that are heavily right-skewed distributed, so we apply a log transformation to
    those fields. We add `0.01` first to avoid getting a non-numeric result from attempting
    to get a log of a zero value *(log(0)= -Inf)*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节的代码在`dunnhumby`文件夹中创建了一个名为`predict.csv`的新文件。这个数据集为每个客户创建了一行，其中包含一个0/1字段，表示他们在过去两周是否访问过，以及基于这两周前的销售数据的预测变量。现在，我们可以继续构建一些机器学习模型。`Chapter4/binary_predict.R`文件包含了我们第一个预测任务——二分类的代码。代码的第一部分加载数据，并通过包含所有列（除了客户ID、二分类预测变量和回归预测变量）来创建预测变量数组。特征列都是数值型字段，这些字段分布呈现出明显的右偏，因此我们对这些字段应用了对数变换。我们首先加上`0.01`，以避免尝试对零值取对数时得到非数值结果（*log(0)=
    -Inf*）。
- en: 'The following plot shows the data before transformation, on the left, and the
    data after transformation, on the right:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了数据在变换前后的情况，左边是变换前的数据，右边是变换后的数据：
- en: '![](img/024086e4-ca40-4310-977a-96187c710ba4.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/024086e4-ca40-4310-977a-96187c710ba4.png)'
- en: 'Figure 4.5: Distribution of a feature variable before and after transformation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：特征变量在变换前后的分布。
- en: 'The large bar on the left in the second plot is where the original field was
    zero *(log(0+0.01) = -4.6)*. The following code loads the data, performs the log
    transformation, and creates the previous plot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张图中左边的大条表示原始字段为零的地方*（log(0+0.01) = -4.6）*。以下代码加载数据，执行对数变换，并生成前面的图：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Before we train a deep learning model, we train three machine learning models
    – a logistic regression model, a `Random Forest` model, and an `XGBoost` model
    – on the data as a benchmark. This code section contains the data load, transformation,
    and three models:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度学习模型之前，我们先在数据上训练了三种机器学习模型——逻辑回归模型、`Random Forest`模型和`XGBoost`模型——作为基准。这个代码部分包含了数据加载、转换和三个模型：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We create logistic regression, `Random Forest`, and `XGBoost` models for a
    number of reasons. Firstly, most of the work is already done in preparing the
    data, so it is trivial to do so. Secondly, it gives us a benchmark to compare
    our deep learning model to. Thirdly, if there were a problem in the data-preparation
    tasks, these machine learning algorithms would highlight these problems more rapidly
    because they will be quicker than training a deep learning model. In this case,
    we only have a few thousand records, so these machine learning algorithms will
    easily run on this data. If the data were too large for these algorithms, I would
    consider taking a smaller sample and running our benchmark tasks on that smaller sample.
    There are many machine learning algorithms to choose from, but I used these algorithms
    as benchmarks for the following reasons:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了逻辑回归、`Random Forest`和`XGBoost`模型，原因有几个。首先，大部分工作已经在数据准备阶段完成，因此做这些模型是微不足道的。其次，这为我们提供了一个基准，可以将其与深度学习模型进行比较。第三，如果数据准备过程中存在问题，这些机器学习算法会更快速地发现这些问题，因为它们比训练深度学习模型要快。在这种情况下，我们只有几千条记录，因此这些机器学习算法能够轻松运行在这些数据上。如果数据过大而这些算法无法处理，我会考虑取一个较小的样本来运行我们的基准任务。有许多机器学习算法可供选择，但我选择这些算法作为基准，原因如下：
- en: Logistic regression is a basic model and is always a good benchmark to use
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归是一个基础模型，始终是一个不错的基准选择。
- en: '`Random Forest` is known to train well using the default parameters and is
    robust to overfitting and correlated variables (which we have here)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Random Forest`以其默认参数训练表现良好，并且对过拟合和相关变量（我们这里有的）具有鲁棒性。'
- en: '`XGBoost` is consistently rated as the one of the best-performing machine learning
    algorithms'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XGBoost`被一致认为是表现最好的机器学习算法之一。'
- en: All three algorithms achieve a similar amount of accuracy, the highest accuracy
    was achieved by `Random Forest` with an 80.2% accuracy. We now know that this
    dataset is suitable for prediction tasks and we have a benchmark to compare against.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种算法的准确度相似，最高的准确度由`Random Forest`模型取得，准确率为80.2%。我们现在知道这个数据集适用于预测任务，并且有了基准可以进行比较。
- en: 'Now we will build a deep learning model using MXNet:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将使用MXNet构建一个深度学习模型：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The deep learning model achieved a `77.16%` accuracy on the test data, which
    is only beaten by the `Random Forest` model. This shows that a deep learning model
    can be competitive against the best machine learning algorithms. It also shows
    that deep learning models on classification tasks do not always beat other machine
    learning algorithms. We used these models to provide a benchmark, so that we would
    know that our deep learning model was getting decent results; it gives us confidence
    that our deep learning model is competitive.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在测试数据上的准确度为`77.16%`，仅被`Random Forest`模型超越。这表明，深度学习模型可以与最好的机器学习算法竞争。这也表明，深度学习模型在分类任务中并不总是优于其他机器学习算法。我们使用这些模型提供基准，以便知道我们的深度学习模型是否得到了不错的结果；它让我们有信心相信我们的深度学习模型是有竞争力的。
- en: Our deep learning model uses 20% dropout in each layer and weight decay for
    regularization. Without dropout, the model overtrained significantly. This was
    probably because the features are highly correlated, as our columns are the spend
    in various departments. It figures that if one column is for a type of bread,
    and another column is for a type of milk, then these change together, namely someone
    who has more transactions and spends more is likely to buy both.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度学习模型在每一层使用20%的**dropout**并且使用权重衰减进行正则化。如果没有使用**dropout**，模型会明显过拟合。这可能是因为特征之间高度相关，因为我们的列表示的是各个部门的开支。可以推测，如果一列是某种类型的面包，另一列是某种类型的牛奶，那么这些开支会一起变化，也就是说，花费更多且交易更多的人更可能同时购买这两种商品。
- en: The regression model
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归模型
- en: 'The previous section developed a deep learning model for a binary classification
    task, this section develops a deep learning model to predict a continuous numeric
    value, regression analysis. We use the same dataset that we used for the binary
    classification task, but we use a different target column to predict for. In that
    task, we wanted to predict whether a customer would return to our stores in the
    next 14 days. In this task, we want to predict how much a customer will spend
    in our stores in the next 14 days. We follow a similar process; we load and prepare
    our dataset by applying log transformations to the data. The code is in `Chapter4/regression.R`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节开发了一个用于二分类任务的深度学习模型，本节开发了一个用于预测连续数值的深度学习回归模型。我们使用与二分类任务相同的数据集，但使用不同的目标列进行预测。在那个任务中，我们想要预测客户是否会在接下来的14天内回到我们的商店。而在这个任务中，我们想要预测客户在接下来的14天内将会在我们的商店花费多少。我们遵循类似的流程，通过对数据应用对数变换来加载和准备数据集。代码位于`Chapter4/regression.R`中：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then perform regression analysis on the data using `lm` to create a benchmark
    before creating a deep learning model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`lm`对数据进行回归分析，以便在创建深度学习模型之前建立基准：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We output two metrics, rmse and mae, for our regression task. We covered these
    earlier in the chapter. Mean absolute error measures the absolute differences
    between the predicted value and the actual value. **Root mean squared error**
    (**rmse**) penalizes the square of the differences between the predicted value
    and the actual value, so one big error costs more than the sum of the small errors.
    Now let''s look at the deep learning regression code. First we load the data and
    define the model:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为回归任务输出两个指标，rmse 和 mae。我们在本章前面已经介绍过这些指标。平均绝对误差（mae）衡量的是预测值与实际值之间的绝对差异。**均方根误差**（**rmse**）对预测值与实际值之间差异的平方进行惩罚，因此一个大的错误比多个小错误的总和代价更高。现在，让我们来看一下深度学习回归代码。首先，我们加载数据并定义模型：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we train the model; note that the first comment shows how to switch to
    using a GPU instead of a CPU:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始训练模型；注意，第一个注释展示了如何切换到使用GPU而非CPU：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For regression metrics, lower is better, so our rmse metric on the deep learning
    model (28.92) is an improvement on the original regression model (29.30). Interestingly,
    the mae on the the deep learning model (14.33) is actually worse than the original
    regression model (13.89). Since rsme penalizes big differences between actual
    and predicted values more, this indicates that the errors in the deep learning
    model are less extreme than the regression model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归指标，越低越好，因此我们在深度学习模型上的rmse指标（28.92）比原始回归模型（29.30）有所改进。有趣的是，深度学习模型上的mae（14.33）实际上比原始回归模型（13.89）更差。由于rmse对实际值和预测值之间的大差异惩罚更多，这表明深度学习模型的误差比回归模型的误差更为温和。
- en: Improving the binary classification model
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进二分类模型
- en: This section builds on the earlier binary classification task and looks to increase
    the accuracy for that task. The first thing we can do to improve the model is
    to use more data, 100 times more data in fact! We will download the entire dataset,
    which is over 4 GB data in zip files and 40 GB of data when the files are unzipped.
    Go back to the download link ([https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles))
    and select **Let’s Get Sort-of-Real** again and download all the files for the
    **Full dataset**. There are nine files to download and the CSV files should be
    unzipped into the `dunnhumby/in` folder. Remember to check that the CSV files
    are in this folder and not a subfolder. You need to run the code in `Chapter4/prepare_data.R`
    again. When this completes, the `predict.csv` file should have 390,000 records.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本节基于之前的二分类任务，并着眼于提高该任务的准确性。我们可以改善模型的第一件事是使用更多的数据，实际上是使用100倍的数据！我们将下载整个数据集，压缩文件大小超过4GB，解压后数据大小为40GB。回到下载链接（[https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles)），再次选择**Let’s
    Get Sort-of-Real**，并下载**完整数据集**的所有文件。共有九个文件需要下载，CSV文件应解压到`dunnhumby/in`文件夹中。记得检查CSV文件是否在该文件夹中，而不是在子文件夹中。你需要再次运行`Chapter4/prepare_data.R`中的代码。完成后，`predict.csv`文件应包含390,000条记录。
- en: You can try to follow along here, but be aware that preparing the data and running
    the deep learning model are going to take a long time. You also may run into problems
    if you have a slow computer. I tested this code on an Intel i5 processor with
    32 GB RAM, and it took the model 30 minutes to run. It also requires over 50 GB
    hard disk space to store the unzipped files and temporary files. If you have problems
    running it on your local computer, another option is to run this example in the
    cloud, which we will cover in a later chapter.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试跟着这个步骤进行，但要注意，准备数据并运行深度学习模型将会花费很长时间。如果你的计算机较慢，可能还会遇到一些问题。我在一台配备32 GB RAM的Intel
    i5处理器的电脑上测试了这段代码，模型运行了30分钟。而且，它还需要超过50 GB的硬盘空间来存储解压后的文件和临时文件。如果你在本地计算机上运行时遇到问题，另一种选择是将这个例子运行在云端，我们将在后续章节中介绍。
- en: 'The code for this section is in the `Chapter4/binary_predict2.R` script. Since
    we have more data, we can build a more complicated model. We have 100 times more
    data, so our new model adds an extra layer, and more nodes to our hidden layers.
    We have decreased the amount of regularization and the learning rate. We have
    also added more epochs. Here is the the code in `Chapter4/binary_predict2.R`,
    which constructs and trains the deep learning model. We have not included the
    boilerplate code to load and prepare the data, as that has not changed from the
    original script:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码在`Chapter4/binary_predict2.R`脚本中。由于我们有更多数据，我们可以构建一个更复杂的模型。我们有100倍的数据量，因此我们的新模型增加了一层，并在隐藏层中添加了更多的节点。我们减少了正则化量并调整了学习率，还增加了训练的周期数。下面是`Chapter4/binary_predict2.R`中的代码，构建并训练深度学习模型。我们没有包含加载和准备数据的样板代码，因为这些部分与原始脚本没有变化：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The accuracy has increased from `77.16%` in the earlier model to `77.88%` for
    this model. This may not seem significant, but if we consider that the large dataset
    has almost 390,000 rows, the increase in accuracy of 0.72% represents about 2,808
    customers that are now classified correctly. If each of these customers is worth
    $50, that is an additional $140,000 in revenue.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率已经从早期模型的`77.16%`提高到该模型的`77.88%`。这看起来可能不算显著，但如果考虑到大数据集包含了近39万行数据，准确度提高了0.72%就意味着大约2,808个客户现在被正确分类。如果每个客户的价值为50美元，那么这将为公司带来额外的14万美元收入。
- en: In general, as you add more data, your model should become more complicated
    to generalize across all the patterns in the data. We will cover more of this
    in [Chapter 6](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml), *Tuning and Optimizing
    Models*, but I would encourage you to experiment with the code in `Chapter4/binary_predict.R`.
    Try changing the hyper-parameters or adding more layers. Even a small improvement
    of 0.1 - 0.2% in accuracy is significant. If you manage to get over 78% accuracy
    on this dataset, consider it a good achievement.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，当你增加更多的数据时，你的模型应该变得更加复杂，以便能够在所有数据模式中进行泛化。我们将在[第六章](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml)《调优与优化模型》中详细讲解这一点，但我鼓励你在`Chapter4/binary_predict.R`中实验代码。试着更改超参数或添加更多层。即使准确度提高了0.1%
    - 0.2%，也是一个显著的进步。如果你能在这个数据集上达到超过78%的准确度，那就算是一个不错的成就。
- en: 'If you want to explore further, there are other methods to investigate. These
    involve making changes in how the data for the model is created. If you really
    want to stretch yourself, here are a few more ideas you can try:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步探索，还有其他方法可以调查。这些方法涉及如何创建模型数据的变化。如果你真的想挑战自己，以下是一些你可以尝试的额外想法：
- en: Our current features are a combination of department codes and weeks, we use
    the `PROD_CODE_40` field as the department code. This has only nine unique values,
    so for every week, only nine fields represent that data. If you use `PROD_CODE_30`, `PROD_CODE_20`, or
    `PROD_CODE_10`, you will create a lot more features.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们当前的特征是部门代码和周数的组合，我们使用`PROD_CODE_40`字段作为部门代码。这个字段只有九个唯一值，因此每一周只有九个字段表示这些数据。如果你使用`PROD_CODE_30`、`PROD_CODE_20`或`PROD_CODE_10`，你将会创建更多的特征。
- en: In a similar manner, rather than using a combination of department codes and
    weeks, you could try department codes and day. This might create too many features,
    but I would consider doing this for the last 14 days before the cut-off date.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以类似的方式，你可以尝试使用部门代码和天数，而不是使用部门代码和周数。这可能会产生太多的特征，但我建议你考虑在截止日期之前的最后14天进行此操作。
- en: Experiment with different methods of preparing the data. We use log scale, which
    works well for our binary classification task, but is not the best method for
    a regression task, as it does not create data with a normal distribution. Try
    applying z-scaling and min-max standardization to the data. If you do this, you
    must ensure that it is applied correctly to the test data before evaluating the
    model.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的数据准备方法。我们使用对数尺度，在我们的二分类任务中效果很好，但对于回归任务来说并非最佳方法，因为它不会创建具有正态分布的数据。尝试应用z-scaling和min-max标准化到数据中。如果这样做，必须确保在评估模型之前正确地应用到测试数据上。
- en: The training data uses the sales amount. You could change this to item quantities
    or the number of transactions an item is in.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据使用销售金额。您可以将此更改为商品数量或商品所在的交易次数。
- en: You could create new features. One potentially powerful example would be to
    create fields based on a day of the week, or a day of the month. We could create
    features for the spend amounts and number of visits for each day of the week.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以创建新的特征。一个潜在强大的例子是基于一周中的某一天或月份的字段。我们可以为每周的消费金额和访问次数创建特征。
- en: We could create features based on the average size of a shopping basket, how
    frequently a customer visits, and so on.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以基于购物篮平均大小、顾客访问频率等创建特征。
- en: We could try a different model architecture that can take advantage of time-series
    data.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以尝试一个可以利用时间序列数据的不同模型架构。
- en: These are all things I would try if I was given this task as a work assignment.
    In traditional machine learning, adding more features often leads to problems
    as most traditional machine learning algorithms struggle with high-dimensionality
    data. Deep learning models can handle these cases, so there usually is no harm
    in adding more features.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给我这样一个工作任务，这些都是我会尝试的事情。在传统机器学习中，添加更多特征通常会导致问题，因为大多数传统机器学习算法在高维数据上很难处理。深度学习模型可以处理这些情况，因此通常添加更多特征是没有害处的。
- en: The unreasonable effectiveness of data
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的不合理有效性
- en: 'Our first deep learning models on the binary classification task had fewer
    than 4,000 records. We did this so you could run the example quickly. For deep
    learning, you really need a lot more data, so we created a more complicated model
    with a lot more data, which gave us an increase in accuracy. This process demonstrated
    the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个二分类任务的深度学习模型少于4,000条记录。我们这样做是为了您可以快速运行示例。对于深度学习，您确实需要更多的数据，因此我们创建了一个更复杂的模型，并使用了更多的数据，这提高了我们的准确性。这个过程证明了以下事实：
- en: Establishing a baseline with other machine learning algorithms provides a good
    benchmark before using a deep learning model
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用深度学习模型之前，与其他机器学习算法建立基准提供了一个很好的参考点
- en: We had to create a more complex model and adjust the hyper-parameters for our
    bigger dataset
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不得不创建一个更复杂的模型，并调整我们更大数据集的超参数
- en: The Unreasonable Effectiveness of Data
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的不合理有效性
- en: 'The last point here is borrowed from an article by Peter Norvig, available
    at [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf).
    There is also a YouTube video with the same name. One of the main points in Norvig''s
    article is this: invariably simple models and a lot of data trump more elaborate
    models based on less data.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最后一点来自彼得·诺维格的一篇文章，可在[https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)找到。同名的YouTube视频也有。诺维格文章的一个主要观点是：简单模型和大量数据始终胜过基于较少数据的更复杂模型。
- en: We have increased the accuracy on our deep learning model by 0.38%. Considering
    that our dataset has highly correlated variables and that our domain is modelling
    human activities, this is not bad. People are, well predictable; so when attempting
    to predict what they do next, a small dataset usually works. In other domains,
    adding more data has much more of an effect. Consider a complex image-recognition
    task with color images where the image quality and format are not consistent.
    In that case, increasing our training data by a factor of 10 would have much more
    of an effect than in the earlier example. For many deep learning projects, you
    should include tasks to acquire more data from the very beginning of the project.
    This can be done by manually labeling the data, by outsourcing tasks (Amazon Turk),
    or by building some form of feedback mechanism in your application.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将深度学习模型的准确率提高了0.38%。考虑到我们的数据集包含高度相关的变量，而且我们所处的领域是建模人类活动，这个提升还算不错。人类，嗯，是可以预测的；所以在尝试预测他们接下来做什么时，一个小的数据集通常就足够了。而在其他领域，增加数据往往会有更大的效果。考虑一下一个复杂的图像识别任务，图像包含颜色，且图像的质量和格式不一致。在这种情况下，若将我们的训练数据增加10倍，比之前的例子会产生更大的效果。对于许多深度学习项目，应该从项目一开始就包括获取更多数据的任务。可以通过手动标注数据，外包任务（如
    Amazon Turk），或在你的应用中建立某种形式的反馈机制来完成。
- en: 'While other machine learning algorithms may also see an improvement in performance with
    more data, eventually adding more data will stop making a difference and performance
    will stagnate. This is because these algorithms were never designed for large
    high-dimensional data and so cannot model the complex patterns in very large datasets.
    However, you can build increasingly complex deep learning architectures that can
    model these complex patterns. This following plot illustrates how deep learning
    algorithms can continue to take advantage of more data and performance can still
    improve after performance on other machine algorithms stagnates:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然其他机器学习算法在增加数据后也可能会看到性能提升，但最终增加数据将不再产生任何变化，性能将停滞不前。这是因为这些算法从未设计用于处理大型高维数据，因此无法建模非常大的数据集中的复杂模式。然而，你可以构建越来越复杂的深度学习架构来建模这些复杂的模式。以下图表展示了深度学习算法如何继续利用更多数据，并且在其他机器学习算法的性能停滞后，性能依然能够提升：
- en: '![](img/ae562de4-ee33-4e60-b895-149310aa23f1.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae562de4-ee33-4e60-b895-149310aa23f1.png)'
- en: 'Figure 4.6: How model accuracy increases by dataset size for deep learning
    models versus other machine learning models'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：深度学习模型与其他机器学习模型在数据集大小增加时准确率的变化
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We covered a lot of ground in this chapter. We looked at activation functions
    and built our first true deep learning models using MXNet. Then we took a real-life
    dataset and created two use cases for applying a machine learning model. The first
    use case was to predict which customers will return in the future based on their
    past activity. This was a binary classification task. The second use case was
    to predict how much a customer will spend in the future based on their past activity. This
    was a regression task. We ran both models first on a small dataset and used different
    machine learning libraries to compare them against our deep learning model. Our
    deep learning model out-performed all of the algorithms.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了许多内容。我们研究了激活函数，并使用MXNet构建了我们的第一个真正的深度学习模型。接着，我们使用一个实际的数据集，创建了两个应用机器学习模型的用例。第一个用例是预测哪些客户将来会回来，基于他们过去的活动。这是一个二分类任务。第二个用例是预测客户将来会花费多少钱，基于他们过去的活动。这是一个回归任务。我们先在一个小数据集上运行了两个模型，并使用不同的机器学习库将它们与我们的深度学习模型进行比较。我们的深度学习模型超越了所有算法。
- en: We then took this further by using a dataset that was 100 times bigger. We built
    a larger deep learning model and adjusted our parameters to get an increase in
    our binary classification task accuracy. We finished the chapter with a brief
    discussion on how deep learning models out-perform traditional machine learning
    algorithms on large datasets.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进一步使用了一个比之前大100倍的数据集。我们构建了一个更大的深度学习模型，并调整了参数，提升了我们在二分类任务中的准确性。我们以简短的讨论结束本章，探讨了深度学习模型如何在大数据集上超越传统的机器学习算法。
- en: In the next chapter, we will look at computer vision tasks, which deep learning
    has revolutionized.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨计算机视觉任务，深度学习在这一领域已带来革命性变化。
