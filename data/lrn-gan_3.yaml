- en: Chapter 3. Transfer Image Style Across Various Domains
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。在各个领域之间传递图像风格
- en: Generative Adversarial Network is the most rapidly emerging branch of deep learning
    that is suitable for a wide range of creative applications (such as image editing
    or painting, style transfer, object transfiguration, photo enhancement, and many
    more).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络是深度学习中最迅速发展的分支之一，适用于各种创意应用（如图像编辑或绘画、风格转移、物体变换、照片增强等）。
- en: In this chapter, you will first learn the technique of generating or editing
    images based on certain conditions or characteristics. Then, you will stabilize
    GAN training to overcome the mode-collapse problem and apply a convergence measure
    metric with the **Boundary Equilibrium** approach. Finally, you will perform image
    to image translation across various domains (such as changing apple to orange
    or horse to zebra) using **Cycle Consistent Generative Network**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将首先学习根据特定条件或特征生成或编辑图像的技术。然后，您将通过边界平衡方法稳定GAN训练，以克服模式崩溃问题，并应用收敛度量度量标准。最后，您将使用Cycle
    Consistent生成网络在各个领域进行图像到图像的翻译（例如将苹果变成橙子或马变成斑马）。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节将涵盖以下主题：
- en: What is CGAN? Its concept and architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是CGAN？其概念和架构
- en: Generating fashion wardrobe from `Fashion-MNIST` data using CGAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CGAN从Fashion-MNIST数据生成时尚衣橱
- en: Stabilizing GAN training using Boundary Equilibrium GAN with Wasserstein distance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用边界平衡GAN与Wasserstein距离稳定GAN训练
- en: Image style transfer across different domains using CycleGAN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CycleGAN在不同领域之间传递图像风格
- en: Generating oranges from apples using Tensorflow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Tensorflow从苹果生成橙子
- en: Changing horse images into zebras automatically
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动将马的图像转换为斑马
- en: Bridging the gap between supervised and unsupervised learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架起监督学习和无监督学习之间的桥梁
- en: Humans learn by observing and experiencing the physical world and our brains
    are very good at prediction without doing explicit computations to arrive at the
    correct answer. Supervised learning is all about predicting a label associated
    with the data and the goal is to generalize to new unseen data. In unsupervised
    learning, the data comes in with no labels, and the goal is often not to generalize
    any kind of prediction to new data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过观察和体验物理世界来学习，我们的大脑非常擅长在不进行显式计算的情况下进行预测以得出正确答案。监督学习是关于预测与数据相关联的标签，其目标是泛化到新的未见数据。在无监督学习中，数据没有标签，并且其目标通常不是对新数据进行任何形式的泛化预测。
- en: In the real world, labeled data is often scarce and expensive. The Generative
    Adversarial Network takes up a supervised learning approach to do unsupervised
    learning, by generating fake/synthetic looking data, and tries to determine if
    the generated sample is fake or real. This part (a discriminator doing classification)
    is a supervised component. But the actual goal of GAN is to understand what the
    data looks like (that is, its distribution or density estimation) and be able
    to generate new examples of what it has learned.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，标记数据通常稀缺且昂贵。生成对抗网络采用监督学习方法进行无监督学习，通过生成看起来伪造的数据，试图确定生成的样本是虚假还是真实的。这部分（分类器进行分类）是一个监督组件。但GAN的实际目标是理解数据的外观（即其分布或密度估计），并能够生成其学习到的新样本。
- en: Introduction to Conditional GAN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件GAN简介
- en: A **Generative Adversarial Network** (**GAN**) simultaneously trains two networks—a
    generator that learns to generate fake samples from an unknown distribution or
    noise and a discriminator that learns to distinguish fake from real samples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）同时训练两个网络——生成器学习从未知分布或噪声生成伪样本，鉴别器学习区分虚假和真实样本。
- en: In the **Conditional GAN** (**CGAN**), the generator learns to generate a fake
    sample with a specific condition or characteristics (such as a label associated
    with an image or more detailed tag) rather than a generic sample from unknown
    noise distribution. Now, to add such a condition to both generator and discriminator,
    we will simply feed some vector *y*, into both networks. Hence, both the discriminator
    *D(X,y)* and generator *G(z,y)* are jointly conditioned to two variables, *z*
    or *X* and *y*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在条件GAN（CGAN）中，生成器学习根据特定条件或特征（例如与图像相关联的标签或更详细的标签）生成伪样本，而不是从未知噪声分布生成通用样本。现在，为了向生成器和鉴别器添加这样的条件，我们将简单地向两个网络提供一些向量*y*。因此，鉴别器*D(X,y)*和生成器*G(z,y)*都联合条件于两个变量*z*或*X*和*y*。
- en: 'Now, the objective function of CGAN is:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，CGAN的目标函数是：
- en: '![Introduction to Conditional GAN](img/B08086_03_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![条件GAN介绍](img/B08086_03_01.jpg)'
- en: The difference between GAN loss and CGAN loss lies in the additional parameter
    *y* in both a discriminator and generator function. The architecture of CGAN shown
    in the following figure now has an additional input layer (in the form of condition
    vector **C**) that gets fed into both the discriminator network and generator
    network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GAN损失和CGAN损失之间的区别在于判别器和生成器函数中额外的参数*y*。下面的CGAN架构现在有一个额外的输入层（以条件向量**C**的形式），该层同时输入到判别器网络和生成器网络中。
- en: '![Introduction to Conditional GAN](img/B08086_03_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![条件GAN介绍](img/B08086_03_02.jpg)'
- en: Generating a fashion wardrobe with CGAN
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CGAN生成时尚衣橱
- en: In this example, we will implement conditional GAN to generate a fashion wardrobe
    using a `Fashion-MNIST` dataset ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
    The `Fashion-MNIST` dataset is similar to the original `MNIST` dataset with a
    new set of gray-scale images and labels.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将实现条件GAN，使用`Fashion-MNIST`数据集（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）来生成时尚衣橱。`Fashion-MNIST`数据集类似于原始的`MNIST`数据集，但使用了一组新的灰度图像和标签。
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![使用CGAN生成时尚衣橱](img/B08086_03_03.jpg)'
- en: Let's jump into the code to understand the working of CGAN with simple neural
    network architecture for both generator and discriminator.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入代码，了解使用简单的神经网络架构来实现CGAN的工作原理，其中生成器和判别器都采用这种架构。
- en: 'First, we will define a new input variable to hold our condition:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个新的输入变量来保存我们的条件：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we incorporate the new variable `y` into the discriminator `D(X)` and
    generator `G(z)`. Now, the discriminator`(x,y)` and generator`(z,y)` are different
    than the original GAN:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将新的变量`y`加入到判别器`D(X)`和生成器`G(z)`中。现在，判别器`(x,y)`和生成器`(z,y)`与原始的GAN不同：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will use our new networks and define a `loss` function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用我们的新网络，并定义一个`loss`函数：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'During training, we feed the value of `y` into both a generator network and
    discriminator network:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将`y`的值同时输入到生成器网络和判别器网络：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we generate new data samples based on certain conditions. For this
    example, we use the image label as our condition and set the label to be `7`,
    that is, generating the image of `Sneaker`. The conditional variable `y_sample`
    is a collection of one-hot encoded vectors with value `1` in the seventh index:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们根据特定条件生成新的数据样本。在这个例子中，我们使用图像标签作为条件，并将标签设置为`7`，即生成`Sneaker`的图像。条件变量`y_sample`是一个集合，包含在第七索引位置值为`1`的独热编码向量：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let us execute the following steps to generate wardrobe images based on
    class label condition. First download the `Fashion-MNIST` dataset and save it
    under the `data`/`fashion` directory by running the `download.py` script:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行以下步骤，根据类别标签条件生成衣橱图像。首先，下载`Fashion-MNIST`数据集，并通过运行`download.py`脚本将其保存到`data`/`fashion`目录下：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![使用CGAN生成时尚衣橱](img/B08086_03_04.jpg)'
- en: 'Next train the CGAN model using the following command, which will generate
    sample images after every 1000 iterations under the `output` directory:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令训练CGAN模型，这将生成每1000次迭代后的示例图像，并保存在`output`目录下：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_05.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![使用CGAN生成时尚衣橱](img/B08086_03_05.jpg)'
- en: 'The following is the output of running CGAN using a condition label set to
    **4 (Coat)** after **80k** iteration and **7 (Sneaker)** after **60k** iteration:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用条件标签设置为**4（外套）**在**80k**次迭代后和**7（运动鞋）**在**60k**次迭代后运行CGAN的输出结果：
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_06.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![使用CGAN生成时尚衣橱](img/B08086_03_06.jpg)'
- en: Stabilizing training with Boundary Equilibrium GAN
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用边界平衡GAN稳定训练
- en: 'The popularity of GAN is rising rapidly among machine learning researchers.
    GAN researches can be categorized into two types: one that applies GAN into challenging
    problems and one that attempts to stabilize the training. Stabilizing GAN training
    is very crucial as the original GAN architecture suffers and has several shortcomings:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GAN在机器学习研究人员中迅速流行。GAN的研究可以分为两类：一类是将GAN应用于具有挑战性的问题，另一类是尝试稳定训练。稳定GAN训练至关重要，因为原始的GAN架构存在许多缺点和不足：
- en: '**Mode collapse**: Where generators collapse into very narrow distribution
    and the samples generated are not diverse. This problem of course violates the
    spirit of GAN.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式崩塌**：生成器崩塌到非常窄的分布中，生成的样本缺乏多样性。当然，这个问题违反了GAN的精神。'
- en: '**Evaluation of convergence metric**: There is no well-defined metric that
    tells us about the convergence between discriminator loss and generator loss.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收敛度量评估**：没有一个明确的度量可以告诉我们判别器损失和生成器损失之间的收敛情况。'
- en: 'The improved **Wasserstein GAN** (*arXiv: 1704.00028,2017*) is a newly proposed
    GAN algorithm that promises to solve the preceding problems by minimizing the
    Wasserstein distance (or Earth-Mover distance) by providing simple gradients to
    the networks (+1 if the output is considered real and -1 if the output is considered
    fake).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '改进的**Wasserstein GAN**（*arXiv: 1704.00028,2017*）是一种新提出的GAN算法，通过最小化Wasserstein距离（或地球搬运距离）来解决前述问题，通过为网络提供简单的梯度（如果输出被认为是真实的，则为+1；如果输出被认为是假的，则为-1）。'
- en: 'The main idea behind the **BEGAN** (*arXiv: 1703.10717,2017*) is to have a
    new `loss` function by using **auto-encoder** as a discriminator, where the real
    loss is derived from the Wasserstein distance (to cater the problem of mode collapse)
    between the reconstruction losses of real and generated images:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**BEGAN**（*arXiv: 1703.10717,2017*）的主要思想是使用**自动编码器**作为判别器，提出一种新的`损失`函数，其中真实损失是由真实图像和生成图像的重建损失之间的Wasserstein距离（以解决模式崩塌问题）得出的：'
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_07.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![通过边界平衡GAN稳定训练](img/B08086_03_07.jpg)'
- en: 'A hyper-parameter gamma is added through the use of a weighting parameter *k*
    to give users the power to control the desired diversity:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用加权参数 *k* 添加超参数gamma，使用户能够控制期望的多样性：
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_08.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![通过边界平衡GAN稳定训练](img/B08086_03_08.jpg)'
- en: 'Unlike most GANs where discriminator and the generator are trained alternatively,
    BEGAN allows simultaneous training of both the networks in an adversarial way
    at each time step:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数GAN不同，BEGAN允许在每个时间步骤同时对这两个网络进行对抗性训练：
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_09.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![通过边界平衡GAN稳定训练](img/B08086_03_09.jpg)'
- en: 'Finally, it allows an approximate measure of convergence *M* to understand
    the performance of the whole network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，它允许对收敛的近似度量 *M* 进行评估，以理解整个网络的性能：
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_10.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![通过边界平衡GAN稳定训练](img/B08086_03_10.jpg)'
- en: The training procedure of BEGAN
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BEGAN的训练过程
- en: 'Steps involved in training BEGAN are described as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BEGAN训练过程中涉及的步骤如下：
- en: The discriminator (the autoencoder) updates its weights to minimize the reconstruction
    loss of real images and in that way, starts to reconstruct real images better.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判别器（自动编码器）更新其权重，以最小化真实图像的重建损失，从而开始更好地重建真实图像。
- en: Simultaneously, the discriminator starts to maximize the reconstruction loss
    of generated images.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与此同时，判别器开始最大化生成图像的重建损失。
- en: The generator works in an adversarial way to minimize the reconstruction loss
    of generated images.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器以对抗性方式工作，最小化生成图像的重建损失。
- en: Architecture of BEGAN
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BEGAN架构
- en: 'As shown in the following figure, the discriminator is a convolutional network
    with both deep encoder and decoder. The decoder has multiple layers of 3x3 convolution
    followed by an **Exponential Linear Unit** (**ELU**). Downsampling is done with
    stride 2 convolutions. The embedding state of the autoencoder is mapped to fully
    connected layers. Both the generator and the decoder are deep deconvolution with
    identical architectures, but with different weights, and the upsampling is done
    using nearest-neighbors:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，判别器是一个卷积网络，具有深度的编码器和解码器。解码器有多个3x3卷积层，后跟**指数线性单元**（**ELU**）。下采样通过步幅为2的卷积完成。自动编码器的嵌入状态被映射到全连接层。生成器和解码器都是深度反卷积，具有相同的架构，但权重不同，上采样通过最近邻方法完成：
- en: '![Architecture of BEGAN](img/B08086_03_11.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![BEGAN架构](img/B08086_03_11.jpg)'
- en: 'Figure-1: The architecture of the BEGAN.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图-1：BEGAN架构。
- en: 'Source: *arXiv: 1703.10717,2017,2017*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '来源：*arXiv: 1703.10717,2017,2017*'
- en: In the preceding figures, both the generator and the decoder of the discriminator
    is shown on the left-hand side. The encoder network of the discriminator is shown
    on the right-hand side.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，判别器的生成器和解码器显示在左侧。判别器的编码器网络显示在右侧。
- en: Implementation of BEGAN using Tensorflow
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Tensorflow实现BEGAN
- en: Let us now dive deep into the code and implement the preceding concept along
    with the architecture to generate realistic attractive images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入研究代码，并实现上述概念及架构，以生成逼真且具有吸引力的图像。
- en: 'The generator network has multiple layers of 3x3 convolution with an `elu activation`
    function, followed by nearest neighbor upscaling, except at the final layer. The
    number of convolution layers is calculated from the height of the image:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络具有多个3x3卷积层，并使用`elu激活`函数，接着进行最近邻上采样，除最后一层外。卷积层的数量是根据图像的高度计算的：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The encoder of the discriminator network has multiple layers of convolution
    with the `elu activation` function, followed by down-sampling using maxpooling
    except at the final convolution layer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的编码器具有多个卷积层，并使用`elu激活`函数，接着进行最大池化下采样，除最后一层卷积层外：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The decoder of the discriminator network is similar to the generator network,
    having multiple layers of convolution with an `elu activation` function followed
    by upsampling using nearest neighbor except at the final convolution layer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络的解码器与生成器网络相似，具有多个卷积层，并使用`elu激活`函数，接着进行最近邻上采样，除最后一层卷积层外：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now the generator loss and discriminator loss for both real, fake images discussed
    previously are optimized using **Adam Optimizer** by executing the following code
    block:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过执行以下代码块，使用**Adam优化器**优化之前讨论的真实和伪图像的生成器损失和判别器损失：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now it''s time to execute the code for generating impressive celebrity images:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候执行代码来生成令人印象深刻的名人图像了：
- en: 'First clone the following repository and then change the directory:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先克隆以下仓库，并切换到相应的目录：
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, run the following scripts to download the `CelebA` dataset under the
    `data` directory, and split it into training, validation, and test set:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下脚本下载`CelebA`数据集到`data`目录，并将其划分为训练集、验证集和测试集：
- en: '[PRE12]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Make sure p7zip is installed on your machine.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请确保您的机器上已安装p7zip。
- en: 'Now start the training process as follows, which will save the generated samples
    under the `logs` directory:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在开始训练过程，训练过程中生成的样本将保存在`logs`目录下：
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you face the error **Conv2DCustomBackpropInputOp only supports NHWC**, then
    refer to the following issue:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到错误**Conv2DCustomBackpropInputOp仅支持NHWC**，请参考以下问题：
- en: '[https://github.com/carpedm20/BEGAN-tensorflow/ issues/29](https://github.com/carpedm20/BEGAN-tensorflow/
    issues/29)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/carpedm20/BEGAN-tensorflow/ issues/29](https://github.com/carpedm20/BEGAN-tensorflow/
    issues/29)'
- en: 'After executing the preceding command, while the training is going on you will
    notice information such as `Model` directory, logging directory, and various losses
    as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述命令后，在训练过程中，您将看到如下信息，包括`Model`目录、日志目录和各种损失值：
- en: '![Implementation of BEGAN using Tensorflow](img/B08086_03_12.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![使用Tensorflow实现BEGAN](img/B08086_03_12.jpg)'
- en: 'The output faces generated by BEGAN are visually realistic and attractive as
    shown in the following screenshot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BEGAN生成的输出人脸在视觉上非常逼真且具有吸引力，如下图所示：
- en: '![Implementation of BEGAN using Tensorflow](img/B08086_03_13.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![使用Tensorflow实现BEGAN](img/B08086_03_13.jpg)'
- en: 'Figure-2: Generator output images (64x64) with gamma=0.5 after 350k steps'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：经过350k步后，生成器输出的图像（64x64），gamma=0.5
- en: 'The following sample output images (128 x 128) are generated after 250k steps:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是经过250k步后生成的示例输出图像（128x128）：
- en: '![Implementation of BEGAN using Tensorflow](img/B08086_03_14.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![使用Tensorflow实现BEGAN](img/B08086_03_14.jpg)'
- en: 'Figure-3: Generator output images (128x128) with gamma=0.5 after 250k steps'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：经过250k步后，生成器输出的图像（128x128），gamma=0.5
- en: Image to image style transfer with CycleGAN
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于CycleGAN的图像到图像风格转换
- en: 'The **Cycle Consistent Generative Network** (**CycleGAN**), originally proposed
    in the paper *Unpaired image-to-image translation using CycleGAN*—*arXiv: 1703.10593,
    2017*, aims at finding mapping between the source domain and a target domain for
    a given image without any pairing information (such as greyscale to color, image
    to semantic labels, edge-map to photograph, horse to zebra, and so on).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环一致生成网络**（**CycleGAN**），最初在论文《*Unpaired image-to-image translation using
    CycleGAN*—*arXiv: 1703.10593, 2017*》中提出，旨在为给定的图像寻找源领域和目标领域之间的映射，而无需任何配对信息（例如灰度到彩色、图像到语义标签、边缘图到照片、马到斑马等）。'
- en: The key idea behind CycleGAN is to have two translator's F and G, where F will
    translate an image from domain *A* to domain *B*, and *G* will translate an image
    from domain *B* to domain *A*. So, for an image *x* in domain *A*, we should expect
    the function *G(F(x))* to be equivalent to *x* and similarly for an image *y*
    in domain *B*, we should expect the function *F(G(y))* to be equivalent to *y*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN背后的关键思想是拥有两个转换器F和G，其中F将图像从领域*A*转换为领域*B*，而G将图像从领域*B*转换为领域*A*。因此，对于领域*A*中的图像*x*，我们应期望函数*G(F(x))*与*x*等效；同样，对于领域*B*中的图像*y*，我们应期望函数*F(G(y))*与*y*等效。
- en: Model formulation of CycleGAN
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN的模型公式
- en: 'The main goal of the CycleGAN model is to learn mapping between the two domains
    *X* and *Y* using the training samples *{xi}Ni=1* *∈* *X* and *{yj}Mj=1* *∈* *Y*.
    It also has two adversarial discriminators *D* [X] and *D* [Y]: where *D* [X]
    tries to distinguish between original images *{x}* and translated images *{F(y)}*,
    and similarly, *D* [Y] tries to distinguish between *{y}* and *{G(x)}*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN模型的主要目标是学习使用训练样本*{xi}Ni=1* *∈* *X*和*{yj}Mj=1* *∈* *Y*之间的映射，领域*X*和*Y*之间的关系。它还有两个对抗判别器*D*[X]和*D*[Y]：其中*D*[X]试图区分原始图像*{x}*和翻译后的图像*{F(y)}*，同样，*D*[Y]试图区分*{y}*和*{G(x)}*。
- en: 'CycleGAN model has two `loss` functions:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN模型有两个`loss`函数：
- en: '**Adversarial loss**: It matches the generated image''s distribution to the
    target domain distribution:![Model formulation of CycleGAN](img/B08086_03_15.jpg)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗损失**：它将生成图像的分布与目标领域的分布匹配：![CycleGAN模型公式](img/B08086_03_15.jpg)'
- en: '**Cycle consistency loss**: It prevents the learned mappings *G* and *F* from
    contradicting each other:![Model formulation of CycleGAN](img/B08086_03_16.jpg)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环一致性损失**：它防止学习到的映射*G*和*F*相互矛盾：![CycleGAN模型公式](img/B08086_03_16.jpg)'
- en: 'The full CycleGAN objective function is given by:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的CycleGAN目标函数如下所示：
- en: '![Model formulation of CycleGAN](img/B08086_03_17.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![CycleGAN模型公式](img/B08086_03_17.jpg)'
- en: Transforming apples into oranges using Tensorflow
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Tensorflow将苹果转化为橙子
- en: 'In this example, we will transfer the style from an image in domain *A* to
    an image in another domain *B*: more specifically, we will apply CycleGAN to transform
    apples into oranges or vice-versa by executing the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将把领域*A*中的图像风格迁移到另一个领域*B*中的图像：更具体地说，我们将应用CycleGAN将苹果转化为橙子，或将橙子转化为苹果，步骤如下：
- en: 'First clone the following `git` repository and change the directory to CycleGAN-tensorflow:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，克隆以下`git`仓库并将目录更改为CycleGAN-tensorflow：
- en: '[PRE14]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now download the `apple2orange` dataset ZIP file using the `download_dataset.sh`
    script, extract and save it under the `datasets` directory:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`download_dataset.sh`脚本下载`apple2orange`数据集ZIP文件，解压并将其保存在`datasets`目录下：
- en: '[PRE15]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next train the CycleGAN model using the downloaded `apple2orange` dataset.
    During the training phase, the model will be saved in the `checkpoint` directory
    and logging is enabled in the `logs` directory for visualization with TensorBoard:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用下载的`apple2orange`数据集训练CycleGAN模型。在训练阶段，模型将保存在`checkpoint`目录中，并在`logs`目录中启用日志记录，以便通过TensorBoard进行可视化：
- en: '[PRE16]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Transforming apples into oranges using Tensorflow](img/B08086_03_18.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用Tensorflow将苹果转化为橙子](img/B08086_03_18.jpg)'
- en: 'Run the following command to visualize various losses (discriminator loss and
    generator loss) during the training phase in your browser, by navigating to `http://localhost:6006/`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以在浏览器中可视化训练阶段的各种损失（判别器损失和生成器损失），通过访问`http://localhost:6006/`：
- en: '[PRE17]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Transforming apples into oranges using Tensorflow](img/B08086_03_19.jpg)'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用Tensorflow将苹果转化为橙子](img/B08086_03_19.jpg)'
- en: 'Finally, we will load the trained model from the `checkpoint` directory to
    transfer a style across images, hence generating oranges from apple or vice-versa
    (based on the value passed (`AtoB` or `BtoA`) to the `which_direction` parameter
    that indicates a style transfer from domain 1 to domain 2):'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将从`checkpoint`目录加载训练好的模型，将风格从一种图像传递到另一种图像，从而生成橙子（或者相反，取决于传递的值（`AtoB`或`BtoA`）来指示从领域1到领域2的风格迁移）：
- en: '[PRE18]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The following are the sample output images generated in the `test` phase:![Transforming
    apples into oranges using Tensorflow](img/B08086_03_20.jpg)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是`test`阶段生成的样本输出图像：![使用Tensorflow将苹果转化为橙子](img/B08086_03_20.jpg)
- en: 'Figure- 4: The left-hand side shows transforming apples to oranges by passing
    AtoB in the direction parameter, whereas the right-hand side shows the output
    generated by passing BtoA in the direction parameter.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4：左侧显示通过传递AtoB在方向参数中将苹果转变为橘子，而右侧显示通过传递BtoA在方向参数中生成的输出。
- en: Transfiguration of a horse into a zebra with CycleGAN
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CycleGAN将马转变为斑马
- en: 'Just like the previous example, in this section we will use CycleGAN to transform
    a horse into a zebra or vice-versa by executing the following steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前的示例一样，在本节中，我们将使用CycleGAN通过执行以下步骤将马转变为斑马，或将斑马转变为马：
- en: 'First clone the following `git` repository and change the directory to `CycleGAN-tensorflow`
    (you can skip this step if you have already executed the previous example):'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先克隆以下`git`仓库并切换目录到`CycleGAN-tensorflow`（如果你已经执行了前面的示例，可以跳过此步骤）：
- en: '[PRE19]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now download the `horse2zebra` ZIP file from Berkley, extract it, and save
    it under the `datasets` directory using the `download_dataset.sh` script:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在从伯克利下载`horse2zebra`压缩包，解压并通过`download_dataset.sh`脚本将其保存到`datasets`目录下：
- en: '[PRE20]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will train our CycleGAN model using the `horse2zebra` dataset and
    use TensorBoard for visualizing the losses while training is going on:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`horse2zebra`数据集训练我们的CycleGAN模型，并使用TensorBoard可视化训练过程中的损失：
- en: '[PRE21]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Transfiguration of a horse into a zebra with CycleGAN](img/B08086_03_21.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用CycleGAN将马转变为斑马](img/B08086_03_21.jpg)'
- en: 'Run the following command and navigate to `http://localhost:6006/` for the
    visualizing of various generator or discriminator losses:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令并导航到`http://localhost:6006/`以可视化各种生成器或判别器损失：
- en: '[PRE22]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Transfiguration of a horse into a zebra with CycleGAN](img/B08086_03_22.jpg)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用CycleGAN将马转变为斑马](img/B08086_03_22.jpg)'
- en: 'Finally, we will use the trained model from the `checkpoint` directory to transform
    a horse into a zebra or vice-versa, depending on whether the value `AtoB` or `BtoA`
    is passed to the `which_direction` parameter:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，我们将使用`checkpoint`目录中的训练模型，将马转变为斑马，或将斑马转变为马，这取决于传递给`which_direction`参数的值是`AtoB`还是`BtoA`：
- en: '[PRE23]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following sample output images are generated in the `test` phase:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`test`阶段生成的示例输出图像：
- en: '![Transfiguration of a horse into a zebra with CycleGAN](img/B08086_03_23.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![使用CycleGAN将马转变为斑马](img/B08086_03_23.jpg)'
- en: 'Figure-5: The left-hand side shows transforming horse to zebra, whereas the
    right-hand side shows translating zebra into horse.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：左侧显示将马转变为斑马，而右侧显示将斑马转变为马。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: So far you have learned the approach of creating images based on certain characteristics
    or conditions, by passing that condition vector into both generator and discriminator.
    Also, you have understood how to overcome model collapse problems by stabilizing
    your network training using BEGAN. Finally, you have implemented image to image
    style transfer by generating an orange from an apple and a zebra from a horse,
    or vice-versa, using CycleGAN. In the next chapter, we will solve complex real-life
    problems such as text to image synthesis and cross domain discovery by stacking
    or coupling two or more GAN models together.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学会了通过将条件向量传递给生成器和判别器，基于特定的特征或条件来创建图像。同时，你也理解了如何通过使用BEGAN稳定网络训练来克服模型崩溃问题。最后，你已经实现了图像到图像的风格迁移，通过使用CycleGAN将苹果变成橘子，将马变成斑马，或者反过来。在下一章中，我们将通过堆叠或结合两个或更多GAN模型来解决复杂的现实问题，如文本到图像合成和跨领域发现。
