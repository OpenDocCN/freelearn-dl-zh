- en: Deep Learning for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的深度学习
- en: In this chapter, you will learn how to create document summaries. We will begin
    by removing parts of documents that should not be considered and tokenizing the
    remaining text. Next, we will apply embeddings and create clusters. These clusters
    will then be used to make document summaries. Also, we will learn how to use **restricted
    Boltzmann machines** (**RBMs**) as building blocks to create deep belief networks
    for topic modeling. We will begin with coding the RBM and defining the Gibbs sampling
    rate, contrastive divergence, and free energy for the algorithm. We will conclude
    by compiling multiple RBMs to create a deep belief network.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何创建文档摘要。我们将从删除不应被考虑的文档部分并标记化剩余文本开始。接下来，我们将应用嵌入并创建集群。这些集群将被用来生成文档摘要。我们还将学习如何使用**限制玻尔兹曼机**（**RBMs**）作为构建模块，创建用于主题建模的深度信念网络。我们将从编写RBM的代码并定义吉布斯采样率、对比散度和算法的自由能开始。最后，我们将通过编译多个RBM来创建深度信念网络。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Formatting data using tokenization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记化格式化数据
- en: Cleaning text to remove noise
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理文本以去除噪声
- en: Applying word embeddings to increase usable data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用词嵌入以增加可用数据
- en: Clustering data into topic groups
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据聚类成主题组
- en: Summarizing documents using model results
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型结果总结文档
- en: Creating an RBM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建RBM
- en: Defining the Gibbs sampling rate
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义吉布斯采样率
- en: Speeding up sampling with contrastive divergence
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对比散度加速采样
- en: Computing free energy for model evaluation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算自由能以进行模型评估
- en: Stacking RBMs to create a deep belief network
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠RBM创建深度信念网络
- en: Formatting data using tokenization
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标记化格式化数据
- en: The first step we will take to begin analyzing text is loading text files and
    then tokenizing our data by transforming the text from sentences into smaller
    pieces, such as words or terms. A text object can be tokenized in a number of
    ways. In this chapter, we will tokenize text into words, although other sized
    terms could also be tokenized. These are referred to as n-grams, so we can get
    two-word terms (2-grams), three-word terms, or a term of any arbitrary size.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始分析文本的第一步是加载文本文件，然后通过将文本从句子转化为更小的片段（如单词或术语）来对数据进行标记化。文本对象可以通过多种方式进行标记化。在本章中，我们将文本标记化为单词，尽管也可以标记化为其他大小的术语。这些被称为n-grams，因此我们可以获得由两个单词组成的术语（2-grams）、三个单词组成的术语，或者任何任意大小的术语。
- en: 'To get started with the process of creating one-word tokens from our text objects,
    we will use the following steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始从文本对象中创建单词标记的过程，我们将使用以下步骤：
- en: 'Let''s load the libraries that we will need. For this project, we will use
    `tidyverse` for data manipulation, `tidytext` for special functions to manipulate
    text data, `spacyr` for extracting text metadata, and `textmineR` for word embeddings.
    To load these libraries, we run the following code:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载将需要的库。对于这个项目，我们将使用`tidyverse`进行数据处理，使用`tidytext`来执行处理文本数据的特殊函数，使用`spacyr`提取文本元数据，使用`textmineR`进行词嵌入。要加载这些库，我们运行以下代码：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, the data that we will use will be the 20 Newsgroups dataset.
    This consists of pieces of text that come from one of 20 Newsgroups. The format
    of the data that we will pull in has a unique ID, the group the text belongs to,
    and the group.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用的数据是20个新闻组数据集。该数据集包含来自20个新闻组之一的文本片段。我们提取的数据格式具有唯一的ID、文本所属的组以及该组的内容。
- en: 'Let''s read in the data using the following code:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过以下代码读取数据：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After running this code, you should see the `twenty_newsgroups` object appear
    in your `Environment` window. The object has 11,314 rows and 3 columns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，您应该会在`Environment`窗口中看到`twenty_newsgroups`对象。该对象有11,314行和3列。
- en: 'Let''s take a look at a sample of the data. In this case, let''s print the
    first row of data to our console. We look at the first row of data by running
    the following code:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看一下数据的一个示例。在这种情况下，我们通过运行以下代码来打印第一行数据到控制台：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After running this code, you will see the following printed to your console:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，您将在控制台中看到以下内容：
- en: '![](img/9bf0f4f8-febd-470f-95cd-97b18ade0ee1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bf0f4f8-febd-470f-95cd-97b18ade0ee1.png)'
- en: 'Now, let''s break this text into tokens. Tokens are some atomic portion of
    the text character string that we see in the preceding screenshot. In this case,
    we will break this string into word tokens. The final result will be a row for
    each word listed, alongside the ID and newsgroup ID. We tokenize the text data
    using the following code:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们把这段文本拆分成词元。词元是我们在前面截图中看到的文本字符串的一个基本部分。在这种情况下，我们将把这个字符串拆分成单词词元。最终结果将是每个单词列出一行，旁边是ID和新闻组ID。我们使用以下代码对文本数据进行分词：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After running this code, we can see that our data object has grown substantially.
    We now have 3.5 million rows when we previously only had 11,000, since each word
    now gets its own row.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们可以看到数据对象已经大幅增长。现在，我们的数据有350万行，而之前只有11,000行，因为每个单词现在都有了自己的一行。
- en: 'Let''s take a quick look at term frequency, now that we have each word separated
    out into its own line. In this step, we can begin to see whether certain terms
    are used more than others with the text included in this dataset. To plot the
    frequency of each term in the data, we will use the following code:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们快速查看一下词频情况，既然每个单词已经被分隔到各自的一行。在这一步中，我们可以开始观察某些词汇是否比其他词更频繁出现在数据集中。为了绘制数据中每个词的频率，我们将使用以下代码：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running this code, we will see the following plot generated:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们将看到生成的以下图表：
- en: '![](img/aa636335-74b1-4319-83c6-225e5edcd427.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa636335-74b1-4319-83c6-225e5edcd427.png)'
- en: We have successfully taken some text and divided it into tokens. However, we
    can see from the plot that terms such as the, to, of, and a are most frequent.
    These types of words are often bundled into a collection of terms referred to
    as **stop words**. Next, we will learn how to remove these types of terms that
    have no information value.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地将一些文本分割成了词元。然而，从图表中我们可以看到，像“the”、“to”、“of”和“a”这样的词汇最为常见。这类词通常会被打包成一组词汇，称为**停用词**。接下来，我们将学习如何移除这些没有信息价值的词汇。
- en: Cleaning text to remove noise
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理文本以移除噪音
- en: The next step we will take to prepare for text analysis is doing some preliminary
    cleaning. This is a common way to get started, regardless of what machine learning
    method will be applied later. When working with text, there are several terms
    and patterns that will not provide meaningful information. Some of these terms
    are generally not useful and steps to remove these pieces of text data can be
    used every time, while others will be more context-dependent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为文本分析做的下一步准备工作是进行一些初步的清理。这是一种常见的起步方式，无论后续将应用何种机器学习方法。当处理文本时，有一些词汇和模式无法提供有意义的信息。部分词汇通常没有用处，移除这些文本数据的步骤可以每次都用，而其他的则更多依赖于具体的上下文。
- en: 'As previously noted, there are collections of terms referred to as stop words.
    These terms have no information value and can usually be removed. To remove stop
    words from our data, we use the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有一些词组被称为停用词。这些词没有信息价值，通常可以移除。为了从我们的数据中移除停用词，我们使用以下代码：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After running the preceding code, our row count goes down from 3.5 million
    to 1.7 million. In effect, our data (`word_tokens`) has almost been cut in half
    by removing all the stop words. Let''s run the plot we ran earlier to see which
    terms are most frequent now. We can identify the term frequency as we did before
    with the following lines of code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们的行数从350万降到了170万。实际上，通过移除所有停用词，我们的数据（`word_tokens`）几乎减少了一半。接下来，我们运行之前的绘图代码，看看现在哪些词汇是最常见的。我们可以通过以下代码行来识别词频：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running this chunk of code, the following plot is generated:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，将生成以下图表：
- en: '![](img/00d142aa-0671-49a2-bda3-0b8928435982.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00d142aa-0671-49a2-bda3-0b8928435982.png)'
- en: 'In this plot, we can see that terms such as the, to, of, and a are now removed.
    However, we also now see that there are some numbers showing up as frequent terms.
    This could be context-dependent and there may be cases where pulling numbers from
    text is very important for a project. However, here we will focus on actual words
    and will remove all terms that contain non-alphabetic characters. We can accomplish
    this by using some regular expressions, also known as **regex**. We can remove
    the terms that do not contain any characters from the alphabet by using the following
    code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，我们可以看到诸如the、to、of和a这样的词语已被移除。然而，我们也可以看到一些数字作为常见术语出现在图中。这可能是上下文相关的，并且在某些情况下，从文本中提取数字对项目来说非常重要。然而，在这里我们将重点关注实际的单词，并移除所有包含非字母字符的术语。我们可以使用一些正则表达式（regex）来完成这一点。通过使用以下代码，我们可以移除不包含字母表字符的术语：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After running this regex code, we can run the same plot code again, as we did
    previously. When we do, we generate a plot that looks like the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完这个正则表达式（regex）代码后，我们可以再次运行之前的绘图代码。当我们这么做时，会生成如下所示的图表：
- en: '![](img/7a0bb9f6-e4dc-4bee-b4db-0a4ab31c4404.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a0bb9f6-e4dc-4bee-b4db-0a4ab31c4404.png)'
- en: Based on this plot, we see our top twenty terms are all words, including one
    possible acronym (nntp). With our data object now reduced to 1.4 million rows,
    which includes only terms that start and end with characters from the alphabet,
    we are ready to move on to the next step where we will use embeddings to add extra
    context to each term.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个图表，我们可以看到我们的前二十个术语都是单词，其中包括一个可能的缩写词（nntp）。现在，我们的数据对象已减少至140万行，只包含以字母表中的字符开头和结尾的术语，接下来我们准备进入下一步，即使用嵌入（embeddings）为每个术语添加额外的上下文。
- en: Applying word embeddings to increase usable data
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用词嵌入以增加可用数据
- en: Extracting terms from text is a good starting point for text analysis. With
    the text tokens we have created so far, we can compare term frequency for different
    categories, which begins to tell us a story about the content that dominates a
    particular newsgroup. However, the term alone is just one part of the overall
    information we can glean from a given term. The previous plot contained `people`
    and, of course, we know what this word means, although there are multiple nuanced
    details connected to this term. For instance, `people` is a noun. It is similar
    to terms such as *person* and *human* and is also related to a term such as *household*.
    All of these details for `people` could be important but, by just extracting the
    term, we cannot directly derive these other details. This is where embeddings
    are especially helpful.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取术语是文本分析的良好起点。通过我们目前创建的文本词元，我们可以比较不同类别的术语频率，这开始讲述一个关于特定新闻组中占主导地位内容的故事。然而，单独的术语只是我们从某个术语中可以获得的整体信息的一部分。之前的图表中包含了`people`这个词，当然我们知道这个词的含义，尽管它与该术语相关的多个细微差别需要进一步解释。例如，`people`是一个名词。它与诸如*person*和*human*之类的术语相似，也与*household*之类的术语相关。所有这些与`people`相关的细节可能都很重要，但仅通过提取术语，我们无法直接推导出这些其他细节。这就是嵌入特别有用的地方。
- en: 'Embeddings, in the context of natural language processing, are pre-trained
    neural networks that perform the type of mapping just described. We can use these
    embeddings to match parts of speech to terms, as well as to find the lexical distance
    between words. Let''s get started by looking at the parts-of-speech embeddings.
    To examine the parts of speech for every term in our text dataset, we run the
    following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理的背景下，嵌入（embeddings）是经过预训练的神经网络，执行如上所述的映射类型。我们可以使用这些嵌入将词性与术语匹配，并计算词语之间的词汇距离。让我们从查看词性嵌入开始。为了检查文本数据集中每个术语的词性，我们运行以下代码：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using the preceding code, we first install `spacy` on our machine. Next, we
    initialize `spacy` using a small (`sm`) English (`en`) model that is trained on
    web text (`web`) for the core `spacy` elements: named entities, part-of-speech
    tags, and syntactic dependencies. Afterward, we apply the model to the first piece
    of text in our dataset. Once we do this, we will see the following results printed
    to the console:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们首先在机器上安装`spacy`。接下来，我们使用一个小型（`sm`）英语（`en`）模型来初始化`spacy`，该模型经过网络文本（`web`）的训练，涵盖`spacy`的核心元素：命名实体、词性标注和句法依赖。然后，我们将该模型应用于数据集中的第一条文本。这样做后，我们会在控制台中看到以下结果：
- en: '![](img/a3b0807f-e1dd-4a72-aeb0-c1e2488d292c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3b0807f-e1dd-4a72-aeb0-c1e2488d292c.png)'
- en: In the preceding example, we see that `spacy` stores each token separately with
    a token ID and a sentence ID. The three additional pieces of data supplied are
    listed next to each token. Let's look at the example for the `11` token ID. In
    this case, `Cubs`, which the model has identified as a part of speech, is a proper
    noun and the named entity type is **organization**. We see the `ORG_B` code, which
    means this token begins with the name of an organization. In this case, the one-term
    begins and ends with the name of the organization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们看到`spacy`将每个词汇分别存储，并为其分配一个词汇ID和句子ID。每个词汇旁边列出了三项附加数据。我们来看一下`11`词汇ID的例子。在这个例子中，`Cubs`被模型识别为一个词性，它是一个专有名词，命名实体类型是**组织**。我们看到`ORG_B`代码，这意味着这个词汇以一个组织的名字开始。在这个例子中，这个单一的词汇就代表着组织名称的开始和结束。
- en: 'Let''s look at a few other examples. If you scroll down the results in your
    console, you should find a section that looks like the following output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看几个其他例子。如果你在控制台中向下滚动结果，你应该能找到类似以下输出的部分：
- en: '![](img/f268edfd-aa45-4734-ac1d-df1333588863.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f268edfd-aa45-4734-ac1d-df1333588863.png)'
- en: In the preceding screenshot, we see additional information that `spacy` can
    identify. Let's look at lines `76` and `77`. We see that the term used in the
    text is `won't`. However, the `spacy` model used lemmatization to break up this
    contraction. Of course, `won't` is just a contracted form of `will not` and the
    model has split out the two terms that are part of the contracted term. In addition,
    the part of the speech for each term is included. Another example is lines `90`
    and `91`. Here, the terms `this` and `season` are adjacent and the model correctly
    identifies these two terms together to refer to a particular date part of the
    speech, which means that it is not `last season` or `next season`, but `this season`.
    In the named entities column, `this` has a `DATE_B` tag, which means the term
    refers to a date and this term is the beginning of this particular date-type.
    Similarly, `season` has a tag of `DATE_I`, which means that it refers to a date-type
    piece of data and the token is inside the entity. We know from these two tags
    that `this` and `season` are related and together refer to a specific point in
    time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们看到了`spacy`可以识别的额外信息。我们来看一下`76`和`77`行。我们看到文本中使用的词是`won't`。然而，`spacy`模型使用了词形还原来拆分这个缩写。自然，`won't`只是`will
    not`的缩写形式，模型已经将构成这个缩写的两个词分开了。另外，每个词的词性也被包含在内。另一个例子是`90`和`91`行。这里，`this`和`season`是相邻的，模型正确地将这两个词一起识别为指向特定日期部分的词性，这意味着它不是`last
    season`或者`next season`，而是`this season`。在命名实体列中，`this`具有`DATE_B`标签，这意味着该词表示日期，并且它是这个特定日期类型的开始部分。类似地，`season`具有`DATE_I`标签，这意味着它表示一个日期类型的数据，并且该词在实体内部。从这两个标签，我们可以知道`this`和`season`是相关的，并且一起指代一个特定的时间点。
- en: Another way we can use word embedding is to cluster our text data into topic
    groups. Topic grouping will result in a data object with lists of terms that co-occur
    near each other in the text. Through this process, we can see which topics are
    being discussed the most in the text data that we are analyzing. We will create
    topic group clusters next.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用词嵌入将文本数据聚类为主题组。主题分组将生成一个数据对象，其中包含在文本中彼此靠近共现的词汇列表。通过这个过程，我们可以看到在我们分析的文本数据中，哪些话题被讨论得最多。接下来，我们将创建主题组聚类。
- en: Clustering data into topic groups
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据聚类为主题组
- en: 'Let''s use word embeddings to find all semantically similar words. To do this,
    we will use the `textmineR` package to create a skip-gram model. The objective
    of the skip-gram model is to look for terms that occur often within a given window
    of another term. Since these terms are so frequently close to each other within
    sentences in our text, we can conclude they have some connection to each other.
    We will start by using the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用词嵌入来找到所有语义相似的词汇。为此，我们将使用`textmineR`包来创建一个skip-gram模型。skip-gram模型的目标是查找在给定窗口内与另一个词汇经常同时出现的词汇。由于这些词汇在我们的文本句子中如此频繁地靠近在一起，我们可以得出它们彼此之间有某种联系的结论。我们将通过以下步骤开始：
- en: 'To begin building our skip-gram model, we first create a term co-occurrence
    matrix by running the following code:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了开始构建我们的skip-gram模型，我们首先通过运行以下代码创建一个词汇共现矩阵：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After running the code, you will have a `sparse` matrix in your environment
    window. The matrix has every possible term along both dimensions, as well as a
    value at the intersection of the terms if they occur together within the skip-gram
    window, which in this case is `10`. An example of what a portion of this matrix
    looks like can be seen here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你的环境窗口中将会有一个`sparse`矩阵。该矩阵的两个维度上有所有可能的术语，并且在这些术语相互出现在跳字窗口内时，矩阵中相应位置会显示一个值，跳字窗口在本例中为`10`。矩阵的部分内容如下所示：
- en: '![](img/b912df1f-e53f-4c78-83a6-fad328df1c5b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b912df1f-e53f-4c78-83a6-fad328df1c5b.png)'
- en: 'Next, we will fit a **Latent Dirichlet allocation** (**LDA**) model on the
    text co-occurrence matrix that we just made. For our model, we will choose to
    create 20 topics and will have the model perform 500 Gibbs iterations, setting
    the `burning` value to `200`, which is the number of samples we will discard first.
    We will set `calc_coherence` to `TRUE` to include this metric. `coherence` is
    the relative distance between terms for a topic and we will use this distance
    value to rank the strength of the topics we have found. We define our LDA model
    by running the following code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将基于我们刚刚创建的文本共现矩阵，拟合一个**潜在狄利克雷分配**（**LDA**）模型。对于我们的模型，我们将选择创建20个主题，并让模型执行500次Gibbs迭代，设置`burning`值为`200`，即我们首先丢弃的样本数。我们将设置`calc_coherence`为`TRUE`以包括该度量。`coherence`是主题中术语之间的相对距离，我们将使用这个距离值来对找到的主题强度进行排名。我们通过运行以下代码来定义我们的LDA模型：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Our next step will be to get the top terms for each topic. We will use `phi`,
    which represents a distribution of words over topics as the topics and the argument,
    `M`, to choose how many terms to include in each topic cluster. We can retrieve
    our top terms per topic by running the following code:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，我们将为每个主题提取最重要的术语。我们将使用`phi`，它表示主题中单词的分布，以及参数`M`，用来选择每个主题集群中包含多少个术语。我们可以通过运行以下代码来获取每个主题的最重要术语：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will take our topics and top terms and add in the `coherence` score along
    with a `prevalence` score, which shows how often the terms occur in the entire
    text corpus we are analyzing. We can assemble this summary data object by running
    the following code:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将提取我们的主题和重要术语，并添加`coherence`评分以及`prevalence`评分，后者表示这些术语在我们分析的整个文本语料库中出现的频率。我们可以通过运行以下代码来构建这个摘要数据对象：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we have created this summary data object, we can look at the top five
    topics by the `coherence` value. We can identify which topics have the most terms
    that are relatively close to each other by running the following code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了这个摘要数据对象，我们可以根据`coherence`值查看前五个主题。我们可以通过运行以下代码，识别出哪些主题的术语相对更接近：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we run the preceding code, we see the top five topics that we have identified
    in the text object. You will see the following topics printed to your console:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们会看到文本对象中识别出的前五个主题。你将在控制台中看到以下主题：
- en: '![](img/4f671001-4c4e-45d0-96bb-0ab8278c8584.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f671001-4c4e-45d0-96bb-0ab8278c8584.png)'
- en: We have loaded in text data, extracted terms from the text, used a model to
    identify associated information for the terms—such as named entity details and
    part of speech—and organized terms according to topics discovered in the text.
    Next, we will reduce our text objects by using modeling to summarize documents
    in our text.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经加载了文本数据，从文本中提取了术语，使用模型识别了与术语相关的信息——如命名实体细节和词性——并根据在文本中发现的主题组织了这些术语。接下来，我们将通过建模来减少我们的文本对象，以总结文档内容。
- en: Summarizing documents using model results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型结果总结文档
- en: 'In this last step, before moving on to building our own model, we will use
    the `textrank` package to summarize the text. The approach this algorithm uses
    to summarize text is to look for a sentence with the most words that are also
    used in other sentences in the text data. We can see how this type of sentence
    would be a good candidate for summarizing the text since it contains many words
    found elsewhere. To get started, let''s select a piece of text from our data:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，在开始构建我们自己的模型之前，我们将使用`textrank`包来总结文本。该算法用于总结文本的方法是寻找一个包含最多在文本数据中其他句子中出现的词汇的句子。我们可以看到，这种类型的句子非常适合用来总结文本，因为它包含了在其他地方出现的许多词汇。为了开始，我们从我们的数据中选择一段文本：
- en: 'Let''s view the text in row `400` by running the following code:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过运行以下代码来查看第`400`行的文本：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we run this line of code, we will see the following piece of text printed
    to the console:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这行代码时，我们会在控制台看到以下文本：
- en: '![](img/456bb460-e652-457c-b8e6-3964dff1fe43.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/456bb460-e652-457c-b8e6-3964dff1fe43.png)'
- en: In this email, we can see that the subject matter regards objecting to someone
    else's email because it is off-topic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这封邮件中，我们可以看到主题是反对他人邮件的内容，因为它与主题无关。
- en: 'Let''s see which sentence the `textrank` algorithm will extract to summarize
    the text. To get started, we will first perform tokenization on the text. However,
    unlike earlier where we created word tokens, this time we will create sentence
    tokens. In addition, we will use the row numbers for each sentence extracted as
    the sentence ID. To create sentence tokens from our text, we run the following
    code:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看`textrank`算法将提取哪一句来总结文本。首先，我们将对文本进行分词处理。然而，不像之前我们创建了词汇分词，这次我们将创建句子分词。此外，我们还将使用每个提取句子的行号作为句子ID。为了从文本中创建句子分词，我们运行以下代码：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will create word tokens as we did previously. Remember that the reason
    we create sentence and word tokens is because we need to see which words occur
    in the most sentences and, of those words, which sentence contains the most frequently
    occurring words. To create the data object with one word per row, we run the following
    code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将像之前一样创建词汇分词。记住，创建句子和词汇分词的原因是我们需要查看哪些词汇出现在最多的句子中，而在这些词汇中，哪一个句子包含最多频繁出现的词汇。为了创建每行一个词的数据对象，我们运行以下代码：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we run the `textrank_sentences` function, which calculates the best summary
    sentences in the way previously described. We calculate the `textrank` score,
    which measures which sentences best summarize the text, by running the following
    code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们运行`textrank_sentences`函数，它按之前描述的方式计算最佳的摘要句子。我们通过运行以下代码计算`textrank`分数，这个分数衡量哪些句子最能总结文本：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have now ranked the sentences. If we view the summary, we can see the top
    five sentences by default. However, in this case, let's start by looking at the
    very top-ranked sentence and see how well that does at summarizing the overall
    text.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经对句子进行了排名。如果查看摘要，我们可以看到默认显示的是前五个句子。然而，在这种情况下，让我们从排名最高的句子开始，看看它如何总结整体文本。
- en: 'To look at just the top-ranked sentence, we first have to look at the first
    object in the returned list, which is a data frame of the sentences with their
    corresponding `textrank` score. Next, we arrange them by descending `textrank`
    score to select the highest-rated sentence. Afterward, we select the top row and
    extract just the sentence data. To print the top-ranked sentence based on the
    `textrank` algorithm, we run the following code:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看排名最高的句子，我们首先必须查看返回列表中的第一个对象，这个对象是一个包含句子及其对应`textrank`分数的数据框。接着，我们按照`textrank`分数降序排列，选择评分最高的句子。然后，我们选择顶部的行，并仅提取句子数据。为了打印基于`textrank`算法的排名最高的句子，我们运行以下代码：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After running this code, you will see the following console output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，您将看到如下的控制台输出：
- en: '![](img/90529700-5a88-4e79-8489-5ce1022ff9e0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90529700-5a88-4e79-8489-5ce1022ff9e0.png)'
- en: The sentence selected is restrict the discussion to appropriate newsgroups.
    If we read the entire text again, we can see that this sentence does capture the
    essence of what the writer is communicating. In fact, if the email only had this
    line it would convey almost the same information. In this way, we can confirm
    that the `textrank` algorithm performed well and that the selected sentence is
    a good summary of the entire text.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选中的句子是将讨论限制在适当的新闻组中。如果我们重新阅读整篇文本，可以看到这句话确实捕捉到了作者想要表达的核心内容。实际上，如果邮件只有这一行，它几乎能传达相同的信息。通过这种方式，我们可以确认`textrank`算法表现良好，选中的句子是整个文本的一个很好的总结。
- en: Now that we have covered some of the essential text analytics tools offered
    by various R packages, we will proceed with creating our own deep learning text
    model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些由各种R包提供的基础文本分析工具，接下来我们将开始创建自己的深度学习文本模型。
- en: Creating an RBM
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RBM
- en: So far, we have extracted elements from text, added metadata, and created term
    clusters to discover latent topics. We will now identify latent features by using
    a deep learning model known as an RBM. As you may recall, we have discovered latent
    topics in the text by looking for term co-occurrence within a given window size.
    In this case, we will go back to using a neural network approach. The RBM is half
    the typical neural network. Instead of taking data through hidden layers to an
    output layer, the RBM model just takes the data to the hidden layers and this
    is the output. The end result is similar to factor analysis or principal component
    analysis. Here, we will begin the process of finding each of the 20 Newsgroups
    in the dataset and throughout the rest of this chapter, we will make modifications
    to the model to improve its performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从文本中提取了元素，添加了元数据，并创建了术语集群来发现潜在主题。接下来，我们将通过使用一种深度学习模型——RBM（受限玻尔兹曼机）来识别潜在特征。正如你可能记得的，我们曾通过在给定的窗口大小内寻找术语共现来发现文本中的潜在主题。在这种情况下，我们将重新回到使用神经网络的方法。RBM是典型神经网络的一半。它不是通过隐藏层将数据传递到输出层，而是仅将数据传递到隐藏层，输出就是这个隐藏层的内容。最终结果类似于因子分析或主成分分析。在这里，我们将开始查找数据集中每个20个新闻组的过程，并且在本章的其余部分，我们将对模型进行修改以提高其性能。
- en: 'To get started with building our RBM, we will need to load two libraries. The
    first library will be `tm`, which is used for text mining in R, and has functions
    for creating a document-term matrix and performing text cleanup. The other library
    that we will need is `deepnet`, which has a function for the RBM. To load these
    two libraries, we run the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始构建我们的RBM，我们需要加载两个库。第一个库是`tm`，它用于R中的文本挖掘，具有创建文档-术语矩阵和执行文本清理的功能。另一个库是`deepnet`，它有一个用于RBM的函数。为了加载这两个库，我们运行以下代码：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we will take our text data and create a corpus, which in this case will
    place the contents from each newsgroup email into a separate list element. From
    there, we will remove some non-informative elements. We will also cast all text
    to lowercase to decrease the unique term count, as well as group like terms together
    regardless of their letter case. Afterward, we will cast the remaining terms to
    a document-term matrix, where all the terms make up one dimension of the matrix
    and all the documents make up the other dimension and the represented value in
    the matrix if the term is present in the document. We will also use **term frequency-inverse
    document frequency** (**tf-idf**) weighting.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将取出我们的文本数据并创建一个语料库，在这个语料库中，每个新闻组的电子邮件内容将被放置在单独的列表元素中。然后，我们将移除一些不含有信息的元素。我们还将把所有文本转换为小写，以减少唯一术语的数量，并将相同的术语按字母大小写归类。之后，我们将把剩余的术语转换为文档-术语矩阵，其中所有术语构成矩阵的一个维度，所有文档构成另一个维度，矩阵中表示的值是该术语是否出现在文档中。我们还将使用**词频-逆文档频率**（**tf-idf**）加权。
- en: 'In this case, the value in the matrix will not be binary but rather a float
    representing the uniqueness of the term within the document, down-weighting terms
    that occur frequently in all documents and giving more weight to terms that are
    only present in one or some documents but not all. To perform these steps and
    prepare our text data to be inputted into an RBM model, we run the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，矩阵中的值将不再是二进制的，而是一个浮动值，表示术语在文档中的唯一性，减少那些在所有文档中都频繁出现的术语的权重，并增加那些仅出现在一个或一些文档中，而不是所有文档中的术语的权重。为了执行这些步骤并准备我们的文本数据输入到RBM模型中，我们运行以下代码：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will now split our data into `train` and `test` sets as with any modeling
    exercise. In this case, we will create our `train` and `test` sets by running
    the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像任何建模任务一样，现在将我们的数据分割成`train`和`test`集合。在这种情况下，我们将通过运行以下代码来创建我们的`train`和`test`集合：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With the data in the proper format and split into `train` and `test` sets,
    we can now train our RBM model. Training the model is quite straightforward and
    there are not too many parameters to configure. For now, we will modify a few
    arguments and make changes to others as we progress through the chapter. To start,
    we will train a starter RBM model by running the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据已经按照正确的格式，并分割成`train`和`test`集合时，我们现在可以训练我们的RBM模型。训练模型非常直接，配置的参数并不多。暂时，我们将修改一些参数，并随着章节的进展对其他参数做出调整。首先，我们将通过运行以下代码来训练一个初步的RBM模型：
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, we set the `hidden` layers to the number of newsgroups
    to see if there is enough latent information to map the text to the newsgroups.
    We start with `100` rounds and leave everything else as a default.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将 `hidden` 层设置为新闻组的数量，以查看是否有足够的潜在信息将文本映射到新闻组上。我们从 `100` 轮开始，其他设置保持默认。
- en: 'We can now explore the latent features found in the text. We use our trained
    model to perform this task by passing in data as input, which results in inferred
    hidden units being produced as output. We infer the hidden units for the `test`
    data by running the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以探索文本中找到的潜在特征。我们使用训练好的模型来执行此任务，通过将数据作为输入，从而生成推断的隐藏单元作为输出。我们通过运行以下代码来推断
    `test` 数据的隐藏单元：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After running this code, we have defined the latent feature space for the `test`
    data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们已经定义了 `test` 数据的潜在特征空间。
- en: Defining the Gibbs sampling rate
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义吉布斯采样率
- en: Gibbs sampling plays a key role in constructing an RBM, so we will take a moment
    here to define this sampling type. We will briefly walk through a couple of quick
    concepts that lead to how to perform Gibbs sampling and why it matters for this
    type of modeling. With RBM models, we are first using a neural network to map
    our input or visible units to hidden units, which can be thought of as latent
    features. After training our model, we want to either take a new visible unit
    and define the probability that it belongs to the hidden units in the model, or
    do the reverse. We also want this to be computationally efficient, so we use a
    Monte Carlo approach.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 吉布斯采样在构建 RBM 模型中起着关键作用，因此我们在这里稍作停留，定义这一采样类型。我们将简要介绍几个快速概念，帮助理解如何执行吉布斯采样，以及为什么这种方法对这种类型的建模至关重要。在
    RBM 模型中，我们首先使用神经网络将输入或可见单元映射到隐藏单元，这些可以看作是潜在特征。在训练我们的模型后，我们希望通过给定一个新的可见单元，定义它属于模型中隐藏单元的概率，或者反过来做。我们还希望这个过程在计算上是高效的，因此我们使用了蒙特卡罗方法。
- en: Monte Carlo methods involve sampling random points to approximate an area or
    distribution. A classic example involves drawing a 10-by-10 inch square and inside
    this square draw a circle. We know that a circle with a 10-inch diameter has an
    area of 78.5 inches. Now, if we use a random number generator to choose float
    pairs between 0 and 10 and do this 20 times and plot the points, we will likely
    end up with around 15 points in the circle and 5 outside the circle. If we use
    just these points, then we would approximate the area is 75 inches. Now, if we
    try this again with a less conventional shape but something with many curves and
    angles, then it would be much more difficult to calculate the area. However, we
    could use the same approach to approximate the area. In this way, Monte Carlo
    approaches work well when a distribution is difficult or computational costly
    to define precisely, which is the case with our RBM model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法涉及通过采样随机点来近似一个区域或分布。一个经典的例子是绘制一个 10x10 英寸的正方形，并在正方形内绘制一个圆。我们知道，直径为 10
    英寸的圆的面积是 78.5 平方英寸。现在，如果我们使用随机数生成器选择 0 到 10 之间的浮动对，并进行 20 次操作并绘制这些点，我们可能会得到约 15
    个点位于圆内，5 个点位于圆外。如果我们仅使用这些点，那么我们会估算该区域面积为 75 平方英寸。现在，如果我们尝试用一个不那么传统但有许多曲线和角度的形状来进行类似的操作，那么计算面积将变得更加困难。然而，我们仍然可以使用相同的方法来近似该区域。通过这种方式，当一个分布难以精确地定义或计算成本高昂时，蒙特卡罗方法会发挥很好的作用，这正是我们的
    RBM 模型所面临的情况。
- en: Next, a Markov Chain is a technique in defining conditional probability that
    only takes into account the event that just preceded the event probability we
    are trying to predict, rather than events that happened two or more steps back.
    This is a very simple form of conditional probability. A classic example for explaining
    this concept is the game Chutes and Ladders. In this game, there are 100 squares
    and a player rolls a six-sided die to determine the number of spaces to move,
    with the object being to get to square 100\. Along the way, a square may contain
    a slide that will move the player backward a certain number of squares, or a ladder
    that will move the player forward a certain number of squares.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，马尔科夫链是一种定义条件概率的技术，它仅考虑紧接在我们试图预测的事件概率之前发生的事件，而不是考虑发生在两步或更多步之前的事件。这是一种非常简单的条件概率形式。一个经典的例子用于解释这个概念的是“滑梯与梯子”游戏。在这个游戏中，有
    100 个方格，玩家掷一个六面骰子来决定移动的步数，目标是到达第 100 格。在过程中，有些方格可能包含一个滑梯，玩家会倒退一定数量的方格，或者有一个梯子，玩家会向前移动一定数量的方格。
- en: When determining the likelihood of landing on a given square, the only thing
    that matters is the previous roll that resulted in the player landing on a certain
    square. Whichever combination of rolls got the player to that point does not have
    any impact on the probability of reaching a certain square based on the square
    the player is currently on.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定停留在某个特定方格的可能性时，唯一重要的是导致玩家停留在某个方格上的上一轮掷骰子。无论是哪种掷骰子的组合将玩家带到这个点，都不会影响基于玩家当前所在方格到达某个特定方格的概率。
- en: 'For context, we will discuss these two concepts briefly because they are both
    involved in Gibbs sampling. This type of sampling is a Monte Carlo Markov Chain
    method, which means that we start from an initial state and afterward we predict
    the likelihood that a certain event, `x`, happens given another event, `y`, and
    vice versa. By calculating this type of back-and-forth conditional probability
    over a certain number of samples, we efficiently approximate the probability that
    a given `visible` unit belongs to a given `hidden` unit. We can perform a few
    very simple examples of sampling from a Gibbs distribution. In this example, we
    will create a function with an argument, `rho`, as a coefficient value to modify
    the given term when calculating the value for the other variable, while in our
    RBM model the learned weights and the bias term perform this function. Let''s
    create a sampler using the following steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一些背景，我们将简要讨论这两个概念，因为它们都涉及到吉布斯采样。这种类型的采样是一种蒙特卡洛马尔可夫链方法，这意味着我们从一个初始状态开始，然后我们预测在给定另一个事件`y`的情况下，某个事件`x`发生的可能性，反之亦然。通过计算这种相互之间的条件概率，经过一定数量的样本后，我们可以高效地近似给定的`visible`单元属于某个`hidden`单元的概率。我们可以通过一些非常简单的吉布斯分布采样示例来进行实验。在这个示例中，我们将创建一个函数，使用参数`rho`作为系数值，在计算另一个变量的值时修改给定项，而在我们的RBM模型中，学习到的权重和偏置项执行了这个功能。让我们使用以下步骤创建一个采样器：
- en: 'Let''s first define a very simple Gibbs sampler to understand the concept by
    running the following code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个非常简单的吉布斯采样器，通过运行以下代码来理解这个概念：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we have defined the function, let's calculate two separate 10 x 2 matrices
    by choosing two different values for `rho`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了函数，让我们通过选择两个不同的`rho`值来计算两个独立的10 x 2矩阵。
- en: 'We calculate our first 10 x 2 matrix by running the following code using a
    value of `0.75` for `rho`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过以下代码计算第一个10 x 2矩阵，使用`0.75`作为`rho`的值：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we calculate a 10 x 2 matrix using a value of `0.03` for `rho`, using
    the following line of code:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`0.03`作为`rho`的值，通过以下代码计算一个10 x 2的矩阵：
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After running each of these, you should see a 10 x 2 matrix printed to your
    console. This function involves drawing random values from a normal distribution,
    so the matrix printed to your console will be slightly different. However, you
    will see the way the values are generated iteratively using the values from the
    previous iteration to determine the value in the current iteration. We see here
    how the Monte Carlo randomness is employed in calculating our value along with
    the Markov Chain conditional probability. Now, with an understanding of Gibbs
    sampling, we will explore contrastive divergence, which is a way we can use what
    we learned about Gibbs sampling to modify our model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行完每个步骤后，你应该能在控制台看到打印出的10 x 2矩阵。这个函数涉及从正态分布中抽取随机值，因此你在控制台上看到的矩阵会略有不同。不过，你会看到值是如何通过使用前一个迭代的值来逐步生成当前迭代的值的。我们可以看到蒙特卡洛的随机性是如何在计算我们的值时与马尔可夫链的条件概率一起使用的。现在，通过理解吉布斯采样，我们将探讨对比发散，它是我们可以利用吉布斯采样的知识来修改我们模型的一种方式。
- en: Speeding up sampling with contrastive divergence
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过对比发散加速采样
- en: Before proceeding, we need to change up the dataset being used. While the 20
    Newsgroups dataset has worked well up until this point for all the concepts on
    text analysis, it becomes less usable as we try to really tune our model to predict
    latent features. All the additional changes that we will do next actually have
    minimal impact on the model when using the 20 Newsgroups, so we will switch to
    the spam versus ham dataset, which is similar. However, instead of involving emails
    to a newsgroup, these are SMS text messages. In addition, instead of the target
    variable being a given newsgroup, the target is either that the message is spam
    or a legitimate text message.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要更换使用的数据集。虽然20个新闻组数据集到目前为止很好地适用于所有文本分析概念，但当我们尝试真正调优模型以预测潜在特征时，它变得不再那么适用了。接下来我们所做的所有额外修改对使用20个新闻组数据集的模型几乎没有影响，因此我们将切换到垃圾邮件与正常邮件的数据集，它们相似。不过，与其说是新闻组的电子邮件，这里是短信文本信息。此外，目标变量不再是一个给定的新闻组，而是判断消息是否是垃圾邮件或合法短信。
- en: 'Contrastive divergence is the argument that allows us to leverage what we learned
    about Gibbs sampling. The value that we pass to this argument in the model will
    adjust how many times the Gibbs sampling is performed. In other words, this controls
    the length of the Markov Chain. The lower the value, the faster each round of
    the model will be. If the value is higher, then each round is computationally
    more costly, although the model may converge more quickly. In the following steps,
    we can train a model with three different values for contrastive divergence to
    see how adjusting this argument affects the model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对比散度是一个可以帮助我们利用吉布斯采样学习的参数。我们在模型中传递给这个参数的值将调整吉布斯采样的执行次数。换句话说，这控制着马尔可夫链的长度。值越低，每一轮模型的执行速度越快。如果值较高，那么每一轮的计算开销较大，尽管模型可能更快收敛。在接下来的步骤中，我们可以使用三个不同的对比散度值来训练模型，看看调整这个参数对模型的影响：
- en: 'To begin, we will load in the spam versus ham dataset using the following code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用以下代码加载垃圾邮件与正常邮件的数据集：
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will move our target variables to a vector, `y`, and the predictor
    text data to a variable, `x`. Afterward, we will perform some basic text preprocessing
    by removing special characters and one- and two-character words, as well as removing
    any white space. We define our target variable and predictor variables, along
    with cleaning the text, by running the following code:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将目标变量移到一个向量`y`中，将预测文本数据移到变量`x`中。之后，我们将进行一些基本的文本预处理，移除特殊字符和一字或二字词，并去除任何空白字符。我们通过运行以下代码定义目标变量和预测变量，并清理文本：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we convert this cleaned up text into a `corpus` data object and then
    into a document-term matrix. We convert our text data into a suitable format for
    modeling by running the following code:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将这段清理后的文本转换成一个`corpus`数据对象，然后再转化为文档-词矩阵。我们通过运行以下代码，将文本数据转换为适合建模的格式：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, let''s divide our data into `train` and `test` sets, exactly as we did
    with the 20 Newsgroups dataset. We get our data divided and ready for modeling
    using the following code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分成`train`和`test`两部分，就像我们在20个新闻组数据集上做的那样。我们使用以下代码将数据划分并准备好用于建模：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that all of our data is prepared, let''s run our three models and see how
    they compare. We run a quick version of the three RBM models to evaluate the impact
    of adjusting the contrastive divergence value by running the following code:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在所有数据都已准备好，让我们运行三个模型并看看它们的比较。我们运行这三个RBM模型的简化版，通过以下代码评估调整对比散度值对模型的影响：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: To measure how much change this argument has had on the model, we will use the
    free energy values in the model object.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量这个参数对模型的影响，我们将使用模型对象中的自由能值。
- en: Computing free energy for model evaluation
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算模型评估的自由能
- en: RBMs belong to a class of energy-based models. These use a free energy equation
    that is analogous to the cost function in other machine learning algorithms. Just
    like a cost function, the objective is to minimize the free energy values. A lower
    free energy value equates to a higher probability that the visible unit variables
    are being described by the hidden units and a higher value equates to a lower
    likelihood.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: RBM（受限玻尔兹曼机）属于一类基于能量的模型。它们使用的自由能方程类似于其他机器学习算法中的代价函数。就像代价函数一样，目标是最小化自由能值。较低的自由能值意味着可见单元变量更有可能被隐藏单元描述，而较高的自由能值则表示较低的可能性。
- en: 'Let''s now look at the three models we just created and compare free energy
    values for these models. We compare the free energy to identify which model is
    performing better by running the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们查看我们刚刚创建的三个模型，并比较这些模型的自由能量值。我们通过运行以下代码来比较自由能量，从而识别哪个模型表现更好：
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After running this code, an output similar to the following will be printed
    to your console:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，控制台将打印出类似以下的输出：
- en: '![](img/82ac6d3b-213c-4147-9012-f77cd576d146.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82ac6d3b-213c-4147-9012-f77cd576d146.png)'
- en: In this case, using just one round of Gibbs sampling produces the best performing
    model in terms of reducing free energy in the quickest way.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只使用一轮 Gibbs 采样就能生成在最快方式下减少自由能量的最佳模型。
- en: Stacking RBMs to create a deep belief network
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠 RBM 创建深度置信网络
- en: 'RBM models are a neural network with just two layers: the input, that is, the
    visible layer, and the hidden layer with latent features. However, it is possible
    to add additional hidden layers and an output layer. When this is done within
    the context of an RBM, it is referred to as a **deep belief network**. In this
    way, deep belief networks are like other deep learning architectures. For a deep
    belief network, each hidden layer is fully connected meaning that it learns the
    entire input.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 模型是一个只有两层的神经网络：输入层，即可见层，以及具有潜在特征的隐藏层。然而，可以添加额外的隐藏层和输出层。当在 RBM 的上下文中进行此操作时，它被称为
    **深度置信网络**。因此，深度置信网络就像其他深度学习架构一样。对于深度置信网络，每个隐藏层是完全连接的，意味着它学习整个输入。
- en: The first layer is the typical RBM, where latent features are calculated from
    the input units. In the next layer, the new hidden layer learns the latent features
    from the previous hidden layer. This, in turn, can lead to an output layer for
    classification tasks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是典型的 RBM，其中潜在特征是从输入单元计算得出的。在下一层中，新的隐藏层从前一个隐藏层学习潜在特征。这样，最终可以得到一个用于分类任务的输出层。
- en: 'Implementing a deep belief network uses a similar syntax to what was used to
    train the RBM. To get started, let''s first perform a quick check of the latent
    feature space from the RBM we just trained. To print a sample of the latent feature
    space from the model, we use the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 实现深度置信网络使用的语法与训练 RBM 时所用的语法相似。为了开始，我们首先对刚训练好的 RBM 的潜在特征空间进行快速检查。为了打印出模型的潜在特征空间的样本，我们使用以下代码：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the preceding code, we use the `up` function to generate a matrix of latent
    features using the model that we just fit. The `up` function takes as input an
    RBM model and a matrix of visible units and outputs a matrix of hidden units.
    The reverse is also possible. The `down` function takes a matrix of hidden units
    as input and outputs visible units. Using the preceding code, we will see an output
    like the following printed to the console:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `up` 函数通过我们刚刚拟合的模型生成潜在特征矩阵。`up` 函数以一个 RBM 模型和一个可见单元矩阵为输入，并输出一个隐藏单元矩阵。反之也可以。`down`
    函数以一个隐藏单元矩阵为输入，输出可见单元。通过前面的代码，我们将看到类似以下内容的输出打印到控制台：
- en: '![](img/e4c44db4-9007-4aae-b297-e035f4641a06.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4c44db4-9007-4aae-b297-e035f4641a06.png)'
- en: We can see variance in the feature space of this first layer. To prepare for
    the next step, we imagine using this matrix now as input to another RBM that will
    further learn features. In this way, we can code our deep belief network using
    an almost identical syntax to the syntax used for training the RBM. The exception
    will be that for the hidden layer argument, rather than a single value representing
    the number of units in a single hidden layer, we can now use a vector of values
    that represent the number of units in each successive hidden layer. For our deep
    belief network, we will start with `100` units, just like in our RBM.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第一层特征空间中的方差。为了准备下一步，我们可以想象将此矩阵作为输入传递给另一个 RBM，以便进一步学习特征。通过这种方式，我们可以使用与训练
    RBM 时几乎相同的语法来编码我们的深度置信网络。唯一的区别是，对于隐藏层参数，我们不再使用表示单个隐藏层单元数量的单一值，而是可以使用一个表示每个连续隐藏层单元数量的值向量。对于我们的深度置信网络，我们将从
    `100` 个单元开始，就像我们的 RBM 一样。
- en: 'We will then reduce this to `50` units in the next layer and `10` units in
    the layer after that. The other difference is that we now have a target variable.
    While an RBM is an unsupervised, generative model, we can use our deep belief
    network to perform a classification task. We train our deep belief network using
    the following code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在下一层将单元数减少到`50`，在那之后的层减少到`10`。另一个不同之处是，我们现在有了一个目标变量。虽然RBM是一个无监督的生成模型，但我们可以利用我们的深度置信网络执行分类任务。我们使用以下代码训练深度置信网络：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'With the deep belief network trained, we can now make predictions using our
    model. We perform this prediction task in a similar way to how we generate predictions
    for most machine learning tasks. However, in this case, we will use the `nn.predict`
    function to use our trained neural network to predict whether the new test input
    should be classified as spam or a legitimate text. We make a prediction on the
    `test` data using the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练好深度置信网络之后，我们现在可以使用该模型进行预测。我们执行预测任务的方式与大多数机器学习任务的预测生成方式类似。然而，在这种情况下，我们将使用`nn.predict`函数来使用训练好的神经网络预测新的测试输入是否应该被分类为垃圾邮件或合法文本。我们使用以下代码对`test`数据进行预测：
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We now have the probability values that tell us whether a given message is
    or is not spam. The probabilities are currently within a constrained range; however,
    we can still use it. Let''s make a cut in the probabilities and assign `1` for
    those above the threshold signifying that the message is predicted to be spam,
    and everything under the cut point will receive a value of `0`. After making this
    dividing line and creating a vector of binary values, we can create a confusion
    matrix to see how well our model performed. We create our binary variables and
    then see how well our model performed by running the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经得到了概率值，这些概率值告诉我们给定的消息是否是垃圾邮件。这些概率目前在一个受限范围内；然而，我们仍然可以使用它。让我们在概率上设置一个截断点，对于高于该阈值的概率值，我们将其标记为`1`，表示消息被预测为垃圾邮件，而低于该截断点的所有值将被赋值为`0`。在设置好这个分界线并创建一个二值化的向量后，我们可以创建混淆矩阵来查看模型的表现。我们创建了二进制变量，然后通过运行以下代码来评估模型的表现：
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After running the preceding code, we will see the following output to our console:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们将在控制台看到以下输出：
- en: '![](img/42061ee5-9adc-45eb-8058-900c569ea209.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42061ee5-9adc-45eb-8058-900c569ea209.png)'
- en: As we can see, even this very simple implementation of a deep belief network
    has performed fairly well. From here, additional modification can be made to the
    number of hidden layers, units in these layers, the output activation function,
    learning rate, momentum, and dropout, along with the contrastive divergence and
    the number of epochs or rounds.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，即使这个非常简单的深度置信网络实现也表现得相当不错。从这里开始，可以进一步修改隐藏层的数量、每层的单元数、输出激活函数、学习率、动量、丢弃率，以及对比散度和迭代次数等参数。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a number of methods for analyzing text data. We
    started with techniques for extracting elements from text data, such as taking
    a sentence and breaking it into tokens and comparing term frequency, along with
    collecting topics and identifying the best summary sentence and extracting these
    from the text. Next, we used some embedding techniques to add additional details
    to our data, such as parts of speech and named entity recognition. Lastly, we
    used an RBM model to find latent features in the input data and stacked these
    RBM models to perform a classification task. In the next chapter, we will look
    at using deep learning for time series tasks, such as predicting stock prices, in
    particular.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了多种分析文本数据的方法。我们从提取文本数据中的元素技巧开始，比如将句子分解为标记并比较词频，收集主题、识别最佳摘要句子，并从文本中提取这些内容。接下来，我们使用了一些嵌入技术，为我们的数据添加更多细节，如词性标注和命名实体识别。最后，我们使用了RBM模型来发现输入数据中的潜在特征，并将这些RBM模型堆叠起来执行分类任务。在下一章中，我们将探讨如何利用深度学习处理时间序列任务，尤其是股票价格预测。
