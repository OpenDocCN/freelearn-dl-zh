- en: Signal Processing - Data Analysis with Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信号处理 - 使用神经网络进行数据分析
- en: Having acquired substantial knowledge on neural networks, we are now ready to
    perform our first operation using them. We will start with processing signals,
    and see how a neural network is fed data. You will be mesmerized at how increasing
    the levels and complexity of neurons can actually make a problem look simple.
    We will then look at how language can be processed. We will make several predictions
    using datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了大量关于神经网络的知识后，我们现在准备使用它们进行第一个操作。我们将从处理信号开始，并看看神经网络如何处理数据。您将会被增加神经元的层次和复杂性如何使问题看起来变得简单所迷住。然后，我们将看看如何处理语言。我们将使用数据集进行几次预测。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Processing signals
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理信号
- en: Images as numbers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像视作数字
- en: Feeding a neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向神经网络输入数据
- en: Examples of tensors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量的示例
- en: Building a model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立模型
- en: Compiling the model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译模型
- en: Implementing weight regularization in Keras
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中实现权重正则化
- en: Weight regularization experiments
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重正则化实验
- en: Implementing dropout regularization in Keras
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中实现 dropout 正则化
- en: Language processing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言处理
- en: The internet movie reviews dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互联网电影评论数据集
- en: Plotting a single training instance
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制单个训练实例的图表
- en: One-hot encoding
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独热编码
- en: Vectorizing features
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量化特征
- en: Vectorizing labels
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量化标签
- en: Building a network
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建网络
- en: Callbacks
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回调
- en: Accessing model predictions
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问模型预测
- en: Feature-wise normalization
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐特征归一化
- en: Cross validation with the scikit-learn API
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn API 进行交叉验证
- en: Processing signals
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理信号
- en: There may only be four fundamental forces in our universe, but they are all
    signals. By signal, we mean any kind of feature representations we may have of
    a real-world phenomenon. Our visual world, for example, is full of signals that
    indicate motion, color, and shapes. These are very dynamic signals, and it is
    a miracle that biology is able to process these stimuli so accurately, even if
    we do say so ourselves. Of course, in the grander scheme of things, realizing
    that nature has had hundreds of millions of years to perfect this recipe may humble
    us, if only a little. For now, we can admire the marvel that is the human visual
    cortex, which is equipped with 140 million densely interconnected neurons. In
    fact, an entire series of layers (V1 – V5) exist through which information propagates
    as we engage in progressively more complex image processing tasks. The eye itself,
    using rods and cones to detect different patterns of light intensity and colors,
    does an excellent job of piecing together electromagnetic radiation and converting
    it into electrical impulses through photo transduction.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 宇宙中可能只存在四种基本力，但它们都是信号。所谓信号，是指我们对现实世界现象的各种特征表示。例如，我们的视觉世界充满了指示运动、颜色和形状的信号。这些是非常动态的信号，令人难以置信的是生物能够如此准确地处理这些刺激，即使我们这样说也是如此。当然，在更宏观的视角下，意识到自然已经花费了数亿年来完善这个配方，可能会让我们感到一点谦卑。但就目前而言，我们可以赞叹于人类视觉皮层的奇迹，这里配备有1.4亿个密集连接的神经元。事实上，存在一整套层次（V1
    - V5），我们在进行日益复杂的图像处理任务时，信息就会通过这些层次传播。眼睛本身使用棒状和锥状细胞来检测不同的光强度和颜色模式，非常出色地将电磁辐射拼接在一起，并通过光转导将其转化为电信号。
- en: When we look at an image, our visual cortex is actually interpreting the specific
    configuration of electromagnetic signals that the eye is converting into electrical
    signals and feeding it. When we listen to music, our eardrum, or myringa, simply
    converts and amplifies a successive pattern of vibrational signals so that our
    auditory cortex may process it. Indeed, it appears that the neural mechanisms
    in the brain are extremely efficient at abstracting and representing patterns
    that are present in different real-world signals. In fact, neuroscientists have
    even found that some mammalian brains have the capacity to be rewired in a manner
    that permits different cortices to process types of data that they were originally
    never intended to encounter. Most notably, scientists found that rewiring the
    auditory cortex of ferrets allowed these creatures to process visual signals from
    the auditory regions of the brain, allowing them to *see* using very different
    neurons that they previously employed for the task of audition. Many scientists
    cite such studies to put forth the case that the brain may be using a master algorithm,
    which is capable of handling any form of data, and turning it into efficient representations
    of the world around it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看一张图像时，我们的视觉皮层实际上是在解读眼睛将电磁信号转化为电信号并传递给它的特定配置。当我们听音乐时，我们的耳鼓，或称耳膜，仅仅是将一连串的振动信号转化并放大，以便我们的听觉皮层可以处理这些信号。实际上，大脑中的神经机制似乎非常高效，能够抽象和表征在不同现实世界信号中出现的模式。事实上，神经科学家们甚至发现一些哺乳动物的大脑具备重新连接的能力，这使得不同的皮层能够处理它们最初并未设计用来处理的数据类型。最值得注意的是，科学家们发现，通过重新连接雪貂的听觉皮层，这些生物能够处理来自大脑听觉区域的视觉信号，从而使它们能够使用先前用于听觉任务的非常不同的神经元来*“看见”*。许多科学家引用这些研究，提出大脑可能在使用一种主算法，能够处理任何形式的数据，并将其转化为高效的周围世界表征。
- en: As intriguing as this is, it naturally raises a thousand more questions about
    neural learning than it answers, and we sadly do not have the scope to address
    all of them in this book. Suffice it to say, whatever algorithm—or sets of algorithms—that
    lets our brain achieve such efficient representations of the world around us,
    are naturally of great interest to neurologists, deep learning engineers, and
    the rest of the scientific community alike.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这非常引人入胜，但它自然引发了更多关于神经学习的问题，而这些问题远远超出了我们在本书中能够解答的范围。可以说，不论是何种算法，或者一组算法，让我们的大脑实现如此高效的世界表征，都无疑是神经学家、深度学习工程师以及其他科学界人士所关注的重点。
- en: Representational learning
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表征学习
- en: As we saw earlier with our perceptron experiments on the TensorFlow Playground,
    sets of artificial neurons seem to be capable of learning fairly simple patterns.
    This is nothing remotely close to the sort of complex representations we humans
    can perform and wish to predict. However, we can see that, even in their nascent
    simplicity, these networks seem to be able to adapt to the sort of data we provide
    them with, at times even outperforming other statistical predictive models. So,
    what's going on here that is so different than previous approaches to teach machines
    to do things for us?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 TensorFlow Playground 上进行感知机实验时看到的那样，一组人工神经元似乎能够学习相当简单的模式。这与我们人类能够执行并希望预测的复杂表征差距甚远。然而，我们可以看到，即使在它们刚刚起步的简单性中，这些网络似乎能够适应我们提供的数据类型，有时甚至超过其他统计预测模型的表现。那么，究竟发生了什么，让它与过去教机器为我们做事的方法如此不同呢？
- en: It can be very useful to teach a computer what skin cancer looks like, simply
    by showing it the vast number of medically relevant features that we may have.
    Indeed, this is what our approach has been toward machines thus far. We would
    hand-engineer features so that our machines could easily digest them and generate
    relevant predictions. But why stop there? Why not just show the computer what
    skin cancer actually *looks* like? Why not show it millions of images and let
    *it* figure out what is relevant? Indeed, that's exactly what we try to do when
    we speak of deep learning. As opposed to traditional **Machine Learning** (**ML**)
    algorithms, where we represent the data in an explicitly processed representation
    for the machine to learn, we take a different approach with neural networks. Here,
    what we actually wish to achieve is for the network to learn these representations
    on its own.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 教会计算机识别皮肤癌的样子非常有用，方法就是展示我们可能拥有的大量医学相关特征。事实上，这正是我们迄今为止对机器的方法。我们会手动设计特征，使机器能够轻松处理它们并生成相关的预测。但为何仅仅停留在这里呢？为什么不直接向计算机展示皮肤癌的实际*样子*呢？为什么不展示数百万张图片，让*它*自己弄清楚什么是相关的呢？事实上，这正是我们在谈论深度学习时所尝试做的。与传统的**机器学习**（**ML**）算法不同，在传统算法中，我们将数据以明确处理过的表示形式提供给机器学习，而在神经网络中，我们采取不同的方法。我们实际希望实现的目标是让网络自己学习这些表示。
- en: 'As shown in the following diagram, a network achieves this by learning simple
    representations and using them to define more and more complex representations
    in successive layers, until the ultimate layer learns to represent output classes
    accurately:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，网络通过学习简单的表示，并利用它们在连续的层中定义越来越复杂的表示，直到最终的层能够准确地表示输出类别：
- en: '![](img/884630bc-15a4-4cfe-980e-51bbe6ed2471.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/884630bc-15a4-4cfe-980e-51bbe6ed2471.png)'
- en: This approach, as it turns out, can be quite useful for teaching your computer
    to detect complex movement patterns and facial expressions, just as we humans
    do. Say you want it to accept packages on your behalf when you're away, or perhaps
    detect any potential robbers trying to break in to your house. Similarly, what
    if we wanted our computer to schedule our appointments, find potentially lucrative
    stocks on the market, and keep us up to date according to what we find interesting?
    Doing so involves processing complex image, video, audio, text, and time-series
    data, all of which come in complex dimensional representations, and cannot be
    modeled by just a few neurons. So, how do we work with the neural learning system,
    akin to what we saw in the last chapter? How do we make neural networks learn
    the complex and hierarchical patterns in eyes, faces, and other real-world objects?
    Well, the obvious answer is that we make them bigger. But, as we will see, this
    brings in complexities of its own. Long story short, the more learnable parameters
    you put in a network, the higher the chance that it will memorize some random
    patterns, and hence will not generalize well. Ideally, you want a configuration
    of neurons that perfectly fits the learning job at hand, but this is almost impossible
    to determine a priori without performing experiments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这种方法对教会计算机识别复杂的运动模式和面部表情非常有用，就像我们人类一样。比如你希望它在你不在时替你接收包裹，或者检测任何潜在的盗贼试图闯入你的房子。类似地，如果我们希望计算机为我们安排约会，找到市场上可能有利可图的股票，并根据我们感兴趣的内容更新信息，该怎么办呢？这样做需要处理复杂的图像、视频、音频、文本和时间序列数据，这些数据都以复杂的维度表示，无法仅凭几个神经元建模。那么，我们如何与神经学习系统合作，就像我们在上一章看到的那样？我们如何让神经网络学习眼睛、面部和其他现实世界物体中的复杂层次模式呢？显然的答案是让它们变得更大。但正如我们将看到的，这带来了自身的复杂性。长话短说，你在网络中放入的可学习参数越多，它记住一些随机模式的可能性就越大，因此它的泛化能力就越差。理想情况下，你希望神经元的配置能够完美地适应当前的学习任务，但在没有进行实验之前，几乎不可能事先确定这种配置。
- en: Avoiding random memorization
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免随机记忆
- en: Another answer can be to manipulate not only the overall number of neurons,
    but also the degree of interconnectivity among those neurons. We can do this through
    techniques such as *dropout regularization* and *weighted parameter*, as we will
    see soon enough. So far, we have already seen the various computations that can
    be performed through each neuron as data propagates through a network. We also
    saw how the brain leverages hundreds of millions of densely interconnected neurons
    to get the job done. But, naturally, we can't just scale up our networks by arbitrarily
    adding more and more neurons. Long story short, simulating a neural structure
    close to the brain is likely to require thousands of **petaflops** (a unit of
    computing speed equal to one thousand million million (10^(15)) floating-point
    operations per second) of computing power. Maybe this will be possible in the
    near future, with the aforementioned paradigm of massively parallelized computing,
    along with other advances in software and hardware technologies. For now, though,
    we have to think of clever ways to train our network so that it can find the most
    efficient representations without wasting precious computational resources.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方法是，不仅操控神经元的总数，还要调整这些神经元之间的连接程度。我们可以通过技术手段来实现这一点，比如*dropout正则化*和*加权参数*，稍后我们将详细了解。目前为止，我们已经看到了通过每个神经元在数据通过网络传播过程中可以执行的各种计算。我们还看到了大脑如何利用数亿个密集连接的神经元完成任务。然而，自然地，我们不能仅仅通过任意增加更多神经元来扩展我们的网络。简而言之，模拟接近大脑的神经结构，可能需要成千上万的**千万亿次运算**（petaflops，一种计算速度单位，等于每秒进行一千万亿（10^(15)）次浮点运算）。也许在不久的将来，借助大规模并行计算范式，以及软件和硬件技术的其他进展，这将成为可能。不过，眼下，我们必须想出巧妙的方式来训练我们的网络，使其能够找到最有效的表示，而不会浪费宝贵的计算资源。
- en: Representing signals with numbers
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用数字表示信号
- en: In this chapter, we will see how we can layer sequences of neurons to progressively
    represent more and more complex patterns. We will also see how concepts such as
    regularization and batched learning are essential in getting the most out of a
    training session. We will learn to process different types of real-world data
    in the form of images, texts, and time-series dependent information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何将神经元的序列层叠起来，逐步表示出越来越复杂的模式。我们还将看到像正则化和批量学习等概念，在最大化训练效果中是多么重要。我们将学会处理各种现实世界数据，包括图像、文本和时序相关信息。
- en: Images as numbers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像作为数字
- en: For tasks such as these, we need deep networks with multiple hidden layers if
    we are hoping to learn any representative features for our output classes. We
    also need a nice dataset to practice our understanding and familiarize ourselves
    with the tools we will be using to design our intelligent systems. Hence, we come
    to our first hands-on neural network task as we introduce ourselves to the concepts
    of computer vision, image processing, and hierarchical representation learning.
    Our task at hand is to teach computers to read numbers not as 0 and 1s, as they
    already do, but more in the manner of how we would read digits that are composed
    by our own kin. We are speaking of handwritten digits, and for this task, we will
    be using the iconic MNIST dataset, the true *hello world* of deep learning datasets.
    For our first example, there are good theoretical and practical reasons behind
    our choice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的任务，我们需要深度网络并拥有多个隐藏层，如果希望为我们的输出类别学习任何具有代表性的特征的话。我们还需要一个良好的数据集来练习我们的理解，并让自己熟悉我们将在设计智能系统时使用的工具。因此，我们迎来了第一个实际操作的神经网络任务，同时也将开始接触计算机视觉、图像处理和层次化表示学习的概念。我们当前的任务是教计算机读取数字，不是像它们已经做的那样读取0和1，而是更像我们如何阅读由我们自己所写的数字。我们说的是手写数字，为此任务，我们将使用经典的MNIST数据集，深度学习数据集中的真正*hello
    world*。对于我们的第一个例子，选择它背后有着很好的理论和实践依据。
- en: 'From a theoretical perspective, we need to understand how we can use layer
    neurons to progressively learn more complex patterns, like our own brain does.
    Since our brain has had about 2,000 to 2,500 years worth of training data, it
    has gotten quite good at identifying complex symbols such as handwritten digits.
    In fact, we normally perceive this as an absolutely effortless task, since we
    learn how to distinguish between such symbols from as early as preschool. But
    this is actually quite a daunting task. Consider the vast variations in how each
    of these digits may be written by different humans, and yet our brains are still
    able to classify these digits, as if it were much ado about nothing:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论的角度来看，我们需要理解如何使用层神经元来逐步学习更复杂的模式，正如我们的大脑所做的那样。由于我们的大脑大约有2000到2500年的训练数据，它在识别复杂符号（如手写数字）方面变得非常熟练。事实上，我们通常认为这是一项完全不费力的任务，因为我们从学前教育开始就学习如何区分这些符号。但实际上，这是一项非常艰巨的任务。想一想，不同的人写这些数字时可能会有如此巨大的变化，而我们的脑袋却能够分类这些数字，就像这并不是什么大事一样：
- en: '![](img/cc8d8dcd-ce9c-4f0e-a1d9-180cbafdab03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc8d8dcd-ce9c-4f0e-a1d9-180cbafdab03.png)'
- en: While exhaustively coding explicit rules would drive any programmer insane,
    as we look at the preceding image, our own brain intuitively notice some patterns
    in the data. For example, it picks up on how both **2** and **3** have a half-loop
    at the top of them, and how **1**, **4**, and **7** have a straight downward line.
    It also perceives how a **4** is actually one downward line, one semi-downward
    line, and another horizontal line in between the others. Due to this, we are able
    to easily break down a complex pattern into smaller patterns. This is specifically
    easy to do with handwritten digits, as we just saw. Therefore, our task will be
    to see how we can construct a deep neural network and hope for each of our neurons
    to capture simple patterns from our data, such as line segments, and then progressively
    construct more complex patterns in deeper layers using the simple patterns we
    learned in the previous layers. We will do this to learn about the accurate combinations
    of representations that correspond to our output classes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然穷举地编码明确的规则会让任何程序员发疯，但当我们看着前面的图像时，我们的大脑直觉地注意到数据中的一些模式。例如，它注意到**2**和**3**的顶部都有一个半圆圈，**1**、**4**和**7**都有一条向下的直线。它还感知到**4**实际上由一条向下的直线、一条半向下的线和另一条水平线组成。由于这个原因，我们能够轻松地将一个复杂的模式分解成更小的模式。这在手写数字上特别容易做到，正如我们刚才看到的那样。因此，我们的任务是看看如何构建一个深度神经网络，并希望每个神经元能够从我们的数据中捕捉简单的模式，例如线段，然后使用我们在前一层学到的简单模式，逐步构建更复杂的模式，并最终学习到与我们的输出类别相对应的准确表示组合。
- en: Practically speaking, the MNIST dataset has been studied for about two decades
    by many pioneers in the field of deep learning. We have gained a good wealth of
    knowledge out of this dataset, making it ideal for exploring concepts such as
    layer representations, regularization, and overfitting, among others. As soon
    as we understand how to train and test a neural network, we can repurpose it for
    more exciting tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际应用的角度来看，MNIST数据集已经被深度学习领域的许多先驱研究了近二十年。从这个数据集中，我们获得了大量的知识，这使得它成为探索诸如层次表示、正则化和过拟合等概念的理想数据集。一旦我们理解了如何训练和测试神经网络，我们就可以将其用于更具挑战性的任务。
- en: Feeding a neural network
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入神经网络
- en: 'Essentially, all of the data that enters and propagates through the network
    is represented by a mathematical structure known as a **tensor**. This applies
    to audio data, images, video, and any other data we can think of, to feed our
    data-hungry network. In mathematics ([https://en.wikipedia.org/wiki/Mathematics](https://en.wikipedia.org/wiki/Mathematics)),
    a tensor is defined as an abstract and arbitrary geometric ([https://en.wikipedia.org/wiki/Geometry](https://en.wikipedia.org/wiki/Geometry))
    entity that maps aggregations of vectors in a multi-linear ([https://en.wikipedia.org/wiki/Linear_map](https://en.wikipedia.org/wiki/Linear_map))
    manner to a resulting tensor. In fact, vectors and scalars are considered simpler
    forms of tensors. In Python, tensors are defined with three specific properties,
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，所有进入并通过网络传播的数据都由一种数学结构——**张量**表示。这适用于音频数据、图像、视频以及我们能想到的任何数据，以供我们贪婪的数据网络使用。在数学中（[https://en.wikipedia.org/wiki/Mathematics](https://en.wikipedia.org/wiki/Mathematics)），张量被定义为一种抽象和任意的几何（[https://en.wikipedia.org/wiki/Geometry](https://en.wikipedia.org/wiki/Geometry)）实体，它以多线性（[https://en.wikipedia.org/wiki/Linear_map](https://en.wikipedia.org/wiki/Linear_map)）方式映射向量的聚合，得到一个结果张量。事实上，向量和标量被视为张量的简单形式。在Python中，张量定义有三个特定的属性，如下：
- en: '**Rank**: Specifically, this denotes the number axes. A matrix is said to have
    the rank 2, as it represents a two-dimensional tensor. In Python libraries, this
    is often indicated as `ndim`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**：具体来说，这表示轴的数量。一个矩阵的秩为2，因为它表示一个二维张量。在Python库中，通常用`ndim`表示这一点。'
- en: '**Shape**: The shape of a tensor can be checked by calling the shape property
    on a NumPy *n*-dimensional array (which is how a tensor is represented in Python).
    This will return a tuple of integers, indicating the number of dimensions a tensor
    has along each axis.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状**：张量的形状可以通过调用NumPy *n*维数组（在Python中张量的表示方式）上的shape属性来检查。这将返回一个整数元组，表示张量在每个轴上的维度数量。'
- en: '**Content**: This refers to the type of data that''s stored in the tensor,
    and can be checked by calling the `type()` method on a tensor of interest. This
    will return data types such as float32, uint8, float64, and so on, except for
    string values, which are first converted into vector representations before being
    represented as a tensor.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容**：这指的是存储在张量中的数据类型，可以通过对感兴趣的张量调用`type()`方法来检查。这将返回诸如float32、uint8、float64等数据类型，字符串值除外，字符串会先转换成向量表示，再以张量形式表示。'
- en: 'The following is a tensor graph. Don''t worry about the complex diagram—we
    will look at what it means later:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个张量图。不要担心复杂的图表——我们稍后会解释它的含义：
- en: '![](img/6e3eaf25-2ec1-4b81-b958-4b51b38399a4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e3eaf25-2ec1-4b81-b958-4b51b38399a4.png)'
- en: Examples of tensors
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量示例
- en: 'The illustration we previously saw was that of a three dimensional tensor,
    yet tensors can appear in many forms. In the following section, we will overview
    some tensors of different ranks, starting with a tensor of rank zero:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的插图是一个三维张量的示例，但张量可以以多种形式出现。在接下来的部分，我们将概述一些不同秩的张量，从零秩张量开始：
- en: '**Scalar**: Values simply denote a single numeric value on its own. This can
    also be described as a tensor of dimension 0\. An example of this is processing
    a single grayscale pixel of an image through a network.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**：标量表示单一的数值。它也可以被描述为一个维度为0的张量。一个例子是通过网络处理单一的灰度像素。'
- en: '**Vector**: A bunch of scalars or an array of numbers is called a **vector**,
    or a tensor of rank 1\. A 1D tensor is said to have exactly one axis. An example
    of this is processing a single flattened image.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：一堆标量或一组数字被称为**向量**，或者是一个秩为1的张量。一个一维张量被认为有一个轴。一个例子是处理单一的平展图像。'
- en: '**Matrix**: An array of vectors is a matrix, or 2D tensor. A matrix has two
    axes (often referred to as rows and columns). You can visually interpret a matrix
    as a rectangular grid of numbers. An example of this is processing a single grayscale
    image.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：向量数组是一个矩阵，或者称为2D张量。矩阵有两个轴（通常称为行和列）。你可以将矩阵形象地解释为一个矩形的数字网格。一个例子是处理单一的灰度图像。'
- en: '**Three-dimensional tensor**: By packing several matrices into a new array,
    you get a 3D tensor, which is visually interpretable as a cube of numbers. An
    example of this is processing a dataset of grayscale images.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**三维张量**：通过将多个矩阵打包到一个新数组中，你得到一个3D张量，可以将其形象地解释为一个数字立方体。一个例子是处理一组灰度图像数据集。'
- en: '**Four-dimensional tensor**: By packing 3D tensors in an array, you can create
    a 4D tensor, and so on. An example of this is processing a dataset of colored
    images.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**四维张量**：通过将3D张量打包到一个数组中，可以创建一个4D张量，依此类推。一个例子是处理彩色图像的数据集。'
- en: '**Five-dimensional tensor**: These are created by packing 4D tensors in an
    array. An example of this is processing a dataset of videos.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**五维张量**：这些是通过将4D张量打包到一个数组中创建的。一个例子是处理视频数据集。'
- en: Dimensionality of data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的维度
- en: So, consider the tensor of a shape (400, 600, 3). This is a common input shape
    that refers to a three-dimensional tensor that's used to represent a color image
    of 400 x 600 pixels. Since the MNIST dataset uses binary grayscale pixel values,
    we only deal with matrices of 28 x 28 pixels when representing an image. Here,
    each image is a tensor of dimension two, and the whole dataset can be represented
    by a tensor of dimension three. In a color image, each pixel value actually has
    three numbers, representing the amount of red, green, and blue light intensity
    represented by that pixel. Hence, with colored images, the two-dimensional matrices
    that are used to represent an image now scale up to three-dimensional tensors.
    Such a tensor is denoted by a tuple of (*x*, *y*, 3), where *x* and *y* represent
    the pixel dimensions of the image. Hence, a dataset of color images can be represented
    by a four-dimensional tensor, as we will see in later examples. For now, it is
    useful to know that we can use NumPy *n*-dimensional arrays to represent, reshape,
    manipulate, and store tensors in Python.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，考虑一个形状为(400, 600, 3)的张量。这是一个常见的输入形状，表示一个400 x 600像素的彩色图像的三维张量。由于MNIST数据集使用的是二进制灰度像素值，我们在表示图像时只处理28
    x 28像素的矩阵。在这里，每张图像是一个二维张量，整个数据集可以用一个三维张量表示。在彩色图像中，每个像素值实际上有三个数字，分别表示该像素的红、绿、蓝光强度。因此，在彩色图像中，用于表示图像的二维矩阵现在扩展为三维张量。这样的张量用(*x*,
    *y*, 3)的元组表示，其中*x*和*y*代表图像的像素维度。因此，彩色图像的数据集可以用一个四维张量表示，正如我们在后续示例中看到的那样。现在，了解我们可以使用NumPy的*n*维数组在Python中表示、重塑、操作和存储张量是很有用的。
- en: Making some imports
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入一些库
- en: 'So, let''s get started, shall we? We will perform some simple experiments by
    leveraging all the concepts we have learned about in the previous chapters, and
    perhaps also encounter some new ones while on the job. We will use Keras, as well
    as the TensorFlow API, allowing us to also explore the eager execution paradigm.
    Our first task will be to implement a simple version of the multi-layered perceptron.
    This version is known as the **feedforward neural network**, and is a basic architecture
    that we can use to further explore some simple image classification examples.
    Obeying customary deep learning tradition, we will begin our first classification
    task by using the MNIST dataset for handwritten digits. This dataset has 70,000
    grayscale images of digits between 0 and 9\. The large size of this dataset is
    ideal, as machines require about 5,000 images per class to be able to come close
    to human-level performance at visual recognition tasks. The following code imports
    the libraries we will be using:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！我们将通过利用前几章中学习的所有概念，进行一些简单的实验，或许在这个过程中也会遇到一些新的概念。我们将使用Keras，以及TensorFlow
    API，这也让我们可以探索即时执行模式。我们的第一个任务是实现一个简单版本的多层感知器。这个版本被称为**前馈神经网络**，它是一种基本的架构，我们可以用它来进一步探索一些简单的图像分类示例。遵循深度学习的传统，我们将通过使用MNIST数据集进行手写数字分类任务来开始我们的第一次分类任务。这个数据集包含70,000张0到9之间的灰度数字图像。这个数据集的规模非常适合，因为机器通常需要每个类别约5,000张图像，才能在视觉识别任务中接近人类水平的表现。以下代码导入了我们将使用的库：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keras's sequential API
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras的顺序API
- en: 'As you may well know, each Python library often comes with a core data abstraction
    that defines the data structure that the library is able to manipulate to perform
    computations. NumPy has its arrays, while pandas has its DataFrames. The core
    data structure of Keras is a model, which is essentially a manner to organize
    layers of interconnected neurons. We will start with the simplest type of model:
    the sequential model ([https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)).
    This is available as a linear stack of layers through the sequential API. More
    complex architectures also allow us to review the functional API, which is used
    to build custom layers. We will cover these later. The following code imports
    the sequential model, as well as some of the layers we will be using to build
    our first network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，每个Python库通常都有一个核心数据抽象，定义了该库能够操作的数据结构，以执行计算。NumPy有它的数组，而pandas有它的DataFrame。Keras的核心数据结构是模型，实际上它是一种组织相互连接的神经元层的方式。我们将从最简单的模型类型开始：顺序模型（[https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)）。它作为一个线性堆叠的层通过顺序API提供。更复杂的架构也允许我们查看功能API，它用于构建自定义层。稍后我们会讲解这些。以下代码导入了顺序模型，以及一些我们将用来构建第一个网络的层：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading the data
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'Now, let''s load in the data and split it up. Thankfully, MNIST is one of the
    core datasets that''s already implemented in Keras, allowing a nice one-liner
    import, which also lets us split up our data in training and test sets. Of course,
    real-world data is not that easy to port and split up. A lot of useful tools exist
    for this purpose in `Keras.utils`, which we will cover briefly later, but also
    encourage you to explore. Alternatively, other **ML** libraries such as scikit-learn
    come with some handy tools (such as `train_test_split`, `MinMaxScaler`, and `normalizer`,
    to name a few methods), which, as their names indicate, let you split up, scale,
    and normalize your data as often required to optimize neural network training.
    Let''s import and load the datasets, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载数据并进行拆分。幸运的是，MNIST是Keras中已经实现的核心数据集之一，允许通过简洁的一行代码导入，并且还可以让我们将数据拆分为训练集和测试集。当然，现实世界中的数据没有那么容易移植和拆分。为此目的，有很多有用的工具存在于`Keras.utils`中，我们稍后会简要介绍，也鼓励你自己探索。此外，其他**ML**库（如scikit-learn）也提供了一些方便的工具（如`train_test_split`、`MinMaxScaler`和`normalizer`等方法），这些工具顾名思义，能够帮助你根据需要拆分、缩放和归一化数据，以优化神经网络的训练。让我们导入并加载数据集，如下所示：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Checking the dimensions
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查维度
- en: 'Next, we need to check out what our data looks like. We will do this by checking
    its type, then shape, and finally by plotting our individual observations using
    `matplotlib.pyplot`, like so:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要查看我们的数据是什么样子的。我们将通过检查其类型、形状，然后使用`matplotlib.pyplot`绘制单独的观察结果来实现这一点，如下所示：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will get the following result:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下结果：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Plotting the points:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制点：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will plot a figure similar to what''s shown in the following screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将绘制出类似于下图的图形：
- en: '![](img/dc711508-0aac-4fb2-8243-f1056e15fef0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc711508-0aac-4fb2-8243-f1056e15fef0.png)'
- en: 'As we can see, our training set has 60,000 images, with each image represented
    by a 28 x 28 matrix. When we represent our whole dataset, we are just representing
    a tensor of three dimensions (60,000 x 28 x 28). Now, let''s rescale our pixel
    values, which usually lie between 0 and 225\. Rescaling these values to values
    between 0 and 1 makes it a lot easier for our network to perform computations
    and learn predictive features. We encourage you to carry out experiments with
    and without normalization so that you can assess the difference in predictive
    power:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的训练集有60,000张图片，每张图片由一个28 x 28的矩阵表示。当我们表示整个数据集时，实际上是表示一个三维张量（60,000 x
    28 x 28）。现在，让我们重新缩放像素值，这些值通常在0到225之间。将这些值缩放到0到1之间，可以让我们的网络更容易进行计算和学习预测特征。我们建议你进行带有和不带有归一化的实验，这样你就可以评估预测能力的差异：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code generates the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会生成以下输出：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following plot is acquired:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是获得的图形：
- en: '![](img/369416be-0620-432c-8a0f-b9f1e2d10161.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/369416be-0620-432c-8a0f-b9f1e2d10161.png)'
- en: Building a model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: Now we can move on and build our predictive model. But before jumping into the
    interesting code, we must know the theory that surrounds a few important things.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续构建我们的预测模型。但在进入有趣的代码之前，我们必须了解一些重要概念的理论。
- en: Introducting Keras layers
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入Keras层
- en: 'The core building blocks of a neural network model in Keras is its layers.
    Layers are basically data-processing filters that *contort* the data they are
    fed into more useful representations. As we will see, prominent architectures
    of neural networks mostly vary in the manner in which layers are designed, and
    the interconnection of neurons among them. The inventor of Keras, Francois Chollet,
    describes this architecture as performing a *progressive distillation* on our
    data. Let''s see how this works:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型在Keras中的核心构建模块是其层。层基本上是数据处理过滤器，它们*扭曲*它们接收到的数据，将其转换为更有用的表示。正如我们将看到的，神经网络的主要架构通常在于层的设计方式和它们之间神经元的相互连接。Keras的发明者Francois
    Chollet将这种架构描述为对我们的数据进行*渐进蒸馏*。让我们看看这是如何工作的：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We define our model by initializing an instance of a blank model with no layers.
    Then, we add our first layer, which always expects an input dimension corresponding
    to the size of the data you want it to ingest. In our case, we want the model
    to ingest sets of 28 x 28 pixels, as we defined previously. The extra comma we
    added refers to how many examples the network will see at a time, as we will soon
    see. We also call the `Flatten()` method on our input matrix. All this does is
    convert each 28 x 28 image matrix into a single vector of 784-pixel values, each
    corresponding to its own input neuron.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过初始化一个空的模型实例来定义我们的模型，这个实例没有任何层。然后，我们添加第一层，这一层总是期望一个输入维度，对应于我们希望其接收的数据大小。在我们的例子中，我们希望模型接收28
    x 28像素的图像数据，正如我们之前定义的那样。我们添加的额外逗号表示网络一次将看到多少个样本，正如我们很快会看到的那样。我们还在输入矩阵上调用了`Flatten()`方法。这样做的作用是将每个28
    x 28的图像矩阵转换为一个由784个像素值组成的单一向量，每个像素值对应于一个输入神经元。
- en: We continue adding the layers until we get to our output layer, which has a
    number of output neurons corresponding to the number of our output classes—in
    this case, the 10 digits between 0 and 9\. Do note that only the input layer needs
    to specify an input dimension of data entering it, as the progressive hidden layers
    are able to perform automatic shape inference (and only the first, because the
    following layers can do automatic shape inference).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续添加层，直到到达输出层，该层具有与输出类别数量对应的输出神经元——在这种情况下，是介于0和9之间的10个数字。请注意，只有输入层需要指定输入数据的维度，因为逐步的隐藏层能够执行自动形状推断（并且仅是第一个层需要，后续层可以自动推断形状）。
- en: Initializing weights
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化权重
- en: We also have the option to initialize the neurons on each layer with specific
    weights. This is not a prerequisite, as they will be automatically initialized
    with small random numbers if not specified otherwise. Weight initialization practices
    are actually a whole separate sub-field of study in neural networks. It is prominently
    noted that the careful initialization of the network can significantly speed up
    the learning process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择为每一层的神经元初始化特定的权重。这不是一个前提条件，因为如果没有特别指定，它们会被自动初始化为小的随机数。权重初始化的实践实际上是神经网络中的一个独立子领域。需要特别注意的是，网络的谨慎初始化可以显著加速学习过程。
- en: 'You can use the `kernel_initializer` and `bais_initializer` parameters to set
    both the weights and biases of each layer, respectively. Remember that these very
    weights will represent the knowledge that''s acquired by our network, which is
    why ideal initializations can significantly boost its learning:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kernel_initializer`和`bias_initializer`参数分别设置每一层的权重和偏置。记住，这些权重将代表我们网络所获得的知识，这就是为什么理想的初始化可以显著提升学习效率：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A comprehensive review of the different parameter values is beyond the scope
    of this chapter. We may encounter some use cases where tweaking these parameters
    is beneficial later on (refer to chapter optimization). Some values for the `kernel_initializer`
    parameter include the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对不同参数值的全面审查超出了本章的范围。我们以后可能会遇到一些需要调整这些参数的用例（请参阅优化章节）。`kernel_initializer`参数的一些值包括：
- en: '`glorot_uniform`: The weights are drawn from samples of uniform distributions
    between `-limit` and `limit`. Here, the term `limit` is defined as `sqrt(6 / (fan_in
    + fan_out))`. The term `fan_in` simply denotes the number of input units in the
    weight tensor, while `fan_out` is the number of output units in the weight tensor.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glorot_uniform`：权重是从`-limit`和`limit`之间的均匀分布样本中提取的。这里，`limit`定义为`sqrt(6 / (fan_in
    + fan_out))`。术语`fan_in`表示权重张量中的输入单元数量，而`fan_out`表示权重张量中的输出单元数量。'
- en: '`random_uniform`: The weights are randomly initialized with small uniform values
    ranging between -0.05 and 0.05.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_uniform`: 权重被随机初始化为-0.05到0.05之间的小的均匀值。'
- en: '`random_normal`: The weights are initialized for obeying a Gaussian distribution[1],
    with a mean of 0 and a standard deviation of 0.05.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_normal`: 权重按高斯分布初始化[1]，均值为0，标准差为0.05。'
- en: '`zero`: The layer weights are initialized at zero.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero`: 层的权重初始化为零。'
- en: Keras activations
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 激活函数
- en: 'At the moment, our network is composed of a flattened input layer, followed
    by a sequence of two dense layers, which are fully connected layers of neurons.
    The first two layers employ a **Rectified Linear Unit** (**ReLU**) activation
    function, which plots out a bit differently than the sigmoid we saw in [Chapter
    2](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml), *A* *Deeper Dive into Neural Networks*.
    In the following diagram, you can see how some of the different activation functions
    that are provided by Keras plot out. Remember, picking between them requires an
    intuitive understanding of the possible decision boundaries that may help with
    or hinder the partitioning your feature space. Using the appropriate activation
    function in conjunction with ideally initialized biases can be of paramount importance
    in some scenarios, but trivial in others. It is always advisable to experiment,
    leaving no stone unturned:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的网络由一个扁平化的输入层组成，接着是两层全连接的密集层，这些都是神经元的完全连接层。前两层使用**修正线性单元**（**ReLU**）激活函数，其图形绘制方式与我们在[第2章](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml)《深入神经网络》一章中看到的Sigmoid函数有所不同。在以下的图示中，你可以看到Keras提供的一些不同激活函数的绘制方式。记住，在它们之间进行选择需要直观理解可能的决策边界，这些边界可能有助于或妨碍你的特征空间划分。某些情况下，使用合适的激活函数并与理想初始化的偏置一起使用可能至关重要，但在其他情况下则可能无关紧要。总之，建议进行实验，尽可能不留任何未尝试的方案：
- en: '![](img/17d76c00-abee-4ac8-96d2-e2378f0f609b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17d76c00-abee-4ac8-96d2-e2378f0f609b.png)'
- en: The fourth (and last) layer in our model is a 10-way Softmax layer. In our case,
    this means it will return an array of ten probability scores, all of which will
    add up to 1\. Each score will be the probability that the current digit image
    belongs to one of our output classes. Hence, for any given input, a layer with
    the Softmax activation computes and returns the class probability of that input,
    with respect to each of our output classes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中的第四层（也是最后一层）是一个10类Softmax层。在我们的例子中，这意味着它将返回一个包含十个概率值的数组，所有这些值的总和为1。每个概率值表示当前数字图像属于我们输出类别之一的概率。因此，对于任何给定的输入，Softmax激活函数层会计算并返回该输入相对于每个输出类别的类别概率。
- en: Summarizing your model visually
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以视觉方式总结模型
- en: 'Going back to our model, let''s summarize the output of what we are about to
    train. You can do this in Keras by using the `summary()` method on the model,
    which is actually a shortcut for the longer `utility` function (and hence harder
    to remember), which is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的模型，让我们总结一下我们即将训练的输出。在Keras中，你可以通过在模型上使用`summary()`方法来做到这一点，这实际上是一个更长的`utility`函数的快捷方式（因此更难记住），其代码如下：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using this, you can actually visualize the shapes of the individual layers
    of the neural network, as well as the parameters in each layer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个，你实际上可以可视化神经网络各个层的形状，以及每一层的参数：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code generates the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, contrary to the perceptron we saw in [Chapter 2](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml),
    *A* *Deeper Dive into Neural Networks*, this extremely simple model already has
    51, 600 trainable parameters that are capable of scaling its learning almost exponentially
    compared to its ancestor.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与我们在[第2章](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml)《深入神经网络》一章中看到的感知机不同，这个极其简单的模型已经有了51,600个可训练的参数，相比其前身，几乎可以以指数级的速度扩展其学习能力。
- en: Compiling the model
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'Next, we will compile our Keras model. Compilation basically refers to the
    manner in which your neural network will learn. It lets you have hands-on control
    of implementing the learning process, which is done by using the `compile` method
    that''s called on our `model` object. The method takes at least three arguments:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译我们的Keras模型。编译基本上指的是神经网络的学习方式。它让你亲自控制实现学习过程，这通过调用`model`对象的`compile`方法来完成。该方法至少需要三个参数：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, we describe the following functions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们描述以下函数：
- en: '**A** `loss` **function**: This simply measures our performance on the training
    data, compared to the true output labels. Due to this, the `loss` function can
    be used as an indication of our model''s errors. As we saw earlier, this metric
    is actually a function that determines how far our model''s predictions are from
    the actual labels of the output classes. We saw the **Mean Squared Error** (**MSE**)
    `loss` function in [Chapter 2](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml), *A*
    *Deeper Dive into Neural Networks*, of which many variations exist. These `loss`
    functions are implemented in Keras, depending on the nature of our **ML** task.
    For example, if you wish to perform a binary classification (two output neurons
    representing two output classes), you are better off choosing binary cross-entropy.
    For more than two categories, you may try categorical cross-entropy, or sparse
    categorical cross-entropy. The former is used when your output labels are one-hot
    encoded, whereas the latter is used when your output classes are numerical categorical
    variables. For regression problems, we often advise the MSE `loss` function. When
    dealing with sequence data, as we will later, then **Connectionist Temporal Classification**
    (**CTC**) is deemed a more appropriate type of `loss` function. Other flavors
    of loss may differ in the manner they measure the distance between predictions
    and actual output labels (for example, `cosine_proximity` uses a cosine measure
    of distance), or the choice of probability distribution to model the predicted
    values (for example, the **Poisson loss function** is perhaps better if you are
    dealing with count data).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** `loss` **function**：这只是用来衡量我们在训练数据上的表现，与真实输出标签进行比较。因此，`loss` 函数可以作为我们模型错误的指示。如我们之前所见，这个度量实际上是一个函数，用来确定我们的模型预测与实际输出类标签之间的差距。我们在[第2章](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml)，*深入探讨神经网络*中看到了**均方误差**（**MSE**）`loss`
    函数，存在许多不同的变体。这些 `loss` 函数在 Keras 中被实现，具体取决于我们的**ML**任务的性质。例如，如果你希望执行二分类（两个输出神经元代表两个输出类别），你最好选择二元交叉熵。对于两个以上的类别，你可以尝试分类交叉熵或稀疏分类交叉熵。前者用于你的输出标签是独热编码的情况，而后者则用于你的输出类是数值型类别变量的情况。对于回归问题，我们通常建议使用
    MSE `loss` 函数。处理序列数据时，正如我们稍后将讨论的那样，**连接时序分类**（**CTC**）被认为是更合适的`loss`函数。其他类型的 `loss`
    可能在衡量预测与实际输出标签之间的距离方式上有所不同（例如，`cosine_proximity` 使用余弦距离度量），或者选择不同的概率分布来建模预测值（例如，**泊松损失函数**，如果你处理的是计数数据，可能更合适）。'
- en: '**An** `optimizer`: An intuitive way to think of an optimizer is that it tells
    the network how to get to a global minimum loss. This includes the goal you want
    to optimize, as well as the size of the step it will take in the direction of
    your goal. Technically, the optimizer is often described as the mechanism that''s
    employed by the network to self-update, which is does by using the data it is
    fed and the `loss` function. Optimization algorithms are used to update weights
    and biases that are the internal parameters of a model in the process of error
    reduction. There are actually two distinct types of optimization functions: functions
    with constant learning rates (such as **Stochastic Gradient Decent** (**SGD**))
    and functions with adaptive learning rates (such as Adagrad, Adadelta, RMSprop,
    and Adam). The latter of the two are known for implementing heuristic-based and
    pre-parameterized learning rate methods. Consequentially, using adaptive learning
    rates can lead to less work in tuning the hyperparameters of your model.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**An** `optimizer`：直观地看，优化器可以理解为告诉网络如何达到全局最小损失。这包括你希望优化的目标，以及它将在朝着目标的方向上采取的步长。技术上讲，优化器通常被描述为网络用于自我更新的机制，网络通过使用它所接收的数据和
    `loss` 函数来进行自我更新。优化算法用于更新权重和偏差，这些是模型的内部参数，用于在误差减少过程中进行调整。实际上，优化函数有两种不同的类型：具有恒定学习率的函数（如**随机梯度下降**（**SGD**））和具有自适应学习率的函数（如
    Adagrad、Adadelta、RMSprop 和 Adam）。后者因实现基于启发式和预设学习率方法而闻名。因此，使用自适应学习率可以减少调整模型超参数的工作量。'
- en: '`metrics`: This simply denotes the evaluation benchmark we monitor during training
    and testing. Accuracy is most commonly used, but you may design and implement
    a custom metric through Keras, if you so choose. The main functional difference
    between the loss and accuracy score that''s shown by the metric is that the accuracy
    measure is not involved in the training process at all, whereas loss is used directly
    in the training process by our optimizer to backpropagate the errors.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics`：这仅表示我们在训练和测试期间监控的评估基准。最常用的是准确度，但如果您愿意，您也可以通过 Keras 设计并实现自定义度量。损失和准确度评分之间的主要功能差异在于，准确度度量完全不参与训练过程，而损失则直接在训练过程中由优化器用来反向传播误差。'
- en: Fitting the model
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'The `fit` parameter initiates the training session, and hence should be thought
    of as synonymous to training our model. It takes your training features, their
    corresponding training labels, the number of times the model sees your data, and
    the number of learning examples your model sees per training iteration as training
    measures, respectively:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit` 参数启动训练过程，因此它可以被认为是训练模型的同义词。它接受您的训练特征、对应的训练标签、模型查看数据的次数，以及每次训练迭代中模型看到的学习示例数量，作为训练度量标准：'
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also have additional arguments to shuffle your data, create validation
    splits, or give custom weights to output classes. Shuffling training data before
    each epoch can be useful, especially to ensure that your model does not learn
    any random non-predictive sequences in our data, and hence simply overfit the
    training set. To shuffle your data, you have to set the Boolean value of the shuffle
    argument to **True**. Finally, custom weights can be particularly useful if you
    have underrepresented classes in your dataset. Setting a higher weight is equivalent
    to telling your model, *Hey, you, pay more attention to these examples here*.
    To set custom weights, you have to provide the `class_weight` argument with a
    dictionary that maps class indices to custom weights corresponding to your output
    classes, in order of the indices that are provided.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以添加额外的参数来打乱数据、创建验证集划分或为输出类别分配自定义权重。在每个训练周期前打乱训练数据很有用，特别是可以确保模型不会学习到数据中的任何随机非预测性序列，从而仅仅过拟合训练集。要打乱数据，您必须将
    `shuffle` 参数的布尔值设置为 **True**。最后，自定义权重对于数据集中类别分布不均的情况特别有用。设置较高的权重相当于告诉模型，*嘿，你，更多关注这些示例*。要设置自定义权重，您必须提供
    `class_weight` 参数，传递一个字典，将类别索引映射到与输出类别对应的自定义权重，按照提供的索引顺序。
- en: 'The following is an overview of the key architectural decisions you will face
    when compiling a model. These decisions relate to the training process you instruct
    your model to undergo:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是编译模型时会面临的关键架构决策概览。这些决策与您指示模型执行的训练过程相关：
- en: '`epochs`: This argument must be defined as an integer value, corresponding
    to the number of times your model will iterate through the entire dataset. Technically,
    the model is not trained for a number of iterations given by epochs, but merely
    until the epoch of index epochs is reached. You want to set this parameter *just
    right*, depending on the nature of complexity you want your model to represent.
    Setting it too low will lead to simplistic representations that are used for inference,
    whereas setting it too high will make your model overfit on your training data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：该参数必须定义为整数值，对应模型将遍历整个数据集的次数。从技术上讲，模型并不会根据 `epochs` 给出的迭代次数进行训练，而仅仅是直到达到
    `epochs` 索引的周期为止。您希望将此参数设置为 *恰到好处*，具体取决于您希望模型表现的复杂性。如果设置过低，将导致用于推理的简化表示，而设置过高则会导致模型在训练数据上过拟合。'
- en: '`batch_size`: The `batch_size` defines the number of samples that will be propagated
    through the network per training iteration. Intuitively, this can be thought of
    as the number of examples the network sees at a time while learning. Mathematically,
    this is simply the number of training instances the network will see before updating
    the model weights. So far, we have been updating our model weights at each training
    example (with a `batch_size` of 1), but this can quickly become a computational
    and memory management burden. This becomes especially cumbersome in instances
    where your dataset is too big to even load into memory. Setting a `batch_size`
    helps prevent this. Neural networks also train faster in mini-batches. In fact,
    batch size even has an impact on the accuracy of our gradient estimate during
    the backpropagation process, as shown in the following diagram. The same network
    is trained using three different batch sizes. Stochastic denotes random, or a
    batch size of one. As you can see, the direction of the stochastic and mini-batch
    gradients (green) fluctuates much more in comparison to the steady direction of
    the larger full-batch gradient (blue):'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：`batch_size` 定义了每次训练迭代中，通过网络传播的样本数量。从直观上讲，这可以被看作是网络在学习时一次性看到的示例数量。在数学上，这只是网络在更新模型权重之前将看到的训练实例的数量。到目前为止，我们一直是在每个训练样本后更新模型权重（即
    `batch_size` 为 1），但这种做法很快会成为计算和内存管理的负担。当数据集过大，甚至无法加载到内存中时，尤其如此。设置 `batch_size`
    可以防止这种情况。神经网络在小批量上训练得也更快。事实上，批量大小甚至会影响我们在反向传播过程中梯度估计的准确性，正如下图所示。同一网络使用三种不同的批量大小进行训练。随机表示随机梯度，或批量大小为1。如你所见，相比于较大完整批量梯度（蓝色），随机和小批量梯度（绿色）的方向波动要大得多：'
- en: '![](img/5ba338bd-2549-4d34-9bc5-a36c8b994344.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ba338bd-2549-4d34-9bc5-a36c8b994344.png)'
- en: The **number of** **iterations** (which don't need to be explicitly defined)
    simply denotes the number of passes, where each pass contains the number of training
    examples denoted by the `batch_size`. To be clear, by one pass, we mean a forward
    filtering of data through our layers, as well as the backpropagation of errors.
    Suppose that we set our batch size to 32\. One iteration encompasses our model
    by viewing 32 training examples, then updating its weights accordingly. In a dataset
    of 64 examples with a batch size of 32, it will take only two iterations for your
    model to cycle through it.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代次数**（无需显式定义）仅表示通过的次数，每次通过包含由 `batch_size` 定义的训练示例数量。明确来说，一次通过是指数据通过我们的各层进行正向过滤，同时进行反向传播误差。假设我们将批量大小设置为
    32。一次迭代包含模型查看 32 个训练示例，然后相应地更新其权重。在一个包含 64 个示例的数据集中，如果批量大小为 32，模型需要进行两次迭代才能遍历完所有数据。'
- en: 'Now that we have called the `fit` method on our training samples to initiate
    the learning process, we will observe the output, which simply displays the estimated
    training time, loss (in errors), and accuracy per epoch on our training data:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经调用了 `fit` 方法来初始化学习过程，我们将观察输出，输出显示每个 epoch 的估计训练时间、损失（错误）和训练数据的准确性：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In only five full runs through our data, we achieve an accuracy of 0.96 (96.01%)
    during training. Now, we must verify whether our model is truly learning what
    we want it to learn by testing it on our secluded test set, which our model hasn''t
    seen so far:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集完整训练五次后，我们在训练过程中达到了 0.96（96.01%）的准确率。现在，我们必须通过在模型之前未见过的隔离测试集上进行测试，来验证我们的模型是否真的学到了我们想要它学习的内容：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Evaluating model performance
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: Whenever we evaluate a network, we are actually interested in our accuracy of
    classifying images in the test set. This remains true for any ML model, as our
    accuracy on the training set is not a reliable indicator of our model's generalizability.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们评估一个网络时，我们实际上关心的是它在测试集上分类图像的准确性。这对于任何机器学习模型都是适用的，因为在训练集上的准确率并不能可靠地反映我们模型的泛化能力。
- en: In our case, the test set accuracy is 95.78%, which is marginally lower than
    our training set accuracy of 96%. This is a classic case of overfitting, where
    our model seems to have captured irrelevant noise in our data to predict the training
    images. Since that inherent noise is different on our randomly selected test set,
    our network couldn't rely on the useless representations it had previously picked
    up on, and so performed poorly during testing. As we will see throughout this
    book, when testing neural networks, it is important to ensure that it has learnt
    correct and efficient representations of our data. In other words, we need to
    ensure that our network is not overfitting on our training data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，测试集的准确率是95.78%，略低于我们训练集的96%。这是典型的过拟合案例，我们的模型似乎捕捉到了数据中的无关噪声，用来预测训练图像。由于这种固有的噪声在我们随机选择的测试集上不同，因此我们的网络无法依赖之前所捕捉到的无用表示，因此在测试时表现不佳。正如我们将在本书中看到的那样，测试神经网络时，确保它已经学习到正确且高效的数据表示是非常重要的。换句话说，我们需要确保我们的网络没有在训练数据上过拟合。
- en: 'By the way, you can always visualize your predictions on the test set by printing
    out the label with the highest probability value for the given test subject and
    plotting the said test subject using Matplotlib. Here, we are printing out the
    label with maximum probability for test subject `110`. Our model thinks it is
    an `8`. By plotting the subject, we see that our model is right in this case:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，你可以通过打印出给定测试对象的最高概率值的标签，并使用Matplotlib绘制该测试对象，来始终可视化你的预测结果。在这里，我们打印出了测试对象`110`的最大概率标签。我们的模型认为它是一个`8`。通过绘制该对象，我们可以看到我们的模型在这个案例中是正确的：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code generates the following output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/2ae3e1bf-72b5-43c4-8cf9-f8680081245d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ae3e1bf-72b5-43c4-8cf9-f8680081245d.png)'
- en: 'Once satisfied, you can save and load your model for later use, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦满意，你可以保存并加载模型以备后用，如下所示：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Regularization
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: So, what can you do to prevent a model from learning misleading or irrelevant
    patterns from the training data? Well, with neural networks, the best solution
    is to almost always get more training data. A model that's trained on more data
    will indeed allow your model to have better out-of-set predictivity. Of course,
    getting more data is not always that simple, or even possible. When this is the
    case, you have several other techniques at your disposal to achieve similar effects.
    One of them is to constrain your model in terms of the amount of information that
    it may store. As we saw in the *behind enemy lines* example in [Chapter 1](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml),
    *Overview of Neural Networks*, it is useful to find the most efficient representations
    of information, or representations with the lowest entropy. Similarly, if we can
    only afford our model the ability to memorize a small number of patterns, we are
    actually forcing it to find the most efficient representations that generalize
    better on other data that our model may encounter later on. This process of improving
    model generalizability through reducing overfitting is known as **regularization**,
    and will be go over it in more detail before we use it in practice.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你可以做些什么来防止模型从训练数据中学习到误导性或无关的模式呢？对于神经网络来说，最好的解决方案几乎总是获取更多的训练数据。一个训练在更多数据上的模型，确实会让你的模型在外部数据集上具有更好的预测能力。当然，获取更多数据并不总是那么简单，甚至有时是不可能的。在这种情况下，你还有其他几种技术可以使用，以达到类似的效果。其一是约束你的模型在可存储信息的数量方面。正如我们在[第一章](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml)《神经网络概述》中看到的*敌后*例子，找到最有效的信息表示，或者具有最低熵的表示是非常有用的。类似地，如果我们只能让模型记住少量的模式，实际上是在强迫它找到最有效的表示，这些表示能更好地推广到我们模型未来可能遇到的其他数据上。通过减少过拟合来提高模型的泛化能力的过程被称为**正则化**，我们将在实际使用之前更详细地讲解它。
- en: Adjusting network size
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整网络大小
- en: 'When we speak of a network''s size, we simply mean the number of trainable
    parameters within the network. These parameters are defined by the number of layers
    in the network, as well as the number of neurons per each layer. Essentially,
    a network''s size is a measure of its complexity. We mentioned how having too
    large a network size can be counterproductive and lead to overfitting. An intuitive
    way to think about this is that we should favor simpler representations over complex
    ones, as long as they achieve the same ends—sort of a *lex parsimoniae*, if you
    will. The engineers who design such learning systems are indeed deep thinkers.
    The intuition here is that you could probably have various representations of
    your data, depending on your network''s depth and number of neurons per layer,
    but we will favor simpler configurations and only progressively scale a network
    if required, to prevent it from using any extra learning capacity to memorize
    randomness. However, letting our model have too few parameters may well cause
    it to underfit, leaving it oblivious to the underlying trends we are trying to
    capture in our data. Through experimentation, you can find a network size that
    fits just right, depending on your use case. We force our network to be efficient
    in representing our data, allowing it to generalize better out of our training
    data. Beneath, we show a few experiments that are performed while varying the
    size of the network. This lets us compare how our loss on the validation set differs
    per epoch. As we will see, larger models are quicker to diverge away from the
    minimum loss values, and they will start to overfit on our training data almost
    instantly:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论一个网络的规模时，我们指的是网络中可训练参数的数量。这些参数由网络中的层数以及每层的神经元数量决定。本质上，网络的规模是其复杂性的度量。我们曾提到，网络规模过大会适得其反，导致过拟合。直观地理解这一点，我们应该倾向于选择更简单的表示方式，而不是复杂的，只要它们能够实现相同的目标——可以说，这是一种*简约法则*。设计此类学习系统的工程师确实是深思熟虑的。这里的直觉是，你可以根据网络的深度和每层的神经元数量，采用多种数据表示方式，但我们将优先选择更简单的配置，只有在需要时才会逐步扩大网络规模，以防它利用过多的学习能力去记忆随机性。然而，让模型拥有过少的参数可能导致欠拟合，使其忽视我们试图在数据中捕捉的潜在趋势。通过实验，你可以找到一个适合的网络规模，具体取决于你的应用场景。我们迫使网络在表示数据时保持高效，使其能够更好地从训练数据中进行泛化。下面，我们展示了一些实验，过程中调整了网络的规模。这让我们可以比较在每个周期中验证集上的损失变化。如我们所见，较大的模型更快地偏离最小损失值，并几乎立刻开始在训练数据上发生过拟合：
- en: '![](img/ae411f1a-470f-40de-9077-037c56a5be17.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae411f1a-470f-40de-9077-037c56a5be17.png)'
- en: Size experiments
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络规模实验
- en: Now we will perform some short experiments by varying the size of our network
    and gauging our performance. We will train six simple neural networks on Keras,
    each progressively larger than the other, to observe how these separate networks
    learn to classify handwritten digits. We will also present some of the results
    from the experiments. All of these models were trained with a constant batch size
    (`batch_size=100`), the `adam` optimizer, and `sparse_categorical_crossentropy`
    as a `loss` function, for the purpose of this experiment.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将通过改变网络的规模并评估我们的表现，进行一些简短的实验。我们将在 Keras 上训练六个简单的神经网络，每个网络的规模都逐渐增大，以观察这些独立的网络如何学习分类手写数字。我们还将展示一些实验结果。所有这些模型都使用固定的批次大小（`batch_size=100`）、`adam`优化器和`sparse_categorical_crossentropy`作为`loss`函数进行训练，目的是为了本次实验。
- en: 'The following fitting graph shows how increasing our neural network''s complexity
    (in terms of size) impacts our performance on the training and test sets of our
    data. Note that we are always aiming for a model that minimizes the difference
    between training and test accuracy/loss, as this indicates the minimum amount
    of overfitting. Intuitively, this simply shows us how much our networks learning
    benefits if we allocate it more neurons. By observing the increase in accuracy
    on the test set, we can see that adding more neurons does help our network to
    better classify images that it has never encountered before. This can be noticed
    until the *sweet spot*, which is where the training and test values are the closest
    to each other. Eventually, however, increases in complexity will lead to diminishing
    returns. In our case, our model seems to overfit the least at a dropout rate around
    0.5, after which the accuracy of the training and test sets start to diverge:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下拟合图展示了增加我们神经网络的复杂度（从规模上来说）如何影响我们在训练集和测试集上的表现。请注意，我们的目标始终是寻找一个模型，使训练和测试准确度/损失之间的差异最小化，因为这表明过拟合的程度最小。直观来说，这只是向我们展示了如果分配更多神经元，我们的网络学习会有多大的好处。通过观察测试集上准确度的提升，我们可以看到，添加更多神经元确实有助于我们的网络更好地分类它从未遇到过的图像。直到*最佳点*，即训练和测试值最接近的地方，这一点可以被注意到。然而，最终，复杂度的增加将导致边际效益递减。在我们的案例中，模型似乎在丢弃率约为
    0.5 时最少发生过拟合，之后训练和测试集的准确度开始出现分歧：
- en: '![](img/b6d1ddb0-1ab8-46e3-936e-8690f653ea00.png)   ![](img/a153a0d8-4f37-47d0-bcfa-ef6e702f760d.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6d1ddb0-1ab8-46e3-936e-8690f653ea00.png)   ![](img/a153a0d8-4f37-47d0-bcfa-ef6e702f760d.png)'
- en: 'To replicate these results by increasing the size of our network, we can tweak
    both the breadth (number of neurons per layer) and the depth of the network (number
    of layers in network). Adding depth to your network is done in Keras by adding
    layers to your initialized model by using `model.add()`. The `add` method takes
    the type of layer (for example, `Dense()`), as an argument. The `Dense` function
    takes the number of neurons to be initialized in that specific layer, along with
    the activation function to be employed for said layer, as arguments. The following
    is an example of this:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过增加网络的规模来复制这些结果，我们可以调整网络的宽度（每层的神经元数量）和深度（网络的层数）。在 Keras 中，增加网络的深度是通过使用 `model.add()`
    向初始化的模型中添加层来完成的。`add` 方法的参数是层的类型（例如，`Dense()`）。`Dense` 函数需要指定该层中要初始化的神经元数量，以及为该层使用的激活函数。以下是一个例子：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Regularizing the weights
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化权重
- en: 'Another way to make sure that your network doesn''t pick up on irrelevant features
    is through regularizing the weights of our model. This simply allows us to put
    a constraint on the complexity of the network by limiting its layer weights to
    only take small values. All this does is make the distribution of layer weights
    more regular. How do we do this? By simply adding a cost to the `loss` function
    of our network. This cost actually represents a penalization for neurons that
    have larger weights. Conventionally, we implement this cost in three ways, namely
    L1, L2, and elastic net regularization:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种确保网络不会拾取无关特征的方法是通过正则化我们模型的权重。这使得我们能够通过限制层权重只取小值，来对网络的复杂度施加约束。这样做的结果就是使得层权重的分布更加规则。我们是怎么做到这一点的？通过简单地将成本添加到我们网络的`loss`函数中。这个成本实际上代表了对权重大于正常值的神经元的惩罚。传统上，我们通过三种方式来实现这种成本，分别是
    L1、L2 和弹性网正则化：
- en: '**L1 regularization**: We add a cost that is proportional to the absolute value
    of our weighted coefficients.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1 正则化**：我们添加一个与加权系数的绝对值成正比的成本。'
- en: '**L2 regularization**: We add a cost that is proportional to the square of
    the value of the weighted coefficients. This is also known as **weight decay**,
    as the weights exponentially decay to zero if no other update is scheduled.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 正则化**：我们添加一个与加权系数的平方成正比的成本。这也被称为**权重衰减**，因为如果没有其他更新计划，权重将会指数衰减为零。'
- en: '**Elastic net regularization**: This regularization method allows us to capture
    the complexity of our model by using a combination of both L1 and L2 regularization.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性网正则化**：这种正则化方法通过同时使用 L1 和 L2 正则化的组合，帮助我们捕获模型的复杂性。'
- en: Using dropout layers
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用丢弃层
- en: Finally, adding dropout neurons to layers is a technique that's widely used
    to regularize neural networks and prevent them from overfitting. Here, we, quite
    literally, drop out some neurons from our model at random. Why? Well, this results
    in a two-fold utility. Firstly, the contributions these neurons had for the activations
    of neurons further down our network are randomly ignored during a forward pass
    of data through our network. Also, any weight adjustments during the process of
    backpropagation are not applied to the neuron. While seemingly bizarre, there
    is good intuition behind this. Intuitively, neuron weights are adjusted at each
    backward pass to specialize a specific feature in your training data. But specialization
    breeds dependence. What often ends up happening is the surrounding neurons start
    relying on the specialization of a certain neuron in the vicinity, instead of
    doing some representational work themselves. This dependence pattern is often
    denoted as complex co-adaptation, a term that was coined by **Artificial Intelligence**
    (**AI**) researchers. One among them was Geoffrey Hinton, who was the original
    co-author of the backpropagation paper and is prominently referred to as the godfather
    of deep learning. Hinton playfully describes this behavior of complex coadaptation
    as *conspiracies* between neurons, stating that he was inspired by a fraud prevention
    system at his bank. This bank continuously rotated its employees so that whenever
    Hinton paid the bank a visit, he would always encounter a different person behind
    the desk.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将 dropout 神经元添加到层中是一种广泛应用的技术，用于正则化神经网络并防止过拟合。在这里，我们实际上是随机地从模型中丢弃一些神经元。为什么这么做？其实，这带来了双重效用。首先，这些神经元对网络中更深层神经元激活的贡献会在前向传播过程中被随机忽略。其次，在反向传播过程中，任何权重调整都不会应用到这些神经元。尽管这一做法看起来有些奇怪，但背后其实有合理的直觉。从直觉上讲，神经元的权重在每次反向传播时都会进行调整，以专门化处理训练数据中的特定特征。但这种专门化会带来依赖关系。最终的结果往往是，周围的神经元开始依赖于某个附近神经元的专门化，而不是自己进行一些表征性工作。这种依赖模式通常被称为复杂共适应（complex
    co-adaptation），这是**人工智能**（**AI**）研究人员创造的一个术语。其中之一就是 Geoffrey Hinton，他是反向传播论文的原始合著者，并被广泛称为深度学习的教父。Hinton
    幽默地将这种复杂共适应的行为描述为神经元之间的*阴谋*，并表示他受到银行防欺诈系统的启发。这个银行不断轮换员工，因此每当 Hinton 访问银行时，他总是会遇到不同的人在柜台后面。
- en: Thinking about dropout intuitively
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观理解 dropout
- en: For those of you who are familiar with Leonardo Dicaprio's movie *Catch me if
    you can*, you'll recall how Leonardo charmed the bank workers by asking them on
    dates and buying them treats, only so he could defraud the bank by cashing in
    his fake airline checks. In fact, due to the frequent fraternization of the employees
    and DiCaprio's character, the bank workers were paying more attention to irrelevant
    features such as DiCaprio's charm. What they should have actually been paying
    attention was the fact that DiCaprio was cashing out his monthly salary checks
    more than three times each month. Needless to say, businesses don't usually behave
    so generously. Dropping out some neurons is synonymous to rotating them to ensure
    that none of them get lazy and let a sleazy Leonardo defraud your network.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 Leonardo DiCaprio 的电影 *Catch me if you can*，你一定记得他是如何通过约会和请银行工作人员吃点心来迷住他们，结果却通过兑现伪造的航空公司支票来欺诈银行。事实上，由于员工们与
    DiCaprio 角色的频繁交往，他们开始更加关注一些无关的特征，比如 DiCaprio 的魅力。实际上，他们应该注意的是，DiCaprio 每月兑现工资支票的次数超过了三次。无需多说，商界通常不会如此宽松。丢弃一些神经元就像是轮换它们，确保没有任何神经元懒惰，并让一个滑头的
    Leonardo 欺诈你的网络。
- en: When we apply a dropout to a layer, we simply drop some of the outputs it would
    have otherwise given. Suppose a layer produces the vector [3, 5, 7, 8, 1] as an
    output for a given input. Adding a dropout rate of (0.4) to this layer would simply
    convert this output to [0, 5, 7, 0, 1]. All we did was initialize 40% of the scalars
    in our vector as zero.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向一层应用 dropout 时，我们只是丢弃了它本应输出的一部分结果。假设一层对给定输入的输出是向量 [3, 5, 7, 8, 1]。如果我们给这个层添加一个
    dropout 比例（0.4），则该输出将变为 [0, 5, 7, 0, 1]。我们所做的只是将向量中 40% 的标量初始化为零。
- en: Dropout only occurs during training. During testing, layers with dropouts have
    their outputs scaled down by the factor of the dropout rate that was previously
    used. This is actually done to adjust for the fact that more neurons are active
    during testing than training, as a result of the dropout mechanism.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 仅在训练过程中发生。在测试过程中，使用过 dropout 的层会将其输出按先前使用的 dropout 比例缩小。这实际上是为了调整测试时比训练时更多神经元激活的情况，因为
    dropout 机制的存在。
- en: Implementing weight regularization in Keras
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中实现权重正则化
- en: So far, we have visited the theories behind three specific ways that allow us
    to improve our model's generalizability on unseen data. Primarily, we can vary
    our network size to ensure it has no extra learning capacity. We can also penalize
    inefficient representations by initializing weighted parameters. Finally, we can
    add dropout layers to prevent our network from getting lazy. As we noted previously,
    seeing is believing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了三种特定方法的理论，这些方法可以提高我们模型在未见数据上的泛化能力。首先，我们可以改变网络大小，确保其没有额外的学习能力。我们还可以通过初始化加权参数来惩罚低效的表示。最后，我们可以添加
    dropout 层，防止网络变得懒散。如前所述，看得见才相信。
- en: 'Now, let''s implement our understanding using the MNIST dataset and some Keras
    code. As we saw previously, to change the network size, you are simply required
    to change the number of neurons per layer. This can be done in Keras during the
    process of adding layers, like so:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过 MNIST 数据集和一些 Keras 代码来实现我们的理解。如前所述，要改变网络的大小，您只需要更改每层的神经元数量。这可以在 Keras
    中通过添加层的过程来完成，如下所示：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Weight regularization experiments
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重正则化实验
- en: 'Simply put, regularizers let us apply penalties to layer parameters during
    optimization. These penalties are incorporated in to the `loss` function that
    the network optimizes. In Keras, we regularize the weights of a layer by passing
    a `kernel_regularizer` instance to a layer:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，正则化器让我们在优化过程中对层参数施加惩罚。这些惩罚被纳入网络优化的 `loss` 函数中。在 Keras 中，我们通过将 `kernel_regularizer`
    实例传递给层来正则化层的权重：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we mentioned previously, we add L2 regularization to both our layers, each
    with an alpha value of (0.0001). The alpha value of a regularizer simply refers
    to the transformation that''s being applied to each coefficient in the weight
    matrix of the layer, before it is added to the total loss of our network. In essence,
    the alpha value is used to multiply each coefficient in our weight matrix with
    it (in our case, 0.0001). The different regularizers in Keras can be found in
    `keras.regularizers`. The following diagram shows how regularization impacts validation
    loss per epoch on two models that are the same size. One observes that our regularized
    model is much less prone to overfitting, since the validation loss does not significantly
    increase as a function of time. On the model without regularization, we can clearly
    see that this is not the case, and after about seven epochs, the model starts
    overfitting, and so performs worse on the validation set:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们对每一层都添加了 L2 正则化，alpha 值为（0.0001）。正则化器的 alpha 值表示在将其添加到网络总损失之前，应用于层的权重矩阵中每个系数的变换。实质上，alpha
    值用于将每个系数与它相乘（在我们的例子中是 0.0001）。Keras 中的不同正则化器可以在 `keras.regularizers` 中找到。下图展示了正则化如何影响两个相同大小模型每个
    epoch 的验证损失。我们可以看到，正则化后的模型更不容易过拟合，因为验证损失在时间的变化中没有显著增加。而没有正则化的模型则完全不是这样，经过大约七个
    epoch 后，模型开始过拟合，因此在验证集上的表现变差：
- en: '![](img/b4c91389-24a7-4a3b-b6ba-d1febaf38000.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4c91389-24a7-4a3b-b6ba-d1febaf38000.png)'
- en: Implementing dropout regularization in Keras
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中实现 dropout 正则化
- en: 'In Keras, adding a dropout layer is also very simple. All you are required
    to do is use the `model.add()` parameter again, and then specify a dropout layer
    (instead of the dense layer that we''ve been using so far) to be added. The `Dropout`
    parameter in Keras takes a float value that refers to the fraction of neurons
    whose predictions will be dropped. A very low dropout rate might not provide the
    robustness we are looking for, while a high dropout rate simply means we have
    a network prone to amnesia, incapable of remembering any useful representations.
    Once again, we strive for a dropout value that is just right; conventionally,
    the dropout rate is set between 0.2 and 0.4:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，添加一个 dropout 层也是非常简单的。你只需要再次使用 `model.add()` 参数，然后指定一个 dropout 层（而不是我们一直使用的全连接层）来进行添加。Keras
    中的 `Dropout` 参数是一个浮动值，表示将被丢弃的神经元预测的比例。一个非常低的 dropout 率可能无法提供我们所需的鲁棒性，而一个高 dropout
    率则意味着我们的网络容易遗忘，无法记住任何有用的表示。我们再次努力寻找一个恰到好处的 dropout 值；通常，dropout 率设定在 0.2 到 0.4
    之间：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Dropout regularization experiments
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout 正则化实验
- en: 'The following are two experiments that we performed using a network of the
    same size, with different dropout rates, to observe the differences in performance.
    We started with a dropout rate of 0.1, and progressively scaled to 0.6 to see
    how this affected our performance in recognizing handwritten digits. As we can
    see in the following diagram, scaling our dropout rate seems to reduce overfitting,
    as the model''s superficial accuracy on the training set progressively drops.
    We can see that both our training and test accuracy converges near the dropout
    rate of 0.5, after which they exhibit divergent behavior. This simply tells us
    that the network seems to overfit the least when a dropout layer of rate 0.5 is
    added:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们使用相同大小的网络进行的两个实验，采用不同的 dropout 率，以观察性能上的差异。我们从 0.1 的 dropout 率开始，逐渐增加到
    0.6，以查看这对我们识别手写数字的性能有何影响。正如下图所示，增加 dropout 率似乎减少了过拟合，因为模型在训练集上的表面准确度逐渐下降。我们可以看到，在
    dropout 率接近 0.5 时，我们的训练准确度和测试准确度趋于收敛，之后它们出现了分歧行为。这简单地告诉我们，网络似乎在添加 dropout 率为 0.5
    的层时最不容易过拟合：
- en: '![](img/dc7da382-5d59-4ffe-90e8-5960e26cd6ff.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc7da382-5d59-4ffe-90e8-5960e26cd6ff.png)'
- en: '![](img/b9f13e80-67a8-47eb-b72b-275a841d8dbc.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9f13e80-67a8-47eb-b72b-275a841d8dbc.png)'
- en: Complexity and time
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂性与时间
- en: 'Now, you have seen some of the most prominent tricks in our repertoire to reduce
    overfitting through regularization. In essence, regularization is just a manner
    of controlling the complexity of our network. Complexity control is not just useful
    to restrict your network from memorizing randomness; it also brings more direct
    benefits. Inherently, more complex networks are computationally expensive. They
    will take longer to train, and hence consume more of your resources. While this
    makes an insignificant difference when dealing with the task at hand, this difference
    is still quite noticeable. In the following diagram is a time complexity chart.
    This is a useful way of visualizing training time as a function of network complexity.
    We can see that an increase in our network''s complexity seems to have a quasi-exponential
    effect on the increase in average time taken per training iteration:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经看到了我们减少过拟合的一些最突出技巧，都是通过正则化实现的。本质上，正则化就是控制我们网络复杂度的一种方式。控制复杂度不仅仅是限制网络记忆随机性的手段，它还带来了更多直接的好处。从本质上讲，更复杂的网络在计算上代价更高。它们需要更长的训练时间，因此消耗更多资源。虽然在处理当前任务时这种差异几乎可以忽略不计，但它仍然是显著的。下图是一个时间复杂度图。这是一种将训练时间与网络复杂度之间的关系可视化的有用方式。我们可以看到，网络复杂度的增加似乎对每次训练迭代所需的平均时间的增加产生了近乎指数的影响：
- en: '![](img/44b86b50-9754-4361-a6c5-6b9c9b979971.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44b86b50-9754-4361-a6c5-6b9c9b979971.png)'
- en: A summary of MNIST
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 总结
- en: So far in our journey, you were introduced to the fundamental learning mechanisms
    and processes that govern a neural network's functionality. You learned that neural
    networks need tensor representations of input data to be able to process it for
    predictive use cases. You also learned how different types of data that are found
    in our world, such as images, videos, text, and so on, can be represented as tensors
    of *n*-dimensions. Furthermore, you saw how to implement a sequential model in
    Keras, which essentially lets you build sequential layers of interconnected neurons.
    You used this model structure to construct a simple feedforward neural network
    for the task of classifying handwritten digits with the MNIST dataset. In doing
    so, you learned about the key architectural decisions to consider at each stage
    of model development.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的学习旅程中，你已经了解了支配神经网络功能的基本学习机制和过程。你了解到，神经网络需要输入数据的张量表示才能进行预测性处理。你还学习了我们世界中不同类型的数据，如图像、视频、文本等，如何表示为*
    n *维的张量。此外，你学会了如何在 Keras 中实现一个顺序模型，该模型基本上让你构建一层层相互连接的神经元。你使用这个模型结构，构建了一个简单的前馈神经网络，用于分类手写数字的
    MNIST 数据集。在此过程中，你了解了在模型开发的每个阶段需要考虑的关键架构决策。
- en: During model construction, the main decisions pertain to defining the correct
    input size of your data, choosing a relevant activation function per layer, and
    defining the number of output neurons in your last layer, according to the number
    of output classes in your data. During the compilation process, you got to choose
    the optimization technique, `loss` function, and a metric to monitor your training
    progress. Then, you initiated the training session of your newly minted model
    by using the `.fit()` parameter, and passing the model the final two architectural
    decisions to be made before initiating the training procedure. These decisions
    pertained to the batch size of your data to be seen at a time, and the total number
    of epochs to train the model for.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建过程中，主要的决策是定义数据的正确输入大小，选择每一层的相关激活函数，以及根据数据中输出类别的数量来定义最后一层的输出神经元数量。在编译过程中，你需要选择优化技术、`loss`
    函数和监控训练进展的度量标准。然后，你通过使用 `.fit()` 参数启动了新模型的训练会话，并传递了最后两个架构决策，作为启动训练过程之前必须做出的决定。这些决策涉及数据一次性处理的批次大小，以及训练模型的总轮数。
- en: 'Finally, you saw how to test your predictions, and learned about the pivotal
    concept of regularization. We concluded this classification task by experimenting
    with regularization techniques to modify our model''s size, layer weights, and
    add dropout layers, which in turn helped us improve the generalizability of our
    model to unseen data. Lastly, we saw that increasing model complexity is unfavourable
    unless explicitly required due to the nature of our task:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学会了如何测试预测结果，并了解了正则化这一关键概念。我们通过实验不同的正则化技术来修改模型的大小、层权重，并添加丢弃层，从而帮助我们提高模型对未见数据的泛化能力。最后，我们发现，除非任务的性质明确要求，否则增加模型复杂度是不利的：
- en: '**Exercise x**: Initialize different weighted parameters and see how this affects
    model performance'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**练习 x**：初始化不同的加权参数，观察这如何影响模型的表现'
- en: '**Exercise y**: Initialize different weights per layer and see how this affects
    model performance'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**练习 y**：初始化每一层的不同权重，观察这如何影响模型的表现'
- en: Language processing
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言处理
- en: So far, we have seen how we can train a simple feedforward neural network on
    Keras for an image classification task. We also saw how we can mathematically
    represent image data as a high-dimensional geometric shape, namely a tensor. We
    saw that a higher-order tensor is simply composed of tensors of a smaller order.
    Pixels group up to represent an image, which in turn group up to represent an
    entire dataset. Essentially, whenever we want to employ the learning mechanism
    of neural networks, we have a way to represent our training data as a tensor.
    But what about language? How can we represent human thought, with all of its intricacies,
    as we do through language? You guessed it—we will use numbers once again. We will
    simply translate our texts, which are composed of sentences, which themselves
    are composed of words, into the universal language of mathematics. This is done
    through a process known as **vectorization**, which we will explore first-hand
    during our task of classifying the sentiment of movie reviews by using the **internet
    movie database** (**IMDB**) dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何在Keras上训练一个简单的前馈神经网络来进行图像分类任务。我们还看到如何将图像数据数学地表示为一个高维几何形状，也就是一个张量。我们了解到，较高阶的张量实际上是由较低阶的张量组成的。像素聚集在一起，代表一个图像，而图像又聚集在一起，代表一个完整的数据集。从本质上讲，每当我们想要利用神经网络的学习机制时，我们都有一种方法来将训练数据表示为一个张量。那么语言呢？我们如何像通过语言表达一样，将人类的思想及其复杂性表示出来？你猜对了——我们将再次使用数字。我们将简单地将由句子组成的文本（而句子又由单词组成）翻译成数学的通用语言。这是通过一种被称为**向量化**的过程完成的，在我们的任务中，我们将通过使用**互联网电影数据库**（**IMDB**）数据集来亲身体验这一过程。
- en: Sentiment analysis
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: As our computing power improved over the years, we started applying computational
    techniques to domains that were previously frequented only by linguists and qualitative
    academics. It turns out that tasks that were initially considered too time-consuming
    to pursue became ideal for computers to optimize as processors increased in potency.
    This led to an explosion of computer-assisted text analysis, not only in academia,
    but also in the industry. Tasks such as computer-assisted sentiment analysis can
    be specifically beneficial for various use cases. This can be used if you're a
    company trying to track your online customer reviews, or an employer wanting to
    do some identity management on social media platforms. In fact, even political
    campaigns increasingly consult services that monitor public sentiments and conduct
    opinion mining on a large variety of political topics. This helps politicians
    prepare their campaign points and understand the general aura of opinions that
    are held by people. While such use of technology can be quite controversial, it
    can vastly help organizations understand their flaws in products, services, and
    marketing strategies, while catering to their audience in a more relevant manner.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的计算能力逐年提升，我们开始将计算技术应用于以前仅由语言学家和定性学者频繁涉足的领域。事实证明，最初被认为太耗时的任务，随着处理器性能的提升，变成了计算机优化的理想对象。这导致了计算机辅助文本分析的爆炸式增长，不仅在学术界，而且在工业界也得到了广泛应用。像计算机辅助情感分析这样的任务，在各种应用场景中尤其有益。如果你是一家企业，试图跟踪在线客户评论，或者是一个雇主，想要进行社交媒体平台上的身份管理，这项技术都可以派上用场。事实上，甚至连政治竞选活动也越来越多地咨询那些监测公共情感并对各种政治话题进行舆情挖掘的服务。这帮助政治人物准备他们的竞选要点，理解公众的普遍情绪。尽管这种技术的使用可能颇具争议，但它可以极大地帮助组织了解其产品、服务和营销策略中的缺陷，同时以更符合受众需求的方式进行调整。
- en: The internet movie reviews dataset
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互联网电影评论数据集
- en: The simplest form of sentiment analysis task deals with categorizing whether
    a piece of text represents a positive or negative opinion. This is often referred
    to as a *polar* or *binary sentiment classification task*, where 0 refers to a
    negative sentiment and 1 refers to a positive sentiment. We can, of course, have
    more complex sentiment models (perhaps using the big-five personality metrics
    we saw in [Chapter 1](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml), *Overview of
    Neural Networks*), but for the time being, we will concentrate on this simple
    yet conceptually loaded binary example. The example in question refers to classifying
    movie reviews from the Internet Movie Database or IMDB.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的情感分析任务是判断一段文本是否代表正面或负面的观点。这通常被称为 *极性* 或 *二元情感分类任务*，其中 0 代表负面情感，1 代表正面情感。当然，我们也可以有更复杂的情感模型（也许使用我们在
    [第 1 章](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml) 中看到的五大人格指标，*神经网络概述*），但目前我们将专注于这个简单但概念上充实的二元示例。这个示例指的是从互联网电影数据库
    IMDB 分类电影评论。
- en: 'The IMDB dataset consists of 50,000 binary reviews, which are evenly split
    into positive and negative opinions. Each review consists of a list of integers,
    where each integer represents a word in that review. Once again, the guardians
    of Keras have thoughtfully included this dataset for practice, and hence can be
    found in Keras under `keras.datasets`. We encourage you to enjoy this importing
    data using Keras, as we won''t be doing so in future exercises (nor will you be
    able to do it in the real world):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: IMDB 数据集包含 50,000 条二进制评论，正负情感评论数量均等。每条评论由一个整数列表组成，每个整数代表该评论中的一个词汇。同样，Keras 的守护者们贴心地为练习提供了这个数据集，因此可以在
    Keras 的 `keras.datasets` 中找到。我们鼓励你享受通过 Keras 导入数据的过程，因为我们在以后的练习中不会再这样做（在现实世界中你也无法做到）：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Loading the dataset
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: As we did previously, we load our dataset by defining our training instances
    and labels, as well as our test instances and labels. We are able to use the `load_data`
    parameter on `imdb` to load in our pre-processed data into a 50/50 train–test
    split. We can also indicate the number of most frequently occurring words we want
    to keep in our dataset. This helps us control the inherent complexity of our task
    as we work with review vectors of reasonable sizes. It is safe to assume that
    rare words occurring in reviews would have to do more with the specific subject
    matter of a given movie, and so they have little influence on the *sentiment*
    of that review in question. Due to this, we will limit the number of words to
    12,000.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前所做的，我们通过定义训练实例和标签，以及测试实例和标签来加载数据集。我们可以使用 `imdb` 上的 `load_data` 参数将预处理后的数据加载到
    50/50 的训练-测试拆分中。我们还可以指定我们想要保留在数据集中的最常见词汇数量。这帮助我们控制任务的固有复杂性，同时处理合理大小的评论向量。可以安全地假设，评论中出现的稀有词汇与给定电影的特定主题相关，因此它们对该评论的
    *情感* 影响较小。因此，我们将词汇量限制为 12,000 个。
- en: Checking the shape and type
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查形状和类型
- en: 'You can check the number of reviews per split by checking the `.shape` parameter
    of `x_train`, which is essentially a NumPy array of *n*-dimensions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查 `x_train` 的 `.shape` 参数来查看每个数据拆分的评论数量，它本质上是一个 *n* 维的 NumPy 数组：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Plotting a single training instance
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制单个训练实例
- en: 'As we can see, there are 25,000 training and test samples. We can also plot
    out an individual training sample to see how we can represent a single review.
    Here, we can see that each review simply contains a list of integers, where each
    integer corresponds to a word in a dictionary:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，有 25,000 个训练和测试样本。我们还可以绘制一个单独的训练样本，看看如何表示单个评论。在这里，我们可以看到每条评论仅包含一个整数列表，每个整数对应词汇表中的一个单词：
- en: '[PRE25]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Decoding the reviews
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码评论
- en: 'If you''re curious (which we are), we can of course map out the exact words
    that these numbers correspond to so that we can read what the review actually
    says. To do this, we must back up our labels. While this step is not essential,
    it is useful if we want to visually verify our network''s predictions later on:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感到好奇（我们也很感兴趣），我们当然可以映射出这些数字所对应的确切单词，以便我们能够读懂评论的实际内容。为了做到这一点，我们必须备份我们的标签。虽然这一步不是必需的，但如果我们希望稍后直观验证网络的预测结果，这将非常有用：
- en: '[PRE26]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we need to recover the words corresponding to the integers representing
    a review, which we saw earlier. The dictionary of words that were used to encode
    these reviews is included with the IMDB dataset. We will simply recover them as
    the `word_index` variable and reverse their order of storage. This basically allows
    us to map each integer index to its corresponding word:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要恢复与整数对应的单词，这些整数表示了评论，我们之前已经看到过。用于编码这些评论的单词字典包含在IMDB数据集中。我们将简单地将其恢复为`word_index`变量，并反转其存储顺序。这基本上允许我们将每个整数索引映射到其对应的单词：
- en: '[PRE27]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The following function takes two arguments. The first one (`n`) denotes an integer
    referring to the n^(th) review in a set. The second argument defines whether the
    n^(th) review is taken from our training or test data. Then, it simply returns
    the string version of the review we specify.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数接受两个参数。第一个参数（`n`）表示一个整数，指代数据集中第n条评论。第二个参数定义该n条评论是否来自训练数据或测试数据。然后，它简单地返回我们指定的评论的字符串版本。
- en: 'This allows us to read out what a reviewer actually wrote. As we can see, in
    our function, we are required to adjust the position of indices, which are offset
    by three positions. This is simply how the designers of the IMDB dataset chose
    to implement their coding scheme, and so this is not of practical relevance for
    other tasks. The offset of the three positions in question occurs because positions
    0, 1, and 2 are occupied by indices for padding, denoting the start of a sequence,
    and denoting unknown values, respectively:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们读取评论者实际写的内容。如我们所见，在我们的函数中，我们需要调整索引的位置，偏移了三个位置。这仅仅是IMDB数据集的设计者选择实现其编码方案的方式，因此对于其他任务来说，这并不具有实际意义。这个偏移量之所以存在，是因为位置0、1和2分别被填充、表示序列的开始和表示未知值的索引占据：
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Using this function, we can decode review number five from our training set,
    as shown in the following code. It turns out that this is a negative review, as
    denoted by its training label, and inferred by its content. Note that the question
    marks are simply an indication of unknown values. Unknown values can occur inherently
    in the review (due to the use of emojis, for example) or due to the restrictions
    we have imposed (that is, if a word is not in the top 12,000 most frequent words
    that were used in the corpus, as stated earlier):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，我们可以解码来自训练集中的第五条评论，如下代码所示。结果表明，这是一条负面评论，正如其训练标签所示，并通过其内容推断得出。请注意，问号仅仅是未知值的指示符。未知值可能会自然出现在评论中（例如由于使用了表情符号），或者由于我们施加的限制（即，如果一个词不在前12,000个最常见的单词中，如前所述）：
- en: '[PRE29]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Preparing the data
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Well then, what are we waiting for? We have a series of numbers representing
    each movie review, with their corresponding label, indicating (1) for positive
    or (0) for negative. This sounds like a classic structured dataset, so why not
    start feeding it to a network? Well, it's not that simple. Earlier, we mentioned
    that neural networks have a very specific diet. They are almost exclusively *Tensor-vores*,
    and so feeding them a list of integers won't do us much good. Instead, we must
    represent our dataset as a tensor of *n*-dimensions before we attempt to pass
    it on to our network for training. At the moment, you will notice that each of
    our movie reviews is represented by a separate list of integers. Naturally, each
    of these lists are of different sizes, as some reviews are smaller than others.
    Our network, on the other hand, requires the input features to be of the same
    size. Hence, we have to find a way to *pad* our reviews so that each of them represents
    a vector of the same length.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们在等什么呢？我们有一系列数字表示每条电影评论及其对应的标签，表示（1）为正面评论或（0）为负面评论。这听起来像是一个经典的结构化数据集，那么为什么不开始将其输入网络呢？实际上，事情并没有那么简单。我们之前提到过，神经网络有一个非常特定的“饮食”。它们几乎是*张量食者*，所以直接喂给它们一个整数列表并不会有什么效果。相反，我们必须将数据集表示为一个*n*维的张量，才能尝试将其传递给网络进行训练。目前，你会注意到，每条电影评论都是由一个单独的整数列表表示的。自然地，这些列表的大小不同，因为有些评论比其他评论短。另一方面，我们的网络要求输入特征的大小相同。因此，我们必须找到一种方法来*填充*评论，使它们的表示向量长度相同。
- en: One-hot encoding
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'Since we know that the maximum number of unique words in our entire corpus
    is 12,000, we can assume that the longest possible review can only be 12,000 in
    length. Hence, we can make each review a vector of length 12,000, containing binary
    values. How does this work? Suppose we have a review of two words: *bad* and *movie*.
    A list containing these words in our dataset may look like [6, 49]. Instead, we
    can represent this same review as a 12,000-dimensional vector populated with 0s,
    except for the indices of 6 and 49, which would instead be 1s. What you''re essentially
    doing is creating 12,000 dummy features to represent each review. Each of these
    dummy features represents the presence or absence of any of the 12,000 words in
    a given review. This approach is also known as **one-hot encoding**. It is commonly
    used to encode features and categorical labels alike in various deep learning
    scenarios.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道整个语料库中最多有12,000个独特的词汇，我们可以假设最长的评论只能有12,000个单词。因此，我们可以将每个评论表示为一个长度为12,000的向量，包含二进制值。这个怎么操作呢？假设我们有一个包含两个单词的评论：*bad*
    和 *movie*。我们数据集中包含这些词汇的列表可能看起来像是[6, 49]。相反，我们可以将这个相同的评论表示为一个12,000维的向量，除了索引6和49，其余位置为0，6和49的索引位置则是1。你所做的基本上是创建12,000个虚拟特征来表示每个评论。这些虚拟特征代表了给定评论中12,000个词汇的存在或不存在。这个方法也被称为**独热编码**（one-hot
    encoding）。它通常用于在各种深度学习场景中对特征和类别标签进行编码。
- en: Vectorizing features
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化特征
- en: 'The following function will take our training data of 25,000 lists of integers,
    where each list is a review. In return, it spits out one-hot encoded vectors for
    each of the integer lists it received from our training set. Then, we simply redefine
    our training and test features by using this function to transform our integer
    lists into a 2D tensor of one-hot encoded review vectors:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将接收我们的25,000个整数列表的训练数据，每个列表都是一个评论。它将返回每个它从训练集收到的整数列表的独热编码向量。然后，我们简单地使用这个函数将整数列表转换成一个2D张量的独热编码评论向量，从而重新定义我们的训练和测试特征：
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can see the result of our transformations by checking the type and shape
    of our training features and labels. You can also check what one individual vector
    looks like, as shown in the following code. We can see that each of our reviews
    is now a vector of length `12000`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查训练特征和标签的类型和形状来查看我们的转换结果。你还可以检查一个单独向量的样子，如下代码所示。我们可以看到，每个评论现在都是一个长度为`12000`的向量：
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Vectorizing labels
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化标签
- en: 'We can also vectorize our training labels, which simply helps our network handle
    our data better. You can think of vectorization as an efficient way to represent
    information to computers. Just like humans are not very good at performing computations
    using Roman numerals, computers are notoriously worse off when dealing with unvectorized
    data. In the following code, we are transforming our labels into NumPy arrays
    that contain 32-bit floating-point arithmetic values of either 0.0 or 1.0:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以向量化我们的训练标签，这有助于我们的网络更好地处理数据。你可以把向量化看作是以一种高效的方式将信息表示给计算机。就像人类不擅长使用罗马数字进行计算一样，计算机在处理未向量化的数据时也常常力不从心。在以下代码中，我们将标签转换为包含32位浮动点值0.0或1.0的NumPy数组：
- en: '[PRE32]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, we have our tensor, ready to be consumed by a neural network. This
    2D tensor is essentially 25,000 stacked vectors, each with its own label. All
    that is left to do is build our network.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们得到了张量，准备好被神经网络使用。这个2D张量本质上是25,000个堆叠的向量，每个向量都有自己的标签。剩下的就是构建我们的网络。
- en: Building a network
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络
- en: The first architectural constraints that you must consider while building a
    network with dense layers are its the depth and width. Then, you need to define
    an input layer with the appropriate shape, and successively choose from different
    activation functions to use per layer.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用密集层构建网络时，必须考虑的第一个架构约束是其深度和宽度。然后，你需要定义一个具有适当形状的输入层，并依次选择每一层要使用的激活函数。
- en: 'As we did for our MNIST example, we simply import the sequential model and
    the dense layer structure. Then we proceed by initializing an empty sequential
    model and progressively add hidden layers until we reach the output layer. Do
    note that our input layer always requires a specific input shape, which for us
    corresponds to the 12,000 - dimensional one-hot encoded vectors that we will be
    feeding it. In our current model, the output layer only has one neuron, which
    will ideally fire if the sentiment in a given review is positive; otherwise, it
    won''t. We will choose **Rectified Linear Unit** (**ReLU**) activation functions
    for our hidden layers and a sigmoid activation function for the ultimate layer.
    Recall that the sigmoid activation function simply squished probability values
    between 0 and 1, making it quite ideal for our binary classification task. The
    ReLU activation function simply helps us zero out negative values, and hence can
    be considered a good default to begin with in many deep learning tasks. In summary,
    we have chosen a model with three densely interconnected hidden layers, containing
    18, 12, and 4 neurons, respectively, as well as an output layer with 1 neuron:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为MNIST示例所做的那样，我们简单地导入了顺序模型和密集层结构。然后，我们通过初始化一个空的顺序模型，并逐步添加隐藏层，直到达到输出层。请注意，我们的输入层总是需要特定的输入形状，对于我们来说，这对应于我们将要馈送的12,000维度的独热编码向量。在我们当前的模型中，输出层仅有一个神经元，如果给定评论中的情感是积极的，则理想情况下会激活该神经元；否则，不会。我们将选择**修正线性单元**（**ReLU**）激活函数作为隐藏层的激活函数，并选择sigmoid激活函数作为最终层的激活函数。请记住，sigmoid激活函数简单地将概率值压缩到0到1之间，非常适合我们的二元分类任务。ReLU激活函数帮助我们将负值归零，因此可以被认为是许多深度学习任务中的一个良好默认选择。总之，我们选择了一个具有三个密集连接的隐藏层模型，分别包含18、12和4个神经元，以及一个具有1个神经元的输出层：
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Compiling the model
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: Now we can compile our freshly built model, as is deep learning tradition. Recall
    that in the compilation process, the two key architectural decisions are the choice
    of the `loss` function, as well as the optimizer. The `loss` function simply helps
    us measure how far our model is from the actual labels at each iteration, whereas
    the optimizer determines how we converge to the ideal predictive weights for our
    model. In [Chapter 10](cd18f9ea-65ed-4ebd-af06-0403d3774be1.xhtml), *Contemplating
    Present and Future Developments*, we will review advanced optimizers and their
    relevance in various data processing tasks. For now, we will show how you can
    manually adjust the learning rate of an optimizer.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编译我们新构建的模型，这是深度学习的传统做法。请记住，在编译过程中，两个关键架构决策是选择`loss`函数以及优化器。`loss`函数帮助我们在每次迭代中衡量模型与实际标签的差距，而优化器则确定了我们如何收敛到理想的预测权重。在[第10章](cd18f9ea-65ed-4ebd-af06-0403d3774be1.xhtml)，*思考当前和未来的发展*，我们将审视先进的优化器及其在各种数据处理任务中的相关性。现在，我们将展示如何手动调整优化器的学习率。
- en: 'We have chosen a very small learning rate of 0.001 on the **Root Mean Square**
    (**RMS**) prop for demonstrative purposes. Recall that the size of the learning
    rate simply determines the size of the step we want out network to take in the
    direction of the correct output at each training iteration. As we mentioned previously,
    a big step can cause our network to *walk over* the global minima in the loss
    hyperspace, whereas a small learning rate can cause your model to take ages to
    converge to a minimum loss value:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们选择了非常小的学习率 0.001，使用**均方根传播**（**RMS**）优化器。请记住，学习率的大小仅仅决定了我们的网络在每次训练迭代中朝着正确输出方向迈出的步长大小。正如我们之前提到的，大步长可能会导致我们的网络在损失超空间中“跨越”全局最小值，而小学习率则可能导致模型花费很长时间才能收敛到最小损失值：
- en: '[PRE34]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Fitting the model
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: In our previous MNIST example, we went over the least number of architectural
    decisions to get our code running. This lets us cover a deep learning workflow
    quite quickly, but at the expense of efficiency. You will recall that we simply
    used the `fit` parameter on our model and passed it our training features and
    labels, along with two integers denoting the epochs to train the model for, and
    the batch size per training iteration. The former simply defines how many times
    our data runs through the model, while the latter defines how many learning examples
    our model will see at a time before updating its weights. These are the two paramount
    architectural considerations that must be defined and adapted to the case at hand.
    However, there are several other useful arguments that the `fit` parameter may
    take.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的MNIST示例中，我们简要地介绍了最少的架构决策来让代码运行起来。这让我们可以快速覆盖深度学习的工作流，但效率相对较低。你可能还记得，我们只是简单地在模型上使用了`fit`参数，并传递了训练特征和标签，同时提供了两个整数，分别表示训练模型的epoch次数和每次训练迭代的批次大小。前者仅仅定义了数据通过模型的次数，而后者则定义了每次更新模型权重之前，模型会看到多少个学习样本。这两者是必须定义并根据具体情况调整的最重要的架构考量。不过，`fit`参数实际上还可以接受一些其他有用的参数。
- en: Validation data
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证数据
- en: You may be wondering why we train our model blindly for an arbitrary number
    of iterations and then test it on our holdout data. Wouldn't it be more efficient
    to gauge our model to some unseen data after each epoch, just to see how well
    we are doing? This way, we are able to assess exactly when our model starts to
    overfit, and hence end the training session and save some expensive hours of computing.
    We could show our model the test set after each epoch, without updating its weights,
    purely to see how well it does on our test data after that epoch. Since we do
    not update our model weights at each test run, we don't risk our model overfitting
    on the test data. This allows us to get a genuine understanding of how generalizable
    our model is *during* the training process, and not after. To test your model
    on a validation split, you can simply pass the validation features and labels
    as a parameter, as you did with your training data, to the `fit` parameter.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们要盲目地训练模型，迭代任意次数，然后再在保留数据上进行测试。难道不应该在每个epoch之后，就用一些看不见的数据来评估我们的模型表现，这样岂不是更高效吗？这样，我们就能准确评估模型开始过拟合的时机，从而结束训练过程，节省一些昂贵的计算时间。我们可以在每个epoch后展示测试集给模型，但不更新其权重，纯粹是为了看看它在该epoch后在测试数据上的表现如何。由于我们在每次测试运行时都不更新模型的权重，我们就不会让模型在测试数据上过拟合。这使我们能够在训练过程中*实时*了解模型的泛化能力，而不是训练完成后再去评估。要在验证集上测试你的模型，你只需要像传递训练数据那样，把验证数据的特征和标签作为参数传递给`fit`参数即可。
- en: 'In our case, we have simply used the test features and labels as our validation
    data. In a rigorous deep learning scenario with high stakes, you may well choose
    to have separate test and validation sets, where one is used for validation during
    training, and the other is reserved for later assessments before you deploy your
    model to production. This is shown in the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们只是将测试特征和标签作为验证数据使用。在一个高风险的深度学习场景中，你可能会选择将测试集和验证集分开使用，其中一个用于训练过程中的验证，另一个则保留在后期评估中，确保在部署模型到生产环境之前进行最后的测试。以下是相应的代码示例：
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now, when you execute the preceding cell, you will see the training session
    initiate. Moreover, at the end of each training epoch, you will see that our model
    takes a brief pause to compute the accuracy and loss on the validation set, which
    is then displayed. Then, without updating its weights after this validation pass,
    the model proceeds to the next epoch for another training round. The preceding
    model will run for 20 epochs, where each epoch will iterate over our 25,000 *training*
    examples in batches of 100, updating the model weights after each batch. Note
    that in our case, the model weights are updated 250 times per epoch, or 5,000
    times during the preceding training session of 20 epochs. So, now we can better
    assess when our model starts to memorize random features of our training set,
    but how do we actually interrupt the training session at this point? Well, you
    may have noticed that instead of just executing `model.fit()`, we defined it as
    `network_metadata`. As it happens, the `fit()` parameter actually returns a history
    object containing the relevant training statistics of our model, which we are
    interested in recovering. This history object is recorded by something called
    a **callback** in Keras.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你执行前面的单元格时，你将看到训练会话开始。此外，在每个训练周期结束时，你将看到我们的模型暂停片刻，计算并显示验证集上的准确度和损失。然后，模型在不更新权重的情况下，继续进入下一个周期进行新的训练轮次。前述模型将在
    20 个周期中运行，每个周期会批量处理 25,000个*训练*样本，每批 100 个，在每批次后更新模型权重。请注意，在我们的案例中，模型的权重每个周期更新
    250 次，或者在 20 个周期的训练过程中，总共更新 5,000 次。所以，现在我们可以更好地评估我们的模型何时开始记忆训练集中的随机特征，但我们如何在此时中断训练会话呢？嗯，你可能已经注意到，与其直接执行`model.fit()`，我们将其定义为`network_metadata`。实际上，`fit()`参数会返回一个历史对象，其中包含我们模型的相关训练统计数据，我们希望恢复该对象。这个历史对象是通过
    Keras 中名为**回调**的机制记录的。
- en: Callbacks
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回调函数
- en: A `callback` is basically a Keras library function that can interact with our
    model during the training session to check on its internal state and save relevant
    training statistics for later scrutiny. While quite a few callback functions exist
    in `keras.callbacks`, we will introduce a few that are crucial. For those of you
    who are more technically oriented, Keras even lets you construct custom callbacks.
    To use a callback, you simply pass it to the `fit` parameter using the keyword
    argument `callbacks`. Note that the history callback is automatically applied
    to every Keras model, and so it does not need to be specified as long as you define
    the fitting process as a variable. This lets you recover the associated history
    object.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`callback`本质上是 Keras 库的一个函数，可以在训练过程中与我们的模型进行交互，检查其内部状态并保存相关的训练统计数据，以便后续审查。在`keras.callbacks`中存在很多回调函数，我们将介绍一些至关重要的回调。对于那些更倾向于技术性的用户，Keras
    甚至允许你构建自定义回调。要使用回调，你只需要将它传递给`fit`参数，使用关键字参数`callbacks`。需要注意的是，历史回调会自动应用于每个 Keras
    模型，因此只要你将拟合过程定义为变量，就不需要指定它。这使得你能够恢复相关的历史对象。'
- en: 'Importantly, if you initiated a training session previously in your Jupyter
    Notebook, then calling the `fit()` parameter on the model will continue training
    the same model. Instead, you want to reinitialize a blank model, before proceeding
    with another training run. This can be done by simply rerunning the cells where
    you previously defined and compiled your sequential model. Then, you may proceed
    by implementing a callback by passing it to the `fit()` parameter by using the
    `callbacks` keyword argument, as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，如果你之前在 Jupyter Notebook 中启动了训练会话，那么在模型上调用`fit()`参数将会继续训练同一个模型。相反，你需要重新初始化一个空白模型，然后再进行另一次训练。你可以通过重新运行之前定义并编译顺序模型的单元格来实现这一点。然后，你可以通过使用`callbacks`关键字参数将回调传递给`fit()`参数，从而实现回调，示例如下：
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Early stopping and history callbacks
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早停和历史回调
- en: 'In the preceding cell, we used a callback known as **early stopping**. This
    callback allows us to monitor a specific training metric. Our choices are between
    our accuracy or loss on the training set or on the validation set, which are all
    stored in a dictionary pertaining to our model''s history:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的单元格中，我们使用了一个名为**早停**的回调。这个回调允许我们监控一个特定的训练指标。我们可以选择的指标包括训练集或验证集上的准确度或损失，这些信息都存储在一个与模型历史相关的字典中：
- en: '[PRE37]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Choosing a metric to monitor
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一个监控的指标
- en: 'The ideal choice is always *validation loss* or *validation accuracy*, as these
    metrics best represent the out of set predictability of our model. This is simply
    due to the fact that we only update our model weights during a training pass,
    and not a validation pass. Choosing our *training accuracy* or *loss* as a metric
    (as in the following code) is suboptimal in the sense that you are benchmarking
    your model by its own definition of a benchmark. To put this in a different way,
    your model might keep reducing its loss and increasing in accuracy, but it is
    doing so by rote memorization—not because it is learning general predictive rules
    as we want it to. As we can see in the following code, by monitoring our *training*
    loss, our model continues to decrease loss on the training set, even though the
    loss on the validation set actually starts increasing shortly after the very first
    epoch:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的选择始终是*验证损失*或*验证准确率*，因为这些指标最能代表我们模型在外部数据集上的可预测性。这仅仅是因为我们只在训练过程中更新模型权重，而不是在验证过程中。选择*训练准确率*或*损失*作为指标（如以下代码所示）并不是最佳选择，因为你是在通过模型自身对基准的定义来评估模型。换句话说，你的模型可能一直在减少损失并提高准确率，但它这样做是通过死记硬背——而不是因为它正在学习我们希望它能掌握的普适预测规则。正如我们在以下代码中看到的，通过监控*训练*损失，我们的模型继续减少训练集上的损失，尽管验证集上的损失在第一次训练后不久就开始增加：
- en: '[PRE38]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code generates the following output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/15feadd5-d291-4653-8c98-d90b611a6984.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15feadd5-d291-4653-8c98-d90b611a6984.png)'
- en: 'We used Matplotlib to plot out the preceding graph. Similarly, you can clear
    out the previous loss graph and plot out a new accuracy graph of our training
    session, as shown in the following code. If we had used validation accuracy as
    a metric to track our early stopping callback, our training session would have
    ended after the *first* epoch, as *this* is the point in time where our model
    appears to be the most generalizable to unseen data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Matplotlib绘制了上述图表。同样，你也可以清除之前的损失图，并绘制出新的训练准确率图，如以下代码所示。如果我们将验证准确率作为指标来跟踪早停回调，那么我们的训练会在*第一个*训练周期后结束，因为*此时*我们的模型似乎对未见过的数据具有最好的泛化能力：
- en: '[PRE39]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding code generates the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/a902e7f2-2276-4c1a-ad14-22e84927d07d.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a902e7f2-2276-4c1a-ad14-22e84927d07d.png)'
- en: Accessing model predictions
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问模型预测
- en: 'In the MNIST example, we used the *Softmax* activation function as our last
    layer. You may recall that the layer generated an array of 10 probability scores,
    adding up to 1 for a given input. Each of those 10 scores referred to the likelihood
    of the image being presented to our network corresponding to one of the output
    classes (that is, it is 90% sure it sees a 1, and 10% sure it sees a 7, for example).
    This approach made sense for a classification task with 10 categories. In our
    sentiment analysis problem, we chose a sigmoid activation function, because we
    are dealing with binary categories. Using the sigmoid here simply forces our network
    to output a prediction between 0 and 1 for any given instance of data. Hence,
    a value closer to 1 means that our network believes that the given piece of information
    is more likely to be a positive review, whereas a value closer to zero states
    our network''s conviction of having found a negative review. To view our model''s
    predictions, we simply define a variable called `predictions` by using the `predict()`
    parameter on our trained model and passing it our test set. Now we can check our
    network predictions on a given example from this set, as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST示例中，我们使用了*Softmax*激活函数作为最后一层。你可能记得，这一层生成了一个包含10个概率值的数组，总和为1，表示给定输入的概率。每一个概率值代表输入图像属于某个输出类别的可能性（例如，它90%确定看到的是数字1，10%确定看到的是数字7）。对于一个具有10个类别的分类任务，这种方法是合理的。在我们的情感分析问题中，我们选择了Sigmoid激活函数，因为我们处理的是二分类任务。在这里使用Sigmoid函数强制我们的网络对任何给定数据实例输出0到1之间的预测值。因此，越接近1的值意味着我们的网络认为该信息更有可能是积极评论，而越接近0的值则表示网络认为该信息是负面评论。要查看我们模型的预测，只需定义一个名为`predictions`的变量，使用`predict()`方法对训练好的模型进行预测，并传入我们的测试集。现在，我们可以查看网络在该测试集中某个示例上的预测结果，具体如下：
- en: '[PRE40]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In this case, it appears that our network is quite confident that review `5`
    from our test set is a positive review. Not only can we check whether this is
    indeed the case by checking the label stored in `y_test[5]`, we can also decode
    the review itself due to the decoder function we built earlier. Let''s put our
    network''s prediction to the test by decoding review `5` and checking its label:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的网络似乎非常确信我们测试集中的`5`号评论是一个正面评论。我们不仅可以通过检查`y_test[5]`中存储的标签来验证这是否真是如此，还可以利用我们之前构建的解码器函数解码评论本身。让我们通过解码`5`号评论并检查其标签来验证网络的预测：
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It turns out our network is right. This is an example of a complex linguistic
    pattern that requires a higher-level understanding of linguistic syntax, real-world
    entities, relational logic, and the propensity for humans to blabber aimlessly.
    Yet, with only 12 neurons, our network has seemingly understood the underlying
    sentiment that's encoded in this piece of information. It makes a prediction with
    a high degree of certainty (99.99%), despite the presence of words such as *disgusting*,
    which are very likely to appear in negative reviews.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明我们的网络是对的。这是一个复杂的语言模式示例，它需要对语言语法、现实世界的实体、关系逻辑以及人类胡乱叨叨的倾向有更高层次的理解。然而，凭借仅仅12个神经元，我们的网络似乎已经理解了这段信息中所编码的潜在情感。尽管出现了像*disgusting*这样的词汇，这些词在负面评论中非常常见，但它依然做出了高达99.99%的高可信度预测。
- en: Probing the predictions
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探测预测结果
- en: 'Let''s examine another review. To probe our predictions, we will make a few
    functions that will help us visualize our results better. Such a gauging function
    can also be used if you want to restrict your model''s predictions to instances
    where it is most certain:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再检查一个评论。为了更好地探测我们的预测结果，我们将编写一些函数来帮助我们更清晰地可视化结果。如果你想将模型的预测限制为最有信心的实例，这样的评估函数也可以派上用场：
- en: '[PRE42]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will make two functions to help us better visualize our network''s errors,
    while also limiting our predictive accuracy to upper and lower bounds. We will
    use the first function to arbitrarily define good predictions as instances where
    the network has a probability score above 0.7, and bad instances where the score
    is below 0.4, for a *positive review*. We simply reverse this scheme for the negative
    reviews (a good prediction score for a *negative review* is below 0.4 and a bad
    one is above 0.7). We also leave a middle ground between 40 and 70%, labeled as
    uncertain predictions so that we can better understand the reason behind its accurate
    and inaccurate guesses. The second function is designed for simplicity, taking
    an integer value that refers to the *n*th review you want to probe and verify
    as input, and returning an assessment of what the network thinks, the actual probability
    score, as well as what the review in question reads. Let''s use these newly forged
    functions to probe yet another review:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写两个函数来帮助我们更好地可视化网络的错误，同时将我们的预测准确性限制在上限和下限之间。我们将使用第一个函数将网络的概率得分高于0.7的实例定义为好的预测，得分低于0.4的定义为差的预测，针对*正面评论*。对于负面评论，我们简单地反转这一规则（*负面评论*的好预测得分低于0.4，差的得分高于0.7）。我们还在40%到70%之间留下一个中间地带，将其标记为不确定预测，以便更好地理解其准确和不准确预测的原因。第二个函数设计得较为简单，接受一个整数值作为输入，表示你想探测和验证的第*n*条评论，并返回网络的评估结果、实际概率得分，以及该评论的内容。让我们使用这些新编写的函数来探测另一个评论：
- en: '[PRE43]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As we can see, our network seems to be quite sure that review `22` from our
    test set is negative. It has generated a probability score of 0.169\. You could
    also interpret this score as that our network believed with 16.9% confidence that
    this review is positive, and so it must be negative (since these are the only
    two classes we used to train our network). It turns out that our network got this
    one wrong. Reading the review, you will notice that the reviewer actually expresses
    their appreciation for what they deemed to be an undervalued movie. Note that
    the tone is quite ambiguous at the beginning, with words like *silly* and *fall
    flat*. However, contextual valence shifters later on in the sentence allow our
    biological neural networks to determine that the review actually expresses a positive
    sentiment. Sadly, our artificial network does not seem to have caught up with
    this particular pattern. Let''s continue our exploratory analysis using yet another
    example:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，网络似乎相当确定我们测试集中 `22` 号评论是负面的。它生成了 0.169 的概率得分。你也可以理解为我们的网络以 16.9% 的信心认为这条评论是正面的，因此它必须是负面的（因为我们只用了这两类来训练我们的网络）。结果证明，网络在这条评论上判断错误。阅读评论后，你会发现评论者实际上是对这部被认为被低估的电影表示赞赏。注意，开头的语气相当模糊，使用了诸如
    *silly* 和 *fall flat* 这样的词汇。然而，句子中的语境情感转折器使得我们的生物神经网络能够确定这条评论其实表达了积极的情感。可惜，我们的人工神经网络似乎没有捕捉到这一特定模式。让我们继续使用另一个例子来进行探索分析：
- en: '[PRE44]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here, we can see that our network is not too sure about the review, even though
    it has actually guessed the correct sentiment in the review, with a probability
    score of 0.59, which is closer to 1 ( positive) than 0 (negative). To us, this
    review clearly appears positive—even a bit promotionally pushy. It is intuitively
    unclear why our network is not certain of the sentiment. Later in this book, we
    will learn how to visualize word embeddings using our network layers. For now,
    let''s continue our probing with one last example:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，尽管我们的网络实际上猜对了评论的情感，且概率得分为 0.59，接近 1（正面）而非 0（负面），但是它对于评论的情感并不太确定。对我们来说，这条评论显然是正面的——甚至有些过于推销。直观上，我们不明白为什么我们的网络对情感没有信心。稍后在本书中，我们将学习如何通过网络层可视化词嵌入。目前，让我们继续通过最后一个例子来探究：
- en: '[PRE45]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This time, our network gets it right again. In fact, our network is 99.9% sure
    that this is a positive review. While reading the review, you''ll notice that
    it has actually done a decent job, as the review contains words like *boring*,
    *average*, and suggestive language such as *mouth shut*, which could all easily
    be present in other negative reviews, potentially misleading our network. As we
    can see, we conclude this probing session by providing a short function that you
    can play around with by randomly checking your network''s predictions for a given
    number of reviews. We then print out our network''s predictions for two randomly
    chosen reviews from our test set:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们的网络再次做对了。实际上，我们的网络有 99.9% 的信心认为这是一条正面评论。阅读评论时，你会发现它实际上做得相当不错，因为评论中包含了
    *boring*、*average* 这样的词汇，还有像 *mouth shut* 这样的暗示性语言，这些都可能出现在其他负面评论中，从而可能误导我们的网络。正如我们所见，我们通过提供一个短小的函数来结束这次探讨，你可以通过随机检查给定数量评论的网络预测来进行实验。然后，我们打印出网络对于测试集中两条随机挑选的评论的预测结果：
- en: '[PRE46]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Summary of IMDB
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IMDB 总结
- en: 'Now you should have a better idea of how to go about processing natural language
    texts and dialogues through a simple feedforward neural network. In this subsection
    of our journey, you learned how to execute a binary sentiment classification task
    using a feedforward neural network. In doing so, you learned how to pad and vectorize
    your natural language data, preparing it for processing with neural networks.
    You also went over the key architectural changes that are involved in binary classification,
    such as using an output neuron and the sigmoid activation function on the last
    layer of our network. You also saw how you can leverage a validation split in
    your data to get an idea of how your model performs on unseen data after each
    training epoch. Moreover, you learned how to indirectly interact with your model
    during the training process by using Keras callbacks. Callbacks can be useful
    for a variety of use cases, ranging from saving your model at a certain checkpoint
    or terminating the training session when a desired metric has reached a certain
    point. We can use the history callback to visualize training statistics, and we
    can use the early stopping callback to designate a moment to terminate the current
    training session. Finally, you saw how you can probe your network''s predictions
    per review to better understand what kind of mistakes it makes:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该对如何通过简单的前馈神经网络处理自然语言文本和对话有了更清晰的了解。在我们旅程的这一小节中，你学习了如何使用前馈神经网络执行二分类情感分析任务。在这个过程中，你了解了如何填充和向量化自然语言数据，为神经网络处理做好准备。你还了解了二分类任务中涉及的关键架构变化，比如在网络最后一层使用输出神经元和sigmoid激活函数。你还看到了如何利用数据中的验证集来评估模型在每次训练周期后在未见数据上的表现。此外，你学会了如何通过使用Keras回调函数间接与模型进行交互。回调函数可以用于多种用例，从在某个检查点保存模型到在达到某个预期指标时终止训练会话。我们可以使用历史回调来可视化训练统计信息，还可以使用早停回调来指定终止当前训练会话的时刻。最后，你看到了如何检查每个评论的网络预测，以更好地理解模型会犯哪些错误：
- en: '**Exercise**: Improve performance with regularization, as we did in the MNIST
    example.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**练习**：通过正则化提高性能，就像我们在MNIST示例中所做的那样。'
- en: Predicting continuous variables
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测连续变量
- en: So far, we have performed two classification tasks using neural networks. For
    our first task, we classified handwritten digits. For our second task, we classified
    sentiments in movie reviews. But what if we wanted to predict a continuous value
    instead of a categorical value? What if we wanted to predict how likely an event
    may occur, or the future price of a given object? For such a task, examples such
    as predicting prices in a given market may come to mind. Hence, we will conclude
    this chapter by coding another simple feedforward network by using the Boston
    Housing Prices dataset.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用神经网络完成了两个分类任务。在第一个任务中，我们对手写数字进行了分类。在第二个任务中，我们对电影评论的情感进行了分类。但如果我们想预测一个连续值而不是分类值呢？如果我们想预测某个事件发生的可能性，或者某个物品未来的价格呢？对于这样的任务，像预测给定市场的价格等示例可能会浮现在脑海中。因此，我们将通过使用波士顿房价数据集来编写另一个简单的前馈网络，作为本章的总结。
- en: This dataset resembles most real-world datasets that data scientists and machine
    learning practitioners would come across. You are given 13 features that refer
    to a specific geographical area located in Boston. With these features, the task
    at hand is to predict the median price of houses. The features themselves include
    various indicators ranging from residential and industrial activity, level of
    toxic chemicals in the air, property tax, access to education, and other socio-economic
    indicators that are associated with location. The data was collected during the
    mid-1970s, and seems to have brought along some bias from the time. You will notice
    that some features seem very nuanced and perhaps even inappropriate. Features
    such as feature number 12 can be very controversial to use in machine learning
    projects. You must always consider the higher-level implications when using a
    certain source or type of data. It is your duty as a machine learning practitioner
    to ensure that your model does not introduce or reinforce any sort of societal
    bias, or contribute in any way to disparities and discomfort for people. Remember,
    we are in the business of using technology to alleviate human burden, not add
    to it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集类似于大多数数据科学家和机器学习从业者会遇到的现实世界数据集。数据集提供了13个特征，分别指代位于波士顿的某个特定地理区域。通过这些特征，任务是预测房屋的中位数价格。这些特征包括从居民和工业活动、空气中的有毒化学物质水平、财产税、教育资源可达性到与位置相关的其他社会经济指标。数据收集于1970年代中期，似乎带有一些当时的偏见。你会注意到某些特征显得非常细致，甚至可能不适合使用。例如，第12个特征在机器学习项目中使用可能会非常具有争议。在使用某种数据源或数据类型时，你必须始终考虑其更高层次的含义。作为机器学习从业者，你有责任确保你的模型不会引入或加强任何社会偏见，或在任何方面加剧人们的不平等和不适感。记住，我们的目标是利用技术减轻人类的负担，而不是增加负担。
- en: Boston Housing Prices dataset
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 波士顿房价数据集
- en: As we mentioned in the previous section, this dataset contains 13 training features
    that are represented on an observed geographical region.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所提到的，该数据集包含13个训练特征，表示的是一个观察到的地理区域。
- en: Loading the data
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: The dependent variable that we are interested in predicting is the housing price
    per location, which is denoted as a continuous variable that denotes house prices
    in thousands of dollars.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的因变量是每个位置的房屋价格，它作为一个连续变量表示房价，以千美元为单位。
- en: 'Hence, each of our observations can be represented as a vector of dimension
    13, with a corresponding scalar label. In the following code, we are plotting
    out the second observation in our training set, along with its corresponding label:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的每个观察值可以表示为一个13维的向量，配有一个相应的标量标签。在下面的代码中，我们绘制了训练集中的第二个观察值，并标出其对应的标签：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Exploring the data
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'This dataset is a much smaller dataset in comparison to the ones we''ve dealt
    with so far. We can only see 404 training observations and `102` test observations:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集与我们之前处理的数据集相比要小得多。我们只看到404个训练观察值和`102`个测试观察值：
- en: '[PRE48]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We will also generate a dictionary containing the description of our features
    so that we can understand what each of them actually encodes:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将生成一个字典，包含各特征的描述，以便我们理解它们实际编码的内容：
- en: '[PRE49]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now let''s create a pandas `DataFrame` and have a look at the first five observations
    in our training set. We will simply pass out the training data, along with the
    previously defined column names, as arguments to the pandas `DataFrame` constructor.
    Then, we will use the `.head()` parameter on our newly forged `.DataFrame` object
    to get a nice display, as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个pandas `DataFrame`，并查看训练集中前五个观察值。我们将简单地将训练数据和之前定义的列名一起传递给pandas `DataFrame`构造函数。然后，我们将使用`.head()`参数在新创建的`.DataFrame`对象上，获取一个整洁的展示，具体如下：
- en: '[PRE50]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Feature-wise normalization
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征归一化
- en: 'We can see that each feature in our observation seems to be on a different
    scale. Some values range in the hundreds, while others are between 1 and 12, or
    even binary. While neural networks may still ingest unscaled features, it almost
    exclusively prefers to deal with features on the same scale. In practice, a network
    can learn from heterogeneously scaled features, but it may take much longer to
    do so without any guarantee of finding an ideal minimum on the loss landscape.
    To allow our network to learn in an improved way for this dataset, we must homogenize
    our data through the process of feature-wise normalization. We can achieve this
    by subtracting the feature-specific mean and dividing it by the feature-specific
    standard deviation for each feature in our dataset. Note that in live-deployed
    models (for the stock exchange, for example), such a scaling measure is impractical,
    as the means and standard deviation values may keep on changing, depending on
    new, incoming data. In such scenarios, other normalization and standardization
    techniques (such as log normalization, for example) are better to use:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在我们的观察中，每个特征似乎都处于不同的尺度上。一些值在数百之间，而另一些则介于1到12之间，甚至是二进制的。尽管神经网络仍然可以处理未经过尺度变换的特征，但它几乎总是更倾向于处理处于相同尺度上的特征。实际上，网络可以从不同尺度的特征中学习，但没有任何保证能够在损失函数的局部最小值中找到理想解，且可能需要更长的时间。为了让我们的网络能够更好地学习此数据集，我们必须通过特征归一化的过程来统一我们的数据。我们可以通过从每个特征的均值中减去特征特定的均值，并将其除以特征特定的标准差来实现这一点。请注意，在实际部署的模型中（例如股市模型），这种尺度化方法不可行，因为均值和标准差的值可能会不断变化，取决于新的、不断输入的数据。在这种情况下，其他归一化和标准化技术（例如对数归一化）更适合使用：
- en: '[PRE51]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Building the model
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: The main architectural difference in this regression model, as opposed to the
    previous classification models we built, is to do with the way we construct the
    last layer of this network. Recall that in a classic scalar regression problem,
    such as the one at hand, we aim to predict a continuous variable. To implement
    this, we avoid using an activation function in our last layer, and use only one
    output neuron.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回归模型的主要架构差异，与我们之前构建的分类模型相比，涉及的是我们如何构建网络最后一层的方式。回想一下，在经典的标量回归问题中，比如当前问题，我们的目标是预测一个连续变量。为了实现这一点，我们避免在最后一层使用激活函数，并且只使用一个输出神经元。
- en: 'The reason we forego an activation function is because we do not want to constrain
    the range that the output values of this layer may take. Since we are implementing
    a purely linear layer, our network is able to learn to predict a scalar continuous
    value, just as we want it to:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们放弃激活函数的原因是因为我们不希望限制这一层的输出值可能采取的范围。由于我们正在实现一个纯线性层，我们的网络能够学习预测一个标量连续值，正如我们希望的那样：
- en: '[PRE52]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Compiling the model
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'The main architectural difference during compilation here is to do with the
    `loss` function and metric we choose to implement. We will use the MSE `loss`
    function to penalize higher prediction errors, while monitoring our model''s training
    progress with the **Mean Absolute Error** (**MAE**) metric:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里编译过程中主要的架构差异在于我们选择实现的`loss`函数和度量标准。我们将使用均方误差（MSE）`loss`函数来惩罚更高的预测误差，同时使用**平均绝对误差**（**MAE**）度量来监控模型的训练进展：
- en: '[PRE53]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As we saw previously, the MSE function measures the average of the squares of
    our network's prediction errors. Simply put, we are simply measuring the average
    squared difference between the estimated and actual house price labels. The squared
    term emphasizes the spread of our prediction errors by penalizing the errors that
    are further away from the mean. This approach is especially helpful with regression
    tasks where small error values still have a significant impact on predictive accuracy.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，MSE函数测量的是我们网络预测误差的平方平均值。简而言之，我们是在测量估计房价标签与实际房价标签之间的平方差的平均值。平方项通过惩罚与均值差距较大的误差来强调预测误差的分布。这种方法在回归任务中尤其有用，因为即使是小的误差值，也会对预测准确性产生重要影响。
- en: In our case, our housing price labels range between 5 and 50, measured in thousands
    of dollars. Hence, an absolute error of 1 actually means a difference of $1,000
    in prediction. Thus, taking using an absolute error-based `loss` function might
    not give the best feedback mechanism to the network.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，房价标签的范围在5到50之间，以千美元为单位。因此，绝对误差为1实际上意味着预测误差为1,000美元。因此，使用基于绝对误差的`loss`函数可能不会为网络提供最佳的反馈机制。
- en: On the other hand, the choice of MAE as a metric is ideal to measure our training
    progress itself. Visualizing squared errors, as it turns out, is not very intuitive
    to us humans. It is better to simply see the absolute errors in our models' predictions,
    as it is visually more informative. Our choice of metric has no actual impact
    on the training mechanism of the model—it is simply providing us with a feedback
    statistic to visualize how good or bad our model is doing during the training
    session. The MAE metric itself is essentially a measure of difference between
    two continuous variables.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，选择MAE作为度量标准非常适合衡量我们的训练进度。事实上，直观地可视化平方误差对我们人类来说并不容易。更好的做法是直接查看模型预测中的绝对误差，因为它在视觉上更具信息性。我们选择的度量标准对模型的训练机制没有实际影响——它只是为我们提供了一个反馈统计数据，用于可视化模型在训练过程中的表现好坏。MAE度量本质上是两个连续变量之间差异的度量。
- en: Plotting training and test errors
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制训练和测试误差
- en: 'In the following graph, we can see that the average error is about 2.5 (or
    $2,500 dollars). While this may be a small variance when predicting the prices
    on houses that cost $50,000, it starts to matter if the house itself costs $5,000:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到平均误差大约是2.5（或$2,500美元）。当预测价格为$50,000的房屋时，这可能是一个小的偏差，但如果房屋本身的价格为$5,000，这就开始变得重要了：
- en: '![](img/bb27c237-7146-42c8-935c-ec9d9825c23c.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb27c237-7146-42c8-935c-ec9d9825c23c.png)'
- en: 'Finally, let''s predict some housing prices using data from the test set. We
    will use a scatter plot to plot the predictions and actual labels of our test
    set. In the following graph, we can see a line of best fit, along with the data
    points. Our model seems to capture the general trend in our data, despite having
    some outlandish predictions for some points:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用测试集中的数据来预测一些房价。我们将使用散点图来绘制测试集的预测值与实际标签。在下图中，我们可以看到最佳拟合线以及数据点。尽管某些点的预测出现偏差，我们的模型似乎仍然能够捕捉到数据中的一般趋势：
- en: '![](img/e6f365c3-5ccb-4e46-b5ac-1c14c9d5819b.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6f365c3-5ccb-4e46-b5ac-1c14c9d5819b.png)'
- en: 'Moreover, we can plot a histogram that shows the distribution of our prediction
    errors. It appears that our model seems to do pretty well on most counts, but
    has some trouble predicting certain values, while overshooting and undershooting
    for a small number of observations, as shown in the following diagram:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以绘制一个直方图，显示预测误差的分布。图表显示，模型在大多数情况下表现良好，但在预测某些值时遇到了一些困难，同时对少数观察值出现过高或过低的预测，如下图所示：
- en: '![](img/ab62f15f-c7ec-4917-9000-a2143ca576fb.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab62f15f-c7ec-4917-9000-a2143ca576fb.png)'
- en: Validating your approach using k-fold validation
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k折交叉验证验证你的方法
- en: 'We noted earlier how our dataset is significantly smaller than the ones we
    previously dealt with. This leads to several complications while training and
    testing. Primarily, splitting the data into training and test samples, as we did,
    left us with only 100 validation samples. This is hardly enough for us to assuredly
    deploy our model, even if we wanted to. Furthermore, our test scores may change
    a lot depending on which segment of the data ended up in the test set. Hence,
    to reduce our reliance on any particular segment of our data for testing our model,
    we adopted a common machine learning approach known as **k-fold cross validation**.
    Essentially, we split our data into *n* number of smaller partitions and used
    the same number of neural networks to train on each of those smaller partitions
    of our data. Hence, a k-fold cross validation with five folds will split up our
    entire training data of 506 samples into five splits of 101 samples (and one with
    102). Then, we use five different neural networks, each of which trains on four
    splits out of the five data splits and tests itself on the remaining split of
    data. Then, we simply average the predictions from our five models to generate
    a single estimation:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，我们的数据集显著小于我们之前处理的数据集。这在训练和测试过程中引发了几个问题。首先，像我们这样将数据分割成训练集和测试集，最终只剩下100个验证样本。即便如此少的样本，也不足以让我们有信心地部署模型。此外，我们的测试得分可能会根据测试集中的数据段而发生很大变化。因此，为了减少我们对任何特定数据段的依赖，我们采用了机器学习中常见的一种方法——**k折交叉验证**。本质上，我们将数据分成*n*个较小的分区，并使用相同数量的神经网络在这些较小的数据分区上进行训练。因此，进行五折交叉验证时，我们会将506个训练样本分成五个分区，每个分区101个样本（最后一个分区有102个）。然后，我们使用五个不同的神经网络，每个神经网络在五个数据分区中的四个分区上进行训练，并在剩余的分区上进行测试。最后，我们将五个模型的预测结果平均，生成一个单一的估计值：
- en: '![](img/32ce8785-1897-46ee-8e63-c0c4a34e6a87.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32ce8785-1897-46ee-8e63-c0c4a34e6a87.png)'
- en: Cross validation with scikit-learn API
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn API进行交叉验证
- en: The advantage of cross validation over repeated random sub-sampling is that
    all of the observations are used for both training and validation, and each observation
    is used for validation exactly once.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证相较于反复随机子采样的优势在于，所有观察值都用于训练和验证，每个观察值仅用于一次验证。
- en: 'The following code shows you how to implement a five-fold cross validation
    in Keras, where we use the entire dataset (training and testing together) and
    print out the averaged predictions of a network on each of the cross validation
    runs. As we can see, this is achieved by training the model on four random splits
    and testing it on the remaining split, per each cross validation run. We use the
    scikit-learn API wrapper provided by Keras and leverage the Keras regressor, along
    with sklearn''s standard scaler, k-fold cross-validator creator, and score evaluator:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何在Keras中实现五折交叉验证，我们使用整个数据集（训练和测试数据一起），并打印出每次交叉验证运行中网络的平均预测值。正如我们所看到的，这通过在四个随机拆分上训练模型并在剩余的拆分上进行测试来实现。我们使用Keras提供的scikit-learn
    API包装器，利用Keras回归器，以及sklearn的标准缩放器、k折交叉验证创建器和评分评估器：
- en: '[PRE54]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You will notice that we constructed a function named `baseline_model()` to
    build our network. This is a useful way of constructing networks in many scenarios,
    but here it helps us feed the model object to the `KerasRegressor` function that
    we are using from the scikit-learn API wrapper that Keras provides. As many of
    you may well be aware, scikit-learn has been the go-to Python library for ML,
    with all sorts of pre-processing, scaling, normalizing, and algorithmic implementations.
    The Keras creators have implemented a scikit-learn wrapper to enable a certain
    degree of cross functionality between these libraries:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们构建了一个名为`baseline_model()`的函数来搭建我们的网络。这是在许多场景中构建网络的一种有用方式，但在这里，它帮助我们将模型对象传递给`KerasRegressor`函数，这是我们从Keras提供的scikit-learn
    API包装器中使用的。正如许多人所知道的，scikit-learn一直是机器学习的首选Python库，提供了各种预处理、缩放、归一化和算法实现。Keras的创建者实现了一个scikit-learn包装器，以便在这些库之间实现一定程度的互操作性：
- en: '[PRE55]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We will take advantage of this cross functionality to perform our k-fold cross
    validation, as we did previously. Firstly, we will initialize a random number
    generator with a constant random seed. This simply gives us consistency in initializing
    our model weights, helping us to ensure that we can compare future models consistently:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用这种跨功能性来执行我们的k折交叉验证，正如我们之前所做的那样。首先，我们将初始化一个随机数生成器，并设置一个常数随机种子。这只会为我们提供一致的模型权重初始化，帮助我们确保未来的模型可以一致地进行比较：
- en: '[PRE56]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We will create a list of estimators to pass to the sklearn transformation pipeline,
    which is useful to scale and process our data in sequence. To scale our values
    this time, we simply use the `StandardScaler()` preprocessing function from sklearn
    and append it to our list. We also append the Keras wrapper object to the same
    list. This Keras wrapper object is actually a regression estimator called `KerasRegressor`,
    and takes the model function we created, along with the desired number of batch
    size and training epochs as arguments. **Verbose** simply means how much feedback
    you want to see during the training process. By setting it to `0`, we ask our
    model to train silently.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个估算器列表并传递给sklearn的转换流水线，这对于按顺序缩放和处理数据非常有用。为了这次缩放我们的值，我们只需使用来自sklearn的`StandardScaler()`预处理函数，并将其添加到我们的列表中。我们还将Keras包装器对象添加到同一个列表中。这个Keras包装器对象实际上是一个回归估算器，叫做`KerasRegressor`，它接受我们创建的模型函数以及期望的批次大小和训练周期数作为参数。**Verbose**仅表示你希望在训练过程中看到多少反馈。通过将其设置为`0`，我们要求模型静默训练。
- en: Note that these are the same parameters that you would otherwise pass along
    to the `.fit()` function of the model, as we did earlier to initiate our training
    sessions.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些与我们之前传递给模型的`.fit()`函数的参数相同，正如我们之前为了启动训练会话所做的那样。
- en: Running the preceding code gives us an estimate of the average performance of
    our network for the five cross-validation runs we executed. The `results` variable
    stores the MSE scores of our network for each run of the cross validator. We then
    print out the mean and standard deviation (average variance) of MSEs over all
    five runs. Notice that we multiplied our mean value by `-1`. This is simply an
    implementational issue, as the unified scoring API of scikit-learn always maximizes
    a given score. However, in our case, we are trying to minimize our MSE. Hence,
    scores that need to be minimized are negated so that the unified scoring API can
    work correctly. The score that is returned is the negative version of the actual
    MSE.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码，我们可以估算网络在执行的五次交叉验证中的平均表现。`results`变量存储了每次交叉验证运行的网络MSE得分。然后，我们打印出所有五次运行的MSE均值和标准差（平均方差）。请注意，我们将均值乘以`-1`。这是一个实现问题，因为scikit-learn的统一评分API总是最大化给定的分数。然而，在我们的案例中，我们是尝试最小化MSE。因此，需要最小化的分数会被取反，以便统一的评分API能够正确工作。返回的得分是实际MSE的负值。
- en: Summary
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how we can perform a regression task with neural networks.
    This involved some simple architectural changes to our previous classification
    models, pertaining to model construction (one output layer with no activation
    function) and the choice of `loss` function (MSE). We also tracked the MAE as
    a metric, since squared errors are not very intuitive to visualize. Finally, we
    plotted out our model's predictions versus the actual prediction labels using
    a scatter plot to better visualize how well the network did. We also used a histogram
    to understand the distribution of prediction errors in our model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用神经网络执行回归任务。这涉及到对我们先前分类模型的一些简单架构更改，涉及到模型构建（一个没有激活函数的输出层）和`loss`函数的选择（MSE）。我们还跟踪了MAE作为度量，因为平方误差不太直观，难以可视化。最后，我们使用散点图将模型的预测与实际预测标签进行对比，以更好地可视化网络的表现。我们还使用了直方图来理解模型预测误差的分布。
- en: Finally, we introduced the methodology of k-fold cross validation, which is
    preferred over explicit train test splits of our data, in cases where we deal
    with very few data observations. What we did instead of splitting our data into
    a training and test split was split it into a *k* number of smaller partitions.
    Then, we generated a single estimate of predictions by using the same number of
    models as our data subsets. Each of these models were trained on a *k*-1 number
    of data partitions and tested on the remaining one data partition, after which
    their prediction scores were averaged. Doing so prevents our reliance on any particular
    split of our data for testing, and hence we get a more generalizable prediction
    estimate.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了k折交叉验证的方法，它在处理非常少量数据时优于显式的数据训练和测试拆分。我们做的不是将数据拆分为训练集和测试集，而是将其拆分为*k*个较小的部分。然后，我们使用与数据子集相同数量的模型生成一个单一的预测估计。每个模型在*k*-1个数据分区上进行训练，并在剩余的一个数据分区上进行测试，之后对它们的预测得分进行平均。这样做避免了我们依赖数据的任何特定拆分进行测试，因此我们获得了一个更具普遍性的预测估计。
- en: In the next chapter, we will learn about **Convolutional Neural Networks** (**CNNs**).
    We will implement CNNs and detect objects using them. We will also solve some
    image recognition problems.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习**卷积神经网络**（**CNNs**）。我们将实现CNN并使用它们进行物体检测。我们还将解决一些图像识别问题。
- en: Exercises
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Implement three different functions, each returning a network varying in size
    (depth and width). Use each of these functions and perform a k-fold cross validation.
    Assess which size fits best.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现三个不同的函数，每个函数返回一个大小（深度和宽度）不同的网络。使用这些函数并执行k折交叉验证。评估哪种大小最合适。
- en: Experiment with MAE and MSE `loss` functions, and note the difference during
    training.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试MAE和MSE `loss`函数，并在训练过程中记录差异。
- en: Experiment with different `loss` functions and note the differences during training.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的`loss`函数，并在训练过程中记录差异。
- en: Experiment with different regularization techniques and note the differences
    during training.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的正则化技术，并在训练过程中记录差异。
