- en: Training NN for Prediction Using Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归训练神经网络进行预测
- en: Welcome to our first proper project in Python deep learning! What we'll be doing
    today is building a classifier to solve the problem of identifying specific handwriting
    samples from a dataset of images. We've been asked (in this hypothetical use case)
    to do this by a restaurant chain that has the need to accurately classify handwritten
    numbers into digits. What they have their customers do is write their phone numbers
    in a simple iPad application. At the time when they can be seated, the guest will
    get a text prompting them to come and see the restaurant's host. We need to accurately
    classify the handwritten numbers, so that the output from the app will be accurately
    predicted labels for the digits of a phone number. This can then be sent to their
    (hypothetical) auto dialer service for text messages, and the notice gets to the
    right hungry customer!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们的第一个Python深度学习项目！今天我们要做的是构建一个分类器，解决从图像数据集中识别特定手写样本的问题。在这个假设的使用案例中，一家餐饮连锁店要求我们做到这一点，他们需要准确地将手写数字分类成数字。他们让顾客在一个简单的iPad应用程序中写下自己的电话号码。当顾客可以入座时，他们会收到一条短信，提示他们前往餐厅接待员处。我们需要准确分类手写数字，以便应用程序的输出能够精确预测电话号码中的各个数字标签。然后，这些信息可以发送到他们（假设的）自动拨号服务中进行短信发送，从而将通知送到正确的饥饿顾客手中！
- en: '**Define success**: A good practice is to define the criteria for success at
    the beginning of a project. What metric should we use for this project? Let''s
    use a global accuracy test as a percentage to measure our performance in this
    project.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义成功**：一个好的做法是在项目开始时定义成功的标准。我们应该用什么指标来衡量这个项目的成功？我们将使用全球准确度测试作为一个百分比来衡量我们在这个项目中的表现。'
- en: The data science approach to the problem of classification can be configured
    in a number of ways. In fact, later in this book, we'll look at how to increase
    accuracy in image classification with convolutional neural networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学在分类问题上的方法可以以多种方式进行配置。事实上，在本书后续部分，我们将研究如何通过卷积神经网络提高图像分类的准确度。
- en: '**Transfer learning**: This means pretraining a deep learning model on a different
    (but quite similar) dataset to speed up the rate of learning and accuracy on another
    (often smaller) dataset. In this project and our hypothetical use case, the pretraining
    of our deep learning **multi-layer perceptron** (**MLP**) on the MNIST dataset
    would enable the deployment of a production system of handwriting classification,
    without having a huge period of time where we were collecting data samples in
    a live but non-functional system. Python deep learning projects are cool!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**：这意味着在一个不同（但非常相似）的数据集上预训练深度学习模型，以加快在另一个（通常较小）数据集上的学习速度和准确度。在这个项目和我们假设的使用案例中，通过在MNIST数据集上对深度学习**多层感知器**（**MLP**）进行预训练，可以使我们在不需要长时间收集数据样本的情况下，部署一个用于手写识别的生产系统，而不是在一个实际但不具备功能的系统中长时间进行数据收集。Python深度学习项目非常酷！'
- en: Let's start with the baseline deep neural network model architecture. We will
    get our intuition and skills firmly established, and this will prepare us for
    learning more complex architectures to solve a wider variety of problems as we
    go progress through the projects in this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基准深度神经网络模型架构开始。我们将牢固地建立我们的直觉和技能，为学习更复杂的架构做好准备，进而解决我们在本书项目中遇到的更广泛问题。
- en: 'What we''ll learn in this chapter includes the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将学习以下内容：
- en: What is an MLP?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是MLP？
- en: Exploring a common open source handwriting dataset—the MNIST dataset
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索一个常见的开源手写数据集——MNIST数据集
- en: Building our intuition and preparations for model architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立我们的直觉和为模型架构做准备
- en: Coding the model and defining hyperparameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写模型代码并定义超参数
- en: Building the training loop
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建训练循环
- en: Testing the model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: Building a regression model for prediction using an MLP deep neural network
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLP深度神经网络构建回归模型进行预测
- en: 'In any real job working in an AI team, one of the primary goals will be to
    build regression models that can make predictions in non-linear datasets. Because
    of the complexity of the real world and the data that you''ll be working with,
    simple linear regression models won''t provide the predictive power you''re seeking.
    That is why, in this chapter, we will discuss how to build world-class prediction
    models using MLP. More information can be found at [http://www.deeplearningbook.org/contents/mlp.html](http://www.deeplearningbook.org/contents/mlp.html),
    and an example of the MLP architecture is shown here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何真实的人工智能团队工作中，其中一个主要目标是构建能够在非线性数据集上进行预测的回归模型。由于现实世界的复杂性以及你将处理的数据，简单的线性回归模型无法提供你所寻求的预测能力。这就是为什么在本章中，我们将讨论如何使用MLP构建世界级的预测模型。更多信息可以在[http://www.deeplearningbook.org/contents/mlp.html](http://www.deeplearningbook.org/contents/mlp.html)中找到，下面展示了一个MLP架构的示例：
- en: '![](img/c57522c1-d374-41bd-a8f7-edcd12debda7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c57522c1-d374-41bd-a8f7-edcd12debda7.png)'
- en: An MLP with two hidden layers
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有两层隐藏层的多层感知器（MLP）
- en: We will implement a neural network with a simple architecture of only two layers,
    using TensorFlow, that will perform regression on the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)) that
    we will provide. We can (and will) go deeper in architecture in later projects! We
    assume that you are already familiar with backpropagation (if not, please read
    article on backpropagation by Michal Nielsen at [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)).
    We'll not spend much time on how TensorFlow works, but you can refer to the official
    tutorial, available at [https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html), if
    you are interested in looking under the hood of that technology.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow实现一个简单架构的神经网络，只有两层，该网络将在我们提供的MNIST数据集([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))上进行回归。我们可以（并且会）在后续的项目中深入探讨架构！我们假设你已经熟悉反向传播（如果没有，请阅读Michal
    Nielsen关于反向传播的文章，链接为[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)）。我们不会花太多时间讲解TensorFlow的工作原理，但如果你有兴趣深入了解这项技术的底层实现，可以参考官方教程，网址为[https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html](https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html)。
- en: Exploring the MNIST dataset
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索MNIST数据集
- en: Before we jump into building our awesome neural network, let's first have a
    look at the famous MNIST dataset. So let's visualize the MNIST dataset in this
    section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建我们令人惊叹的神经网络之前，让我们先来看一下著名的MNIST数据集。所以让我们在本节中可视化MNIST数据集。
- en: '**Words of wisdom**: You must know your data and how it has been preprocessed,
    in order to know why the models you build perform the way they do. This section
    reviews the significant work that has been done in preparation on the dataset,
    to make our current job of building the MLP easier. Always remember: data science
    begins with DATA!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**智慧之言**：你必须了解你的数据以及它是如何预处理的，这样你才能知道为什么你所构建的模型会有那样的表现。本节回顾了数据集准备过程中所做的重要工作，这使得我们当前构建MLP的任务更加轻松。永远记住：数据科学始于数据！'
- en: 'Let''s start therefore by downloading the data, using the following commands:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们通过以下命令开始下载数据：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we examine the `mnist` variable content, we can see that it is structured
    in a specific format, with three major components—**TRAIN**, **TEST**, and **VALIDATION**.
    Each set has handwritten images and their respective labels. The images are stored
    in a flattened way as a single vector:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查`mnist`变量的内容，我们可以看到它以特定格式构建，包含三个主要部分——**TRAIN**（训练集）、**TEST**（测试集）和**VALIDATION**（验证集）。每个集合都有手写图像及其相应的标签。图像以扁平化的方式存储为单个向量：
- en: '![](img/8aa4383e-28f6-4c9d-b7bd-064a4d75da37.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8aa4383e-28f6-4c9d-b7bd-064a4d75da37.png)'
- en: The format of the MNIST dataset
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的格式
- en: 'Let''s extract one image from the dataset and plot it. Since the stored shape
    of a single image matrix is `[1,784]`, we need to reshape these vectors into `[28,28]`
    to visualize the original image:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据集中提取一张图像并绘制它。由于单张图像矩阵的存储形状为`[1,784]`，我们需要将这些向量重塑为`[28,28]`以可视化原始图像：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we have the image matrix, we will use `matplotlib` to plot it, as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了图像矩阵，我们将使用`matplotlib`来绘制它，如下所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output will be as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/e396298d-51ec-46f8-969d-f86bf1a17a99.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e396298d-51ec-46f8-969d-f86bf1a17a99.png)'
- en: A sample of the MNIST dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的一个示例
- en: In the same vein as this image, there are a total of 55,000 similar images of
    handwritten digits [0-9]. The labels in the MNIST dataset are the true value of
    the digits present in the image. Our objective, then, is to train a model with
    this set of images and labels, so that it can predict the labels of any image provided from
    the MNIST dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与这张图类似，共有55,000张类似的手写数字[0-9]图像。MNIST 数据集中的标签是图像中数字的真实值。因此，我们的目标是通过这组图像和标签训练一个模型，使其能够预测任何来自
    MNIST 数据集的图像的标签。
- en: '**Be a deep learning explorer**: If you are interested in playing around with
    the dataset, you can try the Colab Notebook, available at [https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing](https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**成为深度学习探索者**：如果你有兴趣操作数据集，你可以尝试使用 Colab Notebook，链接地址为 [https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing](https://drive.google.com/file/d/1-GVlob72EyiJyQpk8EL2fg2mvzaEayJ_/view?usp=sharing)。'
- en: Intuition and preparation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直觉与准备
- en: Let's build our intuition around this project. What we need to do is build a
    deep learning technology that accurately assigns class labels to an input image.
    We're using a deep neural network, known as an MLP, to do this. The core of this
    technology is the mathematics of regression. The specific calculus proofs are
    outside the scope of this book, but in this section, we provide a foundational
    basis for your understanding. We also outline the structure of the project, so
    that it's easy to understand the primary steps needed to create our desired results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们围绕这个项目建立直觉。我们需要做的是构建一个深度学习技术，它能准确地为输入图像分配类别标签。我们使用的深度神经网络被称为 MLP。这个技术的核心是回归的数学原理。具体的微积分证明超出了本书的范围，但在这一部分，我们为你提供了一个理解的基础。我们还概述了项目的结构，以便你能轻松理解创建我们想要的结果所需的主要步骤。
- en: Defining regression
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义回归
- en: Our first task is to define the model that will perform regression on the provided
    MNIST dataset. So, we will create a TensorFlow model with two hidden layers as
    part of a fully connected neural network. You may also hear it referred to as
    MLP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个任务是定义一个将在提供的 MNIST 数据集上执行回归的模型。因此，我们将创建一个具有两个隐藏层的 TensorFlow 模型，作为一个完全连接的神经网络的一部分。你可能会听到它被称为
    MLP。
- en: 'The model will perform the operation that will fit the following equation,
    where *y* is the label, *x* is the image, *W* is the weight that the model will
    learn, and *b* is the bias, which will also be learned by the model, following
    is the regression equation for the model:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将执行以下方程的操作，其中 *y* 是标签，*x* 是图像，*W* 是模型将学习的权重，*b* 是偏置，模型也会学习它，以下是该模型的回归方程：
- en: '![](img/1a9fcc2d-2c3f-4db7-8253-0e83d069b765.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a9fcc2d-2c3f-4db7-8253-0e83d069b765.png)'
- en: The regression equation for the model
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的回归方程
- en: '**Supervised learning**: When you have data and accurate labels for the training
    set (that is, you know the answer), you are in a supervised deep learning paradigm.
    Model training is a mathematical process by which the features of the data are
    learned and associated with the proper labels, so that when a new data point (test
    data) is presented, the accurate output class label can be produced. In other
    words, when you present a new data point and do not have the label (that is, you
    don''t know the answer), your model can produce it for you with a highly reliable
    class prediction.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**：当你有数据和准确的标签用于训练集（也就是说，你知道答案）时，你处于一个监督式深度学习的范式中。模型训练是一个数学过程，通过这个过程，数据的特征被学习并与正确的标签关联，以便当一个新的数据点（测试数据）被呈现时，能够输出准确的类别标签。换句话说，当你提供一个新的数据点并且没有标签（也就是说，你不知道答案）时，你的模型可以通过高度可靠的类别预测为你生成标签。'
- en: Each iteration will try to generalize the values of weight and bias and reduce
    the error rate. Also, keep in mind that we need to ensure that the model is not
    overfitting, which may lead to wrong predictions for the unseen dataset. We'll
    show you how to code this and visualize the progress to aid in your intuition
    of model performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代将尝试概括权重和偏置的值，并减少误差率。同时，请记住，我们需要确保模型不会发生过拟合，这可能导致对未见数据集的错误预测。我们将向你展示如何编写代码并可视化进度，以帮助你理解模型性能。
- en: Defining the project structure
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义项目结构
- en: 'Let''s structure our project as shown in the following pattern:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下模式组织我们的项目：
- en: '`hy_param.py`: All the hyperparameters and other configurations are defined
    here'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hy_param.py`：所有的超参数和其他配置都在这里定义'
- en: '`model.py`: The definition and architecture of the model are defined here'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.py`：模型的定义和架构在这里进行定义'
- en: '`train.py`: The code to train the model is written here'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.py`：用于训练模型的代码在这里编写'
- en: '`inference.py`: The code to execute the trained model and make predictions
    is defined here'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inference.py`：执行训练好的模型并进行预测的代码在这里定义'
- en: '`/runs`: This folder will store all of the checkpoints that get created during
    the training process'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/runs`：此文件夹将存储在训练过程中创建的所有检查点'
- en: You can clone the code from the repository—the code for this can be found in
    the `Chapter02` folder, available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从代码库克隆代码——代码可以在`Chapter02`文件夹中找到，链接地址是[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/)。
- en: Let's code the implementation!
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们开始编写实现代码吧！
- en: To code the implementation, we'll start by defining the hyperparameters, then
    we will define the model, followed by building and executing the training loop.
    We conclude by checking to see if our model is overfitting and build an inference
    code that loads the latest checkpoints and then makes predictions on the basis
    of learned parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现代码，我们首先会定义超参数，然后定义模型，接着构建并执行训练循环。最后，我们检查模型是否过拟合，并编写推理代码，加载最新的检查点，然后基于学习到的参数进行预测。
- en: Defining hyperparameters
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'We will define all of the required hyperparameters in the `hy_param.py` file
    and then import it as a module in our other codes. This makes it easy in deployment,
    and is good practice to make your code as modular as possible. Let''s look into
    the hyperparameter configurations that we have in our `hy_param.py` file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`hy_param.py`文件中定义所有需要的超参数，并在其他代码中导入它作为模块。这使得部署变得更加方便，并且将代码模块化是一个良好的实践。让我们看看在`hy_param.py`文件中定义的超参数配置：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will be using these values throughout our code, and they're totally configurable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整个代码中使用这些值，并且它们是完全可配置的。
- en: As a Python deep learning projects exploration opportunity, we invite you, our
    project teammate and reader, to try different values of learning rate and numbers
    of hidden layers to experiment and build better models!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个Python深度学习项目的探索机会，我们邀请你，作为我们的项目团队成员和读者，尝试不同的学习率和隐藏层数量，进行实验并构建更好的模型！
- en: Since the flat vectors of images shown previously are of a size of [1 x 786],
    the `num_input=784` is fixed in this case. In addition, the class count in the
    MNIST dataset is `10`. We have digits from 0-9, so obviously we have `num_classes=10`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面展示的图像的平坦向量大小为[1 x 786]，因此`num_input=784`在这种情况下是固定的。此外，MNIST数据集中的类别数为`10`。我们有0到9的数字，因此显然我们有`num_classes=10`。
- en: Model definition
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型定义
- en: 'First, we will load the Python modules; in this case, the TensorFlow package
    and the hyperparameters that we defined previously:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载Python模块；在这种情况下，加载TensorFlow包以及我们之前定义的超参数：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we define the placeholders that we will be using to input data into the
    model. `tf.placeholder` allows us to feed input data to the computational graph.
    We can define constraints with the shape of the placeholder to only accept a tensor
    of a certain shape. Note that it is common to provide `None` for the first dimension,
    which allows us to the size of the batch at runtime.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义将用于输入数据到模型中的占位符。`tf.placeholder`允许我们将输入数据传递给计算图。我们可以定义占位符的形状约束，以便仅接受特定形状的张量。请注意，通常会为第一个维度提供`None`，这允许我们在运行时动态指定批次大小。
- en: '**Master your craft**: Batch size can often have a big impact on the performance
    of deep learning models. Explore different batch sizes in this project. What changes
    as a result? What''s your intuition? Batch size is another tool in your data science
    toolkit!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**精通你的技艺**：批处理大小往往对深度学习模型的性能有很大影响。在这个项目中，尝试不同的批处理大小。结果会有什么变化？你的直觉是什么？批处理大小是你数据科学工具箱中的另一个工具！'
- en: 'We have also assigned names to the placeholders, so that we can use them later
    on while building our inference code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还给占位符分配了名称，以便在构建推理代码时可以使用它们：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we will define variables that will hold values for weights and bias. `tf.Variable` allows
    us to store and update tensors in our graph. To initialize our variables with
    random values from a normal distribution, we will use `tf.random_normal()` (more
    details can be found at [https://www.tensorflow.org/api_docs/python/tf/random_normal](https://www.tensorflow.org/api_docs/python/tf/random_normal)).
    The important thing to notice here is the mapping variable size between layers:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义保存权重和偏置值的变量。`tf.Variable`允许我们在图中存储并更新张量。为了使用正态分布的随机值初始化变量，我们将使用`tf.random_normal()`（更多细节可以在[https://www.tensorflow.org/api_docs/python/tf/random_normal](https://www.tensorflow.org/api_docs/python/tf/random_normal)查看）。这里需要注意的重点是层之间的映射变量大小：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s set up the operation that we defined in the equation earlier in
    this chapter. This is the logistic regression operation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置之前在本章中定义的操作。这是逻辑回归操作：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The logistic values are converted into the probabilistic values using `tf.nn.softmax()`. The
    softmax activation squashes the output values of each unit to a value between
    zero and one:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.nn.softmax()`将逻辑值转换为概率值。Softmax激活将每个单元的输出值压缩到0和1之间：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, let''s use `tf.nn.softmax_cross_entropy_with_logits` to define our cost
    function. We will optimize our performance using the Adam Optimizer. Finally,
    we can use the built-in `minimize()` function to calculate the **stochastic gradient
    descent** (**SGD**) update rule for each parameter in our network:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`tf.nn.softmax_cross_entropy_with_logits`来定义我们的成本函数。我们将使用Adam优化器优化性能。最后，我们可以使用内置的`minimize()`函数来计算每个网络参数的**随机梯度下降**（**SGD**）更新规则：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we make our prediction. These functions are needed to calculate and capture
    the accuracy values in a batch:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行预测。为了计算并捕获批次中的准确度值，以下函数是必需的：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The complete code is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码如下：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Hurray! The heavy lifting part of the code is done. We save the model code
    in the `model.py` file. So, up until now, we''ve defined the simple two-hidden-layer
    model architecture, with 300 neurons in each layer, which will try to learn the
    best weight distribution using the Adam Optimizer and predict the probability
    of ten classes. These layers are shown in the following diagram:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！代码的繁重部分已经完成。我们将模型代码保存在`model.py`文件中。到目前为止，我们已经定义了一个简单的两层隐藏层模型架构，每层有300个神经元，使用Adam优化器来学习最佳权重分布，并预测十个类别的概率。以下是这些层的示意图：
- en: '![](img/c0616a53-71bd-456e-8dd2-c56116074037.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0616a53-71bd-456e-8dd2-c56116074037.png)'
- en: An illustration of the model that we created
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的模型示意图
- en: Building the training loop
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建训练循环
- en: The next step is to utilize the model for training, and record the learned model
    parameters, which we will accomplish in `train.py`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是利用模型进行训练，并记录已学习的模型参数，我们将在`train.py`中完成此任务。
- en: 'Let''s start by importing the dependencies:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入所需的依赖项：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we define the variables that we require to be fed into our MLP:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义需要输入到MLP中的变量：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s create the folder to which we will save our checkpoints. Checkpoints
    are basically the intermediate steps that capture the values of `W` and `b` in
    the process of learning. Then, we will use the `tf.train.Saver()` function (more
    details on this function can be found at [https://www.tensorflow.org/api_docs/python/tf/train/Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver))
    to save and restore `checkpoints`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个文件夹，用于保存我们的检查点。检查点基本上是学习过程中捕获`W`和`b`值的中间步骤。然后，我们将使用`tf.train.Saver()`函数（更多详情请见[https://www.tensorflow.org/api_docs/python/tf/train/Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver)）来保存和恢复`checkpoints`：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to begin training, we need to create a new session in TensorFlow.
    In this session, we''ll initialize the graph variables and feed the model operations
    the valid data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始训练，我们需要在TensorFlow中创建一个新的会话。在此会话中，我们将初始化图中的变量，并将有效数据提供给模型操作：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will extract batches of 128 training image-label pairs from the MNIST dataset
    and feed them into the model. After subsequent steps or epochs, we will store
    the checkpoints using the `saver` operation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从MNIST数据集中提取128个训练图像-标签对，并将其输入到模型中。经过后续步骤或多个epoch后，我们将使用`saver`操作存储检查点：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once we have executed the `train.py` file, you will see the progress on your
    console, as shown in the preceding screenshot. This depicts the loss being reduced
    after every step, along with accuracy increasing over each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们执行了 `train.py` 文件，你将在控制台上看到进度，如前面的截图所示。这显示了每一步损失的减少，以及准确率的提升：
- en: '![](img/d036ba50-4562-4694-95c1-e5c7dd2152a6.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d036ba50-4562-4694-95c1-e5c7dd2152a6.png)'
- en: The training epoch's output with minibatch loss and training accuracy parameters
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时期的输出，包括小批量损失和训练准确率参数
- en: 'Also, you can see in the plot of minibatch loss, shown in the following diagram, that
    it approaches toward the minima with each step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以在下面的图表中看到小批量损失的变化，随着每一步的推进，它逐渐接近最小值：
- en: '![](img/f77e584b-d4d2-4679-8406-d44b89911846.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f77e584b-d4d2-4679-8406-d44b89911846.png)'
- en: Plotting the loss values computed at each step
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制每一步计算出的损失值
- en: It is very important to visualize how your model is performing, so that you
    can analyze and prevent it from underfitting or overfitting. Overfitting is a
    very common scenario when you are dealing with the deeper models. Let's spend
    some time getting to understand them in detail and learning a few tricks to overcome
    them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化模型的表现非常重要，这样你就可以分析并防止模型出现欠拟合或过拟合。过拟合是在处理深度模型时非常常见的情况。让我们花些时间详细了解它们，并学习一些克服它们的技巧。
- en: Overfitting and underfitting
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: With great power comes great responsibility and with deeper models come deeper
    problems. A fundamental challenge with deep learning is striking the right balance
    between generalization and optimization. In the deep learning process, we are
    tuning hyperparameters and often continuously configuring and tweaking the model
    to produce the best results, based on the data we have for training. This is **optimization**.
    The key question is, how well does our model generalize in performing predictions
    on unseen data?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的能力带来巨大的责任，而更深的模型也带来了更深的问题。深度学习的一个基本挑战是找到泛化和优化之间的正确平衡。在深度学习过程中，我们正在调整超参数，并经常不断配置和调整模型，以便基于我们用于训练的数据生成最佳结果。这就是
    **优化**。关键问题是，我们的模型在对未见过的数据进行预测时，泛化能力如何？
- en: 'As professional deep learning engineers, our goal is to build models with good
    real-world generalization. However, generalization is subjective to the model
    architecture and the training dataset. We work to guide our model for maximum
    utility by reducing the likelihood that it learns irrelevant patterns or simple
    similar patterns found in the data used for training. If this is not done, it
    can affect the generalization process. A good solution is to provide the model
    with more information that is likely to have a better (that is, more complete
    and often complex) signal of what you''re trying to actually model, by getting more
    data to train on and to work to optimize the model architecture. Here are few
    quick tricks that can improve your model by preventing overfitting:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 作为专业的深度学习工程师，我们的目标是构建具有良好实际世界泛化能力的模型。然而，泛化能力受到模型架构和训练数据集的影响。我们通过减少模型学习到与目标无关的模式或仅仅是数据中相似的简单模式的可能性来引导模型达到最大的实用性。如果没有做到这一点，它可能会影响泛化过程。一个好的解决方案是通过获取更多的数据来训练模型，并优化模型架构，提供模型可能具有更好（即更完整且通常更复杂）信号的更多信息，这有助于你真正想要建模的内容。以下是一些防止过拟合的快速技巧，可以提高你的模型：
- en: Getting more data for training
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更多用于训练的数据
- en: Reducing network capacity by altering the number of layers or nodes
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调整层数或节点数来减少网络容量
- en: Employing L2 (and trying L1) weight regularization techniques
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 L2（并尝试 L1）权重正则化技术
- en: Adding dropout layers or polling layers in the model
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型中添加丢弃层或池化层
- en: L1 regularization, where the cost added is proportional to the absolute value
    of the weights coefficients, is also known as *L1 norm*. L2 regularization, where
    the cost added is proportional to the square of the value of the weight's coefficients,
    is also known as *L2 norm* or *weight decay.*
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化，其中附加的代价与权重系数的绝对值成正比，也称为 *L1 范数*。L2 正则化，其中附加的代价与权重系数值的平方成正比，也称为 *L2 范数*
    或 *权重衰减*。
- en: 'When the model gets trained completely, its output, as checkpoints, will get
    dumped into the `/runs` folder, which will contain the binary dump of `checkpoints`,
    as shown in the following screenshot:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型完全训练完毕后，其输出会作为检查点被存入 `/runs` 文件夹，其中包含 `checkpoints` 的二进制转储，如下面的截图所示：
- en: '![](img/1a78e26a-0e10-4b5a-ae44-43f3ca99e16f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a78e26a-0e10-4b5a-ae44-43f3ca99e16f.png)'
- en: The checkpoint folder after the training process is completed
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程完成后的检查点文件夹
- en: Building inference
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建推理
- en: 'Now, we will create an inference code that loads the latest checkpoints and
    then makes predictions on the basis of learned parameters. For that, we need to
    create a `saver` operation that will pick the latest checkpoints and load the metadata.
    Metadata contains the information regarding the variables and the nodes that we
    created in the graph:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个推理代码，加载最新的检查点，然后根据已学习的参数进行预测。为此，我们需要创建一个`saver`操作，提取最新的检查点并加载元数据。元数据包含有关我们在图表中创建的变量和节点的信息：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We know the importance of this, because we want to load similar variables and
    operations back from the stored checkpoint. We load them into memory using `tf.get_default_graph().get_operation_by_name()`*, *by
    passing the operation name in the parameter that we defined in the model:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这一点的重要性，因为我们希望从存储的检查点加载类似的变量和操作。我们通过使用`tf.get_default_graph().get_operation_by_name()`将它们加载到内存中，*通过传递在模型中定义的操作名称作为参数：*
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we need to initialize the session and pass data for a test image to the
    operation that makes the prediction, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要初始化会话，并将测试图像的数据传递给执行预测操作的代码，如下所示：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Following is the full code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整代码：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And with that, we are done with our first project that predicts the digits
    provided in a handwritten image! Here are some of the results that the model predicted
    when provided with the test image from the MNIST dataset:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了第一个项目：预测手写图像中的数字！以下是模型在接收来自MNIST数据集的测试图像时所做的一些预测结果：
- en: '![](img/c276e6f7-1b28-4bab-9be0-94d4680b1928.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c276e6f7-1b28-4bab-9be0-94d4680b1928.png)'
- en: The output of the model, depicting the prediction of the model and the input
    image
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出，展示了模型的预测和输入图像
- en: Concluding the project
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目总结
- en: Today's project was to build a classifier to solve the problem of identifying
    specific handwriting samples from a dataset of images. Our hypothetical use case
    was to apply deep learning to enable customers of a restaurant chain to write
    their phone numbers in a simple iPad application, so that they could get a text
    notification that their party was ready to be seated. Our specific task was to
    build the intelligence that would drive this application.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的项目是构建一个分类器，解决从图像数据集中识别特定手写样本的问题。我们的假设使用场景是应用深度学习技术，让餐厅连锁的顾客可以在简单的iPad应用中写下他们的电话号码，从而收到通知，告知他们的桌位已准备好。我们的具体任务是构建能驱动这个应用的智能系统。
- en: '**Revisit our success criteria**: How did we do? Did we succeed? What was the
    impact of our success? Just as we defined success at the beginning of the project,
    these are the key questions that we need to ask as deep learning data scientists,
    as we look to wrap up a project.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**重新审视我们的成功标准**：我们做得怎么样？我们成功了吗？我们的成功有什么影响？正如我们在项目开始时定义的那样，这些是我们作为深度学习数据科学家需要问的关键问题，尤其是当我们准备结束一个项目时。'
- en: Our MLP model accuracy hit 87.42%! Not bad, given the depth of the model and
    the hyperparameters that we chose at the beginning. See if you can tweak the model
    to get an even higher test set accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的MLP模型准确度达到了87.42%！考虑到模型的深度和我们在开始时选择的超参数，这个成绩还不错。看看你能否调整模型，获得更高的测试集准确度。
- en: What are the implications of this accuracy? Let's calculate the incidence of
    an error occurring that would result in a customer service issue (that is, the
    customer not getting the text that their table is ready, and getting upset due
    to an excessively long wait time at the restaurant).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个准确度意味着什么？我们来计算一下错误发生的概率，导致客户服务问题（也就是客户没有收到“桌位已准备好”的短信，并因为在餐厅等待时间过长而感到不满）。
- en: Each customer's phone number is ten digits long. Let's say that our hypothetical
    restaurant has an average of 30 tables at each location, and those tables turn
    over two times per night during the rush hour, when the system is likely to be
    used, and finally, the restaurant chain has 35 locations. This means that each
    day of operation, there are approximately 21,000 handwritten numbers captured
    (30 tables x 2 turns/day x 35 locations x 10 digit phone number).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 每个顾客的电话号码是十位数。假设我们的假设餐厅在每个地点平均有30张桌子，并且这些桌子在高峰时段每晚翻台两次，而此时系统正处于高使用频率，最后，这个餐厅连锁有35个分店。这意味着每天运营中大约会捕捉到21,000个手写数字（30张桌子
    x 每天翻台2次 x 35个分店 x 10位电话号码）。
- en: Obviously, all digits must be correctly classified for the text to get to the
    waiting restaurant patron. So, any single digit misclassification causes a failure.
    A model accuracy of 87.42% would improperly classify 2,642 digits per day in our
    example. The worst case for the hypothetical scenario would be if there occurred
    only one improperly classified digit in each phone number. Since there are only
    2,100 patrons and corresponding phone numbers, this would mean that every phone
    number had an error in classification (a 100% failure rate), and not a single
    customer would get their text notification that their party could be seated! The
    best case, in this scenario, would be if all 10 digits were misclassified in each
    phone number, which would result in 263 wrong phone numbers out of 2,100 (a 12.5%
    failure rate). This is still not a level of performance that the restaurant chain
    would be likely be happy with.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所有数字必须被正确分类，才能将信息传递给等待的餐厅顾客。因此，任何单一数字的错误分类都会导致失败。在我们的示例中，87.42%的模型准确率将导致每天错误分类2,642个数字。假设的最坏情况是，每个电话号码中只有一个数字被错误分类。由于只有2,100个顾客和相应的电话号码，这意味着每个电话号码都会有一个错误分类（100%的失败率），没有一个顾客能收到他们的座位通知！在这种情况下，最佳情景是每个电话号码中的所有10个数字都被错误分类，这将导致2,100个号码中有263个错误电话号码（12.5%的失败率）。即使在这种情况下，餐厅连锁也不太可能满意这一性能水平。
- en: '**Words of wisdom**: Model performance may not equal system or app performance.
    Many factors contribute to a system being robust or fragile in the real world.
    Model performance is a key factor, but other items with individual fault tolerances
    definitely play a part. Know how your deep learning models integrate into the
    larger project so that you can set proper expectations!'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**箴言**：模型表现不一定等同于系统或应用程序的表现。许多因素决定了系统在现实世界中的鲁棒性或脆弱性。模型表现是一个关键因素，但其他具有独立容错能力的因素同样发挥着作用。了解你的深度学习模型如何融入更大的项目中，这样你才能设定正确的期望！'
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In the project in this chapter, we successfully built an MLP to produce a regression
    classification prediction, based on handwritten digits. We gained experience with
    the MNIST dataset and a deep neural network model architecture, which gave us
    the added opportunity to define some key hyperparameters. Finally, we looked at
    the model performance in testing and determined whether we succeeded in achieving
    our goals.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的项目中，我们成功构建了一个多层感知机（MLP），用来基于手写数字进行回归分类预测。我们通过使用MNIST数据集和深度神经网络模型架构积累了经验，同时也有机会定义一些关键的超参数。最后，我们对模型在测试中的表现进行了评估，判断是否达成了我们的目标。
