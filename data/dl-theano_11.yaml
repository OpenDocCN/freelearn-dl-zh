- en: Chapter 11. Learning from the Environment with Reinforcement
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 从环境中学习强化
- en: Supervised and unsupervised learning describe the presence or the absence of
    labels or targets during training. A more natural learning environment for an
    agent is to receive rewards when the correct decision has been taken. This reward,
    such as *playing correctly tennis* for example, may be attributed in a complex
    environment, and the result of multiple actions, delayed or cumulative.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习描述了训练过程中标签或目标的存在与否。对于代理来说，更自然的学习环境是在正确决策时获得奖励。在复杂环境中，这种奖励，例如*正确打网球*，可能是多个动作的结果，延迟或累积。
- en: In order to optimize the reward from the environment for an artificial agent,
    the **Reinforcement Learning** (**RL**) field has seen the emergence of many algorithms,
    such as Q-learning, or Monte Carlo Tree Search, and with the advent of deep learning,
    these algorithms have been revised into new methods, such as deep-Q-networks,
    policy networks, value networks, and policy gradients.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化人工代理从环境中获取的奖励，**强化学习**（**RL**）领域出现了许多算法，如Q学习或蒙特卡洛树搜索，并且随着深度学习的出现，这些算法已经演变为新的方法，如深度Q网络，策略网络，值网络和策略梯度。
- en: We'll begin with a presentation of the reinforcement learning frame, and its
    potential application to virtual environments. Then, we'll develop its algorithms
    and their integration with deep learning, which has solved the most challenging
    problems in artificial intelligence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍强化学习框架及其在虚拟环境中的潜在应用。然后，我们将发展其算法及其与深度学习的整合，后者解决了人工智能中最具挑战性的问题。
- en: 'The points covered in this chapter are the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的要点如下：
- en: Reinforcement learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Simulation environments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟环境
- en: Q-learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: Monte Carlo Tree Search
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索
- en: Deep Q-networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: Policy gradients
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度
- en: Asynchronous gradient descents
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步梯度下降
- en: To simplify the development of our neural nets in this chapter, we'll use Keras,
    the high level deep learning library on top of Theano I presented in [Chapter
    5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 5. Analyzing
    Sentiment with a Bidirectional LSTM"), *Analyzing Sentiment with a Bidirectional
    LSTM*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化本章中神经网络的开发，我们将使用Keras，这是基于我在[第5章](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 5. Analyzing Sentiment with a Bidirectional LSTM")中介绍的Theano之上的高级深度学习库，*使用双向LSTM分析情感*。
- en: Reinforcement learning tasks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习任务
- en: Reinforcement learning consists of training an **agent,** that just needs occasional
    feedback from the **environment**, to learn to get the best feedback at the end.
    The agent performs **actions**, modifying the **state** of the environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习包括训练一个**代理**，它只需偶尔从**环境**中获得反馈，就可以学会在结束时获得最佳反馈。代理执行**动作**，修改环境的**状态**。
- en: 'The actions to navigate in the environment can be represented as directed edges
    from one state to another state as a graph, as shown in the following figure:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境中导航的动作可以表示为从一个状态到另一个状态的有向边，如下图所示：
- en: '![Reinforcement learning tasks](img/00201.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习任务](img/00201.jpeg)'
- en: 'A robot, working in a real environment (walking robots, control of motors,
    and so on) or a virtual environment (video game, online games, chat room, and
    so on) has to decide which movements (or keys to strike) to receive the maximum
    reward:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器人在真实环境中工作（步行机器人，电机控制等）或虚拟环境中（视频游戏，在线游戏，聊天室等），必须决定哪些动作（或按键）可以获得最大的奖励：
- en: '![Reinforcement learning tasks](img/00202.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习任务](img/00202.jpeg)'
- en: Simulation environments
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟环境
- en: Virtual environments make it possible to simulate thousands to millions of gameplays,
    at no other cost than the computations. For the purpose of benchmarking different
    reinforcement learning algorithms, simulation environments have been developed
    by the research community.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟环境使得能够模拟数以万计甚至百万级的游戏过程，仅需计算成本。为了评估不同强化学习算法的性能，研究界开发了模拟环境。
- en: 'In order to find the solutions that generalize well, the Open-AI non-profit
    artificial intelligence research company, associated with business magnate Elon
    Musk, that aims to carefully promote and develop friendly AI in such a way as
    to benefit humanity as a whole, has gathered in its open source simulation environment,
    **Open-AI Gym** ([https://gym.openai.com/](https://gym.openai.com/)), a collection
    of reinforcement tasks and environments in a Python toolkit to test our own approaches
    on them. Among these environments, you''ll find:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到能够很好地泛化的解决方案，Open-AI，一个由商业巨头埃隆·马斯克支持的非盈利人工智能研究公司，致力于以有益于全人类的方式小心推广和开发友好的人工智能，已经在其开源模拟环境**Open-AI
    Gym**（[https://gym.openai.com/](https://gym.openai.com/)）中收集了一系列强化学习任务和环境，供我们在其中测试自己的方法。在这些环境中，你会找到：
- en: Video games from Atari 2600, a home video game console released by Atari Inc
    in 1977, wrapping the simulator from the Arcade Learning Environment, one of the
    most common RL benchmark environment:![Simulation environments](img/00203.jpeg)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Atari 2600的视频游戏，Atari公司于1977年发布的家庭视频游戏机，封装了来自街机学习环境的模拟器，这是最常见的强化学习基准环境之一：![模拟环境](img/00203.jpeg)
- en: MuJoCo, a physics simulator for evaluating agents on continuous control tasks:![Simulation
    environments](img/00204.jpeg)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCo，评估智能体在连续控制任务中的物理模拟器：![模拟环境](img/00204.jpeg)
- en: Other well-known games such as Minecraft, Soccer, Doom, and many others:![Simulation
    environments](img/00205.jpeg)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他著名游戏，如Minecraft、足球、Doom等：![模拟环境](img/00205.jpeg)
- en: 'Let''s install Gym and its Atari 2600 environment:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装Gym及其Atari 2600环境：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is also possible to install all environments with:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过以下方式安装所有环境：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Interacting with the gym environment is pretty simple with the `env.step()`
    method that, given an action we choose for the agent, returns the new state, the
    reward, and whether the game has terminated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与Gym环境交互非常简单，只需使用`env.step()`方法，给定我们为智能体选择的动作，该方法返回新的状态、奖励，以及游戏是否已结束。
- en: 'For example, let''s sample a random action:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们采样一个随机动作：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Gym also provides sophisticated monitoring methods, to record videos and algorithm
    performance. The records can be uploaded to Open-AI API for scoring with other
    algorithms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Gym还提供了复杂的监控方法，可以录制视频和算法表现。这些记录可以上传到Open-AI API，与其他算法一起评分。
- en: 'One might also look at:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以看看：
- en: 3D car racing simulator Torcs ([http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)),
    which is more realistic with smaller discretization of actions, but with less
    sparse rewards than simple Atari games, and also fewer possible actions than continuous
    motor control in MuJoCo:![Simulation environments](img/00206.jpeg)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D赛车模拟器Torcs（[http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)），比起简单的Atari游戏，它在动作的离散化上更小，更加真实，但奖励更稀疏，而且比MuJoCo中的连续电机控制动作还少：![模拟环境](img/00206.jpeg)
- en: 3D environment called Labyrinth for randomly generated mazes:![Simulation environments](img/00207.jpeg)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为迷宫的3D环境，用于随机生成迷宫：![模拟环境](img/00207.jpeg)
- en: Q-learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习
- en: 'A major approach to solve games has been the Q-learning approach. In order
    to fully understand the approach, a basic example will illustrate a simplistic
    case where the number of states of the environment is limited to 6, state **0**
    is the entrance, state **5** is the exit. At each stage, some actions make it
    possible to jump to another state, as described in the following figure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 解决游戏的一个主要方法是Q学习方法。为了完全理解这一方法，以下是一个简单的例子，环境的状态数限制为6，状态**0**为入口，状态**5**为出口。在每个阶段，一些动作可以使智能体跳到另一个状态，如下图所示：
- en: '![Q-learning](img/00208.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习](img/00208.jpeg)'
- en: The reward is, let's say, 100, when the agent leaves state **4** to state **5**.
    There isn't any other reward for other states since the goal of the game in this
    example is to find the exit. The reward is time-delayed and the agent has to scroll
    through multiple states from state 0 to state 4 to find the exit.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体从状态**4**跳到状态**5**时，奖励是100。在其他状态中没有奖励，因为在这个例子中游戏的目标是找到出口。奖励是时间延迟的，智能体必须从状态0滚动通过多个状态到达状态4，才能找到出口。
- en: 'In this case, Q-learning consists of learning a matrix Q, representing the
    **value of a state-action pair**:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Q学习的任务是学习一个Q矩阵，表示**状态-动作对的价值**：
- en: Each row in the Q-matrix corresponds to a state the agent would be in
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q矩阵的每一行对应智能体可能处于的一个状态
- en: Each column the target state from that state
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一列表示从该状态出发到达的目标状态
- en: the value representing how much choosing that action in that state will move
    us close to the exit. If there isn't any action from state *i* leading to state
    *j*, we define a zero or negative value at position *(i,j)* in the Q-matrix. If
    there are one or more possible actions from state *i* to state *j*, then the value
    in the Q-matrix will be chosen to represent how state *j* will help us to achieve
    our goal.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代表选择该状态中的行动将如何使我们接近出口的价值。如果从状态*i*到状态*j*没有任何行动，我们在Q矩阵的位置*(i,j)*定义为零或负值。如果从状态*i*到状态*j*有一个或多个可能的行动，那么Q矩阵中的值将被选择来表示状态*j*如何帮助我们实现目标。
- en: 'For example, leaving state **3** for state **0**, will move the agent away
    from the exit, while leaving state **3** for state **4** gets us closer to the
    goal. A commonly used algorithm, known as a *greedy* algorithm, to estimate **Q**
    in the discrete space, is given by the recursive *Bellman equation* which is demonstrated
    to converge:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，离开状态3到状态0将使代理远离出口，而离开状态3到状态4将使我们更接近目标。一个常用的算法，称为*贪婪*算法，在离散空间中估计**Q**，由递归的*贝尔曼方程*给出，已被证明收敛：
- en: '![Q-learning](img/00209.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习](img/00209.jpeg)'
- en: 'Here, *S''* is the new state when taking action *a* on state *S*; *r* defines
    the reward on the path from state *S* to *S''* (in this case it is null) and ![Q-learning](img/00210.jpeg)
    is the discounting factor to discourage actions to states too far in the graph.
    The application of this equation multiple times will result in the following Q
    values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*S'*是在状态*S*上执行动作*a*后的新状态；*r*定义了从状态*S*到*S'*路径上的奖励（在这种情况下为空），![Q学习](img/00210.jpeg)是折扣因子，用于阻止到图中距离太远的状态的动作。多次应用该方程将导致以下Q值：
- en: '![Q-learning](img/00211.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习](img/00211.jpeg)'
- en: In Q-learning, *Q* stands for *quality* representing the power of the action
    to get the best rewards. Since late rewards are discounted, the values correspond
    to **maximum discounted future rewards** for each (state, action) couple.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，*Q*代表质量，表示行动获得最佳奖励的能力。由于延迟奖励被折扣，这些值对应于每对（状态，行动）的**最大折扣未来奖励**。
- en: 'Note that the full graph outcome is not required as soon as we know the **state
    values** for the output nodes of the search subtree:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只要我们知道搜索子树的输出节点的状态值，完整图形的结果就不是必需的：
- en: '![Q-learning](img/00212.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习](img/00212.jpeg)'
- en: In this figure, the value **10** for nodes **1** and **3** are the **optimal
    state value function v(s);** that is, the outcome of a game under perfect play
    / optimal path. In practice, the exact value function is not known but approximated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，节点1和3的值为10是**最优状态值函数v(s)**；也就是说，在完美游戏/最佳路径下的游戏结果。实际上，确切的值函数是未知的，但是是近似的。
- en: 'Such an approximation is used in combination with a **Monte Carlo Tree Search**
    (**MCTS**) in the **DeepMind** algorithm **AlphaGo** to beat the world champion
    in Go. MCTS consists of sampling actions given a policy, so that only the most
    likely actions from the current node to estimate its Q-value are retained in the
    Bellman equation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种近似与DeepMind算法AlphaGo中的**蒙特卡洛树搜索(MCTS)**结合使用，以击败围棋世界冠军。MCTS包括在给定策略下对动作进行抽样，从而仅保留当前节点到估计其Q值的最可能动作在贝尔曼方程中：
- en: '![Q-learning](img/00213.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习](img/00213.jpeg)'
- en: Deep Q-network
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: 'While the number of possible actions is usually limited (number of keyboard
    keys or movements), the number of possible states can be dramatically huge, the
    search space can be enormous, for example, in the case of a robot equipped with
    cameras in a real-world environment or a realistic video game. It becomes natural
    to use a computer vision neural net, such as the ones we used for classification
    in [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks*, to represent the value of an action given an input image (the
    state), instead of a matrix:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可能的行动数量通常有限（键盘键数或移动数），但可能的状态数量可能极其庞大，搜索空间可能非常庞大，例如，在配备摄像头的机器人在真实环境或现实视频游戏中。自然而然地，我们会使用计算机视觉神经网络，例如我们在[第7章](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "第7章。使用残差网络对图像进行分类")中用于分类的那些网络，来代表给定输入图像（状态）的行动价值，而不是一个矩阵：
- en: '![Deep Q-network](img/00214.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![深度Q网络](img/00214.jpeg)'
- en: 'The Q-network is called a **state-action value network** and predicts action
    values given a state. To train the Q-network, one natural way of doing it is to
    have it fit the Bellman equation via gradient descent:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Q网络被称为**状态-动作值网络**，它根据给定的状态预测动作值。为了训练Q网络，一种自然的方式是通过梯度下降使其符合贝尔曼方程：
- en: '![Deep Q-network](img/00215.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![深度Q网络](img/00215.jpeg)'
- en: Note that, ![Deep Q-network](img/00216.jpeg) is evaluated and fixed, while the
    descent is computed for the derivatives in, ![Deep Q-network](img/00217.jpeg)
    and that the value of each state can be estimated as the maximum of all state-action
    values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![深度Q网络](img/00216.jpeg)被评估并固定，而下降是针对![深度Q网络](img/00217.jpeg)中的导数计算的，且每个状态的值可以估算为所有状态-动作值的最大值。
- en: 'After initializing the Q-network with random weights, the initial predictions
    are random, but as the network converges, the action given a particular state
    will become more and more predictable, so the exploration of new states drops.
    Exploiting a model trained online requires the forcing of the algorithm to **continue
    to explore**: the ![Deep Q-network](img/00218.jpeg) **greedy approach** consists
    of doing a random action with a probability epsilon, otherwise following the maximum-reward
    action given by the Q-network. It is a kind of learning by trial-and-error. After
    a certain number of epochs, ![Deep Q-network](img/00218.jpeg) is decayed to reduce
    exploration.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在用随机权重初始化Q网络后，初始预测是随机的，但随着网络的收敛，给定特定状态的动作将变得越来越可预测，因此对新状态的探索减少。利用在线训练的模型需要强迫算法**继续探索**：![深度Q网络](img/00218.jpeg)
    **贪心方法**包括以概率epsilon做一个随机动作，否则跟随Q网络给出的最大奖励动作。这是一种通过试错学习的方式。在经过一定数量的训练轮次后，![深度Q网络](img/00218.jpeg)会衰减，以减少探索。
- en: Training stability
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练稳定性
- en: 'Different methods are possible to improve stability during training. **Online
    training**, that is, training the model while playing the game, forgetting previous
    experiences, just considering the last one, is fundamentally unstable with deep
    neural networks: states that are close in time, such as the most recent states,
    are usually strongly similar or correlated, and taking the most recent states
    during training does not converge well.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法可以在训练过程中改善稳定性。**在线训练**，即在玩游戏时训练模型，遗忘之前的经验，只考虑最后一条经验，对于深度神经网络来说是根本不稳定的：时间上接近的状态（例如最新的状态）通常是高度相似或相关的，训练时使用最新的状态不容易收敛。
- en: To avoid such a failure, one possible solution has been to store the experiences
    in a **replay memory** or to use a database of human gameplays. Batching and shuffling
    random samples from the replay memory or the human gameplay database leads to
    more stable training, but **off-policy** training.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种失败，一个可能的解决方案是将经验存储在**回放记忆**中，或者使用人类游戏记录数据库。批量处理和打乱从回放记忆或人类游戏记录数据库中抽取的随机样本能够实现更稳定的训练，但属于**离策略**训练。
- en: 'A second solution to improve stability is to fix the value of the parameter
    ![Training stability](img/00219.jpeg) in the **target evaluation** ![Training
    stability](img/00216.jpeg) for several thousands of updates of ![Training stability](img/00217.jpeg),
    reducing the correlations between the target and the Q-values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 改善稳定性的第二个解决方案是将参数![训练稳定性](img/00219.jpeg)的值固定在**目标评估**![训练稳定性](img/00216.jpeg)中，并进行数千次更新![训练稳定性](img/00217.jpeg)，以减少目标值和Q值之间的相关性：
- en: '![Training stability](img/00220.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![训练稳定性](img/00220.jpeg)'
- en: 'It is possible to train more efficiently with n-steps Q-learning, propagating
    the rewards on *n* preceding actions instead of one:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过n步Q学习，能够更高效地训练，将奖励传播到*n*个先前的动作，而不是一个：
- en: 'Q learning formula:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习公式：
- en: '![Training stability](img/00221.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![训练稳定性](img/00221.jpeg)'
- en: 'n-steps Q-learning formula:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: n步Q学习公式：
- en: '![Training stability](img/00222.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![训练稳定性](img/00222.jpeg)'
- en: 'Here, each step will benefit from *n* next rewards:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每一步都会受益于*n*个后续奖励：
- en: '![Training stability](img/00223.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![训练稳定性](img/00223.jpeg)'
- en: 'A last solution for training stability and efficiency is an **asynchronous
    gradient descent** with multiple agents executing in parallel, on multiple instances
    of the environment, with different exploration policies, so that each gradient
    update is less correlated: each learning agent runs in a different thread on the
    same machine, sharing its model and target model parameters with other agents,
    but computing the gradients for a different part of the environment. The parallel
    actor learners have a stabilization effect, enable on-policy reinforcement, a
    reduction in training time, and comparable performances on GPU or multi-core CPU,
    which is great!'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 训练稳定性和效率的最终解决方案是**异步梯度下降**，通过多个代理并行执行，在环境的多个实例上进行，并采用不同的探索策略，这样每次梯度更新之间的相关性就更小：每个学习代理在同一台机器的不同线程中运行，与其他代理共享其模型和目标模型参数，但计算环境的不同部分的梯度。并行的行为体学习者具有稳定化效果，支持策略强化，减少训练时间，并在GPU或多核CPU上表现出可比的性能，这非常棒！
- en: 'The stabilization effect leads to better **data efficiency**: the data efficiency
    is measured by the number of epochs (an epoch is when the full training dataset
    has been presented to the algorithm) required to converge to a desired training
    loss or accuracy. Total training time is impacted by data efficiency, parallelism
    (number of threads or machines), and the parallelism overhead (it is sublinear
    in the number of threads, given the number of cores, machines and algorithm distribution
    efficiency).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定化效果导致更好的**数据效率**：数据效率通过收敛到期望的训练损失或准确率所需的训练周期（一个周期是完整的训练数据集被算法展示一次）来衡量。总训练时间受数据效率、并行性（线程数或机器数）以及并行性开销（在给定核心数、机器数和算法分布效率的情况下，随着线程数增加呈亚线性增长）影响。
- en: 'Let''s see it in practice. To implement multiple agents exploring different
    parts of the environment, we''ll run multiple processes with the Python multiprocessing
    module, one process for the model to update (GPU), and *n* processes for the agents
    exploring (CPU). The manager object of the multiprocessing module controls a server
    process holding the weights of the Q-network to share between processes. The communication
    channel to store the experiences of the agents and serve them once for the model
    update, is implemented with a process-safe queue:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际看一下。为了实现多个代理探索环境的不同部分，我们将使用Python的多进程模块运行多个进程，其中一个进程用于更新模型（GPU），*n*个进程用于代理进行探索（CPU）。多进程模块的管理器对象控制一个持有Q网络权重的服务器进程，以便在进程之间共享。用于存储代理经验并在模型更新时一次性提供的通信通道，通过一个进程安全的队列实现：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let's generate experiences and enqueue them in the common queue object.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们生成经验并将其排入公共队列对象中。
- en: 'For that purpose, where each agent creates its game environment, compile the
    Q-network and load the weights from the manager:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，在每个代理创建其游戏环境时，编译Q网络并从管理器加载权重：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To generate one experience, the agent chooses an action and executes it in
    its environment:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成一次经验，代理选择一个动作并在其环境中执行：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each experience by the agent is stored in a list until the game is terminated
    or the list is longer than *n_step*, to evaluate the state-action value with *n-steps*
    Q-learning :'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理的经验都存储在一个列表中，直到游戏结束或列表长度超过*n_step*，以便使用*n步* Q学习评估状态-动作值：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once in a while, the agent updates its weights from the learning process:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔，代理会从学习进程中更新其权重：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's see now how to update the weights in the learning agent.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何更新学习代理中的权重。
- en: Policy gradients with REINFORCE algorithms
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有REINFORCE算法的策略梯度
- en: 'The idea of **Policy Gradients** (**PG**) / REINFORCE algorithms is very simple:
    it consists in re-using the classification loss function in the case of reinforcement
    learning tasks.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略梯度**（**PG**）/ REINFORCE算法的想法非常简单：它在强化学习任务中，重新使用分类损失函数。'
- en: 'Let''s remember that the classification loss is given by the negative log likelihood,
    and minimizing it with a gradient descent follows the negative log-likelihood
    derivative with respect to the network weights:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，分类损失是由负对数似然给出的，使用梯度下降来最小化它时，遵循的是负对数似然相对于网络权重的导数：
- en: '![Policy gradients with REINFORCE algorithms](img/00224.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![带有REINFORCE算法的策略梯度](img/00224.jpeg)'
- en: Here, *y* is the select action, ![Policy gradients with REINFORCE algorithms](img/00225.jpeg)
    the predicted probability of this action given inputs X and weights ![Policy gradients
    with REINFORCE algorithms](img/00219.jpeg).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y* 是选择的动作，![使用 REINFORCE 算法的策略梯度](img/00225.jpeg) 是给定输入 X 和权重 ![使用 REINFORCE
    算法的策略梯度](img/00219.jpeg) 的该动作的预测概率。
- en: 'The REINFORCE theorem introduces the equivalent for reinforcement learning,
    where *r* is the reward. The following derivative:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 定理引入了强化学习中的等价物，其中 *r* 是奖励。以下是导数：
- en: '![Policy gradients with REINFORCE algorithms](img/00226.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![使用 REINFORCE 算法的策略梯度](img/00226.jpeg)'
- en: 'represents an unbiased estimate of the derivative of the expected reward with
    respect to the network weights:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表示网络权重相对于预期奖励的导数的无偏估计：
- en: '![Policy gradients with REINFORCE algorithms](img/00227.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![使用 REINFORCE 算法的策略梯度](img/00227.jpeg)'
- en: So, following the derivative will encourage the agent to maximize the reward.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，遵循导数将鼓励代理最大化奖励。
- en: 'Such a gradient descent enables us to optimize a **policy network** for our
    agents: a policy ![Policy gradients with REINFORCE algorithms](img/00228.jpeg)
    is a probability distribution over legal actions, to sample actions to execute
    during online learning, and can be approximated with a parameterized neural net
    as well.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的梯度下降使我们能够优化我们代理的**策略网络**：策略 ![使用 REINFORCE 算法的策略梯度](img/00228.jpeg) 是一个合法动作的概率分布，用于在在线学习期间采样要执行的动作，并且可以通过参数化神经网络进行近似。
- en: It is particularly useful in the continuous case, for example for motor control,
    where discretization of the action space might lead to some suboptimal artifacts
    and the maximization over an action-value network Q is not possible under infinite
    action space.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续案例中特别有用，例如在运动控制中，离散化的动作空间可能导致一些次优伪影，并且在无限动作空间下，无法对动作-值网络 Q 进行最大化。
- en: Moreover, it is possible to enhance the policy network with recurrency (LSTM,
    GRU,) so that the agent selects its actions with respect to multiple previous
    states.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以通过递归（LSTM，GRU）增强策略网络，使代理根据多个先前的状态选择其动作。
- en: The REINFORCE theorem gives us a gradient descent to optimize the parametrized
    policy network. To encourage exploration in this policy-based case, it is also
    possible to add a regularization term, the entropy of the policy, to the loss
    function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 定理为我们提供了一个梯度下降方法，用于优化参数化的策略网络。为了鼓励在这种基于策略的情况下进行探索，也可以向损失函数中添加正则化项——策略的熵。
- en: 'Under this policy, it is possible to compute the value of every state ![Policy
    gradients with REINFORCE algorithms](img/00229.jpeg):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在此策略下，可以计算每个状态的值 ![使用 REINFORCE 算法的策略梯度](img/00229.jpeg)：
- en: Either by playing the game from that state with the policy
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过使用该策略从该状态进行游戏
- en: Or, if parameterized into a **state value network**, by gradient descent, the
    current parameter serving as target, as for the state-action value network seen
    in the previous section with discounted rewards:![Policy gradients with REINFORCE
    algorithms](img/00230.jpeg)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，如果通过梯度下降将其参数化为**状态价值网络**，当前参数作为目标，就像在上一节中看到的带有折扣奖励的状态-动作价值网络一样：![使用 REINFORCE
    算法的策略梯度](img/00230.jpeg)
- en: 'This value is usually chosen as reinforcement baseline *b* to reduce the variance
    of the estimate of the policy gradient, and the Q-value can be used as the expected
    reward:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值通常作为强化基准 *b* 来减少策略梯度估计的方差，Q值可以作为期望奖励：
- en: '![Policy gradients with REINFORCE algorithms](img/00231.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![使用 REINFORCE 算法的策略梯度](img/00231.jpeg)'
- en: 'The first factor in the REINFORCE derivative:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 导数中的第一个因子：
- en: '![Policy gradients with REINFORCE algorithms](img/00232.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![使用 REINFORCE 算法的策略梯度](img/00232.jpeg)'
- en: is called the **advantage of action a in state** *s*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为**动作 a 在状态** *s* 中的优势。
- en: Both gradient descents, for the policy network and for the value network, can
    be performed asynchronously with our parallel actor learners.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 策略网络和价值网络的梯度下降可以通过我们的并行演员学习器异步执行。
- en: 'Let''s create our policy network and state value network, sharing their first
    layers, in Keras:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Keras 中创建我们的策略网络和状态价值网络，共享它们的第一层：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Our learning process builds the model as well, shares the weights to other
    processes, and compiles them for training with their respective losses:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的学习过程还构建了模型，将权重共享给其他进程，并为训练编译它们，并计算各自的损失：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The policy loss is a REINFORCE loss plus an entropy loss to encourage exploration.
    The value loss is a simple mean square error loss.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 策略损失是一个REINFORCE损失加上一个熵损失，以鼓励探索。值损失是一个简单的均方误差损失。
- en: 'De-queueing the experiences into a batch, our learning process trains the model
    on the batch and updates the weights dictionary:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将经验队列化成一个批次，我们的学习过程会在这个批次上训练模型并更新权重字典：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To run the full code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完整代码：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Learning took about 24 hours.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 学习大约花费了24小时。
- en: A policy-based advantage actor critic usually outperforms value-based methods.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的优势演员评论员通常优于基于值的方法。
- en: Related articles
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关文献
- en: 'You can refer to the following articles:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下文章：
- en: '*Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement
    Learning*, Ronald J. Williams, 1992'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接主义强化学习的简单统计梯度跟踪算法*，罗纳德·J·威廉姆斯，1992年'
- en: '*Policy Gradient Methods for Reinforcement Learning with Function Approximation*,
    Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour, 1999'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带有函数逼近的强化学习的策略梯度方法*，理查德·S·萨顿，戴维·麦卡勒斯特，萨廷德·辛格，伊沙·曼索尔，1999年'
- en: '*Playing Atari with Deep Reinforcement Learning*, Volodymyr Mnih, Koray Kavukcuoglu,
    David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller,
    2013'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过深度强化学习玩Atari游戏*，沃洛基米尔·姆尼赫，科雷·卡武克乔乌格，戴维·西尔弗，亚历克斯·格雷夫斯，伊欧尼斯·安东诺格鲁，丹·维尔斯特拉，马丁·里德米勒，2013年'
- en: '*Mastering the Game of Go with Deep Neural Networks and Tree Search*, David
    Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den
    Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc
    Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,
    Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel & Demis Hassabis,
    2016'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过深度神经网络和树搜索掌握围棋游戏*，戴维·西尔弗，阿贾·黄，克里斯·J·马迪森，阿瑟·格兹，洛朗·西弗，乔治·范登·德里斯切，朱利安·施里特维泽，伊欧尼斯·安东诺格鲁，维达·潘内谢尔瓦姆，马克·兰特托，桑德·迪尔曼，多米尼克·格雷韦，约翰·纳姆，纳尔·卡尔赫布雷纳，伊利亚·苏茨克弗，蒂莫西·利利克拉普，马德琳·利奇，科雷·卡武克乔乌格，托雷·格雷佩尔和德米斯·哈萨比斯，2016年'
- en: '*Asynchronous Methods for Deep Reinforcement Learning*, Volodymyr Mnih, Adrià
    Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. LilliCrap,
    David Silver, Koray Kavukcuoglu, Feb 2016'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习的异步方法*，沃洛基米尔·姆尼赫，阿德里亚·普伊格多梅内奇·巴迪亚，梅赫迪·米尔扎，亚历克斯·格雷夫斯，蒂姆·哈利，蒂莫西·P·利利克拉普，戴维·西尔弗，科雷·卡武克乔乌格，2016年2月'
- en: '*Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym*,
    a Gym RL Agent Timothy J. O''Shea and T. Charles Clancy, 2016'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用KeRLym进行深度强化学习无线电控制和信号检测*，Gym RL代理蒂莫西·J·奥谢和T·查尔斯·克兰西，2016年'
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Reinforcement learning describes the tasks of optimizing an agent stumbling
    into rewards episodically. Online, offline, value-based, or policy-based algorithms
    have been developed with the help of deep neural networks for various games and
    simulation environments.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习描述了优化代理通过奖励偶然获得任务的过程。通过深度神经网络开发了在线、离线、基于值或基于策略的算法，应用于各种游戏和模拟环境。
- en: Policy-gradients are a brute-force solution that require the sampling of actions
    during training and are better suited for small action spaces, although they provide
    first solutions for continuous search spaces.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度是一种强行求解方案，需要在训练过程中进行动作采样，适用于小的动作空间，尽管它们为连续搜索空间提供了初步的解决方案。
- en: Policy-gradients also work to train non-differentiable stochastic layers in
    a neural net and back propagate gradients through them. For example, when propagation
    through a model requires to sample following a parameterized submodel, gradients
    from the top layer can be considered as a reward for the bottom network.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度也可以用于训练神经网络中的非可微随机层，并通过这些层进行反向传播梯度。例如，当通过一个模型的传播需要按照参数化子模型进行采样时，来自顶层的梯度可以被视为底层网络的奖励。
- en: In more complex environments, when there is no obvious reward (for example understanding
    and inferring possible actions from the objects present in the environment), reasoning
    helps humans optimize their actions, for which research does not provide any solution
    currently. Current RL algorithms are particularly suited for precise plays, fast
    reflexes, but no long term planning and reasoning. Also, RL algorithms require
    heavy datasets, which simulation environments provide easily. But this opens up
    the question of scaling in the real world.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的环境中，当没有明显的奖励时（例如，从环境中存在的物体推理和理解可能的动作），推理帮助人类优化他们的动作，目前的研究尚未提供任何解决方案。当前的强化学习算法特别适用于精确的操作、快速的反应，但没有长期规划和推理。此外，强化学习算法需要大量的数据集，而模拟环境能够轻松提供这些数据集。但这也引出了现实世界中扩展性的问题。
- en: In the next chapter, we'll explore the latest solutions to generate new data
    undistinguishable from real-world data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨最新的解决方案，用于生成与现实世界数据无法区分的新数据。
