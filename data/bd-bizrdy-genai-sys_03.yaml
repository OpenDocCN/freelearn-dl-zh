- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Integrating Dynamic RAG into the GenAISys
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将动态RAG集成到GenAISys中
- en: A business-ready **generative AI system** (**GenAISys**) needs to be flexible
    and ready to face the rapidly evolving landscape of the AI market. The AI controller
    acts as an adaptive orchestrator for e-marketing, production, storage, distribution,
    and support, but to satisfy such a range of tasks, we need a **retrieval-augmented
    generation** (**RAG**) framework. In the previous chapter, we built a conversational
    AI agent and a function for similarity search for instruction scenarios (AI orchestrator)
    for a generative AI model. In this chapter, we will enhance that foundation and
    build a scalable RAG in a Pinecone index to integrate both instruction scenarios
    and classical data, which the generative AI model will connect to.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个面向商业的**生成式AI系统**（**GenAISys**）需要具备灵活性和随时应对AI市场快速变化的能力。AI控制器充当电子营销、生产、存储、分销和支持的适应性协调者，但要满足如此广泛的任务，我们需要一个**检索增强生成**（**RAG**）框架。在前一章中，我们构建了一个用于指令场景（AI协调器）的对话式AI代理和相似性搜索功能，用于生成式AI模型。在本章中，我们将增强这个基础，并在Pinecone索引中构建一个可扩展的RAG，以整合指令场景和经典数据，这些数据生成式AI模型将与之连接。
- en: We make a clear distinction in this chapter between **instruction scenarios**—expert-crafted
    prompt fragments (or *task tags*, as explained in the previous chapter) that tell
    the model *how* to reason or act—and **classical data**—the reference material
    the RAG system retrieves to ground its answers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们明确区分了**指令场景**——专家精心制作的提示片段（或如前一章所述的*任务标签*），告诉模型*如何*推理或行动，以及**经典数据**——RAG系统检索以支持其答案的参考材料。
- en: Why do we need this dynamic and adaptive RAG framework with vectorized scenarios
    of instructions on top of classical data? Because the global market affects entities
    internally and externally. For example, a hurricane can cause electricity shortages,
    putting the supply chain of businesses in peril. Businesses might have to relocate
    supply routes, production, or distribution. General-purpose AI cloud platforms
    might do some of the job. But more often than not, we will need to provide custom,
    domain-specific functionality. For that reason, we need a dynamic set of instructions
    in a vector store repository as we do for RAG data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要这样一个动态和自适应的RAG框架，其上还有基于经典数据的指令场景向量化？因为全球市场从内部和外部影响实体。例如，一场飓风可能导致电力短缺，使企业的供应链处于危险之中。企业可能不得不重新定位供应路线、生产或分销。通用AI云平台可能完成一些工作。但更常见的情况是我们需要提供定制、特定领域的功能。因此，我们需要一个动态的指令集，在向量存储库中，就像我们对RAG数据所做的那样。
- en: We will begin by defining the architecture scenario-driven task executions for
    a generative AI model, in this case, GPT-4o, through a Pinecone index. We will
    carefully go through the cost-benefits of investing in intelligent scenarios for
    the generative model through similarity search and retrieval. We will introduce
    a dynamic framework to produce ChatGPT-like capabilities that we will progressively
    introduce in the following chapters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过Pinecone索引定义面向生成式AI模型（在本例中为GPT-4o）的架构驱动任务执行。我们将仔细分析投资于智能场景（通过相似性搜索和检索）的成本效益。我们将介绍一个动态框架，以产生类似ChatGPT的能力，这些能力将在接下来的章节中逐步介绍。
- en: Once the architecture is defined, we will first build a Pinecone index to chunk,
    embed, and upsert instruction scenarios. We will make sure the GenAISys vector
    store can embed a query and find a relevant instruction scenario. This capability
    will be a key component in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110), *Building
    the AI Controller Orchestration Interface*, when we design the conversational
    agent’s interface and orchestrator. Finally, we will write a program to upsert
    classical data in a RAG environment to the same Pinecone index alongside the instruction
    scenarios. Differentiation between scenarios and classical data will be maintained
    using distinct namespaces. By the end of this chapter, we will have built the
    main components to link instructions to a generative AI model. We will be ready
    to design a user interface and AI controller orchestrator in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了架构，我们首先将构建一个Pinecone索引来分块、嵌入和更新指令场景。我们将确保GenAISys向量存储可以嵌入查询并找到相关的指令场景。这种能力将是我们在设计对话代理的界面和协调器时，在**第4章**（[Chapter
    4](Chapter_4.xhtml#_idTextAnchor110)）中构建AI控制器协调接口的关键组件。最后，我们将编写一个程序，将经典数据在RAG环境中更新到与指令场景相同的Pinecone索引。我们将使用不同的命名空间来维护场景和经典数据之间的区分。到本章结束时，我们将构建将指令链接到生成式AI模型的主要组件。我们将为**第4章**（[Chapter
    4](Chapter_4.xhtml#_idTextAnchor110)）中设计用户界面和AI控制器协调器做好准备。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Architecting RAG for the dynamic retrieval of instructions and data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用于指令和数据动态检索的RAG
- en: The law of diminishing returns when developing similarity searches
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发相似性搜索时的收益递减定律
- en: Examining the architecture of a hybrid GenAISys CoT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查混合GenAISys CoT的架构
- en: Creating a Pinecone index by chunking, embedding, and upserting instruction
    scenarios
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分块、嵌入和更新指令场景来创建Pinecone索引
- en: Enhancing a Pinecone index with classical data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过经典数据增强Pinecone索引
- en: Querying the Pinecone index
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询Pinecone索引
- en: Our first task is to architect a RAG framework for dynamic retrieval.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一项任务是构建一个用于动态检索的RAG框架。
- en: Architecting RAG for dynamic retrieval
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建动态检索的RAG
- en: 'In this section, we will define a Pinecone index that stores both instruction
    scenarios and classical data. This structure gives GenAISys dynamic, cost-effective
    retrieval: the instruction scenarios steer the generative AI model (GPT-4o in
    our example), while the classical data supplies the factual context used by the
    RAG pipeline.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义一个Pinecone索引，该索引存储指令场景和经典数据。这种结构为GenAISys提供了动态、成本效益的检索：指令场景引导生成式AI模型（以我们为例的GPT-4o），而经典数据为RAG管道使用的实际背景提供信息。
- en: 'We will go through the following components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨以下组件：
- en: '**Scenario-driven task execution**: Designing optimized instructional prompts
    (“scenarios”) that we will upsert to the Pinecone index.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**场景驱动任务执行**：设计优化的指导提示（“场景”），我们将将其更新到Pinecone索引。'
- en: '**Cost-benefit strategies**: Considering the law of diminishing returns to
    avoid overinvesting in automation.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益策略**：考虑收益递减定律，避免过度投资于自动化。'
- en: '**Partitioning Pinecone with namespaces**: Using Pinecone index namespaces
    to clearly differentiate instruction scenarios from classical data.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用命名空间分区Pinecone**：使用Pinecone索引命名空间明确区分指令场景和经典数据。'
- en: '**Hybrid retrieval framework**: Implementing implicit vector similarity searches
    but also triggering explicit instructions for the generative AI model (more on
    this in the *Scenario-driven task execution* section).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合检索框架**：实现隐式向量相似性搜索，但同时也触发生成式AI模型（更多内容请参阅**场景驱动任务执行**部分）的显式指令。'
- en: '**CoT loops**: Explaining how the flexibility of the scenario selection process
    will lead to loops of generative AI functions before finally producing an output.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CoT loops**：解释场景选择过程的灵活性将如何导致生成式AI函数的循环，最终产生输出。'
- en: '**GenAISys framework**: Laying the groundwork for the advanced GenAISys framework
    we are building throughout the book.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GenAISys框架**：为我们在整本书中构建的高级GenAISys框架奠定基础。'
- en: Let’s first dive deeper into scenario-driven task execution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先深入了解场景驱动任务执行。
- en: Scenario-driven task execution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景驱动任务执行
- en: 'In the previous chapter, we saw two complementary ways the AI controller can
    pick what to do next:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了AI控制器可以选择下一步做什么的两种互补方式：
- en: '**Implicit selection**: The controller embeds the user’s prompt, runs a semantic
    similarity search across its scenario library, and chooses the closest match without
    any task tag. This gave us flexible, code-free orchestration (e.g., it automatically
    chose a sentiment-analysis scenario for the *Gladiator II* review).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐式选择**：控制器嵌入用户的提示，在其场景库中运行语义相似性搜索，并选择最接近的匹配项，而不需要任何任务标签。这为我们提供了灵活的、无需代码的编排（例如，它自动为“角斗士
    II”评论选择了情感分析场景）。'
- en: '**Explicit selection**: The desired task is spelled out, either as a task tag
    in the prompt or as a user interface action, such as “Run web search.” Here, the
    controller skips the similarity search and jumps straight to the requested tool
    or workflow.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显式选择**：所需任务明确说明，可以是提示中的任务标签，也可以是用户界面操作，例如“运行网络搜索”。在这里，控制器跳过相似性搜索，直接跳到请求的工具或工作流程。'
- en: 'That same pattern continues in this chapter, but at a larger scale. Instead
    of a few hand-picked prompts, we manage hundreds or even thousands of expert-authored
    instruction scenarios stored in a vector database; instead of single-user experiments,
    we support many concurrent users and workflows. This scenario-driven (implicit)
    approach has three advantages:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相同的模式在本章中继续，但规模更大。我们不是管理几个精选的提示，而是管理存储在向量数据库中的数百甚至数千个专家撰写的指令场景；我们不是支持单个用户实验，而是支持许多并发用户和工作流程。这种以场景驱动的（隐式）方法有三个优点：
- en: Professional experts typically create these advanced prompts/instruction scenarios,
    often surpassing the expertise level of mainstream users.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专业专家通常创建这些高级提示/指令场景，通常超过主流用户的专家水平。
- en: The scenarios can be co-designed by AI specialists and subject-matter experts,
    covering a wide range of activities in a corporation, from sales to delivery.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些场景可以由 AI 专家和领域专家共同设计，涵盖公司从销售到交付的广泛活动。
- en: The order of execution of the scenarios is prompt-driven, flexible, and unordered.
    This dynamic approach avoids hardcoding the order of the tasks, increasing adaptability
    as much as possible.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 场景的执行顺序是由提示驱动的、灵活的且无序的。这种动态方法避免了硬编码任务顺序，尽可能增加了适应性。
- en: 'However, while implicit task planning maximizes flexibility, as we move toward
    building business-ready systems, we must balance flexibility with cost-efficiency.
    In some cases, therefore, *explicit* instructions, such as triggering a web search
    by selecting the option in the user interface, can significantly reduce the potential
    costs, as shown in *Figure 3.1*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管隐式任务规划最大化了灵活性，但当我们朝着构建业务就绪系统迈进时，我们必须在灵活性和成本效益之间取得平衡。因此，在某些情况下，*显式*指令，例如通过在用户界面中选择选项来触发网络搜索，可以显著降低潜在成本，如图
    *图 3.1* 所示：
- en: '![Figure 3.1: Diminishing returns as costs increase](img/B32304_03_1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：成本增加时的收益递减](img/B32304_03_1.png)'
- en: 'Figure 3.1: Diminishing returns as costs increase'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：成本增加时的收益递减
- en: 'The more we automate implicit scenarios that the generative AI model will select
    with vector similarity searches in the Pinecone index, the higher the cost. To
    manage this, we must carefully consider the law of diminishing returns:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Pinecone 指数中自动化的隐式场景越多，即生成式 AI 模型通过向量相似性搜索选择的场景越多，成本就越高。为了管理这一点，我们必须仔细考虑收益递减定律：
- en: '![](img/B32304_03_001.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：成本增加时的收益递减](img/B32304_03_001.png)'
- en: 'In this equation, as illustrated in *Figure 3.1*, in theoretical units, we
    have the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，如图 *图 3.1* 所示，在理论单位中，我们有以下内容：
- en: '![](img/B32304_03_002.png) represents the overall benefit, which is represented
    by roughly 15 when the cost reaches 50.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![整体效益](img/B32304_03_002.png) 表示整体效益，当成本达到 50 时，大约为 15。'
- en: '![](img/B32304_03_003.png) represents the initial benefit of storing instruction
    scenarios in the Pinecone index and asking the generative AI model to select one
    through vector similarity with the user input. In this case, it is nearly 1 benefit
    unit for 1 cost unit.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![初始效益](img/B32304_03_003.png) 表示将指令场景存储在 Pinecone 指数中，并通过与用户输入的向量相似性选择一个场景的初始效益。在这种情况下，几乎每个成本单位对应
    1 个效益单位。'
- en: '![](img/B32304_03_004.png) is the rate at which the benefit begins to increase
    as we increase the cost.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![效益增加率](img/B32304_03_004.png) 是随着成本增加，效益开始增加的速率。'
- en: '![](img/B32304_03_005.png) represents the cost measured in theoretical units
    (currency, human resources, or computational resources).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![成本测量](img/B32304_03_005.png) 表示以理论单位（货币、人力或计算资源）衡量的成本。'
- en: '![](img/B32304_03_006.png) denotes the rate at which returns diminish as cost
    increases.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![收益递减率](img/B32304_03_006.png) 表示随着成本增加，收益递减的速率。'
- en: For example, when the cost reaches 7 theoretical units, the benefit reaches
    7 theoretical units. This 1 unit of cost generating 1 unit of benefit is reasonable.
    However, when the benefit reaches 10 units, the cost could double to 14 units,
    which signals that something is going wrong.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当成本达到 7 个理论单位时，收益也达到 7 个理论单位。每单位成本产生 1 单位收益是合理的。然而，当收益达到 10 个单位时，成本可能翻倍达到
    14 个单位，这表明出了问题。
- en: 'The diminishing ![](img/B32304_03_006.png) factor has a strong negative impact
    ![](img/B32304_03_008.png) on the benefits through squared costs:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 减少的 ![](img/B32304_03_006.png) 因素对通过平方成本带来的益处有强烈的负面影响 ![](img/B32304_03_008.png)：
- en: '![](img/B32304_03_009.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B32304_03_009.png)'
- en: We will carefully monitor the factor ![](img/B32304_03_010.png) as we move through
    the use cases in this book. We will have to make choices between running implicit
    automated scenario selections through the Pinecone index and explicitly triggering
    actions through predefined instructions in the prompt itself.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的使用案例中仔细监控 ![](img/B32304_03_010.png) 因素。我们将在通过 Pinecone 索引运行隐式自动场景选择和通过提示中的预定义指令显式触发动作之间做出选择。
- en: Let’s now explore how we identify instruction scenarios within a Pinecone index.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨如何在 Pinecone 索引中识别指令场景。
- en: Hybrid retrieval and CoT
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合检索和 CoT
- en: 'Our first step is teaching the GenAISys framework to distinguish clearly between
    classical data and instruction scenarios. To achieve this, we will separate the
    instruction scenarios and the data with two namespaces within the same Pinecone
    index, named `genai-v1`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是教会 GenAISys 框架明确区分经典数据和指令场景。为了实现这一点，我们将在同一个 Pinecone 索引中用两个命名空间来区分指令场景和数据，命名为
    `genai-v1`：
- en: '`genaisys` will contain instruction vectors of information'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`genaisys` 将包含信息指令向量'
- en: '`data01` will contain data vectors of information'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data01` 将包含信息数据向量'
- en: We will implement `genai-v1` in code with additional explanations in the *Creating
    the* *Pinecone index* section of this chapter.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在本章的 *创建* *Pinecone 索引* 部分用代码实现 `genai-v1`，并在其中提供额外的解释。
- en: Once the Pinecone index has been partitioned into scenarios and data, we can
    take the GenAISys to another level with hybrid retrieval, as shown in *Figure
    3.2*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Pinecone 索引被划分为场景和数据，我们就可以通过混合检索将 GenAISys 提升到另一个层次，如图 *图 3.2* 所示。
- en: '![Figure 3.2: AI controller orchestrating GenAISys](img/B32304_03_2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：AI 控制器编排 GenAISys](img/B32304_03_2.png)'
- en: 'Figure 3.2: AI controller orchestrating GenAISys'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：AI 控制器编排 GenAISys
- en: 'The hybrid retrieval framework depicted in the preceding figure will enable
    GenAISys to do the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图中描述的混合检索框架将使 GenAISys 能够执行以下操作：
- en: Run the generative AI model using processed, chunked, and embedded data memory
    (see **1**–**3** in *Figure 3.2*) directly without going through the Pinecone
    index (see **3B**). This will reduce costs for ephemeral data, for example.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用处理过的、分块和嵌入的数据内存（见 *图 3.2* 中的 **1**–**3**）直接运行生成式 AI 模型，而不通过 Pinecone 索引（见
    **3B**）。这将减少临时数据的成本，例如。
- en: Run the generative AI model after the chunked and embedded data is upserted
    to the Pinecone index either as a set of instructions in a scenario or as classical
    data.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将分块和嵌入的数据更新到 Pinecone 索引后运行生成式 AI 模型，要么作为场景中的一组指令，要么作为经典数据。
- en: Create a CoT loop between the Pinecone index (see **3B**) and the generative
    AI model controller as an orchestrator. For example, the output of the model can
    serve as input for another CoT cycle that will retrieve scenarios or data from
    the Pinecone index. ChatGPT-like copilots often present their output and then
    finish by asking whether you’d like to explore further—sometimes even suggesting
    ready-made follow-up prompts you can click on.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Pinecone 索引（见 **3B**）和生成式 AI 模型控制器之间创建一个 CoT 循环作为编排者。例如，模型的输出可以作为另一个 CoT 循环的输入，该循环将从
    Pinecone 索引检索场景或数据。类似于 ChatGPT 的共飞行员通常会展示他们的输出，然后通过询问你是否想进一步探索来结束，有时甚至会建议你可以点击的现成后续提示。
- en: CoT loops can operate implicitly via vector similarity search or explicitly
    via direct instruction triggers or task tags (such as “Run a web search”). For
    example, ChatGPT-like copilots can trigger web searches directly through the user
    interface or rules in the AI controller.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 循环可以通过向量相似度搜索隐式运行，也可以通过直接指令触发或任务标签（例如“运行网络搜索”）显式运行。例如，类似于 ChatGPT 的共飞行员可以直接通过用户界面或
    AI 控制器中的规则触发网络搜索。
- en: 'We’ll begin building our GenAISys in this chapter and continue refining it
    over the next few chapters. Starting from [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110),
    *Building the AI Controller Orchestration Interface*, we’ll use the RAG foundations
    introduced here to develop the hybrid retrieval framework shown in *Figure 3.2*.
    The GenAISys we’re building will include dynamic process management—a requisite
    for keeping pace with the shifting market conditions. Specifically, our GenAISys
    will do the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章开始构建我们的 GenAISys，并在接下来的几章中继续对其进行优化。从 [*第 4 章*](Chapter_4.xhtml#_idTextAnchor110)
    开始，*构建 AI 控制器编排接口*，我们将使用这里引入的 RAG 基础来开发 *图 3.2* 中所示的混合检索框架。我们构建的 GenAISys 将包括动态流程管理——这是跟上市场变化趋势的必要条件。具体来说，我们的
    GenAISys 将执行以下操作：
- en: Leverage Pinecone’s vector database or in-memory chunked and embedded information
    with similarity searches to optimize retrieval (instructions or data)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Pinecone 的向量数据库或内存中的分块和嵌入信息，通过相似性搜索优化检索（指令或数据）
- en: Explicitly trigger direct instructions, such as a web search, and include them
    in a CoT loop for summarization, sentiment analysis, or semantic analysis
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确触发直接指令，例如网络搜索，并将其包含在 CoT 循环中进行总结、情感分析或语义分析
- en: Break down complex sets of instructions and data retrieval into manageable steps
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的指令和数据检索组合分解为可管理的步骤
- en: Iteratively refine solutions in a human-like thought process before producing
    an output
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成输出之前，以类似人类思维过程的方式迭代优化解决方案
- en: Get the best out of generative AI models, including OpenAI’s reasoning models
    such as o3, by providing them with an optimized instruction scenario
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供优化的指令场景，充分发挥生成式 AI 模型（包括 OpenAI 的推理模型如 o3）的潜力
- en: Our initial step in this chapter is building the `genai-v1` Pinecone index,
    which the AI controller will use to manage instruction scenarios within the `genaisys`
    namespace. Then, we’ll demonstrate how to chunk, embed, and upsert classical data
    into the `data01` namespace. Let’s get moving!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的初始步骤是构建 `genai-v1` Pinecone 索引，AI 控制器将使用它来管理 `genaisys` 命名空间内的指令场景。然后，我们将演示如何将经典数据分块、嵌入并更新到
    `data01` 命名空间。让我们开始吧！
- en: Building a dynamic Pinecone index
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建动态 Pinecone 索引
- en: 'We’ll focus on creating a Pinecone index designed to manage both instruction
    scenarios and classical data. In the upcoming sections, we’ll begin upserting
    the instruction scenarios as well as classical data. The workflow breaks down
    into three straightforward stages:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于创建一个 Pinecone 索引，用于管理指令场景和经典数据。在接下来的章节中，我们将开始更新指令场景以及经典数据。工作流程分为三个简单的阶段：
- en: Setting up the environment for OpenAI and Pinecone
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 OpenAI 和 Pinecone 的环境
- en: Processing the data, chunking it, and then embedding it
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据，将其分块，然后嵌入
- en: Initializing the Pinecone index
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化 Pinecone 索引
- en: Open the `Pinecone_instruction_scenarios.ipynb` notebook within the Chapter03
    directory on GitHub ([https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main)).
    Our first task is to set up the environment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 的 Chapter03 目录中打开 `Pinecone_instruction_scenarios.ipynb` 笔记本（[https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main)）。我们的第一个任务是设置环境。
- en: Setting up the environment
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'As we move through the book, we will continually reuse functions and features
    implemented in *Chapters 1* and *2*, add new ones for Pinecone, and organize the
    installations into two parts:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们阅读本书，我们将不断重用第 *1* 章和 *2* 章中实现的函数和功能，为 Pinecone 添加新的功能，并将安装过程分为两部分：
- en: Installing OpenAI using the same process as in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021).
    Refer back to that chapter if needed.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与 [*第 1 章*](Chapter_1.xhtml#_idTextAnchor021) 中相同的流程安装 OpenAI。如有需要，请参阅该章节。
- en: Installing Pinecone for this and following chapters.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为本章和后续章节安装 Pinecone。
- en: 'To begin, download the files we need by retrieving `grequests.py` from the
    GitHub repository:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过从 GitHub 仓库检索 `grequests.py` 文件来下载我们需要的文件：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To install OpenAI, follow the same steps as in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021).
    We’ll move on to install Pinecone now, which we will refer to in upcoming chapters
    throughout the book.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 OpenAI，请遵循 [*第 1 章*](Chapter_1.xhtml#_idTextAnchor021) 中的相同步骤。现在我们将继续安装
    Pinecone，在本书的后续章节中，我们将多次提及它。
- en: Installing Pinecone
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Pinecone
- en: 'Download the Pinecone requirements file that contains the instructions for
    the Pinecone version we want to use throughout the book. If another version is
    required, this will be the only file that needs to be updated:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下载包含我们想要在整本书中使用的 Pinecone 版本说明的 Pinecone 需求文件。如果需要其他版本，这将是需要更新的唯一文件：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/3-PPMUMLAP0325.png)**Quick tip**: Enhance your coding experience with
    the **AI Code Explainer** and **Quick Copy** features. Open this book in the next-gen
    Packt Reader. Click the **Copy** button'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/3-PPMUMLAP0325.png)**快速提示**：使用 **AI 代码解释器** 和 **快速复制** 功能来增强您的编码体验。在下一代
    Packt 阅读器中打开此书。点击 **复制** 按钮'
- en: (**1**) to quickly copy code into your coding environment, or click the **Explain**
    button
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: （**1**）快速将代码复制到您的编码环境，或点击 **解释** 按钮
- en: (**2**) to get the AI assistant to explain a block of code to you.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: （**2**）让 AI 助手为您解释一段代码。
- en: '![A white background with a black text  AI-generated content may be incorrect.](img/image_%282%29.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![白色背景上的黑色文字 AI 生成的内容可能不正确。](img/image_%282%29.png)'
- en: '![](img/4.png)**The next-gen Packt Reader** is included for free with the purchase
    of this book. Scan the QR code OR visit [packtpub.com/unlock](http://packtpub.com/unlock),
    then use the search bar to find this book by name. Double-check the edition shown
    to make sure you get the right one.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/4.png)**下一代 Packt 阅读器**随本书免费赠送。扫描二维码或访问 [packtpub.com/unlock](http://packtpub.com/unlock)，然后使用搜索栏通过名称查找此书。请仔细检查显示的版本，以确保您获得正确的版本。'
- en: '![A qr code on a white background  AI-generated content may be incorrect.](img/Unlock_Code1.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![白色背景上的二维码 AI 生成的内容可能不正确。](img/Unlock_Code1.png)'
- en: 'The file contains the installation function, which we will call with the following
    command:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 文件包含安装函数，我们将使用以下命令调用它：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The script is the same as the one for OpenAI described in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021),
    but adapted to Pinecone. We first uninstall Pinecone and then install the version
    we need:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本与在 [*第 1 章*](Chapter_1.xhtml#_idTextAnchor021) 中描述的 OpenAI 脚本相同，但已调整为适用于 Pinecone。我们首先卸载
    Pinecone，然后安装所需的版本：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we verify the installation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们验证安装：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output shows we have successfully installed the client:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们已成功安装客户端：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s go ahead and initialize the Pinecone API key.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续初始化 Pinecone API 密钥。
- en: Initializing the Pinecone API key
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化 Pinecone API 密钥
- en: 'The program now downloads `pinecone_setup.py`, which we will use to initialize
    the Pinecone API key:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在下载 `pinecone_setup.py`，我们将使用它来初始化 Pinecone API 密钥：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This setup mirrors the Google Colab secrets-based approach we used for OpenAI
    in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021), but it’s adapted here for
    initializing the Pinecone API.:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置反映了我们在 [*第 1 章*](Chapter_1.xhtml#_idTextAnchor021) 中为 OpenAI 使用的基于 Google
    Colab 秘密的方案，但在此处调整为初始化 Pinecone API：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If Google secrets was set to `True` for OpenAI in the OpenAI section of this
    notebook, then the Pinecone setup function will be called:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 OpenAI 部分的笔记本中将 Google 秘密设置为 `True`，则 Pinecone 设置函数将被调用：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If Google secrets was set to `False`, then you can implement a custom function
    by uncommenting the code and entering the Pinecone API key with any method you
    wish:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Google 秘密设置为 `False`，则可以通过取消注释代码并使用您希望的方法输入 Pinecone API 密钥来实现自定义函数：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The program is now ready to process the data we will upsert to the Pinecone
    index.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在已准备好处理我们将要上载到 Pinecone 索引中的数据。
- en: Processing data
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据
- en: 'Our goal now is to prepare the scenarios for storage and retrieval so that
    we can then query the Pinecone index. The main steps of the process are represented
    in *Figure 3.2*, which is only one layer of the roadmap for the following chapters.
    We will process the data in the following steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的目标是准备存储和检索的场景，以便我们随后可以查询 Pinecone 索引。该过程的主要步骤在 *图 3.2* 中表示，这是后续章节路线图的一层。我们将按以下步骤处理数据：
- en: '**Data loading and preparation**, in which the data will be broken into smaller
    parts. In this case, each scenario will be stored in one line of a scenario list,
    which will prepare the chunking process. We will not always break text into lines,
    however, as we will see in the *Upserting classical data into the index* section
    later.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据加载和准备**，其中数据将被分成更小的部分。在这种情况下，每个场景将存储在场景列表的一行中，这将准备分块过程。然而，我们不会总是将文本拆分成行，正如我们将在后面的
    *将经典数据插入索引* 部分中看到的。'
- en: '**Chunking functionality** to store each line of scenarios into chunks.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分块功能**将场景的每一行存储到块中。'
- en: '**Embedding** the chunks of text obtained.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入**获取的文本块。'
- en: '**Verification** to ensure that we embedded the corresponding number of chunks.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证**以确保我们嵌入的块数量正确。'
- en: 'Let’s now cover the first two steps: loading and preparing the data, followed
    by chunking.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来介绍前两个步骤：加载数据和准备数据，然后进行块分割。
- en: Data loading and chunking
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据加载和块分割
- en: 'We will use the scenarios implemented in [*Chapter 2*](Chapter_2.xhtml#_idTextAnchor055).
    They are stored in a file that we will now download:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在 [*第 2 章*](Chapter_2.xhtml#_idTextAnchor055) 中实现的场景。它们存储在一个文件中，我们现在将其下载：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will add more scenarios throughout our journey in this book to create a
    GenAISys. For the moment, our main objective is to get our Pinecone index to work.
    The program first initializes `start_time` for time measurement. Then we load
    the lines of scenario instructions directly into `chunks` line by line:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的旅程中，我们将添加更多场景以创建 GenAISys。目前，我们的主要目标是让我们的 Pinecone 索引工作。程序首先初始化 `start_time`
    用于时间测量。然后我们逐行将场景指令的行直接加载到 `chunks` 中：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then the code displays the number of chunks and the time it took to create
    the chunks:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代码显示块的数量和创建块所需的时间：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The program now verifies the first three chunks of scenario instructions:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在验证场景指令的前三个块：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output shows the three scenarios we will be working on in this chapter:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了我们将在本章中处理的三个场景：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The chunks of data are now ready for embedding. Let’s proceed with embedding.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据块现在已准备好进行嵌入。让我们继续进行嵌入。
- en: Embedding the dataset
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入数据集
- en: To embed the dataset, we will first initialize the embedding model and then
    embed the chunks. The program first initializes the embedding model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了嵌入数据集，我们首先初始化嵌入模型，然后嵌入块。程序首先初始化嵌入模型。
- en: Initializing the embedding model
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始化嵌入模型
- en: 'We will be using an OpenAI embedding model to embed the data. To embed our
    data with an OpenAI model, we can choose one of three main models:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 OpenAI 嵌入模型来嵌入数据。为了使用 OpenAI 模型嵌入我们的数据，我们可以选择三种主要模型之一：
- en: '`text-embedding-3-small`, which is fast and has a lower resource usage. This
    is sufficient for real-time usage. It is a smaller model and is thus cost-effective.
    However, as the vector store will increase in size with complex scenarios, it
    might be less accurate for nuanced tasks.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-embedding-3-small`，速度快，资源使用量低。这对于实时使用来说足够了。这是一个较小的模型，因此具有成本效益。然而，随着复杂场景的向量存储库增大，它可能在细微的任务中准确性较低。'
- en: '`text-embedding-3-large`, which provides high accuracy and nuanced embeddings
    and will prove effective for complex semantic similarity searches. It requires
    more resources and costs more.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-embedding-3-large`，提供高精度和细微的嵌入，对于复杂的语义相似性搜索将非常有效。它需要更多的资源，因此成本更高。'
- en: '`text-embedding-ada-002`, which is cost-effective for good-quality embeddings.
    However, it’s slightly slower than models such as `text-embedding-3-small` and
    `text-embedding-3-large`.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text-embedding-ada-002`，对于高质量的嵌入来说既经济又实惠。然而，它比 `text-embedding-3-small` 和
    `text-embedding-3-large` 等模型略慢。'
- en: You can consult the OpenAI documentation at [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
    for more info.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查阅 OpenAI 文档以获取更多信息：[https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
- en: 'To import a limited number of scenarios in this chapter, we will use `text-embedding-3-small`
    to optimize speed and cost. The program initializes the model while the others
    are commented for further use if needed:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在本章中导入有限数量的场景，我们将使用 `text-embedding-3-small` 来优化速度和成本。程序初始化模型，而其他模型则被注释以供将来使用（如果需要）：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We initialize the OpenAI client:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化 OpenAI 客户端：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'An embedding function is then created that will convert the text sent to it
    into embeddings. The function is designed to produce embeddings for a batch of
    input texts (`texts`) with the embeddings model of our choice, in this case, `text-embedding-3-small`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建一个嵌入函数，该函数将发送给它的文本转换为嵌入。该函数旨在使用我们选择的嵌入模型为一批输入文本（`texts`）生成嵌入，在这种情况下，为 `text-embedding-3-small`：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The function first cleans the text by replacing newline characters in each
    text with spaces:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先通过将每个文本中的换行符替换为空格来清理文本：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, the function makes the API embedding call:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，该函数执行 API 嵌入调用：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The embeddings are extracted from the response:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的向量是从响应中提取的：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, the embeddings are returned:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，返回嵌入的向量：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The program is now ready to embed the chunks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在已准备好嵌入块。
- en: Embedding the chunks
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将块嵌入
- en: 'The program first defines a function to embed the chunks:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先定义一个函数来嵌入块：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The parameters of the function are:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的参数是：
- en: '`chunks`: The parts of text to embed'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunks`: 要嵌入的文本部分'
- en: '`embedding_model`: Defines the model to use, such as `text-embedding-3-small`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_model`: 定义要使用的模型，例如`text-embedding-3-small`'
- en: '`batch_size`: The number of chunks the function can process in a single batch,
    such as `batch_size=1000`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: 函数在单个批次中可以处理的块的数量，例如`batch_size=1000`'
- en: '`pause_time`: A pause time in seconds, which can be useful for rate limits'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pause_time`: 以秒为单位的暂停时间，这在速率限制中可能很有用'
- en: 'We then initialize the timing function, `embeddings` variable, and counter:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们初始化计时函数、`embeddings`变量和计数器：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The code is now ready to process the chunks in batches:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 代码现在已准备好批量处理文本块：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Each batch is then sent to the embedding function:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将每个批次发送到嵌入函数：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The embedded batch is appended to the embeddings list:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的批次被追加到嵌入列表中：
- en: '[PRE27]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The number of batches is monitored and displayed and the pause is activated:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 监控批次的数量并显示，然后激活暂停：
- en: '[PRE28]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once all the batches are processed, the total time is displayed:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有批次都处理完毕，将显示总时间：
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The embedding function is ready to be called with the chunks list:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入函数现在可以调用带有块列表：
- en: '[PRE30]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output shows that the scenario data has been embedded:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示场景数据已嵌入：
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first embedding is displayed for verification:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首个嵌入用于验证：
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output confirms that the embeddings have been generated:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认嵌入已生成：
- en: '[PRE33]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The final verification is to check that the number of embeddings matches the
    number of chunks:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最终验证是检查嵌入的数量是否与块的数量匹配：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output confirms that the chunking and embedding process is most probably
    successful:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认分块和嵌入过程很可能成功：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The chunks and embeddings are now ready to be upserted into the Pinecone index.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 文本块和嵌入现在已准备好更新到Pinecone索引。
- en: Creating the Pinecone index
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Pinecone索引
- en: 'The `genai-v1` Pinecone index we will create will contain two namespaces, as
    shown in *Figure 3.3*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要创建的`genai-v1` Pinecone索引将包含两个命名空间，如图*图3.3*所示：
- en: '`genaisys`: A repository of instruction scenarios. These prompts drive generative
    AI behavior and can also trigger traditional functions such as web search.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`genaisys`: 指令场景的存储库。这些提示驱动生成式AI行为，也可以触发传统功能，如网络搜索。'
- en: '`Data01`: The embedded classical data that the RAG pipeline queries.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Data01`: RAG管道查询的经典嵌入数据。'
- en: '![Figure 3.3: Partitioning the Pinecone index into namespaces](img/B32304_03_3.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3：将Pinecone索引分区为命名空间](img/B32304_03_3.png)'
- en: 'Figure 3.3: Partitioning the Pinecone index into namespaces'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：将Pinecone索引分区为命名空间
- en: 'We begin by importing two classes:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入两个类：
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `Pinecone` class is the primary interface to interact with the Pinecone
    index. We will use this class to configure Pinecone’s serverless services.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pinecone`类是与Pinecone索引交互的主要接口。我们将使用这个类来配置Pinecone的无服务器服务。'
- en: Before going further, you will need to set up a Pinecone account and obtain
    an API key. Make sure to verify the cost of these services at [https://www.pinecone.io/](https://www.pinecone.io/).
    This chapter is self-contained, so you can begin by reading the content, comments,
    and code before deciding on creating a Pinecone account.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，您需要设置一个Pinecone账户并获取一个API密钥。请确保在[https://www.pinecone.io/](https://www.pinecone.io/)上验证这些服务的费用。本章是自包含的，因此您可以在决定创建Pinecone账户之前先阅读内容、评论和代码。
- en: 'Once our account is set up, we need to retrieve and initialize our API key:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的账户设置好，我们需要检索并初始化我们的API密钥：
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We now import the specification class, define the name of our index (`genai-v1`),
    and initialize our first namespace (`genaisys`) for our scenarios:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在导入规范类，定义我们的索引名称（`genai-v1`），并为我们的场景初始化第一个命名空间（`genaisys`）：
- en: '[PRE38]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We now have a project management decision to make—use the Pinecone cloud to
    host our index or **Amazon Web Services** (**AWS**)?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要做出一个项目管理决策——使用Pinecone云托管我们的索引或**Amazon Web Services**（**AWS**）？
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The code first checks whether an environment variable (`PINECONE_CLOUD`) is
    set to use the Pinecone cloud. If there is no predefined environment variable
    check set, the variable defaults to AWS with `'aws'` and `'``us-east-1'` as the
    default region.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先检查是否设置了环境变量（`PINECONE_CLOUD`）以使用Pinecone云。如果没有预定义的环境变量检查设置，则变量默认为AWS，默认区域为`'aws'`和`'us-east-1'`。
- en: For more information, refer to the Pinecone Python SDK documentation at [https://docs.pinecone.io/reference/python-sdk](https://docs.pinecone.io/reference/python-sdk).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅Pinecone Python SDK文档[https://docs.pinecone.io/reference/python-sdk](https://docs.pinecone.io/reference/python-sdk)。
- en: 'In this case, AWS was chosen for the following reasons:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，选择 AWS 的原因如下：
- en: '**Market leadership and reliability**: AWS has a market share of over 30% of
    the global infrastructure market. As such, it is deemed reliable by a large number
    of organizations.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场领导力和可靠性**：AWS 在全球基础设施市场占有超过 30% 的市场份额。因此，它被大量组织认为是可靠的。'
- en: '**Compliance and security standards**: AWS has over 140 security standards
    for data security and privacy, including PCI-DSS and HIPAA/HITECH, FedRAMP, GDPR,
    FIPS 140-2, and NIST 800-171.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性和安全标准**：AWS 拥有超过 140 项数据安全和隐私安全标准，包括 PCI-DSS 和 HIPAA/HITECH、FedRAMP、GDPR、FIPS
    140-2 和 NIST 800-171。'
- en: '**Scalability**: AWS has a global network of data centers, making scalability
    seamless.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：AWS 拥有全球数据中心网络，使可扩展性无缝。'
- en: Alternatively, you can create an index manually in your Pinecone console to
    select the embedding model and the host, such as AWS or **Google Cloud Platform**
    (**GCP**). You can also select your pod size from x1 to more, which will determine
    the maximum size of your index. Each choice depends on your project and resource
    optimization strategy.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在 Pinecone 控制台中手动创建索引以选择嵌入模型和主机，例如 AWS 或 **Google Cloud Platform**（**GCP**）。您还可以选择从
    x1 到更多的大小，这将确定您索引的最大大小。每个选择都取决于您的项目和资源优化策略。
- en: In any case, we need metrics to monitor usage and cost. Pinecone provides detailed
    usage metrics accessible via your account, allowing you to manage indexes efficiently.
    For example, we might want to delete information you don’t need anymore, add targeted
    data, or optimize the usage per user.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，我们需要指标来监控使用情况和成本。Pinecone 通过您的账户提供详细的指标，使您能够有效地管理索引。例如，我们可能想要删除不再需要的信息，添加目标数据，或优化每个用户的使用情况。
- en: 'Pinecone provides three key metrics:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Pinecone 提供三个关键指标：
- en: '**Serverless storage usage**: Measured in **gigabyte-hours** (**GB-hours**).
    The cost is calculated at 1 GB of storage per hour. Carefully monitoring the amount
    of data we store is an important factor in any AI project.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器存储使用量**：以 **千兆时**（**GB-hours**）衡量。成本按每小时 1 GB 存储计算。仔细监控我们存储的数据量是任何 AI
    项目的重要因素。'
- en: '**Serverless write operations units**: Measures the resources consumed by write
    operations to the Pinecone database that contains our index.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器写入操作单位**：衡量写入到包含我们索引的 Pinecone 数据库的操作消耗的资源。'
- en: '**Serverless read operations** **units**: Measures the resources consumed by
    read operations.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器读取操作** **单位**：衡量读取操作消耗的资源。'
- en: 'You can download detailed information on your consumption by going to your
    Pinecone account, selecting **Usage**, and then clicking on the **Download** button,
    as shown here:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问您的 Pinecone 账户，选择**使用情况**，然后点击此处所示的**下载**按钮来下载您的详细消费信息：
- en: '![Figure 3.4: Downloading Pinecone usage data](img/B32304_03_4.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4：下载 Pinecone 使用数据](img/B32304_03_4.png)'
- en: 'Figure 3.4: Downloading Pinecone usage data'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：下载 Pinecone 使用数据
- en: The download file is in CSV format and contains a detailed account of our Pinecone
    usage, such as `BillingAccountId` (account identifier), `BillingAccountName` (account
    name), `OrganizationName` (organization name), `OrganizationId` (organization
    ID), `ProjectId` (project identifier), `ProjectName` (project name), `ResourceId`
    (resource identifier), `ResourceName` (resource name), `ChargePeriodStart` (charge
    start date), `ChargePeriodEnd` (charge end date), `BillingPeriodStart` (billing
    start date), `BillingPeriodEnd` (billing end date), `SkuId` (SKU identifier),
    `SkuPriceId` (SKU price ID), `ServiceName` (service name), `ChargeDescription`
    (charge details), `CloudId` (cloud provider), `RegionId` (region), `Currency`
    (currency type), `PricingQuantity` (usage quantity), `PricingUnit` (usage unit),
    `ListCost` (listed cost), `EffectiveCost` (calculated cost), `BilledCost` (final
    cost), and `Metadata` (additional data).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的文件是 CSV 格式，包含我们 Pinecone 使用情况的详细记录，例如 `BillingAccountId`（账户标识符）、`BillingAccountName`（账户名称）、`OrganizationName`（组织名称）、`OrganizationId`（组织
    ID）、`ProjectId`（项目标识符）、`ProjectName`（项目名称）、`ResourceId`（资源标识符）、`ResourceName`（资源名称）、`ChargePeriodStart`（收费开始日期）、`ChargePeriodEnd`（收费结束日期）、`BillingPeriodStart`（账单开始日期）、`BillingPeriodEnd`（账单结束日期）、`SkuId`（SKU
    标识符）、`SkuPriceId`（SKU 价格 ID）、`ServiceName`（服务名称）、`ChargeDescription`（收费详情）、`CloudId`（云提供商）、`RegionId`（区域）、`Currency`（货币类型）、`PricingQuantity`（使用量）、`PricingUnit`（使用单位）、`ListCost`（列出的成本）、`EffectiveCost`（计算成本）、`BilledCost`（最终成本）和
    `Metadata`（附加数据）。
- en: As AI slowly enters its industrial age, straying away from the initial excitement
    of the early 2020s, continuous monitoring of these metrics becomes increasingly
    critical.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能逐渐进入其工业时代，逐渐远离2020年代初的初始兴奋，对这些指标的不断监控变得越来越关键。
- en: 'We will now check whether the index we selected exists or not. The program
    imports the `pinecone` and `time` classes to insert a sleep time before checking
    whether the index exists:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检查我们选择的索引是否存在。程序导入`pinecone`和`time`类，在检查索引是否存在之前插入一个睡眠时间：
- en: '[PRE40]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If the index exists, the following code will be skipped to avoid creating duplicate
    indexes. If not, an index is created:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引已存在，以下代码将被跳过以避免创建重复索引。如果不存在，则创建索引：
- en: '[PRE41]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The parameters are as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '`index_name`, which is the name of our Pinecone index, `genai-v1`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_name`，这是我们的Pinecone索引名称，`genai-v1`'
- en: '`dimension=1536`, the dimensionality of the embedding vectors'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dimension=1536`，嵌入向量的维度'
- en: '`metric=''cosine''`, which sets the distance metric for similarity searches
    to cosine similarity'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric=''cosine''`，这设置了相似度搜索的距离度量标准为余弦相似度'
- en: '`spec=spec`, which defines the region and the serverless specification we defined
    previously for the cloud services'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec=spec`，它定义了我们之前为云服务定义的区域和服务器无规格'
- en: '`time.sleep(1)`, which makes the program wait to make sure the index is fully
    created before continuing'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time.sleep(1)`，这使得程序等待以确保索引完全创建后再继续'
- en: 'If the index has just been created, the output shows its details with `total_vector_count`
    set to `0` (if you see a number other than `0`, the notebook has likely already
    been run):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引刚刚创建，输出将显示其详细信息，其中`total_vector_count`设置为`0`（如果您看到除`0`以外的数字，笔记本可能已经运行过）：
- en: '[PRE42]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If the index already exists, the statistics will be displayed, including `index_fullness`
    to monitor the space used in your index pod from 0 to 1:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引已经存在，将显示统计信息，包括`index_fullness`以监控索引pod中使用的空间从0到1：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In this case, we haven’t populated the index yet. We can connect to the index
    we just created and display its statistics before populating it:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们尚未填充索引。我们可以在填充之前连接到我们刚刚创建的索引并显示其统计信息：
- en: '[PRE44]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output displays the information, confirming that we are connected:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示信息，确认我们已连接：
- en: '[PRE45]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The selected embedding model must match Pinecone’s index dimension (`1536`).
    We will create the parameters of a Pinecone index interactively when we begin
    working on use cases in [*Chapter 5*](Chapter_5.xhtml#_idTextAnchor140). Here,
    we are using `embedding_model="text-embedding-3-small` with its 1,536 dimensions,
    which matches the dimension of the Pinecone index.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的嵌入模型必须与Pinecone索引的维度（`1536`）匹配。当我们开始处理使用案例时，我们将交互式地创建Pinecone索引的参数，见[*第五章*](Chapter_5.xhtml#_idTextAnchor140)。在这里，我们使用`embedding_model="text-embedding-3-small`及其1,536维，这与Pinecone索引的维度相匹配。
- en: Note also that the `'genaisys'` namespace we initialized is taken into account.
    This ensures that when we upsert the scenarios we designed, they will not be confused
    with the classical data that is in another namespace of the same index. We are
    now ready to upsert the data to our Pinecone index.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我们初始化的`'genaisys'`命名空间被考虑在内。这确保了当我们上载我们设计的场景时，它们不会与另一个命名空间中的经典数据混淆。我们现在已准备好将数据上载到我们的Pinecone索引。
- en: Upserting instruction scenarios into the index
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将指令场景上载到索引
- en: 'Upserting embedded chunks into a Pinecone index comes with a cost, as explained
    at the beginning of this section. We must carefully decide which data to upsert.
    If we upsert all the data, we might do the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入块上载到Pinecone索引需要成本，如本节开头所述。我们必须仔细决定要上载哪些数据。如果我们上载所有数据，我们可能会做以下操作：
- en: Overload the index and make retrieval challenging, be it instruction scenarios
    or classical data
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过载索引并使检索变得具有挑战性，无论是指令场景还是经典数据
- en: Drive up the cost of write and read operations
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高读写操作的成本
- en: Add more noise than is manageable and confuse the retrieval functions
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加比可管理的更多的噪声并混淆检索函数
- en: 'If we choose not to upsert the data, we have two options:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择不上载数据，我们有两种选择：
- en: '**Querying in real time in memory**: Loading chunked, embedded data into memory
    and querying the information in real time could alleviate the data store and be
    a pragmatic way to deal with ephemeral information we don’t need to store, such
    as the daily weather forecast. However, we must also weigh the cost/benefit of
    this approach versus upserting at each step for the use cases we’ll be working
    on starting from [*Chapter 5*](Chapter_5.xhtml#_idTextAnchor140).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时内存查询**：将分块嵌入的数据加载到内存中，并实时查询信息可以减轻数据存储的负担，并且是一种处理我们不需要存储的临时信息的实用方法，例如每日天气预报。然而，我们也必须权衡这种方法与我们在从[*第五章*](Chapter_5.xhtml#_idTextAnchor140)开始的工作用例中每一步进行更新时的成本/效益。'
- en: '**Fine-tuning data**: This comes with the cost of building training datasets,
    which requires human and computing resources. In the case of fast-moving markets,
    we might have to fine-tune regularly, which entails high investments. The cost/benefit
    will be up to the project management team to consider. A cost-benefit analysis
    of fine-tuning versus RAG will be explored in [*Chapter 5*](Chapter_5.xhtml#_idTextAnchor140).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调数据**：这伴随着构建训练数据集的成本，这需要人力和计算资源。在快速变化的市场中，我们可能需要定期进行微调，这需要高额投资。成本/效益将由项目管理团队考虑。将在[*第五章*](Chapter_5.xhtml#_idTextAnchor140)中探讨微调与RAG的成本/效益分析。'
- en: 'We first initialize the libraries and start a timer to measure how long it
    takes to run the script:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先初始化库并开始计时器以测量脚本运行所需的时间：
- en: '[PRE46]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The program must then calculate the maximum size of the batch we send to Pinecone.
    It is set to 400,000 bytes, or 4 MB, to play it safe. If the limit is reached,
    the batch size is returned:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 程序随后必须计算发送到Pinecone的批次的最大大小。为了安全起见，它设置为400,000字节，或4 MB。如果达到限制，则返回批次大小：
- en: '[PRE47]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We now need an `upsert` function that takes the batch size into account when
    called:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要一个`upsert`函数，它在被调用时考虑批次大小：
- en: '[PRE48]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In production, we would typically exit on error, but for this educational notebook,
    printing helps us observe without stopping execution.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们通常会因错误而退出，但在这个教育笔记中，打印信息有助于我们观察而不停止执行。
- en: 'Note that we will upsert the instruction scenarios into the namespace, `genaisys`,
    within the Pinecone index. We can now define the main batch upsert function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将把指令场景更新到Pinecone索引中的命名空间`genaisys`内。我们现在可以定义主要的批次更新函数：
- en: '[PRE49]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The function begins by determining the total length of the data and then prepares
    batches that match the batch size that it will calculate with the `get_batch_size`
    function. Then, it creates a batch and sends it to the `upsert_to_pinecone` function
    we defined:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先确定数据的总长度，然后准备与`get_batch_size`函数计算出的批次大小相匹配的批次。然后，它创建一个批次并将其发送到我们定义的`upsert_to_pinecone`函数：
- en: '[PRE50]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'When the upsert is completed, the output will display a success message, signaling
    that we are ready to prepare the upsert process. A Pinecone index requires an
    ID that we will now create:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当更新完成后，输出将显示成功消息，表明我们已准备好准备更新过程。Pinecone索引需要一个ID，我们现在将创建它：
- en: '[PRE51]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Once each embedded chunk has an ID, we need to format the data to fit Pinecone’s
    index structure:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个嵌入块都有一个ID，我们需要格式化数据以适应Pinecone的索引结构：
- en: '[PRE52]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The data is now formatted with an ID, values (embeddings), and metadata (the
    chunks). Let’s call the `batch_upsert` function that will call the related functions
    we created:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已格式化，带有ID、值（嵌入）和元数据（块）。让我们调用`batch_upsert`函数，该函数将调用我们创建的相关函数：
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'When the upserting process is finished, the number of vectors upserted to the
    namespace and the time it took are displayed:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当更新过程完成后，将显示已更新到命名空间中的向量的数量以及所需时间：
- en: '[PRE54]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can also display the statistics of the Pinecone index:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示Pinecone索引的统计信息：
- en: '[PRE55]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note that you might have to wait a few seconds to give Pinecone time to update
    the index information.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可能需要等待几秒钟，以便给Pinecone更新索引信息的时间。
- en: 'The output displays the information:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示以下信息：
- en: '[PRE56]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The information displayed is as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的信息如下：
- en: '`''dimension'': 1536`: Dimension of the embeddings.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''dimension'': 1536`：嵌入的维度。'
- en: '`''index_fullness'': 0.0`: A value between 0 and 1 that shows how full the
    Pinecone index is. We must monitor this value to optimize the data we are upserting
    to avoid having to increase the size of the storage capacity we are using. For
    more information, consult the Pinecone documentation at [https://docs.pinecone.io/guides/get-started/overview](https://docs.pinecone.io/guides/get-started/overview).'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''index_fullness'': 0.0`: 一个介于 0 和 1 之间的值，显示 Pinecone 索引的满载程度。我们必须监控此值以优化我们正在
    upsert 的数据，以避免需要增加我们使用的存储容量的尺寸。有关更多信息，请参阅 Pinecone 文档[https://docs.pinecone.io/guides/get-started/overview](https://docs.pinecone.io/guides/get-started/overview)。'
- en: '`''namespaces'': {''genaisys'': {''vector_count'': 3}}`: Displays the namespace
    and vector count.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''namespaces'': {''genaisys'': {''vector_count'': 3}}`: 显示命名空间和向量数。'
- en: '`''total_vector_count'': 3}`: Displays the total vector count in the Pinecone
    index.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''total_vector_count'': 3}`: 显示 Pinecone 索引中的总向量数。'
- en: We are now ready to upload the classical data into its namespace.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好将经典数据上传到其命名空间。
- en: Upserting classical data into the index
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将经典数据 upsert 到索引中
- en: Building a GenAISys involves teams. So that each team can work in parallel to
    optimize production times, we will upsert the classical data in a separate program/notebook.
    One team can work on instruction scenarios while another team works on gathering
    and processing data.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 GenAISys 需要团队。为了使每个团队能够并行工作以优化生产时间，我们将在一个单独的程序/笔记本中 upsert 经典数据。一个团队可以处理指令场景，而另一个团队可以收集和处理数据。
- en: Open `Pinecone_RAG.ipynb`. We will be reusing several components of the `Pinecone_instruction_scenarios.ipynb`
    notebook built in the *Building a dynamic Pinecone index* section of this chapter.
    Setting up the environment is identical to the previous notebook. The Pinecone
    index is the same, `genai-v1`. The namespace for source-data upserting is `data01`,
    as we’ve already established in earlier sections, to make sure the data is separated
    from the instruction scenarios. So, the only real difference is the data we load
    and the chunking method. Let’s get into it!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `Pinecone_RAG.ipynb`。我们将重用本章“构建动态 Pinecone 索引”部分中构建的 `Pinecone_instruction_scenarios.ipynb`
    笔记本中的几个组件。设置环境与上一个笔记本相同。Pinecone 索引相同，`genai-v1`。源数据 upsert 的命名空间为 `data01`，正如我们在前面的章节中已经建立的，以确保数据与指令场景分开。因此，唯一的真正区别是我们加载的数据和分块方法。让我们开始吧！
- en: Data loading and chunking
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载和分块
- en: This section embeds chunks using the same process as for instruction scenarios
    in `Pinecone_instruction_scenarios.ipynb`. However, this time, GPT-4o does the
    chunking. When importing lines of instruction scenarios, we wanted to keep the
    integrity of the scenario in one chunk to be able to provide a complete set of
    instructions to the generative AI model. In this case, we will leverage the power
    of generative AI and chunk raw text with GPT-4o.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用与 `Pinecone_instruction_scenarios.ipynb` 中指令场景相同的流程嵌入块。然而，这次，GPT-4o 进行分块。当我们导入指令场景的行时，我们希望在一个块中保持场景的完整性，以便能够向生成
    AI 模型提供一组完整的指令。在这种情况下，我们将利用生成 AI 的力量，并使用 GPT-4o 对原始文本进行分块。
- en: 'We begin by downloading data, not scenarios, and setting the path of the file:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先下载数据，而不是场景，并设置文件路径：
- en: '[PRE57]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, the text file is loaded as one big chunk in a variable and displayed:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，文本文件已作为一个大块加载到一个变量中并显示：
- en: '[PRE58]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: While a production application would typically exit on a critical `FileNotFoundError`,
    for this educational notebook, printing the error allows us to observe the outcome
    without interrupting the learning flow.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产应用程序中，通常会因关键 `FileNotFoundError` 而退出，但对于这个教育笔记本，打印错误允许我们观察结果而不中断学习流程。
- en: 'You can comment `print(text)` or only print a few lines. In this case, let’s
    verify that we have correctly imported the file. The output shows that we did:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以注释 `print(text)` 或只打印几行。在这种情况下，让我们验证我们是否正确地导入了文件。输出显示我们已经这样做：
- en: '[PRE59]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The text contains a message from the CTO of the company whose data we are uploading
    to our custom RAG database. A company might have thousands of such internal messages—far
    too many (and too volatile) to justify model fine-tuning. Storing only the key
    chunks in Pinecone gives us searchable context without flooding the index with
    noise.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 文本包含来自我们上传到自定义 RAG 数据库的公司 CTO 的消息。一家公司可能有数千条这样的内部消息——太多（且太不稳定）以至于无法证明模型微调的合理性。仅在
    Pinecone 中存储关键块可以为我们提供可搜索的上下文，而不会使索引充满噪音。
- en: 'The `text` variable is not ready yet to be chunked by GPT-4o. The first step
    is to create an OpenAI instance and give the GPT-4o model instructions:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`text` 变量尚未准备好由 GPT-4o 分块。第一步是创建一个 OpenAI 实例并给出 GPT-4o 模型的指令：'
- en: '[PRE60]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We need to keep an eye on the `max_tokens=1024` setting: GPT-4o will stop generating
    once it hits that limit. For very large documents, you can stream the text in
    smaller slices—then let GPT-4o refine each slice. We can also use ready-made chunking
    functions that will break the text down into optimized *chunks* to obtain more
    nuanced and precise results when retrieving the data. However, in this case, let’s
    maximize the usage of GPT-4o; we send the entire file in one call with a low temperature
    so we can watch the model partition a real-world document from end to end.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要关注 `max_tokens=1024` 设置：GPT-4o 会在达到该限制时停止生成。对于非常大的文档，您可以将文本流式传输为较小的片段——然后让
    GPT-4o 精细化每个片段。我们还可以使用现成的分块函数，这些函数将文本分解为优化的 *块*，以在检索数据时获得更细微和精确的结果。然而，在这种情况下，让我们最大限度地使用
    GPT-4o；我们通过一个低温度的调用发送整个文件，这样我们可以观察模型从头到尾分割真实世界的文档。
- en: 'Now we can retrieve the chunks from the response, clean them, store them in
    a list of chunks, and return the `chunks` variable:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从响应中检索块，清理它们，将它们存储在块列表中，并返回 `chunks` 变量：
- en: '[PRE61]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, we can call the chunking function. We don’t have to display the chunks
    and can comment the code in production. However, in this case, let’s verify that
    everything is working:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以调用分块函数。我们不需要显示块，并且可以在生产环境中注释代码。然而，在这种情况下，让我们验证一切是否正常工作：
- en: '[PRE62]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output shows that the chunks were successfully created:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示块已成功创建：
- en: '[PRE63]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The remaining embedding and upsert steps are identical to those in `Pinecone_instruction_scenarios.ipynb`—just
    remember to use `namespace="data01"` when writing the vectors. After that, we’re
    ready to query the index and verify retrieval.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的嵌入和 Upsert 步骤与 `Pinecone_instruction_scenarios.ipynb` 中的步骤相同——只需记住在写入向量时使用
    `namespace="data01"`。之后，我们就准备好查询索引并验证检索。
- en: Querying the Pinecone index
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询 Pinecone 索引
- en: 'As you know, our vector store now has two logical areas—`genaisys` for instruction
    scenarios and `data01` for classical data. In this section, we’ll query each area
    interactively to prove the retrieval code works before we wire it into the multi-user
    interface in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110). We will query these
    two namespaces in the Pinecone index, as shown in *Figure 3.5*:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，我们的向量存储现在有两个逻辑区域—`genaisys` 用于指令场景，`data01` 用于经典数据。在本节中，我们将在将代码集成到 [*第
    4 章*](Chapter_4.xhtml#_idTextAnchor110) 中的多用户界面之前，交互式地查询每个区域以证明检索代码是有效的。我们将在 Pinecone
    索引中查询这两个命名空间，如图 *3.5* 所示：
- en: '![Figure 3.5: Generative AI model querying either the instruction scenarios
    or the data](img/B32304_03_5.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5：生成式 AI 模型查询指令场景或数据](img/B32304_03_5.png)'
- en: 'Figure 3.5: Generative AI model querying either the instruction scenarios or
    the data'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：生成式 AI 模型查询指令场景或数据
- en: 'Open `Query_Pinecone.ipynb` to run the verification queries. The next steps
    are the same as those in the *Setting up the environment* and *Creating the Pinecone
    index* sections, except for two minor differences:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `Query_Pinecone.ipynb` 运行验证查询。接下来的步骤与 *设置环境* 和 *创建 Pinecone 索引* 部分的步骤相同，除了两个细微的差异：
- en: 'The namespace is not provided when we connect to the Pinecone index, only its
    name: `index_name = ''genai-v1''`. This is because the querying function will
    manage the choice of a namespace.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到 Pinecone 索引时，我们不提供命名空间，只提供其名称：`index_name = 'genai-v1'`。这是因为查询函数将管理命名空间的选择。
- en: The `Upserting` section of the notebook has been removed because we are not
    upserting but querying the Pinecone index.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本中的 `Upserting` 部分已被移除，因为我们不是进行 Upsert 操作，而是在查询 Pinecone 索引。
- en: The `Query` section of the notebook is divided into two subsections. The first
    subsection contains the querying functions and the second one the querying requests.
    Let’s begin with the querying functions.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的 `Query` 部分分为两个子部分。第一个子部分包含查询函数，第二个子部分包含查询请求。让我们从查询函数开始。
- en: Querying functions
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询函数
- en: 'There are four querying functions, as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个查询函数，如下所示：
- en: 'QF1: `query_vector_store(query_text, namespace)`, which receives the query,
    sends the request to QF2, and returns the response. It will use QF4 to display
    the results.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QF1: `query_vector_store(query_text, namespace)`，它接收查询，将请求发送到 QF2，并返回响应。它将使用
    QF4 来显示结果。'
- en: 'QF2: `get_query_results(query_text, namespace)`, which receives the query from
    QF1, sends it to QF3 to be embedded, makes the actual query, and returns a response
    to QF1.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QF2: `get_query_results(query_text, namespace)`，它接收来自 QF1 的查询，将其发送到 QF3 进行嵌入，执行实际查询，并将响应返回给
    QF1。'
- en: 'QF3: `get_embedding(text, model=embedding_model)`, which receives text to embed
    from QF2 and sends the embedded text back to QF2.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QF3: `get_embedding(text, model=embedding_model)`，它接收来自 QF2 的要嵌入的文本，并将嵌入的文本发送回
    QF2。'
- en: 'QF4: `display_results(query_results)`, which receives the results from QF1,
    processes them, and returns them to QF1.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QF4: `display_results(query_results)`，它接收来自 QF1 的结果，处理它们，并将它们返回给 QF1。'
- en: 'We can simplify the representation as shown in *Figure 3.6* by creating two
    groups of functions:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建两个函数组来简化表示，如 *图 3.6* 所示：
- en: A group with QF1, `query_vector_store`, and QF4, `display_results`, in which
    QF1 queries the vector store through QF2 and returns the results to display.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 QF1、`query_vector_store` 和 QF4、`display_results` 的组，其中 QF1 通过 QF2 查询向量存储，并将结果返回以显示。
- en: A group with QF2, `get_query_results`, queries the vector store after embedding
    the query with QF3, `get_embedding`, and returns the results to QF1.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 QF2、`get_query_results` 的组在将查询通过 QF3、`get_embedding` 嵌入后查询向量存储，并将结果返回给
    QF1。
- en: '![Figure 3.6: Querying the vector store with two groups of functions](img/B32304_03_6.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6：使用两组函数查询向量存储](img/B32304_03_6.png)'
- en: 'Figure 3.6: Querying the vector store with two groups of functions'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：使用两组函数查询向量存储
- en: Let’s begin with the first group of functions.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一组函数开始。
- en: Querying the vector store and returning results
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询向量存储并返回结果
- en: 'The first function, QF1, receives the user input:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数，QF1，接收用户输入：
- en: '[PRE64]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Then, the function calls QF2, `query_results`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，函数调用 QF2 的 `query_results`：
- en: '[PRE65]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'QF2 then returns the results in `query_results`, which, in turn, is sent to
    `display_results` to obtain the text and target ID:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: QF2 然后返回 `query_results` 中的结果，该结果随后被发送到 `display_results` 以获取文本和目标 ID：
- en: '[PRE66]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '`display_results` processes the query results it receives and returns the result
    along with metadata to find the text obtained in the metadata of the Pinecone
    index. When it is found, the function retrieves the ID:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`display_results` 处理它接收到的查询结果，并返回结果以及元数据以找到在 Pinecone 索引的元数据中获得的文本。当找到时，函数检索
    ID：'
- en: '[PRE67]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The text and ID are returned to QF1, `query_vector_store`, which, in turn, returns
    the results when the function is called. Note that for educational purposes, this
    function assumes `query_results` will always contain at least one match with `'metadata'`
    and `'text'` fields. Let’s now see how the query is processed.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和 ID 返回给 QF1、`query_vector_store`，然后该函数在调用时返回结果。请注意，出于教育目的，此函数假定 `query_results`
    将始终至少包含一个具有 `'metadata'` 和 `'text'` 字段的匹配项。现在让我们看看查询是如何处理的。
- en: Processing the queries
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理查询
- en: 'The program queries the Pinecone index with `get_query_results` with the input
    text and namespace provided. But first, the input text must be embedded to enable
    a vector similarity search in the vector store:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 程序使用 `get_query_results` 函数查询 Pinecone 索引，输入文本和命名空间提供。但首先，必须将输入文本嵌入，以便在向量存储中进行向量相似度搜索：
- en: '[PRE68]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the input is embedded, a vector search is requested with the vectorized
    input within the namespace specified:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入被嵌入，就在指定的命名空间内请求使用向量化的输入进行向量搜索：
- en: '[PRE69]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Note that `k` is set to `1` in this example to retrieve a single top result
    for precision, and also, the metadata is set to `True` to include the corresponding
    text. The results are returned to QF2,`query_results`:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个例子中，`k` 被设置为 `1` 以获取单个最精确的结果，同时，元数据被设置为 `True` 以包含相应的文本。结果返回到 QF2 的 `query_results`：
- en: '[PRE70]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The embedding function is the same as what we used to upsert the data in the
    Pinecone index:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入函数与我们用于在 Pinecone 索引中更新数据时使用的函数相同：
- en: '[PRE71]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Make sure to use the same model to embed queries as you did to embed the data
    you upserted so that the embedded input is in the same vector format as the embedded
    data stored. This is critical for similarity search to make accurate similarity
    calculations.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 确保使用与嵌入您已更新的数据相同的模型来嵌入查询，以便嵌入的输入与存储的嵌入数据具有相同的向量格式。这对于进行准确的相似度计算至关重要。
- en: 'We’re now ready to run two tests: an instruction scenario query (namespace
    `genaisys`) and a source data query (namespace `data01`).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备运行两个测试：一个指令场景查询（命名空间 `genaisys`）和一个源数据查询（命名空间 `data01`）。
- en: Retrieval queries
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索查询
- en: 'To retrieve an instruction scenario, we will enter a user input and the namespace
    to let the system find the closest instruction to perform:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索一个指令场景，我们将输入用户输入和命名空间，让系统找到最接近的指令来执行：
- en: '[PRE72]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The system should detect the task briefly asked for and return a comprehensive
    instruction scenario. For that, we’ll call the entry point of the functions, `query_vector_store`,
    and display the output returned:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 系统应检测到简短提出的任务，并返回一个全面的指令场景。为此，我们将调用函数的入口点`query_vector_store`，并显示返回的输出：
- en: '[PRE73]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output is satisfactory and is ready to be used in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110)
    in a conversational loop:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输出令人满意，并准备好在[*第4章*](Chapter_4.xhtml#_idTextAnchor110)的对话循环中使用：
- en: '[PRE74]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The program now retrieves data from the Pinecone index. The query functions
    are identical since the namespace is a variable. Let’s just look at the query
    and output. The query is directed to the data namespace:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在从Pinecone索引中检索数据。由于命名空间是一个变量，查询函数是相同的。让我们只看看查询和输出。查询是针对数据命名空间的：
- en: '[PRE75]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output is satisfactory:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 输出令人满意：
- en: '[PRE76]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We have thus populated a Pinecone vector store and queried it. Let’s summarize
    the implementation of the Pinecone index before we move on to adding more layers
    to our GenAISys.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经填充了一个Pinecone向量存储并对其进行了查询。在我们继续添加更多层到我们的GenAISys之前，让我们总结一下Pinecone索引的实现。
- en: Summary
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we pushed our GenAISys project another step forward by moving
    beyond ordinary RAG. First, we layered expert-written instruction scenarios on
    top of the source data corpus, turning a static RAG pipeline into a dynamic framework
    that can fetch not only facts but also the exact reasoning pattern the model should
    follow. The global market is accelerating so quickly that users now expect ChatGPT-level
    assistance the moment a need arises; if we hope to keep pace, our architecture
    must be flexible, cost-aware, and capable of near-real-time delivery.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过超越普通的RAG，将我们的GenAISys项目推进了一步。首先，我们在源数据语料库之上叠加了专家编写的指令场景，将静态的RAG管道转变为一个动态框架，不仅可以检索事实，还可以检索模型应遵循的确切推理模式。全球市场正在迅速加速，用户现在在需要时立即期望ChatGPT级别的帮助；如果我们希望保持同步，我们的架构必须灵活、成本意识强，并且能够实现近乎实时交付。
- en: 'We began by laying out that architecture, then introduced the law of diminishing
    returns to determine when an implicit similarity search is worth its compute bill
    and when a direct, explicit call—such as a simple web search—will do the job more
    cheaply. With the theory in place, we wrote a program to download, chunk, embed,
    and upsert the instruction scenarios into a dedicated namespace inside a Pinecone
    index. Next, we enlisted GPT-4o to perform the same chunk-and-embed routine on
    the source documents, storing those vectors in a second namespace. Once both partitions
    were in place, we verified the retrieval layer: a single query function now routes
    any prompt to the correct namespace and returns the best match along with its
    metadata.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先概述了该架构，然后介绍了递减回报定律，以确定何时隐式相似性搜索值得其计算成本，何时直接、显式的调用——例如简单的网络搜索——能以更低成本完成任务。在理论确立之后，我们编写了一个程序，用于下载、分块、嵌入并将指令场景更新到一个Pinecone索引中的专用命名空间。接下来，我们部署了GPT-4o来对源文档执行相同的分块和嵌入流程，并将这些向量存储在第二个命名空间中。一旦两个分区都到位，我们就验证了检索层：现在单个查询函数会将任何提示路由到正确的命名空间，并返回最佳匹配及其元数据。
- en: With scenarios and data cleanly separated yet instantly searchable, the GenAISys
    has the retrieval backbone it needs. In the next chapter, we will plug these components
    into the conversational loop and let the system demonstrate its full, business-ready
    agility.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景和数据清晰分离且可即时搜索的情况下，GenAISys拥有了所需的检索骨干。在下一章中，我们将将这些组件插入到对话循环中，让系统展示其全面、业务就绪的敏捷性。
- en: Questions
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: There is no limit to automating all tasks in a generative AI system. (True or
    False)
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成式AI系统中自动化所有任务没有限制。（对或错）
- en: The law of diminishing returns is of no use in AI. (True or False)
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递减回报定律在人工智能中毫无用处。（对或错）
- en: Chunking is the process of breaking data into smaller parts to retrieve more
    nuanced information. (True or False)
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分块是将数据分解成更小部分以检索更细微信息的过程。（对或错）
- en: There is only one embedding model you should use. (True or False)
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该只使用一个嵌入模型。（对或错）
- en: Upserting data to a Pinecone index means uploading data to a database. (True
    or False)
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向Pinecone索引中上载数据意味着将数据上传到数据库。（对或错）
- en: A namespace is the name of a database. (True or False).
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命名空间是数据库的名称。（对或错）。
- en: A namespace can be used to access different types of data. (True or False)
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命名空间可用于访问不同类型的数据。（正确或错误）
- en: Querying the Pinecone index requires the user input to be embedded. (True or
    False)
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询 Pinecone 索引需要用户输入进行嵌入。（正确或错误）
- en: Querying the Pinecone index is based on a metric such as cosine similarity.
    (True or False)
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询 Pinecone 索引基于诸如余弦相似度之类的度量。（正确或错误）
- en: The Pinecone index and the query functions are the only components of a GenAISys.
    (True or False)
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pinecone 索引和查询函数是 GenAISys 的唯一组件。（正确或错误）
- en: References
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'OpenAI embeddings documentation: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 嵌入文档：[https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
- en: 'Pinecone Python SDK documentation: [https://docs.pinecone.io/reference/python-sdk](https://docs.pinecone.io/reference/python-sdk)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinecone Python SDK 文档：[https://docs.pinecone.io/reference/python-sdk](https://docs.pinecone.io/reference/python-sdk)
- en: 'Pinecone documentation: [https://docs.pinecone.io/guides/get-started/overview](https://docs.pinecone.io/guides/get-started/overview)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinecone 文档：[https://docs.pinecone.io/guides/get-started/overview](https://docs.pinecone.io/guides/get-started/overview)
- en: Further reading
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*AI Development Cost: Learn What Makes Developing an AI Solution*: [https://www.spaceo.ai/blog/ai-development-cost/](https://www.spaceo.ai/blog/ai-development-cost/)'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AI 开发成本：了解开发 AI 解决方案的因素*：[https://www.spaceo.ai/blog/ai-development-cost/](https://www.spaceo.ai/blog/ai-development-cost/)'
- en: '|'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|'
- en: Unlock this book’s exclusive benefits now
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 现在解锁此书的独家优惠
- en: Scan this QR code or go to [packtpub.com/unlock](http://packtpub.com/unlock),
    then search for this book by name. | ![A qr code on a white background  AI-generated
    content may be incorrect.](img/Unlock.png) |
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扫描此二维码或访问 [packtpub.com/unlock](http://packtpub.com/unlock)，然后通过书名搜索此书。 | ![白色背景上的二维码
    AI生成的内容可能不正确。](img/Unlock.png) |
- en: '| *Note: Keep your purchase invoice ready before you start.* |'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| *注意：在开始之前，请准备好您的购买发票。* |'
