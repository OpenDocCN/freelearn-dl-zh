- en: Generative Adversarial Networks and WaveNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络与WaveNet
- en: 'In this chapter, we will discuss **generative adversarial networks** (**GANs**)
    and WaveNets. GANs have been defined as *the most interesting idea in the last
    10 years in ML* ([https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning))
    by Yann LeCun, one of the fathers of deep learning. GANs are able to learn how
    to reproduce synthetic data that looks real. For instance, computers can learn
    how to paint and create realistic images. The idea was originally proposed by
    Ian Goodfellow (for more information refer to: *NIPS 2016 Tutorial: Generative
    Adversarial Networks*, by I. Goodfellow, 2016); he was worked with the University
    of Montreal, Google Brain, and recently OpenAI ([https://openai.com/](https://openai.com/)).
    WaveNet is a deep generative network proposed by Google DeepMind to teach computers
    how to reproduce human voices and musical instruments, both with impressive quality.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**生成对抗网络**（**GANs**）和WaveNet。GAN被Yann LeCun（深度学习的奠基人之一）称为*过去10年中机器学习领域最有趣的想法*（[https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning)）。GAN能够学习如何生成看起来真实的合成数据。例如，计算机可以学习如何绘画并创造逼真的图像。这个概念最初由Ian
    Goodfellow提出（更多信息请参见：*NIPS 2016教程：生成对抗网络*，I. Goodfellow，2016）；他曾与蒙特利尔大学、Google
    Brain合作，最近也参与了OpenAI的工作（[https://openai.com/](https://openai.com/)）。WaveNet是Google
    DeepMind提出的一种深度生成网络，用于教计算机如何再现人类语音和乐器声音，且质量令人印象深刻。
- en: 'In this chapter, we will cover cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is GAN?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是GAN？
- en: Deep convolutional GAN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积GAN
- en: Applications of GAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的应用
- en: What is a GAN?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是GAN？
- en: 'The key intuition of GAN can be easily considered as analogous to *art forgery*, which
    is the process of creating works of art ([https://en.wikipedia.org/wiki/Art](https://en.wikipedia.org/wiki/Art))
    that are falsely credited to other, usually more famous, artists. GANs train two
    neural nets simultaneously, as shown in the next diagram. The generator *G(Z) *makes
    the forgery, and the discriminator *D(Y)* can judge how realistic the reproductions
    based on its observations of authentic pieces of arts and copies are. *D(Y)* takes
    an input, *Y, *(for instance, an image) and expresses a vote to judge how real
    the input is--in general, a value close to zero denotes *real* and a value close
    to one denotes *forgery*. *G(Z)* takes an input from a random noise, *Z*, and
    trains itself to fool *D* into thinking that whatever *G(Z)* produces is real.
    So, the goal of training the discriminator *D(Y)* is to maximize *D(Y)* for every
    image from the true data distribution, and to minimize *D(Y*) for every image
    not from the true data distribution. So, *G* and *D* play an opposite game; hence
    the name *adversarial training*. Note that we train *G* and *D* in an alternating
    manner, where each of their objectives is expressed as a loss function optimized
    via a gradient descent. The generative model learns how to forge more successfully,
    and the discriminative model learns how to recognize forgery more successfully.
    The discriminator network (usually a standard convolutional neural network) tries
    to classify whether an input image is real or generated. The important new idea
    is to backpropagate through both the discriminator and the generator to adjust
    the generator''s parameters in such a way that the generator can learn how to
    fool the the discriminator for an increasing number of situations. At the end,
    the generator will learn how to produce forged images that are indistinguishable
    from real ones:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的关键直觉可以很容易地类比为*艺术伪造*，即创造那些被错误归功于其他、更著名艺术家的艺术作品的过程 ([https://en.wikipedia.org/wiki/Art](https://en.wikipedia.org/wiki/Art))。GAN同时训练两个神经网络，如下图所示。生成器*G(Z)*进行伪造，而判别器*D(Y)*则根据对真实艺术品和复制品的观察判断复制品的真实性。*D(Y)*接受一个输入，*Y*，（例如一张图片），并做出判断，判定输入的真实性——一般来说，接近零的值表示*真实*，接近一的值表示*伪造*。*G(Z)*从随机噪声*Z*中接收输入，并通过训练让自己欺骗*D*，让*D*误认为*G(Z)*生成的内容是真的。因此，训练判别器*D(Y)*的目标是最大化每个真实数据分布中的图像的*D(Y)*，并最小化每个不属于真实数据分布的图像的*D(Y)*。因此，*G*和*D*进行的是一场对立的博弈；这就是所谓的*对抗训练*。请注意，我们以交替的方式训练*G*和*D*，每个目标都作为损失函数通过梯度下降优化。生成模型学习如何更成功地伪造，而判别模型学习如何更成功地识别伪造。判别器网络（通常是标准的卷积神经网络）试图对输入图像进行分类，判断它是现实的还是生成的。这个新颖的核心思想是通过生成器和判别器进行反向传播，以调整生成器的参数，让生成器学会如何在更多的情境下欺骗判别器。最终，生成器将学会如何生成与真实图像无法区分的伪造图像。
- en: '![](img/B06258_04a_01.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_01.png)'
- en: 'Of course, GANs require finding the equilibrium in a game with two players.
    For effective learning it is required that if a player successfully moves downhill
    in a round of updates, the same update must move the other player downhill too.
    Think about it! If the forger learns how to fool the judge on every occasion,
    then the forger himself has nothing more to learn. Sometimes the two players eventually
    reach an equilibrium, but this is not always guaranteed and the two players can
    continue playing for a long time. An example of learning from both sides has been
    provided in the following graph:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，GAN需要在两方博弈中找到平衡。为了有效学习，需要确保如果一方成功地在一次更新中向下调整，那么另一方也必须在同样的更新中向下调整。想一想！如果造假者每次都能成功地欺骗裁判，那么造假者自己就没有更多的学习空间了。有时，两个玩家最终会达到平衡，但这并不总是能保证，两个玩家可能会继续对抗很长时间。以下图展示了来自双方学习的一个例子：
- en: '![](img/B06258_04a_001.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_001.png)'
- en: Some GAN applications
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些GAN应用
- en: 'We have seen that the generator learns how to forge data. This means that it
    learns how to create new synthetic data, which is created by the network, that
    looks real and like it was created by humans. Before going into details of some
    GAN code, I''d like to share the results of a recent paper: *StackGAN: Text to
    Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks*,
    by Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang,
    and Dimitris Metaxas (the code is available online at: [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经看到生成器学会了如何伪造数据。这意味着它学会了如何创造新的合成数据，这些数据由网络生成，看起来真实且像是由人类创造的。在深入探讨一些GAN代码的细节之前，我想分享一篇近期论文的结果：*StackGAN:
    文本到照片级图像合成的堆叠生成对抗网络*，作者：Han Zhang、Tao Xu、Hongsheng Li、Shaoting Zhang、Xiaolei Huang、Xiaogang
    Wang 和 Dimitris Metaxas（代码可在线获取：[https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)）。'
- en: 'Here, a GAN has been used to synthesize forged images starting from a text
    description. The results are impressive. The first column is the real image in
    the test set, and the rest of the columns contain images generated from the same
    text description by Stage-I and Stage-II of StackGAN. More examples are available
    on YouTube ([https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be](https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be)):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用GAN从文本描述合成伪造图像。结果令人印象深刻。第一列是测试集中的真实图像，剩下的列包含由StackGAN的Stage-I和Stage-II根据相同的文本描述生成的图像。更多例子可在YouTube查看（[https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be](https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be)）：
- en: '![](img/B06258_04a_002.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_002.jpg)'
- en: '![](img/B06258_04a_003.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_003.jpg)'
- en: 'Now let us see how a GAN can learn to *forge* the MNIST dataset. In this case,
    there is a combination of GAN and ConvNets (for more information refer to: *Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks*,
    by A. Radford, L. Metz, and S. Chintala, arXiv: 1511.06434, 2015) used for the
    generator and the discriminator networks. At the beginning, the generator creates
    nothing understandable, but after a few iterations, synthetic forged numbers are
    progressively clearer and clearer. In the following image, the panels are ordered
    by increasing training epochs, and you can see the quality improving among panels:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们看看GAN是如何学会*伪造*MNIST数据集的。在这种情况下，生成器和判别器网络结合了GAN和ConvNets（更多信息请参考：*无监督表示学习与深度卷积生成对抗网络*，A.
    Radford、L. Metz 和 S. Chintala，arXiv: 1511.06434，2015年）用于生成器和判别器网络。一开始，生成器什么都没有生成，但经过几次迭代后，合成的伪造数字逐渐变得越来越清晰。在下图中，面板按训练轮次递增排序，你可以看到面板之间质量的提升：'
- en: '![](img/B06258_04a_004.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_004.png)'
- en: 'The following image represents the forged handwritten numbers as the number
    of iterations increases:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了随着迭代次数的增加，伪造的手写数字的变化：
- en: '![](img/B06258_04a_005.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_005.png)'
- en: 'The following image represents the forged handwritten numbers at the hand of
    computation. The results are virtually indistinguishable from the original:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了伪造的手写数字，计算结果与原始几乎无法区分：
- en: '![](img/B06258_04a_006.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_006.png)'
- en: 'One of the coolest uses of GAN is arithmetic on faces in the generator''s vector
    *Z*. In other words, if we stay in the space of synthetic forged images, it is
    possible to see things like this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GAN最酷的用途之一就是对生成器的向量*Z*中的人脸进行算术操作。换句话说，如果我们停留在合成伪造图像的空间中，就能看到像这样的效果：
- en: '*[smiling woman] - [neutral woman] + [neutral man] = [smiling man]*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*[微笑的女人] - [中立的女人] + [中立的男人] = [微笑的男人]*'
- en: 'Or like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 或者像这样：
- en: '*[man with glasses] - [man without glasses] + [woman without glasses] = [woman
    with glasses]*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*[戴眼镜的男人] - [没有眼镜的男人] + [没有眼镜的女人] = [戴眼镜的女人]*'
- en: 'The next image is taken from the article, *Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*, by A. Radford, L. Metz,
    and S. Chintala, arXiv: 1511.06434, November, 2015:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '下一张图来自文章 *无监督表示学习与深度卷积生成对抗网络*，作者：A. Radford、L. Metz 和 S. Chintala，arXiv: 1511.06434，2015年11月：'
- en: '![](img/B06258_04a_007.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_007.png)'
- en: Deep convolutional generative adversarial networks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络
- en: 'The **deep convolutional generative adversarial networks** (**DCGAN**) are introduced
    in the paper: *Unsupervised Representation Learning with Deep Convolutional Generative
    Adversarial Networks*, by A. Radford, L. Metz, and S. Chintala, arXiv: 1511.06434,
    2015\. The generator uses a 100-dimensional, uniform distribution space, *Z,*
    which is then projected into a smaller space by a series of vis-a-vis convolution
    operations. An example is shown in the following figure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度卷积生成对抗网络**（**DCGAN**）在论文中介绍：*《使用深度卷积生成对抗网络进行无监督表示学习》*，作者A. Radford, L.
    Metz和S. Chintala，arXiv: 1511.06434, 2015。生成器使用一个100维的均匀分布空间*Z*，然后通过一系列卷积操作将其投影到一个更小的空间中。下图展示了一个示例：'
- en: '![](img/B06258_04a_008.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_008.png)'
- en: 'A DCGAN generator can be described by the following Keras code; it is also
    described by one implementation, available at: [https://github.com/jacobgil/keras-dcgan](https://github.com/jacobgil/keras-dcgan):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个DCGAN生成器可以通过以下Keras代码进行描述；它也被一个实现所描述，具体请参见：[https://github.com/jacobgil/keras-dcgan](https://github.com/jacobgil/keras-dcgan)：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that the code runs with Keras 1.x syntax. However, it is possible to run
    it with Keras 2.0 thanks to the Keras legacy interfaces. In this case a few warnings
    are reported as shown in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，代码使用的是Keras 1.x的语法。然而，由于Keras的遗留接口，它也可以在Keras 2.0中运行。在这种情况下，系统会报告一些警告，如下图所示：
- en: '![](img/dcgan-compatibility.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcgan-compatibility.png)'
- en: 'Now let’s see the code. The first dense layer takes a vector of 100 dimensions
    as input and it produces 1,024 dimensions with the activation function `tanh`
    as the output*.* We assume that the input is sampled from a uniform distribution
    in *[-1, 1]*. The next dense layer produces data of 128 x 7 x 7 in the output
    using batch normalization (for more information refer to *Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift*, by S.
    Ioffe and C. Szegedy, arXiv: 1502.03167, 2014), a technique that can help stabilize
    learning by normalizing the input to each unit to zero mean and unit variance*. *Batch
    normalization has been empirically proven to accelerate the training in many situations,
    reduce the problems of poor initialization, and more generally produce more accurate
    results. There is also a `Reshape()` module that produces data of 127 x 7 x 7
    (127 channels, 7 width, and 7 height), `dim_ordering` to `tf`, and a `UpSampling()`
    module that produces a repetition of each one into a 2 x 2 square. After that,
    we have a convolutional layer producing 64 filters on 5 x 5 convolutional kernels
    with the activation `tanh`*,* followed by a new `UpSampling()` and a final convolution
    with one filter, and on 5 x 5 convolutional kernels with the activation `tanh`*.*
    Notice that this ConvNet has no pooling operations. The discriminator can be described
    with the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们看一下代码。第一层全连接层接收一个100维的向量作为输入，并使用激活函数`tanh`输出1,024维。我们假设输入是从均匀分布中采样，范围为*[-1,
    1]*。接下来的全连接层通过批量归一化（更多信息请参见*S. Ioffe和C. Szegedy的《批量归一化：通过减少内部协变量偏移加速深度网络训练》，arXiv:
    1502.03167, 2014*）生成128 x 7 x 7的输出数据，批量归一化是一种通过将每个单元的输入归一化为零均值和单位方差来帮助稳定学习的技术。*批量归一化已被实验证明在许多情况下可以加速训练，减少初始化不良的问题，并且通常能产生更精确的结果。*此外，还有一个`Reshape()`模块，它将数据转换为127
    x 7 x 7（127个通道，7的宽度，7的高度），`dim_ordering`设置为`tf`，以及一个`UpSampling()`模块，它将每个数据重复为2
    x 2的正方形。然后，我们有一个卷积层，在5 x 5卷积核上生成64个滤波器，激活函数为`tanh`*，接着是一个新的`UpSampling()`模块和一个最终的卷积层，具有一个滤波器，使用5
    x 5卷积核，激活函数为`tanh`*。请注意，该卷积神经网络没有池化操作。判别器可以通过以下代码进行描述：'
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The code takes a standard MNIST image with the shape `(1, 28, 28)` and applies
    a convolution with 64 filters of size 5 x 5 with `tanh` as the activation function.
    This is followed by a max-pooling operation of size 2 x 2 and by a further convolution
    max-pooling operation. The last two stages are dense, with the final one being
    the prediction for forgery, which consists of only one neuron with a `sigmoid`
    activation function. For a chosen number of epochs, the generator and discriminator
    are in turn trained by using `binary_crossentropy` as loss function. At each epoch,
    the generator makes a number of predictions (for example, it creates forged MNIST
    images) and the discriminator tries to learn after mixing the prediction with
    real MNIST images. After 32 epochs, the generator learns to forge this set of
    handwritten numbers. No one has programmed the machine to write but it has learned
    how to write numbers that are indistinguishable from the ones written by humans.
    Note that training GANs could be very difficult because it is necessary to find
    the equilibrium between two players. If you are interested in this topic, I''d
    advise you to have a look at a series of tricks collected by practitioners ([https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码接收一个标准的 MNIST 图像，形状为 `(1, 28, 28)`，并应用一个 64 个 5 x 5 滤波器的卷积，激活函数为 `tanh`。随后进行一个
    2 x 2 的最大池化操作，接着再进行一次卷积和最大池化操作。最后两步是全连接层，其中最后一步是伪造的预测，只有一个神经元，并使用 `sigmoid` 激活函数。对于选定的训练轮数，生成器和判别器依次使用
    `binary_crossentropy` 作为损失函数进行训练。在每个训练轮次中，生成器会做出若干预测（例如，它生成伪造的 MNIST 图像），而判别器则尝试在将预测与真实的
    MNIST 图像混合后进行学习。经过 32 个轮次，生成器学会了伪造这一组手写数字。没有人编程让机器写字，但它已经学会了如何写出与人类手写的数字无法区分的数字。请注意，训练
    GAN 可能非常困难，因为需要在两个参与者之间找到平衡。如果你对这个话题感兴趣，我建议你查看一些从业者收集的技巧系列（[https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)）：
- en: '![](img/Capture.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Capture.png)'
- en: Keras adversarial GANs for forging MNIST
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 对抗性 GAN 用于伪造 MNIST
- en: 'Keras adversarial ([https://github.com/bstriner/keras-adversarial](https://github.com/bstriner/keras-adversarial))
    is an open source Python package for building GANs developed by Ben Striner ([https://github.com/bstriner](https://github.com/bstriner) and [https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt](https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt)).
    Since Keras just recently moved to 2.0, I suggest downloading latest Keras adversarial
    package:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 对抗性（[https://github.com/bstriner/keras-adversarial](https://github.com/bstriner/keras-adversarial)）是一个用于构建
    GAN 的开源 Python 包，由 Ben Striner 开发（[https://github.com/bstriner](https://github.com/bstriner)
    和 [https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt](https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt)）。由于
    Keras 最近刚刚升级到 2.0，我建议你下载最新的 Keras 对抗性包：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And install `setup.py`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 并安装 `setup.py`：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that compatibility with Keras 2.0 is tracked in this issue[ https://github.com/bstriner/keras-adversarial/issues/11](https://github.com/bstriner/keras-adversarial/issues/11).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Keras 2.0 的兼容性已在此问题中跟踪：[https://github.com/bstriner/keras-adversarial/issues/11](https://github.com/bstriner/keras-adversarial/issues/11)。
- en: 'If the generator *G* and the discriminator *D* are based on the same model, *M,* then
    they can be combined into an adversarial model; it uses the same input, *M*, but
    separates targets and metrics for *G* and *D*. The library has the following API
    call:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成器*G*和判别器*D*基于相同的模型，*M*，那么它们可以合并为一个对抗模型；它使用相同的输入，*M*，但为*G*和*D*分开目标和度量。该库有以下
    API 调用：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If the generator *G* and the discriminator *D* are based on the two different
    models, then it is possible to use this API call:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成器*G*和判别器*D*基于两个不同的模型，则可以使用此 API 调用：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s see an example of a computation with MNIST:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个关于 MNIST 的计算示例：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let us see the open source code ([https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py](https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py)).
    Note that the code uses the syntax of Keras 1.x, but it also runs on the top of
    Keras 2.x thanks to a convenient set of utility functions contained in `legacy.py`.
    The code for `legacy.py` is reported in [Appendix](c0a1905f-57cc-401c-b485-6bf0854e43e9.xhtml),
    *Conclusion*, and is available at [https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py](https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看开源代码（[https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py](https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py)）。注意，这段代码使用的是Keras
    1.x的语法，但得益于`legacy.py`中包含的便捷实用函数集，它也能在Keras 2.x之上运行。`legacy.py`的代码在[附录](c0a1905f-57cc-401c-b485-6bf0854e43e9.xhtml)、*结论*中有报告，且可以在[https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py](https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py)找到。
- en: 'First, the open source example imports a number of modules. We have seen all
    of them previously, with the exception of LeakyReLU, a special version of ReLU
    that allows a small gradient when the unit is not active. Experimentally, it has
    been shown that LeakyReLU can improve the performance of GANs (for more information
    refer to: *Empirical Evaluation of Rectified Activations in Convolutional Network*,
    by B. Xu, N. Wang, T. Chen, and M. Li, arXiv:1505.00853, 2014) in a number of
    situations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，开源示例导入了一些模块。除了LeakyReLU（ReLU的一个特殊版本，当单元不活跃时允许一个小的梯度）之外，我们之前都见过这些模块。实验表明，LeakyReLU可以在多种情况下提高GAN的性能（有关更多信息，请参阅：*Empirical
    Evaluation of Rectified Activations in Convolutional Network*，B. Xu, N. Wang,
    T. Chen 和 M. Li，arXiv:1505.00853，2014）：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, specific modules for GANs are imported:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，导入GAN的特定模块：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Adversarial models train for multiplayer games. Given a base model with *n*
    targets and *k* players, create a model with *n*k* targets, where each player
    optimizes loss on that player''s targets. In addition, `simple_gan` generates
    a GAN with the given `gan_targets`. Note that in the library, the labels for generator
    and discriminator are opposite; intuitively, this is a standard practice for GANs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性模型用于多玩家游戏。给定一个包含*n*个目标和*k*个玩家的基础模型，创建一个包含*n*k*个目标的模型，其中每个玩家在该玩家的目标上优化损失。此外，`simple_gan`生成一个具有给定`gan_targets`的GAN。注意，在库中，生成器和判别器的标签是相反的；直观地说，这对GAN来说是一种标准做法：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The example defines the generator in a similar way to what we have seen previously.
    However, in this case, we use the functional syntax—each module in our pipeline
    is simply passed as input to the following module. So, the first module is dense,
    initialized by using `glorot_normal`. This initialization uses Gaussian noise
    scaled by the sum of the inputs plus outputs from the node. The same kind of initialization
    is used for all of the other modules. The `mode=2` parameter in `BatchNormlization` function
    produces feature-wise normalization based on per-batch statistics. Experimentally,
    this produces better results:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例以类似于我们之前看到的方式定义了生成器。然而，在这种情况下，我们使用了函数式语法——我们管道中的每个模块只是作为输入传递给下一个模块。因此，第一个模块是密集层，通过使用`glorot_normal`进行初始化。此初始化使用了通过节点的输入和输出总和缩放的高斯噪声。所有其他模块都使用相同类型的初始化。`BatchNormlization`函数中的`mode=2`参数根据每批次的统计数据进行特征标准化。从实验结果来看，这能产生更好的效果：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The discriminator is very similar to the one defined previously in this chapter.
    The only major difference is the adoption of `LeakyReLU`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器与本章之前定义的非常相似。唯一的主要区别是采用了`LeakyReLU`：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, two simple functions for loading and normalizing MNIST data are defined:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义了两个简单的函数来加载和标准化MNIST数据：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As a next step, the GAN is defined as a combination of generator and discriminator
    in a joint GAN model. Note that the weights are initialized with `normal_latent_sampling`,
    which samples from a normal Gaussian distribution:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，GAN被定义为生成器和判别器的组合，形成一个联合GAN模型。注意，权重通过`normal_latent_sampling`进行初始化，它从正态高斯分布中采样：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After this, the example creates our GAN and it compiles the model trained using
    the `Adam` optimizer, with `binary_crossentropy` used as a loss function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，示例创建了我们的GAN，并使用`Adam`优化器编译训练好的模型，`binary_crossentropy`作为损失函数：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The generator for creating new images that look like real ones is defined.
    Each epoch will generate a new forged image during training that looks like the
    original:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建看起来像真实图像的新图像的生成器被定义。每个epoch将在训练期间生成一张看起来像原始图像的伪造图像：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that `dim_ordering_unfix` is utility function for supporting different
    image ordering defined in `image_utils.py`, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`dim_ordering_unfix`是一个实用函数，用于支持在`image_utils.py`中定义的不同图像排序，具体如下：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now let''s run the code and see the loss for the generator and discriminator.
    In the following screenshot, we see a dump of the networks for the discriminator
    and the generator:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行代码，查看生成器和判别器的损失情况。在下面的截图中，我们看到了判别器和生成器的网络输出：
- en: '![](img/keras_adversarial_MNIST.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/keras_adversarial_MNIST.png)'
- en: 'The following screenshot, shows the number of sample used for training and
    for validation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了用于训练和验证的样本数量：
- en: '![](img/B06258_04a_010.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_010.png)'
- en: 'After 5-6 iterations, we already have acceptable artificial images generated
    and the computer has learned how to reproduce handwritten characters, as shown
    in the following image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过5-6次迭代后，我们已经生成了可以接受的人工图像，计算机已经学会如何重现手写字符，如下图所示：
- en: '![](img/B06258_04a_011.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_011.png)'
- en: Keras adversarial GANs for forging CIFAR
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于伪造CIFAR的Keras对抗GAN
- en: 'Now we can use a GAN approach to learn how to forge CIFAR-10 and create synthetic
    images that look real. Let''s see the open source code ([https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py](https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py)).
     Again, note that it uses the syntax of Keras 1.x, but it also runs on the top
    of Keras 2.x thanks to a convenient set of utility functions contained in `legacy.py`
    ([https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py](https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py)).
    First, the open source example imports a number of packages:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用GAN方法学习如何伪造CIFAR-10，并创建看起来真实的合成图像。让我们看看开源代码（[https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py](https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py)）。再次注意，它使用的是Keras
    1.x的语法，但得益于`legacy.py`中包含的一套方便的实用函数，它也能在Keras 2.x上运行（[https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py](https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py)）。首先，开源示例导入了一些包：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, it defines a generator that uses a combination of convolutions with `l1`
    and `l2` regularization, batch normalization, and upsampling. Note that `axis=1`
    says to normalize the dimension of the tensor first and `mode=0` says to adopt
    a feature-wise normalization. This particular net is the result of many fine-tuning
    experiments, but it is still essentially a sequence of convolution 2D and upsampling
    operations, which uses a `Dense` module at the beginning and a `sigmoid` at the
    end. In addition, each convolution uses a `LeakyReLU` activation function and
    `BatchNormalization`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义了一个生成器，它使用卷积与`l1`和`l2`正则化、批归一化和上采样的组合。请注意，`axis=1`表示首先对张量的维度进行归一化，`mode=0`表示采用特征归一化。这个特定的网络是许多精细调整实验的结果，但它本质上仍然是一个卷积2D和上采样操作的序列，在开始时使用`Dense`模块，结束时使用`sigmoid`。此外，每个卷积使用`LeakyReLU`激活函数和`BatchNormalization`：
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, a discriminator is defined. Again, we have a sequence of convolution
    2D operations, and in this case we adopt `SpatialDropout2D`, which drops entire
    2D feature maps instead of individual elements. We also use `MaxPooling2D` and
    `AveragePooling2D` for similar reasons:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义了一个判别器。再次，我们有一系列的二维卷积操作，在这种情况下我们采用`SpatialDropout2D`，它会丢弃整个2D特征图而不是单个元素。我们还使用`MaxPooling2D`和`AveragePooling2D`，原因类似：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It is now possible to generate proper GANs. The following function takes multiple
    inputs, including a generator, a discriminator, the number of latent dimensions,
    and the GAN targets:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以生成适当的GAN。以下函数接受多个输入，包括生成器、判别器、潜在维度的数量和GAN目标：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then two GANs are created, one with dropout and the other without dropout for
    the discriminator:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建了两个GAN，一个带有dropout，另一个没有dropout用于判别器：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The two GANs are now combined into an adversarial model with separate weights,
    and the model is then compiled:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个GAN现在被合并为一个具有独立权重的对抗性模型，模型随后被编译：
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, there is a simple callback to sample images and a print on the file where
    the method `ImageGridCallback` is defined:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，有一个简单的回调，用于采样图像，并打印定义了`ImageGridCallback`方法的文件：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, the CIFAR-10 data is loaded and the model is fit. If the backend is TensorFlow,
    then the loss information is saved into a TensorBoard to check how the loss decreases
    over time. The history is also conveniently saved into a CVS format, and the models''
    weights are also stored in an `h5` format:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，CIFAR-10数据已经加载并且模型已拟合。如果后端是TensorFlow，则损失信息将保存在TensorBoard中，以检查损失如何随着时间的推移减少。历史数据也方便地保存在CVS格式中，模型的权重也存储在`h5`格式中：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, the whole GANs can be run. The generator samples from a space with
    100 latent dimensions, and we''ve used `Adam` as optimizer for both GANs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整个GAN可以运行。生成器从一个具有100个潜在维度的空间中采样，我们为这两个GAN使用了`Adam`优化器：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In order to have a complete view on the open source code, we need to include
    a few simple utility functions for storing the grid of images:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整查看开源代码，我们需要包含一些简单的工具函数来存储图像的网格：
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In addition, we need some utility methods for dealing with different image
    ordering (for example, Theano or TensorFlow):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要一些工具方法来处理不同的图像排序（例如，Theano或TensorFlow）：
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following screenshot, shows a dump of the defined networks:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了定义的网络的转储：
- en: '![](img/keras_adversarial_CIFAR.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/keras_adversarial_CIFAR.png)'
- en: 'If we run the open source code, the very first iteration will generate unrealistic
    images. However, after 99 iterations, the network will learn to forge images that look
    like real CIFAR-10 images, as shown here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行开源代码，第一次迭代将生成不真实的图像。然而，在经过99次迭代后，网络将学会伪造看起来像真实CIFAR-10图像的图像，如下所示：
- en: '![](img/B06258_04a_012.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_012.png)'
- en: 'In the following images, we see the real CIFAR-10 image on the right and the
    forged one on the left:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的图像中，我们看到右边是真实的CIFAR-10图像，左边是伪造的图像：
- en: '| Forged images | Real CIFAR-10 images |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 伪造的图像 | 真实的CIFAR-10图像 |'
- en: '| ![](img/B06258_04a_013.png) | ![](img/B06258_04a_014.png) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B06258_04a_013.png) | ![](img/B06258_04a_014.png) |'
- en: WaveNet — a generative model for learning how to produce audio
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WaveNet——一种用于学习如何生成音频的生成模型
- en: 'WaveNet is a deep generative model for producing raw audio waveforms. This
    breakthrough technology was introduced ([https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/))
    by Google DeepMind ([https://deepmind.com/](https://deepmind.com/)) for teaching
    users how to speak to computers. The results are truly impressive, and you can
    find online examples of synthetic voices where the computer learns how to talk
    with the voices of celebrities such as Matt Damon. So, you might wonder why learning
    to synthesize audio is so difficult. Well, each digital sound we hear is based
    on 16,000 samples per second (sometimes, 48,000 or more), and building a predictive
    model where we learn to reproduce a sample based on all the previous ones is a
    very difficult challenge. Nevertheless, there are experiments showing that WaveNet
    has improved current state-of-the-art **text-to-speech** (**TTS**) systems, reducing
    the difference with human voices by 50% for both US English and Mandarin Chinese.
    What is even cooler is that DeepMind proved that WaveNet can also be used to teach
    computers how to generate the sound of musical instruments such as piano music.
    Now it''s time for some definitions. TTS systems are typically divided into two
    different classes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet是一种深度生成模型，用于生成原始音频波形。这项突破性的技术是由Google DeepMind（[https://deepmind.com/](https://deepmind.com/)）提出的，用于教用户如何与计算机对话。结果令人印象深刻，您可以在网上找到合成语音的例子，其中计算机学会了如何用名人如马特·达蒙的声音进行对话。那么，您可能会想，为什么学习合成音频如此困难呢？好吧，我们听到的每一个数字声音都是基于每秒16,000个样本（有时为48,000或更多），而构建一个预测模型，基于所有先前的样本学习如何重现一个样本，是一个非常困难的挑战。尽管如此，实验表明，WaveNet已经改进了当前最先进的**文本到语音**（**TTS**）系统，在美式英语和普通话中，将与人声的差距缩小了50%。更酷的是，DeepMind证明WaveNet还可以用来教计算机生成钢琴音乐等乐器的声音。现在是时候介绍一些定义了。TTS系统通常分为两类：
- en: '**Concatenative TTS**: This is where single speech voice fragments are first
    memorized and then recombined when the voice has to be reproduced. However, this
    approach does not scale because it is only possible to reproduce the memorized
    voice fragments, and it is not possible to reproduce new speakers or different
    types of audio without memorizing the fragments from the beginning.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续语音合成（Concatenative TTS）**：这是首先记忆单个语音片段，然后在需要重现语音时重新组合这些片段的方法。然而，这种方法无法扩展，因为只能重现已记忆的语音片段，而不能在不重新记忆的情况下重现新的发声者或不同类型的音频。'
- en: '**Parametric TTS**: This is where a model is created for storing all the characteristic
    features of the audio to be synthesized. Before WaveNet, the audio generated with
    parametric TTS was less natural than concatenative TTS. WaveNet improved the state-of-the-art
    by modeling directly the production of audio sounds, instead of using intermediate
    signal processing algorithms that have been used in the past.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化语音合成（Parametric TTS）**：这是创建模型来存储待合成音频的所有特征的过程。在WaveNet之前，使用参数化语音合成生成的音频比连续语音合成的音频自然度低。WaveNet通过直接建模音频声音的生成改进了技术水平，而不是使用过去使用的中间信号处理算法。'
- en: 'In principle, WaveNet can be seen as a stack of 1D convolutional layers (we
    have seen 2D convolution for images in [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml),
    *Deep Learning with ConvNets*), with a constant stride of one and with no pooling
    layers. Note that the input and the output have by construction the same dimension,
    so ConvNet is well-suited to model sequential data such as audio. However, it
    has been shown that in order to reach a large size for the receptive field (remember
    that the receptive field of a neuron in a layer is the cross section of the previous
    layer from which neurons provide inputs) in the output neuron it is necessary
    to either use a massive number of large filters or prohibitively increase the
    the depth of the network. For this reason, pure ConvNets are not so effective
    in learning how to synthesize audio. The key intuition beyond WaveNet is the dilated
    causal convolutions (for more information refer to the article: *Multi-Scale Context
    Aggregation by Dilated Convolutions*, by Fisher Yu, Vladlen Koltun, 2016, available
    at: [https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48](https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48))
    or sometime atrous convolution (*atrous* is the *bastardization* of the French
    expression *à trous*, meaning *with holes*, so an atrous convolution is a convolution
    with holes), which simply means that some input values are skipped when the filter
    of a convolutional layer is applied. As an example, in one dimension, a filter, *w*,
    of size *3* with dilatation *1* would compute the following sum:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，WaveNet可以看作是一堆1维卷积层（我们已经在[第3章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)，*深度学习与ConvNets*中看到了2维卷积用于图像），具有恒定的步幅为1且没有池化层。注意，输入和输出在构造上具有相同的维度，因此ConvNet非常适合模拟音频等序列数据。然而，已经表明，为了达到输出神经元的大接受域大小（记住神经元层的接收域是前一层提供输入的横截面），需要使用大量大滤波器或者昂贵地增加网络的深度。因此，纯粹的ConvNets在学习如何合成音频方面并不那么有效。WaveNet背后的关键直觉是扩张因果卷积（更多信息请参考文章：*Multi-Scale
    Context Aggregation by Dilated Convolutions*，作者Fisher Yu, Vladlen Koltun, 2016，可在[https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48](https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48)获取）或者有时称为空洞卷积（*atrous*是法语表达*à
    trous*的词语，意思是*带孔的*，因此空洞卷积是一种在应用卷积层的滤波器时跳过某些输入值的方式）。例如，在一维情况下，大小为3的滤波器*w*，带有膨胀率*1*将计算以下和：
- en: '![](img/B06258_04a_015.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06258_04a_015.png)'
- en: 'Thanks to this simple idea of introducing *holes*, it is possible to stack
    multiple dilated convolutional layers with exponentially increasing filters, and
    learn long range input dependencies without having an excessively deep network.
    A WaveNet is therefore a ConvNet where the convolutional layers have various dilation
    factors, allowing the receptive field to grow exponentially with depth and therefore
    efficiently cover thousands of audio time-steps. When we train, the input are
    sounds recorded from human speakers. The waveforms are quantized to a fixed integer
    range. A WaveNet defines an initial convolutional layer accessing only the current
    and previous input. Then, there is a stack of dilated ConvNet layers, still accessing
    only current and previous inputs. At the end, there is a series of dense layers
    that combine previous results, followed by a softmax activation function for categorical
    outputs. At each step, a value is predicted from the network and fed back into
    the input. At the same time, a new prediction for the next step is computed. The
    loss function is the cross-entropy between the output for the current step and
    the input at the next step .One Keras implementation developed by Bas Veeling
    is available at: [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and
    can be easily installed via `git`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入*空洞*这一简单思想，能够堆叠多个膨胀卷积层，并使滤波器指数增长，从而在不需要过深网络的情况下学习长距离输入依赖关系。因此，WaveNet 是一种卷积神经网络（ConvNet），其中卷积层具有不同的膨胀因子，允许感受野随着深度的增加而指数增长，从而高效地覆盖成千上万个音频时间步长。在训练时，输入是来自人类发音者的声音。波形被量化为固定的整数范围。WaveNet
    定义了一个初始卷积层，只访问当前和先前的输入。然后是一个堆叠的膨胀卷积网络层，依然只访问当前和先前的输入。最后，有一系列密集层将先前的结果结合起来，后跟一个用于分类输出的
    softmax 激活函数。在每一步，网络会预测一个值并将其反馈到输入中，同时计算下一步的预测。损失函数是当前步骤输出和下一步输入之间的交叉熵。由 Bas Veeling
    开发的一个 Keras 实现可以在此获取：[https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet)，并可以通过
    `git` 轻松安装：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that this code is compatible with Keras 1.x and please check the issue
    at [https://github.com/basveeling/wavenet/issues/29](https://github.com/basveeling/wavenet/issues/29),
    to understand what is the progress for porting it on the top of Keras 2.x. Training
    is very simple but requires a significant amount of computational power (so make
    sure that you have good GPU support):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此代码与 Keras 1.x 兼容，详情请查看 [https://github.com/basveeling/wavenet/issues/29](https://github.com/basveeling/wavenet/issues/29)，了解将其迁移到
    Keras 2.x 上的进展情况。训练非常简单，但需要大量的计算能力（因此请确保你有良好的 GPU 支持）：
- en: '[PRE29]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Sampling the network after training is equally very easy:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后采样网络同样非常简单：
- en: '[PRE30]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can find a large number of hyperparameters online, which can be used for
    fine-tuning our training process. The network is really deep, as explained by
    this dump of internal layers. Note that the input waveform are divided into (`fragment_length
    = 1152` and `nb_output_bins = 256`), which is the tensor propagating into WaveNet.
    WaveNet is organized in repeated blocks called residuals, each consisting of a
    multiplied merge of two dilated convolutional modules (one with `sigmoid` and
    the other with `tanh` activation), followed by a sum merged convolutional. Note
    that each dilated convolution has holes of growing exponential size (`2 ** i`)
    from 1 to 512, as defined in this piece of text:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在网上找到大量的超参数，用于微调我们的训练过程。正如这个内部层的转储所解释的，网络确实非常深。请注意，输入波形被划分为（`fragment_length
    = 1152` 和 `nb_output_bins = 256`），这就是传递到 WaveNet 的张量。WaveNet 以重复的块组织，称为残差块，每个残差块由两个膨胀卷积模块的乘积合并组成（一个使用
    `sigmoid` 激活，另一个使用 `tanh` 激活），然后是一个合并的卷积求和。请注意，每个膨胀卷积都有逐渐增大的空洞，空洞大小按指数增长（`2 **
    i`），从 1 到 512，如下文所定义：
- en: '[PRE31]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After the residual dilated block, there is a sequence of merged convolutional
    modules, followed by two convolutional modules, followed by a `softmax` activation
    function in `nb_output_bins` categories. The full network structure is here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在残差膨胀块之后，有一系列合并的卷积模块，接着是两个卷积模块，最后是 `nb_output_bins` 类别中的 `softmax` 激活函数。完整的网络结构如下：
- en: '[PRE32]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: DeepMind tried to train with data sets including multiple speakers, and this
    significantly improved the capacity to learn a shared representation of languages
    and tones and thus receive results close to natural speech. You'll find an amazing
    collection of examples of synthesized voice online ([https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)),
    and it is interesting to note that the quality of audio improves when WaveNet
    is conditioned on additional text that is transformed into a sequence of linguistic
    and phonetic features in addition to audio waveforms. My favorite examples are
    the ones where the same sentence is pronounced by the net with different tones
    of voice. Of course, it is also fascinating to hear WaveNet create piano music
    by itself. Check it out online!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind尝试使用包含多个发言者的数据集进行训练，这显著提高了学习共享语言和语调的能力，从而使得生成的结果接近自然语音。你可以在线找到一系列合成语音的精彩例子([https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/))，有趣的是，音频质量在WaveNet使用额外文本条件时得到了提升，这些文本会被转化为语言学和语音学特征序列，并与音频波形一起使用。我的最爱例子是同一句话由网络以不同的语调发音。当然，听到WaveNet自己创作钢琴音乐也非常令人着迷。去网上看看吧！
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed GANs. A GAN typically consists of two networks;
    one is trained to forge synthetic data that looks authentic, and the second is
    trained to discriminate authentic data against forged data. The two networks continuously
    compete, and in doing so, they keep improving each other. We reviewed an open
    source code, learning to forge MNIST and CIFAR-10 images that look authentic.
    In addition, we discussed WaveNet, a deep generative network proposed by Google
    DeepMind for teaching computers how to reproduce human voices and musical instruments
    with impressive quality. WaveNet directly generates raw audio with a parametric
    text-to-speech approach based on dilated convolutional networks. Dilated convolutional
    networks are a special kind of ConvNets where convolution filters have holes,
    allowing the receptive field to grow exponentially in depth and therefore efficiently
    cover thousands of audio time-steps. DeepMind showed how it is possible to use
    WaveNet to synthesize human voice and musical instruments, and improved previous
    state-of-the-art. In the next chapter, we will discuss word embeddings—a set of
    deep learning methodologies for detecting relations among words and grouping together
    similar words.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了生成对抗网络（GAN）。一个GAN通常由两个网络组成；一个被训练用来伪造看起来真实的合成数据，另一个则被训练用来区分真实数据与伪造数据。这两个网络持续竞争，通过这种方式，它们相互促进和改进。我们回顾了一个开源代码，学习如何伪造看起来真实的MNIST和CIFAR-10图像。此外，我们还讨论了WaveNet，这是一种由Google
    DeepMind提出的深度生成网络，用于教计算机如何以惊人的质量再现人类语音和乐器声音。WaveNet通过基于扩张卷积网络的参数化语音合成方法直接生成原始音频。扩张卷积网络是一种特殊的卷积神经网络（ConvNets），其中卷积滤波器具有孔洞，这使得感受野在深度上呈指数增长，因此能够高效地覆盖成千上万的音频时间步长。DeepMind展示了如何使用WaveNet合成人的声音和乐器，并在此基础上改进了之前的最先进技术。在下一章中，我们将讨论词嵌入——一组用于检测词语之间关系并将相似词语归类在一起的深度学习方法。
