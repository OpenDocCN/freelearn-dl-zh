- en: Building Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建卷积神经网络
- en: In this chapter, we are going to develop a **convolutional neural network** (**CNN**)
    for an image classification example using DL4J. We will develop the components
    of our application step by step while we progress through the recipes. The chapter
    assumes that you have read [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction
    to Deep Learning in Java*, and [Chapter 2](6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml), *Data
    Extraction, Transformation, and Loading*, and that you have set up DL4J on your
    computer, as mentioned in [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml),
    *Introduction to Deep Learning in Java*. Let's go ahead and discuss the specific
    changes required for this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 DL4J 开发一个**卷积神经网络**（**CNN**）进行图像分类示例。我们将在逐步推进配方的过程中，逐步开发应用程序的各个组件。本章假设你已经阅读了[第
    1 章](f88b350b-16e2-425b-8425-4631187c7803.xhtml)，《*Java 深度学习简介*》以及[第 2 章](6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml)，《*数据提取、转换与加载*》，并且你已经按照[第
    1 章](f88b350b-16e2-425b-8425-4631187c7803.xhtml)《*Java 深度学习简介*》中提到的内容在你的计算机上设置了
    DL4J。现在，让我们开始讨论本章所需的具体更改。
- en: For demonstration purposes, we will have classifications for four different
    species. CNNs convert complex images into an abstract format that can be used
    for prediction. Hence, a CNN would be an optimal choice for this image classification
    problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 出于演示目的，我们将对四种不同物种进行分类。CNN 将复杂的图像转换为可以用于预测的抽象格式。因此，CNN 将是解决此图像分类问题的最佳选择。
- en: CNNs are just like any other deep neural network that abstracts the decision
    process and gives us an interface to transform input to output. The only difference
    is that they support other types of layers and different orderings of layers.
    Unlike other forms of input, such as text or CSV, images are complex. Considering
    the fact that each pixel is a source of information, training will become resource
    intensive and time consuming for large numbers of high-resolution images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 就像任何其他深度神经网络一样，抽象了决策过程，并为我们提供了一个将输入转化为输出的接口。唯一的区别是它们支持其他类型的层和不同的层次顺序。与文本或
    CSV 等其他类型的输入不同，图像是复杂的。考虑到每个像素都是信息源，训练过程对于大量高分辨率图像将变得资源密集且耗时。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Extracting images from disk
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从磁盘提取图像
- en: Creating image variations for training data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练数据创建图像变体
- en: Image preprocessing and the design of input layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像预处理和输入层设计
- en: Constructing hidden layers for a CNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 CNN 的隐藏层
- en: Constructing output layers for output classification
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用于输出分类的输出层
- en: Training images and evaluating CNN output
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练图像并评估 CNN 输出
- en: Creating an API endpoint for the image classifier
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为图像分类器创建 API 端点
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Implementation of the use case discussed in this chapter can be found here: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/04_Building_Convolutional_Neural_Networks/sourceCode](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/04_Building_Convolutional_Neural_Networks/sourceCode).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的用例实现可以在这里找到：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/04_Building_Convolutional_Neural_Networks/sourceCode](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/04_Building_Convolutional_Neural_Networks/sourceCode)。
- en: After cloning our GitHub repository, navigate to the following directory: `Java-Deep-Learning-Cookbook/04_Building_Convolutional_Neural_Networks/sourceCode`.
    Then, import the `cookbookapp` project as a Maven project by importing `pom.xml`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的 GitHub 仓库后，导航到以下目录：`Java-Deep-Learning-Cookbook/04_Building_Convolutional_Neural_Networks/sourceCode`。然后，通过导入`pom.xml`，将`cookbookapp`项目作为
    Maven 项目导入。
- en: You will also find a basic Spring project, `spring-dl4j`, which can be imported
    as a Maven project as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会找到一个基本的 Spring 项目，`spring-dl4j`，也可以作为 Maven 项目导入。
- en: We will be using the dog breeds classification dataset from Oxford for this
    chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用来自牛津的狗品种分类数据集。
- en: 'The principal dataset can be downloaded from the following link:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主要数据集可以从以下链接下载：
- en: '[https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset](https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset](https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset)。'
- en: 'To run this chapter''s source code, download the dataset (four labels only)
    from here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章的源代码，请从以下链接下载数据集（仅限四个标签）：
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/raw/master/04_Building%20Convolutional%20Neural%20Networks/dataset.zip](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/raw/master/04_Building%20Convolutional%20Neural%20Networks/dataset.zip)
    (it can be found in the `Java-Deep-Learning-Cookbook/04_Building Convolutional
    Neural Networks/` directory).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/raw/master/04_Building%20Convolutional%20Neural%20Networks/dataset.zip](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/raw/master/04_Building%20Convolutional%20Neural%20Networks/dataset.zip)（可以在`Java-Deep-Learning-Cookbook/04_Building
    Convolutional Neural Networks/`目录中找到）。'
- en: Extract the compressed dataset file. Images are kept in different directories.
    Each directory represents a label/category. For demonstration purposes, we have
    used four labels. However, you are allowed to experiment with more images from
    different categories in order to run our example from GitHub.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 解压缩数据集文件。图像保存在不同的目录中。每个目录代表一个标签/类别。为演示目的，我们使用了四个标签。但是，您可以尝试使用来自不同类别的更多图像以运行我们在GitHub上的示例。
- en: Note that our example is optimized for four species. Experimentation with a
    larger number of labels requires further network configuration optimization.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的示例针对四个类别进行了优化。使用更多标签进行实验需要进一步的网络配置优化。
- en: 'To leverage the capabilities of the OpenCV library in your CNN, add the following
    Maven dependency:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的CNN中利用OpenCV库的能力，请添加以下Maven依赖项：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will be using the Google Cloud SDK to deploy the application in the cloud. For
    instructions in this regard, refer to [https://github.com/GoogleCloudPlatform/app-maven-plugin](https://github.com/GoogleCloudPlatform/app-maven-plugin).
    For Gradle instructions, refer to [https://github.com/GoogleCloudPlatform/app-gradle-plugin](https://github.com/GoogleCloudPlatform/app-gradle-plugin).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Google Cloud SDK在云中部署应用程序。有关详细说明，请参阅[https://github.com/GoogleCloudPlatform/app-maven-plugin](https://github.com/GoogleCloudPlatform/app-maven-plugin)。有关Gradle的说明，请参阅[https://github.com/GoogleCloudPlatform/app-gradle-plugin](https://github.com/GoogleCloudPlatform/app-gradle-plugin)。
- en: Extracting images from disk
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从磁盘中提取图像
- en: For classification based on *N* labels, there are *N* subdirectories created in
    the parent directory. The parent directory path is mentioned for image extraction.
    Subdirectory names will be regarded as labels. In this recipe, we will extract
    images from disk using DataVec.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于*N*个标签的分类，父目录中将创建*N*个子目录。提及父目录路径以进行图像提取。子目录名称将被视为标签。在本示例中，我们将使用DataVec从磁盘提取图像。
- en: How to do it...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'Use `FileSplit` to define the range of files to load into the neural network:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FileSplit`来定义加载到神经网络中的文件范围：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use `ParentPathLabelGenerator` and `BalancedPathFilter` to sample the labeled
    dataset and split it into train/test sets:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ParentPathLabelGenerator`和`BalancedPathFilter`来对标记数据集进行采样并将其分为训练/测试集：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In step 1, we used `FileSplit` to filter the images based on the file type (PNG,
    JPEG, TIFF, and so on).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，我们使用了`FileSplit`根据文件类型（PNG、JPEG、TIFF等）来过滤图像。
- en: We also passed in a random number generator based on a single seed. This seed
    value is an integer (`42` in our example). `FileSplit` will be able to generate
    a list of file paths in random order (random order of files) by making use of
    a random seed. This will introduce more randomness to the probabilistic decision
    and thereby increase the model's performance (accuracy metrics).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还传入了一个基于单个种子的随机数生成器。此种子值是一个整数（我们的示例中为`42`）。`FileSplit`将能够利用随机种子以随机顺序生成文件路径列表。这将为概率决策引入更多随机性，从而提高模型的性能（准确性指标）。
- en: 'If you have a ready-made dataset with an unknown number of labels, it is crucial
    to calculate `numLabels`. Hence, we used `FileSplit` to calculate them programmatically:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个包含未知数量标签的预制数据集，则计算`numLabels`至关重要。因此，我们使用了`FileSplit`来以编程方式计算它们：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In step 2, we used **`ParentPathLabelGenerator` **to generate the label for
    files based on the directory path. Also, `BalancedPathFilter` is used to randomize
    the order of paths in an array. Randomization will help overcome overfitting issues. `BalancedPathFilter` also
    ensures the same number of paths for each label and helps to obtain optimal batches
    for training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们使用了**`ParentPathLabelGenerator`**来根据目录路径为文件生成标签。同时，使用`BalancedPathFilter`来随机化数组中的路径顺序。随机化有助于克服过拟合问题。`BalancedPathFilter`还确保每个标签具有相同数量的路径，并帮助获得用于训练的最佳批次。
- en: 'With `testSetRatio` as `20`, 20 percent of the dataset will be used as the
    test set for the model evaluation. After step 2, the array elements in `inputSplits` will
    represent the train/test datasets:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `testSetRatio` 为 `20`，数据集的 20% 将用作模型评估的测试集。在第 2 步后，`inputSplits` 中的数组元素将代表训练/测试数据集：
- en: '`inputSplits[0]` will represent the train dataset.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputSplits[0]` 将代表训练数据集。'
- en: '`inputSplits[1]` will represent the test dataset.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputSplits[1]` 将代表测试数据集。'
- en: '`NativeImageLoader.ALLOWED_FORMATS` uses `JavaCV` to load images. Allowed image
    formats are `.bmp`, `.gif`, `.jpg`, `.jpeg`, `.jp2`, `.pbm`, `.pgm`, `.ppm`, `.pnm`,
    `.png`, `.tif`, `.tiff`, `.exr`, and `.webp`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NativeImageLoader.ALLOWED_FORMATS` 使用 `JavaCV` 加载图像。允许的图像格式有：`.bmp`、`.gif`、`.jpg`、`.jpeg`、`.jp2`、`.pbm`、`.pgm`、`.ppm`、`.pnm`、`.png`、`.tif`、`.tiff`、`.exr`
    和 `.webp`。'
- en: '`BalancedPathFilter` randomizes the order of file paths in an array and removes
    them randomly to have the same number of paths for each label. It will also form
    the paths on the output based on their labels, so as to obtain easily optimal
    batches for training. So, it is more than just random sampling.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BalancedPathFilter` 随机化文件路径数组的顺序，并随机移除它们，使每个标签的路径数相同。它还会根据标签在输出中形成路径，以便获得易于优化的训练批次。因此，它不仅仅是随机抽样。'
- en: '`fileSplit.sample()` samples the file paths based on the path filter mentioned.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fileSplit.sample()` 根据前述路径过滤器抽样文件路径。'
- en: It will further split the results into an array of `InputSplit` objects. Each
    object will refer to the train/test set, and its size is proportional to the weights
    mentioned.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它将进一步将结果拆分为一组 `InputSplit` 对象。每个对象将引用训练/测试集，其大小与前述权重成比例。
- en: Creating image variations for training data
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为训练数据创建图像变体
- en: We create image variations and further train our network model on top of them
    to increase the generalization power of the CNN. It is crucial to train our CNN
    with as many image variations as possible so as to increase the accuracy. We basically
    obtain more samples of the same image by flipping or rotating them. In this recipe,
    we will transform and create samples of images using a concrete implementation
    of `ImageTransform` in DL4J.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建图像变体并在其基础上进一步训练我们的网络模型，以增强 CNN 的泛化能力。训练 CNN 时，使用尽可能多的图像变体是至关重要的，以提高准确性。我们基本上通过翻转或旋转图像来获得同一图像的更多样本。在本教程中，我们将使用
    DL4J 中 `ImageTransform` 的具体实现来转换和创建图像样本。
- en: How to do it...
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Use `FlipImageTransform` to flip the images horizontally or vertically (randomly
    or not randomly):'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `FlipImageTransform` 来水平或垂直翻转图像（随机或非随机）：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use `WarpImageTransform` to warp the perspective of images deterministically
    or randomly:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `WarpImageTransform` 来确定性或随机地扭曲图像的透视：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Use `RotateImageTransform` to rotate the images deterministically or randomly:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `RotateImageTransform` 以确定性或随机方式旋转图像：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Use `PipelineImageTransform` to add image transformations to the pipeline:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `PipelineImageTransform` 向管道中添加图像变换：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, if we don''t need a random flip but a specified mode of flip (deterministic),
    then we can do the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 步中，如果我们不需要随机翻转而是指定的翻转模式（确定性），那么我们可以进行如下操作：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`flipMode` is the deterministic flip mode.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`flipMode` 是确定性的翻转模式。'
- en: '`flipMode = 0`: Flips around the *x* axis'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flipMode = 0`: 绕 *x* 轴翻转'
- en: '`flipMode > 0`: Flips around the *y* axis'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flipMode > 0`: 绕 *y* 轴翻转'
- en: '`flipMode < 0`: Flips around both axes'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flipMode < 0`: 绕两个轴翻转'
- en: 'In step 2, we passed in two attributes: `Random(seed)` and `delta`. `delta`
    is the magnitude in which an image is warped. Check the following image sample
    for the demonstration of image warping:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 步中，我们传入了两个属性：`Random(seed)` 和 `delta`。`delta` 是图像扭曲的幅度。查看以下图像示例，了解图像扭曲的演示：
- en: '![](img/770b3c00-d390-4d41-9b67-8cd238a57618.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/770b3c00-d390-4d41-9b67-8cd238a57618.jpg)'
- en: '(Image source: https://commons.wikimedia.org/wiki/File:Image_warping_example.jpg'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （图像来源： https://commons.wikimedia.org/wiki/File:Image_warping_example.jpg
- en: License: CC BY-SA 3.0)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证：CC BY-SA 3.0
- en: '`WarpImageTransform(new Random(seed),delta)` internally calls the following
    constructor:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`WarpImageTransform(new Random(seed), delta)` 内部调用以下构造函数：'
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It will assume `dx1=dy1=dx2=dy2=dx3=dy3=dx4=dy4=delta`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它将假设 `dx1=dy1=dx2=dy2=dx3=dy3=dx4=dy4=delta`。
- en: 'Here are the parameter descriptions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是参数描述：
- en: '`dx1`: Maximum warping in `x` for the top-left corner (pixels)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dx1`: 顶部左角的最大 x 轴扭曲量（像素）'
- en: '`dy1`: Maximum warping in `y` for the top-left corner (pixels)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dy1`: 顶部左角的最大 y 轴扭曲量（像素）'
- en: '`dx2`: Maximum warping in `x` for the top-right corner (pixels)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dx2`: 顶部右角的最大 x 轴扭曲量（像素）'
- en: '`dy2`: Maximum warping in `y` for the top-right corner (pixels)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dy2`: 顶部右角的最大 y 轴扭曲量（像素）'
- en: '`dx3`: Maximum warping in `x` for the bottom-right corner (pixels)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dx3`：右下角在`x`方向的最大变形（像素）'
- en: '`dy3`: Maximum warping in `y` for the bottom-right corner (pixels)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dy3`：右下角在`y`方向的最大变形（像素）'
- en: '`dx4`: Maximum warping in `x` for the bottom-left corner (pixels)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dx4`：左下角在`x`方向的最大变形（像素）'
- en: '`dy4`: Maximum warping in `y` for the bottom-left corner (pixels)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dy4`：左下角在`y`方向的最大变形（像素）'
- en: The value of delta will be auto adjusted as per the normalized width/height
    while creating `ImageRecordReader`. This means that the given value of delta will
    be treated relative to the normalized width/height specified while creating `ImageRecordReader`.
    So, let's say we perform 10 pixels of warping across the *x*/*y* axis in an image
    with a size of 100 x 100\. If the image is normalized to a size of 30 x 30, then
    3 pixels of warping will happen across the *x*/*y* axis. You need to experiment
    with different values for `delta` since there's no constant/min/max `delta` value
    that can solve all types of image classification problems.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建`ImageRecordReader`时，delta的值将根据归一化的宽度/高度自动调整。这意味着给定的delta值将相对于创建`ImageRecordReader`时指定的归一化宽度/高度进行处理。假设我们在一个100
    x 100像素的图像上，在* x * / * y *轴上进行10像素的变形。如果该图像被归一化为30 x 30大小，那么* x * / * y *轴上将进行3像素的变形。由于没有常量/最小/最大`delta`值能解决所有类型的图像分类问题，因此你需要尝试不同的`delta`值。
- en: In step 3, we used `RotateImageTransform` to perform rotational image transformations
    by rotating the image samples on the angle mentioned.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤3中，我们使用了`RotateImageTransform`来执行旋转图像变换，通过在指定角度上旋转图像样本。
- en: In step 4, we added multiple image transformations with the help of `PipelineImageTransform` into
    a pipeline to load them sequentially or randomly for training purposes. We have
    created a pipeline with the `List<Pair<ImageTransform,Double>>` type. The `Double`
    value in `Pair` is the *probability* that the particular element (`ImageTransform`)
    in the pipeline is executed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤4中，我们通过`PipelineImageTransform`的帮助将多个图像变换添加到管道中，以便按顺序或随机地加载它们用于训练。我们创建了一个类型为`List<Pair<ImageTransform,Double>>`的管道。`Pair`中的`Double`值是管道中某个特定元素（`ImageTransform`）执行的*概率*。
- en: Image transformations will help CNN to learn image patterns better. Training
    on top of transformed images will further avoid the chances of overfitting.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图像变换将帮助CNN更好地学习图像模式。在变换图像上进行训练将进一步避免过拟合的可能性。
- en: There's more...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`WarpImageTransform` under the hood makes an internal call to the JavaCPP method,
    `warpPerspective()`, with the given properties, `interMode`, `borderMode`, and `borderValue`. JavaCPP is
    an API that parses native C/C++ files and generates Java interfaces to act as
    a wrapper. We added the JavaCPP dependency for OpenCV in `pom.xml` earlier. This
    will enable us to exploit OpenCV libraries for image transformation.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`WarpImageTransform`在后台会调用JavaCPP方法`warpPerspective()`，并使用给定的属性`interMode`、`borderMode`和`borderValue`。JavaCPP是一个API，它解析本地C/C++文件并生成Java接口作为包装器。我们之前在`pom.xml`中添加了OpenCV的JavaCPP依赖项。这将使我们能够利用OpenCV库进行图像变换。'
- en: Image preprocessing and the design of input layers
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理和输入层设计
- en: Normalization is a crucial preprocessing step for a CNN, just like for any feed
    forward networks. Image data is complex. Each image has several pixels of information.
    Also, each pixel is a source of information. We need to normalize this pixel value
    so that the neural network will not overfit/underfit while training. Convolution/subsampling
    layers also need to be specified while designing input layers for CNN. In this
    recipe, we will normalize and then design input layers for the CNN.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是CNN的一个关键预处理步骤，就像任何前馈神经网络一样。图像数据是复杂的。每个图像包含多个像素的信息。此外，每个像素都是一个信息源。我们需要归一化这些像素值，以便神经网络在训练时不会出现过拟合或欠拟合的问题。卷积/子采样层在设计CNN的输入层时也需要指定。在本方案中，我们将首先进行归一化，然后设计CNN的输入层。
- en: How to do it...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create `ImagePreProcessingScaler` for image normalization:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`ImagePreProcessingScaler`进行图像归一化：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create a neural network configuration and add default hyperparameters:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建神经网络配置并添加默认的超参数：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create convolution layers for a CNN using `ConvolutionLayer`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ConvolutionLayer`为CNN创建卷积层：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Configure subsampling layers using `SubsamplingLayer`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SubsamplingLayer`配置子采样层：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Normalize activation between layers using `LocalResponseNormalization`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`LocalResponseNormalization`在层之间进行激活归一化：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, `ImagePreProcessingScaler` normalizes the pixels in a specified range
    of values (0, 1) . We will use this normalizer once we create iterators for the
    data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，`ImagePreProcessingScaler` 将像素值标准化到指定的范围（0, 1）。我们将在创建数据迭代器后使用此标准化器。
- en: In step 2, we have added hyperparameters such as an L2 regularization coefficient,
    a gradient normalization strategy, a gradient update algorithm, and an activation
    function globally (applicable for all layers).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们添加了超参数，如L2正则化系数、梯度归一化策略、梯度更新算法和全局激活函数（适用于所有层）。
- en: 'In step 3, `ConvolutionLayer` requires you to mention the kernel dimensions
    (11*11 for the previous code). A kernel acts as a feature detector in the context
    of a CNN:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤3中，`ConvolutionLayer` 需要您指定核的维度（之前代码中的11*11）。在CNN中，核充当特征检测器：
- en: '`stride`: Directs the space between each sample in an operation on a pixel
    grid.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`：指定在像素网格操作中每个样本之间的空间。'
- en: '`channels`: The number of input neurons. We mention the number of color channels
    here (RGB: 3).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channels`：输入神经元的数量。我们在这里提到颜色通道的数量（RGB：3）。'
- en: '`OutGoingConnectionCount`: The number of output neurons.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OutGoingConnectionCount`：输出神经元的数量。'
- en: In step 4, **`SubsamplingLayer`** is a downsampling layer to reduce the amount
    of data to be transmitted or stored, and, at the same time, keep the significant
    features intact. Max pooling is the most commonly used sampling method. `ConvolutionLayer` is
    always followed by **`SubsamplingLayer`**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤4中，**`SubsamplingLayer`** 是一个下采样层，用于减少要传输或存储的数据量，同时保持重要特征的完整。最大池化是最常用的采样方法。`ConvolutionLayer`
    后面总是跟着 **`SubsamplingLayer`**。
- en: Efficiency is a challenging task in the case of a CNN. It requires a lot of
    images, along with transformations, to train better. In step 4, `LocalResponseNormalization` improves
    the generalization power of a CNN. It performs a normalization operation right
    before performing ReLU activation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，效率是一个具有挑战性的任务。它需要大量图像以及转换操作来进行更好的训练。在步骤4中，`LocalResponseNormalization`
    提高了CNN的泛化能力。在执行ReLU激活之前，它会执行归一化操作。
- en: 'We add this as a separate layer placed between a convolution layer and a subsampling
    layer:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其作为一个独立的层，放置在卷积层和下采样层之间：
- en: '`ConvolutionLayer` is similar to a feed forward layer, but for performing two-dimensional
    convolution on images.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConvolutionLayer` 类似于前馈层，但用于在图像上执行二维卷积操作。'
- en: '**`SubsamplingLayer`** is required for pooling/downsampling in CNNs.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`SubsamplingLayer`** 是CNN中池化/下采样所必需的。'
- en: '`ConvolutionLayer` and **`SubsamplingLayer`** together form the input layers
    for a CNN and extract abstract features from images and pass them to the hidden
    layers for further processing.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConvolutionLayer` 和 **`SubsamplingLayer`** 一起构成CNN的输入层，从图像中提取抽象特征，并将其传递到隐藏层以进行进一步处理。'
- en: Constructing hidden layers for a CNN
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建CNN的隐藏层
- en: The input layers of a CNN produce abstract images and pass them to hidden layers.
    The abstract image features are passed from input layers to the hidden layers. If
    there are multiple hidden layers in your CNN, then each of them will have unique
    responsibilities for the prediction. For example, one of them can detect lights
    and dark in the image, and the following layer can detect edges/shapes with the
    help of the preceding hidden layer. The next layer can then discern more complex
    objects from the edges/recipes from the preceding hidden layer, and so on.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的输入层生成抽象图像并将其传递给隐藏层。抽象图像特征从输入层传递到隐藏层。如果您的CNN中有多个隐藏层，那么每个层将有独特的责任来进行预测。例如，其中一个层可以检测图像中的亮暗，而随后的层可以借助前一层的特征来检测边缘/形状。接下来的层可以从前一隐藏层的边缘/特征中辨识出更复杂的物体或配方，以此类推。
- en: In this recipe, we will design hidden layers for our image classification problem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将为我们的图像分类问题设计隐藏层。
- en: How to do it...
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Build hidden layers using `DenseLayer`:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `DenseLayer` 构建隐藏层：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Add `AddDenseLayer` to the layer structure by calling `layer()`:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `layer()` 添加 `AddDenseLayer` 到层结构中：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, hidden layers are created using `DenseLayer`, which are preceded
    by convolution/subsampling layers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，隐藏层通过 `DenseLayer` 创建，前面是卷积/下采样层。
- en: In step 2, note that we didn't mention the number of input neurons in hidden
    layers, since it would be same as the preceding layer's (`SubSamplingLayer`) outgoing
    neurons.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，注意我们没有提到隐藏层中输入神经元的数量，因为它与前一层（`SubSamplingLayer`）的输出神经元数量相同。
- en: Constructing output layers for output classification
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建输出层进行输出分类
- en: We need to perform image classification using logistic regression (`SOFTMAX`),
    resulting in probabilities of occurrence for each of the image labels. Logistic
    regression is a predictive analysis algorithm and, hence, more suitable for prediction
    problems. In this recipe, we will design output layers for the image classification
    problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用逻辑回归（`SOFTMAX`）进行图像分类，从而得到每个图像标签的发生概率。逻辑回归是一种预测分析算法，因此更适合用于预测问题。在本配方中，我们将设计图像分类问题的输出层。
- en: How to do it...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Design the output layer using `OutputLayer`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `OutputLayer` 设计输出层：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Set the input type using `setInputType()`:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `setInputType()` 设置输入类型：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, `nOut()` expects the number of image labels that we calculated using
    `FileSplit` in an earlier recipe.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，`nOut()` 预期的是我们在前面的配方中使用 `FileSplit` 计算出的图像标签数量。
- en: In step 2, we have used `setInputType()` to set the convolutional input type. This
    will trigger computation/settings of the input neurons and add preprocessors (`LocalResponseNormalization`)
    to handle data flow from the convolutional/subsampling layers to the dense layers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们使用 `setInputType()` 设置了卷积输入类型。这将触发输入神经元的计算/设置，并添加预处理器（`LocalResponseNormalization`）以处理从卷积/子采样层到全连接层的数据流。
- en: The `InputType` class is used to track and define the types of activations. This
    is most useful for automatically adding preprocessors between layers, and automatically
    setting `nIn` (number of input neurons) values. That's how we skipped specifying `nIn` values
    earlier when configuring the model. The convolutional input type is four-dimensional
    in shape `[miniBatchSize, channels, height, width]`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`InputType` 类用于跟踪和定义激活类型。这对自动添加层间预处理器以及自动设置 `nIn`（输入神经元数量）值非常有用。也正是这样，我们在配置模型时跳过了
    `nIn` 值的指定。卷积输入类型的形状是四维的 `[miniBatchSize, channels, height, width]`。'
- en: Training images and evaluating CNN output
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练图像并评估 CNN 输出
- en: We have layer configurations in place. Now, we need to train the CNN to make
    it suitable for predictions. In a CNN, filter values will be adjusted during the
    training process. The network will learn by itself how to choose proper filters
    (feature maps) to produce the best results. We will also see that the efficiency
    and performance of the CNN becomes a challenging task because of the complexity
    involved in computation. In this recipe, we will train and evaluate our CNN model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置了层的配置。现在，我们需要训练 CNN 使其适合预测。在 CNN 中，过滤器值将在训练过程中进行调整。网络将自行学习如何选择合适的过滤器（特征图），以产生最佳结果。我们还将看到，由于计算复杂性，CNN
    的效率和性能变成了一个挑战。在这个配方中，我们将训练并评估我们的 CNN 模型。
- en: How to do it...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Load and initialize the training data using `ImageRecordReader`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ImageRecordReader` 加载并初始化训练数据：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a dataset iterator using `RecordReaderDataSetIterator`:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `RecordReaderDataSetIterator` 创建数据集迭代器：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Add the normalizer to the dataset iterator:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将归一化器添加到数据集迭代器中：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Train the model by calling `fit()`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `fit()` 来训练模型：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Train the model again with image transformations:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次训练模型，使用图像转换：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Evaluate the model and observe the results:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型并观察结果：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The evaluation metrics will appear as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标将显示如下：
- en: '![](img/ba5b0d89-60a4-4939-b38f-579bc1a573c0.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba5b0d89-60a4-4939-b38f-579bc1a573c0.png)'
- en: 'Add support for the GPU-accelerated environment by adding the following dependencies:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加以下依赖项来支持 GPU 加速环境：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, the parameters included are as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步中包含的参数如下：
- en: '`parentPathLabelGenerator`—created during the data extraction stage (see the *Extracting
    images from disk* recipe in this chapter).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parentPathLabelGenerator`—在数据提取阶段创建（请参见本章的 *从磁盘提取图像* 配方）。'
- en: '`channels`—The number of color channels (default = `3` for RGB).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channels`—颜色通道的数量（默认值 = `3`，即 RGB）。'
- en: '`ImageRecordReader(imageHeight, imageWidth, channels, parentPathLabelGenerator)`—resize
    the actual image to the specified size `(imageHeight, imageWidth)` to reduce the
    data loading effort.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImageRecordReader(imageHeight, imageWidth, channels, parentPathLabelGenerator)`—将实际图像调整为指定大小
    `（imageHeight, imageWidth）`，以减少数据加载的工作量。'
- en: The null attribute in the `initialize()` method is to indicate that we are not
    training transformed images.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialize()` 方法中的 null 属性表示我们没有对转换后的图像进行训练。'
- en: In step 3, we use `ImagePreProcessingScaler` for min-max normalization. Note
    that we need to use both `fit()` and `setPreProcessor()` to apply normalization
    to the data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们使用`ImagePreProcessingScaler`进行最小-最大归一化。注意，我们需要同时使用`fit()`和`setPreProcessor()`来对数据应用归一化。
- en: For GPU-accelerated environments, we can use `PerformanceListener` instead of
    `ScoreIterationListener` in step 4 to optimize the training process further. `PerformanceListener`
    tracks the time spent on training per iteration, while `ScoreIterationListener`reports
    the score of the network every *N* iterations during training. Make sure that
    GPU dependencies are added as per step 7.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU加速的环境，我们可以在第4步中使用`PerformanceListener`替代`ScoreIterationListener`，进一步优化训练过程。`PerformanceListener`跟踪每次迭代的训练时间，而`ScoreIterationListener`每隔*N*次迭代报告一次网络的分数。确保按照第7步添加GPU依赖项。
- en: In step 5, we have trained the model again with the image transformations that
    were created earlier. Make sure to apply normalization to the transformed images
    as well.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步，我们再次使用之前创建的图像变换来训练模型。确保对变换后的图像也进行归一化处理。
- en: There's more...
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Our CNN has an accuracy of around 50%. We trained our neural network using 396
    images across 4 categories. For an i7 processor with 8 GB of RAM, it will take
    15-30 minutes to complete the training. This can vary depending on the applications
    that are running parallel to the training instance. Training time can also change
    depending on the quality of the hardware. You will observe better evaluation metrics
    if you train with more images. More data will contribute toward better predictions.
    And, of course, it demands extended training time.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CNN模型的准确率大约为50%。我们使用396张图像，涵盖4个类别，训练了神经网络。对于一台配备8GB RAM的i7处理器，训练完成需要15-30分钟。这可能会根据与训练实例并行运行的其他应用程序有所变化。训练时间也会根据硬件的质量有所不同。如果你使用更多的图像进行训练，将会观察到更好的评估指标。更多的数据有助于提高预测准确性。当然，这也需要更长的训练时间。
- en: Another important aspect is to experiment with the number of hidden layers and
    subsampling/convolution layers to give you the optimal results. Too many layers
    could result in overfitting, hence, you really have to experiment by adding a
    different number of layers to your network configuration. Do not add large values
    for `stride`*, *or overly small dimensions for the images. That may cause excessive
    downsampling and will result in feature loss.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的方面是实验隐藏层和子采样/卷积层的数量，以获得最佳结果。层数过多可能导致过拟合，因此，你必须通过尝试不同层数的网络配置来实验。不要为`stride`添加过大的值，也不要为图像设置过小的尺寸。这可能会导致过度下采样，从而导致特征丧失。
- en: 'We can also try different values for the weights or how weights are distributed
    across neurons and test different gradient normalization strategies, applying
    L2 regularization and dropouts. There is no rule of thumb to choose a constant
    value for L1/L2 regularization or for dropouts. However, the L2 regularization
    constant takes a smaller value as it forces the weights to decay toward zero.
    Neural networks can safely accommodate dropout of 10-20 percent, beyond which
    it can actually cause underfitting. There is no constant value that will apply
    in every instance, as it varies from case to case:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试不同的权重值或权重在神经元之间的分配方式，并测试不同的梯度归一化策略，应用L2正则化和丢弃法。选择L1/L2正则化或丢弃法的常数值并没有固定的经验法则。然而，L2正则化常数通常较小，因为它迫使权重衰减到零。神经网络通常可以安全地接受10-20%的丢弃率，超过此范围可能会导致欠拟合。没有一个固定的常数值适用于所有情况，因为它会根据具体情况而有所不同：
- en: '![](img/059cd7f6-e130-4234-97bb-71913a34d140.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/059cd7f6-e130-4234-97bb-71913a34d140.png)'
- en: A GPU-accelerated environment will help decrease the training time. DL4J supports
    CUDA, and it can be accelerated further using cuDNN. Most two-dimensional CNN
    layers (such as `ConvolutionLayer` and `SubsamplingLayer`) support cuDNN.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: GPU加速的环境将有助于减少训练时间。DL4J支持CUDA，并且可以通过使用cuDNN进一步加速。大多数二维CNN层（如`ConvolutionLayer`和`SubsamplingLayer`）都支持cuDNN。
- en: The NVIDIA **CUDA Deep Neural Network** (**cuDNN**) library is a GPU-accelerated
    library of primitives for deep learning networks. You can read more about cuDNN
    here: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**NVIDIA CUDA深度神经网络**（**cuDNN**）库是一个针对深度学习网络的GPU加速原语库。你可以在这里阅读更多关于cuDNN的信息：[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)。'
- en: Creating an API endpoint for the image classifier
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建图像分类器的API端点
- en: We want to leverage the image classifier as an API to use them in external applications.
    An API can be accessed externally, and prediction results can be obtained without
    setting up anything. In this recipe, we will create an API endpoint for the image
    classifier.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将图像分类器作为API在外部应用程序中使用。API可以被外部访问，并且预测结果可以在不进行任何设置的情况下获取。在本食谱中，我们将为图像分类器创建一个API端点。
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Persist the model using `ModelSerializer`:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ModelSerializer`持久化模型：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Restore the trained model using `ModelSerializer` to perform predictions:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ModelSerializer`恢复训练好的模型，以执行预测：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Design an API method that accepts inputs from users and returns results. An
    example API method would look like the following:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计一个API方法，接受用户输入并返回结果。一个示例API方法如下所示：
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a URI mapping to service client requests, as shown in the following
    example:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个URI映射来处理客户端请求，如下所示：
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Build a `cookbookapp-cnn` project and add the API dependency to your Spring
    project:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个`cookbookapp-cnn`项目，并将API依赖项添加到你的Spring项目中：
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create the `generateStringOutput()` method in the service layer to serve API
    content:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在服务层创建`generateStringOutput()`方法来提供API内容：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Download and install the Google Cloud SDK: [https://cloud.google.com/sdk/](https://cloud.google.com/sdk/).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并安装Google Cloud SDK：[https://cloud.google.com/sdk/](https://cloud.google.com/sdk/)。
- en: 'Install the Cloud SDK `app-engine-java` component by running the following
    command on the Google Cloud console:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台运行以下命令，安装Cloud SDK的`app-engine-java`组件：
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Log in and configure Cloud SDK using the following command:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令登录并配置Cloud SDK：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Add the following dependency for Maven App Engine in `pom.xml`:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`中添加以下Maven App Engine依赖项：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create an `app.yaml` file in your project as per the Google Cloud documentation:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Google Cloud文档，在你的项目中创建`app.yaml`文件：
- en: '[https://cloud.google.com/appengine/docs/flexible/java/configuring-your-app-with-app-yaml](https://cloud.google.com/appengine/docs/flexible/java/configuring-your-app-with-app-yaml).'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/appengine/docs/flexible/java/configuring-your-app-with-app-yaml](https://cloud.google.com/appengine/docs/flexible/java/configuring-your-app-with-app-yaml)。'
- en: 'Navigate to Google App Engine and click on the Create Application button:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到Google App Engine并点击“创建应用程序”按钮：
- en: '![](img/03aa855c-b3a4-4bdc-9027-aad8828684f2.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03aa855c-b3a4-4bdc-9027-aad8828684f2.png)'
- en: 'Pick a region and click on Create app:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个地区并点击“创建应用”：
- en: '![](img/cf2b0a73-1c7b-4628-96c3-1c20f1cd9d75.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf2b0a73-1c7b-4628-96c3-1c20f1cd9d75.png)'
- en: 'Select Java and click the Next button:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Java并点击“下一步”按钮：
- en: '![](img/91c64ce6-3d2b-498b-9c14-3cb8453e924d.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91c64ce6-3d2b-498b-9c14-3cb8453e924d.png)'
- en: Now, your app engine has been created at Google Cloud.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的应用程序引擎已经在Google Cloud上创建完成。
- en: 'Build the spring boot application using Maven:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Maven构建Spring Boot应用程序：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Deploy the application using the following command:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署应用程序：
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In step 1 and step 2, we have persisted the model to reuse the model capabilities
    in API.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步和第2步中，我们已经将模型持久化，以便在API中重用模型功能。
- en: In step 3, an API method is created to accept user inputs and return the results
    from the image classifier.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，创建一个API方法，接受用户输入并返回图像分类器的结果。
- en: In step 4, the URI mappings will accept client requests (GET/POST). A GET request
    will serve the home page at the very beginning. A POST request will serve the
    end user request for image classification.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，URI映射将接受客户端请求（GET/POST）。GET请求最初将提供主页，POST请求将处理最终用户的图像分类请求。
- en: In step 5, we added an API dependency to the `pom.xml` file. For demonstration
    purposes, we build the API JAR file and the JAR file is stored in the local Maven
    repository. For production, you need to submit your API (JAR file) in a private
    repository so that Maven can fetch it from there.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们将API依赖项添加到了`pom.xml`文件中。为了演示目的，我们构建了API的JAR文件，且该JAR文件存储在本地Maven仓库中。对于生产环境，你需要将你的API（JAR文件）提交到私有仓库，以便Maven可以从那里获取。
- en: In step 6, we are calling the ImageClassifier API at our Spring Boot application
    service layer to retrieve the results and return them to the controller class.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步中，我们在Spring Boot应用程序的服务层调用了ImageClassifier API，以获取结果并将其返回给控制器类。
- en: In the previous chapter, we deployed the application locally for demonstration
    purposes. In this chapter, we have deployed the application in Google Cloud. Steps
    7 to 16 are dedicated to deployment in Google Cloud.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们为了演示目的将应用程序部署到本地。在本章中，我们已将应用程序部署到Google Cloud。第7到16步专门介绍了如何在Google Cloud中进行部署。
- en: We have used Google App Engine, although we can set up the same thing in more
    customized ways using Google Compute Engine or Dataproc. Dataproc is designed
    to deploy your application in a Spark distributed environment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Google App Engine，虽然我们也可以通过Google Compute Engine或Dataproc以更定制化的方式进行相同的配置。Dataproc旨在将你的应用部署到Spark分布式环境中。
- en: 'Once deployment is successful, you should see something like the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署成功，你应该能看到类似以下内容：
- en: '![](img/fa8bb5bd-d9d9-43ff-ae39-b882886b7cf4.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa8bb5bd-d9d9-43ff-ae39-b882886b7cf4.png)'
- en: When you hit the URL (which starts with `https://xx.appspot.com`), you should
    be able to see the web page (the same as in the previous chapter) where end users
    can upload images for image classification.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击URL（以`https://xx.appspot.com`开头），你应该能够看到一个网页（与上一章中的相同），用户可以在该网页上上传图片进行图像分类。
