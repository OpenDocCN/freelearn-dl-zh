- en: '*Chapter 6*: Reinforcement Learning in the Real World – Building Intelligent
    Agents to Complete Your To-Dos'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：现实世界中的强化学习 – 构建智能体来完成您的待办事项'
- en: An RL Agent needs to interact with the environment to learn and train. Training
    RL Agents for real-world applications usually comes with physical limitations
    and challenges. This is because the Agent could potentially cause damage to the
    real-world system it is dealing with while learning. Fortunately, there are a
    lot of tasks in the real world that do not necessarily have such challenges, and
    yet can be very useful for completing the day-to-day real-world tasks that are
    available in our To-Do lists!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: RL智能体需要与环境进行互动，以进行学习和训练。为现实世界的应用训练RL智能体通常会面临物理限制和挑战。这是因为智能体在学习过程中可能会对它所操作的现实世界系统造成损害。幸运的是，现实世界中有许多任务并不一定面临这些挑战，且可以非常有用，帮助我们完成日常待办事项列表中的任务！
- en: The recipes in this chapter will help you build RL Agents that can complete
    tasks on the internet, ranging from responding to annoying popups, booking flights
    on the web, managing emails and social media accounts, and more. We can do all
    of this without using a bunch of APIs that change over time or utilizing hardcoded
    scripts that stop working when a web page is updated. You will be training the
    Agents to complete such To-Do tasks by using the mouse and keyboard, just like
    how a human would! This chapter will also help you build the **WebGym** API, which
    is an OpenAI Gym-compatible generic RL learning environment interface that you
    can use to convert more than 50+ web tasks into training environments for RL and
    train your own RL Agents.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的食谱将帮助您构建能够完成互联网上任务的RL智能体，这些任务包括响应烦人的弹窗、在网页上预订航班、管理电子邮件和社交媒体账户等等。我们可以在不使用随着时间变化而改变的API，或者不依赖硬编码脚本的情况下完成这些任务，这些脚本可能会在网页更新后停止工作。您将通过使用鼠标和键盘训练智能体完成这些待办事项任务，就像人类一样！本章还将帮助您构建**WebGym**
    API，它是一个兼容OpenAI Gym的通用RL学习环境接口，您可以使用它将50多种网页任务转换为RL的训练环境，并训练自己的RL智能体。
- en: 'Specifically, the following recipes will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下具体内容：
- en: Building learning environments for real-world RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为现实世界的强化学习（RL）构建学习环境
- en: Building an RL Agent to complete tasks on the web – Call to Action
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个RL智能体来完成网页上的任务 – 行动呼吁
- en: Building a visual auto-login bot
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个视觉自动登录机器人
- en: Training an RL Agent to automate flight booking for your travel
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个RL智能体来自动化旅行中的航班预订
- en: Training an RL Agent to manage your emails
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个RL智能体来管理您的电子邮件
- en: Training an RL Agent to automate your social media account management
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个RL智能体来自动化社交媒体账户管理
- en: Let's get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in this book has been extensively tested on Ubuntu 18.04 and Ubuntu
    20.04, which means it should work with later versions of Ubuntu if Python 3.6+
    is available. With Python 3.6+ installed, along with the necessary Python packages
    listed in the Getting ready sections of each recipe, the code should run fine
    on Windows and Mac OSX too. It is advised that you create and use a Python virtual
    environment named `tf2rl-cookbook` to install the packages and run the code in
    this book. Installing Miniconda or Anaconda for Python virtual environment management
    is recommended. You will also need to install the Chromium chrome driver on your
    system. On Ubuntu 18.04+, you can install it by using the `sudo apt-get install
    chromium-chromedriver` command.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码已经在Ubuntu 18.04和Ubuntu 20.04上进行了广泛测试，这意味着它应该也能在较新版本的Ubuntu上运行，只要安装了Python
    3.6及以上版本。安装了Python 3.6+，以及每个食谱的准备部分中列出的必要Python包后，代码应该也可以在Windows和Mac OSX上正常运行。建议您创建并使用名为`tf2rl-cookbook`的Python虚拟环境来安装本书中的包并运行代码。推荐使用Miniconda或Anaconda进行Python虚拟环境管理。您还需要在系统上安装Chromium
    Chrome驱动程序。在Ubuntu 18.04+上，您可以使用`sudo apt-get install chromium-chromedriver`命令进行安装。
- en: 'The complete code for each recipe in each chapter will be available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个食谱中的完整代码将在此处提供：[https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook)。
- en: Building learning environments for real-world RL
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为现实世界的强化学习（RL）构建学习环境
- en: 'This recipe will teach you how to set up and build WebGym, a **World of Bits**
    (**WoB**)-based OpenAI Gym compatible learning platform for training RL Agents
    for world wide web-based real-world tasks. WoB is an open domain platform for
    web-based Agents. For more information about WoB, check out the following link:
    [http://proceedings.mlr.press/v70/shi17a/shi17a.pdf](http://proceedings.mlr.press/v70/shi17a/shi17a.pdf).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将教你如何设置并构建基于 **世界比特**（**WoB**）的 OpenAI Gym 兼容学习平台 WebGym，用于训练 RL 代理完成全球 web
    上的真实任务。WoB 是一个开放领域的平台，专为基于 web 的代理而设计。如需了解更多关于 WoB 的信息，请查看以下链接：[http://proceedings.mlr.press/v70/shi17a/shi17a.pdf](http://proceedings.mlr.press/v70/shi17a/shi17a.pdf)。
- en: WebGym provides learning environments for Agents to perceive the world wide
    web how we (humans) perceive it – using the pixels rendered on our display screen.
    The Agent interacts with the environment using keyboard and mouse events as actions.
    This allows the Agent to experience the world wide web how we do, which means
    we don't need to make any additional modifications for the Agents to train. This
    allows us to train RL Agents that can directly work with web-based pages and applications
    to complete real-world tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: WebGym 提供了供代理学习的环境，让代理以我们（人类）感知全球 web 的方式进行感知——通过我们显示器上呈现的像素。代理通过使用键盘和鼠标事件作为操作与环境进行互动。这使得代理能够像我们一样体验全球
    web，这意味着我们不需要对代理进行任何额外修改，代理就能进行训练。这样，我们便能够训练能够直接与 web 页面和应用程序协作，完成现实世界任务的 RL 代理。
- en: 'The following image shows a sample **Click-To-Action** (**CTA**) environment,
    where the task is to click on a specific link to get to the next page or step
    in the process:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个 **点击行动**（**CTA**）任务的示例环境，在此环境中，任务是点击特定链接以进入下一个页面或步骤：
- en: '![Figure 6.1 – Sample CTA task requiring a specific link to be clicked ](img/B15074_06_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 需要点击特定链接的示例 CTA 任务](img/B15074_06_01.jpg)'
- en: Figure 6.1 – Sample CTA task requiring a specific link to be clicked
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 需要点击特定链接的示例 CTA 任务
- en: 'Another example of a CTA task is depicted in the following image:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个 CTA 任务的示例如以下图片所示：
- en: '![Figure 6.2 – Sample CTA task requiring a specific option to be selected and
    submitted ](img/B15074_06_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 需要选择并提交特定选项的示例 CTA 任务](img/B15074_06_02.jpg)'
- en: Figure 6.2 – Sample CTA task requiring a specific option to be selected and
    submitted
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 需要选择并提交特定选项的示例 CTA 任务
- en: Let's get started!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook's code repository. WebGym is built on top of the miniwob-plusplus benchmark,
    which has also been made available as part of this book's code repository for
    ease of use.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本教程，你需要激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新环境，以使其与本教程代码库中的最新 conda
    环境规范文件（`tfrl-cookbook.yml`）匹配。WebGym 是构建在 miniwob-plusplus 基准之上的，后者也作为本书代码库的一部分提供，便于使用。
- en: Now, let's begin!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it…
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We will build WebGym by defining the custom `reset` and `step` methods. Then,
    we will define the state and action spaces for the training environments. First,
    we''ll look at the implementation of the `miniwob_env` module. Let''s get started:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过定义自定义的 `reset` 和 `step` 方法来构建 WebGym。然后，我们将为训练环境定义状态和动作空间。首先，我们来看看 `miniwob_env`
    模块的实现。让我们开始：
- en: 'Let''s begin by importing the necessary Python modules:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的 Python 模块：
- en: '[PRE0]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s specify the directory where we will import the local `miniwob` environment:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们指定导入本地 `miniwob` 环境的目录：
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can start to subclass `MiniWoBEnvironment`. We can then call the super
    class''s initialization function to initialize the environment and set the values
    for `base_url` before we configure the `miniwob` environment:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始创建 `MiniWoBEnvironment` 的子类。然后，我们可以调用父类的初始化函数来初始化环境并在配置 `miniwob` 环境之前设置
    `base_url` 的值：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It''s time to customize the `reset(…)` method. To allow environments to be
    randomized, we will use a `seeds` argument to take a random seed. This can be
    used to generate random start states and tasks so that the Agent we train does
    not overfit to a fixed/static web page:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是定制 `reset(…)` 方法的时候了。为了使环境可以随机化，我们将使用 `seeds` 参数来接收随机种子。这可以用来生成随机的起始状态和任务，确保我们训练的代理不会对固定/静态网页产生过拟合：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will redefine the `step(…)` method. Let''s complete the implementation
    in two steps. First, we will define the method with docstrings that explain the
    arguments:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将重新定义`step(…)`方法。让我们分两步完成实现。首先，我们将定义该方法并添加文档字符串，解释方法的参数：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this step, we will complete our implementation of the `step(…)` method:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将完成`step(…)`方法的实现：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'That completes our `MiniWoBEnv` class implementation! To test our class implementation
    and to understand how to use the class, we will write a quick `main()` function:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就完成了我们的`MiniWoBEnv`类实现！为了测试我们的类实现，并理解如何使用这个类，我们将编写一个简短的`main()`函数：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can save the preceding script as `miniwob_env.py` and execute it to see
    the sample environment being acted on by a random Agent. In the next few steps,
    we will extend `MiniWoBEnv` in order to create an OpenAI Gym-compatible learning
    environment interface. Let''s begin by creating a new file named `envs.py` and
    include with the following imports:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以将前面的脚本保存为`miniwob_env.py`并执行它，查看一个随机智能体在样本环境中的表现。在接下来的几个步骤中，我们将扩展`MiniWoBEnv`，以创建一个与OpenAI
    Gym兼容的学习环境接口。让我们首先创建一个名为`envs.py`的新文件，并包括以下导入：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For the first environment, we will implement the `MiniWoBVisualClickEnv` class:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第一个环境，我们将实现`MiniWoBVisualClickEnv`类：
- en: '[PRE8]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s also define the observation and action space for this environment in
    the `__init__` method:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在`__init__`方法中定义该环境的观察空间和动作空间：
- en: '[PRE9]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will further extend the `reset(…)` method to provide an OpenAI Gym-compatible
    interface method:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将进一步扩展`reset(…)`方法，以提供与OpenAI Gym兼容的接口方法：
- en: '[PRE10]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next important piece is the `step` method. We will implement it in the
    following two steps:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个重要的部分是`step`方法。我们将分以下两步来实现它：
- en: '[PRE11]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To complete the `step` method''s implementation, let''s check if the dimensions
    of the actions are as expected and then bind the actions if necessary. Finally,
    we must execute a step in the environment:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完成`step`方法的实现，让我们先检查动作的维度是否符合预期，然后在必要时绑定动作。最后，我们必须在环境中执行一步：
- en: '[PRE12]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can use a descriptive name for the class to register the environment with
    the Gym registry:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用描述性的类名将环境注册到Gym的注册表中：
- en: '[PRE13]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, to register the environment with OpenAI Gym''s registry locally, we
    must add the environment registration information to the `__init__.py` file:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了将环境本地注册到OpenAI Gym的注册表中，我们必须将环境注册信息添加到`__init__.py`文件中：
- en: '[PRE14]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With that, we have completed this recipe!
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样，我们就完成了这个教程！
- en: How it works…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: We have extended the implementation of `MiniWoB-plusplus` in `MiniWoBEnv` so
    that we can use file-based web pages to represent tasks. We extended the `MiniWoBEnv`
    class even further to provide an OpenAI Gym-compatible interface in `MiniWoBVisualClickEnv`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在`MiniWoBEnv`中扩展了`MiniWoB-plusplus`的实现，以便可以使用基于文件的网页来表示任务。我们进一步扩展了`MiniWoBEnv`类，在`MiniWoBVisualClickEnv`中提供了与OpenAI
    Gym兼容的接口：
- en: 'To get a clear picture of how an RL Agent will be learning to complete the
    task in this environment, consider the following screenshot. Here, the Agent tries
    to understand the objective of the task by trying out different actions, which
    in this environment translates to clicking on different areas of the web page
    (represented on the right-hand side by blue dots). Eventually, the RL Agent clicks
    on the correct button and starts to understand what the task description means,
    as well as what the buttons are intended for, since it was rewarded for clicking
    on the correct spot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚地了解RL智能体如何在这个环境中学习完成任务，请参考以下截图。在这里，智能体通过尝试不同的动作来理解任务的目标，在这个环境中，动作表现为点击网页上的不同区域（右侧由蓝色点表示）。最终，RL智能体点击了正确的按钮，开始理解任务描述的含义，以及按钮的作用，因为它点击正确位置后得到了奖励：
- en: '![Figure 6.3 – Visualizing the Agent''s actions while it''s learning to complete
    the CTA task ](img/B15074_06_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 可视化智能体在学习完成CTA任务时的动作](img/B15074_06_03.jpg)'
- en: Figure 6.3 – Visualizing the Agent's actions while it's learning to complete
    the CTA task
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 可视化智能体在学习完成CTA任务时的动作
- en: Now, it's time to move on to the next recipe!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了继续下一个教程的时候！
- en: Building an RL Agent to complete tasks on the web – Call to Action
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个RL智能体以在网页上完成任务——行动号召
- en: This recipe will teach you how to implement an RL training script so that you
    can train an RL Agent to handle `OK`/`Cancel` dialog boxes, where you need you
    to click to acknowledge/dismiss the pop-up notification, and the `Click to learn
    more` button. In this recipe, you will instantiate a RL training environment that
    provides visual rendering for the web pages containing a CTA task. You will be
    training a **proximal policy optimization** (**PPO**)-based deep RL Agent that's
    been implemented using TensorFlow 2.x to learn how to complete the task at hand.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方将教您如何实现一个 RL 训练脚本，使您能够训练一个 RL 代理来处理 `OK`/`Cancel` 对话框，您需要点击以确认/取消弹出通知，以及
    `Click to learn more` 按钮。在本配方中，您将实例化一个 RL 训练环境，该环境为包含 CTA 任务的网页提供视觉渲染。您将训练一个基于
    **近端策略优化**（**PPO**）的深度 RL 代理，该代理使用 TensorFlow 2.x 实现，学习如何完成当前任务。
- en: 'The following image illustrates a set of observations from a randomized CTA
    environment (with different seeds) so that you understand the task that the Agent
    will be solving:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了来自随机化 CTA 环境（使用不同种子）的一组观察数据，以便您理解代理将要解决的任务：
- en: '![Figure 6.4 – Screenshot of the Agent''s observations from a randomized CTA
    environment ](img/B15074_06_04.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 来自随机化 CTA 环境的代理观察截图](img/B15074_06_04.jpg)'
- en: Figure 6.4 – Screenshot of the Agent's observations from a randomized CTA environment
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 来自随机化 CTA 环境的代理观察截图
- en: Let's begin!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook''s code repository. If the following `import` statements run without
    any issues, then you are ready to get started:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此配方，您需要激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新该环境，使其与本食谱代码库中的最新 conda
    环境规范文件（`tfrl-cookbook.yml`）匹配。如果以下 `import` 语句能够顺利执行，那么您就准备好开始了：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let's begin!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: In this recipe, we will be implementing a complete training script, including
    command-line argument parsing for training hyperparameter configuration. As you
    may have noticed from the `import` statements, we will be using Keras's functional
    API for TensorFlow 2.x to implement the **deep neural networks** (**DNNs**) we
    will be using as part of the Agent's algorithm implementation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将实现一个完整的训练脚本，包括用于训练超参数配置的命令行参数解析。正如您从 `import` 语句中看到的，我们将使用 Keras 的
    TensorFlow 2.x 函数式 API 来实现我们将在代理算法实现中使用的**深度神经网络**（**DNNs**）。
- en: 'The following steps will guide you through the implementation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导您完成实现：
- en: 'Let''s begin by defining the command-line arguments for the CTA Agent training
    script:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先定义 CTA 代理训练脚本的命令行参数：
- en: '[PRE16]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will create a TensorBoard logger so that we can log and visualize
    the live training progress of the CTA Agent:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 TensorBoard 日志记录器，以便记录和可视化 CTA 代理的实时训练进度：
- en: '[PRE17]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the following steps, we will implement the `Actor` class. However, we will
    begin by implementing the `__init__` method:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将实现 `Actor` 类。然而，我们将首先实现 `__init__` 方法：
- en: '[PRE18]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we will define the DNN that will represent the Actor''s model. We will
    split the implementation of the DNN into multiple steps as it''s going to be a
    bit long due to several neural network layers being stacked together. As the first
    and main processing step, we will implement a block by stacking convolution-pooling-convolution-pooling
    layers:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义表示代理模型的 DNN。我们将把 DNN 的实现拆分为多个步骤，因为它会稍长一些，涉及多个神经网络层的堆叠。作为第一个主要处理步骤，我们将通过堆叠卷积-池化-卷积-池化层来实现一个模块：
- en: '[PRE19]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we will flatten the output from the pooling layer so that we can start
    using fully connected or dense layers with dropout to generate the output we expect
    from the Actor network:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将扁平化池化层的输出，以便开始使用带有丢弃层的全连接层或稠密层，生成我们期望的代理网络输出：
- en: '[PRE20]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We need to scale and clip the predicted value so that the values are bounded
    and lie within the range we expect the actions to be in. Let''s use the **Lambda
    layer** to implement custom clipping and scaling, as shown in the following code
    snippet:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要对预测值进行缩放和裁剪，以确保值被限定在我们期望的范围内。让我们使用 **Lambda 层** 来实现自定义的裁剪和缩放，如以下代码片段所示：
- en: '[PRE21]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'That completes our `nn_model` implementation. Now, let''s define a convenience
    function to get an action, given a state:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们的`nn_model`实现。现在，让我们定义一个便捷函数，以便给定状态时获取一个动作：
- en: '[PRE22]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, it''s time to implement the main train method. This will update the parameters
    of the Actor network:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，到了实现主要训练方法的时候。这个方法将更新Actor网络的参数：
- en: '[PRE23]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Although we are using `compute_loss` and `log_pdf` in the preceding `train`
    method, we haven''t really defined them yet! Let''s implement them one after the
    other, starting with the `compute_loss` method:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管我们在之前的`train`方法中使用了`compute_loss`和`log_pdf`，但我们还没有真正定义它们！让我们一个接一个地实现它们，从`compute_loss`方法开始：
- en: '[PRE24]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this step, we will implement the `log_pdf` method:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将实现`log_pdf`方法：
- en: '[PRE25]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The previous step concludes out Actor implementation. Now, it''s time to start
    implementing the `Critic` class:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一步已经完成了Actor的实现。现在是时候开始实现`Critic`类了：
- en: '[PRE26]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next up is the `Critic` class''s neural network model. Like the Actor''s neural
    network model, this is going to be a DNN. We will split the implementation into
    a few steps. First, let''s implement a convolution-pooling-convolution-pooling
    block:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是`Critic`类的神经网络模型。与Actor的神经网络模型类似，这也是一个DNN。我们将把实现分为几个步骤。首先，来实现一个卷积-池化-卷积-池化模块：
- en: '[PRE27]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'While we could stack more blocks or layers to deepen the neural network, for
    our current task, we already have a sufficient number of parameters in the DNN
    to learn how to perform well at the CTA task. Let''s add the fully connected layers
    so that we can eventually produce the state-conditioned action value:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们可以堆叠更多的模块或层来加深神经网络，但对于我们当前的任务，DNN中已经有足够的参数来学习如何在CTA任务中表现良好。让我们添加全连接层，这样我们就能最终产生状态条件下的动作值：
- en: '[PRE28]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s implement a method that will compute the Critic''s learning loss, which
    is essentially the mean-squared error between the temporal difference learning
    targets and the values predicted by the Critic:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现一个方法来计算Critic的学习损失，本质上是时间差学习目标与Critic预测的值之间的均方误差：
- en: '[PRE29]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s finalize our `Critic` class by implementing the `train` method to update
    the Critic''s parameters:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过实现`train`方法来更新Critic的参数，从而最终完成`Critic`类的实现：
- en: '[PRE30]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we can utilize the Actor and the Critic implementation to build our PPO
    Agent so that it can work with high-dimensional (image) observations. Let''s begin
    by defining the `PPOAgent` class''s `__init__` method:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以利用Actor和Critic的实现来构建我们的PPO代理，使其能够处理高维（图像）观察。让我们从定义`PPOAgent`类的`__init__`方法开始：
- en: '[PRE31]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will be using **Generalized Advantage Estimates** (**GAE**) to update our
    policy. So, let''s implement a method that will calculate the GAE target values:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用**广义优势估计**（**GAE**）来更新我们的策略。所以，让我们实现一个方法来计算GAE目标值：
- en: '[PRE32]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We are at the core of this script! Let''s define the training routine for the
    deep PPO Agent. We will split the implementation into multiple steps to make it
    easy to follow. We will begin with the outermost loop, which must be running for
    a configurable maximum number of episodes:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们来到了这个脚本的核心！让我们为深度PPO代理定义训练例程。我们将把实现分为多个步骤，以便于跟随。我们将从最外层的循环开始，这个循环必须为可配置的最大回合数运行：
- en: '[PRE33]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will implement the logic for stepping through the environment and
    handling the end of an episode by checking the `done` values from the environments:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现遍历环境并通过检查环境中的`done`值来处理回合结束的逻辑：
- en: '[PRE34]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we will implement the logic that will check for the end of an episode
    or if it is time to update and perform an update step:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现检查回合是否结束或是否需要更新的逻辑，并执行更新步骤：
- en: '[PRE35]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that we have the updated GAE targets, we can train the Actor and Critic
    networks and log the losses and other training metrics for tracking purposes:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经有了更新的GAE目标，可以训练Actor和Critic网络，并记录损失和其他训练指标以便追踪：
- en: '[PRE36]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, let''s implement the `__main__` function to train the CTA Agent:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们实现`__main__`函数来训练CTA代理：
- en: '[PRE37]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That completes this recipe! Let's briefly recap on how it works.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了这个食谱！让我们简要回顾一下它是如何工作的。
- en: How it works…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we implemented a PPO-based deep RL Agent and provided a training
    mechanism to develop a CTA Agent. Note that for simplicity, we used one instance
    of the environment, though the code can scale for a greater number of environment
    instances to speed up training.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们实现了一个基于PPO的深度强化学习代理，并提供了一个训练机制来开发CTA代理。请注意，为了简化起见，我们使用了一个环境实例，尽管代码可以扩展为更多环境实例，以加速训练。
- en: 'To understand how the Agent training progresses, consider the following sequence
    of images. During the initial stages of training, when the Agent is trying to
    understand the task and the objective of the task, the Agent may just be executing
    random actions (exploration) or even clicking outside the screen, as shown in
    the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解智能体训练的进展，考虑以下一系列图像。在训练的初期阶段，当智能体试图理解任务及其目标时，智能体可能只是在执行随机动作（探索），甚至可能点击屏幕外，如以下截图所示：
- en: '![Figure 6.5 – Agent clicking outside the screen (no visible blue dot) during
    initial exploration](img/B15074_06_05.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 智能体在初始探索阶段点击屏幕外（没有可见的蓝点）](img/B15074_06_05.jpg)'
- en: Figure 6.5 – Agent clicking outside the screen (no visible blue dot) during
    initial exploration
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 智能体在初始探索阶段点击屏幕外（没有可见的蓝点）
- en: 'As the Agent learns by stumbling upon the correct button to click, it starts
    to make progress. The following screenshot shows the Agent making some progress:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能体通过偶然发现正确的点击按钮开始取得进展，以下截图显示了智能体取得的一些进展：
- en: '![Figure 6.6 – Deep PPO Agent making progress in the CTA task ](img/B15074_06_06.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 深度PPO智能体在CTA任务中的进展](img/B15074_06_06.jpg)'
- en: Figure 6.6 – Deep PPO Agent making progress in the CTA task
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 深度PPO智能体在CTA任务中的进展
- en: 'Finally, when the episode is complete or ends (due to a time limit), the Agent
    receives an observation similar to the one shown in the following screenshot (left):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当回合完成或结束（由于时间限制），智能体将接收到类似以下截图（左侧）所示的观察：
- en: '![Figure 6.7 – End of episode observation (left) and summary of performance
    (right) ](img/B15074_06_07.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 回合结束时的观察（左）和性能总结（右）](img/B15074_06_07.jpg)'
- en: Figure 6.7 – End of episode observation (left) and summary of performance (right)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 回合结束时的观察（左）和性能总结（右）
- en: Now, it's time to move on to the next recipe!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候进入下一个教程了！
- en: Building a visual auto-login bot
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个可视化自动登录机器人
- en: Imagine that you have an Agent or a bot that watches what you are doing and
    automatically logs you into websites whenever you click on a login screen. While
    browser plugins exist that can automatically log you in, they do so using hardcoded
    scripts that only work on the pre-programmed website's login URLs. But what if
    you had an Agent that only relied on the rendered web page – just like you do
    to perform a task – and worked even when the URL changes and when you are on a
    new website with no prior saved data? How cool would that be?! This recipe will
    help you develop a script that will train an Agent to log in on a web page! You
    will learn how to randomize, customize, and increase the generality of the Agent
    to get it to work on any login screen.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一个智能体或机器人，它观察你正在做的事情，并在你点击登录页面时自动帮你登录网站。虽然浏览器插件可以自动登录，但它们使用的是硬编码的脚本，只能在预先编程的网站登录网址上工作。那么，如果你有一个仅依赖于渲染页面的智能体——就像你自己完成任务一样——即使网址发生变化，或者你进入一个没有任何先前保存数据的新网站时，它仍然能够工作，这会有多酷呢？！这个教程将帮助你开发一个脚本，训练一个智能体在网页上登录！你将学习如何随机化、定制和增加智能体的通用性，使它能够在任何登录页面上工作。
- en: 'An example of randomizing and customizing the usernames and passwords for a
    task can be seen in the following image:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随机化和定制任务的用户名和密码的示例如下图所示：
- en: '![Figure 6.8 – Sample observations from a randomized user login task ](img/B15074_06_08.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 随机化用户登录任务的示例观察](img/B15074_06_08.jpg)'
- en: Figure 6.8 – Sample observations from a randomized user login task
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 随机化用户登录任务的示例观察
- en: Let's get started!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: 'To complete this recipe, make sure you have the latest version. First, you
    will need to activate the `tf2rl-cookbook` Python/conda virtual environment. Make
    sure that you update the environment so that it matches the latest conda environment
    specification file (`tfrl-cookbook.yml`) in this cookbook''s code repository.
    If the following `import` statements run without any issues, then you are ready
    to get started:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个教程，请确保你拥有最新版本。首先，你需要激活`tf2rl-cookbook` Python/conda虚拟环境。确保更新该环境，使其与本教程代码库中最新的conda环境规范文件（`tfrl-cookbook.yml`）匹配。如果以下`import`语句能顺利运行，那么你已经准备好开始了：
- en: '[PRE38]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Let's begin!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: In this recipe, we will implement the deep RL-based Login Agent using the PPO
    algorithm.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将使用PPO算法实现基于深度强化学习的登录智能体。
- en: 'Let''s get started:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'First, let''s set up the training script''s command-line arguments and logging:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们设置训练脚本的命令行参数和日志记录：
- en: '[PRE39]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can now directly jump into the `Critic` class''s definition:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以直接跳入`Critic`类的定义：
- en: '[PRE40]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s define the DNN for the Critic model. We''ll begin by implementing
    a perception block composed of convolution-pooling-convolution-pooling. In the
    subsequent steps, we''ll add more depth to the network by stacking another perception
    block:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义Critic模型的DNN。我们将从实现一个由卷积-池化-卷积-池化组成的感知块开始。在随后的步骤中，我们将通过堆叠另一个感知块来增加网络的深度：
- en: '[PRE41]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we will add another perception block so that we can extract more features:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加另一个感知块，以便提取更多特征：
- en: '[PRE42]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we will add a flattening layer, followed by fully connected (dense) layers,
    to bring down the shape of the network''s output to a single action value:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个展平层，接着是全连接（密集）层，将网络输出的形状压缩成一个单一的动作值：
- en: '[PRE43]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To finalize our Critic implementation, let''s define the `compute_loss` method
    and the `update` method in order to train the parameters:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完成我们的Critic实现，让我们定义`compute_loss`方法和`update`方法，以便训练参数：
- en: '[PRE44]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now work on implementing the `Actor` class. We''ll initialize the `Actor`
    class in this step and continue our implementation in the subsequent steps:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始实现`Actor`类。我们将在这一步初始化`Actor`类，并在随后的步骤中继续实现：
- en: '[PRE45]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We will use a similar DNN architecture for our Actor as we did in our Critic
    implementation. So, the `nn_model` method''s implementation will remain the same
    except for the last few layers, where the Actor and Critic''s implementation will
    vary. The Actor network model produces the mean and the standard deviation as
    output. This depends on the action space dimensions. On the other hand, the Critic
    network produces a state-conditioned action value, irrespective of the dimensions
    of the action space. The layers that differ from the Critic''s DNN implementation
    are listed here:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为我们的Actor使用与Critic实现中相似的DNN架构。因此，`nn_model`方法的实现将保持不变，除了最后几层，Actor和Critic的实现将在此处有所不同。Actor网络模型输出均值和标准差，这取决于动作空间的维度。另一方面，Critic网络输出一个状态条件下的动作值，无论动作空间的维度如何。与Critic的DNN实现不同的层如下所示：
- en: '[PRE46]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s implement some methods that will compute the Actor''s loss and `log_pdf`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现一些方法来计算Actor的损失和`log_pdf`：
- en: '[PRE47]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'With the help of these helper methods, our training method implementation becomes
    simpler:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 借助这些辅助方法，我们的训练方法实现变得更加简洁：
- en: '[PRE48]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, let''s implement a method that will get an action from the Actor when
    it''s given a state as input:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们实现一个方法，当给定一个状态作为输入时，它将从Actor中获取一个动作：
- en: '[PRE49]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'That completes our Actor implementation. We can now tie both the Actor and
    the Critic together using the `PPOAgent` class implementation. Since the GAE target
    calculations were discussed in the previous recipe, we will skip this and focus
    on the training method''s implementation:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们的Actor实现。现在，我们可以通过`PPOAgent`类的实现将Actor和Critic结合起来。由于之前的食谱中已经讨论了GAE目标计算，我们将跳过这一部分，专注于训练方法的实现：
- en: '[PRE50]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The Agent''s update is performed at a preset frequency in terms of the number
    of samples collected or at the end of every episode – whichever occurs first:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Agent的更新是在预设的频率下执行的，频率依据收集的样本数量或每个回合结束时执行——以先到者为准：
- en: '[PRE51]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we can run `MiniWoBLoginUserVisualEnv-v0` and train the Agent using
    the following snippet of code:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以运行`MiniWoBLoginUserVisualEnv-v0`并使用以下代码片段训练Agent：
- en: '[PRE52]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: That completes our script for the auto-login Agent. It's time for you to run
    the script to see the Agent's training process in action!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的自动登录Agent脚本。现在是时候运行脚本，查看Agent的训练过程了！
- en: How it works…
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The login task involves clicking on the correct form field and typing in the
    correct username and/or password. For an Agent to be able to do this, it needs
    to master how to use a mouse and keyboard, in addition to processing the visual
    web page to understand the task and the web login form. With enough samples, the
    deep RL Agent will learn a policy to complete this task. Let's take a look at
    the state of the Agent's progress, snapshotted at different stages.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 登录任务包括点击正确的表单字段并输入正确的用户名和/或密码。为了让Agent能够执行此操作，它需要掌握如何使用鼠标和键盘，以及处理网页视觉信息以理解任务和网页登录表单。通过足够的样本，深度RL
    Agent将学习一个策略来完成此任务。让我们来看一下Agent在不同阶段的进度状态快照。
- en: 'The following image shows the Agent successfully entering the username and
    correctly clicking on the password field to enter the password, but not being
    able to complete the task yet:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了代理成功输入用户名并正确点击密码字段输入密码，但仍未完成任务：
- en: '![Figure 6.9 – Screenshot of a trained Agent successfully entering the username
    but not a password ](img/B15074_06_09.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 训练有素的代理成功输入用户名，但未输入密码的屏幕截图](img/B15074_06_09.jpg)'
- en: Figure 6.9 – Screenshot of a trained Agent successfully entering the username
    but not a password
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 训练有素的代理成功输入用户名，但未输入密码
- en: 'In the following image, you can see that the Agent has learned to enter both
    the username and password, but they are not quite right for the task to be classed
    as complete:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到代理已经学会输入用户名和密码，但它们并不完全正确，因此任务尚未完成：
- en: '![Figure 6.10 – Agent entering both the username and password but incorrectly
    ](img/B15074_06_10.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 代理输入用户名和密码，但输入错误](img/B15074_06_10.jpg)'
- en: Figure 6.10 – Agent entering both the username and password but incorrectly
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 代理输入用户名和密码，但输入错误
- en: The same Agent with a different checkpoint, after several thousand more episodes
    of learning, is close to completing the task, as shown in the following image
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的代理，通过不同的检查点，在经历了几千次学习后，已经接近完成任务，如下图所示
- en: '![Figure 6.11 – A well-trained Agent model about to complete the login task
    successfully ](img/B15074_06_11.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 一位训练有素的代理模型即将成功完成登录任务](img/B15074_06_11.jpg)'
- en: Figure 6.11 – A well-trained Agent model about to complete the login task successfully
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 一位训练有素的代理模型即将成功完成登录任务
- en: Now that you understand how the Agent works and behaves, you can customize it
    to your liking and use use cases to train the Agent to automatically log into
    any custom website you want!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了代理是如何工作的及其行为，你可以根据自己的需求对其进行定制，并使用案例来训练代理自动登录任何你想要的自定义网站！
- en: Training an RL Agent to automate flight booking for your travel
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个 RL 代理来自动化你的旅行机票预订
- en: 'In this recipe, you will learn how to implement a deep RL Agent based on the
    `MiniWoBBookFlightVisualEnv` flight booking environment:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，你将学习如何基于 `MiniWoBBookFlightVisualEnv` 飞行预订环境实现一个深度 RL 代理：
- en: '![Figure 6.12 – Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv
    environment ](img/B15074_06_12.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 来自随机化 MiniWoBBookFlightVisualEnv 环境的样本起始状态观察](img/B15074_06_12.jpg)'
- en: Figure 6.12 – Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv
    environment
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 来自随机化 MiniWoBBookFlightVisualEnv 环境的样本起始状态观察
- en: Let's get started!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook''s code repository. If the following `import` statements run without
    any issues, then you are ready to get started:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个食谱，你需要激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新环境，以便它与本食谱代码仓库中的最新 conda
    环境规范文件 (`tfrl-cookbook.yml`) 匹配。如果以下的 `import` 语句能正常运行，那么你就可以开始了：
- en: '[PRE53]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How to do it…
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: In this recipe, we will be implementing a complete training script that you
    will be able to customize and train to book flights!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将实现一个完整的训练脚本，你可以定制并训练它来预订机票！
- en: 'Let''s get started:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'First, let''s expose the hyperparameters as configurable arguments to the training
    script:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们将超参数暴露为可配置的参数，以便在训练脚本中使用：
- en: '[PRE54]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we''ll set up TensorBoard logging for live visualization of the training
    progress:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置 TensorBoard 日志记录，以便实时可视化训练进度：
- en: '[PRE55]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We''ll be using a Replay Buffer to implement Experience Reply. Let''s implement
    a simple `ReplayBuffer` class:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用重放缓冲区来实现经验回放。让我们实现一个简单的 `ReplayBuffer` 类：
- en: '[PRE56]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s start by implementing the `Actor` class:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从实现 `Actor` 类开始：
- en: '[PRE57]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The DNN model for the Actor will be composed of two perception blocks, each
    containing convolution-pooling-convolution-pooling layers, as in our previous
    recipe. We''ll skip this here and look at the implementation of the `train` method
    instead. The full source code, as always, will be available in this cookbook''s
    code repository. Let''s continue with our `train` and `predict` method implementations:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Actor的DNN模型将由两个感知块组成，每个感知块包含卷积-池化-卷积-池化层，正如我们之前的配方。我们将在这里跳过这一部分，直接查看`train`方法的实现。像往常一样，完整的源代码将会在本食谱的代码库中提供。让我们继续实现`train`和`predict`方法：
- en: '[PRE58]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The last piece of our `Actor` class is to implement a function to get the action:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Actor`类的最后一部分是实现一个函数来获取动作：'
- en: '[PRE59]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'With that, our `Actor` class is ready. Now, we can move on and implement the
    `Critic` class:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们的`Actor`类就准备好了。现在，我们可以继续并实现`Critic`类：
- en: '[PRE60]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Similar to the `Actor` class''s DNN model, we will be reusing a similar architecture
    for our `Critic` class from the previous recipe, with two perception blocks. You
    can refer to the full source code of this recipe or the DNN implementation in
    the previous recipe for completeness. Let''s jump into the implementation of the
    `predict` and `g_gradients` computations for the Q function:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于`Actor`类的DNN模型，我们将重新使用之前食谱中的类似架构来构建`Critic`类，包含两个感知块。你可以参考这个食谱的完整源代码或前一个食谱中的DNN实现，以获得完整性。让我们深入实现`predict`和`g_gradients`的计算，以计算Q函数：
- en: '[PRE61]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In order to update our Critic model, we need a loss to drive the parameter
    updates and an actual training step to perform the update. In this step, we will
    implement these two core methods:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更新我们的Critic模型，我们需要一个损失函数来驱动参数更新，并且需要一个实际的训练步骤来执行更新。在这一步中，我们将实现这两个核心方法：
- en: '[PRE62]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'It''s time to bring the Actor and Critic together to implement the DDPGAgent!
    Let''s dive into it:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候将Actor和Critic结合起来实现DDPGAgent了！让我们深入了解：
- en: '[PRE63]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s implement a method that will update the target models of our Actor and
    Critic:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们来实现一个方法，用于更新Actor和Critic的目标模型：
- en: '[PRE64]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we will implement a method that will compute the temporal difference
    targets:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个方法，用于计算时序差分目标：
- en: '[PRE65]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Because we are using a deterministic policy gradient and a policy without a
    distribution to sample from, we will be using a noise function to sample around
    the action predicted by the Actor network. The **Ornstein Uhlenbeck** (**OU**)
    noise process is a popular choice for DDPG Agents. We''ll implement this here:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们使用的是确定性策略梯度且没有分布来从中采样，我们将使用一个噪声函数来在Actor网络预测的动作周围进行采样。**Ornstein Uhlenbeck**（**OU**）噪声过程是DDPG代理的一个流行选择。我们将在这里实现它：
- en: '[PRE66]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we will implement a method that will replay the experience from the replay
    buffer:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个方法，用于重放Replay Buffer中的经验：
- en: '[PRE67]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The last but most crucial thing we must do in our Agent implementation is implement
    the `train` method. We will split the implementation into a few steps. First,
    we will start with the outermost loop, which must run for a maximum number of
    episodes:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在代理（Agent）实现中的最后一个但最关键的任务是实现`train`方法。我们将把实现分为几个步骤。首先，我们将从最外层的循环开始，该循环必须运行至最多的回合数：
- en: '[PRE68]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, we will implement the inner loop, which will run until the end of an
    episode:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现内层循环，它将一直运行到回合结束：
- en: '[PRE69]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We are not done yet! We still need to update our Replay Buffer with the new
    experience that the Agent has collected:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还没有完成！我们仍然需要用代理收集到的新经验来更新我们的Replay Buffer：
- en: '[PRE70]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Are we done?! Almost! We just have to remember to replay the experience when
    the buffer size is bigger than the batch size we used for training:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们完成了吗？！差不多！我们只需要记得在Replay Buffer的大小大于我们用于训练的批量大小时重放经验：
- en: '[PRE71]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'That completes our implementation. Now, we can launch the Agent training on
    the Visual Flight Booking environment using the following `__main__` function:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就完成了我们的实现。现在，我们可以使用以下`__main__`函数启动在Visual Flight Booking环境中的Agent训练：
- en: '[PRE72]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: That's it!
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！
- en: How it works…
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The DDPG Agent collects a series of samples from the flight booking environment
    as it explores and uses this experience to update its policy parameters through
    the Actor and Critic updates. The OU noise we discussed earlier allows the Agent
    to explore while using a deterministic action policy. The flight booking environment
    is quite complex as it requires the Agent to master both the keyboard and the
    mouse, in addition to understanding the task by looking at visual images of the
    task description (visual text parsing), inferring the intended task objective,
    and executing the actions in the correct sequence. The following screenshot shows
    the performance of the Agent upon completing a sufficiently large number of episodes
    of training:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 代理从航班预订环境中收集一系列样本，在探索过程中利用这些经验，通过演员和评论员的更新来更新其策略参数。我们之前讨论的 OU 噪声允许代理在使用确定性行动策略的同时进行探索。航班预订环境相当复杂，因为它要求代理不仅掌握键盘和鼠标的操作，还需要通过查看任务描述的视觉图像（视觉文本解析）来理解任务，推断预期的任务目标，并按正确的顺序执行操作。以下截图展示了代理在完成足够多的训练回合后表现出的效果：
- en: '![Figure 6.13 – A screenshot of the Agent performing the flight booking task
    at different stages of learning ](img/B15074_06_13.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 代理在不同学习阶段执行航班预订任务的截图](img/B15074_06_13.jpg)'
- en: Figure 6.13 – A screenshot of the Agent performing the flight booking task at
    different stages of learning
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 代理在不同学习阶段执行航班预订任务的截图
- en: 'The following screenshot shows the Agent''s screen after the Agent progressed
    to the final stage of the task (although it''s not close to completing the task):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了代理在任务的最终阶段进展后的屏幕（尽管离完成任务还有很长距离）：
- en: '![Figure 6.14 – Screenshot of the Agent progressing all the way to the final
    stage of the flight booking task ](img/B15074_06_14.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 代理完成航班预订任务的最终阶段的截图](img/B15074_06_14.jpg)'
- en: Figure 6.14 – Screenshot of the Agent progressing all the way to the final stage
    of the flight booking task
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 代理完成航班预订任务的最终阶段的截图
- en: With that, we will move on to the next recipe!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，我们将进入下一个食谱！
- en: Training an RL Agent to manage your emails
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个 RL 代理来管理你的电子邮件
- en: 'Email has become an integral part of many people''s lives. The number of emails
    that an average working professional goes through in a workday is growing daily.
    While a lot of email filters exist for spam control, how nice would it be to have
    an intelligent Agent that can perform a series of email management tasks that
    just provide a task description (through text or speech via speech-to-text) and
    are not limited by any APIs that have rate limits? In this recipe, you will develop
    a deep RL Agent and train it on email management tasks! A set of sample tasks
    can be seen in the following image:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件已经成为许多人生活中不可或缺的一部分。一个普通的工作专业人士每天处理的电子邮件数量正逐日增加。虽然存在许多用于垃圾邮件控制的电子邮件过滤器，但如果有一个智能代理，可以执行一系列电子邮件管理任务，只需提供任务描述（通过文本或语音转文本的方式），且不受任何具有速率限制的
    API 限制，岂不是很方便？在本食谱中，你将开发一个深度强化学习代理，并训练它执行电子邮件管理任务！以下图片展示了一组示例任务：
- en: '![Figure 6.15 – A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv
    environment ](img/B15074_06_15.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 来自随机化 MiniWoBEmailInboxImportantVisualEnv 环境的一组观察样本](img/B15074_06_15.jpg)'
- en: Figure 6.15 – A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv
    environment
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 来自随机化 MiniWoBEmailInboxImportantVisualEnv 环境的一组观察样本
- en: Let's get into the details!
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入细节！
- en: Getting ready
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To complete this recipe, make sure you have the latest version. First, you
    will need to activate the `tf2rl-cookbook` Python/conda virtual environment. Make
    sure that you update the environment so that it matches the latest conda environment
    specification file (`tfrl-cookbook.yml`) in this cookbook''s code repository.
    If the following `import` statements run without any issues, then you are ready
    to get started:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个食谱，请确保你拥有最新版本。首先，你需要激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新该环境，以便它与本食谱代码库中的最新
    conda 环境规范文件（`tfrl-cookbook.yml`）匹配。如果以下 `import` 语句可以顺利运行，那么你就可以开始了：
- en: '[PRE73]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Let's begin!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Follow these steps to implement a deep RL Agent and train it to manage important
    emails:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现深度 RL 代理，并训练它来管理重要电子邮件：
- en: 'First, we will define an `ArgumentParser` so that we can configure the script
    from the command line. For a complete list of configurable hyperparameters, please
    refer to the source code for this recipe:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个 `ArgumentParser`，以便从命令行配置脚本。有关可配置超参数的完整列表，请参考本食谱的源代码：
- en: '[PRE74]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Next, let''s set up TensorBoard logging:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们设置 TensorBoard 日志记录：
- en: '[PRE75]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now, we can initialize the `Actor` class:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以初始化 `Actor` 类：
- en: '[PRE76]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Because the observations in our email management environment are visual (images),
    we will need perception capabilities for the Actor in our Agent. For this, we
    must make use of convolution-based perception blocks, as follows:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的电子邮件管理环境中的观察结果是视觉的（图像），我们需要为 Agent 的 Actor 提供感知能力。为此，我们必须使用基于卷积的感知块，如下所示：
- en: '[PRE77]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, let''s add more perception blocks comprising convolutions, followed by
    max pooling layers:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们添加更多的感知块，包括卷积层，接着是最大池化层：
- en: '[PRE78]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Now, we are ready to flatten the DNN output to produce the mean (mu) and standard
    deviation that we want as the output from the Actor. First, let''s add the flattening
    layer and the dense layers:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备将 DNN 输出展平，以生成我们希望从 Actor 输出的均值（mu）和标准差。首先，让我们添加展平层和全连接层：
- en: '[PRE79]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We are now ready to define the final layers of our Actor network. These will
    helps us produce `mu` and `std`, as we discussed in the previous step:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备定义我们 Actor 网络的最终层。这些层将帮助我们生成 `mu` 和 `std`，正如我们在前一步中讨论的那样：
- en: '[PRE80]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'That completes our Actor''s DNN model implementation. To implement the remaining
    methods to complete the Actor class, please refer to the full code for this recipe,
    which can be found in this cookbook''s code repository. We will now focus on defining
    the interfaces for the `Critic` class:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们 Actor 的 DNN 模型实现。为了实现剩余的方法并完成 Actor 类，请参考本食谱的完整代码，该代码可以在本食谱的代码库中找到。接下来，我们将专注于定义
    `Critic` 类的接口：
- en: '[PRE81]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The Critic''s DNN model is also based on the same convolutional neural network
    architecture that we used for the `Actor`. For completeness, please refer to this
    recipe''s full source code, which is available in this cookbook''s code repository.
    We will implement the loss computation and training method here:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Critic 的 DNN 模型也基于与我们为 `Actor` 使用的相同的卷积神经网络架构。为了完整性，请参考本食谱的完整源代码，该代码可在本食谱的代码库中找到。我们将在这里实现损失计算和训练方法：
- en: '[PRE82]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'With that, we can now define our Agent''s class:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些，我们现在可以定义我们的 Agent 类：
- en: '[PRE83]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The preceding code should be familiar to you from the previous Agent implementations
    in this chapter. You can complete the remaining methods (and the training loop)
    based on our previous implementations. If you get stuck, you can refer to the
    full source code for this recipe by going to this cookbook''s code repository.
    We will now write the `__main__` function so that we can train the Agent in `MiniWoBEmailInboxImportantVisualEnv`.
    This will allow us to see the Agent''s learning process in action:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码应该对你来说是熟悉的，来自本章中前面的 Agent 实现。你可以基于我们之前的实现完成剩余的方法（和训练循环）。如果遇到困难，可以通过访问本食谱的代码库，查看完整源代码。我们现在将编写
    `__main__` 函数，这样我们就可以在 `MiniWoBEmailInboxImportantVisualEnv` 中训练 Agent。这将使我们能够看到
    Agent 学习过程的实际表现：
- en: '[PRE84]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: How it works…
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The PPO Agent uses convolutional neural network layers to process the high-dimensional
    visual inputs in the Actor and Critic classes. The PPO algorithm updates the Agent's
    policy parameters using a surrogate loss function that prevents the policy parameters
    from being drastically updated. It then keeps the policy updates within the trust
    region, which makes it robust to hyperparameter choices and a few other factors
    that may lead to instability during the Agent's training regime. The email management
    environment poses as a nice sequential decision-making problem for the deep RL
    Agent. First, the Agent has to choose the correct email from a series of emails
    in an inbox and then perform the desired action (starring the email and so on).
    The Agent only has access to the visual rendering of the inbox, so it needs to
    extract the task specification details, interpret the task specification, and
    then plan and execute the actions!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 代理使用卷积神经网络层来处理演员和评论家类中的高维视觉输入。PPO 算法通过使用一个替代损失函数来更新代理的策略参数，从而防止策略参数的剧烈更新。然后，它将策略更新保持在信任区域内，这使得它对超参数选择以及其他可能导致代理训练过程中不稳定的因素具有鲁棒性。电子邮件管理环境为深度
    RL 代理提供了一个很好的顺序决策问题。首先，代理需要从收件箱中的一系列电子邮件中选择正确的电子邮件，然后执行所需的操作（例如标记电子邮件）。代理只能访问收件箱的视觉渲染，因此它需要提取任务规范的细节，解读任务规范，然后进行规划并执行操作！
- en: 'The following is a screenshot of the Agent''s performance at different stages
    of learning (loaded from different checkpoints):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代理在学习不同阶段的表现截图（从不同的检查点加载）：
- en: '![Figure 6.16 – A series of screenshots showing the Agent''s learning progress
    ](img/B15074_06_16.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 展示代理学习进展的一系列截图](img/B15074_06_16.jpg)'
- en: Figure 6.16 – A series of screenshots showing the Agent's learning progress
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 展示代理学习进展的一系列截图
- en: Now, let's move on to the next recipe!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下一个教程！
- en: Training an RL Agent to automate your social media account management
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个 RL 代理来自动化你的社交媒体账号管理
- en: By the end of this recipe, you will have built a complete deep RL Agent training
    script that can be trained to perform management tasks on your social media account!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程结束时，你将构建一个完整的深度 RL 代理训练脚本，可以训练代理来执行社交媒体账号的管理任务！
- en: 'The following image shows a series of (randomized) tasks from the environment
    that we will be training the Agent in:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了一系列（随机化的）来自环境的任务，我们将在其中训练代理：
- en: '![Figure 6.17 – A sample set of social media account management tasks that
    the Agent has been  asked to solve ](img/B15074_06_17.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – 一个示例社交媒体账号管理任务集，代理被要求解决这些任务](img/B15074_06_17.jpg)'
- en: Figure 6.17 – A sample set of social media account management tasks that the
    Agent has been asked to solve
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 一个示例社交媒体账号管理任务集，代理被要求解决这些任务
- en: Note that there is a scroll bar in this task that the Agent needs to learn how
    to use! The tweet that's relevant to this task may be hidden from the visible
    part of the screen, so the Agent will have to actively explore (by sliding the
    scroll bar up/down) in order to progress!
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个任务中有一个滚动条，代理需要学习如何使用它！与此任务相关的推文可能会被隐藏在屏幕的不可见区域，因此代理必须主动进行探索（通过上下滑动滚动条）才能继续！
- en: Getting ready
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook''s code repository. If the following `import` statements run without
    any issues, then you are ready to get started:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个教程，你需要激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新环境，使其与本教程代码库中的最新 conda
    环境规范文件（`tfrl-cookbook.yml`）匹配。如果以下的 `import` 语句能够正常运行，那么你就可以开始了：
- en: '[PRE85]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: How to do it…
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: Let's start by configuring the Agent training script. After that, you will be
    shown how to complete the implementation.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从配置代理训练脚本开始。之后，你将看到如何完成实现。
- en: 'Let''s get started:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'Let''s jump right into the implementation! We will begin with our `ReplayBuffer`
    implementation:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们直接进入实现！我们将从 `ReplayBuffer` 实现开始：
- en: '[PRE86]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Next, we will implement our `Actor` class:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现我们的 `Actor` 类：
- en: '[PRE87]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The next core piece is the DNN definition for our Actor:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个核心部分是我们演员的 DNN 定义：
- en: '[PRE88]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Depending on the complexity of the task, we can modify (add/reduce) the depth
    of the DNN. We will start by connecting the output of the pooling layers to the
    fully connected layers with dropout:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据任务的复杂性，我们可以修改（增加/减少）DNN 的深度。我们将通过将池化层的输出连接到带有丢弃层的全连接层开始：
- en: '[PRE89]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'That completes our DNN model implementation for the Actor. Now, let''s implement
    the methods that will train the Actor and get the predictions from the Actor model:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们对 Actor 的 DNN 模型实现。现在，让我们实现一些方法来训练 Actor 并获取 Actor 模型的预测：
- en: '[PRE90]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We can now get the actions from our Actor:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以从我们的 Actor 获取动作：
- en: '[PRE91]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Let''s get started with our Critic implementation. Here, we will need to implement
    the Agent class that we are after:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始实现 Critic。这里，我们需要实现我们所需要的智能体类：
- en: '[PRE92]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Note that the Critic''s model is initialized with `self.nn_model()`. Let''s
    implement this here:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，Critic 的模型是通过 `self.nn_model()` 初始化的。让我们在这里实现它：
- en: '[PRE93]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We will complete our DNN architecture for the Critic by funneling the output
    through the fully connected layers with dropout. This way, we receive the necessary
    action values:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过将输出传递通过带有丢弃层的全连接层来完成 Critic 的 DNN 架构。这样，我们可以得到所需的动作值：
- en: '[PRE94]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Now, let''s implement the `g_gradients` and `compute_loss` methods. This should
    be pretty straightforward:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实现 `g_gradients` 和 `compute_loss` 方法。这应该是相当直接的：
- en: '[PRE95]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Finally, we can complete the Critic''s implementation by implementing the `predict`
    and `train` methods:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过实现 `predict` 和 `train` 方法来完成 Critic 的实现：
- en: '[PRE96]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We can now utilize the Actor and Critic to implement our Agent:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以利用 Actor 和 Critic 来实现我们的智能体：
- en: '[PRE97]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Next, we will implement the `update_target` method, as per the DDPG algorithm:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将根据 DDPG 算法实现 `update_target` 方法：
- en: '[PRE98]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'We will not look at the implementation of the `train` method here. Instead,
    we will start the outer loop''s implementation, before completing it in the following
    steps:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里不讨论 `train` 方法的实现。相反，我们将从外部循环的实现开始，然后在接下来的步骤中完成它：
- en: '[PRE99]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The main inner loop''s implementation is as follows:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主要的内部循环实现如下：
- en: '[PRE100]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: That completes our training method implementation. For the implementation of
    the `replay_experience`, `add_ou_noise`, and `get_td_targets` methods, please
    refer to the full source code of this recipe, which can be found in this cookbook's
    code repository.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们训练方法的实现。关于 `replay_experience`、`add_ou_noise` 和 `get_td_targets` 方法的实现，请参考本食谱的完整源代码，该代码可以在本食谱的代码库中找到。
- en: 'Let''s write our `__main__` function so that we can start training the Agent
    in our social media environment:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写 `__main__` 函数，这样我们就可以开始在社交媒体环境中训练智能体：
- en: '[PRE101]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: How it works…
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Let''s visually explore how a well-trained Agent progresses through social
    media management tasks. The following screenshot shows the Agent learning to use
    the scroll bar to "navigate" in this environment:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地探索一下一个训练良好的智能体是如何在社交媒体管理任务中进展的。下图展示了智能体学习使用滚动条在这个环境中“导航”：
- en: '![Figure 6.18 – The Agent learning to navigate using the scroll bar  ](img/B15074_06_18.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 智能体学习使用滚动条进行导航](img/B15074_06_18.jpg)'
- en: Figure 6.18 – The Agent learning to navigate using the scroll bar
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 智能体学习使用滚动条进行导航
- en: 'Note that the task specification does not imply anything related to the scroll
    bar or the navigation, and that the Agent was able to explore and figure out that
    it needs to navigate in order to progress with the task! The following screenshot
    shows the Agent progressing much further by choosing the correct tweet but clicking
    on the wrong action; that is, `Embed Tweet` instead of the `Mute` button:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，任务规范并未涉及任何与滚动条或导航相关的内容，智能体能够探索并发现它需要导航才能继续进行任务！下图展示了智能体在选择正确的推文后，点击了错误的操作；也就是点击了
    `Embed Tweet` 而不是 `Mute` 按钮：
- en: '![Figure 6.19 – The Agent clicking on Embed Tweet when the goal was to click
    on Mute ](img/B15074_06_19.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – 当目标是点击“静音”时，智能体点击了“嵌入推文”](img/B15074_06_19.jpg)'
- en: Figure 6.19 – The Agent clicking on Embed Tweet when the goal was to click on
    Mute
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 当目标是点击“静音”时，智能体点击了“嵌入推文”
- en: After 96 million episodes of training, the Agent was sufficiently able to solve
    the task. The following screenshot shows the Agent's performance on an evaluation
    episode (the Agent was loaded from a checkpoint)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在 9600 万个训练回合后，智能体已足够能够解决该任务。下图展示了智能体在评估回合中的表现（智能体是从检查点加载的）
- en: '![Figure 6.20 – The Agent loaded from trained parameters about to complete
    the task successfully  ](img/B15074_06_20.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – 从训练参数加载的代理即将成功完成任务](img/B15074_06_20.jpg)'
- en: Figure 6.20 – The Agent loaded from trained parameters about to complete the
    task successfully
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 从训练参数加载的代理即将成功完成任务
- en: That concludes this recipe and this chapter. Happy training!
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本教程的内容以及本章的总结。祝你训练愉快！
