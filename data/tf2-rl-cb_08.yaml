- en: '*Chapter 8*: Distributed Training for Accelerated Development of Deep RL Agents'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：加速深度强化学习代理开发的分布式训练'
- en: Training Deep RL agents to solve a task takes enormous wall-clock time due to
    the high sample complexity. For real-world applications, iterating over agent
    training and testing cycles at a faster pace plays a crucial role in the market
    readiness of a Deep RL application. The recipes in this chapter provide instructions
    on how to speed up Deep RL agent development using the distributed training of
    deep neural network models by leveraging TensorFlow 2.x’s capabilities. Strategies
    for utilizing multiple CPUs and GPUs both on a single machine and across a cluster
    of machines are discussed. Multiple recipes for training distributed **Deep Reinforcement
    Learning** (**Deep RL**) agents using the **Ray**, **Tune**, and **RLLib** frameworks
    are also provided.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度强化学习代理解决任务需要大量的时间，因为其样本复杂度很高。对于实际应用，快速迭代代理训练和测试周期对于深度强化学习应用的市场就绪度至关重要。本章中的配方提供了如何利用
    TensorFlow 2.x 的能力，通过分布式训练深度神经网络模型来加速深度强化学习代理开发的说明。讨论了如何在单台机器以及跨机器集群上利用多个 CPU
    和 GPU 的策略。本章还提供了使用**Ray**、**Tune** 和 **RLLib** 框架训练分布式**深度强化学习**（**Deep RL**）代理的多个配方。
- en: 'Specifically, the following recipes are a part of this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章包含以下配方：
- en: Building distributed deep learning models using TensorFlow 2.x – Multi-GPU training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.x 构建分布式深度学习模型 – 多 GPU 训练
- en: Scaling up and out – Multi-machine, multi-GPU training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展规模与范围 – 多机器、多 GPU 训练
- en: Training Deep RL agents at scale – Multi-GPU PPO agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模训练深度强化学习代理 – 多 GPU PPO 代理
- en: Building blocks for distributed Deep Reinforcement Learning for accelerated
    training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为加速训练构建分布式深度强化学习的构建模块
- en: Large-scale Deep RL agent training using Ray, Tune, and RLLib
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray、Tune 和 RLLib 进行大规模深度强化学习（Deep RL）代理训练
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04
    and should work with later versions of Ubuntu if Python 3.6+ is available. With
    Python 3.6+ installed along with the necessary Python packages, as listed before
    the start of each of the recipes, the code should run fine on Windows and Mac
    OSX too. It is advised to create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Miniconda or Anaconda installation
    for Python virtual environment management is recommended.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码在 Ubuntu 18.04 和 Ubuntu 20.04 上经过广泛测试，且如果安装了 Python 3.6+，应该也能在之后版本的 Ubuntu
    上运行。只要安装了 Python 3.6+ 以及所需的 Python 包（每个配方开始前都会列出），代码也应该能够在 Windows 和 Mac OSX 上正常运行。建议创建并使用名为
    `tf2rl-cookbook` 的 Python 虚拟环境来安装本书中所需的包并运行代码。推荐使用 Miniconda 或 Anaconda 来管理 Python
    虚拟环境。
- en: 'The complete code for each recipe in each chapter will be available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个配方的完整代码可以在此获取：[https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook)。
- en: Distributed deep learning models using TensorFlow 2.x – Multi-GPU training
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.x 进行分布式深度学习模型训练 – 多 GPU 训练
- en: Deep RL utilizes a deep neural network for policy, value-function, or model
    representations. For higher-dimensional observation/state spaces, for example,
    in the case of image or image-like observations, it is typical to use **convolutional
    neural network** (**CNN**) architectures. While CNNs are powerful and enable training
    Deep RL policies for vision-based control tasks, training deep CNNs requires a
    lot of time, especially in the RL setting. This recipe will help you understand
    how we can leverage TensorFlow 2.x’s distributed training APIs to train deep **residual
    networks** (**ResNets**) using multiple GPUs. The recipe comes with configurable
    building blocks that you can use to build Deep RL components like deep policy
    networks or value networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习利用深度神经网络进行策略、价值函数或模型表示。对于高维观察/状态空间，例如图像或类似图像的观察，通常会使用**卷积神经网络**（**CNN**）架构。虽然
    CNN 强大且能训练适用于视觉控制任务的深度强化学习策略，但在强化学习的设置下，训练深度 CNN 需要大量时间。本配方将帮助你了解如何利用 TensorFlow
    2.x 的分布式训练 API，通过多 GPU 训练深度**残差网络**（**ResNets**）。本配方提供了可配置的构建模块，你可以用它们来构建深度强化学习组件，比如深度策略网络或价值网络。
- en: Let’s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. Having access to a (local or cloud) machine with one or more GPUs will
    be beneficial for this recipe. We will be using the `tensorflow_datasets`. This
    should be already installed if you used `tfrl-cookbook.yml` to set up/update your
    conda environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，你需要首先激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新环境，以匹配食谱代码库中最新的 conda
    环境规范文件（`tfrl-cookbook.yml`）。拥有一台（本地或云端）配备一个或多个 GPU 的机器将对这个食谱有帮助。我们将使用 `tensorflow_datasets`，如果你使用
    `tfrl-cookbook.yml` 来设置/更新了你的 conda 环境，它应该已经安装好了。
- en: Now, let’s begin!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The implementation in this recipe is based on the latest official TensorFlow
    documentation/tutorial. The following steps will help you get a good command over
    TensorFlow 2.x’s distributed execution capabilities. We will be using a ResNet
    model as an example of a large model that will benefit from being trained in a
    distributed fashion, utilizing multiple GPUs to speed up training. We will discuss
    the code snippets for the main components for building a ResNet. Please refer
    to the `resnet.py` file in the cookbook’s code repository for the full and complete
    implementation. Let’s get started:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中的实现基于最新的官方 TensorFlow 文档/教程。接下来的步骤将帮助你深入掌握 TensorFlow 2.x 的分布式执行能力。我们将使用
    ResNet 模型作为大模型的示例，它将从分布式训练中受益，利用多个 GPU 加速训练。我们将讨论构建 ResNet 的主要组件的代码片段。完整的实现请参考食谱代码库中的
    `resnet.py` 文件。让我们开始：
- en: 'Let’s jump right into the template for building residual neural networks:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们直接进入构建残差神经网络的模板：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With the above template for a ResNet block, we can quickly build ResNets with
    multiple ResNet blocks. We will implement a ResNet with one ResNet block here
    in the book, and you will find the ResNet implemented with multiple configurable
    numbers and sizes of ResNet blocks in the code repository. Let’s get started and
    complete the ResNet implementation in the following several steps, focusing on
    one important concept at a time. First, let’s define the function signature:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上面的 ResNet 块模板，我们可以快速构建包含多个 ResNet 块的 ResNet。在本书中，我们将实现一个包含一个 ResNet 块的 ResNet，你可以在代码库中找到实现了多个可配置数量和大小的
    ResNet 块的 ResNet。让我们开始并在接下来的几个步骤中完成 ResNet 的实现，每次集中讨论一个重要的概念。首先，让我们定义函数签名：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s handle the channel ordering in the input image data representation.
    The most common ordering of the dimensions is either: `batch_size` x `channels`
    x `width` x `height` or `batch_size` x `width` x `height` x `channels`. We will
    handle these two cases:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们处理输入图像数据表示中的通道顺序。最常见的维度顺序是：`batch_size` x `channels` x `width` x `height`
    或 `batch_size` x `width` x `height` x `channels`。我们将处理这两种情况：
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s apply zero padding to the input and apply initial layers to start
    processing:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对输入数据进行零填充，并应用初始层开始处理：
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It’s time to add the ResNet blocks using the `resnet_block` function we created:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候使用我们创建的 `resnet_block` 函数来添加 ResNet 块了：
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the final layer, we want to add a `softmax` activated `Dense` (fully connected)
    layer with the number of nodes equal to the number of output classes needed for
    the task:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为最终层，我们希望添加一个经过 `softmax` 激活的 `Dense`（全连接）层，节点数量等于任务所需的输出类别数：
- en: '[PRE5]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The last step in the ResNet model building function is to wrap the layers as
    a TensorFlow 2.x Keras model and return the output:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 ResNet 模型构建函数中的最后一步是将这些层封装为一个 TensorFlow 2.x Keras 模型，并返回输出：
- en: '[PRE6]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using the ResNet function that we just discussed, it becomes quite easy to
    build deep residual networks of varying layer depths by simply changing the number
    of blocks. For example, the following is possible:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们刚才讨论的 ResNet 函数，通过简单地改变块的数量，构建具有不同层深度的深度残差网络变得非常容易。例如，以下是可能的：
- en: '[PRE7]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With our model defined, we can jump to the multi-GPU training code. The remaining
    steps in this recipe will guide you through the implementation that will allow
    you to speed up training the ResNet using all the available GPUs on a machine.
    Let’s start by importing the `ResNet` module that we built along with the `tensorflow_datasets`
    module:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义好我们的模型后，我们可以跳到多 GPU 训练代码。本食谱中的剩余步骤将引导你完成实现过程，帮助你利用机器上的所有可用 GPU 加速训练 ResNet。让我们从导入我们构建的
    `ResNet` 模块以及 `tensorflow_datasets` 模块开始：
- en: '[PRE8]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can now choose which dataset we want to use to exercise our distributed
    training pipeline. For this recipe, we will use the `dmlab` dataset that contains
    images typically observed by RL agents acting in the DeepMind Lab environment.
    Depending on the compute capabilities of the GPUs, RAM, and CPUs on your training
    machine, you may want to use a smaller dataset such as `CIFAR10`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以选择使用哪个数据集来运行我们的分布式训练管道。在这个食谱中，我们将使用`dmlab`数据集，该数据集包含在DeepMind Lab环境中，RL代理通常观察到的图像。根据你训练机器的GPU、RAM和CPU的计算能力，你可能想使用一个更小的数据集，比如`CIFAR10`：
- en: '[PRE9]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next step needs your full attention! We are going to choose the distributed
    execution strategy. TensorFlow 2.x has wrapped a lot of functionality into a simple
    API call like the one listed here:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步需要你全神贯注！我们将选择分布式执行策略。TensorFlow 2.x将许多功能封装成了一个简单的API调用，如下面所示：
- en: '[PRE10]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will declare the key hyperparameters in this step that you can adjust depending
    on your machine’s hardware (such as RAM and GPU memory):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将声明关键超参数，你可以根据机器的硬件（例如RAM和GPU内存）进行调整：
- en: '[PRE11]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Before we start preparing the datasets, let’s implement a preprocessing function
    that performs operations before we pass the images to the neural network. You
    can add your own custom preprocessing operations. In this recipe, we will only
    need to cast the image data to `float32` first and then convert the image pixel
    value ranges to be [0, 1] rather than the typical interval of [0, 255]:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始准备数据集之前，让我们实现一个预处理函数，该函数在将图像传递给神经网络之前执行操作。你可以添加你自己的自定义预处理操作。在这个食谱中，我们只需要首先将图像数据转换为`float32`，然后将图像像素值范围转换为[0,
    1]，而不是典型的[0, 255]区间：
- en: '[PRE12]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are ready to create the dataset splits for training and validation/testing:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好为训练和验证/测试创建数据集划分：
- en: '[PRE13]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are at the crucial step of this recipe! Let’s instantiate and compile our
    model within the scope of the distributed strategy:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经到了这个食谱的关键步骤！让我们在分布式策略的范围内实例化并编译我们的模型：
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s also create callbacks for logging to TensorBoard and checkpointing our
    model parameters during training:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们还创建一些回调，用于将日志记录到TensorBoard，并在训练过程中检查点保存我们的模型参数：
- en: '[PRE15]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With that, we have everything needed to train our model using the distributed
    strategy. With Keras’s user-friendly `fit()` API, it is as simple as the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些，我们已经具备了使用分布式策略训练模型所需的一切。借助Keras用户友好的`fit()`API，它就像下面这样简单：
- en: '[PRE16]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When the preceding line is executed, the training process will start. We can
    also manually save the model using the following lines:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当执行前面的行时，训练过程将开始。我们也可以使用以下几行手动保存模型：
- en: '[PRE17]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once we have a saved checkpoint, it is easy to load the weights and start evaluating
    the model:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们保存了检查点，加载权重并开始评估模型就变得很容易：
- en: '[PRE18]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To verify that the trained model using the distributed strategy works with
    and without replication, we will load it using two different methods in the following
    steps and evaluate. First, let’s load the model without replicating using the
    (same) strategy we used to train the model:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证使用分布式策略训练的模型在有复制和没有复制的情况下都能正常工作，我们将在接下来的步骤中使用两种不同的方法加载并评估它。首先，让我们使用我们用来训练模型的（相同的）策略加载不带复制的模型：
- en: '[PRE19]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, let’s load the model within the distributed execution strategy’s scope,
    which would create replicas and evaluate the model:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们在分布式执行策略的范围内加载模型，这将创建副本并评估模型：
- en: '[PRE20]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When you execute the preceding two code blocks, you will notice that both the
    methods result in the same evaluation accuracy, which is a good sign and signifies
    that we can use the model for prediction without any constraints on the execution
    strategy!
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你执行前面的两个代码块时，你会发现两种方法都会得到相同的评估准确度，这是一个好兆头，意味着我们可以在没有任何执行策略限制的情况下使用模型进行预测！
- en: That completes our recipe. Let’s recap and look at how the recipe works.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们的食谱。让我们回顾一下并看看食谱是如何工作的。
- en: How it works...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: A residual block in a neural network architecture applies convolution filters
    followed by multiple identity blocks. Specifically, a convolution block is applied
    once, followed by (size - 1) identity blocks where size is an integer representing
    the number of constituent convolutional-identity blocks. The identity block implements
    the shortcut or skip connections for the inputs to go through without being filtered
    through convolution operators. The convolutional block implements convolution
    layers followed by batch-normalization activation, followed by one or more sets
    of convolution-batchnorm-activation layers. The `resnet` module we built uses
    these convolution and identity building blocks to build a full ResNet with varying
    sizes that can be configured by simply changing the number of blocks. The size
    of the network is calculated as `6 * num_blocks + 2`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构中的残差块应用了卷积滤波器，后接多个恒等块。具体来说，卷积块应用一次，接着是(size - 1)个恒等块，其中size是一个整数，表示卷积-恒等块的数量。恒等块实现了跳跃连接或短路连接，使得输入可以绕过卷积操作直接通过。卷积块则包含卷积层，后接批量归一化激活，再接一个或多个卷积-批归一化-激活层。我们构建的`resnet`模块使用这些卷积和恒等构建块来构建一个完整的ResNet，并且可以通过简单地更改块的数量来配置不同大小的网络。网络的大小计算公式为`6
    * num_blocks + 2`。
- en: 'Once our ResNet model was ready, we used the `tensorflow_datasets` module to
    generate training and validation datasets. The TensorFlow Datasets module offers
    several popular datasets, such as CIFAR10, CIFAR100, and DMLAB, that have images
    and the associated labels for classification tasks. The list of all the available
    datasets can be found here: [https://tensorflow.org/datasets/catalog](https://tensorflow.org/datasets/catalog).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的ResNet模型准备好，我们使用`tensorflow_datasets`模块生成训练和验证数据集。TensorFlow数据集模块提供了几个流行的数据集，如CIFAR10、CIFAR100和DMLAB，这些数据集包含图像及其相关标签，用于分类任务。所有可用数据集的列表可以在此找到：[https://tensorflow.org/datasets/catalog](https://tensorflow.org/datasets/catalog)。
- en: In this recipe, we used the Mirrored Strategy for distributed execution using
    `tf.distribute.MirroredStrategy`, which enables synchronous distributed training
    using multiple replicas on one machine. Even with distributed execution with multiple
    replicas, we saw that the usual logging and checkpointing using callbacks worked
    as expected. We also verified that loading a saved model and running inference
    for evaluation works both with and without replication, making it portable without
    any added constraints just because the training utilized a distributed execution
    strategy!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用了`tf.distribute.MirroredStrategy`的镜像策略进行分布式执行，它允许在一台机器上使用多个副本进行同步分布式训练。即使是在多副本的分布式执行下，我们发现使用回调进行常规的日志记录和检查点保存依然如预期工作。我们还验证了加载保存的模型并运行推理进行评估在有或没有复制的情况下都能正常工作，这使得模型在训练过程中使用了分布式执行策略后，依然具有可移植性，不会因增加任何额外限制而受影响！
- en: It’s time to advance to the next recipe!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进入下一个食谱了！
- en: Scaling up and out – Multi-machine, multi-GPU training
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展与扩展 – 多机器，多GPU训练
- en: To reach the highest scale in terms of the distributed training of deep learning-based
    models, we need the capability to leverage compute resources across GPUs and across
    machines. This can significantly reduce the time it takes to iterate over or develop
    new models and architectures for the problem you are trying to solve. With easy
    access to cloud computing services such as Microsoft Azure, Amazon AWS, and Google’s
    GCP, renting multiple GPU-equipped machines for an hourly rate has become easier
    and much more common. It is also more economical than setting up and maintaining
    your own multi-GPU multi-machine node. This recipe will provide a quick walk-through
    of training deep models using TensorFlow 2.x’s multi-worker mirrored distributed
    execution strategy based on the official documentation, which you can use and
    easily customize for your use cases. For the multi-machine, multi-GPU distributed
    training example in this recipe, we will train a deep residual network (ResNet
    or resnet) for typical image classification tasks. The same network architecture
    can be used by RL agents for their policy or value-function representation with
    a slight modification to the output layer, as we will see in the later recipes
    of this chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在深度学习模型的分布式训练中实现最大规模，我们需要能够跨 GPU 和机器利用计算资源。这可以显著减少迭代或开发新模型和架构所需的时间，从而加速您正在解决的问题的进展。借助
    Microsoft Azure、Amazon AWS 和 Google GCP 等云计算服务，按小时租用多台 GPU 配备的机器变得更加容易且普遍。这比搭建和维护自己的多
    GPU 多机器节点更经济。这个配方将提供一个快速的演练，展示如何使用 TensorFlow 2.x 的多工作节点镜像分布式执行策略训练深度模型，基于官方文档，您可以根据自己的使用场景轻松定制。在本配方的多机器多
    GPU 分布式训练示例中，我们将训练一个深度残差网络（ResNet 或 resnet）用于典型的图像分类任务。相同的网络架构也可以通过对输出层进行轻微修改，供
    RL 智能体用于其策略或价值函数表示，正如我们将在本章后续的配方中看到的那样。
- en: Let’s get started!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. To exercise the distributed training pipeline, it is recommended to
    set up a cluster with two or more machines equipped with GPUs either locally or
    on a cloud instance such as Azure, AWS, or GCP. While the training script we will
    implement can utilize multiple machines in a cluster, it is not absolutely necessary
    to have a cluster set up, although it is encouraged.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成此配方，您首先需要激活`tf2rl-cookbook` Python/conda 虚拟环境。确保更新环境，以匹配配方代码仓库中的最新 conda
    环境规范文件（`tfrl-cookbook.yml`）。为了运行分布式训练管道，建议设置一个包含两个或更多安装了 GPU 的机器的集群，可以是在本地或云实例中，如
    Azure、AWS 或 GCP。虽然我们将要实现的训练脚本可以利用集群中的多台机器，但并不绝对需要设置集群，尽管推荐这样做。
- en: Now, let’s begin!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Since this distributed training setup involves multiple machines, we need a
    communication interface between the machines and a way to address the individual
    machines. This is typically done using the existing network infrastructure and
    IP address:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此分布式训练设置涉及多台机器，我们需要一个机器之间的通信接口，并且要能够寻址每台机器。这通常通过现有的网络基础设施和 IP 地址来完成：
- en: 'Let’s begin by setting up a configuration parameter describing the cluster
    where we would like to train the models. The following code block is commented
    out so that you can edit and uncomment based on your cluster setup or leave it
    commented if you want to simply try it out on a single machine setup:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先设置一个描述集群配置参数的配置项，指定我们希望在哪里训练模型。以下代码块已被注释掉，您可以根据集群设置编辑并取消注释，或者如果仅想在单机配置上尝试，可以保持注释状态：
- en: '[PRE21]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To leverage multi-machine setups, we will use TensorFlow 2.x’s `MultiWorkerMirroredStrategy`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了利用多台机器的配置，我们将使用 TensorFlow 2.x 的 `MultiWorkerMirroredStrategy`：
- en: '[PRE22]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, let’s declare the basic hyperparameters for the training. Feel free to
    adjust the batch sizes and the `NUM_GPUS` values as per your cluster/computer
    configuration:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们声明训练的基本超参数。根据您的集群/计算机配置，随时调整批处理大小和 `NUM_GPUS` 值：
- en: '[PRE23]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To prepare the dataset, let’s implement two quick functions for normalizing
    and augmenting the input images:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了准备数据集，让我们实现两个快速的函数，用于规范化和增强输入图像：
- en: '[PRE24]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For the sake of simplicity and faster convergence, we will stick with the CIFAR10
    dataset as per the official TensorFlow 2.x sample for training, but feel free
    to choose a different dataset when you explore. Once you choose the dataset, we
    can generate the training and the testing sets:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了简化操作并加快收敛速度，我们将继续使用 CIFAR10 数据集，这是官方 TensorFlow 2.x 示例中用于训练的，但在您探索时可以自由选择其他数据集。一旦选择了数据集，我们就可以生成训练集和测试集：
- en: '[PRE25]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To make the training results reproducible, we will use a fixed random seed
    to shuffle the dataset:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使训练结果可重现，我们将使用固定的随机种子来打乱数据集：
- en: '[PRE26]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We are not ready to generate the training and validation/testing dataset. We
    will shuffle the dataset using the known and fixed random seed declared in the
    previous step and apply the augmentation to the training set:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还没有准备好生成训练和验证/测试数据集。我们将使用前一步中声明的已知固定随机种子来打乱数据集，并对训练集应用数据增强：
- en: '[PRE27]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Similarly, we will prepare the test dataset but we do not want to apply random
    cropping to the test images! So, we will skip the augmentation and use the normalization
    step for preprocessing:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们将准备测试数据集，但我们不希望对测试图像进行随机裁剪！因此，我们将跳过数据增强，并使用标准化步骤进行预处理：
- en: '[PRE28]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Before we can start training, we need to create an instance of an optimizer
    and also prepare the input layer. Feel free to use a different optimizer, such
    as Adam, as per the needs of your task:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始训练之前，我们需要创建一个优化器实例，并准备好输入层。根据任务的需要，您可以使用不同的优化器，例如Adam：
- en: '[PRE29]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we are ready to construct the model instance within the scope of the
    `MultiMachineMirroredStrategy`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们准备在 `MultiMachineMirroredStrategy` 的作用域内构建模型实例：
- en: '[PRE30]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To train the model, we use the simple but powerful Keras API:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了训练模型，我们使用简单而强大的 Keras API：
- en: '[PRE31]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once the model is trained, we easily save, load, and evaluate:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以轻松地保存、加载和评估：
- en: 12.1 Save
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.1 保存
- en: '[PRE32]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That completes our recipe implementation! Let’s summarize what we implemented
    and how it works in the next section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的教程实现！让我们在下一部分总结我们实现了什么以及它是如何工作的。
- en: How it works...
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'For any distributed training runs with TensorFlow 2.x, the `TF_CONFIG` environment
    variable needs to be set on each of the (virtual) machines on your cluster. These
    configuration values inform each of the machines about the role and the training
    information each of the nodes will need to perform its job. You can read more
    about the details of **TF_CONFIG** configurations used by TensorFlow 2.x’s distributed
    training here: [https://cloud.google.com/ai-platform/training/docs/distributed-training-details](https://cloud.google.com/ai-platform/training/docs/distributed-training-details).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 TensorFlow 2.x 的任何分布式训练，需要在集群中每一台（虚拟）机器上设置 `TF_CONFIG` 环境变量。这些配置值将告知每台机器关于角色和每个节点执行任务所需的训练信息。您可以在这里阅读更多关于
    TensorFlow 2.x 分布式训练中使用的**TF_CONFIG**配置的详细信息：[https://cloud.google.com/ai-platform/training/docs/distributed-training-details](https://cloud.google.com/ai-platform/training/docs/distributed-training-details)。
- en: We used TensorFlow 2.x’s `MultiWorkerMirroredStrategy`, which is a strategy
    similar to the Mirrored Strategy we used in the previous recipe in this chapter.
    This strategy is useful for synchronous training across machines with each machine
    potentially having one or more GPUs. All the variables and computations required
    for training the model are replicated on each of the worker nodes as in the Mirrored
    Strategy and, additionally, a distributed collection routine (such as all-reduce)
    is used to collate results from multiple distributed nodes. The remaining workflow
    for training, saving the model, loading the model, and evaluating the model remains
    the same as in our previous recipe.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 TensorFlow 2.x 的 `MultiWorkerMirroredStrategy`，这是一种与本章前面教程中使用的 Mirrored
    Strategy 类似的策略。这种策略适用于跨机器的同步训练，每台机器可能拥有一个或多个 GPU。所有训练模型所需的变量和计算都会在每个工作节点上进行复制，就像
    Mirrored Strategy 一样，并且使用分布式收集例程（如 all-reduce）来汇总来自多个分布式节点的结果。训练、保存模型、加载模型和评估模型的其余工作流程与我们之前的教程相同。
- en: Ready for the next recipe? Let’s do it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好下一个教程了吗？让我们开始吧。
- en: Training Deep RL agents at scale – Multi-GPU PPO agent
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模训练深度强化学习代理 – 多 GPU PPO 代理
- en: RL agents in general require a large number of samples and gradient steps to
    be trained depending on the complexity of the state, action, and the problem space.
    With Deep RL, the computational complexity also increases drastically as the deep
    neural network used by the agent (for Q/value-function representation, for policy
    representation, or for both) has a lot more operations and parameters that need
    to be executed and updated, respectively. To speed up the training process, we
    need the capability to scale our Deep RL agent training to leverage the available
    compute resources, such as GPUs. This recipe will help you leverage multiple GPUs
    to train a PPO agent with a deep convolutional neural network policy in a distributed
    fashion in one of the procedurally generated RL environments using **OpenAI’s
    procgen** library.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，RL代理需要大量的样本和梯度步骤来进行训练，这取决于状态、动作和问题空间的复杂性。随着深度强化学习（Deep RL）的发展，计算复杂度也会急剧增加，因为代理使用的深度神经网络（无论是用于Q值函数表示，策略表示，还是两者都有）有更多的操作和参数需要分别执行和更新。为了加速训练过程，我们需要能够扩展我们的深度RL代理训练，以利用可用的计算资源，如GPU。这个食谱将帮助你利用多个GPU，以分布式的方式训练一个使用深度卷积神经网络策略的PPO代理，在使用**OpenAI的procgen**库的程序生成的RL环境中进行训练。
- en: Let’s get started!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. Although not required, it is recommended to use a machine with two
    or more GPUs to execute this recipe.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，首先你需要激活`tf2rl-cookbook` Python/conda虚拟环境。确保更新环境，以匹配食谱代码库中的最新conda环境规格文件(`tfrl-cookbook.yml`)。虽然不是必需的，但建议使用具有两个或更多GPU的机器来执行此食谱。
- en: Now, let’s begin!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will implement a complete recipe to allow configurable training of a PPO
    agent with a deep convolutional neural network policy in a distributed fashion.
    Let’s start implementing it step by step:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个完整的食谱，允许以分布式方式配置训练PPO代理，并使用深度卷积神经网络策略。让我们一步一步地开始实现：
- en: 'We will begin by importing the necessary modules for our recipe:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入实现这一食谱所需的模块开始：
- en: '[PRE33]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will be using the `procgen` environments from OpenAI. Let’s import that
    as well:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用OpenAI的`procgen`环境。让我们也导入它：
- en: '[PRE34]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In order to make this recipe easy to configure and run, let’s add support for
    command-line arguments with useful configuration flags:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使这个食谱更易于配置和运行，让我们添加对命令行参数的支持，并配置一些有用的配置标志：
- en: '[PRE35]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s use a TensorBoard summary writer for logging:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用TensorBoard摘要写入器进行日志记录：
- en: '[PRE36]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will first implement the `Actor` class in the following several steps, starting
    with the `__init__` method. You will notice that we need to instantiate the models
    within the context of the execution strategy:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先在以下几个步骤中实现`Actor`类，从`__init__`方法开始。你会注意到我们需要在执行策略的上下文中实例化模型：
- en: '[PRE37]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For the Actor’s policy network model, we will implement a deep convolutional
    neural network comprising of multiple `Conv2D` and `MaxPool2D` layers. We will
    start the implementation in this step and finish in the following few steps:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于Actor的策略网络模型，我们将实现一个包含多个`Conv2D`和`MaxPool2D`层的深度卷积神经网络。在这一步我们将开始实现，接下来的几步将完成它：
- en: '[PRE38]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We will add more Conv2D – Pool2D layers to stack up the processing layers depending
    on the needs for the task. In this recipe, we will be training policies for the
    procgen environment, which is somewhat visually rich, so we will stack a few more
    layers:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加更多的Conv2D - Pool2D层，以根据任务的需求堆叠处理层。在这个食谱中，我们将为procgen环境训练策略，该环境在视觉上较为丰富，因此我们将堆叠更多的层：
- en: '[PRE39]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we can use a flattening layer and prepare the output heads for the policy
    network:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用一个扁平化层，并为策略网络准备输出头：
- en: '[PRE40]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As the final step for building the neural model for the policy network, we
    will create the output layer and return a Keras model:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为构建策略网络神经模型的最后一步，我们将创建输出层并返回一个Keras模型：
- en: '[PRE41]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'With the model we have defined in the previous steps, we can start processing
    a state/observation image input and produce the logits (unnormalized probabilities)
    and the action that the Actor would take. Let’s implement a method to do that:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在前面步骤中定义的模型，我们可以开始处理状态/观察图像输入，并生成logits（未归一化的概率）以及Actor将采取的动作。让我们实现一个方法来完成这个任务：
- en: '[PRE42]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, to compute the surrogate loss to drive the learning, we will implement
    the `compute_loss` method:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了计算驱动学习的替代损失，我们将实现`compute_loss`方法：
- en: '[PRE43]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next up is a core method that ties all the methods together to perform the
    training. Note that this is the train method per replica, and we will use it in
    our distributed training method, which will follow in the next steps:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一个核心方法，它将所有方法连接在一起以执行训练。请注意，这是每个副本的训练方法，我们将在后续的分布式训练方法中使用它：
- en: '[PRE44]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To implement the distributed training method, we will make use of the `tf.function`
    decorator to implement a TensorFlow 2.x function:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现分布式训练方法，我们将使用`tf.function`装饰器来实现一个TensorFlow 2.x函数：
- en: '[PRE45]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'That completes our `Actor` class implementation, and we will now start our
    implementation of the `Critic` class:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就完成了我们的`Actor`类实现，接下来我们将开始实现`Critic`类：
- en: '[PRE46]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You must have noticed that we are creating the Critic’s value-function model
    instance within the scope of the execution strategy to support distributed training.
    We will now start implementing the Critic’s neural network model in the following
    few steps:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你一定注意到，我们在执行策略的作用域内创建了Critic的价值函数模型实例，以支持分布式训练。接下来，我们将开始在以下几个步骤中实现Critic的神经网络模型：
- en: '[PRE47]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Like our Actor’s model, we will have similar layering of Conv2D-MaxPool2D layers
    followed by flattening layers with dropout:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们的Actor模型类似，我们将有类似的Conv2D-MaxPool2D层的堆叠，后面跟着带有丢弃的扁平化层：
- en: '[PRE48]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We will add the value output head and return the model as a Keras model to
    complete our Critic’s neural network model:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加值输出头，并将模型作为Keras模型返回，以完成我们Critic的神经网络模型：
- en: '[PRE49]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As you may recall, the Critic’s loss is the mean squared error between the
    predicted temporal-difference target and the actual temporal-difference targets.
    Let’s implement a method to compute the loss:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所记得，Critic的损失是预测的时间差目标与实际时间差目标之间的均方误差。让我们实现一个计算损失的方法：
- en: '[PRE50]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Similar to our Actor implementation, we will implement a per-replica `train`
    method and then use it in a later step for distributed training:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们的Actor实现类似，我们将实现一个每个副本的`train`方法，然后在后续步骤中用于分布式训练：
- en: '[PRE51]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will now finalize our `Critic` class implementation by implementing the
    `train_distributed` method that enables distributed training:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过实现`train_distributed`方法来完成`Critic`类的实现，该方法支持分布式训练：
- en: '[PRE52]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'With our `Actor` and `Critic` classes implemented, we can start our distributed
    `PPOAgent` implementation. We will implement the `PPOAgent` class in the following
    several steps. Let’s begin with the `__init__` method:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实现了我们的`Actor`和`Critic`类后，我们可以开始我们的分布式`PPOAgent`实现。我们将分几个步骤实现`PPOAgent`类。让我们从`__init__`方法开始：
- en: '[PRE53]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we will implement a method to calculate the target for the **generalized
    advantage estimate** (**GAE**):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个方法来计算**广义优势估计**（**GAE**）的目标：
- en: '[PRE54]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We are all set to start our main `train(…)` method. We will split the implementation
    of this method into the following few steps. Let’s set up the scope, start the
    outer loop, and initialize varaibles:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好开始我们的`train(…)`方法。我们将把这个方法的实现分为以下几个步骤。让我们设置作用域，开始外循环，并初始化变量：
- en: '[PRE55]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we can start the loop that needs to be executed for each episode until
    the episode is done:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始为每个回合执行的循环，直到回合结束：
- en: '[PRE56]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Within each episode, if we have reached `update_freq` or just reached an end
    state, we need to compute the GAEs and TD targets. Let’s add the code for that:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个回合内，如果我们达到了`update_freq`或者刚刚到达了结束状态，我们需要计算GAE和TD目标。让我们添加相应的代码：
- en: '[PRE57]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Within the same execution context, we need to train the `Actor` and the `Critic`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同的执行上下文中，我们需要训练`Actor`和`Critic`：
- en: '[PRE58]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, we need to reset the tracking variables and update our episode reward
    values:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要重置跟踪变量并更新我们的回合奖励值：
- en: '[PRE59]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'With that, our distributed `main` method to finalize our recipe:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们的分布式`main`方法就完成了，来完成我们的配方：
- en: '[PRE60]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: That’s it for the recipe! Hope you enjoyed cooking it up. You can execute the
    recipe and watch the progress using the TensorBoard logs to see the training speedup
    you get with a greater number of GPUs!
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配方完成了！希望你喜欢这个过程。你可以执行这个配方，并通过TensorBoard日志观看进度，以查看你在更多GPU的支持下获得的训练加速效果！
- en: Let’s recap what we accomplished and how the recipe works in the next section.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们完成的工作以及配方如何工作的下一部分。
- en: How it works...
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We implemented `Actor` and `Critic` classes where the Actor used a deep convolutional
    neural network for the policy representation and the Critic utilized a similar
    deep convolutional neural network for its value function representation. Both
    these models were instantiated under the scope of the distributed execution strategy
    using the `self.execution_strategy.scope()` construct.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了`Actor`和`Critic`类，其中Actor使用深度卷积神经网络表示策略，而Critic则使用类似的深度卷积神经网络表示其价值函数。这两个模型都在分布式执行策略的范围内实例化，使用了`self.execution_strategy.scope()`构造方法。
- en: The procgen environments, such as coinrun, fruitbot, jumper, leaper, maze, and
    others, are visually (relatively) rich environments and therefore require convolutional
    layers that are relatively deep to process the visual observations. We therefore
    used a deep CNN model for the policy network of the Actor. For distributed training
    using multiple replicas on multiple GPUs, we first implemented a single-replica
    training method (train) and then used `Tensorflow.function` to run across replicas
    and reduce the results to arrive at the total loss.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: procgen环境（如coinrun、fruitbot、jumper、leaper、maze等）是视觉上（相对）丰富的环境，因此需要较深的卷积层来处理视觉观察。因此，我们为Actor的策略网络使用了深度CNN模型。为了在多个GPU上使用多个副本进行分布式训练，我们首先实现了单副本训练方法（train），然后使用`Tensorflow.function`在副本间运行，并将结果进行汇总得到总损失。
- en: 'Finally, while training our PPO agent in the distributed setting, we performed
    all the training operations within the scope of the distributed execution strategy
    by using Python’s `with` statement for context management like this: `with self.distributed_execution_strategy.scope()`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在分布式环境中训练我们的PPO智能体时，我们通过使用Python的`with`语句进行上下文管理，将所有训练操作都纳入分布式执行策略的范围，例如：`with
    self.distributed_execution_strategy.scope()`。
- en: It’s time to move on to the next recipe!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该是进行下一个配方的时候了！
- en: Building blocks for distributed Deep Reinforcement Learning for accelerated
    training
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于加速训练的分布式深度强化学习基础模块
- en: The previous recipes in this chapter discussed how you could scale your Deep
    RL training using TensorFlow 2.x’s distributed execution APIs. While it was straightforward
    after understanding the concepts and the implementation style, training Deep RL
    agents with more advanced architectures such as Impala and R2D2 requires RL building
    blocks such as distributed parameter servers and distributed experience replay.
    This chapter will walk through the implementation of such building blocks for
    distributed RL training. We will be using the Ray distributed computing framework
    to implement our building blocks.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章之前的配方讨论了如何使用TensorFlow 2.x的分布式执行API来扩展深度强化学习训练。理解了这些概念和实现风格后，训练使用更高级架构（如Impala和R2D2）的深度强化学习智能体，需要像分布式参数服务器和分布式经验回放这样的RL基础模块。本章将演示如何为分布式RL训练实现这些基础模块。我们将使用Ray分布式计算框架来实现我们的基础模块。
- en: Let’s get started!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. To test the building blocks we build in this recipe, we will be using
    the `sac_agent_base` module based on our SAC agent implemented in one of the book’s
    earlier recipes. If the following `import` statements run without issues, you
    are ready to start:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个配方，首先需要激活`tf2rl-cookbook`的Python/conda虚拟环境。确保更新环境以匹配食谱代码仓库中的最新conda环境规范文件（`tfrl-cookbook.yml`）。为了测试我们在这个配方中构建的基础模块，我们将使用基于书中早期配方实现的SAC智能体的`self.sac_agent_base`模块。如果以下`import`语句能正常运行，那么你准备开始了：
- en: '[PRE61]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now, let’s begin!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We will implement the building blocks one by one, starting with the distributed
    parameter server:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个实现这些基础模块，从分布式参数服务器开始：
- en: 'The `ParameterServer` class is a simple store for sharing the neural network
    parameters or weights between workers in a distributed training setting. We will
    implement the class as a Ray’s remote Actor:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ParameterServer`类是一个简单的存储类，用于在分布式训练环境中共享神经网络的参数或权重。我们将实现这个类作为Ray的远程Actor：'
- en: '[PRE62]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let’s also add a method to save the weights to the disk:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将添加一个方法将权重保存到磁盘：
- en: '[PRE63]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As the next building block, we will implement the `ReplayBuffer`, which can
    be used by a distributed set of agents. We will start the implementation in this
    step and continue in the next several steps:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为下一个构建块，我们将实现`ReplayBuffer`，它可以被分布式代理集群使用。我们将在这一步开始实现，并在接下来的几步中继续：
- en: '[PRE64]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we will implement a method to store new experiences in the replay buffer:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个方法，将新经验存储到重放缓冲区：
- en: '[PRE65]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To sample a batch of experience data from the replay buffer, we will implement
    a method that randomly samples from the replay buffer and returns a dictionary
    containing the sampled experience data:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了从重放缓冲区采样一批经验数据，我们将实现一个方法，从重放缓冲区随机采样并返回一个包含采样经验数据的字典：
- en: '[PRE66]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'That completes our `ReplayBuffer` class implementation. We will now start implementing
    a method to roll out, which essentially collects experiences in an RL environment
    using an exploration policy with parameters pulled from the distributed parameter
    server object and stores the collected experience in the distributed replay buffer.
    We will start our implementation in this step and complete the `rollout` method
    implementation in the following steps:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们的`ReplayBuffer`类的实现。现在我们将开始实现一个方法来进行`rollout`，该方法本质上是使用从分布式参数服务器对象中提取的参数和探索策略在RL环境中收集经验，并将收集到的经验存储到分布式重放缓冲区中。我们将在这一步开始实现，并在接下来的步骤中完成`rollout`方法的实现：
- en: '[PRE67]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'With the agent intialized and loaded and the environment instance(s) ready,
    we can start our experience-gathering loop:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理初始化并加载完毕，环境实例也准备好后，我们可以开始我们的经验收集循环：
- en: '[PRE68]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let’s handle the case when a `max_ep_len` is configured to indicate the maximum
    length of the episode and then store the collected experience in the distributed
    replay buffer:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们处理`max_ep_len`配置的情况，以指示回合的最大长度，然后将收集的经验存储到分布式重放缓冲区中：
- en: '[PRE69]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Finally, at the end of the episode, sync the weights of the behavior policy
    using the parameter server:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在回合结束时，使用参数服务器同步行为策略的权重：
- en: '[PRE70]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'That completes the implementation of the `rollout` method and we can now implement
    a `train` method that runs the train loop:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了`rollout`方法的实现，我们现在可以实现一个运行训练循环的`train`方法：
- en: '[PRE71]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The final module in our recipe is the `main` function, which puts together
    all the building blocks we have built so far in this recipe and exercises them.
    We will begin the implementation in this step and finish it in the remaining steps.
    Let’s start with the `main` function argument list and capture the arguments in
    a config dictionary:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的配方中的最后一个模块是`main`函数，它将迄今为止构建的所有模块整合起来并执行。我们将在这一步开始实现，并在剩下的步骤中完成。让我们从`main`函数的参数列表开始，并将参数捕获到配置字典中：
- en: '[PRE72]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Next, let’s create an instance of the desired environment, obtain the state
    and observation space, initialize ray, and also initialize a Stochastic Actor-Critic
    agent. Note that we are initializing a single-node ray cluster but feel free to
    initialize ray with a cluster of nodes (local or in the cloud):'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个所需环境的实例，获取状态和观察空间，初始化ray，并初始化一个随机策略-演员-评论家（Stochastic Actor-Critic）代理。注意，我们初始化的是一个单节点的ray集群，但你也可以使用节点集群（本地或云端）来初始化ray：
- en: '[PRE73]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'In this step, we will initialize an instance of the `ParameterServer` class
    and an instance of the `ReplayBuffer` class:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将初始化`ParameterServer`类的实例和`ReplayBuffer`类的实例：
- en: '[PRE74]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We are now ready to exercise the building blocks we have built. We will first
    launch a series of rollout tasks based on the number of workers specified as a
    configuration argument that will launch the rollout process on the distributed
    ray cluster:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好运行已构建的模块了。我们将首先根据配置参数中指定的工作者数量，启动一系列`rollout`任务，这些任务将在分布式ray集群上启动`rollout`过程：
- en: '[PRE75]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The rollout task will launch the remote tasks that will populate the replay
    buffer with the gathered experience. The above line will return immediately even
    though the rollout tasks will take time to complete because of the asynchronous
    function call.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rollout`任务将启动远程任务，这些任务将使用收集到的经验填充重放缓冲区。上述代码将立即返回，即使`rollout`任务需要时间来完成，因为它是异步函数调用。'
- en: 'Next, we will launch a configurable number of learners that run the distributed
    training task on the ray cluster:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将启动一个可配置数量的学习者，在ray集群上运行分布式训练任务：
- en: '[PRE76]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The above statement will launch the remote training process and return immediately
    even though the `train` function on the learners will take time to complete.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述语句将启动远程训练过程，并立即返回，尽管`train`函数在学习者上需要一定时间来完成。
- en: '[PRE77]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Finally, let’s define our entry point. We will use the Python Fire library
    to expose our `main` function, and its arguments to look like an executable supporting
    command-line argument:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们定义我们的入口点。我们将使用Python Fire库来暴露我们的`main`函数，并使其参数看起来像是一个支持命令行参数的可执行文件：
- en: '[PRE78]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'With the preceding entry point, the script can be configured and launched from
    the command line. An example is provided here for your reference:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用前述的入口点，脚本可以从命令行配置并启动。这里提供一个示例供你参考：
- en: '[PRE79]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: That completes our implementation! Let’s briefly discuss how it works in the
    next section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们的实现！让我们在下一节简要讨论它的工作原理。
- en: How it works...
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We built a distributed `ParameterServer`, `ReplayBuffer`, rollout worker, and
    learner processes. These building blocks are crucial for training distributed
    RL agents. We utilized Ray as the framework for distributed computing.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个分布式的`ParameterServer`、`ReplayBuffer`、rollout worker和learner进程。这些构建模块对于训练分布式RL代理至关重要。我们使用Ray作为分布式计算框架。
- en: After implementing the building blocks and the tasks, in the `main` function,
    we launched the two asynchronous, distributed tasks on the ray cluster. The `task_rollout`
    launched a (configurable) number of rollout workers and the `task_train` launched
    a (configurable) number of learners. Both the tasks run on the ray cluster asynchronously
    in a distributed manner. The rollout workers pull the latest weights from the
    parameter server and gather and store experiences in the replay memory buffer
    while, simultaneously, the learners train using batches of experiences sampled
    from the replay memory and push the updated (and potentially improved) set of
    parameters to the parameter server.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了构建模块和任务后，在`main`函数中，我们在Ray集群上启动了两个异步的分布式任务。`task_rollout`启动了（可配置数量的）rollout
    worker，而`task_train`启动了（可配置数量的）learner。两个任务都以分布式方式异步运行在Ray集群上。rollout workers从参数服务器拉取最新的权重，并将经验收集并存储到重放内存缓冲区中，同时，learners使用从重放内存中采样的经验批次进行训练，并将更新（且可能改进的）参数集推送到参数服务器。
- en: It’s time to move on to the next and final recipe of this chapter!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进入本章的下一个，也是最后一个教程了！
- en: Large-scale Deep RL agent training using Ray, Tune, and RLLib
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ray、Tune和RLLib进行大规模深度强化学习（Deep RL）代理训练
- en: In the previous recipe, we got a flavor of how to implement distributed RL agent
    training routines from scratch. Since most of the components used as building
    blocks have become a standard way of building Deep RL training infrastructure,
    we can leverage an existing library that maintains a high-quality implementation
    of such building blocks. Fortunately, with our choice of ray as the framework
    for distributed computing, we are in a good place. Tune and RLLib are two libraries
    built on top of ray, and are available together with Ray, that provide highly
    scalable hyperparameter tuning (Tune) and RL training (RLLib). This recipe will
    provide a curated set of steps to get you acquainted with ray, Tune, and RLLib
    so that you can utilize them to scale your Deep RL training routines. In addition
    to the recipe discussed here in the text, the cookbook’s code repository for this
    chapter will have a handful of additional recipes for you.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的教程中，我们初步了解了如何从头实现分布式RL代理训练流程。由于大多数用作构建模块的组件已成为构建深度强化学习训练基础设施的标准方式，我们可以利用一个现有的库，该库维护了这些构建模块的高质量实现。幸运的是，选择Ray作为分布式计算框架使我们处于一个有利位置。Tune和RLLib是基于Ray构建的两个库，并与Ray一起提供，提供高度可扩展的超参数调优（Tune）和RL训练（RLLib）。本教程将提供一套精选步骤，帮助你熟悉Ray、Tune和RLLib，从而能够利用它们来扩展你的深度RL训练流程。除了文中讨论的教程外，本章的代码仓库中还有一系列额外的教程供你参考。
- en: Let’s get started!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. Ray, Tune, and RLLib will be installed in your `tfrl-cookbook` conda
    environment when you use the provided conda YAML spec for the environment. If
    you want to install Tune and RLLib in a different environment, the easiest way
    is to install it using the following command:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个教程，你首先需要激活`tf2rl-cookbook`的Python/conda虚拟环境。确保更新环境以匹配最新的conda环境规范文件（`tfrl-cookbook.yml`），该文件位于教程代码仓库中。当你使用提供的conda
    YAML规范来设置环境时，Ray、Tune和RLLib将会被安装在你的`tf2rl-cookbook` conda环境中。如果你希望在其他环境中安装Tune和RLLib，最简单的方法是使用以下命令安装：
- en: '[PRE80]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Now, let’s begin!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始吧！
- en: How to do it...
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'We will start with quick and basic commands and recipes to launch training
    on ray clusters using Tune and RLLib and progressively customize the training
    pipeline to provide you with a useful recipe:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从快速和基本的命令与食谱开始，使用Tune和RLLib在ray集群上启动训练，并逐步自定义训练流水线，以为你提供有用的食谱：
- en: 'Launching typical training of RL agents in OpenAI Gym environments is as easy
    as specifying the algorithm name and the environment name. For example, to train
    a PPO agent in the CartPole-v4 Gym environment, all you need to execute is the
    following command:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在OpenAI Gym环境中启动RL代理的典型训练和指定算法名称和环境名称一样简单。例如，要在CartPole-v4 Gym环境中训练PPO代理，你只需要执行以下命令：
- en: '[PRE81]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let’s try to train a PPO agent in the `coinrun` `procgen` environment, like
    in one of our previous recipes:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试在`coinrun`的`procgen`环境中训练一个PPO代理，就像我们之前的一个食谱一样：
- en: '[PRE82]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'You will notice that the preceding command fails with the following (shortened)
    error:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你会注意到，前面的命令会失败，并给出以下（简化的）错误：
- en: '[PRE83]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: This is because, as the error states, RLLib by default supports observations
    of shapes (42, 42, k) or (84, 84, k). Observations of other shapes will need a
    custom model or a preprocessor. In the next few steps, we will see how we can
    implement a custom neural network model implemented using the TensorFlow 2.x Keras
    API, which can be used with ray RLLib.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是因为，如错误所示，RLLib默认支持形状为（42，42，k）或（84，84，k）的观察值。其他形状的观察值将需要自定义模型或预处理器。在接下来的几个步骤中，我们将展示如何实现一个自定义神经网络模型，使用TensorFlow
    2.x Keras API实现，并且可以与ray RLLib一起使用。
- en: 'We will start our custom model implementation (`custom_model.py`) in this step
    and complete it in the following few steps. In this step, let’s import the necessary
    modules and also implement a helper method to return a Conv2D layer with a certain
    filter depth:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在这一步开始实现自定义模型（`custom_model.py`），并在接下来的几步中完成它。在这一步，让我们导入必要的模块，并实现一个辅助方法，以返回具有特定滤波深度的Conv2D层：
- en: '[PRE84]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Next, let’s implement a helper method to build and return a simple residual
    block:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个辅助方法来构建并返回一个简单的残差块：
- en: '[PRE85]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let’s implement another handy function to construct multiple residual block
    sequences:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现另一个方便的函数来构建多个残差块序列：
- en: '[PRE86]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We can now start implementing the `CustomModel` class as a subclass of the
    TFModelV2 base class provided by RLLib to make it easy to integrate with RLLib:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始实现`CustomModel`类，作为RLLib提供的TFModelV2基类的子类，以便轻松地与RLLib集成：
- en: '[PRE87]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'After the `__init__` method, we need to implement the `forward` method as it
    is not implemented by the base class (`TFModelV2`) but is necessary:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`__init__`方法之后，我们需要实现`forward`方法，因为它没有被基类（`TFModelV2`）实现，但却是必需的：
- en: '[PRE88]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We will also implement a one-line method to reshape the value function output:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将实现一个单行方法来重新调整值函数的输出：
- en: '[PRE89]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: With that, our `CustomModel` implementation is complete and is ready to use!
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样，我们的`CustomModel`实现就完成了，并且可以开始使用了！
- en: 'We will implement a solution (`5.1_training_using_tune_run.py`) using ray,
    Tune, and RLLib’s Python API so that you also utilize the model in addition to
    their command-line usage. Let’s split the implementation into two steps. In this
    step, we will import the necessary modules and initialize ray:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将实现一个使用ray、Tune和RLLib的Python API的解决方案（`5.1_training_using_tune_run.py`），这样你就可以在使用它们的命令行工具的同时，也能利用该模型。让我们将实现分为两步。在这一步，我们将导入必要的模块并初始化ray：
- en: '[PRE90]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'In this step, we will register our custom model in RLLib’s `ModelCatlog` and
    then use it to train a PPO agent with a custom set of parameters including the
    `framework` parameter that forces RLLib to use TensorFlow 2\. We will also shut
    down ray at the end of the script:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将把我们的自定义模型注册到RLLib的`ModelCatlog`中，然后使用它来训练一个带有自定义参数集的PPO代理，其中包括强制RLLib使用TensorFlow
    2的`framework`参数。我们还将在脚本结束时关闭ray：
- en: '[PRE91]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We will look at another quick recipe (`5_2_custom_training_using_tune.py`)
    to customize the training loop. We will split the implementation into the following
    few steps to keep it simple. In this step, we will import the necessary libraries
    and initialize ray:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将查看另一个快速食谱（`5_2_custom_training_using_tune.py`）来定制训练循环。我们将把实现分为以下几个步骤，以保持简洁。在这一步，我们将导入必要的库并初始化ray：
- en: '[PRE92]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Now, let’s register our custom model with RLLib’s `ModelCatalog` and configure
    the **IMPALA agent**. We could very well use any other RLLib support agents, such
    as PPO or SAC:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将自定义模型注册到RLLib的`ModelCatalog`中，并配置**IMPALA代理**。我们当然可以使用任何其他的RLLib支持的代理，如PPO或SAC：
- en: '[PRE93]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We can now implement our custom training loop and include any steps in the
    loop as we desire. We will keep the example loop simple by simply performing a
    training step and saving the agent’s model every n(100) epochs:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以实现自定义训练循环，并根据需要在循环中加入任何步骤。我们将通过每隔 n(100) 代（epochs）执行一次训练步骤并保存代理的模型来保持示例循环的简单性：
- en: '[PRE94]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Note that we could continue to train the agent using the saved checkpoint and
    using the simpler ray tune’s run API as shown here as an example:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们可以继续使用保存的检查点和 Ray tune 的简化 run API 来训练代理，如此处示例所示：
- en: '[PRE95]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Finally, let’s shut down ray to free up system resources:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们关闭 Ray 以释放系统资源：
- en: '[PRE96]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: That completes this recipe! In the next section, let’s recap what we discussed
    in this recipe.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了本次配方！在下一节中，让我们回顾一下我们在本节中讨论的内容。
- en: How it works...
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We identified one of the common limitations with the simple but limited command-line
    interface of ray RLLib. We also discussed a solution to overcome the failure in
    step 2 where a custom model was needed to use RLLib’s PPO agent training and implemented
    it in steps 9 and 10.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现了 Ray RLLib 简单但有限的命令行界面中的一个常见限制。我们还讨论了解决方案，以克服第 2 步中的失败情况，在此过程中需要自定义模型来使用
    RLLib 的 PPO 代理训练，并在第 9 步和第 10 步中实现了该方案。
- en: While the solution discussed in steps 9 and 10 looks elegant, it may not provide
    all the customization knobs you are looking for or are familiar with. For example,
    it abstracts away the basic RL loop that steps through the environment. We implemented
    another quick recipe starting from step 11 that allows the customization of the
    training loop. In step 12, we saw how we can register our custom model and use
    it with the IMPALA agent – which is a scalable, distributed Deep RL agent based
    on IMPortance-weighted Actor-Learner Architecture. IMPALA agent actors communicate
    sequences of states, actions, and rewards to a centralized learner where batch
    gradient updates take place, in contrast to the (asynchronous) Actor-Critic-based
    agents where the gradients are communicated to a centralized parameter server.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第 9 步和第 10 步中讨论的解决方案看起来很优雅，但它可能无法提供您所需的所有自定义选项或您熟悉的选项。例如，它将基本的 RL 循环抽象了出来，这个循环会遍历环境。我们从第
    11 步开始实现了另一种快速方案，允许自定义训练循环。在第 12 步中，我们看到如何注册自定义模型并将其与 IMPALA 代理一起使用——IMPALA 代理是基于
    IMPortance 加权 Actor-Learner 架构的可扩展分布式深度强化学习代理。IMPALA 代理的演员通过传递状态、动作和奖励的序列与集中式学习器通信，在学习器中进行批量梯度更新，而与之对比的是基于（异步）Actor-Critic
    的代理，其中梯度被传递到一个集中式参数服务器。
- en: For more information on Tune, you can refer to the Tune user guide and configuration
    documentation at [https://docs.ray.io/en/master/tune/user-guide.html](https://docs.ray.io/en/master/tune/user-guide.html).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多关于 Tune 的信息，可以参考 [https://docs.ray.io/en/master/tune/user-guide.html](https://docs.ray.io/en/master/tune/user-guide.html)
    上的 Tune 用户指南和配置文档。
- en: For more information on RLLib training APIs and configuration documentation,
    you can refer to [https://docs.ray.io/en/master/rllib-training.html](https://docs.ray.io/en/master/rllib-training.html).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多关于 RLLib 训练 API 和配置文档的信息，可以参考 [https://docs.ray.io/en/master/rllib-training.html](https://docs.ray.io/en/master/rllib-training.html)。
- en: That completes the recipe and the chapter! Hope you feel empowered with the
    new skills and knowledge you have gained to speed up your Deep RL agent training.
    See you in the next chapter!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和配方已完成！希望您通过所获得的新技能和知识，能够加速您的深度 RL 代理训练。下章见！
