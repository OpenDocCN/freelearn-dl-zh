- en: Monte Carlo Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法
- en: For this chapter, we will jump back to the trial-and-error thread of **reinforcement
    learning** (**RL**) and look at Monte Carlo methods. This is a class of methods
    that works by episodically playing through an environment instead of planning.
    We will see how this improves our RL search for the best policy and we now start
    to think of our algorithm as an actual agent—one that explores the game environment
    rather than preplans a policy, which, in turn, allows us to understand the benefits
    of using a model for planning or not. From there, we will look at the Monte Carlo
    method and how to implement it in code. Then, we will revisit a larger version
    of the FrozenLake environment with our new Monte Carlo agent algorithm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将回到强化学习（**RL**）的试错线程，并探讨蒙特卡洛方法。这是一类通过周期性地在一个环境中进行游戏而不是进行规划来工作的方法。我们将看到这种方法如何改进我们的最佳策略搜索，现在我们开始将我们的算法视为一个实际的代理——一个探索游戏环境而不是预先规划策略的代理，这反过来又使我们能够理解使用模型进行规划或不使用模型的益处。从那里，我们将探讨蒙特卡洛方法及其在代码中的实现。然后，我们将使用我们新的蒙特卡洛代理算法重新审视一个更大的FrozenLake环境版本。
- en: 'In this chapter, we will continue looking at how RL has evolved and, in particular,
    focus on the trial and error thread with the Monte Carlo method. Here is a summary
    of the main topics we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探讨强化学习（RL）的演变过程，特别是关注蒙特卡洛方法中的试错线程。以下是本章我们将涵盖的主要主题总结：
- en: Understanding model-based and model-free learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于模型和无模型学习
- en: Introducing the Monte Carlo method
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍蒙特卡洛方法
- en: Adding RL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加强化学习
- en: Playing the FrozenLake game
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩FrozenLake游戏
- en: Using prediction and control
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预测和控制
- en: We again explore more foundations of RL, variational inference, and the trial
    and error method. This knowledge will be essential for anyone who is serious about
    finishing this book, so please don't skip this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次探索强化学习、变分推理和试错方法的基础。这些知识对于任何认真完成这本书的人来说都是至关重要的，所以请不要跳过本章。
- en: Understanding model-based and model-free learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于模型和无模型学习
- en: 'If you recall from our very first chapter, [Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml),
    *Understanding Rewards-Based Learning*, we explored the primary elements of RL.
    We learned that RL comprises of a policy, a value function, a reward function,
    and, optionally, a model. We use the word *model* in this context to refer to
    a detailed plan of the environment. Going back to the last chapter again, where
    we used the FrozenLake environment, we had a perfect model of that environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得我们非常第一章节，[第1章](5553d896-c079-4404-a41b-c25293c745bb.xhtml)，“基于奖励的学习理解”，我们探讨了强化学习的主要元素。我们了解到强化学习包括策略、价值函数、奖励函数，以及可选的模型。在这个上下文中，我们使用“模型”一词来指代环境的详细计划。回到上一章，我们使用了FrozenLake环境，我们对该环境有一个完美的模型：
- en: '![](img/2963ac28-ffbe-4c12-b905-03f65c54ff7c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2963ac28-ffbe-4c12-b905-03f65c54ff7c.png)'
- en: Model of the FrozenLake environment
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: FrozenLake环境的模型
- en: Of course, looking at problems with a fully described model in a finite MDP
    is all well and good for learning. However, when it comes to the real world, having
    a full and completely understood model of any environment would likely be highly
    improbable, if not impossible. This is because there are far too many states to
    account for or model in any real-world problem. As it turns out, this can also
    be the case for many other models as well. Later in this book, we will look at
    environments with more states than the number of atoms in the known universe.
    We could never possibly model such environments. Hence, the planning methods we
    learned in [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml), *Dynamic Programming
    and the Bellman Equation*, won't work. Instead, we need a method that can explore
    an environment and learn from it. This is where Monte Carlo comes in and is something
    we will cover in the next section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在有限MDP中用完全描述的模型来观察问题对于学习来说是非常好的。然而，当涉及到现实世界时，拥有任何环境的完整和完全理解模型很可能是不太可能，甚至是不可能的。这是因为任何现实世界问题中都有太多的状态需要考虑或建模。实际上，这也可能适用于许多其他模型。在本书的后面部分，我们将看到比已知宇宙中原子数量还要多的状态的环境。我们永远无法模拟这样的环境。因此，我们在[第2章](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml)，“动态规划和贝尔曼方程”中学到的规划方法将不起作用。相反，我们需要一种可以探索环境并从中学习的方法。这就是蒙特卡洛方法出现的地方，我们将在下一节中介绍。
- en: Introducing the Monte Carlo method
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍蒙特卡洛方法
- en: The Monte Carlo method was so named because of its similarity to gambling or
    chance. Hence, the method was named after the famous gambling destination at the
    time. While the method is extremely powerful, it has been used to describe the
    atom, quantum mechanics, and the quantity of [![](img/28309dc6-92e5-49ba-aea1-e9ad32bfce3c.png)]
    itself. It is only until fairly recently, within the last 20 years, that it has
    seen widespread acceptance in everything from engineering to financial analysis.
    The method itself has now become foundational to many aspects of machine learning
    and is worth further study for anyone in the AI field.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法之所以得名，是因为它与赌博或机会相似。因此，该方法以当时著名的赌博目的地命名。虽然这种方法非常强大，但它已被用来描述原子、量子力学以及[![图片](img/28309dc6-92e5-49ba-aea1-e9ad32bfce3c.png)]本身的数量。直到最近20年，它才在从工程到金融分析等各个领域得到广泛应用。现在，这种方法已成为机器学习许多方面的基础，对于AI领域的任何人来说都值得进一步研究。
- en: In the next section, we will see how the Monte Carlo method can be used to solve
    for [![](img/7b9758bc-0cbb-47ab-8289-66d8157600fb.png)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到蒙特卡洛方法如何用来求解[![图片](img/7b9758bc-0cbb-47ab-8289-66d8157600fb.png)]。
- en: Solving for
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 求解
- en: 'The standard introduction to Monte Carlo methods is to show how it can be used
    to solve for ![](img/cd567828-140e-4cb5-95fd-7a08af97d028.png). Recall from geometry,
    [![](img/a35704d0-4695-4796-bd6a-a8693703de86.png)] represents half the circumference
    of a circle and 2π represents a full circle. To find this relationship and value,
    let''s consider a unit circle with a radius of 1 unit. That unit could be feet,
    meters, parsecs, or whatever—it''s not important. Then, if we place that circle
    within a square box with dimensions of 1 unit, we can see the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法的标准介绍是展示它如何用来求解[![图片](img/cd567828-140e-4cb5-95fd-7a08af97d028.png)]。回想一下几何学，[![图片](img/a35704d0-4695-4796-bd6a-a8693703de86.png)]代表圆周的一半，2π代表整个圆。为了找到这个关系和值，让我们考虑一个半径为1个单位的单位圆。这个单位可以是英尺、米、秒差距，或者任何其他东西——这并不重要。然后，如果我们把这个圆放在一个边长为1个单位的正方形盒子里，我们可以看到以下情况：
- en: '![](img/f97cc8b3-a951-4ddf-85b6-b8bae03b57ad.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f97cc8b3-a951-4ddf-85b6-b8bae03b57ad.png)'
- en: A unit circle inside a unit square
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 单位圆在单位正方形内
- en: 'Given the preceding, we know that we have a square that encompasses dimensions
    of 2 units by 2 units or 100% of the area with an area of 4 square units. Going
    back to geometry again, we know that the area of a circle is given by [![](img/7a492136-0712-4f73-b12b-8c05460618aa.png)].
    Knowing that the circle is within the square and knowing the full area, we can
    then apply the Monte Carlo method to solve for the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的内容，我们知道我们有一个边长为2个单位或4平方单位的正方形，它覆盖了100%的区域。回到几何学，我们知道圆的面积由以下公式给出：[![图片](img/7a492136-0712-4f73-b12b-8c05460618aa.png)]。知道圆在正方形内并且知道总面积，我们就可以应用蒙特卡洛方法来解决以下问题：
- en: '![](img/8ae5a92d-fd87-4bdb-ba2f-5260483a1247.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8ae5a92d-fd87-4bdb-ba2f-5260483a1247.png)'
- en: 'The Monte Carlo method works by randomly sampling an area and then determining
    what percentage of that sample is correct or incorrect. Going back to our example,
    we can think of this as randomly dropping darts onto the square and then counting
    how many land within the circle. By counting the number of darts that land within
    the circle, we can then backcalculate a number for π using the following equation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法通过随机采样一个区域，然后确定该样本中有多少百分比是正确或错误的。回到我们的例子，我们可以将其想象为随机将飞镖扔到正方形上，然后计算有多少落在圆内。通过计算落在圆内的飞镖数量，我们可以使用以下公式回推π的值：
- en: '![](img/fde16558-a28b-439a-9af4-146fb64597ac.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fde16558-a28b-439a-9af4-146fb64597ac.png)'
- en: 'In the preceding equation, we have the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，我们有以下内容：
- en: '*ins*: The total number of darts or samples that fell within the circle'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ins*：落在圆内的飞镖或样本的总数'
- en: '*total*: The total number of darts dropped'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*total*：扔下的飞镖总数'
- en: The important part about the preceding equation is to realize that all we are
    doing here is taking a percentage (*ins*/*total*) of how many darts fell within
    the circle to determine a value for π. This may still be a little unclear, so
    let's look at a couple of examples in the next sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 前面公式的关键是要意识到我们在这里所做的只是取落在圆内的飞镖数与总数(*ins*/*total*)的百分比来确定π的值。这可能仍然有点不清楚，所以让我们在下一节中看看几个例子。
- en: Implementing Monte Carlo
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现蒙特卡洛
- en: In many cases, even understanding simple concepts that are abstract can be difficult
    without real-world examples. Therefore, open up the `Chapter_3_1.py` code sample.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，即使没有现实世界的例子，理解抽象的简单概念也可能很困难。因此，打开 `Chapter_3_1.py` 代码示例。
- en: We should mention before starting that ![](img/bc1fcbba-b85e-406a-bc3c-71556f0e3a90.png),
    in this case, refers to the actual value we estimate at 3.14.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们应该提到，![](img/bc1fcbba-b85e-406a-bc3c-71556f0e3a90.png)，在这种情况下，指的是我们估计的实际值
    3.14。
- en: 'Follow the exercise:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 按照练习进行：
- en: 'The following is the entire code listing for reference:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是为参考而列出的整个代码列表：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code solves for ![](img/812e1640-0feb-46c5-a7eb-d34729e14040.png) using
    the Monte Carlo method, which is quite impressive when you consider how simple
    the code is. Let's go over each section of the code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用蒙特卡洛方法求解 ![](img/812e1640-0feb-46c5-a7eb-d34729e14040.png)，当你考虑到代码的简单性时，这相当令人印象深刻。让我们逐一分析代码的每个部分。
- en: We start with the `import` statements, and here we just import `random` and
    the `math` function, `sqrt`.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 `import` 语句开始，这里我们只导入了 `random` 和 `math` 函数中的 `sqrt`。
- en: From there, we define a couple of variables, `ins` and `n`. The `ins` variable
    holds the number of times a dart or sample is inside the circle. The `n` variable
    represents how many iterations or darts to drop.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那里，我们定义了几个变量，`ins` 和 `n`。`ins` 变量存储飞镖或样本落在圆内的次数。`n` 变量代表要投掷的迭代次数或飞镖数量。
- en: 'Next, we randomly drop darts with the following code:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码随机投掷飞镖：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: All this code does is randomly sample values in the range of `-1` to `1` for
    `x` and `y` and then determine whether they are within a circle radius of `1`, which
    is given by the calculation within the square root function.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有这些代码所做的只是随机在 `-1` 到 `1` 的范围内采样 `x` 和 `y` 的值，然后确定它们是否在半径为 `1` 的圆内，这是由平方根函数中的计算给出的。
- en: Finally, the last couple of lines do the calculation and output the result.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，最后几行执行计算并输出结果。
- en: Run the example as you normally would and observe the output.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式运行示例并观察输出。
- en: What you will likely find is the guess may be a bit off. That all depends on
    the number of samples. You see, the confidence of the Monte Carlo method and therefore
    the quality of the answer goes up with the more samples you do. Hence, to improve
    the last example, you will have to increase the value of the variable, `n`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现猜测可能有些偏差。这完全取决于样本的数量。你看，蒙特卡洛方法的置信度和因此答案的质量会随着样本数量的增加而提高。因此，为了改进最后的例子，你必须增加变量
    `n` 的值。
- en: In the next section, we look at this example again but this time look at what
    those dart samples may actually look like in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们再次查看这个例子，但这次我们将看看那些飞镖样本可能看起来是什么样子。
- en: Plotting the guesses
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制猜测
- en: 'If you are still having problems grasping this concept, visualizing this example
    may be more helpful. Run the exercise in the next section if you want to visualize
    what this sampling looks like:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然难以理解这个概念，可视化这个例子可能更有帮助。如果你想要可视化这种采样看起来是什么样子，请运行下一节的练习：
- en: 'Before starting this exercise, we will install the `matplotlib` library. Install
    the library with `pip` using the following command:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始这个练习之前，我们将安装 `matplotlib` 库。使用以下命令使用 `pip` 安装库：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After the install, open up the `Chapter_3_2.py` code example shown here:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，打开这里显示的 `Chapter_3_2.py` 代码示例：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code is quite similar to the last exercise and should be fairly self-explanatory.
    We will just focus on the important sections of code highlighted in the preceding.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码与上一个练习非常相似，应该相当容易理解。我们只需关注前面突出显示的重要代码部分。
- en: The big difference in this example is we remember where the darts are dropped
    and identify whether they fell inside or outside of the circle. After that, we
    plot the results. We plot a point for each and color them green for inside the
    circle and red for outside.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，最大的不同之处在于我们记住了飞镖落下的位置，并确定了它们是否落在圆内或圆外。之后，我们绘制结果。我们为每个点绘制一个点，并将落在圆内的点用绿色表示，落在圆外的点用红色表示。
- en: 'Run the sample and observe the output, as shown here:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例并观察输出，如图所示：
- en: '![](img/112792e6-4d06-43b4-b15c-7806abcf1979.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/112792e6-4d06-43b4-b15c-7806abcf1979.png)'
- en: Example output from Chapter_3_2.py
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Chapter_3_2.py 的示例输出
- en: The output looks like a circle, as much as we would expect it to. However, there
    is a problem with the output value of π. Notice how the estimated value of π is
    now quite low. This is because the value of *n—*the number of darts or samples—is
    only 1,000\. That means, for the Monte Carlo method to be a good estimator, we
    also need to realize it needs a sufficiently large number of guesses.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像一个圆圈，正如我们预期的那样。然而，π的输出值存在问题。注意现在π的估计值相当低。这是因为n——即飞镖或样本的数量——仅为1,000。这意味着，为了蒙特卡洛方法成为一个好的估计器，我们还需要意识到它需要一个足够大的猜测数量。
- en: In the next section, we look to see how we can apply this method to an expanded
    version of the FrozenLake problem with RL.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何将这种方法应用于强化学习（RL）的FrozenLake问题的扩展版本。
- en: Adding RL
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加强化学习（RL）
- en: Now that we understand the Monte Carlo method, we need to understand how to
    apply it to RL. Recall that our expectation now is that our environment is relatively
    unknown, that is, we do not have a model. Instead, we now need to develop an algorithm
    by which to explore the environment by trial and error. Then, we can take all
    of those various trials and, by using Monte Carlo, average them out and determine
    a best or better policy. We can then use that improved policy to continue exploring
    the environment for further improvements. Essentially, our algorithm becomes an
    explorer rather than a planner and this is why we now refer to it as an agent.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了蒙特卡洛方法，我们需要了解如何将其应用于强化学习（RL）。回想一下，我们的期望现在是我们所处的环境相对未知，也就是说，我们没有模型。相反，我们现在需要通过试错来开发一个算法来探索环境。然后，我们可以通过蒙特卡洛方法对所有这些不同的试验进行平均，并确定最佳或更好的策略。然后，我们可以使用这个改进的策略来继续探索环境以实现进一步的改进。本质上，我们的算法变成了一个探险者而不是规划者，这就是为什么我们现在将其称为智能体的原因。
- en: Using the term **agent** reminds us that our algorithm is now an explorer and
    learner. Hence, our agents not only explore but also learn from that exploration
    and improve on it. Now, this is real artificial intelligence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用术语**智能体**提醒我们，我们的算法现在是一个探险者和学习者。因此，我们的智能体不仅会探索，还会从探索中学习并改进。现在，这真的是人工智能。
- en: Aside from the exploration part, which we already visited earlier in [Chapter
    1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding Rewards-Based Learning*,
    the agent still needs to evaluate a value function and improve on a policy. Hence,
    much of what we covered in [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml),
    *Dynamic Programming and the Bellman Equation*, will be applicable. However, this
    time, instead of planning, our agent will explore the environment and then, after
    each episode, re-evaluate the value function and update the policy. An episode
    is defined as one complete set of moves from the start to termination. We call
    this type of learning episodic since it refers to the agent only learning and
    improving after an episode. This, of course, has its limitations and we will see
    how continuous control is done in [Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml),
    *Temporal Difference Learning*. In the next section, we jump in and look at the
    code and how this all works.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前在[第一章](5553d896-c079-4404-a41b-c25293c745bb.xhtml)“基于奖励的学习”中已经讨论过的探索部分，智能体仍然需要评估价值函数并改进策略。因此，我们在[第二章](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml)“动态规划和贝尔曼方程”中讨论的大部分内容都将适用。然而，这一次，我们的智能体将不是进行规划，而是探索环境，然后在每个回合之后重新评估价值函数并更新策略。一个回合被定义为从开始到终止的完整移动集。我们称这种类型的学习为回合式学习，因为它指的是智能体只在回合结束后学习和改进。当然，这有其局限性，我们将在[第四章](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml)“时序差分学习”中看到如何进行连续控制。在下一节中，我们将深入探讨代码以及这一切是如何工作的。
- en: Monte Carlo control
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛控制
- en: There are two ways to implement what is called **Monte Carlo control** on an
    agent. The difference between the two is how they calculate the average return
    or sampled mean. In what is called **First-Visit Monte Carlo**, the agent only
    samples the mean the first time a state is visited. The other method, **Every-Visit
    Monte Carlo**, samples the average return every time a state is visited. The latter
    method is what we will explore in the code example for this chapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个智能体上实现所谓的**蒙特卡洛控制**有两种方式。这两种方法的区别在于它们如何计算平均回报或样本均值。在所谓的**首次访问蒙特卡洛**中，智能体只在首次访问某个状态时采样均值。另一种方法，**每次访问蒙特卡洛**，每次访问状态时都会采样平均回报。后一种方法是我们将在本章代码示例中探讨的方法。
- en: The original source code for this example was from Ankit Choudhary's blog ([https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/](https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的原始源代码来自Ankit Choudhary的博客（[https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/](https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/)）。
- en: The code has been heavily modified from the original. Ankit goes far more heavily
    into the mathematics of this method and the original is recommended for those
    readers interested in exploring more math.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经从原始版本中进行了大量修改。Ankit对这个方法的数学进行了更深入的探讨，对于那些对探索更多数学感兴趣的读者，原始版本是推荐的。
- en: 'Open up `Chapter_3_3.py` and follow the exercise:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Chapter_3_3.py`并按照练习进行：
- en: Open the code and review the imports. The code for this example is too large
    to place inline. Instead, the code has been broken into sections.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开代码并查看导入。本例的代码太大，无法内联显示。相反，代码已经被分成几个部分。
- en: 'Scroll to the bottom of the sample and review the following lines:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到样本的底部并查看以下行：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the first line, we construct the environment. Then, we create `policy` using
    a function called `monte_carlo_e_soft`. We complete this step by printing out
    the results from the `test_policy` function.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一行，我们构建了环境。然后，我们使用名为`monte_carlo_e_soft`的函数创建`policy`。我们通过打印出`test_policy`函数的结果来完成这一步。
- en: 'Scroll up to the `monte_carlo_e_soft` function. We will get to the name later
    but, for now, the top lines are shown:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`monte_carlo_e_soft`函数。我们稍后会解释这个名字，但现在，显示的是顶部几行：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These lines create a policy if there is none. This shows how the random policy
    is created:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些行创建了一个策略，如果没有的话。这显示了随机策略是如何创建的：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After that, we create a dictionary to store state and action values, as shown
    here:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们创建一个字典来存储状态和动作值，如下所示：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we start with a `for` loop that iterates through the number of episodes,
    like so:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从一个`for`循环开始，遍历所有剧集的数量，如下所示：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Change `display=False` as highlighted in the preceding to `display=True`, as
    shown here:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前面高亮的`display=False`改为`display=True`，如下所示：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, before we get too far ahead, it may be helpful to see how the agent is
    playing a game. Run the code example and watch the output. Don''t run the code
    until completion—just for a few seconds or up to a minute is fine. Make sure to
    undo your code changes before saving:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在我们走得太远之前，看看代理是如何玩游戏可能会有所帮助。运行代码示例并观察输出。不要运行到完成——只需几秒钟或一分钟即可。确保在保存之前撤销你的代码更改：
- en: '![](img/55657369-f70f-447b-abe6-ceb363afb5cd.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/55657369-f70f-447b-abe6-ceb363afb5cd.png)'
- en: Example output of agent playing the game
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 代理玩游戏的示例输出
- en: This screenshot shows an example of the agent exploring the expanded 8 x 8 FrozenLake
    environment. In the next section, we look at how the agent plays the game.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这张截图显示了代理探索扩展的8 x 8 FrozenLake环境的示例。在下一节中，我们将看看代理是如何玩游戏。
- en: Again, make sure you undo your code and change `display=True` to `display=False`
    before proceeding.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 再次确保在继续之前撤销你的代码，并将`display=True`改为`display=False`。
- en: Playing the FrozenLake game
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩FrozenLake游戏
- en: 'The agent code now plays or explores the environment and it is helpful if we
    understand how this code runs. Open up `Chapter_3_3.py` again and follow the exercise:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 代理代码现在正在玩游戏或探索环境，如果我们理解这段代码是如何运行的，这将很有帮助。再次打开`Chapter_3_3.py`并按照练习进行：
- en: 'All we need to focus on for this section is how the agent plays the game. Scroll
    down to the `play_game` function, as shown in the following:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本节，我们只需要关注代理如何玩游戏。滚动到`play_game`函数，如下所示：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see the function takes the `env` and `policy` environment as inputs.
    Then, inside, it resets the environments with `reset` and then initializes the
    variables. The start of the `while` loop is where the agent begins playing the
    game:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到这个函数接受`env`和`policy`环境作为输入。然后，在内部，它使用`reset`重置环境并初始化变量。`while`循环的开始是代理开始玩游戏的地方：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For this environment, we are letting the agent play infinitely. That is, we
    are not limiting the number of steps the agent may take. However, for this environment,
    that is not a problem since it is quite likely the agent will fall into a hole.
    But, that is not always the case and we often need to limit the number of steps
    and agent takes in an environment. In many cases, that limit is set at `100`,
    for example.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个环境，我们让代理无限期地玩游戏。也就是说，我们不会限制代理可能采取的步骤数量。然而，对于这个环境来说，这并不是问题，因为它很可能代理会掉入洞中。但这种情况并不总是如此，我们经常需要限制代理在环境中的步骤数量。在许多情况下，这个限制被设置为`100`，例如。
- en: 'Inside the `while` loop, we update the agent''s state, `s`, then display the
    environment is `display=True`. After that, we set up a `timestep` list to hold
    that `state`, `action`, and `value`. Then, we append the state, `s`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`while`循环内部，我们更新代理的状态`s`，然后显示环境`display=True`。之后，我们设置一个`timestep`列表来保存那个`state`、`action`和`value`。然后，我们添加状态`s`：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we look at the code that does the random sampling of the action based
    on the `policy` values, as shown:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们查看基于`policy`值进行随机采样动作的代码，如下所示：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is essentially where the agent performs a uniform sampling of the policy
    with `random.uniform`, which is the Monte Carlo method. Uniform means that sampling
    is uniform across values and not skewed if it were from a normal or Gaussian method.
    After that, an action is selected in the `for` loop based on a randomly selected
    item in the policy. Keep in mind that, at the start, all actions may have an equal
    likelihood of `0.25` but later, as the agent learns policy items, it will learn
    to distribute accordingly as well.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这实际上是代理使用`random.uniform`对策略进行均匀采样的地方，这是蒙特卡洛方法。均匀意味着采样在值之间是均匀的，并且如果是从正态或高斯方法中来的话，不会偏斜。之后，在`for`循环中根据策略中随机选择的项选择一个动作。记住，在开始时，所有动作可能具有相等的`0.25`概率，但后来，随着代理学习策略项，它也会相应地学习分布。
- en: Monte Carlo methods use a variety of sampling distributions to determine randomness.
    So far, we have extensively used uniform distributions, but in most real-world
    environments, a normal or Gaussian sampling method is used.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法使用各种采样分布来确定随机性。到目前为止，我们广泛地使用了均匀分布，但在大多数现实世界环境中，通常使用正态或高斯采样方法。
- en: 'Then, after a random action is chosen, the agent takes a step and records it.
    It already recorded `state` and it now appends `action` and `reward`. Then, it
    appends the `timestep` list to the `episode` list, as shown:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在选择一个随机动作后，代理采取一步并记录它。它已经记录了`state`，现在它添加`action`和`reward`。然后，它将`timestep`列表添加到`episode`列表中，如下所示：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally, when the agent has `finished`, by finding the goal or dropping in a
    hole, it returns the list of steps in `episode`.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当代理`完成`后，通过找到目标或掉入洞中，它返回`episode`中的步骤列表。
- en: Now, with our understanding of how the agent plays the game, we can move on
    evaluating the game and optimizing it for prediction and control.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们理解了代理如何玩游戏后，我们可以继续评估游戏并优化它以预测和控制。
- en: Using prediction and control
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预测和控制
- en: 'When we previously had a model, our algorithm could learn to plan and improve
    a policy offline. Now, with no model, our algorithm needs to become an agent and
    learn to explore and, while doing that, also learn and improve. This allows our
    agent to now learn effectively by trial and error. Let''s jump back into the `Chapter_3_3.py` code
    example and follow the exercise:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前有一个模型时，我们的算法可以学习在线下规划和改进策略。现在，没有模型，我们的算法需要成为一个代理，并学习探索，同时在这个过程中学习和改进。这使得我们的代理现在可以通过试错有效地学习。让我们回到`Chapter_3_3.py`代码示例，并跟随练习：
- en: 'We will start right from where we left off and review the last couple of lines
    including the `play_game` function:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从我们离开的地方开始，回顾最后几行包括`play_game`函数：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Inside `evaluate_policy_check`, we test to see whether the `test_policy_freq`
    number has been reached. If it has, we output the current progress of the agent.
    In reality, what we are evaluating is how well the current policy will run an
    agent. The `evaluate_policy_check` function calls `test_policy` to evaluate the
    current policy. The `test_policy` function is shown here:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`evaluate_policy_check`内部，我们测试是否达到了`test_policy_freq`数字。如果是，我们输出代理的当前进度。实际上，我们正在评估当前策略将如何运行代理。`evaluate_policy_check`函数调用`test_policy`来评估当前策略。`test_policy`函数如下所示：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`test_policy` evaluates the current policy by running the `play_game` function
    and setting a new agent loose for several games set by `r = 100`. This provides
    a `wins` percentage, which is output to show the agent''s progress.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test_policy`通过运行`play_game`函数并设置由`r = 100`确定的几个游戏来评估当前策略。这提供了一个`wins`百分比，该百分比输出以显示代理的进度。'
- en: 'Back to the main function, we step into a `for` loop that loops through the
    last episode of gameplay in reverse order, as shown here:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到主函数，我们进入一个`for`循环，以相反的顺序遍历最后一轮游戏，如下所示：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Looping through the episode in reverse order allows us to use the last reward
    and apply it backward. Hence, if the agent received a negative reward, all actions
    would be affected negatively. The same is also true for a positive reward. We
    keep track of the total reward with the `G` variable.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以相反的顺序遍历场景允许我们使用最后一个奖励并将其反向应用。因此，如果代理收到了负奖励，所有动作都会受到负面影响。对于正奖励也是如此。我们使用`G`变量跟踪总奖励。
- en: 'Inside the last loop, we then check whether the state was already evaluated
    for this episode; if not, we find the list of returns and average them. From the
    averages, we can then determine the best action, `A_star`. This is shown in the
    code block:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一个循环内部，我们检查状态是否已经为这个场景评估过；如果没有，我们找到回报列表并计算它们的平均值。然后，我们可以从平均值中确定最佳动作`A_star`。这显示在代码块中：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A lot is going on in this block of code, so work through it slowly if you need
    to. The key takeaway is that all we are doing here is averaging returns or a state
    and then determining the most likely best action, according to Monte Carlo, within
    that state.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码块中有很多事情在进行，所以如果你需要的话，要慢慢工作。关键要点是我们在这里所做的只是平均回报或状态，然后根据蒙特卡洛在该状态内确定最可能的最佳动作。
- en: 'Before we jump to the last section of code, run the example as you normally
    would. This should yield a similar output to the following:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们跳转到代码的最后部分之前，像平常一样运行示例。这应该会产生与以下类似的输出：
- en: '![](img/0a04c1ab-7c86-4a9d-9627-4b24ad950da8.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0a04c1ab-7c86-4a9d-9627-4b24ad950da8.png)'
- en: Example output from Chapter_3_3.py
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter_3_3.py的示例输出
- en: Notice how we can now visualize the agent's progress as it randomly explores.
    The percentage of wins you may see could be entirely different and in some cases,
    they may be much higher or lower. This is because the agent is randomly exploring.
    To evaluate an agent entirely, you would likely need to run the agent for more
    than 50,000 episodes. However, continually averaging a mean after a new sample
    is added over 50,000 iterations would be far too computationally expensive. Instead,
    we use another method called an incremental mean, which we will explore in the
    next section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们现在如何可视化代理在随机探索时的进度。你可能会看到的胜利百分比可能完全不同，在某些情况下，它们可能高得多或低得多。这是因为代理正在随机探索。要完全评估一个代理，你可能需要运行代理超过50,000个场景。然而，在添加新样本后，在50,000次迭代中不断平均平均值将非常计算量大。相反，我们使用另一种称为增量平均值的方法，我们将在下一节中探讨。
- en: Incremental means
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增量意味着
- en: 'An incremental or running mean allows us to keep an average for a list of numbers
    without having to remember the list. This, of course, has huge benefits when we
    need to keep a mean over 50,000, 1 million, or more episodes. Instead of updating
    the mean from a full list, for every episode, we hold one value that we incrementally
    update using the following equation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 增量或运行平均值允许我们在不记住列表的情况下保持一系列数字的平均值。当然，当我们需要保持50,000、1,000,000或更多场景的平均值时，这具有巨大的好处。我们不是从完整的列表中更新平均值，而是对于每个场景，我们保持一个值，并使用以下方程增量更新：
- en: '![](img/1b649433-ce9c-4696-b18d-907d8c43b717.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1b649433-ce9c-4696-b18d-907d8c43b717.png)'
- en: 'In the preceding equation, we have the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下内容：
- en: '[![](img/b53a38e6-94e1-43ce-97bb-f35bb4774602.png)] = The current state value
    for the policy'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/b53a38e6-94e1-43ce-97bb-f35bb4774602.png) = 当前策略的状态值'
- en: '[![](img/d5bc9aee-bd81-489b-8c1c-9a946f2bafb5.png)] = Represents a discount
    rate'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/d5bc9aee-bd81-489b-8c1c-9a946f2bafb5.png) = 代表折扣率'
- en: '[![](img/a74c2e17-9f50-4ee2-8ef8-a4cb4ea6b7c6.png)] = The current total return'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/a74c2e17-9f50-4ee2-8ef8-a4cb4ea6b7c6.png) = 当前总回报'
- en: 'By applying this equation, we now have a method to update the policy and, coincidentally,
    we use a similar method in the full Q equation. However, we are not there yet
    and, instead, we update the value using the following algorithm:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这个方程，我们现在有了更新策略的方法，而且巧合的是，我们在完整的Q方程中也使用了类似的方法。然而，我们还没有达到那里，而是使用以下算法来更新值：
- en: '![](img/dcfa8499-fad1-4c12-986d-7243dc20928d.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dcfa8499-fad1-4c12-986d-7243dc20928d.png)'
- en: The Monte Carlo ε-soft policy algorithm
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛ε-soft策略算法
- en: 'The algorithm shows how the e-soft or epsilon soft version of the Monte Carlo
    algorithm works. Recall this is the second method we can use to define an agent
    with Monte Carlo. While the preceding algorithm may be especially scary, the part
    we are interested in is the last one, shown in this equation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法展示了e-soft或epsilon soft版本的蒙特卡洛算法是如何工作的。回想一下，这是我们使用蒙特卡洛定义代理的第二个方法。虽然前面的算法可能特别令人恐惧，但我们感兴趣的部分是最后一个，如以下方程所示：
- en: '![](img/3c6d6aad-729c-42c5-a6ad-d40d85fd9e15.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c6d6aad-729c-42c5-a6ad-d40d85fd9e15.png)'
- en: This becomes a more effective method for policy updates and is what is shown
    in the example. Open up `Chapter_3_3.py` and follow the exercise*:*
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这成为了一种更有效的策略更新方法，这正是示例中展示的。打开`Chapter_3_3.py`并遵循练习：
- en: 'Scroll down to the following section of code:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到以下代码部分：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It is in this last block of code that we incrementally update the policy to
    the best value, as shown here:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正是在这段代码的最后部分，我们逐步更新策略到最佳值，如下所示：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Or we assign it some base value, as shown in the following:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者我们给它一个基值，如下所示：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: From here, we can run the example again and enjoy the output.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，我们可以再次运行示例并享受输出。
- en: Now that you understand the basics of the Monte Carlo method, you can move on
    to more sample exercises in the next section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了蒙特卡洛方法的基础，你可以继续阅读下一节中的更多示例练习。
- en: Exercises
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'As always, the exercises in this section are here to improve your knowledge
    and understanding of the material. Please attempt to complete 1-3 of these exercises
    on your own:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，本节中的练习旨在提高你对材料的知识和理解。请尝试独立完成1-3个这些练习：
- en: What other constants like π could we use Monte Carlo methods to calculate? Think
    of an experiment to calculate another constant we use.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用蒙特卡洛方法计算哪些其他常数，比如π？想想一个实验来计算我们使用的另一个常数。
- en: Open the `Chapter_3_1.py` sample code and change the value of `n`, that is,
    the number of darts dropped. How does that affect the calculated value for π?
    Use higher or lower values for `n`.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_3_1.py`的示例代码并更改`n`的值，即投掷的飞镖数量。这如何影响π的计算值？使用`n`的更高或更低值。
- en: When we calculated π, we assumed a uniform distribution of darts. However, in
    the real world, the darts would likely be distributed in a normal or Gaussian
    manner. How would this affect the Monte Carlo experiment?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们计算π时，我们假设飞镖是均匀分布的。然而，在现实世界中，飞镖可能以正态或高斯方式分布。这会如何影响蒙特卡洛实验？
- en: Refer to sample `Chapter_3_2.py` and change the value of `n`. How does that
    affect plot generation? Are you able to fix it?
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参考示例`Chapter_3_2.py`并更改`n`的值。这如何影响绘图生成？你能否修复它？
- en: Open `Chapter_3_3.py` and change the number of test episodes to run in the `test_policy`
    function to a higher or lower value.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_3_3.py`并更改在`test_policy`函数中运行的测试剧集数量到一个更高的或更低的值。
- en: Open `Chapter_3_3.py` and increase the number of episodes that are used to train
    the agent. How does the agent's performance increase, if at all?
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_3_3.py`并增加用于训练代理的剧集数量。如果有的话，代理的性能如何提高？
- en: Open `Chapter_3_3.py` and change the value of alpha that is used to update the
    incremental mean of averages. How does that affect the agent's ability to learn?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_3_3.py`并更改用于更新增量平均值的alpha值。这如何影响代理的学习能力？
- en: Add the ability to visualize each policy test in a graph. See whether you can
    transfer the way we created the plots in example `Chapter_3_2.py`.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加在图表中可视化每个策略测试的能力。看看你是否能转移我们在示例`Chapter_3_2.py`中创建图表的方式。
- en: Since the code is fairly generic, test this code on another Gym environment.
    Start with the standard 4 x 4 FrozenLake environment and see how well it performs.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于代码相当通用，请在另一个Gym环境中测试此代码。从标准的4 x 4 FrozenLake环境开始，看看它的表现如何。
- en: Think of ways in which the Monte Carlo method given in this example could be
    improved upon.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想想这个示例中给出的蒙特卡洛方法可以如何改进。
- en: These exercises do not take much additional time and they can make the world
    of difference to your understanding of the materials in this book. Please use
    them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习不会花费太多额外的时间，并且它们可以极大地影响你对本书中材料的理解。请使用它们。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we extended our exploration of RL and looked again at trial-and-error
    methods. In particular, we focused on how the Monte Carlo method could be used
    as a way of learning from experimenting. We first looked at an example experiment
    of the Monte Carlo method for calculating π. From there, we looked at how to visualize
    the output of this experiment with matplotlib. Then, we looked at a code example
    that showed how to use the Monte Carlo method to solve a version of the FrozenLake
    problem. Exploring the code example in detail, we uncovered how the agent played
    the game and, through that exploration, learned to improve a policy. Finally,
    we finished this chapter by understanding how the agent improves this policy using
    an incremental sample mean.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们扩展了我们对强化学习（RL）的探索，并再次审视了试错方法。特别是，我们关注了蒙特卡洛方法如何作为一种从实验中学习的方式。我们首先看了一个蒙特卡洛方法计算π的示例实验。从那里，我们探讨了如何使用matplotlib可视化这个实验的输出。然后，我们查看了一个代码示例，展示了如何使用蒙特卡洛方法解决FrozenLake问题的一个版本。通过详细探索代码示例，我们揭示了智能体如何玩游戏，并通过这种探索学习改进策略。最后，我们通过理解智能体如何使用增量样本均值来改进策略，结束了本章的内容。
- en: The Monte Carlo method is powerful but, as we learned, it requires episodic
    gameplay while, in the real world, a working agent needs to continuously learn
    as it controls. This form of learning is called temporal difference learning and
    is something we will explore in the next chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法功能强大，但正如我们所学的，它需要基于短期的游戏玩法，而在现实世界中，一个正在工作的智能体需要在其控制过程中持续学习。这种学习形式被称为时序差分学习，这是我们将在下一章中探讨的内容。
