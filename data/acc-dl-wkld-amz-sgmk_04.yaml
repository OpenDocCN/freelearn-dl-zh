- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Managing Deep Learning Datasets
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理深度学习数据集
- en: '**Deep learning** models usually require a considerable amount of training
    data to learn useful patterns. In many real-life applications, new data is continuously
    collected, processed, and added to the training dataset, so your models can be
    periodically retrained so that they can adjust to changing real-world conditions.
    In this chapter, we will look into SageMaker capabilities and other AWS services
    to help you manage your training data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**模型通常需要大量的训练数据来学习有用的模式。在许多现实应用中，新数据会持续收集、处理，并添加到训练数据集中，以便您的模型可以定期进行重新训练，从而适应不断变化的现实环境。在本章中，我们将探讨SageMaker的能力以及其他AWS服务，帮助您管理训练数据。'
- en: SageMaker provides a wide integration capability where you can use AWS general-purpose
    data storage services such as Amazon S3, Amazon EFS, and Amazon FSx for Lustre.
    Additionally, SageMaker has purpose-built storage for **machine learning** (**ML**)
    called SageMaker Feature Store. We will discuss when to choose one or another
    storage solution, depending on the type of data, consumption, and ingestion patterns.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供了广泛的集成功能，您可以使用AWS通用数据存储服务，如Amazon S3、Amazon EFS和Amazon FSx for Lustre。此外，SageMaker还有专为**机器学习**（**ML**）设计的存储解决方案——SageMaker
    Feature Store。我们将讨论根据数据类型、消费和摄取模式选择存储解决方案的时机。
- en: In many cases, before you can use training data, you need to pre-process it.
    For instance, data needs to be converted into a specific format or datasets need
    to be augmented with modified versions of samples. In this chapter, we will review
    SageMaker Processing and how it can be used to process large-scale ML datasets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，在使用训练数据之前，您需要对其进行预处理。例如，数据需要转换成特定格式，或数据集需要通过修改后的样本版本进行增强。本章将回顾SageMaker
    Processing，以及如何使用它来处理大规模的机器学习（ML）数据集。
- en: We will close this chapter by looking at advanced techniques on how to optimize
    the data retrieval process for TensorFlow and PyTorch models using AWS data streaming
    utilities.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后，我们将探讨一些先进的技术，如何利用AWS数据流工具优化TensorFlow和PyTorch模型的数据检索过程。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Selecting storage solutions for ML datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为ML数据集选择存储解决方案
- en: Processing data at scale
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模处理数据
- en: Optimizing data storage and retrieval
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数据存储和检索
- en: After reading this chapter, you will know how to organize your DL dataset’s
    life cycle for training and inference on SageMaker. We will also run through some
    hands-on examples for data processing and data retrieval to help you gain practical
    skills in those areas.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将知道如何组织您的深度学习（DL）数据集的生命周期，以便在SageMaker上进行训练和推理。我们还将通过一些动手示例来帮助您获得数据处理和数据检索方面的实际技能。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will provide code samples so that you can develop your practical
    skills. The full code examples are available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供代码示例，以便您可以开发实际技能。完整的代码示例可以在[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/)查看。
- en: 'To follow this code, you will need the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随这段代码，您需要以下内容：
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS账户和具有管理Amazon SageMaker资源权限的IAM用户
- en: A SageMaker Notebook, SageMaker Studio Notebook, or local SageMaker-compatible
    environment established
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已建立的SageMaker笔记本、SageMaker Studio笔记本或本地兼容SageMaker环境
- en: Selecting storage solutions for ML datasets
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为ML数据集选择存储解决方案
- en: 'AWS Cloud provides a wide range of storage solutions that can be used to store
    inference and training data. When choosing an optimal storage solution, you may
    consider the following factors:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Cloud提供了一系列可以用来存储推理和训练数据的存储解决方案。在选择最佳存储解决方案时，您可以考虑以下因素：
- en: Data volume and velocity
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量和速度
- en: Data types and associated metadata
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型及相关元数据
- en: Consumption patterns
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费模式
- en: Backup and retention requirements
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和保留要求
- en: Security and audit requirements
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性和审计要求
- en: Integration capabilities
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成能力
- en: Price to store, write, and read data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储、写入和读取数据的价格
- en: Carefully analyzing your specific requirements may suggest the right solution
    for your use case. It’s also typical to combine several storage solutions for
    different stages of your data life cycle. For instance, you could store data used
    for inference consumption with lower latency requirements in faster but more expensive
    storage; then, you could move the data to cheaper and slower storage solutions
    for training purposes and long-term retention.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细分析你的具体需求可能会为你的用例建议正确的解决方案。通常，在数据生命周期的不同阶段会组合使用几种存储解决方案。例如，你可以将用于推理的低延迟需求数据存储在更快但更昂贵的存储中；然后，将数据移动到更便宜、更慢的存储解决方案中用于训练和长期保存。
- en: 'There are several types of common storage types with different characteristics:
    filesystems, object storage and block storage solutions. Amazon provides managed
    services for each type of storage. We will review their characteristics and how
    to use them in your SageMaker workloads in the following subsections. Then, we
    will focus on Amazon SageMaker Feature Store as it provides several unique features
    specific to ML workloads and datasets.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种常见的存储类型，它们具有不同的特性：文件系统、对象存储和块存储解决方案。Amazon 为每种存储类型提供托管服务。我们将在以下小节中回顾它们的特性以及如何在
    SageMaker 工作负载中使用它们。接下来，我们将重点介绍 Amazon SageMaker 特征库，因为它为 ML 工作负载和数据集提供了几个独特的功能。
- en: Amazon EBS – high-performance block storage
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EBS – 高性能块存储
- en: Block storage solutions are designed for quick data retrieval and manipulation.
    The data is broken into blocks on the physical device for efficient utilization.
    Block storage allows you to abstract and decouple data from the runtime environment.
    At data retrieval time, blocks are reassembled by the storage solution and returned
    to users.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 块存储解决方案旨在快速检索和操作数据。数据在物理设备上被分割成块，以便高效利用。块存储使你能够将数据从运行时环境中抽象和解耦。在数据检索时，存储解决方案将这些块重新组合并返回给用户。
- en: Amazon EBS is a fully managed block storage solution that supports a wide range
    of use cases for different read-write patterns, throughput, and latency requirements.
    A primary use case of Amazon EBS is to serve as data volumes attached to Amazon
    EC2 compute nodes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EBS 是一种完全托管的块存储解决方案，支持多种用例，适用于不同的读写模式、吞吐量和延迟要求。Amazon EBS 的一个主要用例是作为附加到
    Amazon EC2 计算节点的数据卷。
- en: 'Amazon SageMaker provides seamless integration with EBS. In the following example,
    we are provisioning a training job with four nodes; each node will have an EBS
    volume with 100 GB attached to it. The training data will be downloaded and stored
    on EBS volumes by SageMaker:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 提供与 EBS 的无缝集成。在以下示例中，我们正在为四个节点配置一个训练任务；每个节点将附加一个 100 GB 的 EBS
    卷。训练数据将由 SageMaker 下载并存储在 EBS 卷上：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that you cannot customize the type of EBS volume that’s used. Only **general-purpose
    SSD** volumes are supported. Once the training job is completed, all instances
    and attached EBS volumes will be purged.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你无法自定义使用的 EBS 卷类型。仅支持**通用 SSD** 卷。一旦训练任务完成，所有实例和附加的 EBS 卷将被清除。
- en: Amazon S3 – industry-standard object storage
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon S3 – 行业标准对象存储
- en: Object storage implements a flat structure, where each file object has a unique
    identifier (expressed as a path) and associated data object. A flat structure
    allows you to scale object storage solutions linearly and sustain high-throughput
    data reads and writes while keeping costs low.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储实现了一个平面结构，每个文件对象都有一个唯一标识符（以路径形式表示）和关联的数据对象。平面结构允许你线性扩展对象存储解决方案，并在保持低成本的同时，支撑高吞吐量的数据读写。
- en: Object storage can handle objects of different types and sizes. Object storage
    also allows you to store metadata associated with each object. Data reads and
    writes are typically done via HTTP APIs, which allows for ease of integration.
    However, note that object storage solutions are generally slower than filesystems
    or block storage solutions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储可以处理不同类型和大小的对象。对象存储还允许你存储与每个对象相关的元数据。数据读写通常通过 HTTP API 进行，这使得集成变得更加容易。但需要注意的是，对象存储解决方案通常比文件系统或块存储解决方案慢。
- en: Amazon S3 was the first petabyte-scale cloud object storage service. It offers
    durability, availability, performance, security, and virtually unlimited scalability
    at very low costs. Many object storage solutions follow the Amazon S3 API. Amazon
    S3 is used to store customer data, but it’s also used for many internal AWS features
    and services where data needs to be persisted.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 S3 是首个 PB 级别的云对象存储服务。它提供持久性、可用性、性能、安全性和几乎无限的可扩展性，同时成本非常低廉。许多对象存储解决方案都遵循亚马逊
    S3 的 API。亚马逊 S3 用于存储客户数据，但它也用于许多 AWS 内部功能和服务，其中数据需要持久化。
- en: 'SageMaker provides seamless integration with Amazon S3 for storing input and
    output objects, such as datasets, log streams, job outputs, and model artifacts.
    Let’s look at an example of a training job and how to define where we will store
    our inputs and outputs:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 提供了与 Amazon S3 的无缝集成，用于存储输入和输出对象，如数据集、日志流、作业输出和模型工件。让我们看一个训练作业示例，以及如何定义我们将存储输入和输出的位置：
- en: The `model_uri` parameter specifies an S3 location of model artifacts, such
    as pre-trained weights, tokenizers, and others. SageMaker automatically downloads
    these artifacts to each training node.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_uri` 参数指定了模型工件的 S3 位置，例如预训练权重、分词器等。SageMaker 会自动将这些工件下载到每个训练节点。'
- en: '`checkpoint_s3_uri` defines the S3 location where training checkpoints will
    be uploaded during training. Note that it is the developer’s responsibility to
    implement checkpointing functionality in the training script.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint_s3_uri` 定义了训练过程中上传训练检查点的 S3 位置。请注意，实现检查点功能由开发者在训练脚本中负责。'
- en: '`output_path` specifies the S3 destination of all output artifacts that SageMaker
    will upload from training nodes after training job completion.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_path` 指定了 SageMaker 在训练作业完成后，从训练节点上传的所有输出工件的 S3 目标位置。'
- en: '`tensorboard_output_config` defines where to store TensorBoard logs on S3\.
    Note that SageMaker continuously uploads these logs during training job execution,
    so you can monitor your training progress in near real time in TensorBoard:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorboard_output_config` 定义了在 S3 上存储 TensorBoard 日志的位置。请注意，SageMaker 会在训练作业执行期间持续上传这些日志，因此你可以在
    TensorBoard 中实时监控训练进度：'
- en: '[PRE1]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We also use S3 to store our training dataset. Take a look at the following
    `estimator.fit()` method, which defines the location of our training data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 S3 来存储我们的训练数据集。请看以下 `estimator.fit()` 方法，它定义了我们训练数据的位置：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, the `"train"` and `"test"` parameters are called `/opm/ml/input/data/{channel_name}`
    directory. Additionally, the training toolkit will create `SM_CHANNEL_{channel_name}`
    environment variables, which you can use in your training script to access model
    artifacts locally.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`"train"` 和 `"test"` 参数被称为 `/opm/ml/input/data/{channel_name}` 目录。此外，训练工具包将创建
    `SM_CHANNEL_{channel_name}` 环境变量，您可以在训练脚本中使用这些变量来本地访问模型工件。
- en: As shown in the preceding code block, Amazon S3 can be used to store the input
    and output artifacts of SageMaker training jobs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面代码块所示，Amazon S3 可用于存储 SageMaker 训练作业的输入和输出工件。
- en: File, FastFile, and Pipe modes
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件、快速文件和管道模式
- en: S3 storage is a common place to store your training datasets. By default, when
    working with data stored on S3, all objects matching the path will be downloaded
    to each compute node and stored on its EBS volumes. This is known as **File**
    mode.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: S3 存储是一个常用的训练数据集存储位置。默认情况下，当使用存储在 S3 上的数据时，所有与路径匹配的对象将被下载到每个计算节点，并存储在其 EBS 卷中。这被称为
    **文件**模式。
- en: However, in many scenarios, training datasets can be hundreds of gigabytes or
    larger. Downloading such large files will take a considerable amount of time,
    even before your training begins. To reduce the time needed to start a training
    job, SageMaker supports **Pipe** mode, which allows you to stream data from the
    S3 location without fully downloading it. This allows you to start training jobs
    immediately and fetch data batches as needed during the training cycle.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多场景中，训练数据集可能有数百 GB 或更大。即使在训练开始之前，下载如此大的文件也需要相当长的时间。为了减少启动训练作业所需的时间，SageMaker
    支持 **管道**模式，允许你从 S3 位置流式传输数据，而无需完全下载。这使你能够立即启动训练作业，并在训练过程中根据需要获取数据批次。
- en: One of the drawbacks of Pipe mode is that it requires using framework-specific
    implementations of data utilities to stream data. The recently introduced **FastFile**
    mode addresses this challenge. FastFile mode allows you to stream data directly
    from S3 without the need to implement any specific data loaders. In your training
    or processing scripts, you can treat **FastFiles** as if they are regular files
    stored on disk; Amazon SageMaker will take care of the read and write operations
    for you.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Pipe 模式的一个缺点是需要使用框架特定的实现来流式传输数据。最近引入的**FastFile**模式解决了这一问题。FastFile 模式允许您直接从
    S3 流式传输数据，而无需实现任何特定的数据加载器。在您的训练或处理脚本中，您可以将**FastFile**视为存储在磁盘上的常规文件；Amazon SageMaker
    会为您处理读写操作。
- en: We will develop practical skills on how to organize training code for S3 streaming
    using **FastFile** and **Pipe** modes in the *Optimizing data storage and retrieval*
    section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*优化数据存储与检索*部分中，学习如何使用**FastFile**和**Pipe**模式组织 S3 流式传输的训练代码。
- en: FullyReplicated and ShardedByKey
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FullyReplicated 和 ShardedByKey
- en: In many training and data processing tasks, we want to parallelize our job across
    multiple compute nodes. In scenarios where we have many data objects, we can split
    our tasks by splitting our full set of objects into unique subsets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多训练和数据处理任务中，我们希望将作业并行化到多个计算节点上。在有大量数据对象的场景中，我们可以通过将完整的对象集拆分成唯一的子集来拆分任务。
- en: To implement such a scenario, SageMaker supports **ShardedByKey** mode, which
    attempts to evenly split all matching objects and deliver a unique subset of objects
    to each node. For instance, if you have *n* objects in your dataset and *k* compute
    nodes in your job, then each compute node will get a unique set of *n/k* objects.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这种场景，SageMaker 支持**ShardedByKey**模式，该模式尝试均匀地拆分所有匹配对象，并将每个节点分配一个独特的对象子集。例如，如果您的数据集中有
    *n* 个对象，而作业中有 *k* 个计算节点，那么每个计算节点将获得一个独特的 *n/k* 个对象的子集。
- en: Unless otherwise specified, the default mode, **FullyReplicated**, is used when
    SageMaker downloads all matching objects to all nodes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，默认模式**FullyReplicated**会在 SageMaker 下载所有匹配对象到所有节点时使用。
- en: We will acquire practical skills on how to distribute data processing tasks
    in the *Distributed data processing* section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*分布式数据处理*部分中学习如何分配数据处理任务的实际技能。
- en: Amazon EFS – general-purpose shared filesystem
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EFS – 通用共享文件系统
- en: Amazon EFS is a managed file storage service that is easy to set up and automatically
    scales up to petabytes. It provides a filesystem interface and file semantics
    such as file locking and strong consistency. Unlike Amazon EBS, which allows you
    to attach storage to a single compute node, EFS can be simultaneously used by
    hundreds or thousands of compute nodes. This allows you to organize efficient
    data sharing between nodes without the need to duplicate and distribute data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EFS 是一种托管的文件存储服务，易于设置并自动扩展到 PB 级别。它提供文件系统接口和文件语义，如文件锁定和强一致性。与 Amazon
    EBS 不同，EBS 允许您将存储附加到单个计算节点，而 EFS 可以同时被数百或数千个计算节点使用。这使您能够组织高效的数据共享，而无需重复和分发数据。
- en: 'Amazon SageMaker allows you to use EFS to store training datasets. The following
    code shows an example of how to use Amazon EFS in the training job configuration
    using the `FileSystemInput` class. Note that in this case, we have configured
    read-only access to the data (the `ro` flag of the `file_system_access_mode` parameter),
    which is typically the case for the training job. However, you can also specify
    read-write permissions by setting `file_system_access_mode` to `rw`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 允许您使用 EFS 存储训练数据集。以下代码展示了如何在训练作业配置中使用 Amazon EFS，通过 `FileSystemInput`
    类进行配置。请注意，在此情况下，我们已配置为数据的只读访问（`file_system_access_mode` 参数的 `ro` 标志），这通常是训练作业的情况。不过，您也可以通过将
    `file_system_access_mode` 设置为 `rw` 来指定读写权限。
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, you can control other EFS resources. Depending on the latency requirements
    for data reads and writes, you can choose from several modes that define the latency
    and concurrency characteristics of your filesystem. At the time of writing this
    book, EFS can sustain 10+ GB per second throughput and scale up to thousands of
    connected compute nodes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以控制其他 EFS 资源。根据数据读写的延迟要求，您可以从几种模式中选择，这些模式定义了文件系统的延迟和并发特性。在本书编写时，EFS 可以支持每秒
    10+ GB 的吞吐量，并可扩展到数千个连接的计算节点。
- en: Amazon FSx for Lustre – high-performance filesystem
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon FSx for Lustre – 高性能文件系统
- en: Amazon FSx for Lustre is a file storage service optimized for ML and **high-performance
    computing** (**HPC**) workloads. It is designed for sub-millisecond latency for
    read and write operations and can provide hundreds of GB/s throughput. You can
    also choose to store data in S3 and synchronize it with the Amazon FSx for Lustre
    filesystem. In this case, an FSx system presents S3 objects as files and allows
    you to update data back to the S3 origin.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon FSx for Lustre 是一种针对机器学习和**高性能计算**（**HPC**）工作负载优化的文件存储服务。它设计用于子毫秒延迟的读写操作，并且能够提供数百
    GB/s 的吞吐量。你还可以选择将数据存储在 S3 中，并与 Amazon FSx for Lustre 文件系统进行同步。在这种情况下，FSx 系统会将
    S3 对象呈现为文件，并允许你将数据更新回 S3 原点。
- en: 'Amazon SageMaker supports storing training data in the FSx for Lustre filesystem.
    Training job configuration is similar to using the EFS filesystem; the only difference
    is that the `file_system_type` parameter is set to `FSxLustre`. The following
    code shows a sample training job:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 支持将训练数据存储在 FSx for Lustre 文件系统中。训练作业配置与使用 EFS 文件系统类似；唯一的区别是 `file_system_type`
    参数设置为 `FSxLustre`。以下代码展示了一个示例训练作业：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that when provisioning your Lustre filesystem, you may choose either SSD
    or HDD storage. You should choose SSD for latency-sensitive workloads; HDD is
    a better fit for workloads with high-throughput requirements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在配置你的 Lustre 文件系统时，你可以选择 SSD 或 HDD 存储。对于对延迟敏感的工作负载，应该选择 SSD；而对于具有高吞吐量需求的工作负载，HDD
    更为适合。
- en: SageMaker Feature Store – purpose-built ML storage
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker 特征存储——专为机器学习设计的存储
- en: 'So far, we’ve discussed general-purpose file and object storage services that
    can be used to store data in your SageMaker workloads. However, real-life ML workflows
    may present certain challenges when it comes to feature engineering and data management,
    such as the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了可以用于存储数据的通用文件和对象存储服务，这些服务可以应用于你的 SageMaker 工作负载。然而，现实中的机器学习工作流在特征工程和数据管理方面可能会遇到一些挑战，如下所示：
- en: Managing the data ingestion pipeline to keep data up to date
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理数据摄取管道，以保持数据的最新状态
- en: Organizing data usage between different teams in your organization and eliminating
    duplicative efforts
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组织内不同团队之间组织数据使用，并消除重复工作
- en: Sharing data between inference and training workloads when needed
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理和训练工作负载之间共享数据（在需要时）
- en: Managing dataset consistency, its metadata and versioning
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理数据集一致性、元数据和版本控制
- en: Ad hoc analysis of data
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的临时分析
- en: To address these challenges, SageMaker provides an ML-specific data storage
    solution called **Feature Store**. It allows you to accelerate data processing
    and curation by reducing repetitive steps and providing a set of APIs to ingest,
    transform, and consume data for inference and model training.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，SageMaker 提供了一种专门为机器学习设计的数据存储解决方案，称为**特征存储**。它通过减少重复步骤并提供一组 API 来获取、转换和使用数据进行推理和模型训练，从而加速数据处理和整理。
- en: Its central concept is a **feature** – a single attribute of a data record.
    Each data record consists of one or many features. Additionally, data records
    contain metadata such as record update time, unique record ID, and status (deleted
    or not). The feature can be of the string, integer, or fractional type. Data records
    and their associated features can be organized into logical units called **feature
    groups**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 它的核心概念是**特征**——数据记录的单一属性。每个数据记录由一个或多个特征组成。此外，数据记录还包含元数据，如记录更新时间、唯一记录 ID 和状态（是否已删除）。特征可以是字符串、整数或小数类型。数据记录及其相关特征可以组织成逻辑单元，称为**特征组**。
- en: Now, let’s review the key features of SageMaker Feature Store.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下 SageMaker 特征存储的关键功能。
- en: Online and offline storage
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线存储和离线存储
- en: 'Feature Store supports several storage options for different use cases:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储支持多种存储选项，以适应不同的使用场景：
- en: '**Offline storage** is designed to store your data in scenarios where data
    retrieval latency is not critical, such as storing data for training or batch
    inference. Your dataset resides in S3 storage and can be queried using the Amazon
    Athena SQL engine.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离线存储**旨在存储数据，其场景中数据检索延迟不关键，例如存储训练或批量推理的数据。你的数据集存储在 S3 中，可以使用 Amazon Athena
    SQL 引擎进行查询。'
- en: '**Online storage** allows you to retrieve a single or batch of records with
    millisecond latency for real-time inference use cases.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线存储**允许你以毫秒级延迟检索单个或一批记录，适用于实时推理场景。'
- en: '**Offline and online storage** allows you to store the same data in both forms
    of storage and use it in both inference and training scenarios.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离线和在线存储**允许你将相同的数据存储在两种形式的存储中，并在推理和训练场景中使用。'
- en: Ingestion Interfaces
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 导入接口
- en: There are several ways to get your data in Feature Store. One way is using Feature
    Store’s `PutRecord` API, which allows you to write either a single or a batch
    of records. This will write the records in both offline and online storage.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以将数据导入 Feature Store。其中一种方法是使用 Feature Store 的 `PutRecord` API，它允许您写入单条或批量记录。这将把记录写入离线存储和在线存储。
- en: Another option is to use a Spark connector. This is a convenient way to ingest
    data if you already have your Spark-based data processing pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用 Spark 连接器。如果您已经有基于 Spark 的数据处理管道，这是一种方便的数据导入方式。
- en: Analytical queries
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析查询
- en: When data is stored in offline storage, you can use Athena SQL to query the
    dataset using SQL syntax. This is helpful when you have a diverse team that has
    different levels of coding skills. As Feature Store contains useful metadata fields,
    such as **Event Time** and **Status**, you can use these times to run *time travel*
    queries, for instance, to get a historical snapshot of your dataset at a given
    point in time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据存储在离线存储中时，可以使用 Athena SQL 通过 SQL 语法查询数据集。这对于拥有不同编码技能水平的多元化团队非常有帮助。由于 Feature
    Store 包含有用的元数据字段，如**事件时间**和**状态**，您可以使用这些时间进行*时间旅行*查询，例如，获取数据集在某一时刻的历史快照。
- en: Feature discovery
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征发现
- en: 'Once data has been ingested into Feature Store, you can use SageMaker Studio
    to review and analyze datasets via an intuitive UI component without the need
    to write any code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被导入 Feature Store，您可以通过 SageMaker Studio 使用直观的 UI 组件查看和分析数据集，而无需编写任何代码：
- en: '![Figure 4.1 – Feature Store dataset discovery via SageMaker Studio UI ](img/B17519_04_01.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 通过 SageMaker Studio UI 发现 Feature Store 数据集](img/B17519_04_01.jpg)'
- en: Figure 4.1 – Feature Store dataset discovery via SageMaker Studio UI
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 通过 SageMaker Studio UI 发现 Feature Store 数据集
- en: Now that we understand the value proposition of Feature Store compared to more
    general-purpose storage solutions, let’s see how it can be used for a typical
    DL scenario when we want to have tokenized text available next to its original
    form.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了 Feature Store 相比于更通用的存储解决方案的价值主张，接下来让我们看看它如何在典型的深度学习场景中使用，当我们希望将分词文本与原始文本并排存放时。
- en: Using Feature Store for inference and training
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Feature Store 进行推理和训练
- en: In this practical example, we will develop skills on how to use SageMaker Feature
    Store to ingest, process, and consume datasets that contain IMDb reviews. We will
    take the original dataset that contains the reviews and run a custom BERT tokenizer
    to convert unstructured text into a set of integer tokens. Then, we will ingest
    the dataset with the tokenized text feature into Feature Store so that you don’t
    have to tokenize the dataset the next time we want to use it. After that, we will
    train our model to categorize positive and negative reviews.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实际示例中，我们将学习如何使用 SageMaker Feature Store 导入、处理和消费包含 IMDb 评论的数据集。我们将采用包含评论的原始数据集，并运行自定义
    BERT 分词器将非结构化文本转换为一组整数令牌。然后，我们将把包含分词文本特征的数据集导入 Feature Store，这样下次使用时就无需再进行分词了。之后，我们将训练模型以分类正面和负面评论。
- en: We will use SageMaker Feature Store SDK to interact with Feature Store APIs.
    We will use the HuggingFace Datasets ([https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/))
    and Transformers ([https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index))
    libraries to tokenize the text and run training and inference. Please make sure
    that these libraries are installed.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 SageMaker Feature Store SDK 与 Feature Store APIs 进行交互。我们将使用 HuggingFace
    Datasets（[https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/)）和
    Transformers（[https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)）库来对文本进行分词，并进行训练和推理。请确保已安装这些库。
- en: Preparing the data
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Follow these steps to prepare the data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤准备数据：
- en: 'The first step is to acquire the initial dataset that contains the IMDb reviews:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是获取包含 IMDb 评论的初始数据集：
- en: '[PRE5]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we must convert the dataset into a pandas DataFrame that is compatible
    with `EventTime` and `ID`. Both are required by Feature Store to support fast
    retrieval and feature versioning:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须将数据集转换为与`EventTime`和`ID`兼容的pandas DataFrame。这两个字段是Feature Store所必需的，以支持快速检索和特征版本控制：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let’s run the downloaded pre-trained tokenizer for the `Distilbert` model
    and add a new attribute, `tokenized-text`, to our dataset. Note that we cast `tokenized-text`
    to a string as SageMaker Feature Store doesn’t support collection data types such
    as arrays or maps:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们运行下载的 `Distilbert` 模型预训练的分词器，并向数据集添加一个新属性 `tokenized-text`。请注意，我们将 `tokenized-text`
    转换为字符串，因为 SageMaker Feature Store 不支持集合数据类型，如数组或映射：
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As a result, we have a pandas DataFrame object that contains the features we
    are looking to ingest into Feature Store.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个 pandas DataFrame 对象，里面包含了我们希望摄取到 Feature Store 中的特征。
- en: Ingesting the data
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据摄取
- en: 'The next step is to provision Feature Store resources and prepare them for
    ingestion. Follow these steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为 Feature Store 配置资源并准备摄取。请按照以下步骤操作：
- en: 'We will start by configuring the feature group and preparing feature definitions.
    Note that since we stored our dataset in a pandas DataFrame, Feature Store can
    use this DataFrame to infer feature types:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从配置特征组并准备特征定义开始。请注意，由于我们将数据集存储在 pandas DataFrame 中，Feature Store 可以使用该 DataFrame
    推断特征类型：
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have prepared the feature group configuration, we are ready to
    create it. This may take several minutes, so let’s add a `Waiter`. Since we are
    planning to use both online and offline storage, we will set the `enable_online_store`
    flag to `True`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了特征组配置，接下来可以创建它。这个过程可能需要几分钟，所以我们来加一个 `Waiter`。由于我们打算同时使用在线存储和离线存储，我们将
    `enable_online_store` 标志设置为 `True`：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the group is available, we are ready to ingest data. Since we have a full
    dataset available, we will use a batch ingest API, as shown here:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦组可用，我们就可以准备摄取数据。由于我们有完整的数据集，我们将使用批量摄取 API，如下所示：
- en: '[PRE10]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the data has been ingested, we can run some analytical queries. For example,
    we can check if we are dealing with a balanced or imbalanced dataset. As mentioned
    previously, Feature Store supports querying data using the Amazon Athena SQL engine:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据被摄取后，我们可以运行一些分析查询。例如，我们可以检查数据集是平衡的还是不平衡的。如前所述，Feature Store 支持使用 Amazon Athena
    SQL 引擎查询数据：
- en: '[PRE11]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It will take a moment to run, but in the end, you should get a count of the
    labels in our dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个过程需要一些时间，但最终你应该能得到我们数据集中标签的数量。
- en: Using Feature Store for training
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Feature Store 进行训练
- en: Now that we have data available, let’s train our binary classification model.
    Since data in Feature Store is stored in Parquet format ([https://parquet.apache.org/](https://parquet.apache.org/))
    in a designated S3 location, we can directly use Parquet files for training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了可用的数据，接下来让我们训练二分类模型。由于 Feature Store 中的数据以 Parquet 格式存储在指定的 S3 位置（[https://parquet.apache.org/](https://parquet.apache.org/)），我们可以直接使用
    Parquet 文件进行训练。
- en: 'To handle Parquet files, we need to make sure that our data reader is aware
    of the format. For this, we can use the pandas `.read_parquet()` method. Then,
    we can convert the pandas DataFrame object into the HuggingFace dataset and select
    the attributes of interest – `tokenized-text` and `label`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理 Parquet 文件，我们需要确保数据读取器能够识别这种格式。为此，我们可以使用 pandas 的 `.read_parquet()` 方法。然后，我们可以将
    pandas DataFrame 对象转换为 HuggingFace 数据集，并选择我们关心的属性——`tokenized-text` 和 `label`：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we need to convert `tokenized-text` from a string into a list of integers:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将 `tokenized-text` 从字符串转换为整数列表：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The rest of the training script is the same. You can find the full code at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的训练脚本是相同的。你可以在 [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_sources/train.py)
    找到完整代码。
- en: 'Now that we’ve modified the training script, we are ready to run our training
    job:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经修改了训练脚本，准备好运行训练任务了：
- en: 'First, we must get the location of the dataset:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须获取数据集的位置：
- en: '[PRE14]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we must pass it to our `Estimator` object:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须将它传递给我们的 `Estimator` 对象：
- en: '[PRE15]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After some time (depending on how many epochs or steps you use), the model should
    be trained to classify reviews based on the input text.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间（取决于你使用的 epoch 或步骤数），模型应该已经训练好，可以根据输入的文本对评论进行分类。
- en: Using Feature Store for inference
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Feature Store 进行推理
- en: 'For inference, we can use the Feature Store runtime client from the Boto3 library
    to fetch a single record or a batch:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，我们可以使用 Boto3 库中的 Feature Store 运行时客户端来获取单个记录或批量记录：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that you need to know the unique IDs of the records to retrieve them:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你需要知道记录的唯一 ID 才能检索它们：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, you can send this request for inference to your deployed model. Refer
    to the following notebook for an end-to-end example: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将这个推理请求发送到你已部署的模型。请参考以下笔记本，查看完整的端到端示例：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/1_Managing_data_in_FeatureStore.ipynb)。
- en: In this section, we reviewed the options you can use to store your ML data for
    inference and training needs. But it’s rarely the case that data can be used “as-is.”
    In many scenarios, you need to continuously process data at scale before using
    it in your ML workloads. SageMaker Processing provides a scalable and flexible
    mechanism to process your data at scale. Let’s take a look.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了可以用于存储 ML 数据以供推理和训练的选项。但数据“原样”使用的情况很少见。在许多场景中，在将数据用于 ML 工作负载之前，你需要在大规模上持续处理数据。SageMaker
    Processing 提供了一个可扩展且灵活的机制，用于大规模处理数据。我们来看看。
- en: Processing data at scale
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模处理数据
- en: SageMaker Processing allows you to run containerized code in the cloud. This
    is useful for scenarios such as data pre and post-processing, feature engineering,
    and model evaluation. SageMaker Processing can be useful for ad hoc workloads
    as well as recurrent jobs.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Processing 允许你在云中运行容器化代码。这对于数据的预处理和后处理、特征工程以及模型评估等场景非常有用。SageMaker
    Processing 对于临时工作负载以及周期性任务都很有用。
- en: As in the case of a training job, Amazon SageMaker provides a managed experience
    for underlying compute and data infrastructure. You will need to provide a processing
    job configuration, code, and the container you want to use, but SageMaker will
    take care of provisioning the instances and deploying the containerized code,
    as well as running and monitoring the job and its progress. Once your job reaches
    the terminal state (success or failure), SageMaker will upload the resulting artifacts
    to the S3 storage and deprovision the cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练任务一样，Amazon SageMaker 提供了一个托管的底层计算和数据基础设施体验。你需要提供一个处理任务配置、代码和要使用的容器，但 SageMaker
    将负责配置实例、部署容器化代码，并运行和监控任务及其进度。一旦任务达到终止状态（成功或失败），SageMaker 将把结果工件上传到 S3 存储并撤销集群。
- en: 'SageMaker Processing provides two pre-built containers:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Processing 提供了两个预构建容器：
- en: A PySpark container with dependencies to run Spark computations
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有运行 Spark 计算依赖关系的 PySpark 容器
- en: A scikit-learn container
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 scikit-learn 容器
- en: When selecting which built-in processing container, note that the PySpark container
    supports distributed Spark jobs. It allows you to coordinate distributed data
    processing in a Spark cluster, maintain it globally across the dataset, and visualize
    processing jobs via the Spark UI. At the same time, the scikit-learn container
    doesn’t support a shared global state, so each processing node runs independently.
    Limited task coordination can be done by sharding datasets into sub-datasets and
    processing each sub-dataset independently.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择内置处理容器时，请注意，PySpark 容器支持分布式 Spark 任务。它允许你在 Spark 集群中协调分布式数据处理，维护数据集中的全局状态，并通过
    Spark UI 可视化处理任务。同时，scikit-learn 容器不支持共享全局状态，因此每个处理节点独立运行。可以通过将数据集分割成子数据集并独立处理每个子数据集，进行有限的任务协调。
- en: 'You can also provide a **Bring-Your-Own** (**BYO**) processing container with
    virtually any runtime configuration to run SageMaker Processing. This flexibility
    allows you to easily move your existing processing code so that it can run on
    SageMaker Processing with minimal effort:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以提供一个**自带容器**（**BYO**）处理容器，几乎可以使用任何运行时配置来运行 SageMaker Processing。这个灵活性使你能够轻松地将现有的处理代码迁移到
    SageMaker Processing 上，几乎无需任何额外努力：
- en: '![Figure 4.2 – SageMaker Processing node ](img/B17519_04_02.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – SageMaker Processing 节点](img/B17519_04_02.jpg)'
- en: Figure 4.2 – SageMaker Processing node
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – SageMaker Processing 节点
- en: Let’s try to build a container for processing and run a multi-node processing
    job to augment the image dataset for further training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个处理容器，并运行一个多节点处理任务，以增强图像数据集以便进一步训练。
- en: Augmenting image data using SageMaker Processing
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Processing 增强图像数据
- en: 'In this example, we will download the 325 Bird Species dataset from Kaggle
    ([https://www.kaggle.com/gpiosenka/100-bird-species/](https://www.kaggle.com/gpiosenka/100-bird-species/)).
    Then, we will augment this dataset with modified versions of the images (rotated,
    cropped, resized) to improve the performance of downstream image classification
    tasks. For image transformation, we will use the Keras library. Then, we will
    run our processing job on multiple nodes to speed up our job. Follow these steps:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将从 Kaggle 下载 325 种鸟类数据集([https://www.kaggle.com/gpiosenka/100-bird-species/](https://www.kaggle.com/gpiosenka/100-bird-species/))。然后，我们将通过修改图像（旋转、裁剪、调整大小）来增强这个数据集，以提高后续图像分类任务的性能。为了进行图像转换，我们将使用
    Keras 库。接下来，我们将在多个节点上运行处理作业，以加速任务的执行。请按照以下步骤操作：
- en: 'We will start by building a custom processing container. Note that SageMaker
    runs processing containers using the `docker run image_uri` command, so we need
    to specify the entry point in our Dockerfile. We are using the official Python
    3.7 container with a basic Debian version:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从构建自定义处理容器开始。请注意，SageMaker 使用 `docker run image_uri` 命令运行处理容器，因此我们需要在 Dockerfile
    中指定入口点。我们使用官方的 Python 3.7 容器，并配备基本的 Debian 版本：
- en: '[PRE18]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will start by building a custom processing container.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从构建自定义处理容器开始。
- en: 'Now, we need to provide our processing code. We will use `keras.utils` to load
    the original dataset into memory and specify the necessary transformations:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要提供我们的处理代码。我们将使用 `keras.utils` 将原始数据集加载到内存中，并指定必要的转换：
- en: '[PRE19]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since the Keras generator operates in memory, we need to save the generated
    images to disk:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 Keras 生成器在内存中操作，我们需要将生成的图像保存到磁盘：
- en: '[PRE20]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We save the augmented images in a similar directory hierarchy, where labels
    are defined by directory name.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将增强后的图像保存在类似的目录结构中，其中标签由目录名称定义。
- en: 'Once we have the BYO container and processing code, we are ready to schedule
    the processing job. First, we need to instantiate the `Processor` object with
    basic job configurations, such as the number and type of instances and container
    images:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们拥有了 BYO 容器和处理代码，就可以准备调度处理作业。首先，我们需要用基本的作业配置实例化 `Processor` 对象，例如实例的数量和类型以及容器镜像：
- en: '[PRE21]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To start the job, we must execute the `.run()` method. This method allows us
    to provide additional configuration parameters. For instance, to distribute tasks
    evenly, we need to split datasets into chunks. This is easy to do using the `ShardedByKey`
    distribution type. In this case, SageMaker will attempt to evenly distribute objects
    between our processing nodes. SageMaker Processing allows you to pass your custom
    script configuration via the `arguments` collection. You will need to make sure
    that your processing script can parse these command-line arguments properly:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动作业，我们必须执行 `.run()` 方法。此方法允许我们提供额外的配置参数。例如，为了均匀分配任务，我们需要将数据集拆分为多个块。使用 `ShardedByKey`
    分发类型可以轻松实现这一点。在这种情况下，SageMaker 将尝试在我们的处理节点之间均匀分配对象。SageMaker Processing 允许你通过
    `arguments` 集合传递自定义脚本配置。你需要确保处理脚本能够正确解析这些命令行参数：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For the full processing code, please refer to [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 有关完整的处理代码，请参考[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/2_sources/processing.py)。
- en: This example should give you an intuition of how SageMaker Processing can be
    used for your data processing needs. At the same time, SageMaker Processing is
    flexible enough to run any arbitrary tasks, such as batch inference, data aggregation
    and analytics, and others.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例应能帮助你直观了解 SageMaker Processing 如何用于数据处理需求。同时，SageMaker Processing 足够灵活，可以运行任何任意任务，比如批量推断、数据聚合与分析等。
- en: In the next section, we will discuss how to optimize data storage and retrieval
    for large DL datasets.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论如何优化大型深度学习数据集的数据存储和检索。
- en: Optimizing data storage and retrieval
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化数据存储和检索
- en: When training **SOTA DL** models, you typically need a large dataset for a model
    to train. It can be expensive to store and retrieve such large datasets. For instance,
    the popular computer vision dataset **COCO2017** is approximately 30 GB, while
    the **Common Crawl** dataset for NLP tasks has a size of hundreds of TB. Dealing
    with such large datasets requires careful consideration of where to store the
    dataset and how to retrieve it at inference or training time. In this section,
    we will discuss some of the optimization strategies you can use when choosing
    storage and retrieval strategies.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练**SOTA DL**模型时，通常需要一个大型数据集来进行训练。存储和检索这样的大型数据集可能会很昂贵。例如，流行的计算机视觉数据集**COCO2017**大约有
    30 GB，而用于 NLP 任务的**Common Crawl**数据集则有数百 TB。处理如此庞大的数据集需要仔细考虑存储数据集的位置以及如何在推理或训练时检索它。在本节中，我们将讨论一些优化策略，帮助您在选择存储和检索策略时做出决策。
- en: Choosing a storage solution
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择存储解决方案
- en: 'When choosing an optimal storage solution, you may consider the following factors,
    among others:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最优存储方案时，您可以考虑以下因素，除此之外还有其他因素：
- en: The cost of storage and data retrieval
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和数据检索的费用
- en: The latency and throughput requirements for data retrieval
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据检索的延迟和吞吐量要求
- en: Data partitioning
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分区
- en: How frequently data is refreshed
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据刷新频率
- en: 'Let’s take a look at the pros and cons of various storage solutions:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下各种存储解决方案的优缺点：
- en: '**Amazon S3** provides the cheapest storage solution among those considered.
    However, you should be aware that Amazon S3 also charges for data transfer and
    data requests. In scenarios where your dataset consists of a large number of small
    files, you may incur considerable costs associated with **PUT** and **GET** records.
    You may consider batching small objects into large objects to reduce this cost.
    Note that there are additional charges involved when retrieving data stored in
    another AWS region. It could be reasonable to collocate your workload and data
    in the same AWS region to avoid these costs. S3 is also generally the slowest
    storage solution. By default, Amazon SageMaker downloads all objects from S3 before
    the training begins. This initial download time can take minutes and will add
    to the general training time. For instance, in the case of the **COCO2017** dataset,
    it takes ~20 minutes to download it on training nodes from S3\. Amazon provides
    several mechanisms to stream data from S3 and eliminates download time. We will
    discuss these in this section.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3** 提供了在考虑的存储方案中最便宜的选择。然而，您应该意识到，Amazon S3 还会对数据传输和数据请求收取费用。在您的数据集包含大量小文件的情况下，您可能会因为**PUT**和**GET**操作记录而产生相当可观的费用。您可以考虑将小对象批量合并成大对象以减少这部分费用。请注意，从另一个
    AWS 区域检索数据时会产生额外的费用。将工作负载和数据放在同一 AWS 区域可能是避免这些费用的合理选择。S3 通常也是最慢的存储解决方案。默认情况下，Amazon
    SageMaker 会在训练开始之前从 S3 下载所有对象。这个初始下载时间可能需要几分钟，并会增加整体训练时间。例如，在**COCO2017**数据集的情况下，从
    S3 下载到训练节点大约需要 20 分钟。亚马逊提供了几种机制，可以直接从 S3 流式传输数据，避免下载时间。我们将在本节中讨论这些机制。'
- en: '**Amazon EFS** storage is generally more expensive than Amazon S3\. However,
    unlike Amazon S3, Amazon EFS doesn’t have any costs associated with read and write
    operations. Since EFS provides a filesystem interface, compute nodes can directly
    mount to the EFS directory that contains the dataset and use it immediately without
    the need to download the dataset. Amazon EFS provides an easy mechanism to share
    reusable datasets between workloads or teams.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon EFS**存储通常比 Amazon S3 更昂贵。然而，与 Amazon S3 不同，Amazon EFS 不会对读写操作收取任何费用。由于
    EFS 提供了文件系统接口，计算节点可以直接挂载到包含数据集的 EFS 目录，并立即使用它，无需下载数据集。Amazon EFS 提供了一个便捷的机制，让不同的工作负载或团队之间共享可重用的数据集。'
- en: '**Amazon FSx for Lustre** provides the lowest latency for data retrieval but
    also the most expensive storage price. Like Amazon EFS, it doesn’t require any
    download time. One of the common use cases for scenarios is to store your data
    in S3\. When you need to run your set of experiments, you can provision FSx for
    Lustre with synchronization from S3, which seamlessly copies data from S3 to your
    filesystem. After that, you can run your experiments and use FSx for Lustre as
    a data source, leveraging the lowest latency for data retrieval. Once experimentation
    is done, you can de-provision the Lustre filesystem to avoid any additional costs
    and keep the original data in S3.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon FSx for Lustre** 提供了最低的延迟，但也具有最高的存储成本。像 Amazon EFS 一样，它不需要任何下载时间。常见的使用场景之一是将数据存储在
    S3 中。当你需要运行实验集时，可以通过从 S3 同步来配置 FSx for Lustre，这样可以无缝地将数据从 S3 复制到你的文件系统中。之后，你可以运行实验，并将
    FSx for Lustre 作为数据源，利用最低的延迟进行数据检索。实验完成后，可以解除配置 Lustre 文件系统，以避免任何额外的成本，同时将原始数据保留在
    S3 中。'
- en: '**SageMaker Feature Store** has the most out-of-the-box ML-specific features;
    however, it has its shortcomings and strong assumptions. Since its offline storage
    is backed by S3, it has a similar cost structure and latency considerations. Online
    storage adds additional storage, read, and write costs. SageMaker Feature Store
    fits well into scenarios when you need to reuse the same dataset for inference
    and training workloads. Another popular use case for Feature Store is when you
    need to have audit requirements or run analytical queries against your datasets.
    Note that since Feature Store supports only a limited amount of data types (for
    example, it doesn’t support any collections), you may need to do type casting
    when consuming data from Feature Store.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker 特征存储**具有最全面的机器学习特定功能；但是，它也有其缺点和强假设。由于其离线存储由 S3 支持，因此它具有类似的成本结构和延迟考虑。在线存储会增加额外的存储、读取和写入成本。SageMaker
    特征存储适合在需要重用相同数据集进行推理和训练工作负载的场景中。特征存储的另一个常见用例是当你需要进行审计要求或对数据集进行分析查询时。请注意，由于特征存储仅支持有限的数据类型（例如，它不支持任何集合类型），因此在从特征存储消费数据时，可能需要进行类型转换。'
- en: AWS provides a wide range of storage solutions and at times, it may not be obvious
    which solution to choose. As always, it’s important to start by understanding
    your use case requirements and success criteria (for instance, lowest possible
    latency, highest throughput, or most cost-optimal solution).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了广泛的存储解决方案，有时选择哪种解决方案可能不太明显。像往常一样，重要的是从理解你的使用案例需求和成功标准开始（例如，最低的延迟、最高的吞吐量或最具成本效益的解决方案）。
- en: Streaming datasets
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式数据集
- en: Amazon S3 is a popular storage solution for large ML datasets, given its low
    cost, high durability, convenient API, and integration with other services, such
    as SageMaker. As we discussed in the previous section, one of the downsides of
    using S3 to store training datasets is that you need to download the dataset to
    your training nodes before training can start.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3 是一个流行的大型机器学习数据集存储解决方案，因其低成本、高耐用性、便捷的 API 以及与其他服务（如 SageMaker）的集成。在前面讨论的部分中，我们提到使用
    S3 存储训练数据集的一个缺点是，训练开始之前需要将数据集下载到训练节点。
- en: You can choose to use the **ShardedByKey** distribution strategy, which will
    reduce the amount of data downloaded to each training node. However, that approach
    only reduces the amount of data that needs to be downloaded to your training nodes.
    For large datasets (100s+ GB), it solves the problem only partially. You will
    also need to ensure that your training nodes have enough EBS volume capacity to
    store data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择使用 **ShardedByKey** 分发策略，这将减少每个训练节点下载的数据量。然而，这种方法仅减少了需要下载到训练节点的数据量。对于大型数据集（100
    GB 以上），它仅部分解决了问题。你还需要确保训练节点有足够的 EBS 卷容量来存储数据。
- en: 'An alternative approach to reduce training time is to stream data from Amazon
    S3 without downloading it upfront. Several implementations of S3 data streaming
    are provided by Amazon SageMaker:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 减少训练时间的另一种方法是直接从 Amazon S3 流式传输数据，而不是事先下载数据。Amazon SageMaker 提供了几种 S3 数据流式传输的实现方式：
- en: Framework-specific streaming implementations, such as `PipeModeDataset` for
    TensorFlow and Amazon S3 Plugin for PyTorch
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对特定框架的流式传输实现，例如 TensorFlow 的 `PipeModeDataset` 和 PyTorch 的 Amazon S3 插件
- en: Framework-agnostic FastFile mode
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框架无关的 FastFile 模式
- en: Let’s review the benefits of these approaches.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些方法的优点。
- en: PipeModeDataset for TensorFlow
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 的 PipeModeDataset
- en: '`PipeModeDataset`, your training program can read from S3 without managing
    access to S3 objects. When using `PipeModeDataset`, you need to ensure that you
    are using a matching version of TensorFlow.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`PipeModeDataset`时，你的训练程序可以直接从S3读取数据，而无需管理对S3对象的访问。在使用`PipeModeDataset`时，需确保你使用的是与之匹配的TensorFlow版本。
- en: SageMaker Pipe mode is enabled when configuring a SageMaker training job. You
    can map multiple datasets to a single pipe if you’re storing multiple datasets
    under the same S3 path. Note that SageMaker supports up to 20 pipes. If you need
    more than 20 pipes, you may consider using Augmented Manifest files, which allow
    you to explicitly list a set of S3 objects to be streamed. During training, SageMaker
    will read objects from the manifest file and stream them into the pipe.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置SageMaker训练作业时启用SageMaker Pipe模式。如果你将多个数据集存储在同一个S3路径下，可以将它们映射到单个管道。请注意，SageMaker最多支持20个管道。如果需要超过20个管道，你可以考虑使用增强的清单文件，它允许你显式列出一组S3对象进行流式传输。在训练过程中，SageMaker将从清单文件中读取对象并将它们流式传输到管道中。
- en: '`PipeModeDataset` supports the following dataset formats: text line, RecordIO,
    and TFRecord. If you have a dataset in a different format (for instance, as separate
    image files) you will have to convert your dataset. Note that the performance
    of `PipeModeDataset` performance on the number and size of the files. It’s generally
    recommended to keep the file size around 100 to 200 MB for optimal performance.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`PipeModeDataset`支持以下数据集格式：文本行、RecordIO和TFRecord。如果你有其他格式的数据集（例如单独的图像文件），则需要将数据集转换为支持的格式。请注意，`PipeModeDataset`的性能受到文件数量和大小的影响。通常建议将文件大小保持在100到200
    MB之间，以获得最佳性能。'
- en: Note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since `PipeModeDataset` implements the TensorFlow Dataset API, you can use familiar
    methods to manipulate your datasets, such as `.apply(), .map()`. `PipeModeDataset`
    also can be passed to TensorFlow Estimator directly.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`PipeModeDataset`实现了TensorFlow Dataset API，你可以使用熟悉的方法来操作数据集，例如`.apply()`、`.map()`。`PipeModeDataset`也可以直接传递给TensorFlow
    Estimator。
- en: 'There are several differences between `PipeModeDataset` and TensorFlow Dataset
    that you should consider:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`PipeModeDataset`与TensorFlow Dataset之间有几个不同之处，你需要考虑以下几点：'
- en: '`PipeModeDataset` reads data sequentially in files. SageMaker supports the
    `ShuffleConfig` ([https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml))
    parameter, which shuffles the order of the files to read. You can also call the
    `.shuffle()` method to further shuffle records.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PipeModeDataset`按顺序从文件中读取数据。SageMaker支持`ShuffleConfig`（[https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ShuffleConfig.xhtml)）参数，用于打乱读取文件的顺序。你还可以调用`.shuffle()`方法进一步打乱记录顺序。'
- en: '`PipeModeDataset` supports only three data types, all of which require data
    to be converted into one of the supported formats.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PipeModeDataset`仅支持三种数据类型，所有数据都需要转换为支持的格式之一。'
- en: '`PipeModeDataset` has limited controls when it comes to manipulating data at
    training time. For instance, if you need to boost the underrepresented class in
    your classification dataset, you will need to use a separate pipe to stream samples
    of the underrepresented file and handle the boosting procedure in your training
    script.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PipeModeDataset`在训练时操作数据的控制功能有限。例如，如果你需要增强分类数据集中低频类别的样本，你将需要使用单独的管道来流式传输低频类别的样本，并在训练脚本中处理增强过程。'
- en: '`PipeModeDataset` doesn’t support SageMaker Local mode, so it can be tricky
    to debug your training program. When using SageMaker Pipe mode, you don’t have
    access to the internals of how SageMaker streams your data objects into pipes.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PipeModeDataset`不支持SageMaker本地模式，因此调试训练程序可能会比较困难。在使用SageMaker Pipe模式时，你无法访问SageMaker如何将数据对象流式传输到管道中的内部实现。'
- en: 'Let’s look at how `PipeModeDataset` can be used. In this example, for training
    purposes, we will convert the CIFAR-100 dataset into TFRecords and then stream
    this dataset at training time using `PipeModeDataset`. We will provide a redacted
    version for brevity instead of listing the entire example. The full source is
    available at [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb).
    Follow these steps:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何使用`PipeModeDataset`。在这个例子中，为了训练目的，我们将CIFAR-100数据集转换为TFRecords，然后在训练时通过`PipeModeDataset`流式传输此数据集。为了简洁起见，我们将提供一个编辑版，而不是列出整个例子。完整源代码可以在[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter4/3_Streaming_S3_Data.ipynb)查看。请按照以下步骤操作：
- en: 'Let’s start by converting our dataset into TFRecord format. In the following
    code block, there is a method that iterates over a batch of files, converts a
    pair of images and labels into a TensorFlow `Example` class, and writes a batch
    of `Example` objects into a single `TFRecord` file:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先将数据集转换为TFRecord格式。在以下代码块中，包含一个方法，该方法遍历一批文件，将一对图像和标签转换为TensorFlow的`Example`类，并将一批`Example`对象写入一个单独的`TFRecord`文件中：
- en: '[PRE23]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have our datasets in TFRecord format, we need to create our training
    script. It will largely follow a typical TensorFlow training script, with the
    only difference being that we will use `PipeModeDataset` instead of `TFRecordDataset`.
    You can use the following code to configure `PipeModeDataset`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集被转换为TFRecord格式，我们需要创建训练脚本。它将主要遵循一个典型的TensorFlow训练脚本，唯一的区别是我们将使用`PipeModeDataset`而不是`TFRecordDataset`。你可以使用以下代码来配置`PipeModeDataset`：
- en: '[PRE24]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When configuring the SageMaker training job, we need to explicitly specify
    that we want to use Pipe mode:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置SageMaker训练作业时，我们需要明确指定要使用Pipe模式：
- en: '[PRE25]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that since the CIFAF100 dataset is relatively small, you may be not able
    to see any considerable decrease in the training start time. However, with bigger
    datasets such as COCO2017, you can expect the training time to reduce by at least
    several minutes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于CIFAR100数据集相对较小，你可能无法看到训练开始时间的明显减少。然而，像COCO2017这样的较大数据集，训练时间至少会减少几分钟。
- en: Amazon S3 Plugin for PyTorch
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon S3插件为PyTorch提供了支持
- en: Amazon S3 Plugin for PyTorch allows you to stream data directly from S3 objects
    with minimal changes to your existing PyTorch training script. Under the hood,
    S3 Plugin uses `TransferManager` from the AWS SDK for C++ to fetch files from
    S3 and utilizes S3 multipart download for optimal data throughput and reliability.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3插件为PyTorch提供了一个功能，允许你通过最少的更改直接从S3对象流式传输数据到现有的PyTorch训练脚本中。在底层，S3插件使用AWS
    SDK for C++中的`TransferManager`从S3获取文件，并利用S3的分段下载功能来优化数据吞吐量和可靠性。
- en: 'S3 Plugin provides two implementations of PyTorch dataset APIs: a map-style
    `S3Dataset` and an iterable-style `S3IterableDataset`. In the following section,
    we will discuss when to use one or another.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: S3插件提供了两种PyTorch数据集API的实现：Map风格的`S3Dataset`和Iterable风格的`S3IterableDataset`。在接下来的部分中，我们将讨论何时使用其中之一。
- en: Map-style S3Dataset
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Map风格的S3Dataset
- en: '`S3Dataset` represents a mapping of indexes and data records and implements
    the `__getitem__()` method. It allows you to randomly access data records based
    on their indices. A map-style dataset works best when each file has a single data
    record. You can use PyTorch’s distributed sampler to further partition the dataset
    between training nodes.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`S3Dataset`表示索引和数据记录的映射，并实现了`__getitem__()`方法。它允许你根据索引随机访问数据记录。当每个文件包含一个数据记录时，Map风格的数据集效果最佳。你可以使用PyTorch的分布式采样器进一步将数据集在训练节点之间进行划分。'
- en: 'Here is an example of using `S3Dataset` for images stored on S3:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用`S3Dataset`访问存储在S3上的图像的一个例子：
- en: 'First, we will define the dataset class that inherits from the parent `S3Dataset`.
    Then, we will define the data processing pipeline using PyTorch functions:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个继承自父类`S3Dataset`的数据集类。然后，我们将使用PyTorch函数定义数据处理管道：
- en: '[PRE26]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will create a PyTorch-native `Dataloader` object that can be passed
    to any training script:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个PyTorch原生的`Dataloader`对象，可以将其传递给任何训练脚本：
- en: '[PRE27]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Iterable-style S3IterableDataset
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Iterable风格的S3IterableDataset
- en: '`S3IterableDataset` represents iterable objects and implements Python’s `__iter__()`
    method. Generally, you use an iterable-style dataset when random reads (such as
    in a map-style dataset) are expensive or impossible. You should use an iterable-style
    dataset when you have a batch of data records stored in a single file object.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`S3IterableDataset`表示可迭代对象，并实现了Python的`__iter__()`方法。通常，当随机读取（例如在映射风格数据集中的读取）代价高昂或不可能时，你会使用可迭代风格的数据集。当你有一批数据记录存储在单个文件对象中时，应该使用可迭代风格的数据集。'
- en: When using `S3IterableDataset`, it’s important to control your file sizes. If
    your dataset is represented by a large number of files, accessing each file will
    come with overhead. In such scenarios, it’s recommended to merge data records
    into larger file objects.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`S3IterableDataset`时，重要的是要控制文件的大小。如果你的数据集由大量文件表示，访问每个文件都会带来开销。在这种情况下，建议将数据记录合并成更大的文件对象。
- en: '`S3IterableDataset` doesn’t restrict what file types can be used. A full binary
    blob of the file object is returned, and you are responsible to provide parsing
    logic. You can shuffle the URLs of file objects by setting the `shuffle_urls`
    flag to true. Note that if you need to shuffle records within the same data objects,
    you can use `ShuffleDataset` accumulates data records across multiple file objects
    and returns a random sample from it.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`S3IterableDataset`不限制可以使用的文件类型。返回的是文件对象的完整二进制数据块，你需要提供解析逻辑。你可以通过将`shuffle_urls`标志设置为true来打乱文件对象的URL。注意，如果你需要在同一数据对象内打乱记录，可以使用`ShuffleDataset`，它会跨多个文件对象汇总数据记录，并从中返回一个随机样本。'
- en: '`S3IterableDataset` takes care of sharding data between training nodes when
    running distributed training. You can wrap `S3IterableDataset` with PyTorch’s
    `DataLoader` for parallel data loading and pre-processing.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`S3IterableDataset`会在进行分布式训练时处理数据的分片问题。你可以将`S3IterableDataset`与PyTorch的`DataLoader`结合使用，以实现并行数据加载和预处理。'
- en: 'Let’s look at an example of how to construct an iterable-style dataset from
    several TAR archives stored on S3 and apply data transformations:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，如何从存储在S3上的多个TAR档案构建一个可迭代风格的数据集并应用数据转换：
- en: 'We will start by defining a custom dataset class using PyTorch’s native `IterableDataset`.
    As part of the class definition, we use `S3IterableDataset` to fetch data from
    S3 and data transformations that will be applied to individual data records:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从使用PyTorch的原生`IterableDataset`定义一个自定义数据集类开始。在类定义的过程中，我们使用`S3IterableDataset`从S3获取数据，并应用到单个数据记录的转换：
- en: '[PRE28]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we define a transformation to normalize images and then instantiate a
    dataset instance with the ability to stream images from S3:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个转换操作来对图像进行归一化，然后实例化一个数据集实例，具备从S3流式传输图像的能力：
- en: '[PRE29]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, let’s look at FastFile mode.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看FastFile模式。
- en: FastFile mode
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FastFile模式
- en: In late 2021, Amazon announced a new approach for streaming data directly from
    S3 called FastFile mode. It combines the benefits of streaming data from S3 with
    the convenience of working with local files. In **FastFile** mode, each file will
    appear to your training program as a POSIX filesystem mount. Hence, it will be
    indistinguishable from any other local files, such as the ones stored on the mounted
    EBS volume.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年底，亚马逊宣布了一种新的从S3直接流式传输数据的方法，称为FastFile模式。它结合了从S3流式传输数据的优势以及操作本地文件的便利性。在**FastFile**模式下，每个文件会作为POSIX文件系统挂载出现在你的训练程序中。因此，它与任何其他本地文件（例如存储在挂载的EBS卷上的文件）没有区别。
- en: When reading file objects in FastFile mode, SageMaker retrieves chunks of the
    file if the file format supports chunking; otherwise, a full file is retrieved.
    FastFile mode performs optimally if data is read sequentially. Please note that
    there is an additional overhead on retrieving each file object. So, fewer files
    will usually result in a lower startup time for your training job.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在FastFile模式下读取文件对象时，如果文件格式支持分块，SageMaker将检索文件的部分内容；否则，将检索完整的文件。如果数据按顺序读取，FastFile模式的性能最佳。请注意，检索每个文件对象会有额外的开销。因此，文件数量较少通常会导致训练作业的启动时间较短。
- en: 'Compared to the previously discussed framework-specific streaming plugins,
    FastFile mode has several benefits:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前讨论的特定于框架的流式插件相比，FastFile模式有几个优势：
- en: Avoids any framework-specific implementations for data streaming. You can use
    your PyTorch or TensorFlow native data utilities and share datasets between frameworks.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免了任何特定于框架的数据流式传输实现。你可以使用PyTorch或TensorFlow的原生数据工具，并在框架间共享数据集。
- en: As a result, you have more granular control over data inputs using your framework
    utilities to perform operations such as shuffling, dynamic boosting, and data
    processing *on the fly*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，你可以通过使用框架实用工具对数据输入进行更精细的控制，执行诸如打乱、动态增强和实时数据处理等操作。
- en: There are no restrictions on the file format.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件格式没有限制。
- en: It is easier to debug your training program as you can use SageMaker Local mode
    to test and debug your program locally first.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试你的训练程序会更容易，因为你可以使用SageMaker本地模式先在本地测试和调试程序。
- en: 'To use FastFile mode, you need to supply an appropriate `input_mode` value
    when configuring your SageMaker `Estimator` object. The following code shows an
    example of a TensorFlow training job:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用FastFile模式，在配置SageMaker的`Estimator`对象时，你需要提供一个合适的`input_mode`值。以下代码展示了一个TensorFlow训练作业的例子：
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: FastFile mode can be a good starting choice, given its ease of use and versatility.
    If, for some reason, you are not happy with its performance, you can always consider
    tuning the configuration of your dataset (file format, file size, data processing
    pipeline, parallelism, and so on) or reimplement the use of one of the framework-specific
    implementations. It may also be a good idea to compare FastFile mode’s performance
    of streaming data from S3 using other methods such as Pipe mode and S3 Plugin
    for PyTorch.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于FastFile模式易于使用且具有多功能性，因此可以作为一个很好的起点。如果出于某种原因，你对它的性能不满意，你始终可以考虑调整数据集的配置（如文件格式、文件大小、数据处理管道、并行度等）或重新实现框架特定的实现方式。你也可以比较FastFile模式与其他方法（如Pipe模式和PyTorch的S3插件）从S3流式传输数据的性能。
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we reviewed the available storage solutions for storing and
    managing DL datasets and discussed their pros and cons in detail, along with their
    usage scenarios. We walked through several examples of how to integrate your SageMaker
    training scripts with different storage services. Later, we learned about various
    optimization strategies for storing data and discussed advanced mechanisms for
    optimizing data retrieval for training tasks. We also looked into SageMaker Processing
    and how it can be used to scale your data processing efficiently.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们回顾了可用于存储和管理深度学习数据集的存储解决方案，并详细讨论了它们的优缺点及使用场景。我们通过多个示例展示了如何将SageMaker训练脚本与不同的存储服务集成。随后，我们了解了存储数据的各种优化策略，并讨论了优化训练任务数据检索的高级机制。我们还探讨了SageMaker处理服务，以及如何有效地扩展数据处理。
- en: This chapter closes the first part of this book, which served as an introduction
    to using DL models on SageMaker. Now, we will move on to advanced topics. In the
    next chapter, we will discuss the advanced training capabilities that SageMaker
    offers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了本书的第一部分，该部分介绍了如何在SageMaker上使用深度学习模型。现在，我们将进入高级主题。在下一章中，我们将讨论SageMaker提供的高级训练功能。
- en: 'Part 2: Building and Training Deep Learning Models'
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：构建和训练深度学习模型
- en: In this part, we will learn how to train DL models using SageMaker-managed capabilities,
    outlining available software frameworks to distribute training processes across
    many nodes, optimizing hardware utilization, and monitoring your training jobs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何使用SageMaker管理的功能训练深度学习模型，概述了可用的软件框架，以便在多个节点之间分布训练过程，优化硬件使用，并监控你的训练作业。
- en: 'This section comprises the following chapters:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 5*](B17519_05.xhtml#_idTextAnchor083), *Considering Hardware for
    Deep Learning Training*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B17519_05.xhtml#_idTextAnchor083)，*考虑硬件在深度学习训练中的作用*'
- en: '[*Chapter 6*](B17519_06.xhtml#_idTextAnchor097), *Engineering Distributed Training*'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B17519_06.xhtml#_idTextAnchor097)，*分布式训练的工程化*'
- en: '[*Chapter 7*](B17519_07.xhtml#_idTextAnchor110), *Operationalizing Deep Learning
    Training*'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B17519_07.xhtml#_idTextAnchor110)，*深度学习训练的操作化*'
