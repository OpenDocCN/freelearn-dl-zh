- en: Temporal Difference Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ—¶åºå·®åˆ†å­¦ä¹ 
- en: '**Temporal difference** (**TD**) learning algorithms are based on reducing
    the differences between estimates that are made by the agent at different times.
    It is a combination of the ideas of the **Monte Carlo**Â (**MC**)Â methodÂ and **dynamic
    programming** (**DP**). The algorithmÂ can learn directly from raw data, without
    a model of the dynamics of the environment (like MC). Update estimates are based,
    in part, on other learned estimates, without waiting for the result (bootstrap,
    like DP). In this chapter, we will learn how to use TD learning algorithms to
    resolve the vehicle routing problem.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ—¶åºå·®åˆ†** (**TD**) å­¦ä¹ ç®—æ³•åŸºäºå‡å°‘ä»£ç†åœ¨ä¸åŒæ—¶é—´åšå‡ºçš„ä¼°è®¡ä¹‹é—´çš„å·®å¼‚ã€‚å®ƒæ˜¯ **è’™ç‰¹å¡æ´›** (**MC**) æ–¹æ³•å’Œ **åŠ¨æ€è§„åˆ’**
    (**DP**) æ€æƒ³çš„ç»“åˆã€‚è¯¥ç®—æ³•å¯ä»¥ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€ç¯å¢ƒåŠ¨æ€æ¨¡å‹ï¼ˆå°±åƒ MCï¼‰ã€‚æ›´æ–°ä¼°è®¡éƒ¨åˆ†ä¾èµ–äºå…¶ä»–å·²å­¦å¾—çš„ä¼°è®¡ï¼Œè€Œæ— éœ€ç­‰å¾…ç»“æœï¼ˆè‡ªä¸¾ï¼Œå°±åƒ
    DPï¼‰ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ TD å­¦ä¹ ç®—æ³•æ¥è§£å†³è½¦è¾†è·¯å¾„è§„åˆ’é—®é¢˜ã€‚'
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Understanding TD methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£ TD æ–¹æ³•
- en: Introducing graph theory and its implementation in R
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»‹ç»å›¾è®ºåŠå…¶åœ¨ R ä¸­çš„å®ç°
- en: Implementing TD methods to the vehicle routing problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°† TD æ–¹æ³•åº”ç”¨äºè½¦è¾†è·¯å¾„è§„åˆ’é—®é¢˜
- en: By the end of this chapter, you will have learned about the different types
    of TD learning algorithms and how to use them to predict the future behavior of
    a system. We will learn about the basic concepts of the Q-learning algorithm and
    use them to generate system behavior through the current best policy estimate.
    Finally, we will differentiate between SARSA and the Q-learning approach.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†å­¦ä¹ åˆ°ä¸åŒç±»å‹çš„ TD å­¦ä¹ ç®—æ³•ï¼Œå¹¶äº†è§£å¦‚ä½•ä½¿ç”¨å®ƒä»¬æ¥é¢„æµ‹ç³»ç»Ÿçš„æœªæ¥è¡Œä¸ºã€‚æˆ‘ä»¬å°†å­¦ä¹  Q å­¦ä¹ ç®—æ³•çš„åŸºæœ¬æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨å®ƒä»¬é€šè¿‡å½“å‰æœ€ä¼˜ç­–ç•¥ä¼°è®¡ç”Ÿæˆç³»ç»Ÿè¡Œä¸ºã€‚æœ€åï¼Œæˆ‘ä»¬å°†åŒºåˆ†
    SARSA å’Œ Q å­¦ä¹ æ–¹æ³•ã€‚
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä»¥ä¸‹è§†é¢‘ï¼Œçœ‹çœ‹ä»£ç çš„å®é™…æ“ä½œï¼š
- en: '[http://bit.ly/2YTB7dD](http://bit.ly/2YTB7dD)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2YTB7dD](http://bit.ly/2YTB7dD)'
- en: Understanding TD methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£ TD æ–¹æ³•
- en: TD methods are based on reducing the differences between the estimates that
    are made by the agent at different times. Q-learning, which we will learn about
    in the following section, is a TD algorithm, but it is based on the difference
    between states in immediate adjacent instants. TD is more generic and may consider
    moments and states that are further away.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TD æ–¹æ³•åŸºäºå‡å°‘ä»£ç†åœ¨ä¸åŒæ—¶é—´åšå‡ºçš„ä¼°è®¡ä¹‹é—´çš„å·®å¼‚ã€‚Q å­¦ä¹ æ˜¯ä¸€ä¸ª TD ç®—æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†å­¦ä¹ ï¼Œå®ƒåŸºäºç›¸é‚»æ—¶åˆ»çŠ¶æ€ä¹‹é—´çš„å·®å¼‚ã€‚TD æ–¹æ³•æ›´åŠ é€šç”¨ï¼Œå¯èƒ½ä¼šè€ƒè™‘æ›´è¿œçš„æ—¶åˆ»å’ŒçŠ¶æ€ã€‚
- en: 'TD methods are a combination of the ideas of the MC method and DP, which, as
    you may recall, can be summarized as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TD æ–¹æ³•ç»“åˆäº† MC æ–¹æ³•å’ŒåŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰çš„æ€æƒ³ï¼Œæ­£å¦‚ä½ å¯èƒ½è®°å¾—çš„é‚£æ ·ï¼Œå¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š
- en: MC methods allow us to solve reinforcement learning problems based on the average
    of the results obtained.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MC æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®è·å¾—ç»“æœçš„å¹³å‡å€¼æ¥è§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚
- en: DP represents a set of algorithms that can be used to calculate an optimal policy
    when given a perfect model of the environment in the form of an MDP.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP ä»£è¡¨ä¸€ç»„ç®—æ³•ï¼Œè¿™äº›ç®—æ³•å¯ä»¥åœ¨ç»™å®šç¯å¢ƒçš„å®Œç¾æ¨¡å‹ï¼ˆMDPï¼‰ä¸‹ï¼Œç”¨äºè®¡ç®—æœ€ä¼˜ç­–ç•¥ã€‚
- en: The TD methods, on the one hand, inherit the idea of learning directly from
    the experience accumulated interacting with the system, without the dynamics of
    the system itself, from the Monte Carlo method. While they inherit from the DP
    methods, the idea is to update the estimate of functions in a state from the estimates
    made in other states (bootstrap). TD methods are suitable for learning without
    a model of dynamic environments. You need to converge using a fixed policy if
    the time step is sufficiently small or if it reduces over time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ–¹é¢ï¼ŒTD æ–¹æ³•ç»§æ‰¿äº†ä»ä¸ç³»ç»Ÿäº¤äº’ä¸­ç§¯ç´¯çš„ç»éªŒä¸­ç›´æ¥å­¦ä¹ çš„æ€æƒ³ï¼Œè¿™ä¸è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ–¹æ³•ç±»ä¼¼ï¼Œè€Œä¸éœ€è¦ç³»ç»Ÿæœ¬èº«çš„åŠ¨æ€ä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒä»¬ç»§æ‰¿äº†åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰æ–¹æ³•çš„æ€æƒ³ï¼Œå³åŸºäºå…¶ä»–çŠ¶æ€çš„ä¼°è®¡æ¥æ›´æ–°æŸä¸€çŠ¶æ€ä¸‹çš„å‡½æ•°ä¼°è®¡ï¼ˆè‡ªä¸¾ï¼‰ã€‚TD
    æ–¹æ³•é€‚åˆåœ¨æ²¡æœ‰åŠ¨æ€ç¯å¢ƒæ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ ã€‚å¦‚æœæ—¶é—´æ­¥é•¿è¶³å¤Ÿå°ï¼Œæˆ–è€…éšç€æ—¶é—´çš„æ¨ç§»å‡å°‘ï¼Œä½ éœ€è¦é€šè¿‡ä¸€ä¸ªå›ºå®šç­–ç•¥æ¥æ”¶æ•›ã€‚
- en: Such methods differ from other techniques because they try to minimize the error
    of consecutive time forecasts. To achieve this goal, these methods rewrite the
    update of the value function in the form of a Bellman equation, thereby improving
    the prediction by bootstrapping. Here, the variance of the forecast is reduced
    in each update step. To get a backpropagation of updates in order to save memory,
    an eligibility vector is applied. Example trajectories are used more efficiently,
    resulting in good learning rates.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•ä¸å…¶ä»–æŠ€æœ¯çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä»¬è¯•å›¾æœ€å°åŒ–è¿ç»­æ—¶é—´é¢„æµ‹çš„è¯¯å·®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œè¿™äº›æ–¹æ³•å°†ä»·å€¼å‡½æ•°çš„æ›´æ–°é‡å†™ä¸ºè´å°”æ›¼æ–¹ç¨‹çš„å½¢å¼ï¼Œä»è€Œé€šè¿‡è‡ªä¸¾æ³•æé«˜é¢„æµ‹ç²¾åº¦ã€‚åœ¨è¿™é‡Œï¼Œæ¯æ¬¡æ›´æ–°æ­¥éª¤éƒ½ä¼šå‡å°‘é¢„æµ‹çš„æ–¹å·®ã€‚ä¸ºäº†å®ç°æ›´æ–°çš„åå‘ä¼ æ’­å¹¶èŠ‚çœå†…å­˜ï¼Œé‡‡ç”¨äº†èµ„æ ¼å‘é‡ã€‚ç¤ºä¾‹è½¨è¿¹çš„ä½¿ç”¨æ•ˆç‡æ›´é«˜ï¼Œä»è€Œè·å¾—äº†è‰¯å¥½çš„å­¦ä¹ é€Ÿç‡ã€‚
- en: 'The methods based on time differences allow us to manage the problem of control
    (that is, to search for the optimal policy) by letting us update the value functions
    based on the results of the transition to the next state. At every step, the function
    *Q* (action-value function) is updated based on the value it has assumed for the
    next state-action pair and the reward that''s obtained through the following equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ—¶é—´å·®å¼‚çš„æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡æ ¹æ®å‘ä¸‹ä¸€ä¸ªçŠ¶æ€è¿‡æ¸¡çš„ç»“æœæ¥æ›´æ–°ä»·å€¼å‡½æ•°ï¼Œä»è€Œç®¡ç†æ§åˆ¶é—®é¢˜ï¼ˆå³å¯»æ‰¾æœ€ä¼˜ç­–ç•¥ï¼‰ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œå‡½æ•°*Q*ï¼ˆè¡ŒåŠ¨-ä»·å€¼å‡½æ•°ï¼‰åŸºäºå®ƒä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹æ‰€å‡å®šçš„å€¼ä»¥åŠé€šè¿‡ä»¥ä¸‹æ–¹ç¨‹è·å¾—çš„å¥–åŠ±æ¥æ›´æ–°ï¼š
- en: '![](img/4fc3644e-c504-4a0e-a6cb-af4305e45efe.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fc3644e-c504-4a0e-a6cb-af4305e45efe.png)'
- en: 'By adopting a one-step look-ahead, it is clear that a two-step formula can
    also be used, as shown in the following formula:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é‡‡ç”¨ä¸€æ­¥å‰ç»ï¼Œå¾ˆæ˜æ˜¾ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä¸¤æ­¥å…¬å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/099626e5-5156-4773-a468-7559189b2dab.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/099626e5-5156-4773-a468-7559189b2dab.png)'
- en: 'The term look-ahead specifies the procedure that tries to predict the effects
    of choosing a branching variable in the evaluation of one of its values. This
    procedure has the following purposes: to choose a variable to be evaluated later
    and to evaluate the order of the values to be assigned to it.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¯è¯­â€œå‰ç»â€æŒ‡çš„æ˜¯ä¸€ç§è¯•å›¾é¢„æµ‹åœ¨è¯„ä¼°æŸä¸ªå€¼æ—¶é€‰æ‹©ä¸€ä¸ªåˆ†æ”¯å˜é‡çš„æ•ˆæœçš„è¿‡ç¨‹ã€‚è¯¥è¿‡ç¨‹æœ‰ä»¥ä¸‹ç›®çš„ï¼šé€‰æ‹©ä¸€ä¸ªå˜é‡ç¨åè¿›è¡Œè¯„ä¼°ï¼Œå¹¶è¯„ä¼°åˆ†é…ç»™å®ƒçš„å€¼çš„é¡ºåºã€‚
- en: 'More generally, with *n*-step look-ahead, we obtain the following formula:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°è¯´ï¼Œé€šè¿‡*n*æ­¥å‰ç»ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹å…¬å¼ï¼š
- en: '![](img/6807f18d-87c3-400d-b7e9-3eb6a25f8909.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6807f18d-87c3-400d-b7e9-3eb6a25f8909.png)'
- en: An aspect of characterizing the different types of algorithms based on temporal
    difference is the methodology of choosing an action. There are "on-policy"Â methods,Â in
    which the update is made based on the results of actions that have been determined
    by the selected policy, and "off-policy" methods, in which various policies can
    be assessed through hypothetical actions, that aren't actually undertaken. Unlike
    "on-policy" methods, the latter can separate the problem of exploration from that
    of control, and learning tactics aren't necessarily applied during the learning
    phase.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ—¶é—´å·®å¼‚çš„ä¸åŒç±»å‹ç®—æ³•çš„ä¸€ä¸ªç‰¹å¾æ˜¯é€‰æ‹©è¡ŒåŠ¨çš„æ–¹æ³•ã€‚æœ‰â€œåœ¨ç­–ç•¥â€æ–¹æ³•ï¼Œå…¶ä¸­æ›´æ–°åŸºäºç”±æ‰€é€‰ç­–ç•¥ç¡®å®šçš„è¡ŒåŠ¨çš„ç»“æœï¼›è¿˜æœ‰â€œç¦»ç­–ç•¥â€æ–¹æ³•ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œå¯ä»¥é€šè¿‡å‡è®¾çš„è¡ŒåŠ¨è¯„ä¼°ä¸åŒçš„ç­–ç•¥ï¼Œè¿™äº›å‡è®¾çš„è¡ŒåŠ¨å®é™…ä¸Šå¹¶æœªæ‰§è¡Œã€‚ä¸â€œåœ¨ç­–ç•¥â€æ–¹æ³•ä¸åŒï¼Œåè€…å¯ä»¥å°†æ¢ç´¢é—®é¢˜ä¸æ§åˆ¶é—®é¢˜åˆ†ç¦»ï¼Œä¸”å­¦ä¹ ç­–ç•¥åœ¨å­¦ä¹ é˜¶æ®µå¹¶ä¸ä¸€å®šè¢«åº”ç”¨ã€‚
- en: 'In the following sections, we will learn how to implement TD methods through
    two approaches: SARSA and Q-learning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸¤ç§æ–¹æ³•å­¦ä¹ å¦‚ä½•å®ç°æ—¶é—´å·®å¼‚æ–¹æ³•ï¼šSARSAå’ŒQå­¦ä¹ ã€‚
- en: SARSA
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA
- en: As we anticipated in [Chapter 1](2362715d-f8f2-435a-9c00-975ed61986a8.xhtml),Â *Overview
    of Reinforcement Learning with R*, the SARSA algorithm implements an on-policy
    TD method, in which the update of the action-value function (*Q*) is performed
    based on the results of the transition from the state *s = s (t)* to the state
    *s' = s (t + 1)* by the action *a (t)*, which is taken based on a selected policy
    *Ï€ (s, a)*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬1ç« ](2362715d-f8f2-435a-9c00-975ed61986a8.xhtml)ã€Šä½¿ç”¨Rè¿›è¡Œå¼ºåŒ–å­¦ä¹ æ¦‚è¿°ã€‹ä¸­æ‰€é¢„æœŸçš„ï¼ŒSARSAç®—æ³•å®ç°äº†ä¸€ç§åœ¨ç­–ç•¥çš„æ—¶é—´å·®å¼‚æ–¹æ³•ï¼Œå…¶ä¸­ï¼Œè¡ŒåŠ¨-ä»·å€¼å‡½æ•°(*Q*)çš„æ›´æ–°æ˜¯åŸºäºä»çŠ¶æ€*s
    = s (t)*è¿‡æ¸¡åˆ°çŠ¶æ€*s' = s (t + 1)*çš„ç»“æœï¼Œå¹¶ä¸”è¯¥è¿‡æ¸¡æ˜¯åŸºäºæ‰€é€‰ç­–ç•¥*Ï€ (s, a)*é‡‡å–çš„è¡ŒåŠ¨*a (t)*ã€‚
- en: Some policies always choose the action providing the maximum reward and nondeterministic
    policies (Îµ-greedy, Îµ-soft, or softmax), which ensure an element of exploration
    in the learning phase.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç­–ç•¥æ€»æ˜¯é€‰æ‹©æä¾›æœ€å¤§å¥–åŠ±çš„è¡ŒåŠ¨ï¼Œè€Œéç¡®å®šæ€§ç­–ç•¥ï¼ˆå¦‚Îµ-è´ªå¿ƒã€Îµ-è½¯ç­–ç•¥æˆ–è½¯æœ€å¤§ç­–ç•¥ï¼‰åˆ™ç¡®ä¿åœ¨å­¦ä¹ é˜¶æ®µæœ‰ä¸€å®šçš„æ¢ç´¢æˆåˆ†ã€‚
- en: In SARSA, it is necessary to estimate the action-value function ğ‘ (ğ‘ , ğ‘) because
    the total value of a state ğ‘£ (ğ‘ ) (value function) is not sufficient in the absence
    of an environment model to allow the policy to determine, given a state, which
    action is performed the best. In this case, however, the values are estimated
    step by step by following the Bellman equation with the update parameter ğ‘£ (ğ‘ ),
    while consideringÂ the state-action pairÂ in place of a state.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨SARSAä¸­ï¼Œå¿…é¡»ä¼°ç®—åŠ¨ä½œä»·å€¼å‡½æ•° ğ‘ (ğ‘ , ğ‘)ï¼Œå› ä¸ºåœ¨æ²¡æœ‰ç¯å¢ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼ŒçŠ¶æ€ ğ‘£ (ğ‘ )ï¼ˆä»·å€¼å‡½æ•°ï¼‰çš„æ€»å€¼ä¸è¶³ä»¥è®©ç­–ç•¥æ ¹æ®ç»™å®šçš„çŠ¶æ€åˆ¤æ–­æ‰§è¡Œå“ªä¸ªåŠ¨ä½œæ˜¯æœ€å¥½çš„ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå€¼æ˜¯é€šè¿‡éµå¾ªè´å°”æ›¼æ–¹ç¨‹ï¼Œå¹¶è€ƒè™‘çŠ¶æ€-åŠ¨ä½œå¯¹ä»£æ›¿çŠ¶æ€ï¼Œé€æ­¥ä¼°ç®—çš„ã€‚
- en: Due to its on-policy nature, SARSA estimates the action-value function based
    on the behavior of the Ï€ policy, and at the same time, modifies the greedy behavior
    of the policy with respect to the updated estimates from the action-value function.
    The convergence of SARSA, and more generally of all TD methods, depends on the
    nature of policies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¶åœ¨ç­–ç•¥ä¸­çš„ç‰¹æ€§ï¼ŒSARSAæ ¹æ®Ï€ç­–ç•¥çš„è¡Œä¸ºä¼°ç®—åŠ¨ä½œä»·å€¼å‡½æ•°ï¼ŒåŒæ—¶æ ¹æ®ä»åŠ¨ä½œä»·å€¼å‡½æ•°ä¸­æ›´æ–°çš„ä¼°ç®—å€¼ï¼Œä¿®æ”¹ç­–ç•¥çš„è´ªå©ªè¡Œä¸ºã€‚SARSAçš„æ”¶æ•›æ€§ï¼Œå’Œæ‰€æœ‰TDæ–¹æ³•ä¸€æ ·ï¼Œä¾èµ–äºç­–ç•¥çš„æ€§è´¨ã€‚
- en: 'The following code block shows the pseudo-code for the SARSA algorithm:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å—å±•ç¤ºäº†SARSAç®—æ³•çš„ä¼ªä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `update` rule of the action-value function uses all five elements (`s[t]`,
    `a[t]`, `r[t + 1]`, `s[t + 1]`, andÂ `a[t + 1]`)Â  and for this reason, it is called
    **State-Action-Reward-State-Action** (**SARSA**).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œä»·å€¼å‡½æ•°çš„`update`è§„åˆ™ä½¿ç”¨æ‰€æœ‰äº”ä¸ªå…ƒç´ ï¼ˆ`s[t]`ï¼Œ`a[t]`ï¼Œ`r[t + 1]`ï¼Œ`s[t + 1]`ï¼Œä»¥åŠ `a[t + 1]`ï¼‰ï¼Œå› æ­¤è¢«ç§°ä¸º**çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±-çŠ¶æ€-åŠ¨ä½œ**
    (**SARSA**)ã€‚
- en: Q-learning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is one of the most used reinforcement learning algorithms. This is
    due to its ability to compare the expected utility of the available actions without
    requiring an environment model. Thanks to this technique, it is possible to find
    an optimal action for every given state in a finished MDP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learningæ˜¯æœ€å¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ã€‚å…¶åŸå› åœ¨äºå®ƒèƒ½å¤Ÿæ¯”è¾ƒå¯ç”¨åŠ¨ä½œçš„æœŸæœ›æ•ˆç”¨ï¼Œè€Œä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ã€‚å¾—ç›Šäºè¿™é¡¹æŠ€æœ¯ï¼Œå¯ä»¥åœ¨å®Œæˆçš„MDPä¸­ä¸ºæ¯ä¸ªç»™å®šçš„çŠ¶æ€æ‰¾åˆ°æœ€ä¼˜åŠ¨ä½œã€‚
- en: A general solution to the reinforcement learning problem is to estimate an evaluation
    function during the learning process. This function must be able to evaluate the
    convenience or otherwise of a particular policy through the sum of the rewards.
    In fact, Q-learning tries to maximize the value of the Q function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in the state *s*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„ä¸€ä¸ªé€šç”¨è§£å†³æ–¹æ¡ˆæ˜¯åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¼°ç®—è¯„ä¼°å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°å¿…é¡»èƒ½å¤Ÿé€šè¿‡å¥–åŠ±çš„æ€»å’Œè¯„ä¼°ç‰¹å®šç­–ç•¥çš„ä¾¿åˆ©æ€§æˆ–å…¶ä»–æ–¹é¢ã€‚äº‹å®ä¸Šï¼ŒQ-learningè¯•å›¾æœ€å¤§åŒ–Qå‡½æ•°ï¼ˆåŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰çš„å€¼ï¼ŒQå‡½æ•°è¡¨ç¤ºæˆ‘ä»¬åœ¨çŠ¶æ€
    *s* ä¸‹æ‰§è¡ŒåŠ¨ä½œ *a* æ—¶çš„æœ€å¤§æŠ˜æ‰£æœªæ¥å¥–åŠ±ã€‚
- en: 'Q-learning, like SARSA, estimates the function value ğ‘ (ğ‘ , ğ‘) incrementally,
    updating the value of the state-action pair at each step of the environment, following
    the logic of updating the general formula for estimating the values for the TD
    methods. Q-learning, unlike SARSA, has off-policy characteristics. That is, while
    the policy is improved according to the values estimated by ğ‘ (ğ‘ , ğ‘), the value
    function updates the estimates following a strictly greedy secondary policy: given
    a state, the chosen action is always the one that maximizes the value *max*ğ‘ (ğ‘ ,
    ğ‘). However, the Ï€ policy has an important role in estimating values because the
    state-action pairs to be visited and updated are determinedÂ through it.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learningå’ŒSARSAä¸€æ ·ï¼Œé€æ­¥ä¼°ç®—å‡½æ•°å€¼ ğ‘ (ğ‘ , ğ‘)ï¼Œåœ¨ç¯å¢ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­æ›´æ–°çŠ¶æ€-åŠ¨ä½œå¯¹çš„å€¼ï¼Œéµå¾ªæ›´æ–°TDæ–¹æ³•ä¼°ç®—å€¼çš„é€šç”¨å…¬å¼é€»è¾‘ã€‚ä¸SARSAä¸åŒï¼ŒQ-learningå…·æœ‰ç¦»ç­–ç•¥ç‰¹æ€§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè™½ç„¶ç­–ç•¥æ˜¯æ ¹æ®
    ğ‘ (ğ‘ , ğ‘) ä¼°ç®—çš„å€¼è¿›è¡Œæ”¹è¿›çš„ï¼Œä½†ä»·å€¼å‡½æ•°æ›´æ–°ä¼°ç®—å€¼æ—¶éµå¾ªä¸¥æ ¼çš„è´ªå©ªæ¬¡çº§ç­–ç•¥ï¼šç»™å®šä¸€ä¸ªçŠ¶æ€ï¼Œé€‰æ‹©çš„åŠ¨ä½œæ€»æ˜¯é‚£ä¸ªèƒ½å¤Ÿæœ€å¤§åŒ–å€¼ *max*ğ‘ (ğ‘ , ğ‘)
    çš„åŠ¨ä½œã€‚ç„¶è€Œï¼ŒÏ€ç­–ç•¥åœ¨ä¼°ç®—å€¼æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ï¼Œå› ä¸ºè¦è®¿é—®å’Œæ›´æ–°çš„çŠ¶æ€-åŠ¨ä½œå¯¹æ˜¯é€šè¿‡å®ƒæ¥å†³å®šçš„ã€‚
- en: 'The following code block shows pseudo-code for the Q-learning algorithm:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å—å±•ç¤ºäº†Q-learningç®—æ³•çš„ä¼ªä»£ç ï¼š
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Q-learning uses a table to store each state-action couple. At each step, the
    agent observes the current state of the environment and using the Ï€ policy selects
    and executes the action. By executing the action, the agent obtains the reward,
    ğ‘…[ğ‘¡+1], and the new state, ğ‘†[ğ‘¡+1]. At this point, the agent can calculate ğ‘„ (s[ğ‘¡],
    a[ğ‘¡]), updating the estimate.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learningä½¿ç”¨ä¸€ä¸ªè¡¨æ ¼æ¥å­˜å‚¨æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ã€‚åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œæ™ºèƒ½ä½“è§‚å¯Ÿå½“å‰ç¯å¢ƒçš„çŠ¶æ€ï¼Œå¹¶ä½¿ç”¨Ï€ç­–ç•¥é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œã€‚é€šè¿‡æ‰§è¡Œè¯¥åŠ¨ä½œï¼Œæ™ºèƒ½ä½“è·å¾—å¥–åŠ±
    ğ‘…[ğ‘¡+1]ï¼Œä»¥åŠæ–°çš„çŠ¶æ€ ğ‘†[ğ‘¡+1]ã€‚æ­¤æ—¶ï¼Œæ™ºèƒ½ä½“å¯ä»¥è®¡ç®— ğ‘„ (s[ğ‘¡], a[ğ‘¡])ï¼Œå¹¶æ›´æ–°ä¼°ç®—å€¼ã€‚
- en: In the following section, the basis of graph theory will be given and how this
    technology can be addressed in R.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œå°†ç»™å‡ºå›¾è®ºçš„åŸºç¡€ï¼Œå¹¶è¯´æ˜å¦‚ä½•åœ¨Rä¸­å¤„ç†è¿™é¡¹æŠ€æœ¯ã€‚
- en: Introducing graph theory and implementing it in R
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•å…¥å›¾è®ºå¹¶åœ¨Rä¸­å®ç°
- en: 'Graphs are data structures that are widely used in optimization problems. A
    graph is represented by a vertex and an edge structure. The vertices can be events
    from which different alternatives (the edges) depart. Typically, graphs are used
    to represent a network unambiguously: vertices represent individual calculators,
    road intersections, or bus stops, and edges are electrical connections or roads.
    Edges can connect vertices in any way possible.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾æ˜¯å¹¿æ³›åº”ç”¨äºä¼˜åŒ–é—®é¢˜çš„æ•°æ®ç»“æ„ã€‚å›¾ç”±é¡¶ç‚¹å’Œè¾¹çš„ç»“æ„è¡¨ç¤ºã€‚é¡¶ç‚¹å¯ä»¥æ˜¯ä»ä¸­å‡ºå‘çš„ä¸åŒé€‰æ‹©ï¼ˆå³è¾¹ï¼‰ã€‚é€šå¸¸ï¼Œå›¾ç”¨äºæ¸…æ™°åœ°è¡¨ç¤ºç½‘ç»œï¼šé¡¶ç‚¹ä»£è¡¨ç‹¬ç«‹çš„è®¡ç®—æœºã€è·¯å£æˆ–å…¬äº¤è½¦ç«™ï¼Œè¾¹åˆ™æ˜¯ç”µæ°”è¿æ¥æˆ–é“è·¯ã€‚è¾¹å¯ä»¥ä»¥ä»»ä½•å¯èƒ½çš„æ–¹å¼è¿æ¥é¡¶ç‚¹ã€‚
- en: Graph theory is a branch of mathematics that allows you to describe sets of
    objects together with their relationships; it was invented in 1700 by Leonhard
    Euler.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è®ºæ˜¯æ•°å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒå…è®¸ä½ æè¿°å¯¹è±¡é›†åˆåŠå…¶å…³ç³»ï¼›ç”±è±æ˜‚å“ˆå¾·Â·æ¬§æ‹‰åœ¨1700å¹´å‘æ˜ã€‚
- en: A graph is indicated in a compact way with *G = (V, E)*, where *V* indicates
    the set of vertices and *E* the set of edges that constitute it. The number of
    vertices is *|V|* and the number of edges is *|E|*.Â The number of vertices of
    the graph, or of a subpart of it, is obviously the fundamental quantity to define
    its dimensions; the number and distribution of edges describe their connectivity.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾é€šå¸¸ç”¨*G = (V, E)*çš„ç´§å‡‘å½¢å¼è¡¨ç¤ºï¼Œå…¶ä¸­*V*è¡¨ç¤ºé¡¶ç‚¹é›†åˆï¼Œ*E*è¡¨ç¤ºæ„æˆå›¾çš„è¾¹é›†åˆã€‚é¡¶ç‚¹çš„æ•°é‡æ˜¯*|V|*ï¼Œè¾¹çš„æ•°é‡æ˜¯*|E|*ã€‚å›¾çš„é¡¶ç‚¹æ•°ï¼Œæˆ–å…¶å­éƒ¨åˆ†çš„é¡¶ç‚¹æ•°ï¼Œæ˜¾ç„¶æ˜¯å®šä¹‰å…¶ç»´åº¦çš„åŸºæœ¬é‡ï¼›è¾¹çš„æ•°é‡å’Œåˆ†å¸ƒæè¿°äº†å®ƒä»¬çš„è¿æ¥æ€§ã€‚
- en: 'There are different types of edges: we are talking about undirected edges for
    which the edges do not have a direction in comparison with those directed. A directed
    edge is called an arc and the relative graph is called aÂ **digraph**. For example,
    undirected edges are used to represent computer networks with synchronous links
    for data transmission (as shown in the following diagram), directed graphs can
    represent road networks, allowing the representation of double-senses and unique
    senses.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸åŒç±»å‹çš„è¾¹ï¼šæˆ‘ä»¬è®¨è®ºçš„æ˜¯æ— å‘è¾¹ï¼Œå…¶è¾¹æ²¡æœ‰æ–¹å‘ï¼Œè€Œä¸æœ‰å‘è¾¹ç›¸æ¯”ã€‚æœ‰å‘è¾¹ç§°ä¸ºå¼§ï¼Œç›¸å…³çš„å›¾ç§°ä¸º**æœ‰å‘å›¾**ã€‚ä¾‹å¦‚ï¼Œæ— å‘è¾¹ç”¨äºè¡¨ç¤ºå…·æœ‰åŒæ­¥é“¾è·¯çš„æ•°æ®ä¼ è¾“è®¡ç®—æœºç½‘ç»œï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼Œè€Œæœ‰å‘å›¾åˆ™å¯ä»¥è¡¨ç¤ºé“è·¯ç½‘ç»œï¼Œå…è®¸è¡¨ç¤ºåŒå‘å’Œå•å‘é“è·¯ã€‚
- en: 'The following diagram represents a simple graph:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾è¡¨ç¤ºä¸€ä¸ªç®€å•çš„å›¾ï¼š
- en: '![](img/9f6d2eaf-d7ed-4e54-8342-da449764a46b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f6d2eaf-d7ed-4e54-8342-da449764a46b.png)'
- en: We say the graph is connected if we can reach all of the other vertices of the
    graph from any given vertex. Weighted graphs are graphs if a weight is associated
    with each edge, which is normally defined by a weight function (*w*). The weight
    can be seen as a cost or the distance between the two knots that the bow unites.
    The cost can be dependent on the flow that crosses the edge through a law. In
    this sense, the function w can be linear or not and depends on the flow that crosses
    the edge (non-congested networks) or also on the flow of nearby edges (congested
    networks).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿä»ä»»ä½•ç»™å®šçš„é¡¶ç‚¹åˆ°è¾¾å›¾ä¸­çš„æ‰€æœ‰å…¶ä»–é¡¶ç‚¹ï¼Œæˆ‘ä»¬å°±è¯´è¿™ä¸ªå›¾æ˜¯è¿é€šçš„ã€‚å¦‚æœæ¯æ¡è¾¹éƒ½å…³è”ä¸€ä¸ªæƒé‡ï¼Œå¹¶ä¸”é€šå¸¸ç”±æƒé‡å‡½æ•°(*w*)å®šä¹‰ï¼Œåˆ™å›¾æ˜¯åŠ æƒå›¾ã€‚æƒé‡å¯ä»¥è§†ä¸ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„æˆæœ¬æˆ–è·ç¦»ã€‚æˆæœ¬å¯èƒ½å–å†³äºæµé‡é€šè¿‡è¾¹çš„è§„å¾‹ã€‚åœ¨è¿™ä¸ªæ„ä¹‰ä¸Šï¼Œæƒé‡å‡½æ•°*w*å¯ä»¥æ˜¯çº¿æ€§çš„ï¼Œä¹Ÿå¯ä»¥ä¸æ˜¯ï¼Œå¹¶ä¸”å–å†³äºé€šè¿‡è¾¹çš„æµé‡ï¼ˆéæ‹¥å¡ç½‘ç»œï¼‰æˆ–å‘¨å›´è¾¹çš„æµé‡ï¼ˆæ‹¥å¡ç½‘ç»œï¼‰ã€‚
- en: 'A vertex is characterized by its degree, which is equal to the number of edges
    that end on the vertex itself. Depending on the degree, the vertices are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: é¡¶ç‚¹çš„ç‰¹å¾æ˜¯å…¶åº¦æ•°ï¼Œåº¦æ•°ç­‰äºä»¥è¯¥é¡¶ç‚¹ä¸ºç»ˆç‚¹çš„è¾¹çš„æ•°é‡ã€‚æ ¹æ®åº¦æ•°ï¼Œé¡¶ç‚¹å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: A vertex of order 0 is called an isolated vertex.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº¦æ•°ä¸º0çš„é¡¶ç‚¹ç§°ä¸ºå­¤ç«‹é¡¶ç‚¹ã€‚
- en: A vertex of order 1 is called a leaf vertex.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº¦æ•°ä¸º1çš„é¡¶ç‚¹ç§°ä¸ºå¶å­é¡¶ç‚¹ã€‚
- en: 'The following diagram showsÂ a graph with vertices labeled by degree:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªæŒ‰åº¦æ•°æ ‡è®°çš„å›¾ï¼š
- en: '![](img/62f921fa-7336-4572-b0dd-c3c5dc96e5f9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62f921fa-7336-4572-b0dd-c3c5dc96e5f9.png)'
- en: In a directed graph, we can distinguish the outdegree (number of outgoing edges)
    from the indegree (number of incoming edges). Based on this assumption, aÂ vertex
    with an indegree of zero is called a source vertex and aÂ vertex with an outdegree
    of zero is called a sink vertex.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ‰å‘å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åŒºåˆ†å‡ºåº¦ï¼ˆå³å‡ºå‘è¾¹çš„æ•°é‡ï¼‰å’Œå…¥åº¦ï¼ˆå³è¿›å…¥è¾¹çš„æ•°é‡ï¼‰ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œå…¥åº¦ä¸ºé›¶çš„é¡¶ç‚¹ç§°ä¸ºæºé¡¶ç‚¹ï¼Œå‡ºåº¦ä¸ºé›¶çš„é¡¶ç‚¹ç§°ä¸ºæ±‡é¡¶ç‚¹ã€‚
- en: 'Finally, a simplicial vertex is one whose neighbors form a clique: every two
    neighbors are adjacent. A universal vertex is a vertex that is adjacent to every
    other vertex in the graph.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç®€çº¦é¡¶ç‚¹æ˜¯å…¶é‚»å±…å½¢æˆå›¢ä½“çš„é¡¶ç‚¹ï¼šæ¯ä¸¤ä¸ªé‚»å±…éƒ½æ˜¯ç›¸é‚»çš„ã€‚é€šç”¨é¡¶ç‚¹æ˜¯ä¸å›¾ä¸­æ‰€æœ‰å…¶ä»–é¡¶ç‚¹ç›¸é‚»çš„é¡¶ç‚¹ã€‚
- en: 'To represent a graph, different approaches are available, such as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå›¾çš„æ–¹æ³•æœ‰å¤šç§ï¼Œä¾‹å¦‚ä»¥ä¸‹å‡ ç§ï¼š
- en: Graphic representation (as shown in the previous diagram)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾å½¢è¡¨ç¤ºï¼ˆå¦‚å‰å›¾æ‰€ç¤ºï¼‰
- en: Adjacency matrix
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‚»æ¥çŸ©é˜µ
- en: List of vertices *V* and of arcs *E*
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¡¶ç‚¹ *V* å’Œå¼§ *E* çš„åˆ—è¡¨
- en: The first way to represent a graph was clearly introduced through a practical
    example (see the previous diagram). In the graphical representation, circles are
    used to represent the vertices and lines to indicate the connections between two
    vertices if they are connected. If this connection has a direction, then it is
    indicated by adding an arrow. In the following section, we will analyze the other
    two ways of representing a graph.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå›¾çš„ç¬¬ä¸€ç§æ–¹æ³•é€šè¿‡å®é™…ç¤ºä¾‹è¿›è¡Œäº†æ¸…æ™°çš„ä»‹ç»ï¼ˆè§å‰é¢çš„å›¾ç¤ºï¼‰ã€‚åœ¨å›¾å½¢è¡¨ç¤ºä¸­ï¼Œåœ†åœˆè¡¨ç¤ºé¡¶ç‚¹ï¼Œçº¿æ¡è¡¨ç¤ºä¸¤ä¸ªé¡¶ç‚¹ä¹‹é—´çš„è¿æ¥ï¼Œå¦‚æœå®ƒä»¬ç›¸è¿ã€‚è‹¥è¯¥è¿æ¥å…·æœ‰æ–¹å‘æ€§ï¼Œåˆ™é€šè¿‡æ·»åŠ ç®­å¤´æ¥è¡¨ç¤ºã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†åˆ†æè¡¨ç¤ºå›¾çš„å…¶ä»–ä¸¤ç§æ–¹æ³•ã€‚
- en: Adjacency matrix
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‚»æ¥çŸ©é˜µ
- en: So far, we have represented a graph through vertices and edges. When the number
    of vertices is small, this way of representing a graph is the best one because
    it allows us to analyze its structure intuitively. When the number of vertices
    becomes large, the graphic representation becomes confusing. In this case, it
    is better to represent the graph through the adjacency matrix. By adjacency matrix
    or connection matrix, we mean a data structure that's commonly used in graph representation.
    It is widely used in the drafting of algorithms that operate on graphs and in
    their computer representation. If it is a sparse matrix, the use of the adjacency
    list is preferable to the matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡é¡¶ç‚¹å’Œè¾¹æ¥è¡¨ç¤ºå›¾ã€‚å½“é¡¶ç‚¹æ•°é‡è¾ƒå°‘æ—¶ï¼Œè¿™ç§è¡¨ç¤ºæ–¹æ³•æ˜¯æœ€å¥½çš„ï¼Œå› ä¸ºå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿç›´è§‚åœ°åˆ†æå›¾çš„ç»“æ„ã€‚å½“é¡¶ç‚¹æ•°é‡å˜å¤§æ—¶ï¼Œå›¾å½¢è¡¨ç¤ºå˜å¾—æ··ä¹±ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡é‚»æ¥çŸ©é˜µè¡¨ç¤ºå›¾ä¼šæ›´å¥½ã€‚é‚»æ¥çŸ©é˜µæˆ–è¿æ¥çŸ©é˜µæ˜¯å›¾è¡¨ç¤ºä¸­å¸¸ç”¨çš„æ•°æ®ç»“æ„ï¼Œå¹¿æ³›åº”ç”¨äºå›¾æ“ä½œç®—æ³•çš„è®¾è®¡ä»¥åŠå›¾çš„è®¡ç®—æœºè¡¨ç¤ºä¸­ã€‚å¦‚æœå®ƒæ˜¯ç¨€ç–çŸ©é˜µï¼Œä½¿ç”¨é‚»æ¥è¡¨ä¼˜äºä½¿ç”¨çŸ©é˜µã€‚
- en: 'Given any graph, its adjacency matrix is â€‹â€‹made up of a square binary matrix
    that has the names of the vertices of the graph as rows and columns. In the place
    (*i, j*) of the matrix, there is a 1 if and only if an edge that goes from the
    vertex *i* to the vertex *j* exists in the graph; otherwise, there is a 0\. In
    the case of the representation of undirected graphs, the matrix is symmetric with
    respect to the main diagonal. For example, check out the graph represented in
    the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä»»ä½•å›¾ï¼Œå…¶é‚»æ¥çŸ©é˜µç”±ä¸€ä¸ªæ–¹å½¢äºŒè¿›åˆ¶çŸ©é˜µç»„æˆï¼ŒçŸ©é˜µçš„è¡Œå’Œåˆ—æ˜¯å›¾ä¸­é¡¶ç‚¹çš„åç§°ã€‚åœ¨çŸ©é˜µçš„ (*i, j*) ä½ç½®ä¸Šï¼Œå¦‚æœå›¾ä¸­å­˜åœ¨ä¸€æ¡ä»é¡¶ç‚¹ *i* åˆ°é¡¶ç‚¹
    *j* çš„è¾¹ï¼Œåˆ™è¯¥ä½ç½®ä¸º1ï¼›å¦åˆ™ä¸º0ã€‚åœ¨æ— å‘å›¾çš„è¡¨ç¤ºä¸­ï¼ŒçŸ©é˜µç›¸å¯¹äºä¸»å¯¹è§’çº¿æ˜¯å¯¹ç§°çš„ã€‚ä¾‹å¦‚ï¼ŒæŸ¥çœ‹ä»¥ä¸‹å›¾ç¤ºæ‰€è¡¨ç¤ºçš„å›¾ï¼š
- en: '![](img/3d3da994-fab2-4b91-bb9e-a5e2d0417f04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d3da994-fab2-4b91-bb9e-a5e2d0417f04.png)'
- en: 'The preceding graph can be represented through the following adjacency matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„å›¾å¯ä»¥é€šè¿‡ä»¥ä¸‹é‚»æ¥çŸ©é˜µæ¥è¡¨ç¤ºï¼š
- en: '![](img/dcd3ca6f-e442-4d4e-bebb-d73792d20ad0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcd3ca6f-e442-4d4e-bebb-d73792d20ad0.png)'
- en: As anticipated, the matrix is symmetric with respect to the main diagonal being
    undirected. If instead of the 1 in the matrix, there are numbers; these are to
    be interpreted as the weight attributed to each connection (edge). Here, the matrix
    is called Markov's matrix, as it is applicable to a Markov process. For example,
    if the set of vertices of the graph represents a series of points on a map, the
    weight of the edges can be interpreted as the distance of the points that they
    connect.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚é¢„æœŸæ‰€ç¤ºï¼ŒçŸ©é˜µç›¸å¯¹äºä¸»å¯¹è§’çº¿æ˜¯å¯¹ç§°çš„ï¼Œè¡¨ç¤ºå›¾æ˜¯æ— å‘çš„ã€‚å¦‚æœçŸ©é˜µä¸­ä¸æ˜¯1è€Œæ˜¯å…¶ä»–æ•°å­—ï¼Œé‚£ä¹ˆè¿™äº›æ•°å­—è¡¨ç¤ºåˆ†é…ç»™æ¯ä¸ªè¿æ¥ï¼ˆè¾¹ï¼‰çš„æƒé‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒçŸ©é˜µè¢«ç§°ä¸ºé©¬å°”å¯å¤«çŸ©é˜µï¼Œå› ä¸ºå®ƒé€‚ç”¨äºé©¬å°”å¯å¤«è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå›¾çš„é¡¶ç‚¹é›†è¡¨ç¤ºåœ°å›¾ä¸Šçš„ä¸€ç³»åˆ—ç‚¹ï¼Œé‚£ä¹ˆè¾¹çš„æƒé‡å¯ä»¥è§£é‡Šä¸ºå®ƒä»¬è¿æ¥çš„ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚
- en: One of the fundamental characteristics of this matrix is that it obtains the
    number of paths from a node *i* to a node *j*, which must cross *n* vertices.
    To obtain all of this, it is sufficient to make the *n* power of the matrix and
    see the number that appears in place *i, j*. Another way of representing graphs
    is using adjacency lists. Let's see how.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªçŸ©é˜µçš„ä¸€ä¸ªåŸºæœ¬ç‰¹ç‚¹æ˜¯ï¼Œå®ƒå¯ä»¥è®¡ç®—ä»èŠ‚ç‚¹*i*åˆ°èŠ‚ç‚¹*j*çš„è·¯å¾„æ•°ï¼Œè¿™äº›è·¯å¾„å¿…é¡»ç»è¿‡*n*ä¸ªé¡¶ç‚¹ã€‚ä¸ºäº†å¾—åˆ°è¿™äº›ä¿¡æ¯ï¼Œåªéœ€å°†çŸ©é˜µçš„*n*æ¬¡æ–¹è®¡ç®—å‡ºæ¥ï¼Œå¹¶æŸ¥çœ‹åœ¨*i,
    j*ä½ç½®ä¸Šçš„æ•°å­—å³å¯ã€‚å¦ä¸€ç§è¡¨ç¤ºå›¾çš„æ–¹å¼æ˜¯ä½¿ç”¨é‚»æ¥åˆ—è¡¨ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ã€‚
- en: Adjacency list
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‚»æ¥åˆ—è¡¨
- en: '**Adjacency lists** are a mode of graph representation in memory. This is probably
    the simplest representation to implement, although, in general, it is not the
    most efficient in terms of the space that''s occupied.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‚»æ¥åˆ—è¡¨**æ˜¯å›¾åœ¨å†…å­˜ä¸­çš„ä¸€ç§è¡¨ç¤ºæ–¹å¼ã€‚è¿™å¯èƒ½æ˜¯æœ€ç®€å•çš„å®ç°æ–¹å¼ï¼Œå°½ç®¡é€šå¸¸æ¥è¯´ï¼Œå®ƒåœ¨å ç”¨ç©ºé—´æ–¹é¢ä¸æ˜¯æœ€æœ‰æ•ˆçš„ã€‚'
- en: Let's analyze a simple graph; next to each vertex is its list of adjacencies.
    The idea of representation is simply that every vertex *Vi* is associated with
    a list containing all of the vertices *Vj* so that there is the edge from *Vi*
    to *Vj*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ†æä¸€ä¸ªç®€å•çš„å›¾ï¼›æ¯ä¸ªé¡¶ç‚¹æ—è¾¹åˆ—å‡ºçš„æ˜¯å…¶ç›¸é‚»é¡¶ç‚¹çš„åˆ—è¡¨ã€‚è¡¨ç¤ºæ–¹æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæ¯ä¸ªé¡¶ç‚¹*Vi*éƒ½ä¸ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸å…¶ç›¸è¿çš„é¡¶ç‚¹*Vj*çš„åˆ—è¡¨ç›¸å…³è”ï¼Œå³å­˜åœ¨ä¸€æ¡ä»*Vi*åˆ°*Vj*çš„è¾¹ã€‚
- en: Assuming that you memorize all of the pairs of the type (*Vi, L*), where *L*
    is the adjacency list of the *Vi* vertex, we obtain a unique description of the
    graph. Alternatively, if you decide to sort adjacency lists, you do not need to
    explicitly store the vertexes as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ è®°ä½äº†æ‰€æœ‰ç±»å‹ä¸º(*Vi, L*)çš„é¡¶ç‚¹å¯¹ï¼Œå…¶ä¸­*L*æ˜¯é¡¶ç‚¹*Vi*çš„é‚»æ¥åˆ—è¡¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±èƒ½å¾—åˆ°å›¾çš„å”¯ä¸€æè¿°ã€‚æˆ–è€…ï¼Œå¦‚æœä½ å†³å®šå¯¹é‚»æ¥åˆ—è¡¨è¿›è¡Œæ’åºï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦æ˜¾å¼åœ°å­˜å‚¨é¡¶ç‚¹äº†ã€‚
- en: 'Let''s take an exampleâ€”we will use the same graph adopted in the previous section,
    which is represented in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­â€”â€”æˆ‘ä»¬å°†ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­é‡‡ç”¨çš„ç›¸åŒå›¾å½¢ï¼Œå›¾ç¤ºå¦‚ä¸‹ï¼š
- en: '![](img/30f10bb6-661c-4f8c-ab4a-031e07f376c2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30f10bb6-661c-4f8c-ab4a-031e07f376c2.png)'
- en: 'From this, we will build the list of adjacencies according to what has been
    said so far. The graphÂ in the previous diagram can be represented as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå‰é¢æåˆ°çš„å†…å®¹ï¼Œæˆ‘ä»¬å°†æ„å»ºé‚»æ¥åˆ—è¡¨ã€‚ä¸Šå›¾ä¸­çš„å›¾å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '| 1 | adjacent to | 2,3 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ç›¸é‚»äº | 2,3 |'
- en: '| 2 | adjacent to | 1,3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ç›¸é‚»äº | 1,3 |'
- en: '| 3 | adjacent to | 1,2,4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 3 | ç›¸é‚»äº | 1,2,4 |'
- en: '| 4 | adjacent to | 3 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 4 | ç›¸é‚»äº | 3 |'
- en: An adjacency list is made up of pairs. There is a pair for each vertex in the
    graph. The first element of the pair is the vertex that is being analyzed, and
    the second is the set formed by all of the vertices adjacent to it, which is connected
    to it by one side.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é‚»æ¥åˆ—è¡¨ç”±å¯¹ç»„æˆã€‚æ¯ä¸ªå›¾ä¸­çš„é¡¶ç‚¹éƒ½æœ‰ä¸€å¯¹ã€‚å¯¹çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ­£åœ¨åˆ†æçš„é¡¶ç‚¹ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ç”±æ‰€æœ‰ä¸å®ƒç›¸é‚»çš„é¡¶ç‚¹ç»„æˆçš„é›†åˆï¼Œè¿™äº›é¡¶ç‚¹é€šè¿‡ä¸€æ¡è¾¹ä¸ä¹‹ç›¸è¿ã€‚
- en: Assuming that we have a graph with *n* vertices and *m* edges (directed) that
    unite them, and supposing the adjacency lists areÂ memorizedÂ in the order (so as
    not to explicitly memorize the indices), we will have each edge appear in one
    and one list of adjacencies, and it appears as the number of the vertex to which
    it points. Due to this, it's necessary to memorize a total of *m* numbers less
    than or equal to *n*, for a total cost of *mlog2n*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«*n*ä¸ªé¡¶ç‚¹å’Œ*m*æ¡è¾¹ï¼ˆæœ‰å‘ï¼‰çš„å›¾ï¼Œä¸”å‡è®¾é‚»æ¥åˆ—è¡¨å·²æŒ‰é¡ºåºè®°å¿†ï¼ˆä¸ºäº†é¿å…æ˜¾å¼è®°å¿†ç´¢å¼•ï¼‰ï¼Œé‚£ä¹ˆæ¯æ¡è¾¹ä¼šå‡ºç°åœ¨ä¸€ä¸ªä¸”ä»…ä¸€ä¸ªé‚»æ¥åˆ—è¡¨ä¸­ï¼Œå¹¶ä¸”å®ƒä¼šä»¥æŒ‡å‘çš„é¡¶ç‚¹ç¼–å·çš„å½¢å¼å‡ºç°ã€‚å› æ­¤ï¼Œéœ€è¦è®°ä½æ€»å…±*m*ä¸ªå°äºç­‰äº*n*çš„æ•°å­—ï¼Œæ€»çš„æˆæœ¬ä¸º*mlog2n*ã€‚
- en: 'There is no obvious way to optimize this representation for non-oriented graphs;
    each arc must be memorized in the adjacency lists of both vertices that it connects,
    hence halving the efficiency. The same argument holds if the graph is oriented,
    but we need an efficient method to know the arcs entering a certain vertex. In
    this case, it is convenient to associate two listsÂ to each vertex: that of the
    incoming arcs and that of the outgoing arcs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ— å‘å›¾æ¥è¯´ï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ–¹æ³•æ¥ä¼˜åŒ–è¿™ç§è¡¨ç¤ºæ³•ï¼›æ¯æ¡å¼§å¿…é¡»åœ¨è¿æ¥çš„ä¸¤ä¸ªé¡¶ç‚¹çš„é‚»æ¥åˆ—è¡¨ä¸­éƒ½è¿›è¡Œè®°å¿†ï¼Œä»è€Œé™ä½äº†æ•ˆç‡ã€‚å¦‚æœå›¾æ˜¯æœ‰å‘å›¾ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥çŸ¥é“æŒ‡å‘æŸä¸ªé¡¶ç‚¹çš„å¼§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†æ¯ä¸ªé¡¶ç‚¹å…³è”ä¸¤ä¸ªåˆ—è¡¨æ˜¯æ¯”è¾ƒæ–¹ä¾¿çš„ï¼šä¸€ä¸ªæ˜¯è¿›å…¥å¼§çš„åˆ—è¡¨ï¼Œå¦ä¸€ä¸ªæ˜¯å‡ºå»å¼§çš„åˆ—è¡¨ã€‚
- en: As far as time efficiency is concerned, representation by adjacency lists behaves
    quite well both in access and in insertion, carrying out the main operations in
    time *O(n)*. So far, we have analyzed the graphic representation techniques. Now,
    let's learn how to use them in the R environment.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—¶é—´æ•ˆç‡æ–¹é¢ï¼Œé‚»æ¥åˆ—è¡¨çš„è¡¨ç¤ºæ–¹å¼åœ¨è®¿é—®å’Œæ’å…¥æ“ä½œä¸­è¡¨ç°å¾—ç›¸å½“ä¸é”™ï¼Œä¸»è¦æ“ä½œåœ¨*O(n)*æ—¶é—´å†…å®Œæˆã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»åˆ†æäº†å›¾å½¢è¡¨ç¤ºçš„æŠ€æœ¯ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•åœ¨Rç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æŠ€æœ¯ã€‚
- en: Handling graphs in R
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨Rä¸­å¤„ç†å›¾å½¢
- en: In R, the set of nodes (*V*) and the set of arcs (*E*) are data structures of
    different types. For *V*, once we assign a unique identifier toÂ each node,Â then
    we can access every node without ambiguity. Hence, it is like saying that the
    data structure hosting the properties of the nodes is one-dimensional and, therefore,
    is a vector.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Rä¸­ï¼ŒèŠ‚ç‚¹é›†ï¼ˆ*V*ï¼‰å’Œå¼§é›†ï¼ˆ*E*ï¼‰æ˜¯ä¸åŒç±»å‹çš„æ•°æ®ç»“æ„ã€‚å¯¹äº*V*ï¼Œä¸€æ—¦æˆ‘ä»¬ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå°±å¯ä»¥æ— æ­§ä¹‰åœ°è®¿é—®æ¯ä¸ªèŠ‚ç‚¹ã€‚å› æ­¤ï¼Œå®ƒå°±åƒæ˜¯åœ¨è¯´ï¼Œæ‰˜ç®¡èŠ‚ç‚¹å±æ€§çš„æ•°æ®ç»“æ„æ˜¯ä¸€ç»´çš„ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªå‘é‡ã€‚
- en: On the contrary, the data structure for the set of arcs (links between nodes)
    *E* cannot be a vector andÂ it does not express the characteristics of single objects
    but expresses relations between pairs of objects (pairs of nodes in this case).
    So if, for example, in *V* (the set of nodes) there are 10 nodes, then the dimensions
    of *E* will be 10 Ã— 10 or all of the relationships between all of the possible
    pairs of nodes. Ultimately, *E* has not one but two dimensions and therefore it
    is not a vector but a matrix.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œå¼§é›†ï¼ˆèŠ‚ç‚¹ä¹‹é—´çš„é“¾æ¥ï¼‰*E*çš„æ•°æ®ç»“æ„ä¸èƒ½æ˜¯å‘é‡ï¼Œå®ƒä¸è¡¨ç¤ºå•ä¸€å¯¹è±¡çš„ç‰¹å¾ï¼Œè€Œæ˜¯è¡¨ç¤ºå¯¹è±¡å¯¹ä¹‹é—´çš„å…³ç³»ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å…³ç³»ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœä¾‹å¦‚åœ¨*V*ï¼ˆèŠ‚ç‚¹é›†ï¼‰ä¸­æœ‰10ä¸ªèŠ‚ç‚¹ï¼Œé‚£ä¹ˆ*E*çš„ç»´åº¦å°†æ˜¯10
    Ã— 10ï¼Œå³æ‰€æœ‰å¯èƒ½èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å…³ç³»ã€‚æœ€ç»ˆï¼Œ*E*æœ‰ä¸¤ä¸ªç»´åº¦ï¼Œå› æ­¤å®ƒä¸æ˜¯å‘é‡ï¼Œè€Œæ˜¯çŸ©é˜µã€‚
- en: In the matrix *E*, we have several rows equal to the number of nodes present
    in *V* and several columns equal to the number of nodes present in *V*. This represents
    the adjacency matrix analyzed in detail in the *Adjacency matrix* section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çŸ©é˜µ*E*ä¸­ï¼Œæˆ‘ä»¬æœ‰å¤šè¡Œç­‰äº*V*ä¸­èŠ‚ç‚¹çš„æ•°é‡ï¼Œå¤šåˆ—ç­‰äº*V*ä¸­èŠ‚ç‚¹çš„æ•°é‡ã€‚è¿™è¡¨ç¤ºåœ¨*é‚»æ¥çŸ©é˜µ*éƒ¨åˆ†ä¸­è¯¦ç»†åˆ†æçš„é‚»æ¥çŸ©é˜µã€‚
- en: To address the graph in R, we can use the `igraph` packageâ€”this package contains
    functions for simple graphs and network analysis. It can handle large graphs very
    well and provides functions for generating random and regular graphs, graph visualization,
    centrality methods, and much more.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨Rä¸­å¤„ç†å›¾å½¢ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`igraph`åŒ…â€”â€”è¯¥åŒ…åŒ…å«ç”¨äºç®€å•å›¾å½¢å’Œç½‘ç»œåˆ†æçš„å‡½æ•°ã€‚å®ƒèƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†å¤§å‹å›¾å½¢ï¼Œå¹¶æä¾›ç”Ÿæˆéšæœºå›¾å’Œè§„åˆ™å›¾ã€å›¾å½¢å¯è§†åŒ–ã€ä¸­å¿ƒæ€§æ–¹æ³•ç­‰åŠŸèƒ½ã€‚
- en: 'The following table gives some information about this package:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹è¡¨æ ¼æä¾›äº†æœ‰å…³è¯¥åŒ…çš„ä¸€äº›ä¿¡æ¯ï¼š
- en: '| Package | `igraph` |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| åŒ… | `igraph` |'
- en: '| Date | 2019-22-04 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| æ—¥æœŸ | 2019-22-04 |'
- en: '| Version | 1.2.4.1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ç‰ˆæœ¬ | 1.2.4.1 |'
- en: '| Title | Network Analysis and Visualization |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| æ ‡é¢˜ | ç½‘ç»œåˆ†æä¸å¯è§†åŒ– |'
- en: '| Maintainer | GÃ¡bor CsÃ¡rdi |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ç»´æŠ¤è€… | GÃ¡bor CsÃ¡rdi |'
- en: 'To start using the available tools, we will analyze a simple example. Suppose
    we have a graph consisting of four nodes and four edges. The first thing to do
    is to define the links between the four nodes; to do this, we will use the `graph`
    function (remember to load the `igraph` library after installing it):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹ä½¿ç”¨å¯ç”¨å·¥å…·ï¼Œæˆ‘ä»¬å°†åˆ†æä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç”±å››ä¸ªèŠ‚ç‚¹å’Œå››æ¡è¾¹ç»„æˆçš„å›¾ã€‚é¦–å…ˆè¦åšçš„æ˜¯å®šä¹‰è¿™å››ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„é“¾æ¥ï¼›ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`graph`å‡½æ•°ï¼ˆè®°å¾—åœ¨å®‰è£…ååŠ è½½`igraph`åº“ï¼‰ï¼š
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `graph` function is part of the `graph.constructors` methods that offer
    various methods for creating graphs: empty graphs, graphs with the given edges,
    graphs from adjacency matrices, star graphs, lattices, rings, and trees. The method
    we used defines the graph by indicating edges using a numeric vector defining
    the edges as follows: the first edge from the first element to the second, the
    second edge from the third to the fourth, and so on.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph`å‡½æ•°æ˜¯`graph.constructors`æ–¹æ³•çš„ä¸€éƒ¨åˆ†ï¼Œæä¾›äº†åˆ›å»ºå›¾å½¢çš„å„ç§æ–¹æ³•ï¼šç©ºå›¾ã€æœ‰ç»™å®šè¾¹çš„å›¾ã€ä»é‚»æ¥çŸ©é˜µæ„å»ºçš„å›¾ã€æ˜Ÿå½¢å›¾ã€æ ¼çŠ¶å›¾ã€ç¯å½¢å›¾å’Œæ ‘å½¢å›¾ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨æ•°å€¼å‘é‡æ¥å®šä¹‰è¾¹æ¥å®šä¹‰å›¾å½¢ï¼Œå‘é‡ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ åˆ°ç¬¬äºŒä¸ªå…ƒç´ ä¸ºç¬¬ä¸€æ¡è¾¹ï¼Œç¬¬ä¸‰ä¸ªå…ƒç´ åˆ°ç¬¬å››ä¸ªå…ƒç´ ä¸ºç¬¬äºŒæ¡è¾¹ï¼Œä»¥æ­¤ç±»æ¨ã€‚'
- en: 'In fact, we can see that four pairs of values have been passed: the first defines
    the connection between nodes 1 and 2, the second between nodes 2 and 3, the third
    between nodes 3 and 1, and finally, the fourth between nodes 4 and 2\. To better
    understand the connections between the nodes of the graph, we will draw it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¼ å…¥äº†å››å¯¹å€¼ï¼šç¬¬ä¸€å¯¹å®šä¹‰äº†èŠ‚ç‚¹1å’ŒèŠ‚ç‚¹2ä¹‹é—´çš„è¿æ¥ï¼Œç¬¬äºŒå¯¹å®šä¹‰äº†èŠ‚ç‚¹2å’ŒèŠ‚ç‚¹3ä¹‹é—´çš„è¿æ¥ï¼Œç¬¬ä¸‰å¯¹å®šä¹‰äº†èŠ‚ç‚¹3å’ŒèŠ‚ç‚¹1ä¹‹é—´çš„è¿æ¥ï¼Œæœ€åï¼Œç¬¬å››å¯¹å®šä¹‰äº†èŠ‚ç‚¹4å’ŒèŠ‚ç‚¹2ä¹‹é—´çš„è¿æ¥ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å›¾ä¸­èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ï¼Œæˆ‘ä»¬å°†ç»˜åˆ¶è¯¥å›¾ï¼š
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following graph was plotted:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶äº†ä»¥ä¸‹å›¾å½¢ï¼š
- en: '![](img/529c85a6-f9a7-4819-a753-17a7dab6a4b4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/529c85a6-f9a7-4819-a753-17a7dab6a4b4.png)'
- en: 'The graph we created is an object with features that we can analyze as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºçš„å›¾æ˜¯ä¸€ä¸ªå…·æœ‰ç‰¹å¾çš„å¯¹è±¡ï¼Œå¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è¿›è¡Œåˆ†æï¼š
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following results are returned:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›çš„ç»“æœå¦‚ä¸‹ï¼š
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The edges between the nodes are indicated.Â We then calculate the shortest path
    between node 1 and node 4:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ä¹‹é—´çš„è¾¹å·²æŒ‡ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—èŠ‚ç‚¹1å’ŒèŠ‚ç‚¹4ä¹‹é—´çš„æœ€çŸ­è·¯å¾„ï¼š
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`get.shortest.paths()Â ` calculates a single shortest path between the source
    vertex to the target vertices. This function uses a breadth-first search for unweighted
    graphs and Dijkstra''s algorithm for weighted graphs. In our case, having added
    the weight attribute, the Dijkstra algorithm was used.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`get.shortest.paths()` è®¡ç®—ä»æºé¡¶ç‚¹åˆ°ç›®æ ‡é¡¶ç‚¹çš„å•ä¸€æœ€çŸ­è·¯å¾„ã€‚è¯¥å‡½æ•°å¯¹äºæ— æƒå›¾ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼Œå¯¹äºæœ‰æƒå›¾ä½¿ç”¨ Dijkstra
    ç®—æ³•ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç”±äºæ·»åŠ äº†æƒé‡å±æ€§ï¼Œå› æ­¤ä½¿ç”¨äº† Dijkstra ç®—æ³•ã€‚'
- en: 'The following result is returned:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, calculate the distance between the two points for this path:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®¡ç®—æ­¤è·¯å¾„ä¸Šä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»ï¼š
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following result is returned:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The graph we have represented so far has limited usefulness to identify the
    shortest path between two locations, which represents our goal. To calculate the
    best route, it is necessary to introduce the concept of edge weight. In our case,
    we can see this attribute as a measure of the length of the path between two nodes;
    in this way, we can evaluate the distance between two nodes through a path. To
    do this, we''ll use the attribute weights as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰æˆ‘ä»¬æ‰€è¡¨ç¤ºçš„å›¾åœ¨è¯†åˆ«ä¸¤åœ°ä¹‹é—´æœ€çŸ­è·¯å¾„æ–¹é¢çš„ç”¨é€”æœ‰é™ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬çš„ç›®æ ‡ã€‚ä¸ºäº†è®¡ç®—æœ€ä½³è·¯å¾„ï¼Œéœ€è¦å¼•å…¥è¾¹æƒé‡çš„æ¦‚å¿µã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ­¤å±æ€§è§†ä¸ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´è·¯å¾„é•¿åº¦çš„åº¦é‡ï¼›é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è·¯å¾„æ¥è¯„ä¼°ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹æ–¹å¼æ¥å®šä¹‰å±æ€§æƒé‡ï¼š
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: First, we defined the weights with a vector, confirming the sequence of edges
    defined in the creation of the graph. So, we added the weight attribute to the
    previously created graph. Now, each edge has its own length. What happens if weights
    are not defined? Simply, they are all set equal to 1; in this case, the shortest
    path would be the one with the least number of nodes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå‘é‡å®šä¹‰äº†æƒé‡ï¼Œç¡®è®¤äº†åœ¨å›¾åˆ›å»ºæ—¶å®šä¹‰çš„è¾¹çš„é¡ºåºã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æƒé‡å±æ€§æ·»åŠ åˆ°å…ˆå‰åˆ›å»ºçš„å›¾ä¸­ã€‚ç°åœ¨ï¼Œæ¯æ¡è¾¹éƒ½æœ‰äº†è‡ªå·±çš„é•¿åº¦ã€‚å¦‚æœæ²¡æœ‰å®šä¹‰æƒé‡ä¼šæ€æ ·å‘¢ï¼Ÿç®€å•åœ°è¯´ï¼Œå®ƒä»¬éƒ½ä¼šè¢«è®¾ä¸º
    1ï¼›åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€çŸ­è·¯å¾„å°†æ˜¯èŠ‚ç‚¹æ•°æœ€å°‘çš„è·¯å¾„ã€‚
- en: 'Now, let''s calculate again the shortest path between node 1 and node 4:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬é‡æ–°è®¡ç®—èŠ‚ç‚¹ 1 å’ŒèŠ‚ç‚¹ 4 ä¹‹é—´çš„æœ€çŸ­è·¯å¾„ï¼š
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following result is returned:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can see that the path now involves multiple nodes. We will verify the distance
    between the two nodes:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨çš„è·¯å¾„æ¶‰åŠå¤šä¸ªèŠ‚ç‚¹ã€‚æˆ‘ä»¬å°†éªŒè¯è¿™ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»ï¼š
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following result is returned:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this way, we have verified that the indicated path is the shortest one since
    the longest connection has been avoided. In the next section, we see how it is
    possible to find the best route using Dijkstra's algorithm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬éªŒè¯äº†æ‰€æŒ‡ç¤ºçš„è·¯å¾„æ˜¯æœ€çŸ­è·¯å¾„ï¼Œå› ä¸ºæœ€é•¿çš„è¿æ¥å·²è¢«é¿å…ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ Dijkstra ç®—æ³•æ‰¾åˆ°æœ€ä½³è·¯å¾„ã€‚
- en: Dijkstra's algorithm
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dijkstra ç®—æ³•
- en: Dijkstra's algorithm is used to solve the problem of finding the shortest path
    from the source *s* to all of the nodes. The algorithm maintains a label *d(i)*
    to the nodes representing an upper bound on the length of the shortest path of
    the node *i*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Dijkstra ç®—æ³•ç”¨äºè§£å†³ä»æºèŠ‚ç‚¹ *s* åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„é—®é¢˜ã€‚è¯¥ç®—æ³•ä¸ºèŠ‚ç‚¹ç»´æŠ¤ä¸€ä¸ªæ ‡ç­¾ *d(i)*ï¼Œè¡¨ç¤ºèŠ‚ç‚¹ *i* æœ€çŸ­è·¯å¾„é•¿åº¦çš„ä¸Šé™ã€‚
- en: 'At each step, the algorithm partitions the nodes in *V* into two sets: the
    set of permanently labeled nodes and the set of nodes that are still temporarily
    labeled. The distance of permanently labeled nodes represents the shortest path
    distance from the source to these nodes, whereas the temporary labels contain
    a value that can be greater than or equal to the shortest path length.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç®—æ³•å°† *V* ä¸­çš„èŠ‚ç‚¹åˆ†æˆä¸¤ç»„ï¼šä¸€ç»„æ˜¯æ°¸ä¹…æ ‡è®°çš„èŠ‚ç‚¹ï¼Œå¦ä¸€ç»„æ˜¯ä»ç„¶æ˜¯ä¸´æ—¶æ ‡è®°çš„èŠ‚ç‚¹ã€‚æ°¸ä¹…æ ‡è®°èŠ‚ç‚¹çš„è·ç¦»è¡¨ç¤ºä»æºèŠ‚ç‚¹åˆ°è¿™äº›èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„è·ç¦»ï¼Œè€Œä¸´æ—¶æ ‡è®°çš„èŠ‚ç‚¹åˆ™åŒ…å«ä¸€ä¸ªå€¼ï¼Œè¯¥å€¼å¯ä»¥å¤§äºæˆ–ç­‰äºæœ€çŸ­è·¯å¾„é•¿åº¦ã€‚
- en: The basic idea of the algorithm is to start from the source and try to permanently
    label the successor nodes. In the beginning, the algorithm places the value of
    the source distance to zero and initializes the other distances to an arbitrarily
    high value (by convention, we will set the initial value of the distances *d[i]
    = + âˆ, âˆ€i âˆˆ V*). At each iteration, the node label *i* is the value of the minimum
    distance along a path from the source that contains, apart from *i*, only permanently
    labeled nodes. The algorithm selects the node whose label has the lowest value
    among those labeled temporarily, labels it permanently, and updates all of the
    labels of the nodes adjacent to it. The algorithm terminates when all of the nodes
    have been permanently labeled.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯ä»æºèŠ‚ç‚¹å¼€å§‹ï¼Œå°è¯•æ°¸ä¹…æ ‡è®°åç»§èŠ‚ç‚¹ã€‚å¼€å§‹æ—¶ï¼Œç®—æ³•å°†æºèŠ‚ç‚¹çš„è·ç¦»å€¼è®¾ä¸ºé›¶ï¼Œå¹¶å°†å…¶ä»–èŠ‚ç‚¹çš„è·ç¦»åˆå§‹åŒ–ä¸ºä¸€ä¸ªä»»æ„é«˜çš„å€¼ï¼ˆæŒ‰ç…§æƒ¯ä¾‹ï¼Œæˆ‘ä»¬å°†è·ç¦»çš„åˆå§‹å€¼è®¾ä¸º
    *d[i] = + âˆ, âˆ€i âˆˆ V*ï¼‰ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒèŠ‚ç‚¹æ ‡ç­¾ *i* æ˜¯ä»æºèŠ‚ç‚¹å‡ºå‘çš„è·¯å¾„ä¸­æœ€å°è·ç¦»çš„å€¼ï¼Œè¯¥è·¯å¾„é™¤äº† *i* ä¹‹å¤–åªæœ‰æ°¸ä¹…æ ‡è®°çš„èŠ‚ç‚¹ã€‚ç®—æ³•é€‰æ‹©é‚£äº›ä¸´æ—¶æ ‡è®°çš„èŠ‚ç‚¹ä¸­æ ‡ç­¾å€¼æœ€ä½çš„èŠ‚ç‚¹ï¼Œå°†å…¶æ°¸ä¹…æ ‡è®°ï¼Œå¹¶æ›´æ–°æ‰€æœ‰ä¸ä¹‹ç›¸é‚»èŠ‚ç‚¹çš„æ ‡ç­¾ã€‚å½“æ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«æ°¸ä¹…æ ‡è®°æ—¶ï¼Œç®—æ³•ç»ˆæ­¢ã€‚
- en: 'From the execution of this algorithm for each destination node *v* of *V*,
    we obtainÂ a shortest path *p* (from *s* to *v*) and we calculate the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ‰§è¡Œè¯¥ç®—æ³•ï¼Œé’ˆå¯¹æ¯ä¸ªç›®æ ‡èŠ‚ç‚¹ *v*ï¼ˆå±äº *V*ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—ä¸€ä¸ªæœ€çŸ­è·¯å¾„ *p*ï¼ˆä» *s* åˆ° *v*ï¼‰ï¼Œå¹¶è®¡ç®—ä»¥ä¸‹å†…å®¹ï¼š
- en: '*d [v]*:Â Distance of node *v* from source node *s* long *p*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d [v]*ï¼šèŠ‚ç‚¹ *v* åˆ°æºèŠ‚ç‚¹ *s* çš„è·ç¦» *p*'
- en: '*Ï€ [v]*: Predecessor of node *v* long *p*'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ï€ [v]*: èŠ‚ç‚¹ *v* çš„å‰é©±èŠ‚ç‚¹ä¸º *p*'
- en: 'For the initialization ofÂ each node v of V, we will use the following procedure:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªèŠ‚ç‚¹ *v*ï¼ˆå±äº *V*ï¼‰çš„åˆå§‹åŒ–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹è¿‡ç¨‹ï¼š
- en: '*d [v] = âˆ if v â‰  s*, otherwise *d [s] = 0*'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d [v] = âˆ å¦‚æœ v â‰  s*ï¼Œå¦åˆ™ *d [s] = 0*'
- en: '*Ï€ [v] = Ã˜*'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ï€ [v] = Ã˜*'
- en: During the execution, we use the relaxation technique of a generic edge *(u,
    v)* of *E*, which serves to improve the estimation of *d*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ³›åŒ–è¾¹ *(u, v)*ï¼ˆå±äº *E*ï¼‰çš„æ¾å¼›æŠ€æœ¯æ¥æ”¹å–„ *d* çš„ä¼°ç®—å€¼ã€‚
- en: 'The relaxation of an edge *(u, v)* of *E*, consists in evaluating whether,
    using *u* as a predecessor of *v*, the current value of distance *d [v]* can be
    improved and, in this case, they update *d [v]* and *Ï€ [v]*. The procedure is
    as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¹ *(u, v)* çš„æ¾å¼›æ“ä½œï¼Œæ—¨åœ¨è¯„ä¼°æ˜¯å¦å¯ä»¥é€šè¿‡å°† *u* ä½œä¸º *v* çš„å‰é©±èŠ‚ç‚¹æ¥æ”¹å–„å½“å‰çš„è·ç¦»å€¼ *d [v]*ï¼Œå¦‚æœå¯ä»¥æ”¹å–„ï¼Œåˆ™æ›´æ–° *d
    [v]* å’Œ *Ï€ [v]*ã€‚è¯¥è¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: If *d[v]> d[u] + w (u, v)* then
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœ *d[v]> d[u] + w (u, v)* åˆ™
- en: '*d[v] = d[u] + w (u, v)*'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*d[v] = d[u] + w (u, v)*'
- en: '*Ï€ [v] = u*'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Ï€ [v] = u*'
- en: 'The algorithm basically performs two operations: a node selection operation
    and an operation to update the distances. The first selects the node with the
    value of the lowest label at each step; the other verifies the condition *d[v]>
    d[u] + w(u, v)* and, if so, updates the value of the label placing *d[v] = d[u]
    + w (u, v)*.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•åŸºæœ¬ä¸Šæ‰§è¡Œä¸¤ä¸ªæ“ä½œï¼šèŠ‚ç‚¹é€‰æ‹©æ“ä½œå’Œæ›´æ–°è·ç¦»çš„æ“ä½œã€‚ç¬¬ä¸€ä¸ªæ“ä½œåœ¨æ¯ä¸€æ­¥é€‰æ‹©æ ‡ç­¾å€¼æœ€ä½çš„èŠ‚ç‚¹ï¼›å¦ä¸€ä¸ªæ“ä½œéªŒè¯æ¡ä»¶ *d[v]> d[u] + w(u,
    v)*ï¼Œå¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œåˆ™æ›´æ–°æ ‡ç­¾å€¼ï¼Œä»¤ *d[v] = d[u] + w (u, v)*ã€‚
- en: In the following section, we will implement a TD method to address a real-life
    application.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ç§ TD æ–¹æ³•æ¥è§£å†³ä¸€ä¸ªå®é™…åº”ç”¨é—®é¢˜ã€‚
- en: Implementing TD methods to theÂ vehicle routing problem
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°† TD æ–¹æ³•åº”ç”¨äºè½¦è¾†è·¯å¾„é—®é¢˜
- en: Given a weighted graph and a designated vertex *V*, it is often requested to
    find the path from a node to each of the other vertices in the graph. Identifying
    a path connecting two or more nodes of a graph is a problem that appears as a
    subproblem of many other problems of discrete optimization and has numerous applications
    in the real world.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªåŠ æƒå›¾å’Œä¸€ä¸ªæŒ‡å®šçš„é¡¶ç‚¹ *V*ï¼Œé€šå¸¸éœ€è¦æ‰¾å‡ºä»ä¸€ä¸ªèŠ‚ç‚¹åˆ°å›¾ä¸­æ¯ä¸ªå…¶ä»–é¡¶ç‚¹çš„è·¯å¾„ã€‚è¯†åˆ«è¿æ¥ä¸¤ä¸ªæˆ–å¤šä¸ªèŠ‚ç‚¹çš„è·¯å¾„æ˜¯è®¸å¤šç¦»æ•£ä¼˜åŒ–é—®é¢˜çš„å­é—®é¢˜ï¼Œå¹¶ä¸”åœ¨ç°å®ä¸–ç•Œä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚
- en: Consider, for example, the problem of identifying a route between two locations
    shown on a road map, where the vertices are the localities, while the edges are
    the roads that connect them. In this case, each cost is associated with the length
    in kilometers of the road or the average time needed to cover it. If instead of
    any path, we want to identify one of the minimum total cost, then the resulting
    problem is known as the problem of the shortest path in a graph. In other words,
    the shortest path between two vertices of a graph is that path that connects these
    vertices and minimizes the sum of the costs associated with crossing each edge.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ªé—®é¢˜ï¼šåœ¨ä¸€å¼ é“è·¯åœ°å›¾ä¸Šæ ‡è¯†ä¸¤åœ°ä¹‹é—´çš„è·¯çº¿ï¼Œå…¶ä¸­é¡¶ç‚¹è¡¨ç¤ºåœ°ç‚¹ï¼Œè¾¹è¡¨ç¤ºè¿æ¥å®ƒä»¬çš„é“è·¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªä»£ä»·éƒ½ä¸é“è·¯çš„å…¬é‡Œé•¿åº¦æˆ–è¦†ç›–è¯¥æ®µé“è·¯çš„å¹³å‡æ—¶é—´ç›¸å…³ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦è¯†åˆ«çš„æ˜¯æœ€å°æ€»ä»£ä»·çš„è·¯å¾„ï¼Œè€Œéä»»æ„è·¯å¾„ï¼Œé‚£ä¹ˆæ‰€å¾—åˆ°çš„é—®é¢˜è¢«ç§°ä¸ºå›¾ä¸­çš„æœ€çŸ­è·¯å¾„é—®é¢˜ã€‚æ¢å¥è¯è¯´ï¼Œå›¾ä¸­ä¸¤ä¸ªé¡¶ç‚¹ä¹‹é—´çš„æœ€çŸ­è·¯å¾„æ˜¯è¿æ¥è¿™ä¸¤ä¸ªé¡¶ç‚¹å¹¶æœ€å°åŒ–ç©¿è¶Šæ¯æ¡è¾¹çš„ä»£ä»·ä¹‹å’Œçš„è·¯å¾„ã€‚
- en: So, let's take a practical exampleâ€”consider a tourist visiting Italy by car
    who wants to reach Venice from Rome. Having a map of Italy available in which,
    for each direct link between the cities, its lengthÂ is marked, how can the tourist
    find the shortest path?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…çš„ä¾‹å­æ¥è€ƒè™‘â€”â€”å‡è®¾ä¸€ä½æ¸¸å®¢å¼€è½¦ä»ç½—é©¬å‰å¾€å¨å°¼æ–¯ã€‚å‡è®¾ä»–æ‰‹ä¸­æœ‰ä¸€å¼ æ„å¤§åˆ©åœ°å›¾ï¼Œä¸Šé¢æ ‡å‡ºäº†å„ä¸ªåŸå¸‚ä¹‹é—´çš„ç›´è¿è·¯å¾„åŠå…¶é•¿åº¦ï¼Œæ¸¸å®¢å¦‚ä½•æ‰¾åˆ°æœ€çŸ­çš„è·¯å¾„ï¼Ÿ
- en: The system can be schematized with a graph in which each city corresponds to
    a vertex, and the roads correspond to the connecting arcs between the vertices.
    You need to determine the shortest path between the source vertex and the target
    vertex of the graph.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç³»ç»Ÿå¯ä»¥é€šè¿‡ä¸€ä¸ªå›¾æ¥ç¤ºæ„ï¼Œå…¶ä¸­æ¯ä¸ªåŸå¸‚å¯¹åº”ä¸€ä¸ªé¡¶ç‚¹ï¼Œé“è·¯å¯¹åº”é¡¶ç‚¹ä¹‹é—´çš„è¿æ¥å¼§ã€‚ä½ éœ€è¦ç¡®å®šå›¾ä¸­æºé¡¶ç‚¹å’Œç›®æ ‡é¡¶ç‚¹ä¹‹é—´çš„æœ€çŸ­è·¯å¾„ã€‚
- en: A solution to the problem is to number all possible routes from Rome to Venice.
    For each route, calculate the total length and then select the shortest. This
    solution is not the most efficient because there are millions of paths to analyze.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥é—®é¢˜çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸ºä»ç½—é©¬åˆ°å¨å°¼æ–¯çš„æ‰€æœ‰å¯èƒ½è·¯çº¿ç¼–å·ã€‚å¯¹äºæ¯æ¡è·¯çº¿ï¼Œè®¡ç®—æ€»é•¿åº¦ï¼Œç„¶åé€‰æ‹©æœ€çŸ­çš„ä¸€æ¡ã€‚è¿™ä¸ªè§£å†³æ–¹æ¡ˆä¸æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œå› ä¸ºéœ€è¦åˆ†æçš„è·¯å¾„æœ‰æ•°ç™¾ä¸‡æ¡ã€‚
- en: In practice, we will model the map of Italy as a weighted oriented graph *G
    = (V, E)*, where each vertex represents a city, each edge *(u, v)* represents
    a direct path from *u* to *v* and each weight *w (u, v)* corresponding to an edge
    *(u, v)* represents the distance between *u* and *v*. So, the problem to be solved
    is that of finding the shortest path that connects the vertex corresponding to
    Rome with that corresponding to Venice.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å°†æ„å¤§åˆ©åœ°å›¾å»ºæ¨¡ä¸ºä¸€ä¸ªåŠ æƒæœ‰å‘å›¾ *G = (V, E)*ï¼Œå…¶ä¸­æ¯ä¸ªé¡¶ç‚¹è¡¨ç¤ºä¸€ä¸ªåŸå¸‚ï¼Œæ¯æ¡è¾¹ *(u, v)* è¡¨ç¤ºä» *u* åˆ° *v*
    çš„ç›´æ¥è·¯å¾„ï¼Œè€Œæ¯ä¸ªæƒé‡ *w(u, v)* å¯¹åº”äºè¾¹ *(u, v)*ï¼Œè¡¨ç¤º *u* å’Œ *v* ä¹‹é—´çš„è·ç¦»ã€‚å› æ­¤ï¼Œè¦è§£å†³çš„é—®é¢˜æ˜¯æ‰¾åˆ°ä»è¡¨ç¤ºç½—é©¬çš„é¡¶ç‚¹åˆ°è¡¨ç¤ºå¨å°¼æ–¯çš„é¡¶ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚
- en: 'Given a weighted directed graph *G = (V, E)*, the weight of a path *p = (v0,
    v1, ..., vk)* is given by the sum of the weights of the edges that constitute
    it, as shown in the following formula:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªåŠ æƒæœ‰å‘å›¾ *G = (V, E)*ï¼Œè·¯å¾„ *p = (v0, v1, ..., vk)* çš„æƒé‡ç”±å…¶ç»„æˆçš„è¾¹çš„æƒé‡ä¹‹å’Œç»™å‡ºï¼Œå¦‚ä¸‹å…¬å¼æ‰€ç¤ºï¼š
- en: '![](img/9e8dcd20-9e19-4a85-a2e6-2af524fe1dd3.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e8dcd20-9e19-4a85-a2e6-2af524fe1dd3.png)'
- en: 'The shortest path from node *u* to node *v* of *V* is a path *p = (u, v1, v2,
    ..., v)* so that *w(p)* is minimal, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ä»èŠ‚ç‚¹ *u* åˆ°èŠ‚ç‚¹ *v* çš„æœ€çŸ­è·¯å¾„æ˜¯ä¸€ä¸ªè·¯å¾„ *p = (u, v1, v2, ..., v)*ï¼Œä½¿å¾— *w(p)* æœ€å°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/4cc61d83-cabb-4482-b856-f6fff9032a19.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc61d83-cabb-4482-b856-f6fff9032a19.png)'
- en: The cost of the minimum path from *u* to *v* is denoted by *Î´(u, v)*. If there
    is no path from *u* to *v* then *Î´ (u, v) = âˆ*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä» *u* åˆ° *v* çš„æœ€çŸ­è·¯å¾„çš„ä»£ä»·ç”¨ *Î´(u, v)* è¡¨ç¤ºã€‚å¦‚æœä» *u* åˆ° *v* æ²¡æœ‰è·¯å¾„ï¼Œåˆ™ *Î´(u, v) = âˆ*ã€‚
- en: Given a connected weighted graph *G = (V, E)* and a source node *s* of *V*,
    there are several algorithms to find a shortest path from *s* toward each other
    node of *V*. In the previous section, we analyzed the Dijkstra algorithm, now
    the time has come to tackle the problem using algorithms based on reinforcement
    learning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªè¿é€šçš„åŠ æƒå›¾ *G = (V, E)* å’Œä¸€ä¸ªæºèŠ‚ç‚¹ *s*ï¼Œæœ‰å¤šç§ç®—æ³•å¯ä»¥æ‰¾åˆ°ä» *s* åˆ° *V* ä¸­å…¶ä»–èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†
    Dijkstra ç®—æ³•ï¼Œç°åœ¨æ˜¯æ—¶å€™ä½¿ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜äº†ã€‚
- en: As anticipated at the beginning of this chapter, the Vehicle Routing Problem
    (VRP) is a typical distribution and transport problem, which consists of optimizing
    the use of a set of vehicles with limited capacity to pick up and deliver goods
    or people to geographically distributed stations. Managing these operations in
    the best possible way can significantly reduce costs. Before tackling the problem
    with Python code, let's analyze the basic characteristics of the topic to understand
    possible solutions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æœ¬ç« å¼€å§‹æ—¶é¢„æœŸçš„é‚£æ ·ï¼Œè½¦è¾†è·¯å¾„é—®é¢˜ï¼ˆVRPï¼‰æ˜¯ä¸€ä¸ªå…¸å‹çš„é…é€å’Œè¿è¾“é—®é¢˜ï¼Œæ—¨åœ¨ä¼˜åŒ–ä½¿ç”¨ä¸€ç»„æœ‰é™å®¹é‡çš„è½¦è¾†æ¥æ¥é€è´§ç‰©æˆ–äººå‘˜ï¼Œå¹¶å°†å…¶è¿é€åˆ°åœ°ç†ä¸Šåˆ†å¸ƒçš„ç«™ç‚¹ã€‚ä»¥æœ€ä½³æ–¹å¼ç®¡ç†è¿™äº›æ“ä½œå¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬ã€‚åœ¨ç”¨Pythonä»£ç è§£å†³é—®é¢˜ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åˆ†æä¸€ä¸‹è¿™ä¸€ä¸»é¢˜çš„åŸºæœ¬ç‰¹å¾ï¼Œä»¥ä¾¿ç†è§£å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚
- en: Based on what has been said so far, it is clear that a problem of this type
    is configured as a path optimization procedure that can be conveniently dealt
    with using graph theory.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿„ä»Šä¸ºæ­¢æ‰€è¿°ï¼Œå¾ˆæ˜æ˜¾ï¼Œè¿™ç±»é—®é¢˜å¯ä»¥è¢«è§†ä¸ºè·¯å¾„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥é€šè¿‡å›¾è®ºæœ‰æ•ˆåœ°è§£å†³ã€‚
- en: 'Suppose we have the following graph with the distances between vertices indicated
    on the edges:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å›¾ï¼Œå…¶ä¸­è¾¹ä¸Šçš„æ•°å­—è¡¨ç¤ºé¡¶ç‚¹ä¹‹é—´çš„è·ç¦»ï¼š
- en: '![](img/cec69904-6c1f-479c-8e23-f1f49bcb12c0.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cec69904-6c1f-479c-8e23-f1f49bcb12c0.png)'
- en: It is easy to see that the shortest path from 1 to 6 isÂ 1 â€“ 2 â€“ 5 â€“ 4 - 6.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œä»1åˆ°6çš„æœ€çŸ­è·¯å¾„æ˜¯1 â€“ 2 â€“ 5 â€“ 4 - 6ã€‚
- en: In the *Understanding TD methods* section, we have seen that the method of choosing
    an action diversifies the types of algorithms based on the TD. In on-policy based
    methods (SARSA), the update is carried out based on the results of the actions
    determined by the selected policy, while in the off-policy methods (Q-learning),
    the policies are evaluated through hypothetical actions, not actually undertaken.
    We will address the problem just introduced through both approaches, highlighting
    the merits and defects of the solutions obtained.Â So, let's see how to deal with
    the problem of vehicle routing using Q-learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*ç†è§£TDæ–¹æ³•*éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œé€‰æ‹©åŠ¨ä½œçš„æ–¹æ³•æ ¹æ®TDçš„ä¸åŒï¼Œç®—æ³•çš„ç±»å‹ä¹Ÿä¼šæœ‰æ‰€ä¸åŒã€‚åœ¨åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼ˆSARSAï¼‰ä¸­ï¼Œæ›´æ–°æ˜¯æ ¹æ®ç”±é€‰å®šç­–ç•¥å†³å®šçš„åŠ¨ä½œç»“æœè¿›è¡Œçš„ï¼Œè€Œåœ¨ç¦»ç­–ç•¥æ–¹æ³•ï¼ˆQ-learningï¼‰ä¸­ï¼Œç­–ç•¥æ˜¯é€šè¿‡å‡è®¾çš„åŠ¨ä½œæ¥è¯„ä¼°çš„ï¼Œè€Œè¿™äº›åŠ¨ä½œå¹¶æœªçœŸæ­£æ‰§è¡Œã€‚æˆ‘ä»¬å°†é€šè¿‡è¿™ä¸¤ç§æ–¹æ³•æ¥è§£å†³åˆšæ‰æåˆ°çš„é—®é¢˜ï¼Œçªæ˜¾è§£å†³æ–¹æ¡ˆçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨Q-learningæ¥å¤„ç†è½¦è¾†è·¯å¾„é—®é¢˜ã€‚
- en: The Q-learning approach
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learningæ–¹æ³•
- en: As we said in theÂ *Q-learning* section, Q-learning tries to maximize the value
    of the Q function (action-value function), which represents the maximum discounted
    future reward when we perform actions *a* in the state *s*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨*Q-learning*éƒ¨åˆ†æ‰€è¯´ï¼ŒQ-learningè¯•å›¾æœ€å¤§åŒ–Qå‡½æ•°ï¼ˆåŠ¨ä½œ-ä»·å€¼å‡½æ•°ï¼‰çš„å€¼ï¼Œè¯¥å‡½æ•°è¡¨ç¤ºå½“æˆ‘ä»¬åœ¨çŠ¶æ€*s*ä¸­æ‰§è¡ŒåŠ¨ä½œ*a*æ—¶ï¼Œèƒ½å¤Ÿè·å¾—çš„æœ€å¤§æŠ˜æ‰£æœªæ¥å¥–åŠ±ã€‚
- en: 'The following block is an implementation of R code that allows us to research
    this path through the technique of Q-learning:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å—æ˜¯ä¸€ä¸ªRä»£ç çš„å®ç°ï¼Œé€šè¿‡Qå­¦ä¹ æŠ€æœ¯è®©æˆ‘ä»¬ç ”ç©¶è¿™ä¸€è·¯å¾„ï¼š
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will analyze the code line by line, starting from the setting of the following
    parameters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€è¡Œåˆ†æä»£ç ï¼Œä»ä»¥ä¸‹å‚æ•°çš„è®¾ç½®å¼€å§‹ï¼š
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we have the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹å†…å®¹ï¼š
- en: '`N`: the number of episodes to iterate'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`: è¿­ä»£çš„å›åˆæ•°'
- en: '`gamma`: the discount factor'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`: æŠ˜æ‰£å› å­'
- en: '`alpha`: the learning rate'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: å­¦ä¹ ç‡'
- en: '`FinalState`: the target node'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FinalState`: ç›®æ ‡èŠ‚ç‚¹'
- en: 'Let''s move on the reward matrix setting:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­è®¾ç½®å¥–åŠ±çŸ©é˜µï¼š
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We see the matrix as it appears:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°çŸ©é˜µçš„å‘ˆç°æ–¹å¼å¦‚ä¸‹ï¼š
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following matrix is printed:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹çŸ©é˜µè¢«æ‰“å°å‡ºæ¥ï¼š
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s try to understand how we set this matrix. It''s all very simple: we
    have associated a high reward at the most convenient edges, those with a lower
    weight (which means shorter). We then associated the highest reward (100) to the
    edge that leads us to the goal. Finally, we associated a negative reward to non-existent
    links. The following chart shows how we set the rewards:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•ç†è§£å¦‚ä½•è®¾ç½®è¿™ä¸ªçŸ©é˜µã€‚ä¸€åˆ‡éƒ½éå¸¸ç®€å•ï¼šæˆ‘ä»¬åœ¨æœ€æ–¹ä¾¿çš„è¾¹ä¸Šï¼ˆé‚£äº›æƒé‡è¾ƒä½çš„è¾¹ï¼Œå³è¾ƒçŸ­çš„è·¯å¾„ï¼‰å…³è”äº†é«˜å¥–åŠ±ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æœ€é«˜çš„å¥–åŠ±ï¼ˆ100ï¼‰èµ‹ç»™äº†é€šå‘ç›®æ ‡çš„è¾¹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è´Ÿå¥–åŠ±åˆ†é…ç»™ä¸å­˜åœ¨çš„è¿æ¥ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†æˆ‘ä»¬å¦‚ä½•è®¾ç½®å¥–åŠ±ï¼š
- en: '![](img/637421dd-564f-49df-8343-4d268a577cb8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/637421dd-564f-49df-8343-4d268a577cb8.png)'
- en: We have simply replaced the weight of the edges with the rewards set to the
    value of the lengths. Edges with longer lengths return a low reward, edges with
    smaller lengths return a high reward. Finally, the maximum reward is achieved
    when the target is reached.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæ˜¯å°†è¾¹çš„æƒé‡æ›¿æ¢æˆä¸é•¿åº¦å€¼ç›¸å¯¹åº”çš„å¥–åŠ±ã€‚è¾ƒé•¿çš„è¾¹è¿”å›è¾ƒä½çš„å¥–åŠ±ï¼Œè€Œè¾ƒçŸ­çš„è¾¹è¿”å›è¾ƒé«˜çš„å¥–åŠ±ã€‚æœ€ç»ˆï¼Œå½“ç›®æ ‡åˆ°è¾¾æ—¶ï¼Œå°†è·å¾—æœ€å¤§çš„å¥–åŠ±ã€‚
- en: As we said in the *Q-learning* section, our goal is to estimate an evaluation
    function that evaluates the convenience of a policy based on the sum of the rewards.
    The Q-learning algorithm tries to maximize the value of the Q function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in the state *s*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨*Q-learning*éƒ¨åˆ†æ‰€è¯´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡ä¸€ä¸ªè¯„ä¼°å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®å¥–åŠ±çš„æ€»å’Œæ¥è¯„ä¼°ç­–ç•¥çš„ä¾¿åˆ©æ€§ã€‚Q-learningç®—æ³•è¯•å›¾æœ€å¤§åŒ–Qå‡½æ•°ï¼ˆè¡ŒåŠ¨å€¼å‡½æ•°ï¼‰çš„å€¼ï¼ŒQå‡½æ•°è¡¨ç¤ºæˆ‘ä»¬åœ¨çŠ¶æ€*s*ä¸­æ‰§è¡ŒåŠ¨ä½œ*a*æ—¶çš„æœ€å¤§æŠ˜ç°æœªæ¥å¥–åŠ±ã€‚
- en: 'Let''s analyze again the procedure that we have to implement using R:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†æ¬¡åˆ†ææˆ‘ä»¬éœ€è¦ä½¿ç”¨Rå®ç°çš„ç¨‹åºï¼š
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At each step, the agent observes the current state of the environment and using
    the Ï€ policy selects and executes the action. By executing the action, the agent
    obtains the reward ğ‘…ğ‘¡ *+ 1* and the new state ğ‘†ğ‘¡ *+ 1*. At this point, the agent
    can calculate ğ‘„ (*s*ğ‘¡, *a*ğ‘¡) updating the estimate.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€æ­¥ï¼Œä»£ç†è§‚å¯Ÿç¯å¢ƒçš„å½“å‰çŠ¶æ€ï¼Œå¹¶ä½¿ç”¨Ï€ç­–ç•¥é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œã€‚é€šè¿‡æ‰§è¡Œè¯¥åŠ¨ä½œï¼Œä»£ç†è·å¾—å¥–åŠ±ğ‘…ğ‘¡ *+ 1*å’Œæ–°çŠ¶æ€ğ‘†ğ‘¡ *+ 1*ã€‚æ­¤æ—¶ï¼Œä»£ç†å¯ä»¥é€šè¿‡æ›´æ–°ä¼°è®¡æ¥è®¡ç®—ğ‘„
    (*s*ğ‘¡, *a*ğ‘¡)ã€‚
- en: 'Therefore, the Q function represents the essential element of the procedure;
    it is a matrix of the same dimensions as the rewards matrix. First, let''s initialize
    it with all zeros:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒQå‡½æ•°ä»£è¡¨äº†ç¨‹åºçš„æ ¸å¿ƒå…ƒç´ ï¼›å®ƒæ˜¯ä¸€ä¸ªä¸å¥–åŠ±çŸ©é˜µç»´åº¦ç›¸åŒçš„çŸ©é˜µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å…¶åˆå§‹åŒ–ä¸ºå…¨é›¶çŸ©é˜µï¼š
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At this point, we have to set up a cycle that will repeat the operations for
    each episode:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»è®¾ç½®ä¸€ä¸ªå¾ªç¯ï¼Œå¯¹æ¯ä¸ªå›åˆé‡å¤æ“ä½œï¼š
- en: '[PRE22]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The initial part of the cycle is used to set the initial state and the initial
    policy; in our case, we will choose an initial state randomly:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯çš„åˆå§‹éƒ¨åˆ†ç”¨äºè®¾ç½®åˆå§‹çŠ¶æ€å’Œåˆå§‹ç­–ç•¥ï¼›åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†éšæœºé€‰æ‹©ä¸€ä¸ªåˆå§‹çŠ¶æ€ï¼š
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After setting the initial state, we must insert a cycle that will be repeated
    until the final state, that is, our target, has been reached:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®åˆå§‹çŠ¶æ€åï¼Œæˆ‘ä»¬å¿…é¡»æ’å…¥ä¸€ä¸ªå¾ªç¯ï¼Œç›´åˆ°è¾¾åˆ°æœ€ç»ˆçŠ¶æ€ï¼Œå³æˆ‘ä»¬çš„ç›®æ ‡ï¼š
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we must choose the next state according to the possible actions available
    in the current state. To move to the next node, what actions can we take? If only
    one possible action is available, we will choose that one. Otherwise, we will
    choose one at random, only to then analyze the others:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¿…é¡»æ ¹æ®å½“å‰çŠ¶æ€ä¸­å¯ç”¨çš„å¯èƒ½åŠ¨ä½œé€‰æ‹©ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ä¸ºäº†ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Ÿå¦‚æœåªæœ‰ä¸€ä¸ªå¯èƒ½çš„åŠ¨ä½œå¯ç”¨ï¼Œæˆ‘ä»¬å°†é€‰æ‹©é‚£ä¸ªåŠ¨ä½œã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œç„¶åå†åˆ†æå…¶ä»–åŠ¨ä½œï¼š
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Based on the results obtained, we can update the action-value function (`QMatrix`):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è·å¾—çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°è¡ŒåŠ¨å€¼å‡½æ•°ï¼ˆ`QMatrix`ï¼‰ï¼š
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The formula used for updating the Q function is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºæ›´æ–°Qå‡½æ•°çš„å…¬å¼å¦‚ä¸‹ï¼š
- en: '![](img/49844e1b-bcec-4e0b-8109-ca1c62ad23b7.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49844e1b-bcec-4e0b-8109-ca1c62ad23b7.png)'
- en: 'Now, we will check the status achieved: if we have reached our target, then
    we will exit the cycle with the break command; otherwise, we will set the next
    status as the current one (`NextState`):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å·²è¾¾æˆçš„çŠ¶æ€ï¼šå¦‚æœæˆ‘ä»¬å·²ç»è¾¾åˆ°äº†ç›®æ ‡ï¼Œåˆ™ä½¿ç”¨breakå‘½ä»¤é€€å‡ºå¾ªç¯ï¼›å¦åˆ™ï¼Œæˆ‘ä»¬å°†æŠŠä¸‹ä¸€ä¸ªçŠ¶æ€è®¾ä¸ºå½“å‰çŠ¶æ€ï¼ˆ`NextState`ï¼‰ï¼š
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once the procedure is finished, we will print the Q matrix:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ç¨‹åºå®Œæˆï¼Œæˆ‘ä»¬å°†æ‰“å°QçŸ©é˜µï¼š
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following result is returned:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's try to understand what this matrix tells us. To start, we can say that
    this matrix allows us to calculate the shortest path starting from any state,
    therefore, not necessarily from node 1\. In our case, we will start from node
    1 in order to confirm what was obtained visually. Recall that each row of the
    matrix represents a state and each value in the column tells us what the reward
    is in the transition to the state marked by the column index.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•ç†è§£è¿™ä¸ªçŸ©é˜µå‘Šè¯‰äº†æˆ‘ä»¬ä»€ä¹ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è¯´è¿™ä¸ªçŸ©é˜µå…è®¸æˆ‘ä»¬è®¡ç®—ä»ä»»ä½•çŠ¶æ€å¼€å§‹çš„æœ€çŸ­è·¯å¾„ï¼Œå› æ­¤ï¼Œä¸ä¸€å®šæ˜¯ä»èŠ‚ç‚¹1å¼€å§‹ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»èŠ‚ç‚¹1å¼€å§‹ï¼Œä»¥ç¡®è®¤è§†è§‰ä¸Šè·å¾—çš„ç»“æœã€‚å›æƒ³ä¸€ä¸‹ï¼ŒçŸ©é˜µçš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªçŠ¶æ€ï¼Œæ¯ä¸€åˆ—ä¸­çš„å€¼å‘Šè¯‰æˆ‘ä»¬è½¬ç§»åˆ°åˆ—ç´¢å¼•æ ‡è®°çš„çŠ¶æ€æ—¶çš„å¥–åŠ±ã€‚
- en: 'In the following flow path, we have the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„æµç¨‹è·¯å¾„ä¸­ï¼Œæˆ‘ä»¬æœ‰å¦‚ä¸‹å†…å®¹ï¼š
- en: Starting from the first line, we see that the maximum value is in correspondence
    of the second column, so the best path takes us from state 1 to 2.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç¬¬ä¸€è¡Œå¼€å§‹ï¼Œæˆ‘ä»¬çœ‹åˆ°æœ€å¤§å€¼ä½äºç¬¬äºŒåˆ—ï¼Œå› æ­¤ï¼Œæœ€ä½³è·¯å¾„å°†æˆ‘ä»¬ä»çŠ¶æ€1å¸¦åˆ°çŠ¶æ€2ã€‚
- en: We then pass to the state 2 identified by the second row; here, we see that
    the greatest value of reward is in correspondence of the fifth column, therefore,
    the best path takes us from state 2 to 5.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è¿›å…¥ç”±ç¬¬äºŒè¡Œæ ‡è¯†çš„çŠ¶æ€2ï¼›åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æœ€å¤§å¥–åŠ±å€¼ä½äºç¬¬äº”åˆ—ï¼Œå› æ­¤ï¼Œæœ€ä½³è·¯å¾„å°†æˆ‘ä»¬ä»çŠ¶æ€2å¸¦åˆ°çŠ¶æ€5ã€‚
- en: Let's then move on to state 5 identified by the fifth line. Here, we see that
    the greatest value of reward is in correspondence with the fourth column, therefore,
    the best path takes us from state 5 to 4.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ç»§ç»­è®¨è®ºç”±ç¬¬äº”è¡Œæ ‡è¯†çš„çŠ¶æ€5ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°å¥–åŠ±çš„æœ€å¤§å€¼ä¸ç¬¬å››åˆ—ç›¸å¯¹åº”ï¼Œå› æ­¤ï¼Œæœ€ä½³è·¯å¾„å°†æˆ‘ä»¬ä»çŠ¶æ€5å¸¦åˆ°çŠ¶æ€4ã€‚
- en: Finally, we pass to the state 4 identified by the fourth line. Here, we see
    that the greatest value of reward is in correspondence with the sixth column,
    therefore, the best path leads us from state 4 to 6.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è½¬åˆ°ç”±ç¬¬å››è¡Œæ ‡è¯†çš„çŠ¶æ€4ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°å¥–åŠ±çš„æœ€å¤§å€¼ä¸ç¬¬å…­åˆ—ç›¸å¯¹åº”ï¼Œå› æ­¤ï¼Œæœ€ä½³è·¯å¾„å°†æˆ‘ä»¬ä»çŠ¶æ€4å¸¦åˆ°çŠ¶æ€6ã€‚
- en: 'We have reached the target and by doing so we have traced the path shorter
    from node 1 to 6, which is the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»åˆ°è¾¾ç›®æ ‡ï¼Œå¹¶ä¸”é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬ä»èŠ‚ç‚¹1åˆ°èŠ‚ç‚¹6ç»˜åˆ¶äº†æ›´çŸ­çš„è·¯å¾„ï¼Œè·¯å¾„å¦‚ä¸‹ï¼š
- en: 1 â€“ 2 - 5 â€“ 4 â€“ 6
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 1 â€“ 2 - 5 â€“ 4 â€“ 6
- en: 'This path coincides with the one obtained visually at the beginning of the
    section. The procedure for extracting the shortest path of the `QMatrix` matrix
    can be easily automated as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¡è·¯å¾„ä¸æœ¬èŠ‚å¼€å§‹æ—¶è§†è§‰ä¸Šå¾—åˆ°çš„è·¯å¾„ä¸€è‡´ã€‚æå–`QMatrix`çŸ©é˜µçš„æœ€çŸ­è·¯å¾„çš„è¿‡ç¨‹å¯ä»¥å¦‚ä¸‹è½»æ¾åœ°è‡ªåŠ¨åŒ–ï¼š
- en: '[PRE30]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following results are returned:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '[PRE31]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we can see, the same result has been returned.Â Now, let's see what happens
    if we try to tackle the same problem but with a different approach.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¿”å›çš„ç»“æœæ˜¯ç›¸åŒçš„ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬å°è¯•ç”¨ä¸åŒçš„æ–¹æ³•è§£å†³åŒæ ·çš„é—®é¢˜ä¼šå‘ç”Ÿä»€ä¹ˆã€‚
- en: The SARSA approach
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSAæ–¹æ³•
- en: As we anticipated in SARSA, starting from the current state *St* an action *At*
    is taken and the agent gets a reward R. In this way, the agent is transferred
    to the next state *St + 1* and takes an action *At + 1* in *St + 1*. In fact,
    SARSA is the acronym of the tuple (*S, A*, *R*, *St + 1*, *At + 1*).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨SARSAä¸­æ‰€é¢„è§çš„ï¼Œä»å½“å‰çŠ¶æ€*St*å‡ºå‘ï¼Œé‡‡å–ä¸€ä¸ªåŠ¨ä½œ*At*å¹¶ä¸”ä»£ç†è·å¾—å¥–åŠ±Rã€‚è¿™æ ·ï¼Œä»£ç†å°±è¢«è½¬ç§»åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€*St + 1*ï¼Œå¹¶åœ¨*St
    + 1*ä¸­é‡‡å–åŠ¨ä½œ*At + 1*ã€‚å®é™…ä¸Šï¼ŒSARSAæ˜¯å…ƒç»„(*S, A*, *R*, *St + 1*, *At + 1*)çš„ç¼©å†™ã€‚
- en: 'The following is the whole code for the SARSA approach:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯SARSAæ–¹æ³•çš„å®Œæ•´ä»£ç ï¼š
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see, much of the code is similar to the previous case (Q-learning),
    as there are many similarities between the two approaches. We will only analyze
    the differences between the two approaches. In the first part of the code, the
    initial parameters are set and the rewards matrix is defined:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œå¤§éƒ¨åˆ†ä»£ç ä¸å‰ä¸€ä¸ªæ¡ˆä¾‹ï¼ˆQ-learningï¼‰ç›¸ä¼¼ï¼Œå› ä¸ºè¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ã€‚æˆ‘ä»¬åªä¼šåˆ†æä¸¤è€…ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨ä»£ç çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œè®¾ç½®äº†åˆå§‹å‚æ•°å¹¶å®šä¹‰äº†å¥–åŠ±çŸ©é˜µï¼š
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s move on to initialize the matrix Q and set the cycle that will
    allow us to update the action-value function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç»§ç»­åˆå§‹åŒ–QçŸ©é˜µå¹¶è®¾ç½®å°†å…è®¸æˆ‘ä»¬æ›´æ–°åŠ¨ä½œä»·å€¼å‡½æ•°çš„å¾ªç¯ï¼š
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Up to this point, nothing has changed compared to the formulation analyzed
    in the previous example. But now there are essential changes. In the *SARSA* section,
    we saw the pseudo-code of the algorithm; for convenience, we repeat it here:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸å‰ä¸€ä¸ªç¤ºä¾‹ä¸­åˆ†æçš„å…¬å¼ç›¸æ¯”ï¼Œæ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚ä½†ç°åœ¨æœ‰äº†ä¸€äº›é‡è¦çš„å˜åŒ–ã€‚åœ¨*SARSA*éƒ¨åˆ†ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ç®—æ³•çš„ä¼ªä»£ç ï¼›ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åœ¨æ­¤é‡å¤ä¸€ä¸‹ï¼š
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Comparing it with the one proposed in the previous section (the Q-learning
    approach), we can see that the substantial difference between the two methods
    lies in the formula used for updating the action-value function and in the calculation
    of the action to be followed in the next state. The next action we will follow
    in the next state is calculated as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å‰ä¸€éƒ¨åˆ†ï¼ˆQ-learningæ–¹æ³•ï¼‰ä¸­æå‡ºçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸¤ç§æ–¹æ³•çš„å®è´¨æ€§åŒºåˆ«åœ¨äºæ›´æ–°åŠ¨ä½œä»·å€¼å‡½æ•°æ‰€ä½¿ç”¨çš„å…¬å¼ä»¥åŠè®¡ç®—ä¸‹ä¸€çŠ¶æ€è¦é‡‡å–çš„åŠ¨ä½œã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªçŠ¶æ€ä¸­æ‰§è¡Œçš„åŠ¨ä½œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The approach is used to evaluate the next state. The formula we will use will
    be the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•ç”¨äºè¯„ä¼°ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„å…¬å¼å¦‚ä¸‹ï¼š
- en: '![](img/d669691c-1ceb-4ff7-bd8e-1992a2684e0f.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d669691c-1ceb-4ff7-bd8e-1992a2684e0f.png)'
- en: 'This formula in code R becomes this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå…¬å¼åœ¨Rä»£ç ä¸­å˜ä¸ºï¼š
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The rest of the code is similar to the previous section:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä½™çš„ä»£ç ä¸å‰ä¸€éƒ¨åˆ†ç±»ä¼¼ï¼š
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can then analyze the results:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥åˆ†æç»“æœï¼š
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The shortest path is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€çŸ­è·¯å¾„å¦‚ä¸‹ï¼š
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The result is identical to the one obtained in the previous example. Let's understand
    how the two approaches are different from each other.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœä¸å‰ä¸€ä¸ªç¤ºä¾‹ä¸­å¾—åˆ°çš„ç»“æœç›¸åŒã€‚è®©æˆ‘ä»¬æ¥ç†è§£è¿™ä¸¤ç§æ–¹æ³•æœ‰ä½•ä¸åŒã€‚
- en: Differentiating SARSA and Q-learning
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŒºåˆ†SARSAå’ŒQ-learning
- en: 'From the algorithmic point of view, the substantial difference between the
    two approaches we analyzed in the previous sections lies in the two equations
    we used to update the action-value function. Let''s compare them to understand
    them better:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç®—æ³•çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚åˆ†æçš„ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å®è´¨æ€§å·®å¼‚åœ¨äºæˆ‘ä»¬ç”¨æ¥æ›´æ–°åŠ¨ä½œä»·å€¼å‡½æ•°çš„ä¸¤ä¸ªæ–¹ç¨‹ã€‚æˆ‘ä»¬æ¥å¯¹æ¯”ä¸€ä¸‹å®ƒä»¬ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£ï¼š
- en: '![](img/3feba7be-e270-4ece-af07-033418ce16e7.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3feba7be-e270-4ece-af07-033418ce16e7.png)'
- en: '![](img/ec0f3f35-3304-4f66-962f-8fcedf1701f1.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec0f3f35-3304-4f66-962f-8fcedf1701f1.png)'
- en: 'Q-learning calculates the difference between *Q (s, a)* and the maximum value
    of the action, while SARSA calculates the difference between *Q (s, a)* and the
    value of the next action. In doing this, you can highlight the following points:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ è®¡ç®—*Q (s, a)*å’ŒåŠ¨ä½œçš„æœ€å¤§å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œè€ŒSARSAè®¡ç®—*Q (s, a)*ä¸ä¸‹ä¸€æ­¥åŠ¨ä½œçš„å€¼ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨è¿™æ ·åšæ—¶ï¼Œæ‚¨å¯ä»¥çªå‡ºä»¥ä¸‹å‡ ç‚¹ï¼š
- en: SARSA uses the policy used by the agent to generate experience in the environment
    (such as epsilon-greedy), in order to select an additional action *A t + 1*. Then,
    it usesÂ *Q (S t + 1, A t +1)* to discount the gamma factor as expected future
    returns in the calculation of the update target.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSAä½¿ç”¨æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­ç”Ÿæˆç»éªŒæ—¶ä½¿ç”¨çš„ç­–ç•¥ï¼ˆå¦‚epsilon-è´ªå¿ƒï¼‰ï¼Œä»¥é€‰æ‹©é¢å¤–çš„åŠ¨ä½œ*A t + 1*ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨*Q (S t + 1, A
    t + 1)*æ¥æŠ˜æ‰£gammaå› å­ï¼Œå¹¶å°†å…¶ä½œä¸ºé¢„æœŸçš„æœªæ¥å›æŠ¥ï¼Œè®¡ç®—æ›´æ–°ç›®æ ‡ã€‚
- en: Q-learning does not use this policy to select an additional action *A t + 1*.
    Instead, it estimates the expected future returns in the update rule as *max Q
    (S t + 1, A)* for all actions.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ ä¸ä½¿ç”¨æ­¤ç­–ç•¥æ¥é€‰æ‹©é¢å¤–çš„åŠ¨ä½œ*A t + 1*ã€‚ç›¸åï¼Œå®ƒåœ¨æ›´æ–°è§„åˆ™ä¸­ä¼°è®¡æœŸæœ›çš„æœªæ¥å›æŠ¥ï¼Œå°†å…¶è¡¨ç¤ºä¸º*max Q (S t + 1, A)*ï¼Œå¹¶å¯¹æ‰€æœ‰åŠ¨ä½œè¿›è¡Œè®¡ç®—ã€‚
- en: 'The two approaches converge to different solutions:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§æ–¹æ³•æ”¶æ•›åˆ°ä¸åŒçš„è§£ï¼š
- en: SARSA converges to the optimal solution by following the same policy that was
    used to generate the experience. This will have elements of randomness to ensure
    convergence.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSAé€šè¿‡éµå¾ªä¸ç”Ÿæˆç»éªŒæ—¶ç›¸åŒçš„ç­–ç•¥æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚è¿™å°†åŒ…å«ä¸€äº›éšæœºæ€§ï¼Œä»¥ç¡®ä¿æ”¶æ•›ã€‚
- en: Q-learning converges into an optimal solution after generating experience and
    training by following a greedy policy.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ é€šè¿‡éµå¾ªè´ªå¿ƒç­–ç•¥ç”Ÿæˆç»éªŒå¹¶è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆæ”¶æ•›åˆ°ä¸€ä¸ªæœ€ä¼˜è§£ã€‚
- en: SARSA is advisable when we need to guarantee the agent's performance during
    the learning process. This is where, in the learning process,Â we must guarantee
    a low number of errors that are expensive for the equipment we are using. Hence,
    we care about its performance during the learning process.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬éœ€è¦ç¡®ä¿æ™ºèƒ½ä½“åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¡¨ç°æ—¶ï¼Œå»ºè®®ä½¿ç”¨SARSAã€‚è¿™æ˜¯å› ä¸ºåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿é”™è¯¯çš„æ•°é‡è¾ƒå°‘ï¼Œè€Œè¿™äº›é”™è¯¯å¯¹äºæˆ‘ä»¬ä½¿ç”¨çš„è®¾å¤‡æ¥è¯´æ˜¯æ˜‚è´µçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å…³å¿ƒå…¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚
- en: An algorithm like Q-learning is advisable in cases where we do not care about
    the agent's performance during the learning process and we only want the agent
    to learn about an optimal greedy policy that we will adopt at the end of the process.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: åƒQå­¦ä¹ è¿™æ ·çš„ç®—æ³•åœ¨æˆ‘ä»¬ä¸å…³å¿ƒå­¦ä¹ è¿‡ç¨‹ä¸­æ™ºèƒ½ä½“çš„è¡¨ç°ï¼Œä»…ä»…å¸Œæœ›æ™ºèƒ½ä½“å­¦ä¼šä¸€ä¸ªæœ€ä¼˜çš„è´ªå¿ƒç­–ç•¥ï¼ˆæˆ‘ä»¬å°†åœ¨è¿‡ç¨‹ç»“æŸæ—¶é‡‡ç”¨ï¼‰æ—¶æ˜¯å€¼å¾—æ¨èçš„ã€‚
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, TD learning algorithms were introduced. These algorithms are
    based on reducing the differences between the estimates that are made by the agent
    at different times. The SARSA algorithm implements an on-policy TD method, while
    Q-learning has off-policy characteristics.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œä»‹ç»äº†TDå­¦ä¹ ç®—æ³•ã€‚è¿™äº›ç®—æ³•åŸºäºå‡å°‘æ™ºèƒ½ä½“åœ¨ä¸åŒæ—¶é—´åšå‡ºçš„ä¼°è®¡ä¹‹é—´çš„å·®å¼‚ã€‚SARSAç®—æ³•å®ç°äº†ä¸€ä¸ªåœ¨ç­–ç•¥çš„TDæ–¹æ³•ï¼Œè€ŒQå­¦ä¹ å…·æœ‰è„±ç¦»ç­–ç•¥çš„ç‰¹ç‚¹ã€‚
- en: Then, the basics of graph theory were addressedâ€”the adjacency matrix and adjacency
    list topics were covered. We have seen how to represent graphs in R using the
    `igraph` package. By doing this, we addressed the shortest path problem. We also
    analyzed the Dijkstra algorithm in R.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä»‹ç»äº†å›¾è®ºçš„åŸºç¡€â€”â€”åŒ…æ‹¬é‚»æ¥çŸ©é˜µå’Œé‚»æ¥åˆ—è¡¨çš„å†…å®¹ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨`igraph`åŒ…åœ¨Rä¸­è¡¨ç¤ºå›¾ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬è§£å†³äº†æœ€çŸ­è·¯å¾„é—®é¢˜ï¼Œå¹¶ä¸”åˆ†æäº†Rä¸­çš„Dijkstraç®—æ³•ã€‚
- en: Finally, the vehicle routing problem was resolved using the Q-learning and SARSA
    algorithms. The differences between the two approaches to solve the problem were
    analyzed in detail.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨Qå­¦ä¹ å’ŒSARSAç®—æ³•è§£å†³äº†è½¦è¾†è·¯å¾„è§„åˆ’é—®é¢˜ã€‚è¯¦ç»†åˆ†æäº†è¿™ä¸¤ç§æ–¹æ³•è§£å†³è¯¥é—®é¢˜çš„å·®å¼‚ã€‚
- en: In the next chapter, we will about learn the fundamental concepts of game theory.
    We will learn how to install and configure the OpenAI Gym library and understand
    how it works. We will learn about the difference between the Q-learning and SARSA
    algorithms and understand how to make a learning and a testing phase. Finally,
    we will learn how to develop OpenAI Gym applications using R.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ åšå¼ˆè®ºçš„åŸºæœ¬æ¦‚å¿µã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å®‰è£…å’Œé…ç½® OpenAI Gym åº“ï¼Œå¹¶ç†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘ä»¬å°†äº†è§£ Q-learning å’Œ SARSA
    ç®—æ³•ä¹‹é—´çš„åŒºåˆ«ï¼Œå¹¶ç†è§£å¦‚ä½•è¿›è¡Œå­¦ä¹ é˜¶æ®µå’Œæµ‹è¯•é˜¶æ®µã€‚æœ€åï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ R å¼€å‘ OpenAI Gym åº”ç”¨ã€‚
