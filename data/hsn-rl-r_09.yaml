- en: Temporal Difference Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序差分学习
- en: '**Temporal difference** (**TD**) learning algorithms are based on reducing
    the differences between estimates that are made by the agent at different times.
    It is a combination of the ideas of the **Monte Carlo** (**MC**) method and **dynamic
    programming** (**DP**). The algorithm can learn directly from raw data, without
    a model of the dynamics of the environment (like MC). Update estimates are based,
    in part, on other learned estimates, without waiting for the result (bootstrap,
    like DP). In this chapter, we will learn how to use TD learning algorithms to
    resolve the vehicle routing problem.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**时序差分** (**TD**) 学习算法基于减少代理在不同时间做出的估计之间的差异。它是 **蒙特卡洛** (**MC**) 方法和 **动态规划**
    (**DP**) 思想的结合。该算法可以直接从原始数据中学习，而无需环境动态模型（就像 MC）。更新估计部分依赖于其他已学得的估计，而无需等待结果（自举，就像
    DP）。在本章中，我们将学习如何使用 TD 学习算法来解决车辆路径规划问题。'
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding TD methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 TD 方法
- en: Introducing graph theory and its implementation in R
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍图论及其在 R 中的实现
- en: Implementing TD methods to the vehicle routing problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 TD 方法应用于车辆路径规划问题
- en: By the end of this chapter, you will have learned about the different types
    of TD learning algorithms and how to use them to predict the future behavior of
    a system. We will learn about the basic concepts of the Q-learning algorithm and
    use them to generate system behavior through the current best policy estimate.
    Finally, we will differentiate between SARSA and the Q-learning approach.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将学习到不同类型的 TD 学习算法，并了解如何使用它们来预测系统的未来行为。我们将学习 Q 学习算法的基本概念，并使用它们通过当前最优策略估计生成系统行为。最后，我们将区分
    SARSA 和 Q 学习方法。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码的实际操作：
- en: '[http://bit.ly/2YTB7dD](http://bit.ly/2YTB7dD)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2YTB7dD](http://bit.ly/2YTB7dD)'
- en: Understanding TD methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 TD 方法
- en: TD methods are based on reducing the differences between the estimates that
    are made by the agent at different times. Q-learning, which we will learn about
    in the following section, is a TD algorithm, but it is based on the difference
    between states in immediate adjacent instants. TD is more generic and may consider
    moments and states that are further away.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TD 方法基于减少代理在不同时间做出的估计之间的差异。Q 学习是一个 TD 算法，我们将在接下来的部分学习，它基于相邻时刻状态之间的差异。TD 方法更加通用，可能会考虑更远的时刻和状态。
- en: 'TD methods are a combination of the ideas of the MC method and DP, which, as
    you may recall, can be summarized as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TD 方法结合了 MC 方法和动态规划（DP）的思想，正如你可能记得的那样，可以总结如下：
- en: MC methods allow us to solve reinforcement learning problems based on the average
    of the results obtained.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MC 方法使我们能够根据获得结果的平均值来解决强化学习问题。
- en: DP represents a set of algorithms that can be used to calculate an optimal policy
    when given a perfect model of the environment in the form of an MDP.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP 代表一组算法，这些算法可以在给定环境的完美模型（MDP）下，用于计算最优策略。
- en: The TD methods, on the one hand, inherit the idea of learning directly from
    the experience accumulated interacting with the system, without the dynamics of
    the system itself, from the Monte Carlo method. While they inherit from the DP
    methods, the idea is to update the estimate of functions in a state from the estimates
    made in other states (bootstrap). TD methods are suitable for learning without
    a model of dynamic environments. You need to converge using a fixed policy if
    the time step is sufficiently small or if it reduces over time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，TD 方法继承了从与系统交互中积累的经验中直接学习的思想，这与蒙特卡洛（MC）方法类似，而不需要系统本身的动态信息。另一方面，它们继承了动态规划（DP）方法的思想，即基于其他状态的估计来更新某一状态下的函数估计（自举）。TD
    方法适合在没有动态环境模型的情况下进行学习。如果时间步长足够小，或者随着时间的推移减少，你需要通过一个固定策略来收敛。
- en: Such methods differ from other techniques because they try to minimize the error
    of consecutive time forecasts. To achieve this goal, these methods rewrite the
    update of the value function in the form of a Bellman equation, thereby improving
    the prediction by bootstrapping. Here, the variance of the forecast is reduced
    in each update step. To get a backpropagation of updates in order to save memory,
    an eligibility vector is applied. Example trajectories are used more efficiently,
    resulting in good learning rates.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法与其他技术的不同之处在于，它们试图最小化连续时间预测的误差。为了实现这一目标，这些方法将价值函数的更新重写为贝尔曼方程的形式，从而通过自举法提高预测精度。在这里，每次更新步骤都会减少预测的方差。为了实现更新的反向传播并节省内存，采用了资格向量。示例轨迹的使用效率更高，从而获得了良好的学习速率。
- en: 'The methods based on time differences allow us to manage the problem of control
    (that is, to search for the optimal policy) by letting us update the value functions
    based on the results of the transition to the next state. At every step, the function
    *Q* (action-value function) is updated based on the value it has assumed for the
    next state-action pair and the reward that''s obtained through the following equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间差异的方法使我们能够通过根据向下一个状态过渡的结果来更新价值函数，从而管理控制问题（即寻找最优策略）。在每一步中，函数*Q*（行动-价值函数）基于它为下一个状态-动作对所假定的值以及通过以下方程获得的奖励来更新：
- en: '![](img/4fc3644e-c504-4a0e-a6cb-af4305e45efe.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fc3644e-c504-4a0e-a6cb-af4305e45efe.png)'
- en: 'By adopting a one-step look-ahead, it is clear that a two-step formula can
    also be used, as shown in the following formula:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用一步前瞻，很明显，也可以使用两步公式，如下所示：
- en: '![](img/099626e5-5156-4773-a468-7559189b2dab.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/099626e5-5156-4773-a468-7559189b2dab.png)'
- en: 'The term look-ahead specifies the procedure that tries to predict the effects
    of choosing a branching variable in the evaluation of one of its values. This
    procedure has the following purposes: to choose a variable to be evaluated later
    and to evaluate the order of the values to be assigned to it.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“前瞻”指的是一种试图预测在评估某个值时选择一个分支变量的效果的过程。该过程有以下目的：选择一个变量稍后进行评估，并评估分配给它的值的顺序。
- en: 'More generally, with *n*-step look-ahead, we obtain the following formula:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，通过*n*步前瞻，我们得到以下公式：
- en: '![](img/6807f18d-87c3-400d-b7e9-3eb6a25f8909.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6807f18d-87c3-400d-b7e9-3eb6a25f8909.png)'
- en: An aspect of characterizing the different types of algorithms based on temporal
    difference is the methodology of choosing an action. There are "on-policy" methods, in
    which the update is made based on the results of actions that have been determined
    by the selected policy, and "off-policy" methods, in which various policies can
    be assessed through hypothetical actions, that aren't actually undertaken. Unlike
    "on-policy" methods, the latter can separate the problem of exploration from that
    of control, and learning tactics aren't necessarily applied during the learning
    phase.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时间差异的不同类型算法的一个特征是选择行动的方法。有“在策略”方法，其中更新基于由所选策略确定的行动的结果；还有“离策略”方法，在这种方法中，可以通过假设的行动评估不同的策略，这些假设的行动实际上并未执行。与“在策略”方法不同，后者可以将探索问题与控制问题分离，且学习策略在学习阶段并不一定被应用。
- en: 'In the following sections, we will learn how to implement TD methods through
    two approaches: SARSA and Q-learning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过两种方法学习如何实现时间差异方法：SARSA和Q学习。
- en: SARSA
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA
- en: As we anticipated in [Chapter 1](2362715d-f8f2-435a-9c00-975ed61986a8.xhtml), *Overview
    of Reinforcement Learning with R*, the SARSA algorithm implements an on-policy
    TD method, in which the update of the action-value function (*Q*) is performed
    based on the results of the transition from the state *s = s (t)* to the state
    *s' = s (t + 1)* by the action *a (t)*, which is taken based on a selected policy
    *π (s, a)*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](2362715d-f8f2-435a-9c00-975ed61986a8.xhtml)《使用R进行强化学习概述》中所预期的，SARSA算法实现了一种在策略的时间差异方法，其中，行动-价值函数(*Q*)的更新是基于从状态*s
    = s (t)*过渡到状态*s' = s (t + 1)*的结果，并且该过渡是基于所选策略*π (s, a)*采取的行动*a (t)*。
- en: Some policies always choose the action providing the maximum reward and nondeterministic
    policies (ε-greedy, ε-soft, or softmax), which ensure an element of exploration
    in the learning phase.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一些策略总是选择提供最大奖励的行动，而非确定性策略（如ε-贪心、ε-软策略或软最大策略）则确保在学习阶段有一定的探索成分。
- en: In SARSA, it is necessary to estimate the action-value function 𝑞 (𝑠, 𝑎) because
    the total value of a state 𝑣 (𝑠) (value function) is not sufficient in the absence
    of an environment model to allow the policy to determine, given a state, which
    action is performed the best. In this case, however, the values are estimated
    step by step by following the Bellman equation with the update parameter 𝑣 (𝑠),
    while considering the state-action pair in place of a state.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在SARSA中，必须估算动作价值函数 𝑞 (𝑠, 𝑎)，因为在没有环境模型的情况下，状态 𝑣 (𝑠)（价值函数）的总值不足以让策略根据给定的状态判断执行哪个动作是最好的。然而，在这种情况下，值是通过遵循贝尔曼方程，并考虑状态-动作对代替状态，逐步估算的。
- en: Due to its on-policy nature, SARSA estimates the action-value function based
    on the behavior of the π policy, and at the same time, modifies the greedy behavior
    of the policy with respect to the updated estimates from the action-value function.
    The convergence of SARSA, and more generally of all TD methods, depends on the
    nature of policies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其在策略中的特性，SARSA根据π策略的行为估算动作价值函数，同时根据从动作价值函数中更新的估算值，修改策略的贪婪行为。SARSA的收敛性，和所有TD方法一样，依赖于策略的性质。
- en: 'The following code block shows the pseudo-code for the SARSA algorithm:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了SARSA算法的伪代码：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `update` rule of the action-value function uses all five elements (`s[t]`,
    `a[t]`, `r[t + 1]`, `s[t + 1]`, and `a[t + 1]`)  and for this reason, it is called
    **State-Action-Reward-State-Action** (**SARSA**).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 动作价值函数的`update`规则使用所有五个元素（`s[t]`，`a[t]`，`r[t + 1]`，`s[t + 1]`，以及 `a[t + 1]`），因此被称为**状态-动作-奖励-状态-动作**
    (**SARSA**)。
- en: Q-learning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is one of the most used reinforcement learning algorithms. This is
    due to its ability to compare the expected utility of the available actions without
    requiring an environment model. Thanks to this technique, it is possible to find
    an optimal action for every given state in a finished MDP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning是最常用的强化学习算法之一。其原因在于它能够比较可用动作的期望效用，而不需要环境模型。得益于这项技术，可以在完成的MDP中为每个给定的状态找到最优动作。
- en: A general solution to the reinforcement learning problem is to estimate an evaluation
    function during the learning process. This function must be able to evaluate the
    convenience or otherwise of a particular policy through the sum of the rewards.
    In fact, Q-learning tries to maximize the value of the Q function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in the state *s*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题的一个通用解决方案是在学习过程中估算评估函数。这个函数必须能够通过奖励的总和评估特定策略的便利性或其他方面。事实上，Q-learning试图最大化Q函数（动作价值函数）的值，Q函数表示我们在状态
    *s* 下执行动作 *a* 时的最大折扣未来奖励。
- en: 'Q-learning, like SARSA, estimates the function value 𝑞 (𝑠, 𝑎) incrementally,
    updating the value of the state-action pair at each step of the environment, following
    the logic of updating the general formula for estimating the values for the TD
    methods. Q-learning, unlike SARSA, has off-policy characteristics. That is, while
    the policy is improved according to the values estimated by 𝑞 (𝑠, 𝑎), the value
    function updates the estimates following a strictly greedy secondary policy: given
    a state, the chosen action is always the one that maximizes the value *max*𝑞 (𝑠,
    𝑎). However, the π policy has an important role in estimating values because the
    state-action pairs to be visited and updated are determined through it.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning和SARSA一样，逐步估算函数值 𝑞 (𝑠, 𝑎)，在环境的每个步骤中更新状态-动作对的值，遵循更新TD方法估算值的通用公式逻辑。与SARSA不同，Q-learning具有离策略特性。也就是说，虽然策略是根据
    𝑞 (𝑠, 𝑎) 估算的值进行改进的，但价值函数更新估算值时遵循严格的贪婪次级策略：给定一个状态，选择的动作总是那个能够最大化值 *max*𝑞 (𝑠, 𝑎)
    的动作。然而，π策略在估算值方面起着重要作用，因为要访问和更新的状态-动作对是通过它来决定的。
- en: 'The following code block shows pseudo-code for the Q-learning algorithm:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了Q-learning算法的伪代码：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Q-learning uses a table to store each state-action couple. At each step, the
    agent observes the current state of the environment and using the π policy selects
    and executes the action. By executing the action, the agent obtains the reward,
    𝑅[𝑡+1], and the new state, 𝑆[𝑡+1]. At this point, the agent can calculate 𝑄 (s[𝑡],
    a[𝑡]), updating the estimate.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning使用一个表格来存储每个状态-动作对。在每个步骤中，智能体观察当前环境的状态，并使用π策略选择并执行动作。通过执行该动作，智能体获得奖励
    𝑅[𝑡+1]，以及新的状态 𝑆[𝑡+1]。此时，智能体可以计算 𝑄 (s[𝑡], a[𝑡])，并更新估算值。
- en: In the following section, the basis of graph theory will be given and how this
    technology can be addressed in R.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，将给出图论的基础，并说明如何在R中处理这项技术。
- en: Introducing graph theory and implementing it in R
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入图论并在R中实现
- en: 'Graphs are data structures that are widely used in optimization problems. A
    graph is represented by a vertex and an edge structure. The vertices can be events
    from which different alternatives (the edges) depart. Typically, graphs are used
    to represent a network unambiguously: vertices represent individual calculators,
    road intersections, or bus stops, and edges are electrical connections or roads.
    Edges can connect vertices in any way possible.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图是广泛应用于优化问题的数据结构。图由顶点和边的结构表示。顶点可以是从中出发的不同选择（即边）。通常，图用于清晰地表示网络：顶点代表独立的计算机、路口或公交车站，边则是电气连接或道路。边可以以任何可能的方式连接顶点。
- en: Graph theory is a branch of mathematics that allows you to describe sets of
    objects together with their relationships; it was invented in 1700 by Leonhard
    Euler.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图论是数学的一个分支，它允许你描述对象集合及其关系；由莱昂哈德·欧拉在1700年发明。
- en: A graph is indicated in a compact way with *G = (V, E)*, where *V* indicates
    the set of vertices and *E* the set of edges that constitute it. The number of
    vertices is *|V|* and the number of edges is *|E|*. The number of vertices of
    the graph, or of a subpart of it, is obviously the fundamental quantity to define
    its dimensions; the number and distribution of edges describe their connectivity.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图通常用*G = (V, E)*的紧凑形式表示，其中*V*表示顶点集合，*E*表示构成图的边集合。顶点的数量是*|V|*，边的数量是*|E|*。图的顶点数，或其子部分的顶点数，显然是定义其维度的基本量；边的数量和分布描述了它们的连接性。
- en: 'There are different types of edges: we are talking about undirected edges for
    which the edges do not have a direction in comparison with those directed. A directed
    edge is called an arc and the relative graph is called a **digraph**. For example,
    undirected edges are used to represent computer networks with synchronous links
    for data transmission (as shown in the following diagram), directed graphs can
    represent road networks, allowing the representation of double-senses and unique
    senses.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同类型的边：我们讨论的是无向边，其边没有方向，而与有向边相比。有向边称为弧，相关的图称为**有向图**。例如，无向边用于表示具有同步链路的数据传输计算机网络（如下图所示），而有向图则可以表示道路网络，允许表示双向和单向道路。
- en: 'The following diagram represents a simple graph:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下图表示一个简单的图：
- en: '![](img/9f6d2eaf-d7ed-4e54-8342-da449764a46b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f6d2eaf-d7ed-4e54-8342-da449764a46b.png)'
- en: We say the graph is connected if we can reach all of the other vertices of the
    graph from any given vertex. Weighted graphs are graphs if a weight is associated
    with each edge, which is normally defined by a weight function (*w*). The weight
    can be seen as a cost or the distance between the two knots that the bow unites.
    The cost can be dependent on the flow that crosses the edge through a law. In
    this sense, the function w can be linear or not and depends on the flow that crosses
    the edge (non-congested networks) or also on the flow of nearby edges (congested
    networks).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够从任何给定的顶点到达图中的所有其他顶点，我们就说这个图是连通的。如果每条边都关联一个权重，并且通常由权重函数(*w*)定义，则图是加权图。权重可以视为两个节点之间的成本或距离。成本可能取决于流量通过边的规律。在这个意义上，权重函数*w*可以是线性的，也可以不是，并且取决于通过边的流量（非拥塞网络）或周围边的流量（拥塞网络）。
- en: 'A vertex is characterized by its degree, which is equal to the number of edges
    that end on the vertex itself. Depending on the degree, the vertices are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 顶点的特征是其度数，度数等于以该顶点为终点的边的数量。根据度数，顶点如下所示：
- en: A vertex of order 0 is called an isolated vertex.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 度数为0的顶点称为孤立顶点。
- en: A vertex of order 1 is called a leaf vertex.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 度数为1的顶点称为叶子顶点。
- en: 'The following diagram shows a graph with vertices labeled by degree:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个按度数标记的图：
- en: '![](img/62f921fa-7336-4572-b0dd-c3c5dc96e5f9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62f921fa-7336-4572-b0dd-c3c5dc96e5f9.png)'
- en: In a directed graph, we can distinguish the outdegree (number of outgoing edges)
    from the indegree (number of incoming edges). Based on this assumption, a vertex
    with an indegree of zero is called a source vertex and a vertex with an outdegree
    of zero is called a sink vertex.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在有向图中，我们可以区分出度（即出发边的数量）和入度（即进入边的数量）。基于这一假设，入度为零的顶点称为源顶点，出度为零的顶点称为汇顶点。
- en: 'Finally, a simplicial vertex is one whose neighbors form a clique: every two
    neighbors are adjacent. A universal vertex is a vertex that is adjacent to every
    other vertex in the graph.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，简约顶点是其邻居形成团体的顶点：每两个邻居都是相邻的。通用顶点是与图中所有其他顶点相邻的顶点。
- en: 'To represent a graph, different approaches are available, such as the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表示图的方法有多种，例如以下几种：
- en: Graphic representation (as shown in the previous diagram)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形表示（如前图所示）
- en: Adjacency matrix
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: List of vertices *V* and of arcs *E*
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶点 *V* 和弧 *E* 的列表
- en: The first way to represent a graph was clearly introduced through a practical
    example (see the previous diagram). In the graphical representation, circles are
    used to represent the vertices and lines to indicate the connections between two
    vertices if they are connected. If this connection has a direction, then it is
    indicated by adding an arrow. In the following section, we will analyze the other
    two ways of representing a graph.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表示图的第一种方法通过实际示例进行了清晰的介绍（见前面的图示）。在图形表示中，圆圈表示顶点，线条表示两个顶点之间的连接，如果它们相连。若该连接具有方向性，则通过添加箭头来表示。在接下来的部分，我们将分析表示图的其他两种方法。
- en: Adjacency matrix
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: So far, we have represented a graph through vertices and edges. When the number
    of vertices is small, this way of representing a graph is the best one because
    it allows us to analyze its structure intuitively. When the number of vertices
    becomes large, the graphic representation becomes confusing. In this case, it
    is better to represent the graph through the adjacency matrix. By adjacency matrix
    or connection matrix, we mean a data structure that's commonly used in graph representation.
    It is widely used in the drafting of algorithms that operate on graphs and in
    their computer representation. If it is a sparse matrix, the use of the adjacency
    list is preferable to the matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过顶点和边来表示图。当顶点数量较少时，这种表示方法是最好的，因为它使我们能够直观地分析图的结构。当顶点数量变大时，图形表示变得混乱。在这种情况下，通过邻接矩阵表示图会更好。邻接矩阵或连接矩阵是图表示中常用的数据结构，广泛应用于图操作算法的设计以及图的计算机表示中。如果它是稀疏矩阵，使用邻接表优于使用矩阵。
- en: 'Given any graph, its adjacency matrix is ​​made up of a square binary matrix
    that has the names of the vertices of the graph as rows and columns. In the place
    (*i, j*) of the matrix, there is a 1 if and only if an edge that goes from the
    vertex *i* to the vertex *j* exists in the graph; otherwise, there is a 0\. In
    the case of the representation of undirected graphs, the matrix is symmetric with
    respect to the main diagonal. For example, check out the graph represented in
    the following diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任何图，其邻接矩阵由一个方形二进制矩阵组成，矩阵的行和列是图中顶点的名称。在矩阵的 (*i, j*) 位置上，如果图中存在一条从顶点 *i* 到顶点
    *j* 的边，则该位置为1；否则为0。在无向图的表示中，矩阵相对于主对角线是对称的。例如，查看以下图示所表示的图：
- en: '![](img/3d3da994-fab2-4b91-bb9e-a5e2d0417f04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d3da994-fab2-4b91-bb9e-a5e2d0417f04.png)'
- en: 'The preceding graph can be represented through the following adjacency matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图可以通过以下邻接矩阵来表示：
- en: '![](img/dcd3ca6f-e442-4d4e-bebb-d73792d20ad0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcd3ca6f-e442-4d4e-bebb-d73792d20ad0.png)'
- en: As anticipated, the matrix is symmetric with respect to the main diagonal being
    undirected. If instead of the 1 in the matrix, there are numbers; these are to
    be interpreted as the weight attributed to each connection (edge). Here, the matrix
    is called Markov's matrix, as it is applicable to a Markov process. For example,
    if the set of vertices of the graph represents a series of points on a map, the
    weight of the edges can be interpreted as the distance of the points that they
    connect.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期所示，矩阵相对于主对角线是对称的，表示图是无向的。如果矩阵中不是1而是其他数字，那么这些数字表示分配给每个连接（边）的权重。在这种情况下，矩阵被称为马尔可夫矩阵，因为它适用于马尔可夫过程。例如，如果图的顶点集表示地图上的一系列点，那么边的权重可以解释为它们连接的点之间的距离。
- en: One of the fundamental characteristics of this matrix is that it obtains the
    number of paths from a node *i* to a node *j*, which must cross *n* vertices.
    To obtain all of this, it is sufficient to make the *n* power of the matrix and
    see the number that appears in place *i, j*. Another way of representing graphs
    is using adjacency lists. Let's see how.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的一个基本特点是，它可以计算从节点*i*到节点*j*的路径数，这些路径必须经过*n*个顶点。为了得到这些信息，只需将矩阵的*n*次方计算出来，并查看在*i,
    j*位置上的数字即可。另一种表示图的方式是使用邻接列表。我们来看一下。
- en: Adjacency list
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邻接列表
- en: '**Adjacency lists** are a mode of graph representation in memory. This is probably
    the simplest representation to implement, although, in general, it is not the
    most efficient in terms of the space that''s occupied.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**邻接列表**是图在内存中的一种表示方式。这可能是最简单的实现方式，尽管通常来说，它在占用空间方面不是最有效的。'
- en: Let's analyze a simple graph; next to each vertex is its list of adjacencies.
    The idea of representation is simply that every vertex *Vi* is associated with
    a list containing all of the vertices *Vj* so that there is the edge from *Vi*
    to *Vj*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一个简单的图；每个顶点旁边列出的是其相邻顶点的列表。表示方法的基本思想是，每个顶点*Vi*都与一个包含所有与其相连的顶点*Vj*的列表相关联，即存在一条从*Vi*到*Vj*的边。
- en: Assuming that you memorize all of the pairs of the type (*Vi, L*), where *L*
    is the adjacency list of the *Vi* vertex, we obtain a unique description of the
    graph. Alternatively, if you decide to sort adjacency lists, you do not need to
    explicitly store the vertexes as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你记住了所有类型为(*Vi, L*)的顶点对，其中*L*是顶点*Vi*的邻接列表，那么我们就能得到图的唯一描述。或者，如果你决定对邻接列表进行排序，那么就不需要显式地存储顶点了。
- en: 'Let''s take an example—we will use the same graph adopted in the previous section,
    which is represented in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子——我们将使用上一节中采用的相同图形，图示如下：
- en: '![](img/30f10bb6-661c-4f8c-ab4a-031e07f376c2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30f10bb6-661c-4f8c-ab4a-031e07f376c2.png)'
- en: 'From this, we will build the list of adjacencies according to what has been
    said so far. The graph in the previous diagram can be represented as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面提到的内容，我们将构建邻接列表。上图中的图可以表示如下：
- en: '| 1 | adjacent to | 2,3 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 相邻于 | 2,3 |'
- en: '| 2 | adjacent to | 1,3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 相邻于 | 1,3 |'
- en: '| 3 | adjacent to | 1,2,4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 相邻于 | 1,2,4 |'
- en: '| 4 | adjacent to | 3 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 相邻于 | 3 |'
- en: An adjacency list is made up of pairs. There is a pair for each vertex in the
    graph. The first element of the pair is the vertex that is being analyzed, and
    the second is the set formed by all of the vertices adjacent to it, which is connected
    to it by one side.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接列表由对组成。每个图中的顶点都有一对。对的第一个元素是正在分析的顶点，第二个元素是由所有与它相邻的顶点组成的集合，这些顶点通过一条边与之相连。
- en: Assuming that we have a graph with *n* vertices and *m* edges (directed) that
    unite them, and supposing the adjacency lists are memorized in the order (so as
    not to explicitly memorize the indices), we will have each edge appear in one
    and one list of adjacencies, and it appears as the number of the vertex to which
    it points. Due to this, it's necessary to memorize a total of *m* numbers less
    than or equal to *n*, for a total cost of *mlog2n*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含*n*个顶点和*m*条边（有向）的图，且假设邻接列表已按顺序记忆（为了避免显式记忆索引），那么每条边会出现在一个且仅一个邻接列表中，并且它会以指向的顶点编号的形式出现。因此，需要记住总共*m*个小于等于*n*的数字，总的成本为*mlog2n*。
- en: 'There is no obvious way to optimize this representation for non-oriented graphs;
    each arc must be memorized in the adjacency lists of both vertices that it connects,
    hence halving the efficiency. The same argument holds if the graph is oriented,
    but we need an efficient method to know the arcs entering a certain vertex. In
    this case, it is convenient to associate two lists to each vertex: that of the
    incoming arcs and that of the outgoing arcs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无向图来说，没有明显的方法来优化这种表示法；每条弧必须在连接的两个顶点的邻接列表中都进行记忆，从而降低了效率。如果图是有向图，我们仍然需要一种有效的方法来知道指向某个顶点的弧。在这种情况下，将每个顶点关联两个列表是比较方便的：一个是进入弧的列表，另一个是出去弧的列表。
- en: As far as time efficiency is concerned, representation by adjacency lists behaves
    quite well both in access and in insertion, carrying out the main operations in
    time *O(n)*. So far, we have analyzed the graphic representation techniques. Now,
    let's learn how to use them in the R environment.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间效率方面，邻接列表的表示方式在访问和插入操作中表现得相当不错，主要操作在*O(n)*时间内完成。到目前为止，我们已经分析了图形表示的技术。接下来，让我们学习如何在R环境中使用这些技术。
- en: Handling graphs in R
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在R中处理图形
- en: In R, the set of nodes (*V*) and the set of arcs (*E*) are data structures of
    different types. For *V*, once we assign a unique identifier to each node, then
    we can access every node without ambiguity. Hence, it is like saying that the
    data structure hosting the properties of the nodes is one-dimensional and, therefore,
    is a vector.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，节点集（*V*）和弧集（*E*）是不同类型的数据结构。对于*V*，一旦我们为每个节点分配唯一标识符，就可以无歧义地访问每个节点。因此，它就像是在说，托管节点属性的数据结构是一维的，因此是一个向量。
- en: On the contrary, the data structure for the set of arcs (links between nodes)
    *E* cannot be a vector and it does not express the characteristics of single objects
    but expresses relations between pairs of objects (pairs of nodes in this case).
    So if, for example, in *V* (the set of nodes) there are 10 nodes, then the dimensions
    of *E* will be 10 × 10 or all of the relationships between all of the possible
    pairs of nodes. Ultimately, *E* has not one but two dimensions and therefore it
    is not a vector but a matrix.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，弧集（节点之间的链接）*E*的数据结构不能是向量，它不表示单一对象的特征，而是表示对象对之间的关系（在这种情况下是节点对之间的关系）。因此，如果例如在*V*（节点集）中有10个节点，那么*E*的维度将是10
    × 10，即所有可能节点对之间的关系。最终，*E*有两个维度，因此它不是向量，而是矩阵。
- en: In the matrix *E*, we have several rows equal to the number of nodes present
    in *V* and several columns equal to the number of nodes present in *V*. This represents
    the adjacency matrix analyzed in detail in the *Adjacency matrix* section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵*E*中，我们有多行等于*V*中节点的数量，多列等于*V*中节点的数量。这表示在*邻接矩阵*部分中详细分析的邻接矩阵。
- en: To address the graph in R, we can use the `igraph` package—this package contains
    functions for simple graphs and network analysis. It can handle large graphs very
    well and provides functions for generating random and regular graphs, graph visualization,
    centrality methods, and much more.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要在R中处理图形，我们可以使用`igraph`包——该包包含用于简单图形和网络分析的函数。它能够很好地处理大型图形，并提供生成随机图和规则图、图形可视化、中心性方法等功能。
- en: 'The following table gives some information about this package:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了有关该包的一些信息：
- en: '| Package | `igraph` |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 包 | `igraph` |'
- en: '| Date | 2019-22-04 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2019-22-04 |'
- en: '| Version | 1.2.4.1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 1.2.4.1 |'
- en: '| Title | Network Analysis and Visualization |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 网络分析与可视化 |'
- en: '| Maintainer | Gábor Csárdi |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 维护者 | Gábor Csárdi |'
- en: 'To start using the available tools, we will analyze a simple example. Suppose
    we have a graph consisting of four nodes and four edges. The first thing to do
    is to define the links between the four nodes; to do this, we will use the `graph`
    function (remember to load the `igraph` library after installing it):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始使用可用工具，我们将分析一个简单的示例。假设我们有一个由四个节点和四条边组成的图。首先要做的是定义这四个节点之间的链接；为此，我们将使用`graph`函数（记得在安装后加载`igraph`库）：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `graph` function is part of the `graph.constructors` methods that offer
    various methods for creating graphs: empty graphs, graphs with the given edges,
    graphs from adjacency matrices, star graphs, lattices, rings, and trees. The method
    we used defines the graph by indicating edges using a numeric vector defining
    the edges as follows: the first edge from the first element to the second, the
    second edge from the third to the fourth, and so on.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph`函数是`graph.constructors`方法的一部分，提供了创建图形的各种方法：空图、有给定边的图、从邻接矩阵构建的图、星形图、格状图、环形图和树形图。我们使用的方法是通过使用数值向量来定义边来定义图形，向量中的第一个元素到第二个元素为第一条边，第三个元素到第四个元素为第二条边，以此类推。'
- en: 'In fact, we can see that four pairs of values have been passed: the first defines
    the connection between nodes 1 and 2, the second between nodes 2 and 3, the third
    between nodes 3 and 1, and finally, the fourth between nodes 4 and 2\. To better
    understand the connections between the nodes of the graph, we will draw it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以看到传入了四对值：第一对定义了节点1和节点2之间的连接，第二对定义了节点2和节点3之间的连接，第三对定义了节点3和节点1之间的连接，最后，第四对定义了节点4和节点2之间的连接。为了更好地理解图中节点之间的连接，我们将绘制该图：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following graph was plotted:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制了以下图形：
- en: '![](img/529c85a6-f9a7-4819-a753-17a7dab6a4b4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/529c85a6-f9a7-4819-a753-17a7dab6a4b4.png)'
- en: 'The graph we created is an object with features that we can analyze as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的图是一个具有特征的对象，可以按如下方式进行分析：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following results are returned:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果如下：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The edges between the nodes are indicated. We then calculate the shortest path
    between node 1 and node 4:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间的边已指示。然后，我们计算节点1和节点4之间的最短路径：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`get.shortest.paths() ` calculates a single shortest path between the source
    vertex to the target vertices. This function uses a breadth-first search for unweighted
    graphs and Dijkstra''s algorithm for weighted graphs. In our case, having added
    the weight attribute, the Dijkstra algorithm was used.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`get.shortest.paths()` 计算从源顶点到目标顶点的单一最短路径。该函数对于无权图使用广度优先搜索，对于有权图使用 Dijkstra
    算法。在我们的例子中，由于添加了权重属性，因此使用了 Dijkstra 算法。'
- en: 'The following result is returned:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, calculate the distance between the two points for this path:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，计算此路径上两点之间的距离：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following result is returned:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The graph we have represented so far has limited usefulness to identify the
    shortest path between two locations, which represents our goal. To calculate the
    best route, it is necessary to introduce the concept of edge weight. In our case,
    we can see this attribute as a measure of the length of the path between two nodes;
    in this way, we can evaluate the distance between two nodes through a path. To
    do this, we''ll use the attribute weights as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们所表示的图在识别两地之间最短路径方面的用途有限，这正是我们的目标。为了计算最佳路径，需要引入边权重的概念。在我们的例子中，我们可以将此属性视为两个节点之间路径长度的度量；通过这种方式，我们可以通过路径来评估两个节点之间的距离。为此，我们将使用以下方式来定义属性权重：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: First, we defined the weights with a vector, confirming the sequence of edges
    defined in the creation of the graph. So, we added the weight attribute to the
    previously created graph. Now, each edge has its own length. What happens if weights
    are not defined? Simply, they are all set equal to 1; in this case, the shortest
    path would be the one with the least number of nodes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过一个向量定义了权重，确认了在图创建时定义的边的顺序。然后，我们将权重属性添加到先前创建的图中。现在，每条边都有了自己的长度。如果没有定义权重会怎样呢？简单地说，它们都会被设为
    1；在这种情况下，最短路径将是节点数最少的路径。
- en: 'Now, let''s calculate again the shortest path between node 1 and node 4:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新计算节点 1 和节点 4 之间的最短路径：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following result is returned:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can see that the path now involves multiple nodes. We will verify the distance
    between the two nodes:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在的路径涉及多个节点。我们将验证这两个节点之间的距离：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following result is returned:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this way, we have verified that the indicated path is the shortest one since
    the longest connection has been avoided. In the next section, we see how it is
    possible to find the best route using Dijkstra's algorithm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们验证了所指示的路径是最短路径，因为最长的连接已被避免。在下一节中，我们将看到如何使用 Dijkstra 算法找到最佳路径。
- en: Dijkstra's algorithm
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dijkstra 算法
- en: Dijkstra's algorithm is used to solve the problem of finding the shortest path
    from the source *s* to all of the nodes. The algorithm maintains a label *d(i)*
    to the nodes representing an upper bound on the length of the shortest path of
    the node *i*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Dijkstra 算法用于解决从源节点 *s* 到所有节点的最短路径问题。该算法为节点维护一个标签 *d(i)*，表示节点 *i* 最短路径长度的上限。
- en: 'At each step, the algorithm partitions the nodes in *V* into two sets: the
    set of permanently labeled nodes and the set of nodes that are still temporarily
    labeled. The distance of permanently labeled nodes represents the shortest path
    distance from the source to these nodes, whereas the temporary labels contain
    a value that can be greater than or equal to the shortest path length.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步中，算法将 *V* 中的节点分成两组：一组是永久标记的节点，另一组是仍然是临时标记的节点。永久标记节点的距离表示从源节点到这些节点的最短路径距离，而临时标记的节点则包含一个值，该值可以大于或等于最短路径长度。
- en: The basic idea of the algorithm is to start from the source and try to permanently
    label the successor nodes. In the beginning, the algorithm places the value of
    the source distance to zero and initializes the other distances to an arbitrarily
    high value (by convention, we will set the initial value of the distances *d[i]
    = + ∞, ∀i ∈ V*). At each iteration, the node label *i* is the value of the minimum
    distance along a path from the source that contains, apart from *i*, only permanently
    labeled nodes. The algorithm selects the node whose label has the lowest value
    among those labeled temporarily, labels it permanently, and updates all of the
    labels of the nodes adjacent to it. The algorithm terminates when all of the nodes
    have been permanently labeled.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的基本思想是从源节点开始，尝试永久标记后继节点。开始时，算法将源节点的距离值设为零，并将其他节点的距离初始化为一个任意高的值（按照惯例，我们将距离的初始值设为
    *d[i] = + ∞, ∀i ∈ V*）。在每次迭代中，节点标签 *i* 是从源节点出发的路径中最小距离的值，该路径除了 *i* 之外只有永久标记的节点。算法选择那些临时标记的节点中标签值最低的节点，将其永久标记，并更新所有与之相邻节点的标签。当所有节点都被永久标记时，算法终止。
- en: 'From the execution of this algorithm for each destination node *v* of *V*,
    we obtain a shortest path *p* (from *s* to *v*) and we calculate the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行该算法，针对每个目标节点 *v*（属于 *V*），我们可以获得一个最短路径 *p*（从 *s* 到 *v*），并计算以下内容：
- en: '*d [v]*: Distance of node *v* from source node *s* long *p*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d [v]*：节点 *v* 到源节点 *s* 的距离 *p*'
- en: '*π [v]*: Predecessor of node *v* long *p*'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*π [v]*: 节点 *v* 的前驱节点为 *p*'
- en: 'For the initialization of each node v of V, we will use the following procedure:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个节点 *v*（属于 *V*）的初始化，我们将使用以下过程：
- en: '*d [v] = ∞ if v ≠ s*, otherwise *d [s] = 0*'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d [v] = ∞ 如果 v ≠ s*，否则 *d [s] = 0*'
- en: '*π [v] = Ø*'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*π [v] = Ø*'
- en: During the execution, we use the relaxation technique of a generic edge *(u,
    v)* of *E*, which serves to improve the estimation of *d*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行过程中，我们使用泛化边 *(u, v)*（属于 *E*）的松弛技术来改善 *d* 的估算值。
- en: 'The relaxation of an edge *(u, v)* of *E*, consists in evaluating whether,
    using *u* as a predecessor of *v*, the current value of distance *d [v]* can be
    improved and, in this case, they update *d [v]* and *π [v]*. The procedure is
    as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 边 *(u, v)* 的松弛操作，旨在评估是否可以通过将 *u* 作为 *v* 的前驱节点来改善当前的距离值 *d [v]*，如果可以改善，则更新 *d
    [v]* 和 *π [v]*。该过程如下：
- en: If *d[v]> d[u] + w (u, v)* then
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *d[v]> d[u] + w (u, v)* 则
- en: '*d[v] = d[u] + w (u, v)*'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*d[v] = d[u] + w (u, v)*'
- en: '*π [v] = u*'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*π [v] = u*'
- en: 'The algorithm basically performs two operations: a node selection operation
    and an operation to update the distances. The first selects the node with the
    value of the lowest label at each step; the other verifies the condition *d[v]>
    d[u] + w(u, v)* and, if so, updates the value of the label placing *d[v] = d[u]
    + w (u, v)*.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法基本上执行两个操作：节点选择操作和更新距离的操作。第一个操作在每一步选择标签值最低的节点；另一个操作验证条件 *d[v]> d[u] + w(u,
    v)*，如果满足条件，则更新标签值，令 *d[v] = d[u] + w (u, v)*。
- en: In the following section, we will implement a TD method to address a real-life
    application.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将实现一种 TD 方法来解决一个实际应用问题。
- en: Implementing TD methods to the vehicle routing problem
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 TD 方法应用于车辆路径问题
- en: Given a weighted graph and a designated vertex *V*, it is often requested to
    find the path from a node to each of the other vertices in the graph. Identifying
    a path connecting two or more nodes of a graph is a problem that appears as a
    subproblem of many other problems of discrete optimization and has numerous applications
    in the real world.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个加权图和一个指定的顶点 *V*，通常需要找出从一个节点到图中每个其他顶点的路径。识别连接两个或多个节点的路径是许多离散优化问题的子问题，并且在现实世界中有广泛的应用。
- en: Consider, for example, the problem of identifying a route between two locations
    shown on a road map, where the vertices are the localities, while the edges are
    the roads that connect them. In this case, each cost is associated with the length
    in kilometers of the road or the average time needed to cover it. If instead of
    any path, we want to identify one of the minimum total cost, then the resulting
    problem is known as the problem of the shortest path in a graph. In other words,
    the shortest path between two vertices of a graph is that path that connects these
    vertices and minimizes the sum of the costs associated with crossing each edge.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个问题：在一张道路地图上标识两地之间的路线，其中顶点表示地点，边表示连接它们的道路。在这种情况下，每个代价都与道路的公里长度或覆盖该段道路的平均时间相关。如果我们想要识别的是最小总代价的路径，而非任意路径，那么所得到的问题被称为图中的最短路径问题。换句话说，图中两个顶点之间的最短路径是连接这两个顶点并最小化穿越每条边的代价之和的路径。
- en: So, let's take a practical example—consider a tourist visiting Italy by car
    who wants to reach Venice from Rome. Having a map of Italy available in which,
    for each direct link between the cities, its length is marked, how can the tourist
    find the shortest path?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们通过一个实际的例子来考虑——假设一位游客开车从罗马前往威尼斯。假设他手中有一张意大利地图，上面标出了各个城市之间的直连路径及其长度，游客如何找到最短的路径？
- en: The system can be schematized with a graph in which each city corresponds to
    a vertex, and the roads correspond to the connecting arcs between the vertices.
    You need to determine the shortest path between the source vertex and the target
    vertex of the graph.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统可以通过一个图来示意，其中每个城市对应一个顶点，道路对应顶点之间的连接弧。你需要确定图中源顶点和目标顶点之间的最短路径。
- en: A solution to the problem is to number all possible routes from Rome to Venice.
    For each route, calculate the total length and then select the shortest. This
    solution is not the most efficient because there are millions of paths to analyze.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题的解决方案是为从罗马到威尼斯的所有可能路线编号。对于每条路线，计算总长度，然后选择最短的一条。这个解决方案不是最有效的，因为需要分析的路径有数百万条。
- en: In practice, we will model the map of Italy as a weighted oriented graph *G
    = (V, E)*, where each vertex represents a city, each edge *(u, v)* represents
    a direct path from *u* to *v* and each weight *w (u, v)* corresponding to an edge
    *(u, v)* represents the distance between *u* and *v*. So, the problem to be solved
    is that of finding the shortest path that connects the vertex corresponding to
    Rome with that corresponding to Venice.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们将意大利地图建模为一个加权有向图 *G = (V, E)*，其中每个顶点表示一个城市，每条边 *(u, v)* 表示从 *u* 到 *v*
    的直接路径，而每个权重 *w(u, v)* 对应于边 *(u, v)*，表示 *u* 和 *v* 之间的距离。因此，要解决的问题是找到从表示罗马的顶点到表示威尼斯的顶点的最短路径。
- en: 'Given a weighted directed graph *G = (V, E)*, the weight of a path *p = (v0,
    v1, ..., vk)* is given by the sum of the weights of the edges that constitute
    it, as shown in the following formula:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个加权有向图 *G = (V, E)*，路径 *p = (v0, v1, ..., vk)* 的权重由其组成的边的权重之和给出，如下公式所示：
- en: '![](img/9e8dcd20-9e19-4a85-a2e6-2af524fe1dd3.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e8dcd20-9e19-4a85-a2e6-2af524fe1dd3.png)'
- en: 'The shortest path from node *u* to node *v* of *V* is a path *p = (u, v1, v2,
    ..., v)* so that *w(p)* is minimal, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从节点 *u* 到节点 *v* 的最短路径是一个路径 *p = (u, v1, v2, ..., v)*，使得 *w(p)* 最小，如下所示：
- en: '![](img/4cc61d83-cabb-4482-b856-f6fff9032a19.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc61d83-cabb-4482-b856-f6fff9032a19.png)'
- en: The cost of the minimum path from *u* to *v* is denoted by *δ(u, v)*. If there
    is no path from *u* to *v* then *δ (u, v) = ∞*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *u* 到 *v* 的最短路径的代价用 *δ(u, v)* 表示。如果从 *u* 到 *v* 没有路径，则 *δ(u, v) = ∞*。
- en: Given a connected weighted graph *G = (V, E)* and a source node *s* of *V*,
    there are several algorithms to find a shortest path from *s* toward each other
    node of *V*. In the previous section, we analyzed the Dijkstra algorithm, now
    the time has come to tackle the problem using algorithms based on reinforcement
    learning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个连通的加权图 *G = (V, E)* 和一个源节点 *s*，有多种算法可以找到从 *s* 到 *V* 中其他节点的最短路径。在上一节中，我们分析了
    Dijkstra 算法，现在是时候使用基于强化学习的算法来解决这个问题了。
- en: As anticipated at the beginning of this chapter, the Vehicle Routing Problem
    (VRP) is a typical distribution and transport problem, which consists of optimizing
    the use of a set of vehicles with limited capacity to pick up and deliver goods
    or people to geographically distributed stations. Managing these operations in
    the best possible way can significantly reduce costs. Before tackling the problem
    with Python code, let's analyze the basic characteristics of the topic to understand
    possible solutions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开始时预期的那样，车辆路径问题（VRP）是一个典型的配送和运输问题，旨在优化使用一组有限容量的车辆来接送货物或人员，并将其运送到地理上分布的站点。以最佳方式管理这些操作可以显著降低成本。在用Python代码解决问题之前，让我们分析一下这一主题的基本特征，以便理解可能的解决方案。
- en: Based on what has been said so far, it is clear that a problem of this type
    is configured as a path optimization procedure that can be conveniently dealt
    with using graph theory.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于迄今为止所述，很明显，这类问题可以被视为路径优化过程，可以通过图论有效地解决。
- en: 'Suppose we have the following graph with the distances between vertices indicated
    on the edges:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下图，其中边上的数字表示顶点之间的距离：
- en: '![](img/cec69904-6c1f-479c-8e23-f1f49bcb12c0.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cec69904-6c1f-479c-8e23-f1f49bcb12c0.png)'
- en: It is easy to see that the shortest path from 1 to 6 is 1 – 2 – 5 – 4 - 6.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，从1到6的最短路径是1 – 2 – 5 – 4 - 6。
- en: In the *Understanding TD methods* section, we have seen that the method of choosing
    an action diversifies the types of algorithms based on the TD. In on-policy based
    methods (SARSA), the update is carried out based on the results of the actions
    determined by the selected policy, while in the off-policy methods (Q-learning),
    the policies are evaluated through hypothetical actions, not actually undertaken.
    We will address the problem just introduced through both approaches, highlighting
    the merits and defects of the solutions obtained. So, let's see how to deal with
    the problem of vehicle routing using Q-learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在*理解TD方法*部分中，我们已经看到，选择动作的方法根据TD的不同，算法的类型也会有所不同。在基于策略的方法（SARSA）中，更新是根据由选定策略决定的动作结果进行的，而在离策略方法（Q-learning）中，策略是通过假设的动作来评估的，而这些动作并未真正执行。我们将通过这两种方法来解决刚才提到的问题，突显解决方案的优点和缺点。那么，让我们看看如何使用Q-learning来处理车辆路径问题。
- en: The Q-learning approach
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning方法
- en: As we said in the *Q-learning* section, Q-learning tries to maximize the value
    of the Q function (action-value function), which represents the maximum discounted
    future reward when we perform actions *a* in the state *s*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*Q-learning*部分所说，Q-learning试图最大化Q函数（动作-价值函数）的值，该函数表示当我们在状态*s*中执行动作*a*时，能够获得的最大折扣未来奖励。
- en: 'The following block is an implementation of R code that allows us to research
    this path through the technique of Q-learning:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块是一个R代码的实现，通过Q学习技术让我们研究这一路径：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will analyze the code line by line, starting from the setting of the following
    parameters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐行分析代码，从以下参数的设置开始：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we have the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '`N`: the number of episodes to iterate'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N`: 迭代的回合数'
- en: '`gamma`: the discount factor'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`: 折扣因子'
- en: '`alpha`: the learning rate'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: 学习率'
- en: '`FinalState`: the target node'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FinalState`: 目标节点'
- en: 'Let''s move on the reward matrix setting:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续设置奖励矩阵：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We see the matrix as it appears:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到矩阵的呈现方式如下：
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following matrix is printed:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵被打印出来：
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s try to understand how we set this matrix. It''s all very simple: we
    have associated a high reward at the most convenient edges, those with a lower
    weight (which means shorter). We then associated the highest reward (100) to the
    edge that leads us to the goal. Finally, we associated a negative reward to non-existent
    links. The following chart shows how we set the rewards:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解如何设置这个矩阵。一切都非常简单：我们在最方便的边上（那些权重较低的边，即较短的路径）关联了高奖励。然后，我们将最高的奖励（100）赋给了通向目标的边。最后，我们将负奖励分配给不存在的连接。下图显示了我们如何设置奖励：
- en: '![](img/637421dd-564f-49df-8343-4d268a577cb8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/637421dd-564f-49df-8343-4d268a577cb8.png)'
- en: We have simply replaced the weight of the edges with the rewards set to the
    value of the lengths. Edges with longer lengths return a low reward, edges with
    smaller lengths return a high reward. Finally, the maximum reward is achieved
    when the target is reached.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是将边的权重替换成与长度值相对应的奖励。较长的边返回较低的奖励，而较短的边返回较高的奖励。最终，当目标到达时，将获得最大的奖励。
- en: As we said in the *Q-learning* section, our goal is to estimate an evaluation
    function that evaluates the convenience of a policy based on the sum of the rewards.
    The Q-learning algorithm tries to maximize the value of the Q function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in the state *s*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*Q-learning*部分所说，我们的目标是估计一个评估函数，该函数根据奖励的总和来评估策略的便利性。Q-learning算法试图最大化Q函数（行动值函数）的值，Q函数表示我们在状态*s*中执行动作*a*时的最大折现未来奖励。
- en: 'Let''s analyze again the procedure that we have to implement using R:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次分析我们需要使用R实现的程序：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At each step, the agent observes the current state of the environment and using
    the π policy selects and executes the action. By executing the action, the agent
    obtains the reward 𝑅𝑡 *+ 1* and the new state 𝑆𝑡 *+ 1*. At this point, the agent
    can calculate 𝑄 (*s*𝑡, *a*𝑡) updating the estimate.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，代理观察环境的当前状态，并使用π策略选择并执行动作。通过执行该动作，代理获得奖励𝑅𝑡 *+ 1*和新状态𝑆𝑡 *+ 1*。此时，代理可以通过更新估计来计算𝑄
    (*s*𝑡, *a*𝑡)。
- en: 'Therefore, the Q function represents the essential element of the procedure;
    it is a matrix of the same dimensions as the rewards matrix. First, let''s initialize
    it with all zeros:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Q函数代表了程序的核心元素；它是一个与奖励矩阵维度相同的矩阵。首先，我们将其初始化为全零矩阵：
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At this point, we have to set up a cycle that will repeat the operations for
    each episode:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们必须设置一个循环，对每个回合重复操作：
- en: '[PRE22]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The initial part of the cycle is used to set the initial state and the initial
    policy; in our case, we will choose an initial state randomly:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 循环的初始部分用于设置初始状态和初始策略；在我们的案例中，我们将随机选择一个初始状态：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After setting the initial state, we must insert a cycle that will be repeated
    until the final state, that is, our target, has been reached:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 设置初始状态后，我们必须插入一个循环，直到达到最终状态，即我们的目标：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we must choose the next state according to the possible actions available
    in the current state. To move to the next node, what actions can we take? If only
    one possible action is available, we will choose that one. Otherwise, we will
    choose one at random, only to then analyze the others:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须根据当前状态中可用的可能动作选择下一个状态。为了移动到下一个节点，我们可以采取哪些行动？如果只有一个可能的动作可用，我们将选择那个动作。否则，我们将随机选择一个动作，然后再分析其他动作：
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Based on the results obtained, we can update the action-value function (`QMatrix`):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 根据获得的结果，我们可以更新行动值函数（`QMatrix`）：
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The formula used for updating the Q function is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 用于更新Q函数的公式如下：
- en: '![](img/49844e1b-bcec-4e0b-8109-ca1c62ad23b7.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49844e1b-bcec-4e0b-8109-ca1c62ad23b7.png)'
- en: 'Now, we will check the status achieved: if we have reached our target, then
    we will exit the cycle with the break command; otherwise, we will set the next
    status as the current one (`NextState`):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将检查已达成的状态：如果我们已经达到了目标，则使用break命令退出循环；否则，我们将把下一个状态设为当前状态（`NextState`）：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once the procedure is finished, we will print the Q matrix:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦程序完成，我们将打印Q矩阵：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following result is returned:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's try to understand what this matrix tells us. To start, we can say that
    this matrix allows us to calculate the shortest path starting from any state,
    therefore, not necessarily from node 1\. In our case, we will start from node
    1 in order to confirm what was obtained visually. Recall that each row of the
    matrix represents a state and each value in the column tells us what the reward
    is in the transition to the state marked by the column index.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解这个矩阵告诉了我们什么。首先，我们可以说这个矩阵允许我们计算从任何状态开始的最短路径，因此，不一定是从节点1开始。在我们的案例中，我们将从节点1开始，以确认视觉上获得的结果。回想一下，矩阵的每一行代表一个状态，每一列中的值告诉我们转移到列索引标记的状态时的奖励。
- en: 'In the following flow path, we have the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的流程路径中，我们有如下内容：
- en: Starting from the first line, we see that the maximum value is in correspondence
    of the second column, so the best path takes us from state 1 to 2.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一行开始，我们看到最大值位于第二列，因此，最佳路径将我们从状态1带到状态2。
- en: We then pass to the state 2 identified by the second row; here, we see that
    the greatest value of reward is in correspondence of the fifth column, therefore,
    the best path takes us from state 2 to 5.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们进入由第二行标识的状态2；在这里，我们看到最大奖励值位于第五列，因此，最佳路径将我们从状态2带到状态5。
- en: Let's then move on to state 5 identified by the fifth line. Here, we see that
    the greatest value of reward is in correspondence with the fourth column, therefore,
    the best path takes us from state 5 to 4.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们继续讨论由第五行标识的状态5。在这里，我们看到奖励的最大值与第四列相对应，因此，最佳路径将我们从状态5带到状态4。
- en: Finally, we pass to the state 4 identified by the fourth line. Here, we see
    that the greatest value of reward is in correspondence with the sixth column,
    therefore, the best path leads us from state 4 to 6.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们转到由第四行标识的状态4。在这里，我们看到奖励的最大值与第六列相对应，因此，最佳路径将我们从状态4带到状态6。
- en: 'We have reached the target and by doing so we have traced the path shorter
    from node 1 to 6, which is the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达目标，并且通过这样做，我们从节点1到节点6绘制了更短的路径，路径如下：
- en: 1 – 2 - 5 – 4 – 6
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 1 – 2 - 5 – 4 – 6
- en: 'This path coincides with the one obtained visually at the beginning of the
    section. The procedure for extracting the shortest path of the `QMatrix` matrix
    can be easily automated as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这条路径与本节开始时视觉上得到的路径一致。提取`QMatrix`矩阵的最短路径的过程可以如下轻松地自动化：
- en: '[PRE30]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following results are returned:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE31]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we can see, the same result has been returned. Now, let's see what happens
    if we try to tackle the same problem but with a different approach.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，返回的结果是相同的。现在，让我们看看如果我们尝试用不同的方法解决同样的问题会发生什么。
- en: The SARSA approach
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA方法
- en: As we anticipated in SARSA, starting from the current state *St* an action *At*
    is taken and the agent gets a reward R. In this way, the agent is transferred
    to the next state *St + 1* and takes an action *At + 1* in *St + 1*. In fact,
    SARSA is the acronym of the tuple (*S, A*, *R*, *St + 1*, *At + 1*).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在SARSA中所预见的，从当前状态*St*出发，采取一个动作*At*并且代理获得奖励R。这样，代理就被转移到下一个状态*St + 1*，并在*St
    + 1*中采取动作*At + 1*。实际上，SARSA是元组(*S, A*, *R*, *St + 1*, *At + 1*)的缩写。
- en: 'The following is the whole code for the SARSA approach:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是SARSA方法的完整代码：
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see, much of the code is similar to the previous case (Q-learning),
    as there are many similarities between the two approaches. We will only analyze
    the differences between the two approaches. In the first part of the code, the
    initial parameters are set and the rewards matrix is defined:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，大部分代码与前一个案例（Q-learning）相似，因为这两种方法之间有许多相似之处。我们只会分析两者之间的差异。在代码的第一部分，设置了初始参数并定义了奖励矩阵：
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s move on to initialize the matrix Q and set the cycle that will
    allow us to update the action-value function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续初始化Q矩阵并设置将允许我们更新动作价值函数的循环：
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Up to this point, nothing has changed compared to the formulation analyzed
    in the previous example. But now there are essential changes. In the *SARSA* section,
    we saw the pseudo-code of the algorithm; for convenience, we repeat it here:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，与前一个示例中分析的公式相比，没有任何变化。但现在有了一些重要的变化。在*SARSA*部分，我们看到了算法的伪代码；为了方便起见，我们在此重复一下：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Comparing it with the one proposed in the previous section (the Q-learning
    approach), we can see that the substantial difference between the two methods
    lies in the formula used for updating the action-value function and in the calculation
    of the action to be followed in the next state. The next action we will follow
    in the next state is calculated as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一部分（Q-learning方法）中提出的方法相比，我们可以看到这两种方法的实质性区别在于更新动作价值函数所使用的公式以及计算下一状态要采取的动作。我们将在下一个状态中执行的动作计算公式如下：
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The approach is used to evaluate the next state. The formula we will use will
    be the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法用于评估下一个状态。我们将使用的公式如下：
- en: '![](img/d669691c-1ceb-4ff7-bd8e-1992a2684e0f.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d669691c-1ceb-4ff7-bd8e-1992a2684e0f.png)'
- en: 'This formula in code R becomes this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式在R代码中变为：
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The rest of the code is similar to the previous section:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码与前一部分类似：
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can then analyze the results:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以分析结果：
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The shortest path is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径如下：
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The result is identical to the one obtained in the previous example. Let's understand
    how the two approaches are different from each other.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与前一个示例中得到的结果相同。让我们来理解这两种方法有何不同。
- en: Differentiating SARSA and Q-learning
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 区分SARSA和Q-learning
- en: 'From the algorithmic point of view, the substantial difference between the
    two approaches we analyzed in the previous sections lies in the two equations
    we used to update the action-value function. Let''s compare them to understand
    them better:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从算法的角度来看，我们在前面章节分析的两种方法之间的实质性差异在于我们用来更新动作价值函数的两个方程。我们来对比一下它们，以便更好地理解：
- en: '![](img/3feba7be-e270-4ece-af07-033418ce16e7.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3feba7be-e270-4ece-af07-033418ce16e7.png)'
- en: '![](img/ec0f3f35-3304-4f66-962f-8fcedf1701f1.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec0f3f35-3304-4f66-962f-8fcedf1701f1.png)'
- en: 'Q-learning calculates the difference between *Q (s, a)* and the maximum value
    of the action, while SARSA calculates the difference between *Q (s, a)* and the
    value of the next action. In doing this, you can highlight the following points:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习计算*Q (s, a)*和动作的最大值之间的差异，而SARSA计算*Q (s, a)*与下一步动作的值之间的差异。在这样做时，您可以突出以下几点：
- en: SARSA uses the policy used by the agent to generate experience in the environment
    (such as epsilon-greedy), in order to select an additional action *A t + 1*. Then,
    it uses *Q (S t + 1, A t +1)* to discount the gamma factor as expected future
    returns in the calculation of the update target.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSA使用智能体在环境中生成经验时使用的策略（如epsilon-贪心），以选择额外的动作*A t + 1*。然后，它使用*Q (S t + 1, A
    t + 1)*来折扣gamma因子，并将其作为预期的未来回报，计算更新目标。
- en: Q-learning does not use this policy to select an additional action *A t + 1*.
    Instead, it estimates the expected future returns in the update rule as *max Q
    (S t + 1, A)* for all actions.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习不使用此策略来选择额外的动作*A t + 1*。相反，它在更新规则中估计期望的未来回报，将其表示为*max Q (S t + 1, A)*，并对所有动作进行计算。
- en: 'The two approaches converge to different solutions:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法收敛到不同的解：
- en: SARSA converges to the optimal solution by following the same policy that was
    used to generate the experience. This will have elements of randomness to ensure
    convergence.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSA通过遵循与生成经验时相同的策略收敛到最优解。这将包含一些随机性，以确保收敛。
- en: Q-learning converges into an optimal solution after generating experience and
    training by following a greedy policy.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习通过遵循贪心策略生成经验并进行训练，最终收敛到一个最优解。
- en: SARSA is advisable when we need to guarantee the agent's performance during
    the learning process. This is where, in the learning process, we must guarantee
    a low number of errors that are expensive for the equipment we are using. Hence,
    we care about its performance during the learning process.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要确保智能体在学习过程中的表现时，建议使用SARSA。这是因为在学习过程中，我们必须确保错误的数量较少，而这些错误对于我们使用的设备来说是昂贵的。因此，我们关心其在学习过程中的表现。
- en: An algorithm like Q-learning is advisable in cases where we do not care about
    the agent's performance during the learning process and we only want the agent
    to learn about an optimal greedy policy that we will adopt at the end of the process.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 像Q学习这样的算法在我们不关心学习过程中智能体的表现，仅仅希望智能体学会一个最优的贪心策略（我们将在过程结束时采用）时是值得推荐的。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, TD learning algorithms were introduced. These algorithms are
    based on reducing the differences between the estimates that are made by the agent
    at different times. The SARSA algorithm implements an on-policy TD method, while
    Q-learning has off-policy characteristics.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，介绍了TD学习算法。这些算法基于减少智能体在不同时间做出的估计之间的差异。SARSA算法实现了一个在策略的TD方法，而Q学习具有脱离策略的特点。
- en: Then, the basics of graph theory were addressed—the adjacency matrix and adjacency
    list topics were covered. We have seen how to represent graphs in R using the
    `igraph` package. By doing this, we addressed the shortest path problem. We also
    analyzed the Dijkstra algorithm in R.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，介绍了图论的基础——包括邻接矩阵和邻接列表的内容。我们已经看到如何使用`igraph`包在R中表示图。通过这样做，我们解决了最短路径问题，并且分析了R中的Dijkstra算法。
- en: Finally, the vehicle routing problem was resolved using the Q-learning and SARSA
    algorithms. The differences between the two approaches to solve the problem were
    analyzed in detail.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用Q学习和SARSA算法解决了车辆路径规划问题。详细分析了这两种方法解决该问题的差异。
- en: In the next chapter, we will about learn the fundamental concepts of game theory.
    We will learn how to install and configure the OpenAI Gym library and understand
    how it works. We will learn about the difference between the Q-learning and SARSA
    algorithms and understand how to make a learning and a testing phase. Finally,
    we will learn how to develop OpenAI Gym applications using R.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习博弈论的基本概念。我们将学习如何安装和配置 OpenAI Gym 库，并理解它是如何工作的。我们将了解 Q-learning 和 SARSA
    算法之间的区别，并理解如何进行学习阶段和测试阶段。最后，我们将学习如何使用 R 开发 OpenAI Gym 应用。
