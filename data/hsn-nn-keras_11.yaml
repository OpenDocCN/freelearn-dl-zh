- en: Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'In the preceding chapter, we familiarized ourselves with a novel area in **machine
    learning** (**ML**): the realm of reinforcement learning. We saw how reinforcement
    learning algorithms can be augmented using neural networks, and how we can learn
    approximate functions that can map game states to possible actions the agent may
    take. These actions are then compared to a moving target variable, which in turn
    was defined by what we called the **Bellman equation**. This, strictly speaking,
    is a self-supervised ML technique, as it is the Bellman equation that''s used
    to compare our predictions, and not a set of labeled target variables, as would
    be the case for a supervised learning approach (for example, game screens labeled
    with optimal actions to take at each state). The latter, while possible, proves
    to be much more computationally intensive for the given use case. Now we will
    move on and discover yet another self-supervised ML technique as we dive into
    the world of **neural autoencoders**.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们熟悉了**机器学习**（**ML**）中的一个新领域：强化学习的领域。我们看到如何通过神经网络增强强化学习算法，以及如何学习近似函数，将游戏状态映射到智能体可能采取的动作。这些动作随后与一个动态的目标变量进行比较，而该目标变量是通过我们所称的**贝尔曼方程**定义的。严格来说，这是一种自监督学习的机器学习技术，因为用来比较我们预测结果的是贝尔曼方程，而不是一组标记的目标变量，这在监督学习方法中才会出现（例如，带有每个状态下应采取的最优动作标签的游戏画面）。后者虽然可能实现，但对于给定的使用场景来说，计算成本要高得多。现在，我们将继续前进，发现另一种自监督机器学习技术，探索**神经自编码器**的世界。
- en: In this chapter, we will explore the utility and advantages of making neural
    networks learn to encode the most representative features from a given dataset.
    In essence, this allows us to preserve, and later recreate, the key elements that
    define a class of observations. The observations themselves can be images, natural
    language data, or even time-series observations that may benefit from a reduction
    in dimensionality, weeding out bits of information representing less informative
    aspects of the given observations. Cui bono ? You ask.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将探讨让神经网络学习从给定数据集中编码最具代表性特征的实用性和优势。本质上，这使我们能够保留并在以后重建定义观察类别的关键元素。观察本身可以是图像、自然语言数据，甚至是可能受益于降维的时间序列观察，通过去除那些表示给定观察中较不具信息性方面的信息。你可能会问，*谁得益*？
- en: 'Following are the topics that will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Why autoencoders?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择自编码器？
- en: Automatically encoding information
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码信息
- en: Understanding the limitations of autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自编码器的局限性
- en: Breaking down the autoencoder
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析自编码器
- en: Training an autoencoder
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练自编码器
- en: Overviewing autoencoder archetypes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概览自编码器原型
- en: Network size and representational power
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络规模和表示能力
- en: Understanding regularization in autoencoders
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自编码器中的正则化
- en: Regularization with sparse autoencoders
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用稀疏自编码器进行正则化
- en: Probing the data
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据
- en: Building the verification model
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建验证模型
- en: Designing a deep autoencoder
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计深度自编码器
- en: Using functional API to design autoencoders
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用功能性API设计自编码器
- en: Deep convolutional autoencoders
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积自编码器
- en: Compiling and training the model
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译和训练模型
- en: Testing and visualising the results
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试并可视化结果
- en: Denoising autoencoders
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Training the denoising network
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练去噪网络
- en: Why autoencoders?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择自编码器？
- en: While, in the past (circa 2012), autoencoders have briefly enjoyed some fame
    for their use in initializing layer weights for deep **Convolutional Neural Networks**
    (**CNNs**) (through an operation known as **greedy layer-wise pretraining**),
    researchers gradually lost interest in such pretraining techniques as better random
    weight initialization schemes came about, and more advantageous methods that allowed
    deeper neural networks to be trained (such as batch normalization, 2014, and later
    residual learning, 2015) surfaced to the general sphere.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 过去（大约2012年），自编码器因其在初始化深度**卷积神经网络**（**CNNs**）层权重方面的应用（通过一种被称为**贪婪逐层预训练**的操作）而短暂地享有一些声誉，但随着更好的随机权重初始化方案的出现，以及允许训练更深层神经网络的更具优势的方法（例如2014年的批量归一化，及2015年的残差学习）逐渐成为主流，研究人员对这种预训练技术的兴趣逐渐减退。
- en: Today, a paramount utility of autoencoders is derived from their ability to
    discover low-dimensional representations of high-dimensional data, while still
    attempting to preserve the core attributes present therein. This permits us to
    perform tasks such as recovering damaged images (or image denoising). A similar
    area of active interest for autoencoders comes from their ability to perform principal
    component analysis, such as transformations on data, allowing for informative
    visualizations of the main factors of variance that are present. In fact, single-layer
    autoencoders with a linear activation function can be quite similar to the standard
    **Principal Component Analysis** (**PCA**) operation that's performed on datasets.
    Such an autoencoder simply learns the same dimensionally reduced subspace that
    would arise out of a PCA. Hence, autoencoders may be used in conjunction with
    the t-SNE algorithm ([https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)),
    which is famous for its ability to visualize information on a 2D plane, to first
    downsample a high-dimensional dataset, then visualize the main factors of variance
    that are observable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，自动编码器的一个重要应用来自于它们能够发现高维数据的低维表示，同时尽量保留其中的核心属性。这使我们能够执行例如恢复损坏图像（或图像去噪）等任务。自动编码器的另一个活跃研究领域是它们能够执行主成分分析，例如数据的变换，从而可以可视化数据中主要方差因素的有用信息。事实上，带有线性激活函数的单层自动编码器与在数据集上执行的标准**主成分分析**（**PCA**）操作非常相似。这样的自动编码器只学习一个维度减少的子空间，这个子空间正是通过PCA得到的。因此，自动编码器可以与t-SNE算法（[https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)）结合使用，后者因其在二维平面上可视化信息的能力而著名，首先对高维数据集进行降采样，然后可视化观察到的主要方差因素。
- en: Moreover, the advantage of autoencoders for such use cases (that is, performing
    a dimensionality reduction) stems from the fact that they may have non-linear
    encoder and decoder functions, whereas the PCA algorithm is restricted to a linear
    map. This allows autoencoders to learn more powerful non-linear representations
    of the feature space compared to results from PCA analysis of the same data. In
    fact, autoencoders can prove to be a very powerful tool in your data science repertoire
    when you're dealing with very sparse and high-dimensional data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自动编码器在此类应用中的优势（即执行降维）源于它们可能具有非线性的编码器和解码器函数，而PCA算法仅限于线性映射。这使得自动编码器能够学习比PCA分析相同数据得到的结果更强大的非线性特征空间表示。事实上，当你处理非常稀疏和高维的数据时，自动编码器可以证明是数据科学工具箱中一个非常强大的工具。
- en: Besides these practical applications of autoencoders, more creative and artistic
    ones also exist. Sampling from the reduced representation that's produced by the
    encoder, for example, has been used to generate artistic images that were auctioned
    for around half a million dollars at one New York-based auction house (see [https://www.bloomberg.com/news/articles/2018-10-25/ai-generated-portrait-is-sold-for-432-500-in-an-auction-first](https://www.bloomberg.com/news/articles/2018-10-25/ai-generated-portrait-is-sold-for-432-500-in-an-auction-first)).
    We will review the fundamentals of such image generation techniques in the next
    chapter, when we cover the variational autoencoder architecture and **Generative
    Adversarial Networks** (**GANs**). But first, let's try to better understand the
    essence of an autoencoder neural network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动编码器的这些实际应用外，还有一些更具创意和艺术性的应用。例如，从编码器生成的低维表示中采样，已被用来生成艺术图像，这些图像在纽约某拍卖行以约50万美元的价格拍卖（见[https://www.bloomberg.com/news/articles/2018-10-25/ai-generated-portrait-is-sold-for-432-500-in-an-auction-first](https://www.bloomberg.com/news/articles/2018-10-25/ai-generated-portrait-is-sold-for-432-500-in-an-auction-first)）。我们将在下一章回顾此类图像生成技术的基础知识，当时我们将介绍变分自动编码器架构和**生成对抗网络**（**GANs**）。但首先，让我们尝试更好地理解自动编码器神经网络的本质。
- en: Automatically encoding information
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码信息
- en: Well then, what's so different about the idea of autoencoders? You have surely
    come across countless encoding algorithms, ranging from MP3 compression that's
    performed to store audio files, or JPEG compression to store image files. The
    reason autoencoding neural networks are interesting is they take a very different
    approach toward representing information compared to their previously stated quasi-counterparts.
    It is the kind of approach you have certainly come to expect after seven long
    chapters on the inner workings of neural networks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，自编码器的理念有什么不同之处呢？你肯定已经接触过无数的编码算法，像是MP3压缩用于存储音频文件，或者JPEG压缩用于存储图像文件。自编码神经网络之所以有趣，是因为它们采用了一种与之前提到的准对等物相比非常不同的信息表示方式。这正是你在阅读完神经网络内部工作机制的七个章节后，理应期待的一种方法。
- en: Unlike the MP3 or JPEG algorithms, which hold general assumptions about sound
    and pixels, a neural autoencoder is forced to learn representative features automatically
    from whatever input it is shown during a training session. It proceeds to recreate
    the given input by using the learned representations that were captured during
    the session. It is important to understand that the appeal of autoencoders do
    not come from simply copying its input. When training an autoencoder, we are typically
    not interested in the decoded output it generates per say, but rather how the
    network transforms the dimensionality of the given inputs. Ideally, we are looking
    for representative encoding schemes by giving the networks incentives and constraints
    to reconstruct the original input as closely as possible. By doing so, we can
    use the encoder function on similar datasets as a feature detection algorithm,
    which provides us with a semantically rich representation of the given inputs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与MP3或JPEG算法不同，后者通常对声音和像素有普遍的假设，而神经自编码器则被迫自动从任何训练过程中显示的输入中学习代表性特征。它接着使用在训练过程中捕获的学习表示来重建给定的输入。重要的是要理解，自编码器的吸引力并不在于简单地复制其输入。当训练自编码器时，我们通常并不关心它所生成的解码输出，而更关心的是网络如何转化给定输入的维度。理想情况下，我们希望通过给网络提供激励和约束，以尽可能准确地重建原始输入，从而寻找代表性的编码方案。通过这样做，我们可以将编码器函数应用于类似的数据集，作为一种特征检测算法，从而为给定的输入提供语义丰富的表示。
- en: These representations can then be used to perform a classification of sorts,
    depending on the use case being tackled. It is thus the architectural mechanism
    of encoding that's employed, and which defines the novel approach of autoencoders
    compared to other standard encoding algorithms.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示方法可以用来执行某种分类，具体取决于所处理的使用案例。因此，采用的正是编码的架构机制，这也定义了自编码器与其他标准编码算法相比的新颖方法。
- en: Understanding the limitations of autoencoders
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自编码器的局限性
- en: As we saw previously, neural networks such as autoencoders are used to automatically
    learn representative features from data, without explicitly relying on human-engineered
    assumptions. While this approach may allow us to discover ideal encoding schemes
    that are specific to different types of data, this approach does present certain
    limitations. Firstly, autoencoders are said to be **data-specific**, in the sense
    that their utility is restricted to data that is considerably similar to its training
    data. For example, an autoencoder that's trained to only regenerate cat pictures
    will have a very hard time generating dog pictures without explicitly being trained
    to do so. Naturally, this seems to reduce the scalability of such algorithms.
    It is also noteworthy that autoencoders, as of yet, do not perform noticeably
    better than the JPEG algorithm at encoding images. Another concern is that autoencoders
    tend to produce a **lossy output**. This simply means that the compression and
    decompression operation degrades the output of the network, generating a less
    accurate representation compared to its input. This problem seems to be a recurrent
    one for most encoding use cases (including heuristic-based encoding schemes such
    as MP3 and JPEG).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络，例如自编码器，被用来自动从数据中学习代表性特征，而不需要明确依赖于人工设计的假设。尽管这种方法可以让我们发现适用于不同类型数据的理想编码方案，但它也确实存在一些局限性。首先，自编码器被认为是**数据特定的**，这意味着它们的作用仅限于与训练数据非常相似的数据。例如，一个仅训练生成猫图像的自编码器在没有明确训练的情况下，几乎无法生成狗的图像。显然，这似乎限制了此类算法的可扩展性。值得注意的是，直到现在，自编码器在编码图像时的表现并没有明显优于JPEG算法。另一个问题是，自编码器往往会产生**有损输出**。这意味着压缩和解压操作会降低网络输出的质量，生成的表示相比输入会不够精确。这个问题似乎在大多数编码应用场景中都有出现（包括基于启发式的编码方案，如MP3和JPEG）。
- en: Thus, autoencoders have unraveled some very promising practices to work with
    *unlabeled* real-world data. However, the vast majority of data that's available
    on the digital sphere today is in fact unstructured and unlabeled. It is also
    noteworthy that popular misconception assigns autoencoders into the unsupervised
    learning category, yet, in reality, it is but another variation of self-supervised
    learning, as we will soon discover. So, how exactly do these networks work?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自编码器揭示了一些非常有前景的实践方法，用于处理*未标记的*真实世界数据。然而，今天在数字领域中可用的绝大多数数据实际上是无结构且未标记的。另一个值得注意的常见误解是将自编码器归类为无监督学习，但实际上，它不过是自监督学习的另一种变体，正如我们很快将要发现的那样。那么，这些网络究竟是如何工作的呢？
- en: Breaking down the autoencoder
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析自编码器
- en: 'Well, on a high level, an autoencoder can be thought of as a specific type
    of feed-forward network that learns to mimic its input to reconstruct a similar
    output. As we mentioned previously, it is composed of two separate parts: an encoder
    function and a decoder function. We can think of the entire autoencoder as layers
    of interconnected neurons, which propagate data by first encoding its input and
    then reconstructing the output using the generated code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次看，自编码器可以被认为是一种特定类型的前馈网络，它学习模仿输入并重构出相似的输出。正如我们之前提到的，它由两部分组成：编码器函数和解码器函数。我们可以将整个自编码器视为一层层互联的神经元，首先通过编码输入数据，然后使用生成的编码重构输出：
- en: '![](img/ebd6bfc2-9000-4ce4-b980-e55b2caf5cb1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebd6bfc2-9000-4ce4-b980-e55b2caf5cb1.png)'
- en: Example of an undercomplete autoencoder
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不完全自编码器的示例
- en: The previous diagram illustrates a specific type of autoencoder network. Conceptually,
    the input layer of an autoencoder connects to a layer of neurons to funnel the
    data into a latent space, known as the **encoder function**. This function can
    be generically defined as *h = f(x)*, where *x* refers to the network inputs and
    *h* refers to the latent space that's generated by the encoder function. The latent
    space may embody a compressed representation of the input to our network, and
    is subsequently used by the decoder function (that is, the proceeding layer of
    neurons) to unravel the reduced representation, mapping it to a higher-dimensional
    feature space. Thus, the decoder function (formulized as *r = g(h)*) proceeds
    to transform the latent space that's generated by the encoder (*h*) into the *reconstructed*
    output of the network (*r*).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个特定类型的自动编码器网络。从概念上讲，自动编码器的输入层连接到一个神经元层，将数据引导到一个潜在空间，这就是 **编码器函数**。该函数可以泛化定义为
    *h = f(x)*，其中 *x* 代表网络输入，*h* 代表由编码器函数生成的潜在空间。潜在空间可能体现了输入到我们网络的压缩表示，随后被解码器函数（即后续的神经元层）用来解开这个简化的表示，将其映射到一个更高维的特征空间。因此，解码器函数（形式化为
    *r = g(h)*) 接着将由编码器生成的潜在空间 (*h*) 转换为网络的 *重构* 输出 (*r*)。
- en: Training an autoencoder
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个自动编码器
- en: The interaction between the encoder and decoder functions is governed by yet
    another function, which operationalizes the distance between the inputs and outputs
    of the encoder. We have come to know this as the `loss` function in neural network
    parlance. Hence, to train an autoencoder, we simply differentiate our encoder
    and decoder functions with respect to the `loss` function (typically using mean
    squared error) and use the gradients to backpropagate the model's errors and update
    the layer weights of the entire network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器函数之间的交互由另一个函数控制，该函数操作输入和输出之间的距离，我们通常称之为神经网络中的 `loss` 函数。因此，为了训练一个自动编码器，我们只需对编码器和解码器函数分别关于
    `loss` 函数（通常使用均方误差）进行求导，并使用梯度来反向传播模型的误差，更新整个网络的层权重。
- en: 'Consequently, the learning mechanism of an autoencoder can be denoted as minimizing
    a `loss` function, and is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自动编码器的学习机制可以表示为最小化一个 `loss` 函数，其公式如下：
- en: '*min L(x, g ( f ( x ) ) )*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*min L(x, g ( f ( x ) ) )*'
- en: In the previous equation, *L* represents a `loss` function (such as MSE) that
    penalizes the output of the decoder function (*g(f( x ))*) for being divergent
    from the network's input, *(x)*. By iteratively minimizing the reconstructed loss
    in this manner, our model will eventually converge to encode ideal representations
    that are specific to the input data, which can then be used to decoded similar
    data with a minimal amount of information loss. Hence, autoencoders are almost
    always trained via mini-batch gradient decent, as is common with other cases of
    feed-forward neural networks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*L* 代表一个 `loss` 函数（如均方误差 MSE），它对解码器函数的输出（*g(f( x ))*）进行惩罚，惩罚的内容是输出与网络输入
    *(x)* 的偏差。通过这种方式反复最小化重构损失，我们的模型最终将收敛到编码适应输入数据的理想表示，这些表示可以用于解码类似数据，同时损失的信息量最小。因此，自动编码器几乎总是通过小批量梯度下降进行训练，这与其他前馈神经网络的训练方法相同。
- en: While autoencoders may also be trained using a technique known as **recirculation**
    (Hinton and McClelland, 1988), we will refrain from visiting this subtopic in
    this chapter, as this method is rarely used in most machine learning use cases
    involving autoencoders. Suffice it to mention that recirculation works by comparing
    network activations on given inputs to network activations on the generated reconstruction,
    instead of backpropagating gradient-based errors that are derived from differentiating
    the `loss` function with respect to network weights. While conceptually distinct,
    this may be interesting to read upon from a theoretical perspective, since recirculation
    is considered to be a biologically plausible alternative to the backpropagation
    algorithm, hinting at how we ourselves may update our mental models of the world
    as new information comes about.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自编码器也可以使用一种称为**再循环**（Hinton和McClelland，1988）的技术进行训练，但我们在本章中不会深入讨论这一子话题，因为这种方法在大多数涉及自编码器的机器学习应用中很少使用。仅需提及，再循环通过将给定输入的网络激活与生成的重建的网络激活进行比较，而不是通过反向传播基于梯度的误差（通过对`loss`函数相对于网络权重求导来获得）来工作。尽管在概念上有所不同，从理论角度来看，这可能是一个有趣的阅读内容，因为再循环被认为是反向传播算法的生物学上可行的替代方案，暗示着我们自己可能如何随着新信息的出现，更新我们对世界的心理模型。
- en: Overviewing autoencoder archetypes
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览自编码器原型
- en: What we described previously is actually an example of an **undercomplete autoencoder**,
    which essentially puts a constraint on the latent space dimension. It is designated
    undercomplete, since the encoding dimension (that is, the dimension of the latent
    space) is smaller than the input dimension, which forces the autoencoder to learn
    about the most salient features that are present in the data sample.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前描述的其实是一个**欠完备自编码器**的例子，基本上它对潜在空间维度进行了约束。之所以称其为欠完备，是因为编码维度（即潜在空间的维度）小于输入维度，这迫使自编码器学习数据样本中最显著的特征。
- en: 'Conversely, an **overcomplete autoencoder** has a larger encoding dimension
    relative to its input dimension. Such autoencoders are endowed with additional
    encoding capacity in relation to their input size, as can be seen in the following
    diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，**过完备自编码器**则具有相对于输入维度更大的编码维度。这种自编码器相比输入大小拥有更多的编码能力，正如下图所示：
- en: '![](img/d520499e-889a-439b-a771-80a816642263.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d520499e-889a-439b-a771-80a816642263.png)'
- en: Network size and representational power
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络规模与表示能力
- en: In the previous diagram, we can see four types of basic autoencoding architectures.
    **Shallow autoencoders** (an extension of shallow neural networks) are defined
    by having just one hidden layer of neurons, whereas deep autoencoders can have
    many layers that perform the encoding and decoding operations. Recall from the
    previous chapters that deeper neural networks may benefit from additional representational
    power compared to their shallow counterparts. Since autoencoders qualify as a
    specific breed of feed-forward networks, this also holds true for them. Additionally,
    it has been noted that deeper autoencoders may exponentially reduce the computational
    resources that are required for the network to learn to represent its inputs.
    It may also greatly reduce the number of training samples that are required for
    the network to learn a rich compressed version of the inputs. While reading the
    last few lines may incentivize some of you to start training hundreds of layered
    autoencoders, you may want to hold your horses. Giving the encoder and decoder
    functions too much capacity comes with its own disadvantages.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图示中，我们可以看到四种基本的自编码架构。**浅层自编码器**（浅层神经网络的扩展）通过仅有一个隐藏层的神经元来定义，而深层自编码器则可以有多个层来执行编码和解码操作。回顾之前章节的内容，较深的神经网络相比浅层神经网络可能具备更强的表示能力。这一原则同样适用于自编码器。除此之外，还注意到，深层自编码器可能会在网络学习表示输入时，显著减少所需的计算资源。它还可以大大减少网络学习输入的丰富压缩版本所需的训练样本数量。虽然读到最后几行可能会促使你们中的一些人开始训练数百层的自编码器，但你可能想要稍安勿躁。赋予编码器和解码器函数过多的能力也会带来自身的缺点。
- en: For example, an autoencoder with excess capacity may learn to perfectly recreate
    input images of Picasso paintings, without ever learning a single representative
    feature related to Picasso's painting style. In this case, all you have is an
    expensive copycat algorithm, which may be paralleled by Microsoft Paint's copy
    function. On the other hand, designing an autoencoder in accordance with the complexity
    and distribution of the data being modeled may well allow an AE to capture representative
    stylistic features, iconic to Picasso's *modus operandi*, from which aspiring
    artists and historians alike may learn. In practice, choosing the correct network
    depth and size may depend on a keen combination of theoretical familiarity with
    the learning process, experimentation, and domain knowledge in relation to the
    use case. Does this sound a bit time-consuming? Luckily, there may be a compromise
    ahead, which can be achieved through the use of regularized autoencoders.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个具有过多容量的自编码器可能学会完美地重建毕加索画作的输入图像，而从未学会与毕加索画风相关的任何代表性特征。在这种情况下，你得到的只是一个昂贵的模仿算法，它可以与微软画图的复制功能类比。另一方面，按照所建模数据的复杂性和分布设计自编码器，可能使得自编码器能够捕捉到毕加索*创作方法*中具有代表性的风格特征，进而成为艺术家和历史学者学习的素材。实际上，选择正确的网络深度和规模可能依赖于对学习过程的理论理解、实验以及与使用场景相关的领域知识的精妙结合。听起来有点耗时吗？幸运的是，可能有一种折衷方案，通过使用正则化自编码器来实现。
- en: Understanding regularization in autoencoders
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自编码器中的正则化
- en: On one extreme, you may always try to limit the network's learning capacity
    by sticking to shallow layers and having a very small latent space dimension.
    This approach may even provide an excellent baseline for benchmarking against
    more complex methods. However, other methods exist that may allow us to benefit
    from the representational power of deeper layers, without being penalized for
    issues of overcapacity up to a certain extent. Such methods include modifying
    the `loss` function that's used by an autoencoder so as to incentivize some representational
    criteria for the latent space being learned by the network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个极端情况下，你可能总是通过坚持浅层网络并设置非常小的潜在空间维度来限制网络的学习能力。这种方法甚至可能为基准测试复杂方法提供一个优秀的基线。然而，也有其他方法可以让我们在不被过度容量问题惩罚的情况下，受益于更深层网络的表示能力，直到某种程度。这些方法包括修改自编码器使用的`loss`函数，以激励学习网络中潜在空间的某些表示标准。
- en: For example, instead of simply copying the inputs, we may require our `loss`
    function to account for the sparsity of the latent space, favoring more rich representations
    over others. As we will see, we may even consider properties such as the magnitude
    of the derivatives of the latent space, or robustness to missing inputs, to ensure
    that our model really captures representative features from the inputs it is shown.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以要求`loss`函数考虑潜在空间的稀疏性，偏向于更丰富的表示，而不是其他表示。正如我们所看到的，我们甚至可以考虑潜在空间的导数大小或对缺失输入的鲁棒性等属性，以确保我们的模型确实捕捉到它所展示的输入中的代表性特征。
- en: Regularization with sparse autoencoders
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用稀疏自编码器进行正则化
- en: 'As we mentioned previously, one way of ensuring that our model encodes representative
    features from the inputs that are shown is by adding a sparsity constraint on
    the hidden layer representing the latent space (*h*). We denote this constraint
    with the Greek letter omega (Ω), which allows us to redefine the `loss` function
    of a sparse autoencoder, like so:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，确保我们的模型从输入中编码代表性特征的一种方法是对表示潜在空间（*h*）的隐藏层添加稀疏性约束。我们用希腊字母欧米伽（Ω）表示这个约束，这样我们就可以重新定义稀疏自编码器的`loss`函数，方式如下：
- en: 'Normal AE loss: *L ( x , g ( f ( x ) ) )*'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常自编码器损失：*L ( x , g ( f ( x ) ) )*
- en: 'Sparse AE loss: *L ( x , g ( f ( x ) ) ) + Ω(h)*'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏自编码器损失：*L ( x , g ( f ( x ) ) ) + Ω(h)*
- en: This sparsity constraint term, *Ω(h)*, can simply be thought of as a regularizer
    term that can be added to a feed-forward neural network, as we saw in previous
    chapters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个稀疏性约束项，*Ω(h)*，可以简单地看作是可以添加到前馈神经网络中的正则化项，就像我们在前几章中看到的那样。
- en: 'A comprehensive review of different forms of sparsity constraint methods in
    autoencoders can be found in the following research paper, which we recommend
    to our interested audience: *Facial expression recognition via learning deep sparse
    autoencoders*: [https://www.sciencedirect.com/science/article/pii/S0925231217314649](https://www.sciencedirect.com/science/article/pii/S0925231217314649).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自编码器中不同形式稀疏约束方法的全面综述可以在以下研究论文中找到，我们推荐感兴趣的读者参考：*通过学习深度稀疏自编码器进行面部表情识别*：[https://www.sciencedirect.com/science/article/pii/S0925231217314649](https://www.sciencedirect.com/science/article/pii/S0925231217314649)。
- en: This frees up some space in our agenda so that we can give you brief overview
    of some other regularization methods that are used by autoencoders before we proceed
    to coding our very own models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们的议程腾出了一些空间，使我们可以简要介绍一些自编码器在实践中使用的其他正则化方法，然后再继续编写我们自己的模型。
- en: Regularization with denoising autoencoders
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用去噪自编码器的正则化
- en: Unlike sparse autoencoders, denoising autoencoders take a different approach
    toward ensuring that our model captures useful representations in the capacity
    that it is endowed. Here, instead of adding a constraint to the `loss` function,
    we can actually modify the reconstruction error term in our `loss` function. In
    other words, we will simply tell our network to reconstruct its input by using
    a noisy version of that very input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与稀疏自编码器不同，去噪自编码器通过不同的方式确保我们的模型能够在其赋予的能力范围内捕捉有用的表示。在这种情况下，我们不是向`loss`函数添加约束，而是可以实际修改`loss`函数中的重建误差项。换句话说，我们只是告诉网络，通过使用该输入的噪声版本来重建其输入。
- en: 'In this case, noise may refer to missing pixels in a picture, absent words
    in a sentence, or a fragmented audio feed. Thus, we may reformulate our `loss`
    function for denoising autoencoders like so:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，噪声可能指的是图像中的缺失像素、句子中的缺失单词，或者碎片化的音频流。因此，我们可以将去噪自编码器的`loss`函数重新公式化如下：
- en: 'Normal AE loss: *L ( x , g ( f ( x ) ) )*'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常AE损失：*L ( x , g ( f ( x ) ) )*
- en: 'Denoising AE loss: *L ( x , g ( f ( ~x) ) )*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪AE损失：*L ( x , g ( f ( ~x) ) )*
- en: Here, the term (*~x*) simply refers to a version of the input *x* that has been
    corrupted by some form of noise. Our denoising autoencoder must then proceed to
    uncorrupt the noisy input that's provided, instead of simply attempting to copy
    the original input. Adding noise to the training data may force the autoencoder
    to capture representative features that are the most relevant for properly reconstructing
    the corrupted versions of the training instances.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，术语(*~x*)仅指被某种噪声形式破坏过的输入*x*的版本。我们的去噪自编码器必须对提供的有噪声输入进行去噪，而不是仅仅尝试复制原始输入。向训练数据中添加噪声可能迫使自编码器捕捉最能代表正确重建损坏版本训练实例的特征。
- en: 'Some interesting properties and use cases (such as speech enhancement) for
    denoising autoencoders have been explored in the following paper, and are noteworthy
    for interested readers: *Speech Enhancement Based on Deep Denoising Autoencoder*:
    [https://pdfs.semanticscholar.org/3674/37d5ee2ffbfee1076cf21c3852b2ec50d734.pdf](https://pdfs.semanticscholar.org/3674/37d5ee2ffbfee1076cf21c3852b2ec50d734.pdf).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有趣的特性和使用案例（例如语音增强）已在以下论文中探讨，对于感兴趣的读者来说值得注意：*基于深度去噪自编码器的语音增强*：[https://pdfs.semanticscholar.org/3674/37d5ee2ffbfee1076cf21c3852b2ec50d734.pdf](https://pdfs.semanticscholar.org/3674/37d5ee2ffbfee1076cf21c3852b2ec50d734.pdf)。
- en: This brings us to the last regularization strategy we will cover in this chapter,
    that is, the contractive autoencoder, before moving on to practical matters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引出我们在本章中将讨论的最后一种正则化策略——收缩自编码器，然后再进入实际的内容。
- en: Regularization with contractive autoencoders
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用收缩自编码器的正则化
- en: 'While we will not dive deep into the mathematics of this subspecies of autoencoder
    network, the **contractive autoencoder** (**CAE**) is noteworthy due to its conceptual
    similarity to the denoising autoencoder, as well as how it locally warps the input
    space. In the case of CAEs, we again add a constraint (Ω) to the `loss` function,
    but in a different manner:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会深入探讨这种自编码器网络亚种的数学，但**收缩自编码器**（**CAE**）因其在概念上与去噪自编码器的相似性以及如何局部扭曲输入空间而值得注意。在CAE的情况下，我们再次向`loss`函数添加一个约束（Ω），但方式有所不同：
- en: '**Normal AE loss**: *L ( x , g ( f ( x ) ) )*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正常AE损失**：*L ( x , g ( f ( x ) ) )*'
- en: '**CAE loss**: *L ( x , g ( f ( x) ) ) + Ω(h,x)*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CAE损失**：*L ( x , g ( f ( x) ) ) + Ω(h,x)*'
- en: 'Here, the term *Ω(h, x)* is represented differently, and can be formulated
    as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，术语*Ω(h, x)*以不同的方式表示，可以按照以下方式公式化：
- en: '![](img/6e3de284-e941-4e1e-a630-70b521476b6d.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e3de284-e941-4e1e-a630-70b521476b6d.png)'
- en: Here, the CAE makes use of the constraint on the `loss` function to encourage
    the derivates of the encoder to be as small as possible. For those of you who
    are more mathematically oriented, the constraint term Ω(h, x) is actually known
    as the **squared Frobenius norm** (that is, the sum of squared elements) of the
    Jacobian matrix that's populated with the partial derivatives of the encoder function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，CAE利用对`loss`函数的约束来鼓励编码器的导数尽可能小。对于那些更加数学倾向的人来说，约束项Ω(h, x)实际上被称为**Frobenius范数的平方**（即元素的平方和），用于填充编码器函数的偏导数的雅可比矩阵。
- en: 'The following paper provides an excellent overview of the inner workings of
    CAEs and their use in feature extraction, for those who wish to expand their knowledge
    beyond the brief summary provided here: *Contractive Auto-Encoders: Explicit Invariance
    During Feature Extraction*: [http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '对于那些希望扩展其知识以了解CAEs内部工作原理和特征提取应用的人，以下论文提供了一个优秀的概述：*Contractive Auto-Encoders:
    Explicit Invariance During Feature Extraction*：[http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf)。'
- en: Practically speaking, all we need to understand here is that by defining the
    omega term as such, CAEs can learn to approximate a function that can map inputs
    to outputs, even if the input changes slightly. Since this penalty is applied
    only during the training process, the network learns to capture representative
    features from the inputs, and is able to perform well during testing, even if
    the inputs it is shown differs slightly from the inputs it was trained on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，我们在这里需要理解的是，通过将ω项定义为这样，CAEs可以学习近似一个函数，该函数可以将输入映射到输出，即使输入略有变化。由于这种惩罚仅在训练过程中应用，网络学会从输入中捕获代表性特征，并且在测试期间能够表现良好，即使所展示的输入与其训练时略有不同。
- en: Now that we have covered the basic learning mechanism as well as some architectural
    variations that define various types of autoencoder networks, we can proceed to
    the implementation part of this chapter. Here, we will be designing a basic autoencoder
    in Keras and progressively updating the architecture to cover some practical considerations
    and use cases.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了基本的学习机制以及定义各种自编码器网络的一些架构变化，我们可以继续进行本章的实现部分。在这里，我们将在Keras中设计一个基本的自编码器，并逐步更新架构，以涵盖一些实际考虑因素和用例。
- en: Implementing a shallow AE in Keras
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中实现浅层AE
- en: 'Now, we will implement a shallow autoencoder in Keras. The use case we will
    tackle with this model will be simple: make an autoencoder generate different
    fashionable items of clothing by using the standard fashion MNIST dataset that''s
    provided by Keras. Since we know that the quality of network output depends directly
    on the quality of the input data available, we must warn our audience to not expect
    to generate the next best-selling clothing item through this exercise. The pixelated
    28 x 28 images that the dataset has will be used to clarify the programmatic concepts
    and implementational steps you must familiarize yourself with when attempting
    to design any type of AE network on Keras.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在Keras中实现一个浅层自编码器。我们将使用标准的时尚MNIST数据集作为这个模型的用例：通过像素化的28 x 28图像来生成不同的时尚服装。由于我们知道网络输出的质量直接取决于可用的输入数据的质量，我们必须警告我们的观众，不要期望通过这种方式生成下一个畅销服装。该数据集提供了程序概念和实现步骤的澄清，您在设计任何类型的Keras
    AE网络时必须熟悉这些步骤。
- en: Making some imports
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入一些库
- en: 'For this exercise, we will be using Keras''s functional API, accessible through
    `keras.models`, which allows us to build acyclic graphs and multioutput models,
    as we did in Chapter 4, *Convolutional Neural Networks*, to dive deep into the
    intermediate layers of convolutional networks. While you may also replicate autoencoders
    using the sequential API (autoencoders are sequential models, after all), they
    are commonly implemented through the functional API, allowing us to gain a little
    more experience of using both of Keras''s APIs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 Keras 的功能性 API，通过 `keras.models` 进行访问，允许我们构建无环图和多输出模型，就像我们在第四章《卷积神经网络》中所做的那样，深入研究卷积网络的中间层。尽管你也可以使用顺序
    API 来复制自动编码器（毕竟自动编码器是顺序模型），但它们通常通过功能性 API 实现，这也让我们有机会更加熟悉 Keras 的两种 API。
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Probing the data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'Next, we simply load the `fashion_mnist` dataset that''s contained in Keras.
    Note that while we have loaded the labels for each image as well, this is not
    necessary for the task we are about to perform. All we need are the input images,
    which our shallow autoencoder will regenerate:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需加载 Keras 中包含的 `fashion_mnist` 数据集。请注意，尽管我们已经加载了每个图像的标签，但对于我们接下来要执行的任务，这并不是必需的。我们只需要输入图像，而我们的浅层自动编码器将重新生成这些图像：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Following is the output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/178b3347-7902-4304-9658-e5186f7c8ea4.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/178b3347-7902-4304-9658-e5186f7c8ea4.png)'
- en: We can proceed by checking the dimensions and types of the input images, and
    then plot out a single example from the training data for our own visual satisfaction.
    The example appears to be a casual T-shirt with some undecipherable content written
    on it. Great – now, we can move on to defining our autoencoder model!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续检查输入图像的维度和类型，然后从训练数据中绘制一个示例图像，满足我们自己的视觉需求。该示例似乎是一件印有一些难以辨认内容的休闲 T 恤。很好——现在，我们可以继续定义我们的自动编码器模型了！
- en: Preprocessing the data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'As we''ve done countless times before, we will now normalize the pixel data
    between the values of 0 and 1, which improves the learning capability of our network
    from the normalized data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前做过无数次的那样，我们现在将像素数据归一化到 0 和 1 之间，这有助于提高我们网络对归一化数据的学习能力：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will also flatten our 28 x 28 pixels into one vector of 784 pixels, just
    as we did in our previous MNIST examples while training a feed-forward network.
    Finally, we will print out the shapes of our training and test arrays to ensure
    that they are in the required format.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将把 28 x 28 像素的图像平铺成一个 784 像素的向量，就像我们在之前训练前馈网络时所做的那样。最后，我们将打印出训练集和测试集的形状，以确保它们的格式符合要求。
- en: Building the model
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: Now we are ready to design our first autoencoder network in Keras, which we
    will do using the functional API. The basics governing the functional API are
    quite simple to get accustomed to, as we saw in previous examples. For our use
    case, we will define the encoding dimension of the latent space. Here, we chose
    32\. This means that each image of 784 pixels will go through a compressed dimension
    that stores only 32 pixels, from which the output will be reconstructed.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好在 Keras 中设计我们的第一个自动编码器网络，我们将使用功能性 API 来实现。功能性 API 的基本原理相当简单，正如我们在之前的示例中所看到的那样。在我们的应用场景中，我们将定义潜在空间的编码维度。在这里，我们选择了
    32。这意味着每个 784 像素的图像将经过一个压缩维度，该维度仅存储 32 个像素，输出将从中重建。
- en: 'This implies a compression factor of 24.5 (784/32), and was chosen somewhat
    arbitrarily, yet can be used as a rule of thumb for similar tasks:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着压缩因子为 24.5（784/32），虽然这个选择有些随意，但可以作为类似任务的经验法则：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we define the input layer using the input placeholder from `keras.layers` and
    specify the flattened image dimension that we expect. As we already know from
    our earlier MNIST experiments (and through some simple math), flattening images
    of 28 x 28 pixels returns an array of 784 pixels, which can then be fed through
    a feed-forward neural network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `keras.layers` 中的输入占位符来定义输入层，并指定我们期望的平铺图像维度。正如我们从之前的 MNIST 实验中知道的那样（通过一些简单的数学计算），将
    28 x 28 像素的图像平铺后得到一个 784 像素的数组，之后可以通过前馈神经网络进行处理。
- en: Next, we define the dimensions of the encoded latent space. This is done by
    defining a dense layer that's connected to the input layer, along with the number
    of neurons corresponding to our encoding dimension (earlier defined as 32), with
    a ReLU activation function. The connection between these layers are denoted by
    including the variable that defines the previous layer in brackets, after defining
    the parameters of the subsequent layer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义编码后的潜在空间的维度。这是通过定义一个与输入层相连的全连接层来完成的，并且该层的神经元数与我们的编码维度（先前定义为32）相对应，使用ReLU激活函数。这些层之间的连接通过在定义后续层的参数后，括号中包含定义前一层的变量来表示。
- en: Finally, we define the decoder function as a dense layer of equal dimension
    as the input (784 pixels), with a sigmoid activation function. This layer naturally
    connects to the encoded dimension representing the latent space, and regenerates
    the output that's drawing upon the neural activations in the encoded layer. Now
    we can initialize our autoencoder by using the model class from the functional
    API, and providing it with the input placeholder and the decoder layer as arguments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义解码器函数作为一个与输入层维度相同的全连接层（784个像素），使用sigmoid激活函数。该层自然地与表示潜在空间的编码维度相连接，并重新生成依赖于编码层神经激活的输出。现在我们可以通过使用功能性API中的模型类来初始化自编码器，并将输入占位符和解码器层作为参数提供给它。
- en: Implementing a sparsity constraint
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现稀疏性约束
- en: As we mentioned earlier in this chapter, there are many ways to perform regularization
    when designing autoencoders. The sparse autoencoder, for example, simply implements
    a sparsity constraint on the latent space to force the autoencoder to favor rich
    representations. Recall that neurons in a neural network may *fire* if their output
    value is close to 1, and refrain from being active if their output is close to
    0\. Adding a sparsity constraint can simply be thought of as constraining the
    neurons in the latent space to be inactive most of the time. As a result, a smaller
    number of neurons may fire at any given time, forcing those that *do* fire to
    propagate information as efficiently as possible, from the latent space to the
    output space. Thankfully, implementing this procedure in Keras is fairly straightforward.
    This can be achieved by defining the `activity_regularizer` argument, while defining
    the dense layer that represents the latent space. In the following code, we use
    the L1 regularizer from `keras.regularizers` with a sparsity parameter very close
    to zero (0.067, in our case). Now you know how to design a sparse autoencoder
    in Keras as well! While we will continue with the unsparse version, for the purpose
    of this exercise, you are welcome to compare the performance between these two
    shallow autoencoders to see the benefits of adding sparsity constraints to the
    latent space when designing such models first-hand.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面提到的，在设计自编码器时有许多方法可以进行正则化。例如，稀疏自编码器通过对潜在空间实施稀疏性约束，强制自编码器偏向丰富的表示。回想一下，当神经网络中的神经元的输出值接近1时，它们可能会*激活*，而当输出接近0时，它们则不会激活。添加稀疏性约束可以简单地理解为限制潜在空间中的神经元大部分时间保持不活跃。因此，任何给定时刻可能只有少数神经元被激活，迫使这些被激活的神经元尽可能高效地传播信息，从潜在空间到输出空间。幸运的是，在Keras中实现这一过程非常简单。这可以通过定义`activity_regularizer`参数来实现，同时在定义代表潜在空间的全连接层时进行设置。在下面的代码中，我们使用`keras.regularizers`中的L1正则化器，稀疏参数非常接近零（在我们这里是0.067）。现在你也知道如何在Keras中设计稀疏自编码器了！虽然我们将继续使用非稀疏版本，但为了这个练习的目的，你可以比较这两种浅层自编码器的性能，亲自感受在设计此类模型时向潜在空间添加稀疏性约束的好处。
- en: Compiling and visualizing the model
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和可视化模型
- en: 'We can visualize what we just did by simply compiling the model and calling
    `summary()` on the model object, like so. We will choose the Adadelta optimizer,
    which restricts the number of accumulated past gradients to a fixed window during
    backpropagation, instead of monotonically decreasing the learning rate by choosing
    something such as an Adagrad optimizer. In case you missed it earlier in this
    book, we encourage you to investigate the vast repertoire of available optimizers
    ([http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/))
    and experiment with them to find a suitable one for your use case. Finally, we
    will define a binary cross entropy as a `loss` function, which in our case accounts
    for pixel-wise loss on the outputs that are generated:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地编译模型并调用模型对象上的`summary()`来可视化我们刚刚做的事情，像这样。我们将选择Adadelta优化器，它在反向传播过程中限制了过去梯度的累计数目，仅在一个固定的窗口内进行，而不是通过选择像Adagrad优化器那样单调减少学习率。如果你错过了本书早些时候提到的内容，我们鼓励你去研究可用优化器的广泛库（[http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/)），并进行实验以找到适合你用例的优化器。最后，我们将定义二元交叉熵作为`loss`函数，它在我们的情况下考虑了输出生成的像素级损失：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Following is the output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/4ca733ce-bd7d-4da1-8ee7-6239a723f6d2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ca733ce-bd7d-4da1-8ee7-6239a723f6d2.png)'
- en: Building the verification model
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建验证模型
- en: Now we have almost all we need to initiate the training session of our shallow
    autoencoder. However, we are missing one crucial component. While this part is
    not, strictly speaking, required to train our autoencoder, we must implement it
    so that we can visually verify whether our autoencoder has truly learned salient
    features from the training data or not. To do this, we will actually define two
    additional networks. Don't worry – these two networks are essentially mirror images
    of the encoder and decoder functions that are present in the autoencoder network
    we just defined. Hence, all we will be doing is creating a separate encoder and
    decoder network, which will match the hyperparameters of the encoder and decoder
    functions from our autoencoder. These two separate networks will be used for prediction
    only after our autoencoder has been trained. Essentially, the encoder network
    will be used to predict the compressed representation of the input image, while
    the decoder network will simply proceed to predict the decoded version of the
    information that's stored in the latent space.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎拥有了启动浅层自编码器训练会话所需的所有内容。然而，我们缺少一个至关重要的组成部分。严格来说，这部分并不是训练自编码器所必需的，但我们必须实现它，以便能够直观地验证我们的自编码器是否真的从训练数据中学习到重要特征。为了做到这一点，我们实际上将定义两个额外的网络。别担心——这两个网络本质上是我们刚刚定义的自编码器网络中编码器和解码器功能的镜像。因此，我们所做的就是创建一个独立的编码器和解码器网络，它们将匹配自编码器中编码器和解码器功能的超参数。这两个独立的网络将在自编码器训练完成后用于预测。基本上，编码器网络将用于预测输入图像的压缩表示，而解码器网络则会继续预测存储在潜在空间中的信息的解码版本。
- en: Defining a separate encoder network
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个独立的编码器网络
- en: 'In the following code, we can see that the encoder function is an exact replica
    of the top half of our autoencoder; it essentially maps input vectors of flattened
    pixel values to a compressed latent space:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们可以看到编码器功能是我们自编码器上半部分的精确副本；它本质上将展平的像素值输入向量映射到一个压缩的潜在空间：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Following is the summary:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是总结：
- en: '![](img/b7ceb46e-178e-4af2-8cbc-71ce29fa66de.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7ceb46e-178e-4af2-8cbc-71ce29fa66de.png)'
- en: Defining a separate decoder network
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个独立的解码器网络
- en: 'Similarly, in the following code, we can see that the decoder network is a
    perfect replica of the bottom half of our autoencoder neural network, mapping
    the compressed representations stored in the latent space to the output layer
    that reconstructs the input image:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在下面的代码中，我们可以看到解码器网络是我们自编码器神经网络下半部分的完美副本，它将存储在潜在空间中的压缩表示映射到重建输入图像的输出层：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the summary:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是总结：
- en: '![](img/478b5a2d-43f4-4be4-ad0d-1d6f8c5b9feb.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/478b5a2d-43f4-4be4-ad0d-1d6f8c5b9feb.png)'
- en: Note that to define the decoder network, we must first construct an input layer
    with a shape that corresponds to our encoding dimension (that is, 32). Then, we
    simply duplicate the decoder layer from our earlier autoencoder model by referring
    to the index corresponding to the last layer of that model. Now we have all the
    components in place to initiate the training of our autoencoder network!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要定义解码器网络，我们必须首先构建一个形状与我们的编码维度（即 32）相匹配的输入层。然后，我们只需通过引用之前自动编码器模型最后一层的索引来复制解码器层。现在，我们已经准备好启动自动编码器网络的训练了！
- en: Training the autoencoder
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练自动编码器
- en: 'Next, we simply fit our autoencoder network, just as we''ve done with other
    networks countless times before. We chose this model to be trained for 50 epochs,
    in batches of 256 images, before weight updates to our network nodes are performed.
    We also shuffle our data during training. As we already know, doing so ensures
    some variance reduction among batches, thereby improving the generalizability
    for our model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们像之前做过无数次的那样，简单地训练我们的自动编码器网络。我们选择将该模型训练 50 个 epoch，每次批次处理 256 张图像，然后再进行网络节点的权重更新。我们在训练过程中还会打乱数据。正如我们所知道的那样，这样做能确保批次之间减少一些方差，从而提高模型的泛化能力：
- en: '![](img/943b863c-df58-4b94-b5e3-7895aef23c5a.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/943b863c-df58-4b94-b5e3-7895aef23c5a.png)'
- en: Finally, we also defined the validation data using our test set, just to be
    able to compare how well our model does on unseen examples, at the end of each
    epoch. Do remember that in normal machine learning workflows, it is common practice
    to have both validation and development splits of your data so that you can tune
    your model on one split and test it on the latter. While this is not a prerequisite
    for our demonstrative use case, such double-holdout strategies can always be beneficial
    to implement for the sake of achieving generalizable results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还使用我们的测试集定义了验证数据，只是为了能够在每个 epoch 结束时比较模型在未见示例上的表现。请记住，在正常的机器学习工作流程中，常见的做法是将数据分为验证集和开发集，这样你可以在一个数据集上调整模型，并在另一个数据集上进行测试。虽然这不是我们演示用例的前提，但为了获得可泛化的结果，实施这种双重验证策略总是有益的。
- en: Visualizing the results
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化结果
- en: 'Now comes the time to bear the fruits of our labor. Let''s have a look at what
    kind of images our autoencoder is able to recreate by using our secluded test
    set. In other words, we will provide our network with images that are similar
    (but not the same) to the training sets, to see how well our model performs on
    unseen data. To do this, we will employ our encoder network to make predictions
    on the test set. The encoder will predict how to map the input image to a compressed
    representation. Then, we will simply use the decoder network to predict how to
    decode the compressed representation that''s generated by the encoder network.
    These steps are shown in the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了收获成果的时候了。让我们看看我们的自动编码器能够通过使用我们隔离的测试集重建出什么样的图像。换句话说，我们将为网络提供与训练集相似（但不完全相同）的图像，看看模型在未见数据上的表现如何。为此，我们将使用编码器网络对测试集进行预测。编码器将预测如何将输入图像映射到压缩表示。然后，我们将简单地使用解码器网络预测如何解码由编码器网络生成的压缩表示。以下代码展示了这些步骤：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we reconstruct a few images and compare them to the input that prompted
    the reconstruction to see whether our autoencoder captures the essence of what
    items of clothing are supposed to look like. To do this, we will simply use Matplotlib
    and plot nine images with their reconstructions under them, as shown here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重构一些图像，并将它们与触发重构的输入进行比较，看看我们的自动编码器是否捕捉到了衣物应有的外观。为此，我们将简单地使用 Matplotlib
    绘制九个图像，并将它们的重构图像展示在下方，如下所示：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Following is the output generated:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成的输出：
- en: '![](img/680df065-9ab9-4127-8b1b-08312a5b008e.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/680df065-9ab9-4127-8b1b-08312a5b008e.png)'
- en: As you can see, while our shallow autoencoder doesn't recreate brand labels
    (such as the **Lee** tag that's present in the second image), it certainly does
    get the general idea of human items of clothing, despite having a considerable
    meager learning capacity. But is this enough? Well, not enough for any practical
    use case, such as computer-aided clothing design. Far too many details are missing,
    partially due to the learning capacity of our network and partially due to the
    lossy compression output. Naturally, this makes you wonder, what can be achieved
    by deeper models? Well, as the old adage goes, *nullius in verba* (or to paraphrase
    in more contemporary terms, let's see for ourselves!).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，虽然我们的浅层自动编码器无法重建品牌标签（例如第二张图片中的**Lee**标签），但它确实能捕捉到人类服装的基本概念，尽管它的学习能力相当有限。但这就足够了吗？对于任何实际应用场景来说，答案是“不够”，比如计算机辅助服装设计。缺少的细节太多，部分是由于我们网络的学习能力有限，部分是因为压缩输出存在损失。自然而然地，这让人想知道，深度模型能做到什么呢？正如那句老话所说，*nullius
    in verba*（用更现代的语言来说，就是让我们自己看看！）。
- en: Designing a deep autoencoder
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计一个深度自动编码器
- en: Next, we will investigate how much better reconstructions from autoencoders
    can get, and whether they can generate images a bit better than the blurry representations
    that we just saw. For this, we will design a deep feed-forward autoencoder. As
    you know, this simply means that we will be adding additional hidden layers between
    the input and the output layer of our autoencoder. To keep things interesting,
    we will also use a different dataset of images. You are welcome to reimplement
    this method on the `fashion_mnist` dataset if you're curious to further explore
    the sense of fashion that's attainable by autoencoders.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究自动编码器的重建效果可以有多好，看看它们是否能生成比我们刚刚看到的模糊表示更好的图片。为此，我们将设计一个深度前馈自动编码器。如你所知，这意味着我们将在自动编码器的输入层和输出层之间添加额外的隐藏层。为了保持趣味性，我们还将使用一个不同的数据集。你可以根据自己的兴趣，在
    `fashion_mnist` 数据集上重新实现这个方法，进一步探索自动编码器能达成的时尚感。
- en: For the next exercise, we will use the 10 Monkey species dataset, available
    at Kaggle. We will try to reconstruct pictures of our playful and mischievous
    cousins from the jungle, and see how well our autoencoder performs at a more complex
    reconstruction task. This also gives us the opportunity to venture out to use
    cases far from the comforts of preprocessed datasets that are available in Keras,
    as we will learn to deal with images of different sizes and higher resolution
    compared to the monotonous MNIST examples: [https://www.kaggle.com/slothkong/10-monkey-species](https://www.kaggle.com/slothkong/10-monkey-species).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个练习，我们将使用位于 Kaggle 的 10 种猴子物种数据集。我们将尝试重建我们这些顽皮、捣蛋的“远房亲戚”——来自丛林的猴子们的图片，并看看我们的自动编码器在一个更复杂的重建任务中表现如何。这也给了我们一个机会，远离
    Keras 中现成预处理数据集的舒适区，因为我们将学会处理不同大小和更高分辨率的图片，而不是单调的 MNIST 示例：[https://www.kaggle.com/slothkong/10-monkey-species](https://www.kaggle.com/slothkong/10-monkey-species)。
- en: Making some imports
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入必要的库
- en: 'We will start by importing the necessary libraries, as is tradition. You will
    notice the usual suspects such as NumPy, pandas, Matplotlib, and some Keras model
    and layer objects:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入必要的库，这已经是传统了。你会注意到一些常见的库，比如 NumPy、pandas、Matplotlib，以及一些 Keras 的模型和层对象：
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that we import a utility module from the Keras `vis` library. While this
    module contains many other nifty features for image manipulation, we will use
    it to resize our training images to a uniform dimension, since this is not the
    case for this particular dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们从 Keras 的 `vis` 库中导入了一个实用模块。虽然该模块包含许多其他方便的图片处理功能，但我们将使用它来将我们的训练图片调整为统一的尺寸，因为该数据集中的图片并不都是统一的。
- en: Understanding the data
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据
- en: 'We chose this dataset for our use case for a specific reason. Unlike 28 x 28
    pixelated images of clothing items, these images represent rich and complex features,
    such as variation in body morphology, and, of course, color! We can plot out the
    composition of our dataset to see what the class distributions look like, purely
    for our own curiosity:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这个数据集用于我们的应用案例有一个特定的原因。与 28 x 28 像素的衣物图片不同，这些图片呈现了丰富和复杂的特征，比如体型的变化，以及当然，还有颜色！我们可以绘制出数据集的组成，看看类别分布的情况，这完全是为了满足我们的好奇心：
- en: '![](img/acf036fe-7edd-4de7-bf8f-31136a0a7df4.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acf036fe-7edd-4de7-bf8f-31136a0a7df4.png)'
- en: 'You will notice that each of the 10 different monkey species has significantly
    different characteristics, ranging from different body sizes, color of fur, and
    facial composition, making this a much more challenging task for an autoencoder.
    The following depiction with sample images from eight different monkey species
    is provided to better illustrate these variations among species. As you can see,
    each of them looks unique:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，这10种不同的猴子物种各自有显著不同的特征，包括不同的体型、毛发颜色和面部构成，这使得对于自编码器来说这是一个更加具有挑战性的任务。以下是来自八种不同猴子物种的示例图像，以便更好地展示这些物种之间的差异。如你所见，它们每一种看起来都独一无二：
- en: '![](img/3fc1fefb-421c-44a3-87d0-4abc240896b2.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fc1fefb-421c-44a3-87d0-4abc240896b2.png)'
- en: Since we know that autoencoders are data-specific, it also stands to reason
    that training an autoencoder to reconstruct a class of images with high variance
    may result in dubious results. Nevertheless, we hope that this will make an informative
    use case so that you can better understand the potentials and limits you will
    face when using these models. So, let's get started!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道自编码器是数据特定的，因此训练自编码器重构一个具有高方差的图像类别可能会导致可疑的结果，这也是合乎逻辑的。然而，我们希望这能为你提供一个有用的案例，以便你更好地理解使用这些模型时会遇到的潜力和限制。那么，让我们开始吧！
- en: Importing the data
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'We will start by importing the images of different monkey species from the
    Kaggle repository. As we did before, we will simply download the data to our filesystem,
    then access the training data folder using the operating system interface that''s
    built into Python (using the `os` module):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Kaggle仓库开始导入不同猴子物种的图像。像之前一样，我们将数据下载到文件系统中，然后使用Python内置的操作系统接口（即`os`模块）访问训练数据文件夹：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You will notice that we nest the image variable in a `try`/`except` loop. This
    is simply an implementational consideration, as we found that some of the images
    in our dataset were corrupt. Hence, if we aren't able to load an image using the
    `load_img()` function from the `utils` module, then we will ignore the image file
    altogether. This (somewhat arbitrary) selection strategy leaves us with 1,094
    images being recovered from the training folder out of a total of 1,097.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们将图像变量嵌套在一个`try`/`except`循环中。这只是一个实现上的考虑，因为我们发现数据集中的一些图像已经损坏。因此，如果我们无法通过`utils`模块中的`load_img()`函数加载图像，我们将完全忽略该图像文件。这种（有些任意的）选择策略使得我们从训练文件夹中恢复了1,094张图像，总共有1,097张。
- en: Preprocessing the data
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Next, we will convert our list of pixel values into NumPy arrays. We can print
    out the shape of the array to confirm whether we indeed have 1,094 colored images
    of 64 x 64 pixels. After doing so, we simply normalize the pixel values between
    the range of 0 – 1 by dividing each pixel value by the maximum possible value
    for any given pixel (that is, 255):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把像素值列表转换成NumPy数组。我们可以打印出数组的形状来确认我们确实有1,094张64 x 64像素的彩色图像。确认之后，我们只需通过将每个像素值除以可能的最大像素值（即255）来将像素值归一化到0
    – 1之间：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, we flatten the four-dimensional array into a two-dimensional array,
    since our deep autoencoder is composed of a feed-forward neural networks that
    propagates 2D vectors through its layers. Similar in spirit to what we did in
    [Chapter 3](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml), *Signal Processing –
    Data Analysis with Neural Networks*, we essentially convert each three-dimensional
    image (64 x 64 x 3) into a 2D vector of dimensions (1, 12,288).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将四维数组展平成二维数组，因为我们的深度自编码器由前馈神经网络组成，神经网络通过其层传播二维向量。这类似于我们在[第3章](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml)中所做的，*信号处理
    – 使用神经网络进行数据分析*，我们实际上是将每张三维图像（64 x 64 x 3）转换成一个二维向量，维度为（1，12,288）。
- en: Partitioning the data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分区
- en: 'Now that our data has been preprocessed and exists as a 2D tensor of normalized
    pixel values, we can finally split it into training and test segments. Doing so
    is important, as we wish to eventually use our model on images that it has never
    seen, and be able to recreate them using its own understanding of what a monkey
    should look like. Do note that while we don''t use the labels that are provided
    with the dataset for our use case, the network itself will receive a label for
    each image it sees. The label in this case will simply be the image itself, as
    we are dealing with an image reconstruction task, and not classification. So,
    in the case of autoencoders, the input variables are the same as the target variables.
    As we can see in the following screenshot, the `train_test_split` function from
    sklearn''s model selection module is used to generate our training and testing
    data (with an 80/20 split ratio). You will notice that both the `x` and `y` variables
    are defined by the same data structure due to the nature of our task:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经经过预处理，并作为一个标准化像素值的 2D 张量存在，我们最终可以将其划分为训练集和测试集。这样做非常重要，因为我们希望最终在网络从未见过的图像上使用我们的模型，并能够利用它对猴子应该是什么样子的理解重建这些图像。请注意，虽然我们在这个用例中并不使用数据集中提供的标签，但网络本身会接收到每个图像的标签。在这种情况下，标签将仅仅是图像本身，因为我们处理的是图像重建任务，而非分类。因此，在自编码器的情况下，输入变量与目标变量是相同的。正如以下截图所示，我们使用
    sklearn 的模型选择模块中的 `train_test_split` 函数来生成训练和测试数据（80/20 的划分比例）。你会注意到，由于我们任务的性质，`x`
    和 `y` 变量都由相同的数据结构定义：
- en: '![](img/10711ab1-5e6a-4187-81d1-2705f44a7e2e.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10711ab1-5e6a-4187-81d1-2705f44a7e2e.png)'
- en: Now we are left with **875** training examples and 219 test examples to train
    and test our deep autoencoder. Do note that the Kaggle dataset comes with an explicit
    test set directory since the original purpose of this dataset was to attempt to
    classify different monkey species using machine learning models. In our use case,
    however, we do not rigidly ensure balanced classes for the time being, and are
    simply interested in how deep autoencoders perform at reconstructing images when
    trained on a high variance dataset. We do encourage further experimentation by
    comparing the performance of deep autoencoders that are trained on a particular
    species of monkey. Logic would dictate that these models would perform better
    at reconstructing their input images due to the latter between training observations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们剩下 **875** 个训练样本和 219 个测试样本，用于训练和测试我们的深度自编码器。请注意，Kaggle 数据集自带一个明确的测试集目录，因为该数据集的最初目的是尝试使用机器学习模型对不同猴子物种进行分类。然而，在我们的用例中，我们暂时并没有严格保证类的平衡，只是对深度自编码器在高方差数据集上进行训练时重建图像的表现感兴趣。我们确实鼓励进一步的实验，比较在特定猴子物种上训练的深度自编码器的表现。逻辑上，这些模型会在重建输入图像时表现更好，因为它们在训练观察中的相似性较高。
- en: Using functional API to design autoencoders
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用功能性 API 设计自编码器
- en: 'Just as we did in the previous example, we will refer to the functional API
    to construct our deep autoencoder. We will import the input and dense layers,
    as well as the model object that we will later use to initialize the network.
    We will also define the input dimension for our images (64 x 64 x 3 = 12,288),
    and an encoding dimension of 256, leaving us with a compression ratio of 48\.
    This simply means that each image will be compressed by a factor of 48, before
    our network attempts to reconstruct it from the latent space:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在前一个示例中做的那样，我们将使用功能性 API 来构建我们的深度自编码器。我们将导入输入层和全连接层，以及我们稍后将用于初始化网络的模型对象。我们还将定义图像的输入维度（64
    x 64 x 3 = 12,288），以及编码维度为 256，这样我们的压缩比为 48。简单来说，这意味着每张图像会被压缩 48 倍，然后网络将尝试从潜在空间中重建它：
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The compression factor can be a very important parameter to consider, as mapping
    the input to a very low dimensional space will result in too much information
    loss, leading to poor reconstructions. There may simply not be enough space to
    store the key essentials of the image. On the other hand, we are already aware
    of how providing our model with too much learning capacity may cause it to overfit,
    which is why choosing a compression factor by hand can be quite tricky. When in
    doubt, it can never hurt to experiment with different compression factors as well
    as regularization methods (provided you have the time).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩因子是一个非常重要的参数，值得考虑，因为将输入映射到非常低的维度空间会导致过多的信息丢失，从而导致重建效果较差。可能根本没有足够的空间存储图像的关键要素。另一方面，我们已经知道，向模型提供过多的学习能力可能会导致过拟合，这也是手动选择压缩因子可能相当棘手的原因。当有疑问时，尝试不同的压缩因子和正则化方法（只要你有时间）总是值得一试。
- en: Building the model
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'To build our deep autoencoder, we will begin by defining the input layer, which
    accepts the dimensions corresponding to our 2D vectors of monkey images. Then,
    we simply start defining the encoder part of our network using dense layers, with
    a decreasing number of neurons for subsequent layers, until we reach the latent
    space. Note that we simply choose the number of neurons in layers leading to the
    latent space to decrease by a factor of 2, with respect to the encoding dimension
    chosen. Thus, the first layer has (256 x 4) 1024 neurons, the second layer has
    (256 x 2) 512 neurons, and the third layer, representing the latent space itself,
    has 256 neurons. While you are not obliged to strictly stick to this convention,
    it is common practice to reduce the number of neurons per layer when approaching
    the latent space, and increase the number of neurons for layers occurring after,
    in the case of undercomplete autoencoders:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的深度自编码器，我们将从定义输入层开始，该层接受与猴子图像的二维向量相对应的维度。接着，我们简单地开始定义网络的编码器部分，使用密集层，每一层的神经元数量逐层减少，直到达到潜在空间。请注意，我们简单地选择了每一层神经元数量相对于所选编码维度减少的比例为2。因此，第一层有(256
    x 4) 1024个神经元，第二层有(256 x 2) 512个神经元，第三层，即潜在空间层，有256个神经元。虽然你不必严格遵守这一约定，但通常在接近潜在空间时减少每一层的神经元数量，而在潜在空间之后的层中增加神经元数量，这是在使用欠完备自编码器时的常见做法：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, we initialize the autoencoder by providing the input and decoder layer
    to the model object as arguments. Then, we can visually summarize what we just
    built.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将输入层和解码器层作为参数传递给模型对象来初始化自编码器。然后，我们可以直观地总结我们刚刚构建的模型。
- en: Training the model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Finally, we can initiate the training session! This time, we will compile the
    model with an `adam` optimizer and operationalize the `loss` function with mean-squared
    errors. Then, we simply start the training by calling `.fit()` on the model object
    and providing the appropriate arguments:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以开始训练会话了！这次，我们将使用`adam`优化器来编译模型，并用均方误差来操作`loss`函数。然后，我们只需通过调用`.fit()`方法并提供适当的参数来开始训练：
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This model ends with a loss of (0.0046) at the end of the 100th epoch. Do note
    that since different `loss` functions have been chosen previously for the shallow
    model, the loss metrics of each model are not directly comparable to one another.
    In reality, the manner in which the `loss` function is defined characterizes what
    the model seeks to minimize. If you wish to benchmark and compare the performance
    of two different neural network architectures (such as a feed-forward network
    and a CNN, for example) it is always advised to use the same optimizer and `loss`
    function at first, before venturing to other ones.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在第100个周期结束时的损失为(0.0046)。请注意，由于之前为浅层模型选择了不同的`loss`函数，因此每个模型的损失指标不能直接进行比较。实际上，`loss`函数的定义方式决定了模型试图最小化的目标。如果你希望基准测试并比较两种不同神经网络架构的性能（例如前馈网络和卷积神经网络），建议首先使用相同的优化器和`loss`函数，然后再尝试其他的选择。
- en: Visualizing the results
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化结果
- en: 'Now, let''s have a look at the reconstructions our deep autoencoder is capable
    of making by testing its performance on our secluded test set. To do that, we
    will simply use our separate encoder network to make a prediction on how to compress
    those images to the latent space from where the decoder network will take up the
    call of decoding and reconstructing the original image from the latent space that''s
    predicted by the encoder:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过在孤立的测试集上测试深度自编码器的表现，来看看它能够进行的重建。为此，我们将简单地使用单独的编码器网络来预测如何将这些图像压缩到潜在空间，然后解码器网络将从编码器预测的潜在空间中接手，进行解码并重建原始图像：
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will get the following ouput:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![](img/1d68f54c-29f6-4a03-9aa8-f38ef58fc0dc.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d68f54c-29f6-4a03-9aa8-f38ef58fc0dc.png)'
- en: While the images themselves are arguably even aesthetically pleasing, it seems
    that the essence of what represents a monkey largely eludes our model. Most of
    the reconstructions resemble starry skies, rather that the features of a monkey.
    We do notice that the network had started learning the general humanoid morphology
    at a very basic level, but this is nothing to write home about. So, how can we
    improve this? At the end of the day, we would like to close this chapter with
    at least a few realistic looking reconstructions of monkeys. To do so, we will
    employ the use of a specific type of network, which is adept at dealing with image
    data. We are speaking of the **Convolutional Neural Network** (**CNN**) architecture,
    which we will repurpose so that we can design a deep convolutional autoencoder
    in the next part of this exercise.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些图像本身可以说在美学上令人愉悦，但似乎代表猴子的本质特征大部分依然没有被我们的模型捕捉到。大多数重建图像看起来像星空，而不是猴子的特征。我们确实注意到，网络已经开始在一个非常基础的层面上学习到一般的人形形态，但这并不足为奇。那么，如何改进呢？归根结底，我们希望至少能够用一些看起来逼真的猴子重建图像来结束这一章。为此，我们将使用一种特定类型的网络，它擅长处理图像数据。我们所说的就是**卷积神经网络**（**CNN**）架构，我们将重新设计它，以便在本练习的下一部分中构建一个深度卷积自编码器。
- en: Deep convolutional autoencoder
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积自编码器
- en: 'Luckily, all we have to do is define a convolutional network and reshape our
    training arrays to the appropriate dimensions to test out how it performs with
    respect to the task at hand. Thus, we will import some convolutional, MaxPooling,
    and UpSampling layers, and start building the network. We define the input layer
    and provide it with the shape of our 64 x 64 colored images. Then, we simply alternate
    the convolutional and pooling layers until we reach the latent space, which is
    represented by the second `MaxPooling2D` layer. The layers leading away from the
    latent space, on the other hand, must be alternating between convolutional layers
    and UpSampling layers. The UpSampling layer, as the name suggests, simply increases
    the representational dimension by repeating the rows and columns of the data from
    the previous layer:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们所要做的就是定义一个卷积网络，并将我们的训练数组调整为适当的维度，以测试它在当前任务中的表现。因此，我们将导入一些卷积、MaxPooling
    和 UpSampling 层，开始构建网络。我们定义输入层，并为其提供 64 x 64 彩色图像的形状。然后，我们简单地交替使用卷积层和池化层，直到达到潜在空间，该空间由第二个
    `MaxPooling2D` 层表示。另一方面，从潜在空间出去的层必须交替使用卷积层和 UpSampling 层。UpSampling 层顾名思义，通过重复前一层的数据的行和列，简单地增加表示维度：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As we can see, this convolutional autoencoder has eight layers. The information
    enters the input layer, from which convolutional layers generate 32 feature maps.
    These maps are downsampled using the max pooling layer, in turn generating 32
    feature maps, each being 32 x 32 pixels in size. These maps are then passed on
    to the latent layer, which stores 16 different representations of the input image,
    each with dimensions of 32 x 32 pixels. These representations are passed to the
    subsequent layers, as the inputs are exposed to convolution and UpSampling operations,
    until the decoded layer is reached. Just like the input layer, our decoded layer
    matches the dimensions of our 64 x 64 colored images. You may always check the
    dimension of a specific convolutional layer (instead of visualizing the entire
    model) by using the `int_shape()` function from Keras''s backend module, as shown
    here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，这个卷积自编码器有八层。信息首先进入输入层，然后卷积层生成32个特征图。这些特征图通过最大池化层进行下采样，生成32个特征图，每个特征图的大小为32
    x 32像素。接着，这些特征图传递到潜在层，该层存储输入图像的16种不同表示，每种表示的尺寸为32 x 32像素。这些表示随后传递到后续层，输入在卷积和上采样操作下不断处理，直到到达解码层。就像输入层一样，我们的解码层与64
    x 64彩色图像的尺寸匹配。你可以通过使用Keras后台模块中的`int_shape()`函数来检查特定卷积层的尺寸（而不是可视化整个模型），如下所示：
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Compiling and training the model
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和训练模型
- en: 'Next, we simply compile our network with the same optimizer and `loss` function
    that we chose for the deep feed-forward network and initiate the training session
    by calling `.fit()` on the model object. Do note that we only train this model
    for 50 epochs and perform weight updates in batches of 128 images at a time. This
    approach turns out to be computationally faster, allowing us to train the model
    for a fraction of the time that was taken to train the feed-forward model. Let''s
    see whether the chosen trade-off between training time and accuracy works out
    in our favor for this specific use case:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简单地使用相同的优化器和`loss`函数来编译我们的网络，这些都是我们为深度前馈网络选择的，并通过调用模型对象的`.fit()`方法启动训练会话。需要注意的是，我们只训练这个模型50个周期，并且在每次批量更新时处理128张图像。证明这种方法在计算上更高效，使得我们能在比训练前馈模型所需的时间短得多的时间内完成训练。让我们看看在这个特定用例中，训练时间和准确性之间的折衷是否对我们有利：
- en: '[PRE18]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The model reaches a loss of (0.0044), by the end of the 50^(th) epoch. This
    turns out to be lower than the earlier feed-forward model, when it was trained
    for half the epochs using a much larger batch size. Let's visually judge for ourselves
    how the model performs at reconstructing images it has never seen before.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到第50^(次)训练周期结束时，模型的损失达到了（0.0044）。这比早期的前馈模型要低，后者在训练时使用了更大的批量大小，并且训练了较少的训练周期。接下来，让我们通过视觉判断模型在重建从未见过的图像时的表现。
- en: Testing and visualizing the results
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和可视化结果
- en: 'It''s time to see whether the CNN really does hold up to our image reconstruction
    task at hand. We simply define a helper function that allows us to plot out a
    number of sampled examples that are generated from the test set and compare them
    to the original test inputs. Then, in the code cell that follows, we define a
    variable to hold the results of our model''s inferences on the test set by using
    the `.predict()` method on our model object. This will generate a NumPy ndarray
    containing all of the decoded images for the inputs from the test set. Finally,
    we call the `compare_outputs()` function, using the test set and the decoded predictions
    thereof as arguments to visualize the results:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看卷积神经网络（CNN）是否真的能够胜任我们当前的图像重建任务了。我们简单地定义了一个辅助函数，允许我们绘制出从测试集中生成的若干样本，并将它们与原始测试输入进行比较。然后，在接下来的代码单元中，我们定义了一个变量，用来存储我们模型对测试集进行推理后的结果，方法是使用模型对象的`.predict()`方法。这将生成一个NumPy
    ndarray，包含所有解码后的测试集输入图像。最后，我们调用`compare_outputs()`函数，使用测试集和对应的解码预测作为参数来可视化结果：
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Following is the output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/607d47f4-a2b2-4218-ba43-059b0e334d9f.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/607d47f4-a2b2-4218-ba43-059b0e334d9f.png)'
- en: As we can see, the deep convolutional autoencoder actually does a remarkable
    job of reconstructing the images from the test set. Not only does it learn body
    morphology and correct color schemes, it even recreates aspects such as red-eye
    from a camera flash (as seen on monkey 4 and its artificial doppelganger). Great!
    So, we were able to reconstruct some ape images. As the excitement soon fades
    off (if it was even present in the first place), we will want to use autoencoders
    for more useful and real-world tasks – perhaps tasks such as image denoising,
    where we commission a network to regenerate an image in its entirety from a corrupted
    input.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，深度卷积自编码器在重建测试集中的图像方面表现非常出色。它不仅学会了身体形态和正确的色彩方案，甚至能够重建一些细节，比如相机闪光灯下的红眼现象（如猴子4及其人工复制品所示）。太棒了！所以，我们成功地重建了一些猿类图像。随着兴奋感的渐渐消退（如果一开始就有的话），我们将希望将自编码器应用于更多有用的现实任务——比如图像去噪任务，在这类任务中，我们委托网络从损坏的输入中完全重建图像。
- en: Denoising autoencoders
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: 'Again, we will continue with the monkey species dataset and modify the training
    images to introduce a noise factor. This noise factor essentially changes the
    pixel values on the original image to remove pieces of information that constitute
    the original image, making the task a little more challenging than a simple recreation
    of the original input. Do note that this means that our input variables will be
    noisy images, and the target variable that''s shown to the network during training
    will be the uncorrupted version of the noisy input image. To generate the noisy
    version of the training and test images, all we do is apply a Gaussian noise matrix
    to the image pixels and then clip their values between 0 and 1:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将继续使用猴子物种数据集，并修改训练图像以引入噪声因子。这个噪声因子本质上是通过改变原始图像的像素值，去除构成原始图像的一些信息，从而使任务变得比简单重建原始输入更具挑战性。需要注意的是，这意味着我们的输入变量将是噪声图像，而在训练期间，网络看到的目标变量将是未损坏的噪声输入图像版本。为了生成训练和测试图像的噪声版本，我们只需对图像像素应用一个高斯噪声矩阵，然后将其值截断在0到1之间：
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see how our arbitrarily chosen noise factor of `0.35` actually affects
    the images by plotting a random example from our data, as shown in the following
    code. The noisy image is barely understandable to the human eye at this resolution,
    and looks just a little more than a bunch of random pixels congregated together:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制来自数据中的一个随机示例，看到我们任意选择的噪声因子`0.35`如何实际影响图像，如下面的代码所示。在这个分辨率下，噪声图像几乎无法被人眼理解，看起来仅仅是一些随机像素聚集在一起：
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This is the output that you will get:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您将得到的输出：
- en: '![](img/e9f19dd7-c11d-456f-9f5e-a8ba356ffdb6.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9f19dd7-c11d-456f-9f5e-a8ba356ffdb6.png)'
- en: Training the denoising network
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练去噪网络
- en: 'We will use the same convolutional autoencoder architecture for this task.
    However, we will reinitialize the model and train it from scratch once again,
    this time with the noisy input variables:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的卷积自编码器架构来处理这个任务。然而，我们将重新初始化模型并从头开始训练，这次使用的是带有噪声输入变量的数据：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we can see, the loss converges much more reluctantly in the case of the denoising
    autoencoder than for our previous experiments. This is naturally the case, as
    a lot of information has now been removed from the inputs, making it harder for
    the network to learn an appropriate latent space to generate the uncorrupted outputs.
    Hence, the network is forced to get a little bit *creative* during the compression
    and reconstruction operations. The training session for this network ends after
    50 epochs, with a loss of 0.0126\. Now we can make some predictions on the test
    set and visualize some reconstructions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在去噪自编码器的情况下，损失收敛速度比之前的实验更为缓慢。这是自然而然的，因为现在输入信息已经丢失了很多，导致网络更难学习到一个适当的潜在空间来生成未损坏的输出。因此，网络在压缩和重建操作中被迫变得稍微有点*创造性*。该网络的训练在50个周期后结束，损失为0.0126。现在我们可以对测试集进行一些预测，并可视化一些重建结果。
- en: Visualizing the results
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化结果
- en: 'Finally, we can test how well the model actually performs once we give it a
    more challenging task such as image denoising. We will use the same helper function
    to compare our network''s outputs with a sample from the test set, as shown here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以测试模型在面对更具挑战性的任务（如图像去噪）时的表现。我们将使用相同的辅助函数，将我们的网络输出与测试集中的一个样本进行比较，如下所示：
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Following is the output:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/190fe966-22f8-4b4c-acc1-1483cc693b62.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/190fe966-22f8-4b4c-acc1-1483cc693b62.png)'
- en: As we can see, the network does a decent job at recreating the images, despite
    the added noise factor! Many of those images are very hard to distinguish for
    the human eye, and so the fact that the network is able to recreate the general
    structure and composition of elements that are present therein is indeed noteworthy,
    especially given the meager learning capacity and training time allocated to the
    network.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，尽管添加了噪声因素，网络在重建图像方面表现相当不错！这些图像对人眼来说很难区分，因此，网络能够重建其中元素的整体结构和组成确实值得注意，尤其是考虑到分配给网络的有限学习能力和训练时间。
- en: We encourage you to experiment with more complex architectures by changing the
    number of layers, filters, and the encoding dimension of the latent space. In
    fact, now may be the perfect time to practice with some exercises, which are provided
    at the end of this chapter.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你通过更改层数、过滤器数量和潜在空间的编码维度来尝试更复杂的架构。事实上，现在可能是进行一些练习的最佳时机，相关练习会在本章末尾提供。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the fundamental theory behind autoencoders at a
    high level, and conceptualized the underlying mathematics that permits these models
    to learn. We saw several variations of the autoencoder architecture, including
    shallow, deep, undercomplete, and overcomplete models. This allowed us to overview
    considerations related to the representational power of each type of model and
    their propensity to overfit given too much capacity. We also explored some regularization
    techniques that let us compensate for the overfitting problem, such as the sparse
    and contractive autoencoders. Finally, we trained several different types of autoencoder
    networks, including shallow, deep, and convolutional networks, for the tasks of
    image reconstruction and denoising. We saw that with very little learning capacity
    and training time, convolutional autoencoders outperformed all of the other models
    in reconstructing images. Furthermore, it was able to generate denoised images
    from corrupted inputs, maintaining the general format of the input data it was
    shown.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们从高层次探讨了自编码器的基本理论，并概念化了允许这些模型进行学习的基础数学。我们看到了几种不同的自编码器架构，包括浅层、深层、不完全和过度完整的模型。这让我们能够概述与每种模型的表示能力相关的考虑因素，以及它们在容量过大时容易过拟合的倾向。我们还探讨了一些正则化技术，帮助我们弥补过拟合问题，例如稀疏自编码器和收缩自编码器。最后，我们训练了几种不同类型的自编码器网络，包括浅层、深层和卷积网络，进行图像重建和去噪的任务。我们看到，尽管学习能力和训练时间非常有限，卷积自编码器在图像重建方面超越了所有其他模型。此外，它能够从受损的输入中生成去噪图像，保持输入数据的整体格式。
- en: 'While we did not explore other use cases, such as dimensionality reduction
    to visualize main factors of variance, autoencoders have found a lot of applicability
    in different spheres, ranging from collaborative filtering in recommender systems,
    to even predicting future patients for healthcare, see *Deep Patient*: [https://www.nature.com/articles/srep26094](https://www.nature.com/articles/srep26094).
    There is one specific type of autoencoder that we purposefully didn''t cover in
    this chapter: the **Variational** **Autoencoder** (**VAE**). This type of autoencoder
    includes a special constraint on the latent space that''s being learned by the
    model. It actually forces the model to learn a probability distribution representing
    your input data, from which it samples its output. This is quite a different approach
    than the one we were perusing so far, which at best allowed our network to learn
    a somewhat arbitrary function. The reason we choose not to include this interesting
    subtopic in this chapter is because VAEs are, in technical parlance, an instance
    of generative models, which is the topic of our next chapter!'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有探索其他使用案例，例如使用降维来可视化主要的方差因子，但自编码器在不同领域中得到了广泛应用，从推荐系统中的协同过滤，到甚至预测未来的病人，见*Deep
    Patient*：[https://www.nature.com/articles/srep26094](https://www.nature.com/articles/srep26094)。有一种特定类型的自编码器是我们故意没有在本章中讨论的：**变分**
    **自编码器**（**VAE**）。这种自编码器在模型学习的潜在空间上施加了特殊的约束。它实际上迫使模型学习一个表示输入数据的概率分布，并从中采样生成输出。这与我们至今探索的方法大不相同，后者最多只能让我们的网络学习一个某种程度上任意的函数。我们选择不在本章中包括这一有趣的子话题的原因是，VAE在技术术语中属于生成模型的一个实例，而生成模型正是我们下一章的主题！
- en: Exercise
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Make a deep AE with the fashion MNIST dataset and monitor when the loss plateaus.
    Then, compare it with shallow AE.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时尚 MNIST 数据集创建深度自动编码器（AE），并监控损失何时趋于平稳。然后，与浅层自动编码器进行比较。
- en: Implement AEs on another dataset of your choice and experiment with different
    encoding dimensions, optimizers, and `loss` functions to see how the model performs.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在另一个您选择的数据集上实现自动编码器（AE），并尝试不同的编码维度、优化器和`loss`函数，看看模型表现如何。
- en: Compare when loss converges for different models (CNN, FF) and how stable or
    erratic the decrease in loss values are. What do you notice?
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较不同模型（CNN、FF）损失何时收敛以及损失值下降的稳定性或不稳定性。您注意到了什么？
