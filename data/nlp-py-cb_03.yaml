- en: Pre-Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下几个步骤：
- en: Tokenization – learning to use the inbuilt tokenizers of NLTK
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词 – 学习使用NLTK的内置分词器
- en: Stemming – learning to use the inbuilt stemmers of NLTK
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取 – 学习使用NLTK的内置词干提取器
- en: Lemmatization – learning to use the WordnetLemmatizer of NLTK
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原 – 学习使用NLTK的WordnetLemmatizer
- en: Stopwords – learning to use the stopwords corpus and seeing the difference it
    can make
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词 – 学习使用停用词语料库并查看它能带来的差异
- en: Edit distance – writing your own algorithm to find edit distance between two
    strings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑距离 – 编写你自己的算法来计算两个字符串之间的编辑距离
- en: Processing two short stories and extracting the common vocabulary between two
    of them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理两个短篇故事并提取它们之间的共同词汇
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we learned to read, normalize, and organize raw data
    coming from heterogeneous forms and formats into uniformity. In this chapter,
    we will go a step forward and prepare the data for consumption in our NLP applications.
    Preprocessing is the most important step in any kind of data processing task,
    or else we fall prey to the age old computer science cliché of *garbage in, garbage
    out*. The aim of this chapter is to introduce some of the critical preprocessing
    steps such as tokenization, stemming, lemmatization, and so on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何读取、规范化并将来自不同形式和格式的原始数据整理成统一格式。在本章中，我们将迈出更大的一步，为我们的NLP应用准备数据。预处理是任何数据处理任务中最重要的步骤，否则我们就会陷入计算机科学中的老生常谈——*垃圾进，垃圾出*。本章的目标是介绍一些关键的预处理步骤，如分词、词干提取、词形还原等。
- en: In this chapter, we will be seeing six different recipes. We will build up the
    chapter by performing each preprocessing task in individual recipes—tokenization,
    stemming, lemmatization, stopwords treatment, and edit distance—in that order.
    In the last recipe, we will look at an example of how we can combine some of these
    preprocessing techniques to find common vocabulary between two free-form texts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到六个不同的步骤。我们将通过执行每个预处理任务来逐步构建本章内容——分词、词干提取、词形还原、停用词处理和编辑距离，按此顺序进行。在最后一个步骤中，我们将展示如何结合一些预处理技术来找出两个自由文本之间的共同词汇。
- en: Tokenization – learning to use the inbuilt tokenizers of NLTK
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词 – 学习使用NLTK的内置分词器
- en: Understand the meaning of tokenization, why we need it, and how to do it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 理解分词的含义，为什么我们需要它，以及如何进行分词。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let's first see what a token is. When you receive a document or a long string
    that you want to process or work on, the first thing you'd want to do is break
    it into words and punctuation marks. This is what we call the process of tokenization.
    We will see what types of tokenizers are available with NLTK and implement them
    as well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看什么是token。当你收到一份文档或一串你想要处理或操作的长字符串时，首先要做的就是将其分解成单词和标点符号。这就是我们所说的分词过程。我们将会看到NLTK提供了哪些类型的分词器，并且实现它们。
- en: How to do it…
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Create a file named `tokenizer.py` and add the following import lines to it:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`tokenizer.py`的文件，并添加以下导入行：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Import the four different types of tokenizers that we are going to explore in
    this recipe.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 导入我们将在本章中探索的四种不同类型的分词器。
- en: 'We will start with `LineTokernizer`. Add the following two lines:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从`LineTokernizer`开始。添加以下两行：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As the name implies, this tokenizer is supposed to divide the input string
    into lines (not sentences, mind you!). Let''s see the output and what the tokenizer
    does:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 顾名思义，这个分词器应该将输入字符串分割成行（注意：不是句子！）。让我们看看输出结果以及分词器的作用：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, it has returned a list of three strings, meaning the given input
    has been divided in to three lines on the basis of where the newlines are. `LineTokenizer`
    simply divides the given input string into new lines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它返回了一个包含三个字符串的列表，意味着给定的输入已经根据换行符分成了三行。`LineTokenizer`仅仅是将给定的输入字符串按换行符分割成新行。
- en: 'Now we will see `SpaceTokenizer`. As the name implies, it is supposed to divide
    on split on space characters. Add the following lines:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看看`SpaceTokenizer`。顾名思义，它应该按照空格字符进行分割。添加以下代码行：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `sTokenizer` object is an object of `SpaceTokenize`. We have invoked the
    `tokenize()` method and we shall see the output now:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sTokenizer`对象是`SpaceTokenize`的一个实例。我们已经调用了`tokenize()`方法，现在来看看输出结果：'
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As expected, the input `rawText` is split on the space character  `""`.  On
    to the next one! It''s the `word_tokenize()` method. Add the following line:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如预期的那样，输入的`rawText`是以空格字符 `""` 为分隔符进行拆分的。接下来是`word_tokenize()`方法。添加以下代码行：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'See the difference here. The other two we have seen so far are classes, whereas
    this is a method of the `nltk` module. This is the method that we will be using
    most of the time going forward as it does exactly what we''ve defined to be tokenization.
    It breaks up words and punctuation marks. Let''s see the output:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看这里的区别。前面我们看到的另外两个是类，而这是`nltk`模块的方法。这是我们接下来会用到的方法，因为它恰好完成了我们定义的分词任务。它将单词和标点符号分开。让我们看看输出：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the difference between `SpaceTokenizer` and `word_tokenize()`
    is clearly visible.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，`SpaceTokenizer`和`word_tokenize()`之间的区别非常明显。
- en: 'Now, on to the last one. There''s a special `TweetTokernizer` that we can use
    when dealing with special case strings:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，进入最后一项。有一个特殊的`TweetTokernizer`，我们可以在处理特殊字符串时使用：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Tweets contain special words, special characters, hashtags, and smileys that
    we want to keep intact. Let''s see the output of these two lines:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推文包含特殊词语、特殊字符、标签和表情符号，我们希望它们保持原样。让我们看一下这两行代码的输出：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we see, the `Tokenizer` kept the hashtag word intact and didn't break it;
    the smileys are also kept intact and are not lost. This is one special little
    class that can be used when the application demands it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`Tokenizer`保留了标签词不变，没有将其拆分；表情符号也被完整保留，没有丢失。这是一个在需要时可以使用的特殊小类。
- en: 'Here''s the output of the program in full. We have already seen it in detail,
    so I will not be going into it again:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是程序的完整输出。我们已经详细查看过它，所以我不会再重复讲解：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works…
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We saw three tokenizer classes and a method implemented to do the job in the
    NLTK module. It's not very difficult to understand how to do it, but it is worth
    knowing why we do it. The smallest unit to process in language processing task
    is a token. It is very much like a divide-and-conquer strategy, where we try to
    make sense of the smallest units at a granular level and add them up to understand
    the semantics of the sentence, paragraph, document, and the corpus (if any) by
    moving up the level of detail.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到在NLTK模块中实现了三种分词类和一个方法来完成这项工作。理解该怎么做并不困难，但值得了解的是我们为什么要这么做。语言处理任务中的最小处理单元是一个token。这有点像分而治之的策略，我们尝试从细粒度的层面理解最小的单元，然后逐渐汇总起来，从而理解句子、段落、文档以及语料库（如果有的话）的语义。
- en: Stemming – learning to use the inbuilt stemmers of NLTK
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取 – 学习使用NLTK内建的词干提取器
- en: Let's understand the concept of a stem and the process of stemming. We will
    learn why we need to do it and how to perform it using inbuilt NLTK stemming classes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下词干的概念和词干提取的过程。我们将学习为什么需要做词干提取以及如何使用内建的NLTK词干提取类来执行。
- en: Getting ready
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: So what is a stem supposed to be? A stem is the base form of a word without
    any suffixes. And a stemmer is what removes the suffixes and returns the stem
    of the word. Let's see what types of stemmers are available with NLTK.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，词干究竟是什么呢？词干是一个词的基本形式，不包含任何后缀。而词干提取器就是去除后缀并返回词干的工具。让我们来看一下NLTK提供了哪些类型的词干提取器。
- en: How to do it…
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Create a file named `stemmers.py` and add the following import lines to it:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`stemmers.py`的文件，并添加以下导入行：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Importing the four different types of tokenizers that we are going to explore
    in this recipe
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 导入我们将在本教程中探索的四种不同类型的分词器
- en: 'Before we apply any stems, we need to tokenize the input text. Let''s quickly
    get that done with the following code:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用任何词干之前，我们需要先对输入文本进行分词。我们可以通过以下代码快速完成这项任务：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The token list contains all the `tokens` generated from the `raw` input string.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: token列表包含从`raw`输入字符串生成的所有`tokens`。
- en: 'First we shall `seePorterStemmer`. Let''s add the following three lines:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先我们来看`seePorterStemmer`。让我们添加以下三行代码：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, we initialize the stemmer object. Then we apply the stemmer to all `tokens`
    of the input text, and finally we `print` the output. Let''s see the output and
    we will know more:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们初始化词干提取器对象。然后，我们将词干提取器应用到输入文本的所有`tokens`上，最后输出结果。让我们来看一下输出，我们就会知道更多：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see in the output, all the words have been rid of the trailing `'s'`,
    `'es'`, `'e'`, `'ed'`, `'al'`, and so on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输出中的所有单词都去除了结尾的`'s'`、`'es'`、`'e'`、`'ed'`、`'al'`等。
- en: 'The next one is `LancasterStemmer`. This one is supposed to be even more error
    prone as it contains many more suffixes to be removed than `porter`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个是`LancasterStemmer`。它应该比`porter`更容易出错，因为它包含了更多需要去除的后缀：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The same drill! Just that this time we have `LancasterStemmer` instead of `PrterStemmer`.
    Let''s see the output:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一样的操作！只不过这次我们使用的是`LancasterStemmer`而不是`PorterStemmer`。让我们看看输出：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We shall discuss the difference in the output section, but we can make out that
    the suffixes that are dropped are bigger than Porter. `'us'`, `'e'`, `'th'`, `'eral'`,
    `"ered"`, and many more!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在输出部分讨论区别，但可以看出，`LancasterStemmer`去掉的后缀比`Porter`要多。比如：`'us'`，`'e'`，`'th'`，`'eral'`，`"ered"`等等！
- en: 'Here''s the output of the program in full. We will compare the output of both
    the stemmers:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是程序的完整输出。我们将比较两个词干提取器的输出：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we compare the output of both the stemmers, we see that `lancaster` is clearly
    the greedier one when dropping suffixes. It tries to remove as many characters
    from the end as possible, whereas `porter` is non-greedy and removes as little
    as possible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们比较两个词干提取器的输出时，发现`LancasterStemmer`显然是更“贪婪”的，去除后缀时它尽可能地移除尽量多的字符，而`PorterStemmer`则是非贪婪的，尽量少移除字符。
- en: How it works…
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: For some language processing tasks, we ignore the form available in the input
    text and work with the stems instead. For example, when you search on the Internet
    for *cameras*, the result includes documents containing the word *camera* as well
    as *cameras*, and vice versa. In hindsight though, both words are the same; the
    stem is *camera*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些语言处理任务，我们忽略输入文本中的形式，而是使用词干。例如，当你在互联网上搜索*cameras*时，结果会包括包含单词*camera*和*cameras*的文档，反之亦然。回想起来，这两个词是相同的；词干是*camera*。
- en: Having said this, we can clearly see that this method is quite error prone,
    as the spellings are quite meddled with after a stemmer is done reducing the words.
    At times, it might be okay, but if you really want to understand the semantics,
    there is a lot of data loss here. For this reason, we shall next see what is called
    **lemmatization**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们可以清楚地看到，这种方法相当容易出错，因为在进行词干提取后，单词的拼写会被大幅篡改。有时候这种情况可能没关系，但如果你真想理解语义，这里会有很多数据丢失。因此，接下来我们将介绍**词形还原**。
- en: Lemmatization – learning to use the WordnetLemmatizer of NLTK
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词形还原 – 学习如何使用NLTK的WordnetLemmatizer
- en: Understand what lemma and lemmatization are. Learn how lemmatization differs
    from Stemming, why we need it, and how to perform it using `nltk` library's `WordnetLemmatizer`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 了解什么是词元和词形还原。学习词形还原如何与词干提取不同，为什么我们需要它，以及如何使用`nltk`库中的`WordnetLemmatizer`来实现它。
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: A lemma is a lexicon headword or, more simply, the base form of a word. We have
    already seen what a stem is, but a lemma is a dictionary-matched base form unlike
    the stem obtained by removing/replacing the suffixes. Since it is a dictionary
    match, lemmatization is a slower process than Stemming.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 词元是词典中的词头或更简单地说，单词的基本形式。我们已经了解了什么是词干，但词元是与词典匹配的基本形式，不同于通过去除/替换后缀得到的词干。由于它是与词典匹配的，词形还原比词干提取更慢。
- en: How to do it…
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Create a file named `lemmatizer.py` and add the following import lines to it:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`lemmatizer.py`的文件，并向其中添加以下导入语句：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We will need to tokenize the sentences first, and we shall use the `PorterStemmer`
    to compare the output.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要先对句子进行分词，然后使用`PorterStemmer`来比较输出结果。
- en: 'Before we apply any stems, we need to tokenize the input text. Let''s quickly
    get that done with the following code:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们进行任何词干提取之前，需要先对输入文本进行分词。让我们通过以下代码快速完成这一步：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The token list contains all the tokens generated from the `raw` input string.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: token列表包含从`raw`输入字符串生成的所有词元。
- en: 'First we will `applyPorterStemmer`, which we have already seen in the previous
    recipe. Let''s add the following three lines:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将`applyPorterStemmer`，这个我们已经在前面的教程中看过了。我们来添加以下三行：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: First, we initialize the stemmer object. Then we apply the stemmer on all `tokens`
    of the input text, and finally we print the output. We shall check the output
    at the end of the recipe.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化词干提取器对象。然后我们对输入文本的所有`tokens`应用词干提取器，最后打印输出。我们将在教程的最后查看输出。
- en: 'Now we apply the `lemmatizer`. Add the following three lines:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们应用`lemmatizer`。添加以下三行：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run, and the output of these three lines will be like this:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行之后，这三行代码的输出将是这样的：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you see, it understands that for nouns it doesn't have to remove the trailing
    `'s'`. But for non-nouns, for example, legions and armies, it removes suffixes
    and also replaces them. However, what it’s essentially doing is a dictionary match.
    We shall discuss the difference in the output section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，它理解到对于名词，它不需要去掉结尾的`'s'`。但是对于非名词，例如“legions”和“armies”，它会移除后缀并进行替换。然而，它基本上是在进行字典匹配。我们将在输出部分讨论其差异。
- en: 'Here''s the output of the program in full. We will compare the output of both
    the stemmers:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是程序的完整输出。我们将比较两种词干提取器的输出：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we compare the output of the stemmer and the `lemmatizer`, we see that the
    stemmer makes a lot of mistakes and the `lemmatizer` makes very few mistakes.
    However, it doesn't do anything with the word `'murdered'`, and that is an error.
    Yet, as an end product, `lemmatizer` does a far better job of getting us the base
    form than the stemmer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们比较词干提取器和`词形还原器`的输出时，我们发现词干提取器犯了很多错误，而`词形还原器`则几乎没有错误。然而，它对于单词`'murdered'`没有做任何处理，这是一个错误。不过，作为最终的产物，`词形还原器`在获取词根形式方面要比词干提取器做得好得多。
- en: How it works…
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: '`WordNetLemmatizer` removes affixes only if it can find the resulting word
    in the dictionary. This makes the process of lemmatization slower than Stemming.
    Also, it understands and treats capitalized words as special words; it doesn’t
    do any processing for them and returns them as is. To work around this, you may
    want to convert your input string to lowercase and then run lemmatization on it.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordNetLemmatizer`只会在能够在字典中找到结果单词时，才会去除词缀。这使得词形还原的过程比词干提取要慢。此外，它还会将大写单词视为特殊单词；它不会对这些单词进行任何处理，直接返回原样。为了解决这个问题，你可以考虑将输入字符串转换为小写，然后再进行词形还原。'
- en: All said and done, lemmatization is still not perfect and will make mistakes.
    Check the input string and the result of this recipe; it couldn't convert `'murdered'`
    to `'murder'`. Similarly, it will handle the word `'women'` correctly but can't
    handle `'men'`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一切完成后，词形还原仍然不完美，依然会犯错误。检查输入字符串和本教程的结果；它未能将`'murdered'`转换为`'murder'`。类似地，它能够正确处理单词`'women'`，但无法处理`'men'`。
- en: Stopwords – learning to use the stopwords corpus and seeing the difference it
    can make
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词——学习如何使用停用词语料库并看到它能带来的不同
- en: We will be using the Gutenberg corpus as an example in this recipe. The Gutenberg
    corpus is part of the NLTK data module. It contains a selection of 18 texts from
    some 25,000 electronic books from the project Gutenberg text archives. It is  `PlainTextCorpus`,
    meaning there are no categories involved with this corpus. It is best suited if
    you want to play around with the words/tokens without worrying about the affinity
    of the text to any particular topic. One of the objectives of this little recipe
    is also to introduce one of the most important preprocessing steps in text analytics—stopwords
    treatment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用古腾堡语料库作为示例。古腾堡语料库是NLTK数据模块的一部分。它包含了来自古腾堡项目文本档案的约25,000本电子书中的18个文本选集。它是`PlainTextCorpus`，意味着该语料库中没有类别。它最适合用来在不关心文本与任何特定话题之间的关联的情况下，玩转单词/标记。这个小教程的一个目标也是介绍文本分析中最重要的预处理步骤之一——停用词处理。
- en: In accordance with the objectives, we will use this corpus to elaborate the
    usage of Frequency Distribution of the NLTK module in Python within the context
    of stopwords. To give a small synopsis, a stopword is a word that, though it has
    significant syntactic value in sentence formation, carries very negligible or
    minimal semantic value. When you are not working with the syntax but with a bag-of-words
    kind of approach (for example, TF/IDF), it makes sense to get rid of stopwords
    except the ones that you are specifically interested in.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据目标，我们将使用这个语料库详细阐述在停用词上下文中，如何使用NLTK模块的频率分布功能。简单来说，停用词是指那些在句子构建中虽然具有重要的句法价值，但语义价值非常微弱或几乎没有的单词。当你使用词袋模型（例如，TF/IDF）进行分析而不是关注句法时，去除停用词是有意义的，除了你特别关注的那些词。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The `nltk.corpus.stopwords` is also a corpus as part of the NLTK Data module
    that we will use in this recipe, along with `nltk.corpus.gutenberg`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.corpus.stopwords`也是NLTK数据模块中的一个语料库，我们将在本教程中与`nltk.corpus.gutenberg`一起使用。'
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Create a new file named `Gutenberg.py` and add the following three lines of
    code to it:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的文件，命名为`Gutenberg.py`，并向其中添加以下三行代码：
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here we are importing the required libraries and the Gutenberg corpus in the
    first two lines. The second line is used to check if the corpus was loaded successfully.
    Run the file on the Python interpreter and you should get an output that looks
    similar to:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们在前两行导入了所需的库和古腾堡语料库。第二行用于检查语料库是否成功加载。在Python解释器中运行文件，你应该会得到类似以下的输出：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, the names of all 18 Gutenberg texts are printed on the console.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，所有18本古腾堡文本的名称都打印在了控制台上。
- en: 'Add the following two lines of code, where we are doing a little preprocessing
    step on the list of all words from the corpus:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下两行代码，我们在对语料库中的所有单词列表进行一些预处理：
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first line simply copies the list of all words in the corpus from the sample
    bible—`kjv.txt` in the `gb_words` variable. The second, and interesting, step
    is where we are iterating over the entire list of words from Gutenberg, discarding
    all the words/tokens whose length is two characters or less.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行简单地将所有单词的列表从示例圣经——`kjv.txt`中的语料库复制到`gb_words`变量中。第二个有趣的步骤是，我们遍历古腾堡中所有单词的列表，丢弃所有长度小于或等于两个字符的单词/标记。
- en: 'Now we will access `nltk.corpus.stopwords` and do `stopwords` treatment on
    the filtered words list from the previous list. Add the following lines of code
    for the same:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将访问`nltk.corpus.stopwords`，并对之前过滤过的单词列表进行`stopwords`处理。为此，添加以下几行代码：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first line simply loads words from the stopwords corpus into the `stopwords`
    variable for the `english` language. The second line is where we are filtering
    out all `stopwords` from the filtered word list we had developed in the previous
    example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行简单地将停用词语料库中的单词加载到`stopwords`变量中，适用于`english`语言。第二行则是在过滤掉前面示例中我们已处理的单词列表中的所有`stopwords`。
- en: 'Now we will simply apply `nltk.FreqDist` to the list of preprocessed `words`
    and the plain list of `words`. Add these lines to do the same:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将简单地将`nltk.FreqDist`应用于预处理后的`words`列表和原始的`words`列表。添加以下几行来实现相同的功能：
- en: '[PRE27]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Create the `FreqDist` object by passing as argument the words list that we formulated
    in steps 2 and 3.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递我们在步骤2和3中制定的单词列表作为参数来创建`FreqDist`对象。
- en: 'Now we want to see some of the characteristics of the frequency distribution
    that we just made. Add the following four lines in the code and we will see what
    each does:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们想看看我们刚刚制作的频率分布的一些特征。请添加以下四行代码，我们将看到每一行的作用：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `most_common(10)` function will return the `10` most common words in the
    word bag being processed by frequency distribution. What it outputs is what we
    will discuss and elaborate now.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`most_common(10)`函数将返回正在被频率分布处理的单词包中最常见的`10`个单词。我们将讨论和详细阐述它的输出内容。'
- en: 'After you run this program, you should get something similar to the following:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此程序后，你应该会得到类似以下的输出：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: If you look carefully at the output, the most common 10 words in the unprocessed
    or plain list of words won't make much sense. Whereas from the preprocessed bag
    of words, the most common 10 words such as `god`, `lord`, and `man` give us a
    quick understanding that we are dealing with a text related to faith or religion.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看输出，未处理的或原始单词列表中最常见的10个单词可能没有太大意义。而从预处理后的单词包中，最常见的10个单词，如`god`、`lord`和`man`，迅速让我们意识到这是一篇与信仰或宗教相关的文本。
- en: The foremost objective of this recipe is to introduce you to the concept of
    stopwords treatment for text preprocessing techniques that you would most likely
    have to do before running any complex analysis on your data. The NLTK stopwords
    corpus contains stop-words for 11 languages. When you are trying to analyze the
    importance of keywords in any text analytics application, treating the stopwords
    properly will take you a long way. Frequency distribution will help you get the
    importance of words. Statistically speaking, this distribution would ideally look
    like a bell curve if you plot it on a two-dimensional plane of frequency and importance
    of words.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的主要目标是向你介绍停用词处理的概念，这是一种文本预处理技术，在你对数据进行复杂分析之前，你很可能需要进行此操作。NLTK的停用词语料库包含11种语言的停用词。当你试图分析文本分析应用程序中关键字的重要性时，正确处理停用词会帮助你走得更远。频率分布将帮助你了解单词的重要性。从统计学角度看，如果你在频率和单词重要性这两个维度的平面上绘制它，这个分布理想情况下应该呈钟形曲线。
- en: Edit distance – writing your own algorithm to find edit distance between two
    strings
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编辑距离——编写你自己的算法来计算两个字符串之间的编辑距离
- en: Edit distance, also called as **Levenshtein distance** is a metric used to measure
    the similarity between two distances. Essentially, it’s a count of how many edit
    operations, deletions, insertions, or substitutions will transform a given String
    `A` to String `B`. We shall write our own algorithm to calculate the edit distance
    and then compare it against `nltk.metrics.distance.edit_distance()` for a sanity
    check.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑距离，也叫**莱文斯坦距离**，是一种衡量两个字符串相似度的度量。实质上，它是通过计算多少次编辑操作（删除、插入或替换）能够将给定的字符串`A`转化为字符串`B`。我们将编写自己的算法来计算编辑距离，然后与`nltk.metrics.distance.edit_distance()`进行对比，进行准确性检查。
- en: Getting ready
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: You may want to look up a little more on the Levenshtein distance part for mathematical
    equations. We will look at the algorithm implementation in python and why we do
    it, but it may not be feasible to cover the complete mathematics behind it. Here’s
    a link on Wikipedia: [https://en.wikipedia.org/wiki/Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想进一步了解莱文斯坦距离部分的数学公式。我们将查看Python中的算法实现以及我们为什么要这样做，但可能无法全面覆盖其背后的完整数学原理。这里有一个维基百科的链接：[https://en.wikipedia.org/wiki/Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance)。
- en: How to do it…
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Create a file named `edit_distance_calculator.py` and add the following import
    lines to it:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`edit_distance_calculator.py`的文件，并在其中添加以下导入行：
- en: '[PRE30]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We just imported the inbuilt `nltk` library's `edit_distance` function from
    the `nltk.metrics.distance` module.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚从内置的`nltk`库的`nltk.metrics.distance`模块中导入了`edit_distance`函数。
- en: 'Let''s define our method to accept two strings and calculate the edit distance
    between the two. `str1` and `str2` are two strings that the function accepts,
    and we will return an integer distance value:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个方法，接受两个字符串并计算它们之间的编辑距离。`str1`和`str2`是该函数接受的两个字符串，我们将返回一个整数的距离值：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The next step is to get the length of the two input strings. We will be using
    the length to create an *m x n* table where `m` and `n` are the lengths of the
    two strings `s1` and `s2` respectively:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是获取两个输入字符串的长度。我们将使用这些长度来创建一个*m x n*的表格，其中`m`和`n`分别是两个字符串`s1`和`s2`的长度：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we will create `table` and initialize the first row and first column:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建`table`并初始化第一行和第一列：
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will initialize the two-dimensional array and the contents will look like
    the following table in memory:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将初始化二维数组，内容将在内存中呈现如下表格：
- en: '![](img/dc28e376-df91-45f1-b9a5-b400657cd191.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc28e376-df91-45f1-b9a5-b400657cd191.png)'
- en: Please note that this is inside a function and I'm using the example strings
    we are going to pass to the function to elaborate the algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是在一个函数内部，我使用了将要传递给该函数的示例字符串来阐述算法。
- en: 'Now comes the tricky part. We are going to fill up the matrix using the formula:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是难点部分。我们将使用公式填充矩阵：
- en: '[PRE34]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `cost` is calculated on whether the characters in contention are the same
    or they edition, specifically deletion or insertion. The formula in the next line
    for is calculating the value of the cell in the matrix, the first two take care
    of substitution and the third one is for substitution. We also add the cost of
    the previous step to it and take the minimum of the three.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`cost` 是通过判断争议中的字符是否相同或进行编辑，具体来说是删除或插入来计算的。下一行的公式用于计算矩阵中单元格的值，前两个处理替换，第三个用于替换。我们还会将前一步的成本加到其中，并取三个中的最小值。'
- en: 'At the end, we return the value of the last cell, that is, `table[m,n]`, as
    the final edit distance value:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们返回最后一个单元格的值，也就是`table[m,n]`，作为最终的编辑距离值：
- en: '[PRE35]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we will call our function and the `nltk` library''s  `edit_distance()`
    function on two strings and check the output:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将调用我们的函数和`nltk`库的`edit_distance()`函数，对两个字符串进行计算并检查输出：
- en: '[PRE36]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Our words are `hand` and `and`. Only a single delete operation on the first
    string or a single insertion operation on the second string will give us a match.
    Hence, the expected Levenshtein score is `1`.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的词是`hand`和`and`。只需在第一个字符串上执行一次删除操作或在第二个字符串上执行一次插入操作，就可以得到匹配。因此，预期的莱文斯坦得分是`1`。
- en: 'Here''s the output of the program:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是程序的输出：
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As expected, the NLTK `edit_distance()` returns `1` and so does our algorithm.
    Fair to say that our algorithm is doing as expected, but I would urge you guys
    to test it further by running it through with some more examples.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，NLTK的`edit_distance()`返回`1`，我们的算法也是如此。可以公平地说，我们的算法按预期运行，但我建议大家通过运行更多的示例来进一步测试它。
- en: How it works…
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'I''ve already given you a brief on the algorithm; now let’s see how the matrix
    *table* gets populated with the algorithm. See the attached table here:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经简要介绍了算法，现在让我们看看算法是如何填充矩阵*表格*的。请查看附带的表格：
- en: '![](img/719ea28d-7009-4ffc-a111-b8f99a86de11.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/719ea28d-7009-4ffc-a111-b8f99a86de11.png)'
- en: You've already seen how we initialized the matrix. Then we filled up the matrix
    using the formula in algorithm. The yellow trail you see is the significant numbers.
    After the first iteration, you can see that the distance is moving in the direction
    of 1 consistently and the final value that we return is denoted by the green background
    cell.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到我们是如何初始化矩阵的。然后我们使用算法中的公式填充矩阵。你看到的黄色路径是关键的数字。在第一次迭代之后，你可以看到距离在朝着1的方向不断变化，最终返回的值由绿色背景的单元格表示。
- en: Now, the applications of the edit distance algorithm are multifold. First and
    foremost, it is used in spell checkers and auto-suggestions in text editors, search
    engines, and many such text-based applications. Since the cost of comparisons
    is equivalent to the product of the length of the strings to be compared, it is
    sometimes impractical to apply it to compare large texts.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编辑距离算法的应用是多方面的。首先，它被用于拼写检查器和文本编辑器中的自动建议、搜索引擎以及许多基于文本的应用程序中。由于比较的成本等于要比较的字符串长度的乘积，所以在比较大型文本时，应用它有时并不实际。
- en: Processing two short stories and extracting the common vocabulary between two
    of them
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理两篇短篇故事并提取它们之间的共同词汇
- en: This recipe is supposed to give you an idea of how to handle a typical text
    analytics problem when you come across it. We will be using multiple preprocessing
    techniques in the process of getting to our outcome. The recipe will end with
    an important preprocessing task and not a real application of text analysis. We
    will be using a couple of short stories from [http://www.english-for-students.com/](http://www.english-for-students.com/).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流程旨在给你一个如何处理典型文本分析问题的思路。我们将使用多种预处理技术来得到结果。这个流程最终会以一个重要的预处理任务结束，而不是实际的文本分析应用。我们将使用来自[http://www.english-for-students.com/](http://www.english-for-students.com/)的几篇短篇故事。
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be removing all special characters, splitting words, doing case folds,
    and some set and list operations in this recipe. We won’t be using any special
    libraries, just Python programming tricks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个过程中移除所有特殊字符、拆分单词、进行大小写折叠以及进行一些集合和列表操作。我们不会使用任何特殊的库，只会用Python编程技巧。
- en: How to do it…
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Create a file named `lemmatizer.py` and create a couple of long strings with
    short stories or any news articles:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`lemmatizer.py`的文件，并创建几个包含短篇故事或新闻文章的长字符串：
- en: '[PRE38]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There we have two short stories from the aforementioned website!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是来自前面提到的网站的两篇短篇故事！
- en: 'First, we will remove some of the special characters from the texts. We are
    removing all newlines (`''\n''`), commas, full stops, exclamations, question marks,
    and so on. At the end, we convert the entire string to lowercase with the `casefold()` function:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将移除文本中的一些特殊字符。我们将移除所有换行符（`'\n'`）、逗号、句号、感叹号、问号等等。最后，我们使用`casefold()`函数将整个字符串转换为小写：
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we will split the texts into words:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将把文本拆分成单词：
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Using `split` on the `""` character, we split and get the list of words from
    `story1` and `story2.` Let''s see the output after this step:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`split`方法对`""`字符进行拆分，得到`story1`和`story2`的单词列表。让我们看看这一步后的输出：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, all the special characters are gone and a list of words is created.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有特殊字符都被移除，并且生成了一个单词列表。
- en: Now let's create a vocabulary out of this list of words. A vocabulary is a set
    of words. No repeats!
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们从这个单词列表中创建一个词汇表。词汇表是一个单词的集合，不允许重复！
- en: '[PRE42]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Calling the Python internal `set()` function on the list will deduplicate the
    list and convert it into a set:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用Python内置的`set()`函数对列表进行去重，将其转换为集合：
- en: '[PRE43]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here are the deduplicated sets, the vocabularies of both the stories.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是去重后的词汇集合，包含了两个故事的词汇。
- en: 'Now, the final step. Produce the common vocabulary between these two stories:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，最后一步。生成这两个故事之间的共同词汇：
- en: '[PRE44]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Python allows the set operation `&` (AND), which we are using to find the set
    of common entries between these two vocabulary sets. Let''s see the output of
    the final step:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python允许使用集合操作符`&`（AND），我们用它来找到这两个词汇集合之间的共同项。让我们看看最终步骤的输出：
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And there it is, the end-goal.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，目标完成。
- en: 'Here is the output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE46]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works…
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: So here, we saw how we can go from a couple of narratives to the common vocabulary
    between them. We didn’t use any fancy libraries, nor did we perform any complex
    operations. Yet we built a base from which we can take this bag-of-words forward
    and do many things with it.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，我们展示了如何从几个叙述中找到它们之间的共同词汇。我们没有使用任何复杂的库，也没有进行任何复杂的操作。但我们构建了一个基础，从这个基础出发，我们可以利用这些词袋进行许多后续操作。
- en: From here on, we can think of many different applications, such as text similarity,
    search engine tagging, text summarization, and many more.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以想到许多不同的应用场景，比如文本相似度、搜索引擎标签、文本摘要等等。
