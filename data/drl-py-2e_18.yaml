- en: Appendix 1 – Reinforcement Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 1 – 强化学习算法
- en: Let's have a look at all the reinforcement learning algorithms we have learned
    about in this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看本书中介绍的所有强化学习算法。
- en: Reinforcement learning algorithm
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习算法
- en: 'The steps involved in a typical reinforcement learning algorithm are given
    as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一般强化学习算法的步骤如下：
- en: First, the agent interacts with the environment by performing an action.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，代理通过执行一个动作与环境进行互动。
- en: The agent performs an action and moves from one state to another.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理执行一个动作，并从一个状态转移到另一个状态。
- en: Then the agent will receive a reward based on the action it performed.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后代理将根据它执行的动作获得奖励。
- en: Based on the reward, the agent will understand whether the action is good or bad.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据奖励，代理会理解该动作是好还是坏。
- en: If the action was good, that is, if the agent received a positive reward, then
    the agent will prefer performing that action, else the agent will try performing
    other actions that can result in a positive reward. So reinforcement learning
    is basically a trial-and-error learning process.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果动作是好的，即代理收到了正向奖励，那么代理会偏向于执行该动作，否则代理会尝试执行其他可能导致正向奖励的动作。所以，强化学习本质上是一个试错学习过程。
- en: Value Iteration
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'The algorithm of value iteration is given as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代的算法如下：
- en: Compute the optimal value function by taking maximum over the Q function, that
    is, ![](img/B15558_18_001.png)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对 Q 函数取最大值来计算最优值函数，即！[](img/B15558_18_001.png)
- en: Extract the optimal policy from the computed optimal value function
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从计算出的最优值函数中提取最优策略
- en: Policy Iteration
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略迭代
- en: 'The algorithm of policy iteration is given as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代的算法如下：
- en: Initialize a random policy
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个随机策略
- en: Compute the value function using the given policy
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定策略计算值函数
- en: Extract a new policy using the value function obtained from *step 2*
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从 *第2步* 获得的值函数提取新策略
- en: If the extracted policy is the same as the policy used in *step 2* then stop,
    else send the extracted new policy to *step 2* and repeat *steps 2* to *4*
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提取的策略与 *第2步* 中使用的策略相同，则停止，否则将提取的新的策略发送到 *第2步*，并重复 *第2步* 到 *第4步*
- en: First-Visit MC Prediction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首次访问 MC 预测
- en: 'The algorithm of first-visit MC prediction is given as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首次访问 MC 预测的算法如下：
- en: Let total_return(*s*) be the sum of the return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_18_002.png) is given as input.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让 total_return(*s*) 为一个状态在多个回合中的回报总和，*N*(*s*) 为计数器，即状态在多个回合中被访问的次数。将 total_return(*s*)
    和 *N*(*s*) 初始化为零，适用于所有状态。策略 ![](img/B15558_18_002.png) 作为输入给出。
- en: 'For *M* number of iterations:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using the policy ![](img/B15558_04_054.png)
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_04_054.png) 生成一个回合
- en: Store all rewards obtained in the episode in a list called rewards
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在一个名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每个步骤 *t*：
- en: 'If the state *s*[t] is occurring for the first time in the episode:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果状态 *s*[t] 在回合中首次出现：
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态 *s*[t] 的回报 *R*(*s*[t]) = sum(rewards[t:])
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + *R*(*s*[t])
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态 *s*[t] 的回报总和为 total_return(*s*[t]) = total_return(*s*[t]) + *R*(*s*[t])
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t]) = *N*(*s*[t]) + 1
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过取平均值来计算一个状态的值，即：![](img/B15558_04_056.png)
- en: Every-Visit MC Prediction
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每次访问 MC 预测
- en: 'The algorithm of every-visit MC prediction is given as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每次访问 MC 预测的算法如下：
- en: Let total_return(*s*) be the sum of the return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_03_084.png) is given as input.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让 total_return(*s*) 为一个状态在多个回合中的回报总和，*N*(*s*) 为计数器，即状态在多个回合中被访问的次数。将 total_return(*s*)
    和 *N*(*s*) 初始化为零，适用于所有状态。策略 ![](img/B15558_03_084.png) 作为输入给出。
- en: 'For *M* number of iterations:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using the policy ![](img/B15558_14_001.png)
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_14_001.png) 生成一个回合
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步 *t* 在该回合中：
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态 *s*[t] 的回报为 *R*(*s*[t]) = sum(rewards[t:])
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + *R*(*s*[t])
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态 *s*[t] 的总回报为 total_return(*s*[t]) = total_return(*s*[t]) + *R*(*s*[t])
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t]) = *N*(*s*[t]) + 1
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单地取平均值来计算状态的值，即：![](img/B15558_04_056.png)
- en: MC Prediction – the Q Function
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MC 预测 – Q 函数
- en: 'The algorithm for MC prediction of the Q function is given as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MC 预测 Q 函数的算法如下：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero. The policy ![](img/B15558_03_084.png)
    is given as input.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令 total_return(*s*, *a*) 为状态-动作对在多个回合中的回报总和，*N*(*s*, *a*) 为状态-动作对在多个回合中被访问的次数。初始化所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 为零。给定策略 ![](img/B15558_03_084.png) 作为输入。
- en: 'For *M* number of iterations:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using policy ![](img/B15558_03_084.png)
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_03_084.png) 生成一个回合
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步 *t* 在该回合中：
- en: Compute the return for the state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair, total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t])
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报，total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) + *R*(*s*[t],
    *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q function (Q value) by just taking the average, that is:![](img/B15558_04_067.png)
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单地取平均值来计算 Q 函数（Q 值），即：![](img/B15558_04_067.png)
- en: MC Control Method
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MC 控制方法
- en: 'The algorithm for the MC control method is given as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: MC 控制方法的算法如下：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_008.png).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令 total_return(*s*, *a*) 为状态-动作对在多个回合中的回报总和，*N*(*s*, *a*) 为状态-动作对在多个回合中被访问的次数。初始化所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 为零，并初始化一个随机策略 ![](img/B15558_03_008.png)。
- en: 'For *M* number of iterations:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using policy ![](img/B15558_04_032.png)
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_04_032.png) 生成一个回合
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回合中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步 *t* 在该回合中：
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 (*s*[t], *a*[t]) 是该回合中第一次出现：
- en: Compute the return of a state-action pair,*R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t])
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报为 total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) +
    *R*(*s*[t], *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q value by just taking the average, that is, ![](img/B15558_18_013.png)
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单地取平均值来计算 Q 值，即 ![](img/B15558_18_013.png)
- en: Compute the new updated policy from ![](img/B15558_03_139.png) using the Q function:![](img/B15558_18_015.png)
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q 函数从 ![](img/B15558_03_139.png) 计算新的更新策略：![](img/B15558_18_015.png)
- en: On-Policy MC Control – Exploring starts
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在政策 MC 控制 – 探索开始
- en: 'The algorithm for on-policy MC control by exploring the starts method is given
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于探索开始方法的在政策 MC 控制算法如下：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_04_099.png).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设 total_return(*s*, *a*) 为多次剧集中状态-动作对的回报总和，*N*(*s*, *a*) 为多次剧集中访问状态-动作对的次数。将所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 初始化为零，并初始化随机策略 ![](img/B15558_04_099.png)。
- en: 'For *M* number of iterations:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Select the initial state *s*[0] and initial action *a*[0] randomly such that
    all state-action pairs have a probability greater than 0
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择初始状态 *s*[0] 和初始动作 *a*[0]，使得所有状态-动作对的概率大于 0
- en: Generate an episode from the selected initial state *s*[0] and action *a*[0]
    using policy ![](img/B15558_04_099.png)
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_04_099.png) 从选定的初始状态 *s*[0] 和动作 *a*[0] 生成一集
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将剧集中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于剧集中的每个步骤 *t*：
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 (*s*[t], *a*[t]) 在这一集是首次出现：
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t])
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报为 total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) +
    *R*(*s*[t], *a*[t])
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
- en: Compute the Q value by just taking the average, that is,![](img/B15558_18_013.png)
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单地取平均值来计算 Q 值，即 ![](img/B15558_18_013.png)
- en: Compute the updated policy from ![](img/B15558_04_099.png) using the Q function:![](img/B15558_18_020.png)
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q 函数从 ![](img/B15558_04_099.png) 计算更新后的策略：![](img/B15558_18_020.png)
- en: On-Policy MC Control – Epsilon-Greedy
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线蒙特卡洛控制 - Epsilon-Greedy
- en: 'The algorithm for on-policy MC control with the epsilon-greedy policy is given
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 epsilon-greedy 策略的在线蒙特卡洛控制算法如下：
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_084.png).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设 total_return(*s*, *a*) 为多次剧集中状态-动作对的回报总和，*N*(*s*, *a*) 为多次剧集中访问状态-动作对的次数。将所有状态-动作对的
    total_return(*s*, *a*) 和 *N*(*s*, *a*) 初始化为零，并初始化随机策略 ![](img/B15558_03_084.png)。
- en: 'For *M* number of iterations:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using policy ![](img/B15558_03_008.png)
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_03_008.png) 生成一集
- en: Store all the rewards obtained in the episode in the list called rewards
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将剧集中获得的所有奖励存储在名为 rewards 的列表中
- en: 'For each step *t* in the episode:'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于剧集中的每个步骤 *t*：
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 (*s*[t], *a*[t]) 在这一集是首次出现：
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:]).
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态-动作对的回报，*R*(*s*[t], *a*[t]) = sum(rewards[t:])。
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t]).
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态-动作对的总回报为 total_return(*s*[t], *a*[t]) = total_return(*s*[t], *a*[t]) +
    *R*(*s*[t], *a*[t])。
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新计数器为 *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1。
- en: Compute the Q value by just taking the average, that is,![](img/B15558_18_013.png)
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过简单地取平均值来计算 Q 值，即 ![](img/B15558_18_013.png)
- en: Compute the updated policy ![](img/B15558_03_084.png) using the Q function.
    Let ![](img/B15558_14_185.png). The policy ![](img/B15558_03_084.png) selects
    the best action ![](img/B15558_18_027.png) with probability ![](img/B15558_04_123.png),
    and a random action with probability ![](img/B15558_13_276.png).
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q 函数计算更新后的策略 ![](img/B15558_03_084.png)。设 ![](img/B15558_14_185.png)。策略 ![](img/B15558_03_084.png)
    以概率 ![](img/B15558_04_123.png) 选择最佳动作 ![](img/B15558_18_027.png)，以概率 ![](img/B15558_13_276.png)
    选择随机动作。
- en: Off-Policy MC Control
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离策略蒙特卡洛控制
- en: 'The algorithm for the off-policy MC control method is given as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 离策略蒙特卡洛控制方法的算法如下：
- en: Initialize the Q function *Q*(*s*, *a*) with random values and set the behavior
    policy *b* to be epsilon-greedy, set the target policy ![](img/B15558_03_140.png)
    to be greedy policy and initialize the cumulative weights as *C*(*s*, *a*) = 0
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 函数 *Q*(*s*, *a*)，赋值为随机值，设置行为策略 *b* 为 epsilon-greedy，设置目标策略 ![](img/B15558_03_140.png)
    为贪婪策略，并初始化累积权重为 *C*(*s*, *a*) = 0
- en: 'For *M* number of episodes:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *M* 次迭代：
- en: Generate an episode using the behavior policy *b*
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用行为策略 *b* 生成一个序列
- en: Initialize return *R* to 0 and weight *W* to 1
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回报 *R* 初始化为 0，权重 *W* 初始化为 1
- en: 'For each step *t* in the episode, *t* = *T* – 1, *T* – 2, . . . , 0:'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤 *t*，*t* = *T* – 1, *T* – 2, . . . , 0：
- en: Compute the return as *R* = *R* + *r*[t+1]
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报为 *R* = *R* + *r*[t+1]
- en: Update the cumulative weights to *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) +*W*
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新累积权重为 *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) +*W*
- en: Update the Q value to ![](img/B15558_18_031.png)
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Q 值为 ![](img/B15558_18_031.png)
- en: Compute the target policy ![](img/B15558_18_032.png)
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标策略 ![](img/B15558_18_032.png)
- en: If ![](img/B15558_18_033.png) then break
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ![](img/B15558_18_033.png) 则跳出循环
- en: Update the weight to ![](img/B15558_18_034.png)
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重为 ![](img/B15558_18_034.png)
- en: Return the target policy ![](img/B15558_03_139.png)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回目标策略 ![](img/B15558_03_139.png)
- en: TD Prediction
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD 预测
- en: 'The algorithm for the TD prediction method is given as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: TD预测方法的算法如下：
- en: Initialize the value function *V*(*s*) with random values. A policy ![](img/B15558_03_084.png)
    is given.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化值函数 *V*(*s*)。给定一个策略 ![](img/B15558_03_084.png)。
- en: 'For each episode:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一集：
- en: Initialize the state *s*
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态 *s*
- en: 'For each step in the episode:'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤：
- en: Perform an action *a* in the state *s* according to given policy ![](img/B15558_03_055.png),
    get the reward *r*, and move to the next state ![](img/B15558_12_264.png)
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据给定策略 ![](img/B15558_03_055.png) 在状态 *s* 中执行动作 *a*，获得奖励 *r*，并移动到下一个状态 ![](img/B15558_12_264.png)
- en: Update the value of the state to ![](img/B15558_18_039.png)
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新状态的值为 ![](img/B15558_18_039.png)
- en: Update ![](img/B15558_18_040.png) (this step implies we are changing the next
    state ![](img/B15558_18_041.png) to the current state *s*)
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_18_040.png) （这一步表示我们将下一个状态 ![](img/B15558_18_041.png) 更改为当前状态
    *s*）
- en: If *s* is not a terminal state, repeat *steps 1* to *4*
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *s* 不是终止状态，重复执行 *步骤 1* 到 *4*
- en: On-Policy TD Control – SARSA
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于策略的TD控制 – SARSA
- en: 'The algorithm for on-policy TD control – SARSA is given as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的TD控制 – SARSA 的算法如下：
- en: Initialize the Q function *Q*(*s*, *a*) with random values
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 函数 *Q*(*s*, *a*)，赋值为随机值
- en: 'For each episode:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一集：
- en: Initialize the state *s*
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态 *s*
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in the
    state *s*
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Q*(*s*, *a*) 中提取一个策略，并选择一个动作 *a* 在状态 *s* 中执行
- en: 'For each step in the episode:'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤：
- en: Perform the action *a*, move to the new state ![](img/B15558_18_041.png), and
    observe the reward *r*
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a*，移动到新状态 ![](img/B15558_18_041.png)，并观察奖励 *r*
- en: In the state ![](img/B15558_18_043.png), select the action ![](img/B15558_18_044.png)
    using the epsilon-greedy policy
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在状态 ![](img/B15558_18_043.png) 下，使用 epsilon-greedy 策略选择动作 ![](img/B15558_18_044.png)
- en: Update the Q value to ![](img/B15558_18_045.png)
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Q 值为 ![](img/B15558_18_045.png)
- en: Update ![](img/B15558_18_046.png) and ![](img/B15558_18_047.png) (update the
    next state ![](img/B15558_03_021.png)-action ![](img/B15558_18_049.png) pair to the
    current state *s*-action *a* pair)
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_18_046.png) 和 ![](img/B15558_18_047.png) （更新下一个状态 ![](img/B15558_03_021.png)-动作
    ![](img/B15558_18_049.png) 对为当前状态 *s*-动作 *a* 对）
- en: If *s* is not the terminal state, repeat *steps 1* to *5*
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *s* 不是终止状态，重复执行 *步骤 1* 到 *5*
- en: Off-Policy TD Control – Q Learning
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离策略的TD控制 – Q 学习
- en: 'The algorithm for off-policy TD control – Q learning is given as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 离策略的TD控制 – Q 学习的算法如下：
- en: Initialize a Q function *Q*(*s*, *a*) with random values
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 函数 *Q*(*s*, *a*)，赋值为随机值
- en: 'For each episode:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一集：
- en: Initialize the state *s*
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化状态 *s*
- en: 'For each step in the episode:'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤：
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in the
    state *s*
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *Q*(*s*, *a*) 中提取一个策略，并选择一个动作 *a* 在状态 *s* 中执行
- en: Perform the action *a*, move to the new state ![](img/B15558_18_050.png), and
    observe the reward *r*
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a*，移动到新状态 ![](img/B15558_18_050.png)，并观察奖励 *r*
- en: Update the Q value to ![](img/B15558_18_051.png)
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Q 值为 ![](img/B15558_18_051.png)
- en: Update ![](img/B15558_18_052.png) (update the next state ![](img/B15558_18_053.png)
    to the current state *s*)
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_18_052.png) （更新下一个状态 ![](img/B15558_18_053.png) 为当前状态 *s*）
- en: If *s* is not a terminal state, repeat *steps 1* to *5*
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *s* 不是终止状态，重复执行 *步骤 1* 到 *5*
- en: Deep Q Learning
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 学习
- en: 'The algorithm for deep Q learning is given as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习算法如下所示：
- en: Initialize the main network parameter ![](img/B15558_10_037.png) with random
    values
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化主网络参数 ![](img/B15558_10_037.png)
- en: Initialize the target network parameter ![](img/B15558_09_061.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_098.png) 初始化目标网络参数 ![](img/B15558_09_061.png)
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_088.png)
- en: For *N* number of episodes, perform *step 5*
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次训练周期，执行 *步骤 5*
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤，即对于 *t* = 0, . . ., *T* – 1：
- en: Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select random action *a*, and with probability
    1-epsilon, select the action as ![](img/B15558_09_072.png)
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察状态 *s* 并使用 epsilon-贪婪策略选择动作，即以 epsilon 的概率选择随机动作 *a*，以 1-epsilon 的概率选择动作 ![](img/B15558_09_072.png)
- en: Perform the selected action and move to the next state ![](img/B15558_14_036.png)
    and obtain the reward *r*
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作并转移到下一个状态 ![](img/B15558_14_036.png)，并获得奖励 *r*
- en: Store the transition information in the replay buffer ![](img/B15558_09_075.png)
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转移信息存储在重放缓冲区 ![](img/B15558_09_075.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_259.png)
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_12_259.png) 随机抽取一个 *K* 转移的小批量
- en: Compute the target value, that is, ![](img/B15558_18_062.png)
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标值，即 ![](img/B15558_18_062.png)
- en: Compute the loss, ![](img/B15558_09_035.png)
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失，![](img/B15558_09_035.png)
- en: Compute the gradients of the loss and update the main network parameter ![](img/B15558_12_330.png)
    using gradient descent, ![](img/B15558_18_065.png)
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度，并使用梯度下降法更新主网络参数 ![](img/B15558_12_330.png)，![](img/B15558_18_065.png)
- en: Freeze the target network parameter ![](img/B15558_18_066.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_054.png)
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结目标网络参数 ![](img/B15558_18_066.png) 若干时间步，然后通过复制主网络参数 ![](img/B15558_09_054.png)
    更新它
- en: Double DQN
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重 DQN
- en: 'The algorithm for double DQN is given as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 双重 DQN 算法如下所示：
- en: Initialize the main network parameter ![](img/B15558_09_054.png) with random
    values
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化主网络参数 ![](img/B15558_09_054.png)
- en: Initialize the target network parameter ![](img/B15558_09_059.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_098.png) 初始化目标网络参数 ![](img/B15558_09_059.png)
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_12_259.png)
- en: For *N* number of episodes, repeat *step 5*
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次训练周期，重复 *步骤 5*
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤，即对于 *t* = 0, . . ., *T* – 1：
- en: Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select random action *a* with probability 1-epsilon,
    and select the action as ![](img/B15558_18_072.png)
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察状态 *s* 并使用 epsilon-贪婪策略选择动作，即以 epsilon 的概率选择随机动作 *a*，以 1-epsilon 的概率选择动作 ![](img/B15558_18_072.png)
- en: Perform the selected action and move to the next state ![](img/B15558_18_073.png)
    and obtain the reward *r*
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作并转移到下一个状态 ![](img/B15558_18_073.png)，并获得奖励 *r*
- en: Store the transition information in the replay buffer ![](img/B15558_18_074.png)
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转移信息存储在重放缓冲区 ![](img/B15558_18_074.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_14_098.png)
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_14_098.png) 随机抽取一个 *K* 转移的小批量
- en: Compute the target value, that is, ![](img/B15558_18_076.png)
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标值，即 ![](img/B15558_18_076.png)
- en: Compute the loss, ![](img/B15558_09_035.png)
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失，![](img/B15558_09_035.png)
- en: 'Compute the gradients of the loss and update the main network parameter ![](img/B15558_09_087.png)
    using gradient descent: ![](img/B15558_18_079.png)'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度，并使用梯度下降法更新主网络参数 ![](img/B15558_09_087.png)，![](img/B15558_18_079.png)
- en: Freeze the target network parameter ![](img/B15558_09_061.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_10_066.png)
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结目标网络参数 ![](img/B15558_09_061.png) 若干时间步，然后通过复制主网络参数 ![](img/B15558_10_066.png)
    更新它
- en: REINFORCE Policy Gradient
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: REINFORCE 策略梯度
- en: 'The algorithm for REINFORCE policy gradient is given as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 策略梯度算法如下所示：
- en: Initialize the network parameter ![](img/B15558_15_152.png) with random values
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化网络参数 ![](img/B15558_15_152.png)
- en: Generate some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_13_124.png)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略 ![](img/B15558_13_124.png) 生成一些 *N* 条轨迹 ![](img/B15558_10_058.png)
- en: Compute the return of the trajectory ![](img/B15558_18_085.png)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算轨迹的回报 ![](img/B15558_18_085.png)
- en: Compute the gradients ![](img/B15558_18_086.png)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度 ![](img/B15558_18_086.png)
- en: Update the network parameter, ![](img/B15558_11_005.png)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新网络参数，![](img/B15558_11_005.png)
- en: Repeat *steps 2* to *5* for several iterations
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 5* 多次迭代
- en: Policy Gradient with Reward-To-Go
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带奖励到达的策略梯度
- en: 'The algorithm for policy gradient with reward-to-go is given as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 带奖励到达的策略梯度算法如下：
- en: Initialize the network parameter ![](img/B15558_09_098.png) with random values
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化网络参数 ![](img/B15558_09_098.png)
- en: Generate some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_10_111.png)
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略 ![](img/B15558_10_111.png) 生成一些 *N* 条轨迹 ![](img/B15558_10_058.png)
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励到达） *R*[t]
- en: Compute the gradients ![](img/B15558_10_128.png)
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度 ![](img/B15558_10_128.png)
- en: 'Update the network parameter: ![](img/B15558_18_092.png)'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新网络参数：![](img/B15558_18_092.png)
- en: Repeat *steps 2* to *5* for several iterations
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 5* 多次迭代
- en: REINFORCE with Baseline
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: REINFORCE 带基线
- en: 'The algorithm for REINFORCE with baseline is given as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 带基线的算法如下：
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_13_234.png)
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_09_098.png) 和价值网络参数 ![](img/B15558_13_234.png)
- en: Generate some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_13_124.png)
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略 ![](img/B15558_13_124.png) 生成一些 *N* 条轨迹 ![](img/B15558_10_058.png)
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励到达） *R*[t]
- en: Compute the policy gradient, ![](img/B15558_10_163.png)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算策略梯度，![](img/B15558_10_163.png)
- en: Update the policy network parameter ![](img/B15558_09_098.png) using gradient
    ascent, ![](img/B15558_18_099.png)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升更新策略网络参数 ![](img/B15558_09_098.png)，![](img/B15558_18_099.png)
- en: Compute the mean squared error of the value network, ![](img/B15558_18_100.png)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算价值网络的均方误差，![](img/B15558_18_100.png)
- en: Update the value network parameter ![](img/B15558_12_213.png) using gradient
    descent, ![](img/B15558_11_013.png)
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新价值网络参数 ![](img/B15558_12_213.png)，![](img/B15558_11_013.png)
- en: Repeat *steps 2* to *7* for several iterations
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 7* 多次迭代
- en: Advantage Actor Critic
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势演员评论员
- en: 'The algorithm for the advantage actor critic method is given as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 优势演员评论员方法的算法如下：
- en: Initialize the actor network parameter ![](img/B15558_10_037.png) and critic
    network parameter ![](img/B15558_10_148.png)
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化演员网络参数 ![](img/B15558_10_037.png) 和评论员网络参数 ![](img/B15558_10_148.png)
- en: For *N* number of episodes, repeat *step 3*
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 条剧集，重复 *步骤 3*
- en: 'For each step in the episode, that is, for, *t* = 0, . . ., *T* – 1:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个时间步，即 *t* = 0, . . ., *T* – 1：
- en: Select an action using the policy ![](img/B15558_11_017.png)
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_11_017.png) 选择一个动作
- en: Take the action *a*[t] in the state *s*[t], and observe the reward *r* and move
    to the next state ![](img/B15558_18_106.png)
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在状态 *s*[t] 中采取动作 *a*[t]，观察奖励 *r* 并转到下一个状态 ![](img/B15558_18_106.png)
- en: 'Compute the policy gradients: ![](img/B15558_11_009.png)'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算策略梯度：![](img/B15558_11_009.png)
- en: 'Update the actor network parameter ![](img/B15558_09_054.png) using gradient
    ascent: ![](img/B15558_11_005.png)'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升更新演员网络参数 ![](img/B15558_09_054.png)：![](img/B15558_11_005.png)
- en: 'Compute the loss of the critic network: ![](img/B15558_18_110.png)'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员网络的损失：![](img/B15558_18_110.png)
- en: 'Compute gradients ![](img/B15558_10_093.png) and update the critic network
    parameter ![](img/B15558_12_371.png) using gradient descent: ![](img/B15558_11_013.png)'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度 ![](img/B15558_10_093.png) 并使用梯度下降更新评论员网络参数 ![](img/B15558_12_371.png)：![](img/B15558_11_013.png)
- en: Asynchronous Advantage Actor-Critic
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步优势演员评论员
- en: 'The steps involved in **Advantage Actor-Critic** (**A3C**) are given below:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势演员评论员**（**A3C**）的步骤如下：'
- en: The worker agent interacts with its own copies of the environment
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作代理与其自己的环境副本交互
- en: Each worker follows a different policy and collects the experience
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个工作代理遵循不同的策略并收集经验
- en: Next, the worker agents compute the loss of the actor and critic networks
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，工作代理计算演员和评论员网络的损失
- en: After computing the loss, they calculates the gradients of the loss and sends
    those gradients to the global agent asynchronously
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算损失后，他们计算损失的梯度，并异步地将这些梯度发送给全局代理
- en: The global agent updates its parameter with the gradients received from the
    worker agents
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全局代理使用从工作代理接收到的梯度来更新其参数
- en: Now, the updated parameter from the global agent will be sent to the worker
    agents periodically
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，全局代理更新的参数将定期发送到工作代理
- en: Deep Deterministic Policy Gradient
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: 'The algorithm for **Deep Deterministic Policy Gradient** (**DDPG**) is given
    as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度确定性策略梯度** (**DDPG**) 算法如下所示：'
- en: Initialize the main critic network parameter ![](img/B15558_09_118.png) and
    main actor network parameter ![](img/B15558_13_234.png)
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主评论员网络参数 ![](img/B15558_09_118.png) 和主演员网络参数 ![](img/B15558_13_234.png)
- en: Initialize the target critic network parameter ![](img/B15558_14_246.png) by
    just copying the main critic network parameter ![](img/B15558_09_123.png)
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过仅复制主评论员网络参数 ![](img/B15558_09_123.png) 来初始化目标评论员网络参数 ![](img/B15558_14_246.png)
- en: Initialize the target actor network parameter ![](img/B15558_18_117.png) by
    just copying the main actor network parameter ![](img/B15558_13_234.png).
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过仅复制主演员网络参数 ![](img/B15558_13_234.png) 来初始化目标演员网络参数 ![](img/B15558_18_117.png)。
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_088.png)
- en: For *N* number of episodes, repeat *steps 6* to *7*
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次实验，重复 *第6步* 到 *第7步*
- en: Initialize an Ornstein-Uhlenbeck random process ![](img/B15558_18_120.png) for
    action space exploration
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Ornstein-Uhlenbeck 随机过程 ![](img/B15558_18_120.png) 以进行动作空间的探索
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤，即对于 *t* = 0, . . ., *T* – 1：
- en: Select action *a* based on the policy ![](img/B15558_18_121.png) and exploration
    noise, that is, ![](img/B15558_12_061.png)
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略 ![](img/B15558_18_121.png) 和探索噪声选择动作 *a*，即 ![](img/B15558_12_061.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_14_105.png)
    and get the reward *r*, and store this transition information in the replay buffer
    ![](img/B15558_12_259.png)
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选定的动作 *a*，移动到下一个状态 ![](img/B15558_14_105.png) 并获得奖励 *r*，并将此转换信息存储在重放缓冲区 ![](img/B15558_12_259.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_075.png)
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_09_075.png) 随机抽取一个 *K* 的过渡小批量
- en: Compute the target value of the critic, that is, ![](img/B15558_12_095.png)
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员的目标值，即 ![](img/B15558_12_095.png)
- en: Compute the loss of the critic network ![](img/B15558_12_047.png)
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员网络的损失 ![](img/B15558_12_047.png)
- en: Compute the gradient of the loss ![](img/B15558_10_028.png) and update the critic
    network parameter using gradient descent, ![](img/B15558_18_129.png)
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度 ![](img/B15558_10_028.png) 并使用梯度下降更新评论员网络参数，![](img/B15558_18_129.png)
- en: Compute the gradient of the actor network ![](img/B15558_10_093.png) and update
    the actor network parameter using gradient ascent, ![](img/B15558_18_131.png)
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算演员网络的梯度 ![](img/B15558_10_093.png) 并使用梯度上升更新演员网络参数，![](img/B15558_18_131.png)
- en: Update the target critic and target actor network parameters, ![](img/B15558_18_132.png)
    and ![](img/B15558_18_133.png)
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标评论员和目标演员网络参数，![](img/B15558_18_132.png) 和 ![](img/B15558_18_133.png)
- en: Twin Delayed DDPG
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双延迟DDPG
- en: 'The algorithm for **Twin Delayed DDPG** (**TD3**) is given as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**双延迟DDPG** (**TD3**) 算法如下所示：'
- en: Initialize two main critic networks parameters, ![](img/B15558_12_211.png) and
    ![](img/B15558_12_217.png), and the main actor network parameter ![](img/B15558_12_213.png)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化两个主评论员网络参数，![](img/B15558_12_211.png) 和 ![](img/B15558_12_217.png)，以及主演员网络参数
    ![](img/B15558_12_213.png)
- en: Initialize two target critic networks parameters, ![](img/B15558_12_214.png)
    and ![](img/B15558_12_320.png), by copying the main critic network parameters
    ![](img/B15558_18_139.png) and ![](img/B15558_12_217.png), respectively
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分别复制主评论员网络参数 ![](img/B15558_18_139.png) 和 ![](img/B15558_12_217.png) 来初始化两个目标评论员网络参数
    ![](img/B15558_12_214.png) 和 ![](img/B15558_12_320.png)
- en: Initialize the target actor network parameter ![](img/B15558_18_141.png) by
    copying the main actor network parameter ![](img/B15558_12_213.png)
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主演员网络参数 ![](img/B15558_12_213.png) 来初始化目标演员网络参数 ![](img/B15558_18_141.png)
- en: Initialize the replay buffer ![](img/B15558_18_143.png)
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_18_143.png)
- en: For *N* number of episodes, repeat *step 6*
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次实验，重复 *第6步*
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个步骤，即对于 *t* = 0, . . ., *T* – 1：
- en: Select action *a* based on the policy ![](img/B15558_18_121.png) and with exploration
    noise ![](img/B15558_18_145.png), that is, ![](img/B15558_12_224.png) where, ![](img/B15558_18_147.png)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略 ![](img/B15558_18_121.png) 和探索噪声 ![](img/B15558_18_145.png) 选择动作 *a*，即
    ![](img/B15558_12_224.png)，其中，![](img/B15558_18_147.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_18_043.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选择的动作 *a*，移动到下一个状态 ![](img/B15558_18_043.png)，获得奖励 *r*，并将转换信息存储在重放缓冲区 ![](img/B15558_09_124.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_374.png)
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_12_374.png) 随机抽取 *K* 个转换的小批量
- en: Select the action ![](img/B15558_18_151.png) for computing the target value
    ![](img/B15558_18_152.png) where ![](img/B15558_12_269.png)
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作 ![](img/B15558_18_151.png) 来计算目标值 ![](img/B15558_18_152.png)，其中 ![](img/B15558_12_269.png)
- en: Compute the target value of the critic, that is, ![](img/B15558_12_230.png)
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论网络的目标值，即 ![](img/B15558_12_230.png)
- en: Compute the loss of the critic network ![](img/B15558_12_227.png).
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论网络的损失 ![](img/B15558_12_227.png)
- en: Compute the gradients of the loss ![](img/B15558_12_234.png) and minimize the
    loss using gradient descent, ![](img/B15558_12_235.png)
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度 ![](img/B15558_12_234.png)，并使用梯度下降法最小化损失 ![](img/B15558_12_235.png)
- en: 'If *t* mod *d* =0, then:'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* =0，则：
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png) and
    update the actor network parameter using gradient ascent, ![](img/B15558_18_131.png)
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_10_093.png)，并使用梯度上升法更新演员网络参数 ![](img/B15558_18_131.png)
- en: Update the target critic networks parameter and target actor network parameter
    as ![](img/B15558_18_132.png) and ![](img/B15558_18_161.png), respectively
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标评论网络参数和目标演员网络参数，分别为 ![](img/B15558_18_132.png) 和 ![](img/B15558_18_161.png)
- en: Soft Actor-Critic
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Soft Actor-Critic
- en: 'The algorithm for **Soft Actor-Critic** (**SAC**) is given as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**Soft Actor-Critic** (**SAC**) 算法如下：'
- en: Initialize the main value network parameter ![](img/B15558_12_302.png), the
    Q network parameters ![](img/B15558_12_216.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_14_245.png)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主价值网络参数 ![](img/B15558_12_302.png)、Q网络参数 ![](img/B15558_12_216.png) 和 ![](img/B15558_12_206.png)，以及演员网络参数
    ![](img/B15558_14_245.png)
- en: Initialize the target value network ![](img/B15558_12_364.png) by just copying
    the main value network parameter ![](img/B15558_12_302.png)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主价值网络参数 ![](img/B15558_12_302.png) 来初始化目标价值网络 ![](img/B15558_12_364.png)
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_088.png)
- en: For *N* number of episodes, repeat *step 5*
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 个回合，重复 *步骤 5*
- en: For each step in the episode, that is, for *t* = 0, . . ., *T* – 1
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合中的步骤，即对于 *t* = 0, . . ., *T* – 1
- en: Select action *a* based on the policy ![](img/B15558_18_169.png), that is, ![](img/B15558_18_170.png)
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于策略 ![](img/B15558_18_169.png) 选择动作 *a*，即 ![](img/B15558_18_170.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_09_126.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选择的动作 *a*，移动到下一个状态 ![](img/B15558_09_126.png)，获得奖励 *r*，并将转换信息存储在重放缓冲区 ![](img/B15558_09_124.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区随机抽取 *K* 个转换的小批量
- en: Compute target state value ![](img/B15558_12_380.png)
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标状态值 ![](img/B15558_12_380.png)
- en: Compute the loss of value network ![](img/B15558_12_323.png) and update the
    parameter using gradient descent, ![](img/B15558_18_175.png)
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算价值网络的损失 ![](img/B15558_12_323.png)，并使用梯度下降法更新参数 ![](img/B15558_18_175.png)
- en: Compute the target Q value ![](img/B15558_18_176.png)
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标Q值 ![](img/B15558_18_176.png)
- en: Compute the loss of the Q networks ![](img/B15558_12_383.png) and update the
    parameter using gradient descent, ![](img/B15558_12_386.png)
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算Q网络的损失 ![](img/B15558_12_383.png)，并使用梯度下降法更新参数 ![](img/B15558_12_386.png)
- en: Compute gradients of the actor objective function,![](img/B15558_18_179.png)
    and update the parameter using gradient ascent, ![](img/B15558_18_180.png)
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算演员目标函数的梯度 ![](img/B15558_18_179.png)，并使用梯度上升法更新参数 ![](img/B15558_18_180.png)
- en: Update the target value network parameter, ![](img/B15558_18_181.png)
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标值网络参数 ![](img/B15558_18_181.png)
- en: Trust Region Policy Optimization
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任域策略优化
- en: 'The algorithm for **Trust Region Policy Optimization** (**TRPO**) is given
    as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**信任域策略优化** (**TRPO**) 算法如下：'
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_18_183.png)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_09_098.png) 和价值网络参数 ![](img/B15558_18_183.png)
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_111.png)
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成 *N* 条轨迹 ![](img/B15558_10_058.png)，遵循策略 ![](img/B15558_10_111.png)
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励累积）*R*[t]
- en: Compute the advantage value *A*[t]
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算优势值 *A*[t]
- en: Compute the policy gradients ![](img/B15558_13_232.png)
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算策略梯度 ![](img/B15558_13_232.png)
- en: Compute ![](img/B15558_13_238.png) using the conjugate gradient method
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用共轭梯度法计算 ![](img/B15558_13_238.png)
- en: Update the policy network parameter ![](img/B15558_09_098.png) using the update
    rule ![](img/B15558_13_240.png)
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更新规则 ![](img/B15558_13_240.png) 更新策略网络参数 ![](img/B15558_09_098.png)
- en: Compute the mean squared error of the value network, ![](img/B15558_18_100.png)
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的均方误差，![](img/B15558_18_100.png)
- en: Update the value network parameter ![](img/B15558_13_289.png) using gradient
    descent, ![](img/B15558_10_150.png)
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新值网络参数 ![](img/B15558_13_289.png)，![](img/B15558_10_150.png)
- en: Repeat *steps 2* to *9* for several iterations
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 9* 进行多次迭代
- en: PPO-Clipped
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO-Clipped
- en: 'The algorithm for the PPO-clipped method is given as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: PPO-Clip方法的算法如下所示：
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_10_152.png)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_09_098.png) 和值网络参数 ![](img/B15558_10_152.png)
- en: Collect some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_10_036.png)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一些 *N* 条轨迹 ![](img/B15558_10_058.png)，跟随策略 ![](img/B15558_10_036.png)
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励累积）*R*[t]
- en: Compute the gradient of the objective function ![](img/B15558_09_043.png)
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_09_043.png)
- en: Update the policy network parameter ![](img/B15558_09_098.png) using gradient
    ascent, ![](img/B15558_13_316.png)
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升更新策略网络参数 ![](img/B15558_09_098.png)，![](img/B15558_13_316.png)
- en: Compute the mean squared error of the value network, ![](img/B15558_10_166.png)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的均方误差，![](img/B15558_10_166.png)
- en: Compute the gradient of the value network ![](img/B15558_10_093.png)
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的梯度 ![](img/B15558_10_093.png)
- en: Update the value network parameter ![](img/B15558_13_289.png) using gradient
    descent, ![](img/B15558_10_150.png)
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新值网络参数 ![](img/B15558_13_289.png)，![](img/B15558_10_150.png)
- en: Repeat *steps 2* to *8* for several iterations
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 8* 进行多次迭代
- en: PPO-Penalty
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO-Penalty
- en: 'The algorithm for the PPO-penalty method is given as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: PPO-惩罚方法的算法如下所示：
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_14_249.png) and initialize the penalty coefficient
    ![](img/B15558_13_309.png) and the target KL divergence ![](img/B15558_18_207.png)
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_09_098.png) 和值网络参数 ![](img/B15558_14_249.png)，并初始化惩罚系数
    ![](img/B15558_13_309.png) 和目标KL散度 ![](img/B15558_18_207.png)
- en: 'For iterations ![](img/B15558_18_208.png):'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迭代次数 ![](img/B15558_18_208.png)：
- en: Collect some *N* number of trajectories following the policy ![](img/B15558_13_124.png)
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一些 *N* 条轨迹，跟随策略 ![](img/B15558_13_124.png)
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励累积）*R*[t]
- en: Compute ![](img/B15558_18_210.png)
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 ![](img/B15558_18_210.png)
- en: Compute the gradient of the objective function ![](img/B15558_18_211.png)
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_18_211.png)
- en: Update the policy network parameter ![](img/B15558_18_212.png) using gradient
    ascent, ![](img/B15558_13_286.png)
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升更新策略网络参数 ![](img/B15558_18_212.png)，![](img/B15558_13_286.png)
- en: If *d* is greater than or equal to ![](img/B15558_18_214.png), then we set ![](img/B15558_18_215.png);
    if *d* is lesser than or equal to ![](img/B15558_18_216.png), then we set, ![](img/B15558_18_217.png)
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *d* 大于或等于 ![](img/B15558_18_214.png)，则设置 ![](img/B15558_18_215.png)；如果 *d*
    小于或等于 ![](img/B15558_18_216.png)，则设置，![](img/B15558_18_217.png)
- en: 'Compute the mean squared error of the value network: ![](img/B15558_10_166.png)'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的均方误差：![](img/B15558_10_166.png)
- en: Compute the gradients of the value network ![](img/B15558_10_093.png)
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的梯度 ![](img/B15558_10_093.png)
- en: Update the value network parameter ![](img/B15558_13_234.png) using gradient
    descent, ![](img/B15558_10_150.png)
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新值网络参数 ![](img/B15558_13_234.png)，![](img/B15558_10_150.png)
- en: Categorical DQN
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别DQN
- en: 'The algorithm for a categorical DQN is given as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 类别DQN的算法如下所示：
- en: Initialize the main network parameter ![](img/B15558_09_098.png) with random
    values
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用随机值初始化主网络参数 ![](img/B15558_09_098.png)
- en: Initialize the target network parameter ![](img/B15558_12_025.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_098.png) 来初始化目标网络参数 ![](img/B15558_12_025.png)
- en: Initialize the replay buffer ![](img/B15558_15_027.png), the number of atoms,
    and also ![](img/B15558_18_226.png) and ![](img/B15558_18_227.png)
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_15_027.png)，原子数，以及 ![](img/B15558_18_226.png) 和 ![](img/B15558_18_227.png)
- en: For *N* number of episodes, perform *step 5*
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次实验，执行 *步骤 5*
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步骤，即对于 *t* = 0, . . ., *T* – 1：
- en: Feed the state *s* and support values to the main categorical DQN parameterized
    by ![](img/B15558_09_098.png), and get the probability value for each support
    value. Then compute the Q value as ![](img/B15558_14_103.png).
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态 *s* 和支持值输入主分类 DQN，参数化为 ![](img/B15558_09_098.png)，并获得每个支持值的概率值。然后计算 Q 值为
    ![](img/B15558_14_103.png)。
- en: After computing the Q value, select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select a random action *a* and with probability
    1-epsilon, select an action as ![](img/B15558_18_230.png).
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 Q 值后，使用 epsilon-greedy 策略选择一个动作，即以概率 epsilon 选择一个随机动作 *a*，并以概率 1-epsilon
    选择一个动作，如 ![](img/B15558_18_230.png)。
- en: Perform the selected action and move to the next state ![](img/B15558_12_376.png)
    and obtain the reward *r.*
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选定的动作并移动到下一个状态 ![](img/B15558_12_376.png)，并获得奖励 *r*。
- en: Store the transition information in the replay buffer ![](img/B15558_09_088.png).
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将过渡信息存储在重放缓冲区 ![](img/B15558_09_088.png) 中。
- en: Randomly sample a transition from the replay buffer ![](img/B15558_09_124.png).
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_09_124.png) 随机抽取一个过渡。
- en: Feed the next state ![](img/B15558_12_376.png) and support values to the target
    categorical DQN parameterized by ![](img/B15558_12_025.png) and get the probability
    value for each support. Then compute the value as ![](img/B15558_18_236.png).
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下一个状态 ![](img/B15558_12_376.png) 和支持值输入目标分类 DQN，参数化为 ![](img/B15558_12_025.png)，并获得每个支持的概率值。然后计算值为
    ![](img/B15558_18_236.png)。
- en: After computing the Q value, we select the best action in the state ![](img/B15558_12_376.png)
    as the one that has the maximum Q value ![](img/B15558_14_112.png).
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 Q 值后，我们在状态 ![](img/B15558_12_376.png) 中选择具有最大 Q 值 ![](img/B15558_14_112.png)
    的最佳动作。
- en: Initialize the array *m* with zero values with its shape as the number of support.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用零值初始化数组 *m*，其形状为支持值的数量。
- en: 'For *j* in the range of the number of support values:'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *j* 在支持值的范围内：
- en: 'Compute the target support value: ![](img/B15558_18_239.png)'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支持值： ![](img/B15558_18_239.png)
- en: 'Compute the value of *b*: ![](img/B15558_18_240.png)'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *b* 的值： ![](img/B15558_18_240.png)
- en: 'Compute the lower bound and upper bound: ![](img/B15558_14_047.png)'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算下界和上界： ![](img/B15558_14_047.png)
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_116.png)'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下界上分布概率： ![](img/B15558_14_116.png)
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_117.png)'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上界上分布概率： ![](img/B15558_14_117.png)
- en: Compute the cross entropy loss ![](img/B15558_14_131.png).
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算交叉熵损失 ![](img/B15558_14_131.png)。
- en: Minimize the loss using gradient descent and update the parameter of the main
    network
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降最小化损失并更新主网络的参数
- en: Freeze the target network parameter ![](img/B15558_18_066.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结目标网络参数 ![](img/B15558_18_066.png) 若干时间步，然后通过复制主网络参数 ![](img/B15558_09_098.png)
    来更新它
- en: Distributed Distributional DDPG
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式分布式 DDPG
- en: 'The **Distributed Distributional Deep Deterministic Policy Gradient** (**D4PG**)
    algorithm is given as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式分布式深度确定性策略梯度** (**D4PG**) 算法如下所示：'
- en: Initialize the critic network parameter ![](img/B15558_09_054.png) and the actor
    network parameter ![](img/B15558_18_248.png)
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化评论家网络参数 ![](img/B15558_09_054.png) 和演员网络参数 ![](img/B15558_18_248.png)
- en: Initialize the target critic network parameter ![](img/B15558_18_066.png) and
    the target actor network parameter ![](img/B15558_18_250.png) by copying from
    ![](img/B15558_09_098.png) and ![](img/B15558_12_283.png), respectively
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化目标评论家网络参数 ![](img/B15558_18_066.png) 和目标演员网络参数 ![](img/B15558_18_250.png)，通过从
    ![](img/B15558_09_098.png) 和 ![](img/B15558_12_283.png) 复制初始化。
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_09_088.png)
- en: Launch *L* number of actors
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 *L* 个演员
- en: For *N* number of episodes, repeat *step 6*
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次实验，重复 *步骤 6*
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步骤，即对于 *t* = 0, . . ., *T* – 1：
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_088.png)
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_09_088.png) 随机抽取一个 *K* 的小批量过渡
- en: Compute the target value distribution of the critic, that is, ![](img/B15558_18_255.png)
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员的目标值分布，即 ![](img/B15558_18_255.png)
- en: Compute the loss of the critic network and calculate the gradient as ![](img/B15558_14_229.png)
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员网络的损失并计算梯度，如 ![](img/B15558_14_229.png) 所示
- en: 'After computing gradients, update the critic network parameter using gradient
    descent: ![](img/B15558_12_052.png)'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度后，使用梯度下降更新评论员网络参数： ![](img/B15558_12_052.png)
- en: Compute the gradient of the actor network ![](img/B15558_11_014.png)
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算演员网络的梯度 ![](img/B15558_11_014.png)
- en: 'Update the actor network parameter by gradient ascent: ![](img/B15558_12_068.png)'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过梯度上升更新演员网络参数： ![](img/B15558_12_068.png)
- en: 'If *t* mod ![](img/B15558_18_260.png) then:'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod ![](img/B15558_18_260.png) 则：
- en: Update the target critic and target actor network parameters using soft replacement
    as ![](img/B15558_18_261.png) and ![](img/B15558_12_077.png), respectively
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用软替换更新目标评论员和目标演员网络的参数，如 ![](img/B15558_18_261.png) 和 ![](img/B15558_12_077.png)
    所示
- en: 'If *t* mod ![](img/B15558_14_261.png) then:'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod ![](img/B15558_14_261.png) 则：
- en: Replicate the network weights to the actors
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将网络权重复制到演员中
- en: 'We perform the following steps in the actor network:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在演员网络中执行以下步骤：
- en: Select action *a* based on the policy ![](img/B15558_14_262.png) and exploration
    noise, that is, ![](img/B15558_18_265.png)
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于策略 ![](img/B15558_14_262.png) 和探索噪声选择动作 *a*，即 ![](img/B15558_18_265.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_016.png)
    and get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_12_259.png)
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选择的动作 *a*，移动到下一个状态 ![](img/B15558_12_016.png) 并获得奖励 *r*，将过渡信息存储在重放缓冲区 ![](img/B15558_12_259.png)
- en: Repeat *steps 1* and *2* until the learner finishes
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行 *步骤 1* 和 *2*，直到学习者完成
- en: DAgger
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DAgger
- en: 'The algorithm for DAgger is given as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: DAgger 算法如下：
- en: Initialize an empty dataset ![](img/B15558_18_268.png)
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化空数据集 ![](img/B15558_18_268.png)
- en: Initialize a policy ![](img/B15558_18_269.png)
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个策略 ![](img/B15558_18_269.png)
- en: 'For iterations *i* = 1 to *N*:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迭代 *i* = 1 到 *N*：
- en: Create a policy ![](img/B15558_18_270.png).
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个策略 ![](img/B15558_18_270.png)。
- en: Generate a trajectory using the policy ![](img/B15558_15_086.png).
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_15_086.png) 生成一个轨迹。
- en: Create a dataset ![](img/B15558_18_272.png) by collecting states visited by
    the policy ![](img/B15558_18_273.png) and the actions of those states provided
    by the expert ![](img/B15558_15_053.png). Thus, ![](img/B15558_15_090.png).
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过收集策略 ![](img/B15558_18_273.png) 访问的状态和专家提供的这些状态的动作来创建数据集 ![](img/B15558_18_272.png)。因此，
    ![](img/B15558_15_090.png)。
- en: Aggregate the dataset as ![](img/B15558_18_276.png).
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集汇总为 ![](img/B15558_18_276.png)。
- en: Train a classifier on the updated dataset ![](img/B15558_12_259.png) and extract
    a new policy ![](img/B15558_18_278.png).
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新后的数据集 ![](img/B15558_12_259.png) 上训练分类器，并提取新策略 ![](img/B15558_18_278.png)。
- en: Deep Q learning from demonstrations
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 学习来自演示
- en: 'The algorithm for **Deep Q Learning from Demonstrations** (**DQfD**) is given
    as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自演示的深度 Q 学习**（**DQfD**）的算法如下：'
- en: Initialize the main network parameter ![](img/B15558_09_098.png)
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主网络参数 ![](img/B15558_09_098.png)
- en: Initialize the target network parameter ![](img/B15558_18_280.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_098.png) 初始化目标网络参数 ![](img/B15558_18_280.png)
- en: Initialize the replay buffer ![](img/B15558_09_124.png) with the expert demonstrations
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用专家演示初始化重放缓冲区 ![](img/B15558_09_124.png)
- en: Set *d*, the number of time steps we want to delay updating the target network
    parameter
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *d*，即我们希望延迟更新目标网络参数的时间步数
- en: '**Pre-training phase**: For steps *t* = 1, 2, . . ., *T*:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练阶段**：对于步骤 *t* = 1, 2, . . ., *T*：'
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_088.png)
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_12_088.png) 中抽取一个小批量经验
- en: Compute the loss *J*(*Q*)
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失 *J*(*Q*)
- en: Update the parameter of the network using gradient descent
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新网络参数
- en: 'If *t* mod *d* = 0:'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0：
- en: Update the target network parameter ![](img/B15558_18_284.png) by copying the
    main network parameter ![](img/B15558_09_098.png)
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_098.png) 更新目标网络参数 ![](img/B15558_18_284.png)
- en: '**Training phase**: For steps *t* =1, 2, . . ., *T*:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练阶段**：对于步骤 *t* = 1, 2, . . ., *T*：'
- en: Select an action
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个动作
- en: Perform the selected action and move to the next state, observe the reward,
    and store this transition information in the replay buffer ![](img/B15558_09_088.png)
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选择的动作并进入下一个状态，观察奖励，并将此过渡信息存储在重放缓冲区中 ![](img/B15558_09_088.png)
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_259.png)
    with prioritization
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从回放缓冲区 ![](img/B15558_12_259.png) 采样一个优先级的经验小批量
- en: Compute the loss *J*(*Q*)
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失 *J*(*Q*)
- en: Update the parameter of the network using gradient descent
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新网络的参数
- en: 'If *t* mod *d* = 0:'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0：
- en: Update the target network parameter ![](img/B15558_18_066.png) by copying the
    main network parameter ![](img/B15558_09_098.png)
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_098.png) 来更新目标网络参数 ![](img/B15558_18_066.png)
- en: MaxEnt Inverse Reinforcement Learning
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大熵逆强化学习
- en: 'The algorithm for maximum entropy inverse reinforcement learning is given as
    follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最大熵逆强化学习的算法如下所示：
- en: Initialize the parameter ![](img/B15558_09_098.png) and gather the expert demonstrations
    ![](img/B15558_15_027.png)
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化参数 ![](img/B15558_09_098.png) 并收集专家示范 ![](img/B15558_15_027.png)
- en: 'For *N* number of iterations:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次迭代：
- en: Compute the reward function ![](img/B15558_18_292.png)
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算奖励函数 ![](img/B15558_18_292.png)
- en: Compute the policy using the value iteration with the reward function obtained
    in the previous step
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一步骤获得的奖励函数通过值迭代计算策略
- en: Compute the state visitation frequency ![](img/B15558_18_293.png) using the
    policy obtained in the previous step
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一步骤获得的策略计算状态访问频率 ![](img/B15558_18_293.png)
- en: Compute the gradient with respect to ![](img/B15558_09_098.png), that is, ![](img/B15558_15_176.png)
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相对于 ![](img/B15558_09_098.png) 的梯度，即 ![](img/B15558_15_176.png)
- en: Update the value of ![](img/B15558_09_106.png) as ![](img/B15558_18_297.png)
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ![](img/B15558_09_106.png) 的值更新为 ![](img/B15558_18_297.png)
- en: MAML in Reinforcement Learning
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的 MAML
- en: 'The algorithm for MAML in the reinforcement learning setting is given as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中 MAML 的算法如下所示：
- en: Say we have a model *f* parameterized by a parameter ![](img/B15558_09_118.png)
    and we have a distribution over tasks *p*(*T*). First, we randomly initialize
    the model parameter ![](img/B15558_09_054.png).
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个由参数 ![](img/B15558_09_118.png) 参数化的模型 *f*，并且我们有一个关于任务的分布 *p*(*T*)。首先，我们随机初始化模型参数
    ![](img/B15558_09_054.png)。
- en: Sample a batch of tasks *T*[i] from a distribution of tasks, that is, *T*[i]
    *~ p(T).*
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从任务分布中采样一批任务 *T*[i]，即 *T*[i] *~ p(T)。
- en: 'For each task *T*[i]:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个任务 *T*[i]：
- en: 'Sample *k* trajectories using ![](img/B15558_17_022.png) and prepare the training
    dataset: ![](img/B15558_17_128.png)'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ![](img/B15558_17_022.png) 采样 *k* 条轨迹，并准备训练数据集：![](img/B15558_17_128.png)
- en: Train the model ![](img/B15558_18_302.png) on the training dataset ![](img/B15558_17_089.png)
    and compute the loss
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集 ![](img/B15558_17_089.png) 上训练模型 ![](img/B15558_18_302.png)，并计算损失
- en: Minimize the loss using gradient descent and get the optimal parameter ![](img/B15558_17_056.png)
    as ![](img/B15558_18_305.png)
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降最小化损失并得到最优参数 ![](img/B15558_17_056.png)，其值为 ![](img/B15558_18_305.png)
- en: 'Sample *k* trajectories using ![](img/B15558_17_041.png) and prepare the test
    dataset: ![](img/B15558_17_115.png)'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ![](img/B15558_17_041.png) 采样 *k* 条轨迹，并准备测试数据集：![](img/B15558_17_115.png)
- en: 'Now, we minimize the loss on the test dataset ![](img/B15558_18_308.png). Parameterize
    the model *f* with the optimal parameter ![](img/B15558_18_309.png) calculated
    in the previous step and compute the loss ![](img/B15558_17_137.png). Calculate
    the gradients of the loss and update our randomly initialized parameter ![](img/B15558_09_098.png)
    using our test (meta-training) dataset: ![](img/B15558_17_139.png)'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们在测试数据集 ![](img/B15558_18_308.png) 上最小化损失。使用在前一步骤计算得到的最优参数 ![](img/B15558_18_309.png)
    对模型 *f* 进行参数化，并计算损失 ![](img/B15558_17_137.png)。计算损失的梯度，并使用我们的测试（元训练）数据集 ![](img/B15558_17_139.png)
    更新我们随机初始化的参数 ![](img/B15558_09_098.png)。
- en: Repeat *steps 2* to *4* for several iterations.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *4* 进行若干次迭代。
