- en: '*Chapter 3*: Generative Adversarial Network'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：生成对抗网络'
- en: '**Generative Adversarial Network**, more commonly known as **GANs**, are currently
    the most prominent method in image and video generation. As the inventor of the
    convolutional neural network, Dr. Yann LeCun, said in 2016, *"...it is the most
    interesting idea in the last 10 years in machine learning."* The images generated
    using GANs are superior, in terms of realism, to other competing technologies
    and things have advanced tremendously since their invention in 2014 by then graduate
    student Ian Goodfellow.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**，通常称为 **GANs**，目前是图像和视频生成中最突出的技术。正如卷积神经网络的发明者 Yann LeCun 博士在 2016
    年所说，*“…这是过去 10 年里机器学习中最有趣的想法。”* 使用 GAN 生成的图像在真实性方面优于其他竞争技术，自从 2014 年由当时的研究生 Ian
    Goodfellow 发明以来，技术已经取得了巨大的进展。'
- en: In this chapter, we will first learn about the fundamentals of GANs and build
    a DCGAN to generate Fashion MNIST. We'll learn about the challenges in training
    GANs. Finally, we will learn how to build a WGAN and its variant, WGAN-GP, to
    resolve many of the challenges involved in generating faces.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先了解 GAN 的基本原理，并构建一个 DCGAN 来生成 Fashion MNIST。我们将学习训练 GAN 时面临的挑战。最后，我们将学习如何构建
    WGAN 及其变体 WGAN-GP，以解决生成面孔时遇到的许多挑战。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the fundamentals of GANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 GAN 的基本原理
- en: Building a Deep Convolutional GAN (DCGAN)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度卷积生成对抗网络（DCGAN）
- en: Challenges in training GANs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 GAN 的挑战
- en: Building a Wasserstein GAN (WGAN)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 Wasserstein GAN（WGAN）
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The Jupyter notebooks and code can be found here:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 笔记本和代码可以在此找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03)'
- en: 'The notebooks used in the chapter are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的笔记本如下：
- en: '`ch3_dcgan.ipynb`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch3_dcgan.ipynb`'
- en: '`ch3_mode_collapse`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch3_mode_collapse`'
- en: '`ch3_wgan_fashion_mnist.ipynb`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch3_wgan_fashion_mnist.ipynb`'
- en: '`ch3_wgan_gp_fashion_mnist.ipynb`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch3_wgan_gp_fashion_mnist.ipynb`'
- en: '`ch3_wgan_gp_celeb_a.ipynb`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch3_wgan_gp_celeb_a.ipynb`'
- en: Understanding the fundamentals of GANs
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 GAN 的基本原理
- en: The purpose of generative models is to learn a data distribution and to sample
    from it to generate new data. With the models that we looked at in the previous
    chapters, namely PixelCNN and VAE, their generative part gets to look at the image
    distribution during training. Thus, they are known as **explicit density models**.
    In contrast, the generative part in a GAN never gets to look at the images directly;
    rather, it is only told whether the generated images look real or fake. For this
    reason, GANs are categorized as **implicit density models**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的目的是学习数据分布，并从中采样生成新的数据。对于我们在前几章中讨论的模型，例如 PixelCNN 和 VAE，它们的生成部分在训练过程中可以查看图像分布。因此，它们被称为**显式密度模型**。相比之下，GAN
    的生成部分从未直接查看过图像；它只知道生成的图像是真实的还是伪造的。因此，GAN 被归类为**隐式密度模型**。
- en: We could use an analogy to compare the explicit and implicit models. Let's say
    an art student, G, was given a collection of Picasso paintings and asked to learn
    how to draw fake Picasso paintings. The student can look at the collections as
    they learn to paint, so that is an explicit model. In a different scenario, we
    ask student G to forge Picasso paintings, but we don't show them any paintings
    and they don't know what a Picasso painting looks like. The only way they learn
    is from the feedback they get from student D, who is learning to spot fake Picasso
    paintings. The feedback is simple – the painting is either *fake* or *real*. That
    is our implicit density GAN model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个类比来比较显式模型和隐式模型。假设一位艺术生 G 获得了一些毕加索的画作，并被要求学习如何画伪造的毕加索画作。学生可以在学习绘画的过程中参考这些画作，因此这是一个显式模型。在另一个场景中，我们要求学生
    G 伪造毕加索画作，但我们不展示任何画作，他们也不知道毕加索的画作是什么样的。他们唯一的学习方式是通过来自学生 D 的反馈，学生 D 正在学习识别伪造的毕加索画作。反馈非常简单——画作要么是*伪造的*，要么是*真实的*。这就是我们的隐式密度
    GAN 模型。
- en: Perhaps one day they painted a twisted face by chance and learned from the feedback
    that it looked like a real Picasso painting. Then they start to draw in that style
    to fool student D. Students G and D are the two networks in a GAN, known as the
    **generator** and **discriminator**. This is the biggest difference in the network
    architecture compared with other generative models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或许有一天，它们偶然画了一个扭曲的面孔，并从反馈中学到这看起来像一幅真正的毕加索画作。于是，它们开始以这种风格作画来愚弄学生D。学生G和D是生成对抗网络（GAN）中的两个网络，分别称为**生成器**和**判别器**。这是与其他生成模型相比，网络架构的最大不同之处。
- en: We will start this chapter by learning about the GAN building blocks, followed
    by the losses. The original GAN does not have reconstruction loss, which is another
    thing that sets it apart from other algorithms. Then, we will create custom training
    steps for a GAN, and we'll be ready to train our first GAN.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从学习GAN的构建模块开始，然后是损失函数。原始GAN没有重构损失，这是它与其他算法不同的另一个方面。然后，我们将为GAN创建自定义的训练步骤，之后我们就可以训练我们的第一个GAN了。
- en: The architecture of a GAN
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN的架构
- en: 'The word *adversarial* in Generative Adversarial Network means *involving opposition
    or disagreement* according to the dictionary definition. There are two networks,
    known as the generator and discriminator, that compete with each other. The generator,
    as the name implies, generates *fake* images; while the discriminator will look
    at the generated images to decide whether they are real or fake. Each network
    is trying to win the game – the discriminator wants to correctly identify every
    real and fake image and the generator wants to fool the discriminator into believing
    the fake images generated by it are real. The following diagram shows the architecture
    of a GAN:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络中的*对抗*一词根据词典定义意味着*涉及对立或分歧*。有两个网络，分别称为生成器和判别器，它们彼此竞争。生成器，顾名思义，生成*假*图像；而判别器会查看生成的图像，以判断它们是“真实”还是“虚假”。每个网络都在努力赢得比赛——判别器希望正确识别每一张真实或虚假的图像，而生成器则希望愚弄判别器，使其相信由其生成的假图像是真实的。下图展示了GAN的架构：
- en: '![Figure 3.1 – Architecture of a GAN'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – GAN架构'
- en: '](img/B14538_03_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_01.jpg)'
- en: Figure 3.1 – Architecture of a GAN
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – GAN架构
- en: 'The GAN architecture bears some resemblance to a VAE (see [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039),
    *Variational Autoencoder*). In fact, you could rearrange the blocks in a VAE block
    diagram and add some lines and switches to produce this GAN block diagram. If
    a VAE was made up of two separate networks, we could think of:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GAN架构与VAE（参见[*第二章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)，*变分自编码器*）有一些相似之处。事实上，你可以重新排列VAE框图中的模块，并添加一些线条和开关，来生成这个GAN框图。如果VAE由两个独立的网络组成，我们可以认为：
- en: The GAN's generator as the VAE's decoder
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的生成器就像VAE的解码器
- en: The GAN's discriminator as the VAE's encoder
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN的判别器就像VAE的编码器
- en: The generator converts low-dimensional and simple distributions into high-dimensional
    images with a complex distribution, just like a decoder does. In fact, they are
    identical; we could simply copy and paste the decoder code and rename it as the
    generator, and vice versa, and it would just work. The input to the generator
    is usually samples from a normal distribution, despite some using uniform distribution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器将低维简单分布转换为具有复杂分布的高维图像，就像解码器的工作原理一样。事实上，它们是完全相同的；我们可以直接复制并粘贴解码器的代码，并将其重命名为生成器，反之亦然，它就能正常工作。生成器的输入通常是来自正态分布的样本，尽管有些使用均匀分布。
- en: We send real and fake images to the discriminator in different minibatches.
    Real images are those from the dataset while fake images are generated by the
    generator. The discriminator outputs a single value probability of whether the
    input is real or fake. It is a binary classifier and we could implement it using
    a CNN. Technically, the discriminator serves a different purpose than the encoder
    but they both reduce the dimensionality of their inputs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将真实图像和假图像分别送入判别器的不同小批次。真实图像来自数据集，而假图像由生成器生成。判别器输出一个单一的概率值，用于判断输入图像是“真实”还是“假”。它是一个二分类器，我们可以使用CNN来实现它。从技术上讲，判别器的作用与编码器不同，但它们都减少了输入的维度。
- en: Well, it turns out having two networks in a model is not that scary after all.
    The generator and discriminator are our old friends in disguise and under new
    names. We already know how to build those models, therefore let's not worry about
    the details of constructing them now. In fact, the original GAN paper used only
    a multilayer perceptron, which is made up of some basic dense layers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，在模型中有两个网络其实并不可怕。生成器和判别器是我们旧朋友的伪装和新名字。我们已经知道如何构建这些模型，因此现在不用担心构建的细节。事实上，原始的
    GAN 论文只用了一个多层感知器，由一些基本的全连接层组成。
- en: Value functions
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 价值函数
- en: 'The value function captures the fundamentals of how a GAN works. The equation
    is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数捕捉了 GAN 工作原理的基本内容。方程如下：
- en: '![](img/Formula_03_001.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_001.jpg)'
- en: 'Here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '*D* stands for discriminator.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D* 代表判别器。'
- en: '*G* is the generator.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*G* 是生成器。'
- en: '*x* is input data and *z* is a latent variable.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是输入数据，*z* 是潜在变量。'
- en: We will also use the same notation in the code. This is the function that the
    generator tries to minimize while the discriminator wants to maximize it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代码中也会使用相同的符号。这是生成器试图最小化的函数，而判别器则希望最大化它。
- en: When you understand it, the code implementation will be a lot easier and will
    make a lot of sense. Furthermore, much of our later discussion about the challenges
    of GANs and improvements to it revolves around the loss function. Therefore, it
    is well worth your time studying it. The GAN loss function is also known as **adversarial
    loss** in some literature. It looks rather complex now, but I'll break it down
    and show you step by step how it can be converted into simple loss functions that
    we can implement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你理解它后，代码实现会容易很多，而且会变得更加有意义。此外，我们接下来关于 GAN 挑战和改进的讨论，很多都围绕损失函数展开。因此，花时间研究它是非常值得的。GAN
    损失函数在某些文献中也被称为 **对抗性损失**。现在它看起来相当复杂，但我会一步步分解并展示如何将其转化为我们可以实现的简单损失函数。
- en: Discriminator loss
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 判别器损失
- en: 'The first right-hand term of the value function is the value to classify a
    real image correctly. From the left-hand term, we know the discriminator wants
    to maximize it. **Expectation** is a mathematical term that is the sum of the
    weighted average of every sample of a random variable. In this equation, the weight
    is the probability of data, and the variable is the log of the discriminator output
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数的第一项是正确分类真实图像的值。从左侧项我们可以知道，判别器希望最大化它。**期望**是一个数学术语，指的是随机变量每个样本的加权平均值的总和。在这个方程中，权重是数据的概率，变量是判别器输出的对数，如下所示：
- en: '![](img/Formula_03_002.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_002.jpg)'
- en: 'In a minibatch of size *N*, *p(x)* is *1/N*. This is because *x* is a single
    image. Instead of trying to maximize it, we can change the sign to minus and try
    to minimize it instead. This can be done with the help of the following equation,
    called the **log loss**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个大小为 *N* 的小批量中，*p(x)* 是 *1/N*。这是因为 *x* 是一张单独的图像。我们可以改变符号为负号并尝试最小化它，而不是最大化它。这可以通过以下方程实现，称为
    **对数损失**：
- en: '![](img/Formula_03_003.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_003.jpg)'
- en: 'Here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '*y*i is the label, which is *1* for real images.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*i 是标签，对于真实图像，标签为 *1*。'
- en: '*p(y*i*)* is the probability of the sample being real.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(y*i*)* 是样本为真实的概率。'
- en: 'The second right-hand term of the value function is about fake images; *z*
    is random noise and *G(z)* is generated fake images. *D(G(z))* is the discriminator''s
    confidence score of how likely the image is to be real. If we use a label of *0*
    for fake images, we can use the same method to cast it into the following equation:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数的第二项是关于假图像的；*z* 是随机噪声，*G(z)* 是生成的假图像。*D(G(z))* 是判别器对图像是否真实的信心分数。如果我们将假图像的标签设置为
    *0*，我们可以使用相同的方法将其转换为以下方程：
- en: '![](img/Formula_03_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_004.jpg)'
- en: 'Now, putting everything together, we have our discriminator loss function,
    which is binary cross-entropy loss:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将所有内容汇总，我们得到了判别器损失函数，即二元交叉熵损失：
- en: '![](img/Formula_03_005.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_005.jpg)'
- en: 'The following code shows how to implement the discriminator loss. You can find
    the code in Jupyter notebook `ch3_dcgan.ipynb`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何实现判别器损失。你可以在 Jupyter notebook `ch3_dcgan.ipynb` 中找到相关代码：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In our training, we do a forward pass on real and fake images separately using
    the same minibatch size. Therefore, we compute the binary cross-entropy loss for
    them separately and take the average as the loss.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练中，我们分别对真实和假图像进行前向传递，使用相同的批量大小。因此，我们分别计算它们的二元交叉熵损失，并取平均作为损失。
- en: Generator loss
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器损失
- en: 'The generator is only involved when the model is evaluating fake images, thus
    we only need to look at the second right-hand term of the value function and simplify
    it to this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器仅在模型评估伪造图像时参与，因此我们只需要查看值函数的第二个右侧项，并将其简化为以下形式：
- en: '![](img/Formula_03_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_006.jpg)'
- en: 'At the beginning of the training, the generator is not good at generating images,
    therefore the discriminator is confident in classifying it as *0* all the time,
    making *D(G(z))* always *0*, and so is *log (1 – 0)*. When the error in the model
    output is always *0*, then there is no gradient to backpropagate. As a result,
    the generator''s weights are not updated, and the generator is not learning. This
    phenomenon is known as **saturating gradient** due to there being almost no gradient
    in the discriminator''s sigmoid output. To avoid this problem, the equation is
    cast from minimizing *1-D(G(z))* to maximizing *D(G(z))* as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时，生成器在生成图像方面并不擅长，因此鉴别器总是很有信心地将其分类为*0*，使得*D(G(z))*始终为*0*，因此*log (1 – 0)*也为*0*。当模型输出的误差始终为*0*时，就没有梯度可以进行反向传播。因此，生成器的权重不会更新，生成器也无法学习。这种现象被称为**梯度饱和**，因为在鉴别器的sigmoid输出中几乎没有梯度。为了避免这个问题，公式被转化为最大化*D(G(z))*而不是最小化*1-D(G(z))*，如下所示：
- en: '![](img/Formula_03_007.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_007.jpg)'
- en: GANs that use this function are also known as **Non-Saturating GANs (NS-GANs)**.
    In fact, almost every implementation of **Vanilla GAN** uses this value function
    rather than the original GAN function.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数的GAN也被称为**非饱和GAN（NS-GANs）**。事实上，几乎所有的**经典GAN**实现都使用这个值函数，而不是原始的GAN函数。
- en: Vanilla GAN
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 经典GAN
- en: The interest of researchers in GANs exploded soon after their invention and
    many researchers gave their GAN a name. Some tried to keep track of all the named
    GANs over the years, but the list got too long. Vanilla GAN is the name used to
    loosely refer to the first basic GAN without fancy flavors. Vanilla GAN is usually
    implemented with two or three hidden dense layers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN发明后，研究人员对GAN的兴趣激增，许多研究人员给自己的GAN命名。有些人试图追踪多年来命名的GAN，但这个列表变得过长。经典GAN是一个松散的术语，用来指代没有任何复杂变种的第一个基础GAN。经典GAN通常通过两到三层隐藏密集层来实现。
- en: 'We can derive the generator loss using the same mathematical steps for the
    discriminator, which will eventually lead to the same discriminator loss function
    except that labels of one is used for real images. It can be confusing to beginners
    as to why to use real labels for fake images. It will be clear if we derive the
    equation, or we can also understand it as we want to fool the discriminator into
    assuming that those generated images are real, thus we use the real labels. The
    code is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过与鉴别器相同的数学步骤推导出生成器的损失，这最终会得出与鉴别器损失函数相同的结果，唯一的不同是，真实图像使用标签1。对于初学者来说，可能会感到困惑，为什么要为伪造图像使用真实标签。如果我们推导出这个公式，或者我们可以理解为，我们想要欺骗鉴别器，让它认为生成的图像是真的，因此我们使用真实标签。代码如下：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Congratulations, you have turned the most complex equation in a GAN into simple
    binary cross-entropy loss and implemented it in a few lines of code! Now let's
    look at the GAN training pipeline.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你已经将GAN中最复杂的公式转化为简单的二元交叉熵损失，并在几行代码中实现了它！现在让我们看看GAN的训练流程。
- en: GAN training steps
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAN训练步骤
- en: To train a conventional neural network in TensorFlow or any other high-level
    machine learning framework, we specify the model, loss function, optimizer, and
    then call `model.fit()`. TensorFlow will do all the work for us – we just sit
    there and wait for the loss to drop. Unfortunately, we cannot chain the generator
    and discriminator to be a single model, like we did for the VAE, and call `model.fit()`
    to train the GAN.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow或任何其他高级机器学习框架中训练传统神经网络时，我们指定模型、损失函数、优化器，然后调用`model.fit()`。TensorFlow会为我们完成所有工作——我们只需坐着等待损失下降。不幸的是，我们无法像训练VAE时那样将生成器和鉴别器连接为一个单一模型，直接调用`model.fit()`来训练GAN。
- en: 'Before delving into the GAN problem, let''s take a pause and refresh ourselves
    on what happens underneath the hood when doing a single training step:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨GAN问题之前，让我们先暂停一下，回顾一下在进行单次训练步骤时，模型内部到底发生了什么：
- en: Perform a forward pass to compute the loss.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一次前向传播以计算损失。
- en: From the loss, backpropagate the gradients backward with respect to the variables
    (weights and biases).
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从损失中，反向传播梯度，针对变量（权重和偏置）进行调整。
- en: Then, it's the variables update step. The optimizer will scale the gradients
    and add them to the variables, which completes one training step.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后是变量更新步骤。优化器将缩放梯度并将其加到变量中，完成一次训练步骤。
- en: These are the generic training steps in a deep neural network. The various optimizers
    differ only in how they calculate the scaling factors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是深度神经网络中的通用训练步骤。不同的优化器仅在计算缩放因子的方式上有所不同。
- en: 'Now come back to the GAN and look at the flow of gradients. When we train with
    real images, only the discriminator is involved – the network input is a real
    image and the output is a label of *1*. The generator plays no role here and therefore
    we can''t use `model.fit()`. However, we could still fit the model using the discriminator
    only, that is, `D.fit()` so that it is not the blocking issue. The problem arises
    when we use fake images and the gradients backpropagate to the generator via the
    discriminator. So, what is the problem? Let''s take the generator loss and discriminator
    loss for the fake image and put them side by side:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到 GAN，看看梯度的流动。当我们使用真实图像进行训练时，只有判别器参与——网络输入是一个真实图像，输出是标签 *1*。生成器在这里不起作用，因此我们不能使用
    `model.fit()`。然而，我们仍然可以只使用判别器来拟合模型，即 `D.fit()`，这样就不会有阻塞问题。当我们使用假图像时，问题就出现了，梯度通过判别器反向传播到生成器。那么，问题到底是什么呢？让我们将生成器损失和判别器损失放到一起，看看假图像的情况：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you try to spot the difference between them, then you'll find that their
    labels are opposite signs! This means, using generator loss to train the entire
    model will make the discriminator move in the opposite direction and not learn
    to discriminate. This is counterproductive and we don't want to have an untrained
    discriminator that will discourage the generator from learning. For this reason,
    we must train the generator and discriminator separately. We will freeze the discriminator's
    variables when training the generator.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你试着找出它们之间的差异，你会发现它们的标签符号是相反的！这意味着，使用生成器损失来训练整个模型会让判别器朝着相反的方向移动，无法学习区分能力。这是适得其反的，我们不希望有一个未经训练的判别器，这会阻碍生成器的学习。因此，我们必须分别训练生成器和判别器。在训练生成器时，我们会冻结判别器的变量。
- en: There are two ways to design a GAN training pipeline. One is to use the high-level
    Keras model, which needs less code and therefore looks more elegant. We'll only
    need to define the model once, and call `train_on_batch()` to perform all the
    steps, including the forward pass, backpropagation, and weights update. However,
    it is less flexible when it comes to implementing more complex loss functions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 设计 GAN 训练流程有两种方法。一种是使用高级的 Keras 模型，这需要更少的代码，因此看起来更简洁。我们只需定义一次模型，并调用 `train_on_batch()`
    来执行所有步骤，包括前向传播、反向传播和权重更新。然而，当需要实现更复杂的损失函数时，它的灵活性较差。
- en: 'The other method is to use low-level code so we can control every step. For
    our first GAN, we will use a low-level custom training step function from the
    official TensorFlow GAN tutorial ([https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan)),
    as shown in the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用低级代码，这样我们可以控制每个步骤。对于我们的第一个 GAN，我们将使用来自官方 TensorFlow GAN 教程中的低级自定义训练步骤函数（[https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan)），如下所示：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`tf.GradientTape()` is used to record the gradients of a single pass. You may
    have seen another API, `tf.Gradient()`, that has a similar function, but the latter
    does not work in TensorFlow eager execution. We will see how the three procedural
    steps mentioned previously get implemented in `train_step()`. The preceding code
    snippet shows the first step to carry out a forward pass to calculate the losses.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.GradientTape()` 用于记录单次前向传播的梯度。你可能见过另一个类似的 API，`tf.Gradient()`，它有类似的功能，但后者在
    TensorFlow 的急切执行模式下无法使用。我们将看到之前提到的三个步骤如何在 `train_step()` 中实现。前面的代码片段展示了执行前向传播以计算损失的第一步。'
- en: 'The second step is to calculate the gradient of the generator and discriminator
    from their respective losses using a tape gradient:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是使用 tape 梯度计算生成器和判别器的梯度，分别根据它们的损失进行计算：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The third and final step is to use the optimizer to apply the gradients to
    the variables:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步也是最后一步，是使用优化器将梯度应用到变量上：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You have now learned everything you need to train a GAN. What is left to be
    done is to set up the input pipeline, generator, and discriminator, and we will
    go over that in the coming section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了训练GAN所需的一切。剩下的任务是设置输入管道、生成器和判别器，我们将在接下来的章节中详细介绍。
- en: Custom model fit
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义模型拟合
- en: After TensorFlow 2.2, it is now possible to create a custom `train_step()` for
    a Keras model without re-writing the entire training pipeline. Then, we can use
    `model.fit()` in the usual way. This will also enable the use of multiple GPUs
    for training. Unfortunately, this new feature was not released in time to make
    it into the code in this book. However, do check out the TensorFlow tutorial at
    [https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)
    and feel free to modify the GAN's code to use a custom model fit.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2.2之后，现在可以为Keras模型创建一个自定义的`train_step()`，而无需重新编写整个训练管道。然后，我们可以像往常一样使用`model.fit()`。这也使得使用多个GPU进行训练成为可能。不幸的是，这一新特性没有及时发布，因此未能在本书中的代码中使用。然而，请查看[https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)的TensorFlow教程，并随时修改GAN的代码以使用自定义模型拟合。
- en: Building a Deep Convolutional GAN (DCGAN)
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度卷积GAN（DCGAN）
- en: Although Vanilla GAN has proven itself as a generative model, it suffers from
    a few training problems. One of them is the difficulty in scaling networks to
    make them deeper in order to increase their capacities. The authors of **DCGAN**
    incorporated a few recent advancements in CNNs at that time to make networks deeper
    and stabilize the training. These include the removal of the **maxpool** layer,
    replacing it with strided convolutions for downsampling, and the removal of fully
    connected layers. This has since become the standard way of designing a new CNN.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Vanilla GAN已经证明自己是一个生成模型，但它在训练过程中仍存在一些问题。其中之一就是很难扩展网络，使其更深以增加容量。**DCGAN**的作者在当时引入了一些卷积神经网络（CNN）的最新进展，使得网络更深并稳定了训练。这些进展包括去除**maxpool**层，用步长卷积替代它进行下采样，并去除了全连接层。这些方法已经成为设计新CNN的标准方式。
- en: Architecture guidelines
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构设计指南
- en: DCGAN is not strictly a fixed neural network that has layers pre-defined with
    a fixed set of parameters such as kernel size and the number of layers. Instead,
    it is more like architecture design guidelines. The use of batch normalization,
    activation, and upsampling in DCGAN has influenced the development of GANs. We
    will therefore look into them more, which should provide guidance in designing
    our own GAN.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN并不是一个严格固定的神经网络，它的层不是预定义的，也没有固定的参数集，如卷积核大小和层数。相反，它更像是一种架构设计指南。DCGAN中使用的批量归一化、激活函数和上采样方法对GAN的发展产生了影响。因此，我们将进一步探讨这些内容，这将为设计我们自己的GAN提供指导。
- en: Batch normalization
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '**Batch normalization** is informally called **batchnorm** within the machine
    learning community. In the early days of deep neural network training, a layer
    updated its weights after backpropagation to produce outputs that are closer to
    the targets. However, the weights of the subsequent layers have also changed,
    so it is like a moving goal, and this makes the training of deep networks difficult.
    Batchnorm solves this by normalizing the input to every layer to have zero mean
    and unity variance, hence stabilizing the training. These are operations that
    happen within batchnorm:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量归一化**在机器学习领域非正式地称为**batchnorm**。在深度神经网络训练的早期，每一层都会在反向传播后更新其权重，以产生更接近目标的输出。然而，后续层的权重也发生了变化，这就像一个不断变化的目标，使得深度网络的训练变得困难。Batchnorm通过将每一层的输入标准化为零均值和单位方差来解决这一问题，从而稳定训练。这些操作发生在batchnorm内部：'
- en: Calculate the mean *µ* and standard deviation *σ* of tensor *x* in a minibatch
    for every channel (hence the name *batch* normalization).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算小批量中每个通道的张量*x*的均值*µ*和标准差*σ*（因此得名*batch*归一化）。
- en: 'Normalize the tensor: *x'' = (x – µ) / σ*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化张量：*x' = (x – µ) / σ*。
- en: 'Perform an affine transformation: *y = α * x'' + β*, where *α* and *β* are
    trainable variables.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行仿射变换：*y = α * x' + β*，其中*α*和*β*是可训练变量。
- en: In a DCGAN, batchnorm is added to both the generator and discriminator, except
    for the first layer of the discriminator and the last layer of the generator.
    One thing to note is that newer researches show that batchnorm is not the best
    normalization technique to use for image generation as it removes some of the
    important information. We will look at other normalization techniques in later
    chapters, but we will keep using batchnorm in our GAN until then. One thing that
    we should know is that in order to use batchnorm, we will have to use a large
    minibatch, otherwise, the batch statistics can vary greatly from batch to batch
    and make the training unstable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在DCGAN中，批归一化（batchnorm）被添加到生成器和判别器的所有层中，除了判别器的第一层和生成器的最后一层。需要注意的是，新的研究表明，批归一化并不是图像生成的最佳归一化技术，因为它会去除一些重要信息。我们将在后续章节中研究其他归一化技术，但在那之前我们会继续在我们的GAN中使用批归一化。我们需要知道的一点是，为了使用批归一化，我们必须使用较大的小批量数据，否则，批统计量可能在不同批次之间差异很大，导致训练不稳定。
- en: Activations
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'The following figure shows the activations that we will use in DCGAN:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了我们将在DCGAN中使用的激活函数：
- en: '![Figure 3.2 – ReLU and leaky ReLU are used in intermediate layers of the generator
    and discriminator'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2 – 在生成器和判别器的中间层使用ReLU和泄漏ReLU'
- en: '](img/B14538_03_02.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_02.jpg)'
- en: Figure 3.2 – ReLU and leaky ReLU are used in intermediate layers of the generator
    and discriminator
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 在生成器和判别器的中间层使用ReLU和泄漏ReLU
- en: As the discriminator's job is to be a binary classifier, we use sigmoid to squeeze
    the output to within the range of *0* (fake) and *1* (real). On the other hand,
    the generator's output uses *tanh*, which bounds the images between *-1* and *+1*.
    Therefore, we will need to scale our images to this range in the preprocessing
    step.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于判别器的任务是作为二分类器，因此我们使用sigmoid函数将输出压缩到*0*（假）和*1*（真）之间。另一方面，生成器的输出使用*tanh*，它将图像的值限定在*-1*和*+1*之间。因此，在预处理步骤中，我们需要将图像缩放到这个范围。
- en: For intermediate layers, the generator uses ReLU in all layers, but the discriminator
    uses leaky ReLU instead. In standard ReLU, the activation increases linearly with
    positive input but is zero for all negative input values. This limits the gradient
    flow when it is negative and thus the generator does not receive gradients in
    order to update its weights and learn. Leaky ReLU alleviates that problem by allowing
    small gradients to flow when the activation is negative.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于中间层，生成器在所有层中使用ReLU，而判别器则使用泄漏ReLU。标准的ReLU中，激活值对于正输入呈线性增加，而对于所有负输入值则为零。这在负值时限制了梯度流动，从而导致生成器无法接收到梯度以更新其权重并进行学习。泄漏ReLU通过允许负激活值时小的梯度流动来缓解这个问题。
- en: As we can see in the preceding figure, for input above and equal to *0*, it
    is identical to ReLU where the output equals input with a slope of *1*. For input
    below *0*, the output is scaled to *0.2* of the input. The default slope of leaky
    ReLU in TensorFlow is *0.3* while the DCGAN uses *0.2*. It is just a hyperparameter
    and you are free to try any other values.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的图中看到的，对于大于或等于*0*的输入，它与ReLU相同，其中输出等于输入，斜率为*1*。对于小于*0*的输入，输出被缩放为输入的*0.2*。在TensorFlow中，泄漏ReLU的默认斜率为*0.3*，而DCGAN使用的是*0.2*。这只是一个超参数，你可以尝试任何其他的值。
- en: Upsampling
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上采样
- en: In DCGAN, upsampling in the generator is performed using the transpose convolutional
    layer. However, it has been shown that this will produce a checkerboard pattern
    in the generated image, especially in images with strong colors. As a result,
    we replace it with `UpSampling2D`, which performs conventional image resizing
    methods by using bilinear interpolation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在DCGAN中，生成器的上采样是通过转置卷积层实现的。然而，已经证明，这会在生成的图像中产生棋盘格图案，尤其是在颜色较强的图像中。因此，我们用`UpSampling2D`替代它，该方法通过使用双线性插值来执行常规的图像大小调整方法。
- en: Building a DCGAN for Fashion-MNIST
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Fashion-MNIST的DCGAN
- en: The Jupyter notebook for this exercise is `ch3_dcgan.ipynb`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的Jupyter notebook是`ch3_dcgan.ipynb`。
- en: MNIST has been used in many introductory machine learning tutorials and we are
    all familiar with it. With recent advancements in machine learning, this dataset
    began to look a bit trivial for deep learning. As a result, a new dataset, Fashion-MNIST,
    has been created as a direct drop-in replacement for the MNIST dataset. It has
    exactly the same number of training and test examples, 28x28 grayscale images
    of 10 classes. This is what we will train our DCGAN with.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 已经在许多机器学习入门教程中被使用，大家都非常熟悉它。随着机器学习的最新进展，这个数据集开始显得有些对深度学习来说过于简单。因此，一个新的数据集
    Fashion-MNIST 被创建，作为 MNIST 数据集的直接替代品。它的训练和测试样本数量完全相同，都是 10 类的 28x28 灰度图像。我们将用这个数据集来训练我们的
    DCGAN。
- en: '![Figure 3.3 – Examples of images from the Fashion-MNIST dataset'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3 – Fashion-MNIST 数据集的图像示例'
- en: '](img/B14538_03_03.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_03.jpg)'
- en: Figure 3.3 – Examples of images from the Fashion-MNIST dataset
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – Fashion-MNIST 数据集的图像示例
- en: Generator
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器
- en: 'The design of the generator can be broken into two parts:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的设计可以分为两部分：
- en: Convert the 1D latent vector into a 3D activation map.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 1D 潜在向量转换为 3D 激活图。
- en: Double the activation map's spatial resolution until it matches the target image.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将激活图的空间分辨率翻倍，直到它与目标图像匹配。
- en: The first thing to do is to work out the number of upsampling stages. As the
    images have a shape of 28x28, we can use two upsampling stages to increase the
    dimension from 7->14->28\.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是计算上采样阶段的数量。由于图像的形状是 28x28，我们可以使用两个上采样阶段将维度从 7->14->28 增加。
- en: For simple data, we can use one convolution layer per upsampling stage, but
    we could also use more layers. This method is similar to CNNs in that you have
    several convolution layers working on the same spatial resolution before downsampling.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单数据，我们可以在每个上采样阶段使用一个卷积层，但也可以使用更多的层。此方法类似于 CNN，其中有多个卷积层在相同的空间分辨率下工作，然后再进行下采样。
- en: 'Next, we will decide the channel numbers of the first convolutional layer.
    Let''s say we use [512, 256, 128, 1], where the last channel number is the image
    channel number. With this information, we know the neuron numbers in first dense
    layer to be `7 x 7 x 512`. The `7x7` is the spatial resolution we worked out and
    `512` is the filter number in the first convolutional layer. After the dense layer,
    we reshape it to `(7,7,512)` so it can be fed into a convolutional layer. Then,
    we only need to define the filter number of convolutional layers and add the batchnorm
    and ReLU, as shown in the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将决定第一个卷积层的通道数。假设我们使用 [512, 256, 128, 1]，其中最后一个通道数是图像通道数。根据这些信息，我们可以得出第一个全连接层的神经元数为
    `7 x 7 x 512`。`7x7` 是我们计算出的空间分辨率，`512` 是第一个卷积层的滤波器数量。在全连接层之后，我们将其重塑为 `(7,7,512)`，这样它就可以输入到卷积层。然后，我们只需要定义卷积层的滤波器数量，并添加
    batchnorm 和 ReLU，如以下代码所示：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model summary for the generator is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的模型摘要如下：
- en: '![Figure 3.4 – DCGAN generator model summary'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4 – DCGAN 生成器模型摘要'
- en: '](img/B14538_03_04.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_04.jpg)'
- en: Figure 3.4 – DCGAN generator model summary
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – DCGAN 生成器模型摘要
- en: The generator's model summary shows the activation map shapes that are doubling
    in spatial resolution `(7×7 to 14×14 to 28×28)` while halving in the channel numbers
    `(512 to 256 to 128)`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的模型摘要展示了激活图的形状，这些激活图的空间分辨率在 `(7×7 到 14×14 到 28×28)` 之间翻倍，而通道数在 `(512 到 256
    到 128)` 之间减半。
- en: Discriminator
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 判别器
- en: 'The design of the discriminator is straightforward, just like a simple classifier
    CNN but with leaky ReLU as activation. As a matter of fact, the discriminator
    architecture was not even mentioned in the DCGAN paper. As a rule of thumb, the
    discriminator should have fewer or an equal number of layers as the generator,
    so it doesn''t overpower the generator to stop the latter from learning. The following
    is the code to create the discriminator:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的设计非常简单，就像一个普通的分类器 CNN，但使用了泄漏 ReLU 激活函数。实际上，DCGAN 论文中甚至没有提到判别器的架构。根据经验，判别器的层数应该少于或等于生成器的层数，以免判别器过强，导致生成器无法学习。以下是创建判别器的代码：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The model summary of the discriminator, which is a simple CNN classifier, is
    shown as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的模型摘要是一个简单的 CNN 分类器，摘要如下：
- en: '![Figure 3.5 – DCGAN discriminator model summary'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – DCGAN 判别器模型摘要'
- en: '](img/B14538_03_05.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_05.jpg)'
- en: Figure 3.5 – DCGAN discriminator model summary
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – DCGAN 判别器模型摘要
- en: Training our DCGAN
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练我们的 DCGAN
- en: 'Now we can start training our first GAN. The following diagram shows the samples
    generated during different steps in the training:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练我们的第一个GAN。下图展示了在不同训练步骤中生成的样本：
- en: '![Figure 3.6 – Generated images during DCGAN training'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6 – DCGAN训练中的生成图像'
- en: '](img/B14538_03_06.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_06.jpg)'
- en: Figure 3.6 – Generated images during DCGAN training
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – DCGAN训练中的生成图像
- en: The first row of samples is generated right after network weight initialization
    and before any training steps. As we can see, they are just some random noise.
    As training progresses, the generated images become better. However, the generator
    loss is higher than when it was only generating random noise.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第一排的样本是在网络权重初始化后、任何训练步骤之前生成的。正如我们所看到的，它们只是一些随机噪声。随着训练的进行，生成的图像变得越来越好。然而，生成器的损失比只生成随机噪声时要高。
- en: The loss is not an absolute measurement of generated image quality; it merely
    provides relative terms to compare the performance of the generator relative to
    the discriminator and vice versa. The generator loss was low simply because the
    discriminator had not learned to do its job well. This is one of the challenges
    of a GAN where the loss does not give sufficient information about the model's
    quality.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 损失并不是生成图像质量的绝对度量，它仅提供相对的标准，用于比较生成器相对于判别器的表现，反之亦然。生成器的损失较低，仅仅是因为判别器尚未学会如何执行其任务。这就是GAN的一个挑战，损失无法提供足够的信息来衡量模型的质量。
- en: 'The following graphs show the discriminator loss and generator loss during
    training:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了训练过程中判别器和生成器的损失：
- en: '![Figure 3.7 – Discriminator and generator training losses'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 判别器和生成器训练损失'
- en: '](img/B14538_03_07.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_07.jpg)'
- en: Figure 3.7 – Discriminator and generator training losses
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 判别器和生成器训练损失
- en: We can see that the equilibrium achieved in the first 1,000 steps and the loss
    remain roughly stable after that. However, the loss isn't definitive in gauging
    when to stop training. For now, we can save the weights every few epochs and eyeball
    to select the one that generates the best-looking images!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在前1,000步中达到了平衡，损失在此之后大致保持稳定。然而，损失并不是判断何时停止训练的决定性指标。现在，我们可以每过几轮保存一次权重，并通过目测选择生成最美观图像的那个！
- en: In theory, the global optimal for the discriminator is achieved when *pdiscriminator
    = pdata*. In other words, if *pdata = 0.5* as half of the data is real and half
    is fake, then *pdiscriminator = 0.5* will mean it can no longer distinguish between
    the two classes and the prediction is no better than flipping a coin.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，当*判别器 = 数据分布*时，判别器达到全局最优。换句话说，如果*数据分布 = 0.5*（一半数据为真实，一半为假数据），那么*判别器 = 0.5*意味着它无法再区分这两类数据，预测结果和掷硬币一样。
- en: Challenges in training GANs
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN训练中的挑战
- en: GANs are notoriously difficult to train. We'll discuss some of the main challenges
    in training a GAN.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GAN训练 notoriously 难。我们将讨论训练GAN的一些主要挑战。
- en: Uninformative loss and metrics
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无信息损失与度量标准
- en: When training a CNN for classification or detection tasks, we can look at the
    shape of the loss plots to tell whether the network has converged or is overfitting
    and we'll know when to stop training. Then the metrics will correlate with the
    loss. For example, classification accuracy is normally the highest when the loss
    is the lowest. However, we can't do the same with GAN loss, as it doesn't have
    a minimum but fluctuates around some constant values after training for a while.
    We also could not correlate the generated image quality with the loss. A few metrics
    were invented to address this in the early days of GANs and one of them is the
    **inception score.**
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练CNN进行分类或检测任务时，我们可以通过观察损失图形的形状来判断网络是否已经收敛或是否出现过拟合，从而知道何时停止训练。然后，度量指标会与损失相关联。例如，当损失最小的时候，分类准确率通常是最高的。然而，我们无法对GAN的损失做同样的处理，因为它没有最小值，而是在训练一段时间后围绕某些常数值波动。我们也不能将生成的图像质量与损失相关联。为了应对这个问题，早期的GAN提出了一些度量标准，其中之一就是**启发得分（inception
    score）**。
- en: A classification CNN known as `ImageNet` dataset. If high confidence is recorded
    for a class, it is more likely to be a real image. There is another metric known
    as the **Fréchet inception distance**, which measures the variety of generated
    images. These metrics are normally used only in academic papers to make a comparison
    with other models (so they can claim their models are superior), so we will not
    cover them in detail in this book. Human visual inspection is still the most reliable
    way of assessing the quality of generated images.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个被称为 `ImageNet` 数据集的分类 CNN。如果某个类别的置信度很高，它更可能是真实的图像。还有一个叫做 **Fréchet 启动距离**（Fréchet
    Inception Distance，FID）的度量，它衡量生成图像的多样性。这些度量通常只在学术论文中使用，用来与其他模型做对比（以便声称他们的模型更优），因此我们在本书中不会详细介绍这些内容。人类的视觉检查仍然是评估生成图像质量的最可靠方式。
- en: Instability
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不稳定性
- en: 'GANs are extremely sensitive to any change in hyperparameters, including learning
    rate and filter kernel size. Even after a lot of hyperparameter tuning, and the
    correct architecture, there are instances while retraining the model where the
    following can occur:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 对超参数的变化极为敏感，包括学习率和滤波器内核大小。即便在经过大量超参数调优，并且架构正确的情况下，在重新训练模型时，仍然可能发生以下情况：
- en: '![Figure 3.8 – Generator stuck in local minima'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – 生成器陷入局部最小值'
- en: '](img/B14538_03_08.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_08.jpg)'
- en: Figure 3.8 – Generator stuck in local minima
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 – 生成器陷入局部最小值
- en: If the network weights are unfortunately randomly initialized to some bad values,
    the generator could get stuck in some bad local minima and may never recover,
    while the discriminator keeps improving. As a result, the generator gives up and
    produces only nonsensical images. This is also known as **convergence failure**,
    where the losses fail to converge. We'll need to stop the training, re-initialize
    the network, and restart the training. This is also the reason why I haven't chosen
    a more complex dataset such as CelebA to introduce GANs, but don't worry, we'll
    get there before the chapter ends.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络权重不幸地随机初始化为一些不良值，生成器可能会陷入某些不良的局部最小值，并且可能永远无法恢复，而判别器却一直在改进。因此，生成器会放弃，生成的图像仅为无意义的图像。这也被称为**收敛失败**，即损失函数无法收敛。我们需要停止训练，重新初始化网络，并重新开始训练。这也是我没有选择更复杂的数据集，如
    CelebA 来介绍 GAN 的原因，但不用担心，我们会在本章结束前解决这个问题。
- en: Vanishing gradient
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度
- en: 'One reason for instability is the vanishing gradient of the generator. As we''ve
    already mentioned, when we train the generator, the gradient will flow through
    the discriminator. If the discriminator is confident that the images are fake,
    then there will be little or even zero gradient to backpropagate to the generator.
    The following points are some of the methods of mitigation:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不稳定性的一个原因是生成器的消失梯度。正如我们之前提到的，当我们训练生成器时，梯度会通过判别器流动。如果判别器非常确定图像是假的，那么反向传播到生成器的梯度就会非常小，甚至为零。以下是一些缓解方法：
- en: Reformulating the value function from minimizing log *(1-D(G(z))* to maximizing
    log *D(G(z))*, which we already did. In practice, this alone is still not enough.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将价值函数从最小化 log *(1-D(G(z)))* 改为最大化 log *D(G(z))*, 这我们已经做过了。实际上，仅此一项仍然不足以解决问题。
- en: Using activation functions that allow more gradients to flow, such as leaky
    ReLU.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用允许更多梯度流动的激活函数，如 leaky ReLU。
- en: Balancing between the generator and the discriminator by reducing the discriminator's
    network capacity or increasing the training steps for the generator.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少判别器的网络容量或增加生成器的训练步骤，来平衡生成器和判别器之间的关系。
- en: Using *one-sided label smoothing,* where the label of the real image is decreased
    from *1* to, say, *0.9* to reduce the discriminator's confidence.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 *单边标签平滑*，将真实图像的标签从 *1* 降低到例如 *0.9*，以减少判别器的置信度。
- en: Mode collapse
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式崩溃
- en: '**Mode collapse** happens when the generator is producing images that look
    like each other. This is not to be confused with convergence failure where the
    GAN produces only garbage images.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式崩溃**发生在生成器生成的图像彼此相似时。这与收敛失败不同，后者是指 GAN 只生成无意义的图像。'
- en: Mode collapse can happen even when the generated images look great but are limited
    to small subsets of classes (inter-class mode collapse) or a few of the same images
    within the class (intra-class mode collapse). We can demonstrate mode collapse
    by training a Vanilla GAN on a mixture of two Gaussian distributions, which you
    could run in the `ch3_mode_collapse` notebook.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 即使生成的图像看起来很好，模式崩溃仍然可能发生，且它会限制在类的小子集（类间模式崩溃）或同一类中的几张相同图像（类内模式崩溃）之间。我们可以通过在一个包含两个高斯分布的混合体上训练
    Vanilla GAN 来演示模式崩溃，您可以在`ch3_mode_collapse`笔记本中运行此实验。
- en: 'The following figure shows the shape of the generated samples during training
    taking the form of two Gaussian blobs. One sample is round and the other is elliptical:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了训练过程中生成样本的形态，呈现出两个高斯块的形状。一个样本是圆形的，另一个是椭圆形的：
- en: '![Figure 3.9 – The top figure is the real samples. The bottom figures show
    generated samples in two different epochs during training](img/B14538_03_09.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.9 – 上面的图是实际样本，下面的图展示了训练过程中在两个不同周期生成的样本](img/B14538_03_09.jpg)'
- en: Figure 3.9 – The top figure is the real samples. The bottom figures show generated
    samples in two different epochs during training
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 – 上面的图是实际样本，下面的图展示了训练过程中在两个不同周期生成的样本
- en: As the Vanilla GAN trains, the generated samples can look like one of two modes
    in a minibatch but never two modes at the same time. For Fashion-MNIST, it may
    be that the generator is producing shoes that look the same every time, regardless.
    After all, the objective of the generator is to produce realistic-looking images,
    and it is not penalized for showing the same shoes every time as long as the discriminator
    deems the images to be real. As proven in the original GAN paper mathematically,
    after the discriminator achieved optimality, the generator will work toward optimizing
    for **Jensen-Shannon divergence** (**JSD**).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Vanilla GAN 的训练，生成的样本可能看起来像一个小批次中的两个模式之一，但永远不会是两个模式同时出现。对于 Fashion-MNIST，可能生成器每次都会生成看起来相同的鞋子。毕竟，生成器的目标是生成真实感的图像，只要判别器认为这些图像是真实的，生成器就不会因为每次显示相同的鞋子而受到惩罚。正如原始
    GAN 论文中数学证明的那样，当判别器达到了最优时，生成器将朝着优化**Jensen-Shannon 散度**（**JSD**）的方向进行调整。
- en: 'For our purposes, we only need to know that JSD is a symmetrical version of
    **Kullback-Leibler divergence** (**KLD**) with an upper bound of *log(2)* rather
    than an infinite upper bound. Unfortunately, JSD is also the cause of mode collapse,
    as can be illustrated in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们只需要知道 JSD 是**Kullback-Leibler 散度**（**KLD**）的对称版本，其上界为*log(2)*，而不是无限大的上界。不幸的是，JSD
    也是模式崩溃的原因，如下图所示：
- en: '![Figure 3.10 – A standard Gaussian distribution fit on data drawn from a mixture
    of Gaussians by minimizing KLD, MMD, and JSD (Source: L. Theis et al, 2016, "A
    Note On The Evaluation of Generative Models," https://arxiv.org/abs/1511.01844)](img/B14538_03_10.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10 – 通过最小化 KLD、MMD 和 JSD 对从高斯混合分布中抽取的数据进行拟合的标准高斯分布（来源：L. Theis 等，2016年，《关于生成模型评估的笔记》，https://arxiv.org/abs/1511.01844)](img/B14538_03_10.jpg)'
- en: 'Figure 3.10 – A standard Gaussian distribution fit on data drawn from a mixture
    of Gaussians by minimizing KLD, MMD, and JSD (Source: L. Theis et al, 2016, "A
    Note On The Evaluation of Generative Models," https://arxiv.org/abs/1511.01844)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – 通过最小化 KLD、MMD 和 JSD 对从高斯混合分布中抽取的数据进行拟合的标准高斯分布（来源：L. Theis 等，2016年，《关于生成模型评估的笔记》，https://arxiv.org/abs/1511.01844）
- en: We will not talk about **maximum mean discrepancy** (**MMD**), which is not
    used in a GAN. The data is two Gaussian distributions where one has more mass
    density than the other. A single Gaussian is fitted on the data. In other words,
    we try to estimate one best mean and standard deviation to describe the two type
    of Gaussian distribution. With KLD, we see that although the fitted Gaussian leans
    toward the bigger Gaussian blob, it still provides some coverage to the smaller
    Gaussian blob. This is not the case for JSD, where it is fitted to only the most
    prominent Gaussian blob. This explains mode collapse in a GAN – when the probability
    of some particular generated images is high, when these few modes are *locked*
    by the optimizer.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会讨论**最大均值差异**（**MMD**），因为它在 GAN 中并未使用。数据是两个高斯分布，其中一个的质量密度比另一个大。我们对数据进行拟合一个单一的高斯分布。换句话说，我们试图估计一个最佳的均值和标准差来描述这两种类型的高斯分布。通过
    KLD，我们可以看到，尽管拟合的高斯分布偏向于较大的高斯块，但它仍然覆盖了较小的高斯块。JSD 则不同，它只拟合最显著的高斯块。这也解释了 GAN 中的模式崩溃——当某些生成图像的概率较高时，这些模式会被优化器*锁定*。
- en: Building a Wasserstein GAN
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Wasserstein GAN
- en: Many have attempted to solve the instability of GAN training by using heuristic
    approaches such as trying different network architectures, hyperparameters, and
    optimizers. One major breakthrough happened in 2016 with the introduction of **Wasserstein
    GAN (WGAN)**.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人尝试通过使用启发式方法来解决GAN训练的不稳定性，例如尝试不同的网络架构、超参数和优化器。2016年，一个重大突破发生了，那就是引入了**Wasserstein
    GAN (WGAN)**。
- en: WGAN alleviates or even eliminates many of the GAN challenges we've discussed
    altogether. It no longer requires careful design of network architecture nor careful
    balancing of the discriminator and the generator. The mode collapse problem is
    also reduced drastically.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN减轻甚至消除了我们之前讨论的许多GAN挑战。它不再需要精心设计网络架构，也不需要精确平衡判别器和生成器。模式崩溃问题也大大减少。
- en: The biggest fundamental improvement from the original GAN is the change of the
    loss function. The theory is that if the two distributions are disjointed, JSD
    will no longer be continuous, hence not differentiable, resulting in a zero gradient.
    WGAN solves this by using a new loss function that is continuous and differentiable
    everywhere!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始GAN到WGAN的最大根本改进是损失函数的变化。理论上，如果两个分布是非交叠的，JSD将不再是连续的，因此不可微，从而导致梯度为零。WGAN通过使用一个新的损失函数解决了这个问题，这个损失函数在任何地方都是连续且可微的！
- en: The notebook for this exercise is `ch3_wgan_fashion_mnist.ipynb`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的笔记本是`ch3_wgan_fashion_mnist.ipynb`。
- en: Tips
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: It is alright to not learn how to implement the code in this section, particularly
    WGAN-GP, which is more complex. Although theoretically superior, we could still
    train GANs stably using a simpler loss function with carefully designed model
    architecture and hyperparameters. However, you should try to understand the term
    Lipschitz constraint as it was used in the development of several advanced techniques,
    which we will cover in later chapters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，尤其是WGAN-GP部分，代码实现可以不学习，因为它更复杂。尽管从理论上来说更为优越，我们仍然可以使用一个更简单的损失函数，通过精心设计的模型架构和超参数稳定地训练GAN。然而，你应该尝试理解Lipschitz约束这个术语，因为它在多个先进技术的发展中起到了重要作用，这些技术将在后续章节中介绍。
- en: Understanding Wasserstein loss
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Wasserstein损失
- en: 'Let''s remind ourselves of the non-saturating value function:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下非饱和值函数：
- en: '![](img/Formula_03_008.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_008.jpg)'
- en: 'WGAN uses a new loss function known as the **Earth mover''s distance** or just
    Wasserstein distance. It measures the distance or the effort needed to transform
    one distribution into another. Mathematically, it is the minimum distance for
    every joint distribution between real and generated images, which is intractable,
    with some mathematical assumptions that are outside the scope of this book, and
    the value function becomes:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN使用一个新的损失函数，称为**地球搬运工距离**或简称Wasserstein距离。它衡量将一个分布转换为另一个分布所需的距离或努力。从数学角度来看，它是每个真实图像和生成图像联合分布的最小距离，这是不可计算的，涉及一些超出本书范围的数学假设，值函数变为：
- en: '![](img/Formula_03_009.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_009.jpg)'
- en: 'Now, let''s compare the preceding equation with NS loss and use that to derive
    the loss function. The most prominent change is that the *log()* is gone, and
    another is the sign of the fake image term changes. The loss function of the first
    term is therefore:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将前面的方程与NS损失进行比较，并用它来推导损失函数。最显著的变化是*log()*被去除，另一个变化是虚假图像项的符号发生了变化。因此，第一项的损失函数为：
- en: '![](img/Formula_03_010.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_010.jpg)'
- en: 'This is the average of the discriminator output, multiplied by *-1*. We can
    also generalize it by using *y*i as labels where *+1 is for real images*, and
    *-1 is for fake images*. Thus, we can implement Wasserstein loss as a TensorFlow
    Keras custom loss function as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这是判别器输出的平均值，乘以*-1*。我们还可以通过使用*y*i作为标签来推广，其中*+1表示真实图像*，而*-1表示假图像*。因此，我们可以将Wasserstein损失实现为一个TensorFlow
    Keras自定义损失函数，如下所示：
- en: '[PRE8]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As this loss function is no longer binary cross-entropy, the discriminator's
    objective is no longer classifying or discriminating between real and fake images.
    Instead, it aims to maximize the score for real images with respect to fake images.
    For this reason, in WGAN, the discriminator is given a new name of **critic**.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个损失函数不再是二元交叉熵，判别器的目标不再是对真实与假图像进行分类或区分。相反，它的目标是最大化真实图像相对于假图像的得分。因此，在WGAN中，判别器被赋予了一个新名称——**评论员**。
- en: The generator and discriminator architecture stays the same. The only change
    is that the sigmoid is removed from the discriminator's output. Therefore, the
    critic's prediction is unbounded and can be very large positive and negative values.
    This is put in check by implementing the **1-Lipschitz** constraint.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和鉴别器架构保持不变。唯一的变化是从鉴别器的输出中移除了S形函数。因此，评论家的预测是无界的，可以是非常大的正负值。通过实施**1-Lipschitz**约束来检查这一点。
- en: Implementing the 1-Lipschitz constraint
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施1-Lipschitz约束
- en: 'The mathematical assumption mentioned in Wasserstein loss is the **1-Lipschitz
    function**. We say the critic *D(x)* is *1-Lipschitz* if it satisfies the following
    inequality:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在Wasserstein损失中提到的数学假设是**1-Lipschitz函数**。如果评论家*D(x)*满足以下不等式，则称为*1-Lipschitz*：
- en: '![](img/Formula_03_011.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_011.jpg)'
- en: For two images, *x*1 and *x*2, their absolute critic's output difference must
    be smaller or equal to their average pixel-wise absolute difference. In other
    words, the critic's outputs should not differ too much for different images –
    be it real images or fakes. When WGAN was invented, the authors could not think
    of a proper implementation to enforce inequality. Therefore, they came up with
    a hack, which is to clip the critic's weights to some small values. By doing that,
    the layers' outputs and eventually the critics' outputs are capped to some small
    values. In the WGAN paper, the weights are clipped to the range of *[-0.01, 0.01]*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个图像*x*1和*x*2，它们的绝对评论家输出差异必须小于或等于它们的平均像素绝对差异。换句话说，评论家的输出不应该因为不同的图像（无论是真实图像还是伪造图像）而有太大的差异。当WGAN被发明时，作者们无法想出一个合适的实现来强制不等式。因此，他们想出了一个技巧，即将评论家的权重剪裁到一些小值。通过这样做，层的输出和最终评论家的输出都被限制在一些小值范围内。在WGAN论文中，权重被剪裁到*[-0.01,
    0.01]*的范围内。
- en: 'Weight clipping can be implemented in two ways. One way is to write a custom
    constraint function and use that in instantiating a new layer as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 权重修剪可以通过两种方式实现。一种方法是编写一个自定义约束函数，并在实例化新层时使用它，如下所示：
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can then pass the function to layers that accept constraint functions as
    follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将函数传递给接受约束函数的层，如下所示：
- en: '[PRE10]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, adding the constraint code in every layer creation can make the code
    look bloated. As we don''t need to cherry-pick which layer to clip, we can use
    a loop to read the weights and clips and write them back as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在每层创建时添加约束代码可能会使代码看起来臃肿。由于我们不需要挑选要剪裁的层，我们可以使用循环来读取权重并剪裁它们，然后再写回如下：
- en: '[PRE11]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is the method we use in the code example.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在代码示例中使用的方法。
- en: Restructuring training steps
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重构训练步骤
- en: In the original GAN theory, the discriminator is supposed to be trained optimally
    before the generator. That was not possible in practice due to the vanishing gradient
    of the generator as the discriminator gets better. Now, with the Wasserstein loss
    function, the gradient is derivable everywhere and we don't have to worry about
    the critic being too good to compare with the generator.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始GAN理论中，鉴别器应该在生成器之前被最优化地训练。由于生成器的梯度消失随着鉴别器变得更好而变得不可能实现。现在，通过Wasserstein损失函数，梯度在任何地方都是可导的，我们不必担心评论家过于优秀而与生成器比较。
- en: 'Therefore, in WGAN, the critic is trained for five steps for every one training
    step for the generator. In order to do this, we will split the critic training
    step into a separate function, which we can then loop through multiple times:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在WGAN中，评论家每进行一次生成器训练步骤，就训练五次。为了实现这一点，我们将评论家训练步骤拆分为一个单独的函数，然后可以通过多次循环来执行：
- en: '[PRE12]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will also need to rework the generator training step. In our DCGAN code,
    we use two models – the generator and discriminator. To train the generator, we
    also use gradient tape to update the weights. All these are rather cumbersome.
    There is another way of implementing the training step for the generator by merging
    the two models into one as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要重新调整生成器训练步骤。在我们的DCGAN代码中，我们使用两个模型 - 生成器和鉴别器。为了训练生成器，我们还使用梯度带来更新权重。所有这些都相当繁琐。有另一种方法可以实现生成器的训练步骤，即通过将两个模型合并为一个如下所示：
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we freeze the critic layers by setting `trainable=False`,
    and we chain that to the generator to create a new model and compile it. After
    that, we can set the critic to be trainable again, which will not affect the model
    that we have already compiled.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们通过将`trainable=False`冻结评论家层，并将其链到生成器以创建一个新模型并编译它。之后，我们可以再次将评论家设置为可训练，这不会影响我们已经编译的模型。
- en: 'We use the `train_on_batch()` API to perform a single training step that will
    automatically do the forward pass, loss calculation, backpropagation, and weights
    update:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `train_on_batch()` API 执行单次训练步骤，它会自动完成前向传播、损失计算、反向传播和权重更新：
- en: '[PRE14]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For this exercise, we resize the image shape to 32x32 so we can use deeper
    layers in the generator to upscale the image. The WGAN generator and discriminator
    architecture are shown in the following model summaries:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次练习，我们将图像尺寸调整为32x32，这样我们就可以在生成器中使用更深的层次来放大图像。WGAN的生成器和判别器架构如下所示：
- en: '![Figure 3.11 – Model summary of the WGAN''s generator'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.11 – WGAN生成器的模型摘要'
- en: '](img/B14538_03_11.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_11.jpg)'
- en: Figure 3.11 – Model summary of the WGAN's generator
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 – WGAN生成器的模型摘要
- en: 'The generator architecture follows the usual design with decreasing channel
    numbers as the feature map''s size doubles. The following is the model summary
    of the WGAN''s critic:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器架构遵循常规设计，随着特征图大小的增大，通道数逐渐减小。以下是WGAN判别器的模型摘要：
- en: '![Figure 3.12 – Model summary of the WGAN''s critic'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12 – WGAN判别器的模型摘要'
- en: '](img/B14538_03_12.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_12.jpg)'
- en: Figure 3.12 – Model summary of the WGAN's critic
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – WGAN判别器的模型摘要
- en: Despite the improvement over DCGAN, I found it difficult to train a WGAN and
    the image quality produced is no more superior than DCGAN. We'll now implement
    a WGAN variant that trains faster and produces sharper images.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管相较于DCGAN有所改进，但我发现训练WGAN仍然很困难，而且所生成的图像质量并不优于DCGAN。接下来我们将实现一种WGAN变体，能够更快地训练并生成更清晰的图像。
- en: Implementing gradient penalty (WGAN-GP)
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现梯度惩罚（WGAN-GP）
- en: 'Weight clipping is not an ideal way to enforce a Lipschitz constraint, as acknowledged
    by the WGAN authors. There are two drawbacks: capacity underuse and exploding/vanishing
    gradients. As we limit the weights, we also limit the critic''s ability to learn.
    It was found that weight clipping forces the network to learn only simple functions.
    Therefore, the neural network''s capacity becomes underused.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 权重裁剪并不是强制执行Lipschitz约束的理想方法，WGAN的作者也承认了这一点。它有两个缺点：能力未充分利用和梯度爆炸/消失。由于我们限制了权重，也限制了判别器的学习能力。研究发现，权重裁剪迫使网络只能学习简单的函数。因此，神经网络的能力被低效使用。
- en: 'Secondly, the clipping values require careful tuning. If set too high, the
    gradients will explode, hence violating the Lipschitz constraint. If set too low,
    gradients will vanish as we move the network back.  Also, the weight clipping
    will push the gradients to the two limits, as shown in the following diagram:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，裁剪值需要仔细调节。如果设置得太高，梯度会爆炸，从而违反Lipschitz约束。如果设置得太低，梯度会消失，导致网络回溯时出现问题。另外，权重裁剪会将梯度推向两个极限，具体如下图所示：
- en: '![Figure 3.13 – Left: Weight clipping pushes weights toward two values. Right:
    Gradients produced by gradient penalty. Source: I. Gulrajani et al, 2017, Improved
    Training of Wasserstein GANs](img/B14538_03_13.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13 – 左：权重裁剪将权重推向两个值。右：梯度由梯度惩罚产生。来源：I. Gulrajani等人，2017，改进的Wasserstein
    GAN训练](img/B14538_03_13.jpg)'
- en: 'Figure 3.13 – Left: Weight clipping pushes weights toward two values. Right:
    Gradients produced by gradient penalty. Source: I. Gulrajani et al, 2017, Improved
    Training of Wasserstein GANs'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 左：权重裁剪将权重推向两个值。右：梯度由梯度惩罚产生。来源：I. Gulrajani等人，2017，改进的Wasserstein GAN训练
- en: 'As a result, **gradient penalty** (**GP**) is proposed to replace weight clipping
    to enforce the Lipschitz constraint as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提出了**梯度惩罚**（**GP**）来替代权重裁剪，以强制执行Lipschitz约束，具体如下：
- en: '![](img/Formula_03_012.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_012.jpg)'
- en: We will look at each of the variables in the equation and implement them in
    the code. The Jupyter notebook for this exercise is `ch3_wgan_gp_fashion_mnist.ipynb`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个查看方程中的每个变量，并在代码中实现它们。本次练习的Jupyter笔记本文件为 `ch3_wgan_gp_fashion_mnist.ipynb`。
- en: 'We normally use *x* to denote a real image, but there is now an ![](img/Formula_03_013.png)
    in the equation. This ![](img/Formula_03_014.png) is pointwise interpolation between
    a real image and a fake image. The ratio of the images, or the epsilon, is drawn
    from a uniform distribution of *[0,1]*:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常用 *x* 来表示真实图像，但现在方程中有一个 ![](img/Formula_03_013.png)。这个 ![](img/Formula_03_014.png)
    是在真实图像和假图像之间的逐点插值。图像的比例，或者说 epsilon，是从 *[0,1]* 的均匀分布中抽取的：
- en: '[PRE15]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There is mathematical proof that the *"optimal critic contains straight lines
    with gradient norm 1 connecting coupled points from Pr and Pg",* as quoted from
    the WGAN-GP paper *Improved Training of Wasserstein GANs* ([https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf)).
    For our purposes, we can understand it as the gradient comes from the mixture
    of both real and fake images and we don't need to calculate the penalty for real
    and fake images separately.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 有数学证明*“最优评论员包含连接来自 Pr 和 Pg 的配对点的梯度范数为 1 的直线”，* 这是引用自 WGAN-GP 论文 *改进的 Wasserstein
    GAN 训练方法*（[https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf)）。对于我们的目的，我们可以理解为梯度来自真实和虚假图像的混合，我们不需要分别为真实和虚假图像计算惩罚。
- en: 'The term ![](img/Formula_03_015.png) is the gradient of the critic''s output
    with respect to the interpolation. We can again use gradient tape to get the gradient:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 项目 ![](img/Formula_03_015.png) 是评论员输出相对于插值的梯度。我们可以再次使用梯度带获取梯度：
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The next step is to calculate the L2-norm:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算 L2 范数：
- en: '![](img/Formula_03_016.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_03_016.jpg)'
- en: 'We square every value, add them together, then do a square root as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个值进行平方，将它们相加，然后取平方根，如下所示：
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When doing `tf.reduce_sum()`, we exclude the first dimension in the axis as
    that dimension is the batch size. The penalty aims to bring the gradient norm
    close to `1`, and this is the last step to calculate the gradient loss:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 `tf.reduce_sum()` 时，我们会排除轴上的第一维，因为该维度是批次大小。惩罚的目的是使梯度范数接近 `1`，这是计算梯度损失的最后一步：
- en: '[PRE18]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The lambda in the equation is the ratio of the gradient penalty to other critic
    losses and is set to 10 in the paper. Now we add all the critic losses and gradient
    penalty to backpropagate and update weights:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的 lambda 是梯度惩罚与其他评论员损失的比率，论文中设定为 10。现在，我们将所有评论员损失和梯度惩罚加起来进行反向传播并更新权重：
- en: '[PRE19]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'That is everything you''ll need to add to the WGAN to make it WGAN-GP. There
    are two things to remove though:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要添加到 WGAN 中以将其转换为 WGAN-GP 的所有内容。不过有两件事需要去掉：
- en: Weight clipping
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重剪辑
- en: Batch normalization in the critic
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论员中的批量归一化
- en: The gradient penalty is to penalize the norm of the critic's gradient with respect
    to each input independently. However, batch normalization changes the gradients
    with the batch statistics. To avoid this problem, batch normalization was removed
    from the critic and it was found that it still works well. This has since become
    a common practice in GANs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度惩罚是为了惩罚评论员对每个输入的梯度范数。然而，批量归一化会改变梯度的批量统计信息。为了解决这个问题，批量归一化被移除了，并且发现它仍然有效。这已成为
    GAN 中的常见做法。
- en: 'The critic architecture is the same as WGAN, less the batch normalization:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员架构与 WGAN 相同，只是去除了批量归一化：
- en: '![Figure 3.14 – Model summary of WGAN-GP'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14 – WGAN-GP 模型总结'
- en: '](img/B14538_03_14.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_14.jpg)'
- en: Figure 3.14 – Model summary of WGAN-GP
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – WGAN-GP 模型总结
- en: 'The following are the samples generated by a trained WGAN-GP:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是经过训练的 WGAN-GP 生成的样本：
- en: '![Figure 3.15 – Samples generated by WGAN-GP'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15 – WGAN-GP 生成的样本'
- en: '](img/B14538_03_15.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_15.jpg)'
- en: Figure 3.15 – Samples generated by WGAN-GP
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – WGAN-GP 生成的样本
- en: They look sharp and pretty, much like samples from the Fashion-MNIST dataset.
    The training was very stable and converged quickly! Next, we will put WGAN-GP
    to the test by training it on CelebA!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 它们看起来非常清晰漂亮，就像来自 Fashion-MNIST 数据集的样本。训练过程非常稳定，且很快收敛！接下来，我们将通过在 CelebA 上进行训练来测试
    WGAN-GP！
- en: Tweaking WGAN-GP for CelebA
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整 WGAN-GP 以适应 CelebA
- en: 'We will make some small tweaks to WGAN-GP to train on the CelebA dataset. First,
    as we will use a larger image size of 64 compared to 32 previously, we will need
    to add another stage of upsampling. Then we replace the batch normalization with
    **layer normalization** as suggested by the WGAN-GP authors. The following figure
    shows different types of normalization for tensors with a dimension of **(N, H,
    W, C)** where the notations stand for batch size, height, width, and channel respectively:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对 WGAN-GP 进行一些小的调整，以便在 CelebA 数据集上进行训练。首先，由于我们将使用比之前 32 更大的图像大小 64，因此我们需要添加另一个上采样阶段。然后，我们将批量归一化替换为
    **层归一化**，这是 WGAN-GP 作者建议的做法。下图显示了不同类型的归一化，适用于维度为 **(N, H, W, C)** 的张量，其中符号代表批量大小、高度、宽度和通道：
- en: '![Figure 3.16 – Different types of normalizations used in deep learning. (Source:
    Y. Wu, K. He, 2018, Group Normalization)](img/B14538_03_16.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.16 – 深度学习中使用的不同类型归一化。（来源：Y. Wu, K. He, 2018，群体归一化）](img/B14538_03_16.jpg)'
- en: 'Figure 3.16 – Different types of normalizations used in deep learning. (Source:
    Y. Wu, K. He, 2018, Group Normalization)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16 – 深度学习中使用的不同类型的标准化方法。（来源：Y. Wu, K. He, 2018, Group Normalization）
- en: 'Batch normalization calculates statistics across **(N, H, W)** to produce one
    statistic for each channel. In contrast, layer normalization calculates statistics
    across all tensors within one sample, that is, **(H,W,C)** and therefore does
    not correlate between samples and hence works better for image generation. It
    is a drop-in replacement for batch normalization where we replace the word *Batch*
    with *Layer*:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化通过统计计算 **(N, H, W)** 沿每个通道产生一个统计量。相比之下，层归一化计算一个样本内所有张量的统计量，即 **(H, W, C)**，因此不会在样本之间产生相关性，在图像生成中效果更好。它是批归一化的替代方案，其中我们将
    *Batch* 替换为 *Layer*：
- en: '[PRE20]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The Jupyter notebook for this exercise is `ch3_wgan_gp_celeb_a.ipynb`. The
    following are the images generated by our WGAN-GP. Although the training time
    of WGAN-GP is longer due to the additional step to do gradient penalty, the training
    is able to converge faster:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习的 Jupyter 笔记本是 `ch3_wgan_gp_celeb_a.ipynb`。以下是我们的 WGAN-GP 生成的图像。尽管 WGAN-GP
    的训练时间更长，因为需要额外步骤进行梯度惩罚，但训练能够更快地收敛：
- en: '![Figure 3.17 – Celebrity faces generated by WGAN-GP'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17 – WGAN-GP 生成的名人面孔'
- en: '](img/B14538_03_17.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_03_17.jpg)'
- en: Figure 3.17 – Celebrity faces generated by WGAN-GP
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – WGAN-GP 生成的名人面孔
- en: They don't look quite perfect compared to the VAE, partly because there wasn't
    reconstruction loss to make sure the facial features stay in the places they belong.
    Nonetheless, this encourages the GAN to be more imaginative and, as a result,
    more varieties of faces were generated. I also did not notice mode collapse. WGAN-GP
    is a milestone to achieve the training stability of a GAN. Many subsequent GANs
    use Wasserstein loss and gradient penalty, and that includes the Progressive GAN,
    to generate high-resolution images, which we will talk about in detail in [*Chapter
    7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)*, High Fidelity Face Generation*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VAE 相比，它们看起来并不完美，部分原因是没有重建损失来确保面部特征停留在它们应该在的位置上。尽管如此，这鼓励了 GAN 更具想象力，结果生成了更多种类的面孔。我也没有注意到模式崩溃。WGAN-GP
    是实现 GAN 训练稳定性的里程碑。许多后续的 GAN 使用 Wasserstein 损失和梯度惩罚，包括渐进式 GAN，在[*第 7 章*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)*，高保真度人脸生成*中我们将详细讨论。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have definitely learned a lot in this chapter. We started by learning about
    the theory and loss functions of GANs, and how to translate the mathematical value
    function into the code implementation of binary cross-entropy loss. We implemented
    DCGAN with convolutional layers, batch normalization layers, and leaky ReLU to
    make the networks go deeper. However, there are still challenges in training GANs,
    which include instability and being prone to mode collapse due to Jensen-Shannon
    divergence.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章确实学到了很多。我们首先学习了 GAN 的理论和损失函数，以及如何将数学价值函数转化为二元交叉熵损失的代码实现。我们使用卷积层、批归一化层和泄漏
    ReLU 实现了 DCGAN，使网络更深入。然而，训练 GAN 仍然面临挑战，包括由于 Jensen-Shannon 散度导致的不稳定性和模式崩溃。
- en: Many of these problems were solved by WGAN with Wasserstein distance, weight
    clipping, and the removal of the sigmoid at the critic's output. Finally, WGAN-GP
    introduces gradient penalty to properly enforce the 1-Lipztschitz constraint and
    give us a framework for stable GAN training. We then replaced batch normalization
    with layer normalization to train on the CelebA dataset successfully to generate
    a good variety of faces.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些问题都被 WGAN 与 Wasserstein 距离、权重裁剪以及在评论者输出中去除 Sigmoid 解决了。最后，WGAN-GP 引入了梯度惩罚来正确施加
    1-Lipschitz 约束，并为稳定的 GAN 训练提供了框架。然后，我们用层归一化替换了批归一化，在 CelebA 数据集上成功训练，生成了多样的面孔。
- en: This concludes part 1 of the book. Well done to you for making it this far!
    By now, you have learned about using different families of generative models to
    generate images. That includes autoregressive models like PixelCNN in [*Chapter
    1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)*, Getting Started with Image
    Generation Using TensorFlow*, in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder* and GANs in this chapter. You are now familiar with the
    concept of distribution, loss functions, and how to construct neural networks
    for image generation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着本书第一部分的结束。恭喜你走到了这一步！到现在为止，你已经学习了如何使用不同类型的生成模型来生成图像。这包括了自回归模型如PixelCNN，在[*第1章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)中介绍的《使用TensorFlow进行图像生成入门》、变分自编码器以及本章中的GAN。你现在已经熟悉了分布、损失函数的概念，以及如何构建神经网络来进行图像生成。
- en: With this solid foundation, we will explore some interesting applications in
    part 2 of the book, where we will also get to learn about some advanced techniques
    and cool applications. In the next chapter, we will learn how to perform image-to-image
    translation with GANs.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个坚实的基础，我们将在书的第二部分探索一些有趣的应用，在那里我们还将学习一些高级技术和酷炫的应用。在下一章，我们将学习如何使用GAN进行图像到图像的转换。
