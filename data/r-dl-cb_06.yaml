- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'The current chapter will introduce Recurrent Neural Networks used for the modeling
    of sequential datasets. In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍用于序列数据集建模的循环神经网络。在本章中，我们将涵盖：
- en: Setting up a basic Recurrent Neural Network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置基础的循环神经网络
- en: Setting up a bidirectional RNN model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置双向 RNN 模型
- en: Setting up a deep RNN model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置深度 RNN 模型
- en: Setting up a Long short-term memory based sequence model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置基于长短期记忆的序列模型
- en: Setting up a basic Recurrent Neural Network
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置基础的循环神经网络
- en: '**Recurrent Neural Networks** (**RNN**) are used for sequential modeling on
    datasets where high autocorrelation exists among observations. For example, predicting
    patient journeys using their historical dataset or predicting the next words in
    given sentences. The main commonality among these problem statements is that input
    length is not constant and there is a sequential dependence. Standard neural network
    and deep learning models are constrained by fixed size input and produce a fixed
    length output. For example, deep learning neural networks built on occupancy datasets
    have six input features and a binomial outcome.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络** (**RNN**) 用于处理存在高自相关性的序列数据集。例如，使用患者的历史数据集预测患者的就诊路径，或预测给定句子中的下一个单词。这些问题的共同点在于输入长度不固定，并且存在序列依赖性。标准的神经网络和深度学习模型受到固定大小输入的限制，并产生固定长度的输出。例如，基于占用数据集构建的深度学习神经网络有六个输入特征，并且输出为二项式结果。'
- en: Getting ready
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Generative models in machine learning domains are referred to as models that
    have an ability to generate observable data values. For example, training a generative
    model on an images repository to generate new images like it. All generative models
    aim to compute the joint distribution over given datasets, either implicitly or
    explicitly:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域中的生成模型是指那些能够生成可观察数据值的模型。例如，训练一个生成模型，通过图像库生成新的图像。所有生成模型的目标是计算给定数据集的联合分布，无论是隐式还是显式地：
- en: Install and set up TensorFlow.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并设置 TensorFlow。
- en: 'Load required packages:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的包：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: The section will provide steps to set-up an RNN model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将提供设置 RNN 模型的步骤。
- en: 'Load the `MNIST` dataset:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `MNIST` 数据集：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Reset the graph and start an interactive session:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置图形并开始交互式会话：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Reduce image size to 16 x 16 pixels using the `reduceImage` function from [Chapter
    4](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab), *Data Representation
    using Autoencoders*:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [第4章](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab) *使用自编码器的数据表示*
    中的 `reduceImage` 函数，将图像大小缩小至 16 x 16 像素：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Extract labels for the defined `train` and `valid` datasets:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取定义好的 `train` 和 `valid` 数据集的标签：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define model parameters such as size of input pixels (`n_input`), step size
    (`step_size`), number of hidden layers (`n.hidden`), and number of outcome classes
    (`n.classes`):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型参数，如输入像素的大小（`n_input`）、步长（`step_size`）、隐藏层的数量（`n.hidden`）和结果类别的数量（`n.classes`）：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define training parameters such as learning rate (`lr`), number of inputs per
    batch run (`batch`), and number of iterations (`iteration`):'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练参数，如学习率（`lr`）、每批次输入数量（`batch`）和迭代次数（`iteration`）：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define a function `rnn` that takes in batch input dataset (`x`), weight matrix
    (`weight`), and bias vector (`bias`); and returns a final outcome predicted vector
    of a most basic RNN:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数 `rnn`，该函数接收批量输入数据集（`x`）、权重矩阵（`weight`）和偏置向量（`bias`）；并返回最基本 RNN 的最终预测向量：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define `placeholder` variables (`x` and `y`) and initialize weight matrix and
    bias vector:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `placeholder` 变量（`x` 和 `y`），并初始化权重矩阵和偏置向量：
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Generate the predicted labels:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测标签：
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Run the optimization post initializing a session using the global variables
    initializer:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化全局变量初始化器后运行优化：
- en: '[PRE10]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Get the mean accuracy on `valid_data`:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 `valid_data` 上的平均准确率：
- en: '[PRE11]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Any changes to the structure require model retraining. However, these assumptions
    may not be valid for a lot of sequential datasets, such as text-based classifications
    that may have varying input and output. RNN architecture helps to address the
    issue of variable input length.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 任何结构上的变化都需要重新训练模型。然而，这些假设可能不适用于很多序列数据集，比如文本分类，它们可能具有不同的输入和输出。RNN 架构有助于解决可变输入长度的问题。
- en: 'The standard architecture for RNN with input and output is shown in the following
    figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的 RNN 架构，输入和输出如图所示：
- en: '![](img/00135.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00135.jpeg)'
- en: Recurrent Neural Network architecture
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络架构
- en: 'The RNN architecture can be formulated as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 结构可以表述为如下：
- en: '![](img/00129.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpeg)'
- en: 'Where ![](img/00070.jpeg) is state at time/index *t* and ![](img/00096.jpeg)
    is input at time/index *t*. The matrix *W* represents weights to connect hidden
    nodes and *S* connects input with the hidden layer. The output node at time/index
    *t* is related to state *h[t]* as shown as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/00070.jpeg) 是时间/索引 *t* 时的状态，![](img/00096.jpeg) 是时间/索引 *t* 时的输入。矩阵
    *W* 表示连接隐藏节点的权重，*S* 连接输入与隐藏层。时间/索引 *t* 时的输出节点与状态 *h[t]* 的关系如下所示：
- en: '![](img/00019.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00019.jpeg)'
- en: In the previous Equations layer, weights remain constant across state and time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程层中，权重在状态和时间上保持不变。
- en: Setting up a bidirectional RNN model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置双向 RNN 模型
- en: Recurrent Neural Networks focus on capturing the sequential information at time
    *t* by using historical states only. However, bidirectional RNN train the model
    from both directions using two RNN layers with one moving forwards from start
    to end and another RNN layer moving backwards from end to start of sequence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络专注于仅通过使用历史状态来捕捉时间 *t* 的顺序信息。然而，双向 RNN 从两个方向训练模型，一个 RNN 层从开始到结束移动，另一个 RNN
    层从结束到开始移动。
- en: 'Thus, the model is dependent on historical and future data. The bidirectional
    RNN models are useful where causal structure exists such as in text and speech.
    The unfolded structure of bidirectional RNN is shown in the following figure:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型依赖于历史数据和未来数据。双向 RNN 模型在存在因果结构的情况下非常有用，例如文本和语音。双向 RNN 的展开结构如下图所示：
- en: '![](img/00033.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
- en: Unfolded bidirectional RNN architecture
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的双向 RNN 结构
- en: Getting ready
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Install and set up TensorFlow:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并设置 TensorFlow：
- en: 'Load required packages:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的包：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Load `MNIST` dataset.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `MNIST` 数据集。
- en: The image from `MNIST` dataset is reduced to 16 x 16 pixels and normalized (Details
    are discussed in the *Setting-up RNN model* section)`.`
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MNIST` 数据集中的图像被缩小为 16 x 16 像素并进行归一化处理（详细内容请参见 *设置 RNN 模型* 部分）。'
- en: How to do it...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This section covers the steps to set-up a bidirectional RNN model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍设置双向 RNN 模型的步骤。
- en: 'Reset the graph and start an interactive session:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置图并开始交互式会话：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Reduce image size to 16 x 16 pixels using the `reduceImage` function from [Chapter
    4](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab), *Data Representation
    using Autoencoders*:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [第 4 章](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab) 的 `reduceImage`
    函数将图像大小减少为 16 x 16 像素，*使用自编码器的数据表示*：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Extract labels for the defined `train` and `valid` datasets:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取定义的 `train` 和 `valid` 数据集的标签：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define model parameters such as the size of input pixels (`n_input`), step
    size (`step_size`), number of hidden layers (`n.hidden`), and number of outcome
    classes (`n.classes`):'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型参数，例如输入像素的大小（`n_input`）、步长（`step_size`）、隐藏层的数量（`n.hidden`）和结果类别的数量（`n.classes`）：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define training parameters such as learning rate (`lr`), number of inputs per
    batch run (`batch`), and number of iterations (`iteration`):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练参数，例如学习率（`lr`）、每批次运行的输入数量（`batch`）和迭代次数（`iteration`）：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define a function to perform `bidirectional` Recurrent Neural Network:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来执行 `bidirectional` 循环神经网络：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define an `eval_func` function to evaluate mean accuracy using actual (`y`)
    and predicted labels (`yhat`):'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `eval_func` 函数，用于使用实际标签（`y`）和预测标签（`yhat`）评估平均准确度：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define `placeholder` variables (`x` and `y`) and initialize weight matrix and
    bias vector:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `placeholder` 变量（`x` 和 `y`）并初始化权重矩阵和偏差向量：
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Generate the predicted labels:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成预测标签：
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the loss function and optimizer:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和优化器：
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the optimization post initializing a session using global variables initializer:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化会话后，使用全局变量初始化器运行优化：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Get the mean accuracy on valid data:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取验证数据的平均准确度：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The convergence of cost function for RNN is shown in the following figure:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN 的代价函数收敛情况如下图所示：
- en: '![](img/00154.gif)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00154.gif)'
- en: Bidirectional Recurrent Neural Network convergence plot on MNIST dataset
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集上的双向循环神经网络收敛图
- en: Setting up a deep RNN model
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置深度 RNN 模型
- en: 'The RNN architecture is composed of input, hidden, and output layers. A RNN
    network can be made deep by decomposing the hidden layer into multiple groups
    or by adding computational nodes within RNN architecture such as including model
    computation such as multilayer perceptron for micro learning. The computational
    nodes can be added between input-hidden, hidden-hidden, and hidden-output connection.
    An example of a multilayer deep RNN model is shown in the following figure:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 架构由输入层、隐藏层和输出层组成。通过将隐藏层分解为多个组，或通过在 RNN 架构中添加计算节点，例如在微学习中使用多层感知器模型，可以使 RNN
    网络变深。计算节点可以添加在输入-隐藏、隐藏-隐藏和隐藏-输出的连接之间。以下是多层深度 RNN 模型的示例：
- en: '![](img/00093.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpeg)'
- en: An example of two-layer Deep Recurrent Neural Network architecture
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 两层深度递归神经网络架构示例
- en: How to do it...
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The RNN models in TensorFlow can easily be extended to Deep RNN models by using
    `MultiRNNCell`. The previous `rnn` function can be replaced with the `stacked_rnn`function
    to achieve a deep RNN architecture:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的 RNN 模型可以通过使用 `MultiRNNCell` 容易地扩展为深度 RNN 模型。之前的 `rnn` 函数可以用 `stacked_rnn`
    函数替换，从而实现深度 RNN 架构：
- en: 'Define the number of layers in the deep RNN architecture:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义深度 RNN 架构中的层数：
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define a `stacked_rnn` function to perform multi-hidden layers deep RNN:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `stacked_rnn` 函数以执行多隐藏层深度 RNN：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Setting up a Long short-term memory based sequence model
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置基于长短期记忆的序列模型
- en: In sequence learning the objective is to capture short-term and long-term memory.
    The short-term memory is captured very well by standard RNN, however, they are
    not very effective in capturing long-term dependencies as the gradient vanishes
    (or explodes rarely) within an RNN chain over time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列学习中，目标是捕捉短期记忆和长期记忆。标准的 RNN 能很好地捕捉短期记忆，但它们在捕捉长期依赖关系时并不有效，因为在 RNN 链中，梯度会随着时间的推移消失（或极少爆炸）。
- en: The gradient vanishes when the weights have small values that on multiplication
    vanish over time, whereas in contrast, scenarios where weights have large values
    keep increasing over time and lead to divergence in the learning process. To deal
    with the issue **Long Short Term Memory** (**LSTM**) is proposed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当权重值很小时，梯度会消失，这些值在相乘后会随时间消失；相比之下，权重大时，随着时间的推移，值不断增大，并导致学习过程的发散。为了解决这个问题，**长短期记忆**（**LSTM**）被提出。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The RNN models in TensorFlow can easily be extended to LSTM models by using
    `BasicLSTMCell`. The previous `rnn` function can be replaced with the `lstm` function
    to achieve an LSTM architecture:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的 RNN 模型可以通过使用 `BasicLSTMCell` 容易地扩展为 LSTM 模型。之前的 `rnn` 函数可以用 `lstm`
    函数替换，从而实现 LSTM 架构：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: For brevity the other parts of the code are not replicated.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁，代码的其他部分未被复制。
- en: How it works...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The LSTM has a similar structure to RNN, however, the basic cell is very different
    as traditional RNN uses single **multi-layer perceptron** (**MLP**), whereas a
    single cell of LSTM includes four input layers interacting with each other. These
    three layers are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的结构类似于 RNN，然而，基本单元却非常不同，因为传统的 RNN 使用单一的**多层感知器**（**MLP**），而 LSTM 的单个单元包含四个相互作用的输入层。这三个层是：
- en: forget gate
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门
- en: input gate
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门
- en: output gate
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门
- en: The *forget gate* in LSTM decides which information to throw away and it depends
    on the last hidden state output *h[t-1]*, *X[t]*, which represents input at time
    *t.*
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 中的*忘记门*决定丢弃哪些信息，它依赖于上一个隐藏状态输出 *h[t-1]* 和 *X[t]*，其中 *X[t]* 表示时间 *t* 的输入。
- en: '![](img/00131.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpeg)'
- en: Illustration of forget gate
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记门的示意图
- en: 'In the earlier figure, *C[t]* represents cell state at time *t*. The input
    data is represented by *X[t]* and the hidden state is represented as *h[t-1]*.
    The earlier layer can be formulated as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，*C[t]* 表示时间 *t* 时刻的单元状态。输入数据由 *X[t]* 表示，隐藏状态由 *h[t-1]* 表示。前一层可以表示为：
- en: '![](img/00141.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00141.jpeg)'
- en: 'The *input gate* decides update values and decides the candidate values of
    the memory cell and updates the cell state, as shown in the following figure:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*输入门*决定更新值并决定记忆单元的候选值，同时更新单元状态，如下图所示：'
- en: '![](img/00098.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.jpeg)'
- en: Illustration of input gate
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门的示意图
- en: 'The input *i[t]* at time *t* is updated as:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间 *t* 时刻的输入 *i[t]* 更新如下：
- en: '![](img/00049.jpeg)![](img/00100.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpeg)![](img/00100.jpeg)'
- en: 'The expected value of current state ![](img/00136.jpeg) and the output from
    input gate ![](img/00017.jpeg) is used to update the current state ![](img/00024.jpeg)
    at time *t* as:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态![](img/00136.jpeg)的期望值和输入门的输出![](img/00017.jpeg)用于更新时刻*t*的当前状态![](img/00024.jpeg)，计算公式为：
- en: '![](img/00036.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.jpeg)'
- en: 'The output gates, as shown in the following figure, compute the output from
    the LSTM cell based on input *X[t]* , previous layer output *h[t-1],* and current
    state *C[t]*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门，如下图所示，根据输入*X[t]*、前一层输出*h[t-1]*和当前状态*C[t]*来计算LSTM单元的输出：
- en: '![](img/00109.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)'
- en: Illustration of output gate
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门示意图
- en: 'The output based on *output gate* can be computed as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于*输出门*的输出可以按以下方式计算：
- en: '![](img/00094.jpeg)![](img/00143.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.jpeg)![](img/00143.jpeg)'
